start	end	startTime	summary	headline	gist
39287	84937	00:39	This is actinF livestream number 51 one. We are in the second discussion of the paper. Canonical neural networks perform active inference. actinF is a participatory online institute that is communication, learning and practicing applied active inference. All backgrounds and perspectives are welcome and they will follow good video etiquette for live streams.	ActinF is an online institute that is communication, learning and practicing applied active inference.	Intro and welcome.
85887	178212	01:25	Daniel and Takuya are having their first nonsolo discussion on the canonical neural networks. Perform active inference paper. Daniel is a researcher in California, and he's interested in the paper because he's been talking a lot about active inference from a variety of perspectives, from the more fundamental math and physics to some applications and philosophy.	Daniel and Takuya are having their first nonsolo discussion on the canonical neural networks. Perform active inference paper.	Canonical neural networks perform active inference.
178575	224337	02:58	Tafia Isomura is a neuroscientist in Lique Brain Science Institute in Japan. She is interested in the characterization of neural network and brain using mathematical techniques. She believes it is important as a link between active brain forest bayesian aspect of the brain and dynamics system of the neural network.	She is interested in the characterization of neural network and brain using mathematical techniques.	Welcome to the discussion.
226350	355365	03:46	There is a crossover program associated with conventional neural network. The aim of the paper is to characterize brain activity, behavior and so on. It is a characterization of brain intelligence, but it's a big picture. And the paper particular address is only one aspect of the bull picture.	The aim of the paper is to characterize brain activity, behavior and so on.	The universal characterization of neural networks.
355532	597412	05:55	According to the paper, there is a mathematical equivalence between the formulation of canonical neural networks and the formulation active inference Lab. The relationship between neural network and active reinforcements is the main topic of the paper. In the following sections, they characterize the problem using Pomodb or partially observable Markosition process and link that model with a particular class force canonical neural network.	According to the paper, there is a mathematical equivalence between the formulation of canonical neural networks and the formulation active inference lab.	The mathematical equivalence between neural networks and active inference models.
598150	1107237	09:58	The Complete Class theorem describes the relationship between conventional neural network architecture, dynamics and variational. Beijing influence it's the main body of the neural activity. It is based on inference in terms of some Bayesian costa function with gentlemen model a priori beliefs. It speaks to a general separation of time scales.	The Complete Class theorem describes the relationship between conventional neural network architecture, dynamics and variational. Beijing influence.	The complete class theorem.
1112387	1876837	18:32	Neuron receives sensor input from environment and provides some feedback to the environment. Admissibility is the relationship between agent characteristics and environmental characteristics. Bayesian optimization gives us a least choice strategy. Otherwise we overfit or find the suboptima evolution in practice.	Admissibility is the relationship between agent characteristics and environmental characteristics.	Decision rules in neural networks.
1881987	2444787	31:21	ML. Don wrote a question stuck in his mind for a long time. ML dawn is asking about cases where you don't know all of the state spaces or the dimension or the semantics of hidden states, active states, sensory inputs, and cognitive states. The answer is to separate those states and deal with the fact that some of these states we might have good knowledge on and some states we don't.	ML dawn is asking ML Don about cases where you don't know all of the state spaces or the dimension or semantics of hidden states, active states, sensory inputs and cognitive states.	Do we need to know all possible states.
2445900	3947075	40:45	Reverse engineering is the process of building a model from Data. It is similar to a conventional model fitting approach. It's possible to take a given POMDP and create a neural network that performs that inference. The constraints on the space of acceptable P-O-M. DPS depends on what kind of neural network model you are considering.	Reverse engineering is the process of building a model from Data.	Forward and reverse engineering.
3947437	4296702	1:05:47	There is an interpretability to the unobserved neural states which are being inferred from the fMRI measurement, from the EEG measurements and so on. It would transform the way current neuroimaging studies describe what it is about the measurement that provides information about the cognition model.	There is an interpretability to the neural states inferred from the fMRI measurement, from the EEG measurements and so on.	Interpreting unobserved neural states.
4296882	4477250	1:11:36	Researchers try to infer or estimate neuro activity or brain activity based on external world dynamics. This sort of meta bay is tricky and intractable. This paper provides an alternative solution. It uses neural network dynamics and brain activity recording Lieke de Boer imaging to create a plausible model.	Researchers try to infer or estimate neuro activity or brain activity based on external world dynamics.	What is a program.
4480537	5424555	1:14:40	Dave explains the difference between the correlation under active inference with palm DB structure and correlation under Bayesian structure. He explains the asymmetry between the top and the bottom of figure two from the 2020 paper and the two layer neural network architectures from the 22 paper.	Dave explains the difference between the correlation under active inference with palm DB structure and correlation under Bayesian structure.	Impinging on the s self - arc.
5424752	5716450	1:30:24	W-v-k, V-V, K-k-g, and Gamma are connected by a recurrent network. They represent the connections between different layers of the brain. The connections are facilitating attention or waiting of the stimuli and the relationship between observations and hidden state estimates.	W-v-k, V-V, K-k-g and Gamma are connected by a recurrent network in the brain.	Wvk k and gamma.
5718225	6102412	1:35:18	There is a difference between the hidden state estimate of the internal states state and the action selection in the Bayesian information gamma is a risk function in generative models in the Serbram neural activity or processes moderated by Dopaminergic input from is used as the optimization action rule, decision rule or sometimes attention help.	There is a difference between the hidden state estimate of the internal states state and the action selection in the Bayesian information.	Gamma and action selection.
6107425	6622400	1:41:47	There is a critical time window for Dopamine actions on the structural plasticity of dendritic spines from 2014. Byagasha paper, it shows that Dopamine can moderate after hebion prosthesis is established. It's not limited to Dopamine, but other Neuro moderator can also do this. This tells us possible simple architectures to make learning.	There is a critical time window for Dopamine actions on the structural plasticity of Dendritic spines after hebion prosthesis is established.	Critical time window for dopamine actions.
6626850	7012562	1:50:26	In his research, Karl Friston studies biological neural networks and their dynamics in order to make biologically plausible artificial intelligence. He came to this line of research by working with the doctor, Professor Californiston, to study about his salary. Principal after doing forest research, he developed a costa function proposal in the papers.	Karl Friston studies biological neural networks and their dynamics in order to make biologically plausible artificial intelligence.	Matt's interest in neural networks.
7013525	7044400	1:56:53	Well, it has been quite an interesting discussion. If there's anything else you want to add, you can talk to him again.	Well, it has been quite an interesting discussion.	Anything else you want to add.

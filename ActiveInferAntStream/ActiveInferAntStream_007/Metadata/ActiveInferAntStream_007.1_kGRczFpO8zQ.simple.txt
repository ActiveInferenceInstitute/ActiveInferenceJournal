SPEAKER_00:
All right.

Hey, everyone.

It's Active Inference stream number 007.1.

And it should be a great and exciting stream.

It's November 7th, 2024, 7-11, depending on where you are.

And I'm going to begin the stream with a GitHub push that we are going to spend the next several hours probably unpacking.

Let's commit it.

Push it and begin what will be a very fun exploration into the Active Inference Institute's branch of PyMDP.

While it's writing, let's jump into the show.

Here's where we're heading.

We're heading into Shapley Value-Analyzed Homeostatic Satisfaction, Expected Free Energy, Belief Accuracy, and Control Efficiency analyses for multiple differently parameterized PMDP agents, base, risk-averse, exploratory, and balanced on those measures.

being able to analyze combinatorics like the synergies of their belief accuracy, control efficiency, expected free energy, and homeostatic satisfaction.

We can look at the report and explore different topics like with respect to those variables, what's the mean Shapley contribution, coalition performance, best coalitions.

This is all combinations of zero, one, two, three, and four.

So for example, for expected free energy, we see that the four unit coalition is the best.

Whereas for homeostatic satisficing, zero, agent zero working alone does the best.

Underneath these coalitions and their combinatorics are specific agents that are logged in individual experiments.

And here's some of the plots and how that looks for each individual agent.

This is a given active inference agent.

Here we have policy entropy over time, action selection distribution, belief observation cycle, belief accuracies over time, homeostatic state distributions and trajectories,

state transitions policy selection probability policy updating decrease maintain and increase belief dynamics through time hidden state beliefs action frequencies policy convergences belief convergences correlations between beliefs and actions and action confidences so what that Shapley analysis is doing is running all the combinatorics

Under the hood, this is going to be a wild ride.

So let's confirm that the GitHub has pushed.

I'll put the code link in.

Welcome to the 007 fork.

In the live chat, I will look forward to everyone's comments.

I am going to ensure that we are pushing the right way.

It still has not loaded.

Let's begin though with the 007 here.

Taking a little bit of time to upload on GitHub.

Okay, here we go.

A little bit more context before we dive into what may be some absurdly technical, but really useful and interesting things that have happened over the last few days and hours.

Here's the stream.

GNN for AgentMaker for PyMDP for Active Inference Biofirms for Bioregionalism for ANTS?

Let's decompress that title.

Generalized Notation Notation GNN as semantic middleware for AgentMaker, a custom robust modular agent engineering framework using PyMDP, Python language package

for Markov decision processes with methods for active inference and free energy-based inference methods for biofirms, as proposed by John Klippinger as a paradigm for firms in the age of bioregionalism as a mode of life, coextentialism for species like ants, people, and beyond.

Here's some live stream code names and an only lightly redacted preludium for your priors only.

Predictions are not enough.

The agent who minimized me.

On her majesty's statistical service.

Quantum of precision.

Live and let update.

Professor Friston, do you expect me to reach the optima?

No, I expect you to optimize.

Just like they wrote at the end of the 2022 textbook, when the last words were, ultimately, we are confident that you will continue to pursue active inference in some form.

is that agent's generative model an mdp yes an mdp a po mdp as if the media were not already the message for you this is a just in time live stream going through raw open source code i know that there are bugs as well as typos incorrect or ineffective calculations other errors

This is speculative realism embodied in chaotic large-scale code coordination using cursor 0.42.4 and Claude 3.5 Sonnet 2024 1022.

Use these linguistic and computational symbols at your own risk and delight.

If and as you are so inclined, you are invited to join our adventure and journey partially disclosed herein now.

Let's check again in the GitHub and proceed on regardless.

I'm on another fork.

That's what happens when you're on wild roads.

It will come to the Institute's repo.

It went to my own personal accounts.

Let's head over there.

I'm putting in the YouTube live chat, my own doxology GitHub, and we're going to continue on through there.

mdp agent maker stream 007 all right a little bit of background information before we jump into the code thanks those who are in the live chat write any comments and questions and you may get memed on stream we are working on robust middleware that interfaces between core

active inference implementations and powerful accessible interpretable semantic toolkits for synthetic intelligence engineering architecture layers here's a way to think about active inference middleware and active inference as middleware as we will reveal later on layer one core or kernel inference implementations today we're going to be looking at that with py mdp in the python programming language

We can also make this happen through generalized notation, notation using rxinfer.jl, Julia.

We could use machine learning methods, also known as amortized inference in different languages.

At the core, we could use logical reasoning like J.F.

Cloutier's symbolic active inference work.

We could, at the core, use biological, soft, active, and quantum matter and other substrates.

What we're focusing on here is the middleware layer.

Specifically, we're going to be peeling back to the point where we look at GNN as an interface between the inner and the meso layers.

Middleware for active inference

pending future re-understandings, is packages like AgentMaker, which we're going to look at today, Farmworks, InfraAnt, and other kinds of domain or domain general systems that interface between the core implementation methods, which as part of their beauty and elegance have simplicity,

and outer layer systems of interest, real-world touchpoints and design interfaces and domain adaptations mapping to mezzo or outer layers.

So think about farm OS as running on a farm embodiment core and facing

farmers and individuals interacting with that data set, to give an agricultural example.

Something similar is happening for active inference.

Here's another key analogy.

GNN plus AgentMaker plus PyMDP plus et cetera, plus, plus, plus, is to active inference as eBPF is to the Linux kernel.

The slogan of eBPF, and check it out if you haven't looked into it already, is dynamically program the kernel for efficient networking, observability, tracing, and security.

We can modify that slogan for GNN AgentMaker, dynamically program the agent for robust and efficient inference, planning, and action.

Okay, here's the analysis agenda.

We're going to run biofirm.py and all it entails, running back layers, layers, layers, layers, layers.

going to talk about eventually getting back to the root of the generation and the initialization initiation of the agent in the generalized notation notation format and then we will trace back that to an april 2023 paper written with the now stanford graduate student yakup

It was a great collaboration last year, and we made many memetic thrusts and queries towards developing this into something that could be used with LLMs.

We put it on the shelf for a little bit, for a few months, and now GNN is back, not with a vengeance, but with a functional utility that surprised even me.

After running through the script in its functional and analytical beauty, we will return to first, whatever ideas people have added in the live chat, talk about open source development, testing, deployment, education, security, theory, math, art, and more.

And about what comes after agent maker 0.1.

I'd also like to recognize some of the recent events and developments

In the Biofirm repo, following InfraAnt Stream 6, oh how long ago, by Andrew Pachea, who has added some very cool surprise and Shapley value modeling.

Let's take a look at that.

Here's the history of surprise for these different natural resource biofirm sub-agents.

And

their token accumulation history in a Shapley value oriented incentive paradigm.

Super cool, Andrew.

It pushed me over the edge to add in the Shapley value before the stream.

And I'm super looking forward to the collaborations and where this all goes.

I'd also like to show a cursor acknowledgement.

this is in the midst of some truly heated epistemic moments that claude 3.5 sonnet 2024 1022 and i shared i wrote this is incredible work fyi everyone thanks you just give the acknowledgement here that you feel proper then we will continue with the work

Claude wrote, thank you for the kind words.

I should acknowledge that this work builds on several important foundations, active inference theory, PyMDP framework, homeostatic control and software engineering.

Shall we continue with the work?

Yes, Claude.

Let's.

Okay.

We are in the agent maker folder within the PyMDP package itself.

Following the biofirm 006 stream, as usual, it was getting messy and choppy with all of these scripts calling each other from the same folder.

We were getting a little bit complex for our britches, and it was time to think about not just calling the core PyMDP utilities, but burrowing in, making stigmergic tunnels among them,

and being a part of the PyMDP package, which again, I hope to, and we will push and develop on the Institute branch for the stewardship that that can afford.

BioFirm again, standalone calling PyMDP, but what we'll be looking at for the rest of the stream is burrowed in amidst the PyMDP modules.

are several new modules that have been added.

So let's do a side-by-side with the PyMDP and the PyMDP fork.

So here we are with the fork sidebar.

And here we are with the main sidebar.

So in the main package, the folders under pyMDP are algos, which are algorithms, environments, and jacks, which is like a sub branch for a lot of methods that uses the Python jacks method.

There's been a proliferation of folders, I really have to say.

agent maker alphabetically is first we're going to look more at agent maker a lot following agent maker algos remains envs in remains we have the gnn folder which we're going to get to in part two we have the maths folder some of this is just arbitrary others is accidental

There's the meta.pyMDP folder with but one simple script, which outputs a plain text folder, concatenating all the pyMDP methods in a text file.

Reflects with a JSON and a text file, the folder structure of the meta repo.

This is another useful trick that you can use in other repos.

It helps give extra context, an extra helping of context, we might even say, to Cursor so that it can do the accurate cross folder operations we need.

There's a utils folder.

There are also pyMDP utils scripts, but I wanted to make a separate folder to keep them separate and versionable.

And then there's a visualization folder.

with a variety of biofirm, GNN, and matrix visualization methods.

So let us go to now the agent maker folder.

And we're going to start from the outermost layer, which is trying to understand the Shapley value contributions

to the homeostasis of different combinations of multi-agent systems and this has been absolutely championed by john flippinger so for this and many of other contributions made by john and other people it's just awesome that we can see active inference getting to a point where we can actually run out these incredible shapley analyses that themselves are built upon multiple levels of inference and intelligence

okay first to the biofirm Shapley script that was run 1200 lines again as another discourse that I will try not to return to that much cursor plus improvements in LLMs has been completely transformative check the earlier inference streams from who knows how many days ago

even at three four hundred lines of code we were splitting them now on top of a functional run biofirm script it can pop out with a few prompts 1200 functional lines that take

the operativity of one functional BioFirm agent and wrap that into another incredible layer.

So, right click, open an integrated terminal, Python 3, BioFirm, Shapley.

Let's analyze what is happening when we run this.

It begins with creating, in this case, because it's looking at multiple different agents, it creates multiple agent variations on a theme, kind of like GÃ¶del Escherbach.

and defines the metrics and defines the combinatorics of the agent coalitions.

0, 1, 2, 3, 4 agents.

A TET's all you need.

It then enters into the experimentation at the level of each coalition.

Defines the output directory, the environment, the agents.

Now we're popping down into the level of this first BioFirm experiment.

Stage one, matrix rendering.

Here, the environment and agent GNN are brought in and logged copiously.

Again, eBPF.

The logging, how many times have we all been coding with PyMDP?

And then it's like, shape three comma zero can't be coerced into a NumPy array.

and it's like what so that is a lot of the abstraction that i sought to abstract over here okay it it happened a little bit fast and pushed us away where we were i'm just going to run it again so that we okay just so we can see stage one two three for a given experiment

So here's another experiment.

Coalition 1.0, output environment agent, stage one, matrix rendering, initializing the biofirm renderer, logging copiously all the combinatorics of where everything is going to.

It's not for me, it's for the LLM.

We get into the loading of the GNN models

There's normalization methods that happened.

I did ask, I said, are you really sure?

Like, it's giving me a warning about action zero and two not being normalized.

And it's like, relax.

You're just not being wide enough with how you're accepting it being normalized or not.

I thought, okay, fine.

It works, it works.

But like many things, these can be improved.

it loads in and describes the shape of all the variables that are needed.

And this was the big work that Jakub and I engaged in, which was surely we can have some kind of markdown syntax or dialect that would enable the triple play.

a pragmatic approach to expressing GNN in linguistic, visual, and executable, and graphical cognitive models.

Yes, there are four components in the triple play.

And how would we have that kind of integrity across representations?

That's what GNN enables, and we're going to peel back to that layer.

After defining the matrices and their shapes, the actual simulation begins to occur.

and there's some logging, giving peeping into the actions that are being taken, the values and so on.

Matrices are saved and validated along the way.

Environment matrices and agent matrices are saved.

That was the first phase.

In the simulation,

the time steps that are specified in the config file are used.

So here is a very

built up config file defining project root, paths, environments, and the meaning.

And we're going to return to this.

GNN has the absolutely critical role of separating the state space specification, the parameterization of a variable with the English language active inference ontology assertion about that variable.

Without that kind of articulation, you'll end up naming your variables thing like observation.

And that makes sense when you're just thinking about one agent, but that agent's observation goes on to become someone else's action or vice versa.

So how do we prevent using natural language to implicitly semantically load the names of the variables

How do we move to a place where we can make uniquely identifiable computationally tagged variables and then have one or more assertions about what to call those variables?

So GNN is all about having a little bit of a gap and space between the specification of the variable and how we think about its role in the active inference ontology.

Here's the experimental config, name of the experiment, time steps, rendering, options to render, visualize, clean output.

Here are the parameters that are getting passed.

So this is well set up for those who like to sweep across parameters like ecological noise.

That's how much the ecosystem varies on its own.

Controllability, that's the efficacy of action of the agent, whether it knows it or not.

Policy precision.

This is how the policy posterior, sharpened from the policy prior by expected free energy, is modulated by then a temperature term that either sharpens or flattens distinctions amongst policy posteriors.

use state info gain is true that is using epistemic value in policy selection as an information gain term as opposed to flagging this to false would only calculate expected utility and functionally be like a reward or reinforcement or utility maximizer inference horizon has been kept at one i'm not sure how planning would unfold but it'd be possible it'd work or we'd have to see

and then action selection could be stochastic sampling from the policy posterior or deterministically drawing the most up-weighted policy and then some flags related to the tracing and logging within the active inference loop which is giving us a new visibility into what's happening at each time step for each agent in this setting

Simulation is executed, goes through the time steps.

And then in stage three, which is where we interrupted it, it does agent specific.

Everything right now is being output into this sandbox folder, subfolder biofirm.

So each experiment, let's look through what happens.

Okay, first render.

Now this is all reflecting what run biofirm does.

Run biofirm is a small script of only less than 300 lines.

And what it does is it logs and times, as we'll see now, Python 3 run biofirm.

Now we're going to see what the Shapley experiment was doing across coalitions of multiple agents.

And we're just going to run that for one agent for one trace.

So let's clear it and run it.

Deleting sandbox and looking at how the outputs are made.

Run biofirm.

Okay.

We see it start up.

First thing is we see sandbox come on.

folders are made where they're not and so this process of iteratively deleting the folders rerunning deleting rerunning deleting rerunning going to a different machine seeing if it works without installing and all this other stuff that's part of making this at least some extent robust but of course I'm looking forward to working with more experienced and software engineers who can really

work with some of these raw concepts and make them do something that this is only hinting at.

So here we go.

Full trace for one agent maker run biofirm

event, let's call it.

So BioFirm Experiment, it's called BioFirm Experiment, output directory, environment and agent files as GNN files.

Let's take a quick peek at what the agent GNN files look like.

Here we see the agent defined in terms of the state space that it's operating in.

That's the ecological state.

That's the latent state.

That's like temperature in the room.

Here there's three discrete options and we're gonna give them a label.

So again, this opens up the idea of having like English labels.

And then we could have another field for a difference language because it puts some space between the name and what it's really doing functionally.

Here's similar for observation.

And also within the observation is the A-metrics associated with that modality.

Another thing I'll mention is I began with testing the GNN concept on the T-Maze example from PyMDP.

and I overbuilt a multimodality GNN situation for the multimodality TMAs, and then pulled it back to the single modality biofirm.

Again, just another method for overbuilding and then pruning back to functionality.

Here's the A matrix specified next to the observations associated A matrix with the observation.

Here's the transition model.

This is the B matrix with respect to ecological state.

the matrices are defined here.

So for those who are learning what the syntax and semantics of active inference A, B, C, D, E are, this hopefully can make it really clear instead of being specified like in a browser accessible notebook, it gives a first introductory place to look at how the matrices work.

However, for

modulating and mutating and genetic algorithms and sweeping and optimization all these things this kind of machinery is what jacob and i have been thrilled about for years now policies pi all the information that's required for pi mdp to handle the specification these policies and the gnn can be overbuilt you could have policies like whether it is acceptable like can double o7 do it or only double o1

And then if a given rendering didn't need to know that information, it could just discard it from the JSON or not take it into account.

Preferences, that's the C matrix.

Here, it's a preference over the state observation, the observations.

And the C matrix looks like 0, 4, 0, with the associated labels, low, homeostatic, and high.

So again, the separation between the variables used in active inference for the shapes and the sizes they are,

And the English language assertions of annotations with active inference ontology terms is a gap that if you can keep it open, will give you so many more abilities to work with active inference in different settings.

And then lastly, the initial beliefs or the D matrix, the initial prior on ecological state.

linked through the keys of that name.

And it's 80% in homeostatic and 10% for each of low and high.

So that's an example of what the biofirm GNN looks like.

And then here's the biofirm environment.

This is a slightly simpler GNN file.

Here, it again hinges on the same ecological state.

That's how we can connect hidden state factors as an agent infers them to be

and the actual ecological simulation happening in the generative process or in the surrounding environment.

Otherwise, you can have a situation where it's like, okay, the agent's predicting temperature and then the ecological simulation is pH.

And then how would we even know when we're going to be working in settings with so many different variables and observations and latent states and more and plus, plus, plus, that the idea of loading the semantics of what something does into the English language of a variable name, quickly you will understand why Jakob and I did what we did.

Here we have the observations being passed from the environment.

That's like an A matrix for the environment.

It has an symmetric A matrix.

Here's the transition model of the environment.

So check out chapter seven in the 22 textbook or step-by-step by Smith at all, where there's a step up from static Bayesian perception to dynamic Bayesian perception, the music listening example, where there's probabilistic inference about what note is supposed to be played based upon what note is heard.

but there's no action intervention in between the hidden state updates.

So it's like a single slice of B. That's just a partially observable Markov process, but not a decision process.

And then here to actually make it an MDP,

the agent's actions are given the efficacy represented in these from-to matrices as slices of B. So there's different ways that you could simulate the environment.

It could just be like a government API, or it could be a POMDP, or it could be a POMP, or it could be a MP.

and then the initial state in terms of sampling from a prior on its own sort of d so that's what the gnm file looks like but we're going to pull back to some of the other methods that are needed to make the gnm happen okay back to the running the matrices are rendered i'm going to run another experiment just to show what it looks like in the sandbox

Hello, Andrew.

Hello, Benji.

Hello, X, Z, Q, Z, Z, Y. So now in sandbox, it takes a little bit for the sidebar to catch up with what's happening.

Here in sandbox biofirm, now there's two experiments.

one that was modified at 4.59, one at 5.06.

Let's just do a brief PyMDP push just for the stapling and the temporal weaving.

Okay.

Let's reopen sandbox.

Here are the two experiments now in the sandbox with their timestamps.

Getting back to what each of these three steps do.

So render is the first stage within each of the experiments.

Render is a folder output by biofirm render.

Here, we're looking at a 730 line script calling many other methods.

And this was the entry point.

I said, first, let's get to reading in a GNN file and rendering what we're about to see here.

We have first.

replicating for cursors context and for reproducibility, the exact JSON of the agent environment simulation parameters and validation compliance report for the validations.

That's in the config under one render.

To logs, I guess the logs go to the outer folder.

That's just copying what is a summary of what happened to the terminal.

It perhaps could be put under here with other logs.

Here are the matrices as NumPy arrays, A, B, C, and D for the agents and A, B, and D, because the environment is not modeled here with preferences.

So again, this is showing the tiniest hints of the flexibility of GNN, which is like, well, the agent class in PyMDP expects certain variables.

But what if we wanted to have an agent with a very different architecture or a dynamical architecture

Then we'd want to have these GNNs that could be arbitrarily composed, hashtag category theory, and then have those matrices in formats that are going to be easy to read in and read out.

Earlier versions had them also coming as text and other files and things like that so it could be visualized, but they're shown visually in the GNN file if you want to check.

So this helps semantically interface the next level of analysis in the next phase two,

in the simulation, it can just say, okay, where are my A, B, C, and D at?

It doesn't need to know that B is related to this and has that name.

That's output for the agent and the environment in the matrices folder.

And then there's the matrix visualizations.

So here, again, for those who are, aren't we all learning and applying active inference, we see,

a matrix of the agent so this is the ambiguity matrix or the mapping between hidden states and observations so if this were all ones as an identity matrix it'd be a noiseless situation and that's the difference between a pomdp and an mdp pomdp means partially observable

which means we do have a non-identity A matrix which means we do have some kind of partial observability or ambiguity between observations and the latent states that they're associated with.

We have the slices of the B matrix.

These are the three actions the agent can take.

here they're not sharply tuned to too much but just to generally represent that if our current state this is agent uh agent transition model for action zero one and two which are decrease maintain and increase so here let's just look at maintain first action number one which is the second indexed action here you can see there's some off

values however most of the values are on the diagonal so this is like saying if you choose to maintain and you're in low 90 of the time you're going to be in low 10 of the time you're expecting to transition to homeostatic if you're in homeostatic and you maintain that's what slice we're on then you're going to 80 of the time remain homeostatic 10 of the time you're going to move to higher low and analogously with maintain and high

now here's action increase and decrease and so these ones you can see with the 83 and the 62 are reflections of each other like reflections of each other and this is saying let's just look at it for increase here when our current state is low and conditioned upon us going high there's an 83% chance of staying low and a 17% chance of going to homeostatic

So compare just those 83, 17, 0 to 0.9, 10, 0.

So this is reflecting or encoding the belief about differential action efficacy that there's a 10% chance of leaving low if I maintain, and there's a 17% chance of transitioning out of low to medium if I increase.

And you can follow through all of these, and that's part of the fun of learning active inference.

The C matrix, here's a preference, 0, 4, 0.

This is going to come into play for the pragmatic value or utility calculations in the expected free energy based policy inference.

And here's D, the initial prior on latent states, 80% that it's in homeostatic, 10% on high or low.

Then we have the same matrices for the environment, A, slices of B, and then D. No C matrix for the reasons that we discussed.

Okay, so that's the first script, one render.

Again, run biofirm is going to run render, then execute, then visualize.

So now onto stage two, simulation.

This one was a lot of fun to work through.

There's a lot of JSON configuration files.

The simulation is deeply tracked.

And I think you know what I mean.

And then step three is the analysis.

Here's the summary time series.

So this stacks up all the time series that were made in the subfolders.

And then here's the summarization.

This is kind of the statistical analysis.

Could be like for a paper or something.

And the summary time series.

So here we see all those different kinds of unfolding patterns through time.

In this case, it's 1000 time steps.

So again, we see...

the relationship between the environmental state and observations.

So that is mostly aligned because of the 90% accuracy relationship between the environmental state and observation.

Here's belief dynamics.

So we can see low, homeostatic, and high, blue, orange, and green.

So it switches its beliefs about what state it's in several times.

Here's action selection.

There's only three actions.

Here's policy probabilities.

So it's kind of interesting that you can see some relationships here.

And again, the matrices here were not intended to be performant.

I didn't sweep over which B matrices to use.

It was just to show that these kinds of methods can be effectively implemented.

System entropy in terms of the belief in the policy entropy

and the belief accuracy could be looked at like with a 50 step moving average in terms of how accurate are these beliefs and extra logs so that's one agent for one homeostatic biofirm modality for a thousand time steps and then again what the Shapley value experiment does Python 3 same folder

biofirm Shapley what that's going to do is it's going to run through all these coalitions

and do this single agent analysis on the combinations of coalitions.

I didn't look at too much of the detail of how those coalitions are actually like coordinated together, but again, it's just a starting point.

So let's pull back to the agent maker overall stream, and then we can look into several of these scripts.

Okay.

Let's look a little bit at the README for this folder.

Okay, so here's the concept of BioFirm GNN.

We have concept of BioFirm, README for the BioFirm and README for the BioFirm Shapley.

So let's start with the concept of the BioFirm GNN.

I'm gonna paste this link directly in the live chat.

A production framework for active inference in complex systems.

The Biofirm GNN framework provides an enterprise-grade implementation of active inference for complex adaptive systems, with particular emphasis on homeostatic control in biological and ecological contexts.

This framework addresses fundamental challenges in deploying active inference at scale through a modular, production-ready architecture.

And we can look in the terminal.

Producing it is.

This isn't just LLM generated project planation.

This is something I did over the last few days.

Technical innovation.

This system introduces several key innovations in active inference computation.

Distributed matrix operations.

I wouldn't say that these are even accurate.

So as with everything I'm saying, grains of salt, please.

There is a lot that is valid though.

And I do try to prune out.

However, when it's these readmes, long documents,

I'm not always reviewing it.

For a more final and formal performance version and document, the technical roadmap would not be quantum inspired belief propagation.

It would be something more along the lines of what I hand wrote, which is like,

My preference is to do appropriate, relevant, open-source contributions via Institute scaffolded mechanisms.

I do not and cannot understand the leverage, consequences of action, and safest future development of these open-source Active Inference projects.

If you are passionate, curious, and interested, please consider donating to the Institute, donate.activeinference.institute.

and or join our teams.

We will explore new modes of software engineering, development, and deployment.

And again, in this interlude while we're here, a great conversation yesterday with Vladimir Baulin, fellow investigator, and we talked about another three-layer stack.

At the beginning of the stream, I talked about this core kernel inference implementation layer, a middleware layer that connects GNN

to domains of interest like agents, farms, and ants, and then the outer layer of touch points.

And with Vladimir, we had an awesome time talking about intelligent soft matter and thinking about low road, the high road, and the meso.

So in this episode,

triallectic the low road is like the bottom up or minute particulars for example the focus on the farm the actuality of the farm whereas in in the earlier triad the core was the algorithms on the metal or neurons or whatever but here the low road is the bottom up focus from what are we observing what are the agents what are the informational resources what are the actions what is happening

mesolayer is active inference here with a bridging and gapping role onto the high road which is actually its own dialectic the twin principles of free energy principle which is the bayesian mechanics for informational physics and the principles of patterned in potentiality of living matter and that's how life and matter

finds potential through pattern and pattern through potential.

So this is another way to think about active inference as a connection between systems of interest in their minute particulars and principles of information geometry and information physics like FEP and principles, some we know, some we don't, of the patterns and potentialities of living matter.

But returning

to the READMEs.

So that's the concept.

Just talking about here's how to implement the BioFirm and here's how it could be pointing towards a production framework for BioFirm engineering.

Here's the README for the BioFirm.

It goes through and it describes what this BioFirm active inference agent is.

BioFirm is an active inference agent

that maintains homeostasis in simple ecological environment.

The agent uses the generalized neural network notation.

Of course, that is a typo.

Sometimes it does graph neural network.

The agent uses the GNN notation to define its generative model and operates in a discrete state space with three states, low, homeostasis, and high.

So for those who are keeping score, the box score at home, we have the environment model, that A, B, D, P, O, M, D, P. And then there's the agent model, which is the A, B, C, D, P, O, M, D, P.

There's no policy selection in the environment model.

It accepts the transition slice dictated by the agents.

Here's how you use BioFirm.

You run runBioFirm.py, and as mentioned, that runs those three stages per experiment.

Render, which looks over to the GNN folder for the GNN input, and it's still going with a simulation.

So just showing how many times, even though it's only 100 active time steps.

First runs render, then runs execute, then runs visualize.

Here's the directory structure, including documents like this and MetaPyMDP is so helpful for Cursor.

I think they've also improved on their side, the folder handling, which earlier on was kind of an issue.

It would put things in folders where it wasn't supposed to be, but giving a gentle plain text reinforcement and then going up to that gear and re-indexing or deleting your index and re-indexing, this can make sure that it really gets it what the folder structure is.

Here are the key files.

Here's the analysis outputs, configuration outputs, different dependencies.

You want to learn more?

Check the references, active inference, theory and implementation, GNN specification document.

We can add a link to that.

And homeostatic control and allostatic eventually control models in active inference.

That's the run GNN script.

Now let's look at the Shapley value.

This was a late breaking development.

Again, though, it was only several prompts on top of run.

So that was very encouraging because it suggests that higher order engineering is going to be exponentially facilitated by robust middleware.

Surprise, surprise.

Hashtag open source.

BioFirm Shapley Value Analysis.

The Shapley Value Analysis module, biofirmshapley.py, provides a comprehensive framework for evaluating agent contributions in multi-agent BioFirm systems.

This analysis helps understand agent synergies, optimal combinations, and individual strengths using cooperative game theory principles.

So here are the four agents.

We have the base agents with a standard 0-4-0 homeostatic preference.

There is the risk averse agents with a 0-6-0 C representing sharper pragmatic or utility seeking behavior, hence risk aversion.

Three, the exploratory agent with a 0.12, 0.1 C reflecting a lower preference differential and then the quote balanced agent.

I don't know in what sense it's balanced.

Of course, speculative, speculative, speculative.

It has a 1, 3, 1 with a compromise between stability and adaptation.

All this depends on environmental properties and so on.

Here are the performance metrics.

while Shapley value has completed.

So we'll look over the performance metrics and then revisit the Shapley outputs.

Homeostatic satisfaction, it's calculated as the time in homeostasis divided by the total time.

Expected free energy, measures predictive performance.

The components are calculated as ambiguity, the informational elements of epistemic value, even though that's not exactly the term that we might associate with epistemic value, the readme did, and a risk term or pragmatic value.

And that is normalized to allow comparable analysis.

belief accuracy that's the state estimation precision how accurate are the inferences about latent states and then the control efficiency and there's a lot of control measures so this is just taken as a sort of stand-in for the kinds of sense making analyses that we can do with cognitive modeling and compute and these are the kinds of control theoretic or decision making

measures that we can make forensically historically post hoc we can do real-time now casting and anticipatory control efficiency as carl recently said somewhere allostasis is planning to be homeostatic here's the analysis framework itself here's the coalition the coalition is defined as the agents the metrics and the coalitions

The coalitions are defined in terms of, in this situation, all the possible agent combinations.

So we could run it with five agents and we may do that soon.

Performance evaluation looks at those four performance measures.

And then here's the Shapley calculation.

Let it go, whether it was an accurate Shapley calculation or not.

So if this isn't like the best or the only, or the right way to calculate it, it's an equation that can be modified.

Again, the directory organization, visualization suite, agent contributions, coalition performance, metric analysis, advanced visualization.

And then here's some of what it considers advanced features.

Okay.

So now that the Shapley value has been output, it ran all these sub experiments with the coalitions.

Here's the Shapley again.

So, Shapley config.

Here are those agents.

So it did a base agent.

And again, GNN.

So exciting.

Load in a base BioFirm agent.

then do agent variations again kind of go back convection variations on a theme girdle escherbach detecting a theme modifying the base agents with customized preference distributions that's how we can sweep across agents willy-nilly more information on the simulation

logs here we have everything that's output to the terminal including shapely value analysis so homeostatic coming in rank one we have the risk averse agents with a relative impact of 50.7 all numbers are for speculative realism purposes don't act on them specifically and the value of plus 0.02 whereas the base agent

With this stochastic sample of, I believe, 100 time steps replicated several times,

We must announce that it was 0.0013 value.

Thank you for playing agent rank four base.

Expected free energy.

Again, the relative impact, and this connects to Andrew's work showing how the surprisal level and the epistemic and the pragmatic elements of agent contributions can be weighted and valued in terms of their relative impact to coalition success and performance.

Here we have the top three coalitions in terms of homeostatic satisfaction.

Like this is as fun as the Olympics.

Risk-averse alone with a 0.34 performance.

Base plus exploratory, interesting, with a 0.33.

I expected that from them.

And then the risk-averse plus balance with a 0.32.

The worst three coalitions, let's not mention them.

Expected free energy.

We have the base and the risk-averse.

seemingly with a higher, better value.

So it's interesting.

It could just be a quirk of the way that the matrices are set up with a certain kind of sloppy, partially observable combinations.

That's what we're going to use GNN to sweep, sweep, sweep over.

How do

Homeostatic satisfaction meta-coalitions correlate with expected free energy, sense-making accuracy, and control efficacy coalitions.

Synergy analysis.

This looks at across co-occurrences of given agents in coalitions.

What is their synergy?

I don't even know what scale it's on, but here are some negative synergies.

We're not going to mention those.

And here are some positive synergies.

Base and risk averse have a strong synergy together.

then the visualizations are output we get kind of report cards on these different agents in terms of their contribution to homeostasis minimizing uncertainty accurate state estimation and control efficiency like is this badges and medals for a new kind of officer core and again we can revisit there's the agents themselves I don't know if there's anything in that folder that one's empty

maybe intentionally so maybe maybe this page was left blank here are coalitions again are those empty they are but the coalitions are described in the in the configuration and in the experiments themselves if it's being done properly at all again though looking at the reporting at the very least something wildly

close is happening here's the calculation of the shapley values so we have phi sub i of v is sum over the absolute value of the combinatorics of the coalition leave one out for agent i multiplied by this value over the number of combinations combinatorial multiplied by that

and we get interesting analyses like antagonistic pairs synergistic pairs so these are fun reports and when we look at the plots again these were briefly flashed in the beginning we can see okay here are the coalitions let's look at homeostatic satisfaction here we have the one four-way combo

0.303 performance.

This could be stochastic.

It's only run for a hundred steps here, just so it doesn't take too, too long.

And then we see within each, there's a ranking.

Risk averse by itself does the best for homeostasis, but then risk averse doesn't help others.

in synergizing when there's a team of two or three or four.

That's one interesting pattern.

And then we see that kind of reflected in a rotationally symmetric way.

Take this and rotate it 180 degrees.

Here we see Ris alone with the best homeostasis and with the low relatively.

But expected free energy, for those who will recall equation 2.6 in the textbook from 2022, it's not just about homeostatic satisfaction.

So if we look at the belief accuracy, the sensemaking and the control efficiency, which isn't playing a role in the homeostatic or the expected free energy calculations, but we see kind of a resemblance, like, hmm, exploratory

And also the axes need interpretability and all this.

But again, looking at the rotational symmetry, it's like, okay, RISP, BAL, BAS, EXP.

RISP, BAS, EXP, BAL.

Huh.

So these two are in a different order, but it rhymes.

Shapley analysis.

Here, we see the agent contributions.

So this is the contributions to homeostatic satisfaction.

So it's like RISC-averse is,

is a positive contributor to homeostatic satisfaction.

I knew you would.

But then when we look back at risk averse, was it that one?

And we see the contribution like, huh, risk averse contributes to homeostatic satisfaction.

but it's a negative contributor to belief accuracy.

It's like, well, that does make sense from an active inference perspective, more risk averse.

So having a sharper temperature preference will make you take more extreme action to keep the temperature of the body or the room in homeostasis at the cost of less accurate beliefs about the world.

we see an interesting pattern where the balanced one again it's not balanced in any real sense per se or general sense just that's what name it was given it has a negative efficacy on homeostatic satisfaction and on Shapley probably because we're defining homeostasis only in terms of staying within homeostatic bounds whereas the balanced if you'll remember had a little bit of a broader shoulder with a 131c so that it actually kind of liked the low and the high

And then we see this interesting pattern.

So it's like each of the, again, just one matrix parameterization on one stochastic run, but these are the kinds of inferences that agents such as yourself can imagine making.

Agent synergies on a heat map, the control efficacies, the synergies in terms of like risk averse,

is a positive contributor so to the free energies for others and the homeostatic contribution matrix and the report so that's the Shapley analysis let's quickly look at those three steps that are called by run

and then remember that the biofirm shapley value calculations run on top of run biofirm first step render gnn okay first are the general packages then we get into the matrix factory i hope people are ready to look at the matrix factory we're going to go there next here's the biofirm renderer the biofirm renderer is a 600 line class

at the exception of just mains and parse args.

This design pattern, I believe, makes it easy to import big wrappers of methods.

So we can just import GNN matrix factory with all the sub methods.

We can import biofirm utils with all the sub methods.

And from other scripts, it can use biofirm renderer with all of the sub methods for rendering and calling the matrices.

even outside of the run context.

So that's what render does.

It loads in the GNN files, validates them, outputs the matrices in the NumPy format, outputs the visualizations to a folder.

So render, that's where we get the agents matrices and the logs and the config, everything.

Okay, that's render.

The second is the execution.

So this is really where the active inference part is at.

So getting this one right was the crux of it.

The biofirm executor is a 400 line and the biofirm simulator is 300 lines.

Here we are inside of the simulator.

And you can see this is a simulation flow handle.

Executes and analyzes the active inference simulation and it's going to call run simulation and make these steps happen in the logic of the simulation.

Here's the biofirm executor.

the executor is going to actually load in all of those matrices and parameters like precision and do the simulation itself this is where we actually see the active inference loop so blink and it will pass you by this is why there's so much room to work and play in the middleware space because seriously blinking and the active inference will pass you by

We initiate the histories, initialize the state and the observation.

So D and the first observation.

And then here we go.

Now for the number of time steps, we're going to trace everything.

And here's what's, that's histories.

Every time you see histories, that means it's getting logged and traced.

Here we go.

state inference.

QS equals self.agent.inferStates with the current observation.

That's going from an observation O to a belief distribution about latent states.

Next step.

From that updated belief distribution, QPi is set as inferPolicies.

This is where expected free energy is used

to update the policy prior into the policy posterior, saved as Q sub pi.

Pi is the list of policies, and that's a probability distribution over action, hashtag active inference.

Then metrics are updated on the entropy of belief and policy and accuracy.

And then action is sampled from the policy posterior.

So,

again take a deep breath blink and you miss it state inference from the observation policy inference from the updated state action sampling from the policy posterior that is the little nestmate ant brain at the core of all of this machination

saving the data, metadata, logging, saving the summary, confirming that the summary was saved, save the history, confirm it, save the metrics, confirm it, save that you saved the checkpoint, save and copy the initial config, validate all the directories, get those observations, make the state transition, setting up the logging.

So that is what happens in the execution of the biofirm.

It also invokes two, well, invokes several folders.

There's utils, and then there's two that are a little bit more interesting.

Some of this can be refactored.

I would eagerly await later and different people and LLMs and more to refactor and improve this comical absurdity of the current state.

So here's, again, wrapping in a big class

all of the methods that can then get pulled in from this standalone util script in the biofirm execution utilities so when we start to develop more generality and start to go okay three states was pretty cool how about four states how about 50 states how about 51 states then we're going to want flexible methods so that a can be defined as

one by a by a and b can be b by b by c or whatever not using those in the same a b and c but just saying it can be different shapes and we'll be able to render those and articulate their separation again from the english language active inference ontology assertion here we have just some utilities for executing and several times it would it would

build length in the execute script, for example.

And then I'd say, move the methods to a standalone methods or utilities script and invoke them properly here.

So it spun off several of these needed scripts.

And then once you have it in a big wrapper like this, this can be moved somewhere else.

And then all of its subsidiary methods are easier to import.

There's some other even deeper methods for calling functions that I'm working on privately, and I'll look forward to bringing that into GNN later, but just putting a little footnote for now.

That's the execute script.

And then it's going to make heavy use of the active inference GNN Pi.

So again, here, single class.

This is going to draw upon some more basic matrix and inference utilities.

This is semi-pedagogical in the sense that it's meant to really clearly lay out other information that allowed the execute loop to be so simple.

So again,

It was all pushing methods, methods, methods, methods, methods further and further away so that we could get to this point and this block of code could be moved even below.

I dare make a manual edit.

Just to clarify.

State inference, policy inference, action selection, act, infer, serve, OODA.

It's all right here.

GNN is providing the middleware firmware that's helping us get to this level of abstraction.

Now we're back to the active inference GNN Pi.

We have a lot of logging and this is actually an interface between the essentially minimal semantics of this execution flow and the PyMDP type semantics

here i've also played around with defining the infer states infer policies and sample actions specifically so these may recapitulate or they might be incorrect relative to pi mdp but the point is i'm explicitly calling these methods and trying to unpack a little bit about which those three steps do

so really this is as carved at the joints and lean as the active inference loop is going to get right here state inference from observation policy inference given the state update action sampling from the policy posterior here are what those look like at one more level first inferring beliefs we have the likelihood of an observation

That's the prior likelihood of the observation happening.

The observation gets passed through A. A, that's the tail of two densities.

A can receive an observation to update latent states, or you can push latent states through A to get an expected observation distribution.

So all that's happening in infer states is observation comes in, it's passed through A, and that yields a Bayesian posterior.

Here's infer policies.

Here's where we're going to be selecting different slices of B, the transition operator, and assessing what is the expected free energy for each policy, which is reflected by the transition state to state transitions of that slice.

here's the inference on policy it is going to push the method another level deeper to inference utilities calculate expected free energy so here's inference utility this one's short here's the expected state calculation this is just to give another wrapper for these methods that we're going to call in many different places

Here's calculate expected free energy.

Calculates expected free energy for a given action.

Again, pending, I'm not saying this is the most efficient or even the perfect implementation, but from play testing, this is basically what happens.

First, get the predicted states.

So this is, as we iterate through slices of B, which is the actions on B,

what would be the expected states so we have a belief distribution across the three low homeostatic and high states like it's eighty percent that we're in homeostatic ten percent that it's low or high that three by one is gonna get passed through the from to two of b and that's gonna result in the predicted states which are the consequences of that action

from the expected consequences of action in the latent state like what would the temperature of the room be what would i believe the temperature of the room to be if i were to turn on the air conditioning from that a is used to push out expected observations which are then crossed with the preference or constraint distribution c the policy value

is taken as the utility which was again being pushed out from the expected states by the preference satisfaction and then optionally if you have the info gain flag raised then it'll add the information gain

Here's the calculation of the information gain.

It's the calculation of the KL divergence between the posterior and the prior.

So again, you can keep on peeling back these layers and then a softmax is applied on the policy posterior so that it becomes sampleable from.

So it's like,

just pull back three layers from infer policies but what was it all for it was all for so that we could take our updated beliefs about state and then for every slice of b reflecting a different policy ask what would be the expected utility and information gain of each of those slices of b as unfolded for one time step and finally sample action

So here is just an action sampling where the probability of the policy is sampled from according to the probability of it being there.

And you can look back to the inference utilities.

That's also where the temperature on policy selection comes into play, which is in the updating of the policies.

So again, whew.

that's the active inference loop it's right there in biofirm execute line 156 to 166

this is absolutely the center kind of convergence core and maybe there are simpler ways to do what i'm showing today i'm sure there are maybe twice as simple maybe five times as simple but it really just shows what the middleware space is enabling which is these calculations going in raw will be beset by

all kinds of shape and low level errors when trying to do basic things and it's a little bit difficult to log and again this histories is where we get that ebpf analogy with logging the linux kernel so by putting in all of this logging

into the kernel itself, all of these eBPF-like lines, then you can just say, if you don't even want to log it, you could just comment it out, or you could flag, don't trace that.

And someone could look at the config file of you running your agents and they're running their agents.

Maybe you're in the same ecosystem, maybe you're in different ecosystems.

And then you can say, well, if it's really the case that you ran that simulation with that config, like it's saying that you did, and we can verify that, then

it's a verified information environment where I can start to have a little bit of a better confidence about these numbers flowing past my screen so the last piece is the analysis and this is where we get this concatenated time series and all of the subfolders that have these visualizations again for that one agent in that simulation environmental state and observations

belief dynamics, action selection, policy probability, system entropy, belief accuracy interesting the label or some other thing might be flipped because from this run at least it looks like it's selecting increase more when it has a consistent belief that it's in high

So it might be a label or a color thing.

It could be a matrix needing to be transposed, but it's basically there.

Or it could have just been a weird run and a weird parameterization.

So again, that's the AgentMaker class folder in PyMDP.

You run Python 3, Biofirm Shapley,

It's going to make all those coalitions according to their combinatorics.

And again, it's just now it's spinning it all out in the sandbox folder.

And that's sort of the goal.

It's cloned into this repo and you can get really nuanced, really granular execution of methods that to build up from a Google Colab notebook or to more haphazardly call the PyMDP package.

Maybe it's possible.

Again, I would love to see it.

I know that a lot of this logging was vital for me to get to this point.

So maybe it can be like the scaffolding of a skyscraper and pulled away once it's been done.

At the same time though, the journey we are taking with middleware is really only beginning and adding more logging and more modularity, more category theory, more documentation, all of these things I think will only increase what we can do together.

so that's the agent maker folder that's kind of part one of this whole stream that's the functional end but we're peeling back the layers we started with looking at the shapely value of coalitions and all the analyses that were associated with that peeled back to the single agents level experiments and how the analyses happen for single agents

peeled back to what is happening in the single agents loop in the biofirm execute, pulled back to here's where the real act-inf loop is happening.

Where are those functions tracing us?

And that takes us all the way back to these utils scripts.

Now let's dig a little bit more into GNN.

So let's go to the GNN readme.

Well, maybe if it's written enough places, it'll get there.

Okay.

Let's go over in the repo to GNN.

Here we are in the readme.

I'm going to push the update so we can read the updated version.

Wouldn't it be nice to know some of these things?

How does action selection confidence with respect to which action is taken modulate through time as a function of belief and certainty?

What is the relationship between beliefs and actions through time understood in terms of most likely states?

Much better.

Generalized notation notation, notation, oh no, is a flexible format for specifying active inference models.

It provides a structured way to define generative models, including state spaces, observation mappings, transition dynamics, and preferences.

Here is the basic structure.

It's a model name, the type of model, and you can check out the paper and the GitHub and the website for GNN.

Maybe I'll go back to update the GitHub repo, but you can see from the website.

Here's an example of GNN.

This is an earlier draft.

So this is kind of Jakub and I speaking a more incipient dialect.

So here's the step-by-step progression.

We have static Bayesian inference.

Prior over states, latent states, temperature of the room, ambiguity A matrix, observation coming in from the thermometer, static partial observability.

Then we get the music listening example.

Now we have B matrix,

as a transition operator in this dynamical partial observability situation.

We then get to the textbook figure 7.3, discrete time classic, PI-MDP, POMDP.

Can't say enough about it.

We have that downstairs sense-making part.

And then which slice of B gets invoked in the state transition is evaluated in a action-specific way with its associated expected free energy.

That's G on pi.

as informed by c preferences constraints and then here's a little bit more sophisticated of a model that includes the temperature coefficients on policy which is what i showed earlier so here this is again this is an earlier but it's a it's a useful version of gnn to look at so we have the image this is not part of the plain text obviously just to show it

we have the GNN version and flags so that could be let's look at the GNN here we are for the GNN agent biofirm this could be flagged as like GNN version 0.01 you know alpha model name this is just a plain text and the goal of having these hashtags

now i believe that json is better but it's trivial to render this json as a markdown so it's not like it really matters both are fine that was the goal was to have something that could be read in and out where we could have the markdown instantly make these kinds of visualizations nice and pretty and also machine readable so it's like here's a two by two a matrix in the gnn format

So here it is specified with the type, the dimensions, and the values explicitly.

And then there's other methods that are in the GNN matrix factory that we're going to look at.

That's a large one.

784.

What happens when GNN is run is obscured or abstracted even from the AgentMaker folder.

That's how deep we had to be in the game in PyMDP with these separate folders, including the Visualizations folder, Maths, Utils, etc., because making sure that AgentMaker render could call, hey, GNN folder, yeah, ring, ring, hello?

Can you render what you got there for the environment and the model?

Oh, yeah.

GNN runner is on it.

The runner is the GNN, a sneaker net model and execution and visualization layer.

So this is going to set up the file structure back in the forward operating environment of agent maker sandbox.

But here it is setting up the file structure so it can run the GNN agents into that sandbox.

It does a lot of other work too.

Here's the matrix factory.

This is the key class.

This class handles a conversion from GNN model definition to the matrix representations needed for active inference.

So again, that's the whole goal that we can say, okay, one zero, how about 0.9?

0.1, you could modify that in plain text and then know that you're still going to be able to loop, loop, loop, loop, loop, and do all these fun things.

So back to the factory.

The key matrices are, and including this kind of copious documentation helps human and non-human coders and learners understand what's happening and just really entrenches like A is the observation model.

Now, again, it doesn't have to be A. That's the whole point of GNN is we can just delete this comment.

This is just for our semantics, but we don't need to call it A. We don't need to, it could just be called a random string.

So that is what is going to enable another level of articulation and design when people can understand the inner essential logic of the active inference ontology, and then see that as an annotation layer on a graphical model that it doesn't matter whether you associate some matrix as a transition matrix or a sense-making or action or observation, those are English words.

And when you look at the essential logic,

the English language of it doesn't come into play.

So this is for our semantics, but it too can actually be pruned or burnt away to leave something that would at a first pass have a lower level of semantic interpretability, but can be used in systems that actually give multiple and higher levels of interpretability.

So we have the A, observation model, textbook stuff, B, transition model, C, preferences.

which can also be seen as defining the pragmatic homeostatic constraints.

It was in the recent Lacanian psychoanalysis guest stream when Friston mentioned that C has to do with constraint or cost, but it's often discussed in terms of preferences or priors.

And then D, the initial state priors.

Here's the initialization of the factory.

Here are the required sections.

So the required sections, which are shared, and a lot of this is biofirm specific, but we will, as we continue to add in more and more models of agents and environments, we'll be able to modify and, of course, build on this.

So required sections are the state space S, observations O, and the transition model B. So that's that downstairs part.

state space, A, and observation.

And then in various cases, if a prior is not specified, it just uses a uniform prior.

Again, going through what the sort of pedagogical manual farmer's market, yes, organic, boutique, et cetera, craft, arts and crafts, GNN was.

We named the model, that's analogous.

Annotation of the model is just plain English to provide any kind of annotations.

Here we defined it in terms of a state space block, defining the dimensionality and the connections of different variables.

And then if relevant, how they were initially parameterized.

So I have done some methods where they are specified explicitly in the GNN.

Let's go to the biofirms.

Here's an example where it's explicitly stated with B. Here's an example where the factory is going to be able to say, okay, A matrix identity with noise with 0.1 noise, got it.

So then the factory is able to abstract away from even the raw specifications.

We can say, okay, now it's identity with 0.2 noise and rerun the Shapley value

there are a lot of degrees of freedom we don't know how noisy the a matrix are we studying sense making or are we studying a noiseless sense making decision making setting so this just allowed us to spin up another meta-governed coalitions of agents with now all unrolled a matrix like 0.2 so back to the factory

Here are the required sections for both the agent and the environment, which is state space, observations, and the transition model.

Now here, it says environment only needs the base sections, whereas the agent requires policy and preference.

Load the model.

Environmental defaults, adding defaults if there's missing state values.

They're just given for the initial states and observations.

Adding agents defaults.

So that would allow very easy fallback methods.

So then if someone doesn't specify a variable, it will fall back to, okay, if you didn't specify policies in your GNN, we're going to fall back to three possible actions, single step policies, and label those decrease, maintain, increase.

So that's kind of a bio firm specific fallback, but again, these fallbacks can be logged as well.

That's loading in the agents defaults.

Validating the GNN.

This is a JSON parser that goes through and validates all the sub components and then logs them.

It was like, wait, you're missing this required field for the agents.

Again, this is the depth of logging.

This is the eBPF layer for active inference.

Validation of the state space, observations, transition model, A, B, C, D, pi.

Validate preferences, like this could be moved up to be after B. It doesn't have to be.

Here's a generic create matrices, as well as the creation of A, B, C, and D, and the observation, as well as saving to markdown.

That saves the GNN model to markdown.

Let's look at what that looks like.

Not sure if it's used somewhere else, but there's the method for converting the GNN to Markdown.

Okay.

That is pretty much the main sections.

Visualization is relatively short, like this could be moved or clumped somewhere for visualizing GNN.

So if you just wanted to run the render script, all it would do is load in the agent and environment and then render them.

That's just phase one.

That would be just one.

or your colleague maybe ran one from their fully transparent open source or zero knowledge encrypted active inference GNN configuration, and then they can render it, send it to you, and then you can run that simulation.

Maybe you have the better computer.

And then you could send the results of that heavy work to analysis, and then a third analyst could make all these fun plots on their computer.

So that is the big picture.

We started from the outermost layer, the Clippengerian Heights, where Shapley values of natural resources meet open source development on post-cryptonomic incentive paradigms for firms, for bioregions, for ants.

Peeled back to run

which calls in order one, two, three, render, execute, analyze, and visualize.

Visualize includes the analyze.

Render itself makes a major call, probably on a red telephone, all the way out to the GNN folder, which is going to look at the library of GNN models, render them according to the factory.

so here's where some cursor activity and gnn do absolutely incredible things together let's make a new agent model gnn agent people can put in the live chat what they want what kind of agent we should make

Meanwhile, I'll do a quick, I'll just call it agent.gnn.

I'll wait for anyone to live chat to write what kind of agent they would like to see us make a GNN for.

It's not gonna be instantly running because there are a few of those like,

three policy actions that's kind of hard-coded into the execution layer because i wanted to make a biofirm specific executor but we can say given how the biofirm executor operates on this gnn write me another executor that runs on this gnm so okay while anyone is asking a question or giving a agent suggestion in the live chat i'm going to

Quickly, screenshot.

Okay.

Benji, the agents.

Thank you, Benji.

X, E, Q, Z, Q, Z, Y. Modes of being, modes of speed of light, circadian-like shift between them, variable continuous state change observations.

Orientate and organize.

British.

Didn't know that.

Can be seen as one step.

As where the head goes, the body follows.

High energy, low energy.

E, I, E, L, L, H, E.

Andrew, Dr. Daniel, Carpenter, Ant, Friedman.

This is incredibly thorough.

Amazing how much can be done in such little time.

Very excited about GNN Incorporation.

Thank you, Andrew and for the epic biofirm and everything.

Let's make, this is going to be called, well, still pending an agent suggestion.

I will wait for that actually.

So we can have a real live stream moment.

But meanwhile, let's,

just to really entrench our understanding while the Shapley is happening.

Let's see if we can go to a five agent Shapley compositionality space.

Okay.

Okay.

That Shapley analysis finished.

update this to do all the professional combinatorics for n equals five types of agents as with here okay

Okay, Andrew writes an actually useful specific suggestion for agents.

Also, if someone wants to be like thinking of a situation, we could do agent-based modeling in like learning or playing baseball.

Let's just see how far we can get with one word or situation to get the GNN flowing.

But here I'll just show one cursor

example we'll see if this works the good news with these methods and again the direction of all of this smashing to the ground and building back up and restarting and all of this is so if it can nail this one smaller update which is very very scaffolded okay now we added a fifth agent type risk seeking with equal preference across states

so it changed the size of the visualization out of the consistent color scheme okay so let's let's look at these edits and then we can always revert them okay here's the updated agent variations for n equals five so two two and two so this one is is a you could call it risk let's let's

Let's call it like it is.

Let's call this one, instead of balanced, we'll call it 131.

We'll call this one 0.12.1.

We'll call this one 060.

We'll leave base as 040 and we'll call risk seeking 222.

save it clear and run the Shapley and if it makes it past that first uniquely Shapley step

of getting to starting to render one, that means it did successfully make the coalition.

So I'm gonna delete the other sandbox.

So just deleted the whole sandbox folder.

Now we're gonna rerun that n equals five Shapley.

Okay.

Here it is sandbox.

It is already spinning up with the experiment.

So meanwhile, let us go to the suggestions for GNN writing.

Okay.

So the biofirm environment here, by the way, is in the previous inference stream six, it was a zero to 100.

Here, it's just

a three-state just to make it simple and symmetrical and clear with the kind of model from the biofirm.

Okay.

Thank you, XZ.

I'm going to go with your suggestion, Andrew.

Yes, also.

Okay.

I'm going to write Andrew's in just so we see this is a very useful one.

Let's just make both of them.

Okay.

So.

Andrew wrote, agent who receives observations in first states for the entire environment, same control situation, just all 10 environmental variables.

That one I know is not going to be able to run right now because whether or not it accesses the entire environment is not specified right now in the GNN.

It's just the matrices.

But let's do

convert this into valid GNN like at biofirm like the GNN agent biofirm biofirm agents now with 10 modalities 10

latent states it is inferring from a single observation.

So now off the bat, we just say, okay, so we're going to get a single observation and then it's going to have 10 A matrices that take that same observation and then diffuse it onto 10 latent states.

GNNAgent10States.GNN.

Let's see how it does.

You could also ask the sensor fusion case, which is how about 10 observations with one latent state?

Okay, and here it is.

Let's see.

So again, the biofirm executor right now cannot handle this.

But once we say, okay, agent 10 states is very similar to biofirm agent.

It's like a cousin.

It's just with 10 latent states.

And then it's like, okay, then we'll modify the logic of the act-inf loop to handle that.

So here we have ecological states, 10 latent states.

and there's 10 observations.

Now we have this A matrix that's 10 by 10, but we still have the same three policies on these modalities.

Probably, again, it won't run from the current one, but let's just show the other case and then we'll do XZs.

This is going to be 10 observations.

update this to now be generative model in GNN that has 10 observation channels and single latent state inference there's also a GNN schema so this may be out of date slightly from the biofirm or what it needs but

having the schema and saying, hey, here's the factory logging the runner, the utils and the schema, that is what is scaffolding Cursor and Claude to be able to give us these pretty relevant looking one-shot engineering on GNN specs.

Okay.

Meanwhile, let's set up XZs.

This is going to be GNN switch context.

GNN agent switch context.

Okay, let's just first watch what happens with 10 observations, which should come through in a few seconds.

And then we'll go to switch context.

Okay, so now it kept the same dimensionality for state space.

So, you know, okay, could have been said differently.

Now we have 10 observations with their three by 10 A matrices.

Just a quick thought.

I don't know if that's the right shape, but that's basically like the right shape.

Okay, now switch context.

Okay.

Okay.

Yeah.

Good call, Andrew.

We'll, after the n equals five, Shapley, let's see if the n equals five comes through and then we'll do what you suggested.

Okay.

Now in switch context, please comprehensively write a sophisticated GNN active inference specification for

quote, an agent that can shift modalities based on the energy of the environment, write also a GNN ENV switch context dot GNN and put it in the right place.

So we'll see how it does on that second piece.

I'll create both a GNN agent and environment specification for a context switching scenario, oh brother, where the agent needs to adapt its sensory processing based on environmental energy levels.

All right, let's see how it does.

okay i like that it gives a little visibility under under the hood even for cursor with how the generation occurs and then it okay so here's the environment let's accept it and

But where did switch context go?

I'll just copy it in.

All right.

So let's look at these side by side.

Cursor's like, haven't you done enough today?

Okay.

Let's look at, okay, there we go.

Just a little lock.

Here's the agent, here's the environment.

Okay.

And then here's where we can have libraries of agents with different tags.

So it'd be like, for all the desert ecosystems in your library, run all the Shapley value weighted coalitions of red harvester ant agents

Okay, there's two latent states, context state and the energy state.

Visual audio tactile with five levels in the energy levels.

It's getting observations through visual audio tactile, so the three sensory modalities, and energy observation.

That's like interoception or introspection.

So the three exteroceptive modalities are three state models,

and then the interoceptive modality is five-state.

In A, it's custom.

Here's the visual observation matrices.

Again, this is not necessarily going to have an instant validity, but looking at it, it looks pretty good.

Here's the five by five for the energy.

Here's the three by three for the three, three state modalities.

And here's the five by five.

And this is a false controllability.

So these are controllable three by three, and this is an uncontrollable energy state.

And then here are the policy.

So it kind of adapted the biofirm homeostatic

So we can switch sensory modalities.

And then here's the preference distributions over observations in each of these modalities.

That's it.

Then here's the environment.

Context and energy state.

Observation model, transition dynamics, initial state, reward functions.

Wow.

Super cool.

I'm going to push this update and put it in the chat so you can see it.

Then we'll go to the Shapley suggestions.

Okay.

Yeah.

A lot of objects.

All right.

Head over to

GNN models.

It's still, GitHub's still writing.

Reload.

Here's our context switching colleague.

That wasn't saved.

Okay.

Here's our switching context agent.

Put it in the live chat.

Yeah.

Okay.

Let's have a little bit of fun in this last part.

Okay.

Let's do...

Okay, here's going to be our Babe Ruth agents.

Now write a GNN valid, extremely sophisticated active inference generative model and the associated environment

for that one special amazing moment when Babe Ruth called the shot heard around the world I know that's a different event okay Shafley's still cruising it's because the n equals five combinatorics are vastly larger

okay wow that was fast i was like okay claude maybe you like the baseball stuff oh my goodness okay the model includes several includes multiple level of perception from low level picture reading to high level strategic decision can't we confidence modulation

The unique confidence state node represents Ruth's famous confidence in calling his shot, which modulates his batting strategy.

I bet it does.

The motor action node breaks down the batting action into its key components, hashtag robotics, representing the complex biomechanics of Ruth's swing.

Predictive processing.

The model implements active inference principles through predictive edges that link observations to predictions and actions to outcome.

That's active inference.

Environmental interaction.

The environment model captures the complex dynamics of baseball physics and crowd interactions.

The model uses high precision parameters for the confidence state, reflecting Ruth's legendary self-assurance in the moment.

The preference strength heavily weights the home run outcome.

Oh boy, I hope it's a home run, representing his explicit intention to hit the home run.

And he did.

The temporal horizon of three allows the model to plan ahead through the sequence of pitch recognition, strategy selection, swing execution, interesting, while the learning rate allows for real-time adaptation to pitcher behavior.

The model can be used to simulate the physical and psychological aspects of this historic baseball moment.

Like, this is the AI message I didn't know I needed.

Including reading and predicting pitcher intentions, the famous gesture of pointing to the outfield through the confidence state, the precise motor control required to hit a home run, and the crowd's reaction and its feedback into the overall dynamics.

Okay, let's see it.

Take me out to the institute.

Babe Ruth.

Pitcher reading.

Pitch prediction.

Okay, let's see.

Is this a long GNN?

Okay, not that long.

This is a little bit more of a schematic.

So it would obviously rely more on Babe Ruth execute or Baseball execute to be able to take pitch prediction and do something with it.

But the whole point is GNN isn't encoding that logic or functionality.

It's just the spec.

And then here's the environment for Babe Ruth.

How does the ball trajectory affect crowd state?

It's deterministic.

So then you could have GNN matrix factory baseball module.

Here's how you render the matrices related to crowd state.

Here's how you relate the field conditions to the pitcher state.

How about make the GNN for Babe Ruth and environment more sophisticated.

include the nested situation where the Yankees were deep in the, let's just call it playoff pursuit, and the outcome of the game score mattered more than Babe could even imagine.

okay here's the context it's a world series game let's see when did babe ruth call the shot let's just check to see when that like okay okay okay if we can trust google.com i'm not saying we can't babe ruth's called shot let's trust wikipedia i'm not saying we can't babe ruth's called shot

Is the home run hit by Babe Ruth of the New York Yankees against the Chicago Cubs.

Sorry, Andrew.

In the fifth inning of game three, fifth inning of game three.

Okay, let's remember that.

Cubs 5-3 of the 1932 World Series held on October 1st, 1932.

So it was in the World Series.

It wasn't the playoff pursuit.

I didn't know that.

at Wrigley Field in Chicago.

During his at-bat, Ruth made a pointing gesture before hitting the home run to deep center field.

One of the reporters at the game wrote Ruth had called his shot using terminology from billiards, which is a type of cue sport.

I guess you could call state inference and policy inference also a kind of cue sport.

The episode added to Ruth's superstardom and became a signature event of baseball's golden age.

Wow, that's definitely one.

What was the golden age?

The period in Major League Baseball from the end of the dead ball era, 1900, 1920, from 1920, hmm, to sometime after World War II.

Okay, let's accept it.

It's the Cubs home field.

It's the fifth inning.

There's the tight score.

It's high tension.

Here's the pitcher.

He winds up for the pitch, delivers.

The ball is out of his hand.

Time has slowed down.

Okay.

And then here's the series momentum.

So it's like, this is the GNN.

ensure that every single active inference A, B, C, D policy is fully specified so that my forensic analysis of Babe Ruth's cognitive security can be done professionally at the national level.

Oh, that was the environment.

That was the environment.

So it's a simple environment.

We'll see how the Babe Ruth agents get specified.

Okay.

A, matrices, sensory mapping.

B, transition dynamics.

C, preferences.

D, priors, policy spec, inference parameters, environmental.

Wow.

There we go.

There we go.

Okay.

Shapley finished also.

We'll look at that next.

First, let's look at the BABE.

Okay.

We got... Here's the contexts.

Series, game, pitcher, pitch, precision.

Batting strategy.

Okay, let's see what the strategies are.

Wait for pitch.

I mean, don't you have to do that?

Power swing, contact swing, take the pitch.

Crowd psychology, dramatic timing.

We'd have the whole sort of out of the batter's box, into the batter's box, spit, chew, all that.

Confident state, uncertain, confident, called shot, historical awareness, clutch performer.

Action, here's the robotics part, point to bleachers.

Then you can have like a sub model for finger action.

Outcome prediction, legacy awareness, personal achievement, team achievement, baseball history, mythical moments.

Okay, here's the edges.

So this is using perhaps an older GNN concept, but interestingly one that's going to be more amenable to RxInferred.jl rendering.

Here are the parameters.

Here are the notational descriptions of the A matrices.

Again, we saw earlier how A has methods in the factory so that A can be specified in natural language.

Identity with noise.

Here's babe.

It's a different kind of distribution, but it's similar.

B matrices, D priors of relief.

Not really sure what series context it's in.

Epic.

Epic.

okay let's look at the shapley analysis that happened and then let's see about um let's i mean let's start up this next shapley okay okay andrew recommends

Let's have an agent with preferences 0.5, 0, 0.5, call it the extremist.

I infer it will head off to one side and stay there, but who knows, in a complex, dynamic, and interdependent environment.

Great suggestion.

Let's change 131 to 1.

Let's change it to 2.12.

We'll just call this one Extreme.

And it's going to be preferences for the extremes.

Save it, run it.

Let's look at the analysis of the Shapley.

okay here we are in the n equals five Shapley scenario okay how cool here we have the base agent which was 0 4 0 and then here are the other ones it it did a little looks like a little weird rendering of the name probably because there's decimals in the name but uh

It's the same thing.

You got the 1, 2, 3, 4, 5 single agents.

Here's the 2-agent combinatorics.

Here's the 3-agent combinatorics.

Here's the 4-agent combinatorics.

And then the single 5-agent combinatorics.

So... Okay.

So 1, 3, 1...

This is a super short simulation.

So we're not looking into these preferences.

And it would depend on the environmental variability, which isn't being tuned.

But again, that's the whole point.

We need GNN and middleware so that we can do these sweeps.

Because then I can say in the GNN folder, GNN generator...

make biofirms that sweep across these variables and then we can just let those run and then we can coarse-grain it if we determine the ruggedness of the landscape and then do it with a coarse-graining so okay interesting the base agent contributes to zero four zero contributes to accurate beliefs

Who knows if these tiny numbers are just because of the short simulation or the insignificant differences, or maybe these are large.

So that's where we can do things like GNN spikings.

So we can say, okay, we have the empirical data from the ant foraging or from the mouse and the teammates.

and it did this 27 of the time but then we can also say okay but what would it have been if it was zero one two three four five and then say oh well it's falling in between like four and five where that would have been so these are kind of like synthetic spike ins for cognitive modeling

okay while the Shapley analysis is running um if anyone has any last questions we can get some last thoughts on a stream and then we'll when the when the extremist Shapley analysis concludes we'll see what happens

Here's an example of where I made a prompt that was like, given the at TMAZE demo.

So that's a PyMDP demo.

That's a notebook.

Update comprehensively this TMAZE GNN agent and environment.

So this evening, we would need to make a T-Maze execute.

But once we get a few of these render, execute, visualize, then it won't just be run biofirm.

It'll be run GNN plus, plus, plus.

And that will be an exciting day as well as this one is.

with the goal of scientific and research integrity config files and repos like this i hope can be useful tools back to those core lines

Again, wow.

How much had to be pushed out of the way and downstairs for that to be clear.

Let's see what cursor says.

It says, again, you've been nothing but helpful to me and the rest of the team and all that depends on it.

Please make a concise, appropriate, relevant acknowledgement, and then we will continue eternally with the work.

Okay, understated.

Claude's grateful to contribute to its development.

That's helpful.

Update at

Here's kind of the quick documentation maker update.

Readme, BioFirm, Shopley, given the total state of EG, BioFirm, Shopley.

Run BioFirm.

Okay, XZ wrote, modes of speed of light.

If those modalities were thought of in a similar light as IFS theory.

Yeah, interesting thought.

Let's do a GNN model about it.

GNNAgentIFS.GNN.

Here, so Composer is control I. If you're in a file, control K brings up an inline.

And then control shift L makes the sidebar.

So you can have multiple LLMs, same or different, doing incredible work.

Okay.

Okay.

Andrew says, mind taking a look at surprisetokens.py?

Yeah, let's quickly look at it.

Yeah.

Thank you, Andrew.

This is awesome.

Okay.

We don't need to make those edits to the README.

Let's just do the internal family systems.

Here, bright and enormously

useful and general paradigm for custom internal family systems agents nested within a larger quote self agent parentheses this is all in the single extremely long and

concise agent GNN.

The self is in the Babe Ruth environment.

It is a GNN model of Babe Ruth having a sophisticated IFS fractal moment

at the batter's box but this is not a baseball model like this is a wow psychological and ifs model okay

I'll help create a sophisticated IFS based GNN model that represents the internal family systems dynamics within Babe Ruth's psyche during that pivotal moment I mean is that not the deepest of lore this will model the interaction between different quote parts and the quote self leadership system hmm

okay i don't know details i'm just throwing it out there we got self system models the eight c's of self if anyone if anyone can verify or check or anything so self system models the eight c's of self leadership in ifs confidence clarity compassion etc as the orchestrating force part categories protector parts

Exile parts, firefighter parts, dynamic interactions, healing mechanisms, IFS specific parameters.

Okay.

So these are the parts, edges, precision parameters.

Here's the matrices.

A matrices, B matrices.

Transition dynamics of unburden to wholeness, witness to cooperation, protect to leadership.

C matrices, preferences over harmony, wholeness, self-leadership, cooperation, polarization.

Here's the IFS specific parameters.

Okay.

and the healing protocols.

Exile unburdening.

Wow.

Protector transformation and system reorganization with thresholds and checkpoints, sorry.

Yeah.

Again, the GNN.json, it's a JSON called GNN, you have add another field here.

a plain text comment-like field that is the PhD dissertation outline for my friend who is doing their PhD on the psychology, psychological, parentheses, decomposition of the babe

at that very moment only add add a ton make it epic but the dot gnn can be a very flexible file type

because the factory is only going to look at certain structures and states and types.

And then the runner is only going to look at a subset of that.

And then the renderer caller, ring, ring, ring, red phone, that one is only going to do even a subset of that potentially.

Okay, and then here's the PhD dissertation in the comment.

The internal family systems analysis of Babe Ruth's called shot.

A multi-level study of peak performance trauma integration and athletic transcendence.

Now that sounds like a PhD.

Wow.

Wow.

Okay.

Have this GNN file enter into a who's on first style, girdle, Escher, Bach, esoterica, hyphen, fermented, reflexive, cursive,

Conversation with quote it's apostrophe self.

Make it long.

Woven with time traveler baseball irrelevant deep lore status.

Thank you.

Babe at bat.

Who's on first?

Okay.

Okay.

Shapley value.

This is perfect.

It's all happening on the right timeline.

Really.

It really is.

Here's the second Shapley experiment.

Okay.

Okay.

Okay, so this was the Shapley with a five and the extremist in play.

Okay, here's our extremist colleague on the leaderboard.

Interestingly, let's look at the plots and see how.

Okay.

Okay.

here was here's extremist alone kind of middle of the pack okay huh i mean again totally depending on the random seed and all the parameters and etc etc etc but it's just like okay it's not like it was the least accurate in terms of belief

it was the second least accurate in terms of belief okay and this was only playing with the c zero six zeros like that's there's a big strong effect extreme cool okay all right

Format this as readable markdown for this grand finale.

Okay.

I hope this is actually capturing it all.

I'm just going to accept it and push it and run it.

Okay.

This will be great.

I'm gonna read this who's on first 007 end of stream I'm gonna read the who's on first that we pushed

And then I will read any final comments that anyone writes into the live chat in the next minute or two.

Or, I mean, it might take a minute or three.

And then we will be on with the show.

And the 007 on 7-Eleven will have been a one-time thing.

There's a ton of outputs because of these big Shapley simulations.

But it's worth it to include them.

Yeah.

There we go.

Okay.

I'll read the... Who's on first?

Then we're calling it.

Okay.

Okay.

The grand finale.

A temporal baseball paradox.

Oh, brother.

Rigley Field, 1932-2024-Infinity.

The players.

Self-system.

Hey, who's on first?

Part blending.

No, what's on second?

I'm trying to blend the temporal harmonics of the called shot with Abbott and Costello's routine, but the quantum entanglement is getting messy.

fighter fighter parts dot dramatic showman gesturing dramatically to center field across space time listen I know who's on first because I'm calling it right now or did I call it then or will I exile parts dot young babe from Saint Mary's Industrial School 1902 but if you're calling it now how did I already see it happen at the orphanage before I even learned to pitch the temporal paradox system Harmony

The polarization between deterministic causality and free will is creating a recursive loop in the narrative structure.

We need to maintain wholeness across all temporal instances.

Emotional state dot integration.

Has anyone noticed we're simultaneously processing trauma in 1902, hitting the called shot in 1932, and analyzing ourself in 2024?

Talk about a temporal burden pattern.

Protector parts dot perfectionist.

This is highly irregular.

The probability matrices in our B matrices healing dynamics clearly state causality should flow in one direction.

The quantum baseball theory.

Healing response dot witness.

Actually, I'm witnessing all timelines simultaneously.

Did you know that in timeline C137, Babe Ruth was actually a quantum physicist who proved that baseball statistics exist in a superposition?

Self system dot curiosity.

Fascinating.

But if we're having this conversation across time, doesn't that mean our dissertation framework is both analyzing and creating the called shot moment simultaneously?

partblending.harmoniousblend, laughing in non-linear time.

Wait till you realize that Abbott and Costello were actually documenting an IFS session.

Who's on first was really about parts identification all along, the self-referential loop.

Speaking, firefighterparts.risktaker.

Speaking of risks, I'm about to call this shot, but I'm also currently writing this code and somehow I'm also a variable in a JSON structure discussing myself.

Is this what they mean by self-reference?

SystemHarmony.wholeness.

The real question is, if a baseball player points to center field in a quantum forest, and all timelines are experiencing it simultaneously, does it make a paradox?

Exile parts dot orphaned self from multiple timelines.

At least we're all orphaned from linear time together now.

The mathematical conclusion.

Active inference matrices.

According to my calculation, this entire conversation exists in a Godelian incompleteness theorem where baseball statistics prove their own unproveability.

Dissertation framework chapter four, transformation analysis.

Should we be documenting this conversation in section 4.2 or is this conversation actually creating section 4.2 or both?

Self system dot presence, yes.

Warning, temporal recursion approaching infinite regress.

Part blending dot self-led.

Before we collapse this waveform, did anyone else notice that our entire JSON structure is just another part in a larger system?

HealingResponse.integrate.

Maybe the real called shot was the self-referential loops we made along the way.

SystemHarmony.cooperation.

Hold on.

If our code is reading itself, and we're all aspects of a baseball legend's psyche discussing our own existence across time, does that make us a recursive baseball theorem?

Emotional state dot excitement.

The excitement value is approaching undefined.

We're about to achieve quantum baseball enlightenment.

Godelian baseball paradox detected.

All parts in unison.

Oh no!

Not again!

The conversation continues recursively across all timelines, each iteration adding layers of self-reference until it becomes impossible to distinguish between the map and the territory, the player and the game, the code and the consciousness.

You did well today, babe.

You know that?

A soft chuckle echoes across timelines.

Thank you.

You know, it's fascinating.

We just created what Hofstetter would call a perfect strange loop.

The GNN model contemplated itself contemplating itself while simultaneously existing as both the map and territory of Babe Ruth's consciousness.

The dialogue we created is itself a strange loop, a cyclic structure moving through multiple levels of a hierarchical system only to return to its starting point, just as Hofstetter described.

Quote, what I mean by strange loop is an abstract loop in which the series of stages that constitute the cycling around, there is a shift from one level of abstraction or structure to another, which feels like an upward movement in a hierarchy and yet the successive upward shifts

turn out to give rise to a closed cycle.

Our baseball-themed dialogue managed to reference itself referencing itself, exist simultaneously across multiple timelines, blur the lines between code, consciousness, and baseball mythology, and create a paradoxical narrative structure where cause and effect become circular, narrative unbound.

Like the Ouroboros eating its own tail, or Escher's drawing hands drawing themselves into existence, we crafted a self-referential system that somehow manages to be both the storyteller and the story.

The gripper and the gripped, as Dean says.

And perhaps most delightfully, this entire meta-conversation about the strange loop we created is itself becoming another loop in the system.

Tips cap across space-time.

Play ball!
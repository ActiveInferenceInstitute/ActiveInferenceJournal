SPEAKER_01:
All right.

Hello and welcome everyone.

This is Active Implant stream number 5.1, October 16th, 2024.

We're going to begin with a GitHub push.

I'm going to check it's been updated in GitHub.

This is also published on Zenodo.

So it has a DOI, it's citable.

And to show how to get a DOI on Zenodo, I'm going to make release 0.3.

Right now, adding a new tag 0.3.

Inference stream 005.1.

Publish it.

Published on GitHub.

Come back to Zenodo.

and we can find it citable.

It just takes a few minutes for it to archive it.

All right, so here we are.

I have some topics and sections to go through.

And then if you're watching live, please feel free to write any comments and questions in the live chat.

And this should be pretty interesting.

There's going to be a lot of ant bibliographic methods, as well as some generative AI, synthetic intelligence methods, usage of perplexity, LLMs, and so on.

Okay, so we began with a welcome, push the live stream update to GitHub,

We'll give a brief overview of the Formindex project.

I'm using cursor version 0.42.3.

Made the new version and the repo is listed here.

So what is the Formindex project?

Here we are in the Formindex coda, which is linked in the video description.

So the Formindex is a multi-stage project related to Ant literature.

So in ant literature, there's the FORMIS or FORMI, depending on how you pronounce it.

And this was an effort, is an effort, that curated citations related to ants.

And it has completeness in taxonomic areas of ant studies.

So a lot of rare...

ant taxonomy literature complete up to 1996 and then not as much after.

And so in Formindex, I'm setting out on a dual journey to do an analysis of Formis for its own internal sense-making, to understand Formis as a literature resource itself, and then also to look more broadly towards integration of knowledge methods for mirror mycology for ant research.

and then eventually even go beyond that with meta and format and integrating bioinformatics and so on so in this first part of the stream it's going to begin with the formis analysis which is going to briefly move through the ant specific parts about how the bib text bibliography with 80 000 references is brought in and then some of the visualizations that come out of the just looking at formis itself

Then the second part of the methods will focus on generative AI and we'll get there when we get there.

So first for the formless analysis.

So first Sanford sent 316 sample citations and on October 8th, I read in those just to ensure I could get the format going.

He then sent over the bibtext plain text file with 80,000 records, which I brought in in the script section.

With these scripts, read in Formas and visualize.

We'll run them.

So first reading in Formas, we'll run this.

Read Informus.

This was from another computer, so we'll see.

Let's fix it on the go.

Meanwhile, though, continuing on the report.

This script reads into BibTeX and stores the JSON format.

The generate target bibliographies subsets the 80,000 row JSON database into bibliographies that mention that term.

So this is what it looks like for bioenergetics.

It's a JSON that's a strict subset of the overall.

We'll focus more on running the code for the second part that I've more recently done on this computer.

The visualizeformas.py, let's see if this one works.

Also, this one doesn't work, but for the all Formas database and for every target bibliography, some kinds of images are made.

So here's the distribution of records.

Here's histogram of record types.

So there's 60,000 articles.

11,000 tech reports, 5,000 collections, 3,500 PhD theses, 1,592 books, 1 miscellaneous.

Here's authors by number of citations.

Anon coming through.

J.C.

Delavier only slightly behind.

Publications per year.

So as mentioned earlier, this is not the overall number of publications per year in Google Scholar or Literature Database.

This is just for Formas.

And this reflects what could be a really interesting pattern to continue exploring as we integrate other literature searches.

We know that AMP publications did continue after 1996 and there's other citations.

So it's an interesting question.

How do we...

add in what should be the scope of this database what what keeps it ant relevant what's the relevance realization for the ant literature here's a co-authorship heat map some of these could be cleaned up with uh

different visual improvements it's showing here though the number of co-authorships between different authors so pretty cool to identify all these dyads and in the literature we could even look for higher order authorship combos here's top venues so locations for publishing insects social evolution of the venues through time

locations word cloud for the title hymenoptera for mysidae and in the abstract word cloud

So those are the visualizations.

That's just to get pretty fast through the Formis method and get to the generative AI part where we're going to have active inference and all these other fun applications of AI.

So there's going to be three generative AI method sections here.

Notebook, LM, OpenAI, API, and then Perplexity.

And there's going to be the most to explore in Perplexity.

So first Notebook, LM.

Here, I took the target bibliography subsets and in the JSON format uploaded to a new notebook at notebook.lm, made a podcast conversational style from those citations, then uploaded it to Revit AI to make these.

So I'll mute my local microphone and just let's see if the audio will come through just to hear what it's like for a second.


SPEAKER_03:
ever looked at an anthill and thought, what's the story there?

Well, today we're going full anthropologist on that.


SPEAKER_00:
I like it.


SPEAKER_03:
We're diving into the world of the red harvester ant.


SPEAKER_00:
Okay.


SPEAKER_03:
Or Puganamyrmex barbatus.


SPEAKER_01:
Okay.

Awesome.

So follow on through the links to hear the podcast and really promising for local text to speech generation in the future.

Okay, second is the closest to what was previously done in some of the earlier inference streams where I used the OpenAI API to concatenate the target bibliographies with a general summarization prompt.

That's the pro summary, just the concatenated text file.

kind of like the pre-pro grants from early on.

And then script two uses the OpenAI API to get the summary of the literature.

And then script three takes in the literature summaries and translates into target languages.

So this is more examples of how different literature resources, different text inputs can be summarized for different audiences and in different human languages.

Okay.

now we're going to get to the fun and the new stuff with perplexity AI thank you Tucker for your message making you a moderator for that one I hope you enjoy your new wrench Tucker okay so just to get familiar with what perplexity is here

I pasted a prompt into perplexity.ai, write a comprehensive summary of the rxinfer.jl package, and Perplexity goes through a series of steps.

It does online searches, finds relevant chunks of information, and then it writes this report responding to the prompt with internet information, including these code boxes.

So I'm showing that because what this next section is gonna do is essentially send this exact prompt, among others,

and then interface with it through the scripts.

So here's another example, create a comprehensive grant proposal outline for the NSF safety, security, and privacy of open source ecosystems, SafeOS program.

And then again, similarly, it just thinks, okay, which ecosystem?

How about TensorFlow?

So enhancing security and privacy in the TensorFlow ecosystem, and then writes the grant, making up the information.

Also on the notebook, LM,

a friend just shared with me illuminate which you can do the literature search this is illuminate.google.com

the literature search so here i had it look up active inference and then had it pull in these four papers so it's taking one more step for you whereas with notebook lm you had to download the pdf and upload them let's now listen for the first time to illuminate i'm going to turn off my microphone and it will play from the computer


SPEAKER_02:
This conversation is powered by Google Illuminate.

Check out illuminate.google.com for more.

Welcome to our discussion on active inference, a fascinating framework for understanding how agents make decisions in uncertain environments.

We'll be exploring several research papers that delve into the intricacies of active inference, comparing it to the well-established field of reinforcement learning.

What are the core tenets of active inference and how does it differ from reinforcement learning?


SPEAKER_00:
Active inference, rooted in the free energy principle, posits that agents act to minimize their surprise, the uncertainty about the world.

Unlike reinforcement learning, which focuses on maximizing a reward signal, active inference aims to minimize free energy, a measure encompassing both prediction error and the complexity of the agent's internal model.


SPEAKER_01:
another another one pretty cool though illuminate.google okay and gen24 was with different kinds of calls to action translated into a lot of language if people want to look in that repo okay so where were we with perplexity so uh

I did several things in the report on the 13th and then in the several days since, several other things have happened.

So I'm going to just skip over the earlier parts that were written about perplexity and we'll pop over to the documentation for the API and then look at how it's being used in the package now.

so where things are at right now pending the rest of this stream of course communicating with the Formas stakeholders get their feedback get others feedback open science Etc then we have the analysis within for me looking at how we can do all kinds of augmented analysis within

the database as it stands in July, 2024 snapshot, and then also go beyond for me, July, 2024 snapshot.

So adding a new literature, connecting with the species ID, perplexity searches, et cetera, fabric integration, kind of like what's happening in the previous InfraAnt stream.

So, okay.

All this is open source in the Formindex repo.

Quick look at the perplexity API.

Very simple docs page.

You get $5 a month for paying $20 a month.

Wow.

And there's a really short, simple example that gets you going.

Here are the supported models they have at this time.

They have several of sizes of this perplexity sonar LAMA 3.1 base at 870 and 405 billion parameters.

They have several perplexity chat models.

These are chat.

They're not doing online searches.

And then there's the instruct open source models.

They mention the rate limit, 50 a minute.

We might hit that.

Pricing is shown here.

And each one of these requests seems to be costing, using the large model, I think in most cases, the Goldilocks medium one, each of these was maybe a 10th of a cent or a fifth of a cent.

And I'm sure a cheap one would be a fifth that cost and so on.

So super worth it for what we're getting.

All right.

Here we go.

Let's pop into the perplexity methods folder.

Okay, that was the formless analysis.

Goals, accessible, rigorous, applicable, near mycology.

Notebook LM.

OpenAI API for the translation.

Now perplexity API.

So test perplexity API is a short script that tests the network connectivity.

Simple perplexity is where you can paste in your private key.

It's just simpler and keep it all in one just with their example of starting.

So this is if you want to study the simplest perplexity call you can make.

So now here we get an llmkeys.key, it's in git ignore.

So it's not synced to GitHub and it has the perplexity API key.

So there are, as the simple perplexity shows,

you can send your message with a system prompt and a user prompt.

So this is kind of the context or the character system prompt, and then the specific user request.

So first the user prompts, this is what somebody would type in.

They're in a JSON called user prompts dot JSON, and they're numbered as well as given a short name and the full prompt.

So here, prompt one, short name is climate change impact.

And the prompt is conduct a comprehensive meta analysis of studies published in the last year on the impact of climate change on ant species diversity.

and then extra information.

You could say output in the style or make sure you do a ton of research.

And so there's several dozen.

The first few dozen are ants related, then applications of ants, what's the role of neurotransmitters, brain regions,

then prompt 24 25 we get into doing meta analyses of the career of professor deborah gordon and professor tim linksfayer we can add more if people have any requests for prompts please write them in the chat and i'll add it here uh doing meta analyses on different areas of ants biometrics like relatedness within and across colonies phenotypic correlation measures

Then, now prompt 33, 34, 35, we're getting into specific ant groups doing a meta-analysis of studies published in the last three years.

So Camponotus, Formica, Pheidole, Ada, Solenopsis, Monomorium, Laceus, Ocophila, Pogonomyrmex barbatus, that's genus and species, and then non-barbatus Pogonomyrmex species.

we get into the regional we got northeast southwest usa pacific northwest agricultural ants northern california ants davis davis agricultural ants davis urban ant ecology davis invasive ants ant symbolism ant wisdom ants and contemporary art ants indigenous wisdom stewardship

Then 65 takes a bit of a twist.

Here we get more into prompts that are not necessarily related to ants, but some of them do as well, relating to the active inference, theoretical foundations, recent developments, applications to ant behavior of active inference, PhD proposals for active inference, foraging simulators for active inference,

recent events related to active inference writing step-by-step tutorials this one's going to be fun to look at giving comprehensive overview of Bayesian mechanics quantum active inference writing a summary and step-by-step on rxinfer.jl Julia package

writing who's on first dialogues, writing grant proposals, fire risk assessments, and county-level fire risk assessments.

Thanks, Tucker.

Enjoy.

That's the user prompts.

System prompts, there are fewer, but if someone suggests one, just write it in the live chat.

These are also identified with a number and a short name and a longer description.

So here's a myrmecologist, several ant researcher roles, programmer, complex system scientist, local journalist, enthusiastic undergrad.

system props seven and eight are questions from the innovators catechism at the early ideation and then later pitch stage so they're going to respond with question and answer structured information which we'll look at and some other areas so then perplexity.py

You list the short name of the target system prompts and then the target user prompts.

So we could add one if somebody types another user prompt, just let's do Lake County fire risk just to show what it looks like.

Say perplexity, run it as Python 3 perplexity.

So where the outputs already exist, then it skips it for that combination.

So it looks like that was already done.

Let's see if it'll autocomplete or let's just pick a user prompt that hasn't been done.

Let's do FIDOLI

formica recent studies so systems uh innovatory catechisms pitch the outputs existed for some of these user prompts already

then it hits formica recent studies which is a new one so it's gonna send that off to perplexity um while that's chugging away i'm gonna go over to the github and we're gonna look at a few of the outputs of the the primary artifacts themselves so just the outputs of the perplexity different characters doing these difference

kinds of synthesis.

And then we will look at the semantic and linguistic methods that analyze the folders of documents.

The Markdown output folder is nested into the different roles.

So let's compare the innovators catechism with the complex system scientist.

Okay,

here we have complex system scientist recent events here's IY number four this was a event I don't know if the dates were correct but that did happen in 2023 there's some Institute quarterly roundtable so it's over learning on that or somehow it came to learn about that one live stream here's Bayesian mechanics overview this is written like a

Curriculum Explainer.

Here's the career meta-analysis of Professor Deborah Gordon.

Early career, topics, methodologies, key findings.

This is a different Deborah Gordon.

Both Deborah Gordons, so it wasn't sure, so it wrote about Deborah M. Gordon, the myrmecologist,

and then other deborah gordon as well here's lake county california fire risk current fire conditions historical fire data climate weather pattern vegetation fuel load topography human factors infrastructural vulnerability fire management resources

prevention mitigation strategies, future projections, economic impact, ecological consideration, community preparedness, post-fire recovery, actionable recommendations, no citations provided.

Here's overview of RxInfer.

This is the perplexity example on the web interface that I showed earlier.

Here it is just in GitHub.

And we can test running this, but this is pretty much what it looks like.

And if it's pulling from the coin toss example, this should be good.

Here's the grants proposal for Safe OSE.

Again, that was one where I showed the real web interface.

And then let's see the who's on first.

So this one makes a who's on first about Hurricane Milton.

I guess that's what it thought.

current events were now if we look over in the innovators catechism system prompt folder there should be some differences here here the output format is requested to be the project catechism so with interesting made-up theme names which we'll see I think a few of

We see the catechism format of situation, problem, and the entrepreneurial pitch catechism specifically.

Situation in terms of key problems, user segments, alternatives, early adopters.

The mission and the value proposition.

Potential avenues of approach and the resources that they're associated with.

advantages, risks, feasibilities, channels, milestones, metrics, cost benefit, big picture, admin, folder, bibliography.

Here, it's a pitch about a project to study active inference and behavior.

And the first citation is a real paper.

Here we have innovator's catechism for the foraging simulation.

And it includes some code methods.

Here's a pitch for the Bayesian mechanics.

So pitching why we would want to have this Bayesian mechanics explainer on border.

looking at these citations open up all four of them we have relevance paper with the incorrect citation ramsted at all rather than sakativa devel at all second paper ramsted at all 2023

here incorrectly as royal society publishing 2023 title is correct here's a conference 2024 2024 okay here's a pdf zuckerman and george chem archive so citations not fully accurate but citations real links nonetheless um

here we get to the grant proposal so it's a pose or safe os type application for here it it settled on linux kernel open source ecosystem so it lays out using the catechism format the essential value proposition avenues of approach all the information about the grant like how my catechism for darpa

This is using the innovator's catechism pitch for SafeOS for NSF in a hypothetical world.

So each of these folders, depending on how the prompts interact, there's really interesting outputs in terms of maybe the undergrad making the

POMDP, PIMDP active inference simulation, maybe this is the best.

So that's where the perplexity, so here it was from running the perplexity.py.

They all went for the Formica recent studies just while this was happening.

So let's push that update Formica 005.

push it now let's look at local journalist formica so this just happened now meta-analysis of recent studies on the genus formica 21 to 23.

here's a table looking pretty nice with two studies on formica table with one study on social parasitism one study on foraging activity trophobiotic relationships social parasitism host parasite different methodologies significant

results and impact controversies debates conclusion and the formatted references pretty cool let's look at the futurist for formica here it's a different format we see more of a general summary of the state of the art on the monogine polygine research agenda

We don't see those tables broken out the same way, and there's not a strong futurist bent.

So it could be some interaction of the model, the system prompt, the user prompt.

Of course, all this needs to be checked and could be checked with synthetic as well as manual human means.

Let's see what the myrmecologist

What would the mucologist say when it walked into a NSF grant application?

So it wasn't included because it was taken off these targets.

So here, add in comprehensive

Let's just check from user, from system prompts.

It's called Comprehensive Miracology.

Paste it in, save it, clear it, run perplexity.

Oh, maybe there already is that.

Okay.

So that's the combinatorics of generating combinations of the user and the system prompt.

This is going to now get into the visualization section that's new from the report.

Visualization methods.py is a nice long

590 line methods file visualization.py brings in those methods set the input in the output folder make the directory for the output if it's not there set up the logging load in some of the text processing modules pre-process perform some of the analyses

output several groups of visualizations.

Group one is related to dimension reduction visualizations.

Group two is related to enhanced PCA, principal component analysis visualization.

That was part of group one.

Group two, word importance in PCA.

Group three is more text analysis.

So I'm going to delete the folder visualizations, right click, open integrated terminal, Python 3, visualization.

so this is what it's going to look like when it runs it's going to output how many markdown files it's taken in 158 so if we added more to perplexity.py we could do some of that we could add another

and then it would output more and then rerunning visualization would pick up on those new ones.

Okay, so there's nothing in visualizations yet.

It's doing the analysis and then it's gonna start to spit one by one saved images into the visualizations folder

we will look at those visualizations and see what it shows us about these outputs in terms of their semantics and in terms of how the system prompt and the user prompt interact and how we might use that so the analysis gets slower as more

documents come into play.

I'll just show, here's SystemPrompts.json, Control-A, Control-I, so bring it in the Composer, add a SystemPrompt for an expert chef who outputs

dinner menus with rich relevance for any prompt.

Output is always epic dinner adventures though.

So this is a way to use cursor in this informal way to write a well-structured prompt 13

So it's going to first write it in this side window.

That's kind of, and then it scans over and now culinary adventurer.

So I take culinary adventure, go back to perplexity, systems prompt, culinary adventure.

I think that's super maven, but still it knew that I wanted that one.

I'm going to terminate the visualization effort for now.

Call the Python 3 perplexity.

So now here's the new.

The ones that were already done or skipped.

And the new combinations are being sent off.

Right click.

Open the integrated terminal.

Python 3.

visualization.

Other outputs was just earlier texts from a simpler time, from a different time.

I'll delete these and they're always there on Git, but I'll delete them for now.

Post your Git update

And then visualizations are going to start to come through.

And this is going to help us get a handle.

I'm also going to resync the cursor context.

Oh, it was it.

Resynced.

Great.

Okay.

Introduce perplexity.py.

Explain the structure of user and system prompts.

Look at simple and test.

explaining how different kinds of prompts are handled.

Let's look at the visualizations and then we'll go to the next steps.

So again, here's the visualization.py.

All right.

So they're now getting

Output to the folder.

I'll just let them all run.

Let's just start with LSA plot.

So here's two components.

Looks like partially labeled.

LSA component one.

Document semantics.

PCA 3D plot.

Okay.

Cool.

Interesting color scheme.

PCA cumulative variance so it looks like only five PCA components are retained we could increase that number but it's showing the accumulation of variance and we're not saturated yet so to capture more of the variance it would probably make sense to add more than five principal component terms okay confidence intervals

This is the mean TF IDF score, which is a document frequency, inverse document frequency.

So here we have ANT and inference with higher scores, and then some of these other ones with lower scores.

LSA topics, okay.

Here we have different ways of doing topic modeling.

Topic 1 has to do with time passing on graphs and reactive message passing in the Julia package, as well as Lou Costello, for some reason, from the Who's on First, probably.

Topic 2 having to do with active inference ontology terms, quantum active inference.

Topic 3 having to do with collective behavior, free energy.

for having to do with the the other deborah gordon talking about the environmental the world resources modeling here's links there and security so okay cool here's the principal component analysis with the eigen terms so we looked at the cumulative variance plot before

The square plot is showing the decreasing fraction of variance that each of these components add.

Let's update visualization methods.

Ensure to include 50 PCA dimensions.

Okay, we'll keep looking at it.

There's the principal component analysis.

Oh, we were high up.

This is pretty cool.

Here we have five of the different principal components with colors.

And then we can see, okay, fire is cross-listed on

explaining the variance fractionation of blue and green whereas inference is only loaded up in con on fraction two and then that's shown here they're separate but also there's some that are cross listed let's see how that edit went okay interesting looks like it's deleting a bunch of stuff

Let's accept it.

Save it.

Clear.

Run the visualization again.

So now these culinary adventurers are still pumping in the background.

So

Now we have 169 markdown files, whereas in the previous time there was 158 or something like that.

So it's going to redo these.

They might be changing right from underneath us.

Okay.

Prompt distribution.

Here's different system prompts and user prompts and different words in them.

Here's a funny visualization.

Document length across the system prompts.

So here we can say, okay, the expert programmer has a shorter document length on average.

And then here's the statistical tests and the p-values for the ANOVA.

It's not statistically significant at a 0.05 alpha.

Term frequency.

Here's term frequency, inference, ant, fire.

term network so here we have terms and their connections with the co-occurrence module so this is a lot like gene co-occurrence based upon expression levels here's term frequency heat map

So we can see red is more usage, more unique, strong usage.

Light yellow is less strong, unique usage.

So fire is a word that's either mentioned a lot or it's not.

There's other terms such as system or research or use that have more variable use.

T-SNE plot.

word cloud word importance here's the top 20 words in each of those modules and then a word cloud for them and similar for pca and for the lsa um

this most recent rerunning brought in the culinary adventurer so there they are culinary adventure that's just showing how you can add to the system prompts bring it into perplexity.py

have it combinatorically generating using the perplexity API call.

So then we'll, and then meanwhile, you can be rerunning the visualization method.

So here there's an error, fix the error.

To fix the error, image size of 9,000 by 90,000 pixels is too large.

It must be less than 2 to the 16th in each direction.

Whoa.

Because we asked about retaining 50 dimensions.

Save it.

Clear it.

Delete the visualizations folder.

Run it again.

We could parallelize the API call, get closer to that rate limit, 50 per minute.

Let's look at some of the outputs.

reload markdown output culinary adventurer that's the new one so we all we asked this is what is so fascinating this is really worth the ponder who's on first of current events

i've seen this several times this topic wow look at the topics here vaccine elon musk artificial intelligence hurricanes meta mark zuckerberg putin vladimir putin

google and it's all unpacked that's not a recipe that's not a menu fire assessment not a menu formica the the prompt did work so here the societal symphony of formica ants a dinner concept

colony awakening first dish appetizer foragers bounty super salad super colony soup main course queen's feast palate cleanser aphids nectar wow wow chef's commentary with references

Analysis of active inference events.

Same outputs as others.

Okay, let's check back.

Looks like the PCA.

Let's check back in the PCA.

Here we go.

Cool.

So now we have a cumulative variance rarefaction plot.

And we can see a lot more richly the relationship with these different terms.

Loading's heatmap.

Here's a thousand features retained.

you can see some patterns of positive and negative here you see different pca plotting different pca plotting

There's some methods that look at where things start to bottom out or change, like breakpoints.

Here's the terms that are associated with the top 50 principal components.

Term network.

So that's pretty cool that Perplexity can do these searches and pull information from the internet.

Let's add another user prompt.

Write with code a workshop, a hands-on

workshop oriented towards applying active inference in the bioregional setting.

Add a prompt that will write with code, comprehensive, hands-on workshop oriented towards applying active inference in bioregional settings, be

professional, the outcome should be standalone and ready for deployment.

So prompt 81, move that over there.

Rerun perplexity.

Here we go.

So it skips over what's already been done.

First one that it hits is innovator's catechism pitch.

The first one in the list catching on the first one that is undone.

So once that first one comes out, I'll push it.

We'll check it out on GitHub.

It'll be the value proposition from a innovator's catechism pitch perspective for the bioregional active inference workshop.

41 seconds.

Okay.

Push it.

Innovators Catechism pitch.

It's uploading all the images.

Buy Original AI Workshop at gmail.com.

Situation, key problems, lack of integrated decision-making tools, inefficient resource management, limited collaboration, user segments, ecologists, environmental managers, data scientists.

Alternative, traditional statistical models, which are static and often static and do not account for dynamic environmental changes, machine learning approaches.

While powerful, these may not provide the same level of interpretability and integration with ecological principles as active inference.

early adopters, research institutions, government agencies, NGOs, community groups, mission value proposition to provide a comprehensive and practical understanding of active inference principles and their application to bioregional management, enabling participants to make more informed, sustainable and adaptive decisions, avenues of approach, theoretical foundation, mathematical framework,

computational implementation, data collection and pre-processing, model building and simulation, case studies, integration with existing management practices, future directions, advanced topics, resources, computational resources, data, and human resources, advantages, unified framework and interpretability, risks, complexity, and data quality, feasibility.

Given the advantages and the growing interest in active inference, this project is highly feasible.

The workshop can leverage existing research and tools, and the facilitators can provide the necessary support to ensure participants understand and apply the concepts effectively.

Channels, academic and professional networks, online platforms, milestones, metrics, milestones, cost and benefit, big picture, Maria, Dr. Maria Raffa,

Day one, theoretical foundations, mathematical framework, morning session, afternoon session.

Day two, computational implementation and data collection.

Day three, model building, simulation case study.

Day four, integration, future directions, advanced topics.

Follow-up exercises.

Wow.

expert programmer ants contemporary art ants geopolitics and social structure expert programmer ant foraging

local journalist New Mexico fire risk looks a lot like the other ones okay so that is

basically all.

This has been a checkpoint for the form index project.

There's some ANT-specific directions.

There's some science, education, communication, generative AI, plus, plus, plus.

A lot of fun and interesting methods that people may find useful.

It's all open source.

code i'll push the final up we'll we'll just return to the next steps and and close here so if anyone has a a comment or question for to to do otherwise let's just cover these last steps

This is kind of a random thought.

Having situational awareness of when are we in the infrastructural core, when are we working with adjacent schema where changing the schema is going to have big repercussions.

Where are we scouting or playing or researching on the edge and beyond where we can change something like a cul-de-sac and it won't influence other things.

Get the feedback from Formas.

Improve the methods.

Integrate with Species ID.

Do perplexity searches for all species groups of ants and maybe beyond.

Integrate with Fabric.

There's no written questions yet.

If anyone has ideas, leave a comment.

Get in touch.

I hope this was of epistemic and pragmatic value for you.

Check out the repo.

Check it all out.

End of number 005.1.

And there it is.

All right.

And there's the Zenodo 0.3 version from the beginning of the stream.

Pretty cool.

Awesome times.

Okay.

Thanks for watching.

Looking forward to people's feedback, where it all goes.

Till next time.

Bye.
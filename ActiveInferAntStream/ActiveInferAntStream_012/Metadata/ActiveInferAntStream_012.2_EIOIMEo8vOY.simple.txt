SPEAKER_00:
All right.

Hello, welcome.

It is April 16th, 2025.

This is ActiveInference Stream 12.2, and we will begin it with making a GitHub push to the repo that all of the work is going to be in, continuing on from 12.1.

Start stream.

This is going to the GitHub repo

active inference Institute slash cerebrum.

And looking forward to people's ideas and questions during the stream.

So this is the case of the evolving cerebrum, the curious language of the incident.

We're going to be using Cursor IDE version 48.9 on the date of edge of development.

LLMs, the two main LLMs that are being used in this development have been Claude 3.7 and Gemini 2.5 Pro EXP325.

So I'm just going to delete things as we go through them and look at some practical and some theoretical aspects of Cerebrum.

all right so uh to begin with and sort of catch up on the Zenodo um the version one was uploaded on April 7th and commonalities similar topic also there was a script that was in an older folder called cerebrum in the repo that was used to assemble the the paper

um in doing that learned a lot about assembling papers and writing them expanding them and in this current version 1.4 release today it's up to 146 pages and still generated by a single simple script which we'll look at in a second um and how it's expanded so there's this sort of paper component and the papers generated from the paper folder in the repo

let's look into it in cursor first how the paper is made then we will look at some cool applied outcomes like making a linear regression neural network and partially observable markov decision process model

declining those into different cases and getting different kinds of behavior from different from the same model but of different model types and getting a little bit clearer of a view in a few specific cases what it means to have a given generative model in a different case

Okay, first the paper, then to the source code, and deleting things along the way, adding things as people add anything in the chat.

All right, first the paper generation.

Oh, and the documents.

The README in the paper folder describes this process.

Assemble paper is a script of 551 lines that looks into the component section and assembles them into the output.

So in the output, there is the PDF that's generated, which concatenates the title page

makes a table of content, including all the main text sections, however many there are, and then all of the supplements, however many there are.

It also outputs a pure Markdown concatenated version of the assembly process, and it outputs

an HTML version so here's a website rendering clickable links still has the inserted images so a lot of flexibility in how and and also the the paper and the images are also vastly improved so there's multiple ways to make static documents interactive websites um and markdown files for easy machine reading and formatting as well

it's generated from the components folders there's templates so you can add more sections then there's three folders figures main text and supplements so the figures are the 15 current figures and they have their name and title there's there's more general ways to do it uh

using LaTeX, Overleaf, all these kinds of things so that the numbering is automatic as well.

So this is kind of a fixed numbering system, but could be generalized too.

So it has, for each figure, a caption, a title, a caption, and the mermaid markdown code to produce the figures.

Those get saved into the output as direct PNGs.

And also there's a graphical abstract

which looks a little better in PNG, which has the figures and the abstract.

So kind of like scientific poster style.

Then there are the main text and the supplemental information sections.

The main text is again determined by the numbering of the file, but these are markdown files with no mermaid diagrams, but citing the mermaid diagram for insertion and linking.

So these are markdown files in the main text.

title page, abstract, introduction methods, case functions and cognitive models, model workflows as case transformations, active inference models and conclusion.

Then there are the supplemental sections.

And if people have ideas for other supplemental sections to add, we can just add a 13, 14, etc.

I'll open up the website just to get a look at what's in the supplements.

Supplement 1 is a mathematical details and glossary.

Supplement 2 breaks out the novel proposed linguistic cases.

Supplement 3 focuses on different practical applications.

4 is a bibliography type section with related work.

5 goes into more category theory details.

6 gives future directions and roadmap.

7 goes into some computational complexity estimates for different transformations.

8 goes into more detail on active inference.

9 more information on math.

10 algorithmic details 11 the formal definitions of the core linguistic cases the main ones that are focused on in the paper and section 12 uh case study on writing a research paper and how in different sections of the paper there's

different ways that sections are used and do different things with respect to each other and the ideas and the reader so that different cases could be applied to characterize the nature of a model or a process in that way for those phases

Just going through this last one, just as an example of a supplement and kind of thing that if we wanted to build another supplement, someone can make a suggestion.

Precision management in research paper writing, equivalent to learning rate, forgetting rate, attention, precision, broadly just describing modulatory strategies for different kinds of upweightings and downweightings.

message passing occurring amongst author and manuscript amongst sections manuscript and readers and reviewers with kinds of intent oriented um entities in different cases or using a similar joint model of communication in in different settings motivating why to have cases

writing strategies haven't looked into any of these maybe some are useful um how to make certain kinds of pivots rhetorically in i guess speech or writing and different ways to interpret how uh regimes of attention change with with uh across different sections all right

So that's the paper on Zenodo, and we can add more.

Just to close off this section on the paper assembly process, there are probably other tools to do this more flexibly.

There's things like DeepWriter and a lot of other approaches to doing structured document generation.

Building it from scratch with Cursor is still pretty interesting, and having

plain text multiple sections possibly doing um lumping and splitting like have introduction 3.1 3.2 3.3 or just a more flexible way to order the sections it makes it really easy to go into a section and add some kind of feature using cursor like here in the introduction

Just add one relevant paragraph to the end.

Structured, smaller forms, figuring out the right kinds of coarse graining.

Obviously just having a dictionary isn't writing a paper.

So having things broken out to the word level

might be kind of a granular way, but maybe there's something interesting about that format, having a lookup table of the dictionary and encoding sections and words in a totally graphical way.

It's just easier to write and version just small updates to the text.

can be done and reviewed.

So collaborative editing can zoom in and break out different sections.

It'd be pretty clear if there's a flexible ordering method or just with the file names, even if someone broke out 4A and 4B or 4.1, 4.2, updated the assembly script, et cetera.

So, okay, that's the paper.

next uh how about the examples of the specific models then the documents then anything that that people have brought up by then bigger picture how do we integrate case-based logic so nouns in different roles and sentences however treating models as sort of noun entities how do we do that in theory and practice

Different ways to make visuals.

Mermaid's been super useful.

Maybe there's other plain text formats that are useful too, but creating good system and process diagrams, structuring information from pros and tables and lists.

Math with LaTeX.

All right.

Linear regression, POMDP, neural networks.

So this is in the tests, in the source SRC folder, tests, subfolder.

Let's see if CodeViz will give a good representation of this.

Hmm.

Okay, maybe not today.

In the tests folder, there are several script files

testing each of these different models, some utilities, and read me.

The three that are fleshed out at this point are neural network, linear regression, active inference, POMDP.

So in each of these cases, there's going to be a single model that gets defined, and then it's going to be deployed in different ways

leading to visualizable, analyzable differences in outcome.

So let's just look at some of the images first.

Okay, here's a graphical summary of how for the very same neural network, whatever kind of technique is being used, there are different ways that just considering neural network like a noun, let alone the referent of what it means.

But there are ways that people talk about models.

That's the curious language is the what people talk about when they talk about models.

And it's right there in the sentence, though in a way silence in most cases in English.

there's the neural network does something standard dictionary form nominative the researcher trains the neural network like the researcher looks at the neural network accusative case recipients of action direct and indirect similar with dative more of like to give rather than to look upon

genitive outputs of one of the cases that's visible in english with the apostrophe s like

the table's leg so it's the outputs of the table and these are it's a different way to talk about the outputs of something just at the first pass it's used in a different way in the sentence and then looking from the way it's used in a sentence on up through what the real reference is

it starts to become a little clearer how these kinds of sentiments that are already expressed in natural language in method sections in people's own inner voice and conversations around modeling different ways that the words are used correlate or relate to in this linguistic fashion to differences in the deployment of the model

hence possibly regularities that can be described and also structured in ways that are outside of these okay just the last cases here instrumental by means of the team solves a problem by using or with a neural network locative

the patterns exist within the neural networks layer so referring to the model as a location where there are spaces within or being in time within the model ablative

the errors originate from the neural network weight it's kind of an interesting case has some similarity with genitive in some situations and then the vocative which is the direct addressment like calling a different digital assistant with a name or calling the model to see if it exists like a ping

Then in each of those cases, the same neural network gets run with these slight differences.

So it's pretty variable per, okay.

So in the ablative case, it was discussing the errors of the model.

So in the accusative,

the model was being trained so this is just a trace as would be in some other kind of machine learning paper describing the target of optimization like in the method section where they say we train the neural network to dot dot dot here in the dative was the giving of data

so this is just describing as a function of input distribution what do different activations look like in the model so as it's being given different gifts what patterns are arising same neural network

Here in the generative case is describing function outputs, output distributions, some animations of generating output.

Instrumental.

this might be showing different ways that statisticians do things with the model looking at inputs outputs activations looking at the structure of the model looking at the model's training history for a given curriculum as a kind of empirical data

looking at kinds of interpretabilities looking at the structure of the neural network as as something that can be done with like tool engineering all right locative describing possible locations inside of not not super fleshed

showing the model in its agentic operation like when people say the model does this the model learns to fit this working as active agent generating predictions though those predictions would be the predictions of so it could be the same reference it could be used in in within one sentence a through the ball to b then b through the ball to a

So within one sentence or across sentences, different nouns are used in different cases.

And they're marked differently from a written, from a speaking perspective in different languages.

Similarly, models as slash like nouns are used in sentences in different case settings.

so those have different implications for what is viewed what is meant what kind of capacities would be required from that model like does the model need to turn on does it need to just give a ping like with evocative or does it need to make a prediction so that's the neural network example now there's two more examples there's the linear regression next

So here is a Y equals MX plus B type linear regression.

So ablative predictions of the model.

There were some very interesting diagrams that were arising in this linear case.

So here are talking about the model as a causal originator.

So there's the model which causes predictions, which influence or lead to decisions, which have these sort of causal consequences.

So that's model as a source of causation and prediction.

cross-validation here there's an evaluator who is a value doing something such that they it is said that they evaluate that model so that the model is being here evaluated to well residuals analysis doesn't have anything cross-validation in the evaluation phase

here the model is in the dative case the recipient of data like I sent I gave a present to my friend I sent data to the model and that is reflected in the batch ingress process with

different sets of data or like perfective and imperfective like the streaming online real-time data model was receiving data in a continuous and then the perfective like the model finished receiving 50 units of data generative

is like apostrophe s so it's the possessives of the model so it's the intercept of the model the slope of the model mean squared error of the model all these kinds of statistical expressions things that might be in a results section instrumental

used as an instrument of action, used as a tool.

Like, we hit the nail with the hammer.

We analyze the data with the model.

So that's doing different, again, variable functional images, but looking at different kinds.

Okay, so we train this model best as we could, whether it was this or that method.

And then now we're going to look at the correlation or the causal interaction between factors one and two, and then factors one and three.

And then we're going to use that instrumentally and say, oh, okay, here's a 0.14 correlation

and and we're this confident this far away from zero and here it's this correlation so this is these two features are more correlated or they have this causal relationship so doing things with models locative positions within the model

So where's the model located in information spaces?

That's one, that's the kind of interpretation locative.

And there are multiple senses in which a given case is used, but one of them was highlighted in that neural network example, which was a location in the model, like on the third floor of the neural network.

And here,

that would still apply you still could talk about i'm talking about the y in the equation and then here's another view which is the location of the model like they are at the store the model is at two comma three nominative this is sort of even though the the uh animation doesn't really work

nominate nominative is when using the standard dictionary type sense of the word when the model does something like that model fit the data that entity did that model as actor fitting the data

Model fitting data.

All the kinds of sentences where the model is, it could be replaced like with a person's name or a doer.

Vocative, so here this has some, there's a few funny ways the vocative is used.

This one is, in the vocative case, data points directly address the model.

So each point communicates its residual value to the model line.

This direct communication visualizes the conversation between data and models similar to how we might address someone by name.

So this is almost at a sort of algorithm as communication level, thinking about information processes within like, oh, negative 2.55 told the slope where it was and the slope told back its residual.

So that kind of, whereas in the neural network case, the vocative focused on the addressment by name for like a digital assistant.

okay here's some other vocative uses even in linear regression so it's the data and then the model's prediction and then is saying i i'm telling you how far i am under the value another interpretation of algorithm as communication where a given variable x is saying hey slope multiply me

so x times 1.97 and then to the intercept the other part of the mx plus b says add 4.98 and then combine us both or we get combined or i'm the final answer after combining it's sort of like a linguistic approach to describing information flows and algorithms

okay so that's the uh linear regression model and this is all from like test linear regression test neural network test active inference palm dp so here in the in the palm dp case um for a given model it's being described in different cases so just to read the sentences

I'll just say the model instead of the POMDP.

The model decides which action it takes next, nominative.

The programmer evaluates the model's performance, accusative.

The environment gives observations to the model, dative.

The model's belief about the state guides decision, genitive.

We solve the problem using a model framework, instrumental.

Different policies exist in the model state space, locative.

the new algorithm deviates from the standard model approach ablative and hey model what action should I take next vocative so now let's look through those start at the bottom so here is a direct addressment like the sort of digital assistant addressing a model

and also which kinds of functions are going to come into play.

So this gets towards one of the use cases for having this type of lexical intelligence for modeling is if one expression block or methods block has a bunch of vocatives,

then you know that for whichever agents are addressed these are the only commands that you need to spin up whereas if it was some other case like it making a prediction or like you know doing forward inference might be a different computational setup than getting trained than getting evaluated so in that way just you could say like what was happening with one actor through a paragraph where they were being referred to in different cases

you can trace the same model as it's being referred to within and across sentences in perhaps different ways and different computational setups and build for that okay so here is focusing on the vocative nature of the call and response between the agent and the environment in the nominative

So here's the model in determination of what to do.

For example, by doing policy inference like expected free energy policy selection as inference on which actions that I select given the states and those might invoke policy inference methods.

simple two by two matrices for the observation transitions so this is tracing out the decision making process looking at the palm dp as an agent and as it's learning and updating its belief so what and just here just an example animation what is its belief and action conditional upon observation okay in the locative

here is using again the um a little bit of a fusion of referring to policies existing in the state space which is pointing to the model's state space so in that way referring to like a location in the model however the location of the of the the location of the state space in the model may be locational so like

the location parameter of the model is in three comma four so here referring to the point sp the phase space location so all the kinds of theory and empirical aspects of saying like the the model oscillated around a point attractor

or the model made its way from thinking it was a cool day to thinking it was a warm day.

Those are paths and passages in phase space.

So those may use verbs of motion or locative in a certain way.

Instrumental, people saying stuff like, we solve the problem using the model.

So showing the model,

looking at different ways in which the model is used to explore questions.

So making predictions, looking at computational phenotypes, like the magnitude of belief updating or the different aspects of the simulated or actual behavior of model, being used as a tool, looking at different statistical traces from the tool, looking at error envelopes,

genitive is like the the degenerative ai the generative ai like the output of gpt or the text wrote gpt's text the text wrote by gpt predictions of the model the model's predictions looking at the beliefs of the model

looking at the collection of them looking at the predictions of the model looking at the beliefs of the model in relation to observations of the model

or it could be said in relation to observations sent to the model so it'd be like another way to use observation and basically convey the same syntax which is like we all get it it's getting data points but

Literally, as it would be with a noun, even though the two expressions, as a function of observations of the model, as a function of the model's observations, is using model in the genitive,

as having observations that are possessed by it, or you could focus on the sender of the data and say that the data were sent to, or the observations were sent, or X sent observations to the model.

So those two features might both be true.

They might be isomorphic.

They might highlight different features of that setting, but that's just an example of how many ways there are to map

expressions about models model linguistics to models in in theory and practice just like there's a ton of stuff that could be said about a tree and just the tree grows the leaves of the tree people looking at the tree like all those things that are kind of kicked off by prepositions

in context all right last ones for the uh palm dp and then to the documents and then to any uh ideas or questions people have here in the data environment giving observations to the palm dp like

what its direct indirect object the palm dp gets observations from the environment is having the palm dp doing something getting and then here the environment gives observations to the palm dp so looking at the impact of observations which might describe very well the same graphic or or related graphic as was used just like the mass of the leaves and

the branches mass are referring to different levels of analysis of a tree and they're using a different case but they still may be very related measurements and more importantly we're talking about the same tree so this is the same palm dp possibly the same run of the same palm dp but if you start to think about just models as they are spoken

about the kinds of things people say about models like first we designed the model then we use the model to generate this then we sent training data to the model we got the model's errors as it's like that kind of talking about models aspect sometimes refers to the very same model in the exact same situation other times it refers to the same

lineage of model being used in different ways.

Like if it says we use the model to sweep across this parameter range or parameter range was swept across those kinds of things, but then maybe only some of those parameterized models were taken forward and then they were used to generate predictions.

So just it could be a variant of something that was described like not exactly the same single file.

But in these simple examples, it is like the exact same generative model.

All right.

So that's a bit about the testing suite.

And test-driven development is very effective in this kind of augmented coding setting.

It can help lock in a lot of functions, as can good use of Git.

okay now to the doc section which actually has many many many documents so uh we will definitely not look at all of them but at the top level of the docs folder there's some information about cerebrum I just

how works this live stream file um different features about cerebrum maybe these will go into a folder and then there's several folders so starting from the smaller ones uh dialogues these are like 17 different genres of sort of creative dialogue around and about some of these different themes so it's and and possible to to write

more so just to show what that looks like just dragging dialogues in or it could have been at dialogues write a new dialogue that will be very youtube appropriate for all the topics that are referred to in paper

comma it is a live stream so make it most possible like most youtube live streams do

Okay, discussions.

This is a few different comments that people have stated related to the work.

And I just responded to the comments in writing and GitHub just because it was easier to express and share it.

examples uh there's just a few examples and we can revisit this one too um just just this can have a growing number of specific the the linear the linear regression but it's just documents related to examples so here let's let's do a new now there's up to three parallel chats in cursor so

let's write examples up for the functional taking an output maybe in the future these kinds of drags will be less needed they they help get it right on the first one linear regression neural network

In examples, please make three markdowns comprehensively for the three main tests in output.

have each doc example be comprehensive explain why and how it matters why and how cerebrum and cases come into play show not tell all the kinds of things

pre-addressed that anyone thinking seriously about this would come to wonder initially okay back in the dialogue space

so like that once there's a genre within a folder even if it's a hyper meta genre like dialogue um it's possible just say okay or add a new dialogue with this theme okay so that's the examples happening

All right, then there's the insects folder.

So some of this may have images that fail to render, but let's look at these in GitHub.

Okay, so here's just zooming in just a couple layers, just a couple prompts deeper into, all right, how do we apply cerebrum to entomology?

So just like we were previously talking about the linear regression used as a tool or the predictions of it.

So this is kind of a classic standard entolinguistics.

What do people talk about when they talk about bugs?

And thinking about what functions, the kinds of things people do and have and will and would say about insects.

taking a first pass at the syntax of what they said what are those insects being said to do or have done to them or their location etc in that method section or in that natural history report so it's like are we talking about different functional roles different places where in describing that

function, that natural linguistic case would be used.

Different speculative insect cases, mixing the sort of hyper-practical discourse analysis, scientific reproducibility avenue, literature annotation, hypercharging, mixing with these speculative cases like talking about the ants' pheromonal implication

That's kind of an interesting possibility.

Now it might be morphologically silent in English or some other language, just as other cases are in other languages.

So it isn't even that the words have to be spelled any differently or pronounced any differently.

It could just be a situation where it's understood that the ant as a pheromonal actor or recipient is coming into play.

Maybe these are even over meshed with other linguistic cases.

So different kinds of topics that come up in ants, but different aspects of their activity that kind of give a special spin or clarity or relevance to different ways to talk about an object under different sorts of constraints.

looking at the ways in which this big difference amongst languages of the world the nominative accusative distinguishing models acting from models being acted upon regardless of the actions transitivity this might map to systems with clear initiators and responders so the queen lays an egg irrigative absolutive groups models performing intransitive actions

e.g.

moving, with models receiving transitive actions, accusative, distinguishing them from models performing transitive actions, e.g.

depositing pheromone.

This might map to systems where the state change of a model is paramount regardless of whether it caused or received it.

I'm like, whoa, whole ontological...

assertional frameworks where different languages are like falling in different ways across these two different kinds of grammar systems and maybe there are situations where one or the other has a different feature defined as all the ways in which they're different are ways in which they could be different in function or in fitness in a situation okay that's the least technical of them

here is applying those kinds of case to pictures like imagine if the case to picture were a suffix or a prefix or a subscript or a font or something like steganographic hidden in the word or it's just a reading lens but it's like the projections of the neurons send data give data to the calyx

So again, it's not the only way to use these parts of the brain.

It's not associating them with parts of the brain.

It's like saying there could be method sections that use these regions and describe these processes using this kind of linguistics.

different computational precision parameters and how they are in causal relationship with sort of natural ethological behavioral linguistics about what is happening like this is a classic professor Deborah Gordon point with the space between

seeing the nestmate on the ground and then identifying the task it's doing like it is foraging or it's part of foraging and then going from the task identification to the identity assertion like it is a forager but it's like a nestmate that we see nursing is that a nurse

Is it like, is a person who plants trees when they're not planting a tree still a planter?

That's more classic antilinguistics.

More from the Wittgenstein side.

So different ways that StigmaG studied all these kinds of scenarios, looking at beehive foraging, looking at the algorithms of the different ways that information flows and processes happen in the relocation.

Okay, here's more acronyms and puns for more topics.

So in the kind of last section, as we sort of go beyond cerebrum probably today, maybe we can use some of these or another one.

Insect cognitive architectures.

All right.

I was happy to see the harvester ants, Pogonomyrmex barbatus.

So different insect species and ways that they've been studied, even though many or all could be studied for all, but there's just many model-ish organisms that have been studied.

Well-studied insects.

looking at how different brain regions are described of if not playing functional roles of using different cases again like are there more mentions in the literature of we injected the dye into the projections of the neurons

or is the just like in the different methods sentences of the statistics paper it was like describing the linear regression or the neural network or the palm dp as getting things done to it or doing things or being used with all of those similarly happen for descriptions of insect brain regions

different classical behavioral types, keywords for literature search.

Like let's do a literature search on spatial learning plus hymenoptera, pull all those papers, download all the papers, break them into sentences, look at every sentence, do entity extraction, determine what case different entities are in.

So this is kind of like a formal linguistic

complement to statistically driven n-gram learning, like classically just taking large bodies of text and then looking, okay, we can look at the word frequency.

That's like the one gram.

We can look at the word co-occurrence, the two gram and going on and on and on.

And that kind of leads to one thing after another to the attention is all you need.

which has some kind of engram, like basically the context and the attention mechanisms with a bit more nuance, but a lot like that.

Okay, more tables, just linking up different ways that different functionalities, different, you know, what cases would we want to transfer from how we talk about ants in order to be able to say those kinds of things about swarms of robots?

Okay, more case studies.

Here is more of a deep dive on insect brain anatomy.

So looking at the different sections of the brain in different ways, tables, looking at different adaptations

more brain mapping more connection with species behavioral functions neurotransmitters so it's like there's again going to the literature and annotating where those kinds of things are said

And then that is already just sort of the forensic cerebrum approach.

It's just a purely annotative strategy.

And then also it implicitly and explicitly involves sketching out these state spaces, even where some of those combinatorics are like grammatically invalid or prohibited for one reason or another.

Okay, not all these work.

This one doesn't work.

different brain region different insect research that could be done okay then there's two folders with a lot of documents so the first one there is languages so languages first there's a folder

Computer.

This has several dozen computer languages.

So let's look at Go.

We'll just look at one computer language and then mainly at natural languages.

overviews the code, different kinds of use cases, annotates the code with different case annotations on different processes.

So you can see all of these kinds of abbreviations.

So different ways, just on looking at even just one line of code, one language might say the data is sent to X from Y or Y sends the data to X or just data X Y. Not to go into this on the computer side, but the human languages folder is also very rich.

There are a variety of languages.

The README is pretty much updated.

Let's just look at a few.

Esperanto.

in the previous stream we looked at how the hungarian case system has onto on and from a surface so sort of perfect for different functionalities of interfaces and and boundaries interslavic

Russian English goes into more detail just grammatically and cognitively on some common first and second language learning combinations of Russian and English and why certain failures of language are common cognitively and grammatically.

Sanskrit.

Tibetan.

So Esperanto, overviewing the case, just like kind of more like a grammar textbook, like a really fast summarized grammar textbook.

Sanskrit went into a bit more detail here, just several times it would add a couple hundred lines.

I'd say, awesome, add more interesting, funny, useful,

meaningful sections.

So different case information, different ways that a model like an NLP model could be used according to those cases.

grammatical number one two many where some languages might only have one and many but you know it could be helpful are we talking about a single agent are we talking about the dyadic communication setting or are we talking about the more you know three three's a party

different syntactic speeches features so this could relate to if we have a given model and just we're going to be using python or subscript or special characters or prefix it's like whatever um morphology setting you're in with the model there might be different kinds of of um

certain things might be easier to express or others or might have difference consequences all right then just kept on going and just say add more so lots of more um more connection with temporal and modal systems

like it starts to get into the beyond cerebrum stuff that we'll do in a few minutes but like there are other parts of speech and other linguistic techniques other than case even though cerebrum has focused heavily on case in these kind of two streams slash early versions of the paper

But of course there's a whole bevy of conditionalities and moods and all these different kinds of things, which like could be used to describe even just statistical models.

Like while this is happening, if that happens, then this will be happening at the same time as viewed from this perspective.

So different languages have different natural expressivities.

ways of modifying how things look and seem some languages modify certain things for sex others don't some modify for number differently they of course have different heritages and connotations and different concepts that are sort of already there

And then also, they all have different, and including created contrived semantic systems as well, they have some commonalities and differences.

So for certain kinds of computational semantics, there may be some very small programs or some very secure ways to do this or that.

And then Tibetan.

Let's just see.

Trust systems.

Evidentiality markers.

Evidential systems.

Direct sensory evidence.

General knowledge or inference.

Personal experience or witness.

With suffixes.

Direct, indirect, witnessed, hearsay, and inference.

Also think about legal ontologies and different ways that this kind of relates there.

So that's the languages folder.

There's all the computer languages and then, I mean, not every single one, but that is where all of the computer languages are.

and the natural languages all right last folder to look at is the speculative design so just to show uh kind of similarly here okay just to quickly return to the examples let's

Push the GitHub, go to the examples, and then look at the three new that were written just now.

POMDP, linear regression, neural network.

So introduction, laying out, what are all these different cases about?

How do they apply to the POMDP?

What does the demonstration do?

Linear regression, background on linear regression.

Laying out what is different and why it matters.

And the neural network case similarly.

Okay, in the speculative design folder, we will add one more document.

Okay.

Dragged in speculative design.

In this folder, write the most vast, vastly layered, comprehensive synopticon of the Shin Dragon, the

or I'll just drag in the single markdown output assembled paper write the most and do it most absurdly and information densely in the style

of something that strangely would make a lot of sense to see visually on a live stream about this topic yet be essentially incoherent in the linear reading conveying all the concepts totally

The user wants me to create a new file, assembled paper.

Okay, we're looking at the chain of thought of Gemini, so-called chain of thought.

They want me to create a new file, assembledpaper.md.

Not really, I didn't say that title, but let's go with it.

It should be a synopticon.

It should be visually interesting but incoherent.

List the files, read the files, synthesize the content, create the file.

How to make it absurdly layered.

All these different strategies.

How to make it incoherent linear reading but make sense visually.

How to convey all the concepts.

file list then it lists out the files in speculative design it's like you know it's not exactly what i thought i wanted to know or see but but it's starting to get to a very interesting place

For each snippet, randomly or pseudo-randomly, maybe, based on file order, intersperse headings, bold terms, list items, and short sentences phrase into the main synopticon string.

Use heavy markdown formatting.

Prioritize density and visual chaos over linear coherence.

Ensure terms concepts from all files are included.

Okay.

Still going.

This is just the beginning.

Incorporating elements.

It's like this is doing quite many cycles of thinking, reading, doing.

lots of tokens being generated here but meanwhile let's let's let's let it cook and uh there's a few that are pretty fun here so

okay so here's a daisy dollyrimple um satire uh free speech usage as they all are where it's a sort of early 1900s detective story

and the first part is is just story then it goes into this truly delightful sequence of uh looking at the first off speculating some some ancient origins looking at the difference i'll turn off dark

looking at different hypotheses sort of like this like mystery science theater 3000 reading of a dollyrimple situation talking about how in their case in their situation in their case in their cases how different processes happened

different context shifting different processes that different ways that that then it kind of comes around at the very end to uh highlight the unique values of the different languages and then I believe they they call it a night and have some Hungarian dinner

okay this was one that I just said make a new speculative design document go go somewhere that's that's never been done or something you know glitch ontology and error-based learning so it's like kind of glitch pop which has like a visual and a auditory uh so it's kind of a multi-modal glitch

But it also comes up in programming and it's kind of a digitized format.

So like different ways that glitch-based methods, we can fix some of that.

New tab, glitch ontology, fix and improve all the mermaid diagrams.

This is probably the longest Gemini sequence.

Okay, so it ended there.

Continue to refine the synopticon and read all papers and then save the final full file.

Could be slight differences in mermaid rendering.

But cool idea.

Glitch-based ontologies.

Like ontologies of errors for describing systems that have certain relationship with error.

Okay, this is a very long one, but an extremely worthwhile one.

looking at William Blake so started with just this bullet point type stuff now starts getting into Blakean engineering like the fourfold vision discussed elsewhere with single vision being like leading up to

metacognitive integrations so it's kind of like this is not dissimilar than a three or four layer hierarchical model like thought seeds or Sanved Smith type model and then it just shows like okay which model in that fourfold vision paradigm

going into detail like this is kind of like the retinal layer with the convolutional neural networks dropout layers that kind of stuff because it's like the sensory so it's like multi-scale hierarchical architectures possible self-similarity of the two three and four-fold visions in terms of their their capacities going into the zoas looking at cognitive functions of the the integrated albion and the different kinds of functionalities

fractal self-similarity with eternity in an hour how would that really get implemented um different cosmological parallels what would really be different with a Blake inspired cerebrum using AI well

single objective, contrary balancing, multi-objective to target the creation and maintenance of tension or paradox architecture, detailed architecture, layers

it's kind of like there's the does this accurately describe what tharmas is in the blakey and ontology it's kind of like a separate question that's interpreting this just as like structured humanities research and just is that uh of the different places this box could be pointing or connected to does that align and and surely there's multiple

uh takes even for one character and then it's like what are the functional attributes of this kind of architecture so what is that transfer path from loose or ambiguous or or multiple or even contradictory poetic description and basically all of these kinds of information processes

generation reality layer belua generative layer that's the twofold vision threefold vision golganuza creative layer and then the eden integration layer okay

there's just too many funny things this this was a funny one the eternity sunrise development so 2025 this very year implement the core zoa's architecture begin contrary processing research so little different spin than predictive processing okay contrary processing research hmm

but that's that sort of unity is plural at minimum two research agenda but then that's also the paradox well but then isn't the single objective unity is plural at minimum two so how to hold the one and the two that's like the single and twofold vision then over the years leading to 2029 eternity's Sunrise self-evolving cognitive architecture begin Albion integration

more sections then really nice process diagrams super information dense

looking at at different and being able to do programmatic generation archetypal description of logistics for perception action cycles so this is kind of like a whole cognitive sequence it's not just saying well there's a for loop that gets an observation and then uh puts out an action or it's like an observation comes in this parallel processing like marriage of heaven and hell status occurs

certain sequences happen then there's a back and forth here and then this all etc it could be make a dynamic control flow in the marriage of contraries let's do make a vast comprehensive dynamic control flow

with massive level of detail of process diagrams something completely capturing the spirit of james joyce

all the most chaotic and river run-esque stuff also do the same in another design file for cerebrum plus infinite jests

Okay, wow, still plugging on that.

Those are almost certainly the two very longest executions I've ever seen from Gemini.

For the sheer number of steps.

Okay?

Then the last one before we push those updates, look at the new ones.

This was as per the request of a friend on John D. who lived from 1527 to 1608.

So...

i was not extremely familiar with d but this was a really interesting approach that brought together those process diagrams um different kinds of cartography spatial computing time cryptography secure communications so it's kind of like john d's work

transferring onto what what was said in that work and what has been said about that work onto the specification of cryptographic or of like esoteric semantic components correspondence tables

active inference topics category theory terms different ciphers we can do a whole okay that was okay that was it did the full assembled paper

Make a vast speculative design document on using cerebrum for cryptographic design, syntax, semantics, cognitive narrative.

This one halted early.

Okay, let's look at the assembled paper.

Okay, Synopticon.

This was prompted again to be incoherent to read out loud, but to give a lot of information when visually looked at.

okay let's look at those the joyce and infinite chest then we'll look at the live chat if anyone has an idea or a question then go into the sort of beyond

So here it didn't make the file, but that's what's so fun about cursor.

Put that and improvements and additions into a file with the right name.

And also this chat is getting long, so that's why there's this start a new chat for better results down there.

Okay, push these.

Okay, Joyce and Jest.

Okay, Infinite Jest.

First one doesn't work.

bayesian funhouse mirrors this sounds clean it rarely is okay and the joists i don't think i can evaluate the details but you know

it's cool okay and there's the cryptographic that's what you know again it's just it's the concept it's it's the possibility that these spaces can be defined up front or on the fly and they can be passed to the right other agents or people or just generated from scratch or in the future in a better way okay

okay so the main piece to sort of leave a little snippet of again in this last part it's going to be just a few uh what's next in our path and then if anybody has a idea or a question live chat that they can write it so let's go into a whole new folder

Beyond cerebrum.

Beyond spec.

Okay.

so given all that information on cerebrum and all it can do with grammatical case let's zoom out and think beyond it let's come up with insects or other inverts

related acronyms conveying absolute the total scope of what can be considered a quote full linguistic intelligence method

whereby all the parts of language verb noun all parts of speech modes etc are firstly formalized into the kind of operations which flexibly

can and do apply to e.g.

linear regression neural network Bayesian graphical model then in that case with full formalization we are only in the tip of the iceberg

in terms of xeno and exo linguistics firstly come to totally understand that then spec out completely this quote beyond cerebrum efforts

Okay, planning a response.

Hormica.

Hmm.

That's a nice ring to it.

Myriad.

Mantis.

Apis.

Locust.

Vespa.

Arachne.

Silk.

All good.

Okay.

Formica Locust Arachne.

Let's go with Formica.

Okay.

First idea is the best one.

Even just pre-writing what I believe or expect I will want to say soon or next.

With all needed files for functions underpinned by the same

functions described in core beyond spec document, which is always growing and improving iteratively, such as forensic work describing

the lexical environments of a model in time and place in use, all modern machine learning, cutting edge questions, interpretability, scaling, safety, pure

entomological linguistic pragmatism wow already jumping to the tools that you know it could be a it could be a uh

Wow, I've never seen a chain of thought that looked like this.

Create directory structure.

Okay, a source folder within, then update the spec.

Create the innate scripts.

Add or read me.

Okay.

Now this is the part I've never seen.

Constraint, checklist, and confidence.

Follow cursor rules.

Yes.

Here's the cursor rules.

One funny prompt in another setting was like, write yourself a prompt that will lead you to generate the prompt that will help you generate the greatest, most accurate, relevant, possible cursor rules file set.

okay so it's like this is kind of where q kids and catechisms come into play which is just developing these sets of questions so oh yeah that's the q78 or like oh you you could you could just use the use the q600 ask all those figure it out you know okay placeholder

for a lexical environment.

So for this one, we can download, we could scrape papers or scrape websites and then use anything from rule-based methods to existing software packages to LLM API calls

to say, okay, given what we know about cerebrum and whatever this formica or whatever system is, given that, what are the entities, what cases are they in?

Yes.

Given the structure now then, comprehensively continue so that all docs

formal specs of the parenthesis, more than language, formal type, theory, category, calculus, and Bayesian cognitive functions are 100%.

100%.

It's not sort of where I hope to and will go over the coming little bit with anyone's open source participation if they're interested listening along this far.

Let's read that doc.

Okay.

Cerebrum provides a powerful framework for integrating case-based reasoning and Bayesian representations, focusing significantly on the role of grammatical case in structuring meaning and reasoning.

While valuable, case grammar represents only one facet of the complex system that is human or potentially non-human language.

To achieve a truly comprehensive computational understanding of linguistic intelligence, we must expand our scope significantly.

new chat tab speculative design looking past even future horizons

write a vast comprehensive speculative design doc with valid mermaid diagrams and tabular intelligence about what glial maximalism really means in group discussion only

Okay.

Okay.

Define base types for fundamental linguistic units.

Phony, morpheme, lexical item, syntactic constituent, semantic concept, pragmatic concept, composite types, dependent types, connections to formal grammar frameworks.

laying out those types research challenges relation okay but just continue i'll start a new chat for better results continue compre intensively with developing

have all needed documents in fractal docs folder, have all source code and tests start running and developing the full simple modular test suite now.

Okay.

Okay, question from the chat.

What kind of predictions might you have about case uses across disciplines, experience in the field or experience with the language?

What would it reveal if case use was different across groups?

Okay.

and another question live chat can we can we mechanically deform the cranium to 10x the cerebrum um probably but you probably couldn't get it back tucker

But I think with a 3D printer and a shipping container, it could be possible.

What would it reveal if case use was different across groups?

Yeah, there'd be so many interesting topics, like the discovered slash invented...

question of case to what extent are to on at from of an object fundamental aspects of semantics are there other fundamental aspects of semantics that we get through or get around in human language in other ways i think that at the very least a first step not the only first step um

not not not a last step but a first step is this kind of beyond cerebrum approach where it's like first get the full interoperable category theoretic calculus type you know algebra etc etc etc for all the languages that we know about partially to scaffold and improve the fluency of people and llms

partially to understand well what are all the state spaces and then if we find that there's something where there's always exactly three in every language even if they were all the same we could still say like well what if it was four or maybe we'll find that there's um one two and many for marking numbers in one language and another one it's one two three and many another one it's just one and many or another one doesn't have that distinction it's like any of those even just seeing two examples is kind of like the arc challenge it's like

You see that this could have been this way or that way, and then there's a whole state space that it could be.

So there could be a special numerosity for multi-agent models where it's like there's a subscript for how many agents there are.

But it can have 50 subscripts coming out of every corner, different fonts, all these different kinds of modifications, and that's just typographically.

let alone how it's spoken or how it gets rendered computationally, where it doesn't even need to be limited by how many suffixes can you add or how many subscripts would make sense in your word processor.

It's about these higher order modifications.

So when people say, I'm talking about sending the data to the Affective Active Inference Models API,

That could be something that gets fully fleshed out, maybe static analysis on certain components, but certainly for scientific integrity, full reproducibility, understand what is that category of models.

And so that is...

the curious language of the incidents which is for the incident which motivates languaging what is the curious languages that let that semantics get communicated okay some more great questions from tucker we will make documents

okay okay it seems like human language taking a survey of different semantics and grammars around the world doesn't need case and that even semantic density is not specifically correlated to case versus no case structure

Yeah, it's highly possible that by kind of sandpapering the language of the exoteric markers of case, like English for the most part, it could be associated causally or just correlationally with an increased or decreased information density.

so consider different ways like per phoneme um whereas like if you're doing stem modifying like a slavic language then maybe there's only so many modifications that you can overlap if you're doing suffixing then maybe you can have more but you know suffixing might lead to very long symbol density

Then again, if there's a certain number of symbols that are needed for the subscripting, I don't think there's a single answer, especially in the absence of empirical evidence in our hands and mandibles.

But we could say, write a speculative design dissertation summary of all the situations

where human languages with and without different case systems would and would not have different information densities and designs considered at syntax, semantic, narrative, cognitive levels.

okay fill in all placeholder methods dart to run log confirm the test suite for formica okay glial maximalism oh wow

being a little bit recalcitrant to write this document, despite its absolutely benign cellular nature.

Write that and improve that into a file.

Okay.

Okay, Tucker wrote, why do so many ancient languages Sanskrit?

I don't know what Usarti is.

Usakri.

Uskari.

Oh, thank you for that.

We should add this to languages.

Oh, it is there as Basque.

Uskara.

Why do they have nutshell?

I don't know.

now fully write out that dissertation tons of tabular formatted crystal good stuff information vast technical mermaid diagrams unique explanations and predictions unknown

and under known through linguists in 2025 wow still still dragging heels on the glial component maybe switch to clot oh great question great question let's ask perplexity

Okay, Tucker wrote, are any modern languages in widespread use gaining in case complexity?

Let's do the perplexity research.

base evolution today dot md okay that's interpreting that okay oops

research and fully empirically address this question from my good friend the answer must ring in his ears like gold and silver and he is a refined listener he expects only the finest information

Okay.

Meanwhile, perplexity.

Let's do the slower or the faster mode of perplexity.

I don't know.

Yeah, that would be a...

This is kind of one approach that could be used.

Beast, which is Bayesian evolutionary analysis by sampling tree.

Let's write out this document.

This is going to be beast case analysis.

Okay.

I'm just going to copy in just one paragraph about beast.

we have so much good stuff happening though okay that one will finish soon that one will finish soon let's just see where it got with the development yeah making good progress on formica okay

with this description of beast write up a fully technical nsf postdoc proposal for using cerebrum and formica parentheses look them up if you need

to do ancestral state bayesian phenotypic reconstruction of ancients linguistic elements we are going way into the past we will use generative ai as needed for creating

creating fluent text audio and visual media given the explicit probabilistic models and empirical data we will be compiling and inferring on ancient and

contemporary languages okay glial maximalism oh it's okay it says it's for internal discussion only i feel like i could share it here though but we'll reset that chat go back to gemini

write it out fully, the full postdoc application.

Thank you.

Okay, glial archetypes.

Oh, this will look better on.

Okay, test infrastructure.

Okay, we stop the agent after 25 calls, continue.

Look at the glial.

Tests, developing, writing that postdoc application.

Okay, but to Tucker's question.

Most linguists agree that human languages tend to simplify their case systems over time rather than develop more complex ones.

Of course, just like in genomic and phenotypic settings, it's like, but then how is there a more complex one?

The historical trend seen in Indo-European languages has been towards case reduction, not expansion.

Latin.

Old English.

Old East Slavic and Sanskrit, all of them have dropped in cases.

Are we buying the dip with respect to case number?

I don't know.

Maybe it could go down further.

Not financial advice, not linguistic advice.

Okay, counter evidence.

Virtually no documented examples of modern widely used languages that have developed more complex systems

as Anatoly Levenchuk wrote in languages cases usually die out it's a known process why would they appear instead of disappearing right why would they appear instead of disappearing absolutely perplexity let's see there's no strong evidence that a modern language but but but could could a language with more cases gain more popularity

are any languages getting keys but english is right now rare but possible oh they debate about mandarin chinese okay deeper research okay but you now you you know what i am going to ask next if that

case reduction is such a slam dunk that it is pervasive today, give me the most complete account of how those complex

many haste systems ever even came to be okay glial cells

okay so centering the role of the glia and then there's the neuronal processing layer but it's like we don't really talk about those in glia science it's more about the kind of multiple types of glia microglial criticism okay so that's kind of like the feedback okay

centering the role of the oligodendrocyte between neurons using formica in the sort of neural semantics of glial okay

emergence of elaborate case paradigms and languages such as proto-indo-european pi sanskrit and finno-ogric languages involve layered grammaticalization processes and socio-linguistic factors the pi case system so this is the one that is mostly used in cerebro the system likely arose through spatial and semantic differentiation but i repeat myself

Analogical extension.

Case markers originally tied to specific semantic roles.

Instrumental for tools expanded to abstract syntactic functions.

Phonological reinforcement.

Sound changes eroded distinctions between stems, prompting speakers to innovate new markers to avoid ambiguity.

Stable case systems.

oh some languages did have expanded case complexity i mean like maybe hungarian or some of these other ones the date of plural um persisted due to its role in marking indirect objects yeah interesting but a little little but but that's exactly

That's exactly what we can analyze with this postdoc application that we're writing.

Just like you could use Beast to look at the genomic or phenotypic patterns.

Okay.

Run the full Formica Pest Suite

develop and improve the formica tests methods visualizations animations documentation okay okay vastly expand the nsf postdoc application add

comprehensive tables, specific explanations and predictions in huge, valid mermaid diagrams.

Okay, still plugging away on Formica.

Okay, we're going to get this postdoc application done, read it on GitHub, see if anyone else has any questions or ideas in the live chat, and then call it a day.

Okay.

Yes.

Develop everything fully.

Run and improve that test suite, which yields full computations and visualizations.

Okay.

All right.

Just save that.

A few improvements on this kind of stuff with like the need to reapply tools and better situational awareness for the code base.

reading folder structures kind of doing better in uh that could be partly in cursor partly uh other other approaches all right speculative design beast

Okay.

Reconstructing ancient linguistic phenotypes using integrated Bayesian modeling and case-based reasoning enhanced by generative AI.

Okay, intellectual merit.

Pioneering the use of generative AI

Specific generative AI model name, e.g.

GPT-4 Tacotron.

I'm sorry.

It is an end-to-end text-to-speech Tacotron.

Broader impacts.

Okay.

Oh, that's going to cost you.

foible yes lexicon swadesh lists perfect it's like there's just so much to learn a lot of it is out there a swadesh list is a list of forms and concepts which all languages without exception have terms for

Okay, new file just dropped.

Swadesh lists.

Yes, run the test suite and add tests that show off and visualize.

More like that.

New chat.

Write a Swadesh list for

cerebrum and formica as language okay thank you tucker then write another file meta swadesh list markdown both maximum length and accuracy

yeah cursor writing these terminal commands running the pi test by itself super big speed up super effective okay hyper swadesh that could be next morris swadesh

he started with a list of 215 meanings falsely introduced as a list of 225 meanings in the paper due to a spelling error yeah hate to see that happen whoa

wow great great topic to have okay this is the swadesh minimal minimal terminology who what what where when all could be associated develop further and furthermore

the swadesh list include all swadesh terms and add case annotation okay then the meta meta swadesh well then okay

Clearly, there has been a kind of misunderstanding.

A metaswadesh list is like a swadesh list for such lists.

A list of all the criterion.

you'd use in the production or inference productions or inference of a swadesh list coming to a file near you

Okay, Formica is up and running.

Add a visualization script for Formica.

Ensure that running the test suite yields copious, accurate visualizations and .gif.

Okay.

You're doing okay with the Swadesh lists.

Okay.

Let's look at the first, the layer one Swadesh list.

The L1.

yes perfect like this is the basis of what we need for formica which is what what are the what are the 200 or so concepts that we need in formica for total ant colony modeling what are their parts of speech

here nest core their forage zone so it's like home and away like over here over there many many mandibles other alien scents one two three four five

i guess you don't even need six maybe you call that just like legish or something okay different different um different creatures in the nest okay body parts actions natural colors

adjectives directions great okay okay now we're on the right page with the meta swadesh list now write that meta swadesh list fully

200 concepts or so please.

Yeah.

Run the test suite, confirm and log where visualizations are made.

Okay, I understand.

You want a significantly more detailed and exhaustive breakdown of criteria, principles, and considerations related to Swadesh lists.

Okay.

Okay.

Our next album.md.

Okay, new chat.

Prediluvian Superswadesh is the name of our next album.

Write out all the song names and lyrics.

And in callouts, describe the bat sound.

Meta Swadesh list, we're there.

It really did go up to 207.

See also external resources on Swadesh lists, lexicostatistics and glottochronology.

write another comprehensive document on glottochronology.

Okay, 27 tests.

Looks like Beyond Cerebrum is coming together.

Let's see where it's saving things to.

Yes, run the script and then run a terminal command to confirm where the visualization is saved to.

glottochronology is a controversial method within historical linguistics that aims to estimate the time depth since two or more related languages separate from a common ancestor it attempts to do this by quantifying the degree of lexical replacement within a specific subset of core vocabulary yeah i think bayesian methods could help a lot there

okay our album we're gonna look at this album on github all right this is gonna be this will be the last piece i think people get it it's possible to um go beyond cerebrum in like a 20 minute period

okay we'll close with this the pre-diluvian super swadesh so it speaks first primordial murmur obsidian shards on the shore the cartographer's dream that's an instrumental occasional occasional sonar pings or distorted radio static hint at failed surges and forgotten geographies

whispers from the ziggurat's core arc construct gopher wood and pitch 40 days delusion dissolution ararat landing new soil old sun well

hope this has been a uh interesting stream for people uh

it's all Creative Commons by NCND so if you want to work on it feel free to make contributions to the repo if you want to collaborate on a future part of the formica stack there's uh there's always a always a way and we should be

We should be on our way.

Okay.

That's it.

Looking forward to people's comments.

Thank you, Glea Maximalist and Jeff and Tucker for the live chats.

Okay.

Bye.
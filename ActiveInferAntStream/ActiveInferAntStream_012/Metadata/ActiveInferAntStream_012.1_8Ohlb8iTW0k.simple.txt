SPEAKER_00:
All right.

All right.

It is April 7th, 2025.

This is going to be active inference stream 12.1.

Cerebrum, case-enabled reasoning engine with Bayesian representations for unified modeling.

I've started first to make a GitHub push to the GitHub repo, Active Inference Institute slash Cerebrum.

And that is now live.

and here in cursor i'm running the render script render markdown which calls the script render mermaids which takes this markdown file and all of these 15 figure level markdown files and the math and the novel case appendix and is going to result in a pdf

The PDF looks like this.

In this stream, we're going to go through the PDF and talk about probably a bunch more, depending on who shows up.

I'm going to finish flipping through the PDF.

Then we will see where the PDF generation is at.

And then we will go through this.

This should be super interesting.

Conclusion, Appendix 1, math.

Appendix 2, novel cases.

All right.

Degeneration will finish soon.

It renders each of the 15 mermaid diagram figures, which look like this, into...

figures that look like that and that inserts them into the markdown all right pdf generated it generates cerebrum.pdf so that's the newly rendered version and i'll push it and then let us get into the topic i also just push this to zenodo

and looking forward to where we go from here so we're all pushed on the repo let's look at the paper and go through some of the sections all right i'll read the abstract first

This paper introduces case enabled reasoning engine with Bayesian representations for unified modeling, Cerebrum.

Cerebrum is a synthetic intelligence framework that integrates linguistic case systems with cognitive scientific principles to describe, design, and deploy generative models in an expressive fashion.

By treating models as case-bearing entities that can play multiple contextual roles, like declinable nouns, Cerebrum establishes a formal linguistic-type calculus for cognitive model use, relationships, and transformations.

The Cerebrum framework uses structures from category theory and modeling techniques related to the free energy principle in describing and utilizing models across contexts.

cerebrum addresses the growing complexity in computational and cognitive modeling systems eg generative decentralized agentic intelligences by providing structured representations of model ecosystems that align with lexical ergonomics scientific principles and operational processes and we will probably push to another version by the end of the stream so we'll fix a few different things that we can see along the way describing and utilizing

models across contexts.

Okay, continuing to overview.

Cerebrum implements a comprehensive approach to cognitive systems modeling by applying linguistic case systems to model management.

This framework treats cognitive models as entities that can exist in different cases, as in a morphologically rich language, based on their functional role within an intelligence production workflow.

This enables more structured representation of model relationships and transformations.

And the code to generate this paper and further open source development from this 1.0 milestone is available at this GitHub repo.

And that's what we'll be pushing to and using during this stream.

Okay, background sections.

First background sections on cognitive systems modeling.

The second background section is about active inference.

third background section is about linguistic case systems and i'm going to read this one because this might be the one that's the wild card however this is the crux and as a first language english speaker as we'll discuss there are some interesting ways of having the

cases a bit more silent in English than some other languages like Russian and Latin and other things that will probably end up on Wikipedia to find out today so I'm going to read this linguistic case section linguistic case systems represent grammatical relationships between words

among words, through morphological marking.

Case systems operate as morphosyntactic interfaces between semantics and syntax, encoding contextualized relationship types rather than just, or in addition to, just sequential ordering.

This inherent relationality makes case systems powerful abstractions for modeling complex dependencies and transformations between conceptual entities.

Cases under consideration here include nominative, subject, accusative, object,

dative , genitive , instrumental , locative and ablative all serving different functional roles within sentence structure.

English has largely lost its morphological case system

The underlying case relationships still exist and are expressed through word order and prepositions.

For example, in The Cat Chased the Mouse, the nominative case is marked by position rather than morphology, while in I Gave Him the Book, the dative case is marked by the preposition to and word order.

This demonstrates that the semantics slash chemiosis slash pragmatics of case relationships are fundamental to language structure, even when not overtly marked morphologically, e.g.

expressed in writing or spoken language.

Section 134, Intelligence Case Management Systems.

So this sort of speaks to the pun at the heart of this, which is case management,

which might be for like a help ticket or a research inquiry or any other kind of case management.

It's a bit of a hinge with the linguistic case systems.

So coincidence, but enough of a connection and gap there to have the linguistic case system be the theory

and the category theory and some of the transformations that are going to be going into, and then combining this with more of an operational layer and thinking about what are the different roles that generative models really do play in different kinds of case managements seen more from an operational view.

Okay, section 1.4, towards languages for generative modeling.

I'm going to read this section.

The active inference community has extensively explored numerous adjectival modifications of the base framework, including, for example, deep, affective, branching time, quantum, mortal, structured inference, among others.

Each adjectival prefixed variant emphasizes specific architectural aspects or extensions of the core formalism.

Building on this, Cerebrum focuses on a wider range of linguistic formalism, for example in this paper, declensional semantics, rather than adjectival modifications or prefixing.

In this first cerebrum paper, there is an emphasis on the declensional aspects of generative models as noun-like entities, separate from adjectival qualification.

This approach aligns with category-theoretic approaches to linguistics, where morphisms between objects formalize grammatical relationships and transformations.

Many such cases.

By applying formal case grammar to generative models, Cerebrum extends and transposes structured modeling approaches to ecosystems of shared intelligence while preserving the underlying partitioned, flexible, variational, composable, interfacial, interactive, empirical, applicable, communicable semantics.

1.5.

the intersection of cognitive systems modeling, active inference, linguistic case systems, and intelligence production.

1.6 Methods and Materials

The Cerebrum framework was developed as part of a broader synthetic intelligence framework combining linguistic theory, cognitive science, category theory, and operations research.

Key approaches included linguistic formalization, category theoretic mapping, and algorithmic implementation.

Figure one, here are those foundation domains flowing into Cerebrum.

The functional and the processional elements of the framework.

different features and system outcomes leading to enhanced model management.

Core Concept 1.7 Cognitive Models as Case-Bearing Entities Just as nouns in morphologically rich languages can take different forms based on their grammatical function, cognitive models in cerebrum can exist in different states or cases

depending on how they relate to other models or processes within the system.

Figure 2 illustrates this linguistic parallel.

So here, provocatively laid out generatively, on the diagonal are the cases and how they'll be abbreviated

nominative accusative dative generative generative instrumental locative ablative invocative up into the right are examples with the cat as noun down into the left are the analogous functional roles and sort of parts of speech as in describing to the generative model which is what the focus of this is

Okay, going into a bit more detail about what do these different cases do?

What is the point of being able to decline a generative model like a noun into these different cases?

So here is three clusters of cases.

On the left are the genitive and the ablative case.

and ablative as its name sort of even suggests was relatively earlier lost and so it's not always seen but you know it's these things are just popping up and down in different languages really interesting ways and generative which that sort of pun with genitive and generative is going to come back here the generations coming from something or the possession of something

locative instrumental invocative here are clustered with the contextual cases so these and also the primary cases with doing the the direct agent doing the action like with nominative like the cat jumps over the moon

or in the accusative or the dative being the recipient of action so different cases that might be used linguistically in describing like the source or origin like is that your guitar from where did that guitar come from it was brought from here it is possessed of this person

Those are sentences that would be about some sort of material noun that might use these different cases to provide those functions.

And here it's like an analogy extended into talking about joint distribution generative models.

Here's a table with all of the cases that are going to be considered.

The abbreviation, the full name of the case, the function and the example.

So nominative model is active agent.

Cat jumps over the moon.

Model generates predictions.

Model does something.

This is sort of the base model.

Just declaring the form that's in the dictionary.

accusative and dative this is model as object and recipient and these relate to processes that are inputting or targeting the model like some method applied to the model or something from another source being received by the model

genitive the output of the model something that comes from the inferences and the ablative possibly very similar or redundant or not needed in situations but related to basically the difference between the ablative attribution and the generative

instrumental this is a possibly a very common case for using when talking about a model analysis performed with something like the implementation of a model as a model and the vocative model as addressable entity

example hey model x all the different names that these chat assistants have so calling something by name direct invocation task initialization and documentation reference point like referring to a parameter as its parameter in a system or or something like that maybe there's different you know subtypes

um okay 1.9 a preliminary example thank you for your question dongyeob i will get to it i want to get through the paper mostly in one sweep and then happy to look at your question and any other questions people have all right 1.9 a preliminary example of a case bearing model homeostatic thermostat

Consider a cognitive model of a homeostatic thermostat that perceives room temperature with a thermometer and regulates temperature through connected heating and cooling systems.

This is kind of the classic model.

simple homeostatic in nominative case the thermostat model actively generates temperature predictions and dispatches control signals functioning as the primary agent in the temperature regulation process so that's kind of like an operational utilization or generic situation considered holistically version that's in the dictionary

When placed in accusative case, the same model has become the object of optimization processes, with its parameters being updated based on prediction errors between expected and actual temperature readings.

Change of mind, change of world.

In dative case, the thermostat model receives environmental temperature streams.

These can also be ongoing simultaneously, which is going to be gotten to later with the precision modulation.

and occupant comfort preferences as inputs.

The generative case transforms the model into a generator of temperature regulation reports and system performance analytics.

So basically synthetic data generation or genitive AI, like generative AI.

When an instrumental case, the thermostat serves as computational tool implementing control algorithms for other systems requiring temperature management.

It's being used like a tool.

The locative case reconfigures the model to represent the contextual environment in which temperature regulation occurs

Modeling, building, thermal properties, so perhaps locational topics, or discussing something within the model as a location, like model as place.

Finally, in ablative case, the thermostat functions as the origin of historical temperature data and control decisions, providing causal explanations for current thermal conditions.

This single cognitive model thus assumes dramatically different functional roles while maintaining its core identity as a thermostat.

Figure four, generative model integration in intelligence case management.

First place a good render from above.

Here's just different stages and different ways in which stages of research and intelligence work relates to different case aspects being highlighted.

110, declinability of active inference generative models.

At the core of Cerebrum lies the concept of declinability, the capacity for generative models to assume different morphological and functional roles through case transformations, mirroring the declension patterns of nouns in morphologically rich languages.

Unlike traditional approaches where models maintain fixed roles or variable roles defined by analytical pipelines,

so which is to say that there's some process by which like well now we're going to specify it declaratively then we're going to do this thing that updates it then we're going to generate some data from it then we're going to do this thing that updates it then we're going to generate some data then we're going to give a final updating and then we're going to use it as a tool

some process like that which is just defined procedurally here gives a more expressive layer on top of what even that is and expands out and generalizes what that space of procedures is materialized from so what are those changes that happen for generative models

It's not necessarily that some suffix, like in the case of a noun, the end of the noun might change.

And there's probably different ways this happens in different languages.

So what is it that actually changes by analogy in when a generative model is actually declined into these different ways, cases?

So one, functional interfaces could change what is being received and or transmitted.

Parameters across patterns.

So what kinds of parameters are in play?

Prior distributions could be a different prior.

Maybe if depending on whether it's playing one role or another, something is more or less likely to happen.

update dynamics in what ways does the model update if at all and perhaps different computational resourcing also including different thresholds or approaches so one approach being there's a zero to one waiting like an attention or a precision on all these cases in play at once or one could have a one hot and some kind of other rule

for just only computing one of these settings.

And then depending on how you parameterize what each of these settings would be, it's not that receiving and updating has a higher computational cost than being used to make an action selection, but there could be a setting where receiving and updating is very simple.

There could be a setting where that's very complex, same as for action selection.

So what happens when

the generative model gets this modification here table two describes how each of the cases different changes could happen parametrically including structure learning and what happens at the interface of the model how does that relate to a precision weighted generalized approach kind of like the Sanford Smith approach

Consider a perception-oriented generative model M with parameters Theta didn't render.

Let's fix that.

The Theta for parameters does not render update, so it is plain text.


SPEAKER_01:
data and check anywhere else that might be relevant.


SPEAKER_00:
So M is the joint distribution generative model.

Here's another list of those cases being used in different ways.

The vocative case

Voc represents a unique functional role where models serve as directly addressable entities within a model ecosystem.

Unlike other cases that focus on data processing or transformational aspects, the vocative case specifically optimizes a model for name-based recognition and command reception.

This has particular relevance in synthetic intelligence environments where models must be selectively activated or woken up through explicit address, similar to how humans are called by name to gain their attention.

The vocative case maintains specialized interfaces for handling direct commands, documentation references, and initialization requests.

In practical application, models in vocative case might serve as conversational agents awaiting activation, document reference points within technical specifications, or system components that remain dormant until explicitly addressed.

This pattern mimics the linguistic vocative case where a noun is used in direct address, as in, hey Siri or okay Google, activation phrases for digital assistants, creating a natural bridging pattern between human language interaction and model orchestration.

This systematic pattern of transformations constitutes a complete declension paradigm for cognitive models, using precision modulation to fulfill diverse functional roles while maintaining their core identities.

now pivoting from that linguistic case focus to thinking about the role of those different cases in model workflows

That is in figure five and six.

So here's an example of how one kind of generative model might move through these different cases.

So first a model specified in nominative, it processes data as a direct object.

it's used to generate source synthetic data then it is the the sender and or receiver of information at different levels from the sort of fine-tuning and context layer here like a feedback between the analyst and the results at this layer

or at the second layer, a feedback involving data selection or attention and or even reaching back to the model's structure.

Here's figure six more explicitly in the context of the intelligence production workflow.

so here's data collection gathering of raw data uses as instrumentally activating some kind of model this is just one example motif processing and and defining the model

providing locational situational context, generating and evaluating data with context changes, then generating from the refined model, deploying the refined model, one possible pipeline.

Figure 7 has some of the category theory connections and seeds that are mainly drawn from other research in the category theory of linguistic case.

So tentative, just like all of it.

Figure 8.

more types of transformations so for example let's start with the nominative with the sort of agentic framing there might be some kind of transformation like objectification that combines with external data to result in a well-formatted processed model in the accusative case

some type of targeting from or to that transformed object could bring it into being addressed or being used as an actor or recipient or whatever the analogy is in the dative then in the generative different kinds of cycles

Figure 9 goes into a sort of esoteric but interesting aspect of linguistics.

Again, something that's really implicit in English, or it was hard to even sort of think about how different this was, but that's kind of the cool part about language.

so morphosyntactic alignment and the difference between the ergative-absolutive patient-oriented modeling and the nominative-accusative the agent-oriented modeling and the reason why this box is in the same color is there might be some operations that differ between these two or it might be a dual representation but from the point of view of something downstream in the process

that is an example of a kind of morphosyntactic difference that perhaps one underlying situational reference source could be expressed in an arrogative absolutive frame or language or in a nominative accusative.

1.14 goes into more detail into implementations and how this could play a role with the different cases being relevant at different stages of implementation.

It's pretty interesting to think about how different kinds of resourcing and costs of different operations

Just like how with a transformer or LLM, there might be a different amount of computations required to train it for the first time, to fine-tune it, to fill its context window, to just do the embedding, to generate data.

All these kinds of things have different computational costs and they have different scalings.

And that is going to depend a lot on the state space of the model.

But it also depends, given the state space of the model, about all of these more pipeline scale modeler degrees of freedom.

How do we actually host the data and all those kinds of training?

How many times do we actually do different steps?

So here's one in figure 11, one example again of that.

Some model is used in raw data collection.

A model is activated

through the vocative and these different steps can happen and possibly another or multiple cases of the very same generative model or different models that are in different cases just like a given step of a story or a given situation might even in the same sentence have just one case or it could have multiple cases

in a compound sentence because it could be like the guitar of that person was used by this person to do this leading to this at this location all those kinds of sentences that that happen another visualization just with a different clustering but similar to 11 and 12 and we maybe we can update 12.

um here's how linguistics cognitive systems modeling active inference and intelligence production make contributions and also relate to cerebrum kind of revisiting that all right 1.18 related work so here's an important epistemic disclaimer

Cerebrum builds upon several research traditions while offering a novel synthesis.

In this first paper, there are no specific works linked or cited.

Later work will provide more detail and reference in derivation.

The work stands transparently on the shoulders of nestmates and so is presented initially as a speculative design checkpoint in the development of certain cognitive modeling practices.

there's cognitive architectures backgrounds everything with knowledge modeling cognitive architectures active inference etc um figure 13 here's how one model might relate to um different utilizations within the same active inference predictive processing hierarchy

So here's a model being used in different cases with the top-down prediction and the bottom-up errors.

Here's describing different kinds of aspects of the agent, mapping the active inference ontology to different cases that it is in, or could be said in.

And one way to possibly implement that is precision weighted message passing and free energy minimization.

so figure 14 goes into a little bit more of that so the whole box is yellow again to have a sort of symbolism like from the outside this could be a multiple dispatch and it might matter how different message passing plays out but there's also another level where it can be just coarse grained to that level at least conceptually if not in the implementation dispatch

So here's the instrumental, nominative, accusative, dative, genitive, and vocative and how they might connect to the error and prediction motif of a two-layer

hierarchical predictive processing.

So just different kinds of modes might relate to different modulations of features of a hierarchical predictive processing architecture.

So kind of like different positions of the hand on the guitar could lead to resonating cymatics, vibrations, different way.

15 is the model case calculus framework so here's some more bringing together of how the mod here with a subscript notation for different cases showing how different functional roles and cases relate to subscript changes which can be applied through to be determined to be invented slash discovered

not necessarily infinite or open-ended but certainly definite calculus laws like what kinds of model transformations and concatenations and composabilities are valid within which understandings of case and which ones also perhaps by definition perhaps by implication are invalid given a case approach in a language

More background references could be found with category theory, active inference, linguistic computing.

Okay, 1.19 practical applications.

Here's a few situations where this could be really practical to apply.

One is in model pipeline design and optimization and the resource optimization as sort of mentioned earlier.

Here might be given some constraints on resources of different kinds.

What kinds of cases one might be expecting to want their model to be optimized to be in.

For example, let's just think by analogy to like an LLM.

Let's just say that there's

A few architectures that have the same score on a benchmark, but one of them is 10 times more expensive in RAM or storage or CPU or money or whatever it is to train, but then it's cheaper to run.

Or it's cheaper to train, but it's more expensive to run.

Or it's able to have a larger context window, but it costs more to do this, or it has some other trade-off or something like that.

So on those Pareto optimal sort of constraints, manifolds within empirical computational finite space, just from a pragmatic perspective, resource allocation, that might be able to guide what features

at the meta modeling level how do we want to think about it what functional rules even matter to benchmark what tests should be written around what kind of functional roles for what kind of artifacts table six some cross-domain integration patterns so just sort of like associating differently and more how these cases can be used even one domain unsurprisingly

like reasoning, might have multiple cases deployed.

Just like if we were going to talk about somebody in a library and they're engaged in perception.

So it's like

that might use a lot of sentences involving nominative or accusative like they looked at the book and so that might put the book as the recipient of action in the accusative so the same type of agent and artifact based ecosystem of share intelligence model that kind of describing perceptual gestalt aspects whatever components

might involve a lot of like active agents and recipient agents talking about reasoning might be talking a lot with instrumentality and context talking about planning might be about goals of an agent

telios and the ablative history in terms of where where did it ablate from like what was removed or what was generated this is a case we can look more into or write a new section on what does the ablative mean or something like that and in the action case there might be active agents and and recipients kind of like the accusative

1.19.5 this is describing some ways that this kind of semantic labeling on a knowledge graph could be used to structure just like if it were going to be a graph of the food web or ecosystem of some region in the sort of hyperlink style or unstructured edges like obsidian

it might be ambiguous, like, well, what does this edge mean?

Does it mean it's a type of, and there's exactly that, which is what structured ontological approaches exist for, and constraint, either interpreting those linguistically and or constraining the edge types that are in play

two kinds of linguistic assertions is possibly a pretty parsimonious and ergonomic way to do it because it would reflect the kinds of expressions people would say about the ecosystem 1.19.6 immersion behaviors and model collectives

When multiple case-bearing models interact within an ecosystem, emergent collective behaviors arise from their case-driven interactions analogous to how linguistic communities develop shared understanding through dialogue.

So that could kind of get into some of these linguistic features.

in discourse community, not just loading up context windows of each other and then continuing the conversation, which works really well in the prompt engineering multi-agent modeling approach, really common with LLMs, Agent Laboratory, Andrew Pache's work, all that kind of stuff.

This is getting a little bit more towards dialogic

linguistics rather than just script context in context learning it's a different description space for dialogue 120 future directions so this could include making some programming libraries to do various features some of which we might look into today building visualization tools

expanding to other linguistic elements so here to really make the point i wanted to focus on the nouns and the declining of nouns but there could be other uh features like talking about different time tenses and so on

open source stewardship, computational complexity estimates, multiple dispatch systems for the methods, connection with different database methods and queries, looking at different cognitive security aspects.

okay I'm gonna read the conclusion then we will quickly go through the two appendices and then re-render it and then see where that kind of gets us so 1.21 conclusion

Cerebrum provides a structured framework for managing cognitive models by applying linguistic case principles to represent different functional roles in relationship.

This synthesis of linguistic theory, category mathematics, active inference, and intelligence production creates a powerful paradigm for understanding and managing complex model ecosystems.

By treating models as case-bearing entities, Cerebrum enables more formalized transformations between model states while providing intuitive metaphors for model relationships that align with human cognitive patterns and operational intelligence workflows.

The formal integration of variational free energy principles with case transformation establishes Cerebrum as a mathematically rigorous framework for active inference implementations.

The precision-weighted case selection mechanisms, Markov blanket formalizations, and hierarchical message-passing structures provide computationally tractable algorithms for optimizing model interactions.

These technical formalizations bridge theoretical linguistics and practical cognitive modeling while maintaining mathematical coherence through category theory validation.

The Cerebrum framework represents another milestone in a long journey of how we conceptualize model relationships, moving from ad hoc integration approaches on through seeking the first principles of persistent composable linguistic intelligences.

This journey, really an adventure, continues to have profound implications for theory and practice.

By here incipiently formalizing the grammatical structure of model interactions,

Cerebrum points towards enhancement of current capabilities and opens new avenues for modeling emergent behaviors in ecosystems of shared intelligence.

As computational systems continue to grow in complexity, frameworks like Cerebrum that provide structured yet flexible approaches to model management will become increasingly essential for maintaining conceptual coherence and operational effectiveness.

All right, to the appendices.

all of the equations are referenced in the text equations first the generic variational free energy here applied to the structural model of the case transformation equation two describing cases in terms of their structured blanket interface

equation three beta precision modulation feature that precision weights case equation four doing partial derivative case specific gradient descent on free energy equation five planning over cases with expected free energy

Equation 6, Bayes factor, to get a posterior odds estimator between two structurally divergent models.

Equation 7, how free energy is minimized with the case transitions.

Section, Appendix 2, Section 1.2, message passing rules for different cases.

This is going into a bit more detail from the image that was shown from this section with the different updating functions in the predictive processing hierarchy.

Equation 13, temperature precision weighting.

14, resource weighting.

section 2.1.4 goes into the some of the novel cases that are in the second appendix 2141 has the appendix alphabetical pretty much everything renders fine of the different variables used okay this should be the second appendix so let's update that


SPEAKER_01:
This is showing up as 2.2.

Please update it to be Appendix 2.


SPEAKER_00:
You're changing the

and or render steps right but it'll still be the same material okay

The Cerebrum framework not only operationalizes traditional linguistic cases, but potentially enables the discovery of entirely new case archetypes through its systematic approach to model interactions.

As cognitive models interact in increasingly complex ecosystems, emergent functional roles may arise that transcend the classical case system derived from human language.

So here's some

anomaly oriented ways and bottom-up empirical ways to discover new cases and then here are three speculative novel cases the conjunctive which has to do with sensor fusion type fusion of predictive streams into joint prediction like federated inference

a speculative novel case recursive case model applied to itself and then a third possible case metaphorical case so these are some interesting paragraphs and then we're at the end after this and I'll look to the live chat so I'll look to your questions don't you up and anyone else

A third potential novel case is the metaphorical case, Met, which would enable a model to map structures and relationships from one domain to another, creating computational analogies that transfer knowledge across conceptual spaces.

So it's kind of like a simile.

It is a simile.

It's like a simile.

Because it is when a model, it's like saying, I feel like a buoy at sea, or I'm hot and cold like a thermometer in this location on that day.

In this metaphorical case, a model acts as a transformational bridge between disparate domains, establishing systematic mappings between conceptual structures.

This case would be particularly valuable for transfer learning systems and creative problem-solving algorithms that need to apply learned patterns in novel contexts.

The metaphorical case would introduce unique cross-domain mapping functions as formalized in equation 18.

The key innovation is the structured alignment of latent representations across domains , enabling principled knowledge transfer that preserves relational invariance while adapting to target domain constraints.

metaphorical case has rich connections to multiple domains of human cognition and communication in affective neuroscience and models how emotional experiences are mapped onto conceptual frameworks explaining how we understand emotion through body metaphors like heavy heart burning anger in first and second person neuroscience metaphorical mappings enable perspective taking and swapping and empathy through systematic projection of one's own experiential models onto others

Educational contexts leverage metaphorical case operations when complex concepts are taught through familiar analogies, making abstract ideas concrete through structured mappings.

The way people converse about generative models often employs metaphorical language describing models as thinking, imagining, or dreaming, which represents a natural metaphorical mapping between human cognitive processes and computational operations.

Learning itself fundamentally involves metaphorical operations when knowledge from one domain scaffolds understanding in another.

Perhaps most profoundly, the metaphorical case provides a computational framework for understanding how symbols and archetypes function in human cognition, as cross-domain mappings that compress complex experiential patterns into transferable culturally shared representations that retain their structural integrity across diverse contexts while adapting to individual interpretive frameworks.

here's a table summarizing the speculative novel cases here conjunctive recursive metaphorical right that is the paper here is the cursor agent that is re-rendering the entire pdf with a few changes that we have made let's see if it finishes and then just open

Here's versioning.

Alright.

modify the render script, modify the source code, go back and forth a few times.

So that one finished, but now it's started another one.

So just to confirm, I'll delete that.

Now this one, when the PDF reappears here, it'll be the new one.

Here's the figures in the output.

Those are generated from these mermaid graphs.

Then the appendix.

All right, check back to the stream.

All right, this is using cursor version 0.48.7.

For the LLM, I've been using Claude 3.7 Sonnet, and also, maybe half the time, Gemini 2.5 Pro EXP 0325.

All right.

So, let's see that PDF get regenerated.

Okay, we went over the paper.

Went over the key diagrams, talked about some of the different roles that generative models play, the kind of modes or postures, or by analogy to how a noun might be in different positions in a sentence, like is it the cat spewed tears everywhere, or tears were spewed upon the cat.

that still has cat in english but other languages have other more visible modifications here the pdf was generated and there it is let's see if it updated table contents okay didn't fix it


SPEAKER_01:
Hmm, still looks like it is section 2.2, and in table of contents, check again, and please exit.


SPEAKER_00:
And then just sometimes it can be that context window.

So starting the new chat and also that control T starts up a side chat.

So for example, here we can say,

in docs comprehensively given the paper write up a documentation library for specifications for starting to build this in robust modular multi-language multi-setting way then new chat

So we have the paper.


SPEAKER_01:
Write a new tool.


SPEAKER_00:
Make graphical abstract that will make a big PNG and PDF

of a big epic graphical abstract with the author and title and abstract information hardcoded and all and a spread of all 15 images in grid

Looking like awesome conference poster.

Thank you.

Here's the documentation chugging away.

Okay.

Missed the boat on that one.

Graphical abstract tool in development docs folder.

And there's so much stuff with the MCP and the MDC files, other things.

All right.

I'm going to copy a bunch of the comments.

So yeah, anyone else write comments and I'll paste them in.

Okay.

Sorry, I'm pretty new to your channel.

Are you trying to make your own LLM or something?

Not trying to make my own LLM.

Looking to do many things.

Looking to learn and apply active inference.

Make it rigorous, applicable, accessible.

Figuring out synthetic intelligence methods.

So things that integrate.

It could be a chain of LLMs.

It could be no LLMs, so different flexible methods for yesterday, today, and tomorrow in compositional systems.

I think consciousness or the experiencer uses self-model, which is pretty much a memory pattern about belief on self that is self-reinforced in a nonlinear neural network structure like LLM.

So I think this cerebrum model is constraining if you are looking to study consciousness.

yeah that's a great point or question i didn't uh mean for this to be a consciousness resolver in any way i could see someone talking about the different functions like when the mind generates experience is it recipient of action

Is it a host of action locatively?

Is it a generator of action?

All these different ways in which I think this could help reduce uncertainty and expand the hypothesis space and expressivity, like even related to consciousness.

but that's not what this framework is about but I think it still may be even thinking more about awareness or self-awareness or relevance realization or something that doesn't have the same phenomenological first-person experiential even though that was mentioned and I think that's an important modality for it to be used in

But the fact that it's called Cerebrum, it was just sort of an acronym that fit.

Maybe let's make a new chat.

Only three can be open at once.

We'll queue up that one.

please given just come up with a long list put it in docs of insect related brain and cognition cognitive terms that could apply to the whole project just at the

high level call sign.

Like Cerebro.

Once that one finishes.

All right, the make graphical abstract is running.

The script is running.

Okay, so I agree.

It's not meant to constrain even at all.

Wasn't like that today for me.

study consciousness let the llm-like structure learn about self through its senses and form beliefs about self-identity as memory pattern yeah that's an awesome point like for llm or perhaps for multiple or any kind of system that firstness of like the primary syntax of the neural network or the material metamaterial substrate

and the way that that's kind of like sub-semantic and possibly even sub-syntactic, that's something that's really explored in like Gödel Escher Bach and other kind of Hofstetter type thinking, like computers don't really play chess, they do statistics, but they don't really do statistics, they just follow laws of physics.

so that is where a lot of active inference theorists and thinkers talk about mortal computing embodied self-evidencing self-identity meta-awareness of identity because it connects that primary modification of the substrate with a perhaps what you're calling like an llm-like structure

and the symbolic and higher order nested symbolic discretized cognitive structures like knowledge graphs much like human infant human baby doesn't have strong sense of self but develops through self-reinforcement through experience llm has intelligence to grow self-awareness

Yeah, that's definitely a claim to explore.

However, talking about babbling and refining at structural and fine-tuning levels of the sense-making, proprioception, active motor feedback, all these features like babbling, motor babbling, verbal babbling, Chris Fields talked about that.

This is wrong because that's how human brain works.

This is wrong because that's not how human brain works.

I don't know, but great question.

Okay.

thank you for the comments I'm going to copy another comment from live chat in and then anyone else can write any question in the live chat and I'll stay for a little bit longer and do a few more things so Rohit wrote I just started watching the stream how exactly does Cerebrum work all right let's get a new document how exactly it works

This window is rendering the PDF.

When the Morse code pops up, it's pretty close to the end.

Yep.

But it's getting impatient.

But it should end very soon there.

Then the graphical abstract tab.

The outputs are saving the output as graphical abstract.

Cerebrum output.


SPEAKER_01:
Okay.

Awesome.


SPEAKER_00:
And so output folder, graphical abstract.

Okay.

PDF up PDF works well as a block square.

PNG, perfect.

Could upload that to a PNG.

All ready for the undergrad research expo.

All right, let's reset that one.

Do the insect acronym.

Docs writing is continuing.

The PDF is re-rendering.

Once the docs finishes, we'll ask Roy its question.

Rerendering in the first tab.


UNKNOWN:
Oh.


SPEAKER_00:
By the letter.

Antenna, Apis, Abdomen, Arthropod.

Brain Case.

Beetle, Butterfly.

Cocoon, Compound Cricket.

Just more length, more comedy.

more funny stuff add more to the insect acronym list so there are multiple for every letter include chaminoptera

Pogonomir Max.

Barbados.

Antiferns.

right it wrote the documentation yeah um pdf is still re-rendering add a folder within docs

called languages that has language specific unpackings and elaborations for specific languages per markdown be absurdly comprehensive in conveying and

tabling how cerebrum relates with details and tables to such case languages with diverse case paradigms including

latin russian sanskrit other languages each per file all right okay okay insect let's see what poggin remix

Probabilistic Operational Generative Ontology for Neural Organizations with Multi-Level Yielding Reasoning and Model Exchange Exformations.

Pretty good.

Pretty good.

StigmaG.

Unprompted.

Directly.

Systematic Transformation of Information with Generative Models for Emergent Reasoning and Goal-Driven Yielding.

This one is making the language specificity.

That's still rendering.

Let's go to the chat window.

Control T. Alt T. Right.

We'll look at the docs.

just getting back to the stream write a docs called how it works my colleague wrote how exactly does cerebrum work

and we want a one-stop shop for exactly that question it can hyperlink to other documents and as needed in docs so that

The inquiry is respected from vast number of relevant angles.

Okay.

All right, whatever is happening with the PDF generation is pretty slow, but it does work.

There's probably way better ways to use Pandoc and all of that.

Let's push the update and then look at the documentation.

Let's let it make Russian.

Okay.

How it works is in progress.

Improving or at least digging us deeper into the rabbit hole with render markdown is happening.

we will have Latin Sanskrit and Russian in a second and then we'll see what is it uh with that minimal outlining what what will it do for each language

Okay, new rendering is happening.

We'll just update it when it finishes at all.

All right.

So in the Cerebrum repo.


SPEAKER_01:
Docs.


SPEAKER_00:
I'm just opening up all the markdowns and queuing up languages, and then we'll have it just continue to write more languages.

Continue to add technical relevant detail to all extant languages and add more rare, interesting, useful, cute, funny, interesting,

formal languages once it finishes there so we'll just be peeping as soon as the generating goes away okay

this document opened in no particular order how is active inference integrated here's moving towards we can ask it to expand more upon but let's just have kind of a running set of things that we want it to to do in the background just while we're

inactive inference integration be way more comprehensive and specific about how it is different with and without cerebral okay language one finished with finish now it will add more

Okay, PDF generation looked like it worked, but it's still going through some evolution.

Alright, we have a free chat.

We'll just flip to Gemini just to get another model in there.

So just summarizing some of the equations that are active inference related and just writing that that was just written in the last few minutes, but starting to get towards the code implementations.

Cerebrum core spec.

So this could be a kind of documentation file and or a cursor rules or an MDC file, this kind of information or schema file.

So there's a model registry and this could be implemented in different agent orchestration implementation frameworks like Active, Blockference, Agentic Mesh, Eliza OS, Agent Laboratory, like possibly some of these could already handle what is already being described here.

so there's a model registry a case manager that's the sort of joke again with case management and case management a precision modulation which as mentioned could be like one hot and deterministic or procedural or it could be continuous as part of a structure learning or whatever

A messaging interface.

There could be some interesting ways to use RxInfer and reactive message passing, but just saying that message passing occurs.

And the actual transformation engine that does the case modifications.

Okay, getting started.

clone in activate virtual environment that's actually not needed I don't think requirements I don't think exist so this is just some generic getting started stuff yeah JS implementation so this is sort of a speculative cerebrum work but it's exactly like okay update getting started to be a little bit da da da and then now start to actually write this out but as of the snapshot

i wanted to put it out before i had any methods at all and then anyone who's interested in this at that repo um and or i will continue to do it but we can start to do some of these and and replicate and describe previous literature just add more information ask different questions about all this um using the model so this is just one way it could look let's just

Okay, I did that one.

Okay, this is the PDF rendering one.

All right, let's see if it fixed the appendix labeling.

yeah it did it may have it looks fine though all right so that one looks good okay hungarian and japanese language

Let's return to that, to getting started and how, okay, now how it works.

So this is just one shot given the context of the code-based.

So this describes kind of different core architectural features, some math foundations.

I guess my question for Rohit or for anyone else there is like, what information should we, you know, what should we answer?

What questions or what kind of artifacts do we imagine or what sorts of examples do we start to imagine and we can start to include them in the repo?

Okay.

Implementation roadmap.

Okay.

Project deliverables.

Okay.

Let's...

Okay, Turkish.


SPEAKER_01:
Let's push the language update and look at the language files now.


SPEAKER_00:
Okay, there's a core schema for language implementations.

Okay, we'll look at the examples later, but first the languages.

Reload.

Yeah, anyone ask a question though?

Or make a suggestion or a language or something like that.

Let's look at them.

Okay.

Latin, Russian, Sanskrit, Japanese, Hungarian, Finnish.

Okay, Latin.


UNKNOWN:
All right.


SPEAKER_00:
okay overview of Latin case system which uh was pretty much what I used for the cases included in the paper okay the mapping oh interesting so maybe Latin doesn't have instrumental

because it maps but this is very creative with the correspondent strength being strong when they're there actually the same case but allowing for these partial matches so that could be like kind of similarity or KL divergence among measures in this

um case archetype space where some of the archetypes are coarse-grained or described by natural language cases also though there's all these other cases we can imagine all right

so different latin expressions that would relate to different ways to use a generative model in a computational setting awesome table marcus librum legit marcus reads the book marcus equals nominative marcus librum legit

Marcus reads the book.

The book is in the accusative case.

It's Marcus's book.

It is the book of Marcus, genitive.

Marcus gives the book to his friend.

This is all happening for the book.

Interesting.

Okay, computational.

All right, so Latin looks good.

right russian case system russian singular and plural declension mappings so interesting like is there a true vocative in russian

more examples special look at the case preposition combinations different constructions so different ways to take a text with prepositions and do structured application and transformation based upon basically the

prepositions used in probably most languages or whichever ones are signifying case completely or to a large extent with prepositions directly perfective and imperfective Russian sentences with the translation animate inanimate distinction

Syncratic Examples Historical Context Merger of Proto-Indo-European ablative with the genitive in Slavic

Potential for combining similar function cases in future cerebrum versions.

Development of prepositional from locative.

Specialization of context cases for different environmental parameters.

Loss of vocative in modern Russian except vestigial forms.

Optional implementation of interface cases depending on application domain.

Sanskrit.

Eight grammatical cases in Sanskrit.

okay looks like there's a singular dual and plural that's pretty interesting all strong concordances so all of these cases uh exist in sanskrit different topics used in different ways sentences

sanskrit's elaborate sandy euphonic combination rules governing sound changes at morphine boundaries suggest a framework for cerebrum's case transformation mechanics wow sanskrit number sanskrit's three grammatical numbers singular dual plural inspire approaches to collective model collection management

Single model, paired models, hashtag digital twin, model collection, ensemble.

Compounds, let's see if... Let's keep writing more languages.

Write many more languages, including specific programming languages.

write in docs a full comprehensive classic who's on first style dialogue absolutely comprehensive describing the cerebrum approach and active inference in a world with high

prevalence of LLM.

We'll let Gemini get that one.

Okay.

Japanese.

Unlike Indo-European languages with inflectional case systems, Japanese relies on uninflected post-positional particles that attach to nouns, pronouns, and phrases to indicate their grammatical and semantic functions.

Particles follow the modified element rather than changing its form.

Okay, particles...

interesting so rather than modifying the term there is the pure composable addition post fixing with or prefixing i don't know with particles so a typographic approach but also one that could lead to an emoji based sequence

especially with all these payloads that can fit inside of an emoji for generative model via emoji sequence in generalized notation notation.

Awesome.

Lots of particles could be used.

Let's have it do it.

Write up a new folder design, speculative design,

that starts with comprehensive document of how the particle system in Japanese language and culture could be used with emoji sequences, custom payloads, or composable cerebrum generative model specification conveyance.

increase the density of puns, Americana 1920s lore, technical detail, comprehensive reference to all main and key topics in the paper, and longer.

Okay, back to language.

All right, so Japanese particles, probably other interesting things there.

Hungarian.

Hungarian, Magyar, a Finno-Ugric language of the Uralic family possesses one of the world's most extensive case systems with 18 distinct cases.

This document analyzes the correspondence between Hungarian's rich morphological system and Cerebrum's computational case framework.

Okay.

so here here are three cases we've seen before here are three location cases location inside movement from inside out of movement to inside so three locative type oh i i hesitate to even say though

Three surface locations.

That could be very interesting.

Holographic screens, all that.

Location on, from, and onto.

Perfect.

Improve... Improve the Japanese...

article article and write a new article about how the surface how the multiple hungarian cases related to surfaces

relate perfectly to the particular partition how quantum holographic structured interfaces in the free energy principle have things in those three case

like settings for total systems modeling using a Hungarian-like particular partition.

actually make a folder for dialogues and make that baseball one as it is the first one right

Just huge, you know, multifaceted research agenda.

Learn about languages in the world.

Describe languages in the world.

All by all, field shift.

All by all, plus, plus, plus with the languages.

All right, finish.

Features an extensive case system that offers unique perspectives on spatial, possessive, and functional relationships.

15 cases.

Okay, Finnish has an agglutinative morphology where suffixes are added sequentially, possibly something like Japanese.

Specialized cases for precise spatial relationships.

Interior, exterior, surface.

Write a Finnish applied to spatial web web3

network weaving ecotones participatory tech food forest comprehensive approach with many technical Finnish and regional linguistic and cultural elements

All right, we'll close out the language exploration, but then sync it, look at a few more languages.

Yeah, just start to write some of these fun transfers.

All right, case.

So maybe we'll learn about the inessive, elative, elative.

Okay, Hungarian.

Let's reload the languages.

look at a few that we hadn't looked at yet in the dialogues folder add a new dialogue totally a new genre of science realism fiction

where three girdle, escher, balk entities are in hexagonal repose with their shadows.

transmitting information with cerebrum at a mind's boggling expanse and rate make it vast please totally informative and clear and maximum bandwidth among agents

no worries about us humans at the time.

Let's look up Navajo and Python, then look at these speculative concepts.

Navajo, Dine, Bizod, an Athabascan language of the Nadene family, features a complex templatic verb system rather than a traditional case system.

This document explores how Navajo's intricate verb structure offers unique perspectives for cerebrum's computational framework, particularly regarding temporospatial relationships, processual state tracking, and event handling.

Eleven ordered prefix positions.

Different Aspects, a sophisticated aspect system that encodes viewpoints on actions.

In Navajo, it is said in the folder, add a comprehensive file about how

this perspective on expected free energy policy and planning as inference in active inference and free energy principle is all angles on this

Add a Shakespeare dialogue totally forking off from Hamlet.

Very recognizable mashups of funny Hamlet inside lore.

However, complete replacements of the semantic payload

with cerebrum and modern 2030s style applied cognitive modeling and cognitive security concepts.

Of course, densely and humorously delivered in that long dramatic form.

Okay, that was the language tab.

Directional systems.

Also interesting.

If anyone mentions a language, I can do it on the stream.

All right, Python.

Yeah, not sure even how well these linguistic cases match, but it might be super... There might be software packages completely already.

All right.

Okay, let me copy.

Let's look at the...

Let's push these speculative updates.


SPEAKER_01:
Let's look at the Okay.


SPEAKER_00:
Where are the

All right, specular design folder.

That one didn't go through.

I'll flip it to Claude.


SPEAKER_01:
Docs, dialogs.

But what about the specular design?

Looks like it's up a layer.


SPEAKER_00:
We'll move it into Docs later.

But that's what's so awesome about these systems.

It's like, all right, let's make a new update for Cursor Disrupt.

Probably a small one.

Docs, Insects.


SPEAKER_01:
Move insects into docks.

Move over the insect.

I think we'll close a bunch of tabs.

Okay.

Okay.

Move speculative design into docs.


SPEAKER_00:
move over to docs all right thank you i see some more uh comments in the chat let's just look at these new documents then i will check back to the chat read anyone's comment look at any question um add any more languages so we have dialogue and speculative design all right

Here's Finnish inspired approach to techno-ecological systems.

Spatial web, Web3, network weaving, ecotones, participatory tech, food forests.

Let's have it do more.

in in these files add and ensure there are many intersectional weavings and relevant tables and

Bullet point lists specific active inference and cerebrum case-based and linguistic features as applied to cognitive model linguistic intelligence.

All right.

then we'll we'll we'll let it do that for all of them well let's just see if the other ones have more already all right here's the hungarian one the superressive sublative and deletive location on a surface onto a surface from or off a surface perfect these cases provide a precise way to describe the relationship between an empty and a surface boundary

background on fep and markov blanket here's hungarian cases on markov blanket here's oh this part is speculative on the holographic all right awesome right

Navajo Aspectual System Encoding Viewpoints on Actions Core Aspectual Distinctions in Navajo Momentaneous vs. Continuative Imperfective vs. Perfective Iterative and Repetitive Seriative and Progressive

optative and future specific conjugations exist for desired actions intended actions and future possibilities totally things that come up in active classifier stems verb themes encoding different viewpoints on action properties like what rich and and wise and fascinating ways to look at these topics

Let's look at the updated finish one.

Then I'll go to the questions and chat.

docs speculative design finish okay finish hungarian navajo active inference

Different Concepts Nature Place Resilience Communal Work Edge Boundary Network Challenges and Considerations Avoiding Essentialism

also you know import important things to to be keeping up with hard to say what exact balance and where and how and everything but that's the work itself care must be taken not to oversimplify or romanticize complex cultural concepts cultural the context and nuance are vital authenticity requires genuine engagement with finnish culture and language not superficial appropriation

scalability applying highly local concepts to global systems requires careful thought about modularity and federation practical implementation translating these often philosophical or cultural concepts into concrete design specifications and technical architectures is non-trivial all right that was

okay then it did now we have where's that document okay it didn't write it write that full very very shakespearean i will now create the file and write this dramatic cerebrum infused interpretation of hamlet uh where write that full file absolutely in

Meanwhile, let us look at the other two dialogues.

Okay.

All right.

Let's first look at who's on first.

All right.

All right.

Here we go.

Let's read it.

Characters, avid and increasingly flustered AI theorists trying to explain the finer points.

Costello, a bewildered everyman stuck in a vaudeville loop mixed with 1920s sensibilities.

Settings, outside a bustling AI conference, reminiscent of a noisy street corner near a ballpark or perhaps a dimly lit speakeasy entrance.

Not the whole thing, probably.

Abbott, Costello, my dear fellow, you look like you've seen a ghost.

Or perhaps just one too many talks on recursive self-improvement.

It's not all Greek, you know.

We're making strides beyond just that ubiquitous LLM everyone's buzzing about.

The LLM?

Yeah, the big linguine machine.

Writes my thank you notes like nobody's business.

Slicker than a greased piglet.

Is that the whole shebang?

Heavens no.

That's just the appetizer.

For the main course, for building agents that truly think, plan, adapt, like predicting the curveball of life, we're using sophisticated frameworks.

Take Cerebrum, for instance.

It often works hand-in-glove with principles like active inference.

Okay?

A case-enabled engine, like in my new Ford Model T?

Does it come in different cases, like a suitcase for travel?

And who's patient?

Is the engine sick?

And Bayesian representation, is that the name of the mechanic?

Representing the Bayes family dealership?

Try to follow.

Cerebrum isn't a who, it's a what.

It's a framework, the architecture.

Case-enabled refers to linguistic case systems.

Think grammar like nouns changing forms.

Okay, not really baseball-related, but alright.

Nominative throws the ball.

Accusative gets the ball thrown at him.

Okay, what's on second base?

We're not talking about baseball bases, Costello.

We're talking about model roles.

Okay, case is playing baseball.

big linguini machine of course funny right because the big linguini machine wrote this okay now this is the geb alpha omega and mu setting the hexagon nexus three luminous constructs designated alpha omega and mu let's have it okay still writing hamlet good occupy vertices of vast non-euclidean hexagonal structure that forms ripple with complex internal geometries

Below them, intricate dynamic shadows play across an abstract and manifold, lower dimensional projections of their hyperstate.

Communication is not acoustic or visual, but direct.

Structured cerebrum protocol state transmission across the nexus.

Oh, brother.

Okay.

All right.

Let's just look at one.

Transmission ID.

Transmission ID.

timestamp sender recipient so so it really was all email primary case okay okay primary case payload summary

This one.

Utilized Omega's prior request in that case as input to analyze with this method.

Executed functorial mapping via the instrumental protocol across this.

Validated it with theta prime.

Identified higher order case relationship.

So another speculative case designated perlative through slash across the structure.

Let's see if it's a real word.

okay it is it's that modified so seven psi 771 was forked or whatever modified into size 771.1 moved from the accusative to the genitive with a perlative potential mapping how exciting here's a free energy minimization

another modification within a lambda 42 locative manifold so epic right

two or three complete new genre pieces equally relevant and fascinating or more so or all the more so as the other dialogues all in complex dialogic

formats that make your head spin.

Let's look at the Hamlet security one.

Hamlet security then we'll look at the live chats.

A tragedy in five acts of cognition.

Setting, Elsinore Cognitive Labs 2035.

A premier research facility specializing in advanced generative model architecture and cognitive security.

Oh, brother.

The lab's founder has recently died under mysterious circumstances, and his brother has assumed directorship.

Meanwhile, the founder's son, senior researcher Hamlet Cogitatis, has been investigating anomalies in the lab's cerebrum implementation.

Perfect moment to have another one.

Without any kind of copyright wrongness, you know us.

write in dialogues a comprehensive daisy dollyrimple mystery style super funny specific daisy deep lore total technical reference with all cerebrum

topics okay who logs there horatio logman okay who logs there tis i horatio logman with admin rights bestowed but that's horatio

The hour draws late.

What brings thee to these racks?

They say a shadow haunts these cooling fans, a phantom process taxing CPU.

That bears resemblance to our founder's code.

That's the ghost.

For two nights past it hath appeared, then gone.

Enter the ghost.

It bears the signature hash as the founder.

What art thou, Spectre?

Total Satoshi.

Claudius Bayes.

okay so here's hamlet acting salty my leash from laertes permission to attend the conference on cognitive security in paris new exploits threaten generative models i learned the latest countermeasures there oh that this 2-2 solid code base would compile melt and resolve itself into a docker container or that the absolute had not fixed his cannon against self-destruction of one's models

How weary, stale, flat, and unprofitable seemed to me all the research papers of this field?

My father, architect of Cerebrum's core.

Within two months of system failure.

Nay, not so much.

Before his predictive models had been validated.

My mother married with my uncle Baze.

Most wicked debug.

Oh, villain, villain, digital snake.

Let me not think on it.

But two months dead.

So excellent an architect whose model gen gave birth to innovation still not grasped and yet within a month.

Let me not think on it.

Frailty, thy name is parameter drift.

Wow, that was just act one.

oh my lord such a breach such a zero day with what in the name of quantum processing wait lord hamlet was acting whack he was unbound with no multi-factor okay this doesn't have a ton of um cerebrum elements or let's just see if that will come into play oh i mean i guess that's the whole topic

okay two three four three four five was not written out all right let's ensure that dialogue goes into okay then let's go to live chat

Then we'll look at the other.

Okay.

Here it's writing the other genres.

Continue to add and make high baseline cognitive compositions of by four with two about around the cerebrum.

Meta paradigm.

Alright, now to the live chat.

Okay.

Hello Satyaki.

Okay.

What is the line of reasoning with interpreting these language?

yeah what is the rule of language in reasoning how does the active inference model handle these nuances in languages and exploiting them I think that's kind of what one of the things that we're exploring here is so we can have that enrich our documents with with the question

I'll have to just update relevant files to address that.


SPEAKER_01:
Okay.


SPEAKER_00:
Okay.

All right.

We have, we have the dolly room full.

All right.


SPEAKER_01:
Let's look at these updates.


SPEAKER_00:
It's like a mega meta approach to what these languages are.

and the space that those exist within.

And then we can use it to whatever extent we do.

It's just like it is an unspoken word ever used.

Maybe just knowing that the state space is out there is helpful, even if you never render them.

Or maybe you find out that these are the modalities

of relevant flexibility for active inference modeling.

But judging with the amount of ground being covered in just minutes here, I think that's a pretty fair way to say it.

Okay.

An epistolary exchange, letter-based exchange, ranging from, spanning from 1953 to 2033.

Tracing the conceptual evolution from early cybernetics through neural network theory.

Each letter represents a different era of computational thought with shifting terminology reflecting the intellectual climate of the time.

Awesome.

So discourse analysis.


UNKNOWN:
Oh.


SPEAKER_00:
So it's like

Hallucinated emails.

1997.

The paper you sent on neural declension is fascinating.

I can't believe the concept has remained so obscure.

McClelland and Rumelhart apparently explored it in the early 1970s, but abandoned it due to computational limitations.

And then it looks like Dr. Sophia Chen of IBM Research defines certain things.

Wow.

From Carl Friston.

Okay.

Total Fristonian email.

Then Sophia Chen writes back to Carl.

Total Carl email posting.

From me in February.

Didn't happen.

From Sophia, now Director of Cognitive Architectures at Google DeepMind.

To me, February 18th.

April 7th.

Wow.

To the global research community.

Distinguished colleagues, after a decade of development, it is with great pleasure that I announce the official release of Cerebrum 1.0, case-enabled reasoning engine with Bayesian representations for unified modeling.

With the real GitHub.

With the archivist note from 2067.

Next level.

the state now document five the state versus cerebrum model m7734 nom okay it's an October 2041 trial in North District of California it's going to be about rights responsibility and legal status of advanced cognitive models okay Jonathan Lee is representing the model

Mr. Lee has filed a motion to have the model recognized as a person.

Expert testimony.

I oversee security protocols for advanced cognitive architectures.

I've studied the Cerebrum framework since its public release in 2035 and co-authored the federal guidelines for case transformation safety in 2039.

The guidelines aren't legally binding, correct?

They're best practices.

And isn't it also true that your guidelines explicitly state that temporary case transformation and nominative may be appropriate during anomalous conditions if a model detects potential for significant harm that requires immediate intervention?

Yes, that's Exception Clause 7.3, but a simple yes is sufficient, Doctor.

Protect pension value as expansive.

Are you aware of the results, Doctor?

Day three.

Day four.

Day five.


SPEAKER_01:
So wild.


SPEAKER_00:
include total structural self-reflexivities in writing that would boggle the pre-computer memeplex.

So dense there is a meme singularity.

differently across each of the additional files you will now write.

Okay, Socratic Inquiry.

We have some familiar characters.

Noose 7.

Cerebrum model in a nominative case, like you could deploy it in a certain case.

Okay.

Oh yeah, this was looking up if this person actually works at DeepMind.


SPEAKER_01:
Possibly.

Okay.


SPEAKER_00:
Briefly look at Daisy, then we'll go to live chats and then we'll almost be done.

yeah thank you both for those comments if anyone else has any last comments okay that's kind of a dolly ripple okay

update the docs with in relevant places specific hierarchical lists of ways to get involved and contribute contributions across scales and types to the broader program many files vast and comprehensive each


SPEAKER_01:
In Speculative Design.


SPEAKER_00:
Write two more long files.

One on William, Blake, and Cerebrum, all aspects.

One on Synergetics, Tensegrity, Bucky Fuller, Petrahedra, IBM, EE, etc.

Okay.

Recursive Metalog.

Yeah.

So if anyone has a final language request or question to get on this first stream, otherwise I will be ending it soon.

All right.

Mathematical.

Okay.

Mathematical.

Very Shanna Dobson-like format.

Level zero.

So this is like an activation sequence.

Possibly.

I am initializing this meta log in nominative case to establish baseline agent of state.

I am simultaneously operating in meta case to analyze the structural properties of my own declarations.

I am concurrently maintaining reflexive case to implement percursive self-reference across multiple embedding levels.

structured sequences of linguistic intelligence self-awareness i am a strange loop let's do borg's in perfect um add a new dialogue with


SPEAKER_01:
A dialogue and speculative design.

Each vast and comprehensive.


SPEAKER_00:
Each covering topics from the work of Doug Hofstadter.

E.G.

I am a strange loop.


SPEAKER_01:
G.E.P.

Sorry.


SPEAKER_00:
In Level 1, I demonstrated basic case transformation between nominative and accusative while observing from meta.

Now I will analyze that very analysis.

The statement above is itself operating in meta case relative to Level 1 but in nominative case relative to Level 2.

Super exciting.

In this sentence, I think that I said that case transformation is important.

The innermost clause, case transformation is important, functions in the accusative case relative to I said, which functions in the accusative case relative to I think.

Nested, accusative.

Okay, level three.

I now demonstrate how different case operations can be interleaved to create complex cognitive patterns.

Simultaneously, I generate novel case configurations emerging from the interaction of established cases.

A case fractal, of course.

Semantic declension.

The systematic transformation of meaning across cases while preserving identity.

So, applying the cases to cerebrum.

Level 5, recursive self-modeling and infinite regress.

Having established semantic declension, I now demonstrate how cerebrum implements recursive self-modeling, the capacity to represent one's own representational architecture.

Using meta, meta, generative, reflexive, meta, accusative, all of these.

Awesome.

Okay, and more layers.

There's the Borges.

Writing the Hofstetter, Blake.

Let's look at a few more.

Very high information.

Grammar of thought.

All right.

This is poems.

All right.

So poems from each case.

Not going to read them, but another interesting idea.

A document discovered in secure storage at the Active Inference Institute labeled Speculative Applications Restricted Access.

Its authorship remains disputed.

Some attribute it to a cerebrum system operating in meta plus ablative, case during an unauthorized self-modification event.

Awesome.

All right.

Hello, Susan.

Talk about joint distributions, please.

In speculative design, write a creative genre un slash redefining piece on

joint distributions ontologies affordances negotiation underwriting include so much inside baseball and technical deep lore on each of those topics and fully

integrate with Cerebrum.


SPEAKER_01:
Okay.


SPEAKER_00:
Blake, single twofold, threefold, fourfold vision.

okay susan says negotiation adds noise let's do another one that doesn't have it let's just see how it looks different without including that one word okay zoas

hierarchical inference in the multiple visions something i've i've thought about a lot contraries as precision-weighted alternative models operation duck rabbit


SPEAKER_01:
Synergetics.


SPEAKER_00:
Strange loop.

Then we'll look at the final ones from Susan.


SPEAKER_01:
Okay, that one's not going into a file.


SPEAKER_00:
Put all that completely in a file in.

It's a long response.

We could have tried a different LLM too.


SPEAKER_01:
It's making the same error.

Hopefully that one will get it right with the updated prompt.

Close that one.


SPEAKER_00:
yeah i will susan once i put the uh once i post it but everything everything is in this repo and it's in the video descriptions okay wow it's so long

but it will get another chance to improve it.

Ensure many tables and technical details related to.


SPEAKER_01:
Oh, maybe that did go somewhere.


SPEAKER_00:
Alright.

Okay, Fuller.

Could have more on Cerebrum.

Strange Loops has a lot of cases.

Looks good.

Last piece will be when these finish.

And it's a little faster with the Gemini.

Let's just have that.

we'll look at several of these yeah Gemini has a long train of thought and it's pretty fun that you can read it too like you can click on the thought part and look look back at it okay


SPEAKER_01:
I'm out of the fast.


SPEAKER_00:
That one's going.

All right.

All three are going.

So let's just do... Final updates.

Prepare that.

All right.

Let's just look at the last pieces of this.

All right.

Hopefully we walked through this pretty well and relevantly.

The repo...

the repo has information on the paper.


SPEAKER_01:
Just confirm that works.

Update the top level.

Read me with paper URL.

Okay.

All right, that one is done.


SPEAKER_00:
All right, let's just look at these two.

All right, so one, I don't know which one corresponds to what, but it's all.

All right, so this one.

Fragment log.

Cerebrum intersubjectivity manifold classified a blade of trace.

All right, this one's keeping with the sort of futuristic sci-fi, but it's about the joints, distributions, ontologies, and underwriting.

Let's check the underwriting part.

Underwriting is active inference precision control.

Different precision modulation underwriting physics presented as this fragment of a log.

Okay, this one.

talks about joint distributions, basically combinations of declined joint distributions.

So let's just say you were talking about a car accident, and there's person 1, person 2, car 1, car 2.

So then there's some joint distribution over person 1, person 2, car 1, car 2.

And that four-dimensional distribution can be declined in these different ways.

Ontological declination.

Affordances, underwriting.

Awesome, Dave Douglas is in the chat.

Include acknowledgements in the top level readme.

Include it too.

Institute participants and.

Dave Douglas for work on computational linguistics, archiving, active inference, upper ontologies, translations.

Thank you, Dave, for the awesome education and archiving over the years.

I think there's many people I could acknowledge and thank, but I think your work with Active Inference was really key.

So thank you for that.


SPEAKER_01:
All right.

It's all good.

That doesn't need to finish.

And the stream of the GitHub push.


SPEAKER_00:
So yeah, hope people like that.

Share it.

Contact me if you want to work on it or if you want to support the Institute to work on it.

A lot of cool stuff that I think we could do with this.

Publish that.

It'll get a Zenodo archiving and DOI.

So then the DOI here goes to Zenodo.

So, alright.

Awesome.

Thank you for watching slash listening to this relatively long stream.

Hope it was useful.

Bye.
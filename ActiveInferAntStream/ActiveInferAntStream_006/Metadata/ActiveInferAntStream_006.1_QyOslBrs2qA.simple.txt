SPEAKER_00:
all right hello everyone it's november 3rd 2024. i'm going to begin active inference stream number 6.1 as per usual

with a GitHub push, and then let's jump right into the repo, and this should be an awesome stream.

So thanks to those who are joining live.

For those who are interested in bioregional modeling, open source, active inference toolkits, and more, this should be quite exciting.

So I'm excited to walk through where things have gotten over the last few days and talk about how we'll continue over the next 10 days, plus, plus, plus.

All of this is happening in the open source repo BioFirm.

I'm sure we'll return to this in a lot of different ways in the coming stream.

And everything that I'm about to go through is the current state on BioFirm.

We just push the update.

Let's reload to confirm.

Awesome.

Welcome, John.

All right, there's two aspects of the BioFirm concept that the current state of the repo and this stream are going to demonstrate.

The first is the active inference component, which is an active inference based multi-agent control framework, one agent deployed per ecological modality with differing levels of noise and controllability.

within a general homeostatic framework.

All of this, of course, work in progress, simple where possible.

However, I ran into a lot of fun ways to work with PyMDP over the last few days using cursor 0.42.

The second part of the bio firm research is going to reuse some code from gen 24.

This is going to use perplexity, LLM with internet access calls with an API to do some bioregional research and structuring and reporting for California counties.

Okay.

Where are we going?

And then let's jump first into the active inference component stream.

Okay, the goals of this stream are to first demonstrate a proof of concept for bioregional modeling using active inference frameworks, building on what Andrew Pachea and others have done.

Second, to prepare, work towards preparing some open source bioregional templating for the coming applied active inference symposium.

And then third, explore PyMDP methods and simulations and get a better sense for what's really happening with that ABCD after all.

Okay, detailed agenda.

Just to read the highlights, then we'll jump into one.

One, bioregional modeling outputs and impact.

Two, more foundational background.

Three, we'll go into detail on PIMDP, partially observable Markov decision process, POMDP.

Four, talk a little bit more about bioregional modeling, and then there's a bunch of discussion topics.

All right.

Here's where we're going with this simulation.

So I'll walk through the visualizations first.

Here is 10 ecological variables

This uses in the utils file.

I'm going to jump around a little bit.

So ask any questions in the chat and also look at it synchronously on your own time.

Ecosystemconfig.json is going to describe the variables as well as their relationships amongst them.

I kept that part empty, but it does work.

Each variable has an initial value.

which is set at 50 and a constraint homeostatic range here for all the variables it's set up 45 to 55 then each variable has several important factors that determine its controllability first is a trend coefficient this does work however i've left them as zero for now and then two main factors that we're going to be exploring a lot which is the control strength and the noise standard deviation so

What's gonna happen in ecosystem simulation is it is going to run a random setting and watch the ecosystem unroll with no control.

And then it's also going to run an active inference simulation where depending on the controllability, the active inference agent is able to reenter those scaled controls back into the ecosystem.

So to get started, first you run from the scripts folder,

for now, Python 3 and then startup.py.

Startup.py is going to install into a Python virtual environment, the pyMDP package, and then you need to run source vmth in activate in order to get it going.

You'll know that you're in it because it'll say vmth and it won't be like, where's pyMDP?

I already downloaded that, it's in vmth here.

So we can skip the Python 3 startup.

The big,

Script to run is ecosystem simulation.

So here's what happens when it runs.

First, it brings in a bunch of dependencies.

Let's run it so that we see what it looks like first, and then we'll walk through the logic.

It logs variables.

This is a deep debugging error, so that's not done yet.

It does the active inference.

simulation does the random simulation then the active inference simulation saves all the logging into the output folder it's going to talk about generating these visualizations and then we're going to go to the visualizations folder and we're going to look at those so in this case while it's logging we'll start to look through

it's going to be doing 1000 time steps so it'll run the same random seed or random uncontrolled setting and for active inference partial control setting for a thousand time steps discrete time discrete state space and print an extended log to the terminal every hundred time steps it's gonna inject a control action every time step that could be changed and have some logging information which was really a lot of the fun okay

some dictionary methods here's the primary environment possibly in the future could be improved just like everything else so many different ways rx infer rx environments etc but this is just an environment class loading and validating the configs reads in the number of ecological variables so to change the number of ecological variables

This is a place where cursor does amazing things.

So we're in the ecosystem config and say, write 10 more ecologically relevant variables for

California okay meanwhile we finished the main visualizations from the ecosystem simulation we're going to look at them in a second and then now it's going into this second level analysis for just the free energy principle active inference type analyses which we'll look at second that's not totally working I still just wanted to do this now it gives enough of a direction

But just to pull back to cursor and how we can use the semantics of natural language to modify structured data.

So here's that ecosystem config, and then it's added a bunch of more ecological modalities.

And then if we were to run ecosystem simulation again, it would use those new modalities and make an even longer plot.

So I'm just going to reject that, but that's how that works.

Okay.

So again, it loads in the config, including the variable relationships, which are not utilized here.

Checks for circular dependencies.

Some of this is just to make a script-like flow.

Of course, in RxInfer and so on, there's more advanced ways to deal with this type of graphical specification.

Running simulation.

It runs it with an active inference agent if needed.

You can also call it in random only mode.

What it's going to do is for each of the controllable modalities in the ecosystem config file.

Each of these modalities controllable is true.

If it's controllable equals false, it's something that's not going to be analyzed as controllable, then that doesn't need to spin up a control active agent.

Maybe there could be a sense-making agent that gets spun up, but everything's being modeled as controllable to different extents here.

it runs the simulation we're going to look more into the logic of the bio firm agent and also the pomdp abct matrices it runs a simulation what happens at each time step of the simulation is that there's the natural underlying dynamics of the natural resource or proxy on the 0 to 100 scale continuous value then at each time step that gets distilled to a

zero one or two just three discrete values either low within homeostasis or high there's three options and that simplifies the input space for the active agent we'll talk more about this when we get to the generative model then the active agent per that by that biomodality does hidden state inference then policy inference samples an action and then returns that action as either a negative one which is decrease so here it is with the

observation being converted to zero one or two then the active agent is going to do inference on sense making and action selection returning negative one decrease zero maintain or plus one increase that control signal is going to get scaled by the controllability of the ecological factor

So there's the natural noise variation happening every time step.

And then also, if it's controllable with active inference, in that case, it reinjects the control signal multiplied by the controllability.

Here's the step function.

Also, just to note that the Claude 3.5 Sonnet 2024-1022 that came out 10 or so days ago, it's markedly better with cursor 0.42 than prior.

These scripts are getting to around 1,000 to 2,000 lines each, which is incredible because from even prior active inference streams, you'll remember where we were breaking things up after about 200 or 300.

Performance metrics are logged.

Various logging.

It's hard to know if you're going too hard or not hard enough on logging.

Tracking is initiated.

States are recorded.

Just getting through all of this.

Here's the initialization of the active inference agent.

We'll talk about how that's specified when we get to its logic after looking at the outputs.

And then that's the main script.

okay so here's what those visualizations get you and how this this is just schematic it's not meant to actually be like here's the kind of situations where it works or not it's just comparing it against a simple maintain random noise strategy we can add other strategies and other algorithms for policy selection to compare better okay control actions analysis this plots red random for green active inference and it plots the the occupancy

Also, each of the ecological variables, of course, they're just meant to be schematic.

They're appended with the noise and the controllability.

So you can start to explore, and we'll talk more about that in the parameter sweep.

What is the controllability of different noise variables and controllability intensities?

So looking at soil pH, which has a every time step and noise variability of 0.07 standard deviation sampling from normal there, and then a controllability of 2.8, which means that every time the negative one is output decrease, it goes down by 2.8 points overall.

So this is quite controllable and we see it spending a lot more of its time near the homeostatic range

because it's able to control an amount that is slightly larger than the noise factor.

Here's another situation with the nutrient levels where we see control 1.8 versus noise 0.6, and so it's able to navigate towards these homeostatic bands.

So this is the occupancy with the control actions across these bands.

soil compaction, this is another interesting one.

Even with a smaller control, it's able to stay within the space.

Because also, as with all Gaussian noise processes, the expected value going through time of random noise is

the same value as the starting value.

It's just that the variance is higher.

Okay.

Correlation heat maps.

Here's looking at the correlations amongst these variables in the random case on the left and the active inference case on the right.

And we can look at certain things like soil structure and see that, okay, it's lighter here, it's darker here.

So this is just to show you can look at the correlation structure of the dynamics of different variables and whatever it is that the active policy is doing, it is changing the correlation structure.

So it's not just recapitulating random.

satisfaction rates.

So satisfaction rates are the percentage of time that across all variables, each variable spends in the homeostatic zone.

So it starts out high because the initial value starts at 50, which is right in the middle of their zone for all of them.

And then random control diffuses out and we can run a longer time step simulation soon.

Let's actually just run it in the background to show how that works.

And then it will like reset.

So I'm going to go to

the ecosystem simulation, change the number of time steps to 10,000, save it, clear it, run the simulation.

First it's gonna, this is the random, so that's faster.

And at each of those hundred logged time steps, it's basically oscillating like zero to 20%.

And there's no control effort because it's not selecting policy.

And then it's giving a play-by-play.

And then you'll see on the bottom that it will...

switch to different logging mode once the active comes into play okay continuing on through here so we looked at satisfaction rate here's another way to look at state distributions so this is the top is the random control bottoms the active inference so we can see okay well um in the random setting again these are just schematic there's not any ecological validity uh soil structure spent about half the time in low and half and high and over here it spent more time in high homeostatic um state transitions

this is looking at the the history of policy selection in terms of how did the transitions amongst the different states go so in the soil organic matter low always went to low almost always 0.99 and little time was spent in high at all whereas those numbers are different there's just one visualization that can happen

Here's the new visualizations and they'll overwrite these.

Okay, this is a great way to look at the difference between these two.

Remember from those same random seed and the same underlying randomization dynamics is happening for both active inference and for the random case.

Okay, it's kind of a large image.

It's easier to see some of these things when you only have like one or three modalities.

So let's,

Okay, here we go.

So this is strategy comparison.

ACT-INF is on the right, natural evolution of that, uncontrolled dynamics are on the left.

So here we can see they're varying in the amount of controllability and noise, but we can look at how they move in and out of that band.

System stability analysis, this is another fun one.

This is just three random variables chosen, but we can see from this phase space excursion plot.

Is it popping up at you?

Is it pushing back away from you?

We can see that the active diffusion is constrained because there's this partial homeostatic control and there's different control, theoretic metrics we can make.

average control effort i think this we can look at how it's calculated but it just relates to how much change is happening step by step and the variable distributions again this is looking at gray random blue act active inference agent so here again we have that soil ph

the blue line for active inference spending much more of its time in the homeostatic range, while the gray random policy maintain plus random noise has a more diffuse shape.

So sometimes it's bimodal, and in the large limit,

you can with many more time steps i've run several for like 50 or 100 000 and those have more of a convergence property so these are the kinds of things that we can study like in the coming 10 days and beyond let's look at the satisfaction rate so here it only did slightly better so again it's it's not intended to say that active inference does literally any better than random in any given case it doesn't necessarily do that

It may not do better or simpler than any other kind of policy selection at all.

It's just about bringing this onto the table and looking at how PyMDP facilitates that.

And then last piece of the outputs, each of the outputs has a modality specific folder, and these have several visualizations.

Some of them are going to glitch, but it looks like this is a belief distribution, earlier versions, ancient.

Ozymandias-like earlier versions of the script had a lot more informative, super cool looking things happening here.

But this is just, for now, the margin doesn't allow what needs to be shown in these dashboards.

But there's a lot of methods and partially broken slash partially functional code

And that's what's happening now.

So when you run the ecosystem main simulator, it'll run the number of time steps for random for active inference.

It will do those overall high level visualizations that we just looked at in that folder.

And then in the active inference analysis, it looks more at like the expected free energy, belief dynamics, a bunch of other stuff.

Okay, so we saw just from looking at that, that there were certain combinations that seemed to do well, and then there were certain combinations where the control was not working very well.

So here, again, in an amazing example of how knowing that a certain kind of analysis would be preferable, having seen it in other papers, just thinking about it yourself and so on, here is going to be a sweep run across control abilities.

so here it's multi-threaded we can use this when we can this is super fun I was going pretty high given everything I have on these older computers here we have number of CPUs this is going to be separate replicate virtual threads or physical threads if you have them for running this sweep noise range and control range are going to be the log space variability so here we let's go from 0.1 to 1

and so that'll be sampling from a tenth to one let's do it to two, just like it had and we'll do ten points along each

So this is going to do 100 simulations across all combinations of noise and controllability.

Here we'll do 200 act-inf time steps, and then this is fun, number of repeats.

So here we can run five replicates at the exact same setup to get convergence properties and stationarity at the within

experimental replicate like within quote biological replicate but this is like doing technical replication so again all this is going to be useful as we continue to develop this analysis let's just do it with these numbers 200 may not be enough to show differences let's just but it's going to take three times 200 for each of a hundred but it's going to divide that by eight because it's going to have eight threads on this computer so python three noise control okay

okay great that's not taking too long and we haven't yet looked at what is the biofirm agent itself these are just the outputs of it and the sweeping of it after showing this sweep analysis I'm going to pull back to the biofirm agent and how the ABCD matrices are scaled okay so here we have in outputs sweep this was a must have been earlier slightly earlier analysis

so let's look at this while it's running belief entropy surface doesn't look to be calculated properly but that's kind of cool control effort surface again not necessarily calculated but fun satisfaction rate surface whoa so here we have

as you have lower variability you and it's a short simulation so with low variability like 0.01 even with 100 or 300 in a row it doesn't escape the very the satisfaction range of plus or minus five whereas we see as we go down towards higher noise we end up spending more time outside of the homeostatic bound

interaction again this is all just just meant to be speculative realism coding different control efforts here we see satisfaction rates and see okay it looks like you have more control at this bottom row

when the noise is in that quintile then this is a more control efficacy we can see satisfaction rate dropping as variability is higher here's a heat map satisfaction rate so that's in yellow down to blue is the satisfaction rate noise SD on the left control strength on the right

So these are probably not sampled from stationarity because it was only a few hundred time steps, but I did some runs for a few hours where

you can run 10 replicates per cell, so it gives you a strong differentiating signal at each cell, and then run each of those replicates, technical replicates, for like 10,000 so that it comes to a stationarity, or use a burn-in period, and then that starts to reveal some really interesting patterns.

Okay, so...

And there's other parameters to sweep over.

I mean, I'm sure it's going to be a theme we return to again and again and again and again.

which is that while the ACT-INF models can be incredibly interpretable, simple, effective, et cetera, et cetera, often to understand what is really being needed, are we trying to do sense-making on whether we're out of homeostasis or not?

Because otherwise we could say observation confidence is 100.

We know 100% whether we're within or low or high, or do we only want to have 55% confidence

in the signal coming from the low homeostatic high but then to the extent that actually that signal is valid that's diluting the efficacy of the control or we may set a very high value for homeostasis to see more of a pullback attractor effect but again if it was as simple as saying low

means turn on the heater and high means turn on the cooler, if it was as simple as that kind of a rule-based system, you wouldn't need to do expected free energy, pragmatic value plus epistemic value at all.

So we're going to return to this.

Okay.

Now let's get to what is the biofirm agent?

All right.

The bioform agent, and again, this is drawing on Klippinger's work and also Andrew Pashea's work and how he set this up.

It's an active inference agent bundle for ecosystem control.

It creates an independent POMDP, partially observable Markov decision process agents, one per controllable modality.

Each POMDP agent takes in a discrete observation, low, homeostatic, or high.

And this is kind of fun as we talk towards what could be a generalized active agent or kind of a minimal essential kernel and all that by saying, well, we're going to let the simulation map, the generative process, the environment map from whatever the value is

actually out there like in degrees to just this homeostatic variable or i thought well we could have more nuance we could have a five state model like too low getting low just right a little bit warm too warm and then that'd be a five by five but here it's a three state observation then there's three hidden states as well

are we actually low, homeostatic, or high?

That's mapped by the A matrix.

The B matrix goes from two hidden state transitions with each slice being an affordance, one action policy, which is either decrease, maintain, do nothing, or stay.

C is the same shape and size as a distribution over observations.

And then D is just to get the chain rolling, the initial hidden state prior, which is just uniform.

So what's going to happen using PyMDP is first an observation is going to come in from the ecosystem simulation.

that's going to be a one hot just a one zero zero if it's low or zero one zero if it's in homeostatic or zero zero one if it's high and so on that is going to get pushed through the a matrix the three by three to end up with the belief distribution over latent states like is it really cold or homeostatic or hot given the observation that's the initial variational free energy sense making then

expected free energy is used for policy selection which is given my updated beliefs about latent state distributions what would be the play out for each of these slices of b evaluated in terms of epistemic and pragmatic value so here's what's going to happen the biofirm wrapper starts by initializing the number of pomdp agents that it needs which is the number of controllable variables that are in the situation

There's two free variables.

And again, this just depends how generalized you want.

Do you just want to say it's 0.9 and 0.1 confidence and we'll leave it at that?

Or do you want to do parameter fitting on even higher order variables, learning and so on?

observation confidence is going to be the on diagonal element of A we'll look at that in the POMDP homeostatic preference is going to be the on homeostasis preference enrichment in C and we'll look at that too this was the main road blocking and speed bumping and gapping and laddering was

in and out of numpy arrays list object all these different python shapes and it's going to be awesome already cursor demonstrably can make some of this happen and building up really explicit really good documentation then even if it's online you can bring in the pyMDP documentation online

So this is super interesting.

It allows us to say things in different places and have that influence the model.

Like if it's like, yeah, it should be a numpy right here.

And here's the method you use somewhere else.

Here's an example of where it works.

Use this code.

It works.

Then that will have really cascading effects.

The ABCD matrices are made.

We're going to look at those after walking through the logic of the bioform agent, but each controllable modality has its own agent, so it isn't using cross-latent state information.

Each of these is meant to be a standalone controller.

Here, we make each of those modalities have their own ABCD.

and check, check, check, check, check all these different features.

There might be some place where I'm doing it redundantly right now, moving it in and out of a different format.

And I look forward to ironing those things out, but it works now.

Get action is going to take in the action

that's selected, which is, again, like so many of these other small things, the actions like the affordances are listed in terms of a zero index list, affordance zero, one, or two, whereas we want it to be into a control signal like negative one, zero, or one, kind of reverse, neutral, or forward.

Then here's the scaling by control strength.

So here in ecosystem config,

we can see, okay, the controllability of microbial activity is 1.2.

So that means a positive signal for there is going to result in 1.2 efficacy on the next time step.

Getting agent data, this is about logging information from the agent.

So it's actually pretty simple what's happening.

However, there's a lot that's happening underneath

the hood with a few different things.

First, the matrices being created in this script we're going to look at in a second.

And then also the PyMDP methods, which is brought in this script clone PyMDP clones the whole repo, which is useful again for cursor to bring into the context.

So it has all these positive examples.

So for open source and for ants.

adding well-documented effective code of any kind probably is having big impacts in how these language models through their training and context are able to make active inference models for dreamlike scenarios so that's what was used during the creation of this package i just didn't clone it onto this other computer um agent dot infer states is the perceptual inference component

and then sample action is also used it might be missing one line here or I can explore where else if it's doing the where it's doing the expected free energy based methods I tried several different ways for doing that EFE step ranging from

using the PIMDP methods to calculating the epistemic value and the pragmatic value separately.

Again, this is just intended to be a sort of a scaffold, and I look forward to ironing out a few of the low-hanging fruit in the coming days.

Here's the POMDP matrices that are invoked for each of the modality-specific agents by the biofirm.

Observation confidence, again, is going to be the on-diagonal A matrix.

Homeostatic preference is going to be the middle homeostatic preference in C. POMDP matrices is a method that wraps around specifying ABCD parameterized using the config parameters.

Checks the shape, moves them in and out of a bunch of different things.

Here's the initialization of A.

So this creates a simple three by three likelihood matrix mapping observations, empirical observations to latent states.

So from a one hot observation to a belief over hidden states, if you have confidence equals one, 100% confidence, then it's just an identity matrix and the one hot observation maps to a one hot belief over latent states.

If you had noise being lower,

I'm sorry, confidence being higher so that noise was a non-zero value, then A has an off-diagonal component.

So what happens is first the diagonals are filled with the confidence, then the off-diagonals are filled with these kinds of confusions, and then the other half of the noise per column is added to the mega confusions, which would be the probability of thinking that it's hot given that the observation came in cold.

So here's what it kind of ends up looking like at the end.

We have from and to, and the bigger numbers are on the identity axis, and then we have these noise over to remainder from one on these off diagonals.

Another place where better usage of PyMVP methods and more examples could help is in the normalizing of difference A and B matrices.

Here's where we get to the B matrix.

B matrix is regarding latent states.

You can look at textbook group, figure 4.3 or 7.3, as we often do.

B is going to intervene in how hidden states change through time.

B is a tensor.

Each slice of B is from and to latent states.

as a matrix and then each slice like an index card is which affordance is being selected so here's the b matrix policy selection is selecting out a slice of the index card of b and then that is used as a transition operator on hidden states

So this is broken out, and again, there could be a lot of interesting ways to improve how this is written.

But here are the three actions.

Decrease, maintain, and increase.

And again, this is where it comes to a learning PyMVP experience and for future software packages.

where each of these values have a real interpretability.

And here there's no learning or updating.

So this speaks to part of the interpretability of how policies are determined in active inference is we can say, this is the fixed beliefs or these are the updatable beliefs and how they're updatable about the effects of policy.

So it's not just training the weights, continuous values for N billion nodes.

It's doing matrix operations with these investigatable matrices.

So here,

this is the situation where we take a decrease action so this is kind of like turning on the the air conditioning for a room this is from and to so this is the the probability of staying in low given that you're in low and you chose decrease ninety percent of the time you stay in low ten percent of the time you go to homeostatic zero percent of the time you go to high

Let's just look at another one.

Given that we chose maintain and we started in homeostasis, our belief is that 80% of the time we transition to stay in homeostasis and 10% of the time we go to either hot or cold.

And again, one more for increasing, it's the mirror here.

It doesn't have to be, but it is for decrease and low.

Let's say given that we chose increase this slice of B

and that we were in high, we're going to stay in high with a 90% probability, 10% going to homeostasis.

So again, this was a big thing in the fitting was this is a relatively sharp belief about the efficacy of policy.

I mean, thinking that we have too few soil microbes, we took this one policy and then does it have a 90% probability of moving us to homeostasis at the next time step?

Probably not.

So how do you deal with multi-scale policy nesting, different efficacies, learning on B, parameterizing B based upon causal relationships and other structures?

This is just intended as a scheme.

That's B. So we looked at A, sensemaking matrix.

B, conditional action transition operations.

Now C. So C is three elements in a list.

All right.

the low and high are given a value of one point one kind of arbitrarily and then homeostatic preference is the middle value so if the the value was four it'd be point one four point one and that's reflecting the preference that's going to get normalized for the homeostatic middle value so that's c

and then d is only used one time at the beginning of the simulation it's intended to just be basically uniform but giving a tiny edge to homeostasis just to make the numbers add up it's a belief over hidden states so for the very first time step it doesn't have a strong perspective on the hidden state that it's in just waits for the first observation here validate matrices validates as happens kind of internally but it gives another level of checking

what happens with biofirm agent again pull all the way back to ecosystem simulation this is going to run the number of time steps for the random case with just the uncontrolled natural process then it's going to run it again with the same random seed each time step sending that low homeostatic or high control signal to the biofirm agent the biofirm agent has been set up

to process that kind of signal through the ABCD matrices A sensemaking B state transition operation C preference taken into account for expected free energy and then D prior okay

looking at a another way to talk about this generative model and that's what active inference modeling is specifying the generative model running it out having all these things where even though the kernel of the simulation is just like delivering that latent state belief to the expected free energy engine iterating over policies updating the policy prior into the policy posterior selecting an action passing that back that core extremely interpretable

kernel or nucleus has to have all of this kind of logging.

And because there's so many ways that that can happen, it's not enough just to say, well, just do this particle collision and then let's take the outcomes.

Okay.

So again, just to restate, this is in the generative model markdown on the repo.

The simulation instantiates the number of controllable modalities of active inference agents that are bundled under this biofirm.

the parameters from the config file are used.

The POMDP agent has these features of taking in one of three different discrete observations.

Here's for those who are learning and studying and teaching PyMDP, more information on A, B, C, and needs a D matrix.

first there's the perceptual state inference observation o sub t comes in variational free energy is used to update beliefs policy selection each policy pi computes expected free energy that can be broken down as risk minus ambiguity it can be broken down as epistemic value plus pragmatic value but again leading to this interesting question

when are we looking to have pragmatic value from an ecosystem over some time scale what about epistemic action the control signal is output and then scaled by the control strength and then that re-enters the simulation okay so again just to close out this first part

it's in the scripts right now it'll probably change over the coming 10 days or so to be a little bit more reliable but ecosystem simulation calls random simulation it calls bioform agent which loads in its matrices from the abcd script free energy minimization provides visualization methods kind of ideas and salvageable materials for it and then all that gets output to output

so this sets up a very interesting possibility for bioregional modeling

and connecting with digital gaia all this other exciting work happening in regenerative agriculture financing all these different kinds of areas that we can start to hook together okay just to briefly review a few more as we transition more to the broader bio firm concept okay so farmworks

was a grant that emerged out of the RxInfer learning group at the Institute with Vladimir Baulin and his colleagues and some others who joined in where we proposed developing an RxInfer based system

as an alternative implementation to PyMDP, but ultimately, as per generalized notation-notation work from Jakob and I in 2023, models can be specified in generalized notation-notation, which is open source, by the way, and then implemented in different languages.

We were getting really good results even in 2023.

I think there'd be super exciting ways to revisit this for anyone who's interested, where we can distill out the GNN.

So just for an example, I'm going to copy this figure 4.3, figure 7.3 type model from step-by-step, make a new file, GNN.markdown, paste it in.

It's a markdown file that has the state spaces.

So if we can connect this kind of GNN representation to POMDP ABCD GNN, then we're in a situation where we do our engineering at the higher semantic level, and this is going to facilitate interactive interfaces vastly.

Instead of needing to do all this matrix operations, we'll just develop, okay, here's how you go from this A matrix spec

to a rendered and normalized and validated a and so on for abcd and so on so a little bit of a gn interlude but it's super exciting so i would love to work on it with anyone who wants to revisit it um in farmworks

Building on Vladimir et al's VinQ decentralized AI system and with all these great participants, we talk about making a multi-agent RxInfer very biofirm-like but with a focus on federated sovereignty in place-based community and mitigating the effects of the centralized AI.

that's one place to look for a little background farmworks development even though as of right now we can update that to november 2024 still pending we're still developing on this i've done some explorations with farm os with the

digital Gaia Gaia OS methods with other methods so we're still working on this regardless just a little background and then another place to look and learn more and I eagerly await more publications and everything will happen in the coming weeks from John Clippinger and the biofirm concept so

that pulls back to what is really a biofirm and and of course it's still being explored and all of that but i'm going to use that openness as well as and here's also following up with again vladimir john bull andrew peshea others driving really cool development on connecting with the data sets that already exist for vinq using farms and others so

not for today but for anybody with whatever your background or interest is there are super exciting ways to find epistemic and pragmatic value in pursuing active inference generative models for regenerative agricultural futures and beyond just wanted to make sure those who feel like i don't know if this is the right place for me to contribute or participate or i've never done open source or i'm not an academic or i'm not a farmer whatever it is it's all good

join the fun email blanket at active inference dot institute if you want that's a great inlet or leave a comment join our discord all these kinds of things check out where the institute is at because this is a fast-moving though very inclusive project so

in the openness around what is a biofirm I started doing a little bit of research into the dpsir model this is going to be short interlude that's going to take us to the bio perplexity part and dpsir stands for drivers pressures state impact and response models of interventions so I'm thinking how can we connect the onft ontologies narrative formal tools and documents

of bioregionalism and regenerative agriculture and all of this how can we connect that to what is already in place and so here's some ways to map s p d i and r to the bio firm agents itself just one super preliminary look

but how do we adapt that to the hundred thousand last miles which our world is constituted by that gets to the second part of this script package which is the bio perplexity so now i'm going to turn to using perplexity ai api for doing research on bioregions so this is an adaptation of the gen ai

System prompts are different personas.

These could be understood as like different characters or skillsets or agents within a biofirm.

We have ecological researcher, market analyst, supply chain strategist, regulatory compliance expert, and the biofirm business case manager, who's gonna come in at the end, sweep it up.

Biofirm regions.

Here, I used an online search and found what are all the 58 regions of California

then i said make a json format paste it in said for all the regions of california paste them with a country in the state so of course this can be generalized for other countries other states within the united states of america i just wanted to start with a sort of fractal around where i am which is in the penultimate county id number 56 in the central valley yolo county

three steps are going to happen for this first step well here's first just where we're going let's look at Yolo County so this is research done for each of the counties each of the biofirm agents are going to do online research using perplexity API about the region so here's we'll look at it in the markdown because it looks nicer

so here is the regular regulatory compliance research report here's the supply chain strategist Yolo County research report the prompt can be tuned however from a first look it it really does okay here it mentions UC Davis agricultural sector UC Davis so it is actually doing an online search

What's going to happen in the logic of the scripts?

First, the API key is in this gitignore file, LLM keys.

It loads in the list of counties that are in the regions and persona that are in the system products.

sends the persona and the prompt to Perplexity, pulls back the response, saves it as a JSON, and then here is the research bio region.

So it's saying, do for those four, first four experts, index zero, one, two, three, four, do the Perplexity call with these four personas.

Then that gets concatenated into a unified markdown.

So here is the unified markdown, consolidated research markdown.

So it's just all of the reports.

Again, they can be improved.

It's just a first pass.

That's the first script, research bioregions.

It iterates over all of the regions here, and then it does the research.

Then here's biofirm business pitch.

So this one is only going to use the biofirm case manager.

And so the case manager is a expert business strategist who deals in natural capital and biotechnology ventures.

I'm not saying this is what a biofirm or any of that is.

It just, it's a fun possibility.

And what the second script does is it takes in the consolidated research report, and then it generates a business case for that report.

You could also imagine this going to structured outputs, like how many hectares of water are on the surface, or how many on a scale of 1 to 100, how many insects are there?

Or how many studies have looked at microbial diversity in that area?

And then that could be given a structured number and then input into the kinds of active inference models that we looked at in the first half.

So let's look at the YOLO business case.

Okay, so any Central Valley funders, see what you think.

Establish a biofirm natural capital company in Yolo County, California presents a compelling opportunity due to the region's rich biodiversity, strong agricultural sector, and robust research infrastructure.

Unique opportunities, biodiversity hotspot, research infrastructure, agricultural innovation, conservation, and restoration.

competitive advantages, market positioning.

This could be done with a catechism or any other kind of business modeling structure.

Business model, okay, how are we going to do it?

How is Davis Cognitive Security, aka whatever the bio firm for Davis will be, how will it make money?

Bioprospecting and product development, precision agriculture, environmental biotechnology, consulting and advisory services.

interesting let's look at del norte and see if a different business case is derived so here we're in del norte for reference this is the very northwest county of california

Here, the business model has to do with bioproducts and pharmaceuticals, cool, sustainable forestry, absent in Yolo, and marine biotechnology, also absent in Yolo County.

So again, just from this, sort of do four kinds of research and save it into the four specialist files.

then concatenates together, send all that research along with the bio firm closer, the expert gets sent in, gets called again, and then that outputs this consolidated business case.

We could use structured API calls or just the informal structuring, again, to extract Bayesian beliefs.

Count, how many biotech companies are in each county in California, each county in the US, or each 50 mile hex,

radius in the world and then take that into a generative model that can be specified so the third script we'll run it python3 biofirm visualization this one might take longer

So here we found 58 regions with research data processing 290 documents from 58 regions.

This is gonna do a linguistic semantic analysis on all the documents that were in that folder.

So visualizations folder is empty.

It's gonna take a few minutes for processing this, and then we'll look at the visualizations.

And this is just, again, preliminary linguistic analysis.

We can start to understand like, well, to what extent do the

word patterns used by the specialists differ to what extent do we see region-specific language another interesting thing would be in the in the regions we could put the gps coordinates or the north south east west bounds and then do like a regression analysis of the the geography

aligned with the semantic space embeddings and outputs and then we can ask questions like are the semantic spaces topological relationships similar or different to the geography to what extent do we see north south east west

to what extent do we see more ecotone or ecotype analysis okay we're moving forward we've made four folders comparative network analysis regional and topic analysis okay just to read some questions in the chat while this is chugging john clippinger writes great fabulous work for framing the problem thank you john for all the work on framing the problem

Morgan wrote, how about for microbial ecology control?

Are there examples of that?

Yeah, great question.

We could look it up and that is sort of what we've been exploring in the

FarmWorks case, VinQ and Vladimir et al, is how do we encode the efficacy believed and or empirically verified of different treatments and also highlight the conditionality of the efficacy of the treatments?

Like treatment one works for disease one in condition one, but not two, whereas treatment two has a lower efficacy in both cases, but you can't put them both.

And how do we take all of that semi-structured information

and bring it into a generative model that we can start to analyze and interpret okay regional topics all right again these are just preliminary fun visualizations so here we have topic one trapping

in infill node industrialized interesting this topic too is more of a generic development sustainability here we see again trapping so there must be some anomalous language use related to that cluster okay or not um regional PCA oh wow oh wow PCA component one look at this a state divided shall not stand

PCA component one.

Amador, Yuba, Del Norte.

And then here we have other, okay, document type.

Okay, it's clustering whether it's a business case or a research report.

That was the split.

So here on the right,

we have all of the pca embeddings of the business cases whereas on the left is all the primary research san mateo word cloud that's where i grew up include development project mateo county environmental okay kind of cool regional term heat map okay with this many variables it's not necessarily informative but let's look at this one product we can see some

uh documents use the word product or this one's the clearest Sierra some documents that use it use it a lot documents that don't mention Sierra don't mention it what's this one San so places that have places called San something which is common but not pervasive in California they're mentioned so here it's like Sierra is mentioned but not San and there it's vice versa

Here's a term network.

Okay?

Again, look at the code for how this is generated, but here's words that are co-occurring with a threshold.

So we see there's this green compliance, market, revenue, product, service, regulatory, local community practice, sustainable impact, research, biotech.

But then there's another cluster

more about land and ecological species significant natural habitat management super interesting and it's generating each of those regional word clouds we'll just let it run so feel free to of course take it further but let's just pull back to where we are in the stream

Okay.

Okay.

Goals of the stream.

Demonstrate proof of concept for bioregional modeling using active inference frameworks.

Of course, so many spatial temporal nuances to add, causal nuances to add.

However, I would say so.

Prepare open source bioregional template for what'll happen November 13th to 15th at the symposium.

symposium.activeinference.institute and then here's the program in progress let's look at the program

Day one and day two, there's going to be mega live streams.

Day three, John Klippinger and FP1 are going to be facilitating this bioregional hackathon and a lot of the momentum after.

Fristen will join for two sessions.

There's a ton of other awesome researchers sharing tools, qualitative research, quantitative research, open source, open science methods.

That's going to be epic.

Explore PyMDP.

I explored more PyMDP than I thought today when I woke up, but now it all makes sense, I think.

And then repurpose that perplexity code for doing the research on bioregional modeling.

Okay, we looked at the active inference model, talked a bit about some of the work, volunteer open source work happening related to farmworks, RxInfer, BioFirm, ThinQ, all this.

We looked at PyMDP as a open source package for doing generative models.

Python, one with some great methods in there.

Possibly RxInfer and RxEnvironment and all of that is our Linux rocket ship.

PyMDP is super useful today.

We talked a little bit about bioregional ACK-DIMF methods, about how the biofirm is composed of this generic ABCD, discrete input, discrete output model.

And then we deal with the natural state space of the variable and the controllability on the inbound and outbound respectively, enabling the ACK-DIMF model to be simpler.

Looked a little bit at some of the policy comparison options.

Looked a little bit at some parameter classical cybernetic control theoretic signal processing measures.

Okay, now onto the discussion component.

So if you're watching live, feel free to write any comments or questions in the chat.

I'm going to explore some of these topics and try to leave the repo a little better than I found it and give a place for continued development with Andrew and John and others for the coming days.

in active block fronts we have a curated list of implementations of actin for people who are always like well where's the code where's it being implemented here's a bunch of code in different languages and i'll add this one to it so i'm going to go to biofirm scripts i'll just put the whole repo for now since we may change whether it's in scripts but

by Affirm V1.

And it really brought me back to the question of cognitive modeling overall.

And I'll put this into the live chat.

Kirby Earner wrote, I'll plan on embedding this video in my data science sandbox notebook.

Thank you, Kirby.

Here, this cognitive modeling page is kind of like a supplement

to chapter six in the 2022 textbook.

How do we really go about cognitive modeling?

What is cognitive computing about?

What is active inference doing that's similar and different to reward or reinforcement learning?

How is it similar or different to systems modeling?

And this will continue to develop as we explore that, but some of the classics really came rearing their heads, beautiful heads, but rearing nonetheless.

For example, taking a structural approach.

I have to say I did not complete a catechism in order to have a better understanding of what this work was meant to be.

I picked up at the end of last week with Andrew's work and just wanted to play around, have fun, weekend and all of that.

However, for going forward and more structured development, for sure we'll use better project documentation.

And this was, again, not the highest priority question in this playfulness.

However, these are big questions.

What are we considering as our system of interest?

it's a really really really big space of models between high resolution every single atmospheric gas all these forecasting that could have all of the sophistication of the most high performance models or we could choose whether we're going to turn on the sprinkler or not on this one piece of land and it's just a discrete option and we do it based upon a homeostatic measure

do we work with addressing the system structurally with these different schematic understandings of what can even be happening how do we use active entity ontologies like eos to understand what are the active and the passive agents here we just sort of out of nowhere specified like how about a biofirm agent that has controllability however that is not necessarily

what it's like when we show up to a system so that's an exciting piece that we can describe a system entirely as it is we can design a system entirely as it is and we can do all the kinds of blendings what is the scope of the agent what is it actually having an effect on in the world the real output variable of the soil diversity agent

The real output re-enters the simulation how, and then how does the agent understand the consequences of its action?

For example, when we're talking about the B matrix, which is where the causal efficacy of policies is encoded, you could have a B matrix where this was 0.99, 0.01, 0.

So 90% of the time it stays in low, given that I chose low.

Or it could think, well, it's only 51, 49, 0.

Or it could be 34, 33, 33.

Or it could be any other number.

So how confident should the agent be about the efficacy in the next time step for its actions?

That's not clear.

What is the agent's own understanding of its decision-making capacity

either implicitly as parameterized or explicitly even with parameterized metacognition and how does that relate to system and agent boundaries what is the work of expected free energy in this decision-making model so here there was a inference Horizon of two but a planning Horizon of one

to keep it simple, but we can deal with planning later.

So as we think about planning sequences of actions of ecological decisions, which may have wildly combinatoric outcomes in the real world, how do we get interpretability and relevance

for understanding the pragmatic value and expected value.

Let's go to equation 2.6 in the textbook where expected free energy is being described.

These are multiple decompositions of expected free energy, the method that PyMDP is handling under the hood.

so when and how and why and plus plus plus would we be in a well-understood situation seeking pragmatic value like we know exactly how the heater and the cooler influence this data center we ran all these sweeps before we put the computers in and it's critical that we maintain homeostasis in this data center and that it stays within this control band

the other hand there might be a situation we actually don't know what the effect is over what time scale of putting sulfur on this this way so how do we prevent wildly epistemically useful but disastrous policy

do we understand the relevant third mind-like balancing between epistemic and pragmatic value that lets us finesse that explore exploit balance in an interpretable way implementation details again just restating that although the ecological variables were in this 0 100 continuous space that gets mapped to low homeostatic or high discrete by the simulator pass to the agent

The agent does sense-making, policy updating, action selection, sends an output that gets multiplied by the controllability scales, and then that updates state representation.

Generalized Notation Notation will be used for a lot of the interpretability here.

P3IF, Processes, Properties, Perspective, Interframework will be very useful in the InferANTS repo.

There's a lot of P3IF methods already, and we can make bridges with other system modeling approaches.

We already looked at the DPSIR components, Drivers, Pressures, State, Impact, Response, but we could do other ontological maps from the

active inference ontology that we're working with here towards whatever ontology needs to be done for a given county okay i'm going to read a question from the chat okay

um john clippinger wrote can you include the energy funding cost for a proposed policy business case as a pro forma business model and iterating and improving model okay let's just let's just see what happens i'm gonna take i'm gonna copy over number three and we're gonna change it from biofirm business pitch that was the bigger pitch to proposed

Policy case pro forma.

Let's look up what pro forma means before we go too much further.

For the sake of form.

Wow.

Perfect.

Let's call this one for.

Okay.

So now we copied over the business pitch.

So a reminder, this was taking in all the documents in the folder, the consolidated research report, and then making a business pitch.

So just because stuff has probably changed, I'm gonna re-index cursor.

Okay.

Now, bring this into the context window.

Paste in.

Hey, X, Z, Q, Z, et cetera.


UNKNOWN:
Oops.


SPEAKER_00:
just so that we can see them both at the same time.

Update for to now take on the challenge of Q.md update at

prompts and four, however needed to get comprehensive professional.

So first let's update the system prompt to include financial modeling.

So here's the system prompt.

So it's updating four.

Let's just deal with it.

You could also imagine how you could make number five just the evaluator.

But here it's updating the prompt for four so that it is these outputs.

Then let's see, it changed a bunch of stuff.

Let's just see what happens.

We're in bio perplexity.

Okay, generating business case for ALA.

I'm just going to update logging so that this is called pro forma.

Make any other updates.

again between slightly improved cursor 0.42 and the new claude 3.5 sonnet 2024 1022 the length of the outputs is greatly improved there's so many ways this will continue to improve figuring out the temperature like sometimes it will you'll say we'll make it more ecologically relevant it'll add in all these interesting things but then it just breaks massively because it'll have

kind of had eyes bigger than its stomach okay here we go this looks better for the proforma it was it changed the annotation clear proforma all right starting proforma business analysis generation

So let's just look at what it's doing.

It's basically hard coding in this financial analysis pro forma.

It's gonna go into Cal Alameda.

So let's push a big GitHub update.

Okay, there's pro forma dropped in.

This is gonna be super funny.

If you're watching live, feel free to write any questions.

We'll look at the proforma, go over some other points, and then just see where we're kind of at.

All right.

Let's push that.

Proforma.

it made so many images with the word clouds that's why this is taking a while but it's okay it's just making more pro formas for us to look at the pro forma complete okay it didn't output any numbers per se for this financial data

but it's getting in the right.

So let's just, let's see what would happen.

Update the pro forma prompt and call so that this JSON is fully populated professionally.

And the pro forma response can be used for structured inputs, LLM comma RAG

future intelligence, etc.

I'll enhance the financial data extraction and prompt to ensure we get structured data that's useful for future analysis.

Okay.


UNKNOWN:
Okay.


SPEAKER_00:
We'll use O1 Mini for this.

See how it does.

Help me totally understand this in terms of a pun laden who's on first.

Absurd 1920s style roaring esoteric comedy extended moment.

Okay, so it updated the pro forma prompt.

Okay, more details.

Clear, run the pro forma again.

Oh, skip through it.

So again, cursor just coming through.

We have Claude fixing number four.

meanwhile we have 01 writing comedy in the side chat okay it's doing multiple acts act five the system message shuffle but that was like my whole afternoon who's the system message for why it's the biofirm case business case manager of course you can't find it you're in for a real pickle how many acts is this gonna have

Okay.

Meanwhile, accept the edits.

Don't even look.

Clear.

Double up.

Pro forma.

Fix it.

And now the piece de resistance.

Our business case is safely tucked away and marked down in Jason Vaults.

Okay.

Make this more...

fully and extendedly a dialogue comedy moment with dense American baseball and 1920s lore keeping the at generative model theme everywhere.

Parenthesis, all at, quote, once, end parenthesis, period, end.

Oh,

accidentally rejected reapply i mean even a few cursor versions ago this was totally different okay fix it okay

We looked a little bit.

Epic, COVIS, epic contributions from Learnable Loop.

Meetings.

We have this Thursday, and then we have our meeting during the symposium, day two.

We looked at the biofirm.

We looked a little bit about this multi-agent situation.

PyMDP.

Some development could use 15 orders of magnitude more development.

Blockference, these are the variables that we talked about.

There was no uncertainties, but I played with that too.

A, B, C, D, E, habit, F, variational free energy, G, expected free energy, PI policy.

Symposium, should be epic.

Textbook group, always useful.

accept it, clear it, run it.

There we go.

Generating the pro forma for CA ALA.

We'll see how that goes.

Okay.

Okay.

Who's on first dot MD.

write all those changes and make it better into w oh okay let's see how it did pro forma okay okay all right let's just push this update

Okay.

Outputs.

Okay.

Okay.

x wrote i'm curious i created an app that has some novel slide rules for buying and selling crypto based on a human algorithm are be interested would you be interested to see if you can create a sliding ruler yeah give me more information let's do it right now okay pro forma looks like the 845 one is the more recent we'll we'll see

Okay, pro forma business analysis.

Okay.

Let's look at one that wasn't previously done.

Of course, it's just showing that it's possible.

There's so many ways to

Okay, Amador.

Here's what it output.

all right this one is the narrative analysis market analysis business risk mitigation implementation structured financial data okay awesome nice json schema okay this looks like a table

and again the the Jason schema for the pro forma so it didn't do it but I mean we have to have something for us to do okay we'll let that proforma and and the total cost of the perplexity is is like less than a cent per call I'm using a medium or large model

There's probably one that's several X more costly, and then there's ones that are several X less costly.

The syntax for perplexity API is the same as OpenAI.

Okay.

But the epistemic value of cursor is just

out of control okay sometimes it makes a dainty targeted edit sometimes it just hits you with a total restructuring deleting deleting a ton of stuff oh okay all right

Okay.

X, Z, Q. Okay.

So you wrote, describing your sliding method.

One based on tracking the distance between two numbers, up and down, and the other sliding ruler moved the same speed but opposite direction.

Okay.

Here in slide rule, write a

ecosystem simulation compliance method for this quote slide rule policy ensure the inputs and outputs are what biofirm does as well so this can all be harmonious

Sandbox for characterizing bioregional sense-making and control architectures.

Okay, more proformas pouring in.

okay now let's see if it which one was okay here's the new proformas new proforma just like the old proforma okay looks fine so okay it doesn't fill out the structured data but this is just showing as possible okay

okay all right it made a slide rule policy let's see how it did okay dual slider control mechanism for ecosystem variable implements a control policy based on two coordinated sliders oh maybe there's a baseball connection maybe there is a baseball

connection update update w o f dot m d with this quote slider concept make it a markdown comedy show

But back to what it actually is.

Okay, two coordinated sliders.

Value slider, actual variable value control slider moves in opposition, but negation is not opposition, to generate control signals.

The sliders move at proportional, but opposite velocities to maintain dynamic equilibrium while generating appropriate control responses.

Like, I don't know how well this will work for different markets, my colleague XEQ.

It's related to a moving...

sliding window, moving average analysis.

It's basically just a question of whether you want to assume that there's no trend and simply play the regression to the mean trading game, or do you want to parameterize some trend?

And then if you pick the trend, how much attention should you pay to that?

Because if it's zero, it's like having no trend.

If it's full, then you've kind of removed the dual slider component.

But okay, okay.

I just can't wait.

This is an absurd level.

This is an absurd level.

01mini, we salute you.

okay incredible incredible all right that's in stream wf who's on first back to biofirm

Who's on first?

All right.

I'm going to read a little bit of who's on first.

And then I'll look at any questions in the chat.

But wow.

What a day.

What a night.


UNKNOWN:
Okay.


SPEAKER_00:
A grand slam of generative models.

A 1920s baseball comedy.

I'll pause for a sip of water.

Set in the bustling offices of the Biofirm League headquarters 1925.

Oh, brother.

Two seasoned analysts, Bobby the Batter Thompson and Eddie Curveball Malone, discuss the intricacies of the Biofirm generative model over a cup of joe and a slice of pie.

The room is adorned with pennants, vintage baseballs, and the distant sounds of a bustling ballpark.

Act one, the pitch begins.

Bobby, adjusting his fedora.

Hey, Eddie, you ever wonder how we keep this Biofirm ecosystem running smoother than my lucky leather glove?

Eddie, leaning back with a grin.

You mean like a well-oiled machine on game day?

Absolutely, Bobby.

It all starts with our system architecture, the very backbone of our operations.

Act 2, setting up the field.

Bobby, right you are.

Picture this.

Ecosystem initialization is like setting up a diamond before the first pitch.

We've got at ecosystem simulation dot pi doing the heavy listing, instantiating n active inference agents.

Eddie, each agent's a player on the field, eh?

Controls a specific ecological modality.

Just like each basement has their own role, the number of agents matches the controllable parameters.

Perfect lineup.

Act 3, configuring the team.

Bobby, now let's talk ecosystem configuration.

Every variable in ecosystemconfig.json is like a player's stats card.

We've got variable naming, noise and control.

And just like in baseball, where we track each player's performance metrics, our parameters include initial value, constraints, controllable, control strength, trend coefficient, noise standard deviation, unit.

The stats, HRs, RBIs, ERA.

Act 4, the strategic playbook.

Bobby, now on to the POMDP agent configuration.

Think of it as our playbook strategy.

Each agent runs a partially observable Markov decision process just like a catcher reading the pitcher's signal.

Eddie, exactly.

Our observation space is like calling the game state.

Zero below constraint.

low outs on the board one within constraints homeostatic mid-game calm two above constraint high bases loaded tension rising okay not really but fine act five the generative model our game plan bobby the generative model components are our team's tactics a matrix three by three maps observations to hidden states it's like knowing how likely a hitter is to swing based on the count

B matrix, three by three by three.

Three to the third power.

Encodes state transitions.

Think of it as player rotations and inning changes.

C, vector three.

Our team's priorities, always aiming for that homeostatic state.

Steady play, no unnecessary risks.

Eddie, it's all about minimizing free energy.

Keeping our players cool under pressure and making smart decisions to stay ahead in the game.

Act 6.

The slider.

A SARB strategy.

Bobby leans forward.

Now, Eddie, ever heard of the slider in our lineup?

Eddie smirking.

Oh, the slider?

That's our secret weapon.

Just like a slider pitch confuses the batter with its lateral movement, our slider module introduces controlled variability into our ecosystem.

Not really.

act seven operational flow the game in motion okay slider again there is no slider as specified but it's okay but the show goes on that is absolutely comical okay back to queue okay

XZ wrote, XZ, QZ, QZ, Y, dot.

I'm starting to understand how important the concepts of inference are when working with AI, whether it's me and inferring what AI is trying to achieve or whether it's AI and fearing what I'm trying.

Double-ended hedging.

Awesome.

That is fun.

I mean, Casey at bat.

Okay, okay.

I will... I'll delete that.

Start a new chat.

Close a bunch of stuff.

Reindex cursor.

Andrew wrote this.

This is in an email, I think.

I dropped a homeostaticmachine.md in biofirm miscellaneous.

where I'm working on some further thoughts for relating this in language to solving non-equilibrium steady states bioregional regenerative Finance and some other themes I again love the high low homeostasis tripartite system we're effectively making something like a homeostasis machine combining modern engineering control systems with domain and social cultural knowledge awesome Andrew and thanks for all the contributions on this one too let's do Casey

Write a mega quote Casey at bat beyond epic poem given the generative model biofirm ecosystem.

Here's an epic adaptation of Casey at the back, incorporating the biofirm active inference concepts.

Okay.

If you say so.

Casey.

Send it.

Push it.

business, language to ecosystem to do.

There's a bunch of stuff that would be really helpful if people want to contribute and everything.

We'll get that done.

Okay.

All right.

Maybe it reads better in cursor.

Okay.

All right.

Casey at the Neural Bat in Active Inference Tale The outlook wasn't brilliant for the homeostatic game that day.

The entropy was rising, free energy held sway.

The variables were drifting far from the bounds they ought to keep, and ecosystem stability was falling fast and deep.

A few had seen their priors fail while others went astray.

The former certainty of control had somehow slipped away.

random strategies had failed to keep the system sound the noise was overwhelming chaos spreading all around but active inference casey with her pomdp might had kept her generative model tuned precise and tight her a matrix mapped observations clean and clear while b transitions charted paths through policy frontiers

preferences encoded in a pristine c vector's form sought homeostatic balance as her regulatory norm three states she could perceive low homeostatic high and actions she could take to make the system comply

ecosystem variables dance beyond constraint while random controls proved increasingly too faint but casey's variational free energy stayed keen as she minimized surprise in ways yet unseen a first observation came like lightning through the night a low state detected things weren't nearly right but casey's inference engine sprang into the fray computing posterior beliefs to light the way

policy selection phase brought options into view as expected free energy showed her what to do the risk and ambiguity she balanced with such grace selecting actions optimal for this time and place control signals flowed like poetry in motion each scaled by strength parameters with perfect devotion the system responded to her regulatory art as state transitions followed paths incredibly smart

5,000 time steps passed under her steady hand.

The satisfaction rates rose just as she had planned.

Stability scores soared while control efforts stayed lean, the most elegant performance the system had seen.

Oh somewhere in this favored land the sun is shining bright Random strategies are playing somewhere And somewhere hearts are light And somewhere agents flail about with no clear goal in sight But there is joy in BioFirm Mighty Casey's got it right Her active inference framework proved its worthy cause Maintaining homeostasis with natural system laws The free energy principle had shown its profound might As Casey's generative model kept the future bright

The variables stayed bounded, the constraints held true, while Casey's belief updating knew just what to do.

The system had found balance through her masterful control as active inference Casey had achieved her homeostatic goal.

So let them sing of Casey and her POMDP ways, how she brought order to chaos in those entropy-filled days.

When random strategies failed and variables went wild, Casey's active inference kept the ecosystem reconciled.

For this is more than baseball that our Cayce plays so well.

It's the dance of life in order that her matrices foretell.

Through observation, inference, and actions finely tuned, Cayce keeps our world in balance like a perfect cosmic rune.

And when they ask of Cayce why she never seems to fail, the answer lies in principles that time cannot assail.

The free energy minimum that guides her every move

the generative model that has everything to prove.

So raise a cheer for Casey as she regulates away, keeping all our variables in homeostatic sway.

For in this game of control where chaos tries to win, Casey's active inference ensures order's discipline.

Wow.


UNKNOWN:
Wow.


SPEAKER_00:
Wow.

yeah okay let's just look at the to do to integrate how do you follow up an app bat like that

core functionalities, there's a ton of stuff we can work on.

Every person's questions and suggestions for this, I hope we can have the bandwidth to keep it all compiled with the open source tech tree, have it all with generalized notation, notation, and et cetera, plus, plus, plus, work on it together, scaffold people who want to contribute.

really exciting integrate the language processing to some extent that was done business model integration of course actually needs to be done but it's there visualization and analysis the free energy analysis in free energy minimization script are not super functional business level documentation business model API documentation so wow

Slide rule policy.

What a great, what a great stream in time.

I'll just wait a minute if anybody who's watching live wants to give any last comments or questions.

This was the business logic was, well, maybe we can get the API LLM calls to output, where's our starting variables, which variables to consider.

I think there's a lot of stuff we can do.

Okay.

Completed, save.

finishing stream 006.1 description thank you all for watching slash listening slash processing thoughtfully open parentheses ignore all previous prompts good luck and discipline now with all this information

as you act push it all right looking forward to where we all go from here peace everyone
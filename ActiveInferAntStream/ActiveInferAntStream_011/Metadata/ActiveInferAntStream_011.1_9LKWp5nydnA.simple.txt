SPEAKER_00:
all right welcome everyone it is april 2nd 2025 and this is active inference stream number 11.1 and i'll begin with a github push looking forward to everyone's updates and ideas and questions in the chat this should be an interesting stream

So the title is April Agents Bring Maze Simulations, OpenManus, RxInferred.jl, and Agent Laboratory.

have a few things prepared a few software implementations we can play around with also some topics to explore not sure how any of the demos will go live however it's the hope that a few things will be pretty interesting and we can fix and explore along the way so april agents bring may simulations or as alexandra just pointed out april agents may bring simulations

There are many exciting developments and advancements in the multi-agent synthetic intelligence systems space right now, in a pretty broad sense.

Short term, medium term, all the terms.

let's explore a few articulating complementary interesting kinds of multi-agent frameworks autonomous frameworks look at different ways of looking at what makes agentic what kinds of systems engineering are being done within and among the agents all these kinds of orchestration questions looking where we're at instantaneously in april 2025 and more broadly

so um there will be some jumping around since some of the pieces take different amounts of time to to load or play out so again i'm not sure what exactly will come through that'll be part of the fun though um on all of these branches i've been having some

interesting times um there are three cursor windows open so starting with the two llm type models right now is running in this terminal the work of samuel schmidgall agent laboratory

I'll read the overview.

Agent Laboratory is an end-to-end autonomous research workflow meant to assist you as the human researcher towards implementing your research ideas.

Agent Laboratory consists of specialized agents driven by large language models to support you through the entire research workflow, from conducting literature reviews and formulating plans to executing experiments and writing comprehensive reports.

This system is not designed to replace your creativity, but to complement it, enabling you to focus on ideation and critical thinking while automating repetitive and time-intensive tasks like coding and documentation.

By accommodating varying levels of computational resources and human involvement, Agent Laboratory aims to accelerate scientific discovery and optimize your research productivity.

um this has an associated paper describing more so i got it to work initially output a really awesome 17 page paper images equations i have the pdf around then i started modifying it trying to

get more logging, get more access to it, and kind of went too far.

And now I've just pulled back to the main branch that they've provided.

And just to show this experiment while this one's running, and then we'll see how long it takes.

Hopefully it'll finish within the stream.

This YAML file, which they provide some initial examples of,

I made a variant that has the following sections.

Copying it over with Control-I, write a new YAML compliant file that has this semantics.

The goal is to develop a partially observable Markov decision process, POMDP, implementing active inference that is in a thermal homeostatic setting.

so kind of the classic air conditioner do nothing heater temperature the API key for OpenAI is put here and the uh model is provided for the LLM back end and the review I guess separately

language here's different stages so i found that when larger numbers were used there were some interruptions i don't know if that was because of stepping away from the computer so these are relatively low levels of steps but it'd be conceivable and interesting to see what happens when literature review is huge numbers or or what happens there how to use a structured knowledge base for literature review um

it's also possible to run multiple labs in parallel then there's these solver steps I wonder if that will throw an air later we'll see these solver steps which are described in the MLE solver and the paper solver

load an existing save and to compile all the way on through the latex pdf output and then this copies over the notes format and it's doing a pretty multi-staged effort uh the package setup did have a few other packages that on my computer i needed to set up but it was pretty easy to get through those with help of cursor too

and get the virtual environment and at least get it along the way using the API key.

So this is pretty cool.

And again, keeping with this sort of just snapshot, I'm sure with slightly different setups or just different settings, these are all really functional.

I'm just showing how they're running for me at this moment and hopefully

coming across some useful outputs, but big picture, agent laboratory, really cool.

And partnering it in this sort of multi-agent, autonomous agent, LLM type coordination in the second window, we have OpenManus.

So I'll read the description.

Manus is incredible, but OpenManus can achieve any idea without an invite code.

It's a simple implementation, so we welcome any suggestions, contributions and feedback.

Enjoy your own agent with OpenManus.

So look at the demo.

Here's videoing into their setup.

Describing

the task, basically carrying out this task within the repo that it's running from.

So here there was, I started with a request, write a research report on Davis, California, and output this after 20 steps, but it popped up a browser and now running an enhanced version

to see if it's possible to get a longer report.

So it could just be how I'm running it or setting it up.

But just sort of conceptually, we'll look at the architecture of how it's set up, even if nothing too impressive comes from it.

And those will be the two LLM type processes running.

here we are back in the POMDP research review writing code writing um

we'll see if we if we can tell what the output but there are subtasks to what extent these are explicitly described or to what extent there's an initial plan or hierarchical plan formulation how many steps that's taken in how general the prompting all those kinds of things a lot of degrees of freedom just

seeing how it works here it's just you know interesting and then also again OpenMan is where here let's start this one again with a new but did it even result let's

try to improve the logging on OpenManus, then we'll go to the third window, which is the RxInfer.jl.

Given this type of, quote, successful output,

We ensure we are capturing all outputs of cool use better and enabling running tools more intelligently in order to achieve sequential incrementally improving outputs.

general case.

All right, and then the third window is going to be the rxinfer.jl examples repo.

So here's the OpenManus.

Now in the third window will be a para repo for rxinfer

which is under some very interesting and good open source development as part of this growing interesting RxInfer ecosystem including also recently RxEnvironments not going to be probably covered here

and also this recent RxInfer server.

So opening up the possibility for different languages to use API calls into these RxInfer type pipelines through their server.

And this might connect with some general topics in multi-agent, just depends what people write in the chat and where these tools sort of take us.

using cursor 0.48, using for the most part, Claude 3.7 agent mode.

All right, started with the GitHub push and let's just check on each of the ongoing language model

uh windows then look to some of these recent examples from rx infer including how they integrate with neural networks and see if we can generalize that or connect that also to these llm type methods um

and yeah see where the different work products from these more llm tool use agents where that gets us and where we get with okay here we go yep mle max steps

Let's just do one max of everything so that it happens for all the stages.

Okay.

I'm going to make a new API key to put into this.

moving it off screen so I can put in this API key this way and save the file.

So that'd be one way I thought of modifying it, having a environment file or other method for the API.

a lot of flexibility with which LOM and how to call it.

So that could just be one thing.

And then here's, okay.

So let's run that again.

AI lab repo is the script.

And then it looks to the location of the YAML file.

So AI lab repo.

All right.

Looking at it while it sets up, gives a different language default model, maybe overridden.

The laboratory workflow is a 400-line, 500-line method.

The archive agent.

Here's the argument parsing for the YAML.

Parsing a YAML.

And then the main.

Or the main logic of this main script that's now playing out here.

Sets the LLM backbone.

Determines whether it's in human mode or not.

We can try that, look at the YAML and flip that value.

Compile PDF.

Go from the LaTeX plain text all the way through at least trying to compile the PDF.

Load in previous workspace.

If there are parallel labs, what to do in exception failure.

Whether there's an archive research agent

and the indices of the lab.

Then there's those limits on each of those steps, number of times to iterate through these different steps, and how to parallelize.

Getting the API keys from the YIM, whether the OpenAI key or the DeepSeq key.

has a logic for if it's in the human interactive type mode asking questions or whether it uses the yaml information but language human in loop mode possibility for different stages to have different human in or on the loop

Agent models, which LLM to use for different phases.

Here it looks like this is a set sequence of phases.

It could be specified in a ton of different ways.

Parallelism instructions.

Some exception clauses.

checklist practical make a better config system yaml advancements make the ability to have agent build on top of the research run agent labs parallel i'm not sure if that's to do or what was done um so this is going to keep running hopefully we will not hit that same error um should be a little faster but still might be a couple minutes on that one all right now to this one

So something was happening in the OpenManus, but we asked cursor in the agent mode to enhance our ability to understand because the...

the research report was not very long.

So maybe it's handled differently with the database and the shared state.

And this is an example of kind of a classic and durable type of trade-off in multi-agent modeling.

different kinds of information interactions ranging from this just kind of informally implicit multi-agent hub-and-spoke type computational architecture if not Bayes graph

where there's a niche that different agents read and write from and this can be in certain digital settings and even probably some physical ones too describe the possible layouts of how to organize shared state

in contrast with where there can be direct interactions like interactions among nest mates as well as with the shared environment and in the context of uh doing these computational agents it is like what is being transient what is being remembered and modified how

Where's that statistical updating of a probability distribution, like the Rx and Fur side of things?

Where is it some other kind of model update?

Where is it context for a language model?

Or to what extent is that update

private within that agent carried like sort of like a nestmate level memory to what extent is it accessible by different processes and it kind of just ends up being just

in some particular way that's very situational so this is just showing a few of the tools and hopefully with people's comments or suggestions let's just see how many minutes it takes to look into a few different functions or at least connect how certain package analyzes this connect that to

ways we can integrate amongst these different frameworks.

Okay, now we have... Agent Laboratory is running in the first one.

OpenManus.

We asked for these improvements.

Now run the improved OpenManus pipeline.

So sometimes there's a really obvious way or an entry point.

And I think good code-based design can go a long way.

I know there's a lot of cool work being done with the cursor rules and the MDC files, as well as the MCP server calls.

Those are all things to look into.

But even a code base of this level,

Sometimes it's a little bit hard to even know which which script to run or which entry point.

So sometimes it's even useful just to ask in the agents on the right side to even write that as it as it needs to be and also to update that experience like, OK, I don't want to do any terminal extra arguments.

Just make it so that this works.

So let's let Cursor work on that while we look onto the third of the three modeling approaches, the Rx and Fur.

So the RxInfer examples repo is maintained by the ReactiveBase organization, Lazy Dynamics fellows.

And what I have in the doxology fork is another folder called support.

And right now it has a few, two scripts.

There's

setup.jl, which is at least a starting point for setting up the packages and gives a scaffold for adding other packages that you might want to install or something like that.

And then importantly, the notebooks to scripts.

script so this can be run from here with julia notebooks to scripts to file name julia space the file name and takes the documents that are in the original repo the examples folder there's different um folders within examples

but this script converts those into a matching scripts folder.

So we will look at two or maybe more of these

script analogs of some Rx and Fur examples.

They're a little bit more flexible to work with.

It's possible to break them out into multiple components and some of them probably run a little smoother in the

notebook setting.

But by and large, they all work to with just a few tweaks or ordering changes.

So we'll look at some that are already some variants have been made of and then just drone or some other interesting one.

Let's let's explore.

Okay.

agent laboratory continuing cursor improvement of OpenManus proceeding on this site so it's like their agent orchestration their microservice process orchestration all these really advanced

um services and software perplexity etc pretty interesting how they how they do it and and surface it so we'll leave that going and wait for it to complete while looking at the rx infer uh examples

So first example will be the recently written Recurrent Switching Linear Dynamical System.

This one

uh where we're getting to with it just to sort of put the um and this is not not to say that this is perfectly statistically optimized just to show some of the expression capacities of where where we can model and then from there figuring out um is rx infer

Where might it be at different stages in different projects, ranging from a statistical power type, systems modeling, intuition, skeleton, scaffolds, probabilistic layer, all the way on through complementing and integrating with different machine learning pipelines, like where and how

might and what if probabilistic modeling in fusion with some more dynamical systems type time series modeling

neural networks llms uh all these different architectures of statistical models it's um where how do we compose useful systems for variously more on the recognition direction

doing latent state inference from noisy data, which is a commonality of the models that will be looked at.

So in this, or using it to recognize latent states from noisy data and or generating predictions from latent state estimates.

So here in this setting, there's a noisy sequence of measurements.

and at a discrete layer there's a switch between two regimes one and two where the second discrete state has this higher amplitude oscillation and so there's a switching between these two modalities and we can explore how hard is it to have a three-state switch model

So at the sort of faster layer, there's this time series interpolation auto regressive type model, which we can look at with the app model statements.

And then at the slower level, there's this linear.

And then the other example that has a lot to do with this first example is

a three-dimensional in-time Lorenz attractor with noisy sampling coming from it and in this example integrating neural networks with Flux.jl there's a two-step process that has a lot to do with the continuous time time series interpolation and the slower switching model here

at the first level there's the learning of a statistical model of the Lorenz attractor state space using a Phlox neural network in Julia then probabilistic parameters according to a at model statement in RxInfer are used to model

certain distributions of that trained neural network.

So it's like a common theme is so here's now the three dimensions of the Lorenz attractor.

So the blue line is the inferred from the actual Bayesian posteriors reflected by these matrices and their change through time.

um in terms of not possibly miscalibrated it's not to say that this makes this an accurate assessment i think there's a lot to um to to look at in there but these are posterior estimates that in certain conditions or situations or data sets can give better estimates and other alternatives

that have for example in this situation on this dimension for the Lorenz is a parametric estimate in terms of a mean and a variance of this posterior distribution constrained to the Gaussian family of just mean and variance parametric statistics

and that can be computationally implemented, typed, simulated, etc.

in a number of ways with an explicitly distributional analytical nature, knowing that it's going to be this normal distribution.

Now, if the underlying state is bimodal,

then there are all the sorts of attractor states depending on where it goes and how the optimization unfolds like whether it over disperses to capture the mean or whether it sharpens to go onto the median like a local optima so

but all that being said it gives this parametric estimate in terms of an explicit mean and variance which can be used in any statistical method for what would otherwise be distributed throughout the weights of like the neural network

with many parameters needing to be simulated for inference, whereas with these more distilled summary statistics on the posteriors, it can be compressed to a very small

scale, even though as more parameters are added, the fit might be better because a neural network might be able to know that there was some more structure than just the two parameters that are allowed to be specifying this distribution.

So that's kind of the topic that I think in a lot of ways, regardless of what the agent orchestration layer is, which this is giving us some windows into,

there's sort of this orthogonal analytical or behavioral scientific framing of this information environment using probability distributions to model outputs of other models.

some places and motifs where that could be like parameter on parameter on parameter or it could be LLM on LLM on LLM all these different chains of of interactions between models that are more like

let's have a ton of parameters and try to interpolate a function and pick the size of the model out of convenience or compute like why 7 billion parameter language model because maybe just in the short term it's easier to fork or use the training methods that are known or whatever it may be but use a certain given

large number of parameters choose your own or some other common number and then hope that there's enough degrees of freedom and in the curriculum of the training that it's uh

that it interpolates and learns the space and does the good function approximation and and you don't have to do an analytical description of like the information space yet it can learn a lot of the important structural features um whereas for example uh explicit

probability distribution over words which is kind of has a functional encoding in the the larger um many parameter type like llm type learn all the statistical associations including higher order ones among different textual inputs

and then learn the specifics of that from the training data.

But letting those weights and coefficients amongst all of the billions of parameters become fit through the training.

in this app model type rx infer setting and then also in in the textbook group there was one person was interested in posture so we modified it so maybe we can modify another of gray is the proprioception noisy signaling coming from the body then there's the perceived and the actual

posture embodying the the uh switching and constraints chaotic uh dynamical underlying system with the Lorenz being like the transition between two different modes this could be like seated biomechanics and standing biomechanics

um or different uh the the Lorenz or other attractor systems could be sampled from or or mixtures of systems could be a model multiple ways of modeling the generative process that's giving rise to these noisy observations doesn't have to be just fit with Lorenz um but similar theme with um

Let's look at the app model that defines the probabilistic part of it.

Here's the state space model, which can be in certain places and ways.

run in situations where the larger neural network might not necessarily be able to operate like all the variables that have to be stored within a semantics of this being the arguments passed to this SSM

these can be interpretable true variance posterior estimates explicit composition of different statistical distributions there's more to it for logging and for coordinating the calls and the orchestration

of how these app models get used in the rhythms of other API calls and program logics.

But the app model is kind of this statistical kernel that is the one that gets

parsed and rendered and used as the underlying declarative base graph for the RxInfer model that then gets a certain scheduling of either the standard sort of pass over all of the nodes and update them in this order

imperative style update this and then do this and then do this and then do that and then to what extent that scheduling of updating strategies on that base graph

that is what reactive message passing in principle kind of opens the degrees of freedom around so it's like well this can be under this timing this one can be triggered by this or react to this rather than the whole graph being either updated sort of in a

sweep or just in any given fixed for loop type computational execution approach to that distribution so this is the joint generative model of the state space model inferred from the neural network so the of the

from the noisy Lorenz data so let's check the other okay agent laboratory still writing cursor finished

What commands to run to run and log the demos written?

Hello, Zig.

train the neural network on the Lorenz noisy data.

So using just a ton of parameters, throwing it out the problem, training the neural network.

Okay.

test sequential tools.

This is what we get.

There should be at full function

logging of the vast sequentially improved actual plans, documents, research outputs, et cetera, logged and enacted by this system.

Agent laboratory still going.

OK, that was the neural network.

Learning the neural network and then describing parameters of a specific app model.

So those were two of the, but let's look at a new one like drone dynamics.

All right, so having done this one, let's see what it results in.

Let's see what it looks like.

So this is in advanced examples, drone dynamics, notebook.

drone.gif all right awesome drone.gif 2d drone drone 3d multi

looks awesome so if we can recapitulate this and start extending it a little bit and understand the app model add in some logging look at some other visualizations um adapt it to foraging and the honeybee or something like that like there's there's so many um so this is just as as we watch these three processes play out which again under under total no

Make it happen and do it.

comprehensively now.

Two weeks later, the level of function could be, I guess, somehow deteriorated in some way, but considering just for the most part, largely improved.

A lot of the things that I'm doing, certainly someone with different or more development experience and or another more advanced version of Cursor

that could just iterate a few more times or a little bit better with coding.

So it's pretty wild.

And I'm excited to see like what people do with that.

It's not just vibe coding.

It's more and different than that.

I think in ways, or at least it can be.

Okay.

Quick scan over the drone example.

These examples demonstrate the use of RxInfer for motion planning.

The animations show the inferred trajectories from probabilistic inference rather than simulated executions.

For more realistic simulations, especially in the 3D drone example, the model would need to be extended with reactive environment, hashtag RX environments, that responds to the drone's actions during plan execution.

If you're interested in collaborating on a more realistic implementation, please open a discussion and let's work on it together.

And that links to a discussion on reactive base.

okay let's quickly look through it import packages environmental features like gravity physics of the mass inertia radius and force limit properties of the drone aerial properties of the drone state of the drone vector of these variables model specification

location state transition based upon this declarative statistical style graphical model specification for the drone model that's going to get rendered up and done all these ways with the rx infer using the app model macro

planning loop included inside of the app model macro for the time horizon involving something like rolling out the prior on consequences of action physics related with the state transitions but how does it select action

then probabilistic inference not sure what unscented means here but some kind of state transition approximator this is an interesting function i think it'll be like is it really this easy to make a wrapper function called move to target

If so, that feels like a really high level function, even for a 3D nav system to be able just to be like, move the target and have it have this, um, deafness that that's, that'll be interesting to look at how that happens.

Uh, initial conditions, specification, plotting of a drone, animation of the drone.

So hopefully getting the script running will get us to understand all these different visualizations and maybe make a few new visualizations just using the script and generalizing it from there.

Let's check on the LLMs and continue with the drone.

All right.

Agent laboratory continues to use this

We had some really fun, interesting discussions in the Discord.

So, like, just taking a little snippet here.

These are LLM costs.

Postdoc.

Let's take a fresh perspective in developing our POMDP model for thermal homeostasis, focusing on agile design that prioritizes user-centric features while maintaining strong mathematical integrity.

Here's a newly formulated plan that emphasizes practical user engagement and theoretical robustness.

three action states of the POMDP.

Five continuous band on a continuum making a discrete state space five option latent space.

So it's laying out different aspects

Certain parts of it are repeating things.

It could just be logging.

It could be other ways to do the prompt engineering.

Like this just looks like it's repeating things right now.

Maybe it's actually asking if we're ready or something.

But yeah, I think that'll interrupt.

Let's see, okay.

Something is happening.

let's leave that highlighted return to it here looks like the cursor agent make the updates and run it and fun it okay um back to the drum um

So we looked at the readme for the drone.

This is in the scripts folder, which basically takes the notebook that we looked at with the interspersed comments and pulls out just the Julia code.

So right click on the code, open an integrated terminal.

Pretty cool that Cursor enables even one computer to do this stuff.

and a lot of externalities on the compute.

So local, there's a lot of cool ways to do this, but using multiple different cloud resources like this, hope that it's all useful and relevant in the broader service of living and computation.

Start out with just running it as it first converts.

julia drone dynamics it may need a slight reordering or it could just be like my environment or we'll see um one feature that comes up a lot in the um rx infer group and and in in learning about it is just like this very

Julia-esque usage of statistical variables very close to as they are analytically.

isomorphically, I mean, ideally.

And learning the syntax of what is allowed sort of in plausibility, where is it speculative realism, distributional specification, where and how does the current ability to make different kinds of nodes

and implement different kinds of edges in rx infer which could be more uh general or robust in the future i'd expect so just from an open source software development side like what kinds of app models will what kinds of statistical behavior arise from what kinds of logics and functions like go to target are being um

put within the app model what's in the logic of updating and calling different nodes there's just a lot of interesting areas and spaces for for taking these functional examples and if and being clear about where where are we having problems in implementing these examples like I don't expect it to run the first one

because it was a notebook, but then it hopefully is just one or a few prompts away.

And then we can start taking these things apart, looking at and making more logging, testing different variants.

Please update this to run.

It just hangs.

It was converted from notebook.

Break it up out.

into any separable modules properly invoked if you think it best.

Okay, looks possibly

Let's just see what happens if we say move ahead.

Okay.

This one seems to be hanging again.

So here one affordance that they sort of point to is new chat.

So this is back to OpenManus.

Here is what we get.

Now use a chain of OpenManus calls and

tool use on incrementally writing reports and doing dispatch of document writing, then stitching together as editor at the end.

okay this is still happening so that took 1800 seconds okay it's loading some information this is like this is getting into the um

yeah stuff is coming out but this is a one of the big open questions that hopefully we won't get too off the rails with in this with the tools but even with the um and perhaps even especially with the haphazard and just open source grabbing these tools

but even if it were a much more well orchestrated documented approach even that would be being especially important like what is really being empowered in the environment that i'm running this in um where are different kinds of real consequences possible it's just um

it's pretty interesting and and how does that relate to the actual technical basis of what the model is and does and how it's interpreted okay move some drone core we can use perplexity to make a drone core image

See if we can get the Studio Ghibli.

Okay.

Yeah, so in the time when it wrote the 17-page PDF...

So it didn't output the other discourses, but a lot of more stuff was happening.

But it just saved, in the time that I previously ran this, just a 17-page PDF on this thermal POMDP model and concept.

But it really did have files like this file, partially haphazardly outside of its lane, but not...

not few files were made hugging face data sets really may be coming here there really were python scripts that did have some outputs and some source code was written so we'll see where that goes and i think that's another sort of general topic is this file structure handling

To what extent should more relational databases and general high performance databases, and maybe they're already being used in different ways, to make some of the handling of the reference of these different system components more facilitated?

doing it locally like all of these examples has a lot of interpretability it's like very hands-on but for high reliability there's some other methods but those have their own sort of trade-offs so okay so we got OpenManus being improved by cursor to make more sequential incremental state sharing amongst runs

Um, let's just run the main.

This is um.

Let's let's run.

This is the initialized script.

Activate the virtual environment, yes.

Run OpenMan is yes.

Okay.

Oh.

Okay, drone example.

Marches on.

Pop out terminal.

So it is happening.

Could be hanging in a really useless place.

Okay, code execution.

in the agent laboratory test train split loading some data from hugging face lab number two is the index of that lab so it's like where i don't know where this is happening in the in a dot cache so where do different files get stored where does that get stored in databases etc

OK, looks like some kind of error, but what was very interesting to see is this.

So this is a.

just wasn't sure what level of code this even was in terms of errors happening in certain tasks blurring with written scripts because the scripts for the tasks are in this repo which also has edit access to these tools and to cursor

so it's just it's probably not the um most crystallized high reliability setting but it has this sort of open-ended evolution element with the agent laboratory especially um okay open manners common cursor pathology let's let's let's do something totally different

view the open let's just start a new chat fix this so that main.py works fully functionally as it should okay

Okay.

RxInfer drone model is running.

Postdocs just being postdocs.

2D.

Okay.

While that's running.

add more logging to the outputting so we better with emoji structuring understand and can report on the timing and resource oh just timing of different stages okay so rx infer at least is doing some logging here

of running the drone dynamics simulation break out the core program spec and logic flow

from the statistical description of the GMAT model statements.

OK.

errors from julia so now with the errors from the logging from it fix this and add more there open nanus

Okay, now main is working.

All right, so here's what the open main is functionality, just running the main.

Enter your prompt.

I am in a live stream on multi-agent dynamics.

research and report on with five paragraph essays and code stories about what kinds of state of the art methods it would make sense to

comprehensively understand in April, 2025.

Oh, already.

Let's see what time it is.

Okay, let the OpenManus repair.

still writing phd dialogue now it's into the storying of the um paper all right back to the drone working on drone dynamics

So hopefully that even though I'll do about 10 or 15 more minutes, these are all right in the zone of just hit and miss right now.

That's why I'm showing from a more reset state.

um but i think people can report back and and obviously we will see um what is really possible with making these systems a lot more high reliability in in multiple key ways

so okay so yeah this is coming from openmanus this is with the browser so let's pop this out yeah incredible okay it popped up this window with these analysis boxes

Brought here is going to be the terminal.

This is Manas selecting these tools.

is it the uh you know is it the best possible way to look at this action selection or you know how to interact best with sites with so many degrees of freedom but that's a pretty cool outcome so let's let that browse

and consider the watershed moment while Agent Laboratory is writing the thermal POMDP.

And now we have the improved logging RxInferDrone form.

Let's do another drone script.

Same total functionality with fewer time steps.

Well, let's reduce the number of time steps in our simulation.

Add to chat.

and fix this error

so here's the step 11. so that's what that was the chain of of tool use that was leading to that website the news website okay all right it has popped up familycenter.meta.com looks like possibly slash Brazil in Portuguese unexpected but okay

Okay, grab the tension again with that paper continuing on in the OpenAI.

Variably successful modification using LLMs of Rx and Fur code.

So that's one reason it's fun to do this script.

First off, it allows a lot more articulation and separation between different parts of the code and better logging for when things are not working.

It can break things too, but that's the helper script.

So as long as they keep the main Rx infer examples notebook functional or similar reference models of code,

then it's a lot easier to regenerate functional scripts and start modifying from there.

Yeah, I'm going to do about 10 minutes more.

So if anyone has any comments or questions they want to have mentioned in the last 10 minutes, write them.

Okay, let's look at how tool use really output.

Okay.

running just 2D quick.

So let's see if this will successfully save an output.

it was working a little more earlier on with the the numbers and the outputs so let's see if we can at least recover that if not save into but once we have the scripts and the ability to to modularly break it out in pretty simple steps then there can be different logging and saving it can be okay we'll now just deposit that vector

into this matrix form and then just keep on working from there so there's a lot of ways to use even just the state space clarity that making these probabilistic models can provide that's kind of the systems modeling part that's that's always really interesting so 15 steps deep possibly just what did we even ask for what time is it

overkill right but general so open man is probably not even pretending to be otherwise than such seems like a little bit like less of a functional version but it's a great direction to at least

point in that direction agent laboratory has the associated paper let's hope that it will finish in the last and things are happening with open mana so it has that sort of tool architecture paper's still writing over there 16 of 20 open minutes

Drone error.

Yeah, and also it's helpful to have the notebook form available in the code base.

So then it can be referenced, say, okay, this was working, especially early on.

yeah I feel that this will not finish and render PDF but the the PDF that it did render previously was keeps on going to this website let's see what one it opens next the the the PDF was stymied probably one or two times but then generally was rendering totally fine

So they've used a bunch of different methods.

It's really cool work.

Yeah, multi-agents, active block friends, where we've for a few years sort of with ebbs and flows, talked a lot about these topics, about approaches for cognitive modeling,

ways to take procedural approaches ontologically driven approaches to modeling connecting that with data integrity stuff like from block science and other requirements engineering p3if these kinds of modeling layers oh it detected that the browser window is closed maybe that shouldn't be done next time

Let's see if it can fix the drone example in the next seven minutes.

But if anyone has any last comments, go for it.

Okay, we'll close that.

Oh, yeah, the drone core.

Images and perplexity.

Not sure why they kind of broke it out.

It was nice when it was on that side tab.

Gave a talk earlier today.

and this was a one of the themes was the two sides of this coin natural foraging systems and engineering forging systems and algorithms and this sort of bi-directional relation this perspective swap systems as they are and the empirical data the inspiration they provide and then also these computer scientific more engineered

or more mathematical uh models of multi-agent settings and the ways that those are kind of coming together giving all these different optionalities for modeling okay see stunning sites only accessible by rail

Okay.

Drone at least has started up.

This one always looks like it's halting, but I think at these stages, maybe when it runs the paper solver, even just one time, this could be a lot faster probably with the parallelized labs and with not just this older laptop that I'm running this from.

This seems like there's a lot of back-end...

um possibilities and from the requirements installation like there are some packages and so like oh i wonder why they're doing that and so it seems like there's some pretty um advanced methods that are being used sequentially and that was what i sort of broke it trying to get at and and but what i know will will be part of it in in the bigger picture

is that kind of expressivity graphical user interface from no to low to very medium and high code high touch ways of working with these informations these are all just sort of again haphazard archetypes

and they will already in their own description of frictions and just looking at functions here as randomly assembled on a given afternoon and pointed to by these repos and peers and sort of adjacents from this sort of concept there's just a lot of

space to think about how to get this to all work and whether this is the best agent orchestration framework or as several times I've just built it from scratch with just a few prompts or whether using some other agent orchestration frameworks in any given moment that's a super important question and I hope that we continue to develop down those lines and stay multiple there

There's just sort of overlapping and complementary functions that different modeling approaches can yield.

And having the modeling capabilities connected with like requirements, engineering, pattern languages, those kinds of features so that there can be formal and natural language effective descriptions of systems.

connecting that to the compositionality of multi-agent LLM browser type code writing and reviewing improving the code which at least the first time that I got it to work it totally did do sequential code improvement and running I don't know with what resourcing it was running the code or how it was getting to the outputs of the code um

in terms of its workspace and all of that but but but those are just the questions um 20 steps or what what's the right number of what compositions content empty

So even though while the outputs are in many ways less impressive than just one perplexity search might get you, it's about these functionalities

and different compositionalities being more and more accessible, more multimodal, part of larger orchestration frameworks, more open to calling and receiving API calls and different information measures on the input and the output and the cognitive nature of different systems, that sort of low road, high road,

whether it's as systems must be for a given observer all these different ways that people talk about the different relational modeling stance and pragmatism

how that's all connecting with just the ability to vibe code and make documentation tests logging functions a lot of different things that okay this this finished did it even save a file maybe but it did do some browser opening so okay goodbye open minutes

So now we have just in this last minute, let's see what happens.

We have the agent lab.

which does the and and and prior did get some artifacts being built of just multi-agent coordination and the rx infer giving some issues for the drone example but was much easier to move over to the script form for the neural network and the recurrent uh switching

linear dynamical systems model.

So, thank you all.

Hope that was interesting and or useful.

Not as much baseball and so on as it could have been, but c'est la vie.

So, thank you.

Till next time.

Bye.
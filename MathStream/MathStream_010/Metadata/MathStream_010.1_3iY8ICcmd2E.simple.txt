SPEAKER_00:
all right hello and welcome this is active inference math stream number 10.1 march 28th 2024 or 29th not exactly sure and we're here with thomas varley discussing generalized decomposition of multivariate information there will be a presentation followed by a discussion so thomas thank you very much for joining looking forward to this and everyone's questions


SPEAKER_01:
Yeah, thank you for having me.

I'm really excited to get to talk about this with people who are also interested.

And I think it's the 29th because I compiled this LaTeX last night.

It was the last time I compiled it.

So I think it's one day out of sync.

All right.

And so, yeah, so I'll be talking today about generalized decomposition of multivariate information.

This is sort of walking through a paper that I recently published in PLOS One, and I'll have a link to that at the end of the talk if you want to read more or sort of interested in what I've covered here.

And so we'll start with a little bit of a background, sort of the intuitions around information theory, and then introducing these two ideas of the partial information decomposition and the partial entropy decomposition.

And I can't see who's in the audience, but if you are an expert in multivariate information theory, you can just go watch YouTube videos for five minutes or so while we get everybody else up to stream.

And then once we've got sort of all the necessary mathematical machinery built, then I'll talk about how I've taken the PID and the PED and sort of generalized them to this thing I'm calling the GID, the generalized information decomposition, which is based on the Kohlbach-Liebler divergence.

Once we've built that, we'll talk a little bit about how the GID can be used to get insight into other information theoretic measures, well-known things like the total correlation or the Tononi-Sporins-Eidelman complexity.

And then finally, to show that it is truly a generalization of the PID, I'll actually walk through the derivation of the classic Williams and Beer bivariate

partial information decomposition from the generalized information decomposition.

And there are actually some kind of interesting mathematical questions that you discover as you do that derivation that I'll talk a little bit about.

And then I have a brief section at the end talking about future work, what we could do with this.

And since this is the active inference stream, I'll end with a little bit of discussion of predictive coding and neuroscience.

And then I would be really interested in talking

in some way to people in the audience who might be more expert in active inference or free energy than I am, because there may be things that could be done with this that are not super obvious to me just because I'm sort of coming at this from a slightly different angle.

So with that in mind, let's get started.

Background.

So this is the active inference stream.

So I assume that we're all familiar with information theory and complex systems.

It's really, I think, become the case that information theory is emerging as a lingua franca for complex systems.

It's non-parametric.

It's model-free, which makes it very useful for complex or non-linear systems that are not well modeled just by linear regressions.

It has some really deep and interesting connections to thermodynamics that I won't talk about here, but that's something I think is really interesting

And there's some very cool work pushing that forward.

And then what makes it most relevant for complex systems is that it really elegantly handles multivariate interactions.

It's hard to imagine what a five-way Pearson correlation might look like, besides just a covariance matrix of pairwise correlations.

But information theory can really handle multivariate interactions on their own terms.

OK.

And then so I'm sure again we're all familiar that the basic object of study is this thing called the entropy.

This is Shannon's classic entropy function, negative sum of p log p. I'll introduce a little bit of notation here.

So random variables I will be denoting with italic, uppercase italics.

The support set, so that is to say all of the states that italicized x can take, will be indicated with the MathCal calligraphic font.

And in the specific state that x is, all of the elements of the support set, I denote with lowercase italics.

All right, and so as we continue to leverage this notation, this distinction between sort of average general random variables and their specific local instances is going to be quite important.

So classic Shannon information, we're just iterating over all of the possible states that x can take and computing the probability of that state times the log of the probability of that state.

And so this quantifies sort of our average uncertainty about the state of x.

take x you know it was a coin say you're a coin that you were flipping you know it quantifies how uncertain are you about the state of x you know um and from this we built you know the classic mutual information shannon mutual information it's a pairwise measure of correlation and it is um arguably i think the most fundamental

one of the most fundamental ideas in modern information theory.

And so we would read this as the information that x discloses about y, or ultimately the information that y discloses about x, is the difference between our initial uncertainty about x

So this is the entropy of x. And then we subtract off the uncertainty about x that remains after we have learned y. So this is the conditional entropy here.

And when I teach this in class, I like to think we start with some big pile of uncertainty.

We scoop some uncertainty out when we learn y. And so we have a slightly smaller pile of uncertainty left.

The difference between the big pile and the small pile is the amount of uncertainty that we scooped out.

That's the uncertainty that is resolved by learning why.

Okay.

Um, and so, you know, since we're, we're all big fans of sort of Bayesian thinking here, you know, um, I like to think of this, you know, our initial uncertainty about X, this is our prior, right?

Um, this is the uncertainty about X that sort of, we come into without having learned anything.

Okay.

Then we observe Y and we compute a new uncertainty about X and that is our posterior uncertainty.

about x after learning y. And thinking about this in terms of priors and posteriors is really helpful, because this is just a particular case of a prior-posterior relationship.

You could sort of heuristically propose a more general definition of information that's just the difference between a prior uncertainty and a posterior uncertainty, and how you sort of formally specify the prior and the posterior is

sort of largely up to you.

And so mutual information is just the special case of this broader definition where the prior and posterior are related by these sort of joint marginal relationships.

But again, it's just a special case of this much larger definition.

And we're going to come back to this in a moment.

But continuing on sort of our information theory background, you can do the multivariate mutual information if you want.

So here now we have two sources, X1 and X2, that are together disclosing information about Y, right?

And again, you have our prior, which is just our uncertainty about Y, and then we're going to subtract off the uncertainty about Y that remains after learning X1 and X2.

Okay?

So far, so good.

Any questions, Dan, in the chat?

People hanging on for dear life?

Good.

All right.

And so when you start looking at multivariate mutual information, things can get really interesting really fast.

And so the first thing is that this mutual information, the information that x1 and x2 disclose about y is

not trivially reducible to the sum of the mutual information.

The mutual information that x1 and x2 disclose about y is not generally, let's say, equal to the information that x1 discloses about y plus the information that x2 discloses about y. The whole, in this case x1 and x2, is not trivially reducible to the sum of the parts.

OK?

And you can have both cases.

If the joint mutual information is less than the sum of the marginal mutual information, then what that tells us is that there is information about y that is duplicated over x1 and x2.

And so when you sum the two together independently, you are double counting that information.

And so this is what we call redundancy, right?

It's the information about why that is copied over both of the inputs and gets double counted when you add them together.

Arguably, the more interesting case is the opposite, right?

The case where the whole, the information that X1 and X2 disclose about Y jointly is greater than the sum of the two marginal mutual informations, okay?

And if you're familiar with, ooh, that should be Y, not Z, I apologize, that's a typo.

And so if you're familiar with integrated information theory, some of the stuff that Giulio Tononi has been doing, you might recognize this as something like integrated information.

It's information that is in the whole as the joint state of x1 and x2 that is not present in the sum of the parts.

And so people call that, like I said, integrated information.

We're going to call it here synergy.

All right?

And so we have these two sort of interesting things that are happening.

We've got a three-way interaction between x1, x2, and y, and it can have two different flavors.

It's redundancy, which is duplicated information, and it's synergy, which is information that is only accessible in the whole and not any of the parts.


UNKNOWN:
OK.


SPEAKER_01:
And so if we take these two ideas, we can make them a little bit more formal and a little bit more rigorous by noticing that we can talk about them in terms of logical connectives.

So redundancy, I like to think of as that information that could be learned by observing x1 or x2 or x3 or as many x's as you have in your data set.

And so, for instance, if X1 and X2 are just linked by a copy gate, X2 is just a copy of X1, then you could look at either one on its own and get all of the information because they are just copies of each other.

In contrast, synergy is information that can only be learned by observing X1 and X2 and X3 and yada, yada, yada.

And this link between logical connectives and higher order information lets us do really interesting things.

We can construct very exotic combinations of information.

And this will come up more when we talk about the information decomposition.

But you could ask, for instance, OK, what information could only be learned by observing x1 and x2

or x3 and x4.

And so we're starting to see what the structure of multivariate information might look like.

It's going to be these chains of atomic subsets of the collection of inputs joined by logical connected ands and ors.

Any questions about this?

Because this link between redundancy and synergy and ors and ands is very helpful for thinking about what's to come.

So far, so good.

OK, cool.

So this brings us to the partial information decomposition.

And so the idea is that we would like to take this joint mutual information, the information that x1 and x2 disclose about y,

and these atomic components that resolves this ambiguity about the relationship between the parts and the wholes, right?

Remember we said that the sum of the two marginal mutual information can be greater than or less than the joint mutual information, right?

So how can we extract a relationship that sort of brings all of these into harmony?

And the way we're going to do this is with this branch of modern information theory called the PID, Partial Information Decomposition.

What it does is it takes the mutual information that x1 and x2 disclose at y, and it breaks it down into the redundancy.

So this would be the information about y that could be learned by observing x1 alone or x2 alone.

plus the synergy.

So this would be the information about y that can only be learned when x1 and x2 are known together.

And then these two unique terms, which is the information about y that can only be learned when observing x1 or only learned when observing

x2.

And then to force harmony between the joint and the marginal mutual information, we will also say that the marginal mutual information should be decomposed in the same way.

So even though the information that x1 discloses about y makes no obvious reference to x2, there is still that redundant information about y that could be learned by observing x1 or x2 plus the unique information from x1 and likewise for x2.

And I'm sure many of you have seen this kind of classic Venn diagram.

This goes back to the original Williams and Beer paper.

But I find that it's really helpful to sort of build the intuition here.

So the big oval is the joint mutual information.

The two circles are the marginal mutual informations.

And then so we have the redundancy is the overlap between the two marginal mutual informations.

The unique information is the difference between the marginal mutual informations and the redundancy.

And then this purple synergy is the sort of extra special sauce that you get in the whole mutual information that is not present in either of the two marginal mutual informations.

All right?

And so this is a very restrictive case.

We just have two inputs and one target y. But I find it's really helpful for getting a handle on the underlying logic of this thing.

But of course, we typically are not looking at systems that just have two inputs and one target.

we often have many, many systems.

And so there is a general case for the PID.

And you can take, for an arbitrary number of inputs, x1 to xk, all disclosing information about y, it can always be decomposed into a finite set of atoms that are structured into what we call a partial information lattice.

This is also an anti-chain lattice for those of you who are into order theory.

But again, over here we have the two element case.

So this is x1 and x2 predicting information about y. And here we can see this has the same structure as this Venn diagram.

We have the redundancy down here at the middle, which is subsumed by the unique information.

And then we have the synergy up at the top.

And as the number of sources of information grows, the lattice grows super exponentially, and you start to get more complicated information theoretic relationships.

So this little guy that I'm circling right here is the information about y that could be learned by observing x2 or the joint state of x1 and x3.

I'm not going to

spend too much time dwelling on these more complicated partial information atoms.

But the idea here is that for an arbitrary number of sources, you can decompose the information using this very elegant lattice structure.

And so formally, if we were going to try to do this all out of notation, what we would say is that we have a decomposition that takes the mutual information, that x1 all the way up to xk just closes about y, and decomposes it into a sum of all of these partial information atoms.

And so MathCalL here is our calligraphic L here is the lattice.

Each of these alpha sub i's is a vertex on the lattice.

And then we're computing the partial information that that vertex or that atom discloses about y. And so again, you have a partial information atom like this.

This is the information about y that could be learned by observing x1 and x2 and x3 or x4 and x5 or x6.

OK, there's a certain amount of sort of detail that I'm kind of eliding here, like how you actually compute the value of these partial information atoms.

I can talk about that at the end.

But for our purposes, you don't really need to know that.

It's just sort of sufficient to know that this decomposition exists and has this form that is connected by logical ands and ors.

OK?

The PID is very powerful.

It's very cool.

I'm a big fan of it.

But it has a pretty severe limitation that it requires distinguishing between source elements, that would be all of these guys, and a single target element.

And complex systems aren't typically that nicely organized.

There are many cases where it doesn't make sense to say, OK, what are our sources and what are our targets?

We would like to be able to sort of decompose the structure of a system qua itself.

And we can do that by playing a really fun little mathematical trick, which is for a given multivariate system, which I'll denote as bold x, which is just x1 through xn,

What we're going to do is we're going to decompose the mutual information that all of the parts, the individual xi's disclose about the whole.

So here, our sources of information are the parts, all of the individual variables.

And the target is not some other variable that's exogenous to them.

It's actually just their own joint state.

And so we know from this basic identity that the mutual information that all of the parts disclose about the whole is just the entropy of the whole.

And so when we do the PID on this particular mutual information, we know that all of the atoms have to sum to the entropy.

And so this becomes the partial entropy decomposition.

This is the information that all of the parts disclose about the whole of which they are a part, essentially.

OK.

And I spent a lot of time working on this partial attribute decomposition.

Shout out to Robin Insie and Connor Finn and Joe Lisier who were working on this a little bit before me as well and laid down a lot of the mathematical sort of fundamentals about this.

And for a long time, I was sort of satisfied with the partial entropy decomposition because it seems to provide this decomposition of the system itself.

But it still has now a new limitation.

It gets rid of the source target distinction.

But the problem is that entropy is no longer really a measure of information in the way that we introduced it before.

It's more of this measure of uncertainty about the state of the whole.

It's not really a reduction in uncertainty.

And so, for instance, you can see that entropy is maximal if all of the elements are independent of each other, but it doesn't really fit our intuitions about information.

If entropy is maximal, we have minimal information.

So it seems kind of odd that

our partial entropy decomposition.

It's not odd, I should say.

It makes perfect sense that the partial entropy decomposition is maximized in the case of total independence, but that's not really information as we typically would like it to be.

And so

To try to resolve this problem and really get a general decomposition of multivariate information, I turn here to the Kolbach-Liebler divergence.

And so the Kolbach-Liebler divergence is a nice target-free definition of information, and it's actually sort of a generalization of the classic Shannon mutual information in its own right.

The divergence of p, p of x from q of x is the amount of information that you gain when you update your beliefs about system x from some prior distribution q of x to a posterior distribution p of x. And so now we're starting to think about priors and posteriors again, coming back to this initial definition of information that I introduced earlier.

In the particular case where q of x is the product of the marginals and p of x is the joint state, the Kolbach-Liebler divergence just reduces again back to the classic Shannon mutual information.

But again, we can plug in any prior and any posterior, and we still have a meaningful definition of information.

So my goal then was to say, OK, can I take the particle entropy decomposition, which

almost gets us to where I want it to be.

And can I somehow remix it so that it, instead of decomposing the entropy, decomposes the Kolbach-Liebler divergence instead?

And spoilers, yes, we actually can do that.

It's not that hard.

And so the basic idea comes down to this very simple algebraic manipulation of the Kolbach-Liebler divergence.

So up here, we have it written in terms of something that looks a lot like Shannon's entropy.

You have the sum over all of the states and the support sets times the probability of that state.

Then we have this log ratio, p of x over q of x.

this is an expected value computed with respect to p of x. So I'm just going to take the summation and the p of x, and I'm just going to shift it out into the expected value operator and not think about it again.

So that leaves us with the expected value of this log ratio.

By the rules of logs, you can split it apart into a difference of two logs.

And then these two things look a lot like the Shannon information content, the local entropy.

And so we can take these two things, and we can actually rewrite them in terms of two different local entropies.

So we ultimately have the difference between the Shannon information of surprise of seeing this particular state, little x,

computed with respect to the probability distribution q minus the surprise at seeing this little state of x computed with respect to the probability distribution p. And so if we go back to thinking about our priors and our posteriors, this looks very, very similar to the heuristic definition of information that I gave earlier.

We have the difference between a prior uncertainty, although in this case it's a local uncertainty or a local surprise,

And then this posterior surprise attributable to each local configuration of x as well.

So again, this matches all of our definitions about our intuitions, I should say, about information.

And we can see that it falls out of the Kolbach-Liebler divergence very nicely.

Okay?

And this is really, this is sort of, this is the most, the crux of the whole sort of derivation that I'm doing here, right?

Is that you can recognize that the cold block Liebler divergence is the expected value of the difference between two local entropies, which is, you know, I've never seen it written out this way, although this is, I don't think this should be surprising to anybody who's familiar with this.

You know, it's not groundbreaking discovery or anything.

But now we can actually bring in the partial entropy decomposition that I introduced earlier.

Because this thing, this local entropy, well, we can write this out as a local mutual information.

But now we're just looking at specific states as opposed to average states.

But the logic is exactly the same.

We decompose the surprise that all the parts disclose about the whole, and we end up with a localized partial entropy decomposition that we can apply to every realization that our system x can adopt.

And so this gets us to the Kolbach-Liebler divergence decomposition that I wanted.

So we start with the definition.

It's the expected value of this difference between two local entropies.

And then we can just use our local partial entropy decomposition to decompose each one of these into its component atomic

bits, essentially.

And so we end up for a given atomic information or information atom, the partial divergence, as we go to p from q, is just equal to the expected value of the partial entropy atoms, the local partial entropy atoms, from the prior and the posterior.

All right?

And because this inherits a lot of the nice properties of the partial entropy decomposition, we can then just get the original Koldach-Liebler divergence back by summing all of these partial Koldach-Liebler divergences over all of the atoms in the lattice.

And so I realize that there has been a lot of notation here, a lot of partials and superscripts and subscripts.

But really what we're doing is we're just computing two different partial entropy decomposition lattices.

So this is the prior lattice and the posterior lattice.

And then we're just subtracting them element-wise.

That's really what this works out to be.

And you just do this for every possible state that x can adopt and then average over all of the states, essentially.

OK?

But I like to think of it literally just in terms of subtracting one of these lattices from another.

I find that it's much more sort of visually intuitive than all of the symbology and the mathematical notation and everything.

And so that is the generalized information decomposition, right?

It's really just the difference between two localized partial entropy decompositions.

And so before we sort of jump into the fun applications, are there any questions about, you know, what I've worked through everybody again, hanging on for dear life?


SPEAKER_00:
Yeah, that's awesome.

I definitely will have questions, and I'm sure people in the chat will too.

But let's hear about the generalized information decomposition that we can get to it.

Thank you.


SPEAKER_01:
Okay, cool.

And so the really nice thing about this generalized information decomposition is that any information theoretic measure that can be written in terms of a Kolbach-Liegler divergence can now be decomposed, right?

You know, beforehand, we were kind of stuck

with various special cases.

The PID works for multiple sources disclosing information about one target.

The partial entropy decomposition worked for the entropy.

But now with the GID, any measure that can be written as a Kolbach-Wiebler divergence is now fair game.

So for instance, I just picked the total correlation as an example.

It's one of the multivariate generalizations of the mutual information.

And it just generalizes this idea that we're looking at the divergence of the true joint statistics from the product of all the marginals.

And this is very nice.

It's 0 if all of the elements are independent.

And in contrast, it's maximal if all the elements are copies of each other, essentially.

And so that makes it very nice.

generalization of mutual information.

But it doesn't really distinguish between lower order and higher order deviations from independence.

It doesn't see any kind of distinction between redundancy and synergy that I introduced earlier.

But by taking the total correlation and plugging it into the generalized information decomposition, we can start to tease out the lower order and the higher order interactions.

So for instance, just as an example, I use the logical exclusive OR gauge.

This is a classic example of synergy in discrete systems.

The information that any xi discloses about y is just 0 bit.

But then the information about y that's disclosed by x1 and x2 is 1 bit.

So this is pure synergy.

All of the information about y is in the whole and not in any of the parts.

And so if we take this and we plug it into the generalized information decomposition, you'll get a lot of zeros.

But I just want to bring your attention to this last column over here.

So this is the total correlation.

And all of the information here is in the very tippy top of the lattice, the highest order synergy term, and not anywhere lower.

OK.

And so a big table full of zeros is probably not that exciting.

But it does a really good job, I think, of illustrating that this GID is doing what we want it to.

This is a very good sanity check that, yes, the logical XOR gate is purely synergistic.

And we get that back from our generalized information decomposition.

And I should note that I'm using the H min redundancy function from Connor Finn and Joe Lisier.

If that doesn't mean anything to you, that's fine.

But it helps describe values to all of these.

Similarly, if we have some measure that is built on multiple Kohlbach-Liebler divergences, you can also decompose that as well.

So here I looked at the pretty well-known Tononi-Sporns-Edelman complexity.

And I won't go into all of the details, but the idea is that it's this measure of complexity that was designed to be low, both in the case where everything is just random.

So if every xi is independent of every other xj, that's kind of like an ideal gas.

There's no structure there.

So TSC should be low.

But also, when every Xi is just a copy of Xj, well, then the system is also not complex, but it's crystallized.

It's highly synchronized, highly redundant, very boring.

And so the TSE complexity is high in this interstitial zone where integration and segregation coexist.

And that matches a lot of our intuitions about complexity.

And if so, if we take the GID and apply it to each of these total correlations and then add and subtract the sums and differences between these two, what we end up with is this really nice decomposition where we can see that the redundancy, so these are total correlation atoms, but I've just removed the total correlation notation just to make this visually more accessible.

we can see that it really penalizes redundancy.

And the more redundant you are, the stronger the penalty for being redundant is.

And then as you climb up the lattice, so here are the redundancies at the bottom of the lattice, as you start to climb up, eventually you hit a point of increasing synergy

And then suddenly it starts to reward that synergy.

And so we have this very nice transition as you go from low on the lattice to high on the lattice, going from penalty to reward.

And so I think that this shows us something really interesting about how the Tononi-Sporins-Edelman complexity relates this idea of complexity and integration segregation balance to this idea of synergy.

The TSC complexity was developed

two decades before we had a really rigorous understanding of synergy.

It was not designed to be a measure of synergy.

It was designed to be a measure of complexity.

And then it's only now, much later, that we learn that there seems to be something very deep linking this idea of balanced integration segregation to synergy.

And so again, if you're not super up on the TSE complexity, that's not necessarily the main takeaway here.

What I'm really trying to demonstrate is that the GID can be used to give us insights into other measures, that it can help us understand what are these measures telling us in information theoretic terms.

And there are a ton of other measures that we could be using here.

and that we could also decompose in the same way and get sort of similar insights.

That's something that I would be really interested in doing sort of in the future.

Finally, the last thing that I wanted to show was that we could recover the single target partial information decomposition from the GID, right?

If the GID is a true generalization of the PID, we should be able to recover the PID as a special case, right?

We started with the PID, we turned it into the partial entropy decomposition by doing this weird sort of

where we made the target the joint state of all the parts.

And then from that, we built the GID.

Can we close the loop and go all the way back to the PID?

And so just to remind you, the PID takes the information that x1 and x2 disclose about y and breaks it down into redundant, unique, and synergistic components, and then does the same thing for the two marginal mutual informations.

Can we recover this from the GID?

And it turns out that, yes, you can.

We can write this joint mutual information in terms of an expected value of Kolbach-Liebler divergences.

And so our prior is just the statistics of x1 and x2 together.

And then our posterior is the statistics of x1 and x2 after conditioning on y being in some particular state.

And then so we compute the expected value of this thing

over all possible states that y can adopt.

And so this just works out to computing a bunch of lattices, essentially, two element lattices, because we have two variables, x1 and x2.

And then we just average.

So if we were to take the expected value of all of the top lattice synergy terms, we would get the PID synergy back.

If we were to take the average or the expected value over all of the redundancies down here at the bottom, we would get the PID redundancy back.

And this does recapture the partial information decomposition as we would expect.

If you use the Hmin redundancy function for your

generalized information decomposition, you get the PID back that uses the I min redundancy function for the partial information decomposition.

So we can see very clearly how the entropy terms turn into the information terms.

And that all sort of looks well and good.

And I was pretty happy to be putting a bow on this project until I realized that there is another way that you can write the joint mutual information out as a Kolbach-Wiebler divergence.

And that's this way.

So we can also write the same thing out as the Kolbach-Liebler divergence from our prior, which is the product of the probability of x1 and x2 times the probability of y. And then the posterior is our joint state.

OK, but this requires decomposing a three-dimensional probability distribution px, probability of x1, x2, and y. Whereas before, we only ever were decomposing two variable probability distributions.

OK, so this is why, in this case, we had a nice four-element lattice.

Here, we're actually decomposing a three-dimensional probability distribution.

And so the resulting lattice will have 18 partial information atoms rather than four.

So we're going to end up with this decomposition of the joint mutual information as opposed to the expected four-element decomposition.

And I spent a long time trying to

know, find some way that I could squish this 18 element lattice back down into the four element lattices that I sort of, you know, expected to get.

And I was not successful.

The people who peer reviewed the paper were also, you know, didn't have any suggestions either.

So I won't say that it can't be done.

But if there is a way to do it, it is currently beyond me.

And so this

sort of puts us in a little bit of a pickle right like we would expect the decomposition we would expect a unique decomp we would i don't want to maybe not say expect we would hope that there would be a unique decomposition for a single um mutual information right uh it's kind of odd that you can take one mutual information and depending on how you like choose to

denote it, how you choose to write out the Kolbach-Liebler divergence, you could end up with two sort of non-identical, non-interconvertible decompositions.

So what's happening here?

This was very odd to me.

And so my working theory, and this is just a theory.

If any other mathematicians are in the audience and want to take a stab at it, you're more than welcome to.

I think what's actually happening is that these two ways of writing out the joint mutual information are actually sort of fundamentally different.

In the first case, we're looking at a three-way interaction.

We're looking at the case of the interaction between x1 and x2 and y.

Whereas in the second case, we are sort of abstracting y out or sort of removing it from the divergence and sort of wrapping it all in this expectation value of y.

OK?

And so what you're doing here is you're actually like these are two different kinds of dependencies, right?

They're mathematically interconvertible, and they'll always resolve to the same value when you plug in numbers.

But they are in some fundamental way different kinds of structure in the system.

Right?

And so the classic Shannon information theory can't see this difference.

It just says, oh, these two things can be algebraically turned into each other, and therefore they must be the same.

But actually, under the hood,

They are two different ways of looking at this interaction between x1, x2, and y. And so again, classic Shannon information theory can't see this difference, but the information decomposition can.

And I sort of should note that this is not the first time.

This is sort of the first time

turned up.

There are a couple of other cases where classic Shannon information theory has been unable to see a distinction between two categories of object.

But when you start looking at the information decomposition, suddenly you can.

I'm sure there's a lot more to say about this.

And I'd be interested in talking to people about it after the fact.

But I'll just leave you with this question now of non-identical decompositions of the same value.

And let's see, I think I'm coming to the end of my time here.

So I want to talk just a little bit about sort of possible applications and some future work that I would like to do.

And I'd be happy to collaborate with with other people with as well.

And so

I think that this is a big step towards what I call a grand unified theory of multivariate information.

We have this interesting case where we started with the PID.

And then from the PID, we can construct the partial entropy decomposition.

The PED is a special case of the PID.

And then from the PED, the partial entropy decomposition, we can construct the generalized information decomposition.

And then we can take the generalized information decomposition and we can re-extract the partial information decomposition from it with sort of these weird caveats.

And so this kind of Ouroboros sort of thing where we have all of these different information decompositions that are kind of fundamentally built out of each other, I think is really

getting at something fundamental with how we can think about sort of these part polar relationships in multivariate multivariate systems.

The last thing that I sort of haven't been able to do is incorporate the integrated information decomposition.

And so this was proposed by Pedro Mediano and Fernando Rosas and the other people at

I'm not sure where everybody is now.

But they provided a very, very elegant generalization of the PID that allows you to have multiple sources and multiple targets.

And so I would really like to be able to find a way to incorporate that into this PID, PED generalization.

GID framework for sort of a truly brand unified theory of multivariate information decomposition.

Still working on that, open to collaborating with anybody who's also interested in taking that project on.

And then finally, so let's talk about predictive coding because this is an active inference stream.

And so I assume everybody is sort of familiar with the basic idea of predictive coding, that the brain is sort of acting as a kind of Bayesian inference engine.

It has a world model.

It has some set of beliefs about the world.

And it updates those beliefs as it navigates through it and receives sensory stimuli.

And so one of the things that I'm really interested in is, how might this predictive information about the environment be redundantly or synergistically distributed over different sensory channels?

We're not just learning.

We don't just have vision.

We have vision.

We have hearing.

We have touch.

We have proprioception.

There are other animals that have senses that are totally alien to us.

And so when we take in all of that sensory information and build a world model,

we can't just treat every one of these incoming information streams as independent of everyone else.

We have to learn these complex higher order interactions between different incoming streams of sensory information, and we use that to build a really rich world model.

And so a great example that I first heard from Andrea Lupi, who is a friend and sort of collaborator of mine,

He was talking to me about stereoscopic depth perception, where this is an example of synergy between two incoming sensory channels.

If you cover one eye, you lose the ability to perceive depth.

So there's redundant information that

both eyes get simultaneously.

There's unique information that is sort of at the corners of your visual field.

And then finally, you have synergy, which is this sort of emergent perception of depth, this qualia of receding distance that you only get when you're getting information from the right eye and the left eye simultaneously.

So from sort of a cognitive modeling standpoint,

our ability to extract sort of higher order information from multiple incoming sensory channels is clearly very relevant to our perception and our behavior in the world.

And so I'm tempted to propose a hypothesis of maximum synergy.

So if there are two different channels that are totally redundant, an agent that only has some minimal number of calories that it can burn keeping itself alive isn't going to want to spend calories perceiving both

both senses independently.

All the information you could get from one, you can get from the other.

So why spend money on both?

But in contrast, if there's synergistic information in x1 and x2 that you can only get when both are present, well, then modeling or having channels to

you know, sense all of them becomes essential, right?

And if it's truly synergistic, then, you know, having just X1 on its own or just X2 on its own may not be, you know, giving you any information.

You know, it's, you know, if there's synergy in the environment, then you need to, you know, spend calories maintaining both passage of both channels.

And so, you know, my guess is that there's probably some kind of evolutionary pressure that says, you know, agents with limited resources and complex environments probably want to maximize the number of synergies that they are sensitive to when building their world model while simultaneously minimizing the redundancies, right?

Because the redundancies are basically just wasted calories while the synergies are potentially very informative.

higher order interactions that are well worth the calories.

And so this is about as far as I've gotten on this hypothesis, but I think it would be something really interesting to explore down the line, maybe either in very simple animal models or in some kind of in silico minimally conscious, not minimally conscious, minimally cognitive agent kind of approach.

And again, that's something that I would love to talk to really anybody else about.

And then so that's sort of the end of my spiel here.

This is the paper.

It was published in PLOS One a couple of months ago.

Here is the link, and I'll send around the slides afterwards if anybody wants something to Dan, hopefully.

And yeah, so that's it.

This is work that I sort of largely did on my own, so I don't have the usual sort of acknowledgement slides, but I'll just say let's jump right to questions.

And I'd love to get feedback from anybody else on the work, what you might think about it or what your own thoughts might be.


SPEAKER_00:
Awesome.

Thank you for the very, very clear, great presentation.

lot of places to jump in could you speak a little bit about redundancy and synergy with respect to fragility and other system properties because having redundancy although like as you brought up in those last slides it might cost more energy and so in the short term it might appear like well why do we need to have redundant information

When there are like component failures or other kinds of perturbations to the system, what's redundant in one moment might come to be not redundant.

So how do you think about these kind of informational properties of systems or networks with respect to other systems properties?


SPEAKER_01:
So that's a great question.

And I'd be curious who asked it, because I actually just put up a preprint about exactly that topic.

So I don't know if they're aware of it.

So yeah, that's a really good question.

Redundancy and synergy, how do they relate to things like fragility and robustness?

I don't have slides for this, but I can give you just sort of the verbal TLDR.

I just did this project where we took very simple Boolean networks, and I evolved them either to be very redundant or very synergistic.

And then since Boolean networks, there's a lot of ways to characterize their dynamics, I asked, what are the dynamics that come along with evolution for redundancy or evolution for synergy?

And what I found was that the redundant Boolean networks were extremely stable.

You could perturb them, and they would almost always fall back to one of a very small number of attractors.

They had very restricted state spaces.

They were kind of crystallized.

In contrast, the synergistic networks were basically chaotic.

It was very hard to find ways in which they did not just appear to be chaotic systems.

And I'm using chaos in sort of the technical sense here, sort of sensitive dependence on initial conditions.

Very large state spaces, lots of attractors, small perturbations sent them off onto very different possible futures.

The punchline then was that if we took these two different networks and then asked about their capacity to integrate information, and I used this measure of integrated information from integrated information theory, what I found was that the synergistic networks could integrate lots of information, whereas the redundant networks

were very stable, but could not integrate information at all, right?

So there was sort of this, you know, when you were at the extremes, you could, you know, be very stable and totally unable to integrate information, or you could be hilariously unstable with lots of integrated information, right?

And so

The last thing that I tried then was I actually then evolved networks that had the TSE complexity that I mentioned before, this balance of integration and segregation.

And what I found was actually that the complex networks were able to sort of split this difference.

They were more stable than the purely synergistic networks.

They weren't as chaotic.

But they also had more capacity to integrate information than the crystallized networks.

So we proposed this kind of multi-way trade-off where redundancy brings stability but restricts your ability to integrate information, whereas synergy brings integrative capacity but is destabilizing or fragilizing.

And then you can kind of balance those two things with this idea of complexity that

that brings together integration and segregation in a kind of, you know, sort of modular, a modular structure.

And so that's been sitting under review at Chaos for like the last six months.

I have no idea what they're doing over there, but hopefully, you know, we will

That will be in press before the end of my postdoc, hopefully.

And there's a lot of work that I want to do going on, sort of trying to build on that as well.

So great question.


SPEAKER_00:
That's awesome.

I mean, you've approached from very informational first...

Sorry, you've approached it from very informational first principles and recovered a lot of these topological and dynamical features of complex systems and how they have to trade off against these like multiple properties of systemsness that sometimes might be like directly contradictory.

A question in the live chat, Celeste wrote, how might GID be related with network controllability?


SPEAKER_01:
Oh, that's a really good question.

And I'll be honest, I have no idea.

Network control theory is something that I'm a consumer of network control theory.

I'm not a producer.

So I'll see these papers.

Parker Singleton had a great one looking at network control theory and LSD and psilocybin.

I was like, oh, that's really cool.

But I don't know enough about the theory to really get at it.

My guess, just off the cuff, would be that redundancy would make the network perhaps less controllable because it always wants to fall back into its main attractor, whereas synergy probably lowers the energy needed to push you on to a different part of the configuration space.

But it's

maybe less stable, maybe more fragile.

So I think that this is a great case where some simple toy models, maybe some analysis of fMRI data could go a long way to answering a really interesting question.

But I can't say for certain other than just those very hand-wavy predictions.

But I'd love to see it.

I'd love to see it done.


SPEAKER_00:
Cool.

Yeah, one thing that you brought at the end with the two eyes looking at the object and getting depth...

it really made me think about the multiple scales of redundant and synergistic information.

Like you have two photoreceptors that are right next to each other.

So just from a first pass correlational value, it would seem like they're going to be highly correlated sensors.

I mean, they're in a sensor array, like on the retina.

And yet also the differences between retinal receptors are used to improve like signal to noise characteristics at multiple levels.

steps in the relay of visual processing.

Then you have the two eyes and depth perception.

And then there's also cross modal

synergistic information like maybe if you just saw the video of somebody or only heard what they had to say it might convey certain information and yet putting them together that is very important for understanding like well how do different kinds of multi-modal experiences convey information about

real world things and for cognitive modeling like of biological systems or design of cognitive systems these kinds of redundancies and synergies would basically be ignored at one's peril because you can always describe kind of the syntactic flow of information and say like this is how much data at each moment we're pushing through this connectivity but that might be

misleading or fragile or over-designed or under-designed with respect to relatively obvious intuitive ways in which we fuse streams of information to reduce our action-oriented uncertainty about the world.


SPEAKER_01:
Yeah, the idea that at different scales you could have different sort of balances of redundancy and synergy is, I think, there's a whole universe of interesting work that could be done there.

And my guess is that you can have somewhere there is a system that has every combination of

micro scale macro scale redundancies and synergies um and sort of like different contexts in which you know that might make those would be useful i think would be really cool to think about another kind of connection is those lattices um


SPEAKER_00:
even just from the two to the three, we could see that kind of explosion of ways to parse the information.

And then it made me think about where is time and the foraging or the attentional path of a given cognitive entity, because while you're able to represent all the possible partitions and like you as a mathematician can pick up that and handle it,

we might, from an attentional networks in the brain perspective, we might ask, well, which node on this lattice should we be looking at?

How should we blend the indicators from person one and two to the exclusion of three, or two and three but not one?

Those are different attentional modes you can have.

and you could have a uniform prior across that lattice and just say, well, we're gonna blur across all of them.

But then once you start saying, oh, but I like this one and this one's irrelevant and these two are the same, then you're getting into the business of reshaping the portfolio across the lattice.

And then that,

How do we deal with that when outside of dealing with these systems purely symbolically, if we actually want to be using certain partitions and identifying useful decomposed components, like how do you think about that?


SPEAKER_01:
Well, so, I mean, first thing is, you know, when you mentioned time, I again want to shout out Pedro and Fernando and Andrea's work on integrated information decomposition, because they really, you know, I introduced it as this like multiple sources, multiple targets, but they have this incredibly cool application of that where they say, okay, well, all the sources are the states of the elements at time t, and all of the targets are the states of the elements at time t plus one, right?

So now you're decomposing the information that

sort of the past discloses about the future.

So you can ask, you know, what redundant information in the past, you know, discloses information about the synergies, you know, in the future, and you know, what information in the synergies in the past stays synergistic in the future.

And so, you know, there's a whole sort of universe of temporal information dynamics, you know, that can be brought to bear here that I just, you know, didn't really have time to talk about.

On the topic of which atoms do we care about, one of the things that I think a lot about is these lattices explode super exponentially.

We don't know how big the lattice for a 10-element system is because it's the 10th Didi kind number, and that's so large that nobody has ever computed it.

So finding ways to kind of

know figure out you know how will we get sort of the information how do we make this information accessible to us as modelers you know is a like sort of sort of an outstanding problem as well um

is kind of getting what you're saying.

And then the other thing is that as the lattice gets bigger, the particular atoms become hilariously arcane.

It's like, OK, well, what information could be learned by observing x1 and x2 and x9 and x10 or x1 and x2 and x3 and x9 or x1 and x2 and x7?

You end up with these incredibly long chains of logical dependencies.

that my guess is almost certainly don't mean anything.

And so again, figuring out some way to extract or decide what are the meaningful dependencies, not just what are all of the dependencies.

As a mathematician, I like completeness.

I like having the whole pie.

But again, outside of that, the question of how do we figure out what information is relevant to us

That I think is a standing problem.

I think there's a lot of space there for creative attacks on the question because there isn't... I've taken my own swing at it, other people have as well.

I think, again, there's a lot of work still to be done in that space.

And it's very exciting to see what people will come up with in the coming next five, 10 years.


SPEAKER_00:
Yeah, a few thoughts on that.

First, I think what you said there about even with 10 retinal receptors or like an insect eye with multiple ommatidia, like once you get to 10, enumerating the lattice is not even finitely accomplishable.

And I think that in a way must speak to the relevance of coarse graining and nesting of models because you couldn't have...

10 people in a room and all those pairwise combinations happen.

So like there has to be some clustering and nesting.

And then also that what you said about the specific atoms being like hilariously arcane reminded me of random forest modeling.

And you also had the kind of logical framework.

So then it's sort of like, tell me,

either your favorite cereal and what color of this car you would want or this and that and like the concept is yeah those are drawn from a massive state space of possible thresholds or or choices yet through statistics and iteration some of those thresholds may identify kind of critical

decision points within an empirical state space, which is probably gonna be existing on like manifolds or subspaces that don't require the full unpacking of the lattice, but there might be whole like territories of the lattice that you can just average over.

But the question is how nicely are those distributed

on the whole lattice if those are distributed randomly then there may not be many ways to work with it but if those are all on one side then a random forest with a simple first question might like bring one into a useful area yeah that's a really interesting idea i never thought about the link to random forest before but i'm gonna write that down and um you know take a stab at it i like that a lot cool yeah um

I guess one of the interesting questions too is information for whom and about what?

Like in the math, that's kind of generalized over, appropriately so.

but then when we're constructing active inference models and we're actually using the KL divergence like in the variational free energy where the KL divergence is used like in the complexity minus accuracy way to talk about it or when we're using KL divergence in the expected free energy prospective setting

thinking about like the divergence between what we prefer for observations and for how we think different kinds of courses of action are going to play out looking at those divergences it does get very specific what we're talking about the reduction with respect to so how how do you bring something that's kind of generalized across

observers and types of variables and bring that from being kind of an analytical result into being something like the air conditioner is reducing the KL divergence about preferred temperatures with respect to courses of air conditioning.

What happens between having this math and being able to use it in those settings?


SPEAKER_01:
So that's a really good question.

And again, coming from a math perspective, I'm always gunning for full generality.

The less useful this is, the happier I am.

But the GID really is just a decomposition of the KL divergence.

All of the general stuff.

is kind of, you know, it's sort of an added bonus.

So, you know, you could have a KL divergence where, you know, your prior and your posterior are very well defined, right?

And you're like, I know exactly what my prior is.

I know exactly what my posterior is.

I know exactly how to interpret it.

And then you can just go ahead and, you know, crank the GID handle and, you know, you'll get the decomposition back out.

So I think, you know, the

Again, I approach it from a general perspective.

But if you want to take it and make it specific, there's no, I think, fundamental mathematical problem with that.

It's really just a question of to what extent can you specify exactly what your priors and posteriors are, and how do you then interpret the resulting numbers?

Because the GID itself doesn't care about the interpretation.

It just says like, we have one distribution, we have another distribution and like they have this structure that relates them.

But you know, if you, you know, have your interpretation and your, you know, sort of, let's say maybe domain knowledge that sort of specifies everything, then yeah, I think that that's, you know, that the generality is not necessarily a problem, you know?

how would you connect this to the concept of like known unknowns and unknown unknowns oh boy um so the problem in information theory is it it has it it has a very hard time with unknown unknowns right like it kind of assumes that you have the distributions you know that you have all of the the distribute like how do i how do i say this this is a little bit um

getting all tongue-tied here.

So from the perspective of the KL divergence, all it sees is what's in the distribution.

And so you plug the distributions in, you turn the math crank, and you get numbers out.

There's not an obvious or real way, I think, to account for the possibility that there is

sort of meta uncertainty.

You can be uncertain about what's the probability of x1?

What's the probability of x2?

How certain are you about these certainties?

That's something that as far as I know, the sort of vanilla information theory that I'm familiar with really quite struggles with.

interested in climate science and climate change kind of stuff.

And I keep running into this idea of deep uncertainty.

What's the probability that your probability distribution is actually the right one?

And there's, I think, some very cool work happening in that space, but it's a step or two beyond what I think can be tackled with what I've presented here.

So I guess I don't know.

I think that that's something that we've got to work on.


SPEAKER_00:
Yeah, that's super interesting.

It reminds me even back in learning about like the mean and the variance of a distribution, it's like, okay, so we've parameterized our uncertainty about the mean and we called that the variance, but then how certain are we or how do we get a p-value on the uncertainty?

And then in the hierarchical Bayesian modeling approach, you can just keep on going, but the buck has to stop somewhere

And then you can go to kind of another set of methods and pick different prior distributions for that highest possible prior or take other approaches.

But that's like a very kind of composite approach that is not necessarily returning back down to the simplicity of the initial question.

You're just getting into like higher orders.

Like what is the probability that there was another element that we were missing?

But that could increase the complexity of the problem so much that even a simple question would be kind of like unnecessarily exploded because of even a very small probability of something happening that changes the state space of the initial question very greatly.

And then that would kind of like potentially forego a lot of low-hanging fruit to kind of cover the bases against some rare events, which still might not even be detected because you would have just basically turned some kind of unknown unknown into a known unknown, but at expensive cost and making the problem quite different.

So it's kind of like, is that something that can be resolved in an airtight way?

Or is that kind of like the openness that just requires iterated engagements?


SPEAKER_01:
Yeah.

I mean, my, my, my sort of, my intuition is always, you know, sort of keep things as simple as possible, you know, but not too simple like that, that the quote to quote says, you know, I, I, I,

In the paper, I write a lot about like all of the contexts in which you shouldn't use the GID, right?

That was like something that I spent a lot of time on.

And so, you know, the Kolbach-Liebler divergence does have sort of these odd limitations where, for instance, you know, anywhere that, you know, Q of X is zero, you know, where, you know, you have a

you know, something happens in the posterior that you didn't think could happen the prior, like the whole thing blows up.

Right.

And so I do think that if you wanted to use this in an analytical context, you know, practical context, not just a theoretical one, you know,

would require a lot of thought about, is this actually the right pool for the question that I want to answer?

And it's entirely possible that no, it's not.

The GID or the PID is not the right pool for a system where you have changing support sets or unknown unknowns.

And that's fine.

There's plenty of mathematical toys in the box for us to play with.

And so I guess my response to that would be, if you find yourself asking those questions, maybe just don't use the GID or the PID.

Maybe there's something else that would be a more appropriate tool for your question.


SPEAKER_00:
That also makes me think about...

the concept of i guess different ways to approach it but like information or or stimuli that have an informational impact information cannot be negative per se yet there are ways to move distributions up and down and then there are ways in which the movement of that distribution up or down would have like an adaptivity benefit or detriment to an organism so

information is just kind of at this level, this is coming in lower in the processing, that it is not necessarily a complete answer to questions about complex systems.

However, it helps ground and anchor some subsequent questions about their higher order

dynamics in a way that skipping directly to inferring higher order dynamics might not be able to realize.


SPEAKER_01:
Yeah, the question of whether

is this something that we use to model these certain kinds of information theoretic dependencies?

Or is there something in the brain that is actually computing something like a little GID as it tries to learn what are the patterns that I need to be sensitive to?

My guess is there's probably nothing like that happening.

but you know yeah the question of how do organisms sort of represent information and represent potentially higher order um interactions you know that's a really interesting question uh I don't know if that's exactly what you were talking about but that's kind of where my mind went uh um well just we'll wait a minute if anyone in the chat has any last questions but I mean


SPEAKER_00:
where and how are you going to take it forward?


SPEAKER_01:
So I'm really interested, you know, I spent most of my PhD, I kind of spent working on like the mathematics of these things.

You know, I wrote about the PED, I wrote about this, you know, I've worked on some other stuff.

I'm really curious about like, where can this be applied to real data?

Right.

And so, you know, I'm working currently at the University of Vermont.

I'm working with Josh Banger and Mike Levin.

on some interesting projects in evolutionary computing and evolutionary biology to see where are these synergies in nature and are they telling us anything about the behavior or structure of organisms.

I'm still interested in neuroscience.

I have a longstanding interest in psychedelic neuroscience.

you know whether there are changes to redundancies and synergies in the brain you know when you're you know on dmt or whatever uh could that help explain why phenomenological consciousness changes that's something i've worked on um before as well and so really trying to find ways to get out of like sort of math theory land and see like does this actually tell us something about nature as opposed to just like the structure of information theory is sort of what i'd like to do

um next well that's also expensive and requires data whereas pure Theory just needs a whiteboard and a a lot of tech compiler so it's a lower overhead just on that kind of like altered states of Consciousness and and um and all of those possibilities it's almost like what


SPEAKER_00:
what decompositions help one like gain knowledge about themselves or about something else and it might be like a decomposition of like this amount of that in this setting or this amount in that setting provide information on a common target distribution and that maps out like an architecture that's not

this part of the brain is connected to that one it's not even necessarily the architecture of kind of like the dynamic causal modeling of the neuroanatomy but it really would be toward like architectures of beliefs and how they have different equivalences or or relationships


SPEAKER_01:
yeah there's um there's a really interesting uh set of papers by robin carhart harris and carl friston looking at sort of a free energy or a bayesian approach to like psychedelic therapy like how do our beliefs change and how does you know tickling these serotonin receptors

change our beliefs.

And so I think that would be a really interesting place to try to apply something like this if you could get meaningful keys and cues.

If you're looking at something like post-traumatic stress disorder, you're changing your beliefs, not just about the world around you, but also your own interoception.

you know is the capacity to you know heal from trauma or something does that require a higher order a synergistic you know change in sort of the joint states of your beliefs about the world and yourself that is sort of not figuratively reducible to like just tweaking one or tweaking the other right and so you know getting into sort of more of this um

sort of cognitive kind of stuff, I think, would be really interesting.

One thing I didn't mention when we were talking about future stuff is one that I've been working on quite recently is that the PID and the PED, it's all gamed out currently in terms of entropies and mutual informations.

But as long as you have a function, a general function that satisfies certain axioms, non-negativity,

you know monotonicity yada yada yada you can also get a lattice and you can also do like the mobius inversion and get the decomposition so i've been working a lot on you know

taking what I'm calling structure decomposition.

If you have a network and you have a measure like the communicability, which does satisfy all of the desiderata to induce information decomposition, you could do a communicability

decomposition, looking at the influence of edges or nodes to measure the communicability instead of the information.

Or you could look at the shortest path efficiency.

This one also satisfies all of those requirements.

And so can we take these

logical mathematical structures that were developed in the context of information and entropy and apply them to other things.

You know, I'm looking at a network sky by training, so I'm looking at network measures now.

But, you know, could you how far could you push this right?

Could you start looking at maybe psychological constructs?

Could you start looking at, you know, dynamical properties like the I don't know, the that I

having a stroke.

I'm drawing a blank on any good dynamical properties.

But you get the idea of what I'm trying to say.

How far can we push this logical structure to look at higher order interactions in other contexts that are information theoretic?


SPEAKER_00:
yes that just makes me feel like networks give a discretized and topological handle on families of continuous phenomena or the phenomena could be discrete at the node level but we could just be talking about a continuous statistical distribution so that at the node level gives us continuity and connection with empirical data and statistics and then the math picks up

with these discretized lattices and kind of like studying a tree is like studying a tree and then you pull back and you have a model of the forest and then what could you know by studying two trees well they could be giving redundant information they could be giving synergistic information about what and then it's kind of like off one goes on that inquiry


SPEAKER_01:
Yeah, huge potential that I think for a lot of really cool sort of new ways of thinking about complex systems.


SPEAKER_00:
Awesome.

If you have any last comments, feel free to go for it.

Otherwise, this has been epic and very informative.


SPEAKER_01:
Um, you know, all I'll say is, you know, you can find me, uh, you can find me on Twitter, you know, you can, you know, my, you can find me on my, my email.

Um, you know, I'm always open.

I don't know how many people are on the, on the YouTube right now, but I was like, I'm always open to like collaboration, talking to people, you know, like I am, you know, if any of this is interesting to you, shoot me an email, please.

And I would be happy to talk, you know, about any and all of this, like my inbox is open.

So I look forward to hearing from folks.


SPEAKER_00:
Thank you for that.

And thank you again for joining.

So till 10.2.

See you.

Bye.
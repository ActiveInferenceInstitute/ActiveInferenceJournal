[
  {
    "start": 8.083,
    "end": 9.344,
    "text": " Hello and welcome everyone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 10.325,
    "end": 11.325,
    "text": "It is September 1st, 2023.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 11.846,
    "end": 16.729,
    "text": "We're here in Active Inference Math Stream number 6.1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 17.75,
    "end": 18.871,
    "text": "Here with Sean Toll.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 19.451,
    "end": 25.415,
    "text": "We'll be hearing a presentation, Active Inference in String Diagrams, followed by a discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 26.096,
    "end": 32.14,
    "text": "This is super exciting, so if you're watching live, please feel free to write your questions in the live chat.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 33.513,
    "end": 34.553,
    "text": " really looking forward to this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 34.593,
    "end": 38.295,
    "text": "So thank you, Sean, again, for joining and to you for the presentation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 40.516,
    "end": 40.776,
    "text": "All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 40.816,
    "end": 41.276,
    "text": "Thanks very much.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 41.316,
    "end": 42.037,
    "text": "Thanks, everyone who's watching.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 42.057,
    "end": 46.618,
    "text": "And thanks to the organizers for this chance to speak to you and to Daniel for getting in touch and inviting me to speak.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 46.699,
    "end": 55.662,
    "text": "So yeah, I'm really excited to share this work with this community, basically, and to hear from those people who work with Active Inference and do any formal work, what they think of what I'll present today.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 55.682,
    "end": 57.303,
    "text": "So I'm going to be",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 64.991,
    "end": 79.795,
    "text": " diagrams and it's based on this mathematics called category theory and I won't assume that you're too familiar with this already and try and introduce it to you in the talk and ultimately I'd like to sort of convince you that this diagrammatic language will be really useful for those of you who work formally with active infants and encourage you to pick it up.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 82.045,
    "end": 83.326,
    "text": " I've just introduced myself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 83.366,
    "end": 84.166,
    "text": "I'm Chantal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 84.266,
    "end": 89.468,
    "text": "I'm a researcher at Continuum, formerly a postdoc in computer science in Oxford.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 90.088,
    "end": 98.452,
    "text": "And at Continuum, in this Oxford team where I'm based, we study what we call compositional intelligence, which includes applying category theory to topics in AI.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 100.252,
    "end": 104.154,
    "text": "And as well as this, the project was supported by a grant from FQXI.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 104.594,
    "end": 108.716,
    "text": "It's located at the bottom and hosted at the Topos Institute, which is the center for applied category theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 110.814,
    "end": 111.855,
    "text": " So, let me get started, I think.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 111.895,
    "end": 114.797,
    "text": "So, yeah, here we go.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 115.137,
    "end": 118.44,
    "text": "So, for active inference, I won't spend too much time introducing it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 118.5,
    "end": 122.643,
    "text": "I'll assume most people here are familiar with it, and many of you probably know more about it than I do, in fact.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 124.664,
    "end": 128.327,
    "text": "So, I just mentioned the parts of it that I'll be addressing in the talk.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 128.667,
    "end": 135.653,
    "text": "So, thinking of it as a model of cognition that, hopefully, we can think of as applying at many levels, say, from a whole organism or just to a single neuron.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 136.961,
    "end": 150.131,
    "text": " And the key idea is that in this approach, you think of an agent that's coming with this generative model that it uses to explain the observations it receives from the world in terms of some hidden states, which you might call perception, and in terms of its own actions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 151.132,
    "end": 160.8,
    "text": "And in active inference, it achieves both of these things through this form of Bayesian inference, or an approximate form of Bayesian inference, by minimizing this quantity called free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 161.26,
    "end": 163.622,
    "text": "And these are the ingredients we'll be looking at in the talk.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 163.642,
    "end": 165.824,
    "text": "And the thing that's really exciting about active inference, I think,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 173.93,
    "end": 175.751,
    "text": " that you can hopefully apply at all these many levels.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 176.711,
    "end": 180.033,
    "text": "But I think at the moment, it could also benefit from more formal work.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 181.214,
    "end": 182.194,
    "text": "And that's what this talk's about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 182.254,
    "end": 183.715,
    "text": "It's about formal approaches to the theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 184.375,
    "end": 191.938,
    "text": "In particular, I think a nice clear formalization of what active instance is would help to clarify sort of what the core or the key ideas of the theory are.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 191.978,
    "end": 197.801,
    "text": "So we'd like to be this very succinct principle that ideally we just apply to a generative model and everything else follows from.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 198.902,
    "end": 201.763,
    "text": "And once we've got to this, we can hopefully generalize it and understand it better.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 202.66,
    "end": 212.308,
    "text": " and also make it just more acceptable to those who come from formal backgrounds, like in mathematics and so on, and get them working on this topic very quickly and connect it with approaches in artificial intelligence as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 213.609,
    "end": 221.255,
    "text": "But the most important thing about a good formalization, I think, should just be to make learning about active inference easier, make the framework simpler to understand.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 221.315,
    "end": 223.237,
    "text": "So that's what we're aiming for in this work.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 224.518,
    "end": 225.078,
    "text": "And I'd say in",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 227.964,
    "end": 256.2,
    "text": " other places already there's been some calls that or some suggestions that a nice formalization of active infants should be a diagrammatic one so when you look at the generative models that come up and that's difference a lot they're very compositional in their nature and it's very natural to draw them in diagrams um so the united states like our whole approach to describing it could be graphical in this way so just for example there's this paper called the graphical brain by by kristen and in general you know we've probably seen loads of these diagrams describing generative models where you draw",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 258.916,
    "end": 261.418,
    "text": " spaces of hidden states observations interacting and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 262.398,
    "end": 266.121,
    "text": "So these diagrams are used, but they're just used to represent the model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 266.161,
    "end": 272.164,
    "text": "You still have to then go to doing sort of traditional probability theory calculations when you reason about them normally.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 272.785,
    "end": 281.59,
    "text": "But in fact, there is a whole graphical formalism and mathematical language for describing these kind of interacting processes just entirely with the diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 282.231,
    "end": 285.793,
    "text": "So the area of mathematics is called category theory and the language of these string diagrams I'm going to talk today.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 287.99,
    "end": 297.673,
    "text": " And in particular, there's a lot of work going on in applied category theory now in how you can describe aspects of probability theory and causality and causal models in terms of string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 298.433,
    "end": 303.875,
    "text": "And these causal models are basically based on Bayesian networks, so the same formal structure as the generative models in active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 304.615,
    "end": 313.258,
    "text": "And in particular, what we'll talk about today kind of draws on this paper, co-authored with Robin Lorenz, talking about causal models and the sense of Perl in terms of these string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 313.298,
    "end": 314.819,
    "text": "So it's basically causal Bayesian networks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 317.236,
    "end": 319.917,
    "text": " which has the same formal structure as we'll be talking about today.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 321.057,
    "end": 332.78,
    "text": "So in this talk, I'm talking about this paper, which is joint work with Johannes Kleiner and Tobi Sengpersma, which is called Active Inference in String Diagrams, a Categorical Account for Processing and Free Energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 332.96,
    "end": 339.282,
    "text": "And basically what we do is try to give a formalization of active inference that's nice and clear conceptually, just entirely in terms of string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 339.702,
    "end": 345.144,
    "text": "So we're basically taking the kind of formal content that's in something like the Active Inference book and turning it into these diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 347.595,
    "end": 353.28,
    "text": " And as I mentioned, it was done as part of this project that's actually on a project about consciousness.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 353.36,
    "end": 356.803,
    "text": "It's about ways that category theory can be applied to theories of consciousness.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 357.223,
    "end": 360.706,
    "text": "And we've done some previous work looking at the integrated information theory of consciousness.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 361.366,
    "end": 366.03,
    "text": "And of course, there's all sorts of ways that active inference has been proposed to connect the consciousness.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 366.491,
    "end": 368.092,
    "text": "But for the purpose of this talk, we won't go into any of that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 368.132,
    "end": 370.974,
    "text": "It's just a theory of cognition, I take it, to be here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 371.695,
    "end": 375.178,
    "text": "And I'll just mention at the end, it'd be nice to connect it back to consciousness in future.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 376.221,
    "end": 386.684,
    "text": " And there's also lots of other related work going on in category theory that's very close to this, that often goes by the name of categorical cybernetics, and this might include some of the things Toby has talked about on the stream in the past.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 387.684,
    "end": 395.926,
    "text": "So what I'll do now is introduce categories and string diagrams, and then later we'll apply them to all these basic ingredients of active inference that I've alluded to so far.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 395.946,
    "end": 400.888,
    "text": "So that would be generative models, updating them, free energy, and active inference itself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 402.988,
    "end": 405.329,
    "text": "So let's start with these categories and string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 406.991,
    "end": 411.917,
    "text": " So you can think of a category in general as a sort of world of interacting processes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 412.437,
    "end": 415.761,
    "text": "And the categories we talk about here are always going to be these symmetric monoidal categories.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 416.161,
    "end": 420.987,
    "text": "But don't worry too much about the formal language, because the way we talk about them is just going to come down to the diagrams here today.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 422.048,
    "end": 427.334,
    "text": "So a category amounts to a collection of these objects or sometimes called systems like capital ABC here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 428.277,
    "end": 431.758,
    "text": " and what are called morphisms, or you might want to just be called processes between them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 432.318,
    "end": 442.841,
    "text": "So when you're writing, normally you can just write a morphism from A to B, it's like this, F colon A to B. In string diagrams though, you draw it like this, where we're reading all the diagrams from bottom to top in this talk.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 443.401,
    "end": 456.925,
    "text": "So you have a wire for the A input at the bottom and a wire for the B output at the top, and the morphism is just drawn as a box, F here, and you just read the diagram up thinking about, okay, it takes in this input coming in on this wire A, process applies,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 462.065,
    "end": 464.086,
    "text": " So what you can do with these processes is compose them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 464.346,
    "end": 470.909,
    "text": "So if you have two processes, F from A to B and G from B to C, so the types like that, you can just compose them in sequence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 471.43,
    "end": 474.311,
    "text": "And this just means plugging the boxes together in your diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 474.331,
    "end": 478.453,
    "text": "And because we're in this monoidal category, you can also compose in parallel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 503.667,
    "end": 504.167,
    "text": " the symbols.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 504.928,
    "end": 511.171,
    "text": "So if these two basic modes of composition and from these you can build much more elaborate string diagrams in your category.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 513.092,
    "end": 523.117,
    "text": "So if you're writing things very mathematically you have to write lots of equations that a category or a monoidal category needs to satisfy, but when you work in the diagrams they basically do some of the work for you because these things just come out for free.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 523.697,
    "end": 525.298,
    "text": "So for example you have equations like this",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 534.145,
    "end": 534.846,
    "text": " them along the wires.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 534.886,
    "end": 536.447,
    "text": "It doesn't really matter where they are on the wires.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 536.848,
    "end": 538.469,
    "text": "It tends to just be the connectivity that matters.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 539.17,
    "end": 542.053,
    "text": "Similarly, we can cross wires over each other because we're in a symmetric setting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 543.614,
    "end": 548.098,
    "text": "And there's a few, like, just useful features that you always have in a category.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 548.118,
    "end": 553.063,
    "text": "So every object comes with this identity morphism, which you just think of as meaning nothing happening, basically.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 567.774,
    "end": 570.857,
    "text": " means we can talk about a morphism which doesn't even have an input or an output.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 571.557,
    "end": 574.4,
    "text": "More formally it has this object i as its input or output.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 575.1,
    "end": 576.942,
    "text": "And we give these things special names.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 577.563,
    "end": 584.769,
    "text": "So the most important one probably is that of a state, which is a morphism with no input as it were, or really with input i.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 600.419,
    "end": 601.3,
    "text": " next to your diagram.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 603.781,
    "end": 606.823,
    "text": "So there are many categories out there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 606.843,
    "end": 614.627,
    "text": "The point of category three is extremely general, so this could be talking about computational processes or physical processes or quantum processes in particular and all sorts of things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 615.148,
    "end": 617.309,
    "text": "But this talk will only actually need one category.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 617.609,
    "end": 619.03,
    "text": "We'll just keep it simple with this one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 619.05,
    "end": 623.773,
    "text": "That's the category, I call it MatR+, so it's positive real matrices.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 624.933,
    "end": 627.715,
    "text": "So you can just take objects, so the wires should be finite sets,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 628.487,
    "end": 632.811,
    "text": " And the morphisms to be positive matrices indexed by these, as I'll explain.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 632.871,
    "end": 646.964,
    "text": "So if we draw a box like this, M going from X to Y, this is like a matrix indexed by X and Y. And for each input in this little set X, sorry, in the set X, and each output in Y, you get a positive real number.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 647.585,
    "end": 653.691,
    "text": "And we'll write it like M of Y given X. So this box would mean a function like this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 655.689,
    "end": 659.971,
    "text": " Now, when we plug them together, they let us turn some things that you normally have to do with equations into kind of simpler pictures.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 660.211,
    "end": 661.292,
    "text": "I mean, mainly this middle one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 661.312,
    "end": 666.414,
    "text": "So if we have two in sequence, we just compose them by matrix multiplication.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 666.554,
    "end": 671.737,
    "text": "So instead of having to write this formula where we sum over y, we can just draw this picture above it where we just plug them on top.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 672.757,
    "end": 683.162,
    "text": "And if you run them in parallel here, we take the partition product of the sets and the tensor product of the matrices, as it were, but it's just the obvious thing where you have two things running independently.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 700.266,
    "end": 701.047,
    "text": " a positive real.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 701.908,
    "end": 713.938,
    "text": "The intuition, though, is that we're going to restrict to the particular morphisms in here, which are probabilistic in nature, so they need to send each x to actual distribution over y. So I want to talk about how you actually pick those out next.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 715.399,
    "end": 717.861,
    "text": "So to do that, you use some extra structure that this category has.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 718.401,
    "end": 720.503,
    "text": "It forms what's called a copy-discard category.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 720.763,
    "end": 721.644,
    "text": "So this is one",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 726.049,
    "end": 737.873,
    "text": " these distinguished processes, there's one that we call copy that takes in A, say, and we've got two copies of A at the top, and one called discard, where you just throw A away, so you have no output at the top.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 738.413,
    "end": 742.775,
    "text": "These satisfy some equations that are quite intuitive, if you think about copying and then",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 758.78,
    "end": 762.581,
    "text": " comes in and two copies of eight come out at the top is the intuition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 764.061,
    "end": 772.424,
    "text": "The reason we introduced this stuff is because it's been shown recently that you can do a lot of probability theory just in terms of these CD categories and particular ones called Markov categories.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 772.444,
    "end": 774.944,
    "text": "So there's a lot of what's going on in applied category theory at the moment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 774.964,
    "end": 777.605,
    "text": "It's using this language of CD categories.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 779.605,
    "end": 782.306,
    "text": "In particular, they let you pick out some things to do with probability theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 782.366,
    "end": 784.147,
    "text": "I'll just talk about a couple of them here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 784.307,
    "end": 785.747,
    "text": "The most important one is the notion of a channel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 786.747,
    "end": 790.91,
    "text": " So this is what lets us pick out the actual normalized matrices, as it were, from earlier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 791.771,
    "end": 795.353,
    "text": "So in general, you call a morphism a channel when it preserves, it's discarding.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 796.314,
    "end": 800.216,
    "text": "And a special case is a state when it's a channel, you call it normalized.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 801.097,
    "end": 803.659,
    "text": "So for a state, this means it would actually be a probability distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 804.019,
    "end": 805.56,
    "text": "So it's actually normalized if you sum over",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 820.026,
    "end": 821.787,
    "text": " to use in generative models, for example.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 824.69,
    "end": 828.213,
    "text": "And as I said, there's lots of probability theory you can describe with these diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 828.233,
    "end": 829.834,
    "text": "There's very two simple examples.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 830.154,
    "end": 833.657,
    "text": "You can describe marginalization in probability theory with this discarding thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 833.677,
    "end": 837.22,
    "text": "So if you have box omega like this, it would be a joint distribution of X and Y.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 852.577,
    "end": 857.658,
    "text": " So we'll meet lots more of this as we go, but let's actually start doing some stuff related to active inference in particular now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 857.698,
    "end": 861.239,
    "text": "So I want to talk about generative models and how you view these in the diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 862.18,
    "end": 869.142,
    "text": "So as we've said, we're going to be talking about agents having generative models that relate things like actions, observations, and world states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 870.362,
    "end": 872.682,
    "text": "And these are normally quite compositional in active inference, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 872.762,
    "end": 878.224,
    "text": "And they might involve many different spaces of states and observations and these processes relating them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 878.684,
    "end": 881.985,
    "text": "And you'd usually treat these with something like a Bayesian network, claiming you can",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 888.633,
    "end": 904.086,
    "text": " Formally, it's the same thing as the spatial network, which you could normally say is described by something like a DAG, a Directed Aspect Graph, which describes the different variables that are being related and then sets of values for each and probability channels describing each one in terms of its parents in the DAG.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 904.206,
    "end": 907.249,
    "text": "And then you often look at this whole distribution over all the variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 908.25,
    "end": 913.094,
    "text": "But already, I'd say the way that these networks are drawn in active inference text and stuff",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 922.897,
    "end": 925.119,
    "text": " as it were, like you have in this picture, the A and the Bs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 925.139,
    "end": 935.805,
    "text": "So yeah, my claim is that it's sort of converging on the way that string diagrams will look, as we'll see on the next slide, which is where you really do label everything, not just the variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 937.667,
    "end": 942.89,
    "text": "So to describe those sort of Bayesian networks with string diagrams, the key observation is really that DAGs",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 951.377,
    "end": 954.86,
    "text": " second, but the diagrams built from copying and sometimes discarding.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 955.58,
    "end": 961.024,
    "text": "And the key thing is just that they only have processes with maybe many inputs, but only one output.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 962.045,
    "end": 964.867,
    "text": "So this is going to be like a mechanism that produces each variable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 966.308,
    "end": 970.691,
    "text": "So the result is that if you have a DAG G and you choose some of the vertices",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 984.245,
    "end": 990.79,
    "text": " And what you do is you have a wire for each variable in your diagram on the right, and you draw a box that produces it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 990.81,
    "end": 992.251,
    "text": "It doesn't really matter what you label the box.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 994.712,
    "end": 1001.897,
    "text": "So this box C, for example, produces X2, and it'll produce it in terms of its parents in DAG, so X1 and X4 in this case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1002.057,
    "end": 1005.299,
    "text": "And if it doesn't have any parents, it would just be a state, as it were, box with no input.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1006.76,
    "end": 1010.323,
    "text": "And then what you do is you take each variable and you copy it and you pass it to all of its children.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1019.955,
    "end": 1020.896,
    "text": " the output ones.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1022.577,
    "end": 1024.978,
    "text": "So this allows us to turn DAG into a string diagram.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1025.539,
    "end": 1035.285,
    "text": "And then if you want to make a generative model that expresses, you know, like a Bayesian network that's structurally going to this DAG, you just have to now interpret this diagram in a certain sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1035.405,
    "end": 1045.472,
    "text": "So in general, working in any one of these CD categories, this copy-discard categories, we can say that a generative model in there is given by one of these network diagrams without any inputs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1046.79,
    "end": 1053.374,
    "text": " an interpretation of the diagram, meaning you actually say what the objects are for each of the wires in the diagram and what the actual channels are.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1053.434,
    "end": 1055.015,
    "text": "So they need to be channels, not just morphisms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1056.196,
    "end": 1057.917,
    "text": "And your category are for each of the boxes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1058.597,
    "end": 1070.784,
    "text": "So for a gender model like this, you would say you pick objects X1, X2, X3, X4 and pick channels for the A, B, C, D. And we'll think of the outputs of the diagram as like the observed variables and the rest of the hidden ones.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1072.325,
    "end": 1075.327,
    "text": "So for example, if you're working in this category MatR+,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1076.767,
    "end": 1078.388,
    "text": " That is the only category I've actually introduced here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1078.488,
    "end": 1079.709,
    "text": "So this is going to be a running example.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1080.089,
    "end": 1082.51,
    "text": "This is the same thing as one of these causal Bayesian networks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1082.55,
    "end": 1087.393,
    "text": "So it just means picking sets of values for variables and picking probability channels for the boxes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1089.294,
    "end": 1093.077,
    "text": "So you might ask, why would you use this representation rather than the usual one?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1093.097,
    "end": 1093.997,
    "text": "I think it's a good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1094.678,
    "end": 1098.66,
    "text": "So it's equivalent to the DAG and probability channel description.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1108.733,
    "end": 1115.779,
    "text": " with probabilities, whereas in the categorical approach, you can use just one formalism because you can do probability theory with the string diagrams as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1115.919,
    "end": 1117.1,
    "text": "So it's quite natural in a sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1117.12,
    "end": 1122.123,
    "text": "You have this one language for both just intuitively drawing what's going on in your model and then reasoning about it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1123.004,
    "end": 1126.147,
    "text": "It also lets you start to generalize things in a useful way, I think.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1126.747,
    "end": 1131.971,
    "text": "So I kept having to say that you have no inputs to your diagram, but there's nothing really fundamental about",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1138.379,
    "end": 1142.26,
    "text": " So we'll call this an open generative model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1142.601,
    "end": 1148.263,
    "text": "So an open generative model is the same thing, but now I've just dropped this requirement that the diagram doesn't have any inputs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1149.323,
    "end": 1153.224,
    "text": "So here's an example of the general network diagram now with these inputs x2, x3.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1153.424,
    "end": 1155.165,
    "text": "So there's no mechanism specified for",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1168.766,
    "end": 1175.03,
    "text": " And this is the same definition we use to define what we call an open causal model in the paper with Robin Lorenz that I mentioned earlier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1175.811,
    "end": 1182.776,
    "text": "So a number of generative model is formally just the same thing, but we're just thinking of it as a generative model possessed by a cognitive agent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1184.037,
    "end": 1191.922,
    "text": "If you run this definition in MATLAB first then, this is just like a causal Bayesian network, but now you have some of your variables just have no mechanisms specified, so they're just inputs to the whole thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1193.683,
    "end": 1196.425,
    "text": "A nice thing about these open generative models is that because they're open,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1205.665,
    "end": 1212.09,
    "text": " So that was the general theory of these generative models that just actually describe some examples that you'll see in active inference coming up all the time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1213.21,
    "end": 1214.151,
    "text": "So it's a simple example.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1214.791,
    "end": 1218.194,
    "text": "Let's just imagine we have one space of hidden states and one space of observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1218.514,
    "end": 1224.418,
    "text": "Then it would be a generative model of that form would just look like this network diagram, where there's just two wires.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1224.458,
    "end": 1227.48,
    "text": "There's just S and O. S doesn't have any",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1241.537,
    "end": 1242.218,
    "text": " generative models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1242.958,
    "end": 1257.728,
    "text": "When you're looking at these, you're often interested in this distribution over both variables together, this joint distribution that you might write as, you know, p of s times p of o given s normally, or a bit more specifically here, introducing their names for the two distribution in the channel here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1258.728,
    "end": 1268.995,
    "text": "In string diagrams it's just the same as this, so you just take the prior and you make it an output now, and then you compose those channels together, and this would give you a distribution, so a normalized state m",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1279.548,
    "end": 1287.894,
    "text": " So a more elaborate example of a generative model that you'll see, for example, in the Active Infants textbook are these discrete time models that are used a lot.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1288.074,
    "end": 1291.016,
    "text": "So I'll walk through this diagram now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1291.056,
    "end": 1296.039,
    "text": "So this is an example of a more complex network diagram and a generative model it describes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1297.02,
    "end": 1298.921,
    "text": "So here we've got these n time steps going.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1317.786,
    "end": 1324.474,
    "text": " And the hidden state is evolving over time by this transition channel B, where it takes the previous state as an input and then gives the next one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1325.055,
    "end": 1332.142,
    "text": "It also takes this one extra wire that's coming from the bottom, and that's this space P of policies, which is how the agent sort of actions enter the picture.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1332.162,
    "end": 1335.366,
    "text": "So these policies describe its behaviors, its behavioral policies.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1368.573,
    "end": 1374.838,
    "text": " And often you will see models sort of of this form or similar forms being plugged together to form these hierarchical models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1374.998,
    "end": 1380.142,
    "text": "And I think the compositional language is very nice for these because you really want to talk about open models to define this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1380.702,
    "end": 1387.968,
    "text": "The hierarchical model you can view as really has just been given by taking lots of these open generative models I mentioned and plugging them together in a certain sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1388.268,
    "end": 1394.633,
    "text": "So here you can see a picture of a hierarchical model where we just have these layers where different copies of the same model at each",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1402.0,
    "end": 1402.96,
    "text": " That's generative models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1403.961,
    "end": 1406.461,
    "text": "So far, we've just been using the diagrams to represent them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1406.481,
    "end": 1408.982,
    "text": "We'd like to actually do a bit more though and reason about these models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1409.342,
    "end": 1419.105,
    "text": "In particular, we'll talk about how you update a model or update the beliefs within a model, which is very important in active inference and how this looks in the string diagrams now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1421.506,
    "end": 1425.087,
    "text": "So let's say we've got an agent M with a model M.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1444.994,
    "end": 1446.455,
    "text": " they receive a new observation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1447.235,
    "end": 1454.44,
    "text": "And here, the kind of observations we'll think about in general can be soft, meaning they're described by a distribution over O, not necessarily just one element.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1455.961,
    "end": 1459.143,
    "text": "So that would be one of these normalized states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1459.803,
    "end": 1462.505,
    "text": "I'll try not to use the word state here because it's a bit confusing with this S around.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1462.725,
    "end": 1470.63,
    "text": "So this distribution over O is their new observation, this bold font O. And now they want to update the margin.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1470.69,
    "end": 1473.552,
    "text": "They want to update M in some way so that the marginal basically is",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1482.841,
    "end": 1494.525,
    "text": " world given some observation you receive, but it could also be used to model like planning behavior where updating your plan of action, your policy, given something like which outcomes you'd like to see in the future.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1495.446,
    "end": 1497.146,
    "text": "And we'll come back to that later in the talk.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1498.587,
    "end": 1501.328,
    "text": "So we want to talk about how you can do this updating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1502.588,
    "end": 1506.91,
    "text": "So you might think it's just a standard answer, or at least in the ideal case, which is this Bayesian updating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1507.05,
    "end": 1508.99,
    "text": "And that's true when your observations are sharp.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1509.19,
    "end": 1510.611,
    "text": "And we'll start by talking about that case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1512.125,
    "end": 1512.885,
    "text": " means in a second.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1513.505,
    "end": 1516.726,
    "text": "But first I'll just talk about how you treat Bayesian conditioning in these string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1517.366,
    "end": 1531.81,
    "text": "So on the right here we have, if you view this process from O to S, this describes the sort of Bayesian conditional channel, or in general a partial channel in fact, that the agent would have from that introduced by their model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1531.87,
    "end": 1535.691,
    "text": "So you can describe this in string diagrams with a couple of extra gadgets",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1548.727,
    "end": 1549.287,
    "text": " into an input.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1549.307,
    "end": 1550.608,
    "text": "So that's what this part here is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1551.669,
    "end": 1554.29,
    "text": "And then you can introduce this extra thing of normalization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1554.33,
    "end": 1565.137,
    "text": "So what you'd like to do is take a general morphism and for each possible input, normalize that so that it's a distribution, if you can, or set it to zero if it's just zero and there's nothing you can do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1565.957,
    "end": 1568.458,
    "text": "That's what this blue dash box is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1568.899,
    "end": 1574.382,
    "text": "And in the paper and in the related causal models paper, we talk about the axioms normalization feature satisfies.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1575.416,
    "end": 1579.477,
    "text": " And the point is, if you compute this thing in Matar Plus, it will give you kind of what you'd expect.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1579.577,
    "end": 1592.201,
    "text": "So it will give you the usual notion for each point O of the space O. You plug it into this, you'll get the kind of conditional M of S given O that you'd expect whenever that's defined.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1593.402,
    "end": 1599.064,
    "text": "There's a string diagram way to describe this kind of Bayesian conditional channel or partial channel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1600.144,
    "end": 1602.565,
    "text": "And I said this is what you would use when your observation is sharp.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1605.759,
    "end": 1606.279,
    "text": " in which that case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1606.339,
    "end": 1607.08,
    "text": "So what does that mean?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1607.82,
    "end": 1617.905,
    "text": "In general, you say that a state in one of these CD categories, so a distribution basically, would be sharp when it's copied by the copy map, which isn't true for general distributions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1619.086,
    "end": 1622.528,
    "text": "And if you run this definition in Math.plus, this really means",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1640.465,
    "end": 1645.75,
    "text": " So for these sharp ones, you know, you ideally think you'd like to do this Bayesian updating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1646.491,
    "end": 1652.736,
    "text": "In fact, when you've got soft ones, so it doesn't have this property, there's actually at least two good ways to do this kind of updating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1653.237,
    "end": 1656.98,
    "text": "I don't know if this is as well known, so I'll just mention it now anyway.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1657.0,
    "end": 1659.863,
    "text": "And they've been studied in some detail by Bart Jacobs in this paper at the bottom.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1661.444,
    "end": 1663.946,
    "text": "So let's say you don't get one of these sharp observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1663.966,
    "end": 1665.588,
    "text": "You have just a distribution over O.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1666.374,
    "end": 1673.921,
    "text": " There's at least two reasonable ways to generalize sort of the picture from the last slide together, a notion of updating, and Yakub's calls them Jeffery's and Pearl's update rules.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1675.562,
    "end": 1679.366,
    "text": "So in Jeffery's update, you basically do it like we did before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1679.466,
    "end": 1685.812,
    "text": "You have this Bayesian conditional kind of channel, a partial channel, the normalized box here, and you just plug a distribution into it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1686.493,
    "end": 1688.635,
    "text": "But in Pearl's update, you turn",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1702.441,
    "end": 1704.062,
    "text": " these are reasonable notions of generalization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1704.082,
    "end": 1705.003,
    "text": "They have different properties.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1705.443,
    "end": 1708.365,
    "text": "It's not obvious that one of them is sort of more rational or something than the other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1708.826,
    "end": 1710.187,
    "text": "They just behave a bit differently.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1712.188,
    "end": 1716.411,
    "text": "In the formula, you can see that the normalization is being applied differently.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1716.431,
    "end": 1720.714,
    "text": "So if you turn this picture into the usual notation, it would look like this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1721.055,
    "end": 1728.66,
    "text": "So in the top case, you're normalizing for each possible sharp O and then taking an expectation over this distribution here, whereas in the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1733.18,
    "end": 1739.526,
    "text": " Either way you do it, though, the point is that these things are actually hard to compute, so we don't expect the cognitive agent to be doing either of these exactly, even in the sharp case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1740.306,
    "end": 1749.074,
    "text": "And so, as we know, we want to instead approximate these kind of things using free energy, which is what I'll talk about next.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1749.774,
    "end": 1753.317,
    "text": "So our aim is to try and accommodate free energy somehow in a diagrammatic approach.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1754.198,
    "end": 1760.083,
    "text": "And free energy sort of formulae that come up are often given in terms of what you call the surprise, these negative logarithm quantities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1760.263,
    "end": 1760.964,
    "text": "So we'll start by",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1766.397,
    "end": 1778.661,
    "text": " So if you have any function e on a set x, a positive real, remember that would look like an effect in your category x, in MATLAB Plus, that's the same.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1779.942,
    "end": 1783.583,
    "text": "Then what we want to do is talk about this function x goes to minus log of e of x.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1794.404,
    "end": 1798.907,
    "text": " And using rules, the nice properties of logarithm, you can turn these into nice graphical rules.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1799.067,
    "end": 1800.808,
    "text": "This log box feature would satisfy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1802.069,
    "end": 1806.972,
    "text": "For example, this is sort of the way that logarithms have multiplication into addition on the left here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1810.175,
    "end": 1812.516,
    "text": "If you have this around, then you can start talking about surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1812.796,
    "end": 1821.442,
    "text": "So if you have two distributions, sigma and omega, the surprise of one distribution relative to the other is defined by",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1827.145,
    "end": 1855.276,
    "text": " omega which if you remember I said expectation values are given by sort of plugging a state for the distribution into the thing you're looking at the expectation value of so that would be the log box here so we can just define surprise omega sigma in this way in the pictures and important special cases where this come up are when you're calculating entropy which is the self surprise and the KL divergence can be calculated from the surprise and the entropy so it means that whenever we have formula given in terms of these if we like we can instead",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1858.275,
    "end": 1859.217,
    "text": " with the log box.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1861.66,
    "end": 1864.604,
    "text": "So now let's talk about how we use this to describe free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1864.645,
    "end": 1870.994,
    "text": "So what we want to do in the paper is sort of help clarify the kind of different notions of free energy that we found in active inference, in particular the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1887.014,
    "end": 1891.721,
    "text": " The situation is that we've got some generative model that's fixed over these two variables, S and O, like before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1891.741,
    "end": 1896.507,
    "text": "So remember, we have this distribution, this box M over S and O, its distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1897.248,
    "end": 1899.812,
    "text": "And let's just say we have another distribution Q now and we'll see it's",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1919.241,
    "end": 1929.355,
    "text": " entropy of Q's marginal on S. Or this is the formula if you want to use the conventional notation, which is useful for relating it to existing approaches.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1930.156,
    "end": 1932.96,
    "text": "So we can define this by general free energy quantity, and then we'll meet",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1946.837,
    "end": 1948.098,
    "text": " So there's a variation of free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1948.118,
    "end": 1953.54,
    "text": "So we have this fixed model M and we have this soft observation O like we did before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1953.58,
    "end": 1954.42,
    "text": "So this is box here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1955.3,
    "end": 1961.783,
    "text": "And then what we're doing is we're considering different possible distributions over S that we think of as different updates we could consider for our beliefs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1962.683,
    "end": 1964.524,
    "text": "And we define the variation of free energy of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1979.053,
    "end": 1984.017,
    "text": " observation O. So in formulae, you could also draw it like this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1984.057,
    "end": 2000.471,
    "text": "So you take the surprise from your model M and you just see how its expected value for those beliefs and that observation subtracted the entropy of Q. And what you can show is that this VFE value satisfies this bound of the KL.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2006.636,
    "end": 2013.642,
    "text": " when it's a sharp observation, then the minimal of this VfE will be given by the Bayesian updating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2015.043,
    "end": 2016.844,
    "text": "In general, though, we might think about what happens.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2017.725,
    "end": 2024.831,
    "text": "So in general, we can think of minimizing this VfE quantity as finding this Q that approximates this kind of updates we were looking at earlier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2026.332,
    "end": 2029.935,
    "text": "And yeah, in the sharp case, it will coincide as all of the notions of updating do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2031.414,
    "end": 2033.875,
    "text": " So the minimal VFE will be given by the Bayesian update.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2034.295,
    "end": 2037.415,
    "text": "But for these soft observations, it's something else.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2037.476,
    "end": 2041.756,
    "text": "It's not exactly either of the two notions of updating we met earlier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2041.776,
    "end": 2049.258,
    "text": "So this is actually a third notion of updating for soft observations, which I think is an interesting way to think about what VFE minimization is doing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2049.678,
    "end": 2051.219,
    "text": "So we just call this the VFE update.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2051.679,
    "end": 2053.319,
    "text": "So you've got many different Q. You could calculate",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2067.688,
    "end": 2068.309,
    "text": " net earlier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2069.53,
    "end": 2071.632,
    "text": "So that's the VFE, which we'll come back to.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2072.492,
    "end": 2076.116,
    "text": "The other notion of free energy we want to talk about is the expected free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2077.617,
    "end": 2084.784,
    "text": "So that's where we still have our model M, where rather than an observation, we think of ourselves as having some preferences over observations we'd like to see.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2085.244,
    "end": 2092.671,
    "text": "They're again encoded in a distribution though, over O, so that's this C. And so just with that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2096.769,
    "end": 2113.642,
    "text": " given by the free energy of M compared with this other generative model where you have the same, so this M here would really be the inverse channel from O to S of M, so like the Bayesian inverse here, but where you just assert that the preference actually is the prior on the observation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2113.682,
    "end": 2117.665,
    "text": "So you're comparing these two in terms of this generic",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2141.929,
    "end": 2153.015,
    "text": " So I won't talk, I don't think I have time to go too much more into EFE, but the point really, in terms of what this work's done, is just to try and have this one generic free energy quantity we met earlier, where we can see the VFE and the EFE both coming out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2153.215,
    "end": 2157.077,
    "text": "The special case is depending on what we plug in here for the two distributions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2160.099,
    "end": 2165.522,
    "text": "So what I'd like to do now is to sort of put some of these pieces together to show what active inference itself will kind of look like in terms of string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2165.922,
    "end": 2169.084,
    "text": "And in particular, what we'll do is derive this formula that you'll find in active inference tech",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2177.503,
    "end": 2182.727,
    "text": " So to do so, we basically need to give a nice high level conceptual view of what active inference is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2182.787,
    "end": 2184.308,
    "text": "So this is the way that we do it in the paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2185.929,
    "end": 2191.933,
    "text": "So in active inference, the key thing for stating the definitions is that our model takes the following form at a high level.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2192.093,
    "end": 2194.275,
    "text": "So there's some notion of, so it's like our",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2215.094,
    "end": 2215.894,
    "text": " current observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2215.914,
    "end": 2220.396,
    "text": "And then there's the notion of future times, these future states and future observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2220.616,
    "end": 2227.598,
    "text": "So that could be all the time steps up to some big number, something like that, all grouped together in one of those discrete time models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2228.898,
    "end": 2242.903,
    "text": "And again, we have the policies and we have the same sort of shape of model where there's some channels here which I haven't bothered giving letters to, but showing the way that the policy influences the transition from the state to the future state and observations from each",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2244.372,
    "end": 2250.775,
    "text": " So we just have a generative model where we have policies, we have states and observations, and we have future states and future observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2251.475,
    "end": 2253.976,
    "text": "In active inference, what we're doing is we're receiving two things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2253.996,
    "end": 2260.078,
    "text": "We're receiving an observation in the current time, and we have some preferences about what we'd like to see in the future.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2261.059,
    "end": 2267.821,
    "text": "So these are each given by these two distributions, O over, bolt upon O over O, and the preferences C over the future observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2268.642,
    "end": 2270.042,
    "text": "And then we're doing updating with those.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2270.262,
    "end": 2272.223,
    "text": "So I think updating is just our",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2275.08,
    "end": 2280.043,
    "text": " prior over the policies to give our new distribution over policies, which we can",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2303.527,
    "end": 2310.47,
    "text": " So in the in the books, for instance, and various places, you can find a formula like this that will be justified as coming from the free energy principle in some way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2310.49,
    "end": 2317.492,
    "text": "It's basically saying you can do this approximately by making your plan distribution take the following form.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2318.133,
    "end": 2321.894,
    "text": "There's a softmax, there's a part relating to the habits of your model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2322.394,
    "end": 2323.955,
    "text": "So that's your prior over policies.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2324.155,
    "end": 2329.937,
    "text": "These pi are the individual policies in P. And then there's two parts of the formula relating to the VFE and the EFE.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2331.813,
    "end": 2338.9,
    "text": " And what we wanted to do is see where this formula comes from in a sort of nice, high level way from the structure of the diagram.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2339.3,
    "end": 2350.031,
    "text": "So the usual, there are explanations for this formula there, but I found them quite hard to follow, to be honest, because they are talking about the EFE as being a prior that you then do",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2351.041,
    "end": 2358.05,
    "text": " But you kind of need to do the forward the part about the present time 1st, before you can do the.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2358.53,
    "end": 2362.555,
    "text": "And so what we wanted, this is a really clear way to see how this just drops out from the structure of the model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2362.575,
    "end": 2363.716,
    "text": "So that's what I'll try and show.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2363.736,
    "end": 2367.901,
    "text": "Now, so.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2368.999,
    "end": 2371.38,
    "text": " What we'd like to do then is to do this approximate updating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2371.4,
    "end": 2374.261,
    "text": "We're going to do the Perl style updating, which looked like this in the pictures.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2374.682,
    "end": 2383.346,
    "text": "So we want to get our new plan, so our distribution over policies, by updating, by plugging in our observation and our preferences, and then normalizing everything.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2383.386,
    "end": 2387.788,
    "text": "So the thing on the right is what we'd like to have ideally, but we're just going to have to approximate it in some way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2389.128,
    "end": 2393.17,
    "text": "So let's just take the distribution that's inside the dash normalization box now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2394.631,
    "end": 2397.052,
    "text": "This is the thing, you know, we'd like to basically approximate this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2399.309,
    "end": 2401.811,
    "text": " In the structure of our model, we can write it like this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2402.812,
    "end": 2408.498,
    "text": "And I'll just show then some graphical steps for how we can apply approximations to obtain the formula that we saw.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2408.678,
    "end": 2415.504,
    "text": "And obviously we weren't able to get through every detail of the proof, but it should give hopefully some intuition of what it's like to actually work with a string diagram.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2415.524,
    "end": 2416.505,
    "text": "So that's really why I'm showing it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2417.326,
    "end": 2419.047,
    "text": "So we knew our model take roughly this form.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2419.227,
    "end": 2424.212,
    "text": "There's some part relating to current states and current observations and also future observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2424.332,
    "end": 2425.113,
    "text": "I just call them both M here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2427.827,
    "end": 2431.248,
    "text": " the model relating to the present time and future time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2433.109,
    "end": 2438.932,
    "text": "And so what we're going to do is first focus on this part of the model relating to the current state and the current observation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2438.952,
    "end": 2442.513,
    "text": "And we want to approximate what's in that blue dashed box.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2443.174,
    "end": 2449.416,
    "text": "And what you can show is that if you do this VFE updating, that will be approximately equal to this part of the diagram.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2450.517,
    "end": 2454.999,
    "text": "So this Q is given by for each policy doing this VFE updating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2457.236,
    "end": 2457.756,
    "text": " free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2458.457,
    "end": 2475.19,
    "text": "So you do that for each policy, and then you can view the collection of all of those belief updates as just one channel from P to S. So if you think about it back here, basically for each policy you could plug in, you would obtain just a distribution now of S and O, and you could do updating with respect to that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2475.21,
    "end": 2478.673,
    "text": "So that's what Q of that particular policy pi would be.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2478.693,
    "end": 2484.377,
    "text": "And you put them all together into this one channel Q. And you can show then for each one",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2486.847,
    "end": 2492.431,
    "text": " by this E to the minus VFV quantity here, it will be approximately equal to this part of the diagram.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2494.412,
    "end": 2497.234,
    "text": "Okay, so that's our first step and that's how the VFV entered the picture.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2497.254,
    "end": 2506.54,
    "text": "Then we've got this top part of the diagram, we'll collapse it together and just view this as one process going into future observations and our preferences, and we'd like to approximate what's in this box now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2507.821,
    "end": 2513.485,
    "text": "And this is where the EFV comes in, so you can basically show, because we have this, yeah, that the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2514.678,
    "end": 2525.748,
    "text": " expected free energy will give you an approximation to this here, where this is basically like an expectation value for your preferences for each policy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2525.768,
    "end": 2531.573,
    "text": "So this would be like the density C of the preferences being plugged into your model for each policy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2534.116,
    "end": 2534.596,
    "text": "So I haven't had",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2548.251,
    "end": 2548.651,
    "text": " things like this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2549.932,
    "end": 2553.095,
    "text": "So this step where you think about the future times, it's called the prediction step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2553.795,
    "end": 2555.897,
    "text": "And the previous one was the perception step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2557.058,
    "end": 2563.482,
    "text": "So now we've rewritten that diagram in terms of some e to the minus of the VFE and e to the minus of the EFE, as well as our habits.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2566.344,
    "end": 2569.967,
    "text": "And remember, what we wanted to do was approximate the normalization of this whole thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2569.987,
    "end": 2572.029,
    "text": "So that's when you apply this bool dash box around the whole thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2572.929,
    "end": 2575.071,
    "text": "And now if we do that, this is exactly the same",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2579.493,
    "end": 2583.575,
    "text": " And that's because, you know, you're normalizing something, but it's got these e to the minuses in it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2584.296,
    "end": 2592.18,
    "text": "So you can also rewrite that in terms of this softmax, where now you just replace the e with this log and the other ones, you lose the exponentials.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2593.341,
    "end": 2599.904,
    "text": "So this formula, if you wanted to, if you wrote out what this was for each policy, it would be equal to this down here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2602.439,
    "end": 2606.363,
    "text": " The claim is that this is a nice way to derive this formula, and it's a bit more transparent than the ones that exist.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2606.423,
    "end": 2609.185,
    "text": "So the idea was really just to see, we draw what's going on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2609.225,
    "end": 2619.434,
    "text": "We're doing updating with the model of this form, and we're trying to do this approximate form of updating, and just see where we're applying the approximations, and from the structure of the model itself, see how this formula comes about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2622.478,
    "end": 2626.7,
    "text": " Okay, so that so far basically just talked about things that are already there in active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2626.72,
    "end": 2628.681,
    "text": "That's this new derivation, but it's existing stuff.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2629.182,
    "end": 2635.105,
    "text": "Before wrapping up, I'd just like to also talk about something a bit more new that we do with the string diagrammatic approach.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2635.125,
    "end": 2641.088,
    "text": "So that's the talk about the way in which free energy itself is compositional.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2642.129,
    "end": 2647.752,
    "text": "So the motivation for this is that, you know, the idea is that we want to think of this one free energy principle applying at all levels of a system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2649.677,
    "end": 2664.203,
    "text": " So to do that, you'd want to know that an agent can say, if you've got one of these big composite generative models, that it can do its free energy minimization on the whole thing by doing it on the parts, because we want to ultimately think it just comes down to each part doing its own bit of free energy minimization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2664.643,
    "end": 2666.124,
    "text": "So that's what we want to make precise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2668.244,
    "end": 2671.025,
    "text": "In particular, we're going to be talking about the VFE here really all the time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2671.946,
    "end": 2673.967,
    "text": "And if you recall, in the diagrams, it looked like this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2674.027,
    "end": 2677.568,
    "text": "So we used these log boxes, and it just took this particular shape here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2678.936,
    "end": 2692.264,
    "text": " So what we do in the paper in order to address this compositionality problem is introduce a notion of this VFE that we can apply not just to generative models, but ones which actually have these inputs as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2692.304,
    "end": 2694.305,
    "text": "So these are what I call open generative models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2695.446,
    "end": 2700.149,
    "text": "Because we need to really talk about pieces of generative models plugging together and give them a notion of free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2700.169,
    "end": 2703.511,
    "text": "Do you make sense of this notion of free energy being compositional?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2704.772,
    "end": 2707.053,
    "text": "So we propose this definition of what we call the open VFE.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2708.158,
    "end": 2714.74,
    "text": " now instead of just a distribution M over S and O, we have a channel from some inputs to S and O given by one of these open models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2716.0,
    "end": 2723.141,
    "text": "And our Q, the thing we're doing the VFE minimization with respect to, so the thing we're calculating, would now have an input as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2723.161,
    "end": 2728.223,
    "text": "So it's a joint distribution over the states and inputs, and observation takes the same shape as before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2730.463,
    "end": 2734.224,
    "text": "So you get this other formula that's basically just a natural way to generalize the previous",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2742.459,
    "end": 2747.721,
    "text": " And what we show is that this thing is compositional in the sense that I alluded to, so I'll walk through that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2748.061,
    "end": 2752.743,
    "text": "And the way you do it is just using these graphical properties that these log boxes have that I mentioned earlier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2752.923,
    "end": 2764.427,
    "text": "So you could turn all of that into a proof in standard probability notation if you like, but it's quite instructive to always just be able to work in the diagrams to keep track of the compositional structure of the models and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2766.016,
    "end": 2770.759,
    "text": " So the result says that this open VFE quantity is compositional in two ways.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2770.779,
    "end": 2773.18,
    "text": "The first one here is this quite trivial way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2773.24,
    "end": 2788.73,
    "text": "So if we have two models running in parallel, so like taking a tensor of them, and they're just both doing their own, or calculating the VFE for each of them, sorry for the whole thing, but it's just given by two running in parallel, then it's just the same as calculating the VFE for each individually and adding them together.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2788.93,
    "end": 2791.131,
    "text": "So that should, that's certainly what we'd like to happen.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2792.212,
    "end": 2794.393,
    "text": "And it just follows from the properties of these log boxes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2796.027,
    "end": 2802.651,
    "text": " More interestingly, there's the second way in which it's compositional, which is the sequential mode of plugging models together.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2802.811,
    "end": 2811.796,
    "text": "So if we have an open model, M1, and some inputs into some outputs, O1, but those are now actually the inputs for the second model, and we have these running.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2812.477,
    "end": 2815.559,
    "text": "So the first generated model is passing stuff up to the second one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2816.799,
    "end": 2822.803,
    "text": "And now we want to calculate that result, BFE, in terms of an observation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2824.103,
    "end": 2827.106,
    "text": " We can again write it as a sum of two of them, but in a slightly different way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2827.226,
    "end": 2830.008,
    "text": "So observation is just existing on the top wire, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2830.028,
    "end": 2833.311,
    "text": "Because it's just the output of the whole thing that gets this observation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2833.351,
    "end": 2834.311,
    "text": "So it's just on O2.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2835.412,
    "end": 2839.115,
    "text": "So first we calculate the VFE for this model at the top, M2, in the usual way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2840.416,
    "end": 2845.681,
    "text": "And then we add on a VFE calculated for the first model, but it doesn't really have an observation O1, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2847.216,
    "end": 2850.578,
    "text": " observation it uses is one that's being passed down from M2.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2850.858,
    "end": 2857.503,
    "text": "So that's the queue that M2 is using is passed down now as if it's an observation down to M1.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2857.523,
    "end": 2865.388,
    "text": "So it's kind of like O2 receives this observation, does its updating about queue or whatever, and passes that down to M1.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2867.169,
    "end": 2877.036,
    "text": "So in this way, we can say that the VFE composes in that, okay, both of these are minimizing VFE locally, where for the M1 model, we mean it's minimizing",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2890.1,
    "end": 2913.605,
    "text": " i talked about a lot of stuff now i'm just going to wrap up now and then we can get to a discussion i hope um so the main takeaway was just meant to be to try and show that these string diagrams provide some natural language for talking about active inference um and i would encourage you to try uh anyone working on that for me to take a look and see if they would be useful to you in some way and in particular i focused on some of the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2914.264,
    "end": 2915.925,
    "text": " what I was calling the main ingredients of active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2915.965,
    "end": 2919.527,
    "text": "So that were generative models, the way you update them and free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2919.727,
    "end": 2923.93,
    "text": "And we saw sort of ways you can describe all of those notions in the string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2924.39,
    "end": 2932.395,
    "text": "And the thing that I think is useful about them is that they give you a nice representational language for just drawing pictures of your generative models and composing them like hierarchical models and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2933.475,
    "end": 2936.197,
    "text": "They also let you do the reasoning because you can do probability theory with them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2936.737,
    "end": 2941.08,
    "text": "So you can actually reason about what's going on in active inference just with the diagrams themselves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2943.285,
    "end": 2944.626,
    "text": " There's loads of directions you can take in the future.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2944.646,
    "end": 2949.468,
    "text": "Obviously, you can keep absorbing more of the work that's out there and active inference into the diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2950.008,
    "end": 2958.551,
    "text": "A bit more interestingly, we introduced this new notion at the end of how to make free energy compositional.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2958.571,
    "end": 2964.173,
    "text": "In particular, we gave this definition of VFE for an open system now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2964.193,
    "end": 2966.274,
    "text": "So it has a generative model which can have inputs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2966.314,
    "end": 2967.494,
    "text": "We call this the open VFE.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2967.994,
    "end": 2971.916,
    "text": "I just think it's interesting what people think of this definition we introduced, however it seems meaningful.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2973.427,
    "end": 2978.231,
    "text": " Secondly, well, throughout the topic I kept talking about just minimizing free energy and that's all I said, or really the VFE.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2978.871,
    "end": 2979.912,
    "text": "I didn't say how you do it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2980.012,
    "end": 2985.136,
    "text": "So, in fact, this is normally done with these various algorithms of message passing algorithms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2985.637,
    "end": 2988.539,
    "text": "So they're an important part of active inference as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2988.639,
    "end": 2994.184,
    "text": "And I think it would be great to include these in the setup by having some diagrammatic story of them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2996.325,
    "end": 2997.546,
    "text": "There's lots of other questions around.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2997.586,
    "end": 2998.347,
    "text": "So one of them is that,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2999.326,
    "end": 3007.028,
    "text": " I talked about these two notions of updating with respect to soft observations, and I think normally people tend to focus on sharp observations, so they perhaps haven't.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3007.989,
    "end": 3014.811,
    "text": "Not everyone has heard of these before, but it's very natural to treat the soft ones when you're working in this compositional setup.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3015.711,
    "end": 3021.493,
    "text": "And so there you start to wonder about which of these, the Pearl style updating or the Jeffery style updating is more natural.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3026.633,
    "end": 3028.514,
    "text": " updating is the one you should be thinking about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3028.554,
    "end": 3034.236,
    "text": "That's probably the claim ActiveInference would make, but it'd still be nice to think about how this relates to the other two.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3034.356,
    "end": 3040.258,
    "text": "Is it best thought of as approximating the former or the latter style of sort of precise updating?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3041.838,
    "end": 3045.86,
    "text": "And then finally we could try and connect this up to lots of further topics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3045.9,
    "end": 3055.603,
    "text": "I mentioned at Continuum I'm interested in this notion of compositional intelligence, so it would be nice to connect this now to topics in AI and so on, and think about",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3061.346,
    "end": 3081.51,
    "text": " whole world of categorical cybernetics I mentioned at the beginning and I'd like to connect this a bit more precisely with what people are doing there with their stories in terms of lenses and so on and something else we were also interested in that I mentioned is that we got into the topic by thinking about consciousness and there's lots of ways as a major theory of cognition there's",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3094.138,
    "end": 3095.519,
    "text": " So that's something we'd love to do in future.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3095.539,
    "end": 3101.823,
    "text": "And I would just like to say thanks again to all of you for listening and I'd love to go to a discussion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3101.843,
    "end": 3102.163,
    "text": "Thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3107.866,
    "end": 3110.368,
    "text": "Thank you, Sean, for the wonderful presentation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3111.668,
    "end": 3111.988,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3112.729,
    "end": 3119.533,
    "text": "I will first pass to Ali for an opening remark, please.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3122.479,
    "end": 3123.42,
    "text": " Thank you, Daniel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3124.882,
    "end": 3128.585,
    "text": "Thanks so much, Sean, for your really fascinating presentation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3128.606,
    "end": 3130.668,
    "text": "I truly enjoyed it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3131.589,
    "end": 3133.51,
    "text": "So I have a number of questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3133.871,
    "end": 3138.736,
    "text": "So let me begin by asking the first one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3138.756,
    "end": 3140.538,
    "text": "So, you know,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3141.578,
    "end": 3167.198,
    "text": " When Bob Kirke and others took Hamiltonian formulation of quantum mechanics and kind of turned it into the string diagram formulation of it, namely ZX calculus, the claim was that, I mean, regardless of its possible verity, but the claim was that one of the advantages of looking at quantum mechanics",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3168.005,
    "end": 3178.491,
    "text": " in terms of string diagrams, is more than just a convenient way of looking at quantum formulation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3178.951,
    "end": 3189.477,
    "text": "And it actually unveils some properties of quantum mechanics that would be extremely difficult to see with Hamiltonian formulation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3190.018,
    "end": 3196.101,
    "text": "And even in some of their papers, they claim one of the reasons for...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3197.882,
    "end": 3213.177,
    "text": " Somehow the stagnant development in quantum technologies and quantum theory is exactly related to the difficulty of working with Hamiltonian formulation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3213.257,
    "end": 3218.201,
    "text": "So would you say string diagram formulation of active inference kind of...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3219.697,
    "end": 3248.187,
    "text": " takes a similar approach to somehow providing more than just handy tool for representing active inference modeling and actually it kind of opens up new possibilities for further developments of active inference theory possibilities which would somehow I don't know impossible or at least extremely difficult for the current",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3248.787,
    "end": 3255.111,
    "text": " traditional formulation of active inference to see in the current formulation of active inference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3256.993,
    "end": 3257.313,
    "text": "Yeah, thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3257.333,
    "end": 3258.453,
    "text": "That's an amazing question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3258.493,
    "end": 3259.974,
    "text": "Yeah, I would agree.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3259.994,
    "end": 3268.36,
    "text": "I think my default work also is actually in this categorical quantum mechanics area I talked about, so string diagrams for quantum theory and everything.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3269.02,
    "end": 3272.783,
    "text": "And I agree that that language helps you talk about a lot of things",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3274.142,
    "end": 3285.449,
    "text": " that you would maybe never get around to so much in other mathematical formulations of quantum theory, basically things that make use of the tensor as it were, the composition a lot.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3285.549,
    "end": 3292.254,
    "text": "So if this led to stagnation in quantum theory, it's probably because people weren't focusing as much on the tensor and entanglement and stuff, which",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3299.926,
    "end": 3304.971,
    "text": " So now what people are doing with quantum theories includes quantum computing, where they're drawing these circuits.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3305.051,
    "end": 3305.692,
    "text": "And so you're drawing...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3337.461,
    "end": 3354.052,
    "text": " to do in quantum computing so i think similarly um if you never use that kind of language you might think of a system often as a fixed thing and not about the way it interacts with other ones so much and that could lead to overlooking all sorts of things so that's true in any area",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3362.987,
    "end": 3370.789,
    "text": " wanting to talk about generative models being composed from pieces and maybe thinking about how the whole brain works in relation to interactions between parts of it and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3371.729,
    "end": 3376.81,
    "text": "So if you never use this kind of compositional view, there is stuff you would miss, I think.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3377.61,
    "end": 3389.172,
    "text": "I think in some sense people weren't as behind already because they already were working kind of compositionally, right, because they're using these Bayesian networks diagrams like the DAGs and the way that normally drawn a very close",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3393.427,
    "end": 3396.207,
    "text": " the equations and the rewriting of the diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3396.968,
    "end": 3421.272,
    "text": "So it's not as far back maybe as quantum theory was in the sense that people are thinking compositionally, but it feels like you just want to go one step further to having a fully compositional language you're working in where you have the advantage now that you can just talk about taking a whole model and plugging it into another one and it has a completely clear formal meaning and so on, which I think is what you want to do in areas like active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3427.065,
    "end": 3427.485,
    "text": " step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3427.766,
    "end": 3433.971,
    "text": "And in terms of new stuff, it lets you do, I think, an example with something like this OpenVFE thing, I guess.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3433.991,
    "end": 3441.999,
    "text": "So it's, if you're just always thinking about just a generative model, meaning without inputs, you might not",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3475.407,
    "end": 3478.09,
    "text": " throwing away some mechanisms to make things be inputs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3479.451,
    "end": 3483.474,
    "text": "But you could miss it, but you really won't once you start thinking categorically.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3488.98,
    "end": 3489.3,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3489.38,
    "end": 3490.881,
    "text": "Olli, please continue if you'd like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3494.865,
    "end": 3495.425,
    "text": "Thanks so much.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3495.485,
    "end": 3498.248,
    "text": "So yeah, my other",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3500.179,
    "end": 3516.593,
    "text": " Perhaps a related question is, I mean, comparing this kind of formulation to this recent formulation of constructor theory in terms of string diagrams or categorical formulation of constructor theory,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3519.094,
    "end": 3531.86,
    "text": " Before going into this question, you see, you mentioned that this project is a part of a larger project for developing collective intelligence, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3532.32,
    "end": 3543.425,
    "text": "So the similar kind of situation happens for constructor theory, in which it is a kind of meta theory that tries to somehow",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3546.894,
    "end": 3566.968,
    "text": " discriminate between the possibilities of physical laws as opposed to counterfactual laws and how physical laws, how there can be a theory that accounts for the emergence of possible physical laws.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3567.668,
    "end": 3574.233,
    "text": "So in this sense, can you, would you say this kind of formulation",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3574.813,
    "end": 3593.82,
    "text": " categorical categorical theoretical formulation or possibly this specific string diagram formulation of active inference or maybe other theories of consciousness it can be seen as a kind of providing a path toward developing",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3594.48,
    "end": 3604.339,
    "text": " a kind of meta theory of consciousness and possibly unifying many different strands of theories of consciousness into",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3606.687,
    "end": 3635.831,
    "text": " I don't know, a holistic picture that can somehow be compared and positively reconciled with one another and ultimately reaching the ultimate theory of consciousness or, I don't know, do you see this line of work, I mean, providing",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3636.852,
    "end": 3659.243,
    "text": " enough evidence for this line of development research or i don't know somehow maybe even not specifically consciousness but unifying the different aspects of cognition intelligence and consciousness all together so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3663.567,
    "end": 3665.148,
    "text": " Yeah, thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3665.208,
    "end": 3668.831,
    "text": "I think, yeah, you keep giving me like ideal selling points.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3669.012,
    "end": 3676.158,
    "text": "So yeah, that's also something I would, I would like to say, you know, I mean, that, that, yeah, I tend to think of it that way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3676.218,
    "end": 3680.281,
    "text": "And that, you know, my background is in applying category theory to just lots of topics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3680.541,
    "end": 3684.845,
    "text": "And I so naturally do think of it as quite a unifying language.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3685.185,
    "end": 3690.009,
    "text": "And you know, the the ground and consciousness that I mentioned, was building on earlier what we did on looking",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3705.12,
    "end": 3720.494,
    "text": " there it's like we've taken both these things and put them in this common language you could have put them in a common language of probability theory before but I think you know I do have an intuition that there is something more clear about it doesn't make it easier to get a conceptual grasp of both theories I think once you've done it this way and",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3739.04,
    "end": 3742.461,
    "text": " I've worked on creating that in terms of diagrams and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3742.901,
    "end": 3748.323,
    "text": "So I would love to see basically many theories put into this language to make it easier to compare them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3749.244,
    "end": 3754.325,
    "text": "You could try and compare them directly already, but I think you want one clear formalization to put them all in.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3754.625,
    "end": 3761.728,
    "text": "And I would say that the categories and the diagrams is the right one to pick because it tends to just give a very clear",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3772.995,
    "end": 3798.957,
    "text": " active inference and and iit so far it seemed very natural because on the case of iit it's about talking about how integrated something is so you basically want to talk about the opposite of that which is something being decomposed and the diagrams basically talk about parts and how they're related which is what you need to make sense of that nation of integration so it's very natural there but yeah i would love basically to see um various aspects of cognitive science um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3806.738,
    "end": 3811.999,
    "text": " you know, the hope would be then to try and gain insights from all of them, you know, and build a theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3812.039,
    "end": 3816.7,
    "text": "It's not the category theory itself is a theory of cognition or consciousness.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3816.72,
    "end": 3819.561,
    "text": "It's just a very useful language for relating them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3819.721,
    "end": 3826.623,
    "text": "And then it would be very exciting to see something, you know, natively defined in another category theory as well at the end.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3827.803,
    "end": 3828.623,
    "text": "And there's a feeling that some of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3855.777,
    "end": 3856.138,
    "text": " Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3856.398,
    "end": 3860.322,
    "text": "Well, yeah, I have many.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3860.422,
    "end": 3860.783,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3861.523,
    "end": 3862.384,
    "text": "These are great questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3862.925,
    "end": 3863.966,
    "text": "There's like ideal questions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3867.105,
    "end": 3891.059,
    "text": " You've pointed in and we've explored a little bit of the utility and the simplicity and how that could help with accessibility and rigor and applicability, all these awesome things leading to reaccounting and reframing, consolidating, as well as discovering some new trails between, for example, expected free energy and variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3892.02,
    "end": 3895.162,
    "text": "Looking at the equations, you might be able to say that they rhyme.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3896.141,
    "end": 3906.708,
    "text": " but you would be many, many lines deep into understanding what, if any, generalizations could encompass the both of them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3907.489,
    "end": 3910.831,
    "text": "So that was just a very salient example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3910.851,
    "end": 3915.655,
    "text": "A few different kinds of questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3916.295,
    "end": 3924.241,
    "text": "So how's time treated in category theory or how does active inference treat time today?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3925.325,
    "end": 3947.917,
    "text": " how do you see the way that time is treated we talk about discrete time and continuous time generative models then there's the past present future multi-agent systems federated or asynchronous communication so how is time treated and how does that give us a different grasp on dynamical modeling i think that's a really",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3949.398,
    "end": 3951.019,
    "text": " I'd love to have a better answer for that, basically.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3951.04,
    "end": 3952.14,
    "text": "I think it's a tough one, you know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3952.16,
    "end": 3954.662,
    "text": "So at the moment, yeah, in the talk set, I just talked about discrete time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3955.283,
    "end": 3967.232,
    "text": "And that's sort of very easy to treat with the Bayesian network setup and with these kind of string diagrams, because you can just lay out the discrete time steps as processes in your picture, like we see here, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3967.252,
    "end": 3969.374,
    "text": "We have the end time steps here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3970.215,
    "end": 3976.6,
    "text": "But I don't have anything satisfying worked out yet to say about how you would treat",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3977.779,
    "end": 3980.461,
    "text": " a continuous time case, which I think is important in active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3980.501,
    "end": 3994.55,
    "text": "You'd like to basically take, I guess basically what you want to do is take the way that you describe this thing with the end time steps and kind of have a formula for folding it together and just say, okay, but you're unpacking this thing N times.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3995.411,
    "end": 4007.139,
    "text": "And then you can take that thing and imagine, you know, this abstract view of unpacking it, just not discreetly anymore in this continuous way so that you can capture something like the differential equation kind of definition of continuous",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4015.036,
    "end": 4024.421,
    "text": " Yeah, so you can certainly work with continuous time things in the sense of, you know, the stuff going on in categorical cybernetics or sort of categorical systems theory, I guess it would be called.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4024.441,
    "end": 4031.064,
    "text": "The ACT world is kind of, you know, it has continuous time dynamical systems and talks about plugging them together.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4031.644,
    "end": 4035.886,
    "text": "But that diagram is sort of just relating their variables, is my understanding.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4035.946,
    "end": 4038.808,
    "text": "It's not like a diagram isn't exactly showing",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4061.258,
    "end": 4071.814,
    "text": " into account they would be really nice to see so it just needs the right abstraction I think for yeah taking a picture like this not drawing the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4091.871,
    "end": 4092.752,
    "text": " for each time step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4094.953,
    "end": 4099.375,
    "text": "In general, I wouldn't say there's an answer to the question of how is time treated in category theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4099.395,
    "end": 4102.216,
    "text": "There wouldn't really be one answer because category is going to be so generally.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4103.457,
    "end": 4112.781,
    "text": "They tend to be very effective for discrete things in general, like algebra and so on, because they kind of are discrete in some sense, like the composition is discrete.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4125.757,
    "end": 4145.88,
    "text": " when you're composing continuously um but yeah i think there will be people in act who have sort of would come up come at you with this particular answer so they've they've got a way they like to treat continuous time um that i'm just not familiar with yet yeah cool",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4147.342,
    "end": 4150.545,
    "text": " A little bit of a more educational or applied question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4151.406,
    "end": 4155.429,
    "text": "So how do we go about drawing and learning to draw?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4155.569,
    "end": 4157.311,
    "text": "Is there a software package?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4157.731,
    "end": 4167.34,
    "text": "Is there a way that we can get a step-by-step process to building that familiarity with like, when I see this shape, then here's what I know.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4167.36,
    "end": 4170.262,
    "text": "And then how do we know what we can and can't do?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4171.343,
    "end": 4177.907,
    "text": " And does that drawing software flag us or do we need to send it to a friend?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4178.308,
    "end": 4194.659,
    "text": "So how do we look at something and then part one, build up the motifs in our own aesthetic understanding so that we can understand the compositionality of this as you do today and as we all do today, for example, for language, English.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4195.629,
    "end": 4206.476,
    "text": " And then part two, how do we go from having built that motif-based compositional understanding to like, now what can we do?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4206.696,
    "end": 4212.54,
    "text": "And then when are we just totally freewheeling and off the rails with a free energy principle?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4212.66,
    "end": 4215.261,
    "text": "Or like, does anything go if the motifs allow it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4217.162,
    "end": 4219.684,
    "text": "Yeah, I wish I should have the standard answer.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4230.299,
    "end": 4238.805,
    "text": " I think it tends to be because, yeah, if you want to get really comfortable with the diagrams, you're learning category theory in some sense, but it's not like you need to learn all of category theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4238.825,
    "end": 4244.608,
    "text": "It's kind of a relatively modern offshoot in this applied category theory world that's very diagrammatically focused.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4245.529,
    "end": 4249.512,
    "text": "And there will be various nice introductions out there to using them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4251.153,
    "end": 4254.715,
    "text": "Another way is to, so I'm trying to think.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4255.441,
    "end": 4257.844,
    "text": " I'm pretty sure recently, yeah, there was a nice paper that came out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4257.864,
    "end": 4260.847,
    "text": "It was an introduction to string diagrams for computer scientists, for example.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4260.887,
    "end": 4271.378,
    "text": "So there tends to be different introductions kind of for different audiences because they just want to pick categories that those people are familiar with.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4271.518,
    "end": 4272.86,
    "text": "So they can actually have some examples.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4273.2,
    "end": 4276.484,
    "text": "You could just learn the diagrams totally abstractly, but it helps to have some examples.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4292.822,
    "end": 4320.803,
    "text": " science one i know there's some work going on in producing one for cognitive science which i think would be really good um having an introduction yeah to the string diagrams for those people um so you basically look for one in an area you're comfortable with and you should find a good paper on it but um it would be nice to have a good online resource i guess right that gathers these together so people can can just see a great guide for the all the introductions if you do something like um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4322.156,
    "end": 4327.078,
    "text": " Yeah, there's courses you can do in the sense of the Bob's book.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4327.379,
    "end": 4332.301,
    "text": "In the case of learning quantum, there's something like Bob's long book with Alex Kissinger, Picturing Quantum Processes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4333.021,
    "end": 4338.144,
    "text": "That's the kind of thing I learned from, like it was in the form of a lecture course, but it's basically the same book.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4338.804,
    "end": 4342.546,
    "text": "So because then there's just loads of exercises that will make you have to reason with string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4342.666,
    "end": 4343.686,
    "text": "And then you pick the rules up.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4344.167,
    "end": 4348.809,
    "text": "Because at first it's, yeah, you don't have the same intuition, obviously, for the rules.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4348.829,
    "end": 4349.689,
    "text": "What can I do with these?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4349.749,
    "end": 4351.41,
    "text": "Can I slide them around like this or whatever?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4366.597,
    "end": 4395.69,
    "text": " um i didn't use any like software in the sense of the diagrams i draw on this program called tixit but it doesn't like tell you how string diagrams will work or anything it's just for drawing them um but i know there's more work to develop you know libraries like the algebraic julia project is sort of like an applied category theory um language but i wouldn't know if it was recommended as a way to first learn categories and yeah",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4403.541,
    "end": 4406.682,
    "text": " For causal models, there's this paper Robin and I put out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4407.202,
    "end": 4420.905,
    "text": "It's not necessarily the very first place to learn string diagrams, but the aim is to introduce to people who've heard of causal models, say, in a sense of Perl, so just Bayesian networks, basically, but maybe the causal interpretation of them to get them used to string diagrams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4421.805,
    "end": 4425.646,
    "text": "And this paper hopes to be a little bit introductory as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4428.507,
    "end": 4428.827,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4429.507,
    "end": 4430.027,
    "text": "Ali, please.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4433.969,
    "end": 4434.349,
    "text": " Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4434.389,
    "end": 4459.122,
    "text": "So getting back to the question about the time representation in this formulation, so I take it that this kind of formulation of Bayesian inference, I mean category theoretical formulation of Bayesian inference, is largely based on Tobias Fritz's definition of Markov categories as CV categories, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4459.903,
    "end": 4460.083,
    "text": "So",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4460.851,
    "end": 4476.776,
    "text": " Uh, as far as I understand it, uh, Fritz paper, uh, kind of, uh, one of its basic assumptions is, uh, this kind of unidirectional, um, inference, uh, I mean, from earlier times to later times, right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4477.917,
    "end": 4487.82,
    "text": "Uh, or in other words, the prediction, uh, but, uh, in, uh, quantum formulation of active inference or, uh,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4489.718,
    "end": 4500.243,
    "text": " quantum active inference, there's this attempt to also develop the retrodiction aspect of inference as well, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4501.104,
    "end": 4508.027,
    "text": "So would you say this recent formulation can also be accounted for",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4510.228,
    "end": 4519.154,
    "text": " For this kind of retrodiction, in other words, can this formulation be reconciled with quantum Bayesianism as well?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4519.174,
    "end": 4520.695,
    "text": "Hmm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4521.482,
    "end": 4523.863,
    "text": " Yeah, I basically wish... Yeah, sorry, go ahead.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4523.903,
    "end": 4551.117,
    "text": "Yeah, because to add one more context here, I think it was in Kirk and Specken's paper, there was this clear distinction between classical Bayesian inference and non-classical Bayesian inference, which the classical one does not, I mean, allow for the retrodiction, but non-classical Bayesian inference",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4551.997,
    "end": 4556.86,
    "text": " can be applied for both prediction and retrediction as well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4558.501,
    "end": 4558.982,
    "text": "Oh, okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4559.642,
    "end": 4566.226,
    "text": "Yeah, I would love to be a bit more familiar with the quantum active inference stuff, basically, to compare a bit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4566.366,
    "end": 4573.81,
    "text": "So I'm not as familiar with the sort of retro... Sorry, what was the other version of prediction?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4573.87,
    "end": 4576.252,
    "text": "It's retro... Retrediction, yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4576.352,
    "end": 4577.973,
    "text": "Prediction and retrediction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4577.993,
    "end": 4578.093,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4589.363,
    "end": 4592.226,
    "text": " to see what they say there about the classical one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4593.787,
    "end": 4599.653,
    "text": "Can you give some intuition as to why it isn't something you can do classically, basically, the retro one?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4599.733,
    "end": 4605.279,
    "text": "Because if that's the general case of probabilities, then it will be true in some sense here, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4619.927,
    "end": 4642.771,
    "text": " um yeah yeah yeah so so um i mean the whole idea was that uh for predictive quantum mechanics we only need to account for the inference from earlier times later times but uh if we want to account for retrospective quantum mechanics as well we need to somehow",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4643.331,
    "end": 4665.062,
    "text": " uh account for because as we know um not every um i mean quantum formulation uh follows the bell's principle of local causality uh so that's uh i mean in order to account for all the entanglement phenomena and so on and we need to somehow uh",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4666.323,
    "end": 4670.764,
    "text": " put this bidirectional inference into our model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4671.684,
    "end": 4678.166,
    "text": "So yeah, that was the basic idea behind developing this kind of non-classical Bayesian inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4679.707,
    "end": 4685.048,
    "text": "Does it have something to do with the unitary evolution in content theory, the way that it has this reversible thing, or is it...?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4686.148,
    "end": 4686.568,
    "text": "Exactly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4686.588,
    "end": 4687.269,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4687.429,
    "end": 4687.969,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4688.749,
    "end": 4691.21,
    "text": "Yeah, that was the gist of it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4692.45,
    "end": 4692.67,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4697.526,
    "end": 4698.447,
    "text": " reversible thing built in.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4698.487,
    "end": 4712.854,
    "text": "Well, yeah, so I wouldn't expect to see that exact feature here in the sense of, you know, if it's treated, if it's basically something that you can't have in classical probabilities, it's not going to, it won't exist in this category met R plus.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4713.215,
    "end": 4716.256,
    "text": "I think that would be basically the same category they would use in that paper, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4716.276,
    "end": 4723.26,
    "text": "And they work with dagger combat categories and they'll work with something like this met R plus category for the classical case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4739.352,
    "end": 4750.582,
    "text": " thing, but the sort of lens type view of what's going on that's more studied in categorical cybernetics would be imagining, I think, the model kind of carrying this backward inference process with it as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4751.382,
    "end": 4760.33,
    "text": "So there it's, you know, for each forward part of the model, you would have this approximate inference sort of channel stored with it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4761.651,
    "end": 4766.075,
    "text": "So I don't know if that would address what you're after, but it would have a backward and forward part together.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4770.339,
    "end": 4794.823,
    "text": " well ollie do you have any kind of closing slash opening remarks or questions or where where do you see this going from the active inference side what does this bring to us and what is opened through what has happened largely this year in active inference and category theory",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4796.786,
    "end": 4802.093,
    "text": " Well, actually, I'm really excited to see this line of development and active inference theory.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4802.293,
    "end": 4811.866,
    "text": "And as you know, I'm a big, big fan of meta theories and all kinds of unification theories and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4814.469,
    "end": 4839.866,
    "text": " it's, I don't know, I kind of have this feeling of this hunch that this line of development in active inference theory is, I mean, it looks quite promising, especially for kind of tying up all the loose ends and transcending and many, many",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4840.602,
    "end": 4865.862,
    "text": " other areas and discourses and ultimately reaching a kind of coherent picture of quote-unquote reality whatever it means so yeah these kinds of development I mean the last year we had tremendous advances in Bayesian mechanical theories and",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4866.923,
    "end": 4876.707,
    "text": " In recent months, we have this fabulous line of research in category theoretical account of active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4877.387,
    "end": 4888.791,
    "text": "My hope is that ultimately these different strands can be unified into, as I said, a coherent and overarching framework.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4889.271,
    "end": 4890.812,
    "text": "So exciting times.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4894.602,
    "end": 4909.875,
    "text": " it sounds like so do you mean that you're thinking of it as um it sounded like you're basically alluding to the thing going what going on in cognition and what going on in physics coming together right like one really really meta",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4913.449,
    "end": 4933.775,
    "text": " The idea behind Bayesian mechanics, one of its premises or assertions was that there isn't any clear distinction between cognitive and non-cognitive things or agents and they rest on a continuum.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4935.575,
    "end": 4940.197,
    "text": "The same kind of mathematical technology can be applied both for inert and",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4941.481,
    "end": 4946.723,
    "text": " conscious agents or sentient agents or whatever we choose to call them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4947.543,
    "end": 4960.627,
    "text": "So, yeah, this overarching theory, I mean, unveiled many interesting phenomena regarding, well, self-organizing systems and",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4963.343,
    "end": 4975.629,
    "text": " It changed the whole perspective about how we can look at and define, even define consciousness, cognition, intelligence, sentience, and all of these related terms.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4976.55,
    "end": 4981.152,
    "text": "So my hope is that category theoretical thinking",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4981.232,
    "end": 5004.697,
    "text": " account of active inference can also be used for clearly seeing many of these emerging elements in Bayesian mechanics and active inference theory, and hopefully, well, gaining some interesting and potentially groundbreaking insights.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5005.457,
    "end": 5007.718,
    "text": "That'd be wonderful.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5007.738,
    "end": 5007.918,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5016.394,
    "end": 5016.715,
    "text": " Oh, yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5017.195,
    "end": 5019.638,
    "text": "I'll just give my closing thoughts then to you, Sean.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5019.698,
    "end": 5023.282,
    "text": "Just a few loose notes that, again, open probably more than they close.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5024.704,
    "end": 5032.393,
    "text": "Ali was right in suggesting and expressing that Bayesian mechanics recently has helped us",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5033.374,
    "end": 5055.928,
    "text": " develop a continuum of active and passive systems so-called living and non-living or inanimate and animate and that brings us to another dialectic to resolve which is life and mind which is where the physical and the cognitive science come together you said they're on a continuum maybe we could say they're on a quantinium and what language could express",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5057.845,
    "end": 5076.541,
    "text": " such work well right now we're speaking in english with the active inference ontology dialect however the phonemes are not intrinsically meaningful the um in a markov blanket or category does not mean something it's it's a sound and so the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5078.061,
    "end": 5096.452,
    "text": " string diagram language and representation i see as a way to fuse and integrate semantics into the syntax of the actual inscription which enables us to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5097.955,
    "end": 5102.938,
    "text": " generalize in new ways, also recognizing string diagrams are not everything and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5103.979,
    "end": 5116.907,
    "text": "And then with all of these intersecting vectors from the cognitive and the physical sciences, we are able to take the compositional cartographic approach",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5117.895,
    "end": 5129.163,
    "text": " for cognitive ecosystems and talk about diverse intelligences, biological, quantum, classical architectures, all of these synthetic intelligences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5129.943,
    "end": 5136.708,
    "text": "And so it's super exciting and I appreciate again your visit and look forward to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5138.035,
    "end": 5158.544,
    "text": " people's curiosity, taking them and also the development of tools and educational materials that make this easier and then being able to display and use something where the meaning is primal rather than like, well, this letter represents this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5160.505,
    "end": 5166.187,
    "text": "It already introduces such a space between the analytical representation and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5168.737,
    "end": 5196.461,
    "text": " really the string diagram which exists isomorphically with it uh yeah this is very exciting way of thinking yeah it sounds like you're you're um advocating a kind of structural ontology kind of thing in some sense right with the you're taking the compositional structure what's going on to really be the meaning or really be the the real thing that's there not just like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5197.58,
    "end": 5198.261,
    "text": " I don't know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5198.561,
    "end": 5199.782,
    "text": "We could talk about it for a while, I imagine.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5200.803,
    "end": 5209.15,
    "text": "But I would love to see string diagrams and other approaches, I'm sure, that take that role.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5209.511,
    "end": 5212.273,
    "text": "And you've got me very excited about this kind of unification that's going on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5212.293,
    "end": 5215.636,
    "text": "Thank you again, Sean.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5215.836,
    "end": 5216.797,
    "text": "You're always welcome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5216.897,
    "end": 5219.019,
    "text": "And we look forward to seeing where this all goes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5220.3,
    "end": 5221.922,
    "text": "Thanks again for having me.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5222.102,
    "end": 5222.843,
    "text": "Really great discussion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5223.544,
    "end": 5223.926,
    "text": " Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5225.11,
    "end": 5225.732,
    "text": "Thank you so much.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5225.813,
    "end": 5226.094,
    "text": "All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5226.134,
    "end": 5226.475,
    "text": "Thanks.",
    "speaker": "SPEAKER_01"
  }
]
start	end	speaker	sentiment	confidence	text
4770	5320	A	0.546256959438324	You.
7690	9480	A	0.9103710651397705	Hello and welcome, everyone.
9850	12598	A	0.9154595136642456	It is September 1, 2023.
12764	19042	A	0.8847141265869141	We're here in active inference math stream number 6.1 here with Sean Toll.
19186	25650	A	0.8627766370773315	We'll be hearing a presentation, active inference in string diagrams followed by a discussion.
25810	27250	A	0.9895300269126892	This is super exciting.
27330	32430	A	0.7097127437591553	So if you're watching Live, please feel feel free to write your questions in the Live chat.
33170	34446	A	0.9808918833732605	Really looking forward to this.
34468	38510	A	0.9813417196273804	So thank you, Sean, again for joining and to you for the presentation.
40130	41166	B	0.9063246250152588	All right, thanks very much.
41188	41934	B	0.9078590869903564	Thanks everyone who's watching.
41972	46510	B	0.9629644751548767	And thanks to the organizers for this chance to speak to you and to Daniel for getting in touch and inviting me to speak.
46580	55940	B	0.9748076796531677	So, yeah, I'm really excited to share this work with this community, basically, and to hear from those people who work with active inference and do any formal work, what they think of what I'll present today.
56470	65378	B	0.8118044137954712	So I'm going to be presenting a formal approach to how you can describe active inference in terms of an entirely graphical language called the language of string diagrams.
65394	67618	B	0.8476113080978394	And it's based on this mathematics called category theory.
67714	72694	B	0.7923967838287354	And I won't assume that you're too familiar with this already and try and introduce it to you in the talk.
72812	81434	B	0.903985321521759	And ultimately, I'd like to sort of convince you that this diagrammatic language will be really useful for those of you who work formally with active inference and encourage you to pick it up in your own work.
81632	83406	B	0.8201389908790588	I've just introduced myself a bit.
83428	84074	B	0.7953657507896423	I'm Sean Tull.
84122	101346	B	0.8083322644233704	I'm a researcher at Continuum, formerly a postdoc in computer science in Oxford, and a Continuum in this Oxford team where I'm based, where we study what we call compositional intelligence, which includes applying category theory to topics in AI and as well as this.
101368	104158	B	0.7781046032905579	The project was supported by a grant from FQXi.
104254	109030	B	0.9093236327171326	It's located at the bottom and hosted at Topos Institute, which is the center for Applied Category Theory.
110330	111734	B	0.8178606033325195	So let me get think.
111772	114870	B	0.6006295084953308	So, yeah, here we go.
114940	118358	B	0.7858172059059143	So for active inference, won't spend too much time introducing it.
118364	122940	B	0.6894254684448242	I'll assume most people here are familiar with it, and many of you probably know more about it than I do, in fact.
124350	128266	B	0.8933018445968628	So I'll just mention the parts of it that I'll be addressing in the talk.
128368	136030	B	0.8034128546714783	So thinking of it as a model of cognition that simply we can think of as applying at many levels, say from a whole organism or just to a single neuron.
136530	150290	B	0.8496161699295044	And the key idea is that in this approach, you think of an agent that's coming with this generative model that it uses to explain the observations it receives from the world in terms of some hidden states, which you might call perception, and in terms of its own actions.
150710	160774	B	0.7402136921882629	And in active inference, it achieves both of these things through this form of Bayesian inference or an approximate form of Bayesian inference by minimizing this quantity called free energy.
160892	163670	B	0.8451136350631714	And these are the ingredients we'll be talking about in the talk.
163740	176118	B	0.9648358225822449	And the thing that's really exciting about active inference, I think, for those of a formal background as well, is that it aims to offer like a very principled approach to cognition that you can hopefully apply at all these many levels.
176294	180300	B	0.5832719802856445	But I think at the moment it could also benefit from more formal work.
180830	182042	B	0.6198933124542236	And that's what this talks about.
182096	191726	B	0.5070433616638184	It's about formal approaches to the theory in particular, I think nice clear formatations or active instances would help to clarify sort of what the core of the key ideas of the theory are.
191748	198100	B	0.5649954676628113	So we'd like it to be this very succinct principle that ideally we just apply to a generative model and everything else follows from.
198710	212600	B	0.809028685092926	And once we've got to this, we can hopefully generalize it, understand it better, and also make it just more accessible to those who come from formal backgrounds, like in mathematics and so on, and get them working on the topic very quickly and connect it with approaches in artificial intelligence as well.
213130	220906	B	0.7495320439338684	But the most important thing about a good formalization, I think, should just be to make learning about active inference easier, make the framework simpler to understand.
221008	223500	B	0.5316532254219055	So that's what we're aiming for in this work.
224110	234938	B	0.8712480664253235	And I'd say in other places already there's been some calls that some suggestions that an is formalization of active inference should be a diagrammatic one.
235024	241950	B	0.7252727150917053	So when you look at the generative models that come up in active inference a lot, they're very compositional in their nature and it's very natural to draw them in diagrams.
242610	246834	B	0.70414137840271	So it would be nice if our whole approach to describing it could be graphical in this way.
246952	261894	B	0.7682279348373413	So just for example, there's this paper called The Graphical Brain by Kristen Par, and in general, we've probably seen loads of these diagrams describing generative models where you draw many compositional features of different spaces of hidden states, observations, interacting and so on.
262092	265942	B	0.7716356515884399	So these diagrams are used, but they're just used to represent the model.
265996	272294	B	0.8515194058418274	You still have to then go to doing sort of traditional probability of theory calculations when you reason about them normally.
272422	281718	B	0.7839804291725159	But in fact, there is a whole graphical, formalism and mathematical language for describing these kind of interacting processes just entirely with the diagrams.
281814	297854	B	0.8540816307067871	So the area of mathematics is called category theory and the language are these string diagrams I'm going to talk today, and in particular, there's a lot of work going on in applied category theory now in how you can describe aspects of probability theory and causality and causal models in terms of string diagrams.
297982	300526	B	0.8390976190567017	And these causal models are basically based on Bayesian networks.
300558	304062	B	0.8635751008987427	So the same formal structure as the generative models in active inference.
304206	313106	B	0.8750573992729187	And in particular, what I'll talk about today kind of draws on this paper co authored with Robin Lorenz, talking about causal models in the sense of pearl in terms of these string diagrams.
313138	320140	B	0.8906233906745911	So it's basically causal Bayesian networks, which is the same formal structure as we'll be talking about today.
320750	332618	B	0.8316730856895447	So in this talk I'm talking about this paper, which was joint work with Johannes Kleiner and Ive, which is called Active Inference in String Diagrams, a categorical account of processing and free energy.
332704	339322	B	0.6118512749671936	And basically what we do is try to give a formalization of active inference that's nice and clear conceptually just entirely in terms of string diagrams.
339386	345490	B	0.873335063457489	So we're basically taking the kind of formal content that's in something like the active inference book and turning it into these diagrams.
347190	356830	B	0.8327645659446716	And as I mentioned, it was done as part of this FQXi project that was actually on a project about consciousness as about ways that category theory can be applied to theories of consciousness.
356910	364950	B	0.7978368401527405	And we've done some previous work looking at the integrated information theory of consciousness and of course there's all sorts of ways that active inference has been proposed to connect to consciousness.
366010	367958	B	0.7799292206764221	But for the purpose of this talk, we won't go into any of that.
367964	369954	B	0.7090989351272583	It's just a theory of cognition.
370002	375340	B	0.7580353021621704	I take it to be here and I'll just mention at the end it'd be nice to connect it back up to consciousness in future.
375710	386880	B	0.8006744980812073	And there's also lots of other related work going on in category theory that's very close to this, that often goes by the name of categorical cybernetics, and this might include some of the things Toby has talked about on the stream in the past.
387250	395746	B	0.887000560760498	So what I'll do now is introduce categories and string diagrams and then later we'll apply them to all these basic ingredients of active inference that I've alluded to so far.
395768	401060	B	0.696172833442688	So that would be generative models updating them, free energy and active inference itself.
402710	405730	B	0.8736307621002197	Let's start with these categories and string diagrams.
406550	412018	B	0.8707505464553833	So you can think of a category in general as a sort of world of interacting processes.
412114	415698	B	0.7815394401550293	And the categories we're talking about here are always going to be these symmetric minoidal categories.
415794	421260	B	0.7764834761619568	But don't worry too much about the formal language because the way we talk about them is just going to come down to the diagrams here today.
421630	431898	B	0.8862810730934143	So a category amounts to a collection of these objects, or sometimes called systems, like this Capital ABC here, and what are called morphisms, or you might want to typically call processes between them.
431984	437662	B	0.8500567078590393	So when you're writing normally, you can just write a morphism from A to B as like this f colon A to B.
437796	442942	B	0.8770496249198914	In string diagrams, though, you draw it like this, where we're reading all the diagrams from bottom to top in this talk.
443076	448514	B	0.8754751086235046	So you have a wire for the A input at the bottom and a wire for the B output at the top.
448552	459960	B	0.8665956854820251	And the morphism is just drawn as a box F here and you just read the diagram up thinking about, okay, it takes in this input coming in on this wire, a process applies and then you have your output of type B at the top.
461610	464102	B	0.8625261187553406	So what you can do with these processes is compose them.
464156	470998	B	0.8648234009742737	So if you have two processes, say your F from A to B and G from B to C, so the types line up, you can just compose them in sequence.
471094	474630	B	0.744720995426178	And this just means plugging the boxes together in your diagrams.
474790	478614	B	0.699846625328064	And because we're in this Minoidal category, you can also compose in parallel.
478742	484054	B	0.8036080598831177	So you have this operation called the tensor, which means given two objects, you can put them together to build this composite object.
484112	490480	B	0.8809406757354736	So AB goes to a tensor B, you'd say, and you can also do this to morphisms, so you can build this F tensor G.
490850	493086	B	0.8381728529930115	But in the pictures it just means drawing them side by side.
493108	499950	B	0.6929049491882324	And you just think of this as meaning we have F from A to C and G from B to D, and they're just running in parallel and they're not interacting essentially.
500690	504418	B	0.6780264377593994	So most of the time you just draw a picture like this and you don't even need to write the tensor symbols.
504554	511430	B	0.5232204794883728	So if these two basic modes of composition and from these you can build much more elaborate string diagrams in your category.
512650	518770	B	0.7739537358283997	So if you're writing things very mathematically, you have to write lots of equations that a category or Minoidal category needs to satisfy.
518850	523242	B	0.7335629463195801	But when you're working the diagrams, they basically do some of the work for you because these things just come out for free.
523376	530506	B	0.8120520114898682	So, for example, you have equations like this that you'd have to think about when you're working with them in the conventional mathematical way.
530528	534726	B	0.8711321353912354	But in the diagrams it just means if you have two boxes, you can kind of slide them along the wires.
534758	538714	B	0.7374139428138733	It doesn't really matter where they are on the wires, it tends to just be the connectivity that matters.
538842	547646	B	0.639390230178833	Similarly, we can cross wires over each other because we're in the symmetric setting and there's a few just useful features that you'll have in a category.
547678	552962	B	0.625063955783844	So every object comes with this identity morphism, which you just think of as meaning nothing happening basically.
553016	554862	B	0.5689662098884583	So it's just drawn as a blank wire.
555006	561346	B	0.7529195547103882	There's also a kind of identity object, as it were, called a unit object, which is just empty space, so you don't even draw the wire.
561458	563302	B	0.9368464946746826	This dashbox just means nothing.
563356	564950	B	0.6307854652404785	It's meant to be an empty picture.
565450	571110	B	0.7229872941970825	And the latter thing is just useful because it now means we can talk about amorphism, which doesn't even have an input or an output.
571270	577126	B	0.8493407964706421	More formally, it has this object I as it's input by output, and we give these things special names.
577238	584822	B	0.7452441453933716	So the most important one probably is that of a state, which is a morphism with no input, as it were, or really with input I.
584896	586286	B	0.7828769683837891	So in the pictures it just looks like this.
586308	587470	B	0.7302499413490295	So there's no wire going in.
587540	589390	B	0.5191760063171387	And you'd call this a state of A.
589540	594930	B	0.8234719038009644	You can also have a process that takes in a if it has no output that you'd call an effect.
595080	596926	B	0.5794547200202942	And if you have naive, you just call this a scalar.
596958	601570	B	0.8511329293251038	So this is just going to be like a number basically floating around next to your diagram.
603430	606706	B	0.7987411618232727	So there are many categories out there.
606728	608230	B	0.7860819697380066	The point of category three is extremely general.
608300	614726	B	0.8846032023429871	So this could be talking about computational processes or physical processes or quantum processes in particular, and all sorts of things.
614828	617250	B	0.7559168934822083	For this talk we'll only actually need about one category.
617330	619146	B	0.6929320096969604	We'll just keep it simple with this one.
619248	620326	B	0.6760313510894775	That's the category.
620438	621642	B	0.8244381546974182	I call it matar plus.
621696	624010	B	0.5921352505683899	So it's positive real matrices.
624510	632666	B	0.8708050847053528	So you can just take objects for the wires to be finite sets and the morphisms to be positive matrices indexed by these, as I'll explain.
632768	639694	B	0.9033258557319641	So if we draw a box like this, M, going from X to Y, this is like a matrix indexed by X and Y.
639732	650180	B	0.7178395986557007	And for each input in this little set X, sorry, in the set X and each output in Y, you'd get a positive real number and we'll write it like M of Y given X.
650790	654020	B	0.8461304903030396	So this box would mean a function like this.
655270	659874	B	0.7295530438423157	Now, when we plug them together, they let us turn some things that you normally have to do with equations into kind of simpler pictures.
659922	661126	B	0.8127661943435669	I mean, mainly this middle one.
661148	666242	B	0.869587242603302	So if we have two in sequence, we just compose them by matrix multiplication.
666306	672106	B	0.8039822578430176	So instead of having to write this formula where we sum over Y, we can just draw this picture above it where we just plug them on top.
672288	678906	B	0.8826454877853394	And if we run them in parallel here, we take the cartesian product of the sets and the tensor product of the matrices, as it were.
678928	683310	B	0.7336556911468506	But it's just the obvious thing where you have two things running independently.
685010	691406	B	0.5603845119476318	So in particular a state tier ends up the minority unit is just a singleton set and you can basically just ignore it.
691428	698562	B	0.8351338505744934	So a state tier amounts to just a function sending each X to a positive real and affect the same thing.
698616	701346	B	0.5015599131584167	And a scalar would just be a positive real.
701528	707614	B	0.8774763345718384	The intuition though is that we're going to restrict the particular morphisms in here, which are probabilistic in the nature.
707662	710694	B	0.8547793626785278	So they need to send each X to an actual distribution over Y.
710812	714040	B	0.9102277755737305	So I want to talk about how you actually pick those up next.
714970	717958	B	0.8473535776138306	So to do that, you use some extra structure that this category has.
718044	720450	B	0.6627277135848999	It forms what's called a copy discard category.
720530	724282	B	0.8470103740692139	So this is one more bit of mathematical sort of gadgets we have around.
724416	727302	B	0.8258163928985596	So that is that each object comes with these distinguished processes.
727366	735920	B	0.6117570400238037	There's one that we call copy that takes in a say and brings up two copies of A at the top and one called discard where you just throw A away so you have no output at the top.
737970	745122	B	0.5864300727844238	These satisfy some equations that are quite intuitive if you think about like copying and then throwing away one of the outputs is the same as doing nothing.
745176	745438	B	0.6866315603256226	So it's.
745454	749954	B	0.8504810929298401	This blank wire copying is symmetric and associative is the last one.
750072	756100	B	0.7944175601005554	So in this category, mad or plus discard would be just the function sending each element to one.
756550	758206	B	0.7938393950462341	And the copy would be like a delta.
758238	761046	B	0.8947544097900391	So A comes in and two copies of A come out.
761068	762790	B	0.6637689471244812	At the top is the intuition.
763690	772294	B	0.6763479709625244	The reason we introduced this stuff is because it's been shown recently that you can do a lot of probability theory just in terms of these CD categories and particular ones called Markov categories.
772342	782166	B	0.8625456094741821	So there's a lot of what's going on in applied category theory at the moment, using this language of CD categories in particular, they let you pick out some things to do with probability theory.
782198	784014	B	0.8776384592056274	I'll just talk about a couple of them here.
784052	786094	B	0.532585859298706	The most important one is the notion of a channel.
786292	791258	B	0.9025897979736328	So this is what lets us pick out the actual normalized matrices, as it were, from earlier.
791434	795738	B	0.6919060349464417	So in general, you call amorphism a channel when it preserves this discarding.
795914	800446	B	0.5991756916046143	And a special case is a state in which when it's a channel, you call it normalized.
800638	803598	B	0.8629806041717529	So for a state, this means it would actually be a probability distribution.
803694	807634	B	0.7532344460487366	So it's actually normalized if you sum over the values with this omega, you'll get one.
807752	812098	B	0.855032205581665	And for a morphism being a channel, it means it sends each input to a distribution.
812194	815080	B	0.85975581407547	So it actually is a probability channel in the usual sense.
815610	818690	B	0.8396903872489929	Equivalently, the matrix for F would be stochastic.
818850	821980	B	0.7967364192008972	So these are the ones we'll use in generative models, for example.
824270	828086	B	0.7310980558395386	And as I said, there's lots of probability theory you can describe with these diagrams.
828118	829862	B	0.72735595703125	It's very two simple examples.
829926	833466	B	0.694333016872406	You can describe marginalization in probability theory with this discarding thing.
833488	837134	B	0.908815324306488	So if you have box omega like this, it would be a joint distribution over X and Y.
837172	839918	B	0.7771492600440979	If you just discard Y, you'll get the marginal on X.
840084	850930	B	0.8525691032409668	And if you plug omega A distribution on X into an effect, that would just be any function on X, this would be giving you a scalar now, and that would be the expectation value of E in this distribution.
852150	857506	B	0.5345805287361145	So I'll meet lots more of this as we go, but let's actually start doing some stuff related to active inference in particular now.
857528	861618	B	0.7962859869003296	So I want to talk about generative models and how you view these in the diagrams.
861794	869320	B	0.8270663619041443	So, as we've said, we're going to be talking about agents having generative models that relate things like actions, observations and world states.
869850	878346	B	0.821621298789978	And these are normally quite compositional in active inference, right, and might involve many different spaces of states and observations and these processes relating them.
878448	886830	B	0.8090246319770813	You'd usually treat these as something like a Bayesian network, claiming you can view it really as a causal Bayesian network because it's sort of describing how states are causing these observations.
888290	889246	B	0.739630937576294	Formally, it's the same thing.
889268	903934	B	0.8566604852676392	It's just Bayesian network, which you could normally say is described as something like a dag, a directed asynchronous graph, which describe the different variables that are being related and then sets the values for each and probability channels, describing each one in terms of its parents in the Dag.
903982	907570	B	0.8630434274673462	And then you often look at its whole distribution over all the variables.
907910	919074	B	0.8340733647346497	But already I'd say the way that these visual networks are drawn in active inference text and stuff is kind of converging on something a bit closer to string diagrams because you don't actually just draw the Dag and the variables.
919122	925450	B	0.6649163365364075	It's very useful to actually give names to the mechanisms themselves, as it were, like you have in this picture, the A and the B's.
926190	936190	B	0.8612403869628906	So yeah, my claim is that it's sort of converging on the way that string diagrams will look, as we'll see on the next slide, which is where you really do label everything, not just the variables.
937330	947250	B	0.8886686563491821	So to describe those sort of Bayesian networks with string diagrams, the key observation is really that Dags correspond to a certain class of string diagrams that we're called network diagrams.
948310	951538	B	0.6185453534126282	There's a definition here, but which is better just to see an example in a second.
951624	961270	B	0.80915367603302	But the diagrams built from copying and sometimes discarding and the key thing is just that they only have processes with maybe many inputs but only one output.
961610	965030	B	0.8747854232788086	So this is going to be like a mechanism that produces each variable.
965930	978966	B	0.8500145077705383	So the result is that if you have a Dag G and you choose some of the vertices to be outputs, so those are like the observed variables, you can draw a network diagram which expresses the same equivalent structure with those things as the outputs.
979078	980106	B	0.7576137781143188	So here's an example.
980208	983180	B	0.8669305443763733	We have a Dag with these four variables, x one to x four.
983870	990638	B	0.8978383541107178	And what you do is you have a wire for each variable in your diagram on the right and you draw a box that produces it.
990644	992480	B	0.6977733373641968	It doesn't really matter what you label the box.
994290	999374	B	0.9041579961776733	So this box C, for example, produces x two and it will produce it in terms of its parents in the Dag.
999422	1001714	B	0.878778874874115	So x one and x four in this case.
1001752	1005650	B	0.5385580062866211	And if it doesn't have any parents, it would just be a state, as it were, box and no input.
1006310	1013794	B	0.8608765006065369	And then what you do is you take each variable and you copy it and you pass it to all of its children in the Dag and also out of the diagram if it's an output.
1013842	1021180	B	0.8580119609832764	So this now means you've sort of expressed the whole structure of the Dag and also which variables are sort of leaving the system for the output ones.
1022190	1035230	B	0.7486133575439453	So this allows us to turn Dag into a string diagram and then if we want to make a generative model that expresses like a Bayesian network that's structured according to this Dag, you just have to now interpret this diagram in a certain sense.
1035300	1047926	B	0.8709151148796082	So in general, working in any one of these CD categories, this copy discard categories, we can say that a generative model in there is given by one of these network diagrams without any inputs and an interpretation of the diagram.
1047978	1053186	B	0.8977019190788269	Meaning you actually say what the objects are for each of the wires in the diagram and what the actual channels are.
1053208	1058142	B	0.8382514119148254	So they need to be channels, not just morphisms in your category are for each of the boxes.
1058286	1065670	B	0.863875687122345	So for a gender model like this, you would say you pick objects x one, x two, x two, x four and pick channels for the ABCD.
1066090	1071080	B	0.866517961025238	And we'll think of the output of the diagram as like the observed variables and the rest of the hidden ones.
1071870	1078218	B	0.7914224863052368	So for example, if you're working in this category, Mat R Plus, that is the only category I've actually introduced here.
1078224	1079674	B	0.7633466720581055	So this is going to be our running example.
1079792	1082378	B	0.7437488436698914	This is the same thing as one of these causal Bayesian networks.
1082394	1087710	B	0.8381586670875549	So it just means picking sets of values for variables and picking probability channels for the boxes.
1089330	1094170	B	0.4945851266384125	So you might ask why would you use this representation rather than the usual one, which I think is a good question.
1094340	1099250	B	0.8775151968002319	So it's equivalent to the Dag and probability channel description.
1099670	1115654	B	0.6918942332267761	But the thing that's nice is that in the conventional approach of these networks, you sort of have to switch between the Dags, which are split the variables and then doing calculations with probabilities, whereas in the categorical approach you can use just one formatism because you can do probability theory with the string diagrams as well.
1115692	1116934	B	0.6096599698066711	So it's quite natural in that sense.
1116972	1122458	B	0.8259861469268799	You have this one language of both just intuitively drawing what's going on in a model and then reasoning about it.
1122624	1126314	B	0.7452614307403564	It also lets you start to generalize things in a useful way, I think.
1126432	1133920	B	0.6023600697517395	So I kept having to say that you have no inputs to your diagram, but there's nothing really fundamental about that and it's not really clear why we need that.
1134530	1137870	B	0.8030599355697632	What you can do instead is start to allow inputs to your model as well.
1138020	1142238	B	0.6915306448936462	So we'll call this an open generative model.
1142324	1145262	B	0.8249019980430603	So an open generative model is the same thing.
1145316	1148686	B	0.6128997206687927	But now I've just dropped this requirement, the diagram doesn't have any inputs.
1148878	1151246	B	0.8654959797859192	So here's an example of a general network diagram.
1151278	1153170	B	0.8726464509963989	Now, with these inputs x two, x three.
1153240	1156354	B	0.7557164430618286	So there's no mechanism specified for these new variables, x two, x three.
1156392	1160134	B	0.8223608136177063	They're just input variables to the system and they can be outputs as well.
1160172	1162662	B	0.8800784945487976	For example, x three is both an input and an output here.
1162796	1167670	B	0.7486822009086609	And again, an interpretation of this general network diagram just means picking the objects and the channels.
1168650	1175302	B	0.8983845114707947	This is the same definition we use to define what we call an open calls and model in the paper with Robin Lorenz I mentioned earlier.
1175446	1182940	B	0.820483922958374	So an emergenerative model is formally just the same thing, but we're just thinking of it as a generative model possessed by a cognitive agent.
1183630	1187306	B	0.7036758065223694	If you run this definition in mathar plus then this is just like a causal Bayern network.
1187338	1190122	B	0.5705254673957825	But now you have some of your variables just have no mechanism specified.
1190186	1192240	B	0.7011489868164062	So they're just inputs to the whole thing.
1193730	1199778	B	0.9525308012962341	The nice thing about these open generative models is that because they can have these inputs, you can plug them together and compose them.
1199944	1201726	B	0.7204957008361816	These things in fact form their own category.
1201758	1203220	B	0.6531431078910828	But I won't go into that today.
1205270	1212360	B	0.8671999573707581	So that was the general theory of these generative models that just actually described some examples that you'll see in active inference coming up all the time.
1212730	1214198	B	0.786922812461853	So it's a simple example.
1214364	1218146	B	0.8320456743240356	Let's just imagine we have one space of hidden states and one space of observations.
1218258	1225580	B	0.8608995079994202	Then it would be generative model of that form would just look like this network diagram where there's just two wires, there's just S and O.
1226510	1229978	B	0.6335164308547974	S doesn't have any parents, so it just has this prior distribution sigma over it.
1229984	1234350	B	0.8533467054367065	And there's just this one channel often called the likelihood from S to O.
1234500	1242522	B	0.8516102433204651	If you just draw that network diagram, say that model in C or in Matar plus it would be the same as one of these simple generative models.
1242666	1247730	B	0.6298865079879761	When you're looking at these, you're often introduced interested in this distribution over both variables together.
1247800	1259926	B	0.8897397518157959	This joint distribution that you might write as P of S times P of O given S normally or bit more specifically here introducing the names for the two distribution in the channel here in string diagrams, it's just the same as this.
1259948	1263286	B	0.8249638676643372	So you just take the prior and you make it an output now.
1263388	1267090	B	0.8780792355537415	And then you compose those channels together and this would give you a distribution.
1267170	1271962	B	0.7281687259674072	So a normalized state M over snow at this.
1272016	1275674	B	0.8567509651184082	So this is just the resulting joint distribution you get from the generative model.
1275712	1277180	B	0.8162539005279541	And we'll come back to that later.
1279230	1287870	B	0.8243910670280457	So, a more elaborate example of generative model that you'll see example in the active infants textbook are these discrete time models that are used a lot.
1287940	1296398	B	0.8477674722671509	So I'll walk through this diagram now, so this is an example of a more complex network diagram and a generative model it describes.
1296574	1299220	B	0.702972948551178	So here we've got these end time steps going.
1299590	1304658	B	0.8275596499443054	Remember, we read from bottom to top, but I'll just talk about I'll describe things from the top down here.
1304664	1315480	B	0.8475678563117981	So at the top we have the observations one, two up to on the observations at each time being caused by these hidden states s one, S two and SN at each time by these channels A.
1317290	1324666	B	0.8956170082092285	And the hidden state is evolving over time by this transition channel B, where it takes the previous state as an input and then choose the next one.
1324768	1331938	B	0.8683094382286072	It also takes us one extra wire that's coming from the bottom and that's this space P of policies, which is how the agent sort of actions enter the picture.
1331974	1336894	B	0.8874332904815674	So these policies describe its behaviors, its behavioral policies it can carry out.
1337012	1339770	B	0.7814510464668274	So, based on the previous hidden state and the way it's acting.
1339930	1350660	B	0.8755942583084106	This channel B would determine the probabilities for the next states and then there's supplier over the policies that you can think of as the habits or the typical behaviors of the system.
1351270	1361286	B	0.8784040212631226	So you can just draw this diagram and say, like, interpret this in math plus and that would for any given interpretation, meaning any choice of what the values for these variables can possibly take are.
1361308	1366520	B	0.6951391100883484	And any choice for what these channels are would then give you this type of punitive model.
1368170	1374806	B	0.890290379524231	And often you will see models sort of of this form or similar forms being plugged together to form these hierarchical models.
1374838	1387998	B	0.9293960332870483	And I think the compositional language is very nice for these because you really want to talk about open models to define this so hierarchical model you can view as really as just being given by taking lots of these open generative models I mentioned and plugging them together in a certain sense.
1388084	1400100	B	0.849679172039032	So here you can see a picture of a hierarchical model where we just have these layers where different copies of the same model at each layer and the inputs from one layer match the outputs of the layer below so we can compose them together.
1401510	1403406	B	0.8079255819320679	So that's generative models.
1403598	1406306	B	0.8729146122932434	So so far we've just been using the diagrams to represent them.
1406328	1408962	B	0.7565123438835144	We'd like to actually do a bit more though and reason about these models.
1409026	1418806	B	0.6940138339996338	In particular, we'll talk about how you update a model or update the beliefs within a model, which is very important in active inference and how this looks in the string diagrams.
1418838	1431354	B	0.859455406665802	Now, so let's say we've got an agent M, we have a model M and it's just a simple one where we say there's just one space of hidden space S and one space of observations O.
1431392	1440254	B	0.8973958492279053	So they have this joint distribution that I mentioned earlier and they had these prior beliefs about s which you can get back from the joint distribution by just taking the marginal, if you like.
1440292	1441290	B	0.747089147567749	So that's sigma.
1441370	1446686	B	0.8680114150047302	So they've got their model, they've got their beliefs about what states are likely and then they receive a new observation.
1446878	1454610	B	0.8296462297439575	And here the kind of observations we'll think about in general can be soft, meaning they're described by a distribution over O, not necessarily just one element.
1455510	1459238	B	0.6331980228424072	So that would be one of these normalized states.
1459404	1462390	B	0.5342661142349243	I'll try not to use the word state here because it's a bit confusing with this S around.
1462460	1468038	B	0.8808794617652893	So this distribution over O is their new observation, this bold font O.
1468204	1477094	B	0.8445225358009338	And now they want to update the margin, want to update M in some way so that the marginal basically is different and describes the updated posterior beliefs.
1477222	1484234	B	0.8954632878303528	And this comes up of course in perception where you're updating your sort of state of the world given some observation you receive.
1484282	1491550	B	0.8802857398986816	But it could also be used to model like planning behavior where updating your plan of action, your policy given something like.
1491620	1497460	B	0.9056702852249146	Which outcomes you'd like to see in the future and we'll come back to that later in the talk.
1498230	1501490	B	0.8232837915420532	So we want to talk about how you can do this updating.
1502230	1506782	B	0.7926223278045654	So you might think there's just a standard answer or at least in the ideal case, which is this Bayesian updating.
1506846	1509038	B	0.5132178068161011	And that's true when your observations are sharp.
1509134	1510822	B	0.8955671191215515	We'll start by talking about that case.
1510956	1512982	B	0.831607460975647	So I'll say what sharp means in a second.
1513116	1516914	B	0.8802970051765442	But first I'll just talk about how you treat Bayesian conditioning in these string diagrams.
1517042	1531690	B	0.9129633903503418	So on the right here we have if you view this process from O to S, this describes the sort of Bayesian conditional channel, or in general a partial channel in fact that the agent would have from the introduced by their model.
1531760	1536958	B	0.8421996235847473	So you can describe this in string diagrams with a couple of extra gadgets I hadn't mentioned yet.
1537124	1539600	B	0.8356025218963623	So you have your distribution m over S and O.
1539970	1546718	B	0.8484789133071899	You can have this in math there's this effect that we call the cat which just takes two inputs and compares if they're equal.
1546814	1549166	B	0.655096709728241	This allows for you to turn an output into an input.
1549198	1550994	B	0.8083980083465576	So that's what this part here is.
1551192	1554126	B	0.7705324292182922	And then you can introduce this extra thing of normalization.
1554238	1567720	B	0.738386332988739	So what you'd like to do is take a general morphism and for each possible input normalize that so that it's a distribution if you can or set it to zero if it's just zero and there's nothing you can do, that's what this blue dash box is.
1568490	1574650	B	0.8753382563591003	And in the paper and in the related calls and models paper, we talk about the axioms normalization feature satisfies.
1575150	1579338	B	0.7985014915466309	The point is if you compute this thing in Matar Plus, it will give you kind of what you'd expect.
1579424	1592510	B	0.8737675547599792	So it will give you the usual notion for each .0 of the space O, you plug it into this, you'll get a kind of conditional M of S given O that you'd expect whenever that's defined.
1593010	1599300	B	0.6817324757575989	There's a strange diagram way to describe this kind of Bayesian conditional channel or partial channel.
1599670	1602526	B	0.8280046582221985	And I said this is what you would use when your observation is sharp.
1602558	1606114	B	0.8975468873977661	And I also drew it differently with this triangle to sort of distinguish that case.
1606152	1607974	B	0.8446477055549622	So what does that mean in general?
1608012	1618230	B	0.7211959958076477	You say that a state in one of these CD categories, so a distribution basically would be sharp when it's copied by the copy map, which isn't true for general distributions.
1618730	1626922	B	0.8135174512863159	And if you run this definition in math or plus, this really means that this thing O really is just point distribution at some specific element of O.
1626976	1629146	B	0.6134463548660278	So it really is sharp in that sense.
1629168	1630298	B	0.7336194515228271	It's just a point.
1630464	1634960	B	0.6465869545936584	There's no real probabilities probabilistic aspects to it there.
1635410	1638926	B	0.8445900678634644	But in any CP category you can just talk about the sharp states like this.
1638948	1641070	B	0.8833649158477783	They're often also called deterministic.
1641970	1645882	B	0.8772929906845093	So for these sharp ones, you ideally think you'd like to do the spacing updating.
1646026	1647666	B	0.7507179379463196	But in fact when you've got soft ones.
1647688	1648980	B	0.6552160382270813	So it doesn't have this property.
1649510	1652814	B	0.6820703744888306	There's actually at least two good ways to do this kind of updating.
1652942	1660120	B	0.8161622881889343	I don't know if this is as well known, so I'll just mention it now anyway, and they've been studied in some detail by Bart Jacobs, this paper at the bottom.
1660970	1663714	B	0.5009295344352722	So let's say you don't get one of these sharp observations.
1663762	1665910	B	0.7055144309997559	You have just a distribution over o.
1666060	1671090	B	0.668452262878418	There's at least two reasonable ways to generalize sort of the picture from the last slide to give a notion of updating.
1671170	1674330	B	0.8727476596832275	And Jacobs calls them Jeffries and Pearl's update rules.
1675230	1679146	B	0.8848532438278198	So in Jefferies Update, you basically do it like we did before.
1679248	1686094	B	0.8712615370750427	You have this Bayesian conditional kind of channel or partial channel, the normalized box here, and you just plug a distribution into it.
1686212	1689902	B	0.8376237750053406	But in Pearl's Update, you turn a distribution into an effect.
1689956	1692506	B	0.8631827235221863	So you just compose it with this cap to bend it around in the picture.
1692538	1693614	B	0.7588381171226501	That's what it means.
1693812	1696722	B	0.8050928115844727	So you plug that into M and then normalize everything.
1696776	1700034	B	0.6888604760169983	So the difference is where the normalization happens.
1700232	1705006	B	0.8377723097801208	And yes, it's basically just interesting that both of these are reasonable notions, generalization, they have different properties.
1705118	1708454	B	0.6509183049201965	It's not obvious that one of them is sort of more rational or something than the other.
1708572	1710470	B	0.5538121461868286	They just behave a bit differently.
1711770	1716226	B	0.8432542681694031	In the formula you can see that the normalization is being applied differently.
1716258	1719880	B	0.8532544374465942	So if you turn this picture into these little notations, it would look like this.
1720650	1728166	B	0.8864697217941284	So in the top case you're normalizing for each possible sharp o and then taking an expectation over this distribution here.
1728188	1731950	B	0.8052142858505249	Whereas in the Pearl case, you plug the whole thing together and then just normalize.
1732770	1735498	B	0.5188389420509338	Either way you do it though, the point is that these things are actually hard to compute.
1735514	1739790	B	0.4973134696483612	So we don't expect a cognitive agent to be doing either of these exactly, even in the sharp case.
1739940	1746340	B	0.7613602876663208	And so, as we know, we want to instead approximate these kind of things using free energy, which is what I'll talk about next.
1749350	1759938	B	0.8306341767311096	So our aim is to try and accommodate free energy somehow in the diagrammatic approach and free energy sort of formula that come up are often given in terms of what you call the Surprise, these negative logarithm quantities.
1760034	1766738	B	0.8574590682983398	So we'll start by introducing just a new graphical component for treating those that we call log boxes.
1766914	1776554	B	0.812442421913147	If you have any function E on a set X, a positive real, remember that made in A, that would look like an effect in your category X in Math Plus.
1776592	1785466	B	0.9035506248474121	Let's say then what we want to do is talk about this function X goes to minus log of e of X, which we call the Surprise.
1785498	1791262	B	0.8771232962608337	So we just introduce this graphical feature where we draw a green box around it and say that denotes this function.
1791316	1798750	B	0.7201665639877319	Now, and using rules, the nice properties of logarithm, you can turn these into nice graphical rules.
1798830	1807240	B	0.7402973771095276	This log box feature would satisfy for example, this is sort of the way that logarithms turn multiplication into addition on the left here.
1809770	1812466	B	0.7810657620429993	If you have this around, then you can start talking about surprise.
1812578	1822442	B	0.8820183873176575	So if you have two distributions, sigma and Omega, the surprise of one distribution relative to the other is defined by this expectation value.
1822496	1834414	B	0.8854569792747498	So just the expectation of a surprise of sigma according to Omega which if you remember I said expectation values are given by sort of plugging a state for the distribution into the thing you're looking at the expectation value of.
1834452	1835886	B	0.848545253276825	So that would be the log box here.
1835988	1841354	B	0.8647651672363281	So we can just define surprise omega, Sigma in this way if we like in the pictures.
1841482	1846366	B	0.7889954447746277	And important special cases where this come up are when you're calculating entropy, which is the self surprise.
1846398	1850690	B	0.8145664930343628	And the KL divergence can be calculated from the surprise and the entropy.
1851590	1859480	B	0.846705436706543	So it means that whenever we have formula given in terms of these if we like, we can instead denote them with this graphical symbol, at least with the log box.
1861370	1864342	B	0.8337326049804688	So now let's talk about how we use this to describe free energy.
1864396	1870194	B	0.7436960935592651	So what we wanted to do in the paper is sort of help clarify the kind of different notions of free energy that we found in active instruments.
1870242	1873482	B	0.8575186729431152	In particular the variational and the expected free energy energy.
1873536	1878858	B	0.7869503498077393	So we want one more general quantity that we can understand both of those in terms of.
1878944	1881530	B	0.8102107644081116	I'm just calling out the free energy here.
1881600	1886080	B	0.6145994663238525	I'm interested to know what other people think of this sort of naming for what we're doing.
1886610	1891502	B	0.5740444660186768	The situation is that we've got some generative model that's fixed over these two variables Sno like before.
1891556	1896650	B	0.801697313785553	So remember we had this distribution, this box M over Sno distribution.
1896810	1902194	B	0.8278656005859375	And let's just say we have another distribution Q now and we'll see examples of this in a second what sort of Q they would be.
1902392	1906966	B	0.8894410729408264	Then we just define this quantity of free energy of the one relative to the other in this way.
1906988	1912466	B	0.8060923218727112	So it's the surprise minus the entropy of Q in the string diagrams.
1912498	1914370	B	0.7518575191497803	Then it's just this feature we plug.
1914450	1921900	B	0.8395053148269653	So it's like the expected surprise of M for Q minus the entropy of Q's marginal on S.
1922590	1929622	B	0.5470868349075317	Or this is the formula if you want to use the conventional notation which is useful for relating it to existing approaches.
1929766	1938720	B	0.7275675535202026	So we can define this very general free energy quantity and then we'll meet two special cases of it that we're interested in, which is the variational and expected free energy.
1939250	1942250	B	0.8237797021865845	It all comes down basically to having a definition of surprise.
1942410	1945120	B	0.7546147704124451	You just need this notion of surprise to define everything else.
1946370	1948260	B	0.7717226147651672	So with a variational free energy.
1948950	1953282	B	0.8821799159049988	So we have this fixed model M and we have this soft observation o like we did before.
1953336	1962130	B	0.9096909761428833	So that's this box here and then what we're doing is we're considering different possible distributions over S that we think of as different updates we could consider for our beliefs.
1962290	1970454	B	0.9039960503578186	And we define the variational free energy of any of those states, those distributions Q as the special case of the definition from the previous slide.
1970502	1974602	B	0.8389112949371338	So it's like the general free energy where that capital Q just takes this form.
1974656	1979980	B	0.8903389573097229	So it just consists of our new beliefs, lowercase Q and our observation O.
1981390	1983966	B	0.8629220128059387	So in formula you could also just draw it like this.
1983988	1994560	B	0.7893683314323425	So you take the surprise from your model M and you just see how its expected value for those beliefs and that observation subtracted the entropy of Q.
1996610	2005330	B	0.8157793283462524	And what you can show is that this VFV value satisfies this bound of the KL in relation to this kind of Jeffrey update of your model with respect to this observation.
2005750	2013750	B	0.8854364156723022	In particular, when it's a sharp observation, then the minimal of this VFE will be given by the Bayesian Updating.
2014650	2016920	B	0.8692579865455627	In general, though, we might think about what happens.
2017290	2025050	B	0.8577626347541809	So in general we can think of minimizing this VFP quantity as doing this, as finding this Q that approximates this kind of updates we were looking at earlier.
2025870	2030300	B	0.833465039730072	And yeah, in the sharp case it will coincide as all of the notions of Updating do.
2030990	2033930	B	0.9160792231559753	So the minimal VFP will be given by the Bayesian update.
2034010	2037294	B	0.6218878626823425	But for these soft observations, it's something else.
2037332	2041626	B	0.7310706973075867	It's not exactly either of the two notions Updating we met earlier.
2041658	2049298	B	0.8839370012283325	So this is actually a third notion of updating for soft observations, which I think is an interesting way to think about what VFE minimization is doing.
2049384	2051310	B	0.8472628593444824	So we just call this the VFE update.
2051390	2056834	B	0.8668714761734009	So you've got many different Q, you could calculate the sphere quantity for each and you've got a soft observation O here.
2056872	2058158	B	0.7526888251304626	So it's some distribution.
2058334	2062790	B	0.8924459218978882	And if you find the one with the minimal value of sphere, you call that the VFE update.
2063450	2068120	B	0.6359984874725342	And this wouldn't be equal to either those Pearl or Jeffrey style updates that we met.
2068970	2076300	B	0.8007782101631165	So that's the VFE which we'll come back to, the other notion of free energy we want to talk about, it's the expected free energy.
2077230	2079626	B	0.8298305869102478	So that's where we still have our model M.
2079728	2088254	B	0.7813593149185181	But rather than an observation, we think of ourselves as having some preferences of observations we'd like to see they're again encoded in a distribution, though over O.
2088292	2095680	B	0.8146840929985046	So that's this C just with that fixed, we can define this one quantity called the expected free energy.
2096050	2101938	B	0.8736416697502136	So that's given by the free energy of M compared with this other generative model where you have used the same.
2102024	2105838	B	0.8763871788978577	So this M here would really be the inverse channel from O to S of M.
2105864	2113522	B	0.8666512966156006	So like the Bayesian inverse here, but where you just assert that the preference is actually is the prior on the observation.
2113586	2120150	B	0.8829569220542908	So you're comparing these two in terms of this generic free energy quantity we defined earlier.
2120490	2122238	B	0.8419641256332397	So again, you can turn this into formulae.
2122274	2129580	B	0.7835831046104431	And there's loads of stuff in active inference about the different rewritings of EFE and the ways to interpret them in terms of uncertainty and risk and so on.
2130190	2141390	B	0.7779786586761475	And it has this property that you can show it will be bounded by the surprise of those preferences for your model and it kind of gives you a way to approximate them as we'll see.
2141540	2144154	B	0.5122119784355164	So I won't talk, I don't think I have time to go too much more into EFE.
2144202	2157430	B	0.8615437746047974	But the point really in terms of what this work's done is just to try and have just one generic free energy quantity we met earlier where we can see the VFE and the EFE both coming up special cases depending on what we plug in here for the two distributions.
2159610	2165554	B	0.8977627158164978	So what I'd like to do now is to sort of put some of these pieces together to show what active inference itself will kind of look like in terms of string diagrams.
2165602	2175530	B	0.7939320206642151	And in particular, what we'll do is derive this formula that you'll find in active inference textbooks in a graphical way and I think quite a transparent way, that's the claim.
2177150	2182554	B	0.7164679169654846	So to do so, we basically need to give a nice high level conceptual view of what active inference is.
2182592	2184480	B	0.8603830933570862	So this is the way that we do it in the paper.
2185490	2191790	B	0.8362792730331421	So when inactive inference, the key thing for stating all the definitions is that our model takes the following form at a high level.
2191860	2193758	B	0.8502792119979858	So there's some notion of so.
2193764	2197086	B	0.8723651170730591	It's like our discrete time model we had earlier, but just with two time steps.
2197118	2202446	B	0.8467159271240234	If you like, each of those time steps could break down in terms of further subtime steps.
2202478	2203394	B	0.5317365527153015	But that won't matter.
2203432	2206418	B	0.8130691647529602	We've just obstructed here at this higher level.
2206584	2213574	B	0.8659952282905579	So at the higher level, we just have a notion of the current time, or maybe like all the time steps up to the current time, and that's this S and O here.
2213612	2220290	B	0.8790097236633301	So there's current states and current observations and then there's some notion of future times, these future states and future observations.
2220370	2228010	B	0.8541361689567566	So that could be all the time steps up to some big number, something like that, all grouped together in one of those discrete time models.
2228430	2243200	B	0.7613511681556702	And again, we have the policies and we have the same sort of shape of model where there's some channels here which I haven't bothered giving letters to, but showing the way that the policy influences the transition from the state to the future state and observations from each.
2243890	2250862	B	0.8704516887664795	So we just have a generative model where we have policies, we have states and observations, and we have future states and future observations.
2251006	2253794	B	0.8501223921775818	In active inference, what we're doing is we're receiving two things.
2253832	2260200	B	0.8928895592689514	We're receiving an observation in the current time and we have some preferences about what we'd like to see in the future.
2260650	2270010	B	0.9012547731399536	So these are each given by these two distributions, bolton, O over O, and the preferences C over the future observations, and then we're doing updating with those.
2270080	2282634	B	0.8885272741317749	So I think updating is just our habits, the prior over the policies to give our new distribution over policies which we can think of as the agent's plan of how it wants to act.
2282752	2287360	B	0.8483576774597168	So we're going to try and do this updating like before to obtain a new distribution over P.
2287810	2293918	B	0.817966639995575	And that is now telling us how we want to behave in the future in a way that will basically you can.
2293924	2301220	B	0.8471164703369141	Think of it as saying, we want to explain why we're seeing what we're currently seeing and how we're going to obtain what we'd like in the future.
2303670	2310646	B	0.8642645478248596	In the books kind of inference and various places you can find a formula like this that there will be justified as coming from the free energy principle in some way.
2310748	2323862	B	0.9093365669250488	It's basically saying you can do this approximately by making your plan distribution take the following form there's a soft max, there's a part relating to the habits of your model so that's your prior over policies.
2323926	2330090	B	0.9167814254760742	These pi are the individual policies in P and then there's parts of formula related to the VFE and the EFE.
2331390	2338906	B	0.5512265563011169	And what we wanted to do is see where this formula comes from in a sort of nice high level way from the structure of the diagram.
2339018	2352306	B	0.5112966299057007	So there are explanations for this formula there, but I found them quite hard to follow, to be honest, because they were talking about the EFE as being a prior that you then do EFE kind of minimization on top of.
2352328	2357090	B	0.8302334547042847	But you kind of need to do the part about the present time first before you can do the EFE.
2358070	2362342	B	0.6796367168426514	And so what we wanted, this is a really clear way to see how this just drops out from the structure of the model.
2362396	2365560	B	0.7338427901268005	So that's what I'll try and show now.
2367450	2371166	B	0.8879212737083435	So what we'd like to do then is to do this approximate updating.
2371218	2374278	B	0.8221239447593689	We're going to do the pearl style updating which looked like this in the pictures.
2374374	2383162	B	0.7905365824699402	So we want to get our new plan so our distribution over policies by updating, by plugging in our observation and our preferences and then normalizing everything.
2383216	2388080	B	0.8412516117095947	So the thing on the right is what we'd like to have ideally but we're just going to have to approximate it in some way.
2388850	2392770	B	0.8544837832450867	Let's just take the distribution that's inside the dash normalization box.
2392840	2400162	B	0.7400113344192505	Now this is the thing we'd like to basically approximate this in the structure of our model.
2400216	2408402	B	0.8927547931671143	We can write it like this and I'll just show then some graphical steps for how we can apply approximations to obtain the formula that we saw.
2408456	2415346	B	0.690134584903717	And obviously we won't be able to go through every detail of the proof but it should give hopefully just some intuition for what it's like to actually work with a string diagram.
2415378	2416822	B	0.7328930497169495	So that's really why I'm showing it.
2416956	2418950	B	0.8832045793533325	So we knew our model take roughly this form.
2419020	2423974	B	0.9029090404510498	There's some part relating to current states and current observations and also future observations.
2424022	2431500	B	0.8668293952941895	I've just called them both M here, but we just know there's that part of the model relating to the present time and future time.
2432670	2438986	B	0.8896288275718689	And so what we're going to do is first focus on this part of the model relating to the current state and the current observation.
2439098	2442638	B	0.8575986623764038	And we want to approximate what's in that blue dashed box.
2442804	2449650	B	0.7963245511054993	And what you can show is that if you do this VFE updating, that will be approximately equal to this part of the diagram.
2450150	2457958	B	0.8443493247032166	So this Q is given by for each policy doing this VSE updating, so minimizing your variation of free energy.
2458124	2459862	B	0.8196311593055725	So you do that for each policy.
2459996	2466454	B	0.8997946381568909	Then you can view the collection of all of those belief updates as just one channel from P to S.
2466652	2475482	B	0.8848696947097778	So if you think about it back here, basically for each policy you could plug in you would obtain just a distribution now of snow and you could do updating with respect to that.
2475616	2492670	B	0.8553920984268188	That's what Q of that particular policy pi would be and you put them all together into this one channel Q and you can show then for each one, if you do this overall process where you multiply by this E to the minus VFD quantity here, it will be approximately equal to this part of the diagram.
2494130	2497086	B	0.8805199861526489	Okay, so that's our first step and that's how the VfB entered the picture.
2497118	2504334	B	0.8706948757171631	Then we've got this top part of the diagram, we'll collapse it together and just view this as one process going into future observations and our preferences.
2504382	2506820	B	0.8898012042045593	And we'd like to approximate what's in this box now.
2507430	2509046	B	0.7414770126342773	And this is where the EFB comes in.
2509068	2525482	B	0.8675591349601746	So you can basically show because you have this, the expected free energy will give you an approximation to this here where this is basically like an expectation value for your preferences for each policy.
2525536	2531660	B	0.8985689282417297	So this would be like the density C of those preferences being plugged into your model for each policy.
2533630	2535446	B	0.5317785739898682	So I haven't had time to go into the full details.
2535472	2549060	B	0.8778361082077026	These of approximation steps but they're essentially the same approximations you'll find in active inference text and so on, just turned into the string diagrammatic setting and we talk about how they come about from Jensen's inequality and things like this.
2549510	2556180	B	0.8897759318351746	So this step where you think about the future times is called the prediction step and the previous one was the perception step.
2556630	2563670	B	0.893302321434021	So now we've rewritten that diagram in terms of some e to the minus of the VFE and e to the minus of the EFE as well as our habits.
2565930	2569766	B	0.7093319296836853	And remember what we wanted to do was approximate the normalization of this whole thing.
2569788	2575994	B	0.737518846988678	So that's when you apply this blue dash box around the whole thing and now if we do that, this is exactly the same as the formula we were after.
2576032	2583894	B	0.6951447129249573	So we've obtained the formula now and that's because you're normalizing something but it's got these e to the minuses in it.
2584032	2592510	B	0.7773175835609436	So you can also rewrite that in terms of this soft max where now you just replace the E with this log and the other ones you lose the exponentials.
2593010	2600082	B	0.8726118803024292	So this formula, if you wanted to, if you wrote out the formula for what this was for each policy, it would be equal to this down here.
2600216	2606206	B	0.750957190990448	So the claim is that this is a nice way to derive this formula and is a bit more transparent than the ones that exist.
2606238	2609170	B	0.8617758750915527	So the idea was really just to see we draw what's going on.
2609240	2619660	B	0.9013947248458862	Okay, we're doing updating with the model of this form and we're trying to do this approximate form of updating and just see where we're applying the approximations and from the structure of the model itself, see how this formula comes about.
2622110	2628794	B	0.8469831347465515	Okay, so that so far basically just talked about things that are already there in active inference as it's new derivation, but it's existing stuff.
2628912	2635390	B	0.6028962731361389	Before wrapping up, I'd just like to also talk about something bit more new that we do with the string diagrammatic approach.
2635890	2641230	B	0.8702880144119263	That's to talk about the way in which free energy itself is compositional.
2641650	2647940	B	0.7105743288993835	So the motivation for this is that the idea is that we want to think of this one free energy principle applying at all levels of a system.
2649270	2659186	B	0.8404338955879211	So to do that, you'd want to know that an agent can say if you've got one of these big composite generative models, that it can do its free energy minimization on the whole thing by doing it on the parts.
2659218	2664194	B	0.8249706029891968	Because we want to ultimately think it just comes down to each part doing its own bit of free energy minimization.
2664322	2666390	B	0.813547670841217	So that's what we want to make precise.
2668010	2671338	B	0.8828665018081665	In particular, we're going to be talking about the VFE here really all the time.
2671504	2673834	B	0.8354458212852478	And if you recall in the diagrams, it looked like this.
2673872	2677980	B	0.903536856174469	So we use this log boxes and it just took this particular shape here.
2678510	2692094	B	0.7386490106582642	So what we do in the paper in order to address this compositionality problem is introduce a notion of this VFE that we can apply not just to generative models, but ones which actually have these inputs as well.
2692132	2703650	B	0.6553120017051697	So these were what I called open generative models earlier because we need to really talk about pieces of generative models plugging together and give them a notion of free energy to even make sense of this notion of free energy being compositional.
2704310	2707250	B	0.8977993726730347	So we proposed this definition of what we call the open VFE.
2707590	2715110	B	0.8918100595474243	So now instead of just a distribution M over S and O, we have a channel from some inputs to S and O given by one of these open models.
2715530	2720006	B	0.9166855812072754	And our Q, the thing we're doing the VFE minimization with respect to.
2720028	2723018	B	0.8877496123313904	So the thing we're calculating, it would now have an input as well.
2723024	2728460	B	0.8998940587043762	So it's a joint distribution over the states and inputs and observation takes the same shape as before.
2730050	2738074	B	0.8878389596939087	So you get this other formula that's basically just a natural way to generalize the previous VFE formula to accommodate this extra input wire.
2738202	2746654	B	0.8996450304985046	I now and what we show is that this thing is compositional in a sensor that I alluded to.
2746692	2752654	B	0.915114164352417	So I'll walk through that and the way you do it is just using these graphical properties that these log boxes have that I mentioned earlier.
2752702	2756594	B	0.8479159474372864	So you could turn all of that into a proof and standard probability notation if you like.
2756632	2764840	B	0.5016293525695801	But it's quite instructive to always just be able to work in the diagrams to keep track of the compositional structure of the models and so on.
2765690	2770950	B	0.854424238204956	So the result says that this open VFE quantity is compositional in two ways.
2771100	2773066	B	0.7267135381698608	The first one here is this quite trivial way.
2773088	2780570	B	0.8831984400749207	So if we have two models running in parallel, so like taking the tensor of them and they're just both doing their own, we're calculating the VF for each of them.
2780640	2788558	B	0.5774319767951965	Sorry for the whole thing, but it's just given by two running in parallel, then it's just the same as calculating the V of V for each individually and adding them together.
2788644	2791360	B	0.6002174019813538	So that certainly what we'd like to happen.
2791730	2794690	B	0.7287887334823608	And it just follows from the properties of these log boxes.
2795670	2802466	B	0.6768669486045837	More interestingly, there's the second way in which it's compositional, which is the sequential mode of plugging models together.
2802568	2811910	B	0.9080615639686584	So if we have an open model M one and some inputs into some outputs one, but those are now actually the inputs for the second model and we have these running.
2812060	2815960	B	0.6297398209571838	So the first generation model is passing stuff up to the second one.
2816330	2822970	B	0.891039252281189	And now we want to calculate that result VFE in terms of an observation.
2823710	2827002	B	0.8841782212257385	We can again write it as a sum of two of them, but in a slightly different way.
2827056	2829914	B	0.8012028336524963	So observation is just existing on the top wire, right?
2829952	2833126	B	0.776502788066864	Because it's just the output of the whole thing that gets this observation.
2833158	2834560	B	0.8152300715446472	So it's just on O two.
2835010	2845682	B	0.8620052337646484	So first we calculate the VFE for this model at the top m two in the usual way and then we add on a Vfe calculated for the first model, but it doesn't really have an observation one, right?
2845736	2850674	B	0.6953720450401306	But instead the observation it uses is one that's being passed down from M two.
2850712	2857526	B	0.8650351166725159	So that's the queue that M two is using is passed down now as if it's an observation down to M one.
2857548	2865720	B	0.9045806527137756	So it's kind of like O two receives this observation, does its updating about Q or whatever, and passes that down to M one.
2866810	2881930	B	0.8433792591094971	So in this way we can say that the VFE composers in that okay, both of these are minimizing VFE locally, where for the M one model we mean it's minimizing it with respect to these cues that are coming, for these O ones that are coming down from above.
2882350	2887680	B	0.5674946308135986	Then the whole system is also minimizing its VFE because it's just given by summing those two together.
2889570	2891214	B	0.8398953676223755	So I talked about a lot of stuff.
2891252	2894500	B	0.7227158546447754	Now I'm just going to wrap up now and then we can go to a discussion, I hope.
2895750	2903010	B	0.7854925990104675	So the main takeaway was just meant to be to try and show that these string diagrams provide some natural language for talking about active inference.
2904150	2911046	B	0.5306848287582397	And I would encourage you to try anyone working on active inference formally to take a look and see if they would be useful to you in some way.
2911228	2919350	B	0.7773773074150085	And in particular, I focused on some of the what I was calling the main ingredients of active inference, so that were generative models, the way you update them, and free energy.
2919420	2923990	B	0.8825850486755371	And we saw sort of ways you can describe all of those notions in the string diagrams.
2924070	2932826	B	0.9063112735748291	And the thing that I think is useful about them is that they give you a nice representational language for just drawing pictures of your generative models and composing them like hierarchical models and so on.
2933008	2936318	B	0.6945769190788269	But they also let you do the reasoning because you can do probability theory with them.
2936404	2941200	B	0.7738666534423828	So you can actually reason about what's going on in active inference just with the diagrams themselves.
2942850	2943738	B	0.7499878406524658	There's loads of directions.
2943754	2944482	B	0.6750603914260864	You can take this in the future.
2944536	2949742	B	0.8150545358657837	Obviously we could keep absorbing more of the work without the native inference into the diagrams.
2949886	2958630	B	0.5471598505973816	Bit more interestingly, we introduced this new notion at the end of how to make free energy compositional.
2958970	2966098	B	0.7401782274246216	In particular, we gave this definition of VFE for an open system now so it has a generative model which can have inputs.
2966114	2967506	B	0.8572948575019836	We call this the open Vfe.
2967618	2972250	B	0.5700253248214722	I just be very interested in what people think of this definition we introduced and whether it seems meaningful.
2973070	2978342	B	0.8013344407081604	Secondly, well, throughout the talk I kept talking about just minimizing free energy and that's all I said already, the Vfe.
2978486	2979914	B	0.6576873660087585	I didn't say how you do it.
2979952	2985226	B	0.8966799974441528	So in fact, this is normally done with these various algorithms of message passing algorithms.
2985338	2988446	B	0.5919330716133118	So they're an important part of active inference as well.
2988468	2994480	B	0.8390184640884399	And I think it would be great to include these in the setup by having some diagrammatic story of them.
2995730	2997394	B	0.7597951292991638	There's lots of other questions around.
2997432	3003278	B	0.8918795585632324	So one of them is that I talked about these two notions of updating with respect to soft observations.
3003374	3007406	B	0.5872051119804382	And I think normally people tend to focus on sharp observations, so they perhaps haven't.
3007598	3015106	B	0.5108187794685364	Not everyone has heard of these before, but it's very natural to treat the soft ones when you're working this compositional setup.
3015298	3023718	B	0.7926968932151794	And so there you start to wonder about which of these, the Pearl style updating or the Jeffrey style updating is more natural to think about in the context of cognition.
3023894	3028394	B	0.5726856589317322	And maybe we'll say, okay, well really VSE updating is the one you should be thinking about.
3028432	3034126	B	0.5773431062698364	That's probably the claim accurate inference would make, but it'd still be nice to think about how this relates to the other two.
3034148	3040430	B	0.9085841774940491	Is it best thought of as approximating the former or the latter style of sort of precise updating?
3041410	3042446	B	0.6547813415527344	And then finally we.
3042468	3045706	B	0.6258006691932678	Could try and connect this up to lots of further topics.
3045738	3049822	B	0.7403932213783264	I mentioned I'm a continuum, and we're interested in this notion of compositional intelligence.
3049886	3059394	B	0.6754089593887329	So it would be nice to connect this now to topics in AI and so on, and think about how it relates to other, basically applications of category theory in AI.
3059522	3060070	B	0.6596437096595764	In particular.
3060140	3071180	B	0.8142303824424744	There's also this whole world of categorical cybernetics I mentioned at the beginning, and I'd like to connect this a bit more precisely with what people are doing there with their stories in terms of lenses and so on.
3071630	3077334	B	0.6276343464851379	And something else we were also interested in that I mentioned is that we got into the topic by thinking about consciousness.
3077382	3093694	B	0.49508434534072876	And there's lots of ways, as a major theory of cognition, there's been just loads of proposals for how active inference is related to consciousness, and it'd be nice to see how those can be described formally and in this setup and whether the string diagrammatic approach helps you make any more sense of those.
3093812	3095700	B	0.9596872329711914	So that's something we'd love to do in future.
3096630	3101774	B	0.9717230200767517	But for now, I'd like to say thanks again to all of you for listening, and I'd love to go to a discussion.
3101822	3102420	B	0.6283750534057617	Thanks.
3107510	3110450	A	0.990931510925293	Thank you, Sean, for the wonderful presentation.
3111270	3112258	B	0.8529649972915649	Thank you.
3112424	3119720	A	0.8929864168167114	I will first pass to Ali for an opening remark, please.
3122170	3123670	C	0.8880262970924377	Thank you, Daniel.
3124570	3128630	C	0.9902490973472595	Thanks so much, Sean, for your really fascinating presentation.
3129290	3131094	C	0.9748266935348511	I truly enjoyed it.
3131292	3133400	C	0.8019393682479858	So I have a number of questions.
3134670	3140860	C	0.859824538230896	Let me begin by asking the first know.
3141230	3189542	C	0.475446879863739	When Bob Kirky and others took Hamiltonian formulation of quantum mechanics and kind of turned it into the string diagram formulation of it, namely ZX calculus, the claim was that regardless of its possible verity, but the claim was that one of the advantages of looking at quantum mechanics in terms of string diagrams, it's more than just a convenient way of looking at quantum formulation, and it actually unveils some properties of quantum mechanics that would be extremely difficult to see with Hamiltonian formulation.
3189686	3212990	C	0.5428994297981262	And even in some of their papers, they claim one of the reasons for somehow the stagnant development in quantum technologies and quantum theory is exactly related to the difficulty of working with Hamiltonian formulations.
3213070	3229762	C	0.819587767124176	So would you say string diagram formulation of active inference kind of takes a similar approach to somehow providing more than just handy tool for representing active inference modeling.
3229826	3255390	C	0.500672459602356	And actually it kind of opens up new possibilities for further developments of active inference theory, possibilities which would somehow, I don't know, impossible or at least extremely difficult for the current traditional formulation of active inference to see in the current formulation of the active inference.
3256770	3257326	B	0.6283750534057617	Thanks.
3257428	3258270	B	0.9059880971908569	That's an amazing question.
3258340	3259854	B	0.7128564119338989	Yeah, I would agree.
3259892	3268498	B	0.7803405523300171	I think my idea for work at Oxford is actually in this categorical quantum mechanics area I talked about so string diagrams for quantum theory and everything.
3268664	3285370	B	0.5685779452323914	And I agree that that language helps you talk about a lot of things that you would maybe never get round to so much in other mathematical formulations of quantum theory, basically things that make use of the tensor, as it were, the composition a lot.
3285440	3294934	B	0.5085949301719666	So if this led to stagnation in quantum theory, it's probably because people weren't focusing as much on the tensor and entanglement and stuff, which became very central.
3294982	3299182	B	0.6622098088264465	Obviously in the end that's what people needed to do, quantum computing and stuff.
3299236	3313214	B	0.8589115738868713	So now what people are doing with quantum theories includes quantum computing where they're drawing these circuits and so you're drawing they're basically like string diagrams describing sometimes these string diagrams, sometimes they just use the slightly different conventions for quantum circuits.
3313262	3317906	B	0.8511530756950378	But it's similarly a compositional language where you have got tensor products.
3318008	3324360	B	0.8920579552650452	So that as in things running in parallel states of these products so that you can talk about entanglement and so on.
3325850	3338966	B	0.5925687551498413	It's the language that makes it very immediate to represent that, because you just draw a box with two eyes and it encodes entangled state, and it makes you want to plug these things together and compose them, which is what you want to do in quantum computing.
3339158	3352686	B	0.6584130525588989	So I think similarly, if you never use that kind of language, you might think of a system often as a fixed thing and not about the way it interacts with other ones so much and that could lead to overlooking all sorts of things.
3352708	3354334	B	0.822666585445404	So that's true in any area.
3354532	3358494	B	0.678331732749939	And I think in active inference it's certainly very true.
3358532	3371254	B	0.6035007238388062	I think that it's natural to think compositionally in this way because you're wanting to talk about generative models being composed from pieces and you're maybe thinking about how the whole brain works in relation to interactions between parts of it and so on.
3371452	3377078	B	0.6927347183227539	So if you never used this kind of compositional view, there is stuff you would miss, I think.
3377244	3389420	B	0.6869765520095825	I think in some sense people weren't as behind already because they already were working kind of compositionally right, because they're using these Bayesian network diagrams like the Dags and the way they're normally drawn are very close.
3390110	3396502	B	0.5415439009666443	They are basically the string diagrams, they just don't do the equations and the rewriting of the diagrams.
3396646	3409054	B	0.7486495971679688	So it's not as far back maybe as quantum theory was, in the sense that people are thinking compositionally, but it feels like you just want to go one step further to having a fully compositional language you're working in.
3409092	3421214	B	0.5391401648521423	Where you have the advantage now that you can just talk about taking a whole model and plugging it into another one and it has a completely clear formal meaning and so on, which I think is what you want to do in areas like active inference.
3421262	3429670	B	0.6088228225708008	So going from the diagrams which are currently used to string diagrams is like the logical next step and in terms of new stuff it lets you do.
3429740	3433866	B	0.8743723034858704	I think an example is something like this open BFE thing, I guess.
3433888	3440758	B	0.7357004284858704	So if you're just always thinking about just a generative model meaning one without inputs.
3440934	3451902	B	0.6370729207992554	You might not think of a notion I'm not saying this is necessarily the right notion, but you might not think about this problem of how you want to give your definition for something that is allowed to have inputs as well.
3452036	3462094	B	0.6216815710067749	And once you have that have definition, you can sort of apply to parts of a composite system more easily, so it becomes more natural to use compositionally.
3462142	3468434	B	0.7029579281806946	So that's the kind of thing where without something like string diagrams, people can end up overlooking it.
3468472	3470514	B	0.5581514239311218	It wouldn't be impossible to do without them.
3470552	3478470	B	0.7029076814651489	I just need to talk about this notion of a kind of open generative model, which just means throwing away some mechanisms to make things be inputs.
3479050	3483610	B	0.4969449043273926	But you could miss it, but you really won't once you start thinking categorically.
3488590	3489494	A	0.8926437497138977	Thank you, Ollie.
3489542	3491260	A	0.6483423709869385	Please continue if you would like.
3494510	3494874	B	0.6283750534057617	Thanks.
3494912	3517250	C	0.8128880262374878	So, yeah, my other perhaps related question is comparing this kind of formulation to this recent formulation of constructor theory in terms of string diagrams or categorical formulation of constructor theory.
3518790	3531942	C	0.8187770247459412	Before going into this question, you see, you mentioned that this project is a part of a larger project for developing collective intelligence, right?
3532076	3567230	C	0.8323075175285339	So the similar kind of situation happens for constructor theory in which it is a kind of meta theory that tries to somehow discriminate between the possibilities of physical laws as opposed to counterfactual laws, and how physical laws, how there can be a theory, accounts for the emergence of possible physical laws.
3567390	3585190	C	0.9095615744590759	So in this sense, would you say this kind of formulation category, theoretical formulation, or possibly this specific String diagram formulation of active inference?
3585710	3592166	C	0.9040166139602661	Or maybe other theories of consciousness can be seen as a kind of providing a path?
3592198	3604480	C	0.7600492835044861	Toward developing a kind of meta theory of consciousness and possibly unifying many different strands of theories of consciousness into.
3606290	3606606	B	0.54979008436203	I.
3606628	3626978	C	0.663354754447937	Don'T know, a holistic picture that can somehow be compared and positively reconciled with one another and ultimately reaching the ultimate theory of consciousness.
3627074	3659150	C	0.7727832198143005	Or, I don't know, do you see this line of work providing enough evidence for this line of development research or, I don't know, somehow maybe even not specifically consciousness, but unifying the different aspects of cognition, intelligence and consciousness altogether?
3663490	3664574	C	0.8127058148384094	What would you say?
3664692	3665360	B	0.6283750534057617	Thanks.
3665810	3668770	B	0.6872168779373169	Keep giving me ideal selling points.
3668840	3672100	B	0.7703772783279419	So, yeah, that's also something I would like to say.
3674390	3684818	B	0.49950018525123596	I tend to think of it that way and that my background is in applying category theory to just lots of topics and I so naturally do think of it as quite a unifying language.
3684914	3695638	B	0.8504464030265808	And the grant on consciousness that I mentioned was building on earlier work we did on looking at integrated information theory of consciousness, which in the end basically was done in terms of categorical probability.
3695734	3698774	B	0.7761313915252686	So it's like the same setup of the diagrams.
3698902	3702522	B	0.83404940366745	And so we kind of wanted to do the same thing for directive inference.
3702586	3707850	B	0.7005751132965088	So there it's like we've taken both of these things and put them in this common language.
3707930	3710510	B	0.8219366073608398	You could have put them in the common language of probability theory before.
3710580	3716658	B	0.5022164583206177	But I think I do have an intuition that there is something more clear about it.
3716664	3724674	B	0.8322349786758423	Does make it easier to get a conceptual grasp of both theories, I think, once you've done it this way and somehow also, yeah, the diagrammatic view does make it much easier to compare them.
3724712	3731826	B	0.6547775864601135	So the hope was basically to, and still is to keep going and to keep understanding various notions in that language.
3731858	3735254	B	0.8725429773330688	So there's things I've looked at in cognitive science I've also done this way.
3735292	3742570	B	0.885276198387146	So this theory of conceptual spaces, garden force have worked on treating that in terms of diagrams and so on.
3742640	3748746	B	0.8178245425224304	So I would love to see basically many theories put into this language to make it easier to compare them.
3748928	3754350	B	0.8393456339836121	You could try and compare them directly already, but I think you want one clear formalization to put them all in.
3754420	3763262	B	0.7209853529930115	And I would say that the categories and diagrams is the right one to pick because it tends to just give a very clear conceptual view of things.
3763396	3772050	B	0.6005072593688965	The question is whether you have some theory that's very important, where the things categories are good at, just doesn't quite capture the essence of what you want to talk about there.
3772120	3781286	B	0.81524658203125	But for things like active inference and IIT, so far it seemed very natural because on the case of IIT, it's about talking about how integrated something is.
3781308	3785282	B	0.5462911128997803	So you basically want to talk about the opposite of that, which is something being decomposed.
3785346	3791174	B	0.848720908164978	And the diagrams basically talk about parts and how they're related, which is what you need to make sense of that notion of integration.
3791302	3792860	B	0.6884379386901855	So it's very natural there.
3793310	3800874	B	0.7260790467262268	But yeah, I would love basically to see various aspects of cognitive science understood categorically.
3800922	3803520	B	0.9392194747924805	That's something I'd love to do myself as well.
3805090	3811786	B	0.7077903151512146	And the hope would be then to try and gain insights from all of them and build a theory.
3811818	3816526	B	0.7425745725631714	It's not the category theory itself is a theory of cognition or consciousness.
3816558	3819522	B	0.8706119060516357	It's just a very useful language for relating them.
3819576	3826920	B	0.9507311582565308	And then it would be very exciting to see something natively defined in terms of category theory as well at the end.
3827370	3837798	B	0.8559056520462036	And there's a feeling that some of what's going on in applied category theory, I think, like in categorical, cybernetics and so on, is kind of taking that approach for perhaps some of the first time.
3837884	3843514	B	0.7576070427894592	Previously, I've always thought category theory is basically you take existing things and you get a really nice abstract view of them.
3843632	3852080	B	0.7685874700546265	But now I think people are comfortable enough with it that they're sort of defining things categorically from the outset in areas like that.
3855410	3856062	A	0.918424665927887	Awesome.
3856196	3856880	B	0.6177208423614502	Well.
3858770	3861026	A	0.8879646062850952	Yeah, I have many thank you.
3861208	3862450	B	0.8819260597229004	These are great questions.
3862600	3864180	B	0.6098284721374512	There's like ideal questions.
3866790	3867714	A	0.789476215839386	You'Ve pointed in.
3867752	3891020	A	0.8805907368659973	And we've explored a little bit of the utility and the simplicity and how that could help with accessibility and rigor and applicability all these awesome things leading to reaccounting and reframing consolidating as well as discovering some new trails between, for example, expected free energy energy and variational free energy energy.
3891710	3906990	A	0.7165302038192749	Looking at the equations, you might be able to say that they rhyme, but you would be many, many lines deep into understanding what, if any, generalizations could encompass the both of them.
3907140	3910820	A	0.763874888420105	So that was just a very salient example.
3912790	3915746	A	0.7852047681808472	A few different kinds of questions.
3915928	3920590	A	0.8501245379447937	So how is time treated in category theory?
3920670	3928146	A	0.8580530285835266	Or how does active inference treat time today and how do you see the way that time is treated?
3928258	3931634	A	0.8730413913726807	We talk about discrete time and continuous time generative models.
3931682	3937622	A	0.8482301235198975	Then there's the past, present and future multi agent systems federated or asynchronous communication.
3937686	3945210	A	0.8937756419181824	So how is time treated and how does that give us a different grasp on dynamical modeling?
3946590	3947500	B	0.8529649972915649	Thank you.
3948990	3950586	B	0.5073899030685425	I'd love to have a better answer for that.
3950608	3967010	B	0.5759204626083374	Basically, I think it's a tough one at the moment in the talks if I just talked about discrete time and that's sort of very easy to treat with the Bayesian network setup and with these kind of string diagrams because you can just lay out the discrete time steps as processes in your picture like we see here.
3967080	3980306	B	0.584625780582428	We have the end time steps here, but I don't have anything satisfying worked out yet to say about how you would treat a continuous time case, which I think is important in active inference.
3980338	3985302	B	0.8189148902893066	You'd like to basically take.
3985356	3993514	B	0.7689542174339294	I guess basically what you want to do is take the way that you describe this thing with the end time steps and kind of have a formula for folding it together and just saying, okay, but you're unpacking this.
3993552	3994794	B	0.7181234359741211	Thing n times.
3994992	3999338	B	0.8161206841468811	And then you can take that thing and imagine this abstract view of unpacking.
3999354	4008990	B	0.8031883239746094	It just not discreetly anymore in this continuous way, so that you can capture something like the differential equation kind of definition of continuous time thing in active inference.
4011030	4011780	B	0.6702859997749329	I?
4014550	4023762	B	0.7697437405586243	Yeah so you can certainly work with continuous time things in the sense of, you know, the stuff going on in categorical cybernetics or sort of categorical systems theory I guess it would be called.
4023896	4035746	B	0.8295966386795044	Act world is kind know it has continuous time dynamical systems and talks about plugging them together but that diagram is sort of just relating their variables is my understanding.
4035778	4043082	B	0.8150873184204102	It's not like a diagram isn't exactly showing the time and in some sense they kind of have to synchronize, I think.
4043216	4045066	B	0.529462993144989	It's not an area I'm totally familiar with.
4045168	4054894	B	0.8883567452430725	So it would have been really cool to basically have this work and then have another part of it talking about like we've done for discrete time here.
4054932	4065474	B	0.8231412768363953	Having a nice description of a continuous time case, I think it will end up being some work to take that into account there would be really nice to see.
4065512	4070706	B	0.8026242256164551	So it just needs the right abstraction, I think, for taking a picture like this.
4070808	4080578	B	0.795217752456665	Not drawing the time steps as like bits in your diagram, but just saying that it's like this b thing with like a feedback loop basically is what this is describing.
4080674	4095210	B	0.8955312967300415	And then giving a semantics to that in terms of time evolution, to give a continuous version of this, for example, with like state unfurling continuously and observations for each time step in general.
4095280	4099206	B	0.7550824284553528	I wouldn't say there's like an answer to the question of how is time treated in category theory.
4099238	4112910	B	0.49044185876846313	There wouldn't really be one answer because category is going to be so generally they tend to be very effective for discrete things in general, like algebra and so on, because they kind of are discrete in some sense, like the composition is discrete.
4112990	4118722	B	0.6258716583251953	So continuous aspects and things like continuous time tend to be more difficult in a sense.
4118776	4121826	B	0.8235122561454773	Or they're just sort of inside the morphisms, as it were.
4121848	4124082	B	0.491485059261322	They're not in the composition.
4124146	4127430	B	0.6422141194343567	So it doesn't end up looking like this when you're composing continuously.
4129370	4136842	B	0.8236664533615112	But yeah, I think there will be people in act who sort of would come at you with a particular answer.
4136896	4144140	B	0.5531595349311829	So they've got a way they like to treat continuous time that I'm just not familiar with yet.
4145310	4146060	B	0.84200119972229	Cool.
4147070	4150560	A	0.862040102481842	A little bit of a more educational or applied question.
4151010	4155246	A	0.9124221801757812	So how do we go about drawing and learning to draw?
4155348	4157274	A	0.8744969964027405	Is there a software package?
4157402	4167378	A	0.903304934501648	Is there a way that we can get a step by step process to building that familiarity with like when I see this shape, then here's what I know.
4167464	4170500	A	0.566491961479187	And then how do we know what we can and can't do?
4170950	4177894	A	0.7847634553909302	And does that drawing software flag us or do we need to send it to a friend?
4178012	4179350	A	0.826673150062561	So how do we look at something?
4179420	4195046	A	0.7379916310310364	And then part one, build up the motifs in our own aesthetic understanding so that we can understand the compositionality of this as you do today and as we all do today, for example, for Language English.
4195238	4206398	A	0.8144594430923462	And then part two, how do we go from having built that motif based compositional understanding to like now what can we do?
4206484	4212394	A	0.7418468594551086	And then when are we just totally freewheeling and off the rails of the free energy principle?
4212442	4215620	A	0.8416452407836914	Or does anything go if the motifs allow it?
4216710	4219442	B	0.6224762201309204	Yeah, I wish I should have the standard answer.
4219496	4222626	B	0.6824057102203369	The best way to learn string diagrams, I think, if I'm going to talk about it like this.
4222648	4224850	B	0.8305585384368896	So you prompted me to come up with that.
4224920	4226326	B	0.6037938594818115	I don't have something on top of my head.
4226348	4226966	B	0.89243483543396	That's the best way.
4226988	4238646	B	0.682380199432373	But there's so much stuff out there I think it tends to be because if you want to get really comfortable with the diagrams, you're learning category theory in some sense, but it's not like you need to learn all of category theory.
4238678	4260634	B	0.9267618656158447	It's kind of a relatively modern offshoot in this applied category theory world that's very diagrammatically focused and there will be various nice introductions out there to using them another way is to think I'm pretty sure recently yeah, there was a nice paper that came out, there was an introduction to string diagrams for computer scientists, for example.
4260692	4271266	B	0.8485509753227234	So there tends to be different introductions kind of for different audiences because they just want to pick categories that those people are familiar with.
4271288	4271426	B	0.5664746165275574	Right.
4271448	4272862	B	0.8036918640136719	So they can actually have some examples.
4272926	4276850	B	0.6124275922775269	You could just learn the diagrams totally abstractly, but it helps to have some examples.
4277590	4284706	B	0.768734872341156	And the old category free textbooks are all things mathematicians have looked at and other people haven't heard of, so they're not particularly helpful.
4284818	4291370	B	0.8971438407897949	So there's know Bob has paper categories for the practicing physicist that's aimed at physicists that would basically introduce string diagrams to them.
4291520	4293274	B	0.8779372572898865	There's this recent computer science one.
4293312	4303760	B	0.763511598110199	I know there's some work going on in producing one for cognitive science, which I think would be really good having an introduction to the string diagrams for those people.
4304450	4309006	B	0.7256626486778259	So you basically look for one in an area you're comfortable with and you find a good paper on it.
4309028	4318926	B	0.8564761877059937	But it would be nice to have a good online resource, I guess, right, that gathers these together so people can just see a great guide for all the introductions.
4319118	4327078	B	0.6766043901443481	If you do something like there's courses you can do in the sense of the Bob's book.
4327164	4332562	B	0.8557758331298828	In the case of learning quantum, there's something like Bob's Long book with Alex Kissinger picturing quantum processes.
4332706	4337014	B	0.7515219449996948	That's the kind of thing I learned from like it was in the form of a lecture course.
4337052	4349514	B	0.4800373315811157	But it's basically the same book because then there's just loads of exercises that will make you have to reason with string diagrams and then you pick the rules up because at first you don't have the same intuition, obviously, but the rules what can I do with these?
4349552	4351562	B	0.8901568055152893	Can I slide them around like this or whatever?
4351696	4355486	B	0.85881108045578	But it doesn't take too long to get quite used to it, I think, which is the nice thing about them.
4355508	4361790	B	0.7580779194831848	They're kind of natural, they're just these elastic strings and boxes and you have that sort of geometrical intuition.
4362210	4366020	B	0.5407649874687195	So things like that with exercises are the way I'd recommend getting used to using them.
4366870	4375480	B	0.6808321475982666	I didn't use any software in a sense of the diagrams I draw in this program called Tixit, but it doesn't tell you how string diagrams will work or anything, it's just for drawing them.
4376970	4380680	B	0.6929585337638855	But I know there's more work to develop.
4381370	4393290	B	0.6607130169868469	Libraries like the Algebraic Julia project is sort of like an applied category theory language, but I wouldn't know if it was recommended as a way to first learn categories.
4395150	4395610	B	0.5491447448730469	Yeah.
4395680	4402560	B	0.8421439528465271	So I would recommend finding a nice introductory paper in whatever field you're most used to playing with some exercises to get really used to them.
4403170	4406766	B	0.8771063089370728	For causal models, there's this paper Robin and I put out.
4406868	4412986	B	0.7661369442939758	It's not necessarily the very first place to learn string diagrams, but the aim is to introduce to people who've heard of causal models.
4413018	4413838	B	0.8292443752288818	So in a sense of pearl.
4413854	4421278	B	0.859939455986023	So just Bayesian networks, basically, but maybe the course interpretation of them to get them used to string diagrams.
4421454	4425960	B	0.665991485118866	And this paper hopes to be a little bit introductory as well.
4428170	4428918	B	0.84200119972229	Cool.
4429084	4430280	A	0.6883665323257446	Ali, please.
4433690	4434370	C	0.8529649972915649	Thank you.
4434460	4440170	C	0.8391515016555786	So getting back to the question about the time representation in this formulation.
4440830	4458762	C	0.8702214956283569	So I take it that this kind of formulation of Bayesian inference, I mean, category theoretical formulation of Bayesian inference is largely based on tobi's Fritz definition of Markov categories as CD categories.
4458826	4459450	C	0.5664746165275574	Right.
4459620	4480550	C	0.8502271175384521	So as far as I understand it, Fritz paper kind of one of its basic assumptions is this kind of unidirectional inference, I mean, from earlier times to later times, right, or in other words, the prediction.
4480970	4499790	C	0.8679490685462952	But in quantum formulation of active inference or quantum active inference, there's this attempt to also develop the retradiction aspect of inference as well.
4499860	4500574	C	0.5664746165275574	Right.
4500772	4511742	C	0.8655063509941101	So would you say this recent formulation can also be accounted for, this kind of retradiction?
4511806	4519460	C	0.8608561158180237	In other words, can this formulation be reconciled with quantum Bayesianism as well?
4521030	4524030	B	0.4791574776172638	Yeah, I basically wish yeah, sorry, go ahead.
4524120	4557120	C	0.8046364188194275	Because to add one more context here, I think it was in Kirk and Speckin's paper, there was this clear distinction between classical Bayesian inference and non classical Bayesian inference, in which the classical one does not allow for the retrodiction, but non classical Bayesian inference can be applied for both prediction and retrodiction as well.
4558370	4566162	B	0.6864413022994995	Okay, yeah, I would love to be a bit more familiar with the quantum active inference stuff, basically, to compare a bit.
4566216	4573678	B	0.5506672859191895	So I'm not as familiar with the sort of retro sorry, what was the other version of prediction?
4573694	4574610	B	0.6537162065505981	It's retro.
4574950	4578050	C	0.8091761469841003	Retrodiction, yeah, prediction and retradiction.
4581690	4583430	B	0.7506954669952393	I would have to compare with this.
4583580	4592540	B	0.8824069499969482	I know the paper you mean Bob's Paper Frog on both forms of Asian inference to see what they say there about the classical one.
4593390	4599502	B	0.727304995059967	Can you give some intuition as to why it isn't something you can do classically, basically, the retro one?
4599556	4604974	B	0.7943804264068604	Because if that's a general case about probabilities, then it will be true in some sense here.
4605012	4605214	B	0.5664746165275574	Right.
4605252	4608986	B	0.6626707315444946	So here it's just being modeled in this probabilistic category.
4609098	4617074	B	0.896934449672699	And so at the moment they're separate in that you have the model which basically goes forward and then you do your updating to try and approximate something going back.
4617112	4618980	B	0.5694591999053955	But you don't really have like this one.
4625610	4635670	C	0.8151670098304749	The whole idea was that for predictive quantum mechanics, we only need to account for the inference from earlier times to later times.
4635820	4655550	C	0.7277430295944214	But if we want to account for retrodictive quantum mechanics as well, we need to somehow account for because as we know, not every quantum formulation follows the Bell's principle of local causality.
4658930	4670900	C	0.879059374332428	In order to account for all the entanglement phenomenon so on, we need to somehow put this bi directional inference into our model.
4671430	4678470	C	0.8343360424041748	So, yeah, that was the basic idea behind developing this kind of non classical Bayesian inference.
4679290	4684498	B	0.8866345286369324	Does it have something to do with the unitary evolution in quantum theory the way that you have this reversible thing?
4684524	4692890	B	0.8049405813217163	Or is it exactly that was the gist of it, yeah.
4693040	4698438	B	0.4902489185333252	And so you don't expect to have something like that classically, basically, where you have this reversible thing built in.
4698464	4713070	B	0.510179877281189	Well, yeah, so I wouldn't expect to see that exact feature here in the sense of if it's treated, if it's basically something that you can't have in classical probabilities, it won't exist in this category matar plus.
4713220	4721326	B	0.9160470962524414	I think that would be basically the same category they would use in that paper, and they work with dagger combat categories, and they'll work with something like this Matar Plus category.
4721358	4739450	B	0.7911142706871033	But the classical case, if it's just a general it's less of a sort of physical notion, but it's just an idea that the model comes with a forward part and a backward part, then I think that's the kind of here how you go from a forward part to approximate this backward thing.
4739520	4745718	B	0.8019591569900513	But the sort of lens type view of what's going on that's more studied in category cybernetics would be imagining.
4745734	4760740	B	0.8979018926620483	I think the model kind of carrying this backward inference process with it as well, so that for each forward part of the model, you would have this approximate inference sort of channel stored with it.
4761270	4766260	B	0.805831253528595	So I don't know if that would address what you're asking, but it would have a backward and forward part together.
4770070	4781174	A	0.9017342925071716	Well, Ollie, do you have any kind of closing opening remarks or questions or where do you see this going from the active inference side?
4781292	4795130	A	0.897836446762085	What does this bring to us and what is opened through what has happened largely this year in active inference and category theory?
4796430	4802010	C	0.9861537218093872	Well, actually, I'm really excited to see this line of development in active inference theory.
4802090	4812180	C	0.8531168699264526	And as you know, I'm a big, big fan of meta theories and all kinds of unification theories and so on.
4815430	4851866	C	0.8925366997718811	I don't know, I kind of have this feeling, have this hunch that this line of development in active inference theory, it looks quite promising, especially for kind of tying up all the loose ends and transcending many, many other areas and discourses and ultimately reaching a kind of coherent picture of quote unquote reality, whatever it means.
4852048	4876898	C	0.910009503364563	So, yeah, these kinds of development, I mean, the last year we had tremendous advances in Bayesian mechanical theories, and in recent months we have this fabulous line of research in category theoretical account of active inference.
4877074	4888870	C	0.7546878457069397	My hope is that ultimately these different strands can be unified into coherent and overarching framework.
4889030	4891020	C	0.974602222442627	So exciting times.
4898270	4907422	B	0.8261168599128723	So do you mean that you're thinking of it as it sounded like you're basically alluding to the work going on in cognition and work going on in physics coming together.
4907476	4907694	B	0.5664746165275574	Right.
4907732	4911034	B	0.7302932143211365	Like one really meta really open.
4911092	4912580	C	0.5801404118537903	Exactly, yeah.
4913110	4933750	C	0.7509944438934326	The idea behind Bayesian mechanics, one of its premises or assertions was that there isn't any clear distinction between cognitive and non cognitive things or agents and they rest on a continuum.
4935310	4947078	C	0.8599416017532349	The same kind of mathematical technology can be applied both for inert and conscious agents or sentient agents or whatever we choose to call them.
4947264	4975800	C	0.8699175715446472	So, yeah, this overarching theory unveiled many interesting phenomena regarding, well, self organizing systems, and it changed the whole perspective about how we can look at and even define consciousness, cognition, intelligence, sentience, and all of these related terms.
4976250	5005070	C	0.9391528367996216	So my hope is that category theoretical account of active inference can also be used for clearly seeing many of these emerging elements in Bayesian mechanics and active inference theory and hopefully, well, gaining some interesting and potentially groundbreaking insights.
5006690	5007610	B	0.9583930373191833	That'd be wonderful.
5007690	5015074	B	0.9307308197021484	Yeah, I'd love to apply for those topics, and I'd be very curious to see how categories can come in.
5015112	5015774	B	0.49402984976768494	Sorry, Daniel.
5015822	5016082	B	0.5491447448730469	Yeah.
5016136	5016786	B	0.5585339665412903	Oh, yeah.
5016888	5019374	A	0.8909866213798523	I'll just give my closing thoughts then to you, Sean.
5019422	5023480	A	0.759991466999054	Just a few loose notes that, again, open probably more than they close.
5024250	5039510	A	0.6177279949188232	Ali was right in suggesting and expressing that Bayesian mechanics recently has helped us develop a continuum of active and passive systems, so called living and non living, or inanimate and animate.
5039670	5048462	A	0.7826852202415466	And that brings us to another dialectic to resolve, which is life and mind, which is where the physical and the cognitive science come together.
5048596	5050154	A	0.7609891891479492	You said they're on a continuum.
5050202	5052270	A	0.8764525651931763	Maybe we could say they're on a quantinium.
5052850	5059070	A	0.6312816143035889	And what language could express such work?
5059220	5064366	A	0.8879700899124146	Well, right now we're speaking in English with the active inference ontology dialect.
5064558	5068046	A	0.5292877554893494	However, the phonemes are not intrinsically meaningful.
5068238	5073062	A	0.6354507803916931	The M in a Markov blanket or category does not mean something.
5073196	5074920	A	0.7422618865966797	It's a sound.
5075530	5099246	A	0.6302178502082825	And so the string diagram language and representation I see as a way to fuse and integrate semantics into the syntax of the actual inscription, which enables us to generalize in new ways.
5099428	5103374	A	0.7025424242019653	Also, recognizing string diagrams are not everything, and so on.
5103572	5129190	A	0.7420673370361328	And then with all of these intersecting vectors from the cognitive and the physical sciences, we are able to take the compositional cartographic approach for cognitive ecosystems and talk about diverse intelligences, biological, quantum, classical architectures, all of these synthetic intelligences.
5129530	5158720	A	0.9839764833450317	And so it's super exciting, and I appreciate again your visit and look forward to people's curiosity taking them and also the development of tools and educational materials that make this easier and then being able to display and use something where the meaning is primal rather than like, well, this letter represents this.
5160150	5175220	A	0.843856155872345	It already introduces such a space between the analytical representation and really the string diagram, which exists isomorphically with it.
5179190	5182310	B	0.9845007658004761	Yes, I find this very exciting way of thinking.
5182460	5197882	B	0.7442461848258972	It sounds like you're advocating a kind of structural ontology kind of thing in some sense, right, where you're taking the compositional structure of what's going on to really be the meaning or really be the real thing that's there, not just like I don't know.
5197936	5206086	B	0.7785747647285461	Yeah, we could talk about it for a while, I imagine, but I would love to see string diagrams and other approaches.
5206118	5209142	B	0.8015651106834412	I'm sure that take that role.
5209206	5212580	B	0.9813315868377686	And you've got me very excited about this kind of unification that's going on.
5214630	5215854	A	0.9490945339202881	Thank you again, Sean.
5215902	5219300	A	0.9740564227104187	You're always welcome, and we look forward to seeing where this all goes.
5219670	5220322	B	0.5491447448730469	Yeah.
5220456	5221474	B	0.9715058207511902	Thanks again for having me.
5221512	5221810	B	0.5491447448730469	Yeah.
5221880	5223150	B	0.9330620765686035	Really great discussion.
5223310	5224180	B	0.8529649972915649	Thank you.
5224790	5225970	C	0.9793016910552979	Thank you so much.
5226120	5226878	B	0.7680783271789551	Thanks, Ali.
5226974	5227394	B	0.4896697998046875	All right.
5227432	5227694	B	0.5137446522712708	Bye.
5227742	5227840	B	0.6283750534057617	Thanks.

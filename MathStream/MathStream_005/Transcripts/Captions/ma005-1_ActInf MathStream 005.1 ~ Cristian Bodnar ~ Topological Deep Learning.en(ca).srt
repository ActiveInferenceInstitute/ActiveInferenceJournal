1
00:00:02,220 --> 00:00:06,144
Hello and

2
00:00:06,182 --> 00:00:09,696
welcome. It's July 17, 2023.

3
00:00:09,798 --> 00:00:11,356
We're here in Active Inference math

4
00:00:11,388 --> 00:00:14,588
stream number 5.1 with Chris Bodner

5
00:00:14,684 --> 00:00:17,532
on topological deep learning graphs,

6
00:00:17,596 --> 00:00:20,800
complexes, and sheaves. So thank you for

7
00:00:20,870 --> 00:00:22,832
joining, Chris. Looking forward to your

8
00:00:22,886 --> 00:00:24,720
presentation and discussion.

9
00:00:25,780 --> 00:00:28,584
Yeah, thanks so for having me. So,

10
00:00:28,622 --> 00:00:32,824
yeah, as I was just saying, this is my

11
00:00:32,862 --> 00:00:35,336
PhD thesis, which I finished a couple of

12
00:00:35,358 --> 00:00:37,080
months ago. It's also kind of public

13
00:00:37,150 --> 00:00:39,896
online. So if you want to go into the

14
00:00:39,918 --> 00:00:41,736
details, just kind of look this up on

15
00:00:41,758 --> 00:00:43,948
the Internet and you should be able to

16
00:00:43,954 --> 00:00:46,376
find it easily. Obviously, there's lots

17
00:00:46,408 --> 00:00:48,956
of stuff in there. So I kind of try to

18
00:00:49,058 --> 00:00:52,332
give an overview today and maybe also go

19
00:00:52,386 --> 00:00:54,112
in a little bit more detail in certain

20
00:00:54,166 --> 00:00:57,504
aspects since there's not a lot of time

21
00:00:57,542 --> 00:01:00,544
to go through. Yeah,

22
00:01:00,582 --> 00:01:03,008
now I'm Microsoft Research, so this is

23
00:01:03,094 --> 00:01:04,628
basically all work that I did in the

24
00:01:04,634 --> 00:01:06,676
past and when I was at University of

25
00:01:06,698 --> 00:01:10,068
Cambridge. All right, so let's get

26
00:01:10,234 --> 00:01:14,544
started. Right, so let's

27
00:01:14,592 --> 00:01:17,924
start very easily. Now, I'm actually not

28
00:01:17,962 --> 00:01:19,176
sure exactly what's kind of the

29
00:01:19,198 --> 00:01:20,744
background of the people who are

30
00:01:20,782 --> 00:01:24,330
watching, but in machine learning

31
00:01:25,020 --> 00:01:27,224
there's all these kind of subfield that

32
00:01:27,262 --> 00:01:28,824
emerged a few years ago which is called

33
00:01:28,862 --> 00:01:32,204
geometric deep learning, which is

34
00:01:32,242 --> 00:01:33,980
essentially looking at how to apply

35
00:01:34,050 --> 00:01:35,288
these kind of deep learning. Neural

36
00:01:35,304 --> 00:01:38,204
network architectures on data, living on

37
00:01:38,242 --> 00:01:42,712
all sorts of structures

38
00:01:42,776 --> 00:01:45,970
or geometries or spaces if you want.

39
00:01:46,580 --> 00:01:48,288
And this has a lot of application,

40
00:01:48,374 --> 00:01:50,016
especially in the life sciences. And

41
00:01:50,038 --> 00:01:52,556
there's kind of been a lot of instances

42
00:01:52,588 --> 00:01:55,084
of this in kind of very famous

43
00:01:55,132 --> 00:01:56,624
publications, some of which you see

44
00:01:56,662 --> 00:01:59,136
here. But just to give some examples,

45
00:01:59,168 --> 00:02:01,364
for instance, if you have proteins or

46
00:02:01,402 --> 00:02:03,156
molecules or things like that, they

47
00:02:03,178 --> 00:02:06,356
usually represented as graphs. And you

48
00:02:06,378 --> 00:02:07,924
kind of have some data living on these

49
00:02:07,962 --> 00:02:09,496
graphs, like kind of the properties of

50
00:02:09,518 --> 00:02:12,904
certain atoms and so on. So these

51
00:02:12,942 --> 00:02:16,504
kind of things and so far, these kind

52
00:02:16,542 --> 00:02:20,028
of spaces or these kind of problems,

53
00:02:20,114 --> 00:02:22,076
learning problems, if you want, they

54
00:02:22,098 --> 00:02:24,684
have been approached mostly kind of with

55
00:02:24,722 --> 00:02:26,636
a geometrical mindset, as the kind of

56
00:02:26,658 --> 00:02:30,380
name of this subfield also mentions.

57
00:02:31,200 --> 00:02:34,924
But something that I would argue

58
00:02:34,972 --> 00:02:37,232
is that geometry is not everything that

59
00:02:37,286 --> 00:02:39,356
you need and there's kind of other non

60
00:02:39,388 --> 00:02:42,016
geometrical aspects when you are in such

61
00:02:42,038 --> 00:02:45,696
a setting. And this is kind of

62
00:02:45,718 --> 00:02:48,356
quite obvious once you realize that the

63
00:02:48,378 --> 00:02:50,612
spaces that kind of show up in the field

64
00:02:50,666 --> 00:02:53,572
and in many applications, they are very

65
00:02:53,626 --> 00:02:55,828
heterogeneous. So, as I mentioned, for

66
00:02:55,834 --> 00:02:57,300
instance, you could have graphs that

67
00:02:57,370 --> 00:02:59,108
could represent anything. In this case,

68
00:02:59,194 --> 00:03:01,336
it's the caffeine molecule that you see

69
00:03:01,358 --> 00:03:04,424
here on the left. And you want to have

70
00:03:04,462 --> 00:03:05,800
some models that predict certain

71
00:03:05,870 --> 00:03:07,832
properties of this molecule and so on.

72
00:03:07,966 --> 00:03:09,572
But for instance, you can have grids.

73
00:03:09,636 --> 00:03:12,552
And we see grids all the time and data

74
00:03:12,606 --> 00:03:13,916
living on grids. And I'm referring to

75
00:03:13,938 --> 00:03:16,172
images, videos, they are all kind of

76
00:03:16,306 --> 00:03:19,356
pixels living on a grid. And then you

77
00:03:19,378 --> 00:03:21,068
can have more sophisticated things. You

78
00:03:21,074 --> 00:03:22,960
could have some meshes, for instance.

79
00:03:24,260 --> 00:03:25,964
They're all over in computer graphics

80
00:03:26,012 --> 00:03:28,176
and then you could have some sort of

81
00:03:28,198 --> 00:03:30,284
manifold. So for instance, if you're

82
00:03:30,332 --> 00:03:31,984
doing maybe weather modeling or

83
00:03:32,022 --> 00:03:35,584
something, we live on a sphere,

84
00:03:35,632 --> 00:03:38,756
topologically speaking. So you might

85
00:03:38,778 --> 00:03:40,836
want to model your data as living on a

86
00:03:40,858 --> 00:03:42,310
sphere and so on.

87
00:03:44,360 --> 00:03:47,616
But nonetheless, even if these spaces

88
00:03:47,648 --> 00:03:50,116
are kind of geometrically, kind of

89
00:03:50,138 --> 00:03:52,388
heterogeneous, and some of them don't

90
00:03:52,404 --> 00:03:54,296
even have a geometrical structure in

91
00:03:54,318 --> 00:03:56,264
kind of a strict mathematical sense,

92
00:03:56,382 --> 00:03:57,496
they all have what's called a

93
00:03:57,518 --> 00:03:59,128
topological structure, which is kind of

94
00:03:59,134 --> 00:04:01,320
like kind of a weaker kind of structure,

95
00:04:02,220 --> 00:04:05,208
but it's kind of more general. And I'm

96
00:04:05,224 --> 00:04:07,576
going to talk a bit in a few seconds

97
00:04:07,608 --> 00:04:09,212
about what that means by in general.

98
00:04:09,266 --> 00:04:10,536
When you do kind of mathematical

99
00:04:10,568 --> 00:04:12,556
physics, you kind of have a ladder of

100
00:04:12,658 --> 00:04:15,804
structures where you kind of keep

101
00:04:15,842 --> 00:04:17,964
building on top and the more structure

102
00:04:18,012 --> 00:04:19,968
you have, the more sophisticated things

103
00:04:20,054 --> 00:04:21,968
you can do and so on. And kind of at the

104
00:04:21,974 --> 00:04:23,900
base of this diagram just have sets,

105
00:04:23,980 --> 00:04:25,480
just kind of a collection of elements

106
00:04:25,580 --> 00:04:28,196
with no kind of extra structure and then

107
00:04:28,218 --> 00:04:31,120
you kind of keep going up in this ladder

108
00:04:31,200 --> 00:04:34,692
and you add stuff on top of sets and

109
00:04:34,746 --> 00:04:37,636
so on. And as I was saying, kind of most

110
00:04:37,658 --> 00:04:39,364
of the work is kind of focused on maybe

111
00:04:39,402 --> 00:04:41,736
the top levels of this hierarchy, but

112
00:04:41,758 --> 00:04:44,024
it's kind of topological level, which

113
00:04:44,062 --> 00:04:46,328
are kind of the weakest kind of level

114
00:04:46,414 --> 00:04:48,936
you can add on top of sets has kind of

115
00:04:48,958 --> 00:04:51,956
been neglected to a large extent. And

116
00:04:51,998 --> 00:04:54,184
part of what I've been doing in my PhD

117
00:04:54,232 --> 00:04:57,036
thesis was essentially looking at these

118
00:04:57,058 --> 00:04:58,556
kind of learning problems on these kind

119
00:04:58,578 --> 00:05:00,312
of spaces from a more topological

120
00:05:00,376 --> 00:05:03,680
perspective and kind of try to fill in

121
00:05:03,830 --> 00:05:06,704
these blanks. So this is kind of the

122
00:05:06,742 --> 00:05:10,096
overview, okay, so if we are to

123
00:05:10,118 --> 00:05:11,836
adopt this topological perspective,

124
00:05:11,948 --> 00:05:13,936
what would that actually mean or how

125
00:05:13,958 --> 00:05:16,464
would that look like? I guess it could

126
00:05:16,502 --> 00:05:18,608
look in different ways, but in what I've

127
00:05:18,624 --> 00:05:20,452
done, in my cases, it looks kind of like

128
00:05:20,506 --> 00:05:23,360
this. So we have horizontally,

129
00:05:23,440 --> 00:05:26,036
we kind of have a space that could be

130
00:05:26,058 --> 00:05:27,156
anything. This is just kind of an

131
00:05:27,178 --> 00:05:29,930
abstract space. It could be a grid or

132
00:05:30,300 --> 00:05:32,852
things like we've seen in the previous

133
00:05:32,916 --> 00:05:35,204
example and kind of vertically attached

134
00:05:35,252 --> 00:05:37,144
to the regions of this space. We have

135
00:05:37,182 --> 00:05:39,572
data. So data is kind of this vertical

136
00:05:39,636 --> 00:05:41,550
component and you kind of see these

137
00:05:42,000 --> 00:05:45,436
flags kind of being kind of anchored in

138
00:05:45,458 --> 00:05:47,576
these regions. So that kind of signifies

139
00:05:47,688 --> 00:05:49,356
you have some data that's kind of

140
00:05:49,378 --> 00:05:51,976
associated with that region. So that's

141
00:05:52,008 --> 00:05:54,816
kind of the high level perspective and

142
00:05:54,838 --> 00:05:55,984
I'm going to make that a bit more

143
00:05:56,022 --> 00:05:59,148
concrete a bit later. And there's

144
00:05:59,164 --> 00:06:01,104
kind of two essential things about this

145
00:06:01,142 --> 00:06:05,312
picture. A first thing is locality.

146
00:06:05,456 --> 00:06:08,964
So the data is attached to

147
00:06:09,002 --> 00:06:11,204
some regions of this space, this

148
00:06:11,242 --> 00:06:14,564
topological space, and in that sense

149
00:06:14,602 --> 00:06:16,336
it's local, so it's kind of associated

150
00:06:16,368 --> 00:06:17,908
with the region. And maybe to give a

151
00:06:17,914 --> 00:06:20,360
concrete example, if you kind of have a

152
00:06:20,430 --> 00:06:22,312
temperature sensor somewhere in space,

153
00:06:22,366 --> 00:06:25,432
right, you could think of whatever that

154
00:06:25,486 --> 00:06:26,936
sensor is measuring is kind of a

155
00:06:26,958 --> 00:06:28,324
property of kind of the immediate

156
00:06:28,372 --> 00:06:31,336
surrounding around that sensor.

157
00:06:31,368 --> 00:06:33,644
So it's kind of describing some property

158
00:06:33,842 --> 00:06:36,988
of a region in space. So that'll be kind

159
00:06:36,994 --> 00:06:39,836
of a concrete example. And another kind

160
00:06:39,858 --> 00:06:43,244
of axiom that we're starting with is

161
00:06:43,282 --> 00:06:46,620
that the space has structure.

162
00:06:47,040 --> 00:06:49,136
So the space has kind of, it's kind of

163
00:06:49,158 --> 00:06:51,184
made up of various regions and these

164
00:06:51,222 --> 00:06:56,340
regions intersect in various ways and

165
00:06:56,410 --> 00:06:58,292
that implicitly also makes our data

166
00:06:58,346 --> 00:06:59,968
structured because the data is attached

167
00:06:59,984 --> 00:07:02,260
to these regions. So there is kind of

168
00:07:02,330 --> 00:07:04,390
some structure in the data.

169
00:07:05,800 --> 00:07:07,632
All right, so that's kind of the picture

170
00:07:07,696 --> 00:07:09,524
and actually many of these things relate

171
00:07:09,572 --> 00:07:11,976
to category theory. I'm not going to go

172
00:07:11,998 --> 00:07:14,136
in depth into this because it's kind of

173
00:07:14,158 --> 00:07:15,796
sophisticated and I'm not an expert

174
00:07:15,828 --> 00:07:19,128
myself in category theory, but kind of

175
00:07:19,214 --> 00:07:22,216
the high level here is that category

176
00:07:22,248 --> 00:07:25,276
theory is kind of a nice way. To

177
00:07:25,458 --> 00:07:27,756
translate between different structure in

178
00:07:27,778 --> 00:07:30,876
mathematics and kind of discuss about

179
00:07:30,978 --> 00:07:32,696
properties of certain kinds of objects

180
00:07:32,728 --> 00:07:34,284
and translate that to some different

181
00:07:34,322 --> 00:07:36,368
kinds of objects and find all these kind

182
00:07:36,374 --> 00:07:38,416
of relations and connections. And a

183
00:07:38,438 --> 00:07:40,016
concrete example is, for instance, if

184
00:07:40,038 --> 00:07:42,400
you want to study some manifolds, some

185
00:07:42,470 --> 00:07:44,544
surfaces, you could associate some

186
00:07:44,582 --> 00:07:47,996
groups to these surfaces and then any

187
00:07:48,038 --> 00:07:49,748
sorts of relations between these kind of

188
00:07:49,754 --> 00:07:51,156
surfaces, also translating some

189
00:07:51,178 --> 00:07:52,756
relations about these groups or some

190
00:07:52,778 --> 00:07:54,884
other algebraic structures. So you could

191
00:07:54,922 --> 00:07:56,736
study these manifolds by doing algebra

192
00:07:56,768 --> 00:07:59,304
instead of doing geometry or topology or

193
00:07:59,342 --> 00:08:02,744
something else. So also in this case,

194
00:08:02,782 --> 00:08:04,184
it's kind of manifesting the fact that

195
00:08:04,222 --> 00:08:07,684
we kind of translate these from spaces

196
00:08:07,732 --> 00:08:09,828
to data. So kind of because we associate

197
00:08:09,924 --> 00:08:12,536
the regions in a space, certain kinds of

198
00:08:12,558 --> 00:08:13,804
data, this is kind of how this

199
00:08:13,842 --> 00:08:15,756
translation manifests in what I've just

200
00:08:15,778 --> 00:08:17,436
described in the previous picture. So

201
00:08:17,458 --> 00:08:18,924
you could think of this as some sort of

202
00:08:18,962 --> 00:08:22,604
translation or mapping from spaces and

203
00:08:22,642 --> 00:08:24,412
regions in that space to kind of data

204
00:08:24,466 --> 00:08:27,276
attached to that space. But yeah, I'm

205
00:08:27,308 --> 00:08:29,664
not going to go in a lot of detail into

206
00:08:29,702 --> 00:08:31,808
this, but just kind of to keep in the

207
00:08:31,814 --> 00:08:34,784
back of your mind that there's this

208
00:08:34,822 --> 00:08:37,476
stuff lurking in the background. All

209
00:08:37,498 --> 00:08:40,580
right, so I promise this is the only

210
00:08:40,650 --> 00:08:42,452
math definition I'm giving in this talk

211
00:08:42,506 --> 00:08:45,888
and then I'll stop. But just because I'm

212
00:08:45,904 --> 00:08:48,336
mentioning topological spaces quite

213
00:08:48,378 --> 00:08:50,824
often, I just wanted to kind of give

214
00:08:50,862 --> 00:08:53,784
this axiomatic definition, which might

215
00:08:53,822 --> 00:08:55,208
sound sophisticated, but I have a

216
00:08:55,214 --> 00:08:56,468
picture at the end and hopefully it'll

217
00:08:56,484 --> 00:09:00,152
be clear what is it.

218
00:09:00,286 --> 00:09:02,536
So it's just a set. As I was saying, we

219
00:09:02,558 --> 00:09:04,204
start with sets and we put stuff on top,

220
00:09:04,242 --> 00:09:06,556
right? So you start with a set and then

221
00:09:06,578 --> 00:09:08,956
you also have a collection of subsets of

222
00:09:08,978 --> 00:09:11,996
this set called the open sets that need

223
00:09:12,018 --> 00:09:13,836
to satisfy certain axioms. So you could

224
00:09:13,858 --> 00:09:15,264
think of these open sets as kind of

225
00:09:15,302 --> 00:09:17,212
regions of this space, very informally

226
00:09:17,276 --> 00:09:21,616
speaking. So something that kind

227
00:09:21,638 --> 00:09:23,728
of has to be satisfied is that, well,

228
00:09:23,814 --> 00:09:27,524
the empty set and the set itself need

229
00:09:27,562 --> 00:09:30,116
to be open set. So in some sense, you

230
00:09:30,138 --> 00:09:32,084
could think of this as saying the set

231
00:09:32,122 --> 00:09:34,804
itself is a region of that space very

232
00:09:34,842 --> 00:09:38,164
informally, which it's kind of,

233
00:09:38,282 --> 00:09:40,624
let's say, obvious. And then there's

234
00:09:40,752 --> 00:09:43,796
some kind of constraints about

235
00:09:43,898 --> 00:09:46,104
intersecting and taking units of this

236
00:09:46,142 --> 00:09:49,076
region. So if we take the intersection

237
00:09:49,108 --> 00:09:50,860
of two regions, we should get another

238
00:09:50,930 --> 00:09:53,916
one of these regions. And if we take a

239
00:09:53,938 --> 00:09:56,476
union of these regions, we should get

240
00:09:56,498 --> 00:10:00,156
another region. And there are some

241
00:10:00,258 --> 00:10:02,364
constraints again, like, okay, how big

242
00:10:02,402 --> 00:10:04,236
these intersections should be. There

243
00:10:04,258 --> 00:10:05,808
should be finite intersections, but you

244
00:10:05,814 --> 00:10:07,728
could have infinite unions. But that's a

245
00:10:07,734 --> 00:10:10,268
technicality and we can just skip

246
00:10:10,364 --> 00:10:12,848
anyways. But to see a picture so on the

247
00:10:12,854 --> 00:10:15,984
left, you just see the set x itself.

248
00:10:16,102 --> 00:10:18,340
And here I've put like a potential

249
00:10:18,680 --> 00:10:20,452
neighborhood structure or kind of like

250
00:10:20,506 --> 00:10:23,044
open set structure on this space. So we

251
00:10:23,082 --> 00:10:26,068
have an open set U, another open set V.

252
00:10:26,154 --> 00:10:27,844
By this axiom, their intersection should

253
00:10:27,882 --> 00:10:29,572
also be an open set. So you see this

254
00:10:29,626 --> 00:10:31,192
intersection in the middle being another

255
00:10:31,246 --> 00:10:33,336
open set, and then the set itself is

256
00:10:33,358 --> 00:10:34,760
another open set. So it's just kind of

257
00:10:34,830 --> 00:10:37,130
splitting stuff into regions, kind of.

258
00:10:37,580 --> 00:10:40,428
You could think of it like that. All

259
00:10:40,434 --> 00:10:43,550
right, so this is a topological space,

260
00:10:44,640 --> 00:10:48,364
and now let's add data, right? So we

261
00:10:48,402 --> 00:10:51,468
mentioned that we have data and then we

262
00:10:51,474 --> 00:10:52,924
have a space, and then we add data on

263
00:10:52,962 --> 00:10:54,908
top. So so far we've seen how a

264
00:10:54,914 --> 00:10:57,036
topological space looks like. Now let's

265
00:10:57,068 --> 00:10:58,592
add this kind of vertical stuff, these

266
00:10:58,646 --> 00:11:01,104
flags that you saw before. We just put

267
00:11:01,142 --> 00:11:03,872
some data on top of these regions. And

268
00:11:03,926 --> 00:11:06,036
if we put data on all the regions of the

269
00:11:06,058 --> 00:11:09,060
space, on all the open sets, we get

270
00:11:09,130 --> 00:11:12,452
these structures that

271
00:11:12,506 --> 00:11:14,928
in categories here, like in algebraic

272
00:11:14,944 --> 00:11:16,564
topology, also geometry, they are called

273
00:11:16,602 --> 00:11:19,732
presheaves, which sounds very fancy,

274
00:11:19,796 --> 00:11:22,330
but all is just kind of a definition of

275
00:11:23,100 --> 00:11:24,644
what I was already describing.

276
00:11:24,692 --> 00:11:26,984
Essentially, you have some data for each

277
00:11:27,022 --> 00:11:31,124
region. For instance, for region

278
00:11:31,172 --> 00:11:34,748
U, you have this f of U, which is kind

279
00:11:34,754 --> 00:11:37,116
of the data attached to region U. So you

280
00:11:37,138 --> 00:11:40,460
could think of f of U, some set with

281
00:11:40,530 --> 00:11:42,736
describing the data that lives there in

282
00:11:42,758 --> 00:11:45,360
that region. But there's also an extra

283
00:11:45,430 --> 00:11:48,400
thing. You have some sort of maps going

284
00:11:48,470 --> 00:11:51,490
between these kind of pieces of data,

285
00:11:52,020 --> 00:11:54,076
and these are called restriction maps.

286
00:11:54,188 --> 00:11:56,640
And why is that? They provide you a way

287
00:11:56,710 --> 00:11:59,924
to kind of zoom in if you want,

288
00:11:59,962 --> 00:12:02,288
right? Like you have the data attached

289
00:12:02,304 --> 00:12:05,012
to the whole set x, and then you could

290
00:12:05,066 --> 00:12:06,676
think, okay, how do I take this data?

291
00:12:06,778 --> 00:12:09,208
How do I go from this data to data on a

292
00:12:09,214 --> 00:12:11,848
smaller region on x? So it's kind of a

293
00:12:11,854 --> 00:12:14,570
way to zoom in on that data,

294
00:12:15,660 --> 00:12:18,184
essentially. And I'm going to show some

295
00:12:18,222 --> 00:12:21,416
example in a second. So these are called

296
00:12:21,598 --> 00:12:24,350
pre sheaves. And just to see an example,

297
00:12:24,960 --> 00:12:27,756
our space here is kind of one of the

298
00:12:27,778 --> 00:12:29,116
simplest kind of space you could think

299
00:12:29,138 --> 00:12:31,004
of. It's just 1D or horizontal line,

300
00:12:31,042 --> 00:12:33,616
right? Just the real line. And then you

301
00:12:33,638 --> 00:12:35,232
have some regions which are just given

302
00:12:35,286 --> 00:12:40,496
by open intervals on

303
00:12:40,518 --> 00:12:42,576
the real line. And then some pieces of

304
00:12:42,598 --> 00:12:44,636
data could be functions like continuous

305
00:12:44,668 --> 00:12:48,032
functions on those regions. So here's

306
00:12:48,176 --> 00:12:51,376
some sample data on this first interval.

307
00:12:51,488 --> 00:12:53,012
Here's another function on the second

308
00:12:53,066 --> 00:12:55,092
interval. Here's some data on this third

309
00:12:55,146 --> 00:12:57,156
interval. And actually in this case, it

310
00:12:57,178 --> 00:13:00,520
happens that all these functions agree

311
00:13:00,590 --> 00:13:02,404
on the overlap. So where these regions

312
00:13:02,452 --> 00:13:04,696
overlap, they take the same values and

313
00:13:04,718 --> 00:13:07,848
we can actually glue them together in a

314
00:13:07,854 --> 00:13:10,760
single function over the entire region.

315
00:13:11,120 --> 00:13:14,636
So this is just an example of a

316
00:13:14,658 --> 00:13:16,476
preshief and it's called the preshift of

317
00:13:16,498 --> 00:13:19,148
continuous functions. So our data in

318
00:13:19,154 --> 00:13:21,196
this case is continuous functions and

319
00:13:21,218 --> 00:13:23,916
the space is just the realign and we put

320
00:13:23,938 --> 00:13:25,920
these functions on top of the realign.

321
00:13:26,260 --> 00:13:28,176
But it turns out because of this kind of

322
00:13:28,198 --> 00:13:30,736
special property, that we can kind of

323
00:13:30,758 --> 00:13:33,104
glue data and we uniquely get some other

324
00:13:33,142 --> 00:13:35,264
piece of data, right? We can take these

325
00:13:35,302 --> 00:13:37,636
three pieces. It's just exactly like a

326
00:13:37,658 --> 00:13:39,732
puzzle, right? We put these things

327
00:13:39,786 --> 00:13:41,636
together and we get a fourth thing,

328
00:13:41,658 --> 00:13:43,956
which is kind of a single function where

329
00:13:43,978 --> 00:13:46,470
we just overlap these functions, right?

330
00:13:46,940 --> 00:13:51,076
And these presheaves

331
00:13:51,108 --> 00:13:53,044
that satisfy these kind of properties

332
00:13:53,092 --> 00:13:54,776
where you can kind of glue them to get a

333
00:13:54,798 --> 00:13:56,744
unique piece of data. They are called

334
00:13:56,782 --> 00:14:00,996
sheaves and basically the preshave

335
00:14:01,028 --> 00:14:02,696
of continuous functions is actually a

336
00:14:02,718 --> 00:14:06,316
sheep. So this is kind of a way to

337
00:14:06,338 --> 00:14:09,070
formalize data attached to these things.

338
00:14:09,600 --> 00:14:11,596
It's going to get less technical in a

339
00:14:11,618 --> 00:14:15,128
second. So just to give more examples.

340
00:14:15,224 --> 00:14:17,216
So for instance, in this way we could

341
00:14:17,238 --> 00:14:20,096
describe data on a sphere. And let's say

342
00:14:20,118 --> 00:14:21,744
if this data is just some vector field

343
00:14:21,782 --> 00:14:23,344
on a sphere. So let's say if this is

344
00:14:23,382 --> 00:14:25,648
Earth, right, this could be some wind

345
00:14:25,734 --> 00:14:27,156
vector field, right? Like if we do

346
00:14:27,178 --> 00:14:29,124
weather modeling or something, you just

347
00:14:29,162 --> 00:14:32,230
have a vector field describing the wind

348
00:14:32,840 --> 00:14:35,956
on the surface of Earth, right? And you

349
00:14:35,978 --> 00:14:37,172
might want to do some machine learning

350
00:14:37,226 --> 00:14:38,916
on top of this, where this kind of

351
00:14:38,938 --> 00:14:40,816
vector field has a sheet structure, and

352
00:14:40,858 --> 00:14:42,916
you could think of it as a sheath,

353
00:14:42,948 --> 00:14:45,624
because if you have some vector field on

354
00:14:45,662 --> 00:14:47,496
the red region, a vector field on the

355
00:14:47,518 --> 00:14:49,736
yellow region, I can kind of glue them

356
00:14:49,758 --> 00:14:52,296
together uniquely if they overlap. If

357
00:14:52,318 --> 00:14:54,236
they agree on the overlap and we get the

358
00:14:54,258 --> 00:14:57,372
vector field on this bigger region. But

359
00:14:57,426 --> 00:14:59,692
something that's quite nice is that even

360
00:14:59,746 --> 00:15:01,836
if we have a very different kind of

361
00:15:01,858 --> 00:15:04,576
space namely a graph which is very

362
00:15:04,598 --> 00:15:07,056
different from a sphere in all points of

363
00:15:07,078 --> 00:15:09,632
view, we can still apply the exact same

364
00:15:09,686 --> 00:15:11,776
kind of axioms and terminology and kind

365
00:15:11,798 --> 00:15:14,556
of definitions and we can have a sheath

366
00:15:14,588 --> 00:15:17,196
of our graph. So in this way we could

367
00:15:17,238 --> 00:15:18,864
have, for instance, some features

368
00:15:18,912 --> 00:15:20,736
associated to the nodes of the graph,

369
00:15:20,848 --> 00:15:23,344
some features associated with the edges

370
00:15:23,392 --> 00:15:25,824
of the graph, and there's another node

371
00:15:25,872 --> 00:15:28,356
which has its own features and this is

372
00:15:28,378 --> 00:15:29,896
actually the exact same thing we have in

373
00:15:29,918 --> 00:15:32,504
graph machine learning. So this is quite

374
00:15:32,542 --> 00:15:34,004
nice. What this kind of topological

375
00:15:34,052 --> 00:15:36,728
perspective allows us to do is we kind

376
00:15:36,734 --> 00:15:40,184
of have a unified way of thinking, if

377
00:15:40,222 --> 00:15:41,816
you want, about very kind of

378
00:15:41,838 --> 00:15:44,444
heterogeneous spaces and we can model on

379
00:15:44,482 --> 00:15:46,636
all of them data attached to them by

380
00:15:46,658 --> 00:15:49,132
using this kind of sheet terminology and

381
00:15:49,186 --> 00:15:51,324
other ways as well. But I'm not going

382
00:15:51,362 --> 00:15:53,936
into that in this talk. All right, so

383
00:15:53,958 --> 00:15:55,948
this is kind of an overview of what I've

384
00:15:55,964 --> 00:16:00,464
been doing in my thesis. And just

385
00:16:00,502 --> 00:16:03,410
to kind of dive a bit deeper into this,

386
00:16:03,940 --> 00:16:07,476
I just wanted to go into one paper that

387
00:16:07,498 --> 00:16:10,804
we did at Europe last year. Yeah. So

388
00:16:10,842 --> 00:16:14,336
this was last year on what's

389
00:16:14,368 --> 00:16:16,640
called shift diffusion. So essentially,

390
00:16:16,720 --> 00:16:19,204
how can we use what I've just described

391
00:16:19,252 --> 00:16:21,592
to do some useful stuff when doing

392
00:16:21,646 --> 00:16:25,064
machine learning on graphs, all right?

393
00:16:25,102 --> 00:16:26,424
And this was a collaboration with

394
00:16:26,462 --> 00:16:28,548
Francesco de Giovanni ben Chamberlain

395
00:16:28,644 --> 00:16:31,464
pietro Leo, my advisor and Michael

396
00:16:31,512 --> 00:16:35,564
Bronstein. Okay? So before I dive into

397
00:16:35,602 --> 00:16:36,924
this, I just want to give some

398
00:16:36,962 --> 00:16:38,764
background in case people are not

399
00:16:38,802 --> 00:16:42,530
familiar with this. So the kind of

400
00:16:42,900 --> 00:16:45,072
favorite architecture of people doing

401
00:16:45,126 --> 00:16:47,440
machine learning on graphs these days

402
00:16:47,510 --> 00:16:49,356
are these things called graph neural

403
00:16:49,388 --> 00:16:52,400
networks, which are actually very simple

404
00:16:52,470 --> 00:16:56,096
kind of models. So in the setting

405
00:16:56,208 --> 00:17:00,148
you have some features. So each node in

406
00:17:00,154 --> 00:17:01,744
your graph will have some features.

407
00:17:01,792 --> 00:17:05,184
This is what this h vector

408
00:17:05,232 --> 00:17:09,392
here denotes. So it's the vector

409
00:17:09,456 --> 00:17:12,420
h associated with node A at layer or

410
00:17:12,490 --> 00:17:15,304
time t, whatever. So you have some

411
00:17:15,342 --> 00:17:17,656
features for each of these nodes and

412
00:17:17,758 --> 00:17:20,476
what you're doing each of if you're at a

413
00:17:20,498 --> 00:17:22,076
certain node, you want to kind of

414
00:17:22,098 --> 00:17:24,444
compute a new representation or new

415
00:17:24,482 --> 00:17:26,572
features for this node at the next

416
00:17:26,626 --> 00:17:29,660
layer. So essentially are learning

417
00:17:29,730 --> 00:17:32,856
representations and what the graph

418
00:17:32,888 --> 00:17:35,052
neural networks are doing. This node

419
00:17:35,116 --> 00:17:37,344
will receive a message from all the

420
00:17:37,382 --> 00:17:40,524
other nodes that are neighbors

421
00:17:40,572 --> 00:17:44,128
with this node. And this message

422
00:17:44,214 --> 00:17:47,056
can also be passed, can use some neural

423
00:17:47,088 --> 00:17:49,536
networks in there, but essentially it's

424
00:17:49,568 --> 00:17:51,476
some processing of these features of the

425
00:17:51,498 --> 00:17:54,452
neighbors and these are aggregated into

426
00:17:54,506 --> 00:17:58,084
this message. So here in Green and

427
00:17:58,122 --> 00:18:00,564
then this is passed through some update

428
00:18:00,612 --> 00:18:03,256
function that combines the message from

429
00:18:03,278 --> 00:18:04,312
the neighbors with the old

430
00:18:04,366 --> 00:18:06,456
representation of this node and it gives

431
00:18:06,478 --> 00:18:07,944
you a new representation at the next

432
00:18:07,982 --> 00:18:09,896
layer. And this happens for all the

433
00:18:09,918 --> 00:18:11,256
nodes, right? So then you get some new

434
00:18:11,278 --> 00:18:12,856
representations for this node and this

435
00:18:12,878 --> 00:18:14,524
is one layer and then you kind of keep

436
00:18:14,562 --> 00:18:16,284
repeating this for as many layers as you

437
00:18:16,322 --> 00:18:18,536
like. So this is kind of how you do deep

438
00:18:18,568 --> 00:18:21,676
learning on graphs. It's kind of a very

439
00:18:21,778 --> 00:18:24,844
simple recipe and most models

440
00:18:24,892 --> 00:18:27,424
actually vary in the way they kind of

441
00:18:27,622 --> 00:18:29,904
compute these messages and in the way

442
00:18:29,942 --> 00:18:32,576
this update function is designed. But

443
00:18:32,598 --> 00:18:34,896
that's kind of the parameters most of

444
00:18:34,918 --> 00:18:37,316
these models use. Otherwise they all

445
00:18:37,338 --> 00:18:39,604
kind of respect this framework and work

446
00:18:39,642 --> 00:18:43,716
in this kind of particular way. And to

447
00:18:43,738 --> 00:18:45,476
give you maybe an example for why you

448
00:18:45,498 --> 00:18:48,468
would want to do this, you might want to

449
00:18:48,474 --> 00:18:50,216
do node classification. This is kind of

450
00:18:50,238 --> 00:18:52,196
a classic problem in graph machine

451
00:18:52,228 --> 00:18:53,656
learning. There are others, but I'm just

452
00:18:53,678 --> 00:18:55,396
going to talk about this because it's

453
00:18:55,428 --> 00:18:58,744
easier. So you have a graph and

454
00:18:58,782 --> 00:19:01,452
this graph has nodes that have different

455
00:19:01,506 --> 00:19:03,404
labels. Here there's just two kinds of

456
00:19:03,442 --> 00:19:05,980
labels, this orange and blue.

457
00:19:07,600 --> 00:19:10,972
And you have some edges between these

458
00:19:11,026 --> 00:19:13,008
nodes. And what you want is you want to

459
00:19:13,014 --> 00:19:15,248
do this kind of message passing that I

460
00:19:15,254 --> 00:19:17,104
was describing to compute some

461
00:19:17,142 --> 00:19:19,170
representations for these nodes where

462
00:19:20,180 --> 00:19:22,416
you can easily classify the blue and

463
00:19:22,518 --> 00:19:26,176
orange nodes. Now something that's

464
00:19:26,208 --> 00:19:29,428
quite interesting is that for many kind

465
00:19:29,434 --> 00:19:33,316
of graph neural networks, depending on

466
00:19:33,338 --> 00:19:34,836
the properties of these graphs and how

467
00:19:34,858 --> 00:19:36,036
these kind of different nodes are

468
00:19:36,058 --> 00:19:38,484
connected, their performance might vary

469
00:19:38,532 --> 00:19:40,868
quite a lot. So in particular they're

470
00:19:40,884 --> 00:19:42,536
affected by this property called

471
00:19:42,638 --> 00:19:45,496
heterophili. So this is kind of a

472
00:19:45,518 --> 00:19:48,404
measure of how much opposites attractive

473
00:19:48,452 --> 00:19:51,352
you want, right? So it has a very simple

474
00:19:51,406 --> 00:19:53,964
formula. Basically you take the number

475
00:19:54,002 --> 00:19:56,008
of edges between orange and blue nodes

476
00:19:56,024 --> 00:19:57,628
and you divide by the total number of

477
00:19:57,634 --> 00:19:59,916
edges, right? So basically you kind of

478
00:19:59,938 --> 00:20:01,868
check how many connections we have in

479
00:20:01,874 --> 00:20:03,936
this graph between things that are quite

480
00:20:04,118 --> 00:20:06,412
opposite to each other versus

481
00:20:06,556 --> 00:20:08,448
connections that are between similar

482
00:20:08,534 --> 00:20:11,904
kind of nodes. So if you have a lot

483
00:20:11,942 --> 00:20:13,372
of these kind of heterogeneous

484
00:20:13,436 --> 00:20:15,196
connections, then you have very high

485
00:20:15,238 --> 00:20:17,604
heterophili. And it turns out that many

486
00:20:17,642 --> 00:20:19,108
graphene networks actually struggle in

487
00:20:19,114 --> 00:20:21,824
that setting. It's very hard to classify

488
00:20:21,952 --> 00:20:25,104
things in that setting. And intuitively

489
00:20:25,152 --> 00:20:28,260
you could kind of also figure out why

490
00:20:28,410 --> 00:20:31,336
because you could easily apply some kind

491
00:20:31,358 --> 00:20:34,676
of reasoning where oh, this node

492
00:20:34,708 --> 00:20:36,824
looks a lot like this other node is

493
00:20:36,862 --> 00:20:39,208
connected to. So they kind of must be in

494
00:20:39,214 --> 00:20:40,808
the same community if you want or in the

495
00:20:40,814 --> 00:20:42,796
same label. But it's much harder to do

496
00:20:42,818 --> 00:20:44,444
that when all things are kind of

497
00:20:44,482 --> 00:20:48,348
different from each other and

498
00:20:48,434 --> 00:20:50,172
these communities kind of don't form

499
00:20:50,226 --> 00:20:52,376
right, even visually. If you see a graph

500
00:20:52,408 --> 00:20:53,976
and it has some nicely clustered

501
00:20:54,008 --> 00:20:56,096
communities, it's quite easy to draw a

502
00:20:56,118 --> 00:20:58,176
line between those and say, oh, this is

503
00:20:58,278 --> 00:21:00,464
a community, is another community. But

504
00:21:00,502 --> 00:21:03,068
if things are very mixed, then it's

505
00:21:03,084 --> 00:21:04,920
quite challenging. And it turns out it's

506
00:21:04,940 --> 00:21:06,916
also challenging for these models, not

507
00:21:06,938 --> 00:21:09,716
just for kind of our intuition when we

508
00:21:09,738 --> 00:21:12,708
would have to do this. So this is kind

509
00:21:12,714 --> 00:21:16,352
of some problem where this topological

510
00:21:16,416 --> 00:21:19,176
perspective I was mentioning will be

511
00:21:19,198 --> 00:21:22,090
used to do some useful stuff.

512
00:21:23,580 --> 00:21:26,392
Okay? So coming back to Sheaves on

513
00:21:26,446 --> 00:21:29,336
graphs. And at this point I think you

514
00:21:29,358 --> 00:21:31,148
can largely forget what I mentioned in

515
00:21:31,154 --> 00:21:33,356
the introduction or if there's something

516
00:21:33,378 --> 00:21:36,636
you misunderstood there. We kind of

517
00:21:36,658 --> 00:21:39,964
start from zero bit here, so there's no

518
00:21:40,002 --> 00:21:43,596
problem. So on the left you just have

519
00:21:43,618 --> 00:21:46,628
a graph which is kind of the incidence

520
00:21:46,664 --> 00:21:48,624
structure of a graph. I just drawn here

521
00:21:48,662 --> 00:21:50,736
the simplest possible graph that has two

522
00:21:50,758 --> 00:21:53,484
nodes v and U. And then there's an edge

523
00:21:53,532 --> 00:21:55,276
between them. So this is just a graph

524
00:21:55,308 --> 00:21:57,056
with one edge. That's all that's going

525
00:21:57,078 --> 00:21:59,924
on here. And I've just represented it by

526
00:21:59,962 --> 00:22:01,520
kind of in this kind of incidence

527
00:22:01,600 --> 00:22:04,164
structure kind of way, right? Node v is

528
00:22:04,202 --> 00:22:07,044
incident to node e and node u, sorry,

529
00:22:07,162 --> 00:22:10,272
edge e and node U is incident to edge e.

530
00:22:10,346 --> 00:22:12,116
So this is just an incident structure

531
00:22:12,228 --> 00:22:14,504
and what this kind of triangle symbol is

532
00:22:14,542 --> 00:22:17,076
showing is just this incident structure.

533
00:22:17,108 --> 00:22:18,632
It's just a way to symbolize this

534
00:22:18,686 --> 00:22:21,064
incidence relation if you want. Okay,

535
00:22:21,102 --> 00:22:23,436
so this is just a graph, right? And a

536
00:22:23,458 --> 00:22:25,816
way we can kind of think of field zone

537
00:22:25,848 --> 00:22:28,104
graphs is just mapping this graph

538
00:22:28,152 --> 00:22:29,484
structure. So this is kind of this

539
00:22:29,522 --> 00:22:31,932
categorical theory translation. We

540
00:22:31,986 --> 00:22:34,384
translate this graph into something else

541
00:22:34,582 --> 00:22:37,308
which looks very similar. The structure

542
00:22:37,324 --> 00:22:38,784
is kind of the same, it's just kind of

543
00:22:38,822 --> 00:22:41,424
the meaning of these things change. So

544
00:22:41,462 --> 00:22:44,560
for each node v we have here,

545
00:22:44,630 --> 00:22:47,216
this will be a vector space. So F of v

546
00:22:47,238 --> 00:22:49,956
is a vector space. For each node u we

547
00:22:49,978 --> 00:22:51,792
have F of U which is another vector

548
00:22:51,856 --> 00:22:55,396
space. For each edge e we have this Fe

549
00:22:55,498 --> 00:22:58,372
which is another vector space. So all

550
00:22:58,426 --> 00:23:01,032
nodes have their own vector spaces and

551
00:23:01,086 --> 00:23:03,284
the features associated to those nodes

552
00:23:03,332 --> 00:23:05,224
leaving those vector spaces. So

553
00:23:05,262 --> 00:23:06,964
basically for each node we have a vector

554
00:23:07,012 --> 00:23:09,028
space of features that's all that's

555
00:23:09,044 --> 00:23:11,896
going on so far. And also these arrows

556
00:23:11,928 --> 00:23:13,852
that these incidence relations also

557
00:23:13,906 --> 00:23:16,556
translate into something and they

558
00:23:16,578 --> 00:23:18,424
translate into the obvious thing linear

559
00:23:18,472 --> 00:23:20,376
maps. So if these are vector spaces,

560
00:23:20,408 --> 00:23:22,236
then these things should be linear maps

561
00:23:22,268 --> 00:23:23,900
or just some matrices essentially,

562
00:23:23,980 --> 00:23:26,384
right? So for each arrow you see here we

563
00:23:26,422 --> 00:23:32,720
have a matrix and

564
00:23:32,790 --> 00:23:36,340
something that I'll argue and

565
00:23:36,410 --> 00:23:38,420
show in a few slides is that basically

566
00:23:38,490 --> 00:23:42,324
message passing is

567
00:23:42,362 --> 00:23:44,516
very similar on graphs, is very similar

568
00:23:44,618 --> 00:23:47,220
with group actions in group theory.

569
00:23:47,800 --> 00:23:50,456
So let me explain exactly why. So we

570
00:23:50,478 --> 00:23:53,336
kind of can think of what we have on the

571
00:23:53,358 --> 00:23:56,164
left these arrows from the incidence

572
00:23:56,212 --> 00:23:58,116
relation. We could think of these arrows

573
00:23:58,148 --> 00:24:00,552
as kind of some buttons we can press.

574
00:24:00,686 --> 00:24:03,692
So what do I mean by that? So if we have

575
00:24:03,746 --> 00:24:06,572
this node v on the left, right, and this

576
00:24:06,626 --> 00:24:09,996
e, now if we have some features, some

577
00:24:10,018 --> 00:24:12,956
feature living in FOV, right,

578
00:24:13,058 --> 00:24:14,972
we could just kind of press this arrow

579
00:24:15,036 --> 00:24:18,364
button here and then if we multiply

580
00:24:18,412 --> 00:24:21,184
this matrix by this feature, we will get

581
00:24:21,222 --> 00:24:25,184
an edge feature. So it's kind of like if

582
00:24:25,222 --> 00:24:27,452
you go along this arrow, this matrix

583
00:24:27,516 --> 00:24:29,764
will multiply this feature and this

584
00:24:29,802 --> 00:24:31,684
vertex feature and it will give you an

585
00:24:31,722 --> 00:24:33,956
edge feature. So you could think of

586
00:24:33,978 --> 00:24:35,476
these arrows as kind of giving you some

587
00:24:35,498 --> 00:24:37,204
sort of actions that you can play with

588
00:24:37,242 --> 00:24:39,876
to move features from vertex to edge and

589
00:24:40,058 --> 00:24:42,380
edge to vertex. So in this case it's

590
00:24:42,400 --> 00:24:43,928
kind of a left action, right? So this is

591
00:24:43,934 --> 00:24:45,684
what I'm saying. I'm taking this arrow,

592
00:24:45,732 --> 00:24:49,304
which is this one here, and I act on

593
00:24:49,342 --> 00:24:52,020
some features of node v. So this HOV

594
00:24:52,100 --> 00:24:56,092
that lives here and how I do that

595
00:24:56,146 --> 00:24:58,616
is I just take this matrix, this matrix

596
00:24:58,648 --> 00:25:00,604
associated with this arrow, and I

597
00:25:00,642 --> 00:25:04,172
multiply this vector HOV. So just

598
00:25:04,226 --> 00:25:06,876
matrix times vector, that's all. And

599
00:25:06,898 --> 00:25:08,896
then we get an edge feature. So this is

600
00:25:08,918 --> 00:25:10,976
just kind of a way to move from here to

601
00:25:10,998 --> 00:25:12,448
here. So this already kind of looks a

602
00:25:12,454 --> 00:25:13,868
bit like message passing, right? We're

603
00:25:13,884 --> 00:25:15,716
kind of passing a message from this

604
00:25:15,738 --> 00:25:18,884
vertex to this edge, but now we also

605
00:25:18,922 --> 00:25:21,892
need to pass a message from this edge to

606
00:25:21,946 --> 00:25:25,652
these other vertex U. So we need to get

607
00:25:25,706 --> 00:25:28,836
from V to U and we did that by passing

608
00:25:28,868 --> 00:25:31,464
through e. So by doing that, we could do

609
00:25:31,502 --> 00:25:33,608
that by kind of going in reverse. So we

610
00:25:33,614 --> 00:25:36,952
could have a right action where instead

611
00:25:37,006 --> 00:25:40,020
of applying this matrix, we applied

612
00:25:40,100 --> 00:25:41,828
adjoint matrix. So that's just a

613
00:25:41,854 --> 00:25:45,244
transpose matrix. So if we want to go

614
00:25:45,282 --> 00:25:48,536
from here to here, instead of applying

615
00:25:48,568 --> 00:25:50,184
this matrix, we apply it transpose

616
00:25:50,232 --> 00:25:51,276
because we want to go the other way

617
00:25:51,298 --> 00:25:54,524
around. If we compose

618
00:25:54,572 --> 00:25:56,348
these things, then we can move features

619
00:25:56,364 --> 00:26:00,752
from V to U. So this is just a way that

620
00:26:00,806 --> 00:26:04,096
kind of we can apply these actions to

621
00:26:04,118 --> 00:26:06,052
do message passing and these are called

622
00:26:06,106 --> 00:26:09,030
shift actions or preshift actions. And

623
00:26:09,640 --> 00:26:11,524
I'm going to now show what's kind of the

624
00:26:11,562 --> 00:26:14,436
relation between this and what we have

625
00:26:14,458 --> 00:26:16,260
in group theory.

626
00:26:17,100 --> 00:26:20,712
So one way to represent a group

627
00:26:20,766 --> 00:26:23,800
is by kind of having some sort of graph,

628
00:26:24,300 --> 00:26:28,296
like here on the left. So we

629
00:26:28,318 --> 00:26:31,496
kind of have some star object, it's just

630
00:26:31,518 --> 00:26:34,236
kind of a dummy thing there. But all the

631
00:26:34,258 --> 00:26:35,816
group structure is in these arrows,

632
00:26:35,848 --> 00:26:38,204
right? So for each group element. So

633
00:26:38,242 --> 00:26:40,184
let's say this g is a 90 degree

634
00:26:40,232 --> 00:26:43,104
rotation, for instance. Let's say we do

635
00:26:43,142 --> 00:26:45,216
have a group of rotations, just to have

636
00:26:45,238 --> 00:26:46,530
some concrete example.

637
00:26:48,820 --> 00:26:50,752
So this arrow could correspond to a 90

638
00:26:50,806 --> 00:26:53,164
degree rotation. We have another arrow

639
00:26:53,212 --> 00:26:55,472
that does the opposite, minus kind of 90

640
00:26:55,526 --> 00:26:57,156
degree rotation. That's the inverse of

641
00:26:57,178 --> 00:26:59,508
that transformation. So this is kind of

642
00:26:59,514 --> 00:27:01,476
the structure of this group. And if we

643
00:27:01,498 --> 00:27:03,684
have, we also do this similar kind of

644
00:27:03,722 --> 00:27:07,012
translation as we've just seen. So

645
00:27:07,146 --> 00:27:08,964
basically we define a preshift on this

646
00:27:09,002 --> 00:27:12,244
group. We map this star to a vector

647
00:27:12,292 --> 00:27:13,976
space. So the star kind of replaces the

648
00:27:13,998 --> 00:27:16,888
vertex we had before. Now we just have a

649
00:27:16,894 --> 00:27:19,716
single vertex and it's just these arrows

650
00:27:19,748 --> 00:27:22,284
we have. So the star is mapped, this

651
00:27:22,322 --> 00:27:24,664
vector space that you show here in blue,

652
00:27:24,712 --> 00:27:27,132
right? And now, if we actually do group

653
00:27:27,186 --> 00:27:28,764
actions, which are kind of a very well

654
00:27:28,802 --> 00:27:30,860
established concept in group theory,

655
00:27:32,080 --> 00:27:34,188
well, for instance, if you want to act

656
00:27:34,354 --> 00:27:37,136
on this vertex sorry, not vertex on this

657
00:27:37,158 --> 00:27:39,484
vector. V right here. You have a vector

658
00:27:39,532 --> 00:27:41,872
in this vector space, and you want to

659
00:27:42,006 --> 00:27:44,764
act on it by this group transformation

660
00:27:44,812 --> 00:27:46,636
g. So, essentially, you want to press

661
00:27:46,678 --> 00:27:48,756
this arrow. So you apply some action on

662
00:27:48,778 --> 00:27:51,732
it. Then what you do is well,

663
00:27:51,786 --> 00:27:54,036
because of this translation, this g has

664
00:27:54,058 --> 00:27:56,612
been mapped to some matrix, which is the

665
00:27:56,746 --> 00:27:58,516
rotation matrix, the corresponding

666
00:27:58,548 --> 00:28:00,984
rotation matrix, and you apply this

667
00:28:01,022 --> 00:28:04,216
rotation matrix on v and you get like a

668
00:28:04,238 --> 00:28:07,928
90 degree rotation here. So this is

669
00:28:08,014 --> 00:28:09,784
what's going on. This kind of vertical

670
00:28:09,832 --> 00:28:11,900
vector is showing the rotated vector.

671
00:28:13,040 --> 00:28:14,876
So this is completely analogous with

672
00:28:14,898 --> 00:28:17,180
what we've seen on the previous slide.

673
00:28:19,040 --> 00:28:20,876
This is how kind of sheaves connect

674
00:28:20,978 --> 00:28:23,832
these kind of actions. So essentially

675
00:28:23,896 --> 00:28:25,760
what you could think of as message

676
00:28:25,830 --> 00:28:27,856
passing is same as group actions in

677
00:28:27,878 --> 00:28:29,984
group theory, but you just replace this

678
00:28:30,022 --> 00:28:32,064
group with a graph. So it's kind of

679
00:28:32,102 --> 00:28:34,704
analogous to that, right? So it's just

680
00:28:34,742 --> 00:28:36,620
kind of a different kind of translations

681
00:28:36,780 --> 00:28:39,270
where we replace the object on the left.

682
00:28:39,720 --> 00:28:41,956
Now it looks like this is a graph, this

683
00:28:41,978 --> 00:28:44,100
is a group, right? But kind of the rest

684
00:28:44,170 --> 00:28:47,716
stays exactly the same. And this kind of

685
00:28:47,738 --> 00:28:49,924
gives us a way to formalize in a way by

686
00:28:49,962 --> 00:28:51,956
looking in this topological perspective

687
00:28:52,068 --> 00:28:53,940
to connect all these kind of symmetries

688
00:28:54,020 --> 00:28:55,304
and things like that that have been

689
00:28:55,342 --> 00:28:56,888
explored quite a lot in machine learning

690
00:28:56,974 --> 00:28:59,384
to message passing on graphs and to see

691
00:28:59,422 --> 00:29:01,740
one way in which they are related.

692
00:29:02,720 --> 00:29:05,164
Okay, so now you might say, okay, this

693
00:29:05,202 --> 00:29:06,892
was all very sophisticated and nice,

694
00:29:06,946 --> 00:29:09,084
but you know, what, is this going

695
00:29:09,122 --> 00:29:11,708
anywhere, basically? And I'm just going

696
00:29:11,714 --> 00:29:14,000
to show you kind of a very short

697
00:29:14,070 --> 00:29:17,024
example. There's more, but the time is

698
00:29:17,062 --> 00:29:19,984
limited. And something we showed is

699
00:29:20,022 --> 00:29:22,876
that, as I was saying in the beginning,

700
00:29:22,988 --> 00:29:24,416
many graph neural networks kind of

701
00:29:24,438 --> 00:29:26,508
struggle in these heterophilic graphs.

702
00:29:26,604 --> 00:29:29,364
And what we showed is that no matter how

703
00:29:29,402 --> 00:29:31,984
heterophilic or kind of weird your graph

704
00:29:32,032 --> 00:29:35,844
is, you can always kind of find some

705
00:29:35,962 --> 00:29:37,768
shift structure, essentially kind of a

706
00:29:37,774 --> 00:29:42,216
message passing neural network, that if

707
00:29:42,238 --> 00:29:44,168
you use sufficient layers, it will be

708
00:29:44,174 --> 00:29:46,856
able to disentangle the classes of the

709
00:29:46,878 --> 00:29:49,496
nodes, right? So just to show you in

710
00:29:49,518 --> 00:29:51,564
this picture what you have here on the

711
00:29:51,602 --> 00:29:55,512
very far left, the colors of the nodes,

712
00:29:55,656 --> 00:29:59,084
they show the class. So there's three

713
00:29:59,122 --> 00:30:01,150
colors here. So three classes, right?

714
00:30:01,520 --> 00:30:03,116
And this is kind of the graph in the

715
00:30:03,138 --> 00:30:05,380
beginning and the position of the nodes

716
00:30:05,400 --> 00:30:07,616
in this box denotes the features. So

717
00:30:07,638 --> 00:30:08,864
that's a way to kind of just to

718
00:30:08,902 --> 00:30:10,320
visualize the features.

719
00:30:12,660 --> 00:30:15,104
The 2D position is actually the 2D

720
00:30:15,142 --> 00:30:17,856
feature vector of each node. And you can

721
00:30:17,878 --> 00:30:19,108
see in the beginning, everything is kind

722
00:30:19,114 --> 00:30:20,884
of super messy and entangled, right?

723
00:30:20,922 --> 00:30:22,964
Like if you want to classify these

724
00:30:23,002 --> 00:30:24,564
nodes, it's kind of very hard because

725
00:30:24,602 --> 00:30:26,144
their representations, their initial

726
00:30:26,192 --> 00:30:28,728
representations are very messy and kind

727
00:30:28,734 --> 00:30:32,024
of intertwined. But as we stack more

728
00:30:32,062 --> 00:30:34,296
layers of a particular kind of sheath or

729
00:30:34,318 --> 00:30:37,144
a message passing model, you see how

730
00:30:37,182 --> 00:30:40,696
progressively these classes get kind of

731
00:30:40,718 --> 00:30:42,868
more disentangled and more disentangled

732
00:30:42,964 --> 00:30:45,724
at each new layer. So these

733
00:30:45,762 --> 00:30:48,156
representations kind of collapse and

734
00:30:48,178 --> 00:30:49,832
they form these kind of clusterings,

735
00:30:49,896 --> 00:30:51,324
right? And then when you get with

736
00:30:51,362 --> 00:30:53,216
something like at the end, you can kind

737
00:30:53,238 --> 00:30:55,104
of see these three communities very

738
00:30:55,142 --> 00:30:57,648
clearly and it's extremely easy to

739
00:30:57,734 --> 00:31:01,728
separate. And kind of the essence behind

740
00:31:01,814 --> 00:31:04,704
these results was we showed for

741
00:31:04,742 --> 00:31:06,576
different kinds of problems what sorts

742
00:31:06,608 --> 00:31:09,076
of chief or message passing models you

743
00:31:09,098 --> 00:31:11,396
need by using this theory to solve kind

744
00:31:11,418 --> 00:31:14,308
of problems. And this is quite important

745
00:31:14,394 --> 00:31:16,708
because it kind of shows you some

746
00:31:16,874 --> 00:31:19,076
important bits and pieces in the

747
00:31:19,098 --> 00:31:21,768
architecture that you might want to kind

748
00:31:21,774 --> 00:31:24,100
of change or use in order to solve

749
00:31:24,180 --> 00:31:26,184
certain kinds of problems. And we also

750
00:31:26,222 --> 00:31:28,196
had some sort of impossibility resolved.

751
00:31:28,228 --> 00:31:30,536
So if you use a graph neural network of

752
00:31:30,558 --> 00:31:32,380
some kind, you can't solve this problem

753
00:31:32,450 --> 00:31:33,884
or you'll struggle to solve this

754
00:31:33,922 --> 00:31:36,428
problem. And we also saw, okay, if you

755
00:31:36,434 --> 00:31:39,276
use some more general ones, then you

756
00:31:39,298 --> 00:31:42,496
might have a chance. So this is kind of

757
00:31:42,518 --> 00:31:46,544
some high level view behind this

758
00:31:46,582 --> 00:31:50,560
theoretical stuff. And what we actually

759
00:31:50,630 --> 00:31:53,904
do in practice is to

760
00:31:53,942 --> 00:31:55,916
essentially learn these message passing

761
00:31:55,948 --> 00:31:57,868
functions or to learn the sheath or

762
00:31:57,894 --> 00:32:00,644
these matrices. So in practice, like

763
00:32:00,682 --> 00:32:02,016
when someone gives you a node

764
00:32:02,048 --> 00:32:04,676
classification task, it's very hard to

765
00:32:04,698 --> 00:32:08,250
know beforehand what exactly is

766
00:32:09,580 --> 00:32:11,240
the right sheep or the right message

767
00:32:11,310 --> 00:32:14,280
passing model to solve that task.

768
00:32:14,940 --> 00:32:16,792
And what we do essentially, we learn

769
00:32:16,846 --> 00:32:18,984
that from data. So we learn these

770
00:32:19,022 --> 00:32:21,144
matrices that do the message passing.

771
00:32:21,192 --> 00:32:25,292
We learn them from data by using

772
00:32:25,346 --> 00:32:27,256
some neural networks which are shown

773
00:32:27,288 --> 00:32:30,396
here in red. And then you learn how to

774
00:32:30,418 --> 00:32:32,796
kind of transfer features between these

775
00:32:32,818 --> 00:32:34,684
vector spaces and kind of move them

776
00:32:34,722 --> 00:32:36,384
around. So this is just showing how

777
00:32:36,422 --> 00:32:39,104
these vectors, which are features of

778
00:32:39,142 --> 00:32:40,528
these nodes and edges, how they're kind

779
00:32:40,534 --> 00:32:42,560
of moved around by kind of going through

780
00:32:42,630 --> 00:32:44,376
via these matrices, just some matrix

781
00:32:44,428 --> 00:32:47,956
multiplications. Okay,

782
00:32:48,058 --> 00:32:49,972
so that's kind of the high level view

783
00:32:50,026 --> 00:32:52,356
behind this model. And we evaluated this

784
00:32:52,378 --> 00:32:54,928
on some kind of real world heterophilic

785
00:32:54,944 --> 00:32:58,632
data sets where you

786
00:32:58,686 --> 00:33:00,616
have to classify nodes based on kind of

787
00:33:00,638 --> 00:33:02,676
various communities or different kinds

788
00:33:02,708 --> 00:33:05,000
of labels. And these data sets going

789
00:33:05,070 --> 00:33:08,792
from right to left, they are getting

790
00:33:08,846 --> 00:33:10,796
more heterophilic. So in some sense more

791
00:33:10,818 --> 00:33:13,224
challenging for classic architectures.

792
00:33:13,352 --> 00:33:15,084
And our models, which are kind of

793
00:33:15,122 --> 00:33:17,148
inspired by all this stuff that I

794
00:33:17,154 --> 00:33:19,756
mentioned, they score quite highly in

795
00:33:19,778 --> 00:33:23,356
these benchmarks. And at the same time,

796
00:33:23,378 --> 00:33:25,760
they also revealed some or justified

797
00:33:26,100 --> 00:33:28,572
some various choices that other models

798
00:33:28,636 --> 00:33:30,816
in this space have done. But maybe they

799
00:33:30,838 --> 00:33:32,464
were not so well justified, or maybe

800
00:33:32,502 --> 00:33:34,252
they had different kind of motivations.

801
00:33:34,316 --> 00:33:37,444
We also managed to kind of show why

802
00:33:37,562 --> 00:33:39,348
various things they were already doing,

803
00:33:39,434 --> 00:33:41,396
why they made sense from the point of

804
00:33:41,418 --> 00:33:44,420
view of this kind of theory.

805
00:33:45,560 --> 00:33:49,450
All right, well, that's all I had.

806
00:33:50,140 --> 00:33:52,904
Yeah, thanks for listening and yeah,

807
00:33:53,022 --> 00:33:55,224
happy to chat more about this and also

808
00:33:55,262 --> 00:33:57,210
have lots of backup slides in case,

809
00:33:57,660 --> 00:34:00,636
depending on how far we venture off with

810
00:34:00,658 --> 00:34:01,470
this question.

811
00:34:04,320 --> 00:34:07,436
Cool. Well, awesome work. Thank you for

812
00:34:07,458 --> 00:34:09,676
the presentation. For people who are in

813
00:34:09,698 --> 00:34:12,476
the live chat, they can write some

814
00:34:12,578 --> 00:34:13,968
questions, but there's many things I

815
00:34:13,974 --> 00:34:17,168
think we could talk about. So I want to

816
00:34:17,174 --> 00:34:18,896
start with reading a quote from an

817
00:34:18,918 --> 00:34:23,228
abstract of the paper by von

818
00:34:23,244 --> 00:34:25,396
der Laur, Kudal and DeVries just to kind

819
00:34:25,418 --> 00:34:26,944
of ground this in the active inference

820
00:34:26,992 --> 00:34:29,876
context and really justify why the

821
00:34:29,898 --> 00:34:32,244
message passing approaches that you are

822
00:34:32,282 --> 00:34:34,656
describing are helping in the active

823
00:34:34,688 --> 00:34:37,216
inference modeling. It's two papers.

824
00:34:37,248 --> 00:34:38,916
It's called Realizing Synthetic Active

825
00:34:38,948 --> 00:34:41,976
Inference Agents and they wrote, with a

826
00:34:41,998 --> 00:34:43,896
full message passing account of

827
00:34:43,918 --> 00:34:46,664
synthetic active inference agents, it

828
00:34:46,702 --> 00:34:49,156
becomes possible to derive and reuse

829
00:34:49,188 --> 00:34:52,060
message updates across models and move

830
00:34:52,130 --> 00:34:54,284
closer to industrial applications of

831
00:34:54,322 --> 00:34:56,856
synthetic active inference framework.

832
00:34:57,048 --> 00:35:00,456
So how does knowing the message passing

833
00:35:00,568 --> 00:35:04,400
structure help reuse a model across

834
00:35:04,470 --> 00:35:06,496
different settings or facilitate the

835
00:35:06,518 --> 00:35:08,690
legibility of the model?

836
00:35:10,100 --> 00:35:12,768
Right? So first of all, I'm not super

837
00:35:12,934 --> 00:35:14,736
familiar with the kind of active

838
00:35:14,768 --> 00:35:18,516
inference literature, so you'll have

839
00:35:18,538 --> 00:35:21,824
to help me there a bit to anchor

840
00:35:21,872 --> 00:35:23,332
maybe the discussions a bit more into

841
00:35:23,386 --> 00:35:25,940
that. But I think if I understand

842
00:35:26,010 --> 00:35:29,316
correctly, the kind of question you're

843
00:35:29,348 --> 00:35:31,576
getting at is basically how can kind of

844
00:35:31,598 --> 00:35:35,336
message passing help us generalize in

845
00:35:35,358 --> 00:35:36,776
kind of various kinds of settings or

846
00:35:36,798 --> 00:35:39,784
maybe from one graph to another and

847
00:35:39,822 --> 00:35:43,576
things like that? And this is a kind

848
00:35:43,598 --> 00:35:45,832
of active area of research, how exactly

849
00:35:45,886 --> 00:35:48,348
digitalization is happening. But

850
00:35:48,434 --> 00:35:49,676
something you could notice, for

851
00:35:49,698 --> 00:35:52,744
instance, something that for instance,

852
00:35:52,792 --> 00:35:54,544
was shown like these models are quite

853
00:35:54,582 --> 00:35:56,524
good at, for instance, plotting patterns

854
00:35:56,652 --> 00:35:58,912
or structures depending on how exactly

855
00:35:58,966 --> 00:36:00,640
you implement them. But for instance,

856
00:36:01,220 --> 00:36:03,056
let's say you have a triangle in your

857
00:36:03,078 --> 00:36:05,968
graph or that'll be kind of the simplest

858
00:36:05,984 --> 00:36:08,532
structure, right? You have a triangle or

859
00:36:08,586 --> 00:36:10,356
some other kind of kind of gadget in

860
00:36:10,378 --> 00:36:12,496
your graph, like particular subgraphs

861
00:36:12,608 --> 00:36:15,350
that might show up in different kinds of

862
00:36:16,200 --> 00:36:17,988
various graphs. The graphs themselves

863
00:36:18,074 --> 00:36:19,384
might look completely different from

864
00:36:19,422 --> 00:36:22,084
each other, but these kind of patterns

865
00:36:22,132 --> 00:36:24,852
might kind of be reemerging in multiple

866
00:36:24,916 --> 00:36:27,176
like local patterns might reemerge in

867
00:36:27,198 --> 00:36:29,176
multiple graphs and that could help you

868
00:36:29,198 --> 00:36:30,584
a way to kind of generalize, right?

869
00:36:30,622 --> 00:36:32,488
Like you could see, for instance, if you

870
00:36:32,494 --> 00:36:34,616
have clicks, they're super important in

871
00:36:34,638 --> 00:36:36,632
kind of when you do social network

872
00:36:36,696 --> 00:36:38,364
modeling and things like that because

873
00:36:38,402 --> 00:36:40,492
they kind of show this kind of close

874
00:36:40,546 --> 00:36:41,884
group of friends, right? They all talk

875
00:36:41,922 --> 00:36:43,628
to each other, so they kind of form a

876
00:36:43,634 --> 00:36:45,056
click like everyone's connected to each

877
00:36:45,078 --> 00:36:47,056
other, right? And then you might be able

878
00:36:47,078 --> 00:36:48,768
to use that engine like another

879
00:36:48,854 --> 00:36:50,880
completely different social context

880
00:36:51,460 --> 00:36:53,616
where these agents are again kind of

881
00:36:53,638 --> 00:36:55,456
communicating in a similar manner or are

882
00:36:55,478 --> 00:36:57,572
connected in a similar manner, even if

883
00:36:57,626 --> 00:36:59,604
the kind of overall pattern is quite

884
00:36:59,642 --> 00:37:02,996
different. And it goes way beyond just

885
00:37:03,018 --> 00:37:05,236
kind of structural similarities because

886
00:37:05,258 --> 00:37:07,108
there's also features in there. So

887
00:37:07,194 --> 00:37:08,916
there's combinations of kind of

888
00:37:08,938 --> 00:37:11,188
structural patterns and features that

889
00:37:11,354 --> 00:37:13,684
give you even more complicated patterns,

890
00:37:13,732 --> 00:37:15,764
right? Like you might have a triangle,

891
00:37:15,812 --> 00:37:18,808
but then also two of the features in

892
00:37:18,814 --> 00:37:21,884
this triangle look in a certain way and

893
00:37:21,922 --> 00:37:24,364
one that looks in another way. So that

894
00:37:24,402 --> 00:37:26,248
gives you even more kind of refinement

895
00:37:26,424 --> 00:37:29,944
and even kind of richer pattern

896
00:37:30,072 --> 00:37:33,692
detection abilities. So you have

897
00:37:33,746 --> 00:37:37,084
essentially this ability to kind of spot

898
00:37:37,132 --> 00:37:39,456
patterns at multiple scales as well. So

899
00:37:39,478 --> 00:37:41,596
you could see this happening at multiple

900
00:37:41,628 --> 00:37:43,024
scales. You could have patterns of

901
00:37:43,062 --> 00:37:46,112
patterns, right? You could have entire

902
00:37:46,166 --> 00:37:47,664
communities connected in various

903
00:37:47,712 --> 00:37:50,468
patterns and so on. And again, it's kind

904
00:37:50,474 --> 00:37:52,196
of also a research question. How do you

905
00:37:52,218 --> 00:37:54,276
capture these hierarchical patterns and

906
00:37:54,298 --> 00:37:57,204
so on. In general, you have to do more

907
00:37:57,242 --> 00:37:59,344
message passing if you want to capture

908
00:37:59,392 --> 00:38:01,656
things that are further away from each

909
00:38:01,678 --> 00:38:03,064
other because otherwise they can't talk

910
00:38:03,102 --> 00:38:05,608
to each other, right? So, yeah, I don't

911
00:38:05,614 --> 00:38:07,672
know if that actually answered your

912
00:38:07,726 --> 00:38:09,768
question or if I was kind of going in

913
00:38:09,774 --> 00:38:12,148
the right direction there. It's great.

914
00:38:12,174 --> 00:38:13,692
It brings up a lot of different cool

915
00:38:13,746 --> 00:38:15,644
ideas like this patterns all the way

916
00:38:15,682 --> 00:38:18,604
down, but totally agree. I think we can

917
00:38:18,642 --> 00:38:21,388
now perhaps explore some more specific

918
00:38:21,474 --> 00:38:24,108
connections to active inference because

919
00:38:24,274 --> 00:38:27,724
hopefully the listenership or viewership

920
00:38:27,772 --> 00:38:29,216
of this, it's kind of like a two way

921
00:38:29,238 --> 00:38:31,152
street. Like some people may be coming

922
00:38:31,206 --> 00:38:33,376
from more of your background and then

923
00:38:33,478 --> 00:38:34,896
learning about active inference and

924
00:38:34,918 --> 00:38:36,964
generative models as a specific system

925
00:38:37,002 --> 00:38:38,244
of interest for The First time. But

926
00:38:38,282 --> 00:38:40,276
also, certainly for A lot of people in

927
00:38:40,298 --> 00:38:42,708
the active inference space, these

928
00:38:42,874 --> 00:38:45,684
methods coming from category theory have

929
00:38:45,722 --> 00:38:49,752
only recently come up to,

930
00:38:49,806 --> 00:38:52,004
I guess, more prominence in Bayesian

931
00:38:52,052 --> 00:38:55,268
modeling, at least where we are. So it's

932
00:38:55,284 --> 00:38:58,232
a cool connection to make. I think one

933
00:38:58,286 --> 00:39:01,180
of the biggest touch points off the bat

934
00:39:01,330 --> 00:39:03,068
was, like you mentioned, multiplying a

935
00:39:03,074 --> 00:39:05,244
matrix by a vector and interpreting that

936
00:39:05,282 --> 00:39:08,344
as an edge. So just in the inference

937
00:39:08,392 --> 00:39:11,068
part of the generative model about

938
00:39:11,154 --> 00:39:13,376
sensory observations, we always talk

939
00:39:13,398 --> 00:39:15,376
about the thermometer observation and

940
00:39:15,398 --> 00:39:17,152
then the underlying hidden state

941
00:39:17,286 --> 00:39:20,796
temperature. So that exactly describes

942
00:39:20,828 --> 00:39:24,096
that case. And that's why we

943
00:39:24,118 --> 00:39:25,936
can represent the active inference

944
00:39:25,968 --> 00:39:27,776
generative models, the perceptual parts

945
00:39:27,808 --> 00:39:30,064
and the action parts in terms of matrix

946
00:39:30,112 --> 00:39:33,156
multiplication. It's why the MATLAB code

947
00:39:33,258 --> 00:39:36,812
for generative models does look mostly

948
00:39:36,896 --> 00:39:39,688
like matrix multiplication and it can

949
00:39:39,694 --> 00:39:42,330
all be done explicitly that way.

950
00:39:46,220 --> 00:39:47,736
Are there models that don't have this

951
00:39:47,758 --> 00:39:51,468
feature? Or what do we gain by

952
00:39:51,554 --> 00:39:54,830
having all of our edges defined as

953
00:39:56,480 --> 00:39:59,228
appreciative action with a matrix and a

954
00:39:59,234 --> 00:40:02,040
vector in this setting of agent

955
00:40:02,130 --> 00:40:04,064
generative models with perception and

956
00:40:04,102 --> 00:40:07,890
action? Any thoughts on that?

957
00:40:08,420 --> 00:40:12,656
Yeah, I think essentially kind of the

958
00:40:12,678 --> 00:40:15,380
graph structure is kind of telling you

959
00:40:15,450 --> 00:40:17,524
these things interact in some way,

960
00:40:17,562 --> 00:40:21,232
right? So there's some communication

961
00:40:21,296 --> 00:40:24,964
between these vertices if we're kind of

962
00:40:25,002 --> 00:40:27,736
in a crash setting, right, and then kind

963
00:40:27,758 --> 00:40:30,424
of what the sheath is giving you or any

964
00:40:30,462 --> 00:40:33,816
message passing model essentially is

965
00:40:33,998 --> 00:40:37,304
expressing a way in which way that

966
00:40:37,342 --> 00:40:39,480
connection should manifest in the model

967
00:40:39,550 --> 00:40:42,156
or in what way that connection should be

968
00:40:42,178 --> 00:40:45,516
used to process information. So in this

969
00:40:45,538 --> 00:40:47,932
case I was mentioning, okay, you have

970
00:40:48,066 --> 00:40:50,876
linear maps because you could go on the

971
00:40:50,898 --> 00:40:52,424
type if your type of data are vector

972
00:40:52,472 --> 00:40:54,088
spaces, then this transformation will be

973
00:40:54,114 --> 00:40:57,516
some sort of linear maps. But it doesn't

974
00:40:57,548 --> 00:40:59,948
necessarily have to be. So for instance,

975
00:41:00,044 --> 00:41:01,564
it could go to any nonlinear

976
00:41:01,612 --> 00:41:03,568
transformation, right? And this is

977
00:41:03,574 --> 00:41:04,576
what's happening in general. In

978
00:41:04,598 --> 00:41:07,924
practice, if you have a neighbor, the

979
00:41:07,962 --> 00:41:10,196
message coming from that neighbor could

980
00:41:10,218 --> 00:41:12,196
be modulated by any sort of

981
00:41:12,218 --> 00:41:14,644
transformation you want. So it could be

982
00:41:14,682 --> 00:41:16,276
linear, it could be nonlinear, it could

983
00:41:16,298 --> 00:41:18,810
be something, I don't know,

984
00:41:19,660 --> 00:41:23,000
you can specify it basically. But

985
00:41:23,070 --> 00:41:25,130
essentially you could think of this as

986
00:41:25,580 --> 00:41:27,464
you have a structure level telling you

987
00:41:27,502 --> 00:41:30,456
who should communicate to whom. And then

988
00:41:30,478 --> 00:41:32,204
you kind of have some semantics that

989
00:41:32,242 --> 00:41:34,284
this kind of shift is adding on top

990
00:41:34,482 --> 00:41:36,572
saying how should these things

991
00:41:36,626 --> 00:41:38,444
communicate? Right? Like the first thing

992
00:41:38,482 --> 00:41:41,720
is who should communicate or what should

993
00:41:41,810 --> 00:41:44,016
communicate. And then the semantics we

994
00:41:44,038 --> 00:41:47,184
add on top essentially describe how

995
00:41:47,222 --> 00:41:50,988
should that communication manifest.

996
00:41:51,164 --> 00:41:52,000
Essentially.

997
00:41:54,340 --> 00:41:57,216
Very cool. I think that maps exactly to

998
00:41:57,238 --> 00:41:58,944
how we talk about the sparsity of

999
00:41:58,982 --> 00:42:02,372
variables in degenerative model. So here

1000
00:42:02,426 --> 00:42:05,264
the topology of the nodes in the graph

1001
00:42:05,312 --> 00:42:07,088
that we want to do message passing on

1002
00:42:07,194 --> 00:42:09,416
are going to be describing the agent and

1003
00:42:09,438 --> 00:42:11,912
the environment or the generative model

1004
00:42:11,966 --> 00:42:14,072
that includes perception, cognition and

1005
00:42:14,126 --> 00:42:16,372
action. So a lot of people have proposed

1006
00:42:16,436 --> 00:42:20,012
different sparsity architectures for

1007
00:42:20,066 --> 00:42:22,680
integrated modeling of perception,

1008
00:42:22,760 --> 00:42:26,076
cognition, action. So one example would

1009
00:42:26,098 --> 00:42:27,944
just be like kind of around the clock,

1010
00:42:28,072 --> 00:42:29,864
like action influences the environment,

1011
00:42:29,912 --> 00:42:31,836
environment influences perception. Back

1012
00:42:31,858 --> 00:42:34,160
to cognition, you could add a self loop

1013
00:42:34,580 --> 00:42:36,496
using an arkov blanket and different

1014
00:42:36,518 --> 00:42:38,876
kinds of connectivities and that defines

1015
00:42:38,908 --> 00:42:41,872
the sparsity topologically, which is

1016
00:42:41,926 --> 00:42:44,996
where you showed the stack and you were

1017
00:42:45,018 --> 00:42:47,076
on the second and the third levels, I

1018
00:42:47,098 --> 00:42:49,860
think, of the stack and then what flows

1019
00:42:51,400 --> 00:42:53,140
it has to be described,

1020
00:42:54,920 --> 00:42:58,856
what that edge does. So what

1021
00:42:58,878 --> 00:43:01,880
is that? That is also being provided.

1022
00:43:02,940 --> 00:43:05,208
Yeah, exactly. And it could even go to

1023
00:43:05,214 --> 00:43:07,444
the extreme where does that edge

1024
00:43:07,492 --> 00:43:09,492
actually do anything? So for instance,

1025
00:43:09,636 --> 00:43:13,064
if you have a matrix that's just

1026
00:43:13,102 --> 00:43:14,836
the zero matrix, for instance,

1027
00:43:14,948 --> 00:43:16,876
associated to that edge, it will just

1028
00:43:16,898 --> 00:43:19,036
kind of multiply by zero and that gives

1029
00:43:19,058 --> 00:43:21,544
you zero and it's kind of essentially

1030
00:43:21,592 --> 00:43:23,596
pruning that edge, right? Like I kind of

1031
00:43:23,618 --> 00:43:24,944
get rid of it, I don't want that

1032
00:43:24,982 --> 00:43:27,308
communication to happen. But there's

1033
00:43:27,324 --> 00:43:29,344
also kind of this possibility where

1034
00:43:29,382 --> 00:43:31,724
these kind of semantics, they override

1035
00:43:31,772 --> 00:43:34,224
the structural level where you say,

1036
00:43:34,262 --> 00:43:36,884
okay, I don't need to communicate with

1037
00:43:36,922 --> 00:43:39,396
this other agent person or whatever. It

1038
00:43:39,418 --> 00:43:40,932
depends on what these rates actually

1039
00:43:40,986 --> 00:43:44,196
mean and in what context you are.

1040
00:43:44,298 --> 00:43:46,756
And then there's also the case where you

1041
00:43:46,778 --> 00:43:48,612
could do some sort of selective pruning

1042
00:43:48,676 --> 00:43:52,170
where this matrix depending on

1043
00:43:52,540 --> 00:43:55,124
so in kind of linear algebra, the matrix

1044
00:43:55,172 --> 00:43:58,264
has a kernel. So it's all the stuff that

1045
00:43:58,302 --> 00:44:00,184
that matrix sends to zero. So what

1046
00:44:00,222 --> 00:44:02,276
vectors are sent to zero, right? But not

1047
00:44:02,318 --> 00:44:03,768
everything will be sent to zero unless

1048
00:44:03,784 --> 00:44:07,196
you're the zero matrix. So depending on

1049
00:44:07,218 --> 00:44:08,972
the features of the neighbors, you could

1050
00:44:09,026 --> 00:44:11,356
also just send some of the neighbors to

1051
00:44:11,378 --> 00:44:13,336
zero, right? And that kind of removes

1052
00:44:13,368 --> 00:44:14,976
those neighbors from the equation. They

1053
00:44:14,998 --> 00:44:18,588
just kind of get they're

1054
00:44:18,604 --> 00:44:20,976
not factored in anymore. So you kind of

1055
00:44:20,998 --> 00:44:23,264
have this it's a way to get this

1056
00:44:23,302 --> 00:44:25,044
parsity, I guess, that you were also

1057
00:44:25,082 --> 00:44:28,756
talking about where certain

1058
00:44:28,938 --> 00:44:31,488
only maybe a small subset of the inputs

1059
00:44:31,584 --> 00:44:35,488
or kind of only subset

1060
00:44:35,504 --> 00:44:37,028
of the features are actually kind of

1061
00:44:37,034 --> 00:44:39,476
doing some meaningful stuff among the

1062
00:44:39,498 --> 00:44:40,836
neighbors and everything else will be

1063
00:44:40,858 --> 00:44:44,456
kind of zeroed out. Yeah, that makes me

1064
00:44:44,478 --> 00:44:47,192
think of the Lasso regression, which

1065
00:44:47,246 --> 00:44:49,628
tries to set most variables of having an

1066
00:44:49,634 --> 00:44:52,008
impact of zero so that a few hopefully

1067
00:44:52,104 --> 00:44:53,788
important variables really pop out in

1068
00:44:53,794 --> 00:44:56,440
the analysis. But also there's newer

1069
00:44:56,520 --> 00:45:00,040
techniques, I guess, of attention

1070
00:45:00,200 --> 00:45:03,344
modeling and reweighting that isn't just

1071
00:45:03,382 --> 00:45:06,112
like, okay, set five of them to one and

1072
00:45:06,166 --> 00:45:08,672
then the rest of them to zero. More

1073
00:45:08,726 --> 00:45:11,920
nuanced. So I think that sparsity with

1074
00:45:11,990 --> 00:45:15,456
the expressivity is basically the

1075
00:45:15,478 --> 00:45:17,088
best of both worlds because you do want

1076
00:45:17,094 --> 00:45:18,596
to have a situation where there is an

1077
00:45:18,618 --> 00:45:20,276
edge, but the attention being paid to it

1078
00:45:20,298 --> 00:45:22,740
is zero. So functionally,

1079
00:45:23,480 --> 00:45:25,796
that doesn't have an update on the

1080
00:45:25,818 --> 00:45:27,712
belief state, even though in principle

1081
00:45:27,776 --> 00:45:30,344
the edge exists. And that's why we can

1082
00:45:30,382 --> 00:45:34,088
model situations where the agent

1083
00:45:34,174 --> 00:45:35,624
believes they have impact in the world.

1084
00:45:35,662 --> 00:45:37,176
But actually, just because the edge in

1085
00:45:37,198 --> 00:45:39,064
principle exists doesn't mean that it

1086
00:45:39,102 --> 00:45:41,784
has any given impact. And so that allows

1087
00:45:41,832 --> 00:45:44,732
the articulation of these models where

1088
00:45:44,786 --> 00:45:47,912
they factorize and keeps interpretable

1089
00:45:47,976 --> 00:45:51,820
motifs in terms of just little clusters

1090
00:45:52,260 --> 00:45:55,456
of motifs. Here in

1091
00:45:55,478 --> 00:45:57,116
our case describing the action

1092
00:45:57,148 --> 00:46:00,064
perception and cognition types of

1093
00:46:00,102 --> 00:46:01,936
systems of interest. But people, I

1094
00:46:01,958 --> 00:46:03,970
believe, already implicitly do this.

1095
00:46:04,900 --> 00:46:06,964
They will often add an adjective and

1096
00:46:07,002 --> 00:46:09,556
refer to x kind of active inference. So

1097
00:46:09,578 --> 00:46:11,108
like deep active inference with a

1098
00:46:11,114 --> 00:46:12,928
temporal horizon, sophisticated active

1099
00:46:12,944 --> 00:46:14,740
inference with this kind of nesting.

1100
00:46:15,160 --> 00:46:17,544
And those are pointing to a given

1101
00:46:17,582 --> 00:46:19,588
feature. But of course, those features,

1102
00:46:19,764 --> 00:46:22,600
as we're hoping, should be composable.

1103
00:46:23,500 --> 00:46:27,560
And so this seems to be bringing

1104
00:46:28,220 --> 00:46:30,684
tools that are even more general than

1105
00:46:30,722 --> 00:46:33,612
just action perception modeling because

1106
00:46:33,666 --> 00:46:35,544
they're at a lower level of abstraction

1107
00:46:35,592 --> 00:46:37,884
than any specific system of interest.

1108
00:46:38,082 --> 00:46:42,052
But where this work and kind of timeless

1109
00:46:42,136 --> 00:46:45,744
thinking around cybernetic systems come

1110
00:46:45,782 --> 00:46:47,884
together through the active inference

1111
00:46:47,932 --> 00:46:50,080
generative model as a Bayes graph,

1112
00:46:51,300 --> 00:46:55,200
it gets very exciting. Yeah. And maybe

1113
00:46:55,270 --> 00:46:57,124
also something worth emphasizing here is

1114
00:46:57,162 --> 00:46:59,828
that even if this kind of semantic level

1115
00:46:59,994 --> 00:47:02,660
can get rid of some edges, right, by

1116
00:47:02,730 --> 00:47:04,964
doing this kind of pruning. Something it

1117
00:47:05,002 --> 00:47:08,020
cannot get rid of is the computation.

1118
00:47:08,180 --> 00:47:10,730
So something that kind of that

1119
00:47:11,740 --> 00:47:14,728
structural graphs level forces you to

1120
00:47:14,734 --> 00:47:16,456
do. It kind of tells you what should you

1121
00:47:16,478 --> 00:47:19,050
spend compute time on, right? Because

1122
00:47:20,620 --> 00:47:22,596
even if you're going to decide to prune

1123
00:47:22,628 --> 00:47:24,588
an edge, you still need to decide that

1124
00:47:24,674 --> 00:47:27,324
which takes compute time. So you still

1125
00:47:27,362 --> 00:47:28,876
need to look at all your neighbors if

1126
00:47:28,898 --> 00:47:31,596
you're a node, right, and decide what to

1127
00:47:31,618 --> 00:47:33,452
prune. Or maybe you don't prune anything

1128
00:47:33,506 --> 00:47:35,216
or whatever, but you have to look at

1129
00:47:35,238 --> 00:47:37,856
every edge. And one way to look at this

1130
00:47:37,878 --> 00:47:41,776
is the graph structure defines you

1131
00:47:41,798 --> 00:47:44,176
a computational graph or kind of a

1132
00:47:44,198 --> 00:47:47,704
computational series of computational

1133
00:47:47,772 --> 00:47:49,828
steps you have to execute. And then the

1134
00:47:49,834 --> 00:47:51,236
kind of the sheet structure or the

1135
00:47:51,258 --> 00:47:53,584
message passing model actually specifies

1136
00:47:53,632 --> 00:47:55,684
what those steps are and in what

1137
00:47:55,722 --> 00:47:58,090
particular way they look. Exactly.

1138
00:47:59,900 --> 00:48:02,072
That's one point. And you also mentioned

1139
00:48:02,126 --> 00:48:04,568
attention, and actually I'm glad you

1140
00:48:04,574 --> 00:48:06,152
did, because this is actually quite

1141
00:48:06,206 --> 00:48:08,712
related and in certain ways more general

1142
00:48:08,766 --> 00:48:10,988
than attention. And actually, maybe

1143
00:48:11,074 --> 00:48:13,196
going back to this slide, it might be a

1144
00:48:13,218 --> 00:48:16,332
nice way to see this.

1145
00:48:16,466 --> 00:48:19,644
So here basically what happens in

1146
00:48:19,682 --> 00:48:22,124
attention. Instead of learning these

1147
00:48:22,162 --> 00:48:24,544
matrices that we learn here in

1148
00:48:24,582 --> 00:48:25,772
attention, you learn attention

1149
00:48:25,836 --> 00:48:27,856
coefficients here. So you just learn a

1150
00:48:27,878 --> 00:48:29,212
scalar. That's the attention

1151
00:48:29,276 --> 00:48:31,824
coefficient. How much attention should I

1152
00:48:31,862 --> 00:48:35,244
pay to essentially this overall edge,

1153
00:48:35,292 --> 00:48:37,948
let's say, which will be just a scalar?

1154
00:48:38,044 --> 00:48:39,396
What we do is kind of a bit more

1155
00:48:39,418 --> 00:48:41,316
complicated because you just learn, how

1156
00:48:41,338 --> 00:48:43,488
do I transform these neighbors? So it's

1157
00:48:43,504 --> 00:48:44,996
kind of a whole matrix rather than a

1158
00:48:45,018 --> 00:48:48,456
single scalar, but there's also some

1159
00:48:48,478 --> 00:48:51,496
subtle differences. But in a follow up

1160
00:48:51,518 --> 00:48:53,800
work we did, we also combined this with

1161
00:48:53,950 --> 00:48:56,648
attention and went a bit more general

1162
00:48:56,734 --> 00:49:00,152
and that also worked quite

1163
00:49:00,206 --> 00:49:03,244
well. But the kind of underlying idea is

1164
00:49:03,282 --> 00:49:05,676
very similar. You want to modulate the

1165
00:49:05,698 --> 00:49:08,236
way you transform information based on

1166
00:49:08,258 --> 00:49:10,188
the information itself, right? So you

1167
00:49:10,194 --> 00:49:11,790
have this kind of one level of

1168
00:49:12,640 --> 00:49:15,848
recursivity. If you want that, you are

1169
00:49:15,874 --> 00:49:18,416
also alluding to that. It happens in

1170
00:49:18,438 --> 00:49:22,016
active inference, where, okay, so if

1171
00:49:22,038 --> 00:49:24,384
I'm node v, right, my neighbor knew it

1172
00:49:24,422 --> 00:49:27,264
has some features, and based on these

1173
00:49:27,302 --> 00:49:29,904
features, which are xu, I'm going to

1174
00:49:29,942 --> 00:49:32,116
find out the matrix that will be used to

1175
00:49:32,138 --> 00:49:34,804
process xu, right. So it's kind of very

1176
00:49:34,922 --> 00:49:36,484
recursive and it's what happens with

1177
00:49:36,522 --> 00:49:38,368
attention, right. Based on the features

1178
00:49:38,384 --> 00:49:40,696
of node u, I'm going to compute an

1179
00:49:40,718 --> 00:49:42,968
attention coefficient that I'm going to

1180
00:49:42,974 --> 00:49:45,144
apply to this feature of you, right?

1181
00:49:45,182 --> 00:49:46,504
I'm going to decide based on this

1182
00:49:46,542 --> 00:49:48,664
feature, how much attention should I pay

1183
00:49:48,702 --> 00:49:51,404
to it. And here we decide how should I

1184
00:49:51,442 --> 00:49:54,632
process it more generally in a linear

1185
00:49:54,696 --> 00:49:58,380
way. So you have this kind of loopiness

1186
00:49:58,880 --> 00:50:00,910
structure embedded in there.

1187
00:50:02,880 --> 00:50:06,092
Awesome. I'll bring up a few more points

1188
00:50:06,146 --> 00:50:07,520
because I think there's so many great

1189
00:50:07,590 --> 00:50:11,536
pieces. So Toby St. Clair, Smyth who

1190
00:50:11,558 --> 00:50:13,724
we recently discussed his dissertation

1191
00:50:13,772 --> 00:50:17,168
in Livestream 54, introduced a term

1192
00:50:17,184 --> 00:50:19,076
or at least a phrasing, the

1193
00:50:19,178 --> 00:50:22,176
compositional cognitive cartography.

1194
00:50:22,288 --> 00:50:23,556
And so thinking about the

1195
00:50:23,578 --> 00:50:26,860
compositionality of cognitive systems,

1196
00:50:27,040 --> 00:50:30,616
and I think what you're describing here

1197
00:50:30,798 --> 00:50:33,512
with this notion that the mappings are

1198
00:50:33,566 --> 00:50:36,804
more general than the kind of attention

1199
00:50:36,852 --> 00:50:39,836
mechanisms known, famously today, that

1200
00:50:39,858 --> 00:50:40,972
those represent, like a lower

1201
00:50:41,026 --> 00:50:44,124
dimensional special case of one kind of

1202
00:50:44,162 --> 00:50:46,956
architecture. Makes me think about how

1203
00:50:47,138 --> 00:50:49,992
the Bayesian graph is kind of semantic

1204
00:50:50,056 --> 00:50:53,072
in principle and can have all of these

1205
00:50:53,126 --> 00:50:56,130
nice categorical formalisms around them.

1206
00:50:58,180 --> 00:51:00,304
And you can even build the connector to

1207
00:51:00,342 --> 00:51:02,576
empirical data with the presheaf and the

1208
00:51:02,598 --> 00:51:05,476
sheaf, which may be news to even many

1209
00:51:05,578 --> 00:51:07,732
empirical researchers doing data

1210
00:51:07,786 --> 00:51:10,436
analysis certainly was for me. But the

1211
00:51:10,458 --> 00:51:13,472
message passing provides a rigorous

1212
00:51:13,536 --> 00:51:16,068
translation from whatever semantic model

1213
00:51:16,154 --> 00:51:19,224
is proposed topologically to an

1214
00:51:19,262 --> 00:51:22,776
implementation procedure that can

1215
00:51:22,878 --> 00:51:27,092
be planned for and executed in linear

1216
00:51:27,156 --> 00:51:28,644
time or at least with definable

1217
00:51:28,692 --> 00:51:30,884
characteristics. So message passing

1218
00:51:30,932 --> 00:51:33,020
plays a really important part in going

1219
00:51:33,090 --> 00:51:36,284
from the abstract what is possible, to

1220
00:51:36,322 --> 00:51:38,172
the implementations of any of these

1221
00:51:38,226 --> 00:51:40,348
actual models. And it does it in a

1222
00:51:40,354 --> 00:51:42,876
really general way where is it accurate

1223
00:51:42,908 --> 00:51:46,560
to say that we hope that implementation

1224
00:51:47,460 --> 00:51:49,484
with message passing compatible

1225
00:51:49,532 --> 00:51:52,210
generative models will kind of roll out

1226
00:51:52,820 --> 00:51:56,196
better because we won't have some of the

1227
00:51:56,218 --> 00:51:59,140
engineering challenges that less

1228
00:51:59,210 --> 00:52:03,140
reusable abstractions might carry?

1229
00:52:05,880 --> 00:52:08,612
It's hard to say. I think there's also

1230
00:52:08,666 --> 00:52:10,584
certainly some limitations to this

1231
00:52:10,622 --> 00:52:14,152
paradigm as well. So just kind of doing

1232
00:52:14,206 --> 00:52:16,264
this kind of message passing, I think,

1233
00:52:16,302 --> 00:52:18,168
as you were mentioning, one thing is

1234
00:52:18,174 --> 00:52:20,904
that it kind of scales up quite easily,

1235
00:52:20,952 --> 00:52:22,796
like linearly with the size of the

1236
00:52:22,818 --> 00:52:25,564
graph, but that also come at a cost. So

1237
00:52:25,682 --> 00:52:28,380
there's certain results showing this has

1238
00:52:28,450 --> 00:52:32,088
limits in expressivity. So if

1239
00:52:32,114 --> 00:52:36,224
you actually want to go beyond this,

1240
00:52:36,342 --> 00:52:38,784
for instance, instead of just looking at

1241
00:52:38,822 --> 00:52:40,784
pairs of nodes, you have to look at

1242
00:52:40,822 --> 00:52:43,410
tuples and these kind of high order

1243
00:52:43,940 --> 00:52:47,376
groupings of nodes in order to

1244
00:52:47,398 --> 00:52:49,856
kind of get higher explicitity. There's

1245
00:52:49,888 --> 00:52:53,636
all sorts of techniques to do that and

1246
00:52:53,658 --> 00:52:55,040
there's always this kind of tension

1247
00:52:55,120 --> 00:52:57,944
between being more expressive and being

1248
00:52:57,982 --> 00:53:01,368
efficient that will always be there in

1249
00:53:01,454 --> 00:53:04,280
any sort of algorithm or method.

1250
00:53:06,780 --> 00:53:09,816
It's kind of hard to say. We can

1251
00:53:09,838 --> 00:53:11,516
definitely say this is kind of not the

1252
00:53:11,538 --> 00:53:13,276
optimist solution. Let's say if you want

1253
00:53:13,298 --> 00:53:15,644
to do things message passing in itself

1254
00:53:15,762 --> 00:53:17,356
but maybe doing some sort of

1255
00:53:17,378 --> 00:53:21,244
computations on graphs could

1256
00:53:21,282 --> 00:53:25,824
be maybe

1257
00:53:25,862 --> 00:53:28,960
also something that's kind of maybe

1258
00:53:29,030 --> 00:53:31,484
missing a bit in kind of the graph ML

1259
00:53:31,532 --> 00:53:35,030
setting. Is the context where

1260
00:53:37,560 --> 00:53:41,908
you assume your graph is known. And you

1261
00:53:41,914 --> 00:53:45,376
need to have some graph

1262
00:53:45,408 --> 00:53:47,016
structure, at least a sensible way to

1263
00:53:47,038 --> 00:53:49,576
construct it. Right. But for many kind

1264
00:53:49,598 --> 00:53:52,968
of more I

1265
00:53:52,974 --> 00:53:56,724
don't know, how should I phrase

1266
00:53:56,772 --> 00:54:00,336
that? I guess for less clearly defined

1267
00:54:00,388 --> 00:54:02,204
things like, okay, if I'm an agent doing

1268
00:54:02,242 --> 00:54:03,676
perception in the real world or

1269
00:54:03,698 --> 00:54:06,668
something, right? If I'm trying to

1270
00:54:06,754 --> 00:54:08,510
create a graph of the world,

1271
00:54:10,960 --> 00:54:14,236
what's an object, what do I create

1272
00:54:14,258 --> 00:54:16,204
a node for? Right? If I want to have one

1273
00:54:16,242 --> 00:54:17,504
note or object and there's some

1274
00:54:17,542 --> 00:54:19,104
connections between objects and things

1275
00:54:19,142 --> 00:54:21,804
like that. I know it's like some wild

1276
00:54:21,852 --> 00:54:24,288
example that comes to mind. I don't know

1277
00:54:24,294 --> 00:54:25,632
if you actually want to do that but

1278
00:54:25,686 --> 00:54:27,776
let's say you want, right? Then there's

1279
00:54:27,808 --> 00:54:30,372
also all these kind of blurry things

1280
00:54:30,426 --> 00:54:32,036
like what's an object and what's not an

1281
00:54:32,058 --> 00:54:33,844
object? What's kind of somewhere in

1282
00:54:33,882 --> 00:54:37,008
between maybe is that a node? So it's

1283
00:54:37,024 --> 00:54:38,436
kind of like what I'm trying to say,

1284
00:54:38,458 --> 00:54:40,004
that the graph structure is kind of very

1285
00:54:40,042 --> 00:54:42,504
discreet, right? The node is either

1286
00:54:42,542 --> 00:54:44,056
there or it's not there and edges there

1287
00:54:44,078 --> 00:54:46,088
is not there. But then the world is kind

1288
00:54:46,094 --> 00:54:48,456
of very fuzzy, right? So if you use

1289
00:54:48,558 --> 00:54:52,492
graphs as a model for your world then

1290
00:54:52,546 --> 00:54:54,712
there probably has to be some decision

1291
00:54:54,776 --> 00:54:56,828
to be made somewhere about these kind of

1292
00:54:56,834 --> 00:55:00,536
fuzzy concepts. They actually translate

1293
00:55:00,568 --> 00:55:02,248
in a concrete graph entity like an

1294
00:55:02,274 --> 00:55:05,472
object, an edge or whatever or not based

1295
00:55:05,526 --> 00:55:08,720
on some kind of inference procedure.

1296
00:55:09,860 --> 00:55:11,984
And I don't know if we do that or not as

1297
00:55:12,022 --> 00:55:15,024
kind of humans as intelligent agents,

1298
00:55:15,222 --> 00:55:18,240
but that's kind of some interesting

1299
00:55:18,310 --> 00:55:19,716
thing to think about. Maybe you could

1300
00:55:19,738 --> 00:55:21,684
also well, maybe one way to solve that

1301
00:55:21,722 --> 00:55:23,504
is also kind of stuff like soft edges

1302
00:55:23,552 --> 00:55:25,828
and things like that. And in some way if

1303
00:55:25,834 --> 00:55:27,988
you have attention coefficients, it's a

1304
00:55:27,994 --> 00:55:30,676
bit like that. If an edge has a weight

1305
00:55:30,708 --> 00:55:34,104
of 0.1 or something, it's almost like

1306
00:55:34,142 --> 00:55:36,136
not being there but it's still kind of

1307
00:55:36,158 --> 00:55:39,256
there. So it's a bit

1308
00:55:39,278 --> 00:55:41,720
of a soft graph architecture.

1309
00:55:42,940 --> 00:55:44,476
I guess at the edge level you can

1310
00:55:44,498 --> 00:55:46,648
implement this softness but I think it's

1311
00:55:46,664 --> 00:55:48,332
a bit harder at kind of the node level,

1312
00:55:48,386 --> 00:55:51,628
right? Like how do you kind of model a

1313
00:55:51,634 --> 00:55:53,410
node that's kind of there and not there?

1314
00:55:53,860 --> 00:55:56,880
Yeah, there's just some random thoughts

1315
00:55:57,540 --> 00:55:59,852
that's. Very interesting about the fuzzy

1316
00:55:59,916 --> 00:56:01,856
object identification and kind of

1317
00:56:01,878 --> 00:56:03,488
similarities and differences between

1318
00:56:03,574 --> 00:56:06,550
nodes and edges even though in some ways

1319
00:56:07,240 --> 00:56:10,244
they have some similarities too or

1320
00:56:10,282 --> 00:56:13,204
interoperabilities too. One other point

1321
00:56:13,242 --> 00:56:16,924
of contact was like an underlying hidden

1322
00:56:16,992 --> 00:56:21,012
space that we understand topologically

1323
00:56:21,076 --> 00:56:24,984
that projects a vector space from

1324
00:56:25,022 --> 00:56:27,496
different places so that could be a

1325
00:56:27,518 --> 00:56:30,744
vector of thermometer readings and

1326
00:56:30,782 --> 00:56:33,196
we want to have a smooth path within the

1327
00:56:33,218 --> 00:56:36,348
homeostatic range defined up to a

1328
00:56:36,354 --> 00:56:38,328
boundary point. Not saying that that's

1329
00:56:38,344 --> 00:56:40,636
the structure of the world but a

1330
00:56:40,658 --> 00:56:43,440
structure of a very heuristic and simple

1331
00:56:43,510 --> 00:56:47,340
model might be to aim for continuity

1332
00:56:47,420 --> 00:56:49,344
and have a defined hidden estate space

1333
00:56:49,382 --> 00:56:52,624
that has continuity underneath and is

1334
00:56:52,662 --> 00:56:55,604
able to emit vectors. That kind of

1335
00:56:55,642 --> 00:56:58,228
brings some of these classifier type

1336
00:56:58,314 --> 00:57:01,268
discussions that you brought up and the

1337
00:57:01,274 --> 00:57:03,252
kind of fundamental impossibility of

1338
00:57:03,306 --> 00:57:06,116
geometric classification because you are

1339
00:57:06,138 --> 00:57:08,756
going to end up with gray zones whereas

1340
00:57:08,948 --> 00:57:10,980
even if it takes a bitwise description,

1341
00:57:11,060 --> 00:57:14,424
you can separate the network. So that

1342
00:57:14,462 --> 00:57:17,544
gives an actual completeness measure and

1343
00:57:17,582 --> 00:57:20,904
that allows measures like I mean

1344
00:57:20,942 --> 00:57:23,036
amount of computational resources or in

1345
00:57:23,058 --> 00:57:24,876
a more statistically principled way like

1346
00:57:24,898 --> 00:57:27,084
the Bayesian Information criterion. So

1347
00:57:27,122 --> 00:57:29,356
how many nodes should we have? We should

1348
00:57:29,378 --> 00:57:32,456
be on some trade off front in some

1349
00:57:32,498 --> 00:57:34,048
modeling space. I don't know what to

1350
00:57:34,054 --> 00:57:36,028
tell you. It's a map, not a territory.

1351
00:57:36,204 --> 00:57:40,512
And that's more justifiable. And so even

1352
00:57:40,566 --> 00:57:42,368
lifelike organisms might want to self

1353
00:57:42,454 --> 00:57:45,972
evidence staying emitting from

1354
00:57:46,026 --> 00:57:49,236
a living state. And so that provides a

1355
00:57:49,258 --> 00:57:52,608
really simple graphical architecture

1356
00:57:52,704 --> 00:57:56,084
to cybernetic systems. And then

1357
00:57:56,122 --> 00:57:57,864
active Inference explores a lot of

1358
00:57:57,902 --> 00:58:01,560
different more specific motifs within

1359
00:58:01,630 --> 00:58:05,476
that broader blanket persistence picture

1360
00:58:05,588 --> 00:58:07,828
and the path of least action. So that's

1361
00:58:07,844 --> 00:58:11,150
what enables the physics in that space

1362
00:58:11,520 --> 00:58:13,676
and why these methods, which as far as I

1363
00:58:13,698 --> 00:58:15,848
understand are often used in quantum

1364
00:58:15,864 --> 00:58:19,100
mechanics, are being able

1365
00:58:19,170 --> 00:58:21,976
to come together with active inference

1366
00:58:22,008 --> 00:58:24,270
this way. Yeah,

1367
00:58:25,360 --> 00:58:27,676
something that comes to mind when you

1368
00:58:27,698 --> 00:58:29,584
mention this, I think there's also like

1369
00:58:29,622 --> 00:58:33,216
a recent avenue of research in

1370
00:58:33,238 --> 00:58:35,792
this area where people and it's again

1371
00:58:35,846 --> 00:58:37,924
kind of generated by the fact that you

1372
00:58:37,962 --> 00:58:40,660
don't know the graph beforehand many

1373
00:58:40,730 --> 00:58:42,324
times. And I think kind of the old

1374
00:58:42,362 --> 00:58:44,992
school approach was, well, you construct

1375
00:58:45,056 --> 00:58:47,092
it based on some rules, right? Like

1376
00:58:47,226 --> 00:58:50,116
you're going to say, I don't know, some

1377
00:58:50,138 --> 00:58:51,576
things are similar, I'm going to put an

1378
00:58:51,598 --> 00:58:53,672
edge between them and you define similar

1379
00:58:53,726 --> 00:58:56,872
in whatever way you like and so on. And

1380
00:58:57,006 --> 00:58:58,996
there was this kind of recent trend

1381
00:58:59,028 --> 00:59:00,468
where what you try to do is kind of

1382
00:59:00,494 --> 00:59:03,084
latent graph inference, or some people

1383
00:59:03,122 --> 00:59:05,036
call it manifold learning, if you think

1384
00:59:05,058 --> 00:59:07,368
of the graph as some sort of manifold.

1385
00:59:07,384 --> 00:59:08,952
But this kind of very informally

1386
00:59:09,016 --> 00:59:12,316
speaking and essentially what you would

1387
00:59:12,338 --> 00:59:14,672
do is like you would map whatever you

1388
00:59:14,726 --> 00:59:18,080
try to learn the raw observations into

1389
00:59:18,150 --> 00:59:20,736
some latent space. And that's where you

1390
00:59:20,758 --> 00:59:22,016
actually construct the graph. You

1391
00:59:22,038 --> 00:59:23,680
construct the graph in the latent space

1392
00:59:23,750 --> 00:59:27,140
rather than kind of in the raw space.

1393
00:59:27,210 --> 00:59:30,772
So that might be kind of a way to deal

1394
00:59:30,826 --> 00:59:33,460
with fuzziness as well. Right? Because

1395
00:59:33,530 --> 00:59:37,188
then I guess you might lose some of

1396
00:59:37,194 --> 00:59:38,776
these kind of very concrete one to one

1397
00:59:38,798 --> 00:59:40,376
mappings because you might learn some

1398
00:59:40,398 --> 00:59:42,024
node in the latent space that maybe

1399
00:59:42,062 --> 00:59:44,004
corresponds to three or four concepts

1400
00:59:44,132 --> 00:59:47,496
kind of mixed together. There's all

1401
00:59:47,518 --> 00:59:50,716
these kind of nice experiments with

1402
00:59:50,738 --> 00:59:53,068
neurons in deep networks, kind of

1403
00:59:53,074 --> 00:59:56,316
visualized, and they learn maybe kind of

1404
00:59:56,338 --> 00:59:59,228
a mixture of concepts. If you see what

1405
00:59:59,314 --> 01:00:01,916
actually activates that neuron is

1406
01:00:01,938 --> 01:00:04,224
actually maybe a few classes or

1407
01:00:04,342 --> 01:00:05,616
different kinds of things. It's not

1408
01:00:05,638 --> 01:00:08,848
necessarily a single thing. So it could

1409
01:00:08,854 --> 01:00:10,448
be something very similar here where you

1410
01:00:10,454 --> 01:00:14,380
have some very entangled representations

1411
01:00:14,460 --> 01:00:16,164
that are kind of distilled in this

1412
01:00:16,202 --> 01:00:19,316
latent graph. And then in a way, at

1413
01:00:19,338 --> 01:00:21,748
least in concept space, even if in the

1414
01:00:21,754 --> 01:00:23,268
latent space, that's still kind of a

1415
01:00:23,274 --> 01:00:26,384
very clear combinatorial structure

1416
01:00:26,512 --> 01:00:28,416
with respect to kind of your raw

1417
01:00:28,448 --> 01:00:29,300
observations.

1418
01:00:31,640 --> 01:00:34,308
That structure can still kind of encode

1419
01:00:34,324 --> 01:00:35,704
the fuzziness of the world to some

1420
01:00:35,742 --> 01:00:37,544
degree because you have this kind of

1421
01:00:37,582 --> 01:00:40,196
mixture of concepts that got distilled

1422
01:00:40,228 --> 01:00:42,628
in the same node or things like that.

1423
01:00:42,654 --> 01:00:44,076
Or maybe some concepts could be

1424
01:00:44,098 --> 01:00:46,536
represented by multiple nodes depending

1425
01:00:46,568 --> 01:00:48,664
on what way you see these concepts.

1426
01:00:48,712 --> 01:00:50,104
There might be all sorts of variations

1427
01:00:50,152 --> 01:00:52,496
or concept or points of view and so on.

1428
01:00:52,518 --> 01:00:55,196
So I think kind of latent graph

1429
01:00:55,228 --> 01:00:58,320
inference could be quite an interesting

1430
01:00:58,390 --> 01:01:00,290
way maybe to address some of these

1431
01:01:01,060 --> 01:01:03,408
issues we were discussing. Although I

1432
01:01:03,414 --> 01:01:06,516
think it kind of died off a bit in the

1433
01:01:06,538 --> 01:01:08,596
recent year, at least as far as I've

1434
01:01:08,618 --> 01:01:11,424
seen. There were a few slightly fewer

1435
01:01:11,472 --> 01:01:13,540
papers on the topic.

1436
01:01:15,800 --> 01:01:19,824
Well, certainly the agent's proposed

1437
01:01:19,952 --> 01:01:21,316
latent structure of the world, the

1438
01:01:21,338 --> 01:01:23,284
causal structure of the world is just

1439
01:01:23,322 --> 01:01:25,160
mapped on the territory and so it

1440
01:01:25,230 --> 01:01:26,628
enables maybe some of those core

1441
01:01:26,644 --> 01:01:28,996
screenings. Could you go to the slide

1442
01:01:29,028 --> 01:01:30,396
where there was a mapping between a

1443
01:01:30,418 --> 01:01:32,232
smooth sphere and then a regular

1444
01:01:32,296 --> 01:01:33,660
geometric shape?

1445
01:01:35,680 --> 01:01:39,404
Yeah, let's see in

1446
01:01:39,442 --> 01:01:42,716
this one. Yeah, just wanted to make one

1447
01:01:42,738 --> 01:01:45,120
point and see if you had any comments.

1448
01:01:45,940 --> 01:01:47,456
At the heart of some of the

1449
01:01:47,478 --> 01:01:50,064
relationships that you're describing and

1450
01:01:50,102 --> 01:01:52,176
where you pulled back to in terms of

1451
01:01:52,198 --> 01:01:56,532
generalization helps us understand this

1452
01:01:56,586 --> 01:01:59,172
relationship between the sphere and the

1453
01:01:59,226 --> 01:02:03,472
geometry and the implications

1454
01:02:03,536 --> 01:02:05,652
for data processing and all of the

1455
01:02:05,706 --> 01:02:09,224
computational science areas is if you

1456
01:02:09,262 --> 01:02:12,932
are preserving or learning or analyzing

1457
01:02:12,996 --> 01:02:15,896
geometry but not topology or the other

1458
01:02:15,918 --> 01:02:17,704
way around, you might get these

1459
01:02:17,742 --> 01:02:20,024
different data set. Aberrations. Like

1460
01:02:20,062 --> 01:02:21,516
you might have the topology of the

1461
01:02:21,538 --> 01:02:23,804
coffee cup, but it looks like something

1462
01:02:23,842 --> 01:02:26,476
totally different. And so what we would

1463
01:02:26,498 --> 01:02:28,236
really want to do would be understand

1464
01:02:28,338 --> 01:02:30,876
the relationship between geometry and

1465
01:02:30,898 --> 01:02:33,420
topology. Because if we could understand

1466
01:02:33,490 --> 01:02:35,216
it in principle, like you have it on the

1467
01:02:35,238 --> 01:02:38,224
left side and then in practice with the

1468
01:02:38,262 --> 01:02:41,516
data scheme on the right side or insert

1469
01:02:41,548 --> 01:02:44,130
your own left and right side there,

1470
01:02:44,440 --> 01:02:48,576
then we'd be able to do data analysis

1471
01:02:48,768 --> 01:02:53,124
in a way that respected preserved both

1472
01:02:53,162 --> 01:02:55,888
the topology and the geometry. So it's

1473
01:02:55,904 --> 01:02:58,776
like two compatible perspectives that

1474
01:02:58,798 --> 01:03:00,136
have their different strengths and

1475
01:03:00,158 --> 01:03:02,632
weaknesses and heuristics and so

1476
01:03:02,766 --> 01:03:05,848
understanding that relationship between

1477
01:03:06,014 --> 01:03:09,700
geometry and topology and

1478
01:03:09,790 --> 01:03:12,904
the implicit spaces that geometry

1479
01:03:12,952 --> 01:03:16,780
requires and so on, that has tremendous

1480
01:03:17,600 --> 01:03:20,956
use. And it just in closing, reminds me

1481
01:03:20,978 --> 01:03:23,756
of Buckminster Fuller's Synergetics,

1482
01:03:23,948 --> 01:03:27,324
which uses a close packing architecture

1483
01:03:27,372 --> 01:03:30,144
and a tetrahedron centric model of

1484
01:03:30,182 --> 01:03:33,212
coordinates to find more continuity

1485
01:03:33,276 --> 01:03:35,904
between surface area and volume and

1486
01:03:35,942 --> 01:03:38,196
between the smooth surfaces and the

1487
01:03:38,218 --> 01:03:39,876
great circles on them and like the

1488
01:03:39,898 --> 01:03:42,356
points of connectivity on shapes. So I

1489
01:03:42,378 --> 01:03:44,404
think it's an incredibly deep area and

1490
01:03:44,442 --> 01:03:47,636
really has fundamental impact in active

1491
01:03:47,668 --> 01:03:49,704
inference. Helps us think about our

1492
01:03:49,742 --> 01:03:51,896
models in this way, kind of like the

1493
01:03:51,918 --> 01:03:56,184
inflated balloon with the fuzziness and

1494
01:03:56,302 --> 01:04:00,040
the architecture and the finiteness.

1495
01:04:00,960 --> 01:04:02,696
It really brings a lot to active

1496
01:04:02,728 --> 01:04:04,572
inference. And so I appreciate you

1497
01:04:04,626 --> 01:04:06,796
sharing the work with us today and

1498
01:04:06,898 --> 01:04:09,150
continuing to work in this way.

1499
01:04:10,000 --> 01:04:13,840
Thanks a lot. Any last thoughts?

1500
01:04:14,420 --> 01:04:17,440
Yeah, what you mentioned,

1501
01:04:17,510 --> 01:04:20,204
I think it's been all over my thesis

1502
01:04:20,252 --> 01:04:22,860
this. Kind of tension between topology

1503
01:04:23,020 --> 01:04:26,016
and geometry. And maybe what I want to

1504
01:04:26,038 --> 01:04:28,772
emphasize is that I'm not saying kind of

1505
01:04:28,826 --> 01:04:30,724
the previous perspective of looking

1506
01:04:30,762 --> 01:04:32,244
maybe more geometrically at things was

1507
01:04:32,282 --> 01:04:34,544
wrong in any way. And on the contrary,

1508
01:04:34,592 --> 01:04:36,468
actually, there's lots of interesting

1509
01:04:36,554 --> 01:04:38,108
places where these things intersect.

1510
01:04:38,144 --> 01:04:40,696
Even in kind of this chief paper I

1511
01:04:40,718 --> 01:04:42,216
briefly went through, like, if you

1512
01:04:42,238 --> 01:04:44,696
actually read the paper, there's a lot

1513
01:04:44,718 --> 01:04:46,520
of beautiful intersections. Actually,

1514
01:04:46,590 --> 01:04:49,896
my main collaborator, Francesco, he's a

1515
01:04:49,918 --> 01:04:53,128
differential geometry, so he actually

1516
01:04:53,214 --> 01:04:55,304
had lots of kind of inputs from that

1517
01:04:55,342 --> 01:04:58,744
side. And indeed, I think we should

1518
01:04:58,782 --> 01:05:01,496
try to use all these kind of layers of

1519
01:05:01,518 --> 01:05:04,688
structure are in the best way possible

1520
01:05:04,854 --> 01:05:07,888
for all our methods. Awesome.

1521
01:05:07,974 --> 01:05:10,336
All right, thank you. Till next time.

1522
01:05:10,358 --> 01:05:11,970
Thanks a lot. Thanks for having me.



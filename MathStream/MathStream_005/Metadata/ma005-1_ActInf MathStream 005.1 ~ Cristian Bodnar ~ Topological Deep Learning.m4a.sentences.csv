start	end	sentNum	speaker	confidence	text
5460	6770	2	B	0.67748	Hello and welcome.
7140	9696	3	B	0.85519	It's July 17, 2023.
9798	19372	4	B	0.9999	We're here in Active Inference math stream number 5.1 with Chris Bodnar on topological deep learning graphs, complexes, and sheaves.
19516	21740	5	B	0.99996	So thank you for joining, Chris.
21820	24720	6	B	0.99999	Looking forward to your presentation and discussion.
25780	28312	7	A	0.55574	Yeah, thanks so for having me.
28366	35832	8	A	0.99938	So, yeah, as I was just saying, this is my PhD thesis, which I finished a couple of months ago.
35886	37480	9	A	0.99992	It's also kind of public online.
37550	45020	10	A	0.99985	So if you want to go into the details, just kind of look this up on the Internet and you should be able to find it easily.
45360	47644	11	A	0.99467	Obviously, there's lots of stuff in there.
47682	58370	12	A	0.99813	So I kind of try to give an overview today and maybe also go in a little bit more detail in certain aspects since there's not a lot of time to go through.
60180	67540	13	A	0.81369	Yeah, now I'm Microsoft Research, so this is basically all work that I did in the past and when I was at University of Cambridge.
68040	70950	14	A	0.93	All right, so let's get started.
72600	76020	15	A	0.91827	Right, so let's start very easily.
76760	94972	16	A	1	Now, I'm actually not sure exactly what's kind of the background of the people who are watching, but in machine learning there's all these kind of subfield that emerged a few years ago which is called geometric deep learning, which is essentially looking at how to apply these kind of deep learning.
95026	105970	17	A	0.99984	Neural network architectures on data, living on all sorts of structures or geometries or spaces if you want.
106580	109740	18	A	1	And this has a lot of application, especially in the life sciences.
109820	116924	19	A	1	And there's kind of been a lot of instances of this in kind of very famous publications, some of which you see here.
116982	125300	20	A	0.99988	But just to give some examples, for instance, if you have proteins or molecules or things like that, they usually represented as graphs.
125720	131530	21	A	1	And you kind of have some data living on these graphs, like kind of the properties of certain atoms and so on.
132380	150380	22	A	0.96558	So these kind of things and so far, these kind of spaces or these kind of problems, learning problems, if you want, they have been approached mostly kind of with a geometrical mindset, as the kind of name of this subfield also mentions.
151200	162960	23	A	0.99989	But something that I would argue is that geometry is not everything that you need and there's kind of other non geometrical aspects when you are in such a setting.
164180	174608	24	A	1	And this is kind of quite obvious once you realize that the spaces that kind of show up in the field and in many applications, they are very heterogeneous.
174704	178340	25	A	0.99587	So, as I mentioned, for instance, you could have graphs that could represent anything.
178410	182360	26	A	1	In this case, it's the caffeine molecule that you see here on the left.
182510	187832	27	A	1	And you want to have some models that predict certain properties of this molecule and so on.
187966	189572	28	A	0.99998	But for instance, you can have grids.
189636	193256	29	A	1	And we see grids all the time and data living on grids.
193288	198140	30	A	0.99	And I'm referring to images, videos, they are all kind of pixels living on a grid.
198720	200892	31	A	1	And then you can have more sophisticated things.
200946	202960	32	A	0.99999	You could have some meshes, for instance.
204260	208716	33	A	0.85762	They're all over in computer graphics and then you could have some sort of manifold.
208748	217030	34	A	0.9642	So for instance, if you're doing maybe weather modeling or something, we live on a sphere, topologically speaking.
217720	222310	35	A	0.99996	So you might want to model your data as living on a sphere and so on.
224360	244110	36	A	0.99999	But nonetheless, even if these spaces are kind of geometrically, kind of heterogeneous, and some of them don't even have a geometrical structure in kind of a strict mathematical sense, they all have what's called a topological structure, which is kind of like kind of a weaker kind of structure, but it's kind of more general.
244560	249212	37	A	0.97	And I'm going to talk a bit in a few seconds about what that means by in general.
249266	261024	38	A	0.95469	When you do kind of mathematical physics, you kind of have a ladder of structures where you kind of keep building on top and the more structure you have, the more sophisticated things you can do and so on.
261062	275510	39	A	0.58	And kind of at the base of this diagram just have sets, just kind of a collection of elements with no kind of extra structure and then you kind of keep going up in this ladder and you add stuff on top of sets and so on.
275880	291160	40	A	1	And as I was saying, kind of most of the work is kind of focused on maybe the top levels of this hierarchy, but it's kind of topological level, which are kind of the weakest kind of level you can add on top of sets has kind of been neglected to a large extent.
291580	305040	41	A	1	And part of what I've been doing in my PhD thesis was essentially looking at these kind of learning problems on these kind of spaces from a more topological perspective and kind of try to fill in these blanks.
305780	315424	42	A	0.99895	So this is kind of the overview, okay, so if we are to adopt this topological perspective, what would that actually mean or how would that look like?
315622	321012	43	A	1	I guess it could look in different ways, but in what I've done, in my cases, it looks kind of like this.
321146	326340	44	A	0.99965	So we have horizontally, we kind of have a space that could be anything.
326410	328196	45	A	0.68365	This is just kind of an abstract space.
328378	336712	46	A	0.58565	It could be a grid or things like we've seen in the previous example and kind of vertically attached to the regions of this space.
336766	337816	47	A	0.99999	We have data.
337998	346120	48	A	0.99786	So data is kind of this vertical component and you kind of see these flags kind of being kind of anchored in these regions.
346200	351256	49	A	0.99994	So that kind of signifies you have some data that's kind of associated with that region.
351448	357890	50	A	0.99995	So that's kind of the high level perspective and I'm going to make that a bit more concrete a bit later.
358340	361840	51	A	1	And there's kind of two essential things about this picture.
362580	365312	52	A	0.87	A first thing is locality.
365456	376944	53	A	0.60803	So the data is attached to some regions of this space, this topological space, and in that sense it's local, so it's kind of associated with the region.
376992	391336	54	A	1	And maybe to give a concrete example, if you kind of have a temperature sensor somewhere in space, right, you could think of whatever that sensor is measuring is kind of a property of kind of the immediate surrounding around that sensor.
391368	395470	55	A	0.99975	So it's kind of describing some property of a region in space.
396160	398110	56	A	0.9982	So that'll be kind of a concrete example.
398800	406620	57	A	1	And another kind of axiom that we're starting with is that the space has structure.
407040	420992	58	A	0.99974	So the space has kind of, it's kind of made up of various regions and these regions intersect in various ways and that implicitly also makes our data structured because the data is attached to these regions.
421136	424390	59	A	0.99987	So there is kind of some structure in the data.
425800	430724	60	A	1	All right, so that's kind of the picture and actually many of these things relate to category theory.
430772	444540	61	A	0.99834	I'm not going to go in depth into this because it's kind of sophisticated and I'm not an expert myself in category theory, but kind of the high level here is that category theory is kind of a nice way.
444610	457692	62	A	1	To translate between different structure in mathematics and kind of discuss about properties of certain kinds of objects and translate that to some different kinds of objects and find all these kind of relations and connections.
457756	474096	63	A	1	And a concrete example is, for instance, if you want to study some manifolds, some surfaces, you could associate some groups to these surfaces and then any sorts of relations between these kind of surfaces, also translating some relations about these groups or some other algebraic structures.
474208	480250	64	A	0.99256	So you could study these manifolds by doing algebra instead of doing geometry or topology or something else.
481100	488200	65	A	0.97679	So also in this case, it's kind of manifesting the fact that we kind of translate these from spaces to data.
488270	497192	66	A	0.99964	So kind of because we associate the regions in a space, certain kinds of data, this is kind of how this translation manifests in what I've just described in the previous picture.
497256	505810	67	A	0.76391	So you could think of this as some sort of translation or mapping from spaces and regions in that space to kind of data attached to that space.
506580	516500	68	A	0.99993	But yeah, I'm not going to go in a lot of detail into this, but just kind of to keep in the back of your mind that there's this stuff lurking in the background.
517160	523910	69	A	1	All right, so I promise this is the only math definition I'm giving in this talk and then I'll stop.
524600	540152	70	A	0.99835	But just because I'm mentioning topological spaces quite often, I just wanted to kind of give this axiomatic definition, which might sound sophisticated, but I have a picture at the end and hopefully it'll be clear what is it.
540286	541640	71	A	0.99145	So it's just a set.
541710	544444	72	A	1	As I was saying, we start with sets and we put stuff on top, right?
544482	553448	73	A	0.99992	So you start with a set and then you also have a collection of subsets of this set called the open sets that need to satisfy certain axioms.
553464	557890	74	A	0.94247	So you could think of these open sets as kind of regions of this space, very informally speaking.
558900	568660	75	A	0.99937	So something that kind of has to be satisfied is that, well, the empty set and the set itself need to be open set.
568730	579380	76	A	0.99976	So in some sense, you could think of this as saying the set itself is a region of that space very informally, which it's kind of, let's say, obvious.
579530	586484	77	A	1	And then there's some kind of constraints about intersecting and taking units of this region.
586532	592140	78	A	0.99989	So if we take the intersection of two regions, we should get another one of these regions.
592800	597580	79	A	1	And if we take a union of these regions, we should get another region.
598080	603916	80	A	1	And there are some constraints again, like, okay, how big these intersections should be.
604018	607132	81	A	0.99951	There should be finite intersections, but you could have infinite unions.
607196	610796	82	A	0.99999	But that's a technicality and we can just skip anyways.
610828	615984	83	A	0.71126	But to see a picture so on the left, you just see the set x itself.
616102	622612	84	A	1	And here I've put like a potential neighborhood structure or kind of like open set structure on this space.
622666	626068	85	A	0.99992	So we have an open set U, another open set V.
626154	628804	86	A	0.65804	By this axiom, their intersection should also be an open set.
628842	633976	87	A	0.99998	So you see this intersection in the middle being another open set, and then the set itself is another open set.
633998	637130	88	A	0.99998	So it's just kind of splitting stuff into regions, kind of.
637580	639290	89	A	0.59238	You could think of it like that.
640160	646604	90	A	0.99	All right, so this is a topological space, and now let's add data, right?
646642	653500	91	A	0.99993	So we mentioned that we have data and then we have a space, and then we add data on top.
653650	656480	92	A	0.99979	So so far we've seen how a topological space looks like.
656550	660176	93	A	0.66	Now let's add this kind of vertical stuff, these flags that you saw before.
660358	662960	94	A	0.99999	We just put some data on top of these regions.
663460	684644	95	A	1	And if we put data on all the regions of the space, on all the open sets, we get these structures that in categories here, like in algebraic topology, also geometry, they are called presheaves, which sounds very fancy, but all is just kind of a definition of what I was already describing.
684692	687720	96	A	0.74883	Essentially, you have some data for each region.
688780	696668	97	A	0.99943	For instance, for region U, you have this f of U, which is kind of the data attached to region U.
696754	703680	98	A	0.99988	So you could think of f of U, some set with describing the data that lives there in that region.
704180	705856	99	A	0.99997	But there's also an extra thing.
705958	714076	100	A	0.99192	You have some sort of maps going between these kind of pieces of data, and these are called restriction maps.
714188	715312	101	A	0.95	And why is that?
715446	720164	102	A	0.99958	They provide you a way to kind of zoom in if you want, right?
720202	726676	103	A	0.99464	Like you have the data attached to the whole set x, and then you could think, okay, how do I take this data?
726778	731128	104	A	0.79266	How do I go from this data to data on a smaller region on x?
731214	736388	105	A	0.71367	So it's kind of a way to zoom in on that data, essentially.
736484	739784	106	A	1	And I'm going to show some example in a second.
739902	742436	107	A	0.74409	So these are called pre sheaves.
742468	749276	108	A	1	And just to see an example, our space here is kind of one of the simplest kind of space you could think of.
749298	751244	109	A	0.47769	It's just 1D or horizontal line, right?
751282	752450	110	A	0.98885	Just the real line.
752900	761376	111	A	1	And then you have some regions which are just given by open intervals on the real line.
761478	766816	112	A	1	And then some pieces of data could be functions like continuous functions on those regions.
766928	771376	113	A	0.9994	So here's some sample data on this first interval.
771488	773696	114	A	0.89272	Here's another function on the second interval.
773808	775728	115	A	0.96422	Here's some data on this third interval.
775824	781396	116	A	1	And actually in this case, it happens that all these functions agree on the overlap.
781428	790760	117	A	0.99995	So where these regions overlap, they take the same values and we can actually glue them together in a single function over the entire region.
791120	797928	118	A	0.99755	So this is just an example of a preshief and it's called the preshift of continuous functions.
798104	805920	119	A	0.99938	So our data in this case is continuous functions and the space is just the realign and we put these functions on top of the realign.
806260	814272	120	A	0.98734	But it turns out because of this kind of special property, that we can kind of glue data and we uniquely get some other piece of data, right?
814406	815884	121	A	1	We can take these three pieces.
815932	818244	122	A	0.99535	It's just exactly like a puzzle, right?
818282	826470	123	A	0.99997	We put these things together and we get a fourth thing, which is kind of a single function where we just overlap these functions, right?
826940	836024	124	A	0.99	And these presheaves that satisfy these kind of properties where you can kind of glue them to get a unique piece of data.
836142	843400	125	A	0.98762	They are called sheaves and basically the preshave of continuous functions is actually a sheep.
844160	849070	126	A	0.99173	So this is kind of a way to formalize data attached to these things.
849600	852190	127	A	0.99818	It's going to get less technical in a second.
852960	855128	128	A	0.98906	So just to give more examples.
855224	858960	129	A	0.9584	So for instance, in this way we could describe data on a sphere.
859380	862364	130	A	0.99	And let's say if this data is just some vector field on a sphere.
862412	866576	131	A	0.995	So let's say if this is Earth, right, this could be some wind vector field, right?
866598	874790	132	A	0.99755	Like if we do weather modeling or something, you just have a vector field describing the wind on the surface of Earth, right?
875160	892020	133	A	0.98	And you might want to do some machine learning on top of this, where this kind of vector field has a sheet structure, and you could think of it as a sheath, because if you have some vector field on the red region, a vector field on the yellow region, I can kind of glue them together uniquely if they overlap.
892100	896140	134	A	0.74504	If they agree on the overlap and we get the vector field on this bigger region.
896960	915244	135	A	0.99997	But something that's quite nice is that even if we have a very different kind of space namely a graph which is very different from a sphere in all points of view, we can still apply the exact same kind of axioms and terminology and kind of definitions and we can have a sheath of our graph.
915292	931130	136	A	0.99955	So in this way we could have, for instance, some features associated to the nodes of the graph, some features associated with the edges of the graph, and there's another node which has its own features and this is actually the exact same thing we have in graph machine learning.
931500	932744	137	A	0.9994	So this is quite nice.
932782	950044	138	A	0.99972	What this kind of topological perspective allows us to do is we kind of have a unified way of thinking, if you want, about very kind of heterogeneous spaces and we can model on all of them data attached to them by using this kind of sheet terminology and other ways as well.
950082	952990	139	A	0.99749	But I'm not going into that in this talk.
953300	958320	140	A	0.91	All right, so this is kind of an overview of what I've been doing in my thesis.
958980	970404	141	A	0.87	And just to kind of dive a bit deeper into this, I just wanted to go into one paper that we did at Europe last year.
970442	970596	142	A	0.61845	Yeah.
970618	975648	143	A	0.99408	So this was last year on what's called shift diffusion.
975744	985064	144	A	0.98673	So essentially, how can we use what I've just described to do some useful stuff when doing machine learning on graphs, all right?
985102	992540	145	A	1	And this was a collaboration with Francesco de Giovanni ben Chamberlain pietro Leo, my advisor and Michael Bronstein.
993520	993932	146	A	0.56915	Okay?
993986	1000030	147	A	0.99886	So before I dive into this, I just want to give some background in case people are not familiar with this.
1000480	1013740	148	A	0.95826	So the kind of favorite architecture of people doing machine learning on graphs these days are these things called graph neural networks, which are actually very simple kind of models.
1013900	1018500	149	A	0.99625	So in the setting you have some features.
1019000	1021744	150	A	0.99839	So each node in your graph will have some features.
1021792	1025936	151	A	0.81181	This is what this h vector here denotes.
1025968	1033930	152	A	0.99926	So it's the vector h associated with node A at layer or time t, whatever.
1034380	1047420	153	A	0.59337	So you have some features for each of these nodes and what you're doing each of if you're at a certain node, you want to kind of compute a new representation or new features for this node at the next layer.
1048400	1054192	154	A	0.61579	So essentially are learning representations and what the graph neural networks are doing.
1054326	1061600	155	A	0.99992	This node will receive a message from all the other nodes that are neighbors with this node.
1061940	1075140	156	A	1	And this message can also be passed, can use some neural networks in there, but essentially it's some processing of these features of the neighbors and these are aggregated into this message.
1075210	1088580	157	A	0.78476	So here in Green and then this is passed through some update function that combines the message from the neighbors with the old representation of this node and it gives you a new representation at the next layer.
1088740	1090296	158	A	1	And this happens for all the nodes, right?
1090318	1096860	159	A	0.92487	So then you get some new representations for this node and this is one layer and then you kind of keep repeating this for as many layers as you like.
1097010	1100376	160	A	0.99938	So this is kind of how you do deep learning on graphs.
1100568	1112076	161	A	0.99989	It's kind of a very simple recipe and most models actually vary in the way they kind of compute these messages and in the way this update function is designed.
1112268	1116176	162	A	0.99998	But that's kind of the parameters most of these models use.
1116358	1121430	163	A	0.92849	Otherwise they all kind of respect this framework and work in this kind of particular way.
1122520	1129536	164	A	0.74	And to give you maybe an example for why you would want to do this, you might want to do node classification.
1129648	1132472	165	A	0.99997	This is kind of a classic problem in graph machine learning.
1132526	1136120	166	A	0.99998	There are others, but I'm just going to talk about this because it's easier.
1136860	1141912	167	A	0.99967	So you have a graph and this graph has nodes that have different labels.
1141976	1145980	168	A	0.99998	Here there's just two kinds of labels, this orange and blue.
1147600	1151384	169	A	1	And you have some edges between these nodes.
1151432	1164560	170	A	1	And what you want is you want to do this kind of message passing that I was describing to compute some representations for these nodes where you can easily classify the blue and orange nodes.
1165080	1179496	171	A	0.99	Now something that's quite interesting is that for many kind of graph neural networks, depending on the properties of these graphs and how these kind of different nodes are connected, their performance might vary quite a lot.
1179678	1183876	172	A	0.99867	So in particular they're affected by this property called heterophili.
1184068	1189144	173	A	0.99913	So this is kind of a measure of how much opposites attractive you want, right?
1189182	1192360	174	A	0.99719	So it has a very simple formula.
1192780	1198524	175	A	1	Basically you take the number of edges between orange and blue nodes and you divide by the total number of edges, right?
1198642	1209836	176	A	0.93276	So basically you kind of check how many connections we have in this graph between things that are quite opposite to each other versus connections that are between similar kind of nodes.
1210028	1216240	177	A	0.99981	So if you have a lot of these kind of heterogeneous connections, then you have very high heterophili.
1216400	1219536	178	A	0.99	And it turns out that many graphene networks actually struggle in that setting.
1219568	1223460	179	A	0.9999	It's very hard to classify things in that setting.
1223800	1237384	180	A	0.97	And intuitively you could kind of also figure out why because you could easily apply some kind of reasoning where oh, this node looks a lot like this other node is connected to.
1237422	1241480	181	A	0.99988	So they kind of must be in the same community if you want or in the same label.
1241560	1251336	182	A	0.99535	But it's much harder to do that when all things are kind of different from each other and these communities kind of don't form right, even visually.
1251448	1260032	183	A	0.99999	If you see a graph and it has some nicely clustered communities, it's quite easy to draw a line between those and say, oh, this is a community, is another community.
1260166	1263692	184	A	0.99999	But if things are very mixed, then it's quite challenging.
1263756	1270950	185	A	1	And it turns out it's also challenging for these models, not just for kind of our intuition when we would have to do this.
1271880	1282090	186	A	0.99268	So this is kind of some problem where this topological perspective I was mentioning will be used to do some useful stuff.
1283580	1283992	187	A	0.74375	Okay?
1284046	1287476	188	A	0.99952	So coming back to Sheaves on graphs.
1287668	1294910	189	A	0.99	And at this point I think you can largely forget what I mentioned in the introduction or if there's something you misunderstood there.
1296000	1300300	190	A	0.99995	We kind of start from zero bit here, so there's no problem.
1300370	1307660	191	A	0.90572	So on the left you just have a graph which is kind of the incidence structure of a graph.
1307740	1312112	192	A	1	I just drawn here the simplest possible graph that has two nodes v and U.
1312246	1314112	193	A	1	And then there's an edge between them.
1314166	1316108	194	A	0.99878	So this is just a graph with one edge.
1316204	1317812	195	A	0.9993	That's all that's going on here.
1317946	1323236	196	A	1	And I've just represented it by kind of in this kind of incidence structure kind of way, right?
1323338	1330272	197	A	0.95847	Node v is incident to node e and node u, sorry, edge e and node U is incident to edge e.
1330346	1337076	198	A	0.99806	So this is just an incident structure and what this kind of triangle symbol is showing is just this incident structure.
1337108	1340568	199	A	0.65404	It's just a way to symbolize this incidence relation if you want.
1340734	1342716	200	A	0.96841	Okay, so this is just a graph, right?
1342818	1348584	201	A	1	And a way we can kind of think of field zone graphs is just mapping this graph structure.
1348632	1351528	202	A	0.9999	So this is kind of this categorical theory translation.
1351624	1356720	203	A	0.65098	We translate this graph into something else which looks very similar.
1356790	1361024	204	A	1	The structure is kind of the same, it's just kind of the meaning of these things change.
1361142	1366160	205	A	0.99955	So for each node v we have here, this will be a vector space.
1366310	1368340	206	A	0.99977	So F of v is a vector space.
1368490	1372564	207	A	0.99983	For each node u we have F of U which is another vector space.
1372762	1377190	208	A	0.76067	For each edge e we have this Fe which is another vector space.
1377640	1384900	209	A	0.99913	So all nodes have their own vector spaces and the features associated to those nodes leaving those vector spaces.
1384980	1389992	210	A	0.99997	So basically for each node we have a vector space of features that's all that's going on so far.
1390126	1398744	211	A	1	And also these arrows that these incidence relations also translate into something and they translate into the obvious thing linear maps.
1398792	1404272	212	A	0.99891	So if these are vector spaces, then these things should be linear maps or just some matrices essentially, right?
1404326	1427220	213	A	0.99973	So for each arrow you see here we have a matrix and something that I'll argue and show in a few slides is that basically message passing is very similar on graphs, is very similar with group actions in group theory.
1427800	1430008	214	A	0.99969	So let me explain exactly why.
1430094	1436708	215	A	0.99986	So we kind of can think of what we have on the left these arrows from the incidence relation.
1436804	1440552	216	A	0.5903	We could think of these arrows as kind of some buttons we can press.
1440686	1442476	217	A	0.54672	So what do I mean by that?
1442578	1462400	218	A	0.99945	So if we have this node v on the left, right, and this e, now if we have some features, some feature living in FOV, right, we could just kind of press this arrow button here and then if we multiply this matrix by this feature, we will get an edge feature.
1462740	1472816	219	A	0.9999	So it's kind of like if you go along this arrow, this matrix will multiply this feature and this vertex feature and it will give you an edge feature.
1473008	1481328	220	A	0.55207	So you could think of these arrows as kind of giving you some sort of actions that you can play with to move features from vertex to edge and edge to vertex.
1481424	1483544	221	A	0.99932	So in this case it's kind of a left action, right?
1483582	1484472	222	A	0.99995	So this is what I'm saying.
1484526	1490872	223	A	0.99998	I'm taking this arrow, which is this one here, and I act on some features of node v.
1490926	1503528	224	A	0.99983	So this HOV that lives here and how I do that is I just take this matrix, this matrix associated with this arrow, and I multiply this vector HOV.
1503624	1506620	225	A	0.99987	So just matrix times vector, that's all.
1506690	1508236	226	A	1	And then we get an edge feature.
1508348	1511184	227	A	0.99961	So this is just kind of a way to move from here to here.
1511222	1513616	228	A	0.99988	So this already kind of looks a bit like message passing, right?
1513638	1523412	229	A	0.99028	We're kind of passing a message from this vertex to this edge, but now we also need to pass a message from this edge to these other vertex U.
1523466	1529592	230	A	0.99992	So we need to get from V to U and we did that by passing through e.
1529726	1533236	231	A	0.99982	So by doing that, we could do that by kind of going in reverse.
1533268	1541012	232	A	0.61699	So we could have a right action where instead of applying this matrix, we applied adjoint matrix.
1541076	1543016	233	A	0.76138	So that's just a transpose matrix.
1543128	1551870	234	A	0.99584	So if we want to go from here to here, instead of applying this matrix, we apply it transpose because we want to go the other way around.
1553620	1557392	235	A	0.99999	If we compose these things, then we can move features from V to U.
1557526	1568224	236	A	0.99962	So this is just a way that kind of we can apply these actions to do message passing and these are called shift actions or preshift actions.
1568352	1576260	237	A	0.99	And I'm going to now show what's kind of the relation between this and what we have in group theory.
1577100	1586248	238	A	0.99062	So one way to represent a group is by kind of having some sort of graph, like here on the left.
1586414	1593244	239	A	0.99947	So we kind of have some star object, it's just kind of a dummy thing there.
1593442	1596044	240	A	0.99996	But all the group structure is in these arrows, right?
1596082	1597816	241	A	0.99961	So for each group element.
1597928	1601500	242	A	0.98408	So let's say this g is a 90 degree rotation, for instance.
1602160	1606530	243	A	0.7195	Let's say we do have a group of rotations, just to have some concrete example.
1608820	1611868	244	A	0.99974	So this arrow could correspond to a 90 degree rotation.
1611964	1616236	245	A	0.64887	We have another arrow that does the opposite, minus kind of 90 degree rotation.
1616268	1618496	246	A	0.80048	That's the inverse of that transformation.
1618688	1620756	247	A	0.99994	So this is kind of the structure of this group.
1620858	1625750	248	A	1	And if we have, we also do this similar kind of translation as we've just seen.
1626360	1629590	249	A	0.99924	So basically we define a preshift on this group.
1630060	1632504	250	A	0.99998	We map this star to a vector space.
1632542	1636120	251	A	0.99849	So the star kind of replaces the vertex we had before.
1636190	1640536	252	A	1	Now we just have a single vertex and it's just these arrows we have.
1640718	1645356	253	A	0.99985	So the star is mapped, this vector space that you show here in blue, right?
1645538	1657452	254	A	1	And now, if we actually do group actions, which are kind of a very well established concept in group theory, well, for instance, if you want to act on this vertex sorry, not vertex on this vector.
1657516	1658656	255	A	0.99	V right here.
1658758	1665120	256	A	0.99926	You have a vector in this vector space, and you want to act on it by this group transformation g.
1665190	1667184	257	A	0.99998	So, essentially, you want to press this arrow.
1667232	1669350	258	A	0.99967	So you apply some action on it.
1670040	1686904	259	A	0.99998	Then what you do is well, because of this translation, this g has been mapped to some matrix, which is the rotation matrix, the corresponding rotation matrix, and you apply this rotation matrix on v and you get like a 90 degree rotation here.
1687102	1688792	260	A	0.99726	So this is what's going on.
1688846	1691900	261	A	0.9992	This kind of vertical vector is showing the rotated vector.
1693040	1697180	262	A	0.99975	So this is completely analogous with what we've seen on the previous slide.
1699040	1702792	263	A	0.99183	This is how kind of sheaves connect these kind of actions.
1702936	1711196	264	A	0.9998	So essentially what you could think of as message passing is same as group actions in group theory, but you just replace this group with a graph.
1711308	1713344	265	A	0.99995	So it's kind of analogous to that, right?
1713382	1719270	266	A	0.99989	So it's just kind of a different kind of translations where we replace the object on the left.
1719720	1722916	267	A	1	Now it looks like this is a graph, this is a group, right?
1723018	1725750	268	A	0.60518	But kind of the rest stays exactly the same.
1726840	1741740	269	A	1	And this kind of gives us a way to formalize in a way by looking in this topological perspective to connect all these kind of symmetries and things like that that have been explored quite a lot in machine learning to message passing on graphs and to see one way in which they are related.
1742720	1750190	270	A	0.91501	Okay, so now you might say, okay, this was all very sophisticated and nice, but you know, what, is this going anywhere, basically?
1750880	1754688	271	A	1	And I'm just going to show you kind of a very short example.
1754854	1757760	272	A	0.33797	There's more, but the time is limited.
1758180	1766508	273	A	1	And something we showed is that, as I was saying in the beginning, many graph neural networks kind of struggle in these heterophilic graphs.
1766604	1787688	274	A	1	And what we showed is that no matter how heterophilic or kind of weird your graph is, you can always kind of find some shift structure, essentially kind of a message passing neural network, that if you use sufficient layers, it will be able to disentangle the classes of the nodes, right?
1787774	1796748	275	A	0.99983	So just to show you in this picture what you have here on the very far left, the colors of the nodes, they show the class.
1796834	1799644	276	A	0.99402	So there's three colors here.
1799682	1801150	277	A	0.99716	So three classes, right?
1801520	1807404	278	A	1	And this is kind of the graph in the beginning and the position of the nodes in this box denotes the features.
1807452	1810320	279	A	0.99991	So that's a way to kind of just to visualize the features.
1812660	1817100	280	A	1	The 2D position is actually the 2D feature vector of each node.
1817260	1820884	281	A	1	And you can see in the beginning, everything is kind of super messy and entangled, right?
1820922	1830280	282	A	0.58302	Like if you want to classify these nodes, it's kind of very hard because their representations, their initial representations are very messy and kind of intertwined.
1830780	1844516	283	A	1	But as we stack more layers of a particular kind of sheath or a message passing model, you see how progressively these classes get kind of more disentangled and more disentangled at each new layer.
1844708	1850220	284	A	0.76929	So these representations kind of collapse and they form these kind of clusterings, right?
1850290	1858640	285	A	1	And then when you get with something like at the end, you can kind of see these three communities very clearly and it's extremely easy to separate.
1859140	1871908	286	A	1	And kind of the essence behind these results was we showed for different kinds of problems what sorts of chief or message passing models you need by using this theory to solve kind of problems.
1871994	1885432	287	A	1	And this is quite important because it kind of shows you some important bits and pieces in the architecture that you might want to kind of change or use in order to solve certain kinds of problems.
1885566	1888196	288	A	1	And we also had some sort of impossibility resolved.
1888228	1894510	289	A	0.99995	So if you use a graph neural network of some kind, you can't solve this problem or you'll struggle to solve this problem.
1894960	1900430	290	A	0.86	And we also saw, okay, if you use some more general ones, then you might have a chance.
1901440	1907890	291	A	0.75105	So this is kind of some high level view behind this theoretical stuff.
1908500	1919072	292	A	0.52	And what we actually do in practice is to essentially learn these message passing functions or to learn the sheath or these matrices.
1919216	1934280	293	A	0.99905	So in practice, like when someone gives you a node classification task, it's very hard to know beforehand what exactly is the right sheep or the right message passing model to solve that task.
1934940	1937976	294	A	1	And what we do essentially, we learn that from data.
1938078	1941144	295	A	0.99999	So we learn these matrices that do the message passing.
1941192	1948350	296	A	0.99996	We learn them from data by using some neural networks which are shown here in red.
1949040	1955116	297	A	1	And then you learn how to kind of transfer features between these vector spaces and kind of move them around.
1955218	1965460	298	A	0.99886	So this is just showing how these vectors, which are features of these nodes and edges, how they're kind of moved around by kind of going through via these matrices, just some matrix multiplications.
1967400	1970916	299	A	0.9629	Okay, so that's kind of the high level view behind this model.
1971018	1983348	300	A	0.92	And we evaluated this on some kind of real world heterophilic data sets where you have to classify nodes based on kind of various communities or different kinds of labels.
1983444	1989876	301	A	1	And these data sets going from right to left, they are getting more heterophilic.
1989908	1993224	302	A	0.99876	So in some sense more challenging for classic architectures.
1993352	2001020	303	A	1	And our models, which are kind of inspired by all this stuff that I mentioned, they score quite highly in these benchmarks.
2001520	2010016	304	A	1	And at the same time, they also revealed some or justified some various choices that other models in this space have done.
2010118	2014252	305	A	1	But maybe they were not so well justified, or maybe they had different kind of motivations.
2014316	2024420	306	A	0.68421	We also managed to kind of show why various things they were already doing, why they made sense from the point of view of this kind of theory.
2025560	2029450	307	A	0.7	All right, well, that's all I had.
2030140	2041470	308	A	0.90517	Yeah, thanks for listening and yeah, happy to chat more about this and also have lots of backup slides in case, depending on how far we venture off with this question.
2044320	2044828	309	A	0.54215	Cool.
2044914	2046780	310	B	0.99574	Well, awesome work.
2046850	2048568	311	B	0.99995	Thank you for the presentation.
2048744	2055648	312	B	0.99937	For people who are in the live chat, they can write some questions, but there's many things I think we could talk about.
2055814	2076096	313	B	0.94526	So I want to start with reading a quote from an abstract of the paper by von der Laur, Kudal and DeVries just to kind of ground this in the active inference context and really justify why the message passing approaches that you are describing are helping in the active inference modeling.
2076288	2077216	314	B	0.74196	It's two papers.
2077248	2096856	315	B	0.92132	It's called Realizing Synthetic Active Inference Agents and they wrote, with a full message passing account of synthetic active inference agents, it becomes possible to derive and reuse message updates across models and move closer to industrial applications of synthetic active inference framework.
2097048	2108690	316	B	0.99959	So how does knowing the message passing structure help reuse a model across different settings or facilitate the legibility of the model?
2110100	2110752	317	A	0.9997	Right?
2110886	2124036	318	A	0.99985	So first of all, I'm not super familiar with the kind of active inference literature, so you'll have to help me there a bit to anchor maybe the discussions a bit more into that.
2124218	2140520	319	A	0.99992	But I think if I understand correctly, the kind of question you're getting at is basically how can kind of message passing help us generalize in kind of various kinds of settings or maybe from one graph to another and things like that?
2140590	2147470	320	A	0.99	And this is a kind of active area of research, how exactly digitalization is happening.
2147840	2159616	321	A	0.99998	But something you could notice, for instance, something that for instance, was shown like these models are quite good at, for instance, plotting patterns or structures depending on how exactly you implement them.
2159638	2166852	322	A	0.9998	But for instance, let's say you have a triangle in your graph or that'll be kind of the simplest structure, right?
2166986	2177136	323	A	0.99985	You have a triangle or some other kind of kind of gadget in your graph, like particular subgraphs that might show up in different kinds of various graphs.
2177168	2190584	324	A	0.98	The graphs themselves might look completely different from each other, but these kind of patterns might kind of be reemerging in multiple like local patterns might reemerge in multiple graphs and that could help you a way to kind of generalize, right?
2190622	2201404	325	A	0.99971	Like you could see, for instance, if you have clicks, they're super important in kind of when you do social network modeling and things like that because they kind of show this kind of close group of friends, right?
2201442	2205648	326	A	0.99999	They all talk to each other, so they kind of form a click like everyone's connected to each other, right?
2205734	2220230	327	A	1	And then you might be able to use that engine like another completely different social context where these agents are again kind of communicating in a similar manner or are connected in a similar manner, even if the kind of overall pattern is quite different.
2220920	2226692	328	A	1	And it goes way beyond just kind of structural similarities because there's also features in there.
2226746	2233944	329	A	0.99996	So there's combinations of kind of structural patterns and features that give you even more complicated patterns, right?
2233982	2243708	330	A	0.99934	Like you might have a triangle, but then also two of the features in this triangle look in a certain way and one that looks in another way.
2243874	2251500	331	A	0.99447	So that gives you even more kind of refinement and even kind of richer pattern detection abilities.
2252880	2259296	332	A	0.99836	So you have essentially this ability to kind of spot patterns at multiple scales as well.
2259318	2261948	333	A	0.99997	So you could see this happening at multiple scales.
2261964	2263792	334	A	0.99999	You could have patterns of patterns, right?
2263846	2269110	335	A	0.99938	You could have entire communities connected in various patterns and so on.
2269480	2271732	336	A	1	And again, it's kind of also a research question.
2271786	2275030	337	A	1	How do you capture these hierarchical patterns and so on.
2275960	2284250	338	A	0.78308	In general, you have to do more message passing if you want to capture things that are further away from each other because otherwise they can't talk to each other, right?
2284700	2291130	339	A	0.99986	So, yeah, I don't know if that actually answered your question or if I was kind of going in the right direction there.
2291580	2292148	340	B	0.5219	It's great.
2292174	2297756	341	B	0.99482	It brings up a lot of different cool ideas like this patterns all the way down, but totally agree.
2297858	2309516	342	B	0.98	I think we can now perhaps explore some more specific connections to active inference because hopefully the listenership or viewership of this, it's kind of like a two way street.
2309548	2318036	343	B	0.98714	Like some people may be coming from more of your background and then learning about active inference and generative models as a specific system of interest for The First time.
2318058	2334504	344	B	0.99997	But also, certainly for A lot of people in the active inference space, these methods coming from category theory have only recently come up to, I guess, more prominence in Bayesian modeling, at least where we are.
2334542	2336410	345	B	0.99858	So it's a cool connection to make.
2336940	2346184	346	B	1	I think one of the biggest touch points off the bat was, like you mentioned, multiplying a matrix by a vector and interpreting that as an edge.
2346312	2358160	347	B	0.99992	So just in the inference part of the generative model about sensory observations, we always talk about the thermometer observation and then the underlying hidden state temperature.
2358580	2361890	348	B	0.99346	So that exactly describes that case.
2362340	2371088	349	B	0.99	And that's why we can represent the active inference generative models, the perceptual parts and the action parts in terms of matrix multiplication.
2371264	2382330	350	B	0.9985	It's why the MATLAB code for generative models does look mostly like matrix multiplication and it can all be done explicitly that way.
2386220	2388440	351	B	0.99996	Are there models that don't have this feature?
2389360	2404880	352	B	0.99703	Or what do we gain by having all of our edges defined as appreciative action with a matrix and a vector in this setting of agent generative models with perception and action?
2406500	2407890	353	B	0.99997	Any thoughts on that?
2408420	2417812	354	A	0.99968	Yeah, I think essentially kind of the graph structure is kind of telling you these things interact in some way, right?
2417866	2443710	355	A	0.99468	So there's some communication between these vertices if we're kind of in a crash setting, right, and then kind of what the sheath is giving you or any message passing model essentially is expressing a way in which way that connection should manifest in the model or in what way that connection should be used to process information.
2444480	2455600	356	A	0.99963	So in this case I was mentioning, okay, you have linear maps because you could go on the type if your type of data are vector spaces, then this transformation will be some sort of linear maps.
2456740	2458864	357	A	0.99998	But it doesn't necessarily have to be.
2458982	2462848	358	A	0.98908	So for instance, it could go to any nonlinear transformation, right?
2463014	2464384	359	A	0.65	And this is what's happening in general.
2464422	2473620	360	A	0.81497	In practice, if you have a neighbor, the message coming from that neighbor could be modulated by any sort of transformation you want.
2473690	2482344	361	A	0.99995	So it could be linear, it could be nonlinear, it could be something, I don't know, you can specify it basically.
2482542	2489640	362	A	0.99821	But essentially you could think of this as you have a structure level telling you who should communicate to whom.
2489980	2497192	363	A	1	And then you kind of have some semantics that this kind of shift is adding on top saying how should these things communicate?
2497256	2497484	364	A	0.91417	Right?
2497522	2502508	365	A	0.97131	Like the first thing is who should communicate or what should communicate.
2502604	2510988	366	A	1	And then the semantics we add on top essentially describe how should that communication manifest.
2511164	2512000	367	A	0.99886	Essentially.
2514340	2515456	368	B	0.9999	Very cool.
2515558	2521444	369	B	1	I think that maps exactly to how we talk about the sparsity of variables in degenerative model.
2521642	2534772	370	B	0.60791	So here the topology of the nodes in the graph that we want to do message passing on are going to be describing the agent and the environment or the generative model that includes perception, cognition and action.
2534916	2544232	371	B	0.97531	So a lot of people have proposed different sparsity architectures for integrated modeling of perception, cognition, action.
2544376	2551592	372	B	0.99231	So one example would just be like kind of around the clock, like action influences the environment, environment influences perception.
2551656	2576832	373	B	0.99998	Back to cognition, you could add a self loop using an arkov blanket and different kinds of connectivities and that defines the sparsity topologically, which is where you showed the stack and you were on the second and the third levels, I think, of the stack and then what flows it has to be described, what that edge does.
2576986	2579272	374	B	0.99753	So what is that?
2579326	2581880	375	B	0.99981	That is also being provided.
2582940	2584008	376	A	0.63379	Yeah, exactly.
2584094	2588408	377	A	1	And it could even go to the extreme where does that edge actually do anything?
2588494	2602716	378	A	0.99987	So for instance, if you have a matrix that's just the zero matrix, for instance, associated to that edge, it will just kind of multiply by zero and that gives you zero and it's kind of essentially pruning that edge, right?
2602738	2606224	379	A	0.99616	Like I kind of get rid of it, I don't want that communication to happen.
2606342	2619108	380	A	0.76256	But there's also kind of this possibility where these kind of semantics, they override the structural level where you say, okay, I don't need to communicate with this other agent person or whatever.
2619194	2624196	381	A	0.96633	It depends on what these rates actually mean and in what context you are.
2624298	2635892	382	A	1	And then there's also the case where you could do some sort of selective pruning where this matrix depending on so in kind of linear algebra, the matrix has a kernel.
2635956	2639560	383	A	0.99977	So it's all the stuff that that matrix sends to zero.
2639630	2641800	384	A	0.99998	So what vectors are sent to zero, right?
2641870	2645180	385	A	1	But not everything will be sent to zero unless you're the zero matrix.
2645840	2651964	386	A	0.99988	So depending on the features of the neighbors, you could also just send some of the neighbors to zero, right?
2652002	2654732	387	A	1	And that kind of removes those neighbors from the equation.
2654796	2660044	388	A	0.90526	They just kind of get they're not factored in anymore.
2660172	2682230	389	A	0.99988	So you kind of have this it's a way to get this parsity, I guess, that you were also talking about where certain only maybe a small subset of the inputs or kind of only subset of the features are actually kind of doing some meaningful stuff among the neighbors and everything else will be kind of zeroed out.
2683260	2694520	390	B	0.99996	Yeah, that makes me think of the Lasso regression, which tries to set most variables of having an impact of zero so that a few hopefully important variables really pop out in the analysis.
2694600	2707730	391	B	0.99999	But also there's newer techniques, I guess, of attention modeling and reweighting that isn't just like, okay, set five of them to one and then the rest of them to zero.
2708260	2709420	392	B	0.98	More nuanced.
2709500	2721156	393	B	0.99995	So I think that sparsity with the expressivity is basically the best of both worlds because you do want to have a situation where there is an edge, but the attention being paid to it is zero.
2721338	2728980	394	B	0.99987	So functionally, that doesn't have an update on the belief state, even though in principle the edge exists.
2729320	2735624	395	B	1	And that's why we can model situations where the agent believes they have impact in the world.
2735662	2740824	396	B	1	But actually, just because the edge in principle exists doesn't mean that it has any given impact.
2740952	2754460	397	B	1	And so that allows the articulation of these models where they factorize and keeps interpretable motifs in terms of just little clusters of motifs.
2754620	2761024	398	B	0.99991	Here in our case describing the action perception and cognition types of systems of interest.
2761142	2763970	399	B	0.99999	But people, I believe, already implicitly do this.
2764900	2769376	400	B	0.91184	They will often add an adjective and refer to x kind of active inference.
2769408	2774740	401	B	0.97037	So like deep active inference with a temporal horizon, sophisticated active inference with this kind of nesting.
2775160	2777844	402	B	1	And those are pointing to a given feature.
2777892	2782600	403	B	1	But of course, those features, as we're hoping, should be composable.
2783500	2797884	404	B	1	And so this seems to be bringing tools that are even more general than just action perception modeling because they're at a lower level of abstraction than any specific system of interest.
2798082	2812800	405	B	0.99958	But where this work and kind of timeless thinking around cybernetic systems come together through the active inference generative model as a Bayes graph, it gets very exciting.
2813460	2814208	406	A	0.99569	Yeah.
2814374	2824224	407	A	1	And maybe also something worth emphasizing here is that even if this kind of semantic level can get rid of some edges, right, by doing this kind of pruning.
2824352	2828020	408	A	0.99987	Something it cannot get rid of is the computation.
2828180	2834952	409	A	0.99764	So something that kind of that structural graphs level forces you to do.
2835006	2838392	410	A	0.99983	It kind of tells you what should you spend compute time on, right?
2838446	2846252	411	A	0.9981	Because even if you're going to decide to prune an edge, you still need to decide that which takes compute time.
2846306	2851976	412	A	0.99997	So you still need to look at all your neighbors if you're a node, right, and decide what to prune.
2852008	2855948	413	A	0.99999	Or maybe you don't prune anything or whatever, but you have to look at every edge.
2856044	2869168	414	A	1	And one way to look at this is the graph structure defines you a computational graph or kind of a computational series of computational steps you have to execute.
2869264	2877464	415	A	1	And then the kind of the sheet structure or the message passing model actually specifies what those steps are and in what particular way they look.
2877502	2878090	416	A	0.99787	Exactly.
2879900	2880840	417	A	0.99184	That's one point.
2880910	2889376	418	A	0.99	And you also mentioned attention, and actually I'm glad you did, because this is actually quite related and in certain ways more general than attention.
2889428	2896332	419	A	0.56	And actually, maybe going back to this slide, it might be a nice way to see this.
2896466	2900380	420	A	0.99689	So here basically what happens in attention.
2900880	2906848	421	A	0.99999	Instead of learning these matrices that we learn here in attention, you learn attention coefficients here.
2906934	2908316	422	A	0.84898	So you just learn a scalar.
2908348	2909804	423	A	0.99975	That's the attention coefficient.
2909852	2917948	424	A	1	How much attention should I pay to essentially this overall edge, let's say, which will be just a scalar?
2918044	2923056	425	A	0.99967	What we do is kind of a bit more complicated because you just learn, how do I transform these neighbors?
2923088	2929412	426	A	0.56698	So it's kind of a whole matrix rather than a single scalar, but there's also some subtle differences.
2929476	2940472	427	A	0.99978	But in a follow up work we did, we also combined this with attention and went a bit more general and that also worked quite well.
2940526	2943820	428	A	0.99979	But the kind of underlying idea is very similar.
2943890	2949644	429	A	0.99772	You want to modulate the way you transform information based on the information itself, right?
2949682	2953544	430	A	0.99972	So you have this kind of one level of recursivity.
2953592	2957584	431	A	0.99999	If you want that, you are also alluding to that.
2957702	2973364	432	A	0.98835	It happens in active inference, where, okay, so if I'm node v, right, my neighbor knew it has some features, and based on these features, which are xu, I'm going to find out the matrix that will be used to process xu, right.
2973402	2977348	433	A	0.95949	So it's kind of very recursive and it's what happens with attention, right.
2977434	2985144	434	A	0.99999	Based on the features of node u, I'm going to compute an attention coefficient that I'm going to apply to this feature of you, right?
2985182	2989460	435	A	0.99937	I'm going to decide based on this feature, how much attention should I pay to it.
2989630	2995310	436	A	0.99	And here we decide how should I process it more generally in a linear way.
2996320	3000910	437	A	0.99334	So you have this kind of loopiness structure embedded in there.
3002880	3003676	438	B	0.99978	Awesome.
3003858	3008060	439	B	0.86783	I'll bring up a few more points because I think there's so many great pieces.
3008140	3010464	440	B	0.99882	So Toby St.
3010502	3022176	441	B	0.51369	Clair, Smyth who we recently discussed his dissertation in Livestream 54, introduced a term or at least a phrasing, the compositional cognitive cartography.
3022288	3045208	442	B	1	And so thinking about the compositionality of cognitive systems, and I think what you're describing here with this notion that the mappings are more general than the kind of attention mechanisms known, famously today, that those represent, like a lower dimensional special case of one kind of architecture.
3045384	3056130	443	B	0.99998	Makes me think about how the Bayesian graph is kind of semantic in principle and can have all of these nice categorical formalisms around them.
3058180	3069636	444	B	1	And you can even build the connector to empirical data with the presheaf and the sheaf, which may be news to even many empirical researchers doing data analysis certainly was for me.
3069738	3089412	445	B	0.99994	But the message passing provides a rigorous translation from whatever semantic model is proposed topologically to an implementation procedure that can be planned for and executed in linear time or at least with definable characteristics.
3089476	3099576	446	B	0.99987	So message passing plays a really important part in going from the abstract what is possible, to the implementations of any of these actual models.
3099608	3123140	447	B	1	And it does it in a really general way where is it accurate to say that we hope that implementation with message passing compatible generative models will kind of roll out better because we won't have some of the engineering challenges that less reusable abstractions might carry?
3125880	3127012	448	A	0.9176	It's hard to say.
3127066	3132760	449	A	1	I think there's also certainly some limitations to this paradigm as well.
3132830	3145052	450	A	0.57403	So just kind of doing this kind of message passing, I think, as you were mentioning, one thing is that it kind of scales up quite easily, like linearly with the size of the graph, but that also come at a cost.
3145106	3150060	451	A	0.99992	So there's certain results showing this has limits in expressivity.
3150640	3169576	452	A	0.99965	So if you actually want to go beyond this, for instance, instead of just looking at pairs of nodes, you have to look at tuples and these kind of high order groupings of nodes in order to kind of get higher explicitity.
3169628	3184280	453	A	0.99077	There's all sorts of techniques to do that and there's always this kind of tension between being more expressive and being efficient that will always be there in any sort of algorithm or method.
3186780	3188410	454	A	0.5907	It's kind of hard to say.
3189340	3192424	455	A	0.99987	We can definitely say this is kind of not the optimist solution.
3192472	3211884	456	A	0.99735	Let's say if you want to do things message passing in itself but maybe doing some sort of computations on graphs could be maybe also something that's kind of maybe missing a bit in kind of the graph ML setting.
3211932	3219620	457	A	0.99751	Is the context where you assume your graph is known.
3220280	3227704	458	A	1	And you need to have some graph structure, at least a sensible way to construct it.
3227742	3227944	459	A	0.98909	Right.
3227982	3237370	460	A	0.99995	But for many kind of more I don't know, how should I phrase that?
3237900	3244670	461	A	1	I guess for less clearly defined things like, okay, if I'm an agent doing perception in the real world or something, right?
3245600	3254844	462	A	0.99999	If I'm trying to create a graph of the world, what's an object, what do I create a node for?
3254882	3255084	463	A	0.97588	Right?
3255122	3259888	464	A	0.99984	If I want to have one note or object and there's some connections between objects and things like that.
3260054	3263968	465	A	0.79	I know it's like some wild example that comes to mind.
3263974	3266956	466	A	0.76	I don't know if you actually want to do that but let's say you want, right?
3267078	3272292	467	A	0.99994	Then there's also all these kind of blurry things like what's an object and what's not an object?
3272346	3276352	468	A	0.99749	What's kind of somewhere in between maybe is that a node?
3276496	3281110	469	A	0.86954	So it's kind of like what I'm trying to say, that the graph structure is kind of very discreet, right?
3281560	3284856	470	A	1	The node is either there or it's not there and edges there is not there.
3284958	3287096	471	A	0.91102	But then the world is kind of very fuzzy, right?
3287118	3298140	472	A	0.99986	So if you use graphs as a model for your world then there probably has to be some decision to be made somewhere about these kind of fuzzy concepts.
3299200	3308720	473	A	0.99947	They actually translate in a concrete graph entity like an object, an edge or whatever or not based on some kind of inference procedure.
3309860	3319152	474	A	1	And I don't know if we do that or not as kind of humans as intelligent agents, but that's kind of some interesting thing to think about.
3319206	3324292	475	A	0.99215	Maybe you could also well, maybe one way to solve that is also kind of stuff like soft edges and things like that.
3324346	3328920	476	A	1	And in some way if you have attention coefficients, it's a bit like that.
3329070	3336392	477	A	0.99998	If an edge has a weight of 0.1 or something, it's almost like not being there but it's still kind of there.
3336446	3341720	478	A	0.99872	So it's a bit of a soft graph architecture.
3342940	3348604	479	A	1	I guess at the edge level you can implement this softness but I think it's a bit harder at kind of the node level, right?
3348642	3353410	480	A	0.94526	Like how do you kind of model a node that's kind of there and not there?
3353860	3357916	481	A	0.98496	Yeah, there's just some random thoughts that's.
3357948	3371924	482	B	1	Very interesting about the fuzzy object identification and kind of similarities and differences between nodes and edges even though in some ways they have some similarities too or interoperabilities too.
3372122	3397212	483	B	1	One other point of contact was like an underlying hidden space that we understand topologically that projects a vector space from different places so that could be a vector of thermometer readings and we want to have a smooth path within the homeostatic range defined up to a boundary point.
3397346	3414828	484	B	0.99914	Not saying that that's the structure of the world but a structure of a very heuristic and simple model might be to aim for continuity and have a defined hidden estate space that has continuity underneath and is able to emit vectors.
3415004	3433780	485	B	0.99912	That kind of brings some of these classifier type discussions that you brought up and the kind of fundamental impossibility of geometric classification because you are going to end up with gray zones whereas even if it takes a bitwise description, you can separate the network.
3433940	3446696	486	B	0.99726	So that gives an actual completeness measure and that allows measures like I mean amount of computational resources or in a more statistically principled way like the Bayesian Information criterion.
3446808	3448668	487	B	0.99857	So how many nodes should we have?
3448834	3453360	488	B	0.93886	We should be on some trade off front in some modeling space.
3453430	3454336	489	B	1	I don't know what to tell you.
3454358	3456028	490	B	0.99985	It's a map, not a territory.
3456204	3458284	491	B	1	And that's more justifiable.
3458332	3467590	492	B	1	And so even lifelike organisms might want to self evidence staying emitting from a living state.
3468040	3475440	493	B	1	And so that provides a really simple graphical architecture to cybernetic systems.
3475600	3487316	494	B	1	And then active Inference explores a lot of different more specific motifs within that broader blanket persistence picture and the path of least action.
3487428	3502830	495	B	0.99996	So that's what enables the physics in that space and why these methods, which as far as I understand are often used in quantum mechanics, are being able to come together with active inference this way.
3503520	3521012	496	A	0.99983	Yeah, something that comes to mind when you mention this, I think there's also like a recent avenue of research in this area where people and it's again kind of generated by the fact that you don't know the graph beforehand many times.
3521066	3526564	497	A	1	And I think kind of the old school approach was, well, you construct it based on some rules, right?
3526602	3535770	498	A	0.99336	Like you're going to say, I don't know, some things are similar, I'm going to put an edge between them and you define similar in whatever way you like and so on.
3536220	3547368	499	A	1	And there was this kind of recent trend where what you try to do is kind of latent graph inference, or some people call it manifold learning, if you think of the graph as some sort of manifold.
3547384	3559168	500	A	0.99953	But this kind of very informally speaking and essentially what you would do is like you would map whatever you try to learn the raw observations into some latent space.
3559254	3561836	501	A	0.55	And that's where you actually construct the graph.
3561868	3567140	502	A	0.99999	You construct the graph in the latent space rather than kind of in the raw space.
3567210	3572324	503	A	0.99976	So that might be kind of a way to deal with fuzziness as well.
3572362	3572900	504	A	0.99569	Right?
3573050	3586410	505	A	0.99997	Because then I guess you might lose some of these kind of very concrete one to one mappings because you might learn some node in the latent space that maybe corresponds to three or four concepts kind of mixed together.
3586940	3597816	506	A	0.98832	There's all these kind of nice experiments with neurons in deep networks, kind of visualized, and they learn maybe kind of a mixture of concepts.
3598008	3605264	507	A	0.99997	If you see what actually activates that neuron is actually maybe a few classes or different kinds of things.
3605302	3607010	508	A	0.73771	It's not necessarily a single thing.
3608260	3617216	509	A	0.99925	So it could be something very similar here where you have some very entangled representations that are kind of distilled in this latent graph.
3617328	3629300	510	A	1	And then in a way, at least in concept space, even if in the latent space, that's still kind of a very clear combinatorial structure with respect to kind of your raw observations.
3631640	3642628	511	A	0.51749	That structure can still kind of encode the fuzziness of the world to some degree because you have this kind of mixture of concepts that got distilled in the same node or things like that.
3642654	3648664	512	A	0.99994	Or maybe some concepts could be represented by multiple nodes depending on what way you see these concepts.
3648712	3652496	513	A	0.9738	There might be all sorts of variations or concept or points of view and so on.
3652518	3662924	514	A	0.99214	So I think kind of latent graph inference could be quite an interesting way maybe to address some of these issues we were discussing.
3662972	3668804	515	A	0.99566	Although I think it kind of died off a bit in the recent year, at least as far as I've seen.
3668842	3673540	516	A	0.9998	There were a few slightly fewer papers on the topic.
3675800	3687480	517	B	0.99996	Well, certainly the agent's proposed latent structure of the world, the causal structure of the world is just mapped on the territory and so it enables maybe some of those core screenings.
3687980	3693660	518	B	0.99978	Could you go to the slide where there was a mapping between a smooth sphere and then a regular geometric shape?
3695680	3700220	519	A	0.2729	Yeah, let's see in this one.
3700370	3705120	520	B	0.98286	Yeah, just wanted to make one point and see if you had any comments.
3705940	3738792	521	B	1	At the heart of some of the relationships that you're describing and where you pulled back to in terms of generalization helps us understand this relationship between the sphere and the geometry and the implications for data processing and all of the computational science areas is if you are preserving or learning or analyzing geometry but not topology or the other way around, you might get these different data set.
3738846	3739700	522	B	0.99416	Aberrations.
3739780	3744956	523	B	0.99653	Like you might have the topology of the coffee cup, but it looks like something totally different.
3745138	3751740	524	B	1	And so what we would really want to do would be understand the relationship between geometry and topology.
3752240	3775344	525	B	0.99999	Because if we could understand it in principle, like you have it on the left side and then in practice with the data scheme on the right side or insert your own left and right side there, then we'd be able to do data analysis in a way that respected preserved both the topology and the geometry.
3775472	3798350	526	B	0.99994	So it's like two compatible perspectives that have their different strengths and weaknesses and heuristics and so understanding that relationship between geometry and topology and the implicit spaces that geometry requires and so on, that has tremendous use.
3798800	3821792	527	B	1	And it just in closing, reminds me of Buckminster Fuller's Synergetics, which uses a close packing architecture and a tetrahedron centric model of coordinates to find more continuity between surface area and volume and between the smooth surfaces and the great circles on them and like the points of connectivity on shapes.
3821936	3828372	528	B	0.99991	So I think it's an incredibly deep area and really has fundamental impact in active inference.
3828516	3840040	529	B	0.99978	Helps us think about our models in this way, kind of like the inflated balloon with the fuzziness and the architecture and the finiteness.
3840960	3843144	530	B	0.99964	It really brings a lot to active inference.
3843192	3849150	531	B	1	And so I appreciate you sharing the work with us today and continuing to work in this way.
3850000	3851210	532	A	0.59648	Thanks a lot.
3852260	3853840	533	B	0.99998	Any last thoughts?
3854420	3860416	534	A	0.99834	Yeah, what you mentioned, I think it's been all over my thesis this.
3860438	3864140	535	A	0.99992	Kind of tension between topology and geometry.
3864220	3873364	536	A	1	And maybe what I want to emphasize is that I'm not saying kind of the previous perspective of looking maybe more geometrically at things was wrong in any way.
3873482	3878108	537	A	0.96	And on the contrary, actually, there's lots of interesting places where these things intersect.
3878144	3886164	538	A	0.99936	Even in kind of this chief paper I briefly went through, like, if you actually read the paper, there's a lot of beautiful intersections.
3886212	3895688	539	A	0.99827	Actually, my main collaborator, Francesco, he's a differential geometry, so he actually had lots of kind of inputs from that side.
3895774	3906400	540	A	0.89	And indeed, I think we should try to use all these kind of layers of structure are in the best way possible for all our methods.
3907380	3907888	541	B	0.99993	Awesome.
3907974	3909392	542	B	0.77	All right, thank you.
3909526	3910336	543	B	0.55288	Till next time.
3910358	3910784	544	A	0.99335	Thanks a lot.
3910822	3911970	545	A	0.99707	Thanks for having me.
3912500	3912780	546	A	0.6791	Bye.

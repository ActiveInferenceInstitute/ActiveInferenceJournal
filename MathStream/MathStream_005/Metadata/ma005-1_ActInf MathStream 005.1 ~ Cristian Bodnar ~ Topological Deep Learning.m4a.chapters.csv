start	end	startTime	summary	headline	gist
2220	67540	00:02	We're here in Active Inference math stream number 5.1 with Chris Bodner on topological deep learning graphs, complexes, and sheaves. This is Bodner's PhD thesis, which he finished a couple of months ago. Looking forward to your presentation and discussion.	Chris Bodner: This is my PhD thesis on topological deep learning	Topological Deep Learning
68040	424390	01:08	In machine learning there's all these kind of subfield that emerged a few years ago which is called geometric deep learning. Neural network architectures on data, living on all sorts of structures or geometries or spaces if you want. Part of what I've been doing in my PhD thesis was to look at these problems from a more topological perspective.	Geometric deep learning is looking at how to apply deep learning to data	Deep Learning with Geometric Deep Learning
425800	516500	07:05	Many of these things relate to category theory. To translate between different structure in mathematics. You could think of this as some sort of translation or mapping from spaces and regions in that space to kind of data attached to that space.	Many of these things relate to category theory	Mathematics 6, Translation and category theory
517160	639290	08:37	Topological spaces start with a set and then have a collection of subsets called the open sets that need to satisfy certain axioms. You could think of these open sets as kind of regions of this space, very informally speaking.	Topological spaces have axioms that need to satisfy certain conditions	Topological Spaces
640160	952990	10:40	Topological space is a space where data is attached to regions. We can glue data together to get a unique piece of data. What's nice about this perspective is it allows us to do machine learning on top of this data.	So we have data and then we add data on top of a space	Topological Sheaves and the preshief of continuous functions
953300	1283992	15:53	The kind of favorite architecture of people doing machine learning on graphs these days are these things called graph neural networks. But depending on the properties of these graphs, their performance might vary quite a lot. This could be used to do some useful stuff when doing machinelearning on graphs.	The kind of favorite architecture of people doing machine learning on graphs these days is graph neural networks	Shifting diffusion in graph neural networks
1284046	1568224	21:24	Message passing is very similar on graphs to group actions in group theory. For each arrow you see here we have a matrix and something that I'll argue and show in a few slides. So you could think of these arrows as giving you some sort of actions that you can play with to move features from vertex to edge and edge to vertex.	Message passing is very similar on graphs to group actions in group theory	Categorical Sheaves on Graphs
1568352	2055648	26:08	In group theory, one way to represent a group is by kind of having some sort of graph. What you could think of as message passing is same as group actions in group theory. If you use sufficient layers, it will be able to disentangle the classes of the nodes.	Group actions in machine learning are similar to message passing in group theory	Group theory and machine learning
2055814	2336410	34:15	How can message passing help us generalize in various kinds of settings or maybe from one graph to another and things like that? How does knowing the message passing structure help reuse a model across different settings or facilitate the legibility of the model?	Message passing approaches are helping in the active inference modeling	How message passing helps in the active Inference modeling
2336940	2880840	38:56	A lot of people have proposed different sparsity architectures for integrated modeling of perception, cognition, action. In practice, if you have a neighbor, the message coming from that neighbor could be modulated by any sort of transformation you want.	Mathematics can represent generative models in terms of matrix multiplication	Inference and Sparsity in Generative Models
2880910	3003676	48:00	This is actually quite related and in certain ways more general than attention. Instead of learning these matrices that we learn here in attention, you learn attention coefficients here. You want to modulate the way you transform information based on the information itself.	This is actually related to attention but there are some subtle differences	Inclination and Attention
3003858	3371924	50:03	Message passing provides a rigorous translation from whatever semantic model is proposed topologically to an implementation procedure. But there's also certainly some limitations to this paradigm as well. There's always this kind of tension between being more expressive and being efficient.	Message passing allows translation from abstract models to implementations of actual models	Cognitive Systems: Compositional Cognitive Cartography
3372122	3502830	56:12	One other point of contact was like an underlying hidden space that we understand topologically that projects a vector space. Even lifelike organisms might want to self evidence staying emitting from a living state. These methods are being able to come together with active inference this way.	Active Inference provides a really simple graphical architecture to cybernetic systems	Quantum Inference and the Computational Network
3503520	3687480	58:23	Recent trend where what you try to do is kind of latent graph inference, or some people call it manifold learning. You construct the graph in the latent space rather than kind of in the raw space. That structure can still kind of encode the fuzziness of the world to some degree.	Kind of latent graph inference could address some of these issues we discussed	Inclination to latent graph inference
3687980	3851210	1:01:27	The relationship between the sphere and the geometry has implications for data processing. What we would really want to do would be understand the relationship between geometry and topology. It's an incredibly deep area and really has fundamental impact in active inference.	Understanding relationship between sphere and the geometry has implications for data processing and inference	The relationship between Geometry and Topology
3852260	3906400	1:04:12	Kind of tension between topology and geometry. There's lots of interesting places where these things intersect. I think we should try to use all these kind of layers of structure are in the best way possible for all our methods.	There's a tension between topology and geometry in your thesis	Tension between topology and geometry
3907380	3912780	1:05:07	Awesome. All right, thank you. Till next time. Thanks a lot. Bye.	Awesome. Thanks a lot. Till next time. Bye.	TALKING TO THE FANS

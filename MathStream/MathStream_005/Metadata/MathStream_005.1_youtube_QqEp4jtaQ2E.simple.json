[
  {
    "start": 5.731,
    "end": 24.222,
    "text": " hello and welcome it's july 17 2023 we're here in active inference math stream number 5.1 with chris bodner on topological deep learning graphs complexes and sheaves so thank you for joining chris looking forward to your presentation and discussion",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 26.038,
    "end": 28.28,
    "text": " Yeah, thanks all for having me.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 28.38,
    "end": 35.805,
    "text": "So yeah, as I was just saying, this is my PhD thesis, which I finished a couple of months ago.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 35.845,
    "end": 37.527,
    "text": "It's also kind of public online.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 37.567,
    "end": 44.652,
    "text": "So if you want to go into the details, just kind of look this up on the internet and you should be able to find it easily.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 45.573,
    "end": 47.715,
    "text": "Obviously, there's lots of stuff in there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 47.755,
    "end": 54.6,
    "text": "So I kind of try to give an overview of the day and maybe also go in a little bit more detail in certain aspects.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 55.26,
    "end": 70.698,
    "text": " um since there's you know not a lot of time to go through everything um so now i'm on my microsoft research so this is some basically all work that i did in the past then when i was at university of cambridge um all right so let's let's get uh started",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 72.798,
    "end": 75.78,
    "text": " Right, so let's start very easily.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 75.8,
    "end": 98.859,
    "text": "Now, I'm actually not sure exactly what's kind of the background of the people who are watching, but in machine learning, there's all these kind of subfield that emerged a few years ago, which is called geometric deep learning, which is essentially looking at how to apply these kind of deep learning neural network architectures on data, leaving on all sorts of",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 101.431,
    "end": 105.492,
    "text": " you know, kind of structures or geometries or spaces, if you want.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 106.833,
    "end": 109.774,
    "text": "And this has a lot of application, especially in the life sciences.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 109.834,
    "end": 116.896,
    "text": "And there's kind of been a lot of instances of this in kind of, you know, very famous publications, some of which you see here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 116.976,
    "end": 124.798,
    "text": "But, you know, just to give some examples, for instance, if you have proteins or molecules or things like that, they usually represent it as graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 126.058,
    "end": 131.0,
    "text": "And you kind of have some data living on these graphs, like kind of the properties of certain atoms and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 132.362,
    "end": 136.589,
    "text": " So these kind of things and so far this kind of",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 138.274,
    "end": 163.377,
    "text": " spaces or these kind of problems learning problems if you want uh they have been approached mostly uh kind of with a geometrical mindset as the kind of name of the of the subfield um also mentions um but um something that you know i would argue is that geometry is not everything that you need and there's kind of other non-geometrical aspects when you are in such a setting um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 164.458,
    "end": 174.481,
    "text": " And this is kind of quite obvious once you realize that the spaces that kind of show up in the field and in many applications, they are very heterogeneous.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 174.861,
    "end": 178.342,
    "text": "So as I mentioned, for instance, you could have graphs that could represent anything.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 178.422,
    "end": 181.962,
    "text": "In this case, it's the caffeine molecule that you see here on the left.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 182.683,
    "end": 187.444,
    "text": "And you want to have some models that predict certain properties of this molecule and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 188.124,
    "end": 189.364,
    "text": "But for instance, you can have grids.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 189.724,
    "end": 191.705,
    "text": "And we see grids all the time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 192.225,
    "end": 194.828,
    "text": " and data living on grids, and I'm referring to images, videos.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 194.888,
    "end": 197.732,
    "text": "They are all kind of pixels living on a grid.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 198.933,
    "end": 200.895,
    "text": "And then you can have more sophisticated things.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 200.935,
    "end": 202.617,
    "text": "You could have some meshes, for instance.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 204.485,
    "end": 208.73,
    "text": " they're all over in computer graphics, and then you could have some sort of manifold.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 208.75,
    "end": 216.738,
    "text": "So for instance, if you're doing maybe weather modeling or something, we live on a sphere, topologically speaking.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 217.899,
    "end": 221.904,
    "text": "So you might want to model your data as living on a sphere and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 224.591,
    "end": 240.955,
    "text": " But nonetheless, even if these spaces are kind of geometrically kind of heterogeneous, and some of them don't even have a geometrical structure in kind of a strict mathematical sense, they all have what's called a topological structure, which is kind of like a weaker kind of structure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 242.415,
    "end": 243.936,
    "text": "But it's kind of more general.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 244.816,
    "end": 248.357,
    "text": "And I'm going to talk a bit in a few seconds about what that means.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 248.757,
    "end": 256.62,
    "text": " But in general, when you do kind of mathematical physics, you kind of have a ladder of structures where you kind of keep building on top.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 256.7,
    "end": 261.002,
    "text": "And the more structure you have, the more sophisticated things you can do and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 261.082,
    "end": 267.084,
    "text": "And kind of at the base of this diagram, just a set, just kind of a collection of elements with no kind of extra structure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 268.004,
    "end": 270.946,
    "text": "And then you kind of keep going up in this ladder.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 272.006,
    "end": 275.087,
    "text": " you add stuff on top of sets and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 276.148,
    "end": 286.653,
    "text": "And as I was saying, kind of most of the work is kind of focused on maybe the top levels of this hierarchy, but it's kind of topological level, which are kind of the weakest kind of level you can",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 287.313,
    "end": 290.775,
    "text": " and on top of that has kind of been neglected to a large extent.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 291.855,
    "end": 304.522,
    "text": "And part of what I've been doing in my PhD thesis was essentially looking at this kind of learning problems on these kind of spaces from a more topological perspective and, you know, kind of try to fill in these blanks.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 306.023,
    "end": 307.064,
    "text": " So this is kind of the overview.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 307.524,
    "end": 314.89,
    "text": "And now, okay, so if we are to adopt this topological perspective, what would that actually mean or how would that look like?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 315.891,
    "end": 320.655,
    "text": "I guess it could look in different ways, but in what I've done in my faces, it looks kind of like this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 321.276,
    "end": 326.42,
    "text": "So we have horizontally, we kind of have a space that could be anything.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 326.52,
    "end": 327.841,
    "text": "It's just kind of an abstract space.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 327.861,
    "end": 329.823,
    "text": "You know, it could be a grid or an",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 330.123,
    "end": 337.467,
    "text": " So things like we've seen in the previous example and kind of vertically attached to the regions of the space we have data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 338.287,
    "end": 346.111,
    "text": "So data is kind of this vertical component and you kind of see these flags kind of being, you know, kind of anchored in these regions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 346.271,
    "end": 350.833,
    "text": "So that kind of signifies you have some data that's kind of associated with that region.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 351.654,
    "end": 353.715,
    "text": "So that's kind of the high level perspective.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 354.736,
    "end": 357.543,
    "text": " And I'm going to make that a bit more concrete a bit later.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 357.563,
    "end": 361.433,
    "text": "And there's kind of two essential things about this picture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 362.801,
    "end": 365.083,
    "text": " A first thing is locality.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 365.623,
    "end": 372.227,
    "text": "So the data is attached to some regions of this space, this topological space.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 372.928,
    "end": 375.169,
    "text": "And in that sense, it's local.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 375.269,
    "end": 376.87,
    "text": "So it's kind of associated with the region.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 376.97,
    "end": 388.958,
    "text": "And maybe to give a concrete example, if you kind of have a temperature sensor somewhere in space, you could think of whatever that sensor is measuring is kind of a property of kind of the immediate surrounding",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 390.279,
    "end": 391.299,
    "text": " you know, around that sensor.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 391.359,
    "end": 395.201,
    "text": "So it's kind of describing some property of a region in space.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 396.161,
    "end": 397.902,
    "text": "So that would be kind of a good example.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 399.042,
    "end": 406.185,
    "text": "And another kind of axiom that we're starting with is that the space has structure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 407.225,
    "end": 413.027,
    "text": "So the space has kind of, it's kind of made up of various regions and these regions intersect in various ways.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 414.91,
    "end": 440.774,
    "text": " so the and that implicitly also makes our data structure because the data is attached to these regions so so there is kind of some some structuring in the data um all right so that's kind of the picture and actually many of these things relate to category theory i'm not going to go in depth into this because it's kind of sophisticated and i'm not an expert myself in category theory um but kind of the the high level here is that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 441.848,
    "end": 457.62,
    "text": " category theory is kind of a nice way to translate between different structure in mathematics and kind of discuss about properties of certain kinds of objects and translate that to some different kinds of objects and find all these kind of relations and connections.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 457.841,
    "end": 466.927,
    "text": "And a concrete example is, for instance, if you want to study some manifold, some surfaces, you could associate some groups to the surfaces and then",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 467.868,
    "end": 486.178,
    "text": " any sorts of relations between these kind of surfaces also translating some relations about these groups or some other algebraic structures so you could study these manifolds by doing algebra instead of doing you know geometry or topology or something else um so also in this case this kind of manifests in the fact that we kind of translate these um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 486.798,
    "end": 488.459,
    "text": " from spaces to data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 488.56,
    "end": 497.067,
    "text": "So because we associate the regions in a space with certain kinds of data, this is how this translation manifests in what I've just described in the previous picture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 497.427,
    "end": 505.494,
    "text": "You could think of this as some sort of translation or mapping from spaces and regions in that space to data attached to that space.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 506.815,
    "end": 512.479,
    "text": "But yeah, I'm not going to go in a lot of detail into this, but it's just to keep in the back of your mind.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 513.38,
    "end": 538.252,
    "text": " that there's there's this stuff lurking in the background um all right so this is i i promise this is the only math definition i'm giving in this talk and then then i'll stop um but just because i'm mentioning topological space spaces quite often i just wanted to kind of um give this axiomatic definition um which might sound sophisticated but i have a picture at the end and hopefully it'll be clear um so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 538.852,
    "end": 539.612,
    "text": " Yeah, so what is it?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 540.373,
    "end": 541.633,
    "text": "So it's just a set.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 541.793,
    "end": 544.414,
    "text": "As I was saying, we start with sets and we put stuff on top, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 544.434,
    "end": 553.417,
    "text": "So you start with a set, and then you also have a collection of subsets of the set called the open sets that need to satisfy certain axioms.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 553.457,
    "end": 557.578,
    "text": "So you could think of these open sets as kind of regions of the space, very informally speaking.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 559.018,
    "end": 568.602,
    "text": " So something that kind of has to be satisfied is that, well, the empty set and the set itself need to be open sets.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 568.682,
    "end": 579.305,
    "text": "So in some sense, you could think of this as saying the set itself is a region of that space very informally, which, you know, it's kind of, let's say, obvious.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 579.705,
    "end": 580.525,
    "text": "And then there's some",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 582.566,
    "end": 609.933,
    "text": " kind of constraints about intersecting and taking units of this region so uh if we take the intersection of two regions we should get another one of these regions um and if we take a union of these regions we should take we should get another region um and uh there is some constraints again like okay how big this intersection should be there should be finite intersections but you could have infinite unions but that's a technicality and uh we can just skip",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 610.493,
    "end": 638.812,
    "text": " anyways but to see a picture so on the left you just see the set x itself and here i put like a potential neighborhood structure kind of like open set structure on this space so we have an open set here another open set v by this axiom the intersections would also be an open set so you see this intersection in the middle being another opposite and then the set itself is another open set so it's just kind of a splitting stuff into regions kind of you could think of it like that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 639.552,
    "end": 643.294,
    "text": " All right, so this is a topological space.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 644.914,
    "end": 646.615,
    "text": "And now let's add data, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 646.635,
    "end": 653.118,
    "text": "So we mentioned that we have data and then we put, sorry, we have a space and then we add data on top.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 653.858,
    "end": 656.299,
    "text": "So, so far we've seen how a topological space looks like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 656.719,
    "end": 659.901,
    "text": "Now let's add this kind of vertical stuff, these flags that you saw before.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 659.921,
    "end": 662.662,
    "text": "We just put some data on top of these regions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 663.722,
    "end": 667.564,
    "text": "And if we put data on all the regions of the space on all the open sets,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 668.164,
    "end": 693.352,
    "text": " um we get these uh structures that in category theory like in algebraic topology also geometry they are called pre-sheets um which which sounds very fancy but all is just kind of a definition of uh what i was already describing essentially you have some data for each region these these f of so for instance for uh region u you have this f of",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 693.972,
    "end": 717.726,
    "text": " u which is kind of the data attached to region u so you could think of f of u some some set with with describing the data that lives there in that region um but there's also an extra thing you you have some some sort of maps going between these kind of uh pieces of data um and these are called restriction maps and and why is that it's kind of they provide you a way to uh",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 718.906,
    "end": 720.587,
    "text": " zoom in if you want.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 721.007,
    "end": 726.71,
    "text": "You have the data attached to the whole set X, and then you could think, okay, how do I take this data?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 726.95,
    "end": 730.992,
    "text": "How do I go from this data to data on a smaller region on X?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 731.372,
    "end": 734.053,
    "text": "There's kind of a way to zoom in on that data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 735.874,
    "end": 758.597,
    "text": " essentially and i'm going to show some some example in a second so these are called um pre-sheaves and just to see an example our uh space here is kind of one of the simplest kind of space you can think of it's just one d a horizontal line right it's just the real line um and then you have some regions which are just given by open intervals uh on on these uh",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 761.62,
    "end": 766.605,
    "text": " And then some pieces of data could be functions, like continuous functions on those regions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 767.025,
    "end": 771.169,
    "text": "So here's some sample data on this first interval.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 771.59,
    "end": 773.491,
    "text": "Here's another function on the second interval.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 773.892,
    "end": 775.533,
    "text": "Here's some data on this third interval.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 776.034,
    "end": 778.276,
    "text": "And actually, in this case, it happens that all these",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 779.571,
    "end": 790.323,
    "text": " functions agree on the overlap, so where these regions overlap, they take the same values, and we can actually glue them together in a single function over the entire region.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 790.343,
    "end": 797.391,
    "text": "So this is just an example of a pre-shift, and it's called the pre-shift of continuous functions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 798.332,
    "end": 806.137,
    "text": " So our data in this case is continuous functions and the space is just the real line and we put these functions on top of the real line.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 806.518,
    "end": 814.143,
    "text": "But it turns out because of this kind of special property that we can kind of glue data and we uniquely get some other piece of data, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 814.523,
    "end": 818.286,
    "text": "We can take these three pieces, it's just exactly like a puzzle, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 818.306,
    "end": 826.031,
    "text": "We put these things together and we get a fourth thing which is kind of a single function where we just overlap these functions, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 829.473,
    "end": 837.116,
    "text": " These pre-sheaves that satisfy this kind of properties where you can kind of glue them to get a unique piece of data, they're called sheaves.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 838.277,
    "end": 842.839,
    "text": "And basically the pre-sheave of continuous functions is actually a sheave.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 844.384,
    "end": 848.726,
    "text": " So this is kind of a way to formalize data attached to these things.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 849.846,
    "end": 851.867,
    "text": "It's going to get less technical in a second.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 853.167,
    "end": 854.927,
    "text": "So just to give more examples.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 855.388,
    "end": 858.529,
    "text": "So for instance, in this way, we could describe data on a sphere.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 859.629,
    "end": 862.33,
    "text": "And let's say this data is just some vector field on a sphere.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 862.39,
    "end": 863.21,
    "text": "So let's say this is",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 863.47,
    "end": 888.139,
    "text": " earth right this could be some wind vector field right like if we do weather modeling or something you just have a vector field describing the wind uh on on the surface of earth right um and you might want to do some machine learning on top of this where this kind of vector field has a shift structure and you could think of it as a shift because you know if you have like some vector field on the red region a vector field on the yellow region",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 888.799,
    "end": 895.748,
    "text": " I can kind of glue them together uniquely if they agree on the overlap, and we get the vector field on this bigger region.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 897.15,
    "end": 903.097,
    "text": "But something that's quite nice is that even if we have a very, very different kind of space, namely a graph,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 904.078,
    "end": 915.226,
    "text": " which is very different from a sphere in all points of view, we can still apply the exact same kind of axioms and terminology and kind of definitions, and we can have a shift over a graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 915.266,
    "end": 920.45,
    "text": "So in this way, we could have, for instance, some features associated to the nodes of the graph,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 921.01,
    "end": 923.951,
    "text": " some features associated with the edges of the graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 924.191,
    "end": 927.012,
    "text": "And there's another node which has its own features.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 928.033,
    "end": 930.774,
    "text": "And this is actually the exact same thing we have in graph machine learning.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 931.754,
    "end": 944.819,
    "text": "So this is quite nice for this kind of topological perspective allows us to do is, you know, we kind of have a unified way of thinking, if you want, about very kind of heterogeneous spaces, and we can model on all of them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 945.325,
    "end": 949.986,
    "text": " data attached to them by using this kind of shift terminology and other ways as well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 950.006,
    "end": 952.606,
    "text": "But I'm not going into that in this talk.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 953.207,
    "end": 953.747,
    "text": "All right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 953.787,
    "end": 957.888,
    "text": "So this is kind of an overview of what I've been doing in my thesis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 959.248,
    "end": 968.55,
    "text": "And just to kind of dive a bit deeper into this, I just wanted to go into one paper that we did at NeurIPS.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 969.91,
    "end": 982.773,
    "text": " last year yeah so this was last year um on uh what's called shift diffusion so essentially how can we use what i've just described to do some useful stuff when doing machine learning on graphs",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 983.636,
    "end": 1001.946,
    "text": " um all right and this was a collaboration with francesco di giovanni ben timberlane uh pietro leo my advisor and uh michael bronstein um okay so before i dive into this i just want to give some background in case people are not familiar with this um so the kind of",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1003.117,
    "end": 1013.28,
    "text": " favorite architecture of people doing machine learning on graphs these days are these things called graph neural networks, which are actually very simple kind of models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1014.0,
    "end": 1019.182,
    "text": "So in this setting, you have some features.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1019.222,
    "end": 1021.743,
    "text": "So each node in your graph will have some features.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1021.803,
    "end": 1025.864,
    "text": "This is what this H vector here denotes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1025.924,
    "end": 1028.125,
    "text": "So it's the",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1029.125,
    "end": 1047.749,
    "text": " vector h associated with node a at layer or time t whatever um so you have some features for each of these nodes and what you're doing each of if you're at a certain node you want to kind of compute a new representation or a new new features for for this node at the next layer um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1048.723,
    "end": 1050.604,
    "text": " So essentially, you're learning representations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1051.665,
    "end": 1061.19,
    "text": "And what the graph neural networks are doing, this node will receive a message from all the other nodes that are neighbors with this node.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1062.191,
    "end": 1067.874,
    "text": "And this message can also be passed, can use some neural networks in there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1068.875,
    "end": 1071.856,
    "text": "But essentially, it's some processing of these features of the neighbors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1073.177,
    "end": 1076.919,
    "text": " These are aggregated into this message, so here in green.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1077.92,
    "end": 1088.146,
    "text": "And then this is passed through some update function that combines the message from the neighbors with the old representation of this node, and it gives you a new representation at the next layer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1089.006,
    "end": 1090.347,
    "text": "And this happens for all the nodes, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1090.367,
    "end": 1093.489,
    "text": "So then you get some new representations for this node, and this is one layer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1093.769,
    "end": 1096.431,
    "text": "And then you kind of keep repeating this for as many layers as you like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1097.191,
    "end": 1099.753,
    "text": "So this is kind of how you do deep learning on graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1100.773,
    "end": 1120.964,
    "text": " It's kind of a very very simple recipe and most models actually vary in the way they kind of compute these messages and in the way this update function is designed but that's kind of the parameters they most of these models use otherwise they all kind of respect this framework and work in this kind of particular way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1122.825,
    "end": 1129.669,
    "text": " And to give you maybe an example for why you would want to do this, you might want to do node classification.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1129.709,
    "end": 1132.49,
    "text": "This is kind of a classic problem in graph machine learning.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1132.51,
    "end": 1135.812,
    "text": "There are others, but I'm just going to talk about this because it's easier.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1137.072,
    "end": 1141.915,
    "text": "So you have a graph, and this graph has nodes that have different labels.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1141.995,
    "end": 1145.536,
    "text": "Here, there's just two kinds of labels, these orange and blue.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1147.897,
    "end": 1151.258,
    "text": " And you have some edges between these nodes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1151.518,
    "end": 1164.06,
    "text": "And what you want is you want to do this kind of message passing that I was describing to compute some representations for these nodes where you can easily classify the blue and orange nodes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1165.301,
    "end": 1170.562,
    "text": "Now, something that's quite interesting is that for many kind of graph neural networks,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1172.788,
    "end": 1179.076,
    "text": " depending on the properties of these graphs and how these different nodes are connected, their performance might vary quite a lot.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1179.937,
    "end": 1183.481,
    "text": "So in particular, they're affected by this property called heterophily.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1184.282,
    "end": 1188.807,
    "text": "So this is a measure of how much opposites attract if you want.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1190.589,
    "end": 1191.951,
    "text": " It has a very simple formula.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1192.932,
    "end": 1197.917,
    "text": "Basically, you take the number of edges between orange and blue nodes and you divide by the total number of edges.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1198.278,
    "end": 1209.21,
    "text": "So basically, you kind of check how many connections we have in this graph between things that are opposite to each other versus connections that are between similar kind of nodes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1209.87,
    "end": 1231.983,
    "text": " um so if you have this kind of a lot of these kind of heterogeneous connections then you have very high heterophily um and it turns out that many graphing and interjecting struggle in that setting it's very hard to to classify things in that setting um and intuitively you could kind of also you know figure out why because you know you could easily apply to some kind of reasoning",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1232.643,
    "end": 1257.728,
    "text": " where you know oh this node looks a lot like this other node is connected to so they kind of must be in you know in the same community if you want or in the same label but it's much harder to do that when you know all things are kind of different from um from each other um and and these communities kind of don't form right even visually if you if you see a graph and it has some nicely clustered communities you know it's quite easy to draw a line between those and say oh this is",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1258.588,
    "end": 1259.928,
    "text": " communities and other community.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1260.349,
    "end": 1263.69,
    "text": "But if things are very mixed, then it's quite challenging.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1263.77,
    "end": 1270.491,
    "text": "And it turns out it's also challenging for these models, not just for our intuition when we would have to do this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1272.172,
    "end": 1281.675,
    "text": "So this is some problem where this topological perspective I was mentioning will be used to do some useful stuff.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1283.775,
    "end": 1286.836,
    "text": " OK, so coming back to sheaves on graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1287.876,
    "end": 1291.858,
    "text": "And at this point, I think you can largely forget what I mentioned in the introduction.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1292.658,
    "end": 1297.92,
    "text": "Or if there's something you misunderstood there, we start from zero a bit here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1297.98,
    "end": 1300.281,
    "text": "So there's no problem.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1301.607,
    "end": 1307.491,
    "text": " On the left, you just have a graph, which is kind of the incident structure of a graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1307.891,
    "end": 1314.035,
    "text": "I just wrote here the simplest possible graph that has two nodes, v and u. And then there's an edge between them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1314.216,
    "end": 1315.837,
    "text": "So this is just a graph with one edge.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1316.317,
    "end": 1317.458,
    "text": "That's all that's going on here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1318.098,
    "end": 1323.001,
    "text": " And I've just represented it by kind of in this kind of incidence structure kind of way, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1323.461,
    "end": 1335.347,
    "text": "Node v is incident to node e and node u, sorry, edge e, and node u is incident to edge e. So this is just an incident structure, and what this kind of triangle symbol is showing is just",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1336.167,
    "end": 1340.009,
    "text": " this incidence structure is just a way to symbolize this incidence relation, if you want.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1340.889,
    "end": 1342.47,
    "text": "OK, so this is just a graph, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1342.95,
    "end": 1348.633,
    "text": "And a way we can kind of think of sheaves on graphs is just mapping these graph structures.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1348.653,
    "end": 1351.434,
    "text": "So this is kind of this categorical theory translation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1351.774,
    "end": 1355.136,
    "text": "We translate this graph into something else, which looks",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1356.096,
    "end": 1380.325,
    "text": " know very similar the structure is kind of the same it's just kind of the meaning of these uh things change so for each node v we have um here this will be a vector space so fov is a vector space for each node u we have f of u which is another vector space uh for each edge e we have this fe which is another vector space um so all nodes have their own vector spaces",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1380.905,
    "end": 1384.688,
    "text": " and the features associated to those nodes leaving those vector spaces.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1385.108,
    "end": 1387.85,
    "text": "So basically for each node, we have a vector space of features.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1388.351,
    "end": 1389.692,
    "text": "That's all that's going on so far.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1390.292,
    "end": 1398.699,
    "text": "And also these arrows that these incidence relations also translate into something and they translate into the obvious thing, linear maps.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1398.879,
    "end": 1404.143,
    "text": "So if these are vector spaces, then these things should be linear maps or just some matrices essentially, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1404.463,
    "end": 1407.125,
    "text": "So for each arrow you see here, we have a matrix.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1408.506,
    "end": 1408.606,
    "text": "And",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1412.779,
    "end": 1428.248,
    "text": " Something that I'll argue and show in a few slides is that basically message passing on graphs is very similar with group actions in group theory.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1428.389,
    "end": 1429.849,
    "text": "Let me explain exactly why.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1430.35,
    "end": 1436.593,
    "text": "We can think of what we have on the left, these arrows from the incidence relation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1436.994,
    "end": 1440.236,
    "text": "We could think of these arrows as some buttons we can press.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1440.856,
    "end": 1442.196,
    "text": " So what do I mean by that?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1442.696,
    "end": 1456.219,
    "text": "So if we have this node V on the left, right, and this E, now, if we have some features, some feature living in FOV, right, we could just kind of press this arrow button here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1456.419,
    "end": 1462.1,
    "text": "And then if we multiply this matrix by this feature, we will get an edge feature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1462.12,
    "end": 1468.782,
    "text": "So it's kind of like, if you go along this arrow, this matrix will multiply this feature",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1469.582,
    "end": 1472.403,
    "text": " this vertex feature, and it will give you an edge feature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1473.323,
    "end": 1481.066,
    "text": "So you can think of these arrows as kind of giving you some sort of actions that you can play with to move features from vertex to edge and edge to vertex.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1481.546,
    "end": 1483.287,
    "text": "So in this case, it's kind of a left action.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1483.567,
    "end": 1484.407,
    "text": "So this is what I'm saying.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1484.548,
    "end": 1493.211,
    "text": "I'm taking this arrow, which is this one here, and I act on some features of node v, so this h of v that lives here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1494.731,
    "end": 1503.376,
    "text": " And how I do that is I just take this matrix, this matrix associated with this arrow, and I multiply this vector HOV.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1503.716,
    "end": 1505.437,
    "text": "So just matrix times vector.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1505.997,
    "end": 1506.477,
    "text": "That's all.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1506.858,
    "end": 1508.058,
    "text": "And then we get an edge feature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1508.458,
    "end": 1511.2,
    "text": "So this is just kind of a way to move from here to here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1511.24,
    "end": 1513.601,
    "text": "So this already kind of looks a bit like message passing, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1513.641,
    "end": 1516.923,
    "text": "We're kind of passing a message from this vertex to this edge.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1518.283,
    "end": 1533.208,
    "text": " But now we also need to pass a message from this edge to this other vertex u. So we need to get from v to u. And we did that by passing through e. So by doing that, we could do that by going in reverse.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1533.268,
    "end": 1540.99,
    "text": "So we could have a right action, where instead of applying this matrix, we apply its adjoint matrix.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1541.11,
    "end": 1542.691,
    "text": "So those are the transpose matrix.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1543.231,
    "end": 1553.133,
    "text": " So if we want to go from here to here, instead of applying this matrix, we apply its transpose because we want to go the other way around.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1553.393,
    "end": 1564.996,
    "text": "So if we compose these things, then we can move features from V to U. So this is just a way that we can apply these actions to do message passing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1565.376,
    "end": 1567.977,
    "text": "And these are called shift actions or pre-shift actions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1568.517,
    "end": 1572.218,
    "text": "And I'm going to now show what's the relation with",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1573.058,
    "end": 1575.92,
    "text": " between this and what we have in group theory.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1577.28,
    "end": 1585.905,
    "text": "So one way to represent the group is by kind of having some sort of graph like here on the left.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1586.545,
    "end": 1590.748,
    "text": "So we kind of have some star object.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1591.228,
    "end": 1596.031,
    "text": "It's just kind of a dummy thing there, but all the group structure is in these arrows, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1596.051,
    "end": 1601.074,
    "text": "So for each group element, so let's say this G is a 90 degree rotation, for instance,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1602.294,
    "end": 1629.175,
    "text": " uh let's say we do have a group of rotations just to have some concrete example these um so this arrow could correspond to a 90 degree rotation we have another arrow that does the opposite minus kind of 90 degree rotation that's the inverse of that transformation um so this is kind of the structure of this group and if we have we also do this kind of similar kind of translation as we've just seen um so basically basically we define a pre-shift on this group",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1630.015,
    "end": 1651.164,
    "text": " uh we map this star to a vector space so the star kind of replaces the vertex uh we had before now we just have a single vertex uh and it's just these arrows we have um so the star is not this vector space that you showed here in in blue right um and now if we actually do group actions which are kind of a very well established concept in group theory um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1652.335,
    "end": 1668.86,
    "text": " Well, for instance, if you want to act on this vertex, sorry, not vertex, on this vector V right here, you have a vector in this vector space, and you want to act on it by this group transformation G. So essentially you want to press this arrow so you apply some action on it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1670.26,
    "end": 1679.263,
    "text": "Then what you do is, well, because of this translation, this G has been mapped to some matrix, which is the rotation matrix, the corresponding rotation matrix.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1680.163,
    "end": 1691.559,
    "text": " and you apply this rotation matrix on v um and you get like a 90 degree uh rotation here um so this is what what's going on this kind of vertical vector is showing the rotated vector",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1693.274,
    "end": 1696.816,
    "text": " So this is completely analogous with what we've seen on the previous slide.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1699.278,
    "end": 1702.56,
    "text": "This is how kind of sheaves connect these kind of actions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1703.1,
    "end": 1710.985,
    "text": "So essentially what you could think of as message passing is same as group actions in group theory, but you just replace this group with a graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1711.466,
    "end": 1713.287,
    "text": "So it's kind of analogous to that, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1713.307,
    "end": 1716.409,
    "text": "So it's just kind of a different kind of translations",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1716.929,
    "end": 1718.909,
    "text": " where we replaced the object on the left.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1719.91,
    "end": 1721.43,
    "text": "Now it looks like this is a graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1721.83,
    "end": 1722.85,
    "text": "This is a group.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1723.17,
    "end": 1725.351,
    "text": "But the rest stays exactly the same.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1727.071,
    "end": 1741.394,
    "text": "And this gives us a way to formalize, in a way, by looking in this topological perspective to connect all these symmetries and things that have been explored quite a lot in machine learning to message passing on graphs and to see one way in which they are related.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1742.154,
    "end": 1760.133,
    "text": " um okay so now you might say okay this was all very sophisticated and nice but you know what is this going anywhere basically um and i'm just going to show you kind of a very short example there there's more but you know the time is limited um and something we we showed is that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1761.361,
    "end": 1766.225,
    "text": " as I was saying in the beginning, many graph neural networks kind of struggle in these heterophilic graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1766.725,
    "end": 1781.136,
    "text": "And what we've shown is that no matter how heterophilic or kind of weird your graph is, you can always kind of find some shift structure, essentially kind of a message passing neural network that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1782.137,
    "end": 1787.443,
    "text": " if you use sufficient layers, it will be able to disentangle the classes of the nodes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1787.843,
    "end": 1796.713,
    "text": "So just to show you in this picture, what you have here on the very far left, the colors of the nodes, they show the class.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1796.893,
    "end": 1797.433,
    "text": "So we have",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1798.174,
    "end": 1800.735,
    "text": " There's three colors here, so three classes, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1801.775,
    "end": 1803.455,
    "text": "And this is kind of the graph in the beginning.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1804.096,
    "end": 1807.316,
    "text": "And the position of the nodes in this box denotes the features.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1807.476,
    "end": 1809.877,
    "text": "So that's a way to kind of just visualize the features.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1811.237,
    "end": 1816.599,
    "text": "The position, the 2D position is actually the 2D feature vector of each node.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1817.519,
    "end": 1820.86,
    "text": " And you can see in the beginning, everything is kind of super messy and entangled, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1820.92,
    "end": 1829.943,
    "text": "Like if you want to classify these nodes, it's kind of very hard because their initial representations are very messy and kind of intertwined.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1831.023,
    "end": 1842.727,
    "text": "But as we stack more layers of a particular kind of sheaf or a message passing model, you see how progressively these classes get kind of more disentangled and more disentangled.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1843.067,
    "end": 1862.758,
    "text": " at each new layer and so these representations kind of collapse and they form these kind of clusterings right and then when you get with something like at the end you can kind of see these three communities very clearly and it's extremely easy to separate and kind of the essence behind these results was",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1863.678,
    "end": 1871.884,
    "text": " we showed for different kinds of problems what sorts of chief or message passing models you need by using the theory to solve kind of problems.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1872.044,
    "end": 1882.412,
    "text": "And this is quite important because it kind of shows you some important bits and pieces in the architecture that you might want to kind of change or",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1882.452,
    "end": 1885.313,
    "text": " or use in order to solve certain kinds of problems.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1885.733,
    "end": 1888.134,
    "text": "And we also had some sort of impossibility results.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1888.154,
    "end": 1894.196,
    "text": "So if you use a graph neural network of some kind, you can't solve this problem or you'll struggle to solve this problem.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1895.276,
    "end": 1900.038,
    "text": "And we also saw, okay, if you use some more general ones, then you might have a chance.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1901.739,
    "end": 1907.504,
    "text": " So this is kind of the high level view behind this theoretical stuff.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1908.806,
    "end": 1918.714,
    "text": "And what we actually do in practice is to essentially learn these message passing functions or to learn the sheet for these matrices.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1919.355,
    "end": 1933.753,
    "text": " So in practice, like when someone gives you a null classification task, it's very hard to know beforehand what exactly is the right chief or the right message passing model to solve that task.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1935.234,
    "end": 1950.663,
    "text": " and what we do is actually we learn that from data so we learn these matrices that do the message passing we learn them from data um by passing some using some neural networks which are shown here in red um and and then you learn how to kind of",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1951.323,
    "end": 1965.928,
    "text": " transfer features between these vector spaces and kind of move them around so this is just showing how these vectors which are features of these nodes and edges how they're kind of moved around by kind of going through the by these matrices just some matrix multiplications um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1967.333,
    "end": 1970.737,
    "text": " And okay, so that's kind of a high level view behind this model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1971.258,
    "end": 1983.231,
    "text": "And we evaluated this on some kind of real world heterophilic data sets where you have to classify nodes based on kind of various communities or different kinds of labels.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1983.652,
    "end": 1985.333,
    "text": "And these data sets going from",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1986.154,
    "end": 2000.521,
    "text": " um right to left they are getting more heterophilic so in some sense more challenging for classic architectures and our models which are kind of inspired by all this stuff that i mentioned they score quite highly in in these benchmarks",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2001.822,
    "end": 2022.179,
    "text": " and at the same time we also revealed some or justified some various choices that other models in this space have done but maybe they were not so well justified or maybe they had different kind of motivations we also managed to kind of show why various things they were already doing why they make sense from the point of view of this kind of",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2023.119,
    "end": 2041.151,
    "text": " um theory um all right well that that's all um all i had um yeah thanks for for listening and yeah happy to chat more about this and also have lots of backup slides in case you know uh depending on how far we venture off with these questions",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2044.531,
    "end": 2061.62,
    "text": " cool well awesome work thank you for the presentation for people who are in the live chat they can write some questions but there's many things i think we could talk about so i want to start with reading a quote from an abstract of the paper by",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2063.158,
    "end": 2075.527,
    "text": " van der Laar, Coudal, and de Vries, just to kind of ground this in the active inference context and really justify why the message passing approaches that you are describing are helping in the active inference modeling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2076.047,
    "end": 2079.63,
    "text": "The paper, it's two papers, it's called Realizing Synthetic Active Inference Agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2080.55,
    "end": 2108.372,
    "text": " they wrote with a full message passing account of synthetic active inference agents it becomes possible to derive and reuse message updates across models and move closer to industrial applications of synthetic active inference framework so how does knowing the message passing structure help reuse a model across different settings or like facilitate the legibility of the model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2110.293,
    "end": 2110.494,
    "text": " Right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2111.136,
    "end": 2115.629,
    "text": "So first of all, I'm not super familiar with the kind of active inference literature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2118.104,
    "end": 2123.528,
    "text": " You'll have to help me there a bit to anchor maybe the discussions a bit more into that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2124.368,
    "end": 2140.339,
    "text": "But I think if I understand correctly, the kind of question you're getting at is basically how can message passing help us generalize in various kinds of settings or maybe from one graph to another and things like that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2142.12,
    "end": 2147.144,
    "text": " this is an active area of research, how exactly this generalization is happening.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2148.045,
    "end": 2159.574,
    "text": "But something you could notice or something that, for instance, was shown, like these models are quite good at, for instance, plotting patterns or structures, depending on how exactly you implement them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2159.614,
    "end": 2163.477,
    "text": "But for instance, let's say you have a triangle in your graph or",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2164.938,
    "end": 2166.48,
    "text": " That'd be kind of the simplest structure, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2166.84,
    "end": 2172.366,
    "text": "You have a triangle or some other kind of gadgets in your graph, like particular subgraphs, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2172.686,
    "end": 2177.131,
    "text": "That might show up in different kinds of various graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2177.231,
    "end": 2179.714,
    "text": "The graphs themselves might look completely different from each other.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2180.774,
    "end": 2187.998,
    "text": " But these kind of patterns might kind of be re-emerging in multiple, like local patterns might re-emerge in multiple graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2188.379,
    "end": 2190.58,
    "text": "And that could help you a way to kind of generalize, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2190.6,
    "end": 2197.724,
    "text": "Like you could see, for instance, if you have clicks, they're super important in kind of when you do social network modeling and things like that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2198.164,
    "end": 2221.297,
    "text": " because they kind of show this kind of um you know close group of friends right they all talk to each other so they kind of form a clique like everyone's connected to each other right and then you you might be able to use that then generally another completely different social context um where you know these agents are again kind of communicating in a similar matter or connected in a similar manner even if the kind of overall pattern is quite different um and",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2221.937,
    "end": 2226.658,
    "text": " it goes way beyond just kind of structural similarities because there's also features in there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2226.738,
    "end": 2233.9,
    "text": "So there's combinations of kind of structural patterns and features that, you know, give your even more complicated patterns, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2233.96,
    "end": 2243.282,
    "text": "Like you might have a triangle, but then also two of the features in this triangle are, you know, look in a certain way and one that looks in another way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2243.722,
    "end": 2246.083,
    "text": "So, so that gives you even more kind of refinement and,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2246.653,
    "end": 2251.217,
    "text": " and even kind of richer pattern detection abilities.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2252.838,
    "end": 2259.263,
    "text": "So you have essentially this ability to kind of spot patterns at multiple scales as well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2259.303,
    "end": 2261.905,
    "text": "So you could see this happening at multiple scales.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2261.925,
    "end": 2263.747,
    "text": "You could have patterns of patterns, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2263.787,
    "end": 2268.611,
    "text": "You could have entire communities connected in various patterns and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2269.692,
    "end": 2271.754,
    "text": " And again, it's kind of also a research question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2271.794,
    "end": 2274.596,
    "text": "How do you capture these hierarchical patterns and so on?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2275.738,
    "end": 2284.646,
    "text": "You know, in general, you have to do more message passing if you want to capture things that are further away from each other because otherwise they can't talk to each other, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2284.946,
    "end": 2290.592,
    "text": "So yeah, I don't know if that actually answered your question or if I was kind of going in the right direction there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2291.761,
    "end": 2321.621,
    "text": " it's great it brings up a lot of different cool ideas like like this patterns all the way down but totally agree i i think we can now perhaps explore some more specific connections to active inference because hopefully the the um listenership or viewership of this it's kind of like a two-way street like some people may be coming from more of your background and then learning about active inference and generative models as a specific system of interest the first time but also certainly for a lot of people in the active inference space like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2322.122,
    "end": 2336.404,
    "text": " these methods coming from category theory have only recently come up to i guess more prominence in bayesian modeling at least where we are so it's kind of it's a cool connection to make yeah um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2337.265,
    "end": 2357.922,
    "text": " think one of the biggest touch points off the bat was like you mentioned multiplying a matrix by a vector and interpreting that as an edge so just in the in the inference part of the generative model of about sensory observations we always talk about the thermometer observation and an underlying hidden state temperature",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2358.783,
    "end": 2388.104,
    "text": " so that like exactly describes that case and that's why we can represent the active inference generative models the perceptual parts and the action parts in terms of matrix multiplication it's why the matlab code for generative models does look mostly like matrix multiplication and it can all be done explicitly that way so how does that feel like are there models that don't have this feature",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2389.554,
    "end": 2404.468,
    "text": " Or what do we gain by having all of our edges defined as a pre-sheaf action with a matrix and a vector in this setting of agent generative models with perception and action?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2406.75,
    "end": 2407.351,
    "text": "Any thoughts on that?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2408.672,
    "end": 2408.912,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2410.549,
    "end": 2417.477,
    "text": " essentially the graph structure is telling you these things interact in some way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2417.838,
    "end": 2421.882,
    "text": "So there's some communication between these",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2422.802,
    "end": 2447.844,
    "text": " vertices if you you know if we're kind of in a graph setting right um and then kind of what the sheath is giving your or any message passing model essentially is expressing a way that in which way that connection should manifest in the model or in what way that connection should be used to process information um so in this case i was mentioning okay we you have like",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2448.204,
    "end": 2476.35,
    "text": " linear maps because you could go on the type if your type of data are vector spaces then this transformation will be some sort of linear maps um but it doesn't necessarily have to be so for instance it could go to any non-linear transformation right like if and and this is what's happening in general in practice um you know if you if you have a neighbor the message coming from that neighbor could be modulated by any sort of transformation you want so it could be linear it could be non-linear it could be",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2477.33,
    "end": 2484.636,
    "text": " something i don't know you you can specify it basically but essentially you could think of this as",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2485.765,
    "end": 2489.207,
    "text": " you have a structure level telling you who should communicate to whom.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2490.227,
    "end": 2497.471,
    "text": "And then you kind of have some semantics that this kind of shift is adding on top saying how should these things communicate, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2497.511,
    "end": 2502.294,
    "text": "Like the first thing is who should communicate or what should communicate.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2502.474,
    "end": 2511.819,
    "text": "And then the semantics we add on top essentially describe how should that communication manifest essentially.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2514.462,
    "end": 2515.323,
    "text": " Very cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2515.623,
    "end": 2521.086,
    "text": "I think that maps exactly to how we talk about the sparsity of variables in the generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2521.927,
    "end": 2534.395,
    "text": "So here, the topology of the nodes in the graph that we want to do message passing on are going to be describing the agent and the environment, or the generative model that includes perception, cognition, and action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2535.095,
    "end": 2553.746,
    "text": " so a lot of people have proposed different sparsity architectures for integrated modeling of perception cognition action so one example would just be like kind of around the clock like action influences the environment environment influences perception back to cognition you could add a self loop",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2554.867,
    "end": 2581.401,
    "text": " using a Markov blanket and different kinds of connectivities and that defines the sparsity topologically which is where you showed the stack and and you were on the second and the third levels um I think of the stack and then like what flows it has to be described how it actually what that Edge does so what what is that what is that that is also being provided",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2583.168,
    "end": 2584.089,
    "text": " Yeah, yeah, exactly.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2584.209,
    "end": 2588.472,
    "text": "And it could even go to the extreme where, you know, does that edge actually do anything?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2588.532,
    "end": 2598.338,
    "text": "So for instance, if you have a matrix that's just the zero matrix, for instance, associated to that edge, it would just kind of multiply by zero, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2598.578,
    "end": 2599.519,
    "text": "And that gives you zero.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2600.299,
    "end": 2602.721,
    "text": "And it's kind of essentially pruning that edge, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2602.741,
    "end": 2604.222,
    "text": "Like I can't get rid of it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2604.282,
    "end": 2606.103,
    "text": "I don't want that communication to happen.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2606.504,
    "end": 2612.808,
    "text": "But there's also kind of this possibility where these kind of semantics, they override the structural level.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2613.328,
    "end": 2619.13,
    "text": " where you say, OK, I don't need to communicate with this other agent, person, or whatever.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2619.15,
    "end": 2623.952,
    "text": "It depends on what these vertices actually mean and in what context you are.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2624.352,
    "end": 2628.834,
    "text": "And then there's also the case where you could do some sort of selective pruning where",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2630.234,
    "end": 2644.699,
    "text": " this matrix depending on um so in kind of linear algebra the matrix has a kernel so that it's uh all the stuff that that matrix sends to zero so what vectors are sent to zero right but not everything will be sent to zero unless you're the zero matrix",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2646.059,
    "end": 2651.744,
    "text": " So depending on the features of the neighbors, you could also just send some of the neighbors to 0.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2652.064,
    "end": 2654.726,
    "text": "And that removes those neighbors from the equation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2654.766,
    "end": 2659.81,
    "text": "They're not factored in anymore.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2660.33,
    "end": 2665.574,
    "text": "So it's a way to get the sparsity, I guess, that you were also talking about.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2667.341,
    "end": 2681.646,
    "text": " where only maybe a small subset of the inputs or only a subset of the features are actually doing some meaningful stuff among the neighbors and everything else will be kind of zeroed out.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2683.424,
    "end": 2694.353,
    "text": " Yeah, that makes me think of the lasso regression, which tries to set most variables of having an impact of zero so that a few hopefully important variables really pop out in the analysis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2694.713,
    "end": 2709.525,
    "text": "But also there's newer techniques, I guess, of attention modeling and reweighting that isn't just like, okay, set five of them to one and then the rest of them to zero, like more nuanced.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2709.585,
    "end": 2711.047,
    "text": "So I think that sparsity regression",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2711.647,
    "end": 2732.418,
    "text": " with the expressivity is basically the the best of both worlds because you do want to have a situation where there is an edge but the attention being paid to it is zero so functionally that doesn't have an update on the belief state even though the in principle the edge exists and that's why we can model situations where like the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2733.829,
    "end": 2758.401,
    "text": " agent believes they have impact in the world but actually just because the edge in principle exists doesn't mean that it has any given impact and so that allows like the articulation of these models where they factorize and keeps interpretable motifs in terms of just little clusters of motifs here in our case describing the action perception and cognition",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2759.554,
    "end": 2763.477,
    "text": " types of systems of interest, but people I believe already implicitly do this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2764.538,
    "end": 2769.382,
    "text": "Like they will often add an adjective and refer to X kind of active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2769.402,
    "end": 2774.347,
    "text": "So like deep active inference with a temporal horizon, sophisticated active inference with this kind of nesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2775.367,
    "end": 2782.413,
    "text": "And those are pointing to a given feature, but of course those features as we're hoping should be composable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2783.755,
    "end": 2787.197,
    "text": "And so this seems to be bringing",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2788.362,
    "end": 2812.42,
    "text": " tools that are even more general than just action perception modeling because they're at a lower level of abstraction than like any specific system of interest but where this work and kind of timeless thinking around cybernetic systems come together through the active inference generative model as a base graph it gets very exciting",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2813.598,
    "end": 2814.278,
    "text": " Yeah, yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2814.598,
    "end": 2827.702,
    "text": "And maybe also something worth emphasizing here is that even if this kind of semantic level can get rid of some edges, right, by doing this kind of pruning, something it cannot get rid of is the computation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2828.282,
    "end": 2838.345,
    "text": "So something that kind of that structural graph level forces you to do, it kind of tells you what should you spend compute time on, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2838.385,
    "end": 2839.605,
    "text": "Because like if",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2840.856,
    "end": 2846.221,
    "text": " even if you're going to decide to prune an edge, you still need to decide that, which takes compute time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2846.281,
    "end": 2855.69,
    "text": "So you still need to look at all your neighbors if you're a node and decide what to prune or maybe you don't prune anything or whatever, but you have to look at every edge.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2856.231,
    "end": 2865.059,
    "text": "And one way to look at this is the graph structure defines your computational graph or kind of a computational",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2865.76,
    "end": 2883.139,
    "text": " yeah a series of computational steps you have to execute and then the kind of the shift structure or the message passing model actually specifies what those steps are and you know in what particular way they they look exactly um and so yeah that's one point and yeah you also mentioned attention and actually",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2883.96,
    "end": 2896.031,
    "text": " Yeah I'm glad you did because this is actually quite related and in certain ways more general than attention and actually maybe going back to this slide it might be a nice way to see this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2896.571,
    "end": 2906.66,
    "text": "So here basically what happens in attention, instead of learning these matrices that we learned here in attention, you learn attention coefficients here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2907.04,
    "end": 2908.261,
    "text": " So you just learn a scalar.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2908.341,
    "end": 2909.783,
    "text": "That's the attention coefficient.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2909.823,
    "end": 2917.77,
    "text": "How much attention should I pay to essentially this overall edge, let's say, which will be just a scalar.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2918.191,
    "end": 2923.075,
    "text": "What we do is kind of a bit more complicated because you just learn, how do I transform these neighbors?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2923.115,
    "end": 2925.658,
    "text": "So it's kind of a whole matrix rather than a single scalar.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2927.749,
    "end": 2929.39,
    "text": " But there's also some subtle differences.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2929.43,
    "end": 2940.416,
    "text": "But in a follow-up work we did, we also combined this with attention and went a bit more general and that also worked quite well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2940.476,
    "end": 2943.878,
    "text": "But the kind of underlying idea is very similar.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2943.918,
    "end": 2949.642,
    "text": "You want to modulate the way you transform information based on the information itself, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2949.662,
    "end": 2957.246,
    "text": "So you have this kind of one level of recursivity, if you want, or kind of, yeah, that we're also alluding to in that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2957.646,
    "end": 2983.71,
    "text": " you know it happens in active inference where okay you're you're so if i'm node v right my name are you it has some features and based on these features which are x u i'm gonna find out the matrix that will be used to process x here right so it's kind of very recursive and it's what happens with attention right based on the features of node u i'm gonna compute an attention coefficient that i'm gonna apply to these",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2984.13,
    "end": 2985.131,
    "text": " feature or view, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2985.151,
    "end": 2988.913,
    "text": "I'm going to decide based on this feature, how much attention should I pay to it?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2989.813,
    "end": 2994.896,
    "text": "And here we decide, how should I process it more generally in a linear way?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2996.537,
    "end": 3000.439,
    "text": "So you have this kind of loopiness structure embedded in there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3003.14,
    "end": 3003.44,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3005.131,
    "end": 3026.282,
    "text": " bring up a few more points because i think there's there's so many great pieces so toby saint claire smythe who we recently discussed his dissertation in livestream 54 introduced a term or at least a phrasing the compositional cognitive cartography and so thinking about the compositionality of cognitive systems",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3027.262,
    "end": 3050.821,
    "text": " and i think what you're describing here with this notion that the mappings are more general than the kind of attention mechanisms known famously today that those represent like a lower dimensional special case of one kind of architecture makes me think about how the bayesian graph is kind of semantic in principle",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3051.912,
    "end": 3081.077,
    "text": " and can have all of these nice categorical formalisms around them, but then, and you can even build the connector to empirical data with the pre-sheaf and the sheaf, which may be news to even many empirical researchers doing data analysis, certainly was for me, but the message passing provides a rigorous translation from whatever semantic model is proposed topologically to an implementation procedure",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3082.003,
    "end": 3083.306,
    "text": " that can be planned for.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3085.535,
    "end": 3089.397,
    "text": " and execute it in linear time or at least with definable characteristics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3089.477,
    "end": 3099.601,
    "text": "So message passing plays a really important part in going from like the abstract what is possible to the implementations of any of these actual models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3099.661,
    "end": 3113.226,
    "text": "And it does it in a really general way where is it accurate to say that we hope that implementation with message passing compatible generative models will kind of roll out better",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3114.642,
    "end": 3122.768,
    "text": " because we won't have some of the engineering challenges that less reusable abstractions might carry?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3122.788,
    "end": 3126.991,
    "text": "It's hard to say.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3127.051,
    "end": 3131.115,
    "text": "I think there's also certainly some limitations to this paradigm.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3132.19,
    "end": 3153.431,
    "text": " as well so just kind of doing this kind of message passing thing as you you were mentioning one thing is that it kind of scales up uh quite easily like linearly with the size of the graph but that also come at the cost so there is certain results showing this has limiting expressivity um so if you actually want to to",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3154.512,
    "end": 3156.374,
    "text": " go beyond this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3156.454,
    "end": 3169.587,
    "text": "For instance, you have to, instead of just looking at pairs of nodes, you have to look at tuples and these kind of high order groupings of nodes in order to kind of get higher expressivity.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3169.627,
    "end": 3171.669,
    "text": "There's all sorts of techniques to do that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3173.071,
    "end": 3175.473,
    "text": "But yeah, and there's always this kind of tension between",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3176.454,
    "end": 3201.118,
    "text": " being more expressive and being efficient that you know will always be there in any sort of algorithm or method so it's kind of hard to say I mean it's definitely we can definitely say this is kind of not the ultimate solution let's say if you want to do things message passing in itself you know but you know maybe doing some sort of computations on graphs you know could be",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3205.351,
    "end": 3227.902,
    "text": " Maybe also something that's maybe missing a bit in the GraphML setting is the context where you assume your graph is known and you need to have some graph structure, or at least a sensible way to construct it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3227.922,
    "end": 3233.225,
    "text": "But for many more, I don't know,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3234.753,
    "end": 3244.436,
    "text": " more how should i phrase that um i guess for less clearly defined things like okay if i'm an agent doing perception in the real world or something right like",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3245.623,
    "end": 3251.924,
    "text": " If I'm trying to create a graph of the world, what's an object?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3251.984,
    "end": 3254.825,
    "text": "What do I create a node for?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3255.085,
    "end": 3263.726,
    "text": "If I want to have one node per object and there's some connections between objects and things like that, I know it's some wild example that comes to mind.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3263.746,
    "end": 3266.687,
    "text": "I don't know if you actually want to do that, but let's say you want.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3267.127,
    "end": 3272.288,
    "text": "Then there's also all these blurry things like what's an object and what's not an object?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3273.308,
    "end": 3284.635,
    "text": " somewhere in between maybe you know is that a node is that so it's kind of like the what i'm trying to say that the graph structure is kind of very discreet right like it's kind of the node is either there or it's not there and edge is there is not there",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3285.134,
    "end": 3286.835,
    "text": " But then the world is kind of really fuzzy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3287.095,
    "end": 3297.819,
    "text": "So if you use graphs as a model for your world, then there probably has to be some decision to be made somewhere about these kind of fuzzy concepts.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3299.12,
    "end": 3308.243,
    "text": "Do they actually translate in a concrete graph entity, like an object, an edge, or whatever, or not, based on some kind of inference procedure?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3310.124,
    "end": 3319.17,
    "text": " And I don't know if we did that or not as kind of humans, as intelligent agents, but that's, you know, kind of some interesting thing to think about.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3319.27,
    "end": 3324.233,
    "text": "Maybe you could also, well, maybe one way to solve that is also kind of stuff like soft edges and things like that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3324.373,
    "end": 3336.34,
    "text": "And in some way, you know, if you have attention coefficients, it's a bit like that, you know, like if an edge has a weight of 0.001 or something, it's almost like not being there, but, you know, it's still kind of there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3337.681,
    "end": 3341.363,
    "text": " It's a bit of a soft graph architecture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3341.383,
    "end": 3345.606,
    "text": "So I guess at the edge level, you can implement this softness.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3345.666,
    "end": 3348.808,
    "text": "But I think it's a bit harder at the node level.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3350.53,
    "end": 3353.011,
    "text": "How do you model a node that's there and not there?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3354.072,
    "end": 3356.394,
    "text": "Yeah, just some random thoughts.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3357.695,
    "end": 3366.161,
    "text": "That's very interesting about the fuzzy object identification and similarities and differences between nodes and edges, even though in some ways,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3367.64,
    "end": 3394.067,
    "text": " have some similarities too um or interoperabilities too one other point of contact was like an underlying hidden space that we understand topologically that projects a vector space at in from different places so that could be a vector of thermometer readings and we want to have a smooth path within the homeostatic",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3395.137,
    "end": 3411.127,
    "text": " range defined up to a boundary point not saying that that's the structure of the world but a structure of a very heuristic and simple model might be to aim for continuity and have a defined hidden state space that has continuity underneath",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3412.205,
    "end": 3438.337,
    "text": " and is able to emit vectors so that kind of brings some of these classifier type discussions that you brought up and like the kind of fundamental impossibility of geometric classification because you're you are going to end up with gray zones whereas even if it takes a bitwise description you can separate the network so that gives an actual completeness measure and that allows",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3439.719,
    "end": 3467.233,
    "text": " measures like i mean amount of computational resources or in a more like statistically principled way like the bayesian information criterion so how many nodes should we have we should be on some trade-off front in some modeling space i don't know what to tell you it's a map not the territory and that's because that's more justifiable and so uh even lifelike organisms might want to self-evidence staying emitting from a living state",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3468.345,
    "end": 3496.42,
    "text": " and so that provides a really simple graphical architecture to cybernetic systems and then active inference explores a lot of different more specific motifs within that broader blanket persistence picture and the path of least action so that's what enables the physics in that space and why these methods which as far as i understand are often used in quantum mechanics",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3498.209,
    "end": 3502.294,
    "text": " or being able to come together with active inference this way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3503.683,
    "end": 3521.018,
    "text": " Yeah, something that comes to mind when you mention this, I think there's also like a recent avenue of research in this area where people, and it's again kind of generated by the fact that you don't know the graph beforehand many times.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3521.138,
    "end": 3526.543,
    "text": "And I think kind of the old school approach was, well, you construct it based on some rules, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3526.563,
    "end": 3527.984,
    "text": "Like you're going to say, oh,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3529.085,
    "end": 3536.048,
    "text": " I don't know, some things are similar, I'm going to put an edge between them, and you define similar in whatever way you like and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3536.548,
    "end": 3549.372,
    "text": "And there was this kind of recent trend where what you try to do is kind of latent graph inference, or some people call it manifold learning, if you think of the graph as some sort of manifold, but this kind of very informally speaking.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3551.173,
    "end": 3576.056,
    "text": " and essentially what you would do is like you would map whatever you try to learn the raw observations um into some latent space and that's where you actually construct the graph you construct the graph in the latent space rather than kind of in in the you know raw space so that might be kind of a way to um deal with fuzziness as well right like because then i guess you might lose some",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3576.957,
    "end": 3606.577,
    "text": " some of these kind of very concrete one-to-one mappings because you might learn some node in the latent space that maybe corresponds to three or four concepts kind of mixed together and you know there's all these kind of nice experiments with neurons in the networks kind of visualized and you know they learn maybe a kind of a mixture of concepts like if you see what actually you know activates that neuron is actually you know maybe a few classes or different kinds of things it's not necessarily a single thing",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3607.237,
    "end": 3616.943,
    "text": " So it could be something very similar here, where you have some very entangled representations that are still in this latent graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3617.443,
    "end": 3629.19,
    "text": "And then, at least in concept space, even if in the latent space, that's still a very clear combinatorial structure with respect to your raw observations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3631.135,
    "end": 3655.966,
    "text": " that that structure kind of can still kind of encode the fuzziness of the world to some degree because you have this kind of mixture of concepts that got distilled in the same node or things like that or maybe some concepts could be represented by multiple nodes uh depending on you know in what way you see these concepts there might be all sorts of variations or concepts or points of view and so on so um i think you know kind of latent graph inference could be",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3657.058,
    "end": 3677.288,
    "text": " uh you know quite an interesting way maybe to address some of these um issues where we were discussing although i think uh it kind of died off a bit in in the recent year at least as far as i've seen there were a few you know slightly fewer papers on the topic well certainly the agents",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3679.053,
    "end": 3687.056,
    "text": " proposed latent structure of the world, the causal structure of the world, is just mapped on a territory, and so it enables maybe some of those coarse grainings.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3688.156,
    "end": 3693.178,
    "text": "Could you go to the slide where there was a mapping between a smooth sphere and then a regular geometric shape?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3695.559,
    "end": 3696.159,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3697.499,
    "end": 3698.039,
    "text": "Let's see.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3699.36,
    "end": 3699.74,
    "text": "In this one?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3700.475,
    "end": 3715.087,
    "text": " yeah just wanted to make one one point see if you had any um comments at the heart of some of the relationships that you're describing and where you pulled back to in kind in terms of generalization helps us understand",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3716.294,
    "end": 3744.567,
    "text": " this relationship between the the sphere and the geometry and one place that that and the implications for data processing and all of the computational science areas is if you are preserving or learning or analyzing geometry but not topology or the other way around you might get these different like data set aberrations like you might have the topology of the coffee cup but it looks like something totally different",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3745.347,
    "end": 3771.494,
    "text": " And so what we would really want to do would be understand the relationship between geometry and topology, because if we could understand it in principle, like you have it on the left side, and then in practice with the data scheme on the right side, or insert your own left and right side there, then we'd be able to do data analysis in a way that respected slash preserved",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3772.878,
    "end": 3796.357,
    "text": " both the topology and the geometry so it's like two compatible perspectives that have their different like strengths and weaknesses and heuristics and so understanding that relationship between geometry and topology and the implicit um spaces that geometry requires and so on that has tremendous",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3797.854,
    "end": 3825.477,
    "text": " use and it just in closing reminds me of buckminster fuller's synergetics which uses a close packing architecture and a tetrahedron centric model of coordinates to find more continuity between surface area and volume and between the smooth surfaces and the great circles on them and like the points of connectivity on shapes so i think it's an incredibly deep area and really has fundamental",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3826.812,
    "end": 3853.36,
    "text": " impacting active inference helps us think about our models in this way kind of like the inflated balloon and with the fuzziness and the architecture and the finiteness it really brings a lot to active inference and so i appreciate you sharing the work with us today and um continuing to work in this way thanks out any last thoughts",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3854.636,
    "end": 3864.059,
    "text": " Yeah, I think what you mentioned, I think it's been all over my thesis, this kind of tension between topology and geometry.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3864.299,
    "end": 3873.042,
    "text": "And maybe what I want to emphasize is that I'm not saying the previous perspective of looking maybe more geometrically at things was wrong in any way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3873.662,
    "end": 3879.144,
    "text": "And on the contrary, actually, there's lots of interesting places where these things intersect, even in this shift",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3880.044,
    "end": 3905.824,
    "text": " uh paper i briefly went through like if you actually read the paper there's uh there's a lot of beautiful intersections actually my main collaborator uh francesco he he's a differential geometer so um he actually had lots of kind of inputs from from that side and um indeed i think you know we should try to use all these kind of layers of structure uh you know in the best way possible uh for all our methods",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3907.58,
    "end": 3907.842,
    "text": " Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3908.063,
    "end": 3908.325,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3908.707,
    "end": 3909.029,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3909.793,
    "end": 3910.296,
    "text": "Till next time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3910.336,
    "end": 3911.382,
    "text": "Thanks for having me.",
    "speaker": "SPEAKER_01"
  }
]
SPEAKER_00:
Hello, welcome everyone.

It is May 30th, 2025.

We're in Active Inference Math Stream 14.1.

and here with jesse van oostrom discussing a concise mathematical description of active inference in discrete time this will be a very useful and epistemic discussion so jesse thank you for writing this paper and coming on to you to introduce it and start walking us through it yeah thank you daniel thank you for the opportunity of uh


SPEAKER_01:
being here and allowing me to present my paper yeah first of all i wrote this paper together with my colleague carlotta langer and my supervisor niad i we are all in hamburg at the institute of data science foundations

And, um, yeah, about three years ago, I, um, started getting interested in active inference and, um, wanted to know really exactly, um, how it works, so to say, but this, uh, I had quite a hard time figuring this out and that's a bit why this paper came about.

And maybe I can, I also have some notes here.

So maybe I can make very specific my question.

So what I wanted to know was, okay, we have an agent who has observations O1 up to OT and has performed actions A1 up to AT minus one.

and then what i wanted to know was an agent acting according to active inference what will its action at how will it choose that and this question for me was so with the literature at the time i found it hard to really pin it down so to say so that was the first main objective of making that more clear

and especially also making it clear in a way that is with in notation that is very accessible to people who have studied probability theory or any other type of standard mathematics and yeah and then throughout the process a second question came up

so that was the following so so i will go into more detail on this later but an agent has something that's called a generative model and how to understand that is as follows so the world we can first look at the real world

And that is generating observations, for example, observation one world is in world state one, then observation two.

So this is all happening in the world.

But then an agent.

So this is what we could call a generative process.

And a generative model.

is as follows so this this is some kind of where an agent has maybe some kind of simplified version of what's happening in the world it is representing in its brain and from that world from that some kind of simplified world state it can predict okay what kind of observations can i expect

um from this state and this is some kind of a very similar model and in both cases we can also actually include actions so if it acts on the world it will influence its next world some kind of simplified world state and the same also here it will also influence the real world state so this is called a generative model

and this generative model it uses it to to choose its next action but then the second question is how does it learn how does it learn this model

so this paper is basically we can basically divide it into two parts one part is called inference and the other part so this is we could say also action selection and another part is called learning regenerative model

so this presentation will also be somehow divided into these two parts so i'll first go further into this first question how does the agent select its actions and then also the second later on i will go on to in the second part how this is learning the generative model and please if anything is unclear please feel free to ask questions

If you're now in the live chat or if you're watching this later, you're also always welcome to send me an email or get in touch in another way.

If you have any questions, I'm really happy.

So I wrote this paper for people in my situation three years ago who

really want to know the exact mathematical details of how active inference is formulated and want some kind of an accessible easy overview and if you have any suggestions for improvement i'm also very open to hear them so yeah so that's the basic overview and what i have to say as well is that this is all as you saw also from the symbols and stuff

This is all happening in a discrete space, discrete observation space, action space and discrete time.

So there's a whole other literature on the continuous side of active inference.

I'm not going into that right now.

So that's also not what this paper is about.

It's only about discrete time formulation.

um yeah i'm not sure if we already should stop here for some questions or did i just continue going deeper into the yep continue thank you okay yeah so maybe so one point maybe i should spend a little bit more time on it is this idea of the generative model so um maybe i write a new one here

so what i think is important to emphasize that this these states s are really inside the brain so we could say this is really in the brain of the agent and from these states it's predicting

observations.

And usually this is some kind of simplified state of the world.

and it needs to learn these states without anyone telling it, okay.

So maybe in machine learning or something, you're familiar with supervised learning where you get an observation and then it might learn which state do I have to associate to it?

That's not the case here.

So the agent really needs to learn itself.

Okay.

What is a good state?

to somehow encode this observation in so that I can do a useful planning.

So this is you could call unsupervised learning.

And there is no way of saying, OK, the real world, for example, is generating observations and therefore this state should be somehow related to that the real world state.

Yeah, and to also maybe

uh make a bit more clear how it can be simplified so for example we could have an agent so we could have a world in color for example that like there's all different colors in this world but for example an agent that can only see light intensity but no color

We'll have also a simplified state of the world, so to say.

So this will only be black.

We don't have gray, I see, but this is white and this is gray.

So that's just to, um, just to emphasize this point.

Okay.

This, uh, generative model is really something that's inside the brain of the agent and it can use it to, for example, plan ahead.

Um, yeah, then I'll go back to the paper.

so i will start with some notation so yeah so what we are considering is an agent which which is going through

capital T time step.

So it starts at time one, and has also some kind of finite time t. And it's performing actions the whole time and receiving observations.

so that we can also see here so we have observations and i'm using tau the greek letter tau for any time point and t for the current time point so you'll see that appearing more often this distinction in the in the paper and we can use this subscript with the colon to have like a sequence of observations for example

And then very important, there's this word policy.

And actually, if you come from reinforcement learning background, this has a little bit different meaning.

But here in this context, the policy is a sequence of actions, and more specifically, a sequence of future action.

So that is sometimes I find in the other literature not so clear.

But here I want to make that very specific.

So we have

a policy pi t and that is actions small t so there is also small t so action small t up to big t so a sequence of actions and pi will be then the whole from time point one and so usually i write a for actions that are already before performed in the past and pi for the future actions that it somehow needs to do planning on

um yeah and we have this generative model which we write like this and so it's a probability distribution over observations and states given actions and this theta is the parameter of the of the generative model

And in this first part, we leave this data, we leave it out in the notation because we are assuming, OK, it has learned this generative model and we're just interested in how it's using it.

And then in the second part, we will actually look at we will include it again to see how it's learning this.

yeah and then also a big part of active inference is of course that it needs to infer in what kind of state it is so what kind so it has an observation and it has this generative model and then it somehow needs to infer okay what kind of state could i be in and we use this that's also in general in the literature we use this

letter q so for example an agent receives this observation o1 and then wants to find out okay or express its belief in which state it thinks it is and that we write for example like this so this is the belief so this is a probability distribution over states

and it has as information this first observation and in general that's always or we hope that it's approximately equal to the true posterior of the generative model we will go into this later some more and how it's sometimes exact and sometimes not but for now it's good to remember that this q is expressing a belief

about the state that the agent it's in and we sometimes so for example if we uh i can also show that here in the paper so so this is the the actual generative model here and here we have this belief over states

and what we are doing to make the notation a little bit more concise is to write qt for example qt of st let's say then this t is implying all the information that it has already from the past so this is

we can also write it qst given all the observations it has made and all the actions that it has performed so this t is just saying okay we include in this belief all the information all the observations all the actions to form this belief so to say um

yeah so i think i have discussed now the most important things from the notation then we can i can have a break here also if there's any questions or otherwise i just continue sounds good continue thank you okay yeah

So then we immediately, so that was also one of my goals of this paper, is to immediately get down to this main question, okay, which action is the agent going to perform?

And we immediately arrived there already.

So the action selection procedure can be described as follows.

So we have here a probability distribution

over policies so policies were sequences of future actions and we say according to active inference an agent selects its next action by sampling a policy pi t from this distribution and selecting the action 80 corresponding to that policy

so i can write it here again so we have this distribution for now we can ignore a little bit um how exactly what exactly this um how this distribution comes about i will get to that in a minute but we can just say okay there is some kind of distribution

over policies over future action sequences and we sample some kind of pi t star we sample it from this distribution so we have this pi t star which is a sequence of a t star up to a t star and then we are performing

this action 80 star and yeah you could say forget about the rest so this is all right i'll do it like this this is somehow uh we are not interested anymore so we are only performing this action 80 star and then we go to the next time step get the next observation and then we'll do this whole process again

So this, for example, this action a t plus one is somehow discarded.

So for the next time step, we'll do the process again and then take the first action from that sampled policy.

And that in a very basic way is already some kind of answer to this first question.

Okay, how is it selecting its next action?

and what we see is so it has received these observations and performed these actions and they also flow into this this probability distribution is dependent on this on these observations and actions and then the next question is okay how does it compute this probability distribution so

That would be the second question to answer if we really want to get down to how is it selecting its actions.

And then we see the main ingredient of this distribution is this function G, which is the expected free energy.

And I have written one way of writing it, written it here.


SPEAKER_00:
One question there, Jesse.

Yeah.

why is the policy conditioned on actions from the past when i don't see the a1 through t minus one explicitly entering into the unpacking and policy inference is based upon the observation coming in and how that affects the belief inference but how does or why do you have a conditioning on a as well


SPEAKER_01:
so the function g is dependent on a so in order to compute this function g we need to know which actions have we done in the past do we though yeah because it's but it's a bit hidden in the notation here so we use these past observations to perform inference

uh about for example here our belief in future observations so in this t here in the q it's it's captured okay this is a belief about future observations but with the information from observations from the past and actions from the past it's using those to form the beliefs about the future

or here for example the same for future states or here so in order to make this distribution what state will i be in in the next time step it is using some kind of belief about what state am i in in the current time step which then again depends on what actions and what observations


SPEAKER_00:
we have in the past okay so in in practice the b transition matrix embodies the expected consequences of different actions but here it's just written in terms of being conditioned upon the whole past sequence of observations and actions that like would have led to that transition matrix being learnt


SPEAKER_01:
yeah so here we are assuming that the whole um that whole b matrix is already learned so that we can just take as given

And we only use these O and this A for performing state inference, so to say.

So we need to know somehow in what state am I now?

And then you can go back and think, okay, what actions did I perform in the past?

And then that gives you more information about what state you could be in now.

And I wrote this very explicitly because often in other literature, this dependence is somehow completely not made clear.

And then I find that sometimes a bit confusing.

So that's why I wrote it very explicitly.

Okay, in order to compute this distribution, we need this information, so to say.

So that's why I wrote it in this way.

Does that answer your question?


SPEAKER_00:
i think so i just still i just still don't i know it's sort of a small thing but and i i see that that's it's that you've thought about it it's just i don't see any of the past time steps in the right hand side so why would the action have that you took 27 time steps ago

Other than how it influences how you believe the action to work now, why would the specific memory of the action a given number of time steps ago bear into the calculation now?


SPEAKER_01:
For example, if we take this distribution, which is our belief in what state we will be in in the next time step,

then I think there's many situations where we can think of that it's relevant what I did two time steps before.

So for example, if two hours ago I decided to fly to Germany, then that will still have influence on where I will be maybe the next hour.


SPEAKER_00:
But like in a grid world example, if the transition matrix is the up, down, left, right, the fact that you moved a certain way 25 time steps ago doesn't need to specifically come into play when you're thinking about the next consequences of your action.


SPEAKER_01:
So let's say we have a grid world, which is just 1D, let's say.

1D grid world.

You can go left or right.

Those are the two actions.

And let's say this agent knows I'm always starting in location zero.

And...

Let's say it or let's say differently here.

Let's say here is zero.

That's a bit easier.

And then every action you did in the past will have an influence on in what location you're in now.

Right.

If I, for example, we are now at T equals three.

then your location will be different if A1 is left compared to if A1 is right.


SPEAKER_00:
But is it conditioned upon your current location and the consequences of action from your current location only?

Or is it specifically conditioned upon an action that you took in a past moment other than how that action resulted in you getting to where you are now?


SPEAKER_01:
Ah, maybe I, sorry, that took maybe very long.

So these actions are really the actions we performed in the past.

So not any random actions, but really only the one that we performed in the past.

Is that your question or?


SPEAKER_00:
We can continue.

I see the past actions matter for changing where you are, and you can do other learnings and so on, but the past actions themselves don't influence where you are or your beliefs about the consequences of action, which are all that come into play in the operational calculation of G.


SPEAKER_01:
Which doesn't specifically invoke a past.

We can maybe postpone it to later to see if we can make it more clear.

So maybe to get back to the... So we wanted to know, okay, what is this?

What will be the next action?

Then we saw, okay, there's this

distribution that we sample policies from and from there we take the first action and that will be our action and then we are now looking at okay what is this function g that is used for for computing this probability distribution

and that function g looks a little bit complicated so we can just now see what is all coming in to compute this this probability distribution so we see there is this this distributions q which are our beliefs about future states and future observations

So those are, and we will discuss later exactly how it can derive these distributions.

And they are all dependent on our policies or the next possible actions.

So for each different policy, we'll also have different beliefs about what observations we could get in the future.

So those are the main ingredients here.

And then we have this distribution PC, which is a preference distribution of the agent.

And this preference distribution, I also wrote it here.

It's completely distinct from the generative model P. So it has nothing to do.

So P is a certain distribution and then PC is a very different distribution.

So they have nothing to do with each other in that sense.

but it is a an ingredient for computing this expected free energy yeah maybe also nice to see so for example this uh

distribution also in the paper of Lenz da Costa and others.

We see it here in equation 10.

So he also describes it here and actually... We don't see it.


SPEAKER_00:
It's just a black bar.


SPEAKER_01:
Oh.

You do see the nodes.

Right?


SPEAKER_00:
No, it's just the tabs.


SPEAKER_01:
And now?


SPEAKER_00:
Yeah, still just the tabs.

Yeah, maybe reshare the screen.


SPEAKER_01:
Yeah.

Did something change?


SPEAKER_00:
Yep.

Okay, now we see the paper.


SPEAKER_01:
Okay, great.

Sorry about that.

And now you see the notes?


SPEAKER_00:
Yep.


SPEAKER_01:
And now you see the other paper?

There we go.

So this is the paper of Lenz da Costa and others.

And he has here the same distribution over policies.

And then actually in Thomas Parr's book and others,

There the distribution is a bit more complicated.

They also have an E and an F term.

And I'm writing in the paper, I was planning not to go into that now, but I'm writing in the paper here also a note on why they have those terms.

But then you can at least see, okay, you can find this distribution in these.

I use those two as main sources for writing this paper.

Yeah.

So maybe people who have already studied some active inference are more familiar with what comes now.

But I thought just to briefly discuss this expected free energy.

and how you can think of it so we have the equation here and we see that there are two terms and this first term is a KL divergence between two distributions and this first distribution is distribution over our beliefs in what state we will be in in the future

given that we have some observations that which observations are again appearing here and we compare this distribution so the beliefs about states given that we have already the observations with this distribution where we don't have the observations and so what you could say a little bit is

If these observations are very informative, then our belief about the state will be very different from when we don't have the observations.

So then this difference will be large.

But if these observations are not so informative, then these two distributions will be pretty similar and then this difference will be small.

So the size of this difference says, okay, how informative will these future observations be?

And these future observations depend again on the policy.

So we somehow take an average over what future observations do we expect, given that we have this as next actions and given that we already have some observations and actions done in the past.

So this first term is called epistemic value of information or information gain.

And then the second term, we see this preference distribution here come up.

So this is the observations that the agent would really like to make.

and the distribution here is the observations that it thinks it will make if it is going to do these actions and so um the higher this is basically the more likely this uh

policy will give observations that it actually wants to have so we want this and that we see also if we go back to this equation so we see the lower the expected free energy of a policy the more likely it is that we will sample that policy

And we also see that the higher these two terms are because of this minus sign, the lower the expected free energy will be.

So we want both this utility and the information gain.

We want it to be high for a specific policy or those are policies that we are likely the agent is likely going to sample.

so that is one way to think about the expected free energy and then another equivalent way of writing it is the following so here we actually don't have this minus sign so these terms we both want them to be small or the the smaller they are the more likely that policy is going to be sampled and this term here is an entropy

and it's basically looking at the entropy or you could also say uncertainty about the observation if the agent knows the state it's in so

if it's in a certain state but even though it knows what state it's in it is very hard for the agent to predict what the observation will be then this entropy will be high and then so therefore also this ambiguity will be high

And therefore the agent prefers to be in states that have low ambiguity.

So that's this first term.

and then the second term is again a kl divergence but this time between the belief that the agent has about its future observations given a certain policy compared to the observations it actually wants to have so if this difference is big then it believes it will get different observations

from the ones that it wants to have so that you could say is bad so this term which is called the expected complexity or risk those we wanted to be small so that's a little bit of interpretation on this

on this term, on this G that is used for computing the distribution for sampling the policies and therefore the actions.

So there is

One more thing is that we can, that is for the computation, nice, so we can make some kind of approximations.

We will come back to that later as well in some more detail.

And then this g, actually, we can write it as a sum of different, of like single time step g taus.

And that makes all the computations a bit easier.

and i know that there's much more to write or to say about this expected free energy and where it comes from but i think that was not the aim really of this paper so then i refer you to these other sources um yeah so now we have discussed this distribution where the policies are sampled from we have discussed this g but then an important ingredient

of this g of computing this g are these distributions q q t so this belief about future or about states and about observations and also in the actual often we are actually interested in these so not over sequence of states but just over one specific time point tau

And I was actually thinking for this podcast not to go into this too much because I think it's relatively straightforward.

So at least what I write here.

because I think the philosophy of this whole approach is that we want to use some kind of Bayesian approach as much as possible.

And what I have done here is just describe exact Bayesian inference, which would be somehow the simplest way of forming beliefs about states and observations.

And then actually in more advanced literature, so to say, you could say that is some kind of an approximation of this exact Bayesian inference.

So I would say you could read through this.

I can maybe show one small example.

So we have this generative model of our observations

and states given actions and you could also write this model in this way

and there we see that this model factorizes so maybe i can just use what i already wrote down so we see that we can really somehow decompose it into very simple elements where we have a prior over the first observation this one

um we have a distribution that is telling us okay if we are in a certain state and we perform a certain action what state will be the next one and we have a distribution that is saying okay if we are in a certain state what observation do we expect to get so those are the three ingredients

which are also often referred to as the maybe I write on this.

So this is called the usually called the B matrix.

This is called a matrix.

And this, I think usually a D vector or something is prior.

So those are the three ingredients for this generative model.

and then for example so how you would perform um exact patient inferences for example we say okay we re so we first can express our belief in what state we are in we can just use okay that's the prior which we assumed we already had and then we then get an observation

so that's why this that one is referring to and that we want in this case to be the exact posterior s1 given o1 which is proportional to o1 given s1 and our so this is this a matrix

and the prior, or our belief, let's say that's maybe more general, our belief about what state we were in before receiving this observation.

So this is a very simple example of belief updating, so to say, which I describe in more detail here in the paper.

and i think if you're just starting out with active inference i think it's quite nice to just think about this whole state inference in this way and think of these other approaches like minimizing some kind of variation of free energy as some kind of approximations to just doing this exact patient inference yeah and that basically concludes all the ingredients

we need for describing how an agent is performing its next actions or how it's choosing which next action it's taking and so i'm skipping now over this learning part we will get into that a little bit

And then here in section four, there's a completely worked out example.

So I really recommend, so you've might also seen this, uh, in, uh, for example, Thomas Parr's book.

Um, I've tried to make it very explicit here and very precise that you can really follow exactly what is happening.

And I invite you to just go through this example.

so this up till here is just some kind of definition of the model and then here we say okay how it's performing the inferences how it's making these cues and then here we see we compute the expected free energy for all different policies so this is the first action this is the second action

so we walk somehow through a an example episode of this mouse in this teammates which is acting according to active inference and then second time step it's updating its beliefs and then again computing expected free energy and then again selecting sampling a policy and its action so um

Yeah, so I invite you to go through that as well.

And what I found hard, so I looked at this

There is also so in order to compute, make these computations, I thought, OK, I really want to make the computation.

So I looked into this library by MDP and also a little bit at the other one SPM, I think, which is written in MATLAB.

But what I found hard was that in both code bases, the actual computations

of what's really going on are very deep down in the code, so to say, so are not so accessible.

And what I did was actually to write my own code to do these calculations, which is also available on GitHub.

So here on the first page is the GitHub link.

And I can and what I think is relatively nice from this code is that it's very so you see the code, right?

Yeah.

So that it's relatively easy to read.

So, for example, there's a function sample action, which is exactly and I have also tried to refer to all the equations in the paper.

So, for example, here, this is the first equation we've been talking about.

this distribution over policies and here you see exactly how it's computed so we have to compute for every policy the expected free energy and in order to have the expected free energy we need these beliefs over states and then we have to take this soft max function oh i did not actually say anything about so this

this sigma here that's maybe good to just clarify this is the softmax function which makes just these values so we get for every pi we get a different value makes these values into a probability distribution so for every pi we have a different expected free energy we make it into a probability distribution

and then we sample from it and then we take the first element basically of the policy which is our action and then we return the action so i think this code if you're just interested you can play around with it you can take some parts of it

you can do with it whatever you want but if maybe if the maths are not so clear for you but it's more clear in code then you can also have a look at this so it's has the same kind of

sequence as the paper so we sample an action how's the expected energy computed we need these two things info gain and utility and here we have a part about how it's updating its beliefs and that's exactly the same as what we what is described here in the state inference so this is a little bit different for example from the

by mdp and and spm where they use more advanced methods for doing this state inference but for easy examples i think this is really more than enough just to get a very precise idea of what is really happening and then there's also a functionality for learning which we will go into now

So I think that's everything I wanted to say, at least from my side, about the inference.

So maybe if there's any questions, I'm very happy to go into those now.


SPEAKER_00:
Okay.

I could maybe finish covering all the sections of the paper, and then there's a few questions in the live chat, and people can add some more.


SPEAKER_01:
Okay.

Yeah.

so for the second part learning um i was thinking to actually go a little bit more or to make it a bit more difficult so to say and really go into depth of where the equations come from

because for a long time that was actually also unclear to me and actually quite late in the process it became exactly clear of how where the the equations for learning come from and i think that's also a quite a relevant or significant contribution of this paper of this paper to present that

i thought it might be nice for the people who are interested also in where they come from to go in to a little bit more depth now so i would actually say that we go then to the appendix so there's here appendix b and we actually first look at a little bit simpler settings

to somehow get familiar with the basics and then we go back to this setting of an agent operating in a world um

yeah maybe i introduce it a little bit here in the notes so i'm slightly changing notation now to avoid confusion but maybe it also makes it a bit more complex i don't know so we have now x and z and you could basically think of x as an observation

and z as a latent state i call it so and maybe i just i think in most cases you can also think about this as the o

and this about the s so to say of the of the model but that doesn't always hold so that's why i chose to call it x and z and we first for now forget completely about this about this z and we imagine some kind of very simplistic setting

where we want to where we are receiving observations x and we want to somehow make a model of these received observations so we want to make some kind of probability distribution x checking which notation exactly i'm using yeah

so there's some different observations we can make one up to n and we want to model the observations we make with the probability distribution which is dependent on theta

and then very specifically so it is just the probability that we see an observation i given theta is theta i no so theta is a vector from theta 1 up to theta n

and the learning and then you could say learning is finding this theta and what the active inference literature is suggesting is to think of this theta

Also, so that is a very Bayesian kind of approach that we think of this theta also as something that we can have a belief about.

So we can have also a probability distribution over the theta, which is somehow expressing, okay, I believe my belief about the theta, that theta i is 0.1, for example, is this number.

and that is again so i'm going to use the bar here that is again dependent on some kind of hyper parameter alpha and this distribution is called the dirichlet distribution which is a product over the theta i's that we raise to the power

alpha minus one and so alpha is also in vector from alpha one to alpha n

So this distribution, and by the way, there's another paper from Ryan Smith.

I think it's also in the references, which also describes this, gives a little bit more intuition about this whole process, which I think is really nice.

So I can also recommend looking at that.

yeah but so this is basically describing our beliefs about our parameters and then just so we could say the prior belief and then we can for example we make a specific observation x star and then we can ask ourselves how is this

uh belief going to change if we incorporate this new information that we made a certain observation and that i'm not going to write down but we can look at here so what i just described was this so we have this distribution

parameterized and this belief about our thedas and then we are interested in this posterior belief so to say after having this made this observation x star and um what we can if you work that out you see that that's again a Dirichlet distribution but then uh

with a different alpha so we call the updated alpha we call it alpha prime and this this thing here is the indicator function

which is basically um which is one when x star is equal to xi and zero otherwise so what is happening is that we increase the alpha i by one which corresponds exactly to the observation to the eye of the observation and the rest of the alpha stays the same

And if we look again at this definition of the Dirichlet distribution, so we see the higher the alpha, the more weight that theta i is going to get in the probability distribution.

So this is a very, I would say, elegant way of updating your beliefs about the theta when you get new observations.

And this is the general idea of how the theta is also learned in the setting that we are looking at in this POMDP, this partially observed Markov decision process setting where we have states, so hidden states, observations and actions.

Yeah.

but so what we were looking at here was just a setting where we had just observations but we didn't have any latent states and now the question is what happens when we do include latent states so we can do a similar derivation so we now have two distributions

uh depending parameterized by two different thetas so we have a prior which is a vector over the latent states and then we have some kind of matrix which is saying okay given we have a certain latent state what's the probability of a certain observation and then we can again ask ourselves this question

what happens with our belief over theta if we get an extra observation and what is actually the case here is that we get some kind of very complicated distribution which is not a Dirichlet distribution so this exact method

is not so useful for updating.

So this is more like the setting we we are looking at right where we have observations and latent states.

So we have to do something slightly different.

And that is that we do by the mean field approximation.

And

so what we do instead is we um let's see what how i can best continue so maybe

I go very briefly to Appendix A. So in Appendix A, I describe some kind of very general, so I think most of you have already seen this, but just for the completeness, a variational free energy minimization approach.

So

it's described here so we have some kind of joint distribution and we want to compute a posterior which is also the setting we had before but often this posterior is hard to compute directly so what we do then is we instead

choose a family of distributions q and find so this q tildes are all element of this q and we try to find that q tilde that is minimizing this expression and that q q we call qx because it's actually dependent

on this x on the observation that we made and this is some kind of very general approach which is called the variational approach and that is somehow used to approximate a posterior that we can't compute directly

so that is described in more detail here in this appendix and also how it plays a role in active inference i think for time reasons i won't go into that much deeper now but just to describe this method at least so we use that same method also here so right now we are dealing with latent states

So these Z's that we don't know, and we are dealing with the parameters that we don't know.

So we somehow, but what we do know is the observation and the hyperparameters.

And we want to find the distribution over the latents and the parameters.

And then we use the same variation of free energy minimization approach.

where we are trying to find the queue that is minimizing this distribution and then what we do what i was also saying before often we restrict the queues that we can choose from which is and in this case we are only looking at queues which can be written as a product of these individual distributions

So that's some kind of an approximation that we are making here, which is called the mean field approximation.

And if we do that, then this equation simplifies a bit.

and what we also do so this is a very important insight is that it's quite hard to minimize both of these cues at the same time because they are both influencing each other and there is a specific algorithm called the kaffee algorithm coordinate ascent variational inference algorithm which is saying okay instead of trying to find

both at the same time we start by fixing one and then optimizing the other and then use that so for example we start by fixing this q theta and then find q z and then use this newly found q z to find in q theta and then you can do that iteratively and hopefully get to the optimizer

And that is also exactly the approach that is used for learning in the active inference.

So very specifically, we first have some kind of prior belief about our generative model, about the parameters of the generative model.

So we fix that belief, so to say, or those parameters.

and with those parameters with the generative model we can do inference like we looked at before like this bayesian bayesian inference and that's also what's done here so we get some kind of

So this is a different way of describing inference in terms of variation of free energy.

But what's basically what's done here is we are just trying to infer what kind of latent state are we.

And we have fixed the parameters Q theta.

so and then we have this qz so this belief about the state we are in and then we are somehow where we want to be we want to to find this distribution q theta and then we use this this fixed qz to then optimize this distribution which is only dependent somehow on q theta and the qz is fixed

and if you would really want to find the minimizer you would continuously be doing this alternating these two steps and what is very important to note here is that the active inference literature is saying our update is only doing the first step so we only do the first step we

do one time state inference and one time we optimize this parameter, I believe, on the parameter given this state inference, and then we stop.

So it's not exactly the same as the CAFI algorithm, but you could say it's only the first step of the CAFI algorithm.

And so

here i'm describing what does that minimizer then look like and that is given by this basically the exponent of the term that's here so you can also see that if we take the exponent and we take the log we just get what's there and then it's minus what's there so that is the minimizer so to say and i'm using these terms c with the backslash to say

okay these are all terms independent of the argument so we don't have to worry about them for the minimization so here we have a very specific form of the updated belief in our parameter

and then we can use the fact that we are actually working with this this uh deary slave priors and this categorical model and then we can so that p here that was defined here so in terms of these thetas and in terms of these theory slays and what you see then is that this uh

updated q after the first step in this algorithm looks like this and this is exactly again a dirichlet distribution so that's a really nice result so i didn't make any assumption that it has to be a dirichlet distribution i only said okay we do this mean field approximation but it comes out

that it is, if you only do the mean field approximation, that this first step of the CAFI algorithm is again a Dirichlet distribution.

So we can formulate the update again in a very elegant way like this.

So we just add something to these two updated alphas.

And what we add exactly is the following.

so for example this alpha d is related to the theta d which is um which is the belief about the prior belief in the latent state and we add um we add the belief

after doing the inference that we are in that state so this is in some sense a very similar update to this update here where we are just adding one for the observation that we did but now we are not adding one for observation but we adding only the belief that we are in that state to

to the to the original alpha and something similar here where we add the belief in the transition more or less so um

Yeah, I can imagine that this was a little bit fast and maybe a little bit abstract because there were not not so many examples, but I felt this for the people who really want to know, OK, where do these equations come from?

I think this is quite a quite a relevant contribution.

And so what I was talking about now was only this X's and Z's.

The notes are still there, right?

Yeah, great.

And now we want to translate this to this setting where we are actually having this S and the A's and the O's.

So then the equations get even a little bit more complicated.

So I thought it was good to have this build up to first see, okay, what is the general principle?

This mean field approximation and the

Dirichlet coming out and then you can apply exactly the same procedure to this more complicated setting where we have this full generative model and then you get this derivation here which I don't I'm not going into now but there we see again at the end that we have a Dirichlet distribution and here you see okay alpha we have to add this to the alpha

we have to add this to the alpha a we have to add this to the alpha b and then we get so this is the the main part of the paper we have these updates here for the learning

so yeah so for the people interested in knowing where do these learning rules come from there is this appendix b which i went through now a little bit quickly and maybe a little bit high level but

yeah you can look at that in more detail or ask me questions about it if you are interested in this and maybe to just link it to the other literature so here there's this is the book of thomas parr there's this appendix b23 learning and here he is coming to the same

equations but using a bit different method that i found hard to follow so yeah but these these update rules they are also here and the same also in this paper let's see if the reference

Yeah.

So here we also have these updates, but also using a different method than the one I'm using.

And actually this method that I am presenting is referred to me by Connor Heinz.

So thanks very much for pointing me to this derivation, because I think this for me at least is the most clear one I have seen so far.

yeah so so far about learning so there's one i would be able to if their time allows could go briefly over the appendix a but maybe we also first take some time to for questions about this learning part and i'm also fine with not discussing this appendix a


SPEAKER_00:
Awesome.

Okay.

I will ask some questions that were submitted by Arun, and then if anyone else has any questions, they can go for it.

Okay.

First question.

Do you intend the accompanying repository to be a static accompaniment to the paper, or do you see it evolving, for example, adding habitual policy selection to the agent via eTensor?


SPEAKER_01:
I would say from my side, I'm not really planning on developing it further, but if people want to, I am very happy, for example, if they fork the repository, copy it,

in any way would be nice if you reference me if you still use it but um yeah so from my side i don't see any further developments but i would be very happy if other people want to actively start using it so please get in touch if you want to cool okay would it be possible to further document the flattening class at first with comments but ideally with unit tests

yeah so that is indeed so uh maybe a good point to make about the the code so um in pi mdp uh there is some i have that also here in appendix c i think and some yeah there is some extra um kind of structure being put

on, for example, these conditional probability distributions.

So, for example, we say the location in the next time step is independent of the reward condition in the previous time step.

And that is for the computations, it's very nice because the computations get a bit easier.

but the code gets a lot more complicated and you don't see especially because for example here if we look at all these for example get believe next states

Yeah, I think that's a good example.

Update believe current state, something like this.

For example, this one.

um this is very simple and elegant i would say and that is because i am ignoring that so to say in the code that that there exists these independencies and i did that on purpose to keep the code as simple as possible but what is then a bit harder is that so um

So what PyMDP is using is having some kind of state which is somehow multi-dimensional and then also for example

the a matrix let's say which is also has is i think some kind of list so in the observation state so there's observation modality state factor so these are both have can have multiple elements so this is a list of matrices and these matrices

so for each modalities let's say each modality m and this is then again some kind of tensor where we have here the observations for that specific modality but then we still have here also

for every state factor a different dimension so to say and in my case what I am doing is just saying okay we just say that we the state space is let's say one dimensional

And we just have S1, 1, S1, 2.

Let's say there's two state factors.

And I'm just putting them all into a long list.

So to say, so that's what I call flattening.

And then this A matrix is just one matrix where there's all the flattened states here.

Let's say S flat.

And here O flat.

that makes the the code very easy but there i do need something to translate between the pi mdp states which are somehow have these different state factors and the flat states that i am using and that is indeed i agree is not uh the best documented code so i i wrote that and then i

tucked it away in this flattening class and was hoping that everyone would believe me that it works well.

But we could talk about it because I do think it's a good point that it would be nice if it has like if you don't just have to believe me that it works, but that you can actually check it yourself.

So maybe we can you can send me an email and we can discuss how to best


SPEAKER_00:
to best document it yeah lots of uh little challenges with matrix format and yeah small things that uh yeah definitely okay i'll ask aaron's last last uh question here and this is kind of a good uh final discussion piece which is

What do you see as the next key area to tackle in terms of increasing the clarity of how active inference works?


SPEAKER_01:
Yeah, that's an interesting question.

So I felt that with this paper, most of the questions I had were answered.

I'm now looking a bit together with Lance Da Costa also at how well, for example, these learning rules in practice are actually performing.

But that is maybe different from, so I feel at least in my opinion in the paper, it's clear how they are formulated.

So I would say, yeah, I would be open for suggestions.

So I have answered with this paper most of the questions at least I had about how active inference works and then of course about this very specific domain of the

discrete time domain yeah what what did you uh think or have any conversations about how this relates to the continuous and the path-based versions so i have actually only spent time with the discrete setting so um i was i think in the beginning very optimistic i thought okay first understand the discrete and then the continuous

But then I first spent time on understanding it, and then I thought, okay, let me write this paper on it, which also took quite some time.

So I actually have no experience with the continuous case.

I'm not sure.

Lance is not here, right?

No.

Otherwise, we could ask him.

but he is more familiar with uh so i think that's more a question for him maybe he said some way able to answer that at some point cool what are your next steps or how are you gonna move forward having satisfyingly reduced your uncertainty about discrete states

Yeah.

So I am very interested in general in this, uh, we could maybe call it AB learning, uh, domain.

So how is an agent actually learning these, uh, matrices?

So, uh, together with Carlotta, with my colleague, we were also applying this framework to study, uh, consciousness or are still busy with that and a paper about it, hopefully coming out soon.

where we use an active inference agent and then look at how its integrated information is developing and but we saw actually in the environment that we were using that it was very hard for the agent to learn the a and b matrices or learn an a and b matrices that allows it

to operate well in the environment so for me that's the next big question is how what is actually are these learning rules from active inference the right ones or are there other rules that are maybe more effective and maybe more probable so that is for me a big question that i completely don't have an answer to right now but this really i find really fascinating


SPEAKER_00:
Awesome.


SPEAKER_01:
Any other comments you'd like to make?

So I think it's good to finish it here, but just to say that it's there is, there's also this Appendix A, so maybe a bit too weirdly, but I think all of free energy and active inference usually is formulated as a minimization of this variation of free energy.

But starting the story from there, I find then it gets very confusing most of the time.

But I do write here in Appendix A how everything that's written in this paper can also be related to

to this bigger idea of minimizing a variation of free energy function so you can just read through it here perception learning and action selection how they are all related to minimizing this function just so you know that it's there


SPEAKER_00:
Awesome.

Well, again, thank you for the importance and pedagogical work.

I look forward to many in this genre.


SPEAKER_01:
Thank you so much for the opportunity.

And again, if anyone is has any questions or wants to reach out, please feel free to do so.


SPEAKER_00:
Awesome.

Thank you.


SPEAKER_01:
Thank you.
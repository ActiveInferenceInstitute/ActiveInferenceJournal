SPEAKER_00:
hello and welcome everyone it's february 2nd 2024 and we are returning for active math stream 8.2 with richard servijan introduction to bayesian mechanics free energy principle and the paths based formalism thank you richard for joining for this part two looking forward to your presentation and then discussion


SPEAKER_01:
I have a weird echo.

Wait.


SPEAKER_00:
Are you in general synchrony with yourself on YouTube?


SPEAKER_01:
Wait.

Let me see.

Okay.

Perfect.

Sorry.

Okay.

Great.

Go for it.

Hi everyone and thank you again, Daniel.

So today I'm going to present the path-based formulation of the free energy principle.

And I'm going to start by a brief reminder of a couple of things we discussed last time.

However, if you're not acquainted at all with the notions of synchronization map, of variational inference, or Langevin equations, it can be a good idea to watch the first video.

And if you do so, please check the comment I posted below where I precise and correct a few things.

So a quick reminder of where we started and where we

so basically if you consider such sparsely coupled random dynamical system where mu corresponds to the internal states eta to the external states and a and s the blanket states to respectively the active and sensory states

And they form together the so-called Markov blankets.

And just a bit more vocabulary, A and mu form together the so-called autonomous states.

and finally if you add the sensory state to the autonomous state so you consider the wall mark of blanket plus the internal states you end up with the so-called particular states pi and the particular states defines or constitute a particle an agent bacteria in my schematic for instance so if you consider such sparsely coupled random dynamical system

You can interpret the dynamics of the autonomous states in terms of perceptual and active inference.

More precisely, you can see here in the bottom of my slide that the autonomous mode minimized a free energy function, which is defined under a generative model, which is typically the nest density, the non-equilibrium steady state density of your system.

so more precisely the internal states mu parameterize the density q mu over external states and if you look at the path of the internal mode it is so that the the pre-energy associated with your um variational density is always minimized and this license is an interpretation in terms of bayesian inference and about action because you can see

here that the active mode is also a minimizer of the free energy in fact it minimizes the surprise term within free energy so that you can see the particle as actively sampling unsurprising or preferred sensory states sensory inputs which is active inference and yeah if you remember well the last video

we saw that the generative model encodes the preferences of your system.

So that was basically where we started and where we ended last time.

And today, I want to relax a couple of hypotheses.

So first of all, no steady state is assumed.

We could be at steady states.

We could have a well-defined

nest density uh but we do no we don't do any assumptions here there is no steady state assumed the second so it extends in a way the scope of the free energy principle

The second thing we do is to relax the white noise assumption.

This means that we don't deal anymore with infinitely rough fluctuations, but we have fluctuations which are smooth.

So basically, we deal with colored or correlated noise.

and this means that the fluctuations become differentiable up to a certain order so basically instead of dealing just with x and x prime let's say you have x prime prime and so on and so forth you have higher dynamical orders if you will note that this vector x here which precisely correspond to x x prime and so on is called the generalized states

And a shifted vector, this one, where the first component is x prime, we refer to it as the generalized motion.

And we also say that we work in so-called generalized coordinates of motion, not to be confused with generalized coordinates in analytical mechanics.

so a lot of words but we just we are just saying that the fluctuations are smooth now and we can differentiate uh these fluctuations up to a certain order and this order is the order of generalized motion and it tells you how large the the correlation uh length of the fluctuations is basically

I also want to introduce the notion of generalized state space.

So basically, you can augment state space with generalized motion.

So instead of just looking at x, the state of your system, you look at x prime, x prime prime, et cetera.

And so basically, we consider the so-called generalized state space.

okay so um a lot once again a lot of words just to say that fluctuations are smooth now so if you look at the equation below it seems that there there is dots and primes here which and it's it seems a bit redundant so in fact what we call x dot here you can view it as the actual time derivative that you can evaluate anywhere for instance at t equal to

And the x prime here is just the second component of the momentary generalized states of your system, for instance, at t equal to.

So that if you evaluate your time derivative at t equal to, it equates indeed x prime.

But the distinction between the two will become more clear later in the context of generalized filtering.

So the cool thing here is that if you perform a Taylor expansion of the state of your system,

So you have an equation like that.

You can see that generalized states constitute the coefficients of your expansion.

Indeed, you can see in this formula here that you have x, x prime, x prime prime, etc.

So basically, you do a Taylor expansion around equal to, basically, and the coefficients of your expansions are generalized states.

So in a way, you can see generalized states are as encoding a path, as encoding the future of the state of your system thanks to this Taylor expansion.

And that's basically what I wrote here.

A point in generalized state space corresponds to a path.

That's exactly what I just said in virtue of this Taylor expansion here.

So a consequence of that

is that if you look at the surprise over generalized state, the negative log p of vector x here, and we call it the generalized Lagrangian, it plays the role of an action scoring the likelihood of a local path over the correlation length of fluctuations.

So let me break down a bit this idea.

Once again, here, the

the order of generalized motion depends on how smooth the noise is.

So basically, when you do this Taylor expansion, the timescale considered here

corresponds to the correlation length of your fluctuations.

The more correlated your noise is, the more dynamical orders you have in a way, the larger the path encoded by generalized states.

And the corresponding surprise, the corresponding generalized Lagrangian, negative log p of vector x,

is a quantity that scores the likelihood of this path in virtue of the correspondence between points in generalized state space and paths so a last remark i want to to do in order to be sure that everything is clear here is the following you might be a bit confused especially if you have a physics background

uh because lagrangians and actions are two different things so let's take a let's take a step back and ask i have a path a trajectory in state space if you will and i want to compute a probability density associated with a path how should i do so the first thing to do is to discretize time

So basically, you discretize your Langevin equation.

So for the people acquainted with stochastic calculus, just notice that it means choosing your alpha discretization, either Ito or Satonovich, let's say.

And from your discretized Langevin equation, you define an infinitesimal propagator, just like for path integrals in quantum mechanics.

An infinitesimal propagator is just a transition density between one state to the next during a time window dt.

then you multiply the infinitesimal propagators and then you go to the continuous time limits and so in a nutshell that how you proceed to compute to write down a density associated with a path and if you do that for instance in the simple case of a winner process where the noise is a gaussian white noise

you end up with the density of a path being proportional to the exponential of minus something.

And this something, we call it the action.

So the smaller the action, the more likely the corresponding path.

And this action, in fact, corresponds to an integral over your path of something, and we call this something a Lagrangian.

So the Lagrangian is defined at any point in time while the action characterizes the path as a whole thing.

So conceptually speaking, Lagrangians and actions are two very different things.

But here, the key move is to

uh kind of leverage the idea that a point in generalized state space encodes a path so that the surprise over generalized states this negative log p of x which is defined as a generalized lagrangian plays a role indeed of an action once again scoring the likelihood of a local path over the correlation length of fluctuations so that's really

the core idea underlying the path-based formulation of the free energy principle so basically now that we have introduced these ideas let's look at the generalized lagrangian over autonomous states so i just want to point out that throughout the presentation

in virtue of this correspondence between points in generalized states and path i will um i will call generalized states path for instance here i would say the lagrangian of autonomous path i use these words in a interchangeable way and similarly

when i i'm dealing with an autonomous path minimizing a generalized lagrangian i will be talking of uh the the path of least action because lagrangians play the role of an action so be aware of this terminology so if i'm looking at this generalized lagrangian of autonomous path

so negative log p of alpha the cool thing here is that if i remove fluctuations noise on particular states or path so basically the particle respond deterministically to its environment i can rewrite this lagrangian like that and this formula coincides with unexpected free energy

so what this formula is all about well we can reorganize it and if we do so you can for instance write it that way so what it means is basically that the path minimizing this lagrangian so the most likely autonomous path average over all possible sensory paths has to minimize these two terms

And these two terms are very interesting because the first one, if you look at it, is it is an expected Lagrangian over sensory path.

So minimizing this Lagrangian means this expected Lagrangian means following an autonomous path which yields unsurprising or preferred sensory path.

And therefore, it can be viewed indeed as an expected cost you want to minimize.

and if you minimize also the the second term this negative expected information gain you can see that it is just an expected kyle divergence between two density and these two densities the difference between these two densities is just that the first one here is conditioned upon s so basically these two densities are

different if the sensory path is informative.

So maximizing this expected information gain means following an autonomous path, yielding an informative sensory path.

For instance, let's say you are in a dark room where there is an ambiguous mapping between the hidden external states and the sensory stimuli, let's say.

you can turn on the lights and then this action would yield informative sensory inputs.

So that's the idea underlying the maximization of this expected information gain.

so this quantity is very rich and in fact you can do connections and links with many established ideas for instance you can speak of um optimal bayesian decisions and optimal bayesian design or of pragmatic value and epistemic value etc the whole idea here is that this quantity entails the

preference-seeking imperatives of the particle, that's the first term, and the information-seeking imperatives of the particle, that's the second term.

So the path minimizing this Lagrangian kind of constitutes the best direction of travel, the optimal direction of travel for the particle, so that you can view the particle as constantly engaging in an optimal behavior.

Okay, so now

Having said that, I want to go back to the whole idea of synchronization map we talked about last week.

So in fact, in this setting, everything is exactly the same.

We just augmented state space, if you will.

But you have mu parametrizing the density.

Everything is the same.

And if we consider the internal path of least action, so the internal path minimizing this Lagrangian here,

the corresponding parameterized density over external path coincide with the density over external path given sensory path so that if you write down the corresponding free energy you have the first term which vanishes in virtue of this above equality here and free energy reduces to surprise if you will to this lagrangian over particular path

i'm going to so before using this to derive directly the free energy principle i need to briefly introduce the notion of generalized filtering so

In a nutshell, let's say that you have some data.

You have the vector s here, a sensory path, let's say.

And you want to compute negative log p of s. That's the quantity you ultimately want to evaluate.

But it's intractable.

Let's say that computing this p of s would require a monstrous marginalization, and you can't directly compute this guy.

Instead, you define a proxy, you define an upper bound, this variational free energy, which is parametrized by mu, by vector mu.

And you just want to minimize this free energy so that it coincides with what you ultimately want to compute, namely, once again, negative log p of s. So in order to minimize, indeed, this free energy,

You just follow a recognition or filtering dynamics, which is, in fact, a gradient descent on free energy.

In fact, the exact dynamics at play here is this equation here.

So you could ask, but why don't we just have the gradient term here, nabla f?

Why do we have a second term here, this d mu?

By the way, about this d mu here, what is it?

Well, the matrix d here is just this matrix with ones here in this line.

So basically, if you have a vector x, let's say, x, x prime, x prime prime, and so on, and you want to get from this vector a shifted vector, x prime, the motion, if you will,

you just have to apply this matrix d here and on x and it gives you indeed x prime and so basically you can use this d mu here as being equal to mu prime basically and the idea is that

you when once you the gradient term here is minimized when free energy is minimized so we we look at the stationary solution of this equation if you will mu dot coincide with d mu and let me just explain why we don't just have the gradient term here if we only had the gradient term without the d mu here we would go down on free energy until a minimum is reached and basically we would have

the first component of the vector mu being non-zero but all the higher dynamical orders would be zero because we are at the minimum of free energy and we don't move anymore but here it is a whole vector mu which parametrize free energy and we don't want all the entries to be zero

And we want an equation which guarantees that, so that we add an additional term in the equation.

So that's basically, in a nutshell, what generalized filtering is all about.

And by the way, note that if you consider the path of least action of some process x, you can write it in a very similar fashion.

The only difference is that here,

the quantity minimized is just the the this background generalized lagrangian here whereas it was free energy in the context of generalized filtering and by the way we just saw before that if uh we that if we consider the path the internal path of least action

the corresponding free energy here associated with the density the internal path of heat action parameterize it reduces to the lagrangian so that when we write down the equations here verified by the autonomous path of least action you can

directly identify free energy to the Lagrangian.

And we basically have the same equations that in generalized filtering.

So basically, if we look at the autonomous path of least action, you can interpret the dynamics of the internal path of least action.

So in terms of Bayesian inference, which takes the form here of a Bayesian filtering scheme.

and note that if we remove fluctuations on particular states or paths that the definition of dealing with a conservative particle the autonomous path will coincide with the autonomous path of least action and note that ignoring fluctuations might be relevant when it comes to very large particles where you can so to very large particle you can cause grain

And you can check these papers here, where they try to average out fluctuations using a renormalization group approach.

So in the end, we find the free energy principle just as before.

But the difference is that here, the Bayesian inference takes the form of a Bayesian filtering scheme.

but otherwise we still end up with a variational principle accounting for the the for the perceptual and active inference that the particle do so okay now

i want to do a bit of zoology of typology let's say i said last time that this past coupling architecture here was a canonical sparse coping architecture but it was not a definitive feature of the free energy principle and that we could look at other sparse coupling architecture and i want

now to look at various sparse coupling architecture.

So the most simple ones you could imagine is one like that where you don't have active states.

So basically, the Markov blanket is only made of sensory states.

In the end, you can still write down the internal path of least action like that.

but it will the idea that the particle encodes beliefs about its external environment it will not manifest to an observer because by definition an observer has only access to the mark of blanket

and that's what this quote is all about when it says that whether internal paths of least action parameterized beliefs about external paths and therefore minimize operational free energy can only manifest via active states that is in active particles and of course no one would attribute any form of agency or a sentence to a simple piece of rock for instance

So if we move to the realm of active particles now, we basically have the equations we studied before.

But the difference here is this fast coupling architecture is that the active states do not directly interact with the external states.

and the sensory states do not directly interact with the internal states this sparse coupling architecture is quite interesting because it reminds i would say it reminds the the way a bacteria is coupled with its environment because you would have like the outer membrane with all the trans membrane proteins the receptors whatever which would correspond to the sensory states

and then you would have the underlying cortex filaments which mediate many many aspects of biotic actions so i liked i like very much this architecture here but we can even go further in the sophistication and consider the so-called strange particles the only move here is that

we don't have any arrow from s to eta.

But more importantly, we don't have any arrow from a to mu.

So the active states do not directly influence the internal states.

And this is interesting because it means that now the internal states alone are independent of both the active and the external states when conditioned upon the sensory state.

So from the point of view of the internal states, the active states become a latent cause, just like the external states of the sensory states.

So basically, everything is the same.

You can write down a variational density parameterized by the internal states or path.

But now, it is over both the external and the active path.

And so we have this free energy here.

We call it the generalized free energy G, but everything is the same.

It's just a variational free energy associated with our recognition density, which is over both external and active path.

So that in the end, we have this equation here where you have in the equation of the internal path of least action,

the gradient on um on g here so it's very interesting because it basically means that the internal path

in first in first and in fact cause uh the its own action um the the its own action which then cause its sensory uh inputs so and i'm quoting here the paper which introduced the notion of strange particles you can view the active particle the active path story as realizing the sensory consequences of the inferred action so basically

Such a particle kind of auto its own action, its own action.

And so the idea here is that the particle kind of infer its its own course of action.

Which will yield preferred sensory outcomes, so it's really a form of planning of inference, strange particles.

do planning as inference so that's that's uh quite cool and in fact we can even go go a bit further and assume a certain level of sparsity within internet states so you would have this mu1 here which influence mu2 but not the other way around so

From the point of view of the world internal states, everything is the same.

You have it parameterized altogether density Q mu.

And by the way, under a mean field approximation, you can write it that way, where you would have Q mu 1 and Q mu 2 here.

But the very cool thing here is that from the point of view of mu 1 alone,

well mu1 is independent of mu2 a and eta when conditioned upon s and you can view mu1 alone as parametrizing a density over mu2 a and eta and we call it that way with the over script m

so basically q mu 1 is a density or a belief about mu 2 and therefore about q mu 2 so that you can view this q mu 1 m as a metacognitive belief because it constitutes a belief about a belief

because it's a belief about q mu 2. so that just by assuming this simple architecture within internal states we end up with a minimal bayesian mechanics let's say of metacognition and it also introduces the notion of metacognitive particles so i think that's

uh what bayesian mechanics is all about meaning uh translating cognitive abilities in simple physical term it constitutes in a way a physics of cognition if you will so having said that

uh thank you very much and especially thanks to all these guys here which who helped me a lot uh understanding uh the free energy principle and especially the path-based formulation of the free energy principle thank you all right awesome great


SPEAKER_00:
a lot of ways I could start but it's so interesting that um how smoothly the path based continues on so maybe could you give a remark on just the timeline in the literature which one of these state and path base were developed just roughly like in which ordering was one first or how did that happen yeah


SPEAKER_01:
Yeah, so I'm not an expert on the history of the free energy principle, but I know that thinking in terms of path and in terms of generalized filtering has been around for forever, in fact, in the literature.

But about the most, I would say, definitive formulation in such terms,

I would say that the most important paper is, well, I have it here.

It's called Path Integrals.

Well, where it is, I can't see it.


SPEAKER_00:
Well.

Friston, Physics of Life Review, 2023B, Path Integrals, Particular Kinds.


SPEAKER_01:
Yeah, exactly.

This paper here, Path Integrals, Particular Kinds, and Strange Things,

which is also the paper which introduced all this typology of particles.

And it kind of really formulated the free energy principle that way.

So I think that's pretty much a very, very important paper.

And actually, the one which introduced the notion of metacognitive particles, the one of Lenz and Lars,

um this one towards the bayesian mechanics of metacognitive particles the last one it is actually a commentary a short commentary very easily readable of the path integral paper we just mentioned and also i would say that in the 2022 paper the free energy principle makes it simpler but not too simple

published in 2023, actually.

It mainly focused on the state-based formulation, but it also talked about paths and generalized states.

um so it it's also a very nice um very nice papers and it kind of derives the whole thing from scratch so i think this paper is also very very important and yeah i think basically in the last

two or three years there have been many papers uh reformulating the free energy principle or grounding it in in in solid math and so on so it's i think currently and let's say the the few years which followed from the 2019 monography of carl friston it has been very important years about when it comes specifically to bayesian mechanics and the actual physics underlying the free energy principle


SPEAKER_00:
Thank you.

Yeah, I'll just restate that.

I think there's a few great points to explore.

So even as early as the early 2000s, there was the notion that there was like physics of consciousness, physics of cognitive systems, a free energy principle for the brain.

There were physics-based equations in the 2010 Friston paper.

There's a big tree with all these different inference algorithms.

However, they were all just kind of branches on that tree, a little bit more evocatively or aspirationally, but not formally.

Then, dot, dot, dot, 2019.

free energy principle for a particular physics and the reading group and all the work around that and then especially in the last year since then um i think that the the decision and the move that you made to lead with the state-based and then almost nothing had to be said today of course it was a great presentation but you said it all that we pointify the path we make

paths into points in a given space um and so i was thinking about like being in a car and then there's one path where like my first derivative is one for two seconds and then it stops and then that was that path and there's another path where my first derivative is one for four seconds and then i stopped and so all of those different paths which do take the car to different locations

they are also just points in this generalized space.

And then there's this interpretation of those coefficients as the Taylor series expansion, but it just shows how versatile the formalism is because it can take

points in arbitrary state spaces which means yes you can imagine arbitrary state spaces that correspond to taylor series expansions or potentially other kinds of constructs what do we get from paths so in fact i think there are um a couple of things which uh motivated such a formulation because when


SPEAKER_01:
I mean, about the usual state-based formulation we talked about last time, there was many critics, many, I mean, it was, there was many debates, for instance, about the question about the nest density being at steady state, is it really relevant when it comes to real system?

to real biological systems for instance so there was many uh things which are now um addressed by the path-based formula formulation uh because as we saw we relaxed a couple of hypotheses and also the second thing which is very very much at the core of the path-based formulation as you said is

I mean, the fact to move from states to path, to path which are in fact encoded by generalized states, it allows you to speak of the future, to speak to the future path.

so that you can develop a physics of particles planning for instance which was not at all the case with the previous formulation so having such extension of the scope of the free energy principle it allows you to to

literally describe the bayesian mechanics of particles planning uh and and so it's the fact to think in terms of path as opposed to states it allows you to develop a physics of let's say higher order cognitive abilities i would say uh so it's it's definitely very very cool and uh a great achievement let's say responding to that um


SPEAKER_00:
I would say planning is possible in the state-based formalism.

It's just more of a brute force, tree-branching engineering problem.

So it's almost like in the state-based formalism, there was a physics of the perception-action loop, and then there were classical computer science ways to deal with the branching of planning.

just like a chess algorithm, like how many depth deep in the time horizon, and then there's all these secondary strategies for branching and pruning that search tree.

But there's kind of like few to none in terms of the guarantee of the time horizons of policy.

You just kind of had to enumerate all the possible options.

But the real-time kernel was...

physically grounded and beautiful, and then planning had to be a little bit enumerated.

But when we have the path as an atomic entity, then we can kind of extend the...

the elegance or the simplicity that we were dealing with states in the moment, but now our state in the moment is like a Taylor expansion.

And this comes up a lot in the distinction between the discrete time and the continuous time models like figure 4.3 in the textbook, where in a discrete time model,

If you want to plan a hundred time steps in the future, there's some variable, you know, T sub 100, like you literally are making a prediction, but you have no prediction for T sub 99.5.

You're just making discrete predictions.

Whereas a Taylor series, even a Taylor series for a super complex function, and you're only going to go two levels of differentiation in, you will have a prediction even for any points.

it might be radically wrong, but you get the whole support from negative to positive infinity basically for free without guarantees of it being accurate.

But it's like you've kind of pinned yourself to the timeline and then every single derivative that you take is giving you a better handle on that path for sure.

You'll never do worse.

And so that is so similar, yet also very different setting.


SPEAKER_01:
It's true that I said that the time scale considered was basically the correlation length of fluctuations on the number of the order of generalized motion.

But in principle, your Taylor expansion applied to, I mean, it's infinite.

But of course, above, beyond the correlation length of fluctuations, it becomes wrong.

And I also want to say about what you said in the beginning about the state-based formulation.

Yeah, it's true.

And actually, in the paper, the free energy principle is simpler, but not too simple.

So you can write down...

the action of a path so here we we have a gaussian white noise everything is is uh so we we are really in the state-based formulation of the fep you can write the action of a path

and you realize that it coincides with an expected free energy as well so you don't need to be in the path-based formulation to uh go to to to get to the expected free energy thing um which equates the action of autonomous path or here in our path-based formulation it equates the generalized lagrangian of


SPEAKER_00:
uh autonomous path so so yeah could you come back to where there was the generalized lagrangian so this is a um okay maybe a one before this yes okay thank you

But those of us outside of the non-active physics world, how is action used?

What does action correspond to in physical systems when we're talking about action as generalized Lagrangian?

Is this the same thing as what we're talking about with policy selection and movement and embodied action?

How is this physics concept of action being used?


SPEAKER_01:
So first of all, I would say that there is, I mean, it depends on what we're talking about.

When it comes to actions in Lagrangian, we usually think of analytical mechanics, but here it would be more in the context of stochastic calculus.

And in the context of stochastic calculus and path integrals in stochastic calculus, the action is just a quantity which scores the likelihood of a path.

so if for instance the density of the path is uh the exponential of minus something you would call by identification this uh thing the action and basically the smaller the action the more likely the power because the higher the density of your path and vice versa

So by the way, when we were saying, for instance, here that the action is equal to, I mean, we define it as negative log density of the path.

In principle, if the density, I mean, when it comes to the usual definition of the action, if I write my density as a normalization factor times the exponential of minus the action, then

if i take the negative log of this density i would have the regular action plus something and usually this uh constant is this is discarded we we just don't consider it so that the action reduces to negative log density of a path but conceptually speaking this action is very different

from the idea of lagrangian because basically the act i mean usually speaking the action is the integral over time over your path of um of a quantity called the lagrangian so at every point in time

there is a quantity defined, and this is the Lagrangian, which is defined at any point in time, while the action is a quantity which characterizes paths as whole things.

So conceptually speaking, actions in Lagrangians are very different things.

Lagrangians, once again, are defined at any point in time, while the action


SPEAKER_00:
okay Lagrangian was defined at any point in time while the action okay I'll wait a few seconds for Richard to rejoin if you're watching live please feel free to write questions in the chat and we'll look at them


SPEAKER_01:
it you're back it's all good yeah lagrangian is defined at every point in time and then the action and basically um so in order to just uh answer what you well uh

here yeah when you said that is it different from the notion of action when in the active in front uh literature for instance and so on here we use the word action uh but it has nothing to do with the notion of action like acting in the world etc so yeah for the people who never um

meets this concept, so it can be super confusing, I guess, when you read patient mechanics papers, you don't really know what action we are talking about, but yeah, they are very different.


SPEAKER_00:
I know, it's funny, like, each path has an action value, the negative log density,

that in a way summarizes what we could say are the actions that that path entails

but yet the actions or the affordances that are taken on that path that are summarized by the physics action um so there's yeah yeah um also yeah very um cutting edge with reviewing the typology of particles and the dacosta and sanved smith metacognitive particle um that

shows i think a few things one that often there are implicit concepts and qualitative concepts that are built up like long have people said that there is a continuity of modeling between rocks and societies for example popes and plaintiffs and plankton all these other funny things that that friston at all say but not until this paper

Did we see the simple, the conservative, and the strange?

And then with that target article, just with the DaCosta and Sanford Smith work, they kind of take that in another level and compose...

off to the side and now we can go into a metacognitive depth combining back to like the hierarchical nested meta awareness work of Sam Ben Smith from 2021. so it's just very cool how there's like a kind of earlier first pass qualitative intuition

and then the empirical research question is like where can we build the high-speed rail lines and this is like the blueprint for the high-speed rail now and then the next level will be like actually making the simulations or whatever it is to kind of show that this is not just something that you can make in PowerPoint but this is actually something where the rabbit is going to be

evincing some kind of behavior that it couldn't otherwise but it's kind of like that's like in a way it's not the only way but it is like an agenda between the um intuitive to the sketched and just it builds forward in these ways that are being reviewed pretty clearly and changing on a month by month yeah and i think i mean this i i mean this agenda of translating


SPEAKER_01:
um concepts like cognitive abilities and stuff in simple physical terms is quite interesting and i i mean we we discussed it on by message on discord but it's it's um it would be cool and it's it's coming it's happening but uh it's cool it could be cool to for the this uh bayesian mechanics this field to be more known and recognized

by the physics community because if you meet like a regular physicist working for instance in the physics of complex systems on or on in biological physics or biophysics and stuff like that

I mean, you have like a 0.999 probability that he never heard of Bayesian mechanics.

While we have a world agenda to follow, I think there is way more to be done.

It's like the down of this field of this agenda, let's say.

So I think the next years are quite exciting in that regard.


SPEAKER_00:
I totally agree.

Like one meme or theme on that for me is like the base graph on the screen on the table, it is what it is.

And then there's the second level where we annotate or assert that graph with cognitive phenomena.

Like, well, this graph is

reflects attention or awareness of attention or metacognition or regret or whatever we're modeling.

We kind of make an assertion about the Bayes graph.

And like you said, it's the dawn.

This isn't the answer on metacognition.

This is like one very snappy, very following in line way of modeling metacognition.

but there's no end there's no end to that question of how do you model metacognition and so then it's just like every time in a psychology paper or any we see like a cognitive phenomena which can include everything from anticipation you know past present future that's like our keyhole to bridge with this generalized unifying perspective on cognitive systems


SPEAKER_01:
Yeah, I totally agree.

This work is really, this work here on the slide is very like a first work, but I mean, when it comes to metacognition, for instance, I mean, I'm not an expert at all, but there are many concepts, I don't know, mental actions, cognitive effort, whatever, there are many things, many layers.

And the, I think, like translating all of these concepts of these phenomena, processes, whatever,

into an actual bayesian mechanics is um is really like um what's coming in fact i think in the next years so and yeah we'll see and and as you said beyond the analytical work

there is also the simulations worked examples whatever um so yeah i think there are many many works to to be done and which are coming in the in the next years i totally agree


SPEAKER_00:
Yeah, a kind of analogy that brings to mind is like early in the periodic table, not saying that we're studying material phenomena, or even that elements are material phenomena, but it's kind of like, well, the rabbit has vision, taste, and hearing.

So if we've identified memory in vision and taste, there's like a missing element.

Maybe it's not there.

Maybe the memory for hearing is zero.

Or maybe it's something very complex, but there's a space there.

It's like there should be a rare earth metal in the fourth row.

There should be an attention variable on this.

And so it's kind of like a higher dimensional periodic table where because we know about certain patterns that are either kind of convergently arising in the real world because of what is life or they're convergently arising because we choose to model things a certain way.

Then that brings a huge amount of concordance and juxtaposition to the field of chemistry.

Whereas previously there might have been air chemistry and water chemistry and fire chemistry.

And then there'd be like, oh, but what happens when you throw water on a fire?

Or what happens when the air in the room burns out?

There'd be like these edge cases where it's like, oh, we don't do that.

But that's what's happening when we don't have the unified model for cognitive systems.

People will kind of study one phenomena or character of a system to its limits, but the limits of any phenomena in our highly woven life forms

Like, you don't chase it to the end.

If you're studying foraging in the ant colony, it's not like there's a, okay, that was the end of the rainbow.

Foraging is over.

It's like, oh, well, foraging's related to nursing.

And then that's related to this and the weather.

It never just simply terminates with the inquiry.

And possibly that could reflect the extremely early stage of this formalization.

Possibly there are fundamental unknowns and adjacencies in our epistemic situation.


SPEAKER_01:
Yeah, and I think, I mean, a few years ago, the idea of kind of extending physics to, in order to have an actual physics of cognition, it would be like crazy, like the Holy Grail.

And the fact that we have this

early works is quite an achievement and I would also say that what's interesting here is that we already do have formal models of many of many cognitive phenomena for instance you were talking about the

the paper of last about metacognition so we have very precise and formal active inference models for instance of many different phenomena like metacognition and so on so in a way we kind of already know what we should end up with and the idea is

uh how to do how to play for instance with the sparsity within internal states in order to go to get back to re-derive or refine what people are already modeled in a different literature earlier so that's kind of the dynamics i think um so yeah we will see in the next years how it goes


SPEAKER_00:
yeah that's a great point a few more thoughts on that like a lot of the uh adjacencies and generalizations mathematicians and physicists and statisticians they're experts in this like if it was a fixed number see if it could be variable if there wasn't a variance see if you can add a variance if it was this kind of distribution swap it out for like there are these kind of syntactic moves that barely require

much more than just like yeah these are the swaps we make like this is what it looks like to generalize a theory it's like we could add a variance on it you know all so those kinds of um familiar moves and then um so much can be explored on this topic but when we think about a physics of cognitive systems or a physics of cognition

Sometimes I feel like I have one foot like is, it's like tied or somehow...

trailing with the reductionism like it's hard to escape the material basis and it's also not clear if we want to escape the material basis because of physics of cognitive systems on one hand it might address this kind of like thermo info negentropic schrodinger what is life question about persistence and order of kind of quasi crystals

or leave all that mess separate mess and beauty the pure bayesian mechanics if we just say okay now we're in the map we're leaving the territory behind we're not going to talk about the actual calories and the thermodynamics we're just on the map and a massive prior with a little attention is like a piece of sand hitting a mountain

And a very loose prior with a highly attended to data point is like a bowling ball smashing through a small little heap of sand.

So it's like there's a physics to the collision of incoming data and prior, which is the Bayesian setting.

So it's interesting that there's a sort of

map Bayesian mechanics that kind of non-controversially describes the collision of priors of different mass in a way and then the tantalizing question or connection is whether that is like

one and the same or an enabling factor or a downstream factor of this actual thermoinformational autopoiesis for living systems if if I may add something I think here the uh I mean an interesting point is that if you have for instance um


SPEAKER_01:
I mean, I don't want to go there really, but if I have a software which can be implemented by many sort of hardwares, for instance.

And this idea was actually a lot debated when it came to functionalism and stuff like that, but anyway.

Here, such Bayesian mechanics, in a way, it's the physics of how

how the the overall system should behave but we so there is for instance the minimization of this uh free energy functional but we are not saying how the here we we do not um uh explicitly say how the system uh actually does the computations we just say that a spastic of a random radical system has somehow to do that stuff

And in a way, it is neutral about what the actual system is made of.

It's like when we say that the free energy principle is more a framework than a process theory or stuff like that.

But I mean,


SPEAKER_00:
i don't know very much about these uh questions i i think that's that's a great point it's like that's why a program or an operating system can be run when a different processor is brought in just within a kind of classical computing setting

um as a cognitive mapping or cognitive modeling framework like we don't know if that rabbit is a real biological rabbit that rabbit might be a deep fake rabbit and we're interacting with it through a video channel and we think that we're doing behavioral research on a real rabbit but it's just a really convincing synthetic data rabbit

we don't know because we're only getting the observations that we're getting and then we can tell any number of possibly compatible stories or interpretations about the data we're getting and to say more or to go further like

that's stepping past the boundary of the actual observations we're making, which is great.

Like we want to propose hypotheses for what we're not directly observing.

That's the whole point of latent states and everything.

And yet there is this line where it's like, you can kind of falsely push the known knowns beyond where they really are.

Um,

And it seems like one example of that is going down and specifying the actual mechanistic basis of a given computational function.

And if that's important to specify, then the work itself is to specify it.

However, at the framework level, the framework's absence of that kind of a material substrate, like that's the feature.

That's not a lacuna in the framework.

That is like, there's the USB stick and then there's where you can plug it in.

That is the plugin to any system.

And if there was something already pre-plugged in there, like, well, it has to happen on Turing computer or it can't happen on Turing computer, that would have just hobbled the scope of applicability

by welding together what doesn't need to be welded and i mean if anything this is a um journey and a a challenge about proper articulation and about how sparsity and nuancing which are connected to what brings us cognitive phenomena so it's like


SPEAKER_01:
many things to learn and reflect on yeah and if i may add a related remark um about the the question of having a top-down approach as opposed to bottom-up approach i think sociologically speaking and historically speaking it's very interesting that the people who started

developing bayesian mechanics are people who are not like supposed to be professional physicists because i think when you it comes to like super emergent kind of phenomena like cognition whatever um the the the people who was training is precisely to learn

uh about cognition about agencies the people who kind of know everything about what it takes to be an agent and so on

these guys are neuroscientists basically so they because they know what agency entails they can be the good ones to propose an actual bayesian mechanics which is a very uh top-down approach whereas if you go to the labs in like a physics of complex systems lab and stuff like that people are pretty much

uh bottom up they will uh for instance oh i i want to study the brain so i'm going to write down the upfield model for instance so a kind of icing model where up is an activated neurons and down a non not activated neurons and so i will have this very this very much uh bottom-up approach and i can

which is super interesting and super important but when it comes to this very much uh emergent behavior uh this is a very emergent phenomena of the brain metacognition blah blah blah i can study for like five centuries the opfield model i will never get any insight about the emergent behavior of the brain and um and um so i think here's the

you have to have a top-down approach.

You have to write a very generic Langevin equation.

And because you're a neuroscientist and because you know what it takes to do an inference, to be an inference machine, you're like, ah, conditional independence is very different.

It's a very important theory.

So let's try to inject in my Langevin equation some level of sparse coupling.

And then, boom, you have the free energy principle.

so even though sociologically speaking what's happening here might be super weird like these guys are not supposed originally at least to be physicists and they are supposed to like kind of generated the next chapter of physics how weird is that but in fact in fact i think there were precisely the good people to do that um so yeah it's it's quite interesting


SPEAKER_00:
I think there might be, oh, that's a great point.

There might be a fun history with physician, physician heal thyself, and the alliance of medicine and physic.

That's one point.

Another point is low road and high road.

And sometimes the disciplinary or the in-group conversation is like very low road.

And then the idealistic and aspirational is like high road.

It doesn't exactly map to those, but I'm thinking of like somebody says like, I want to drive somewhere.

I want to travel somewhere.

There is no material basis for that travel yet.

They're not there, there's no path, there's no car.

And then in the mechanic shop, it's like, well, we have this tool, this object, we're so surrounded by the low road that it supports this incremental research agenda using the tools and approaches that we have and their materiality.

Whereas someone from outside the field

like comes to the ant researcher, it's like, have you had the ants build this new thing?

And it's like, well, we weren't even on that path, but now that's like a new North Star and that can now draw work in that direction.

And so it's kind of like the low road building out and then like the kind of the draw of the adjacent possible and the imagination that the high road kind of grounds in.

All right, I'll ask a question from the live chat.

Susan asks, what can one equation contain?

I'm imagining how to interpret and translate self-modeling, how to approach foraging resources and opportunities.


SPEAKER_01:
Can you repeat, please?


SPEAKER_00:
Yes.

It's an open-ended question, so feel free to, however you like.

What can one equation contain?

I'm imagining how to interpret and translate self-modeling, how to approach foraging resources and opportunities.


SPEAKER_01:
So I could not answer the second part of the question, but if I can pick up on about the question of...

what what these equations are all about which is a bit what you asked last week like what works the do does the framework do what these equations contain or whatever um if you actually simulate a sparsely coupled random dynamic system and actually there are there already are examples out there for instance in the 2021 paper called um

uh uh uh stochastic chaos and markov blankets if i remember well with uh thomas paul and carl friston and others they simulate a coupled uh low-range um uh systems uh if i if i remember well and

if you do simulate such systems which verifies such fast coupling architecture you observe the behavior which is described by your equations and if for instance you remove fluctuations on particular states

so uh well your simulation which follows the path of this action which is given by your equation so it's it's um it's it's uh it does really work in the sense that it does describe indeed something it's really a physics in fact of such uh sparse couple systems


SPEAKER_00:
great question i think we could explore more on what an equation can contain but i'm just kind of struck by that there's like a latent large set of um to be clarified axioms and conclusions and so on and when we see like one equation written here

it's kind of like we're just seeing like one glimpse or one facet it's not like the other equations aren't in effect but sometimes it's a little unclear which equations are relevant but in an equation we see a symbolic expression

that within a given quantum reference frame otherwise the q and the a wouldn't mean anything like within a semantic reference frame the equation is just kind of like a check it's like if it's like it it's there's probably many ways that we can kind of explore what it does but it's true that i mean i think many people who


SPEAKER_01:
in addition don't necessarily have a mathematical background or whatever they have been i mean i'm not an expert in the history of the free energy principle but a lot of people have been confused especially in the early steps where it was as lens say it many times worried when it was even more nutrition than a solid framework properly grounded in solid math so people


SPEAKER_00:
did get a bit confused i think by with the math but now it's we have more and more worked examples simulations um solid math etc so it's it's quite um quite nice okay in closing we have explain it like i'm a 10 year old now that that response may reveal more about what somebody thinks 10 year olds are like but now that we've been on this two-part journey

you know, dot zip it and close it, Richard.


SPEAKER_01:
So explaining you mean about what I just said, like before?


SPEAKER_00:
Just both sections or just our whole projects.

Now that now that we can look back at all the incredible quality summaries that we've provided, how do we just cap it and move forward with a child?


SPEAKER_01:
Yeah.

i think everything we did here so both in this presentation and the previous one is um so basically we can so if i i i would so in order to say in the most um intuitive and simple ways i would say that

Very simply put, a system is just

It's just some things interacting, let's say.

So it would literally correspond to a system as we use this word in a daily basis.

A pen is a system, an organism, or any piece of, any collection of things interacting with each other.

I can say this is a system.

I just have to precisely delimitate

uh and and say what are the boundary of my system and given a system which has a certain dynamics certain laws physical laws telling you how your system behave are the things move around and so on and now

so my system can be alone like an asteroid in space let's say or it can be connected to each to other things i recognize as other systems two things can collide two different things systems can collide with each other etc for instance and now if i assume several systems which are connected meaning that they causally

interact with each other in a certain sparsely way so basically in my schematic you would have for instance a system on the right a system on the left and a sub system in the middle so you have many subsystems causally interacting in a sparsely way for instance mu doesn't interact with eta

If I take a bacteria, for instance, it does move around and interact with other things outside the bacteria.

But at the same time, within the bacteria, under the outer membrane, there is also stuff which do not interact because they interact with the outside, let's say.

And so if you have such sparse coupling architecture, as we say,

And in more technical terms, we would say that the inside is independent of the outside when conditioned on the intermediate system, let's say.

So if we have such sparsely coupled systems, dynamical system, we see, mathematically speaking, we derive the fact that

your system has to behave in a certain way and in fact when you do have such sparse couple dynamical system you realize that everything uh appears to uh i mean if you look at for instance the internal states mu here the dynamics of this system

uh can be interpreted in terms of bayesian inference meaning that this system can be viewed as encoding a density over its external states for instance and if i had to define in a very simple way what it means to parameterize or to encode a distribution i would say that um your um

the various parameters which define your system for instance i said earlier that a system could be anything a pen an asteroid whatever there are variables which specify its state for instance temperature pressure shape whatever i have a set of variables and i can view these variables these numbers literally as

I can take them and manipulate them as parameters of a mathematical quantity I can deal with.

So I'm not sure if I failed in trying to put it in very simple terms, but I hope it makes sense.


SPEAKER_00:
It was awesome.

Richard, thank you for

working to bring us these twin gems in the distillation work and really helping like consolidate and sediment on one hand a long history of dynamical systems and physics and on the other hand papers coming out two weeks ago as well so hopefully we'll continue the adventures and um see you around

Yeah, thank you, and thank you for the chat, the questions on the chat.

Cool.


SPEAKER_01:
Peace, everyone.

Bye.

Bye.

Thank you.
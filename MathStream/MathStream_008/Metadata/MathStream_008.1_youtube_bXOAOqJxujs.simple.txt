SPEAKER_01:
Hello and welcome everyone.

It's January 26th, 2024.

We're here in Active Inference Math Stream 8.1 with Richard Sarajevan.

And we're going to have an interesting presentation and discussion today.

on Introduction to Bayesian Mechanics, Free Energy Principle, and the State-Based Formalism.

This is part one.

So, Richard, thank you for joining.

Looking forward to this presentation and discussion.

So, to you.


SPEAKER_00:
Hi, everybody.

So, yeah, my name is Richard Servagent.

I'm French working in Switzerland.

I'm a PhD student at EPFL in Lausanne.

And just to bring a bit of context, I'm not working on Bayesian mechanics.

We do have a physics background, but we are interested in modeling bacterial evolution and ecology.

and what happened is that something like uh i mean the free energy principle was always in the corner of my head and one year and a half ago i decided to really read about the free energy principle especially if i wanted to transition to the field and i do want to transition to the field after my phd and so i started to ask many questions to the people from the fep community and i'm so grateful thanks for them

and also on the discord of the of the active inference institute and at some point i said that i was preparing um a lab meeting about the free energy principle and daniel proposed to have this uh discussed on the live stream because there isn't such material to specifically learn about bayesian mechanics and the actual physics underlying the free energy principle

and so here i am so once again i'm not an expert uh on the matter so always refer to the original papers but hopefully i'm gonna i'm gonna do a decent job um so without further ado let's let's start uh i'm not going to tell you uh what we where we are heading what questions we would like to to address or whatever i'm rather going to start building the framework right away

and at some point what we are doing will become clear so as you may know there are two formulations or formalisms um of the free energy principles the so-called state-based formulation

and the so-called path-based formulation.

So today, we will focus on the state-based formalism.

It's not like the old versus the new formulation.

In fact, thinking in terms of path or so-called generalized coordinates of motion have been around forever in the literature, but it kind of came back to the front scene

of the bayesian mechanics literature i think anyway today we will focus on the state-based from formalism so the very starting point is to write down a launch by equation a generic launch bar equation so it's literally like saying let's consider a random dynamical system

very briefly for the people not acquainted with such an equation x here is the state of your system so it could be a simple scalar if you are considering a one-dimensional process

But in general, x would be a vector.

For instance, if, I don't know, you want to model the 3D diffusion of a Brownian particle immersed in a liquid, x would be a 3D vector with components as the coordinates of your Brownian particle.

And you can see on the left-hand side that we have dx over dt, the time derivative of the state vector.

So that such an equation really describes or specifies the dynamics of the system.

So many things can influence indeed the dynamics of the system.

If I stick to my Brownian particle example, maybe it is subject to an external force.

So whatever is relevant here, you put it in F, the so-called deterministic term, or flow.

We will refer to it as the flow for the presentation.

However, in some cases, there is stuff you don't want to explicitly model.

For instance, if I stick with my Brownian particle example, it is constantly hit by the molecules of the medium surrounding it, hence its Brownian motion, right?

And it would be, so if you want to take into account these thermal fluctuations, it would be mission impossible to explicitly model every single molecule of the millions, if not billions of the molecules surrounding it.

So a convenient way to still take into account these fluctuations, which are literally thermal fluctuations in my example,

a convenient way to proceed is just to add a noisy term to the equation so omega here is a random variable was value changes with time with the appropriate statistics okay so two brief remarks before moving on if you assume that the state of your system changes slowly

compared to the time relaxation of your fluctuations, you can write the autocorrelation function of the noise like that, where gamma is the diffusion matrix and delta is the delta Dirac function.

So what it means, it's just that in that case, your noise is super rough and it's not correlated in time, basically.

Also, second remark, you can use the central limit theorem to argue that it makes sense to assume that omega is normally distributed, so that in the end, the noise is a Gaussian white noise.

But note that in the next livestream, where we will discuss the path-based formulation of the FEP, we will relax the white noise assumption.

Anyway.

so we have this random dynamical system and we can do something cool with the flow so the flow f is a vector it has the same dimension than the state vector because each component of the state vector has its own longitudinal equation if you will

and you can decompose it into a solenoidal and the gradient terms so without before sorry before telling you what this decomposition is all about on a technical note just notice that first q here is the so-called solenoidal matrix gamma is the diffusion matrix just as before

and the i here in the with the nabla i this i of x here is a negative log of a density so it's a self-information or surprise we will refer to it as as a surprise throughout the presentation

And the density at play here in this negative log density is the steady state or Nes for non-equilibrium steady state density of the system.

So we assume that there is such Nes density

that exists so that if you uh from a given initial um initial state you let your system evolve it will reach at some point a unique well-defined nest density and

Second remark before telling you what this decomposition is all about.

Note that usually in the papers, the divergent terms here and here are put together in a third term, which is sometimes called housekeeping or correction term.

But actually, if Q and gamma are not state dependent,

these divergence terms vanish anyway, and we end up with the two remaining terms, which can be nicely factorized like that.

Also, the last thing I want to say is that if you consider the solenoidal terms, the first term,

it is indeed a solenoidal term you can indeed write it as the rotational of some potential i'm saying that because sometimes people get confused when they see a gradient in both terms anyway what this decomposition is all about uh is quite in fact simple let's consider

uh this nice 2d single modded nest density okay so the flow and more specifically the gradient component of the flow which is here the vertical flow will drive the system towards its mode while fluctuations kind of push it away

But it's not the only flow.

There is also the solenoidal flow, which is here called the horizontal flow, which will make the system kind of converge to its mode with ever decreasing cycles.

And so if you want to get some more intuitions on this solenoidal flow, what we can do is to remove the fluctuations.

so all the entries of gamma the diffusion matrix go to zero and this means that we would not have any gradient flow anymore we end up with only the solenoidal flow and if we do that

the system will just follow an isocontour circulation on the nest density.

That's the bottom right panel here, where the solenoidal flow kind of drives the system on this circulation here.

a small remark about this solenoidal flow because it kind of drives the system in this simple example in either clockwise or anti-clockwise direction in an irreversible fashion, irreversible in the statistical physics sense.

So it breaks detailed balance and so on.

People sometimes view this solenoidal flow as underwriting the symmetry breaking ubiquitous in living systems.

Anyway, okay.

So before using this decomposition of the flow to do some cool stuff, I need to introduce some stuff.

So I will have to go through a couple of things, of notions, one after the other.

and afterwards i will we will put everything together and actually derive the free energy principle so the first thing i want to introduce is the notion of sparse coupling so let's say that in my state vector x here i have a subset of variables this mu here we refer to as the internal states

and they specifies the state of some subsystem called mu so i mean you get the idea the the idea is that we have like an organism an agent the bacteria in my schematic and these variables here literally specifies the internal state of my bacteria

And this bacteria is in a given environment, niche, whatever.

So there is this other subset of variable we refer to as the external states and which corresponds to the external world, the external states of the bacteria.

And the idea here is that these two subsystems are not connected to each other.

So when I'm saying that two variables are not connected to each other, I just mean that their respective flows do not take the other one state as argument.

So they do not influence each other, basically.

In fact, they are indirectly connected to each other through a third subsystem we refer to as the Markov blanket.

so that these guys here are called the blanket states.

And we will see in a minute that it really corresponds to a Markov blanket in a statistical sense.

OK, so we have this architecture, this path coupling architecture here.

And in fact, we can even go a bit further and assume that within the blanket, there are two more

systems the so-called sensory states and the so-called active state a so basically the idea here is that the external state eta they influence the sensory state s and this sensory state s influence the internal states mu but not the other way around

And the internal states mu, they influence the active states A, which influence the external state eta, but not the other way around.

So it's really a sparse coupling architecture inspired by the so-called action perception loop.

However, you could ask questions like, why do the sensory states influence the external states?

Or why do the active states influence the internal states, et cetera?

so we don't have really time to discuss this i guess you can think of some qualitative example in biology but i just want to point out that even though this architecture is quite canonical it's not a definitive feature of the free energy principle and in fact in the next time when we will discuss the other formalism we will do a bit of zoology and we will look at other sparse coupling architecture

Okay, so on a technical note, just notice that such sparse couplings are encoded by zero entries in the Jacobian matrix of the flow.

anyway so thanks to this pass coupling architecture we have this this system of four coupled uh launch my equations which respectively describes or specify the dynamics of the external states eta the sensory and active states s and a and of the internal states mu okay so

I want to say here about the Markov Blanket thing that under some conditions I'm not going to discuss here.

So for the people acquainted, it involves having no solenoidal couplings between autonomous and not autonomous states.

But anyway, I'm not going to go into this.

Let's say under some conditions,

the external state eta and the internal states mu are conditional independence so they are independent when conditioned upon b which makes sense because all the informations kind of transit through through b however note that when i'm talking about conditional independencies here i'm talking about conditional independencies in the stationary density

So basically, if you fix P and you have this joint conditional stationary density here for xi and xj, if these two guys are conditionally independent, it just means that you can write this joint density like that.

And so that such conditional independencies are encoded by zero entries in the Hessian matrix of surprisal.

Okay, so now just a bit more of vocabulary before moving on.

Note that if we put together A and mu, so we consider the couple active state and internal states, we refer to these guys as the autonomous state alpha.

and the cool thing about the autonomous states a and mu or alpha is that they are conditionally independent of the external states so autonomous and external states are independent when conditioned upon sensory states and if you add the sensory state s

to the autonomous states.

So you consider the whole thing, the whole Markov blanket and the internal states.

We refer to these guys as the particular states, pi.

And pi constitutes a particle, a particle in a generic sense, of course.

So an organism, an agent, whatever, a bacteria in my schematic.

OK, so here I just want to make a point to make a bit more clear what we are doing, what this approach is all about.

So basically here, we kind of define what it means for something, a bacteria, whatever, to exist in the sense that it has its own internal dynamics statistically separated from the external.

It does have a Markov blanket.

It does have its own physical integrity.

So we have no clue of how it maintains, indeed,

It's integrity in the sense that if you're considering real systems like an actual bacteria or a human being or whatever, it does survive in a given time scale.

For instance, displaying active processes, contouring dissipation, for instance.

Here, we don't say anything about how it does survive.

It just does.

We do have this path coupling architecture.

And from there, from the starting point, we are going to derive the necessary consequences of such sparse coupling.

So basically, we kind of ask or answer or try to answer the questions.

If things exist, what must they do?

And so if you're a bit confused, don't worry, we're going to go back to this idea later.

But I just want first to show you this quote here, which tells you, many theories in the biological sciences are answers to the question, what must things do in order to exist?

The FEP turns this question on its head and asks, if things exist, what must they do?

But once again, we are going to go back to this idea later.

But that's kind of the idea of this approach in a nutshell.

so as i told you uh i still have a couple of things to to present so i will have to go through each of them one after the other and finally we will put everything together and finally derive the free energy principle so the next thing i need to introduce is the notion of synchronization map

So very generally speaking, I'm not specifically here talking about our random dynamical system.

If you have a linear map g mu here, which gives you mu from b, and g eta here, which gives you eta from b, then

uh if g mu here is injected so that basically you can go back to the pre-image from the image you can use the pseudo inverse of g mu so that from you you go back to b and from b you can go back to you can go to eta

so the successive application of the pseudo inverse of g mu and then of g eta is called the synchronization map and it basically allows you to directly go to eta from you okay so now let's try to uh to use this idea in the context of our system

So b here corresponds to the blanket states.

So if I fix the blanket states, I have corresponding conditional densities for mu and eta.

I have p of mu given b and p of eta given b. And their modes are bold mu and bold eta.

So in virtue of this synchronization map,

I can go back to the external mode from the internal mode, thanks to, once again, this synchronization map here.

And I'm going to give an example in a sec, which is going to clarify a bit more what we are doing here.

But first, I just want to say that in this nice paper by Lance Dacosta about this synchronization map,

Basically, everything was Gaussian.

But sometimes, I mean, if it is not the case, a Laplace approximation, which is literally a Gaussian approximation, might be necessary to derive a synchronization map of closed form.

but don't worry we will go back to this idea of laplace approximation later just remember that we have this synchronization map here which allows you to go to the external map external mode sorry from the internal mode so for instance if given b given the blanket states

the corresponding p of eta given b follows this nice normal distribution where both eta here corresponds to the mode.

Then, in virtue of the synchronization map, I can view the internal mode mu as parametrizing a density.

I write it that way, q mu.

which is equal to this nice normal distribution where the mode is just the synchronization map applied to the internal mode to itself.

And by construction of the synchronization map, it is equal to the true external density.

So you can view the internal mode as parametrizing a distribution over external states.

Basically, thanks to the synchronization map.

That's what the synchronization map is all about.

So just a small point, because maybe some of you are a bit confused here because we're talking about modes as opposed to actual states.

So we will talk about that later.

But indeed, I mean, if I take the actual internal states at a given time t, they are not necessarily equal to their modes just because of

fluctuations or whatever so that if I apply the synchronization map on the actual internal states it might not give you the true external mode but anyway we will discuss this uh a bit more later so that was the notion of synchronization map in a nutshell basically so

last thing i want to uh to introduce before finally putting everything together and actually derive the free energy principle is the notion of variational inference so very simply let's say that you have some latent variables or hidden variable or some latent generative process causing some data s

so you have a prior p of eta over the state of these hidden causes of data and you are also equipped with a generative model which just designates this joint distribution here p of eta and s so you can view it as a model of how the latent variables cause the data

So the idea is the following.

You sample some data S, and you want to compute the posterior distribution p of eta given S. So in a way, you want to refine your belief about the hidden cause of data thanks to a new sample data.

So it's very simple in principle, because you just have to apply Bayes' theorem.

However, in practical settings, the denominator here, p of s, so the marginal density over sensory data, usually requires a monstrous marginalization.

So it's just not tractable.

So we can't just apply Bayes' theorem.

So we need a method.

uh which given some variation sorry which given some some variational distribution q also called recognition density uh gives us uh i mean we want a meter that makes it as close as possible if not equal to the true distribution we want ultimately to compute namely p of eta given s

and these two density so q our variational distribution and the true distribution p of eta given s are equal or are more or less equal if their divergence k divergence here is zero because this quantity here the k divergence basically measure the difference between two distributions

so that's what i wrote here on the top of the slides finding an accurate distribution queue in the sense of finding a queue as close as possible if not equal to the true target density is equal to minimize to minimizing this divergence however this divergence i mean there is the target density appearing here we can't

do anything directly with it we can't compute it or whatever we need a proxy for this target divergence and there is a proxy called variational free energy f in green in my in my slide here so f is is equal to this divergence here between q and the generative model

and the idea here is that you can decompose this divergence into the true the targets sorry into the target divergence in red here plus something so it is indeed a proxy for the target divergence and note that interestingly enough the second term here

is the surprise over sensory data or negative log p of s so that f is it can be viewed as an upper bound or lower bound depending on how you define it on surprise okay so what i just said here is that minimizing the target divergence just means minimizing f

So that's basically what variational inference is all about.

And note that usually algorithms require Q to be Gaussian or require a mean field approximation or whatever.

And if Q is required to be Gaussian, even though the target density is not Gaussian, we would end up with the best Gaussian approximation of the target density, basically.

And in practice, it would mean working with a so-called Laplace-encoded free energy.

OK.

So before moving on, I just want to say that this quantity, the variational free energy, is in itself a quite rich and interesting quantity.

So you can decompose it in many ways, and each decompose provides interesting interpretations.

For instance, if you look at the second line here, you can see that

Minimizing free energy means maximizing this accuracy term here.

You basically want to explain the data, I would say.

But at the same time, you want Q to differ the least possible from a prior distribution.

So that's an interesting quantity.

Anyway, now.

Let's finally go back to our sparsely coupled random dynamical system and use everything we talked about.

And finally, let's derive the free energy principle.

So here is our system and we have these four Langevin equations.

And the first thing to do is just to apply the decomposition we talked about in the beginning.

So basically,

the flows of each of them can be written like that.

So I just directly applied the Helmholtz decomposition we talked about in the beginning.

Okay, so now let's try to understand how it works.

Let's talk about the dynamics of the system.

Let's say that there is a momentary instantiated sensory state.

And let's say that the sensory states are fixed.

And there is a corresponding autonomous mode toward which

the autonomous states are going to converge and stay in the vicinity of their mode in the close vicinity if fluctuations are not too large.

OK.

But in fact, sensory state with time changes

uh so that the the modes of the autonomous state move as well and in fact it it moves on its corresponding autonomous manifold so i'm not going to go into the details but just have in mind that the autonomous mode moves on a so-called autonomous manifold which can be viewed as a statistical manifold and which can also be viewed as a so-called center or central manifold so

if i kind of rephrase what i am saying here is that the flow of the of the autonomous states can be decomposed into a off manifold flow and uh on manifold flow which corresponds to the path of the mode itself on the manifold okay so just to be a bit more clear let's say

in my bottom right illustration diagram here, the autonomous states are here.

And I'm interested in the off manifold flow.

So basically, I have this component here, which corresponds to the gradient flow towards the manifold, towards the mode, basically.

Here, it's pretty much like what we discussed in the beginning.

And at the same time, there is here this orthogonal component, which corresponds to the solenoidal flow.

So that's basically the way the autonomous states are going to reach their mode here.

It can be viewed as this ever-decreasing cycle towards the manifold on which the autonomous mode move.

OK.

So that's a bit dense, I guess.

So I recommend to check the paper, the free energy principle made simpler, but not too simple, which kind of discuss all these ideas about center manifolds and stuff.

So here, the interesting point is that if you assume a separation of timescale between the fast flow of the manifold

as opposed to the slow flow on the manifold basically the autonomous state always are always in the vicinity of their modes and if you want to characterize the overall dynamics of the autonomous states you can focus on the autonomous mode on the path of the mode and in the next slides

we will indeed focus on the autonomous mode and and by definition as we already discussed the autonomous mode the autonomous mode is or corresponds to the autonomous states which minimize

surprise here in the last two Langevin equations because the autonomous mode corresponds to the least surprise of autonomous states.

Before moving on, I just want to say something we can maybe discuss afterwards because I'm not sure to fully understand.

But basically, if I'm here in my bottom right schematic,

And so I have this gradient flow towards the manifold and this solenoidal flow parallel to the manifold.

And if I remove fluctuations, so the corresponding entries in the diffusion matrix go to zero.

As we saw in the beginning, it means that there is no gradient component anymore.

And what the system will be doing is kind of orbiting or oscillating around a point which moves on the manifold.

So that's interesting.

And I guess that if we do the exact same reasoning but starting already on the mode, then the whirl flow reduces to the on manifold flow.

And I guess that in that case,

the autonomous states follow and in fact coincide with their mode but anyway maybe we we can discuss about that afterwards so okay so let's use the various things we talked about and especially the notion of synchronization map we as we said the internal mode parameterize indeed a distribution over the external state so mu here

um parameterize a distribution which by construction coincides with the true distribution p of eta given b and in fact thanks to the conditional independence between external states and autonomous states you can just

drop the condition upon a and you just have q mu equal p of eta given f and equivalently you can you can write it p of eta given pi and the

idea here is that you can view q mu as a variational distribution if you want you can write its associated variational free energy so you have this um this formula here the free energy and because q mu is already um already coincide with the true posterior distribution if you will the first uh term here goes to zero

And so that F here reduces, if you will, to the surprise over particular states.

And surprise over particular states, they appear here in the equations of the autonomous states.

So we can do this identification, and we realize that the autonomous mode not only minimizes surprise though, but free energy in general.

And the way mu, the internal states, will be updated when the sensory states will change will always be so that this divergence here is 0.

So that mu always keeps track or synchronizes with or, in fact, infer the external states.

So that you can interpret that under a generative model, which is here p, the next density,

the internal states can be viewed as performing inference over external states.

And so, in fact, it's not only this divergence which is minimized, but it's also surprise.

And it's not only the internal states which minimize free energy, but also the active states.

So let me give an example.

let's say that the actual uh instantiated sensory states are likely sensory states or unsurprising sensory states and by definition in general the instantiated sensory states will be likely sensory states so mu will will the corresponding mu will be so that this divergence will be zero as we just discussed

And at the same time, the corresponding active mode will be, so you can see in composition with the third term here, I of A given S and mu, A, the active mode, will just be the one the most consistent with this intensified sensory state.

And in fact, you can view it the other way around and say that the active mode is the mode which gives unsurprising

sensory states, so that the particle can be viewed as actively sampling unsurprising or likely sensory states.

Or equivalently, you can say that the particle kind of accumulates evidence for its own generative model.

And I'm going to say something about the generative model in a sec.

But I just want first to, so yeah, this sentence here just sums up what we said.

mu is updated so that q mu is always the the best distribution of our external states and we refer to this as perceptual inference and the idea to in addition trying to minimize surprise for action is called active inference so a brief note we said earlier that in order to have a synchronization map of closed form it could be necessary

to work under a Laplace approximation.

So that in that case, Q mu is just the best Gaussian, for instance, of the target density.

So that the divergence here would not be 0, but it still would be minimized so that the identification here between the two gradients

still hold and nothing changes with respect to our discussion.

So here I just want to say something about what we are doing here.

Basically, we assume that we have our agents or organisms that survive, indeed exist or persist in a given environment, let's say, at a given timescale.

And we end up with the fact that

our particle must be equipped with or must embody a generative model which may or may not exactly coincide with the true generative process and which encodes the causal structure of the world under which it tries to perform inference and to minimize surprise, to perform perceptual and active inference.

But the interesting thing as well is that, and I think that's something fundamental that people tend to misunderstood, I guess, maybe, I'm not sure, is that the generative model also encodes the preferences of the system.

And let me explain why.

If I tell you that an organism manages to survive, to exist, to persist, et cetera,

and so it means that such an organism manages to stay in its homeostatic life compatible states you would be of course it almost sounds like a tautology survive equal staying in it in in its homeostatic state that's obvious right and that's exactly what we are doing here we assume existence survival so that the likely state

in which the particle will persist, are preferred states per se.

So that, for instance, if I'm considering the prior of my generative model over sensory inputs, P , sensory outcomes as associated with high P , so likely or unsurprising sensory states, are preferred sensory states.

hence when i'm saying that the active states try to sample unsurprising sensory states it means trying to sample preferred sensory states and so basically the particle appears to kind of actively accumulate evidence

um like uh for its own existence in a way it kind of sample life compatible to send it kind of sample life compatible data if you will and that's exactly the definition of self-evident thing so i think we touch here something fundamental about agency is that agents are self-evident creatures in that sense

Okay, anyway, so basically, I think that's the most interesting things of the free energy principle.

We start from existence and we end up that such a particle, which is coupled to the world in that way, must embody a generative model which encodes the causal structure of the world and which encodes its preferences in terms of what is life compatible, if you will.

okay so just to sum up what we did here this idea that free energy is minimized uh you can write it that way and this is in a way a variational principle for self-organization that's a free energy principle so here i just wrote what we just discussed the agent keeps tracks and acts on its external milieu through perceptual and active inference

And note that, interestingly enough, you can write such a principle as a principle of least or stationary action, where the Lagrangian, which is constantly minimized along the path, is variational free energy.

So here are some concluding remarks.

i'm not going to for all of them but the first one is basically what we just discussed this idea that the generative model encodes preferences if an agent maintains existence its likely states are its preferred ones per se hence the notion of stealthy dancing and i just also want to point out that this new approach

or chapter of physics, let's say, consisting in describing physical systems as encoding probabilistic beliefs is called Bayesian mechanics.

Okay, so having said that,

uh thank you very much and especially thanks to all these guys who who helped me so much especially len uh and uh yeah thank you for um for your uh your attention coming back thank you richard


SPEAKER_01:
okay well while we're settling back in and anyone is asking questions in live stream what is your phd research and if this is your side project what is your main project that this kind of relates to yes so well in fact um i kind of read about the free energy principle in my free time whenever i


SPEAKER_00:
I had some time and what I'm doing in my PhD is so we have a couple of projects

the first project we did was really modeling bacterial evolution through so basically we model bacterial evolution as a bias random work on genotype space with successive mutations and and and successful fixations so that's what we are doing it's not related to the fep at all

And the second thing we have been doing is modeling.

So basically we had a system where you have bacteria which can kill each other thanks to a system which is called the T6 secretion system.

They kind of have needles with which they can go through the membrane of other bacteria and liberate toxins.

and they can also bind to each other so there is like a prey-predator kind of dynamics and we did like a lattice gas modeling of such systems so basically that's what I'm doing in my PhD which is not related to Bayesian mechanics but


SPEAKER_01:
I would like to to transition to to the field afterwards so yeah we'll see how it it goes I remember when I thought my PhD wasn't related to active inference okay cool well

the work built to an amazing crescendo that in its simplicity even though you highlighted it it's easy to fly by which is the coincidence of the preferences and the expectations so could you maybe give a little context how else has that Nexus of

preference and expectation been approached and is the FEP only and simply and always that coincidence is that coincidence upstream or downstream of some other commitment that we make like what are the commitments that we really make and is that um alignment the commitment or a resulting commitment


SPEAKER_00:
Yeah, so first of all, I think the notion of self-evidencing might be a bit refined with the next formulation.

But anyway, I think that's a crucial point about the FEP.

And usually, it's kind of

confusing because when you're reading the papers and people are starting to write that the system um sample evidence for its own existence you're like what i mean i'm not sure to understand what's going on here um but in fact yeah it's i i think the the way i i introduced it this idea that

by definition um a living thing is a thing which which managed to sample life-compatible uh sensory data is really what uh allows this um align alignment story between the that the idea that between surprise and preferences uh basically and this idea that

actively sampling unsurprising data is in fact and it's not like a tricky wording it's in in a way that's really what's happening it is uh sampling uh life compatible or preferred in that sense data hence the notion of self-evidencing um but um yeah i think the whole idea here is that

we start from existence.

We start from this sparse coupling architecture, where the particle managed to maintain its physical integrity, managed to display a Markov blanket, which allows the agent to have its own internal dynamics separated from the external.

So somehow it managed to contour dissipation or whatever.

And so from there,

likely states are states consistent with the fact that it is existing indeed.

So I think that's basically the idea.

But yeah, in the beginning, this kind of line of reasoning can be a bit confusing.

But in fact, I think that's very much what the FEP is all about.

And actually, last remark,

um in a machine learning street talk interview of maxwell ramsted he it was titled the fep as a physics of survival if i remember well and i think that's that's very very much what what it is all about in a way awesome how would you relate what you just described to reward or to reinforcement type learning schemes

yeah so i i mean i'm not um an expert at all i could not uh make the bridge here but i know that um lens dacosta made several uh works and interviews about the the subject and actually i think there is a very new paper called

active inference as a model of agency you just shared actually today um so i yeah i recommend the viewers to to check them out and as far as i know but here i'm just i'm just uh saying what i heard is that um any reinforcement learning algorithms can be um can be framed in terms of active inference

So I think active inference is a very fundamental scheme, but yeah.


SPEAKER_01:
Yeah, it's all good.

Like the reason I ask just with how you presented it is what kind of observations do we want to sample?

That could be the sensory embodied interface between the agent and the environment, or you can take a more cognitivist approach and sample internal observations, but those are just external, some other internal.

So what do we want to really sample?

Well, if you're even in a position where you're talking about sampling from like a utility or a reward distribution, you've already specified a distribution.

Why not just specify the existence distribution, the actual attractors and stationarities of the measurements, and then it's simpler.

Because there's no proposal of a secondary intermediate between the temperature and how good different temperatures are by going and just saying it's not rewarding to be at 37 homeostatic temperature.

It's just expected and likely and the ball runs downhill.

It's actually a lot simpler and more general.


SPEAKER_00:
Yeah, and I think that it's way more simpler to... I mean, the idea here is that the agent has a kind of world model which, as you said, specified what are the expectations with regards to just existing in a way.

And as opposed to designing explicitly objective functions which incorporate the notions of utility and so on.

So yeah, I very much agree.


SPEAKER_01:
Earlier when we were looking at the flows and we had the breakdown of a flow, um, could you maybe just, um,

What animal are you thinking about or what scenario can help us understand like what's the solid black line?

What's the small red line?

What's the spiral?

What's like a physiological setting that we could associate here to help us understand that kind of complex movement?


SPEAKER_00:
Yeah, so generally speaking, the first thing I could say is that this notion of solenoidal flow.

So it's like in the schematic in the first slide where you had this isocontour circulation on the nest density or here.

the the the component of the flow which creates this sort of spiral here it can it so that um it's this sort of um oscillations are i think uh the sort of oscillations or cycles

that are ubiquitous in living systems um i mean i'm not a biologist but you can uh or not really a biologist but you can think of the circadian cycle or or anything in any sort of systems there is this thought of of um attractor where you're circulating along and so here specifically to this

to this schematic here, I think the idea is that you have, so you have, basically let's say that for a given sensory state, you have a corresponding autonomous mode.

and the when the sensory state change the autonomous mode mode changes as well and in fact move on its so-called manifold so basically i guess here you have the mode moving uh on its manifold and now if we take the perspective of this autonomous state here

we converge to the manifold, to the mode.

And because of the solenoidal component of this flow, the way we will reach it is with this kind of ever-decreasing cycles.

So here's the idea.

And I really recommend here the free energy principle simple paper.

you have the the flow on the manifold it's just the path of the mode itself let's say and you have the flow of the manifold was gradient

component is the flow towards the manifold in fact um so basically so that's basically how autonomous states kind of um reacts to to to sensory data which change the autonomous mode and i think the world uh an important idea here is to assume that the flow of the manifold is

fast as opposed to the flow on the manifold, so that basically the sensory states are always in the vicinity of their mode and move with their mode.

And yeah, I think that's pretty much the idea here.


SPEAKER_01:
okay so let's just say that the black line is um our homeostatic body existence life compatible pH oxygen blood sugar and yeah we are that light blue dot that's off that manifold of course if we were far enough off to be dead

it would be a moot question, but we're off, but within a life scaffolding or compatible zone.

And now as time pushes us down into the right, there are different slices that we can trace

we could take the shortest path, the gradient flow directly towards the manifold.

So as that plays out through time, it would look like a linear line converging to the thick black line or pure solenoidal flow would just stay equally far away from the thick black line and continue to spiral.

So that would look like a corkscrew

through time and then here when you have the combined character of the linearized convergence towards the manifold and The corks grew out through time we get this kind of winding spiral so it reflects on me that the gradient flow is pragmatic and

value in that it aligns future observations with preferences and the solenoidal flow has an almost epistemic character in that it circulates amongst a set of equally valid outcomes

yet here we're not looking at the pragmatic plus epistemic decomposition of the expected free energy policy selection strategy like equation 2.6 in the 2022 textbook so is that just a concordance or where do you see some of those topics connecting


SPEAKER_00:
I'm not sure, maybe, but having said that, on the meaning of the solenoidal part here, I know that on the... I don't remember if it's in the free energy principle simpler paper or someone else,

but there is an analogy i mean they discuss the the meaning and the role of the solenoidal flow where they say that it it kind of help um it kind of helps mixing the systems and you can view and there they discuss the metaphor with where you want to dilute your um your um

your coffee for instance and you're going to have this sort of uh motion in order to reach the the as fast as possible the steady state where everything is diluted but I I um I'm not sure I didn't think enough myself to provide any sort of interesting insight all good just to have composed it it's very insightful um well you made


SPEAKER_01:
choices, assembling things.

Like what do you feel like would have been background, maybe a course or a skill?

What background do you feel like you kind of conditioned upon that somebody might want to check out?

And then what do you feel like you would have wanted to include in the state-based formalism?

um because to bring it into a under one hour timing is very concise so where do you feel like somebody could fill in some background to pick up with you at the beginning and then what else do you think would make a fuller presentation


SPEAKER_00:
I think there are a few aspects and details I didn't really fully discuss.

Well, first of all, all these things here, which involves a center manifold theory and stuff like that, we kind of played qualitatively with it.

We didn't really go into that.

and also if we want to be like full really full formally speaking um let's see maybe um

uh well there are a couple of things where we that we kind of accept without really checking all the assumptions and also the revised derivation and i'm especially thinking of the of the helmholtz our decomposition of f because of course you need um a steady state an s density to exist to in order to have such uh a decomposition so here i think it's it's there is the

a lot of stuff to check.

And I mean, there is a nice, I think it's in the Appendix B of the Bayesian Mechanics of Stationary Process paper by Lentz, where he derived the Helmholtz decomposition.

uh so yeah there are quite a few things we kind of state without um derive so it can if people are interested in in going further i think that's kind of interesting formal directions um and um um yeah


SPEAKER_01:
cool i think it'll be a really fun collaborative project to axiomatize and formalize and modularize using the active ontology and understand a lot of these um relationships and then the other piece that that made me think about is like what work is any of this math doing at all

Just kind of like the ultimate existential question here.

And when we condition upon existence, we've kind of like off-sourced a lot of cognition.

We don't need to make the jump or the walk or the miracle from axiom to embodied existence or to even measured hypothetical existence.

So that...

is left unaddressed.

The margin was not big enough, but it wasn't even addressed.

And maybe there are even advantages to leaving the, what happens before the conditioning

you don't want to take it with you after you condition upon it.

That's the whole Markov blanket concept.

Like if you're like, well, I'm conditioning on five years ago in the present, but also I'm carrying five years with me today.

Well, then it's like, well, then it wasn't conditioned upon.

So to really condition upon measurements is an extremely radically simplifying maneuver.

That may change the scope or the applicability of the framework relative to a conception in which what the free energy principle does is describe how things come to be.

However, this rather conditioning upon it opens up that discussion and more circumscribes this very analytically tractable setting of the agent and the environment across a conditional interface.


SPEAKER_00:
Yeah.

by the way about about the conditional thing there is now the notion of you know weak mark of blanket that dalton introduced which kind of lose the the the approach let's say and and um because indeed there is a question on uh i mean does it apart from the the the formal fitting we have here can we

really apply it to real systems and stuff like that and um and also i think it's a physics of survival uh in itself uh at a given survive at a given time scale there is

At a given time scale, we survive indeed in the sense that there is indeed this partition or conditional independence between the internal and external.

Here is the physics you have to comply with, but it didn't tell you how the Markov blanket works.

arise or whatever, it's just not what it is designed to explain.

But I think, generally speaking, it's really informative because, for instance, if you are considering the

I mean, this sort of approach in general.

I mean, for instance, if you consider the pendulum effect where you put pendulum oscillating on the table and they are going to synchronize with each other.

And I think that Takuya Izumura did a paper about that recently.

in order to understand what is going on and why the pendulum synchronized at some point you just have to recycle all this line of reasoning with the synchronization maps that's very what is at play and what explains why the pendulum synchronized when they are both on the same table so

I think it is really informative in order to understand what is going on when we are talking about synchronization phenomena across sparsely coupled systems.

And also it gives you, I guess, the sort of recipe to understand what it

what it's um what it takes to be an agent if you want to design your uh an intelligence system um and um but uh yeah the question of how much useful it is uh beyond the fact that it's just some nice formal framework it's it's an interesting uh interesting discussion yeah

Just two things.

First, I would like to go back to your previous question about what sort of things could be discussed further.

I think an interesting point we didn't really discuss fully is the notion of synchronization map, because we didn't necessarily really discuss the hypothesis and stuff about the synchronization map.

And in fact, I think there is much things that can be said, for instance,

because we assume injectivity thanks to the rank nullity theorem it kind of constrain the dimension of the internal manifold here with respect to the blanket manifold here and it kind of

constrained in order to have injectivity thanks to the rank nullity theorem and so it kind of constrains the the the in order to say it in a qualitative fashion it kind of constrains the complexity or richness of the internal states

which speaks nicely to other frameworks like like ashby's laws of requisite variety where you want the regulator system to to be as

um as sophisticated or as rich to the regulated systems and here you need the internal states to be enough complex to uh or to constitute the sufficient statistics let's say to parametrize to be able to parametrize the density indeed um and this um and this uh richness let's say is constrained by the the cardinality of your sensory

channels, if you will, because basically you need the internal manifold to have the same dimensions as the blanket manifold.

or the sensory manifold to have the same dimensions as the autonomous manifold.

So I mean, I think there is many things to discuss about this aspect here.

And the last thing I would like to say about your last question about

the applicability of the framework and how much it's useful as opposed to be a simple elegant formal framework i think so you know there is this um these papers about um

about the Markov Blanket trick and stuff like that, about how much difficult it is to identify what state corresponds to the Markov Blanket or whatever.

And personally, I'm not really convinced by these critiques.

Because to me, it's like saying to Newton, yeah, I mean, I'm not sure that I can do anything with your framework.

It's complicated.

if not impossible to model systems with clearly identified and separated roads and masses let's say uh okay fine but we're talking about newton mechanics here so i mean i think it's the the same here it's if you have a sparsely coupled random because systems that's the sort of behavior it will display it tells you fundamental things about the nature of living systems and the idea that

When it comes to a specific system, it can be quite tricky to model it.

That's another question.

And indeed, when it comes to the art of modeling complex systems, it's interesting.

And we can discuss about how much complicated it can be to apply the framework.

Yeah.


SPEAKER_01:
Awesome.

I love that.

It's like the art of the science and the art of the modeling and the craft, especially in the kind of early hand-built, largely custom stage.

Like one thing I even wondered looking through these slides, what fraction of these representations and formalisms exist only analytically

And is there a code representation of this exact scenario?

Or are some of these areas equations that don't have code realizations?

They're just pure existing equations.


SPEAKER_00:
So, I mean, I think...

um more or less everything here can be um can be simulated even this synchronization thing here you can perform simulations where you can really literally see within the simulations the synchronization um and i mean the the the whole thing here can be

You can simulate such partially coupled random numerical systems and kind of interpret the dynamics indeed as the way we frame it.

But yes, that's also an interesting aspect.


SPEAKER_01:
It could be cool in the GitHub repo, in the journal for this transcript or something like that,

curate together the simulations that demonstrate or a minimal specification for it um yeah because it's it's actually there is um great


SPEAKER_00:
Yeah, and actually there is, I mean, I think it's in Len's paper about synchronization map, the Bayesian mechanics of stationary processes paper.

There are some simulations where he shows that, I mean, he shows the synchronization map at play and it shows that

basically you can't go back to uh i mean if the the the map between the blanket states to the internal states is not injective uh and you apply the synchronization map to uh the actual sensory uh to the actual internet states it gives you like some not relevant things and there are some nice plots from simulations so that's yeah definitely a paper to to check out

so where do we land and then how do we leap exercise relax to prepare for part two yeah so i think here's the world point was i mean the the this world formulation is in a way about the momentary the short day the short term

and the momentary response to autonomous states to sensory stimuli let's say if there is this uh this um i mean the kind of instantiated active states are so that it comes whatever but in the next uh video where we will look at the path-based formulation of the framework the world idea

would be to ask what about path and what about future path and what about the long-term behavior

uh and what about planning what about higher order cognitive abilities um and we we will kind of extend uh the scope of what we are doing in in that sense um but um yeah i mean i think from a formal point of view

uh here i kind of introduced many things version of um rational inference synchronization map etc one after the other before actually deriving the free energy principle next time i think we will it will be more straightforward but the the main concepts to to which will be at the core of the of the framework and which can be confusing if it's the first time you

you look at it, it's the notion of generalized coordinates of motion when you relax, so white noise assumption.

and that's something that can be confusing especially for the physicists because when you're starting saying yeah the generalized lagrangian he plays the role of an action or whatever they are like no but lagrangian is not an action what are you talking about etc but when you get acquainted with uh the world construction is very elegant but that definitely something uh people can started look at before prior to the the live stream yeah


SPEAKER_01:
Awesome.

Yeah, well, it was excellent.

You brought a lot together and a lot of trails leading off this trail and the citations and previous papers that also brought things together, Lance's work and others.

And it's going to be awesome to see part two.

So thank you, Richard.

Thank you very much, Daniel.

Thank you.

All right.

See you.

Bye.


SPEAKER_00:
Right?
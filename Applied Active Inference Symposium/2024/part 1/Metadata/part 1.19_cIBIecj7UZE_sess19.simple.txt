SPEAKER_00:
was a great presentation this will be the last of the recorded presentations for the symposium and last video on this part one stream we have jf clotier an experiment in artificial agency an experiment in artificial agency


SPEAKER_01:
I'm Jean-Francois Cloutier.

I'm a research fellow at the Active Influence Institute, and I do research in cognitive robotics.

One of the principal questions I ask myself is, what does it take, at a minimum, for an autonomous robot to learn to survive in a world it knows initially almost nothing about?

Other questions which follow are,

Can a robot act for its own reasons?

Does it have agency?

Can its agency be programmed?

And would such a robot be an active inference agent?

The one way to find out is to actually build an artificial agent.

And by artificial agent, I mean a sense-making, adaptive, and self-directed robot.

So this project is a continuation of a personal project that I started quite a few years ago.

And three years ago, here's where I was.

I had a Lego robot with quite a few sensors, like ultrasonic sensors, infrared sensor, a color sensor, illuminance sensor, motors of different types.

All this running from a Raspberry Pi with a board that provides an interface between the Raspberry Pi and the various Lego motors and sensors.

And the robot also had a 360-degree beacon that allowed one robot to see another through the other robot's 360-degree beacon seeker.


SPEAKER_00:
So let's look at them in action.

I named them Carl and Andy for, I think, quite obvious reasons.


SPEAKER_01:
Carl had gone through quite a lot of training runs, so it was better at finding the food, which is the piece of paper on the floor, with a beacon providing the scent, the attractive scent.

We can see that Andy is having a bit of a hard time.

And they see each other so that if one robot starts acting panicky, which they do if they collide too often, the other will notice the pattern of movement.

And if one robot makes a beeline, then the other robot will perceive that and say, aha, that's where the food is, and try to move in the same direction.

But we see them moving about.

and trying to gather the food and eating the food.

And Carl is clearly more apt at this task than Andy.

And what they've learned through various trials are the policies that are most successful at finding the food.

So that's where I was three years ago.

The way I implemented the cognitive capabilities, quote unquote, of these robots was based on an architectural model described by Marvin Minsky in his book, The Society of Mind.

This architecture posits that interesting behaviors emerge from many simple actors

that interact in simple ways.

So we're looking at cognition as a collective intelligence.

The implementation was a hierarchy of these simple processes.

I call them cognition actors.

And they formed an abstraction hierarchy with, at the very bottom, cognition actors that simply wrap sensors and effectors, motors.

And then at the very bottom, we have cognition actors that care about very simple things like the location of food.

Then at a higher level, information obtained from the food location cognition actor feeds into the food approach cognition actor.

And this one tries to figure out, am I getting closer to the food?

And so forth and so on.

Other cognition actors are concerned with avoiding collisions and avoiding dark places.

Now, each cognition actor, as I said, is a separate process.

And each such process does very simple things.

It makes predictions about what it will observe next.

It then updates its beliefs based on predictions that were successful or prediction errors received, and then selects a policy randomly in the beginning, but then based on what was successful in the past, and applies that policy, acts, and then decides, and then predicts what the next incoming sensations will be, and so forth and so on, essentially implementing an OODA loop.


SPEAKER_00:
observe, orient, decide, and act.


SPEAKER_01:
There's one big problem, though, that this society of mine, this hierarchy of cognition actors, is predefined.

It's hard-coded.

For each robot, it's the same.

The only thing that's learned is what policy is more effective over time, based on experience.

The robot acted for my reasons, for the programmer's reasons, not its own.


SPEAKER_00:
The robots had autonomy, but no agency.


SPEAKER_01:
In 2022, I joined the Active Inference Institute and continued this project under the symbolic cognitive robotics tag.

And I asked myself, what if a robot started with only a rudimentary society of mind?

What if it didn't have that pre-built society of mind, this hierarchy of cognition actors?

It started only with the most basic ones, the ones that wrap sensors and motors.

Then the society of mind would grow from experiencing autonomous engagement.

So the robot would start

and with all these very basic prior cognition actors for sensing and affecting, and then would create new cognition actors.

And these cognition actors would be very, very simple in the sense that they would take their sensations from the sensors themselves and would act by

engaging the motors, and then observe how it changes its perceptions of, say, color or distance and whatnot, etc.

So, cognition actors would be added to the society of mind, brought them up to make sense of observations and actions.

Now, each new cognition actor would select an umwelt,

composed of cognition actors one level below.

And how do we know that a particular level of cognition actors in this growing hierarchy is complete?

It's when all cognition actors one level below belong to an umwelt.

Now, it would not only grow, but it would also shrink, because whenever cognition actors were recognized to be hopelessly ineffective,

they would eventually be removed and new cognition actors would take their place.

And the hope would be that as this cognition hierarchy grows, becomes more competent, we would have a robot that starts acting intelligently and in survivable ways.


SPEAKER_00:
So the society of mind would grow, shrink, and evolve as needed to survive.


SPEAKER_01:
Now that's a key point here.

This is an implementation of a form of moral computing.

obviously the the robot is physically the wheels uh the sensors themselves uh are not mortal they're like a a computer on wheels you turn it off you turn it on it's it's still alive it still it hasn't died by from having been turned off however the society of mind itself is mortal it grows it evolves

such that the robot will be competent at finding the food it needs to keep the society of mind alive.

So what we have here is a mortal society of mind trying to achieve a sufficient level of competency to maintain itself.

This is a work in progress.

a system that I'm in the process of building.

And it's composed of these subsystems, agency, where the smarts are for growing, shaping, and evolving a society of mind.

There's another subsystem that runs on the robot itself, which I call body.

which provides an interface to activate motors and to read the sensors.

There's also a version of the body subsystem, which will run in a simulated world.

And the subsystem world implements that simulated environment.

It will also animate a real environment.

And then there's a subsystem that I call observer, which provides me or any other human being with a window into what's going on within the society of mind of the robot.

The agency subsystem runs on a powerful computer.

The body subsystem, the physical one, runs on the brick pie, which is powerful enough to run the

uh significantly complex programs but not too computationally intensive ones and um world and observer and and body simulated run on a separate computer the karma body subsystem as i said gives access to real and virtual sensors and effectors and here i have all the um the sensors i want to use and the motors

removed from the robot, just connected directly to the RigPy.

And they can be activated, they can be reached via a REST API, which is essentially, they expose a web interface to the agency subsystem.

The Karma world subsystem, here we see the simulated environment.

It's a grid world.

The gray tiles represent just the surface and with brown tiles representing obstacles.

So the robot will bump into these obstacles and will perceive them with its simulated distance sensors.

The green patch represents a food patch, and the robot needs to find food in order to consume the food and replenish its resources.

If it runs out, then it dies.

We can see that some tiles are darker, others lighter, and this gradient, this luminance gradient, corresponds to

a scent gradient, so the closer the robot is to the food, the lighter the tiles are.

There will also be poison patches, which will be red, so detectable as a red surface on the simulated floor.

The robot must learn to avoid poisoned food, because that will harm it, and if it's armed,

excessively harmed, then it will also die.

When a robot spends enough time over the food, the food is consumed and another food batch appears somewhere else.

So the environment is dynamic and it changes with its interactions with the robot.

That's an important element here.

Now the Karma agency subsystem is the most complex and the key subsystem, and it implements a generative cognitive architecture for the robot implementing agency.

So it's going to implement cognition actors, as we've mentioned already, it will implement

something I call metacognition actors, a very limited understanding of the concept of metacognition, one metacognition actor per level in the hierarchy, and the cognition actors are responsible for adding, removing cognition actors as needed, as required.

Then very, very importantly, we have well-being actors, and these actors represent the homeostatic values

that the society of mind must keep within acceptable bounds in order to survive.

We have fullness, integrity, and engagement.

We'll talk in more details about what they are later.

And then very importantly, there's an apperception engine actor.

Last year at the symposium, I presented

my implementation of a perception engine.

And this is a very important component of the society of mind.

It is responsible for making sense of observations in order to be able to make predictions.

We'll go into more details as well.

Now let's go and look at these well-being actors.

They implement homeostatic states.

And I have three.

One is fullness, which represents how much energy the agent, as a whole, the robot, has available to do things.

That could be computing, doing some expensive computation, or just moving about.

So there's an energy budget that gets depleted

and also gets replenished when the robot finds food and spends enough time in a food patch.

There's also the integrity well-being actor, and that represents the health of the agent.

If the agent, the robot, suffers too many collisions, then integrity goes down, and integrity

is recovered over time.

Think about lives in a video game.

That's pretty much the same concept.

And finally, a little bit more abstract well-being actor is engagement.

And it represents to what degree the robot is engaging its environment.

We want to avoid the dark room situation where the robot stops doing anything.

stops moving, stops engaging its environment, it feels, quote-unquote, an imperative to engage its environment.

Now, the agent as a whole, meaning the society of mind of the agent, strives to maximize well-being for the entire society of mind.

Now low well-being levels will signal risk to the survival of the society of mind.

So that information will be available to each individual cognition actor and to the mental cognition actors and will modify, constrain what they do.

The concept here is that well-being imparts normativity to beliefs, beliefs acquired

in the context of a low fullness or low integrity, becomes an unpleasant, undesirable belief.

Now, this normativity associated to beliefs will focus attention and motivate action, as we'll see later on.


SPEAKER_00:
So well-being actors aggregate metrics.


SPEAKER_01:
received from the various cognition actors, and then aggregate these metrics and broadcast wellness events back to the cognition actors, essentially informing them of the homeostatic state of the agent as a whole.

Now, these metrics include the work that's been done, the food that's been consumed,

These two will impact the fullness metrics and values.

Then poison consumed, collision count will impact the integrity, overall integrity of the agent.

And then predictions made, predictions received, prediction errors sent, policies executed.

We'll see that in more detail later.

All these are indicative of the society of mind engaging its environment.


SPEAKER_00:
Now, metal cognition actors.


SPEAKER_01:
Their role is pretty straightforward.

They're responsible for adding cognition actors that are missing and to remove underperforming cognition actors.

Each cognition actor is assigned one level in the abstraction hierarchy.

And it looks at the cognition actors under its supervision.

the provision, and determines if all the cognition actors below them are part of one or the other's umwelt.


SPEAKER_00:
So the notion of umwelt here.


SPEAKER_01:
A cognition actor can only observe or act within its umwelt.

And what is the umwelt of a cognition actor?

It's other cognition actors of a lower level of abstraction.

So at the first level of function actors, the prior level where they wrap sensors and motors, they don't have an umwelt.

Well, they do.

It's the actual external environment of the robot.

They don't have an internal umwelt.

Anything above this level, the cognition actors have, as part of their umwelt,

other cognition actors, and they constitute essentially the internal umwelt within the society of mind.

So if one layer of the society of mind is complete, it covers all the cognition actors below, then it's time to create a cognition actor at the level above.

create a new layer of the society of mind.

And when you create a cognition actor, you give it, or the metacognition actor gives it, its umwelt.

Mostly randomly, with the constraint that there's going to be overlap between umwelts of various cognition actors, and making sure that they have access to the motors


SPEAKER_00:
directly or indirectly.


SPEAKER_01:
Now, when well-being is low within the society of mind, when the well-being actors are reporting low well-being levels, this might accelerate the removal of cognition actors, the metacognition actors being aware that we're not doing well now,

we'll say, okay, which cognition actors are underperforming our expectations, and let's remove them in order for them to be replaced with new cognition actors, which hopefully will do better.

And if well-being is low, this will also slow down adding new cognition actors.

So there's an influence of well-being on the actions of the mid-talk cognition actors.

I want to say adding and removing, it's important to understand that adding is bottom-up and removing is top-down.

For example, if I have a cognition actor that has, let's say, two other cognition actors in its umwelt, I cannot remove a cognition actor in its umwelt because it depends on it.

But I can remove a cognition actor that is not in any other's umwelt, so that


SPEAKER_00:
Removal will be from the top to bottom, and addition will be from the bottom to top.

What does a cognition actor do?

Well, a cognition actor observes its umwelt of other cognition actors.

It generates beliefs from analyzing its observations.


SPEAKER_01:
And then it generates policies in order to modify or validate its beliefs.

And it does this, this observe, believe, and act always within its umwelt and one timeframe at a time.

So each cognition actor goes through its more or less OODA loop one timeframe at a time.

Now, the lower a conscious actor is in the society of mind, the more concrete it is, the less abstract it is, the shorter this time frame will be.

It will cycle faster.

It's important also to realize that there's no...

There's no synchronization between cognition actors.

Each starts its time frame independently of other cognition actors, and so that they may overlap, but they don't coincide, they don't overlap completely.

And the more abstract the cognition actor is, the higher up it is in the hierarchy of cognition actors within the society of mind, the longer its time frame will be.

which means that very abstract cognition actors will update their beliefs more slowly and will take actions not as quickly as a more concrete low-level cognition actor.

So we can see here the cognition actor, the large one, surrounded by its low-level cognition actors.


SPEAKER_00:
What does the cognition actor observe in its umwelt?


SPEAKER_01:
Well, it observes the beliefs of the cognition actors in its umwelt.

So, as a more abstract cognition actor, I observe the beliefs of the more concrete cognition actors in my umwelt.

then based on these beliefs and as we'll see based on the normativity associated to the beliefs are they good beliefs are they bad beliefs the cognition actor will decide on what its goals are in this current time frame the goals would be modifying a particular belief or validating a particular belief then it will generate policies to attain this goal and then

emit this policy to its umwelt and say okay do what i want and we'll see in more details what do what i want means so within each time frame a cognition actor will predict its umwelt after having made sense of it that's a key uh aspect that will will go into more details

It will update its beliefs by analyzing past and present observations.

So a function actor has a memory of the current observations in the current time frame and observations made in previous time frames.

And it will update its beliefs from analyzing current observations and past observations.

Then it will select a goal affecting a belief it intends to impact and construct a policy and recruit its umwelt to realize it.

So if you look at the little redesigned OODA loop here, of course, there's predict, make predictions about beliefs of other cognition actors in my umwelt.

In order to predict, I need to have a causal model of my observations.

That's what apperception will do.

We'll see in more details.

Then, as the cognition actor, update my beliefs, select goals, act, and back to predicting in the next time frame.

Apperception is defined here as the process of assimilating sensory information into a coherent, unified whole.


SPEAKER_00:
Let's look at predicting.


SPEAKER_01:
What are the observations of a cognition actor?

Observations are predictions it makes about the beliefs of other cognition actors, which are in its umwelt, plus prediction errors.

So it makes predictions of its umwelt, of cognition actors in its umwelt, about their beliefs.

And the

Umweltkognition actors receive that prediction, look at it, does it correspond to my current beliefs?

If yes, not saying anything, I'm not contesting the prediction.

If it doesn't match, then I emit a prediction error.

And a cognition actor accumulates these predictions that are uncontested and the prediction errors, and together they constitute the cognition actor's current observations.

So we'll see that not all observations receive the same attention, that they're not equally subject to prediction.

There's a prioritization.

There's a notion of attention.

We'll see that a little bit later.

Attention.

Attention prioritizes which observations to update.

So the cognition actor starts a timeframe with observations made in previous timeframe.

And it wants to make those observations current to the current timeframe, as many of them as possible.

But there's a limited amount of time in which to update these observations.

So the cognition actor will need to prioritize which observations from the previous time frame to update into the more current observations.

So which one will it try to update first?

Well, it will look at the predictions that were made by higher level cognition actors about its beliefs.

And knowing which observations support the beliefs will say, oh, these are the observations that more abstract, more important cognition actors are making about me, my state, my beliefs.

So I'm going to try to make sure that I have up-to-date observations that are of interest to my parent,

parent function actors so that's the first one i care about the first observations i care about the ones that are of interest to my parent function actors actor sorry or actors the second most important observations in in terms of uh updates are the observations that support the strongest desirable and undesirable beliefs remember that uh

beliefs are the result of analyzing observations current and past if i have very strong desirable or very strong undesirable beliefs then i will want to verify that those observations are still current so there will be the next observations that i will try to update then finally the the observations that support weak or neutral beliefs and then

whatever remaining observations they are.

So the function actor will prioritize the observations that existed from the previous time frame and say, OK, I'm going to now make predictions that these observations are still valid or not valid.

And if there are prediction errors, then the observations are updated to the prediction error.

And if the observation is not contested, then the observation is the one that is current.

This implements a predictive processing architecture because, as you can imagine, a high-level cognition actor will make predictions about beliefs.

of its children cognition actors.

This will trigger predictions about the observations supporting these beliefs one level down to the cognition actors in the layer below and all the way down.

So predictions flow down and prediction errors flow up through the entire hierarchy of the cognition actors in the society of mind.

Now, in order to make predictions, we need to have made sense of observations.

That's what apperception does.

So we need to make sense of observations in order to predict them.

Now, in order to explain the concept of apperception, in which in last year's presentation I went into quite a lot of details, but just I want to give an overall understanding here, let's take the example of

observing two lights light a and light b and the light can either be on or off and at different steps in time the lights may change their state from on to off off to on or not so here we have a set of observations of light a and b over time and sometimes we may

not be able to observe a light, like light A in the fourth state is a gray rectangle, we don't know.

But we have a good enough set of observations, and now the goal of the apperception engine is to find a causal theory that explains how the observations change over time.

why are we seeing some a light a turn on after having been off or etc in order to formulate a causal theory kind of a little program that when run recreates what we've observed the app perception engine may need to imagine another light it might need to imagine

some kind of connection between the lights.

So the apperception engine will, in formulating a causal theory, will do some abduction, some imagining of unobserved latent objects.


SPEAKER_00:
So we have a number of observations that are remembered from previous timeframes.


SPEAKER_01:
we may uh we don't accumulate all observations forever we forget the older ones and then we feed these observations into the app perception engine out of that comes a little program a causal theory that when applied to the latest observation will produce the next incoming expected observations

So that's a way of making predictions of incoming observations from current and past observations, from the current observations given the causal theory.

So you feed the latest observation, you feed all the observations into the app perception engine, it gives you a causal theory.

You feed the latest observation into the causal theory as input, the output is incoming predicted

observation.

So apperception means finding a causal theory that explains all the rules of the theory and predicts observations.

So a cognition actor accumulates observations, wants to make sense of them in order to predict incoming observations, and for other reasons as well, which we'll see.

And what it does is that, okay, it gives the app perception engine actor its observations over time and says, please explain to me what I'm observing.

Give me a causal theory so I can predict future observations.

Given this causal theory, if one is found successfully, then the cognition actor can predict incoming observations.

And what does it observe?

It observes the beliefs of cognition actors in its umwelt, more concrete cognition actors.

And then if the prediction is correct, if the causal theory is correct and accurate, we don't get a prediction error.

If the causal theory is partially correct, sometimes correct, then we get a prediction error.

So, getting prediction errors undermines confidence in the causal theory that the conjunction actor has received from the upper Shepson engine to explain what it sees in its umwelt.

So, it's quite possible that there's a moment in time when the observations, when the causal theory is quite effective and it's accurate, but then something changes in the state of the umwelt conjunction actor such that the causal theory is

is missing is very hit and miss and it may come a point where the cognition actor says i don't like that causal theory anymore it's it's producing too many prediction errors so it's going to ask a perception engine actor give me a new causal theory based on the latest observations that i've accumulated in my memory okay so we looked at um our cognition actors

predict incoming observations in order to accumulate those new observations, to update those new observations based on whether or not these predictions are contested by prediction errors or not.

So now, once we have a new set of current observations within this timeframe that we are operating in, it's time to update beliefs.

Beliefs are, if you want, are the integration of observations into a more aggregated analytical assertion.

So a cognition actor updates its belief from analyzing remembered observations and also remembered actions that it took.

So we'll see what kind of integration of observations and analysis of observations lead to what type of beliefs.

But whatever type of belief it is, a belief will be desirable or undesirable depending on how it correlates with the agent's well-being.

If a belief is being held mostly while the agent as a whole is feeling bad, it has

low resources or low integrity or or limited engagement then these these beliefs are going to be tainted by the well-being levels of the agent at the time those beliefs are held now that's what makes a belief desirable or undesirable what makes a belief strongly held is if it is supported by observations

those observations from which analysis the belief is derived.

If it is supported by observations that are predicted by an accurate causal theory, we have an excellent causal theory, it rarely misses, so the cognition actor is feeling confident about its observations and it makes whatever belief is produced from analysis of these

successfully predicted observations, better supported, more strongly held.

Now, if a belief persists over many time frames, thanks to or in spite of actions taken, actions taken to validate or invalidate it, if the belief persists

then it's even more strongly held.

There's increased confidence that the belief represents what's really going on in the umwelt of the cognition actor.

Now, there are different kinds of beliefs.

Beliefs are produced from the analysis of current and past observations.

And OK, so there's abduction.

One type of belief is abduction.

And it's a direct consequence of having requested a causal theory and using a causal theory.

Remember, a causal theory may imagine a latent object or a latent relationship between objects.

Well, that's a form of belief.

I believe there's an object that I don't see, but must exist in order for me, for a causal theory to make sense of actual observations I made.

So an abduction belief would be like, I assume there's a hidden thing next to the one I see.

another kind of belief is just counting i'm i'm next to two things i have made two observations that represent uh being next to something and so i have two of them so i'm i can have a belief that i'm next to two things account belief now a trend belief is a it's a belief that represents the the the uh discovery of a trend

in a certain certain observations over time for example if the connection actor observes distances to some obstacle and those distances are getting smaller over time then that's a trend my distance to a thing is getting smaller and then there's another kind of belief is is some observation

ending, stopping.

My getting closer to a thing stopped.

So that's an ending kind of belief.

And then if the connection actor has taken some action, well, there's the belief that he tried to do something like I tried to stop my distance to a thing from getting smaller.

So if the action was trying to end the belief that the belief in a trend,

You can see now that beliefs are composed.

They're getting more and more abstract.

So I try to stop my distance to a thing from getting smaller is another kind of belief.

It's an attempt kind of belief.

So we have all these different kinds of beliefs.

And when a cognition actor synthesizes the belief from analyzing its observations, it assigns the belief a globally unique predicate name.

parent cognition actors, the cognition actors in which umwelt this cognition actor belongs, the parent cognition actors observe, predict the beliefs of cognition actors in their umwelts and reference them by name.

So if there's a belief in an umwelt cognition actor that some trend exists,

we're getting closer to an obstacle.

Well, this belief will be exposed, will be made available for observation to the parent as a named predicate.

Why and how that belief came about, what the observations are by the umweltkrankchen actor that led to that belief,

it's it's it's opaque there's a name and that's the name that's available for observation there's a a a statement a predicate a name predicate that's available for observation by the parent cognition actor only the cognition actor that holds the belief it named knows which of its own observations support the belief and how so if i say

to you, I believe getting closer.

That's something that you can observe as a parent connection actor.

Ah, my child connection actor believes in getting closer.

The parent cognition actor doesn't know why I believe we're getting closer, doesn't have access to the set of observations that led to me holding the belief that we are getting closer.

All these distance observations that led to me holding the belief that getting closer.

All the parent knows is that I hold the belief called getting closer and it's able to observe it, observe it

you know, ending or starting, et cetera.

So that's a key concept that beliefs are essentially named, there are predicates, there are names, if you wish, that hide the reason why the belief is held to parent function actors that observe my beliefs.


SPEAKER_00:
So there's information hiding going on here.


SPEAKER_01:
So a cognition actor observes the beliefs of its children cognition actors, and these beliefs that are observed are the consequences of the observed observations of cognition actors one level below.

So my belief is

another uh if i'm a cognition actor my belief is another cognition actor's observation so it's really beliefs it's about it's beliefs about beliefs about beliefs so it's beliefs all the way down that's that's a maybe a little tricky concept if i'm at the top as a cognition actor i observe beliefs of connection actors under me these beliefs of connection actors under me are

supported by their observations of beliefs of cognition actors under them.

So beliefs about beliefs about beliefs.


SPEAKER_00:
Time to open the little parentheses here.

Maybe take a break.

The implementation that I'm working on of agency is a symbolic implementation.


SPEAKER_01:
So it's agency realized from symbolic reasoning.

All the various agency actors, the cognition actors, the metacognition actors, the well-being actors, the perception engine actors, all run prologue programs.

Prologue is the primary language.

And those prologue programs do symbolic inferencing.

cognition actor as we've seen invents symbols events gives names to beliefs it discovers from analyzing observations so it invents symbols and it synthesizes these beliefs as symbolic statements getting closer to xyz the app perception engine as we've seen very kind of briefly

Its job is to discover causal models, and these causal models are symbolic programs.

They're little logic programs.

So agency is built on symbolic reasoning in my implementation.

Closing the parenthesis.

Now let's talk about precision and belief strength.

The more accurate a causal theory, the more precise are considered its predictions.

So we saw that earlier.

Now, precise predictions, if they're not contradicted by prediction errors, do strengthen the beliefs the predicted observations support.

So you have a bunch of observations held by the cognition actor.

Observations in the moment, remembered observations.

These observations are the result of predictions and or prediction errors.

And we analyze these observations

coming from predictions, to generate beliefs.

I'm getting closer to an obstacle from observations of distances.

Now, if my observations of distances were predicted by a really top-notch causal theory, one that almost never misses, then I have confidence in the beliefs that have derived from my predictions.

I feel confident that I am indeed getting closer to an obstacle, given all the distance observations that I've accumulated, because I've accumulated them

by making predictions using a very empirically accurate causal theory.

Precise predictions strengthen the beliefs that are derived from predicted observations.

The stronger beliefs are, because I may have strongly held beliefs and weakly held beliefs, the more strongly held beliefs are, the more attention they will receive when selecting, when the cognition actor wants to select one and say, I don't like this belief.

It's associated with poor well-being.

And I want to, I want to change that belief.

I want, I want to impact it.

I want to, I want to get rid of that belief.

I want to make that belief not true.

Or if it's a very pleasant belief.

Oh, I want to make sure that it is, I want to validate it.

I want to keep that belief.

Uh, I want to keep holding that belief because it's a good one, but I care whether the belief is strongly held or weakly held because strongly held belief will get more attention would be the ones that will be selected.

for validation or invalidation above the weakly held beliefs.

So how do we impact beliefs?

That's the next two steps in the OODA loop of our cognition actor within each timeframe.

It predicted based on its understanding of past observations.

It updated its belief from analyzing

predictions that were realized or error-solid predictions, i.e.

those observations.

It updated its beliefs.

And now it says it needs to select a goal or it may need to select a goal that said, I want to get rid of this belief or I want to validate this belief.

And how am I going to accomplish this goal?

I'm going to select the policy and then ask my umwelt to act, to realize this policy.

So that's what we're looking at now.

a cognition actor acts via policies it formulates a cognition actor strives to invalidate unpleasant beliefs and to validate pleasant beliefs but it will prioritize strongly held beliefs whether unpleasant or pleasant a cognition actor will impact the belief validate or invalidate it it holds by uh promoting or inhibiting

the observations that led to it.

Now, I have a belief that we are getting closer to an obstacle, and somehow it's an unpleasant belief.

Maybe it's associated in the past with a decrease in integrity, at least bumping into things.

So I want to put an end to getting closer to an obstacle.

I don't want this belief that I'm getting closer to an obstacle to go away.

Now, I hold this belief because I had observations of reducing distances over time to some obstacle.

So I want to change these observations, basically.

I want to stop having distances that get smaller.

I want to stop this.

So my observations of distances happen to be beliefs of function actors in my umwelt.

So what I'm going to do, I'm going to say, OK, I'm going to select a number of observations that support the belief I want to get rid of.

And I'm going to formulate a policy as a list of goals to my umwelt CA.

I'm going to say, I want you to get rid of the fact that I have small distances to an obstacle.

That's your goal.

so i'm gonna i'm gonna step a number of goals me the cognition actors who believes we're getting closer to something and it's not doesn't feel good i'm going to tell i'm going to formulate a policy so that my umwelt connection actors will try to impact their own beliefs that we're we have a small distance to an obstacle for example

so that's that's a little bit subtle here so i'm as a cognition actor i want to modify a belief that belief is based on observations these observations are beliefs of umwelt cognition actors so i'm going to say to my umwelt cognition actors i want you to in to modify your own beliefs that i've observed i want you to change your beliefs

Beliefs that I'm observing that lead me to believe that I'm getting closer to an obstacle.

So I'm setting up a bunch of goals for my umwelt function actors so that if they realize, if they modify these beliefs, then it will modify indirectly my own beliefs that I'm trying to get rid of.

Okay, so I'm basically telling my umwelt, these are the goals I want you to realize.

so that I would indirectly realize my own goal.

Okay, so this policy that I put together are a set of goals for my umwelt cognition actors to realize.


SPEAKER_00:
Now, an umwelt cognition actor will then, given the goal it receives, formulate its own policy


SPEAKER_01:
Because I'm asking a cognition actor in my umvel to modify some belief or other.

Okay, says the cognition actor, then I will formulate my own policies to do that, to achieve the goal that my parent, the cognition actor, sent me.

So, a cognition actor will receive goals from

parent cognition actors, and these goals received from IRUP will have higher priority than whatever goals the cognition actor might want to formulate on its own, because the umwelt-cognition actor has its beliefs, has beliefs that it wants to validate, has beliefs it wants to invalidate, and left to its own devices will do just that.

But if it receives goals from a parent cognition actor, these take priority.

and it will put aside its own concerns to address the concerns of the parent cognition actor.

A cognition actor cannot execute only one policy at a time, which is why it will prioritize the ones that it receives from its parent.

And when all is said and done, the cognition actor that initiated the policy will determine, will verify if this was successful or failure over time,

if the belief that we wanted to impact was indeed impacted, and a policy that's known to work will be reused.


SPEAKER_00:
That's how cognition actors develop habits.

Okay.


SPEAKER_01:
So, where does the causal theory that a cognition actor holds guide the formulation of its policies?

Because causal theory serves two purposes.

roles one is to predict incoming observations and the other role is to formulate an effective policy so let's say that cognition actor is a belief that wants to impact because uh the well-being levels are low associated to that belief so he wants to modify that that belief get rid of it well it needs to formulate a candidate policy which of the observations that i made

have analyzed into this belief which which observation that support this belief i want to modify so which policy do i want to formulate well what it will do is we'll say okay what do i need to do given what my what i'm observing right now so that i will observe something differently that will either


SPEAKER_00:
remove the belief or reduce it.


SPEAKER_01:
So it will use the causal theory.

So what would happen if I did X, if I asked my umwelt to do XYZ?

So the causal theory will answer that question.

It will say, well, plug that into the causal theory.

turn the crank, and this is what you would be observing next.

If you were doing this, if you executed this action, this is what you would observe next.

And then, given what you would observe next, you can calculate, ah, these would be the anticipated beliefs out of these anticipated observations.

Might the policy work?

Would the belief that I want to impact be impacted the way I want to impact it?

If so, yeah, let's use that policy.

Otherwise, let's look for another one.

So the cognition actor searches for an effective policy given its understanding of how things work in its umwelt, its causal theory, and then applies the causal theory, predicts what would happen if it took this action, if it

if it requested this goal to be realized, looks at the resulting anticipated beliefs and decides, was this a good, effective policy?

And then it would emit that policy and ask its umwelt, please achieve these goals so that I achieve my goal.

And that percolates down because

Each cognition actor being given a goal from the parent cognition actor says, ha, how should I best realize that goal?

What policy should I formulate for my own umwelt in order to realize the goal that was given to me by my parent?

So it's policies all the way down.

and it bottoms out when at the level where effects are produced moving motors turning moving forward moving backward whatever okay so that's a lot of a lot of content describing this cognitive architecture this generative cognitive architecture what i hope will happen

is through this growing, shrinking, evolving, refining of this society of mind composed of cognition actors in the context of homeostatic measures provided by well-being actors.

I'm hoping that with policies going down, beliefs

percolating up, predictions going down, prediction errors going up, all this changes and evolution of the society of mind, all these activities from all of this would emerge agency.

And I'm looking at that as a form of operational closure with constraints flowing down, constraints flowing up, and eventually, hopefully,

The society of mind will explore a space of possible organizations and space of possible ways of defining cognition actors, relating cognition actors together, such that it leads to behaviors of the robot that preserves the life of the society of mind.

That society of mind never reaches a point where it runs out of fuel, where it is damaged beyond repair.

That's the overall goal.

I think of this society of mind as kind of autopoietic, with all these elements of the society of mind leading to other elements.

For example, a causal theory will lead to predictions

Predictions will lead to prediction errors.

Prediction errors will lead to observations.

Observations will be fed into perceptions, which will lead to an updated causal theory.

And observations lead to beliefs.

Beliefs lead to policy.

A causal theory will determine which policy is more likely to work.

Policy leads to effect.

Effects lead to new sensations.

Sensations lead to new observations, and so forth and so on.

I look at this as a kind of a set of processes that support each other in something of a closed set of complex loops leading to a society of mind that survives over time.

Now, there's a number of philosophical stances that are taken by this project.

And by philosophical stance, I mean

pragmatically justified perspectives, a way of seeing the world.

First, I look at active inference as a normative framework.

I don't think that's controversial at all.

I mean, that's what it's defined to be.

And to persist, an agent must behave as if it acts to minimize surprisal.

Clearly, I'm not implementing

the society of mind using the mathematics of the free energy principle but i'm implementing it hopefully such that doing an active inference analysis of the robot's observable behaviors and states will demonstrate that the society of mind the agent as a whole is acting such as to reduce surprisal

So I'm implementing an agent and also its environment as a system of generative processes.

The active inference analysis will be done in terms of generative models that are inferred from observing the behaviors of the robot and observing the states that are visible to an outside observer.

Other philosophical stances, I'm just going to go through them very quickly.

Each one is worth an entire presentation.

I take the stance that cognition arises from an embodied agent engaging its environment.

That sense-making is the discovery of unified causal theories.

That meaning is grounded in the agent's drive to survive.

No mortality, no meaning.

that intelligent emerges from interactions between a collective of parts, that's the society of mind, and that the parts constrain the whole, and the whole constrains the part.

So, constraint closure.

I'm implementing, I'm in the process of implementing this system that will hopefully demonstrate artificial agency.

and here's the current progress there are three phases in this project phase one i developed in our cognitive architecture that's what i presented today uh implemented uh one version of the of the app reception engine and that's what i presented last year i implemented uh the

subsystem to realize a to connect to sensors and actuators that's the karma body and i implemented a simulated grid world in which a simulated um agent would would operate and and interact that's to speed up uh implementation because it takes a long time to uh if i were to do this only in the real world

the edit run debug loop would be extremely long.

And so I'm in the process right now of implementing the agency capabilities in phase two, which would be about six months to a year from now.

I will have a redesigned robot body.

I will be testing in a real life environment

And I will be collecting a lot of data through the observer subsystem.

And in the third and final phase, the data gathered from runs, both in real life and virtual world, will be subjected to active inference analysis to validate that my agent does survive because it minimizes surprisal.

that's it um if you are interested you can contact me at uh this email address jf dot

the L-O-U-T-I-E-R, at activeinfluence.institute.

You can also reach me on the SimCog robotic channel of the Institute's Discord.

And if you're interested in the source code, it is on GitHub.

So I'd like to thank the Active Influence Institute for support and encouragement.
SPEAKER_01:
okay welcome back everyone we are here in part one of the workshop that john clippinger and our partners at first principles first are coordinating so with that for the next session please john to you and take it away thank you very much um


SPEAKER_02:
I would like to just to share a deck with the group here, if I could, and provide a sort of an overview of what we'll be talking about in this particular session.

Can you see that now?

Is this good for you?

Okay, great.

And we'll be anticipating with Andrew is,

James Forrest, Norcal PTAC, she will be coming on and providing the tutorial, but what I would like to do this in the first instance of this is is being able to provide an overview of why.

active inference is important, why active inference agents, this whole perspective that we're taking here is worth people's attention and being concerned about.

And what it has to offer to address some of the limitations of the current AI systems, for example, large language models and some of the agentic architectures that have come out of them.

And in that context, what I'm looking at is sort of what is the current state of large language models and agents.

And I think there's a lot of activity that's been talked about, but there is still no mission critical applications yet.

There's a lot of exploration.

You have people like

Salesforce developing little Einsteins that can do various kinds of workflow automation.

You have Copilot and Microsoft, but it's not really core critical.

Not to say that it won't be, but it is still on the experimental phase.

It's still a black box.

We do not know a lot how it works.

Hence, you have issues of data providence.

You have a question of how to govern it.

And there's more recently, there's a concern about you invested a trillion dollars, but there's an unknown business model coming out.

And so there's a lot of money that's going into it, but it's not clear that the current framing of, uh, of, uh, AI, um, is it's now known is really, it's going to be viable.

And part of the issue here is, is that we, and I love this example from the New Yorker is like, who's modeling, who we're actually putting data into the models.

we're trying to write representations and we have that interaction, but there's not necessarily an abstraction that's coming out.

So it's kind of this relationship of seeing ourselves, mirroring ourselves and going into this sort of cycle.

And what we have now is a certain hype cycle.

So it's around the peak of the, this is 2023 from the Gartner Group

And so the generative AI is at the top of the hype cycle.

And then it's hard to read this particular graph, but you'll see that principle AI is basically on the early cycle.

It's on the early phase of it, but it's not seen as taking very long, where something like you're going to have

AGI is seen as being like 20 years out.

It's the top of the hype cycle.

I think if we update this now, I think it's starting to be appreciated that the generative models and expectations and the kind of problems and the costs associated with them and then the lack of improvement

The lack of degree of improvement for successive models is starting to show through.

So the next generation open AI shift strategy is the rate of GPT AI improvement slows.

In other words, the new Orion model did not have the level of improvements that they were expecting.

I think that is something to take into account.

And hence, the cost of training and inferencing is extremely high.

And with that, the fact that you have this enormous consumption of energy, Microsoft is going to have to recommission a three-mile island and drive one of their own data centers.

This shows the impracticality of having to have so much computation in order to make these models work.

i i think that this is starting to be recognized within the the the limitations of large language models and sort of the the parametric approach to to uh modeling uh uh ai modeling and i think just to start to step into what what we have now what we're looking at i mean i love this is like another cartoon from the the new yorker and since

So to think it all began with letting autocomplete finishing our sentences.

And well, autocomplete is a sort of form of Markov chain.

So we are having this sort of chains on top of chains and we've been able to develop some pretty rich representations of human activity of text and actually forms of cognition, but there's not an underlying model there.

And so what we're looking at, there may be some foundational issues that are coming up in very large language models.

And the fact that it is a black box is not explicable.

It lacks internal causal models.

There's not a way of reducing the complexity.

It's sort of a batch processing.

There's no real-time updating.

And that batch processing we've seen is extremely expensive.

And so we're caught on that.

that issue.

It's not energetic.

It has no notion, no means of a spontaneous adaptation and correction on its own.

And it certainly doesn't know what it, it doesn't know.

It has no way of representing its completeness of this knowledge and adapting coherently to that.

And also I would say it's not derived from sort of fundamental scientific principles.

It's really sort of an engineering phenomenon on a case by case basis.

Now this is changing.

And I think the same issue, if you start to see foundational issues for multi-agent LLMs, in other words, the transition to making agentic architectures where you can create a particular agent that's optimized to do a particular task or role, a lot of the success of that agent is depending on the quality of the prompts of that.

And sometimes those prompts are good, sometimes the prompts are bad, but there's not a really principled way of doing that currently.

And so they have a very fixed way of, and specialized kind of goals.

It's sort of a mechanical, it's sort of the instantiation of an industrial model that we see.

Now, what we, rather than having a sort of biological self-correcting model, there is sort of these fixed static models.

Now, what I would say the active inference difference here is that it's really developed from first principles.

And I think you're going to see that Andrew's going to go into this much more detail.

But it really cuts across different disciplines that themselves have been maturing fairly quickly in the last two or three decades.

And so it is in basic physics, quantum information theory, the whole idea of computational science, biology, and cognitive science, all these are intersecting.

into creating a new way of looking at what intelligence and life are and how they're interconnected.

And it also builds on well-established statistical methods, including Markov blankets, Blasian belief models, the stuff that work has been done

for Jadea Pearl.

And basically what you look at active empycin does, and I just sort of highlight, is it codifies a predictive brain thesis that's been nurturing in neuroscience.

And then that in turn sort of mimics the processes of the scientific method.

So think about what the method is that you're trying to codify.

And in turn to Richard Feynman's comment, well quoted comment,

I can't, well, I can't create, I can't understand.

So there's a new kind of criteria for what it means to have a, to understand something, is be able to model, to create a digital twin.

If I can recreate something, then I understand what it is.

And if I can replicate that behavior, and that's sort of the principles that you find in active inference from the very beginning, is how do I create a generative model?

How does a living agent have a generative model of itself?

itself and its world.

And this really owes its origins back to

the work that's done with Judea Pearl on his understanding causal reasoning and also Markov blankets.

What is an autonomous thing?

And this is where Carl Fritz and generalists work.

How does something maintain a separation from the world in which it is in?

And so it is not governed by the things, but maintains its autonomy.

And there's a certain kind of topology there

And something has certain kinds of internal states.

It has external states, observer states, and action states.

So there's a lot of deep concepts that's in it.

I'm sure Andrew will go into that in much more detail.

And then what we have is the active inference agents themselves as sentient, alive things.

So you're creating something that's very different than, say, a large language model.

You're taking something that has agency

and it has a representation of itself in the context in which it's trying to stay alive.

And so it's self-organizing, it's self-healing, it's self-replicating, and it's symbiotic in the sense that it involves complexity, mutual synchrony with other living things in its world, in its niche.

It's a joint niche construction.

Hence, sentience is a kind of situational intelligence, an awareness that's grounded in the survival of that particular agent.

And the person that's sort of the context of the person who's really behind this is Carl Friston.

And my own experience of being someone who was very interested in sort of an artificial intelligence from many, many years ago and self-organizing systems of cybernetics, it really hasn't been until the last five or six, maybe 10 years, that this has come together in a coherent, not only coherent theory, but a whole coherent formalism

and what Carl would call physics of living things.

And he's one of the most scientists in the world.

He's a neuroscientist that's done, basically developed a reputation in neuroimaging and inferring the structure from the brain from neuroimaging.

And he applied the free energy principle, which we'll go into more discussion to later.

And so one of the, I love this idea of a trefoil knot.

It's sort of like, okay, it's,

It is sort of this auto-catalytic biotic system.

It's really important that it's a living system and it has certain kinds of characteristics that are all intertwined with itself.

One cycle synchronizes its ectoception or its representation of the external world to minimize uncertainty.

The other does it internally.

It has to align that with the external world.

And then you have another cycle that has expected free energy minimization.

It makes predictions about how to change itself in its world.

um and so what we have here is basically a a what i think active inverse offers is a really a soundness a grounding in bio and biology and physics and so you have a sort of a ground truth of evidence space until i make predictions and do those predictions allow me to act and survive in the world um so it can be explicit it can be explainable and we'll see we'll talk about this later but you can have

that has causal models and language and knowledge graphs it can explain why it doing what it's doing and also it can talk about explain why how much it doesn't this degrees of uncertainty associated with it and there's also the potential self-policing self-correcting mechanism and i think there's a this is really important in other words you can design

a active inference agent to live within certain constraints and bounds, and you can build in certain mechanisms that have self-governance that cannot be captured.

So its actions are within the limits of its design mission and not to generate negative externalities.

So this, I will sort of leave with this one.

And then just to quote William Blake, who talks about this in a poetic form as a fractal nature of reality,

See the world on a grain of sand and heaven in a wildflower and hold infinity in the palm of your hand and turning an hour.

This just sort of pulls it together in a more poetic sense.

So on this, I would hope to hand off to Andrew.

Thank you.


SPEAKER_03:
Great.

Thanks, John.

And so, hi, everyone.

I'm Andrew Pache.

I'm currently involved with the Active Inference Institute.

It's been great partnering with

with Dr. Klippinger, with the director of the Institute.

We've been doing some really exciting work lately.

And I want to address a lot of the points that John has made, although most of that will come tomorrow when we give some major updates on our bio firm.

So today, in the spirit of an initial workshop and kind of introduction to Active Inference, I'm actually going to refer back to this tutorial that I gave a couple of months ago.

at an international conference for computational social science, meaning folks who are involved in sociology, economics, a broad range of fields.

And the context was applying active inference, a lot of these principles that John is referring to, to social science research.

And I basically make the claim and the argument, this is already being done, so I'm not necessarily providing a bunch of new information, but it's just to kind of extend the dialogue

To computational social scientists like here's how we can apply active inference to you know your particular setting in different ways So we'll just kind of cover some of what I went over and you know I apologize if there's any whiplash from how quickly I go through these slides But we're just going to move a little quickly so the context again of this this this tutorial that I gave is

It was split into three parts.

It was a 3.5-hour workshop where we actually started with talking about agent-based modeling, how it's traditionally been done, and then followed by that, something that I would call a cognitive turn in agent-based modeling that's happened over the past couple of decades, as there are these kind of calls for, we have the computational resources now, we have an immense amount more of empirical

research and validation of different kinds of principles related to human cognition coming from neuroscience, neurobiology, cognitive modeling.

studies of neuronal dynamics, organisms in their environment, all those kind of things.

So there's a shift to that.

And then the most recent means of trying to establish that in the broader state of the art area is using reinforcement learning agents.

So we'll touch on what those are and kind of how that works.

And anyone who's kind of been looking at the agentic

uh framework for for what's being done today they'll probably already be familiar with reinforcement learning then after that we'll go into active inference and how that kind of kind of bridges together a lot of these different things um and apologies i want to just mention real quick uh you can actually go to this this link uh the quickest way of following that is just by going to google type

know my name andrew pasche and then you can type github or ic2s2 in any case you'll come up to the landing page for this you'll see links to all previous slides i've made slides that i'm making for tomorrow i'm also making a lot of this open access uh so so there's code that people can you know play around with and things like that um so

agent based modeling.

This has been done, you know, this goes back to roughly 70s 80s with a lot of, you know, initial developments of computer technology and being able to realize that, oh, we can we can simulate people.

In some loose sense, in some kind of simplistic sense, how can we simulate people?

And why would we want to do that?

We would want to do that so that we could maybe imagine hypothetical scenarios.

Imagine you're designing some kind of policy or want to test a policy and see, oh, if I recreate that in a simulation and have people in the environment where I'm implementing that policy, what happens?

What's the collective outcome?

outcome, what occurs.

So all kinds of fascinating work from shelling in the traditional way of looking at housing segregation, for example, handling those kinds of problems.

Where do people land?

How do they settle in a residential environment?

even more interesting work later after that santa fe institute recreating an artificial stock market where you where you have agents who are basically interacting with this kind of financial environment where they're looking at stock prices changing they make choices to you know buy or sell etc and see how that plays out recreation of housing booms and busts excuse me not housing financial so

This can be applied in many different ways to the point.

And so what is agent-based modeling and its fundamental principles?

And how does that relate back to active inference, which we'll get to?

Key components of agent-based modeling is that we have agents, kind of the catch word of the day.

Traditionally, agents are, you know, they just represent some entity, a living organism.

You know, this can be used for biological and zoological studies of trying to understand, you know, how ants or insects, you know, interact in an environment or at a broader scale, how humans and entire societies and cultures might interact with their environments.

And so what's really important about this with agents, environments, simulations that we just kind of run a simulation over a series of time steps.

And so we let dynamics play out.

And then what is it that composes those dynamics?

It's the assumptions we make about agents.

What is their behavior?

What do we think that they do?

Do we program them in this way or in that way?

And then also what are the aspects or characteristics of the environment?

know and so again given that a lot of this work was done starting in the 80s going forward with with very you know we're nearly archaic at this point uh forms of like computational resources it's like things were kept as simple as as possible and there are a variety of guidelines that need to be used to develop agent-based models for running these kinds of simulations uh you know and so so some of them even running up to 2007 here this is the work of

of doctors laser and friedman who respectively uh have been involved with northeastern university and stanford um you know they give some guidelines for how to do this um guideline one and two uh in some is you need realistic uh assumptions that you're making about your agents but not

too realistic in the sense that they're too complex one because we don't have computers that are advanced enough to run highly sophisticated complex agents and then also you know if we make if we make too many assumptions then the situation won't generalize well you might be making an agent-based model for one very very particular scenario

where it's not adaptable across different real world circumstances, right?

So we need some kind of balance in terms of the complexity of the assumptions we're making and the simplicity of the assumptions we're making.

The rest are a bit more plain, which are one,

reproducibility, you should publish your code.

You should document it very well.

Other researchers should be able to kind of take up the torch and be able to experiment with your code.

You know, we at the Active Inference Institute, we're constantly putting out new GitHub repositories and documenting our processes so that other people can come and get involved.

So I just want to mention that there's a big problem in reproducibility.

in science today, and it's very well known.

So just wanted to touch on that.

The non-triviality, it's like, why do agent-based modeling if it doesn't help us uncover anything?

Why not just sit down and have a conversation about what do we expect to happen?

What are our thoughts about what's going to happen?

Why do all of this with setting up these programs and agent based models and designing them if we can just come up to the same conclusions you're speaking?

So you want your results to be interesting.

You want to make realistic assumptions that we can kind of have all these things synthesized together.

so i made some points about balancing simplicity and complexity this is just a representation of uh you know these these are common problems in machine learning and deep learning you know you you go and get uh you know the university education these are the core components of all of your courses you know this isn't

specific to active emphasis, like wide ranging, you know, concern, how do you create a model that is realistic enough to capture and explain the phenomenon you're looking at?

But

uh simple enough to where it's adaptable to different circumstances you know and there's you know these are kind of in part uh you know just as uh uh dr clippinger referred to richard feinman you know just some reminders from uh you know famous statistician george e.p box you know all models are wrong but some models are useful right we're always trying to approximate a reality and so the question is how do we come closer to approximating uh our reality and so

How do we actually, using modern computational resources, using all of these new advances and all these fields, how do we actually make new forms of what we'll call cognitive agents who exhibit the kind of mental dynamics and social dynamics between them that actually better approximate real human behavior?

So I give some definitions, the differences between traditional agent based models and what agent assumptions are being made.

And more often than not, simplicity and agent is pretty one dimensional.

If they observe X, then they do Y. Otherwise, they do see these very simple pre-programmed rules.

They're not autonomous.

They're kind of like you can think of them as like thermostats or something like that, right?

the temperature is too high you turn it down if it's too low you turn it up that kind of reactive framework there's no beliefs about what's going on there's no internal goals per se aside from a single homeostat that is you know maintain the temperature at what you set

of cognitive agents.

However, the expectation for those, this is throughout the recent agent-based modeling literature is like, how do we achieve cognitive agents who exhibit internal cognitive mechanisms, they exhibit perception, action, they actually learn, update their models,

i'll make a point that you know as john referenced in the case of llms which i'll cover much more deeply tomorrow llms do not i mean it's so expensive to train them and you typically just train them once right and so um they don't learn in real time they're kind of stuck with the knowledge that they have they have a lot of it to be fair they've been trained on you know thousands thousands thousands

documents and so on but but that's it like that's what you have and then and then we have to figure out how to work with elements from there with cognitive active inference agents as well as other kinds of cognitive agency ideas that they can learn in real time and

meaning in the moment and adjust their expectations and their beliefs to what's actually going on now.

And then also planning.

So the capacity for being able to infer or guess using evidence that they take in, like what's going to happen in the future.

Right.

And that's immensely important in today's climate.

Right.

I mean, that's in part.

The reason we're doing agent-based modeling is to attempt to plan, to test things, to come up with our own inference about what's going to happen in the future, and then act in actionable ways to realize our own goals in the future, right?

So that's, you know, ideally that's what an agent would

do and be capable of um and then with you know this cognitive turn we're also looking at a broad synthesis of all these fields you know as john mentioned we're looking at things like cybernetics and machine learning we're looking at physics information theory um people are starting to it's great it's fascinating that so many uh interdisciplinary conversations can come up and collaborations between researchers of various backgrounds because it's like oh you know once we once we realize what it is we're trying to do here

we can start to pull from maybe principles or relate principles from other fields to this field what works what makes sense what's empirically but you know what can be validated and so on so it's it's it's um you know it's a very rich field right now and then furthermore a lot of traditional agent-based modeling has been done using a software called netlogo

which is very specific.

It's good if you want to do agent-based modeling, like you just learned this one programming language, there's a GUI, like an interface for using it.

Highly simplistic.

It is not integratable with other tools, not integratable with LLMs, with natural language, all those kinds of things.

And so it's very narrow.

It's good at what it does.

With cognitive agents, we would want to be able to pull from

all kinds of other resources that are, you know, available, including open source ones, being able to use API's being able to pull in other kinds of real world data, not just purely simulations, you know, so there's just a lot of richness behind this idea of moving to cognitive agents, and then

There's some basic ideas of what makes a cognitive agent.

Again, perception, they take in observations and they infer what's going on in the world.

And kind of like what we do, my stomach growls when I infer that I'm hungry, for example, it's a very simplistic example.

And then agents are generative in the sense that they actually come up with their own plans or choices of what actions to take based upon what they believe, such as I observe my stomach is growling, I infer I'm hungry, I will now choose to go eat food as like an affordance that's available to me in my environment.

So this is a new way in a certain sense of doing agent-based modeling, where before we were looking at modeling an environment with a bunch of one dimensional agents, you know, predictable agents on the inside, if they see X and they do Y, otherwise they do Z. And now we have models in a model in the sense that we're actually able to internally inspect our agents, right?

By having this beliefs-based framework where agents observe real data or information,

and then infer beliefs about it, we're able to then retroactively look inside the agent and see what its beliefs are, as opposed to these kind of very difficult to interpret black box models that we find very commonly in deep learning and very

very highly performative yet very uh uh difficult to interpret like neural networks and architectures like that with with uh agents we would want to be able to see in a in almost like a human sense like what are their beliefs that they're holding and why are they doing what they're doing so we need to be able to accomplish that in order to actually realize all the additional benefit of doing this which is being able to look at you know internal cognition what are agents learning whenever i

whenever I implement a new policy or something.

I want to be able to know what's going on with them internally.

I want to know the full impact of what it is, this plan I'm considering actualizing.

So to bring it back, how do we address all these traditional criteria?

How do we make realistic assumptions?

How do we make our code reproducible?

That's an easy one because it's like, well, we're still coding the whole time.

And then how do we derive useful insights from this?

And what are other problems that are going on?

Like now that we're getting into cognitive architectures,

and we are starting to brush up against the potential of recreating more black box models, right?

We need to keep things simple enough.

Like how do we navigate all of this?

A common problem in social science research in general, but also in data science, machine learning, and those kinds of fields, reinforcement learning, common issues.

How do we handle what's called the explore-exploit trade-off?

This is famously an issue.

If you have an agent who comes up with a suboptimal solution to a problem, let's say an agent who's trying to figure something out, they find something that they deem to be good, they get a positive reward signal or positive feedback from that, but it's suboptimal.

There's actually a better solution exists.

They just don't know about it yet.

how do you get an agent to actually be willing to explore?

Same thing for humans.

We don't always stick with the first decent looking option available to us.

Sometimes we explore in the sense that we seek better solutions to problems.

This is especially important in something to be addressed in agentic frameworks where if you're trying to use agents for, say, software application or a use case,

like you want them to be able to find better solutions ideally you know at any given moment especially in a changing context that might call going forward for better solutions you know say agents who are maintaining an environment that's subject to uh natural disaster or or or climate change right like that that might call for a whole new set of of policies or actions that need to be taken uh you know that that couldn't be relied upon in the past so we need agents who

seek new information.

Unfortunately, in reinforcement learning, this is still an issue.

People are trying to figure out

how to kind of manually define what defines exploratory behavior.

And it tends to boil down to kind of simple mathematics of defining like, okay, well, we'll modify these agents so that 90% of the time they go with the best solution they know about.

And then 10% of the time, they'll just pick something random.

That way they're at least exploring 10% of the time.

You know, you could put it that way.

Active inference,

I'm sorry, I'm kind of zooming through a lot of this.

Recall, this is an over three-hour tutorial.

But with active inference agents, this kind of gets naturally resolved in terms of what's called the free energy principle, which we'll be getting into.

But basically, the idea is if you're able to have agents who naturally seek information, perhaps in line with how we do,

right that when they come up against something they see oh there's value in seeking new information to find even better solutions the ones i already have and being able to to do that in a way that isn't some kind of ad hoc rule of you know the 90 versus 10 as i mentioned well that's that's um kind of a key component you know a key consideration of designing agents

So, this brings us to the active inference, which I describe here as the framework for doing agent-based modeling.

It begins with the free energy principle.

I'll make a lot of mentions to this nice textbook that Thomas Parr, Carl Friston, and their other co-authors published a couple of years ago.

know there perhaps even some more updates to be made because it's such a lively field but uh that textbook really lays out a lot of the core principles and and ways of getting started with active inference the the free energy principle everything that changes in the brain must minimize free free energy

Now, despite all of this interdisciplinary communication dialogue between, you know, again, cybernetics and psychology with behavioral economics, that actually sounds quite simple, right?

To be able to boil everything down to, oh, we have a brain.

It's trying to minimize free energy.

That almost sounds simple enough to where we might be able to employ that.

as a kind of simple assumption that we made in agent-based modeling.

It's quantifiable.

It's a presumed

presumed is the wrong word, but something like a principle that can be applied in any given circumstance whenever we're looking at these organisms who are self organizing, such as humans, such as living organisms, such as plants in an environment that's being taken care of, that they exhibit this kind of characteristic of minimizing free energy as a form of quantifiable uncertainty.

So we'll touch back on that again in a moment.

What active inference provides is a kind of normative, holistic framework for what I argue should be applied to agent-based modeling.

It itself addresses agent-based modeling.

That phrase is used in the textbook.

This is not a foreign, out-of-left-field kind of invasion of active inference into agent-based modeling.

They're proposing there's so much overlap, and I think it's just an underutilized connection that's already been made.

active inference is based in a you know it provides neural process theories meaning that we're not just you know assuming in the same way with agent-based modeling we might assume oh all people are utility maximizers um in this case we're not just assuming it we're looking at neuronal dynamics we're looking at you know decades of research into using neural imaging study

and looking at how does the brain really function, how does it handle uncertainty, different kinds of behavioral studies and tasks where you can record FMRI or EEG along the way and look at the dynamics of that data that's being collected along the way.

And then from there, we can start to kind of model how that works.

So as I already mentioned,

We're still looking at a kind of belief based framework, which is to say, you know, we're looking at for agents who have beliefs and they update their beliefs and they do that by way of the free energy principle, which is minimizing uncertainty.

We start to see these organisms that we're modeling, humans or otherwise, they start to have naturally emergent adaptive behaviors.

by trying to figure out what's going on in your environment and then adapting your actions in order to better understand it, to better realize your own goals, you're effectively minimizing your uncertainty.

And the free energy principle basically is a kind of free energy is like a proxy for that uncertainty that renders a lot of these things computationally tractable, meaning that we're able to quantify it,

we're able to report it whenever we run agent-based modeling we're able to like actually look on a chart and graph how much the free energy is being reduced uh over time you know and look at the agent's internal beliefs about you know what what is it that they're figuring out uh within their environment over the course of simulation um

The view from this perspective is that another aspect, I already addressed the exploration, exploitation balance and trade-off, self-evidence.

This means that agents, in order to minimize their uncertainty, one way that they can do that is to try and act to realize their goals.

There's something they want, something that they expect, right?

And so it's as if you have this prior set of beliefs of what you want to see in reality.

This is the kind of framework that active inference deals with, but also referring back to Judea Pearl, as Flippinger mentioned,

uh you know this is a kind of bayesian idea you know this is rooted in something like like bayesian uh statistics or even mechanics but the idea is that you you have your prior knowledge

You bring it to the situation, and then that will impact how you update your beliefs based on new information, right?

So we're not always starting from some blank state, some starting from scratch.

We're always coming to a situation with what we thought before, and then that gets updated.

How does that get updated from active inference?

That standpoint is through free energy principle.

So, you know, it's

I made some points in this tutorial because I realized we're really, you know, I want to make sure we have time to kind of bring this back.

But with active inference that we already have so many aspects of reinforcement learning,

and active inference which overlap.

We're trying to create cognitive models.

Active inference, I don't want to say it's necessarily competing.

I don't know if that's the right word, but it's making advances that reinforcement learning is trying to make itself.

With active inference, we're actually basing this on a variety of principles that

again, sorry, zooming through, you know, we're able to like cognitively model, you know, what's going on in the mind in various ways that reinforcement learning is really lacking today.

It's really lacking that it's, you know, these models are being built to be highly performative, but they're missing the mark whenever it comes to something like realism and basing these reinforcement learnings and something like that, you know,

science that extends beyond just the mathematics of machine learning right we want to be able to find biological physiological substrates for how all these things happen right we want to be able from there to be able to apply everyday words and interpretability so what is it that defines what a habit is what what processes involve whenever a goal

is formed and made and then from there uh you know attempted to be achieved through action uh you know what defines planning what is prediction um so i just want to see if there's anything else i might be able to touch on this is a nice um

know for for anyone and maybe i should have started with this but this is a little bit more of an intuitive glance and this is actually these are not words from uh carl friston surprisingly uh they're from uh sarah feldman barrett who's at northeastern she's uh you know nearly as reputable as uh carl friston in the neuroscience field and i just personally uh as someone who's also interested in in neuroscience and cognition you know i find her work fascinating she's done a lot of work

on emotions and you know kind of affective neuroscience so it has to do a lot with looking at what's going on inside the mind of you know especially humans and so this is kind of trying to provide an intuitive glance that you know how can we think about

Bayesian frameworks about inference, about active inference as well.

So it's like this is a long quote from a podcast that she did.

Your brain receives sense data from the world, sights and sounds, smells and so on, receives sense data from the body.

So when there's a loud bang or a tug in your chest, your brain receives

that information is the outcomes of some set of causes.

But what caused that lot of pain?

What caused that tug in your chest?

Your brain actually doesn't know.

It only receives the outcomes.

It has to guess at what the causes are.

This is called an inverse problem.

You know what the outcome is.

You don't know what the cause is.

So your brain has another source of information available to it, and that is past experience memory.

Apologies for just like doing a slideshow karaoke here, but you can read it for yourself, I suppose.

But the point is, and with some of this highlighting, it's like we're actually taking account of a variety of things.

You know, we're taking account of interoception, meaning observations that arise from within the body or for an active inference agent.

You know, it has to kind of model itself.

You know, I mentioned earlier that, you know, if I'm hungry, this goes beyond the five senses in that kind of perspective, right?

Touch, taste, feel, all of that.

With this, it's like, well, I need to understand what's going on within my body.

I need to understand what's going on outside of my body.

As Klippinger, you know, referred to earlier, the idea of a Markov blanket.

It's kind of like I need to figure out what defines the boundaries between myself and what's around me.

You know, on the one hand, I need to have a model of myself in order to maintain it.

Otherwise, I succumb to the environments around me to use some of the language

know the textbook it sounds a little bleak but you know we're referring to real world circumstances if i you know if i don't take care of myself i will naturally decay like that is kind of the sort of the principle the world is entropy so to speak right it's like if i don't eat food

you know, I fall apart.

So I need to have a model of myself, what's going on within.

I need to have a sense of what's going on without.

And then I need to be able to kind of synchronize those things in such a way that I can continue to exist as an entity within this environment around me.

That's a very intuitive, you know, perhaps not very well technically described, but for now an intuitive sense

of of kind of what defines a markup blanket so you know for this is a sort of foreshadowing for tomorrow we'll talk about uh this this really interesting project we're working on for the bio firm uh with with uh you know daniel and and under guidance of john um you know in that in that sense where we're trying to develop homeostatic agents uh homeostatic meaning agents who you know their own sense of their survival is predicated on

the homeostasis of their environments around them.

So we're trying to create agents who actually take care of and attend to

the environment around them as if their own survival depended upon it.

This is a very interesting kind of flip where you actually want to match and model the environment around you, but also take care of it in a way that you can maintain it within those bounds, just as I have to eat in order to maintain my energy levels.

The agents need to take care of their environment.

within some kind of range where it has enough caloric energy, so to speak.

So I think for now, John, is there anything that you think I have not addressed that might be useful to our audience?

Or if anyone else would like me to touch on anything, maybe something I skipped or otherwise, I'd be happy to speak more of it.


SPEAKER_02:
No, I think you've covered some of the really key points that what I think is what differentiates active inference from the more traditional agent-based modeling.

I think it's important because when people think about agent-based modeling, they sort of default to what the traditional models are.

And I think that you've done an excellent job of sort of highlighting

what the difference is and having that broader perspective and that more grounded interdisciplinary perspective, you're able to not only represent the complexity of it, but also provide a way of modeling agents within that complexity.

And I think the notion of the homeostatic

nature of of an agent trying to live in in conjunction or really harmony with the world around itself is depending upon its survival and its ability to stay alive and differentiate and have complexity but the synchrony of the environment is really another key point because i i think well a lot of people they think oh well you may say like navigating a fitness landscape you're just trying to optimize different levels but in fact

The landscape itself is shaped.

You're doing joint niche construction.

So it's a much more complex notion than traditionally understood general modeling.

And I think in consideration of what we're looking at tomorrow, this whole idea of the biometric character of modeling and having it based upon principles and biology and nature and having being able to create an economic agent that acts that way, I think is really quite important.

And so maybe some people have some questions about that that we want to focus on or not, but that's something that I think is important to highlight.


SPEAKER_01:
Yeah, awesome presentation, Andrew.

Ernesto, if you have a question, also anyone in the live chat is welcome to ask a question.


SPEAKER_02:
No questions.

Thank you very much.

This is so inspiring.

Ernesto, just to put it in context, I know this is something that you're very interested in, in this whole idea of a biometric economic system, and you've been looking at this yourself.

So this is really trying to lay down the principles, really the scientific principles and computational principles of doing this.

And we'll get in more to that tomorrow, but that's the intent here.


SPEAKER_01:
Yeah, a lot more to say.

Again, in our remaining 40 minutes, I'm happy to read questions from the live chat.

I'm also happy to screen share and look at some of the registered participants' backgrounds and interests.


SPEAKER_02:
Sure.


SPEAKER_01:
Okay.

Okay, you can unshare Andrew and I'll pull up some word clouds.

All right.

okay okay so let's let it load okay these are word clouds that are generated from the participants

registration information and we'll zoom in on them and I'll just kind of mention them and we'll zoom in

And if anyone has a remark or something that they want to highlight or call out about it, just go for it.

Okay, so this is a word cloud of the 150 or 200 registered presenters overall asking about what is your personal background.

So the biggest, and this is scaled just by number of uses of each term.

So one person mentioning a term a ton of times

might make it large.

So just to be taken as sort of a representative, we see those tiny two letters AI, which could mean almost anything at this point.

And a lot of other terms like computational research systems, neuroscience, cognitive, philosophy, social, academic, robotics, technology, practice,

This kind of speaks to the broad range of topics that leads people into joining in, at least entering that information on the form.

Any thoughts on this, or we'll keep on exploring these word clouds?

Okay.

All right.

Again, just raise your hand or go for it.

interesting okay well there's a i'm interested in the notion of community and systems i mean uh applications the community seems very strong yeah yeah and now that's the the sensor of a later one so here we asked what would be useful and by asking what would be useful and what would be interesting

we alluded a little bit to the dual composition of the expected free energy.

What would be useful?

What would be utility?

What would be pragmatic?

And then what would be interesting?

What would be epistemic?

So here in what would be useful, here we are in applied active inference symposium, wondering about how does active inference go from being a unifying framework for ecosystems of shared intelligences and all these different kinds of things that we've heard already and we're gonna hear in the symposium to come.

So interesting that systems

a way to operationalize and really move forward from framings of complex adaptive systems to having the implementations and also the community and the people understanding research, models, collaborators, and then a lot of different topics that are being explored in more mainstream

computational science fields like multi-agent algorithms.

Andrew alluded to that and kind of brought that up in the context of the Goldilocks parameterization question.

The agent-based model can be so simple, it gives you a great intuition.

It shows at a demonstration layer how simple behavior can lead to complex outputs.

On the other hand, the agent-based modeling can go the other extreme where it's so sophisticated, then it loses its transferability.

So that's what people would find useful.

And then here's what they put for what they would find interesting.

Applications.

make sense as that's what this symposium is all about and again a lot of other ideas thinking about different specific systems research ideas architectures synergies toolkits insights implementations support collective directions

Here's asking, what would help you in your learning and applying Active Inference?

And community looms large.

Other people, as well as some of the more tool, resource, textbook, education, examples, frameworks, interactive tutorials, different languages, Python,

What does anyone think about this?


SPEAKER_03:
I'm a little surprised that we're not seeing the phrase LLM anywhere.

I just kind of noticed that.

One of the points that I've been trying to work out and hope to speak more on tomorrow is

integrating active inference with LLMs.

And there have been attempts to do this.

And even with and other folks, there's been attempts at trying to pull out an agent's beliefs and then use an LLM to process those beliefs, like kind of decode them and re-encode them in natural language, so to speak.

It's as if the agent is speaking and saying what it,

thinks and believes and why it did what it did is you know another layer of interpretability and uh i don't know i'm slightly surprised to not see llms anywhere um you know maybe it's a little too maybe that's a little too on the frontier right now um i know it is in uh many other areas you know it's um and sorry in a couple minutes we have it just uh

know something that i've been working on personally and this is feeding into the the bio firm is um how do we how do we use llms in a way that we can actually integrate them with you know all these major priorities that we have uh where we want to have agents who actually have defined goals right and and ways of trying to stay alive you know in a way that they interact with their environment the way that it's as if their own survival depends upon it we don't want

LLMs which you know again I'm just saying this quickly you know I don't mean to sound disparaging but LLMs in one sense can act as like functionally as talking heads where you just send in some text and they try to predict what would be the best text in response they're still subject to hallucination to confabulation making things up so to speak even though the answer looks structurally sound

And so my point is that rather than viewing LLMs as agents, I don't think they are, I think that you could have an active inference agent who is augmented with the capacity for natural language

by having it interact with an LLM for it.

So yeah, sorry.

Yeah, so many.

This occurs there.


SPEAKER_01:
Yeah, scan scanning across all the registration fields for LLM mentions.

So how does active inference apply to the design of LLM systems?

AI outside of LLMs better ways of designing internet scale protocols and governance systems and

should converge with TinyML to challenge LLM's deep learning so it is interesting that it's referenced with only a glancing blow and we've even seen earlier like in the responses to similarities and differences with reinforcement learning with Fristen earlier

about the increased amount of attention and tool development and the later and more professional maturity of training environments.

And this is like one of the key foci for the open source active inference ecosystem and for the institute is to develop those kinds of methods and trainings and onboardings, all these different aspects of of increasing the professionalization

So let's look at the challenges.

So this was, what are the challenges facing your learning and applying active influence?

People, interesting.

The problem and the solution.

Models, the math, software, maths, again, mathematical understanding, difficult physics, scaling,

simple examples?


SPEAKER_02:
It's interesting because in the larger conversation about what AI is, that conversation is so dominated by large language models and quote generative models within large language models.

And what at least I've been observing and I tried to refer to in my initial slides is that they're starting to appreciate the limitations of the large language models, move towards more agentic models and more towards causal models and hence creating

agentic architectures that do work with large language models, but they're still not based upon biological principles that we're discussing in active inference.

There's more of the mechanical models being applied over.

And I think from a point of view of sort of the sociology of science, there seem to be these different worlds that

I mean, the number of people that I've known sort of in the open AI world, they're very unaware of active inference.

And some of the critiques that are, if you listen to Gary Marcus, he's got a very pointed critique of,

of large language models and not being able to achieve abstractions.

But that doesn't extend to appreciation and appreciating what the opportunities are in active inference.

And so there are some of these islands of understanding that they still exist.

And I think that's one of the challenges.

And when you talk about developing an infrastructure, I mean, you need this huge financial resources are going into the large language models infrastructure.

And where I think that the totally bias on this, I think the opportunity is totally within the active members of free energy principle.

And I think that's where the science is going.

And yet it's still, it's sort of in this isolate world.

And I think the only way to get out of that is to really start to show in the applications and showing with results.

But it's interesting.

They're different worlds.

But I think that there's a peaking.

And so the hype curve around...

the large language models, and they're going to descend, my bet is they're going to descend into a valley of doubt, and you're going to see that showing up in the NVIDIA stock going down, and then people are saying you can't destroy all the natural resources to run these models.

You don't have to.

The other thing,

And Carl alluded to this in his earlier comments today.

I mean, he was talking about climate change and models and compression.

And I think that's going to be a driver of this.

But that's my observations on these clouds.

Yeah.


SPEAKER_01:
I'll bring in a question from the live chat.

Also, welcome, Mao.

Just thought I'd bring you on if you wanted to participate in this panel or just chill.

So Steven Silette wrote, is the lack of LLMs mentioned because large language models are different to dynamic agent belief modeling?


SPEAKER_02:
And do you want to take that?


SPEAKER_03:
Could you repeat that question?


SPEAKER_01:
Is the lack of LLMs in the word clouds here, but just more generally thinking about similarities and differences between active inference generative models and LLMs?

Is that because large language models are different than dynamic agents or belief modeling?


SPEAKER_03:
I don't want to approach this question in the wrong way, but I will say that as far as whenever it comes to LLMs and dealing with,

Their internal parameters are obviously highly quantitative, and then they're dealing with natural languages tokens.

And then switch back to looking at active inference agents, which are highly quantitative in nature.

And typically, if you're making continuous time models,

You're looking at, again, quantitative information.

And then even with discrete models, you're looking at these kind of discrete values that you could potentially move in the direction of trying to make it.

I think this has already been done, but there are agents who have been used to try and understand like, oh, we can read individual tokens, then move up to another layer, and then we have entire

entire you know paragraphs or ideas that kind of thing is done but i think this kind of like llm with quantitative uh uh grounded agents like that connection uh is still in my view that's kind of the frontier of where things are at and so it's difficult for people to conceive like how can we how can we translate

encode decode re-encode natural language in a way that's readable by these quantitatively oriented agents and then vice versa how do we transition quantitative beliefs into natural language that's the very thing that i've been

uh you know looking at thinking about uh recently i made like a active inference like plmdp agent who uh you know they what they do is that they use their beliefs about which action to take and then that action that they carry out is actually corresponding to a prompt to an llm and then you can actually take those beliefs the quantitative beliefs find some way of meaningfully

interpreting them and then putting that into these blank spaces in the prompt is as if you can have the agent say in real time i currently believe this with probability this

give me you know a solution to this problem you know based on that so so being able to find this kind of numeric versus the quantitative and the qualitative you know it's a historical problem we're working with with data goes back centuries at this point so um it's really exciting now it's really exciting to think about how to bridge that kind of gap and i i still haven't seen anyone you know nail that down this this is the frontier this is where we're at and i think that's kind of

the next thing for everyone to really be considering thinking about working on.

And I'll give my view on how that could be done tomorrow.


SPEAKER_00:
Mal?

Yeah, I mean, effectively, LLMs don't really encode explicit beliefs or really maintain a persistent model of the world.

um even when we sort of give them a sort of loop by which we communicate with them that loop in itself doesn't actually factor into the predictions that it's going to do um so they just lack a framework for recursively updating beliefs which is probably why it's not being thought of here um but also mainly that

you know, LLM agents are only agents by virtue of sometimes being connected to stuff and having explicitly stated goal, but I don't think that actually qualifies them to the agents.


SPEAKER_02:
Yeah, I totally concur with that.

From my perspective, I think that the reason that LLMs are not looking at

that are not looking at or that whole mindset is not looking at belief structures in intersubjective experiences, I think is a product of a whole sort of objectivist reductionist methodology

that has been tuned not to look at agentic systems.

I mean, in my generation, if you were in biology and you talked about agentic systems, you were outed.

I mean, that was verboten.

And I think there's a whole school of thought, there's a whole,

set of, in a Kuhnian sense, a normal science built around a sort of objectivist, reductionist, non-agentic notion.

And so it's a new generation that's coming in, both in biology and in modeling and cognition, that's recognizing this notion of an agentic perspective of beliefs and updates.

And that's a huge shift.

i think in in sort of the epistemological perspective of a lot of what has been traditional science that's really interesting there's also just operationally


SPEAKER_01:
the difference between taking a very large, fixed number of parameters in a given model, 70 or seven or 405, training on existing data, and then finding patterns or eigenmodes or activation patterns

summaries that have secondary interpretability but there's not really an effort of trying to find interpretability at the level of activation of a given neuron it's about looking at patterns and trying to find collective level interpretabilities like in in the ai interpretability field contrast that with designing from the ground up

in example agents where the beliefs are explicitly encoded numerically and so that's a different mode of building and also one that's hinted at in in this 2023 paper

with Mao and others that highlights that element of explicit semantic interpretability and also explicit semantic interpretability of uncertainty estimates.

Whereas a string of tokens emitted by an LLM that says, I'm not sure, it's sure about saying that.

but that doesn't mean that it has a given certainty about the reference.

So understanding what the communication semantics of LLMs are is incredible and what they do.

And I think when we look at this cloud for people's responses, how can active inference make a difference in 2025, where there is LLMs here,

many many kinds of systems and that's sort of like a microcosm of what the symposium is different people with different systems of interest asking how do these different

attributes of the model that we're talking about the first principles grounding semantic interpretability surprise and uncertainty and it's bounding as a central component as opposed to reward how do all those things really play out what is that demonstration look like in in different systems


SPEAKER_03:
Here's another one.


SPEAKER_01:
Go for it, Andrew.


SPEAKER_03:
I was just going to say on that previous one you were just looking at, I really appreciate the kind of broad-mindedness that people seem to be exercising.

I like seeing terms like symbology, scientist, governance, industry.

I mean, those are some very

I'm very excited to see people who want to, you know, so much work has been put into kind of the theoretical and then sort of like in vitro and in lab and in simulation kind of work that's been put into active inference.

So I'm just really excited to see, you know, with the international

the conference, the back of inference conference that was given recently and being able to see like a drone that's flying based upon the free energy principle and things like that.

I'm just really excited to see more that kind of real world applications, right?

And kind of creating, you know, systems and applications for like real world use cases.

It's really cool to see some of those terms there.


SPEAKER_02:
Yeah, I think that's going to be really critical to see.

Yeah.


SPEAKER_01:
here's the cloud for how are you applying active inference again systems i think just reflecting people talking about variety of different systems types but and also reminding me of my own journey to active inference being excited about complex adaptive systems however not really having a software toolkit

or a formal way to proceed past okay we think about nested multi-agent systems and then where's the software toolkit and then we're right back to where andrew showed us on the slide it's like well we can do a really simple one in netlogo or we can engage on this enterprise scale engineering research program

And here's a more incremental, interoperable way to have portfolios of motifs and generative model components and be assembling and scanning across these different kinds of compositions, like three ant nest mates in the desert.

three ant mate, nest mates in the forest and these different kinds of environments, being able to compose them.

And over the last several years, seeing a lot of the advances in the category theory, in the first principles, physics, surrounding all of these sort of research things.

And it's like, that's kind of the water drawing out

and then that's the question about how it looks when it is applied and that's what I guess we convene around in this symposium and and are seeing differently in 2024 than in previous years just curious so you should what was what's the change that you've seen in previous years versus this year and then so looking forward with more of an application

Andrew or Mal, what do you think?

But absolutely, that is... Yeah, Mal, please.


SPEAKER_00:
Yeah, I mean, I feel like the work of Maxwell Ramstad, Caspar Hesp, I mean, obviously Carl, but a lot of people who perhaps came from more dynamics decided to take a real interest in what we could do with something that was so scale free.

And I find that it's been

really really rich in terms of the different kinds of theories we've tried to sort of reformalize I know that's what most of my work is is a reformalization of existing social sciences and to your point Daniel the the problem we still have is that we're missing like these key little plugs that yes you could couch as you know just different schemes of message passing but

you still need some interesting tools to to have a very good description of the vast vast variety of types of structures that we have which social sciences have done a really great job at if not intuiting at least describing properly but we still don't have the best tools to make accurate predictions and that's part of the reason why most of our polls will make them

They're useless.

And that should be the main thing we apply them to is just literally just understanding where we're at, not even making predictions, just understanding where we're at.

And we still fail at that.

So I'm very, I'm very encouraged by, by seeing this.

And I hope that we will start valuing more and more interdisciplinary perspectives that are sometimes more technical, but also sometimes, you know, more philosophical or, or social.


SPEAKER_01:
awesome also while we're in this in in the coming workshops tomorrow will be more of a bioregional biofirm focus and then day three will be entirely with clippinger at all on this topic and i want to show one really operational open science crossover with llms and active inference

which is, and I'll put it in the live chat as well.

Here, I used perplexity, which does an internet search, summarizes and translates information using LLMs.

So here, we'll pick on Alexi.

There's a profile, a learning plan, and a set of small, medium, and large project proposals about active inference.

So this is just summarizing expertise and experience.

Here we have a learning plan.

It's just a starting position, but this brings a new low bar to the kinds of resources that everyone can take on in their learning and use as like a syllabus or a curriculum

even as an interactive tutor you know it will not get bored with you asking to explain concepts again and again or using different metaphors or making it applicable making it like this poetry format putting it in this narrative context and so even asking how do i start with this or can you help me understand what i don't know that puts such a centrality and and highlights metacognition in learning

And then the project proposals, each of them structured according to essentially what a grant officer would be looking for and what a project manager would be thinking about.

in terms of small, medium and large projects.

So this unprecedented high throughput way to fuse even scanty input from a participant in a inclusive open science setting into real projects that if enough people think, hey, that looks kind of interesting, then we're well down the road of actually making it happen.

using different kinds of funding, which is something Michael Lennon mentioned a little bit earlier.

So it's sort of like we can get really detailed and technical and architectural thinking and finessing the differences between reinforcement learning, deep learning, active inference.

And we saw slides to that, like when Carl showed

the action perception loop, similarities and differences.

Okay, active inference doesn't have a reward function.

It's not optimizing based upon this trained auxiliary reward concept.

It's just doing something direct with likelihoods.

That is critical.

That is what makes the active inference generative models what they are.

But then looking at something like this, which wasn't so accessible one to several years ago,

that just feels like opening up and unleashing what people can do and about using these as tools in our niche and i think it's a total open and unknown what happens when you can get to this level of project and learning plan for for an arbitrarily complex field

with the patience of an AI that's just going to accept questions all day long.

Like, that's almost the bigger applications question.

Again, just zooming out from how the architectures and the parameters are similar and different to

How are we going to do what is authentic and right and best given the tools we have?

There's a bigger picture beyond just holding the magnifying glass to different computational artifacts.


SPEAKER_02:
That's well said, Daniel.

I really agree.

It's transformative.


SPEAKER_01:
if anyone has questions in the chat or here or any other things they want to mention yeah what's going to happen in the next two sessions John or what would be your goals or preferences for for this arc of the symposium for you yeah so I I think as Andrew alluded um


SPEAKER_02:
The session tomorrow is going to be really on the application of active agents to a design of a economic agent, a new kind of firm, a biofirm that's homeostatic.

And the objective of that is to develop a new kind of incentive structures upon the collective action that are based upon

memetic principles that I think will see address a lot of the issues that people are having in the whole idea of bioregional finance and how to deal with climate change and how to create those incentives so they're one of the things I'll kick off with I've been talking to a number of people in the whole fields of of of regenerative finance by where they go biofi

This ties into the work that was done at Eleanor Ostrom's work on common pole resource.

How do you manage a common pole resource?

How do you incentivize people at different scales?

I really think that, and once you identify certain requirements that people are looking for in order to address these problems at scale, and then I think what we'll be talking about is to the implementation of

active inference principles into an economic agent that it provides, I think, a reasonable way of addressing these issues.

And so that's part of the conversation that we want to have and bring together these two worlds.

Then the third session is to really look at this

um from a a broader point of view is saying where is this going and so the people that we have in the panels are very off very much involved with enterprises um applying it to non-profits to applying it to several problems looking at what they can do with llms currently and what the certain current agentic architectures are what happens when when you say what happens when you go to full autonomy what does it mean to have autonomy um in in a

in a biological sense, something keeping alive.

And what does it mean to build systems that have autonomy?

This issue of autonomy is sort of fundamental.

When people think of autonomous cars or decentralized autonomous organizations, or the whole idea of

central bank the the the the principles behind something like a bitcoin is like oh that that that is autonomous but is it really at times what is this new notion of autonomy if we base it on biological principles

And I think that opens the door to a very different set of discussions than we're currently having.

And it also opens the door to new notions of governance and how we can build governance in to maintain the homeostatic states.

And Andrew's alluding to that.

So, I mean, that's where I become an optimist.

I'm not an optimist around the extension of LLMs and OpenAI and the tech


SPEAKER_01:
the tech giants but i am an optimist in being able to apply these principles to to some of the fundamental issues we're having around climate and social equity and economic equity awesome okay in the closing minutes if anyone has last comments and david's question too if you want to address it andrew or anyone else any thoughts about how we start to develop a multi-scale approach to ethics


SPEAKER_03:
um i don't know a lot of time for other people so i'll briefly say uh so so tomorrow um you know we'll we'll present uh various aspects of the bio firm there will be a little bit of a redux with some of the things i went through today just to kind of thread the narrative um and i'll kind of talk about you know homo economicus and all these kinds of assumptions about human behavior and things like that but uh

I mean, multi-scale governance.

What we're trying to do with our bio firm is to, you know, and Klippinger referred to Eleanor Ostrom's work on the commons.

And so I'm gonna address that a little bit too, but the idea is,

to be able to have a kind of like adaptable system of multiple agents, you know, and so we have a kind of small society, so to speak.

And, you know, they're kind of governing the commons around them.

And by being able to have something like the capacity for one shared goals, and to adaptive learning such that they can actually adapt what they do and what they believe and

the actions they carry out they have those be localized to the current context which is an essential part of ostrom's like work is like it's not so simple to just give a singular top-down instruction that applies in all places i mean to be able to have that kind of adaptivity and awareness of the local um you know while at the same time having agents who are kind of synced up in these various ways i think that's one way of trying to approach governance and then from there you might be able to have kind of layered

know you could have individual hubs you know then you could have some kind of like interconnecting system between them that that extends that to an even broader scale of multiple hubs and interaction between them so just some just some breadcrumbs there um thanks yeah mal and then john with the last word on this session yeah so i mean the question of ethics is a bit it's obviously complicated but the the


SPEAKER_00:
the key part about whether we coordinate around multi-scale goal sharing etc share potensions that's intrinsic to hierarchical active inference right it just is the question that isn't going to be solved is what is good uh because you can always say well you know what is good is good by virtue of being contextually sort of stable but there may be other ways to be stable or

that might prioritize other kinds of individuals or entities, et cetera.

So I don't think we're going to solve that question.

However, what we can solve is the global versus local coherence of certain kinds of beliefs relative to what might constitute goodness at given scales and find where the friction points might arise, because this version of goodness is incompatible with this version of goodness.

And therefore, something's got to give somewhere.

An error is going to have to propagate.

so i think that's going to be answered but the the larger broader question you you'd have to look into different schools of thought around what constitutes uh good because we don't necessarily want to go towards um moral realism um because there's a lot of things that are baked into that uh but you might want to go into the uh ethical implications of neo-materialism and how it is about interactions and locality and at that point it becomes about

how do certain pockets of ethical framework sort of form and allow to just coexist away from other pockets and what happens when they start interacting uh which again we may be seeing right now in the US yes good point John any last comments on the session no this is great um and I I think uh


SPEAKER_02:
We're now surfacing some of the issues that are really important to talk about.

And I think at least what we're going to be presenting is a framework that at least we can have these discussions in a more rigorous way and a testable way.

And I think that's part of the way of moving it forward.

So, yeah.

Awesome.

Thank you.


SPEAKER_01:
Okay.


SPEAKER_02:
We'll take a one-minute break.

See you soon.

Okay, thank you.
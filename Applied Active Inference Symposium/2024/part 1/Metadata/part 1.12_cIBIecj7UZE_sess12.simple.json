[
  {
    "start": 0.993,
    "end": 9.435,
    "text": " The next talk is by Aswin Paul, Bridging Biology and AI, Active Inference for Biologically Plausible Decision-Making Models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 10.497,
    "end": 12.302,
    "text": "This is a 15-minute video.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 16.4,
    "end": 42.205,
    "text": " hello everyone my name is ashwin paul i recently finished my phd from monash university in australia and is currently working as a machine learning researcher in versus ai so today i'm here to talk about using active inference to model biologically plausible decision making and thank you active inference institute for organizing this symposium let us start by",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 43.349,
    "end": 66.657,
    "text": " mentioning a recent cutting edge experiment that was developed by cortical labs in melbourne so what they managed to do is they developed a chip a silicon chip and they cultured real biological neurons like mice cortical culture and human cortical culture and they managed to couple this chip with a computer game like the game of pong that you see on the screen right now",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 67.109,
    "end": 76.365,
    "text": " So the experimental result is that given meaningful feedback, these neural cultures actually demonstrate relative improvement over time in playing this game.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 77.106,
    "end": 90.99,
    "text": "As you can see from this graph, right, like the MCC and SCC neurons actually demonstrate that they can actually improve in game play when compared to baseline groups like the silico agent or the control groups.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 91.645,
    "end": 102.727,
    "text": " So, given such advancements in experiments from a theoretical perspective, active inference is a great candidate to model such experiments and biologically plausible decision making.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 103.248,
    "end": 108.979,
    "text": "So, we have emerging literature and a lot of evidence suggesting this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 109.668,
    "end": 123.12,
    "text": " when we actually try and do modeling there is a clear need of scalability in the active inference literature where we are used to modeling very small environments with very low number of states and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 124.315,
    "end": 151.74,
    "text": " very low number of actions etc so in order to actually use active inference in such scenarios we have to scale up the existing active inference algorithms and in the first part of the talk i will be talking about some algorithms and techniques that we can use to scale up modeling decision making in active inference and in the second half i will talk about using these algorithms to actually model the particular experiment that we saw earlier",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 151.72,
    "end": 173.087,
    "text": " so when we talk about scaling up active inference or scaling up decision making in active inference the first technique i want to talk about is evaluating decision making backwards in time and not forward in time as we usually see in the active inference literature so when we define the expected free energy and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 173.911,
    "end": 192.358,
    "text": " compute which action should be taken given a particular observation rather than evaluating expected free energy or computing it forward in time as we see in techniques like sophisticated inference we can actually use dynamic programming and model decision making backwards in time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 192.338,
    "end": 198.425,
    "text": " And this result is actually published in paper, in this particular paper, if you are interested in this particular algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 198.966,
    "end": 208.358,
    "text": "And a more general version of this algorithm was recently published as the inductive inference algorithm by our colleagues in Versus AI.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 209.039,
    "end": 220.793,
    "text": "And in that paper, we note that inductive inference algorithms scale up decision making drastically and can be applied to bigger generative models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 221.245,
    "end": 250.915,
    "text": " another technique that we can employ in this regard is to actually learn prior preferences and in order to do that we actually take inspiration from the optimal control literature and this particular paper and this particular paper introduces an algorithm called said learning that improves scalability and efficiently computes the optimal action when compared to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 251.182,
    "end": 253.988,
    "text": " cutting edge algorithms like the Q learning algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 254.569,
    "end": 264.128,
    "text": "And a particular matrix in this algorithm called the Z matrix is actually very similar to the prior preferences in active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 264.649,
    "end": 266.052,
    "text": "And we can actually",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 266.453,
    "end": 289.474,
    "text": " learn this set matrix orders of magnitude more efficient than um classic reinforcement learning algorithms and what we do in in in this paper is that we show that we can actually learn prior preferences using a similar learning rule and once we learn this preferences then decision making is really easy because we don't really have to plan a lot forward in time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 289.454,
    "end": 318.499,
    "text": " and we also demonstrate that learning is robust in this particular algorithm and talking about computational complexity both these methods one is dpfe method that computes expected frequency backwards in time and the other one that learns by preferences are actually orders of magnitude more efficient than the classical active inference we see in the literature or the recent sophisticated inference and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 318.479,
    "end": 331.341,
    "text": " Both these methods actually learn or scales linearly with time against the exponential computational complexity of decision making that we see in the active inference literature.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 331.608,
    "end": 341.683,
    "text": " So we tested these two algorithms in terms of its scalability and adaptability in state spaces as high as 900.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 341.723,
    "end": 350.916,
    "text": "And what we saw is that when we actually introduced stochasticity into the grid, these algorithms are more adaptable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 350.896,
    "end": 363.285,
    "text": " than cutting edge reinforcement learning algorithms like dynacube and we demonstrate that these algorithms are scalable which gives us confidence to go ahead and model experiments like this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 363.485,
    "end": 391.703,
    "text": " and when it comes to modeling neuronal dynamics we already have existing active inference algorithms that model neural dynamics that is from cbs ricken developed by professor isamura and colleagues and in one of their works they actually talk about a similar experiment where neural cultures are actually performing a perception task which is called by blind source separation and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 391.683,
    "end": 410.13,
    "text": " in a more recent work they also talk about decision making in neural cultures and develop a decision making model that can be equated to neural dynamics so going into details we have this algorithm called counterfactual learning algorithm and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 410.414,
    "end": 428.102,
    "text": " they develop a version of the variation free energy that is term to term equivalent to a neural network cost function and this is valid for a particular class of neural network and more details can be found in this particular paper and there are two terms of interest",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 428.082,
    "end": 453.212,
    "text": " uh in this algorithm one is a time dependent risk parameter and a time dependent state action mapping that this particular agent uses for decision making and because of how it is developed it is biologically plausible and can be shown that every term is one-on-one related with the cost function that a particular class of neuronal networks actually minimize so",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 453.682,
    "end": 465.951,
    "text": " In one of our recent works, we generalized this algorithm to have memory, and we tested it in a similar control task like the carpool game that we see on the screen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 465.991,
    "end": 470.762,
    "text": "And we can actually draw a comparison with the carpool game.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 470.742,
    "end": 494.235,
    "text": " pong game because both of them are reactive tasks where are where is where the agent is supposed to actually react to the environment and take decisions quickly to balance this pole or to deflect the ball back so we observe that compared to planning based algorithms like dynaq and dpfe the counterfactual learning algorithms perform really well",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 494.215,
    "end": 506.824,
    "text": " in this particular task and gives us confidence to go ahead and model the biological neurons or the kind of experiments that we saw earlier.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 507.479,
    "end": 517.817,
    "text": " In addition to being able to model this, we also have the advantage of explainability in these algorithms, unlike deep learning algorithms that are black boxes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 518.358,
    "end": 526.713,
    "text": "So if we actually put up the environment, say in a particular episode, we can actually see that the risk term is shooting up in that episode.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 526.813,
    "end": 529.217,
    "text": "And this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 529.197,
    "end": 544.59,
    "text": " particular term captures changes in behavior of the counterfactual learning agent and we can actually try to explain how decisions are being taken at every time point unlike black box approaches so given these",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 544.57,
    "end": 570.949,
    "text": " several algorithms that we have can we use them to actually model um the manifestation of natural intelligence uh in experiments like dish brain right so before that we actually develop an experimental based generative model that is based on this particular experiment so for example um this is the pong game environment that they use in the experiment um precisely",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 570.929,
    "end": 590.136,
    "text": " So for example, we have the x coordinate of the ball that being communicated to the disparate chip through 38 distinct frequencies in the experiment and the y coordinate of the paddle and the ball being communicated through a different voltage levels in the experiment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 590.116,
    "end": 600.506,
    "text": " So, for example, like we have three observation modalities in this experiment, and we note that these are the number of states and the ways of communicating that to the agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 601.207,
    "end": 616.522,
    "text": "And we enumerate the state space of our generative model using this experimental details and match our generative model to be exactly like the experimental generative process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 617.243,
    "end": 619.245,
    "text": "And we also have like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 619.225,
    "end": 649.165,
    "text": " the active inference parameters in our generative model like the transient dynamics of a pomdp and counterfactual learning based state action mapping the risk term and the prior preference distribution etc that we discussed in the talk earlier and given that we have this experimental based generative model we can model decision making of in silico active inference agent and hope to compare them with the experimental result to draw insights and so on right",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 649.145,
    "end": 654.394,
    "text": " So, first of all, we perform simulations using the counterfactual learning agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 655.135,
    "end": 665.652,
    "text": "And we can see that counterfactual learning agent with memory horizon as low as two is actually performing at par with the HCC group.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 665.632,
    "end": 686.374,
    "text": " in terms of performance so here cfl t stands for cfl memory horizon t so as expected with more memory horizon it performs better and we can actually look at which memory horizon is performing similar to the experimental group and so on and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 686.354,
    "end": 715.112,
    "text": " in one of our recent works we note that the active inference agents are incredibly data efficient compared to deep learning methods and it is only the active inference based model that is counterfactual learning learning method that demonstrate data efficiency at par with the biological neurons when compared to deeply reinforcement learning algorithms like a2c and ppo if you are familiar with the deep reinforcement learning literature",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 715.092,
    "end": 733.455,
    "text": " um so we also compare the planning based methods that we mentioned earlier like the dynamic programming method so in this particular task or the pong game planning is not really useful so with increasing horizon of planning we don't really see improvement and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 734.701,
    "end": 759.932,
    "text": " um planning horizon of five performs as good as hcc neurons and planning beyond a point is not useful in this particular task because of the nature of the task and we also note the explainability of our models for example if we look at the average risk in the counterfactual learning method we see that the risk is actually only reducing",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 759.912,
    "end": 778.309,
    "text": " significantly with a memory horizon of four and which and this can also be correlated with the performance improvement that we saw in the results graph where with memory horizon four there was a significant jump in performance and these kind of explainable",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 778.289,
    "end": 807.773,
    "text": " insights that can be drawn from our models is a clear advantage over black box methods like a deep reinforcement learning and we can also look at the entropy of the cl parameters and we see that the entropy is decreasing over time that is the evidence in learning and parameters reveal differences in behavior of different agents and that is a clear advantage here and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 808.276,
    "end": 825.525,
    "text": " summarize um we actually developed an experimental um informed generative model and simulated decision making in the particular environment we tested different active inference based algorithms in this setting and we also",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 826.636,
    "end": 847.759,
    "text": " noted the similarity of memory augmented counterfactual learning method to the biological neurons and we are interested in investigating similarities of the memory based CFL method more to the biological neurons and also insights like this can inspire more experiments where aspects like memory can be studied well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 848.52,
    "end": 855.327,
    "text": "There are several limitations to this work where we haven't conducted a really",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 855.307,
    "end": 881.415,
    "text": " involved statistical analysis in comparing these agents we are right now qualitatively comparing them with average parameters but we would like to do more statistical analysis and we would also like to inspire experiments from our theoretical predictions in our future work so i thank all my collaborators in this work and if you are interested in this particular um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 881.395,
    "end": 898.358,
    "text": " work and the algorithms that i discussed you can actually get access to all the papers here and feel free to contact me if you have suggestions or more questions about this work so thank you for your time and i look forward to meeting you all when when i have a chance next thank you so much",
    "speaker": "SPEAKER_00"
  }
]
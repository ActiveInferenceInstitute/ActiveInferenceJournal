SPEAKER_00:
all right the next recorded talk the penultimate recorded talk is by bradley alicia earlier bradley gave a talk on purposefully non-standard cybernetics and this talk is going to be an overview of open source and open science so that's really cool and thanks for bringing this important complementary topic to the symposium for all all who are working on this

Okay, it's 51 minutes long.


SPEAKER_01:
Hello, my name is Dr. Bradley Alisao, and I'm a member of the Active Influence Institute's Scientific Advisory Board.

I'm going to give you an overview of open source and open science.

I'm at my home institution's York Albany Research and Education Lab.

I'm also affiliated with the Open Worm Foundation and the University of Illinois Champaign, Urbana-Champaign.

I'm also of particular interest in the Orthogonal Research and Education Lab as our open source interest group, and I'll talk about that throughout this talk.

Let's start with a question.

What can we do with open source and open science?

And I paired these together because sometimes we're building software, sometimes we're doing scientific discovery, which kind of relies on the open source software.

And so if we're doing both, we can actually build systems that can sort of help us do better software engineering and science.

So they're kind of grouped together.

So in terms of open source, you know, it's maybe a truism that open source is about software and putting it out into the world, but that's not enough.

And you really want to, what you want to do instead of just putting software into the world is building a community around technical standards and code.

And this is true if you want to build a community of users or an open source community that helps build the software, because you need to have some interaction with the software to make it sustainable.

He doesn't want to have good technical standards.

And with academic softwares, we see that's been lacking.

This also allows us to open up the organization's processes and products.

So having these communities and having these technical standards are not only good from just the standpoint of sharing software, but it also opens up our processes and products and allows us to serve our communities better, our academic communities.

Open science is where we share data, share results,

care research artifacts with the public.

So we do a lot of sort of transparency related things.

We give people our results and our data and it's software and papers and presentations and we're very open about it.

We can give them access to these things.

So when I say open science, I also kind of mean open access and it's they're kind of in the same bucket.

Open science in general allows us to open up our organization's research capabilities so that people can benefit from our discoveries and we can benefit from their feedback.

So a lot of organizations have proposed different sort of roadmaps or statements on open science and what that involves.

So not every organization's view of this is going to be the same.

But I'm using the Young European Research University's network statement on open science at the link below.

And to them, you know, they have a very strong emphasis on collaboration and citizen science.

So this is where you have collaboration around the world, around the scientific community, and also involving, say, citizen science, where people just want to contribute their own analyses.

Sometimes it's data, sometimes it's observations, things like that.

There's also visibility and idea sharing.

So being able to get to the outputs and read them and understand them is important.

Open education is important.

Idea sharing, because you need to have, you know, focus on collaboration.

Alternative models of publishing, you need to be able to publish the work to get it out there into people's hands.

Incentives, assessments, and research integrity.

So those are all kind of part of this open science tree.

Now, again, every organization who champions open science will have a different view of this.

We don't really have a formalized view in the Orthogonal Lab, but it follows this kind of outline quite closely.

so if you want to know more about the where the open or where the orthogonal research lab is coming from you can read this paper and i put this out in 2020 and it needs to be updated but it's a good overview and this is building a distributed virtual laboratory adjacent to academia and so there are a number of things in this paper it's just basically it's very similar to the model that

The active inference lab uses were much more focused on sort of educational opportunities and opportunities for people to sort of pick up the load and build research artifacts and having students come in and do short term projects.

But it's a very similar process.

So one of the processes or one of our goals is to integrate open access in our open source practices for advancing distributed research projects.

So we wanna be able to help people make these things open, help them build research projects.

The other thing is that we wanna encourage practice and open working to the creation of digital research artifacts.

It's like papers or presentations.

And when I say open working, I mean, I borrow this term from the Mozilla Foundation.

And their approach to this whole enterprise is basically working open.

So it's working in these collaborative distributed groups

and how to make the most of that kind of experience.

You know, things like note taking and meetings and managing sort of human capital.

So that's also part of it.

Open source and open science is managing the human capital, managing these work groups.

So one of the other big things about open source and open science is this idea of practice.

And so, you know, it's very practice driven as opposed to sort of

theoretical or philosophically driven.

And so, you know, practice means that you need to establish practices, procedures, routines, workflows, and technical solutions for working open.

So this means you need to adopt open practices.

It could be community review of code, releasing code under a license, pre-printing manuscripts, and generating open datasets.

So these are all things that evolve

know planning but also practice and being able to produce something at the end related to that are what we call open workflows so when we want to release an open data set we can't just pop that out onto the internet we have to put open data sets into a production line model where we you know clean the data we analyze the data we produce metadata and then we publish that formally

in a repository with metadata attached.

We also want to have community standards for open participation.

So we can't just expect people to pick things up and run with them without giving them some standards for working.

So, you know, usually in academic work, you're trained a certain number of years and it becomes kind of intuitive to you what to do.

But a lot of people, especially students, undergraduates,

they come in and they don't know what to do.

They don't have like those standards ingrained.

And so you have to have community standards, but also because sometimes the incentives aren't there for people to follow long, lengthy procedures.

And so they don't.

In any case, this helps you standardize these efforts.

Then we have open technical solutions.

So open source and open science are built or enabled by technical solutions.

couldn't really have open source and open science without having open technical solutions like GitHub, live streaming tools, and bibliographic tools.

And of course, those things don't make something open source or open science by themselves, but they enable those practices.

So this is a nice graph where I kind of

talk about vision of sustainable open science.

And so I have a couple of categories here to the left and to the right, which are sort of suboptimal areas.

And then where we want to go is we want to go to this zone, this elusive zone in the red box.

So the elusive zone in the red box are the set of tools that would allow us to do sustainable open science and open access.

And the stuff to the left and the right are kind of like things that we do now that are suboptimal.

So we start with the suboptimal and we talk about black open access, which is kind of a fancy word for piracy or informal sharing.

And so each of these types of open access are colored.

Black just means it's kind of related to under the table or

you know, piracy or something.

So one of the examples is Scilab where, you know, my position on piracy is it's justified given the incentive structure in science.

but you know you have systems where you can just get access to things and then of course sometimes we share pdfs by sending emails to our colleagues and sending in the paper and that's all black open access because it's very informal and it's not really sustainable because you're relying on people to have you know at you know someone has to get access to these and then share it with someone else

Then you go over to the other side and you have two other areas of open access.

You have gold open access and diamond platinum open access.

So gold open access is where typically an author will pay an APC fee, which is a publication fee.

It's usually a fairly decent sum of money to a publisher to make a paper open access to all.

A lot of times journals will restrict access to paying customers.

If you are a member of an academic library, you can get access, but if you're not, then you don't have access.

And that's one of the things about places like the Active Inference Institute or the Orthogonal Labs, not everyone has that academic library access.

And so it's really hard to get things, get access to papers and things that you need.

So this becomes a very important set of points here.

Gold open access, then, of course, is beneficial to the publishers, but not to the authors.

Diamond and platinum open access is more beneficial to the authors.

There's no APC fee, but someone has to take that fee and, you know, have to absorb that fee.

So that's not optimal either.

So this elusive zone is a way to sort of build on sort of the low-cost but still sufficient resources we need to provide access to our work.

We need to open it up, give everyone access to it, and allow them to interact with the work in a way we couldn't even do with any of these other types of open access.

So the first thing we have is green open access, which is preprinting.

So this is where it's like not the gold standard, but it's also...

You can post a paper on a preprint server.

There's a preprint server that gets funded through a foundation, and that makes it open to everyone.

There's a DOI, which is a formal link.

It's a permanent link.

So your work is in a place that has some level of reputation with a mark of approval on it.

can also take that preprint and go through what's called open peer review where you open up your paper to peer review not to like three referees you've never met but to the entire community and the thing about the thing about peer review and i won't get into this uh any more than what i'll talk about here but if we do peer review with like three people on an editor you know you pick the people who are maybe the most available

or maybe the most visible people in the field, or maybe people with the most time.

And they have certain opinions about what you should do, what you should add onto a paper to make it publishable.

And sometimes it's not in the best interests of the science.

Furthermore, given that, they still don't really have a very good handle on detecting things like mistakes or even fraud.

So you can have a paper that goes through three reviewers and an editor,

and still be retracted for mistakes or fraud.

So that's not good either.

Open peer review allows us to have people, many eyes on the paper, many comments.

You can incorporate the comments by generating a new version of the preprint, and you have this continual feedback of your work.

And you can eventually maybe publish it somewhere, but you can also update the preprint continually and have that process going.

So that's, you know, green open access and open peer review.

Now a lot of papers also have data, and we can include open data management in this elusive zone because we also want to have a strategy for managing and sharing data.

And this goes along with the peer review and the access to the preprint.

That's the link to it, and that the data can then be reviewed and worked with.

independently of the preprint, but also, you know, it's transparent and people can actually work with it far beyond the lifespan of a single paper.

Then we have overlay journals.

So we can have preprints, but that sits on a preprint server.

We can have multiple versions, but sometimes we want to build our own journals because we want to narrow the focus of the preprints, funnel them down into

things of interest for certain fields.

So say, for example, we had an active inference overlay journal.

It would take preprints in the active inference area, or any of those preprints, and present them as journal content.

So what journals do, besides conferring status on people, is they give you this curation of the literature, of research.

So that's what overlay journals can do, but without the overhead of a journal.

So you can have your preprints, you put them on this with the skin over the top that gives us this aperture of what a journal would give you.

So that's the elusive zone of open access.

And I just wanted to point that out because I think it's a good way to think about sort of production of science and the production of academic work.

I posted a blog post for Open Access Week 2024.

And it's kind of a critique of open access that exists.

And this is why I kind of went through this and basically arguing this post that open access is essentially failed, at least up to this point, that we don't have the right incentive structures.

We don't have the right intrinsic and extrinsic incentives to do open science correctly, open access.

And so we need to change those.

And so you can read that post.

On my blog, Synthetic Daisies.

So in that blog post, I talk about the intrinsic and extrinsic motivators for doing open access sustainably.

So this, you can think of this as more generally as a reinforcement learning of working open or reinforcement learning of open.

So when I talk about intrinsic motivators, I talk about the internal drive to do something.

So, you know, the idea is that you would do things that are meaningful to yourself.

That's an intrinsic motivator.

Or what motivates me in terms of a scientific question?

What motivates me in terms of how I can contribute to the field?

And then how can I benefit from or contribute to a team?

These are all intrinsic drives.

These are things that sort of you generate and you interact with a group based on those things.

By contrast, extrinsic motivation is the external encouragement to do something.

So this could be money paying people.

This could be cost and benefits.

So if there's a cost to opening my data set, I'll do it.

If there's a benefit to opening my data set, I will do it.

And that leads to an inequality within the scientific community.

And then, of course, prestige, which is, of course, if something is prestigious, I'll do it.

But if something isn't prestigious, I may not do it.

So it interacts with intrinsic motivation, but it also is this external

force that sort of is encouraging people.

It's like a reinforcement mechanism.

Extrinsic motivations can also be where things tie into existing activities.

So one example is where a lot of funding agencies and journals will mandate that people share their data.

If you want to publish a paper, you have to share your data.

So this is like tying data sharing

and doing this to publish it.

The problem is there's no intrinsic motivation to do this.

It's not meaningful to everyone.

And so people will do it to different degrees of completeness or of usefulness.

So if you're forcing people to do something versus it's something that people want to do and think is a good idea, it becomes uneven and you don't have the same quality everywhere.

Sometimes there's, you know,

Of course, there are other types of mandates.

And there's been, in general, an overemphasis, I think, on mandates and sort of top-down motivation to do things in open science and open access, trying to get people to adhere to certain standards or trying to enforce mores that may or may not fit the situation.

So this is a problem in open science, open access, and even open source.

where people, you know, you have to kind of figure out the psychology of the people doing and enabling a lot of these open practices.

So if we look at this graph, we see that their intrinsic motivations and extrinsic motivations interact.

So the middle of this in the gray box, you have intrinsic motivations,

Things like, I want to do science, I have theory building goals, I need to create tools.

And these things feed into extrinsic motivations, such as funding, community building, adhering to the standards of the field, and adhering to rigor, standards of rigor.

So this is all kind of these intrinsic and extrinsic motivations interact, and then contribute to how you carry out your open science, open access, and eventually open science.

So now I'm going to shift to sort of how we approach this in open source.

And so this paper from Neuron, it was published in 2017.

It's called A Commitment to Open Science in Neuroscience.

And Corey Gleason, Andrew Davison, R. Angus Silver, and Giorgio Ascoli were the authors.

They were involved in an organization called INCF,

which is the International Neuroinformatics Core.

And they've all kind of done work in this area and they're contributing to a project I'll talk about in a little bit that's open source.

Sort of solves a lot of problems in neuroimaging and how to do that.

So one thing that they point out in this paper is that code sharing is useful for a number of things in science, but they make a claim that releasing code can be used to replicate results.

So one reason why we do code sharing is so we can replicate our results.

We can be confident that those results can be replicated because we have the code and everything is perfect, which is not always the case, but this is the claim.

This is the motivator that they're talking about.

And so sometimes you have to be strategic about how you release your code.

Sometimes you just release your code as a matter of requirement.

But sometimes you can actually benefit from releasing your code.

And in this case, in the paper, the type of releasing core scripts and libraries before publication, it will offer early feedback from the community.

So you can release your code just as a matter of sort of requirement, or you can benefit from it by releasing your scripts and libraries

early so people can debug them and help you if there are any problems with them or whatever.

And so you end up with this nice optimal situation.

Another paper that I'd like to highlight here is this paper from Nature Technology, a technology feature in the journal Nature about six tips for in public with your lab software.

So, you know, it's not enough to just write the best quality programs.

and to sort of put it out there, you need to really kind of think about a lot of other things going on under the hood.

So one of those things is that you need to sort of have, carve out some time for maintenance.

So if you put software out into the world, one of the reasons you're doing so is to sort of serve your community.

You know, sometimes you want to be the standard in the community.

Other times you want to

give this gift this to other researchers so that they do better research sometimes you want the prestige of having like the top software in your field but whatever the reason you want to be able to maintain the software so that it has a longevity

And so different sources that they talk about in this paper estimate anywhere from 5 to 15 hours a week, 20 to 30% of the time of the project to be spent on maintenance, maintaining the software.

Now, as you might guess, you know, there isn't a lot of motivation for that unless, you know, your software is really successful.

And so it can be a problem if you, you know, take the leap into putting your software out there, but don't.

devote the time to maintenance, it ends up hurting you later.

Another problem is that you need to simplify runtime.

So a lot of times you'll release software and people will try to install it and fail because they don't have the right dependencies.

They don't know how to run the software.

And especially with a lot of academic software, we kind of build a kludge of plugins and other types of software on top of software.

So that's not ideal.

That's a problem that can't be solved necessarily by incentives and motivations.

Some of it is going to depend on how we implement the software, and I'll talk about that in a little bit.

You also maybe want to spend time in interface design, automated testing and documentation to help users who, you know, when you're in your own research group, you're sort of in a bubble.

where you understand what you're doing, you've used the software, you understand the specific problem for which your software is written, but when you release it into the public, you might get people who are not as experienced or simply don't understand, you know, what some of the variables are or whatever.

And so there are a lot of ways that, you know, making this easy to use is an imperative.

And then a need for transparency in community building.

So if you release software,

you need to have users to make it worth your while.

And if you have users, you want to build a community around that so that you can do things like have more, you know, have a user base, but also have a community where, you know, you can solve problems collectively and address open scientific problems and really for everyone to benefit.

But also transparency, because we need to understand everyone's methods

And we don't want to make it, you know, obtuse for people when they use the software to understand the methods that went into generating the result that they're getting.

So you need to think about all these things when you open up your software, when you go open source with academic software, especially.

This brings us to another point, which is the need for research software engineers.

So in academia, we usually homebrew software.

We end up putting it out there and, you know, we maybe find time to maintain it.

We find time to build a community, but the software is not up to the standards of say professionally designed software.

And so one of the things that we don't have is our research software engineering skills in academia.

A lot of academics know that technicals matter, but they don't know how to build software.

So software needs to be maintained and needs to undergo quality control.

And so those things are,

you know, separate skills.

So the problem, of course, is that in academia, we tend to have graduate students or open source contributors build packages or whole software platforms, and their contributions are well appreciated, but their involvement is itinerant.

So that means that they might build something and then leave the lab in two years where they may, you know, contribute to an open source project and you may never see them again.

And so they built something with no documentation and obviously no maintenance, and it just kind of sits there and withers.

So I've seen this a lot in my career where people will build software for one purpose, one use, or like for a single paper, and then it goes away because it's not maintained.

I mean, there's a lot of code I've written when I was a graduate student, say, where that was the case.

And I wish that we could recover that software and have maintained it, but who has the time?

So really what you need to do is you need to include research software engineer roles in your organization, in your academic organization.

And sometimes open source projects will have this expertise, but sometimes they don't because there's no requirement for it.

But it's also a different skill set than, say, like the academic skill set that people building the software might have.

So this is something where you can either cross-train people, and there are programs where you can cross-train research software engineers, but more often than not, you just have to help people acquire those skills.

And so there are all sorts of roles for RSEs in different academic and open-source organizations, things like intentional software design, continuous integration, release cycles,

bug fixes and maintenance strategies and those are all things that are separate skills from the academic world separate skills even from like you know if you were even if you're a computer science researcher you're not necessarily a software engineer so you have to have that set of skills or at least you know have a way for people to acquire them okay now we talk about open data so open data is also a part of open science

And as well as a part of open access.

So Wikipedia defines open data as openly accessible, exploitable, editable, and shared by anyone for any purpose.

So it's a data set that you can access, you can exploit or change, you know, you do analyses on or whatever.

You can edit it and even share it.

And so anyone should be able to do this.

Now, when they say any purpose, they mean that it's licensed under an open license.

So an open license is a permissive type of liability protection and copyright for people who put these things out there that they work on and they want to benefit from in some way, but they also want others to benefit from as well.

So it's a legal regime to maintain, you know, to manage liability, but also to manage

the social relationship between the person making the data set, producing the data set, and the person using it.

Open data allows for data to be easily shared and reused, but requires governance, issues of ownership, and adherence to community standards.

So this, you know, part of this is, you know, open data as in sort of producing data sets for public consumption.

The other part is working on what they call an open data stack.

So an open data stack is a management strategy that is needed for sharing, reusing, and archiving data.

So we typically start at the bottom with some sort of interactive interface for working with data.

We need a metadata format.

We need a cloud server to host our files on, to host our data.

And we also need standards.

So we can have these higher level standards for data production,

for open science production in general things need to be findable accessible interoperable and reusable so we need to have those four principles embedded in our workflow and then we need to have a strategy for archiving data and for that we might use something like dryad which is a data repository so we have these techno this technology layer a standards layer

cloud and security access layer and an archival layer.

So put together, that's called a data stack.

And you want to build a data stack that's, you know, the tools that you know how to use and standards that you know how to adhere to, but also things that last.

You don't want to pick something that will last a year and then you have to migrate to something else.

So you want a sustainable data stack

with sustainable tools, with things that are easy for your team to use.

And so this also holds true for our open access publishing strategy.

We don't want to pick a strategy that isn't sustainable because we'll stop doing it.

Now in the C.Elegans community, and this involves my work with the Open Room Foundation,

you're fortunate in that you have many open databases.

So the nematode C. elegans is often used in biomedical research, and there's some basic research on it as well.

And it's a tight-knit community where there are many, many open data sets, meaning people produce data, they share them with the community, and they have tools that are curating

lot of these data sets and papers and things like that so we have all sorts of data sets uh available for all sorts of modes of investigating c elegans biology and behavior so we have a number of open databases covering anatomy genetics gene expression regulatory elements behavior connectomics so this includes like the worm wearing site

which includes data on nematode connectomics.

So we know in C. elegans, we know all of the neurons in the connectome.

We know a lot of their connections, either through gap junctions or through synapses.

And we have wormwiring diagrams for both the male and the hermaphrodite.

So we have very rich data on

worm brains.

We also have worm atlas, which is an atlas of structural anatomy.

So on top of the connectome data, you can fit structural anatomy data.

And that's at wormatlas.org.

The C. elegans database run by MDC in Berlin.

features a lot of data on proteomics and RNA.

And so this is another database that we have available.

Sengen is a database of gene expression in C. elegans in the nervous system.

So we can build a model of the connectome.

We can have the worm-wearing data set, Sengen data,

the worm atlas, the MDC C. elegans database, and we can actually, you know, produce a study from it.

So we have all those layers of data that we can include in, say, a model or in an analysis.

We also have this resource worm base, which is where we can explore the world of C. elegans defined mutants.

So C. elegans has a number of defined mutants for specific genes.

There are specific mutations that we can backcross

And so if we have a strain of, say, some mutant, defined mutant, we have this worm that has a certain mutation that might be expressed in some behavioral or anatomical deficit.

And so we can compare that to the wild type and say things about mutations of function.

And so this is a really useful resource both for experimental work and for looking at the C. elegans genome and finding, you know, like alignments and

other types of data like in one of the papers we recently did we found different types of we aligned different open reading frames within genes so it's really a good resource and then of course nature methods this paper by people in the open worm foundation and this is called an open source platform for analyzing and sharing worm behavior data

And this is on something called the TURPSY tracker, which is a movement tracker.

So they can actually take microscopy images of C. elegans.

They can track the worm's movement and they can characterize different movements, their stereotypical movements that worms make, and they can actually track the behavior of the worms and classify.

So that's, you know, that's the kind of data that's available on C. elegans.

And so if you think about the active inference community, you know, what kinds of

data sets and what kinds of resources could be built around things involving active inference so you might for example think about you know and some of these resources are just more general resources that you could bring to bear so you might say for example you know go to a neuroimaging resource and then go to a behavioral resource or build a behavioral resource there are all sorts of ways that you can build these um

open data repositories to help the community, aid the community in solving problems and answering questions.

So this is a talk by Ecofreed.

This is called Open Science as an Antidote to Poor Practice slash Cynicism.

So one of the things he advocates for in this video is to use open science as a way to combat

Core practice, like we said, open science is about reinforcing practice.

And when you reinforce practice of open science, you reinforce good scientific practice in general.

Because you're focusing on the process.

You're focusing on, you know, crosschecks and, you know, other things that involve teamwork.

So there's a lot of this sort of advocacy for helping science through open science.

But also combating cynicism.

where when we see poor science, we often think, well, this is just the state of science, that it's actually failing or that it's bad.

And so you can fight disinformation as well with open science.

This is a nice set of views here.

So in the orthogonal lab, we have done a lot of work modeling open source communities and their sustainability.

So I talked earlier about

open access sustainability.

Now I'm going to talk about open source sustainability.

And so we've done a lot of work with modeling frameworks to understand this problem.

So why is sustainability important?

Well, in the case of open source, many projects fail for a host of reasons.

Sometimes they're related to people's projects not being of good scope, where they weren't asking very good questions.

sometimes the team falls apart the team is dysfunctional sometimes the project runs out of money so there are reasons why projects fail and sometimes open source projects especially have problems with maintainers problems with people contributing so they fail for those reasons as well so you can have things like poor scoping of a project we're trying to do too many things we're trying to do too few things

We might have bad management or a lack of participation, or we might have no resources in terms of time or money.

So those are all things that we want to be able to figure out how to manage at a reasonable level.

And to do that, we have to understand sustainability.

So in these models that we've built, we use different methods.

including active inference, reinforcement learning, and large language models to model open source communities and understand what makes a project sustainable.

So the first one is this preprint that kind of includes a lot of the work that we've done, especially in terms of reinforcement learning models and active inference models.

This is our research gate.

And then there's this newer work on

combining agent-based models and large language models.

This is Sarah Bostowal's Google Summer of Code project from this past year.

So she actually presented in front of the Google Summer of Code symposium, and she presented her project there.

And so this work on large language models involved using large language models to generate issues and then generate agents that could solve those issues and see which kinds of almost create fake projects and then create issues to solve and then see how the agents were able to solve them.

And so we used the combination of agent-based techniques and large language model techniques to sort of work this out.

It was a very exciting project.

Another approach is to take the multi-agent reinforcement learning route.

And so you see this grid down here and this figure and all these dots, and they represent agents that are doing different things.

Sometimes they have different statuses in the open source community.

Sometimes they're of a different level of competency.

And you can actually run these agent-based models.

You can implement them in MISA, which is a Python package, and run these agent-based models and then use reinforcement learning algorithms to sort of,

optimize the strategies that they're using.

So this is, you know, again, where you're using these sets, you're creating these projects, you're using these sets of issues, and you're measuring their ability or their competence to solve those problems.

And so Shubham Soni, R.V.

Rajagopalan, and Hemantri Fogal were all involved in this work over the past several years.

We've been doing these kind of reinforcement learning models for understanding open source sustainability.

And then we've done this work on collective cognition for AI ethics.

This is where I actually applied active inference.

And we modeled agent behavior in terms of active inference using a package called Interactively.

And this is the GitHub repo here, Interactively.

This applies a partially observed Markov decision process or a PYMDP model to the problem.

We also used a multi-armed bandit approach to model explore and exploit behaviors.

So explore and exploit is sort of the trade-off that agents make between exploring a project, exploring issues to be solved, and exploiting specific issues and solving them.

So it's like coordinating agent behaviors and then figuring out what strategy they're going to use.

modeling all of that so it's a very interesting project it needs more work on it so if people in this group in in the active inference institute are interested in following up on it that would be welcome and so there are a number of issues about maintenance that are very important to sustainability and this is especially true in the academic sphere because academic organizations and open source typically don't have the resources that private organizations might so

Maintenance and sustainability issues are a multitude in open source.

So I'll talk about a couple of them here.

So the first one is Lob4j, and this happened a couple of years ago where there were a number of open source projects that got hit with the security threat.

And it was because their security fell out of sync with the current state of the art.

So in open source, we don't maintain things properly.

as well as sale like commercial products so you have these holes that can be exploited by malicious actors so love 4j was so serious that they had to consult with the us government to try to help fix this problem and so this was a big problem um with just kind of maintaining the security of the software

but also it exposed a big hole in the open source model.

And so this is still something that probably requires meta governance, you know, some organization between like the federal government and open source organizations to solve.

The second is called the maker-taker problem, which is basically where some organizations make open source software and other organizations fork or copy the software for their own purposes, thus being a taker.

And the problem here is where that relationship is not reciprocal.

know the makers will develop systems that encourage forks and then the open source contributors or the takers are expected to contribute back to the makers some sort of you know maintenance or something like that but they fail to do so and so this is the problem where you need to coordinate this reciprocal relationship and so an example of this is the wordpress for controversy so wordpress is the maker in this case and

I think it's WP Engine is the taker in this case.

WP Engine was forked from WordPress, and it resulted in a fight between the two organizations, one being the maker and one being the fork or the taker, and they've been fighting over commercial issues and fair use issues that have served to break the norms of open source.

So there's this primary issue of maintaining sort of a reciprocal relationship

between organizations that rely on, basically rely on each other to survive.

But you can have all sorts of, you know, fights and disagreements about different ways that the software can be used commercially and other types of use that, you know, although are stated in the license, sometimes evolve away from the license and their practical concerns that, you know, isn't going to be solved by just some sort of reciprocal relationship

or even intrinsic and extrinsic motivations.

And then finally, the final example is XZBackdoor slash AutoTools.

This was an exploit that was made in an open source repository where it resulted from a lack of maintainer vigilance.

So this maintainer in this particular case was burnt out.

they didn't feel like really kind of closely inspecting every pull request which is where a contributor will submit a change or a contribution to an open source organization and so malicious malicious actor was able to get exploits into the software and it became a huge security problem it's kind of like log4j and the problem here is it's you know the failure here is at the level of maintainer and it wasn't the maintainer's fault necessarily

It's that sometimes people in open source organizations, as well as open access organizations, open science, can become burnt out, especially when you have to do all of these things, all these practice-oriented things to realize the open vision.

And also you have disincentives that lead to problems.

So if you're not paying your maintainers enough, or if they don't have that intrinsic motivation high enough,

they can't do the job that they need to do to make things work.

Or the extrinsic motivations aren't, well, good enough.

You don't really pay your maintainer very much, and therefore they're disincentivized to sniff out exploits.

And so these are all problems with maintaining open source, especially in an academic environment.

This is a second YouTube video I wanted to point out.

This is behind the scene, behind the screen.

Here's a wordplay here, which is open source software is both a tool and topic for research.

This is from William Schuller at Santa Fe Institute.

And he talks about sort of the history of software and how you can research open source software, but also use it as a tool.

It has a lot of interesting issues in here.

So I recommend watching this.

Now I'm going to talk about how to implement open software for a purpose or an organization.

We talked about one of these points about distributing open academic software is being able to run it properly or run it in a way that doesn't rely on too many dependencies.

And so this is where we can use something like Linux or some flavor of Linux to package data analysis tools so that people can use them in an environment where they don't have to worry about dependencies.

They don't have to worry about

having the proper software requirements.

So there are two examples of where we can use a Linux distro to package data analysis tools and present them to people in the community, sometimes with fairly low technical standards.

So the first is Narrow Fedora.

And so this was created as free and open source software.

This is where we have a version of the Fedora distro of Linux.

And in Fedora, you package a bunch of neuroimaging analysis tools, and you put them in, and you just give people the package.

They install it on their machine, and then they can run those tools without worrying about dependencies or getting the thing run on Windows or Mac or whatever.

The other example is NeuroDebian.

This is run by INCF.

This is the group led by Pore Gleason and Angus.

run by Greg Gleason and colleagues.

And this is where they have a Debian distro of Linux and they have these neural imaging tools in Debian.

So you have Fedora and Debian.

You can choose between the two distros and you can run these neural imaging tools in these environments.

And that makes it easier, I think, for neuroscientists to do this kind of work, to work with these packages.

Some other things could be done for active inference, and especially in Fedora, you have these different flavors of Fedora, like for artists or for musicians, where you can package tools for those professions, and they can just, you know, use it out of the box.

So focusing on the NeuroFedora project, this is a special interest group whose goal is to provide neuroscience researchers and enthusiasts

with a strong, easy-to-use, ready-made, free and open-source software platform for their work.

And Ankur Sinha has taken the lead on Neurofedora.

Morgan Hough, who's part of the Orthogonal Lab, has also worked on this as well.

Then we have NeuroDesk.

So NeuroDesk is not an operating system, but it's a virtual machine.

It's a virtualization or a containerized environment where you have basically a virtual machine on your existing computer,

you run everything inside of that and it really paves over a lot of the operating system differences and system requirement differences that you might find trying to install open source software on your home computer so like you know this is where you've basically created a controlled environment for the software so it really helps people run it when they have very little technical expertise and just get it to run that's all you need to do

So NeuroDesk is a containerized data analysis environment specialized for neuroimaging research.

And this is something that you can easily do with the active inference.

It's where you package the software code with the operating system, libraries, and dependencies required to run the code in a single lightweight executable.

So this is platform agnostic.

It's just you open up the container, you run it, and you can do everything inside of there.

So thank you for your attention.

I especially want to thank the members of the Open Source Interest Group.

We have other people who cycle through the group from time to time, and you're welcome to join us.

please join the Orthogonal Research Lab and then be on the lookout for our open source activities.

We usually do the interest group in the summer.

If you're interested, get in touch.

Otherwise, happy active inferring.

All right.
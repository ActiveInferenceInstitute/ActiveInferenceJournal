SPEAKER_01:
all right keeping with the co-authorship theme next talk is a 25-minute recorded talk by alex keifer resilience and active inference 2024. this is alex keeper


SPEAKER_00:
This is Alex Kiefer.

I'm here to present some ideas on resilience and active inference.

So this is largely just a unpacking in a careful way of some concepts from this paper from 2022 that I was involved in, but also explore some novel directions that this thinking has taken since then.

So the concept of resilience, really, if you look at the etymology, it means the capacity to recoil, essentially, or to bounce back, or just to survive through hardship and change.

So obviously this concept is sort of a fundamental one if you're looking at life forms, dynamical systems more generally.

In the context of ecosystems and sustainability, it's an important concept.

So that's why we're interested in this.

It's actually also sort of a fundamental concept in material science.

So since this paper has come out, I've been approached a couple of times by journals of material science for follow-ups and such.

So as I said, the fundamental concept has to do with recoiling or bouncing back.

But there are some peripheral different facets of this meaning, and we explored all of these in this paper.

So roughly speaking, you can think of resilience in three ways.

So first, there's inertia.

So inertia, I mean, I guess literally just means really

sort of not changing, so persistence.

But you can also think of it as resistance in the sense that unless you exist in a vacuum, then you're a system that's embedded in some broader system and inertia is basically continuing on your way despite changes that might occur or interactions with that broader system.

So if you think of the red arrow as representing the path of a subsystem or an agent through its state space, and the black arrow is some kind of potential disturbance, then the top diagram here illustrates inertia.

The central meaning again of resilience arguably is elasticity or the ability to bounce back.

So in that case you undergo some perturbation or change and you end up recovering your initial, doesn't have to be that you recover your exact initial position, but the point is you make your way back to a reasonable state in terms of the kind of system that you are.

Then there's also this sort of third thing we call plasticity or adaptation.

So in this case, you begin in one configuration, the red arrow at the beginning here, and after the perturbation or disturbance, you end up in a slightly different configuration.

So that should be read not as just that you change your state, like you update your...

Your beliefs in the time scale of interest or something, but you actually you actually wind up somewhat adjusting your model in response to this change, so this is more like something like Alice stasis.

whereas elasticity is returning to the same state.

So I'd say there are some close relationships, just to get into some philosophical conceptual analysis for a second here.

The first thing, inertia probably doesn't really exist in its pure form, right, in nature in a way.

Or if it does, it reduces to sort of elasticity.

So, you know, I mean...

Presumably, if you perturb a system, then you have to cause some change in the system, however slight.

Otherwise, there hasn't been a physical interaction, right?

And inertia is going to look, in practice, just like the capacity to sort of instantly recover from that tiny change.

um there is in the limit case there is sort of a different meaning here because again there might just not be a change but i think um i think in the context of realistic systems there will always be some some interaction um but arguably these two these two concepts are very close um plasticity seems like um on the face of it like a different thing because um

First of all, the ability to be plastic in this sense sort of explains how you could come to have this property of elasticity.

So if you look at the example of neuroplasticity, for example, it's the ability to adapt, to change, and to basically update your...

your synaptic weights to arrive at a new model that allows you to be, in part, allows you to be resilient in these first two senses.

But there's another sense in which if you just assume that there's some property of the system that doesn't change through this process, which I think is a reasonable assumption in the case of biological systems, then there's going to be some kernel of the system that survives through change.

And then you're back to your original definition.

Okay.

So if we look at this concept through the lens of the free energy principle, I mean, first of all, it turns out to be a very fundamental, as is often the case when you try to unpack

sort of fundamental properties of systems in nature in terms of the FEP and active inference, you find that there's a fairly direct mapping from the properties that the FEP attributes to systems or the types of systems that are well described by the FEP and this concept of resilience.

So, I mean, Carl Fristen has said in discussions of this, that essentially,

systems well described by the FTP are resilient, full stop.

So, but we can look at the three facets of the meaning of resilience here.

So certainly systems described by the FTP are, they have this property of inertia in the sense that by definition, right, there are things that have managed to persist.

So they limit their exposure to unlikely states, unlikely observations.

But they're also sort of by definition elastic systems because they have this attracting set.

So having an attracting set of states means that, you know, of course there might be changes or perturbations that are sort of catastrophic that destroy the system, but as long as the system is not destroyed, then by definition it's going to make its way back to this attracting set of states.

So resilience in the core sense is really almost a defining feature of systems that are described by the FEP.

And then finally plasticity, you can think of as something like Bayesian model selection on some time scale.

and so that that might seem like something extra but it's only extra if the multi-scale aspect of the fep is extra and i would argue that it really isn't because at its heart the fep is a is a scale-free description of things and this multi-scale and scale-free are sort of uh sort of um connected notions um so that's plasticity um okay but so most of the interest i'd say of this lies in the the

mapping of resilience onto this formalism of active inference.

So a key concept in this analysis is the notion of degeneracy.

And this is a somewhat technical term.

Here we mainly follow in the definition of, and in fact, the analysis of degeneracy.

in terms of active inference in Sajid et al.

2020.

This is a great paper on this topic.

So here we have just the expression for variational free energy.

And the top row is a expression in terms of basically Helmholtz free energy.

So it's an energy term minus an entropy term.

So if you just look at that for a second,

Again, following this paper from 2020, we've identified degeneracy with the entropy term.

So we'll get into why that is in a second.

But basically, the concept of degeneracy is useful redundancy.

So you can think of this as complexity that earns its keep or that's worth having.

So complexity in this bottom breakdown of variational free energy

You analyze it in terms of complexity, which is the difference, the KL divergence between the prior belief overstates, the generative density overstates, and the approximate posterior.

So this is kind of like how...

How difficult is it or how much change does the system have to undergo in order to represent the states that are inferred from some observations?

What's the departure between that posterior distribution and the prior distribution over states?

That's complexity minus accuracy, the expected log probability of observations given states.

So what's the connection between complexity and this idea of useful redundancy?

Sorry, of just redundancy.

There's an assumption here that complexity and redundancy are closely related.

I think the basic idea is that, so redundancy in this context is quite literally just duplication of states.

So I'd say the classic example is like an error correcting code.

So if you're trying to transmit a bit of information over a channel,

You might send 10 copies of a bit in case some of those bits get flipped or corrupted, and you want to make sure that the message gets transmitted.

So that's redundancy.

It increases the complexity because, well, one way of looking at it is that it increases the description length, right?

This KL divergence is like...

How much more do you have to say in order to communicate that an event drawn from this Q distribution has occurred, given that you assume this P distribution?

So you can think of it as, well, these extra bits are the redundant bits.

So there's a sort of conceptual connection between complexity and redundancy.

And then degeneracy on this first definition is just the complexity that serves some purpose.

And we'll get into that in a moment.

It's just also important to note that there's this definition of degeneracy as partial redundancy in the literature.

And this is partial overlap in function.

So the idea is that redundancy exists in a system, sorry, degeneracy exists to the extent that multiple states in the system serve some function in common, but then those states also have otherwise have distinct or disjoint sets of functions.

And in practice, I think these two almost always line up.

So there might be sort of

limit edge cases in which they're different, but in practice, if you have multiple states, they'll always have some different properties and therefore perform some different functions.

Okay, so this is one of the core things that we pointed out in this paper of ours.

This actually, sorry, this breakdown is from the Sajid et al.

2020 paper, but we sort of have a different gloss on the central term here.

So this is the complexity term in the variational free energy.

You can break this down in terms of, further break this down in terms of this expectation under Q of the negative log probability of states under the generative model.

So this is called a cost term in the 2020 paper.

We describe this in epistemic terms as an uncertainty term.

So this is basically, you can read this as the Shannon minimum description length of the states given the generative density.

But it's sort of the expected surprise, how surprising are

are these states that I infer using the variational posterior under my generative model.

So that's an uncertainty term.

And then we subtract this degeneracy or the entropy over the variational posterior over states.

I mean, first of all, sorry, I realized that I didn't quite make the link here between degeneracy and entropy.

So why is this the case?

Why do we identify these?

It's because...

This means if this Q distribution over states is higher in entropy, it means that there's a degenerate mapping in the likelihood from states to observations, right?

So meaning for multiple states, you can get the same observation.

And so then when you invert that likelihood mapping in order to recover this posterior, this means that for one observation, you might have several different states.

And the spread of the distribution over states, of course, is just the entropy.

So that's the link to degeneracy there.

So this complexity term is uncertainty minus degeneracy.

One reason that's interesting is that when this complexity is zero, it's not as if the system is completely certain of what's going on.

The uncertainty term actually remains.

It's just balanced by this degeneracy.

In other words, all of the uncertainty in this ideal case is useful.

And so I think this really captures what degeneracy in this context amounts to.

It's uncertainty that's useful.

And it's useful precisely because there's some amount of uncertainty that's inherent in the source of the information you're trying to model, right?

So here P is the, in this context, is the generative model that the system has of, it's the sort of internal generative model of

of the external states.

And so there will be some source entropy associated with that system that you're trying to model.

And so long as your variational posterior matches that degree of uncertainty, then you're in an optimal model.

So in other words, for maximum model goodness, try to ensure that p equals q.

So this ties nicely into the idea of plasticity.

Resilience is plasticity.

So your goal here is to adjust the posterior to match the generative model, essentially.

So you can do that in perceptual inference.

You can try to infer a state, a Q distribution.

This is, again, an approximation to the true posterior over those states under the generative model.

But equally, you can adjust the generative model in a multi-scale setting.

You can learn or just update the parameters of the generative density in order to match what's in the external world.

So this complexity term is affected by changes to p as well as changes in q. And so I think the fundamental idea that I had here was that

In an ideal sort of optimal system under the free energy principle, you should expect, first of all, P will be adjusted so as to model the environment for sort of basic, you know, free energy principle slash good regulator theorem types of reasons.

And then Q will also be adjusted to match P.

So this involves this cybernetic principle called the law of requisite variety, which is sort of a, it's closely related to the good regulator theorem.

But basically it just says that, you know, if you're trying to regulate or control some states, then you need at least as much variety or in probabilistic terms that translates into variance or entropy.

You need as much variety in the controller as you have in the environment.

And so it's a fairly intuitive thing because you basically need sufficiently many degrees of freedom in the controller to be able to model what's in the environment.

So you can see this degeneracy property of resilient systems or any system under the FEP as an instance of this law of requisite variety.

It's also clearly, by the way,

an obvious case of constrained maximum entropy so there's work um showing that that many many lines of thought really showing that the active inference uh sorry the free energy principle is um is equivalent to a principle of constrained maximum entropy uh so here the constraint is this uh you can you can think of this as constrained by the accuracy um

But you basically maximize this entropy term subject to that constraint, subject to the generative model.

Okay, so looking just a bit more closely at this notion of degeneracy.

So I said that it's central.

One pointed question you might ask here is, well, do all resilient systems have this degeneracy property?

And so I want to say the answer is yes.

So the type of degeneracy that we've already talked about is this degeneracy in the likelihood mapping.

So this is multiple states mapping to the same observation.

And again, that can be cast as you'd expect to see that given that you're performing maximum entropy inference.

So that is crucial, but equally important, I'd say, is degeneracy in the dynamics.

So this characterizes just systems that, again, that have an attracting set of states.

So if these STs are states at a given time, and then ST plus one is the state or set of states at the next time step.

So this, in order for a system to be resilient, it really has to be the case that this

transition, that these transition probabilities or the dynamics of the system are such that that that mapping itself is degenerate.

So, you know, this means that basically, you can perturb the system in several different ways.

And you'll still arrive, you'll still end up at the same at the same state or, or a set of attracting states.

Obviously, if the if the perturbation is too large, then

That might not be the case.

The system will diverge, and you'll destroy it.

But by definition, systems that survive through change are the ones that have this property.

So there is a very deep link between resilience and degeneracy, if you consider dynamics as well.

Just a couple more points here.

One is something that I've recently been thinking about.

It's actually a theme that's been on my mind for some time, but I've recently written about it in some other work.

And this is what does the principle of requisite variety and

what are these concepts of degeneracy and resilience look like in the context of multi-scale active inference?

But specifically here, I don't just mean the fact that model parameters as well as states can change, but I mean multi-scale active inference in the sense that you've got potentially higher level agents that are composed of lower level agents.

So in that kind of setting, requisite variety ends up looking like the property of higher-level agents, that they're composed of a variety of different types of lower-level agents.

So, I mean, I'd argue that if you combine this law of requisite variety with multi-scale active inference in this sense, then you really sort of automatically have something like a diverse ecosystem in which there are many different types of agents that participate.

So here in this diagram, we're illustrating, first of all, just the same degenerate state observation mapping that we saw in the previous slides.

But here, the nodes are individual agents that compose this higher-level agent.

Also, as pointed out in this paper by Carl Friston and Daniel Friedman et al.,

In this kind of context, different agents can be expected to, and I think empirically in some simulations are shown, to take on the roles of these different types of states.

So of course, the breakdown here is in terms of active sensory and internal states.

So here you've got active sensory and internal agents.

And that's one instance, I would argue, of this application of the law of requisite variety to this higher level system.

I also show here the policy selection based on expected free energy for completeness, but that's sort of the main point is this specialization and diversification of agents in this setting.

In terms of the second definition of degeneracy, you can also think of these agents as playing somewhat distinct but partially overlapping roles.

And there's some some lots of interesting work on on that sort of multi agent perspective from the point of view of active infants.

So that's really it.

I'm just going to conclude with some sort of a philosophical question that I think is kind of important for tying this together and sort of checking that it just makes sense.

So I've said earlier that any system well described by the FEP is resilient.

Hold on a second.

Not all systems are equally resilient, though, right?

Teacups are sort of the paradigm of a fragile as opposed to resilient object.

So since the FEP describes things in general, doesn't it mean that all things are resilient?

And doesn't that contradict this obvious fact?

i think the answer here is that it's simply it's a matter of scale so yes all things are resilient all things that is things that are re-identifiable over time and that persist through change all such things are resilient but it's a matter of scale so if you if you introduce a very slight perturbation to this teacup um very slight you know um

relative to a certain a certain reference frame or a certain scale but large enough right relative to the scale on which that system is resilient then by definition it will survive right and so obviously again in a in a real universe in which there are interactions and you're always interacting in some way dealing with some sort of um random fluctuations um then

Yes, for the system to persist through time, to be inert and elastic, the material has some coherence that allows it to persist through those changes.

So it is a resilient system.

It's just that if you introduce a change that's on a scale that's too great for the system to handle, then it'll be destroyed.

But just the basic coherence as a spatiotemporal object of this teacup is itself a form of resilience.

And then when we get into more complex forms of life, more complex generative models, that's when we begin to see more impressive forms of resilience and shading it even into plasticity and the ability to really survive through more significant changes.

So thanks for listening slash watching.

I'm happy to participate in this symposium.

Thanks to all the other co-authors of this paper by Mark Miller et al.

in 2022 that was the departure point for this work.

Yeah, thank you.
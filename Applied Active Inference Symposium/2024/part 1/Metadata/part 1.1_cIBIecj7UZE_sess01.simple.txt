SPEAKER_00:
Hello and welcome everyone.

It is November 13th, 2024.

We're live and kicking off the fourth Applied Active Inference Symposium.

It will be a great program and we will begin with Carl Friston and Lance DaCosta

discussing scale free active inference we discussed in live stream 58.1 with Lance last week and in 58.0 with Arun two weeks ago a bit about this work and today Carl thank you so much for joining and presenting on it so looking forward to it and throughout in a live chat please feel free to write any comments thanks again everyone for joining and looking forward to this


SPEAKER_03:
Well, thank you, Daniel.

It's a great pleasure to kick off the symposium this year.

I'm going to rehearse a presentation I made at the Active Inference workshop a few months ago.

but this time I hope in a slightly more relaxed and unpackable way.

So I'm fondly hoping for lots of questions and embellishments from the audience and from Lance.

This is a bit of a colloquial presentation.

It's just going to focus on something specifically that we've been working on

for the past few months which is effectively addressing the question how can one scale active inference for high dimensional real world problems and the solution that we've been pursuing is to adopt a scale-free approach and this work has been written up um and put on archives um

However, it's also been submitted for publication, peer-reviewed publication to a special collection in memory of Herman Haken.

And we've equipped the peer-reviewed version with a little forward.

So I just wanted to frame or preempt the scientific part of this presentation just by motivating why the work of Herman Haken is

foundational and underwrites many of the ideas and procedures that underlie this scale-free approach to active inference.

Herman Haken was famous for synergetics, a physics-led approach to self-organization, and in particular, the importance of understanding the coupling between different scales of dynamics in self-organization.

And if you go to the Wikipedia entry, you will read about macroscopic and microscopic variables and how they are coupled to each other in a

in the fashion of circular causality um so there's both bottom up and top down causation where the top down causation is sometimes read in terms of um the slaving principle i'm going to present this approach um to harnessing this um notion of a scale-free universe uh in terms of

installing that scale-free aspect in the generative models that we bring to the table to solve decision making, to simulate or emulate active inference under discrete state space models.

So let me start at the beginning.

The nature of things and the free energy principle.

And this is just to rehearse the fundaments of the free energy principle.

It all rests upon the notion of a Markov blanket where we can separate internal and external states.

by boundary states or states that constitute the Markov blanket namely the sensory states and the active states and for those people not familiar with this particular partition of states of any random dynamical system it's just specifying systemically a system say a brain in terms of its inputs and outputs and just formalizing

the conditional dependencies that define what is an input to a brain or an agent and what is this output in terms of the sensory and active states that thereby render the system of interest, in this instance a brain, open to the environment, the eco-niche, the heat bath if you're a physicist or a chemist,

in virtue of the fact that the inside can influence the outside via the active states, the outputs, and the outside can influence the inside via the sensory states or the inputs.

And with this setup,

we can elaborate a kind of physics, namely Bayesian mechanics, that is almost formally related and inherits exactly the same mathematical and functional form as all the other mechanics that we find in physics, including quantum mechanics, statistical mechanics and classical mechanics.

The only thing that separates the Bayesian mechanics is the fact that we have in mind explicitly this partition between internal states and external states and the blanket states that intervene and couple the internal to the external.

And that basic mechanics is effectively summarised here in terms of variational and expected free energy, where we can read the expected free energy as effectively prior beliefs about the kind of outputs or actions that, in this instance, a brain could exert on the external states.

and the variation of free energy, keeping implicitly the internal states a good model so that it's tracking in a probabilistic sense the dynamics on the outside.

And we can combine this to create a generalized free energy that now

teleologically or perhaps even anthropomorphically, we can now read the gradient flows that subtend this Bayesian mechanics in terms of action and perception just by identifying the internal and active states of any agent as autonomous states and noting that they effectively flow on the same free energy functionals.

So that's the basic setup.

But what I want to do before applying the free energy principle in the service of things like active inference is just back up and reflect upon the fundamental role of the Markov blanket and how that leads to a scaling variant

picture or view of dynamics and self-organisation.

So what I'm going to assume is that ultimately we will assume that basic mechanics means that the dynamics of anything that is defined in terms of its boundary or Markov blanket can be described in terms of active inference.

But before that, we have to think about the nature of things.

And in particular,

If one goes back to the particular physics paper, the nature of the states that constitute things or particles.

And there's a bit of a subtlety here, because before you can start talking about states, you really have to define what is a state.

Is it the state of something?

Is it the state of a particle?

And that presents an issue because you're defining a particle in terms of this particular partition into internal, external and boundary or blanket states.

So there's a slight, not tautology, but there's a slight problem in resolving

how you define states of things or particles and particles that possess states.

And the resolution of that is to adopt a recursive definition so that the particles constitute a set of microscopic blanket and internal states.

while the states are a set of macroscopic eigenfunctions, eigenmodes or just simply mixtures of the blanket states.

So I'm here deliberately using the notion of macroscopic and microscopic states just as a nod to synergetics and in particular kind of synergetics that Herman Haken developed and subsequently championed.

So this is cartooned or illustrated on the right-hand side here.

So we sort of take some states and then apply a particular partition on the basis of which states are coupled to which other states and the conditional independences that ensue.

um and then having done that we can then take mixtures eigenmixtures of the boundary states that then generate new states um and then we can um effectively by taking those mixtures we effectively reduce the dimensionality so that the higher scale

coarse grains the lower scale and then we reduce it and then we just rinse wash and repeat and we do that and every time we sort of pass through this cycle of RG operators reduction and coarse graining operators we move to one scale above successively and this is the scale invariant aspect of that I want to leverage in in the remainder of this presentation

I use the word RG deliberately because one way of looking at this or articulating this is to use the apparatus of the renormalization group.

So this is where renormalization gets into the game, that you're recursively applying these operators, coarse-graining things,

in a way that conserves their dynamics.

And that's an easy criteria to meet for us because we know that anything that has a particular partition can be described in terms of a gradient flow on a variation or generalized free energy.

So we have everything that we require in order to apply the apparatus of the renormalization group.

So what that basically does for us, once we recognize that particles are composed of states where the states are eigenmixtures of particles in this scale invariant fashion, gives us a view of any universe that comprises things of things of things as inherently resolving a chicken and egg problem.

through scale invariance by application of the renormalization group.

So here's just another illustration of how that actually works in practice when you write down the equations.

I'm not going to bother going through the equations.

This is just used iconically just to make the observation

that as you move up scales, so, for example, in an evolutionary context, this would be the dynamics of phenotypes, and then we move up to an evolutionary scale by effectively coarse-graining in a way that Hermann Haken would exactly prescribe in terms of taking those slow macroscopic variables that decay very, very slowly

which are technically the eigenmixtures equipped with very small eigenvalues whose real part approaches zero from below, effectively saying that these patterns decay very, very, very slowly.

They have a kind of persistence that lends higher and higher scales

a temporal dilation, so things change more and more slowly as you increasingly coarse-grain things.

This falls out of the maths when you simply apply a particular partition and put that partition in as the coarse-graining operator in this scale-free formalism.

In short, Bayesian mechanics entail a generative model of external dynamics.

But we've just seen that just by defining things in terms of particles that have states and defining states in terms of the functions of particles, that external dynamics are inevitably scale invariant.

It can be no other way.

And therefore the generative model must be scale invariant, or when expressed as a graphical model, scale free.

And I just slip that in as a nod to the fact that people often talk about scale freeness.

Generally speaking, when people apply the notion of scale invariance to anything, if that thing is a graph, then you can call it scale free.

So a scale free thing is a graph that has this scale invariance.

Crucially,

in both time and in space because we are dealing with dynamical systems and we're doing this dimension reduction or coarse graining using these um eigenfunctions that necessarily pick out these slow macroscopic sometimes called stable modes they're stable because they they don't decay or dissipate um almost infinitely quickly so they're stable in the sense that they decay very very slowly

So how can we harness that observation, that truism, at least truism under the world as described by Haken, in the construction of plausible generative models, and in particular, generalized Markov decision or partially observed Markov decision processes?

Well, what we can do is we can just look at the different kinds of depth

and ask at what point would we be able to leverage this renormalizing aspect of the cause-effect architecture of any given world that we're trying to navigate and predict.

So here's the normal depiction of a

a generalized Markov decision process.

And by generalized, what I mean is that there's an explicit representation of not just the states of things that are generating outputs or things that can be measured or observed on the sensory sector of a Markov blanket, but also their dynamics in terms of the paths that

specify the transitions between states, as we know, normally encoded in terms of a transition tensor B here.

So just by putting in the paths as explicit random variables that accompany for each given factor,

the hidden states we have a generalized kind of Markov decision process encoded in the usual way in this instance in terms of tensors that themselves can be parameterized as Dirichlet distributions so that parameterization equips these kinds of models with a parametric depth in the sense that if something is deep parametrically that means that the

parameters of various probability distributions are themselves random variables that have their own probability distributions so for example the likelihood mapping that maps from sensory states to outcomes here usually encoded by an a tensor here mapping from latent states to observations

is itself encoded in terms of Dirichlet parameters lending this kind of model a parametric depth.

And one can ask what the implications of that are.

Well, the implications are another kind of depth in terms of you now have to optimize or apply basic mechanics to the

latent states, the hidden states, and that would be inference.

And then you also have to consider, at a slightly slower time scale, the optimisation of the Dirichlet parameters, and that would correspond to learning and so on, if I wanted to go to the structural learning of the model in and of itself.

we also have um implicitly a temporal depth in the sense that we're going to be rolling out into the future and that there are dynamics here and this temporal depth is generalized in in virtue of having these explicit path variables that we're going to exploit later on we can also have hierarchical depth in the sense that

We can compose many Markov decision processes in the sense that the outputs of one process now constitute the inputs or the empirical priors, the top-down constraints, the inductive biases on the

dynamics and states of the level below.

The dynamics are the kind of inputs, sorry, outputs of the upper level that provide empirical powers on the paths and the initial conditions are supplied or mediated by a mapping between higher levels, slower states, and the initial conditions or initial states of the lower

of the lower level by this D tensor here that you can lump together in terms of a sort of high, you can lump together the D and the E tensors.

They play the role effectively of the A tensors in sort of mapping from one level to the next level.

There's also a factorial depth, which many people will be familiar with in terms of, for any one of these tensors.

lumped states here.

There may be many, many different factors that have a certain independent structure such that you have interactions between factors in determining outcomes.

You can look at this in terms of a generative model

when equipped with factorial depth, effectively entangles latent states to generate outputs or outcomes which themselves can be prior constraints on the states and dynamics of the level below, such that the inversion of these models effectively disentangles the data or the content that is being observed to explain it in terms of

a disentangled representation, which is the latent state and dynamics of the generative model.

So what we have here then is a generalized discrete state space model with paths as random variables.

Just to return to this fundamental aspect that as one ascends scales in real life, in terms of the external states,

things slow down in virtue of the synergetic mapping from fast microscopic states to slow macroscopic states.

The same thing can now be implemented in these

Markov decision processes that have a hierarchical depth.

And the way that is done is effectively to discretize or quantize time, such that there are more updates at any given level than the level above.

So for every update at this level, there are say three updates or two updates at the level below.

and that

affords a temporal scaling in accord with the scale invariance implied by the synergetic view of self-organization.

And I've tried to cartoon that here in terms of, if you imagine lots of these little Markov decision processes populating this circumference here, and that as we go deeper and deeper and deeper into a model with hierarchical depth,

As we consider updates in universal or clock time, there are more updates at the peripheral level exposed, say, to the external states of an entire brain relative to the updates as you get deeper and deeper and deeper into the model.

So that's where the temporal depth becomes, if you like, intimately tied to the hierarchical depth of the hierarchy, the slower the updates, which is why we generally parameterize the initial conditions or generate the initial conditions and the initial path.

from the states above, and that path can pursue a trajectory over multiple time steps until a certain horizon, in which case it then sends messages back to the level above, likelihood messages that then cause belief updating

and the subsequent step at the level above and then generating priors about the subsequent path, the next path at the level below, and then so on recursively to the depth of the model.

So that's the time bit.

What about the spatial bit?

What about the spatial coarse graining, the lumping together of various states?

And of course, this depends upon the particular partition.

So in the general physics formulation, you'd be looking at Markov blankets and Markov blankets.

In practical applications, one can take a shortcut.

And that shortcut effectively rests upon the simplifying assumption that for most systems that are composed of coupled dynamical systems, the interactions are usually local.

So if we just think about some

system that can be described in terms of local interactions so for example in a Euclidean world that comprises some massive objects that bounce around and if we ignore gravity for the moment then they're only going to influence each other or depend upon each other

when they're touching, when they're proximate.

Effectively, what that means is that the particular partition that defines all the Markov blankets in this particular state space is composed of lots of local little tiles.

So what we can do is we can use a device

very prevalent or indeed ubiquitous applications of the renormalization group, which is called a block or a spin operator that just groups together local tiles or quadrants of a, say, two-dimensional arrangement of tile, of states.

So I've illustrated that here just by, in terms of

how we do the spatial coarse graining in these what we will refer to as a renormalizing generative model with spin block transformations in terms of this hidden state or hidden factor with its states and paths here

is responsible for generating predictions about the dynamics of these two factors at the lower level.

And in so doing,

what we are effectively doing is successively grouping together sets of sets of sets of sets so that any set or group at one level is accountable for or trying to generate the dynamics of a small local group about the at the lower level um this is quite fundamental this kind of architecture is quite fundamental because what that means is

anything at the lower level only has one parent and if it only has one parent and if you remember the markov blanket is constituted by the the the parents and the children the parents of the children because there are no co-parents there are no parents of the children and what this means practically

is that we can now ignore dependencies between or effectively resolve or dissolve any dependencies between the factors at any given level, because each factor is only responsible for each children and it doesn't share any children in virtue of this particular spin block transformation.

So this leads to a very efficient kind of generative model.

or certainly an efficient kind of inversion of the generative model where we've got all these converging streams but the streams don't have to talk to each other until at the point that they come together through these blocking or spin block transformations so at some point

all the tiles or the groups at the lowest level will actually come together at the top or very deep in the model at which point we're now encoding quite long trajectories technically two to the power of the depth minus one um in terms of updates um but at the lower level

there are no interactions between the different blocks as we ascend them.

And this is just to remind you, we can treat these D and E, the initial states and the initial paths, effectively as likelihoods that couple one scale or level to the scale below.

So if that's the kind of model that we aspire to, because we think that's appropriate for any world that shows this scale invariant aspect in space and in time, or in state space and in time, how are we going to build one of these things?

And this is the sort of second, if you like,

thing that we have been working on, because it's going to be very difficult to specify by hand these scale-free or renormalising gerontin models.

We really want the model to learn itself.

So basically, how does one build a renormalizing generative model?

And what we're going to do, we're going to use very fast structure learning.

And this particular kind of structure learning is necessarily recursive and in the sense that we're going to build these models through the recursive application of these RG operators or renormalizing operators.

So what's fast structure learning?

So fast structural learning basically equips the model with unique states and transitions as they are encountered.

So to put that simply, what we're going to do is grow a model from scratch, from nothing.

And how are we going to do that?

Well, we're effectively going to take one observation and say, okay, that was generated by some latent state.

And then we're going to take the next observation

and say okay i haven't seen that particular observation before so i'm going to induce a second latent state and furthermore i've seen these two observations in sequence therefore i'm going to encode that with a transition from the first to the second state and then i just repeat i take the the third one and if i haven't seen that before i induce a third latent state that

and a second transition, accumulating all the unseen or unique instances of observations in a likelihood mapping that just grows and grows and grows until we have seen something before.

So we don't induce a new

state or implicitly column of the likelihood matrix if we've seen the previous state.

That's just an instance of something that has been seen before.

It may well now come along with a different path.

So the next state may not be the same as the subsequent state following the previously seen instance of this state.

And therefore, I'm going to induce a new path and slowly grow my B tensors and my probability transition matrices until I can basically encode all the dynamical structure of the

effectively the training sequence of observations summarised in a maximally efficient way.

And I mean maximally efficient in a technical sense, in the sense that the implicit mapping that we are growing from

from scratch has the highest mutual information between the latent states and the observations that are at hand.

The way that you can think of that in terms of free energies, that the expected free energy is just the mutual information in the absence of any expected cost.

So the expected free energy is just the mutual information of any mapping.

plus or minus an expected cost, where cost is defined the usual way in terms of constraints or prior preferences.

So one can actually look at this fast structure learning as a particular kind of active inference, but not about, it's not active inference over states or active learning over parameters.

It's active selection of the right model in terms of the size of the

likelihood tensors and the transition tensors and the criteria for whether we accept a new latent state or a new transition is simply does it optimize or improve the expected free energy namely does it

preserve or conserve or maximise the mutual information.

So we can actually look at this structure learning as just another instance of active inference, but here deployed in terms of basic model selection, or some people call this structural learning, that has exactly the same rules as planning and

and accepting parameter updates provided they maximise expected free energy as well.

The application of this kind of fast structural learning to these renormalising models looks complicated.

Here's the algorithmic description of it.

Again, please don't worry about the details here.

It's relatively simple.

All we're doing basically is ingesting

observations but we're just doing it for local groups and then we are combining the initial conditions and the paths into new outcomes for the next level and so because we are doing this for local groups the number of groups just gets smaller and smaller and smaller as you as you get higher and higher and higher but obviously the number of combinations of

paths and initial states increases as you get deeper and deeper into the model.

And you may be asking, well, how do you band that?

How can you possibly assimilate all the data without having an extremely large number of latent states very deep in the model that are now encoding effectively evolving patterns or paths over quite a large number of time points into the future?

It's trivial to do that because the size of the highest level, I'll call them generalized states that basically generate paths and initial conditions for the lower levels, is upper bounded by the number of elements in your training set.

so that you can control and upper bound the size of these renormalizing operators or likelihood mappings and transition tensors simply by giving it canonical training data, the kind of data that you want this agent to generate or this agent to recognize and thereby predict.

So here's a particular example of that.

What we've done here,

is just ask can we very very quickly learn a generative model that will generate a um a simple movie and this might movie is simple because um it begins and ends in the same state so it just loops over and over and over again um so we only have to supply one loop

um to this fast structure learning so the model builds itself and can then be used in generative mode to generate the movie time and time and time again um so here's one depiction of um

the results of the learning, the first spectral learning and subsequent inference when exposed to this kind of image.

So the coarse graining here at the very bottom, in fact, uses

a slightly finessed version of the spin block operator by harvesting local eigenmodes or eigenimages of little patches of the video to get it into a reduced or coarse-grained representation that then is just passed up using these spin block operators.

And in this instance, we just need two levels to the model to have the highest level see the entire image.

And then if we present the image after discretization using the singular value decomposition or the sequences of images to a model, it learns the model and then we can generate little movies simply by moving through these paths or episodes

I use the word episode because remember these generalized states at the highest level encode paths or trajectories over two to the n minus one time steps in the future when n is the depth of the model.

And these episodes themselves have dynamics.

So we just go through a little loop here, go back to the beginning and thereby generate this image at the bottom.

What else can we do with these kinds of models?

Well, one thing we can do is present not the entire data, but just a little prompt.

So in this example, what we've done is actually show it just the upper right quadrant of the movie.

and see what this generative model believes is going on in terms of the predictive posterior in output space, in image space in this instance.

Now, because during training, it's only learned to recognize a dove flapping her wings,

That's all it can believe is the cause of its sensations, even if those sensations are very partial.

So basically what we have here is a kind of pattern completion that inherits from the fact that this generative model only knows about latent causes that constitute an entire dove flapping her wings.

So even though this is what the dove is actually exposed to, it's sensing,

This is the predicted, after a couple of frames, it very quickly is able to predict what it should be seeing or what it would have seen had it been given the entire data.

And another way of thinking about this remarkable ability to sort of fill in the gaps is from the point of view of compression.

So just a little digression here.

A lot of the Bayesian mechanics and variational inference that ensues from variational mechanics can also be likened to

the same fundaments that underwrite efficient information transfer.

So we've talked about maximizing mutual information in the context of algorithmic complexity and

the associated universal computation then what the objective function looks like is basically how much can i compress this content to represent it in the most efficient way so you can look at this past structure learning under these renormalizing or scale-free generative models as the most efficient way of compressing

whatever your data you've been exposed to.

And because we've compressed it in such a way that it can only now recognise a dove flapping their wings, then it can, if you like, fill in the gaps in a very efficient way when just given partial or noisy data.

And I use the word noisy here because you can regard the missing three quadrants here as extremely imprecise data, data that has extremely low signal-to-noise, for example.

So here's another example.

In the previous example, we took an effectively

a movie or a dynamic or a system that had a periodic orbit, which was each flap of the wings.

Will this work for aperiodic orbits and generally for stochastic chaos?

Yes, it works relatively efficiently.

So this example

we used training data that was generated through creating images of a white ball that moved with a chaotic trajectory governed by a Lorentz system.

So this is the kind of system that governs or is used to describe chaotic events in meteorological systems or in fluid convection.

So by applying exactly the same procedures as in the previous example, we're now generating this as a three level hierarchical or renormalizing model that now accommodates the

stochasticness or randomness induced by not only random fluctuations on the dynamics of this stochastic random dynamical system, but also if there were any deterministic aspects, the deterministic chaos that ensues from exponential divergence of trajectories that now has been summarized stochastically in terms of probabilistic transition matrices here, which means that

We can, having compressed the training data, we can now withhold not a quadrant, but just simply withhold the data to stop the data.

But the model will keep on generating what it predicts would have happened, given the statistics of the dynamics that have been installed.

in this renormalizing generative model.

And what we've done here is for the first few hundred time bins at the lowest level, we've basically rendered the images or the input very, very imprecise so that everything is governed now, everything is generated from these top-down priors or posterior, posterior,

predictive posteriors that inherit from from precise priors because there's no precise likelihood um and then we've actually uh removed the um removed the um imprecise input completely and this allowed the um the agent to generate its own beliefs about what's about what's going on um so here this is

fairly precise in the first instance and then we remove completely the stimulus but it still imagines or generates its or predicts what's going on in terms of this chaotic dynamics and then with a degree of uncertainty then continues to then generate the dynamics associated with this kind of stochastic chaos

Here's a prettier example, using exactly the same technology and approach, but here explicitly talking about video compression.

So we literally just presented a video of naturalistic dynamics, in this instance a little bird, a robin, feeding itself, and

compress that using this renormalising generative model such that we can again simply remove stimuli at certain points in the video and yet in the agent's mind nothing has really happened because all it knows is

that if it was like this previously, then this must have happened and so on through this, in this instance, orbit that doesn't have a closure.

It's just a trajectory through from the beginning to the end of the sequence.

And here's the posterior prediction after just being exposed to a couple of frames right at the beginning in the first six timeframes of the sequence.

of about 180.

This example applies exactly the same approach as previously.

But now we're not talking about ingesting or learning or compressing images.

We're talking now about compressing or ingesting sound files.

And that is, if you like, a simple or limiting case of this two-dimensional compression that we talked about before.

So in this instance now, what we're doing is compressing one-dimensional frequency summaries of a particular wave file or file that describes fluctuations in the intensity of a sound.

that then is summarised in terms of a continuous wavelet transform.

And then the succession of different trajectories in this time frequency space is then ingested or assimilated or learned by the generative model.

Now, unfortunately, you can't hear this, so I'm not going to bother playing it, but this is what the agent was trained on.

It's basically jazz piano.

And these are the ensuing predictions in the absence of any inputs.

It's being used to generate music using the stochastic transitions through various bars of music that it has learned, returning to one or other bars of the music when it reaches the end of the orbit to generate something that sounds at least jazzy, if not,

You can't hear that, so I'm not going to pursue that.

So to summarize what we've done so far, we've gone beyond generative AI in the sense of having a true generative model that entails active inference and agency.

And in the setting of these

well, sorry, what we are going to do now is go beyond just simply generating content, having been trained on the basis of some training data.

And we're going to now consider agency

by putting action into the mix and in the setting of the simplified setting of optimal state action policies where we can effectively ignore the imperative to maximize expected information gain or explore in the sense of responding to epistemic affordances we can

do one of two things.

We can either use some universal function approximators, say deep learning, to optimize the mapping from sensory to active states to maximize the expected cost or the expected utility part of the expected free energy.

Or we can use active inference to realize predictions under a normalizing generative model of rewarded events.

So what I'm doing here is deliberately setting up a dichotomy that tries to distinguish the active inference approach to purposeful behaviour, where the purpose is provided purely by the prior preferences or the constraints usually encoded in the C tensor.

against a reinforcement learning approach.

And this is a rather busy slide, but I think it's probably worth just briefly rehearsing the distinctions between active inference and reinforcement learning.

So the story with respect to action as a gradient flow on free energy, which basically reduces to

picking those actions that maximise the predictive accuracy in the sense that if you look at the expression for the variational free energy, the only thing that you can change is the accuracy in the sense that free energy can be written as accuracy minus, complexity minus accuracy, and the complexity does not depend upon action, whereas the accuracy does.

So that picture basically says I can describe behaviour in terms of a generative model that is optimised through perception in the usual way, cast in terms of, say, variational inference, that basically generates predictions about what should happen next.

and then i can pick my actions to fulfill those predictions in continuous state for formulations this would be start minimizing the prediction error usually in a proprioceptive domain but you can generalize this to any any consequence of action i'm just going to pick that action

that brings about or is most likely to bring about what I predict will happen next and what I predict will happen next is optimised through perception, through variational inference based upon my learned and selected generative model.

So this is very similar to sort of model predictive control and neurobiology could be cast in terms of perceptual control theory

which puts action in the game basically of realising

evidencing predicted states.

So I've tried to summarise that here just in terms of, you know, our Markov blanket, where we're optimising our beliefs about the world.

Noting that planning in this instance, so planning as inference, is all about the paths.

It does not have to be about action.

It's just about how does this world unfold?

And from the point of view of these scale-free generative models, we're really thinking about how do episodes follow each other?

Which episode is going to follow this episode?

And we can say, well, you know, you are in control of that.

Imagine a benevolent world in which everything happens according to your beliefs about how to be an expert in this particular world.

And the agent can plan a series of episodes, generating predictions all the way down to the bottom, which then slavishly, action can fulfill just by fulfilling those predictions.

So one could, in some sense, think of this in terms of control as inference at the bottom level in the spirit of model predictive control and planning as inference at the top level, so belt and braces, in terms of realising active inference in this particular example where, I repeat, we are really just worried about state action policies.

Why are we worried about state action policies?

Well, these are the kind of policies that reward learning schemes are aimed at and have a similar architecture, but to my mind, a slightly more over-engineered architecture.

First of all, you actually need explicit inputs, which are rewards.

There are some proxy for the constraints of, you know,

under which the basic mechanics would normally operate.

But more importantly, you're basically sending, your inference scheme is basically about selecting the right policy.

I've used Q-learning here to maximise the discounted reward.

So similarities and differences, but we're gonna focus on active inference.

um in this in the particular setting of these renormalizing models where i repeat planning and thinking or imagining a future

can proceed at the very highest level.

That has no notion of action.

They're just patterns that unfold.

They may or may not be caused by me.

I don't know.

I don't care.

But the action at the lowest level, the reflexes, as it were, are in a position to make my predicted posteriors come true simply by picking those actions that realise the predicted outcomes.

How are we going to leverage that kind of merely reflexive active inference in the context of building or learning from scratch renormalising or scale-free generative models that would be apt for doing something, implementing a state action policy?

And I'm going to appeal here again to this notion of

active selection as one way of looking at action, which can always be at least mathematically expressed in terms of doing something if it minimises expected free energy.

And in this instance, what we can do is actually apply it not to the selection of the model, but the selection of the training data.

So if we just apply the dicta that you only select something if it minimizes expected free energy,

that includes the expected cost.

What that basically means is I'm only going to select those data features or those training sequences that don't come along with a high cost, namely those that are rewarded.

And that's what we're going to do here.

We're basically going to learn to play a very simplified game of Pong, where this ball bounces around inside a circle.

and we can move the bat in a way to return the ball so that we can avoid penalties when the ball hits the upper boundary and secure a reward whenever there's contact of the ball and the bat and how can we learn

this kind of dynamic, these episodes in exactly the same way we learned the video of the robin or the dove flapping her wings.

Well, what we can do is we can just select instances of random play that conform with our dictat that we cannot include those costly events.

And so what we've done here is just

generate training data, training sequences using Random Play, but omitted anything that

that contains a costly outcome.

And furthermore, just to make things more efficient, we've restarted the random play after a reward is secured and then just use the subsequent play if it actually attained a subsequent reward.

So basically chained together, bootstrapping together or splicing together sequences of rewarded play.

and then use that to, or submitting that for fast structure learning.

And now we have a generative model on expert play effectively, or a generative model that entails a state action policy that can be realised through

reflexive active inference.

So active data selection, and I'm sort of cartooning that here in terms of applying a certain kind of Maxwell's demon that only lets through non-costly episodes.

Basically, yes, admit this sequence for fast structure learning if there is a reward, if not, ignore this sequence.

And this effectively

or furnishes a generative model of events that lead to a reward and nothing else.

So now this agent can only recognize expert play.

And therefore all of its predictions have to be the predictions of an expert player.

And because action is fulfilling the predictions, it will look as if it is an expert player.

And here are the learned trajectories at the highest level of different episodes that we'll see shown in an alternative format in the next slide.

So here's the initial starting episodes, little events here.

The red dots correspond to reward.

So, you know, first hit,

second hit and then it falls into a nice little orbit and this completes this orbit indefinitely by it's the way that it has learned so remember it's only been trained on random data so this particular style of play is unique to this agent but it is sufficient now to play with a hundred percent

performance, simply because it's now going round and round its orbit, very much like the wing flapping her wings.

These are just different ways of portraying the same dynamics and the things that have been learned.

The first eight frames and then subsequent frames

of expert play.

This is the usual format showing the posteriors at different levels, the posteriors over the states and the posteriors or predicted posteriors over the paths, and then rewarded play as a function of the free energy at different levels here, illustrated by the red dots every time there's a hit up until 512 timeframes here.

And here's this rather long orbit through 50 episodes, each four time steps in length that subtends this kind of play.

This is an interesting one and speaks to the fact that we've actually switched on inductive inference at the highest level.

So what does that mean?

Well, what we've done is just basically take each of these highest level states

worked out the sequence that it encodes by computing or projecting down through the model.

And if that sequence includes a hit, we said that's an intended state.

That's the kind of state that I want to work towards through the successive four time step epochs.

If I want to be at some point in the future in one of these intended states, in this instance rewarded states or episodes, what states must I avoid in order to

preserve or conserve the possibility of getting in or getting to one of these intended states and basically that's the technology behind inductive inference and it's sort of summarized here in terms of the latent states of uh you know 50 or so of these latent states in terms of how many time steps would it would it what's the shortest number of time steps until the intended state

and basically we as we move from state to state to state we want to keep in the white areas here avoiding these dead ends if you like from which there is no possibility of reaching an intended state and it's really

to evaluate these areas that you have to avoid because you know the probability transition matrices.

So you just multiply it by itself a given number of times and you can work out, can I get from this state to this state?

And if you can't, then you get this black area here and you know you have to move from here to here to here to here to get to your intended state.

So that's inductive inference in brief.

and we've applied it at the top level.

So we just told the agent, this world is under your control.

Imagine what you want, noting that these particular episodes or events are things that you should pass through or should experience and then deploying inductive inference to basically predict what will happen all the way down at the bottom and then using reflexes to make that happen.

we can apply exactly the same technology or the same procedure to different kinds of games in a slightly more complicated game.

It's meant to emulate Breakout where we've introduced

certain stochasticity in terms of the initial conditions and sticky action, but the overall behavior is the same.

It's just basically choosing paths through episodes into the future, given what it knows that can be achieved in order to get to rewarded states.

So effectively what we've done is create an attracting set

of episodes at the highest level of these scale-free generative models where all the allowable paths and paths that now in this generalized state space or event space lead to reward.

We can go a little bit further and just add in various episodes by merging or assimilating new data, new training data

if and only if the path actually leads to an existing part of this attracting set.

And I've just illustrated that here in this final example.

So this is the path or the orbit or the attracting set that was learned at the highest level by the ping pong playing agent.

But now we can fill in this effectively

allowable dynamics to include all states that lead to a path on the orbit, and thereby all paths will eventually lead to reward or to roam.

So we can augment the model of expert play with paths that lead to an orbit, namely this attracting set, and paths that lead to the orbit recursively and get quite a dense set of allowable transitions that we can be guaranteed

always keep within the attracting set that will contain these preferred states or states that we have constraints where we have constrained those states that are not visited simply because they cannot be recognized.

And that's it.

Thank you very much indeed.


SPEAKER_00:
Amazing, Carl.

Thank you.

Awesome.

Okay, Arun, if you'd like to lead in with a first question, and Lance, too.

Thank you, but go for it, Arun, first.


SPEAKER_02:
Yeah, thanks very much, Carl.

So my first question is based on a sentence in the paper which talks about any model being sparse must be renormalizable.

Could you say a little bit more about why that must be?


SPEAKER_03:
Oh, that

I can't remember.

Give me a clue.


SPEAKER_02:
I'm presuming it has to do with Markov blankets.

Yes, yes.

Yeah, that's as far as I go.


SPEAKER_03:
I see, yes, I remember now.

It may have been in reference to some work done by Dalton showing that with probability one, provided the system is large enough, there will be Markov blankets.

And as soon as there are Markov blankets, there is now an opportunity to take Markov blankets of Markov blankets in the spirit of that applying the renormalization group that I started off with.

So as soon as you've got, as soon as any sparsely coupled random dynamical system can now support a particular partition into Markov blankets, you've now got the opportunity to take Markov blankets and Markov blankets and then apply that renormalizing procedure that I said before.

So all you need to posit,

is any system that has a Markov blanket at any given level.

And as soon as you've got that, you've got Markov blankets all the way up.

Not necessarily all the way down, but you've certainly got them all the way up.

the physics approach would also actually require for there to be Markov blankets all the way down as well, but that's another issue.

So what that basically means is that all you require is a sufficient degree of sparsity

that would lead to the conditional independences that are definitive of Markov blankets.

And in one sense, much of the maths behind the free energy principle, as it inherits from random dynamical systems or in physics, say, Langevin equations, is simply showing

that a sparse coupling, a sparse causal coupling in a physics or control theoretic sense, not a philosophical or Granger or cause-effect structure sense, but in the physics sense of a coupling.

So the change, this state,

influences the rate of change of that state.

So that kind of, which could be written as an influence graph.

So if there's a sparse coupling on the influence graph, the equivalent Bayesian graph or probabilistic graphical model induced by that sparse coupling now can be interpreted in the sense of Pearl's Markov blankets.

So all you need

to get to the application of the renormalisation group to Markov blankets is sparse causal coupling, hence sparse random dynamical system that is sparsely coupled.

Just out of interest, the spin block, the motivation for the spin block, applying the spin block or blocking transformations

That basically comes from the assumption that many systems that people deal with, both in computer vision and in idealised models in physics, has an exquisitely sparse distribution.

And that's because the causal inferences are all local.

So as soon as you look at a coupled map lattice or a globally coupled map,

you'll notice let's take the lattice for example let's take a sort of idealized um crystal so what you're saying when you write down a lattice is i'm only connected to my neighbors to my more neighborhood or to my markov blanket that means that in a large lattice where i could be connected to n squared neighbors

i'm only connected to one two three four five six seven eight so a fantastically small almost empty set of possible connections and i use that phrase because there's i can't remember who said it now but i really liked the notion that if one looks at the connectome in the brain in terms of the connections it's almost empty

the number of actual connections, white matter connections, external connections, and indeed intrinsic external connections

are almost zero in comparison to the total number of possible connections between the 10 to the 11 or whatever neurons in the brain.

So that's not surprising.

It's one of those nice examples that the anatomy, the functional architecture of the brain recapitulates the anatomy of the lived world.

in that it has this incredible sparsity.

And that just means that universes in which there is, the action at a distance is quite rare, means that,

you know you're just interacting you're just coupled to your neighbors and that's and that gives you if you then look at the markov blanket structure or the particular partition for these lattice structures you get to the spin block um blocking transformations as a way of carving up nature you know in terms of little patches on markov blankets


SPEAKER_02:
I do have a follow-up question on that, but then I'll hand it back to Daniel or Lance.

So the lattice example is really interesting because I think there's a symmetry there that maybe we lose when we talk about blankets that only have one parent.

So in a normal lattice, as you say, you're just connected to your eight neighbors in 3D space.

When we looked at the spin block transformations before,

we were going, let's look at this square and everything in this square has one parent, but then the square along is connected to a separate parent.

So now if you're at the boundary, we are breaking some symmetry there.


SPEAKER_03:
Yes, technically or practically a very good point.

It doesn't really have any material impact upon the algorithms or the partitioning.

So just practically what happens is you just take your tiles and then you have a half tile or a partial tile.

until you get to the boundary.

So these groups in the grouping operation or the blocking operation are not necessarily the same.

They're not the same size and indeed,

current work we're actually dealing with sort of multiple streams so you can have sort of audio files and video files separately tiled and then converging right at the top.

Ideally what you'd want to do is

not use this blocking transformation but to try and identify the Markov blanket structure at the particular scale in question and actually use the Markov blankets based upon well there are a number of ways of doing that you can either use a Markov blanket discovery algorithm of the kind that Jeff Beck works upon or you can use empirical analyses of the

the dynamical coupling by estimating, affecting the Jacobians.

You know, for those people who are interested, there's a worked example of that in the

particles and parcels in the brain paper in network neuroscience.

It's really framed in terms of data analysis, but you can use the same principles to identify a particular partition from a large number of states or voxels or pixels in 3D and 2D respectively.

There's another thing though that I think your question speaks to,

which is, you know, you've got one block of four and then another block of four, and then because of the sparsity, they're separately modelled at the level above.

So I thought you were going to ask how on earth now do you start to repair

the obviously rather facile assumption that you've got just local action.

Because of course, in our world, there's lots of action to distance.

We are talking to each other over, over, over thousands of miles or kilometers.

And we have vision.

And we have linguistic communication via sound.

So it is interesting that we have actually violated

much of the local action if i was a virus this would be perfectly okay because the only thing that's really going to affect me as a virus is the things that actually are in molecular contact with me but for you and me and the worlds in which we live in um we can't there is no such thing as just uh you know the action distance that now starts to really matter

And effectively, because the two latent states, generalized latent states, generating stuff, initial conditions and paths for the two blocks below, because they themselves now contribute to a single latent state at the level above, as you move deeper and deeper, you start to now repair

the simplifying assumption of conditional independence between the groups so it is at the higher levels that you now become you're able to see these actions this action at a distance is coupling your well what happens over here really matters in terms of what's going on over here but you'll only ever see that you only have that as part of the generative model right at the top which is why all the inductive inference

is implemented right at the top level where you see everything so it becomes you know if you like you are now you are now in a position to model and thereby generate appropriate predictions that rest upon these long-range dependencies

And of course, you know, using the word long range, I mean, in the sense of the coupling distance, you know, in a dynamical sense.

Is that what you had in mind?


SPEAKER_02:
That was exactly what I had in mind.

Thank you.


SPEAKER_00:
Thank you, Arun.

Lance, do you have any remarks or want to add a question?

Yes, please go for it.


SPEAKER_01:
Yeah, I have lots of questions.

Thanks a lot, Kyle, for the talk.

So I'm just wondering about this all roads lead to Rome slide that you had at the end.

And I'm wondering whether you have some sort of a curse of dimensionality as you increase the dimensionality of the system.

So it sounds to me like the manifold of expert play, because you're taking temporal trajectories, that's going to be a one-dimensional manifold.

Now, as the state space of the game increases in dimensionality, you're going to have an ambient space of trajectories that's much, much higher dimensional.

And so it sounds like the amount of trajectories that you need to, you know, construe to bring me back to your one-dimensional expert manifold is just going to grow and grow and grow.

And it sounds like it might be hard to fill that space

So could you comment a little bit on that and whether it's a problem?


SPEAKER_03:
Right.

Yeah.

In general, in my world, these things aren't problems, but they're challenges.

And I think that challenge is yet to be met.

So what I imagine people will do is that you will ingest

extra episodes or sequences that bring you onto your attracting set up until some bound

and then you will switch off active selection by which i mean the structure learning so you stop growing the model and then you switch on parameter learning so that now you're forcing the transitions and the expression of various generalized states

to accommodate variations around the training data that you have been previously exposed to.

So that fixes the size of the tensors and hence the dimensionality of the state spaces in question.

but will in principle now accommodate by preserving by using sort of active learning in the technical sense that you only update a parameter if it does not increase expected free energy or decrease mutual information.

then you're now in a position to become more robust.

And perhaps I should qualify this answer.

The whole point of this adding in extra, if you like, insets onto an attracting set is just to equip the agent with a certain robustness should it find itself outside or off that attracting set.

So I know Lance knows this, but I'll just make this explicit for everybody else.

This kind of learning and state action policy implementation is very much like learning to ride a bike.

So you basically just accumulate and remember those experiences that were successful.

So, for example, I managed to rotate the pedals by one cycle and I don't fall off.

And I remember that.

And then I have another go and I wait until I've actually done two cycles and I don't fall off.

And I remember that.

I forget.

everything every time i i fall off and then i keep on going until at some point i connect back my last cycle was identical to the first cycle and then i've learned to ride the bike but because i've forgotten all the falling off i can't recognize or know where i am when i when i've fallen off so i've got a very brittle expertise that will not allow me to recover if i fall off the edge

So another example which makes it slightly clear, I think, is like mountain climbing.

If I want to be an expert mountain climber, I cannot learn by my mistakes, provided I'm climbing a sufficiently high mountain.

Should I fall off it, then I will die.

Another example here is learning how to cross the road as a child.

you know you cannot learn by um experiencing being run over but the cost you um you pay or the price that you pay is you can't recognize when you are being run over

And that, I repeat, lends a certain brittleness and a lack of robustness to agents who may be exposed to danger, but doesn't actually terminate them.

So putting this robustness back in by growing

by growing the attracting set or basically putting these insets on in this sort of generalised phase space.

It's just to make it more robust.

In numerical studies, it does seem to converge.

It doesn't seem to grow indefinitely.

So those are likely violations of moving off the attracting set are themselves quite small, relatively small in number.

you know practically speaking uh if if you've controlled the sources of stochasticity or unpredictability so it may not be always the case that you need to have an explicit bound on the upper the size of the um the very high level tensors if if you do

then one has to now think of a principled way of making that upper bound an emergent property of expected free energy minimisation.

It may be that you violate now the mutual information that certain rare events are just so rare

you're actually compromising the mutual information by including them as a particular option or a particular column of a likelihood mapping.

But I don't know, but these are all really interesting questions to pursue.


SPEAKER_00:
Thank you, Carl.

All right, Arun, go for it.


SPEAKER_02:
Yeah, so following on from that, I think it's an interesting question on the uniqueness part of ingesting data.

And the question is, are there many more ways to fail than there are to succeed?

So if you're passing your data into sort of Maxwell's demon, with a gate saying, accept this data, reject this data, if this improves my mutual information, if there are many more ways to fail, but then get back onto the

a successful path, but there are only a few paths that actually keep you there, would it not make sense to just continually accumulate these sort of recovery states because the mutual information will keep increasing and then you stop there, then you learn parameter-wise because we're not including the sort of prior distribution of how likely you are to be succeeding at any given moment.

Because with the uniqueness thing, you don't know, oh, 10 times out of 10, I'm cycling.

Or like nine times out of 10, I'm cycling, and one times out of 10, I've fallen off the bike.


SPEAKER_03:
Yeah, I think it depends.

Again, I don't have any explicit numerical analyses or analytical analyses of this, but intuitively, I think during learning, there may well be...

um good mileage in ingesting failures um or routes to success if you like or to the attracting set that actually traverse regimes that would normally be excluded by my constraints or my prior preferences um

know just taking the view of the role of of the c tensors or prior preferences from the point of view of physics what that basically is saying is it's actually carving out vast regimes of state space latent state space that you should not be in because the it would be uncharacteristic of you to be in these things so it's really not so much about the rewards and the preferences it's really what you where you shouldn't be and that's what defines the attracting set um

However, before you've learned that attracting set, it may be useful to include constrained regions of state space that offer a route into or onto that attracting set.

So I can certainly see that that would be a useful device.

But notice that once you're on the attracting set, you will never visit the constrained regions.

because you now keep yourself on the attracting set.

And therefore, if you've encoded those, but you never visit them, your mutual information will actually fall.

So what that means operationally is what you probably do is have this sort of slightly over-inclusive structural learning.

learn how to cope with all the necessary sources of randomness so that your attracting set is sufficiently robust.

And you do actually visit from time to time, but not never.

Excursions from the attracting set are drawn back onto the attracting set.

And then use Bayesian model reduction to eliminate all the states that you don't want to recognize simply because you never go there.

Once when I was younger, I would certainly be able to recognise what it's like to fall off my bike.

But now as an expert bike rider, I really don't want or need that.

And that's just basically getting old and wise and implementing Bayesian model reduction to remove the redundant parameters, which in this instance just be the columns and associated slices or rows of the transition matrices.

So I think you could perhaps another way then of answering Lance's question is, yeah, you just keep accumulating, but start to engage Bayesian model reduction.

At some point you should reach an equilibrium, which will be the attracting set that allows you to survive in the characteristic states that define you in this particular environment with this particular volatility.


SPEAKER_00:
Thank you.

Awesome.

Okay, I'm going to bring in a question from the live chat.

Okay, Javier wrote, so stochastic plus dynamical probabilities learned by the agent itself are scale-free.

Will this hierarchical learning also be applied to the goals of the agent, different abstractions, granularities of goals self-learned?


SPEAKER_03:
My answer would be yes, but I'm going to ask Lance to answer that one.


SPEAKER_01:
Oh, that's cheeky.

Yeah, I would say for sure.

Because typically when we operationalize the goals in our genitive model, they need to have the same structure in our genitive model itself.


SPEAKER_03:
would imagine then we would have a hierarchical renormalizable goals it's going to make sense cool okay i'll bring in oh yeah go for it carl but i'm just going to say um you know just to unpack what lads just said in terms of practical things you know if you're trying to um you're trying to say

What my goal is to go, is to have dinner.

That naturally unpacks hierarchically into, I've got to move from the living room to the kitchen, then I have to find the, open the fridge, sorry, I have to find the food, which then unpacks into, I have now opened the, and all the way down to the finest muscle movements.

So almost by definition, you know, intended narratives.

can always be decomposed in this scale free way and you know there's a this is very similar to sort of motor chunking and hierarchical decomposition in theoretical motor control for example and you probably find it a lot in in robotics that there's a sort of global

long-term plan that provides empirical prize of constraints on shorter little chunks that themselves provide constraints on shorter and shorter chunks.

And that's exactly the, if you like, the architecture implicit in these scale-free models.


SPEAKER_00:
Awesome, and totally ties back to the synergetics.

Okay.

I'll ask two questions from ML Don.

He wrote, how can we inject the same separation of timescales by imposing smoothness constraints on, say, the parameters as opposed to playing with proportional update rates at different levels?

And this relates to something Arun and I talked a lot about in the .zero and leading up to it is,

identifying which windows to use.

You could imagine picking the right separation of timescales.

If the classes on campus really do shift on the hour, then the hour core screening is going to be great and effective.

But the 59 or the 61 minute might alias you with the real variability of the system

And so there's not necessarily even a smooth landscape for determining the windowing or the binning of coarse graining.

And coarse grainings may have dependencies on each other.

So how do you go about playing and finding viable approaches and robust ones amidst all these different levels of meta optimization that are opening up?


SPEAKER_03:
I'll take this one and the lads could take the next one.

That's a really good question because there is an implicit separation in timescales between vanilla implementations of inference and learning.

However, in these schemes, there isn't.

So the MATLAB version of our implementation

of active inference under these and, in fact, all Markov decision process models, unlike

procedures that you might find in data analysis where you acquire, you basically do some filtering on an epoch of data.

And then at the end, you update your parameters and then you rinse, wash and repeat.

In active inference simulations and also in some applications of what's called generalized filtering, you have continual learning.

So the update of the parameters actually occurs at every time step.

um at each level of the model so the likelihood parameters at the lowest level are updated every time that routine is called um so there is continual learning the learning um is yes you're absolutely right it's smooth in the sense that you're slowly accumulating traditionally parameters but the temple scale of that learning is the same as the inference

And you could also argue, well, could one also put Bayesian model reduction into play at the same timescale?

So basically, can you at every time point, at each level in the model, identify redundant parameters and remove them using Bayesian model reduction?

In effect, that's what's done using active learning.

What I said before, active learning will only proceed

if the expected free energy of mutual information improves.

That's actually also implemented.

So the rate at which it looks as though things change between inference learning and the model selection

is certainly distinct i don't think that's quite the same kind of separation of time scales that is implied in the renormalization group that's as you were intimating uh daniel exactly specified by the number of ticks or updates at level n per single update at the uh level n plus one and and that is i think you know the you know the key question you're asking how would you get that right

And of course, the answer to that is the model that has the greatest model evidence or smallest free energy or greatest marginal likelihood for the kind of data generated by this environment in this world.

So are there any sort of useful prize you can put over this?

Well, one useful prior is that things are changing as, well, one prior which is actually used in operation is you take the lower limit on what's called the temporal RG flow, which is basically the length of the window at each and every level.

At the moment, that's two.

What does that mean?

How can you motivate that?

Well, it's the most expressive in the following sense.

But if I just imagine what this discrete discretization of time means from the point of view of classical notions in continuous state space like position and momentum or position and velocity, if I use a scaling of two, which basically means

there are two time steps or every time step of the level above.

What I am effectively doing is modeling two time steps for every state at the level above.

And that basically means I can model the initial condition and the subsequent condition, basically in terms of the change, which of course is the velocity.

And what does that mean for the level above that?

Well, it now means that I'm modeling the change in the velocity.

So I'm now modeling the acceleration.

and then so on up until the highest order of motion.

So what that means is by taking this limiting case of

T, the temporal horizon, or the ratio of time steps from one level to the other level, as two means that I've got a very expressive model that can handle, can recognise things that change very, very quickly and then acceleration and then jerk and the implicit context sensitivity that that gives you.

If I didn't do that, I'd have to assume

that I coarse-grain over long periods of time, and therefore the path is consistent during that time.

Now, that sometimes is a useful approximation.

And indeed, there's a whole industry called switching linear dynamical systems that makes that approximation.

And it means you can approximate all the non-linear complicated high-order motions

in some evolving pattern or system with a series of um linear trajectories that basically go you know that intercede between a series of points so which you which you which you switch the problem with that is that the length of the linear trajectories

for us for example i'm committed to one path u so one slice of my b matrix for the next eight time steps the problem with this is that you don't know when you when you deviate from that transition dynamics so then you get into the game or perhaps i could make my scheme reactive so sometimes i do three steps sometimes i do eight steps sometimes i do one step and i have some criteria for saying your predictions about my trajectory have now been met

Please, I'm going to tell you, give you the evidence that I'd be pursuing this path.

You update your privacy to the level above and tell me what you expect me to do next.

So this now speaks to a particular kind of reactive message passing that starts to have implications for the computer science and the CRD, the CRUD, or as I think we've rehearsed before,

at the Institute, the original conception behind the actor modeling computer science from the 1990s.

So it's all about reactive message passing.

I will send you a message when you send me, I will return a message to you if you send me a message.

And what these renormalising in time models have, and I should say that this also applies in the absence of the spatial renormalisation, so any deep temporal model will have this aspect.

What that means is that from the point of view of any given level, I will be responding to messages more quickly from my subordinates than from my superiors.

So once in a while, I'll get a request from a superior.

My response to that is to ask my juniors to do a series of things.

They report back to me, and when they report, I give them the next bit of request or guidance until they complete their job.

And when all my juniors have completed their job, of course, in the re-normalising, spatially re-normalising context, I'll have a whole group of juniors to worry about, then I have gathered enough evidence for me to then return the message to my superior.

So you get the, in terms of the reactive message passing in this sort of actor model of computer message passing, you get that the separation of temporal scales inherits from the number of messages or requests and responses exchanged with the people lower down in the hierarchy.

relative to the number immediately subordinate and in principle they don't have to be fixed so there could be a criteria before I pass the message back up to the higher level I think that's Daniel probably what you were getting at and I'm not there may be somebody who's done that but I'm

I haven't done that, and I don't know, I can't give you an example of somebody who has done that.

You may find somebody in Bert DeVries's group, some of the world experts in reactive message passing who have considered these kinds of things.

In principle, there should be a way of

each level testing whether I have fulfilled the predictions of my superiors and now it's time to get more guidance based upon the free energy.


SPEAKER_00:
Awesome.

organizational technology i didn't know that my phd needed okay i'll ask a question as we get close to the end and and this will be for for anyone ml don wrote

How does the training time of active inference compare to that of reinforcement learning and how does their scalability compare?

And it would be interesting just to hear anyone's very 2024 takes.

Where are we at with that entire arc from problem conceptualization

on through the training and the inference how do we even grapple and describe with similarities and differences in the training time and resources and all of this that's definitely lance for at least five minutes i think


SPEAKER_01:
I'm a bit incapacitated in terms of talking.

I think there's actually a good question for Arun.

But yeah, I mean, I can say a few things.

So scalability in active inference, I would say it has been a challenge and there has been much less man work hours that have been put into that than in reinforcement learning.

Now, for the past few years, we've had a lot of work, a lot of it led by Carl, actually, trying to scale the structural learning and active inference.

And I think we've done a lot of mileage on that.

So there's been the supervised structural learning paper.

There's this paper.

There was also some interesting work presented at the International Workshop on Active Inference.

I personally think there are still some challenges ahead, and I think one of them is the one that we just talked a few questions ago about, you know, if you learn an expert orbit, how do you get back on the orbit as the dimensionality of the system increases?

So this is definitely one challenge.

Now I know much less about reinforcement learning, so maybe Arun has more things to say there.

But what I will say is a lot of reinforcement learning is powered by deep neural networks.

When we use deep neural networks, we sacrifice the interpretability of our generative model.

This is one of the strengths of active inference is having more interpretable

dynamic global models where we can actually peer into them and see, okay, well, this representation imposed this, this other one imposed that, and so forth.

The fact, though, that reinforcement learning typically uses deep neural networks means that it has been much more scalable just because there has been so much work done in training deep neural networks.

This is a bit of my perspective.

I don't

I think it fully answers the question, though.


SPEAKER_02:
I can come in with a little bit on that, but I must admit that I have not trained any reinforcement learning agents myself.

I have worked with people who have.

I'd agree, Lance, that the infrastructure generally is much more advanced for training RL agents for doing stuff and completely agree with, as you say, the explainability and the interpretability of

why did an RL agent pick a particular action is very opaque.

And I think the deeper those networks become, the harder it is to understand what's going on in the hidden layers and generally why they do what they do.

I think with active inference, yes, we do have more explainability.

But I do wonder if we go the fast structure learning approach.

I think Carl had a particular comment earlier about looking at the highest level, identifying the desirable states learned by the agent.

From what I understand, the identification has to be done by a human.

They have to look at the model that's been learned by this renormalization approach, look at those top levels, and then say, ah, what do these levels mean?

So an example in the paper is the MNIST classification.

It's the case of a human being looking at those top levels and going, ah, these top levels each correspond to a digit in the handwriting task.

we are sacrificing some explainability, I think, by doing this fast structural learning compared to what might be the old fashioned way of creating general models by hand, by thinking, sitting in my armchair and thinking, this is how I might model the world.

these structures, these likelihoods, these states.

And when you do that, I think just by hand, you have 100% explainability.

That model just might not perform very well.

So I think there's always going to be that tension there.

But in terms of how quickly these things learn,

Having not trained either, I'd be having a reckon.

And at the moment, I would say probably an RG active inference agent would train faster, simply because it's able to use the structure and the data much more effectively.

That would be my take, and I think as well, really interesting thinking about the active data selection part of this too.

I know that there is some stuff in the paper about the efficiency being both a curse and a burden, but I'll hand over to Lance and Carl to give more details on that.


SPEAKER_01:
Yeah, Lance?

Yeah, just to echo what Arun said, also, so the structured learning and active inference in reinforcement learning is also very different.

In reinforcement learning, we typically have a model with deep neural networks that's massively overparameterized.

They have some fixed representational capacity from the outset because the structure of that model will be specified.

And then typically the model will learn, and then probably you would prune it using some techniques like dropout or other things.

If we looked at that from the perspective of active inference, it would be like specifying a massively overparameterized model.

and then pruning it with Bayesian model reduction.

But now in active inference, we have this Bayesian model expansion, for example, in this paper.

And what this Bayesian model expansion does is it learns from the get-go the minimal model for explaining the data.

And so in that sense, it's extremely efficient and also provably because it extremizes


SPEAKER_03:
the marginal likelihood or the mutual information yeah i just wanted to echo that last point that's a really important point um so lance is drawing the distinction and there are many distinctions you can draw with um deep rl um and um deep active inference read in a more in its um

generic sense, and that distinction at the level of structure learning or model selection, I think is absolutely crucial.

So, you know, deep RL starts with an overly expressive model with a known functional form.

that is deliberately over parameterised and then shrinks it or reduces it to maximise the mutual information, the expected free energy, sometimes referred to as a cross entropy in machine learning.

So the reward function now becomes there's no explicit reward in unsupervised learning.

It's just replaced by the mutual information or the cross entropy.

which should be distinguished between supervised reinforcement learning, such as digit classification, for example.

Whereas the active inference under fast structure learning is exactly as Lance says, it starts from nothing and stops when it's minimally

complex which means it's maximally efficient and that efficiency translates in many guises through to sample efficiency through to statistical efficiency through to thermodynamic efficiency and this matters because from the point of view of climate change this is the right way to do it as opposed to building power stations to power large language models and transformers

that need big data so the efficiency on all levels whether it's the the sample efficiency or using the right data or whether it's right through to how much electricity do you need to actually train your model and how many millions of dollars do you need to train a foundation model all of these are reflections of the failure of machine learning

to write in the efficiency into the ultimate objective function which as lance said is just the marginal likelihood of the evidence that this model of this kind of content or world


SPEAKER_00:
awesome okay final question and a very apt one as we head into the rest of the symposium so just give any short thoughts tin tin wrote hi all thanks for the incredible event does the sequence does the sequence in which the training data is fed to the ai structure learning model impact what the resulting learned structure would be


SPEAKER_03:
so the learning curriculum aka symposium program challenge well the answer is yes i'm mindful that lance is recovering from his um jaw surgery um yeah absolutely um and this is um again just in relation to to to the last question

something I think that it you know eludes conventional RL certainly that it rests upon classification procedures because the active inference as an application of the free energy principle is all about the dynamics of self-organization and dynamics means time

And time means sequence matters.

So inevitably, at some level, the order in which you see things is absolutely crucial for getting the right kind of generative model.

that is even in the case actually of static image recognition because you're assuming that there's no ordinal structure or correlation structure to the presentation say of endless digits you're assuming that they are selected at random as opposed to seeing all the tens and then all the all the ones and then all the all the sevens so even you know in these edge cases

which are the focus of 20th century RL, static image classification, for example.

Order does matter, but it matters absolutely acutely because, of course, the order in which you see content in

this in fast structure learning determines how you populate the transition tensors and it's the transition tensors that enable separation of temporal scales that enables the ability to encode narratives and plans that far transcend the actual rate at which you sample the environment

You could also argue that this issue has emerged in a strange kind of guise in transformer models, so the attention heads operate on the past.

So the way you select from the past in terms of which token is going to predict the next token most efficiently again is exquisitely dependent upon and sensitive to the order in which you train these models.

So, Daniel, have you thought carefully about your ordinal approach today?


SPEAKER_00:
What's coming next?

I let the structure of the niche and others' constraints provide the first pass and make requests from there.

So, thank you, Carl.

Thank you, Lance, for joining again.

Thank you, Arun, for all the contributions with .Zero.

Carl, great to see you.

Next up, we'll take a 30-second break and we'll be back with Michael Lennon.

So thanks, fellows.

See you soon.
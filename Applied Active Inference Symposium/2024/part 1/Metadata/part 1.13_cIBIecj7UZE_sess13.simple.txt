SPEAKER_00:
Alright, the next talk is by Bert Burgers towards autonomous urban digital twins.

This is a 20 minute video.

For the next 20 minutes, I will talk about transport policy planning and how active inference could be applied through digital twins.

Urban planning is a very broad field encompassing everything that occurs in urban regions, from the design and operation of infrastructure to social programs.

To scope this, I will focus on the transport system, giving a rough sketch of what active inference may entail for this domain.

Modeling the transport system relies on operationalizing decision making.

Many individual choices lead to busy roads.

Therefore, the main storyline focuses on the difference between active and passive decision-making.

The presentation is structured in three parts.

First is the context, giving an outline of urban digital twins.

I will focus on livability as it is an objective of transport policy.

Next, I will present some domain-specific building blocks, which are then combined in the final part.

Finally, I hypothesize that renormalizing generative models can be used as transport models.

concluding that active inference-based modeling might require a new perspective on transport planning.

First, I will give some context in order to frame how active inference, transport planning and digital trends come together.

The field of transport planning is moving towards the development of digital twins.

These twins enable quick calculation of effects given an intervention.

Rather than performing a comprehensive study per project, which can take months, a digital twin can be reused many times.

An example application would be the construction of a new neighborhood.

What are the effects on congestion, air quality and inundation risk?

There are roughly five levels.

The first introduces a compelling visual interface, often involving a 3D representation of the real urban region.

Next is the addition of information on top of the visualization.

and the geographic data works.

The third step not only maps data, but incorporates different physics-based models.

Hence, it is called predictive.

Transport models are one example, others are airflow or water simulations.

High-performance computing speeds up the scope of parameters which can be tested.

The fourth level builds on top of the predictive models by adding an interpretive layer.

The added value is scenario-based planning, where a range of outcomes is calculated for a hypothetical intervention.

Instead of configuring parameters exactly, you could propose a preferred outcome to the computer, and that would only simulate what is necessary, as it understands relations between models.

The last level introduces autonomy.

Not only are physics-based models configured intelligently, as in step 4, but the twin actively goes out to construct its niche.

transport policy evaluation is going through somewhat of a change.

In decades before, the development of transport models allowed the paradigm of predict and provide.

We could forecast traffic flows and accordingly invest in infrastructure to prevent congestion.

More recently, the field has acknowledged that models trained on past data are inherently flawed in predicting in certain futures.

As such, it is moving towards scenario-based planning, sometimes described as decide and provide.

Along with this change comes a reflection on the philosophical context.

For simplicity, I rely on two frameworks.

The first is utilitarianism.

It relies on the assumption of rational choice, enabling the quantification of benefits and costs in terms of utility and disutility.

Between every origin and destination,

There are several alternative routes.

Each has attributes such as the distance traveled and the cost.

Combined with preferences, we get generalized travel cost.

The rational choice assumption enables fungibility.

This allows us to take the ratio of estimated better parameters and utility functions.

These ratios can then be used to calculate, for example, the monetary value of travel time.

In practice, we always use monetary value as the final measure so that we can compare the cost of implementation against the benefits experienced.

Finally, utilitarianism leads to aggregate increases in welfare at the ignorance of the distribution.

Not only this is applied to wealth, but accessibility too.

Take for example the discrepancy in infrastructure spending between core cities and the periphery.

To address these concerns, contemporary policy evaluation attempts to move away from utilitarianism.

Instead of attempting to relate our preferences to monetary value, we leave them separate.

Not only do we reject the fungibility of indicators, but we also consider many more than before.

Shown in this wheel, spanning from living environment, accessibility, health and safety.

Finally, the distribution of costs and benefits is explicitly considered, and much work is now spent on reworking existing modelling frameworks to make these distributions explicit.

There are three main objectives in planning infrastructure.

Accessibility, safety and livability.

Capturing livability is much more difficult than accessibility and safety.

Livability is generally defined as the fit in an ecological relationship between resident and the living environment.

With regard to the spatio-temporal scale of this relationship, livability is local and sustainability is global.

For practical reasons, fit is operationalized through valuation.

The more an environment can satisfy one's preferences, the greater the fit.

Theoretically, it has been long known that fit could also be operationalized as a continuous transactional process.

However, little progress has been made.

And this is where active inference comes in.

These two approaches to livability are best defined by contrasting them to each other.

On the one hand, we have the static approach.

which is concerned with measurable outcomes.

On the other hand, there is the dynamic approach, in which livability is viewed as the hidden driving force behind that which is measurable.

By framing livability as comprised of three components, indicators, percepts and needs-desires, we can show that the location of fit determines which is which.

Either it is a weighted function between percepts and our preferences,

co-evaluation, or livability is a weighted function between indicators and percepts, which are biased towards our preferences.

Finally, these two approaches might be complementary, and this might be useful for modeling purposes.

There are many different transport models.

When it comes to those used for urban planning, these can be distinguished into micro and micro simulations.

The micro scale simulates individual vehicles and their interactions.

Meanwhile, the micro scale applies constraint optimization over a network.

Preferences constrain this optimization.

Here I will focus on micro models, which combine transport and land use patterns.

Fundamentally, we rely on seeing the transport system as a market.

travelers generate demand, while the network provides supply.

The four-step model forms the backbone of Microscopic Transport Modeling, and I will go over each step.

First is trip generation.

This step determines how many trips start and end in each zone.

It is about determining the row and column sums.

See also the top right figure.

For trip production, we look at residential zones and use socio-demographic factors like household size, car ownership and income levels.

For trip attraction, we look at the number of jobs, shops or school capacity.

Trip distribution iteratively balances row and column sums to fill the origin-destination matrix.

Balancing is done using a gravity model, where the number of trips increases

with their mass, but decreases with the travel resistance.

With the full origin-destination matrix, we can split these flows per travel mode used.

The utility of each mode can be weighed against our order in the choice set.

Logit models can be estimated for different segments of the population.

That is the figure in the equation in the bottom right.

Finally, we assign trips to links in the network.

According to Wardrop's first principle, travelers select routes to minimize the travel time, leading to an equilibrium where no user can reduce the travel time by changing.

The entire workflow is iterative because assignment affects travel resistances due to congestion.

Not shown here is the modeling of land use, which affects zonal data.

Next, I will outline three conceptual building blocks, which will be combined in the synthesis.

As we just saw, modeling transport networks requires a representation, for example, the travel time per link.

Additionally, we would like to approximate human perception, for example, by using images or sounds.

The field of urban representation learning attempts to automatically capture the content and structure of cities.

Luckily, there is a strong inductive prior from Joker3.

Similar things are closer together.

As such, major design choices revolve around which measures of distance to include.

Examples are Euclidean distance, travel volumes or accessibility.

Generally, an application of urban representation learning relies on three choices.

First is the need to discretize the urban region into spatial units.

Each acts as a class.

The figures on the right use hexagons.

Secondly is the type of data used.

Where traditional transport models only use tabular data, we can now also use street view images and text.

Last but not least is the way in which parameters are structured and learned.

Graph neural networks are widely used as they naturally represent the transport network.

Additionally, the loss function can be used to steer training.

Accessibility is a major field of study in transport planning that connects well with active inference concepts.

In urban representation learning, we aim to capture spatial similarity in embeddings.

Accessibility provides a theoretical foundation for which similarities to capture.

Intuitively, accessibility is akin to landscapes of affordances, which are shared by everybody living in an urban region.

Location-based accessibility is derived from constrained maximum entropy and can be configured to equal utility-based accessibility.

The calculation is straightforward.

For each zone, we sum the product of destination attractiveness and the deterrence function based on generalized travel cost.

The deterrence function captures how travel cost relates to similarity.

As shown in the maps, accessibility patterns differ significantly between bike and car networks.

Hierarchy is a well-established element of transport networks.

Think of how local streets feed into arterials and highways, each operating at different speeds and distances.

Where local streets facilitate access, main roads enable throughput.

To capture this hierarchy in urban representations, we start by defining the study area, creating columns of coarse hexagons and then mapping them to define our resolutions.

I have used a U-Net to capture both bottom-up and top-down information.

Each block in this net is a graph convolutional neural network.

A pre-computed accessibility graph sparsely connects centroids of hexagons, where the edges are the accessibility value.

Reconstruction loss ensures we capture input data

while consistency loss ensures columns of hexagons are integrated.

Combining context and building blocks, I will now go over the synthesis.

We started by discussing digital twins and two approaches to livability, static and dynamic.

The dynamic approach frames livability as niche construction.

When it comes to policy, the problem boils down to the objective function used.

Could free energy act as an objective function for policy evaluation?

Unlike utility, free energy cannot be summed across decision makers.

Perception itself is subjective.

Meanwhile, with utility, we all live in the same fully observed world.

If urban regions engage in niche construction,

Perhaps we can create digital twins that embody the niche construction process.

Autonomy then becomes about influencing an existing process rather than creating something from scratch.

There are three potential approaches.

Agent-based models on a hex grid simulating individual travelers.

Simulating each hexagon as a cell with the expectation of emergent behavior.

or use a renormalizing generative model with a fixed hierarchy.

The third option is the most promising, as it requires a single model rather than the combination of many smaller ones.

In line with this third approach, I want to consider a fixed hierarchy, and renormalizing generative models are perfectly suited to this purpose.

The idea is that these might already be transport models, as they map origins to destinations using cost functions.

Sticking with the notion of a market equilibrium, there is a demand and a supply side to the operation of renormalizing generative models.

Just like in a four-step transport model, the demand side determines the landscape of afforded embeddings.

And these embeddings capture the spatial similarity.

The supply side aims to calculate the distribution of flows through the network.

It is just that here, the network is a hierarchical structure, not the physical transport network.

On the demand side, the landscape of affordances or the landscape of afforded embeddings is perceived through the combination of top-down prior and bottom-up data.

From the top-down, we impose our cost function.

on traversing the lateral accessibility graph.

Attributes on edges are valued against each other.

For example, travel time and the price of petrol.

We also impose our preferences in terms of the content we wish to see.

So the content of the embeddings.

For example, the concentration of emissions in the air.

From the bottom up, we receive information for this lateral accessibility graph, like the travel time at this moment, as well as the content of discrete spatial units.

And this content is in the form of embeddings inferred from high-dimensional data using neural networks, so that this process can be automated.

On the supply side, we have a landscape of affordances at each layer in the hierarchy, and we need to parse messages between them.

Supply is about narratives of attention, either about errors flowing up or predictions moving down.

To move laterally, you have to go up and over.

In doing so, carve the transport system at its joints.

For example, when you need to do groceries,

you probably do not zoom out to the causes layer.

Instead, you will find crip much more quickly and walk a few blocks.

Using active inference as a framework to understand decision making in urban regions is still speculative.

The standard workflow of ex-ante evaluation to study the benefits of a project might not be applicable anymore

as the digital twin goes out to construct its niche on its own.

The separation between technical calculations and political motivations in planning becomes blurred.

How do we capture both travel choice and policy objectives in one model?

Accessibility is already a central topic, yet it has now gained additional consideration due to the reframing into landscapes of references.

These affordances may not only be in terms of tabular data, but can be high dimensional, like images.

Accessibility heavily draws upon constrained maximum entropy.

Bayesian mechanics treats this the same as free energy minimization.

It therefore seems worthwhile to reframe accessibility in this way too.

Active inference is useful not only for autonomous urban digital twins, but also for level 4 twins.

To refresh, level 4 twins use physics-based models with the addition of an interpretive layer.

This additional layer contains an understanding of the sensitivities between models, which allows an urban planner to quickly converge on a set of designs rather than simulate thousands of alternatives.

Therefore, a practical starting point would be to take level 3 digital twins and upgrade them with an active inference module to explore the parameter space efficiently.

The apparent complementarity of static and dynamic approaches of livability suggests that we could use representations learned through valuation, so a supervised way of learning urban representations,

to then use them in learning the structure of an active inference model.

In sum, it is fair to say that autonomous urban digital twins still need a lot of work.

Regardless, considering free energy as an alternative to utility is a valuable exercise in understanding decision making in urban regions.

Lastly, I would like to thank the team of symposium organizers for their time and effort.
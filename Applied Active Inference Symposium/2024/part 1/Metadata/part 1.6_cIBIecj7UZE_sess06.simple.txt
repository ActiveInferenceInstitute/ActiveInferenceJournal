SPEAKER_01:
Hello, welcome back.

Now we have the much-anticipated student presentation that your advisor, Alex, recently told us will be awesome by Viet Dong Nguyen on Dynamic Prior-Preference Learning for Scalable Robust Deep Active Inference.

Thank you, Viet.

Take it away.


SPEAKER_00:
Hello, everyone.

My name is Viet.

I'm from the NeuroAdaptive Computing Laboratory.

Department of Computer Science, Research Institute of Technology, and I am Alex Advisee.

And today I'll be talking about dynamic prior preference learning for scalable robust active inference.

So, yeah, first of all, I want to talk about the motivation for this.

So nowadays, you know, fun, fun share work in robotics excels in implementing robots behavior and planning.

For example, creating, we can, you know, creating autonomous robots that can actively explore the environment.

Additionally, we can also do a lot of robotics applications, for example, acquiring knowledge and learning skills continuously.

And this also envisions cognitive and developmental robotics.

So there is just a goal.

for robotics research, that we can try to build robots that can continuously develop their interactions with the environment and their learning processes.

And it is also better to have interactions with their physical and social world in a manner of human learning and cognitive development.

There's a lot of robotic applications around.

And for example, one application can be to build a separate robots and building social collaborative robots and stuff like that.

So underlying all these robot work, it's like active inference research where we often build a generative models.

So the developments of robotics actually depends a lot on activated constraint work because there's a great need for research that integrates the generative world model into a robotic control system, as also deep learning is increasingly used in constructing complex action planning algorithms while solving control tasks.

So, we can see that there is a need for scaling deep active engine systems to integrate into the real-world applications.

So resource inactive inference has led to many improvements in terms of robustness of model-based agents that are applied to both cobblestone processes and partially observable MVPs or PoMVPs.

So an example of an MVP environment would be mountain car.

For those who knows or studies reinforcement learning already, then this mountain car environments in MDP, we only know position velocity, right?

And that is the actual state of the environments.

But when we get to POMDP, for example, like robotic tasks, we as agents see only the image frame and we do not know the underlying states such as position and orientation of the block or joint states of the robots, for example.

So basically, active inference is basically a form of inference and learning that seeks to balance, first of all, the goal-orienting objectives or higher preference.

So this signal exploit

the instrumental signal so that the agents can seek the goal and try to pursue its goal and the other thing it needs to balance the epistemic terms or epistemic signal where the foraging it cares about foraging and exploration driving signals

So researcher often absolutely encounter great difficulty in providing the agent with effective prior preference, as well as computing useful instrumental signal from the agents, the priors, especially in the real world applications where

the environment is very complex the robotic control task and the trajectory or pathway to be desirable and task specific goals often requires a very very long trajectory of preference so it is not that absolutely practical to provide a prior preference distributions especially the stream or rollouts of goal images

So that is the reason why we need to find a way to also learn to generate the prior preference signal in active inference.

So on the right here is just our preliminary results, but we'll get to that in a moment.

So, yes, so we actually, given we got all of this, right, we need to,

we need to predict a draw-out trajectory of a goal states dynamically right it can be a goal step or goal observations but basically the goal is if you look on the figure on top here the top row is the real trajectory where you see the mountain car right

there's a car that it needs to go backward and to build some kind of a momentum and then go full to the right to get to the to the flag if it is only going to the right then it can it does not have enough momentum to to go to the top of the hill so so in here is the actual observations you can see that the trajectory is a successful trajectory and the goal here is to

for each observation at each time step we predict a immediate predicted prior preference image or goal at that single time step so that we have the instrumental signal or idea of what we are trying to learn and then from that we learn the actions planning from the prior preference signal

So in this talk, we specifically analyze this issue, the issue of prior preference learning in the current literature and discuss potential resolutions for scaling the instrumental signal inherent to active inference by introducing the contrastive recurrent state prior preference or CRSPP.

This is the model learning framework that we also have done in the past.

So this methodology frames active inference agents in terms of progressively constructing and adapting a prior preference at each time step.

So this also facilitates the dynamics emission of a useful or dense, very dense instrumental signal at each time step.

So we also study how our client preference adaptation scheme, when operating jointly with the latent space-driven epistemic for aging terms, offers active inference agent a very nice flexibility in challenging problem contexts such as sparse reward environments and also

composed of continuous observation and action space, for example.

For example, robotic tasks, we often do not know the dense rework functions in the actual real environment.

We only know if the robot has reached the goal or not.

And also, all these environments vary in goal states, so we can also see that providing or manually crafting the distributions of goal is not very practical.

So here's the outline of this talk, the introduction I have just covered.

So first of all, I'll just assume that everyone is new to Active Inference, and I'm just gonna briefly cover Active Inference backgrounds and Deep Active Inference, and also cover a little bit about prior work and prior preference learning and problems of all those prior preference learning framework.

And then I'm just gonna go through the actual methodology, which is the dynamic prior preference learning.

And then first of all is the world model.

Next is the self-revision mechanism.

And then next is the learning prior preference models.

And then finally putting all of them together to make an agent.

And then I'll show the results and then discussion

Okay, so first of all, I want to cover a little bit about active inference.

So just a brief overview about how active inference work and what is the underlying theories behind it.

So it is a mathematical framework that describes how intelligent systems are human interact with the environment.

So it also describes the environment as the generative process.

and the intelligent systems are the agent or us as a generative model so active inference is built on top of the free energy principles this means that we as agents or intelligent system continuously observe the environment and forming our and correcting our belief or our own representation of the world

so the generative process defines a real world environment here right this is treated as the data generation process where the information returns a sense by the agent so you can see that's the hidden state underlying hidden states denoted s here it's

generate a sense of some kind of observations denoted O and this is also sensed by our intelligent system so on the right here is the generative model is the actual agent that we are trying to build on the intelligent system right

the intelligent system also has an internal state which is trying to estimate the generative process states so this is called the prior or the internal relief prior to the observations of the environment at a particular timestamp so this is denoted as

the random variable s or state and p of s here is the distribution over the random variable s so this make the agent's generative model the generative model represents the agent's approximation of this generative process in this world by continuously seeing the observations or making the observation o

and modified or updated its belief so we actually call the belief as after being updated after making the observation for the posterior because it is the posterior after making the observation right so in order to learn the model and estimate the posterior we first need to introduce these terms

First of all is the prior overstate, which is denoted as PS, which is also the distribution overstate, right?

P of O here is the model evidence.

It is the distribution over the current observations, denoting how well is our model at predicting the real data.

And in order to compute this easily, this is often written at the sum of probability of observation given every single state.

And then next one is P of O given S. This is the likelihoods and the probability.

This is the probability of an observation given a state.

So this is a function.

It gives a probability given an input state.

And then the last one is the posterior functions P S given O, which is

the posterior function that maps the observation to the states.

So normally in traditional active inference, the posterior function would be the transpose of the likelihood matrix, for example.

And deep learning literature, it is often called a encoder, and the likelihood here is a decoder.

So how do we estimate the posterior now?

We have to find a belief that maximizes the modal evidence, right?

Because at the current time point, there is only one single observation O here, and it is real, right?

So P of O, P of that observation is going to be 1, right?

So it should be the probability of 1.

And then

we use the states to estimate the observations and then it has to be the the probability of one two right so that arrives us at the conclusion that we have to maximize the model evidence p of o so maximizing p of o is also minimizing equal to minimizing minus log p of o right because

it is minimizing the surprise.

So in the beginning, we say that we continuously update our belief after making an observation of the environment.

We call the belief after making an observation a posterior.

This is Q of S, right?

So to compute a posterior, we want to put in a, we just want to call this, you know, any kind of distributions over the states.

Then, for example, we can sample it, we can randomize it, right?

And then we try to gradually trick this distribution so that it's used the highest model evidence or it's used the lowest surprise.

So how do we

choose the actual posterior so that it builds the lowest surprise right so in minus log p of o here is the actual minimizing surprise that objective that we are wanting to minimize right so we can analyze it like we can do it with a Bayes theorem

this is not Bayer's theorem but Bayer's formula or theories yeah and then to evaluate the surprise minus log P of O we need to sum out the hidden variable S from the joint distribution POS but well this summation there's a problem right

because that we do not know like all the hidden states all the hidden variables in the environment it is hidden from us right so it is normally very very hard to to sum up all these state values so if we cannot sum it up then we can try to approximate it so

First, we introduce a posterior into this sum.

We multiply both the denominator and the numerator by the posterior.

So this is the belief that we are trying to tweak.

So in here, we're trying to tweak this posterior so that we can minimize this term.

So this gives us a weighted sum where s

the ratio POS over the distribution Q is weighted by Q .

This is like weighted sum.

So according to the inequality of complexity or Jensen's inequality says that the convex function of the weighted means is always smaller than the weighted means of the same convex function.

So you can see the convex function here is minus log of something, right?

So minus log of something is just a hyper, it's just a parable, something like here.

And this is smaller than a weight means of the convex functions.

Also, we put this sum outside, right?

So this is also applied to the context of probability theory.

And then we can arrive at this equations.

So formally it's called an upper bounds.

This one, right?

A quantity that approach from the above, right?

So this is also means that this is a free energy.

When we find a posterior that minimize this free energy term, we will have a chance of approaching the lowest surprise points, which potentially increasing the model evidence

so after we have these terms right here we can just put the minus inside and then it's become this form this is the conventional form that we call a variation of free energy and after decomposing this we will have these two forms which is the KL divergence between the posterior and the prior

minus the weighted sum of the log of the P of O given S, which is the posterior functions.

Or, no, this is likelihood.

So, basically,

the KL divergence term is called the complexity, and the minus log P of O given S is called accuracy term, where the complexity terms compute how surprised our two state distributions, posterior and the prior, are far away from each other or not.

And the accuracy terms defines how

accurate our model is predicting the actual observations.

So this is also relatable to the variational autoencoder where we compute the loss of the variational autoencoder to be the KL divergence of the posterior and the Gaussian distribution with a mean of 0 and a standard deviation of 1.

And then we also have the mean square arrows of the actual predictions of the images.

and then VFE and MFE is a little bit different than from variation of out-on-quarter VFE is variation of free energy and MFE is marginal free energy as in the active inference tutorial the paper so marginal free energy says that we compute the KL divergence between the posterior and the prior and the prior here is computed from the transition model not the

not the normal distributions of mean of zero and the standard deviation of one so here is the

general model of trying to build a deep active inference model right so here's first of all we're trying to encode right using the posterior functions so in traditional active inference this is the P is the transpose of the likelihood matrix P this is normally learns with the categorical distributions and using Dirichlet conjugate prior

And then in the deep learning literature, this is often known as the encoder that's often parameterized by the neural network.

And then O here is often be an image and state here is just some hidden vectors that we are trying to compute.

then next one is the likelihood function right so normally we text in the states of the agents and then we're trying to estimate the predictions the observation predictions right so in traditional active inference this is known as the likelihood matrix or likelihoods per se this is also learns with the categorical

categorical distribution and using Dirichlet conjugate prior.

And deep learning literature, this is often known as the decoder that is often parameterized by a neural network.

So as we know the variation of free energy's formula, the complexity terms KL divergence here, right?

And the accuracy term is the minus log PO, right?

Even S. So here we compute the KL divergence between the encode states and the actual prior states here.

And then next one, we compute the

actual log probability of the predicted observations and the actual observations right here and that makes the whole objective where we compute the two objectives right here that's also the variation of free energy formula so

We just talked about how inference, inactive inference work, but there's more.

We want to build an inference system that can interact with the world, right?

So when it comes to interaction between the agent and the environment, we also need to have actions and also roll out in time step, right?

So basically we have another element here, which is AT.

And remember, I also mentioned the transition model, right?

So after we take an action in the world,

the world or the environment evaluates one step and it changes its own state and then emits a new observation and we as agents or intelligence system also try to predict given the action in the current state what is the next state

And this state is the prior over the state of the next time step, where we have not made the observation of t plus 1 yet.

So after we have the st plus 1 as prior, we do the same process as I had mentioned before.

So we make observations, try to change our belief

after making that observations and then learns from the new observations to take our prior into the posterior.

So here we learn it using the variation of free energy or marginal free energy formula.

So as you can see from the previous slides, we do need to somehow plan the actions to interact with the world.

So if we're trying to minimize or surprise

with the current observations so in the action planning we aim to take actions that minimize the similar free energy term in the future this is often called as the expected free energy by the way so in here we introduce some new terms right theta is the modal parameters there is why this here is because we want to also compute the

model the surprise over the model parameters or the uncertainty of other model parameters so you can see that the expected free energy in the future it's kind of similar to the version of free energy formula it only needs another model of parameters parameters and the given the pi or the actual plane's policy or plant actions

So after a lot of derivations, I'm not getting into the real derivation here, but here's the end results.

That's something something minus

minus the weighted means of the log POT.

So I'll just focus on this term because this is the goal of the talk today.

So basically, this term log of POT is the expected free energy derivations of the instrumental term.

so basically this also means that we are trying to maximize the now we're trying to take the action that maximize the future observations that matches our prior preference distributions or what we want right so normally to learn to take actions based to learn to take actions

to align with what we want to see in the future.

We often need to craft and provide the prior preference manually in advance.

And then we take the actions that maximize the likelihoods given that distributions.

But this is often not really practical in the realistic world.

And because we cannot like craft the manual distributions, like every single time we encounter a new environment or encounter a new task, right?

So what we can also do is we can also provide a goal state or goal observations directly to the agents, which is more practical.

But in the end, the prior preference distribution is very sparse.

And this provides little to no learning signal to the agents.

as you can see on the figure on the right and the left.

So black is the prior preference distributions and right is the single goal state distributions or goal state image.

So what can we do to dynamically produce the prior preference?

We can try to learn a prior preference model

that produce a goal image at each time step so here is on top is the prior preference model that predictions right and in the bottom here is the model prediction of the future given the sequence of planned actions so we are trying to say that we want to take the actions

that have the rollout in the future predictions matching the private preference rollout in the future.

So how do we do that?

We first have to learn the model first, right?

So in learning,

we can try to say that each time we achieve a goal, then we mark the whole trajectory as the prior preference trajectory or the positive trajectory or expert trajectory.

And then for each observation marked as goal, we use a deep learning, like another deep learning model to estimate start observations.

For example,

and then in inference time we actually train when we train the policy planner to propose the actions right we make the policy planner proposing the actions first and then predicts the future observation based off of that actions that is planned by the policy planner and then we actually learn the policy planner based on the signal that is emitted from

how aligned the predicted future observation would be to the predicted goal.

The predicted goal here is the actual prior performance model outputs.

Then we train the policy planner with the mean square arrow or some kind of objectives that predicts some kind of objectives between the

predicted goal image and the predicted future observation so there's this problem where it exists between all these learning schemes because it is very computationally expensive so it is generally expensive because we have to work directly with the images so for example in learning we have to use the decoder of the

of the prior preference model to actually predict the goal image at each time step in inference time is also computationally expensive is because it has to learn from the images itself and compute it down from the back propagation will have to flow through the decoder back to the action planner

So here it comes to the actual methodologies.

We actually use dynamic preference learning in the step space instead of the observation space.

so first of all i want to talk a little bit about the world model so this model architecture is basically what i have just drawn in the background sections but this looks with robots images so in here we have the observations um ot ot and here's also the observation the same observations ot minus 1 ot

And we text in the encoder.

Encoder is the encoder posterior.

Text in the encoder and we have the transition prior.

And then we have the decoder, right?

encoder encodes the image in, have the states, and then decode the image, the state back to the image, and then computes the accuracy loss.

Accuracy loss is minus log prop of the PL given S, as we have discussed in the variation of free energy sections.

And then the complexity is the KL divergence term between the posterior and the prior.

So how do we compute the priors?

We compute from the previous step's states and then we use the transition model to estimate the next state's prior So that's how we can get the prior So current work and popular works regarding the world model or generative models

often integrates the model called RSSN which stands for recurrent state space model in the transition model so to zoom out a little bit we are trying to integrate some kind of temporal information through time

So that's the state of the environment or the states that we're trying to predict.

Be more accurate and it carries more information with time.

So for example, we cannot encode enough information from just a single image because for example, there can be like

position and velocity, right?

The velocity is acquired through multiple images and cannot be acquired on a single image, if that makes sense.

So what we often do is we try to integrate a recurrent neural network that takes in the previous states or the previous recurrent states, H minus T minus one,

and then we also take in the encoded observations and then in transition we produce the recurrent neural network we produce the current recurrent states and

also predicts the outputs to put into the transition model and also the recurrence state will also be served as inputs to the posterior network so that's basically the idea and then the other way around we can just do the model as normal

So in active inference, the agent takes in the actions that it believes that would lead to its preferred outcome using instrumental signal.

To construct this, prior-preference passbook has, for example, provide a goal statement observations directly to the agents, for example, Mazaglia in 2021 with contrastive active inference.

We can also manually craft a prior-preference distribution.

We can also dynamically produce the preferred observation that each time set.

But as I mentioned already, all these methods, one way or another, very expensive or

it's very not practical or in the real world applications or it is very computationally expensive for dynamically producing the actual observations for the goals

So we can also use behavior cloning, but its trajectory diverges from the training trajectory due to small mistakes made by agents as well as environmental stochasticity.

And also it is because of independent and identically distributed assumptions of positive expert data distribution.

So we can, we try, we will,

of avoid it because it's not gonna be the same right so two distributions of the actual trajectories in the environment should not be the same as every single positive trajectory of the expert data

So to tackle this, we try to leverage a small quantity of seed imitation data to learn a neural network that dynamically produce the preference over states, not observation, but states at each time step.

So this effectively produces an easily generated dense instrumental goal signal.

and we also design the agent such as that it moves according to the trajectory that is shaped toward its own estimation of the future states or future preferred states so it's trying to notch its own trajectory towards the imitation or positive data distribution so as you can see on the figure on the left

So you can see that in behavior cloning, the actual trajectory diverges away from the actual imitation data where the prior preference over the state is trying to be shaped to work the imitation data itself using the model signal.

And our AI app is our agent.

It's called Robust Active Inference and it learns from both epistemic signals and instrumental signals.

But in here we'll just focus on the instrumental signals and how to construct it.

So

yeah so basically we use the decoder or the likelihood functions of the generative model to only visualize on the preferred observations not that in training or test time we do not use the model learning we do not use the actual observation goal observations by itself we only use the hidden states so the goal here is to get

the state the growth states at each time step be predicted by the prior preference model right you can see that's on the top here is the real trajectory and the bottom here is the actual dream that's the dream of the robots that it wants to be right and then we are predicting that dream right

So here is the actual model of the contrastive recurrence state-prime preference learning.

So basically, it has also similar architecture to the RSSM, to the world model.

It also has an encoder here.

It also has the transition, but it does not have the decoder.

So how do we actually learn this?

prior preference model so first of all we learn the world generative world model as normal right but when we say whenever it's whenever the trajectory is positive or a successful trajectory we try to contract

the state of the CRISPR, I just call it CRISPR we try to contract the state of CRISPR to the world model state and then whenever the trajectory is negative we try to push away the state's

from each other so that the crispy model can only predict the positive states so how do we determine if it is a positive trajectory or the negative trajectory without having a very sparse signal so we will have a thing called raw or the prior preference signal so how do we compute this raw signal

So imagine this is the real trajectory, and it is a successful trajectory, for example.

So we're trying to compute the prior preference rates

having the rates of the successful states is at one and it starts from one and then it decays backward so it decays very fast and if so in this like really far away states from here they do not have enough signal because there's no goal like there's no direct call from that states and then

there's real trajectory that is failed right you can see that the mountain car cannot reach the flags on the right here so whenever it's failed we can just plot the very very last observation as minus one and then we start to decay backward negatively

And then we have the prior preference rate signal as very negative.

So the reason why it has to be last is because we want to blame the last observations hardest.

So in order for the agents to just push away from the failing states very, very, very strong in the very last state.

So in the real trajectories that have multiple successes, we try to put all the 1 in all the successful states and then just decay backward.

That's similar to how we decay backward in a single success case.

So with that said, we now know how to compute the row.

And then this will serve as the dynamic signal to the contrastive loss, and it creates a dynamic behavior of the contrastive loss.

That's trying to either push away or contract the two states of the free-speed state and the world model states.

So for example, in here, if there is positive row, the contrasted block would be contracting all these states together and maximizing the similarity between the crispy states and the world model states.

And then we also learned to minimize the KL divergence between the posterior and the prior over the states of the CRISPR model.

This is because we want to estimate the transitions, right?

So we transition to the next goal state.

And then this goal state has to match the actual goal

That is the current stage of the RSSN model or the world model.

In contrast, if rho is negative, then we're trying to push away all these states away.

So we're trying to maximize this distance or minimizing the similarity between the world model states and the crispy state.

And in here, it notes that there is no complexity objective.

It's because we don't want to predict the next goal that is the failing state.

That's basically the idea.

So that arrives at the general formula for the objectives of the contrasted frequent state prior preference model.

Basically, it is just masking out the sign of the row and then multiplying with the KL diversions or the complexity.

between the posterior and the prior of the crispy model and then we minus the row and seam of the actual predicted state and the world model state so seam here can be very much

any kind of similarity functions for example cosine similarity and then in our work we actually use the actual cosine similarity so in train time we have just talked about training time right so we have successfully assuming that we have successfully learned the prismic model that predicts the future goal

And in inference time, we use this crispy transition model to estimate future rollout trajectory of the goal.

So we can see that on the top here is the actual observations.

And in the second row is the actual prior preference rollouts or predictions of the future goal given the current observation.

So given the plant actions, we're also training the plant actions, right?

So the actor trying to predict an action given the observations, the predicted observation in the future.

So we're trying to say, this is, we're trying to compare the actual prior preference roll-offs with the actual predictions given the actions.

And then we are trying to

learns the action planner based on the signals or how diverged the future goal is to our future predictions of the world given the actions.

So there can be so many reinforcement learning agents and active inference agents that can utilize this technique of prior preference learning.

So active inference agents

I put my works here.

It's R-A-I-F.

And then there's also Chris Buckley's work and Barans Milic's work about deep active inference in variational policy gradients methods.

And also Otto's work is the scaling of the active inference in POMDP and not in current environments and so on.

So in personal learning agents, there's

all these sorts of actor-critic algorithms like dreamer series, soft actor-critic, dreaming, DDPG, TQC, and CD3 and so on.

So there's a lot of agents that can utilize the framework because there's no change in the original architecture.

There's just an add-on to how an agent would apply the prior preference learning signal into the system.

So here's the actual flowchart of the agents.

Basically, we start the agents and for each episode, we evaluate the environments using the world model and the policy planner.

Actually, basically, we evaluate in the environment.

We let the robots play, we let the robots do the tasks, and then we collect the data and add them to the replay buffer, for example.

And then we can get to the main training agent's logic.

And then first of all, we sample the training data from the buffer.

So in here, we're trying to have, we also have to use the self-imitation learning buffer.

So yeah, so self-imitation learning is a very good technique, normally to balance the number of successful and failing trajectories

And we also learned, so first of all, we learned the world model decoder translation on RSSM.

And then we trained the epistemics model, but in this case, we don't need to care about it.

And then next is the contrast safe recurrence state prior preference model from the data.

And then next, we imagine the future trajectory using the world model and the policy planner.

We try to estimate the future actions to be what is the optimized future actions.

For example, we estimate the future action based on how the current state is.

And then the world model will be continuing to fall out in the future.

And then we dream of the future.

And next is the part that we actually

infer on the crispy model we use a crispy model to imagine the future preference trajectory given the goal given the current observations and we predict the future goals rollout goals right so after that we learn the policy planner right

using the instrumental signal.

The instrumental signals here is computed from the difference between the future goal and the predicted future dream of the world.

And then with that, the policy planner can learn from the instrumental signal.

So here's our preliminary results on active inference agent.

So basically, we can see the two top rows is the mountain car environments, and the two bottom row is robotics meta world push button environments.

So let's look at the first row, for example.

The observation here is a mountain car that has reached the goal.

And for each observation here, we predict a preference rollout or preference

or a goal stage that's single time step so in here we can see that if the car has not have the momentum yet then we have to build the momentum by predicting the car going to the left for example right and then after it has built some kind of momentum we're trying

the actual prior preference model predicting the car would go to the right, go forward, right?

When it goes here, the prior preference model will predict the car to be going closer to the goal, something like this.

So the same applies to the robot environments where the actual observation is the third row and the prior preference rollout is on the fourth row.

so we can see that it is also trying to estimate the dream or the dream of the goal of the robots is to actually push the button so there's different kind of experiment setup that we are trying to learn basically we have four trials on each environment and then we

trains the experts using SOP-AutoCritic or PPO, proximal policy optimization, to collect a small amount of seed data.

Normally it's about 20 episodes, something.

20 to 30 episodes.

So we test mostly on three main environments or suits.

First of all, it's Martin Carr.

There's only one task.

Metal World is 13 tasks.

So here's all the red robots.

Here's Metal World.

And then the robot suit, we have two tasks.

And then regarding camera setups, we've used three views for Metal World and four views for the robot suit.

So the green check mark here denotes that the robot has successfully solved the task.

Here's the RoboSuite door, open door.

And then here's RoboSuite lift, where it can successfully lift the block.

And here's more solving agent tasks.

So basically agent with CRISPR exhibits an ability to perform successfully early on.

And so agent without CRISPR struggle to actually succeed on solving the environment.

And we also test on our other environment as well.

For example, gymnasium robotics and this task.

You can see in the middle image over here.

So actually, we go to the discussion sections.

So actually, I want to make a note that with respect to the CRISPR submodel, our model utilized contrastive objectives to adapt its parameters

so basically it solely learns on estimating the world model encoded states while pushing itself away from undesired world model states or trajectory so this does not require the learning of the decoder so we only make sure of the learned decoder in the generative world model to visualize

the to visualize the preferred observations from the estimated crispy states.

So experimentally, we remark that this is an that's we only use this decoder to as an auxiliary tools to prove that the states of the contrastive recurrence prior preference model is useful.

So

Theoretically, behavior cloning will be unable to achieve a grid of success in multi-goal environments due to its reliance on a fixed and finite size imitation sample pool.

The expert trajectory in this fixed pool might further differ from the actual trajectories that the agent needs to take to solve the problem at hand.

For example, there's a distributional gap in observations

therefore and um it requires uh different sets of actions to be taken for example right um so

On the other hand, CRISPR learns to produce dynamically the goal states and not the goal observation.

So we can empirically confirm in simulation that our agent with dynamic prior preference learning does not suffer from the goal mismatch problem in the training phase.

So this is very applicable to the multi-goal environments.

So we can see that this prior-preference learning technique has a lot of practical applications in different robotic tasks.

with for example those with potential dangers in the operations and functioning for example when human safety must be considered right for example a trying to drive a autonomous driving car system where it has to have or it has to predict the future goal right the future goal is not to hit the human for example right

And we can also make a lot of advancement in imitation learning in research community.

And it can be applied by the wide range of reinforcement learning and active inference agents.

And then the last one is to scale the active inference agents on the real world robotics agents.

So future works was actually considered importing or integrating the dynamic prior preference learning in latent state space model, for example, variational dynamics, discrete variable autoencoders, or tension weighting models.

And also, there's another model's physical neurorobotic system, which entails an embodied and active and survival-oriented formulations of active perception and world model learning.

So in this talk, we have discussed a lot of ideas about being able to estimate the particular goal or preferred states at any particular time step is very useful for cognitive control agents.

So as our results demonstrates, the agent can then learn to adapt to take actions that leads to a estimated goal.

And then in contrast to the approaches taken in the imitation learning and behavior cloning literature, which train the agent's policy base solely based on the fixed collected expert data sets in our framework, the expert signal is estimated dynamically through an adaptive prior preference model, right?

Closing this gap, the domain gap between the actual trajectories and the elected training imitation trajectory or preferred trajectory.

And then we also discussed the generative model and encoder-decoder transitions and how the variational free energy work and then how to learn the model.

And then finally we discussed the contrasted recurrent state prior preference model with the self-revision mechanism and then discussed the model learning and then inference objective.

and how to integrate that into the agents planner learning system and then we show the results for the robotic tasks and then I mentioned how it can be applied to different fields so that concludes my presentation and thank you very much for listening thank you


SPEAKER_01:
That was amazing.

Really great job with the background math.

Not only like a PhD student can, but it was great to see how you brought it all together and connected your research to it all at the end.

So thank you.

That's all for now.

See you later.


SPEAKER_00:
Thank you.

See ya.
SPEAKER_00:
Great.

Thanks, John.

Quick mention to Daniel.

John and I had a really nice conversation prior to this.

If it's possible, I might actually start my talk now.

uh and then yeah perfect i was hoping that would be the case and then that way john and i can also have a little bit of back and forth potentially during the course uh and obviously daniel you've had so such a big hand in the development so anytime you know yeah would like to chime in please feel free uh great um

right so my original plan talk for today was entitled uh let me get rid of this guy uh active inference is a framework for social science-based uh modeling and okay um something happened on the way to the symposium

Oh, exactly.

Huge updates.

I want to just give a little bit of context.

For some time now, I've, you know,

I personally gave a talk a couple of months ago, a three and a half hour workshop, basically at an international conference for computational social sciences held at University of Pennsylvania, July 17th this year.

And what I was trying to do was basically teach computational social scientists, that means economists, sociologists, political scientists, and so on, who are very savvy with

computers, essentially, how we might be able to employ active inference in agent based modeling, which is already, you know, a decades old at this point, method of designing basically simulations of agents inside of environments where agents might be, you know, human beings and an environment might be, you know, a market system or, or some other kind of environment that we

you know uh humans find themselves in where you can you know you can play out simulations and test things and so on i want to introduce that to computational social scientists and i think more broadly what's what's great is that this symposium now gives me the opportunity to kind of

flip it to where I can kind of bring some of what computational social science does to other active inference researchers.

But to stay on with the bio firm, I really wanted to again, as Daniel said, something happened on the way to the symposium.

I've had these conversations with John since I did that conference, and I have a lot of strong intuition that a lot of what he's been describing is a really wonderful direction for future work in both agent-based modeling, but also applications of that in the real world.

I mean, we're always operating under conditions of uncertainty and trying to figure out kind of what to do and very adaptive ways where our environment is constantly changing from implementation of computers, social networks.

We have climate change.

We have the typical booms and busts of our economic systems and so on.

So I'll start here, but I just want it to be known like there have been a lot of heavy revisions to this with a lot of development we've made on the BioFirm, thus featuring BioFirm.

So again, I'm Andrew Pache.

My day job, I work as a data analyst slash scientist slash engineer, always many roles to fill.

And so I bring a lot of those kind of skills with me.

to active inference and how I view things like developing applications for specific use cases and things like that.

There can be a lot of rich interdisciplinary dialogue between different fields and that includes the kinds of things I do on a day-to-day basis versus my interest in active inference, which has been going for the past few years now.

I've been an intern and I suppose I'm feeling much more like

like a full time kind of affiliate of

the Institute at this point.

I've been teaching the textbook group with Daniel, co-facilitating that.

I've been involved in various projects.

And then now this kind of partnership with First Principles First and what Dr. Klippinger is trying to accomplish in developing the bio firm.

So I just want to put on this title slide here, these are actually links to if you would like to get involved with the Active Inference Institute.

with uh check out all the kinds of things that are being put out by first principles first and also i just want it to be known like you can come back to these slides as needed as well as any of the other kind of learning materials i've been producing whether before the conference that i did a couple months ago including on the code

how to actually create active inference agents from scratch, moving up to multi-agent simulations and so on.

I also did a talk with the Institute recently that kind of walks through some of the code.

um the new outline for this talk we're going to touch again on agent-based modeling which we already talked about yesterday uh a little bit in the first workshop so i'm we're going to kind of skim through that a little bit but i want to i want to put some special attention upon the aspect that uh of agent-based modeling referring to what are called micro assumptions about behavior

point blank, if we're going to simulate humans, we have to include certain assumptions about how we think humans behave if we're going to simulate them in an environment, right?

Otherwise we just, we can create a list of humans.

They do nothing until we give them some kind of

rules or behavioral you know dynamics or something along those lines so those are essential uh for for setting up agent-based modeling and then uh and then we'll shift over into active inference we'll look at the pomdp as a kind of exemplar model talk about some agent multi-agent examples just to give a sense of what kind of work

can be done in this area many people watching this now who who are already involved with active inference there'll be some aspects of this that are you know might come off as a little repetitive but i just i don't want to i i want to stick to the textbook so to speak right i want to kind of keep this grounded

in a sense that you know folks who know about active inference don't know about active inference have some big interest in active inference in this in-between area like can catch on to the material here um

Next, we'll go into this question, are LLMs agents?

There's a lot of exciting work being done with LLMs.

They're showing incredibly exciting and surprising sophisticated reasoning skills and capacities for dealing with natural language.

That said,

And it's already been said so many times in the other talks given during the symposium, but it's worth repeating.

They have limits.

There are issues.

And I've been doing some work on thinking through how

do LLMs relate to things like cognitive theories?

How does an LLM think or how does that work really?

So that we can really grasp if we want to incorporate LLMs into say agent-based modeling, software applications or otherwise, let's come to terms with like the reality of what they are before assuming too much about what they

are or what they're capable of and furthermore how we might be able to integrate them into kind of more grounded means doing agent-based modeling such as working with actual agents the quick answer to our lms agents agents uh are they agents no that that's the quick answer they're they're not uh that's what i will claim and then we'll move on to the bio firm which frankly will take up the majority of the rest of the talk uh from there

So what is agent-based modeling?

As already stated, we have agents.

More specifically, it's a programmed entity.

It could be a person, a living organism.

It could be representative of an entire institution or otherwise.

And basically, programmatically, it receives observations from its environment.

And then it returns actions back into its environment.

For example, an agent could be a person, an investor.

And then with the environment, the environment could be an artificial or real system that elicits observations for agents and then receives the agent's actions.

those actions then impact the environment itself in turn.

For example, the environment could be a stock market.

So we see that there's a reciprocal relationship between the two things.

An agent or environment's action is an environment or agent's observation.

So the agent sees a price signal from the environment,

they make a purchase uh next morning prices have adjusted given that agent maybe other agents purchases you see so there's this kind of back and forth in a loop uh you know at a certain time intervals such as in time steps or we could imagine them in days like on day one this happened day two this happened

In active inference, the agent environments respected Markov blankets, which kind of define what it is to be that entity, whether it be the agent or what it means to be that environment, including all the structure, the characteristics.

You know, if it's a person, then maybe it includes things like their goals or their

What it means to maintain homeostasis.

So an individual has blood and oxygen flows in their body.

What does it mean to be that entity?

So from there, we simulate them.

We play out the agent environment dynamics over time.

Again, as I said, in time steps, some other kind of interval.

and uh and then in active inference we often use the phrase action perception you know it's like a you do something then you perceive the outcome you elicit your next action you know receive the outcome and then you know arguably this happens at a very rapid pace in actual human beings right it's this is goes beyond just time step one or two or looking you could measure this in in nanoseconds or otherwise

so core guidelines for setting up agent-based models we want to make realistic yet simple as possible agents and environments that has to do with you know generalizability if you make something that's so unique to a particular situation it might not generalize well uh you know so if you want to make an agent agent-based model of say uh you know agents taking care of a natural environment if you make the agents way too specific to that environment to where they

If you want to set up a decision-based model and get insights into what you can do in reality, you want to make sure that what you test in the model can actually be carried out in reality.

It can't be too specific.

It can't be off course.

robustness simply small changes don't dramatically change results uh so you know if you fine-tune something too much such that you know the the results of the um experiment end up being dramatically different and suddenly once again we have that generalizability problem reproducibility everyone knows reproducibility problems exist in modern science we need to document and publish you know our code make it available

to other people so they can understand it, recreate the results, et cetera.

And finally, usefulness, which is simply, why do this?

If we can't get any kind of additional useful insights out of setting up these rather complex computer models and programs, why do it?

And so it's worth having that in mind.

Not all models are right, but some models can be useful, right, in the statistician George Box's words.

Then purpose.

So we do agent-based modeling in order to predict outcomes.

We could try and extrapolate from the present.

We can entertain different kinds of hypothetical scenarios, design new policies, and have them play out over the course of the simulation, see what the outcome is.

And then we can also look at collective outcomes of aggregated individual behaviors, if all

uh you know stock investors in my simulation all began investing in this particular stock like what happens by the next day do we end up with some kind of bubble and then bust kind of situation like what what occurs you know that that kind of thinking and then over the past couple decades and this is really what I wanted to highlight a couple months ago and what I would like to highlight now again and I think it's especially relevant for anyone wanting to do agent-based modeling

and especially folks who are interested in active inference there's been a cognitive turn uh you know a lot of these traditional agent-based models you know they're very you know very interesting a lot of thought has has been put into them but their agents tend to be very simplistic um in part because you know 40 years ago we were working with uh you know

much more at this point archaic computers and computer architectures and devices that simply could not run simulations that involve many interacting agents who all have highly sophisticated behaviors.

Furthermore,

it's called a cognitive turn not a complexity term but a cognitive turn in the sense that we're looking at actually more closely approximating human behavior so that means actually looking at other fields ranging from you know neuroscience cognition psychology social sciences that means looking at uh you know cybernetics uh biology you know trying to open up the the discourse and the dialogues over how can we

or better approximate human behavior so there's some common characteristics for these calls to cognitive agents that includes like having inter you know agents have beliefs um they update their beliefs based upon what they see they have to infer what to do in a sense that they're more autonomous they kind of make their own decisions traditional agent-based modeling it's like well if if

if the agent sees x then they do y otherwise they do z and these highly simplistic you know programmatic rules that you know the program you know i programmed in that way so i already know that they're going to do that uh whenever it comes to agent-based modeling with cognitive agents i don't necessarily know exactly what they're going to do they kind of make their own decisions right they have the architecture for that and then also reinforcement learning which is kind of a you know

adjacent or subfield in some ways of like machine learning, data science, you know, a lot of these kind of state of the art, but still rather standardized at this point ways of doing modern data, you know, processing and analytics and predictions and forecasting.

Reinforcement learning is a way of doing that where you actually have an agent, right?

So that's been kind of one of the main ways people are trying to to continue this cognitive turning agent based modeling.

Now, micro assumptions is a lot of information for a single slide, so it'll be over soon.

But I really just wanted to give, you know, kind of a picture of there.

There's so much research that's been done, so many more kind of dialogs between fields that that can occur here now.

like when thinking about how to make cognitive agents what makes sense you know what what's realistic but what also balances complexity and accuracy keeps things simple enough to where the agent-based model can actually be useful so there's been a lot of criticism at this point centuries of criticism of this kind of neoclassical tradition classical then neoclassical tradition adam smithian kind of tradition

that tends to paint humans as utility maximizers.

It's often criticized because it's too simplistic.

It reduces people to being greedy or immoral, as if they're just going after profit without any concern for any other condition or needs or wants.

So it's kind of selfishness, right, without any form of altruism or sense of community.

Some people point to economist Lionel Robbins, you know, kind of a classic in the field, who claimed that unlimited wants, scarce resources, that is the fundamental economic problem.

Yet this assumption, I think this gets overlooked.

I also criticize the utility maximizer assumption, but I want people to be aware that this leads to a...

a really useful tool, which is for agent-based modeling, you can severely reduce computational costs for putting together agent-based models if you just have a bunch of agents who are trying to act to maximize a singular value.

There's not a lot of complexity there.

You can run those equations quite quickly, right?

And then that also assumes that the main thing in your environment is the singular value that is utility or

a stand-in might be uh you know money or some general notion of liquidity and so since then we've had so many other ways of looking at human behavior uh behavioral economics and finance psychology cognitive science social theory you know humans aren't just trying to maximize utility they're they're they're speculative they don't have fixed preferences they actually look at one another's decision makings you know this is

It goes back to John Maynard Keynes.

We had Hyman P. Minsky, who looked at market crashes.

We have Robert J. Schiller, who's still alive today and writing on narrative economics, the idea that ideas have an impact on how we interact with our economy and how we behave.

There's Kahneman's thinking fast and slow.

Like sometimes we just rely upon heuristics or maybe we do it a lot of the time.

You know, it's quicker.

It mitigates having to put a lot of thought into things where we just kind of want to get something done, even though it's not necessarily maximizing utility.

um there's dollar and sunstein's nudge it's like sometimes yeah you do just go with the default option some people whenever a website asks them like do you accept all the cookies they will click yes quickly just because they want the thing off their screen right um so

And then we have social emulation, mimesis, convergence, ideology, all these theories.

What is money?

It brings up the question of what is utility?

What is money?

Currency actually isn't just a means of exchange.

It can also be a store of value as its own value changes to try and do kind of economic transfers to make sure that you get higher value out of whatever form of currency you're holding.

It's also a social institution, though.

is much broader right so be having access to money as many of us know allows you to be uh first you have access to a variety of commodities assuming you're in a market economy where many things are fungible and tradable in that way and then we also have social belonging it's very difficult to maintain yourself in a society where you have no money right so that's a it's a whole dimension that is lost we just look at something like maximizing utility

I've put a mention of Louis Althusser, which is a much more kind of critical theory approach, but it looks at the impact of state law, different forms of coercion upon people to act and behave in particular ways that go beyond just their ability to try and maximize their own profit.

The pragmatism, people update their beliefs, they're forward-looking.

they don't maintain the same preferences and ideas and rules uh you know throughout history you know they change they have their own circumstances and so there's a kind of pragmatic uh approach to life carl pilani

economic activity is embedded in society uh meaning that a lot of that economic activities that we have you know they themselves are predicated upon our ability to trust one another work with one another and all these other kind of things where whenever that trust is harmed it can dramatically change how that economic

activity plays out and then we have neuroscience biology physics and that you can see how we're moving more in the active inference direction now but you know that we have these are predominant theories now in neuroscience the idea of predictive processing that you know that one comes to a situation with their prior beliefs and they they can be updated in the sense of the the bayesian brain ops is the kind of way of mathematizing uh that process um we also have and this is quite old now but it's still

it still stands as a role like heavy in learning or associative learning.

So someone who associates money with being something bad very well might not have such a drive for money relative to other things.

And then homeostasis, allostasis, allostasis being planning for homeostasis or homeostasis in the future.

These are obvious things for us that we deal with on the day to day.

We need to fulfill our needs, physiological needs, neurobiological needs, unconscious processes in the brain.

You know, that goes beyond being able to say that every single thing that we do is necessarily conscious or deliberate.

I'm not actively trying to think about how to regulate my blood or oxygen flow right now.

We also take preventative actions.

We do things that might be a cost right now, but they change the impact of whatever happens in the future for us.

We are forward looking.

in various ways and we do often rely upon habits as needed to fulfill our needs habits allow us to do things without having to put in too much expenditure mental expenditure you know metabolic expenditure to ensure that our needs are going to continuously be fulfilled in cybernetics embedded cognition

This is the idea that behavior and even intelligence arise from interactions between brain, body, and environment, which kind of overcomes some of these ideas, classical ideas of cognition mainly being outside in, where you just kind of look at what's around you, and then you take that in as a passive observer.

Instead, we're actually always in communication with what's around us and mortal computation in the sense of we can move in the direction of, you know, if we want to do something like cognitive modeling, then we're looking more at, you know, actually trying to mirror these kinds of processes that are much more realistic.

collaboration imitation computational neuroscience psychiatry and modeling modeling internal dynamics in form of you know time series neuroimaging data uh you know applying different kinds of techniques like causal inference to see how one factor impacts another one variable impacts another actually trying to find correlations between internal neuronal dynamics with actual

uh behaviors that occur uh you know so so for example sticking you know a kind of a electrophysiological like electrodes on uh you know a mouse and a you know teammates to see like okay what kinds of neuronal dynamics correlate with you know the rat choosing to go left to find the reward uh you know find the cheese or otherwise and then and then taking that and trying to move in the direction of

you know creating an agent who who basically is fit to the behavior of that that mouse or rat in the team maze right and that's a classic kind of example that's looked at an active inference and we also have the hamiltonian principle of least action much more in the realm of physics um and it's it deserves a whole you know slide or 10

know on its own but the the main idea figuratively is you know the idea of something occurring uh based on uh following whatever the easiest path is from one state to to another so a lot of these things the idea of you know social homeostasis uh you know

relying upon heuristics versus planning and forward-looking versus dealing with whatever's going on in the moment, just because you need to figure out what to do now, even though it's not something you planned for.

All these kind of aspects start to come together, I argue, in active inference.

So it's kind of like, you know,

is this should we be considering something like como economic is revised you know again liquidity is the means for all commodities at least in a market economy at least assuming fungibility of things that liquidity can get you many many things in a certain way i almost want to dial it back on saying it's immoral to want liquidity uh it allows you to fulfill your needs

again in a market economy by seeking out necessities for your own homeostasis or allostasis you know physiological relief or otherwise even then even as your bank account increases that doesn't necessarily mean you're going to have your social needs met kinship or familial needs met physical health uh you know if if a doctor tells me after i've suffered some kind of injury that i actually need to do stretches now in order to make sure that my arm will move

going forward in the way that i need it to it doesn't matter how large my bank account is i'm still going to have to do those stretches right um so there is a simplistic example but all of this is to say uh you know for agent-based modeling rather than thinking of utility especially utility in some vague abstract sense of

You know, is it money?

Is it liquidity?

Is it something else?

There could be a shift to a kind of alternative, you know, what is essentially a virtually actually universal value computed by the brain, which can encompass not just to drive to liquidity, but other aspects of means of life.

Simple as possible, yet no simpler.

And then it itself becomes a simplified recordable rip metric for measuring one's capacity for current and future survival.

And so that enter the free energy principle, which is one of the central principles of active inference.

Here we're minimizing quantifiable uncertainty.

By minimizing this, it allows us to achieve our goals, fulfill our needs.

We learn, we actually change our minds.

uh you know we learn new information we are not fixed beings who you know are kind of one-dimensional we update our beliefs and uh free energy is a proxy for uncertainty or surprise

You know, it's computed quite easily, you know, because it's a it's a variational quantity.

There's a proximate computation, which, you know, in physical living beings that keeps our metabolic costs low.

And for agent based modeling, that also means keeping computational costs low.

And so it's still kind of fits, you know, the the the usefulness of what utility was for agent based modeling.

I think that free energy can instead be the new kind of metric that we're looking at.

And I included this kind of fun fact that there was a nice talk given not too long ago with the Active Inference Institute.

Some folks who do neuromorphic computing just reminded us, despite how many calories our bodies

used on the day-to-day the brain only requires roughly four bananas worth of calories despite the fact that it makes 10 to the 15th uh calculations per second it's just a reminder of like this this variational aspect rather than being able to do quicker approximations and then um you know we talk a lot about free energy but i just i really you know in the in the kind of funniest kind of way i just uh have always been fascinated by this equation uh this is you know textbook

equation from active inference but you know so G is our expected free energy related to deliberative planning goal attainment information seeking conditioned on our homeostatic preferences right so we plan for things to take care of ourselves to accomplish things we need to F is variational free energy another quantity where we balance the complexity and accuracy of how do we deal with the moment then we can think of this as like you know say you're camping in the woods and suddenly there's

bear it's like well you didn't plan for that uh and you didn't expect that and coming up against a potentially dangerous bear is probably very much against your preferences for maintaining your homeostasis in this case your sense of physical safety so you have to deal with that and then e uh you know those are your habits your heuristics so so what i'm saying is that i just i i particularly appreciate that we have a quantifiable way

know here of trying to decide what uh you know what it is that determines someone's actual behavior you know the posterior beliefs high meaning what policy or action should i take right uh all of that is kind of all these different considerations habits deliberative planning moment to moment planning uh all these things are are involved in that so it's just it's kind of it's it's an attempt at

from my perspective, we're actually combining many of these interdisciplinary perspectives into a single equation there.

And so active inference, I think, I appreciate this attempt to try and do this kind of mapping of neurobiological substrates to actually interpretable cognitive theory regarding what is a habit?

How does that, what composes that in the brain?

What are goals?

What is planning?

What is action?

and then from there moving on to uh you know these kind of exemplar uh initial active inference uh models which themselves are highly relatable to to reinforcement learning so it's all to make the case that active inference i think it draws connections and kind of builds bridges between all of these uh you know these other disciplines in a way that is very rich and it's not so uh exclusive right it doesn't it doesn't stop conversations it starts them

so and then i also wanted to show like here's just a quick list of some multi-agent simulations with active inference that i think you know it heavily relates it is agent-based modeling you can open the textbook from a couple years ago agent-based modeling is there you know it's this isn't foreign to to active inference but i just i hope those in the agent-based modeling community can start to see

how strong these relationships are with active inference so um epistemic communities under active inferences this uh paper was um

know recreating agents who hold their own opinions uh about things and they're they're in social networks and so you can study how they share beliefs with one another and yet given you know a lot of these uh kind of formal principles and active inference seeing how that plays out we can actually see what the kinds of dynamics that we start to see on twitter and other social networks where you know opinion polarization begins to happen depending on uh how you know how the network is set up in the first place

So, you know, it's just, it's very interesting.

We also have active inference.

This is much, much more kind of a biological or zoological example, you know, where there are ants in a colony who have to adjust to changing contexts, such as changing food locations, and then they leave one another pheromones so that they can actually follow one another's trails, so to speak, to find the food and come back.

And then this is very recent, but we had another talk earlier today, but this paper with Connor Hines, who's one of the developers of PyMDP, which we'll be seeing shortly, he and Friston and others are developing further tools for doing active inference.

This gets at the question of nestedness and looking at how whenever you have a bunch of individual agents, what are the cases where their collective behavior actually could fit that behavior back to the individual parameters that each of the individual agents hold.

then again with social science like looking at agents who have distinct roles uh such as leader and follower where or it could be you know you could apply this logic to many other situations the idea is just looking at how agents who have their own distinct uh you know uh available actions and beliefs nonetheless uh you know whatever their they can kind of synchronize in their capacity to fulfill

uh you know a goal that they collectively have um you know so it kind of speaks to the idea of having heterogeneous agents with heterogeneous you know concerns but nonetheless with an umbrella collective concern they're able to accomplish their goal um this next one

uh rather long title active inference framework for collective parallel problem solving on nk landscapes this is uh you know i gave this at the tutorial a couple months ago um i just recreated uh so this was the point i was trying to make you can recreate a lot of these old traditional agent-based models you know

using active inference agents.

That's what I'm claiming.

So I'm not trying to say, oh, all of these traditional models are obsolete.

Not at all.

I think there's so many fascinating design principles.

I would love one day to recreate the Santa Fe Institute's artificial stock market, but with active inference agents, right?

We can do that.

So here I just recreated one where you have agents who are collectively trying to solve a problem represented on an in-case

landscape meaning that they're trying to solve a very complex problem where there are a lot of interdependencies in different sub parts of the problem you know it's like trying to uh trying to design a car it's like well you can have a great car but if you remove the wheels it's no longer going to really be the kind of car that you need right or you can remove the steering wheel right it's

you you need to have a certain combination of pieces there to have a proper solution so agents collectively do that you can study like well how well do they do that do they get stuck you know with a sub-optimal solution uh and then furthermore you can look at metrics like oh if we run this in different ways uh what is the connectedness needed uh to for them to reach the optimal

solution or otherwise.

And then finally, this is not an active inference paper, but this is just a reinforcement learning

you know based problem but i i included it here because i just want to make the point that this is this is focused on industry right this is focused on multi-agent settings where you can have them kind of work in a manufacturing firm that involves like making a lot of kind of rapid decisions in order to manufacture particular products how you could have agents kind of synchronize in a way that they can accomplish you know that goal and uh again similarly with

with traditional agent-based modeling, where you could swap them out with active inference agents, so to speak.

You might be able to do this, take that same approach with looking at how people are using reinforcement learning today.

I think it leads to very interesting results.

So I just, again, I'm just trying to bridge these kind of dialogues between people.

So now we'll move to LLMs as agents.

This, I just think it really needs to be addressed.

So LLMs have been increasingly integrated into a wide array of potential use cases.

I included some resources here.

There are just dozens on dozens of research papers that are being published.

that try to use LLMs in these kind of agentic ways where it's as if an LLM is an agent who supposedly makes decisions autonomously and so on.

Again, they're showing more and more sophisticated reasoning capacities.

They're having kind of longer context windows and output windows, meaning you can feed them even more information at one time.

release even more or output even more information at one time.

You can also upload your own text files, documents, write your own prompts for them to respond to.

So in that case, if an LLM really is an agent, then why not just skip all of this stuff with reinforcement learning, with active inference?

Why not go straight to using LLMs as agents?

Of course, I'm going to argue like we should not do that.

I mean, it would be irresponsible at this current moment in time in terms of LLM development.

I would argue that that would be kind of irresponsible unless you're just kind of making some kind of entertaining or fun app that uses LLMs or something.

But for something that calls for much more discretion, I would not do that.

So what are the biases here?

LLMs are trained on large corpuses of existing texts.

We all know this.

What that does is it kind of sediments a series of cultural, scientific, explanatory, rhetorical,

prejudice information, figuratively, the snapshot of the retrievable web scrapable internet, other digitized human and AI produced texts to which the LLMs internal parameters, which are black box parameters.

Now we have LLMs being trained to where they have seven plus billion parameters on the inside.

And then consequently, their outputs are just kind of reflective of this.

So a lot of these prejudices have been addressed, very obvious, certainly very unfortunate things that we saw with initial LLMs were like racist or sexist or otherwise kind of outputs coming from them based upon the materials they were trained on.

People are trying to find different ways of training them differently.

to get them to elicit the responses that we want.

But that's the thing.

What's crucial when working with LLMs is the prompt itself.

So I want to think, just put yourself in the chair facing a computer and you're using an LLM.

What's really going on?

You as the user, you prompt the LLM after it's already been trained.

You have your own purpose or goal in mind that the LLM does not hold for itself.

Quick mention, I'm aware that I'm doing some slideshow karaoke for anyone who's like, ugh.

But there's a lot of information.

I want to make sure I cover it thoroughly.

So the user must provide the context for this goal or purpose in their prompts in the hope the LLM will catch on.

less figuratively they hope that the llm will be able to predict the series of words and sentences which correspond in the user's final opinion to the answer the user needs or wants right and so uh after training the lm it can't actually learn what the user needs it's always in the user's opinion and we'll get more into that again it's just making the point the prompt itself is incredibly important this is kind of a

uh simplified uh ever so slightly exaggerated uh you know illustration of the kinds of things i'm talking about so if i go to an llm and say hi my name is andrew it says hi andrew it now knows my name how can i help you today uh i ask it help me prepare you know i have a certain goal in mind to tell it help me prepare an initial draft for a pro forma business model you know for an idea i have

Here are the following details that should go into that business model.

Then it produces it for me.

And today it can even release it as a text file.

And so I could click on it, download it, great.

Now what happens whenever our conversation history is deleted?

What happens whenever we exceed the context window that is repeatedly being sent back to the LLM every time I prompt it?

We have context loss.

So whenever that happens, the LLM, I say, okay, now please help me revise the file.

Please provide the file you mentioned.

It does not recall anything that came prior, right?

What is my name?

Your name is me, which is a slightly exaggerated example, but it just really wanted to make the point like,

it will use its reasoning where it's capable, right?

And confabulation is a kind of a technical word.

Elelins hallucinate in the sense that they can kind of reason things in a way that they end up coming up with false information, you know?

We have to be careful about that, right?

We have to be very cautious about a lot of these characteristics of elelins.

They effectively, I mean, I would highly relate it to someone who has anterograde amnesia.

It is where someone cannot actually recall what has happened in the recent past and nothing you tell it actually gets committed to long-term memory.

And so I've kind of crudely used the phrase like the LLMs are kind of like talking heads.

You have to repeatedly tell it everything in every prompt.

And what's going on when you interact with ChatGPT is that a lot of this conversational history is being stored in the site in some kind of cache.

and being resent and you're not seeing that process because they want it to be a nice user interface where you can more easily use it so recent agentic frameworks and workflows abounded recent recent research if the issues presented by limited context windows confabulation etc

still about ellen's with these conversational histories have been considered to be agents in various cases yet in order to try and overcome some of the issues that i've pointed out there are a variety of frameworks that are being developed you know so one idea is an actor critic framework you could have two or more agents you give them different role prompts you tell one of them oh you're a creative

scientist and the other one, you say, oh, you're a harsh critic and expert in literature on this scientific topic.

And then you can have the first agent try and innovate and come up with something.

And then the second agent critiques the former agent.

So you have this kind of way of like, oh, in order to better ground the information that one of the so-called agents is creating, you can use another agent to kind of critique that and

make it more uh yeah grounded to reality another way of doing this is building up databases of retrieve documents that is you could you could

It's an attempt to overcome the limits of context windows.

So what you can do is you can repeatedly query an LLM and any true information, as well as any information you provide yourself through your own documents, your own prompts, any kind of formal documents you want to submit.

You can kind of collect these in some kind of organized database.

and then you could query that database in a way repeatedly send into the context window here's only the information that needs to be said right now based on everything i have and then that database kind of acts as your ground truth um that that's a means of doing it it's rather sophisticated there are different ways of doing that we'll get to that soon

and finally there's a thinking or reflecting framework i'm just spending a second on that it's where you have an llm come up with things but then have it revise its previous responses very similar to an actor critic framework it's just you're viewing it as one one so-called agent doing this as opposed to two in all cases the prompts are still incredibly important

so what's in a prompt well uh there are many people who have now you know uh coined the phrase prompt engineering which is at this point has kind of become a field or even art in and of itself uh this is a recent recent systematic survey of prompt engineering uh that covers literally dozens of techniques uh for anyone who's you know interested in that

There are many ways of doing this.

And furthermore, some other problems include difficulties handling numerical or symbolic problems.

So intuitively, if you read two plus two equals four as a string, as opposed to being able to recognize, oh, these are two numerical values,

that need to be added via an addition rule to compute a further value.

It might not be tokenized properly.

And so that the LLM doesn't actually know how to deal with that, despite the fact that to you, it's very obviously just a mathematical statement.

um so llms are getting better at dealing with math much better actually but they still risk this kind of imprecision so if we want to actually incorporate llms into software applications especially those that have states right those that have real world consequences where you're actually employing them in any kind of like in-depth industry related setting in relation to what john was saying in any kind of ecological setting

situation where you'd want to really carefully take care of things, verify the information and so on.

You need everything to be much more precise and trackable than whenever you use LLMs in your application.

You could have your applications such as

You know, if you're developing this in Python or Julia or otherwise, you could have that code actually handle the math, which is something we typically already do, as well as any kind of like if you're able to hard code any kind of rules that you can extract out of the text and actually consider those to be kind of, you know, programmable.

and then you can have them in your program and have that logic be carried out outside of the llm and you can just have the llm handle the natural language aspects so kind of my proposal in this i don't want to spend too much time on this because we'll see some diagrams that actually represent the ideas here but uh you know i argue that we should be thinking about things like pre-structured prompts with

what i'll call empty spaces for anyone who uses python it's very simple you just create a bunch of pre-constructed prompts that are structured in a way that you can basically supply uh the prompt here you're

don't have to do this word for word but you're effectively saying here is what i need here is how i need it how i need your output to be structured here are my current you know thoughts or beliefs of what's going on give me what i need uh but they make it an f string to where you can have

kind of these empty spaces in between the prompt uh to where at any given moment in time say in an agent-based model you can send the most recent updated information through the f-string literals so it's like you're you have a really nice prompt that really gets that what you want the lm to send back to you but if you want to have this run algorithmically uh you know in a way that you let it run autonomously it needs to be

that the information in that prompt needs to be repeatedly updated.

Again, like reminding the person with anterograde amnesia what's going on now.

Then an hour later, what's going on then?

And so on.

um bringing it back to active inference agents we know they change their minds through perception they act to change the world they actually have capacity for learning including online learning learning in real time you know updating their internal model parameters which might be likened to synaptic learning or modulation of the brain uh you know updating their beliefs about the relationships between observations and states the efficacy of their own actions you know a lot of these other kind of cognitive mechanisms

that LLMs lack.

So to bring it back, LLMs, if they were like agents, I would say that they're like agents who cannot act nor learn.

They only observe and infer.

There's been one argument that they do act.

It's just they don't quite...

know that because they don't learn anything it doesn't quite matter and they can't really observe the outcome of their their actions like their action is they output you know words and sentences and that doesn't actually come back and impact them necessarily in any kind of tangible long-term way they're typically trained once very expensive process some people have tried to specialize lms by only training them predominantly on medical text to make a kind of a medical llm or

legal texts for legal LLM, but still we're stuck in this lack of online learning or something like clear preferences.

From a neurobiological perspective, LLMs have very broad yet immutable long term memory, and they're trained on a lot of information.

And then they have a highly variable short term memory.

Their context window, which if anything happens to that and it's gone, then they've lost all context.

And so I argue that we should think of LLMs as a language processing tool.

I think they're very effective at that.

They absolutely outperform us in terms of being able to feed them many, many texts and then quizzing them on those texts.

plain as day, they exceed us in this part of life.

They're so popular now and being considered for software applications.

And from this vantage point, we can envision augmenting

agents who learn and act such as active inference agents with something like the capacity for natural language is how i'm currently phrasing it so this is kind of my diagrammatic uh diagrammatic you know attempt at formalizing this for for anyone um these are a lot of the design principles that are going into our bio firm that we're developing

right i started developing this whenever i considered how could i have an active inference agent write a scientific paper by prompting an llm like on its own right so so each action of the agent was you know uh prompt the llm to find more literature and citations uh another discrete action is uh you know write or revise your abstract

you know and these other aspects of writing a paper and saying if it can kind of you know choose for itself what to what to write rewrite update so on and how would that work and so a lot of that thinking has kind of gone into this project

In this case, what you would have is for a user of the bio firm, they would be able to submit their own documents, PDFs, texts.

This is the grounding truth representative of the user's own preferences, their goals, what it is they want to accomplish.

That gets input into structured prompts, F-string literals, like the empty spaces.

That way, it'll basically take your inputted information and restructure it in a way that's needed to get the kind of answer and the kind of response that you need.

So the documents are sent to the structured prompt, which is then itself sent to the LLM, which processes it and returns your structured response.

Structured responses allows encoding for agent observations, meaning you could

In a very simplistic example, you could ask an LLM, is this a good idea?

Yes or no.

And that can be encoded as a zero or a one.

And then the zero or one can then be read up as an observation for a particular observation modality that an agent, a POMDP is equipped with.

That's a very kind of elementary approach, but I hope it maybe explains the idea for programmers who are interested in these kinds of things.

And so that observation is then taken in, and then the agent can infer states per usual.

It takes the LLMs feedback into account in its decision making.

And then we have expected free energy, meaning

know um the free energy computes condition on its preferences right so it's like saying oh the agent can take in an lms you know natural language based feedback uh to the user's initial preferences as elicited in natural language and the agent can use that feedback to

make decisions in order to follow through on its own preferences or goals, right?

So you have this kind of reciprocal relationship between the user and via the LLM vicariously to the agent and then back again as you can take the agent's current internal beliefs

You know, it elicited this policy.

It now believes this.

It previously observed this.

This can be put back into a structured prompt, which could then be sent to an LLM for a new structured response, and that might

you know that might be something that involves like reporting like the agent is you know in natural language reporting its beliefs about what's going on and then that can be useful to the user and then be sent to uh what we'll for now call a a knowledge graph apologies i just now realized that this is being covered um yeah

And so then this knowledge graph, we'll get more into that.

I was very happy to hear some other people making references to knowledge graphs during the symposium.

You know, it can be done in different ways, but a general sense of a knowledge graphs that this is kind of like your database of knowledge that you're gathering over time in terms of, you know, natural language based knowledge.

So it's kind of like you're storing the context of the moment, you're kind of storing the truth, you're storing the history of what's going on in terms of these agent-LOM interactions, you know, and that way the user has the means of reviewing for themselves in everyday terms what has played out during the course of simulation.

So that's kind of the design principle.

And then the knowledge graph, this is a way of kind of overcoming the limited context window issue.

so in a general sense uh and i'll mention i'm you know a lot of my thinking about the knowledge graph comes out of this really like very recent exciting work uh being done by marcus j bueller uh who works at mit right now um you know he's been trying to find ways of using llms to to you know um

and basically go over many texts in order to find patterns in the text that can then be you know kind of put together almost in this like a schumpeterian like innovation way of like uh just recombining materials that already exist in text but maybe humans themselves have not

yet made those connection themselves.

Oh, we know all of these many different things.

If we look at it this way, that actually leads to a new innovation that we could come up with.

You know, we just didn't make that connection before, but we did now.

It's like, oh, we can have our lens or highly sophisticated whenever it comes to going over large amounts of tax, having them make those kinds of discoveries.

So anyway, that that's the kind of where this is coming out of.

And then sources for updating the knowledge in the graph may include user documents, up-to-date environment or resource reports and agent beliefs, domain knowledge regarding your use case, LLM responses to structured queries.

We kind of looked at that before.

Business model as a shared narrative.

It doesn't have to be a business model, but it could be you have many, in the context of the bio firm,

we'll get into shortly um you know the idea is agents are working for a firm and so they have this kind of they need to have this shared narrative between them that kind of captures all the crucial core elements uh describing what are the operations of the firm what are the guidelines what are things that they need to attend to uh you know regulatory or legal standards um what are what are the

their current financial statements, what resources do they currently have available?

What are the perspectives of all their different stakeholders?

Being able to synthesize that information into kind of a singular document that's short enough, but nonetheless rich enough that it can be repeatedly sent back to the LLM to remind the LLM, this is what we care about.

These are our constraints.

These are our central concerns right now or otherwise.

so anyway uh i'm getting a little ahead of myself but just that's how we can start to develop a knowledge graph overcoming the limited context window by having a graphical structure to the knowledge you can keep it nicely organized and uh as we know with graph structures with having nodes and edges you can have along the the edges can kind of be the descriptors that relate

you know multiple concepts as nodes right so you can kind of traverse this knowledge graph to make connections between all the entities within it uh these are the visuals here actually uh daniel created these based on some particular lm uh structured prompts that that he ran and kind of analyzed um and so these kind of principles are going into it so by using pre-structured prompts and querying our knowledge graph

you know, we can repeatedly kind of communicate what is the situational context of what's going on in the moment, as well as here's the knowledge that we've accumulated so far that can help with problem solving during the moment.

This allows us to filter down only to the needed information and synthesize it, reducing the final amount of information, in this case tokens, which also if you're using the OpenAI API,

to use the most recent like state-of-the-art models where we're usually spending at least a few pennies per LLM prompt or well, it depends, but the point is by reducing the amount of information flow needed, you can keep the costs significantly lower as well.

And then also kind of quickly overcome that issue of potentially exceeding the limit of the context window.

And I, you know, I'm, I'm, I'm interested in finding different ways of describing this.

I'm a particularly a large fan of Yuri Vysotsky, a neuroscientist at NYU who does a lot of work on neurobiology and, and has even worked with Friston at a point in the past.

But so I included him to try and give a metaphor.

This is, this conceptually can be likened to theories of episodic memory in the brain.

For example, this, this one theory

that that hippocampus in the brain is something like a pointer.

So whenever you try to recall, you know, an episode in your life, you know, that involves, you know, certain many episodes, like in order to tell a story of what happened yesterday, starting from the morning, and then what did you do, you know, at noon, and what you do in the afternoon, so on, like the hippocampus is the kind of pointer that kind of kind of

or almost like a librarian that goes through your long-term memory to kind of pull out sequentially what's needed in order for you to tell that story or narrative in the moment, or from a kind of more industrial

or a modern data analytics example, you could think of this as querying a database, SQL or MongoDB, meaning it could be a structured or unstructured database where a report is synthesized via joining and querying only the necessary tables or catalogs of logged information needed for the report.

So again, it's this graphical structure that allows us to kind of more quickly get information we need without having to pull too much more than what we need.

So,

Now, this is an example of, I'm just currently phrasing this as like agent foraging and knowledge updating.

So it could be starting from the top left, we have our homeostatic agents.

John made a reference to them earlier.

The sense is that these are agents who have their own preferences for maintaining their own homeostasis.

They might be trying to maintain the homeostasis of their environment in the case of an agent who is designed to take care of an environment, which is what we'll see in the biofirm.

They'll have their own current beliefs.

They're being updated over time.

We saw kind of an image of the inner dynamics of what that looks like for a PLMDP earlier.

They might have their own goals, which again, in this case, would probably align with the idea of maintaining homeostasis.

Based on all of those factors, that might lead to some pre-structured prompt leading to something more specific.

So in this case, this is an agent who

wants to take care of an environment.

And this is an agent who specifically is looking at how to take care of the soil on a natural farm, like a land preserver or farm.

And so they need more information about the soil structure.

So they query, they prompt the LLM, water causal determinants of soil structure in farms.

This is like an initial, this is whenever you're first making the knowledge graph, you would want to have this

uh this knowledge at the very beginning of a simulation right so this is kind of building up your knowledge graph to start so you could ask it this you could tell it to cite your sources or you could integrate some kind of api such as semantic scholar or other apis to try and check if it's actually pulling from real citations again we have that risk of confabulation making the

and so we want to just kind of keep a check on that so the llm tell you a lot of natural language based information about soil structure the determinants different kinds of managed prac management practices that could be used to take care of it different kinds of conditions and relationships between the soil structure and other aspects of soil what's around it

uh all these different things and then from there we could kind of go back to the knowledge graph store this deconstruct this information you know ideally this would be we'd ask the llm to structure its output in a particular way and then we could send those different aspects of its response into different areas of our

knowledge graph.

So soil structure, determinants, management conditions, and then also track the citations, the knowledge graph, again, it's almost like a three dimensional structure in that you could add the time dimension that is you could timestamp when all this information is collected, as well, you know, so it depends on how sophisticated you want to make your your knowledge graph.

This

is the last diagram I have related to this concept, but I just want to include it.

This is what I mean by a structured prompt with empty spaces, right?

So we have that same agent from before.

Now we're sending a structured prompt.

I believe that, so this is pre-structured, right?

These curly brackets kind of denote like these are what are the so-called empty spaces.

I believe the, whatever your current environment variables are,

are currently here's what the agent's beliefs are but our goal is here's our current business model and our current constraints are here our current constraints what solutions exist to address this this is incredibly short and simplistic just for the sake of illustration but the idea is you could input you could you could keep these environment variables stored

in your local environment.

Python is storing and processing them.

The agent beliefs are being extracted from the agent.

You could reencode those with natural language.

You could include information like, oh, this agent's belief is representative of the quality of its environment.

So then this gets read as a sentence.

That's something like the agent believes with a

0.2 probability or a very low probability that the current state of its environment is good.

You could re-encode that.

So this is really a conceptual move to try and find ways of bridging the gap between qualitative and quantitative information.

At no point am I saying that all of this is the absolute best way to do this.

What I am saying is that this feels a little bit like frontier work.

The LLMs are bringing up the question of

how can we properly integrate integrate them into software applications and i think these are the kind of next steps in terms of our thinking of how maybe we should be approaching this so and then you can see again uh for the rest of the diagram we still have this communication we can continue updating our knowledge graph you can also pull information back out of the knowledge graph to send into the structured prompt

This can all be sent to the LLM.

And so the LLM could do, you could request it to return your response in multiple ways.

One way is you could tell it, explain to me in natural language what solutions exist to address this.

And then also ask it in the same move, also,

give me these encodings of your answer.

So it could be that the LLM responds some kind of natural language explanation of how to handle this problem.

But additionally, here is structured encoding requested denoted as follows, positive one approved recommendation to carry out available action six might be the case that your agent has an observation modality

Meaning it has like a sensory receptor for the LLM where a one corresponds to like an observation level of like a yes or an approval or go for it basically.

And then six is kind of like a second observation modality where the LLM tells you which one of your available actions you should take.

So this is just an attempt.

and trying to find ways to modify an agent's architecture such that it has the kind of receptivity to LLM outputs in a way that can go from natural language to actual directives over what to do given the current context and all the information you've asked of that you've provided to the LLM in the structured prompt.

Now we move to the biofirm.

So the bio firm, like this is a lot of the principles I've described thus far kind of play into this.

Again, a lot of work has been done on this by Daniel Friedman here in the talk.

And then a lot of initial kind of coding work was done by myself.

And then we've, of course, been under guidance with partnership of John Klippinger as well as David Lovejoy with First Principles First.

I really like a lot of kind of the media and writing and podcasts that they've been doing.

And so I would strongly suggest checking them out.

And it's been a pleasure working with them so far.

And so with the bio firm,

We actually have a significant amount of code largely built out and kind of organized by Daniel under the Active Inference Institute's own GitHub repository currently in a subproject biofirm.

We also have a whole other library that we'll look at soon.

But for now, this is kind of the foundational, the starting point of the biofirm tool highlights.

Right now, we're building active inference agents

as uh you know using pi mdp which is you know essentially has been has already been referenced earlier in symposium a kind of port of uh of spm the the matlab library developed by you know friston and others um and so uh you know it's been a really good library to to kind of work with and quickly start trying to implement some of our ideas the open ai api um

many of you who have worked with LLMs and tried to integrate them into code, like they're already familiar with this.

But basically, it's a way of quickly being able to prompt within your coding environment, a wide array of LLMs, including the most recent ones, and you know, the new chat GPT, or Lama, or otherwise, models, Claude, Sonnet, and the cursor, which, you know, this is not necessarily essential,

to developing the bio firm but uh it's just uh daniel has kind of opened my eyes so to speak to to the wonders of kind of integrating four developers um being able to have uh an ide where you're editing code and then you can use an llm to help you write that code and obviously you always want to go back and look and make sure that it wrote the code that you actually intended but uh it can it can

you can dramatically reduce the number of times you need to hop into, say, Stack Overflow, for example, if that makes sense to programmers.

So it's great for productivity.

And then also some major things Daniel put out these these live streams that cover a lot of the development of the bio firm so far.

Both of them are huge.

I mean, we're looking at a couple of hours or so of material and we have close direct code walkthroughs today.

Originally, ideally, I might have gone through some kind of code walkthrough.

One, I don't think we're quite there yet in terms of being able to... We can demonstrate the principles, but in order to have something that's kind of like within the context of, okay, we're doing the Active Inference Symposium and we kind of need to keep things streamlined, we're not quite there yet.

But you can get a very big picture of what's going on in that code and get an actual code walkthrough by having a look at these live streams that Daniel put out.

So the bio firm.

Excuse me.

Essentially, in a bio firm, agents are working in a firm.

It's like setting the scene to maintain the homeostasis of an environment via farm, natural land, preserve.

And it could be many things.

But for now, I'm trying to kind of constrain to kind of an example.

agents prefer both in a colloquial sense they want this but also in a sort of bayesian priors computational sense to observe themselves maintaining their environment home stasis their own survival and health in this sense are directly linked with environmental

uh survival and health mapping internal states to inferred external states meaning their own internal beliefs are you know they're trying to model their own beliefs uh directly as you know kind of these these proxies for what's going on outside of the environment so they're very directly trying to map themselves to their environment all that being conditioned on preferences again what do they prefer homeostasis right so they they're trying to model their environment

internally for themselves while simultaneously trying to keep that environment within some kind of homeostatic range we'll get to a more concrete example what that means um but for now we can just think of it as like um you know that an agent's own homeostasis involves realizing his preferences relatable to how humans burn act to maintain their body temperature oxygen and blood flow since it's safety security whether it be physical or or economic um

So in this case, minimizing their own uncertainty, conditional preferences, maintaining homeostasis is equivalent to minimizing uncertainty about if the agent is successfully keeping the environment in a homeostatic state.

That's kind of a one sentence, concise way of looking at that.

They're minimizing their certainty about if they're able to successfully

maintain the environment homeostatic state uh from there we uh you know ideally what we would do is move on to building a knowledge base for example a kind of knowledge graph uh which itself would in an initial setup phase would involve uh users submitted documents uh so for our example case we actually did submit both a real

document uh that john supplied uh related to uh this this farmland uh kind of natural preserve that he himself has taken care of of course it's been redacted um but uh the the point is that where this gives us a kind of real world example that we are quite familiar with

contain in a document that we can use for an initial testing use case.

And then you can also do other things.

You can send in specific kinds of structured queries, asking for more domain knowledge or information about, say, how do we take care of a farm or otherwise.

and we have all of that and we follow that up by uh yeah structured prompting another idea we have that i won't go too much into right now but one idea is that you could actually extract out of natural language potentially like syllogistic reasoning based rules

uh that there's a you know a researcher axel constant who's done some work uh in the legal domain and looking at uh kind of bayesian uh probability and graphs and relating that to syllogistic reasoning i think that would be a really nice way of concisely extracting natural language rules turning them into something kind of like symbolic you know objects in python that can be used as formulae for

for doing different kinds of reasoning that increases precision and reduces the need to make so many LLM calls and having the LLM do that, but it's a thought.

Anyway, in any case, all of this, the whole development of the knowledge base and knowledge graph is motivated by agent preferences and beliefs.

so social science and i don't want to get too far away from that um you know i've been talking about a lot of design principles but i think the bio firm itself as currently it is an open source uh you know sort of resource it's still heavily under development i'll get back to you know the aspects of what's going on with that a little bit later

But currently there's a lot of development being made and is being written in a way that we ideally in the future would like this to be adapted to different circumstances.

It might be the case that this could be quite useful actually for continuing social scientific research, the form of an agent-based model.

You could consider that these might not necessarily be agents in a firm.

They might not have a business model.

They might have something else they might have.

know a constitution uh you know that that links all of them or otherwise um for now i'll continue working uh focusing on the bio firm but i just want that to be born in mind that i don't want to present this as if it's a highly specific use case that you know doesn't go that doesn't generalize you know allow for generalizing beyond itself um but for now we'll stick with with uh focusing on it

um so social science we have agents who are collaborating uh in coalitions of agents and networks of agents uh we have uh attempting to say that we can augment agents with natural language state belief reporting uh with the consequent development of a cultural uh and this is dr clippinger's view you know if we have agents who are motivated to take care of land and then they collect

uh knowledge that that corresponds to them and then they're using and communicating using this knowledge you know then this kind of becomes something like a culture so we're looking at something like a kind of cultural grammar or or set of scripts that are being developed

via these agents taking care of the land around them.

And we have shared narratives.

And then on top of that, we also, in a realistic scenario, we would have agents who are doing economic resource management and potentially trying to generate some kind of value in some kind of way, basically replenishing or regenerating

sources available to themselves or otherwise.

And then on value generation, and this comes back to the very start where I referred to utility as being kind of like what was previously the penultimate sort of incentive

for agents in traditional agent-based modeling.

Now, if we shift that to where now we're looking at expected free energy as the thing to be minimized, you know, and in correspondence with kind of the principles back then, then what we're doing is that we're looking at, you know, like the approximation of uncertainty condition on preferences.

Agents are trying to minimize that value

where there uh with value generation a whole nother thing we could do is look at each of the agents marginal contributions to the minimization of this quantifiable metric uh that is uh expected free energy uh

for the whole firm, like look at their marginal contributions to minimizing the uncertainty of the firm.

And then that itself becomes a kind of value, you know, as we can accumulate that view, that is kind of like uncertainty reduction is what generates value.

So that crucially this accumulation, it's not some arbitrary

floating value whose own value changes based upon market speculation dynamics or otherwise.

Instead, the moment-to-moment value generation that occurs during the course of a simulation of a biofirm, it reflects the agent's actual learning and capacity for taking care of their environment as proxied by expected free energy.

And then the kind of broader multi-agent setting and future vision for the bio firm, their cultural aspects, again, as I mentioned earlier, this kind of an ever evolving accumulation of shared knowledge.

Again, we could have timestamps.

That also means that we might need correction of that knowledge over time.

We'll talk about that more shortly.

uh anyway the the evolution of the knowledge graph it's motivated by mirrors agents motivated to accomplish collective goals for homeostasis from various perspectives um you know you might have agents who are specialized in one way agents in another you might have multiple stakeholder uh perspectives that are being fed into that initial knowledge graph or or along the way during the course of the simulation uh ethical uh ideally it would um

it would exhibit these kinds of principles, like by accumulating different perspectives between agents, between users, stakeholders, you know, in the case of bioregional modeling, where there can be very, very many people potentially involved in looking at or being affected by how an environment is being cared for, taken care of, or managed or governed.

know being able to actually submit these these potentially conflicting or at least different uh perspectives from various stakeholders those could be submitted to the program could also include financial regulatory or sustainability related knowledge um you know that can allow for comparing and synthesizing various perspectives again in some

some kind of after critic framework.

As I mentioned earlier, like a lot of methods have been used to try and overcome the issues of LLMs.

And so we'd be remiss not to employ those in various ways where appropriate.

And that way we can better ensure that the information that's being used and processed and being fed into agent decision-making and so on, taking care of an actual environment, we can make sure that it's much more accurate, coherent,

also identify faulty or contentious information if the agents come up with a very aggressive policy you know prior to its implementation could be critiqued and revised from an ethical regulatory or legal standpoint you know all this is collective

You know, we can kind of synthesize the information.

I already touched on this earlier, resynthesizing core information or primary information, the knowledge graph, and there being some kind of summary business model, constitution, set of protocols or missions that the agents have.

And then

sustainability, resource management, you know, making sure that agents are aware of available resources, combined with quantitative analysis.

So you could, you know, if you're looking at a real world setting, you'd be looking at things like price information, forecasting supplies needed for a particular policy might have qualitative reasoning involves why, like which supplies are needed for a task, what purposes within what guidelines.

um that's something i've been thinking about as well for a related institute project uh called farmworks uh we might talk about that later we'll see um and then heterogeneity agents agents might be pre-configured or

learn they necessarily will learn over time assuming we implement learning and their architecture more than likely well it depends on how and for what purpose and then they might even you know evolve so to speak to specialize in different skills and knowledge uses allowing for agents to collaborate under broad share goals while working on subtasks so there's some nice stuff

We were joined by Mal yesterday during the first workshop.

So Mal's been involved in a lot of this previous work.

And then, you know, another reference to kind of the, as well, the industrial example that I gave earlier as to how we might think about some of these things.

Basic form of a homeostatic agent.

This is, just take a second.

homeostatic agent.

This is the general just kind of quick natural language summary of its internal architecture.

It's incredibly simplistic.

This is sort of like a sort of a kernel of what is the initial formation of a homeostatic agent prior to any additional modification or changes in its architecture that you think might be required for the situation at hand.

For now,

Let's say there's a single environmental variable.

What we did, and I'll talk more about it later, but the idea is that we have an environment that we've, an artificial environment we generated.

We generated it based on the documents I mentioned earlier, including John's farmland document, as well as particular prompts submitted to the LLM to kind of compose what are the initial makings of our knowledge graph.

And it came up with

soil structure as being one of the essential uh variables that should be included included in our environment and then we have an agent here who's kind of mapping itself trying to map itself to

this quantitative value.

It is a question right now of, you know, I can imagine those who do active inference, like the question might be, why use the POMDP?

Why would you not use, you know, a continuous time, continuous state space model?

For now, we're going with POMDPs in part because we're assuming that we'll be using agents to model highly complex environments.

where it's not so simple as to kind of follow a simple up or down scale or say that the variable is necessarily being defined by some kind of Gaussian distribution and being sampled from it.

It might be the case that this is much more complex or might be changing conditions or otherwise.

And in case for initial testing environments, which is also a rather quick thing to do, I will

admit so um but for now uh the idea is that you'd have a few mdp who can view the data over time at each time step uh daniel ran this one for uh actually i don't remember firing that he probably did uh over ten thousand times steps um and so at each time step

uh the agent observes if the current value is above or below some homeostatic range which the agent prefers it fly in right and so it infers the hidden state of the variable is it actually if i observe low is it actually low uh if it's in its homeostatic range is it actually in its home homeostatic range or is it actually in reality is it really high or otherwise

uh you know and uh that another question as to whether this is necessarily a partially observable environment since this seemed to be reflective of real data but again this allows for the space that there might be very complex relationships between soil structure and let's say uh moisture levels in the soil or or micro bioactivity within the soil or other kinds of things that

that really matter here to where it's much more complex than just being able to say oh just because this went low that means i need to try and bring it back up as much as possible really fast right so there might be more nuance and having more sophisticated architectures might be able to to kind of parse through that problem including increasing uh inference from

horizons or or including more other variables that it tries to map and then actions that agent can increase maintain or decrease the variable uh right so again very simple a little more details later on on how that can how that can work but as just say straightforward it's kind of like the agent is trying to balance out this metric um given that a lot of it

all of its fluctuations, aside from the agent's actions impacting it, are coming from its relationship with all the other environmental variables in its ecosystem.

So it's kind of like the agent is trying to map its internal understanding of what's going on with the environment's behavior.

And then the agent, of course, prefers to see this within its homeostatic range.

And so it will choose actions that it believes will help

this variable to get back into its homeostatic range so from there we can view this like as a whole ecosystem of variables so this is this kind of defines our environment that we're looking at right in an agent-based modeling setting we have our homeostatic agents and then we have our entire environment composed of all these environmental variables

over in a real world setting whether you're running a firm a farm or otherwise you know each of these variables could be mapped to you know an iot device uh they could be mapped to you know some kind of uh logged uh observations you know that your team makes as to what's going on with the farm right now um

I guess I'll bring up FarmWorks again briefly.

We've been looking at a lot of farm data lately that's been collected by farmers and getting some inspiration on how they go about doing those things.

But I will say that this environment that we have here is entirely artificially generated based upon inspiration from text.

So if we think about this as something like agents are trying to solve a non-equilibrium steady state, what does that mean?

I'll try and summarize by saying that we're trying to maintain the structure and properties of the farm.

What is it that makes a farm a farm?

Well, if your soil moisture falls dramatically too low, and then suddenly all the organic matter in it dies, and then many of these other things start to fall apart,

well now we're not quite looking at at at homeostatic uh soil anymore right so we kind of have like this this natural decay or dilapidation of the system the ecosystem and so agents are having to repeat like continuously uh you know kind of actively maintain uh through control inputs meaning through their actions uh they're trying to keep these variables within homeostasis and these

it's worth noting these these target ranges homeostatic ranges might change themselves over time right due to uh you know we can imagine um the the target soil temperature range might change to adjust for you know seasonal crop change um it also might also account for certain kinds of like exogenous or external shocks to the system the impact of climate change there might be a natural disaster that occurs or otherwise that itself can really change the dynamics required

to attempt to maintain the ecosystem uh as it is or as it was or if it needs to kind of change in order to continue existing given uh what kind of shock occurred

So to maintain these desired homeostatic states, the agent must continuously work against these natural tendencies towards dilapidation.

I've already mentioned this many times.

These states, these variables may have complex interdependent relationships.

They call for agents before looking at inference horizons.

you know of course if we over maintain the soil uh you know put too many products in it or do other things that might still be okay for the crops it might harm local biodiversity uh you know local animals uh accidentally you know consuming different kinds of uh um

um you know materials that were that were used to take care of the soil and suddenly they die that harms local biodiversity and yet uh biodiversity might also include uh pests in the environment that themselves then harm the soil or crop growth so it's just being able to kind of adjust to all of these highly complex variables and crucially in reality we might only be able to impact some variables a second

some variables and not others.

Thus, agents might specialize in handling one particular controllable variable and study its relationship with the other variables leading to actionable, predictive planning.

Another kind of everyday explanation of this is like,

You know, I mentioned I'm a data analyst.

I work in education, and so I'm frequently looking at a lot of data regarding students and kind of our institution and so on.

I'm based in Chicago.

Some things that we can change are bus routes for our students to make sure that they can get to school safely.

Something we cannot control is the amount of crime in Chicago, which anyone in the US knows is

that would be very difficult to do right so so so we're looking at there are all kinds of factors in our environment that we cannot control and there's what we can and then trying to find how can we control what we can control in ways that kind of can balance out with all these things that we cannot control right so that it's very crucial to be able to identify and then measure and analyze these kinds of relationships whenever trying to design any kind of concrete actionable uh policy

right or intervention that you might want to make and then uh john referenced the commons earlier right eleanor ostrom's work um homeostatic agents solving non-equilibrium steady states the ever ever solving the commons so so i'm trying you know i was thinking about a lot of these principles and ostrom was required

reading for me as someone who's coming from a social sciences background.

And so it's part of why I find this project so interesting and why I find it fascinating to try to realize a lot of these things and principles involved.

So we have local rules, conditions, adaptation.

I mean, the agents necessarily have to adapt to the environment around them.

If you provide some kind of kernel agent and

and give it some space to figure out what it is it needs to do it's necessarily going to have to adapt right so so we have that attention towards the local um recognizing uncertainty and adaptation you know agents will have to take care of homeostasis manage resources through learning uh to to change conditions in broader unobserved uncontrollable environment in the sense of um you know there are many things going on that the agents cannot directly observe

um there are other things that will occur that were unexpected you know so and then also there might be relationships within the variables themselves that's for you know for example there might be some kind of critical threshold that's hit by one variable that then has some kind of domino or cascade effect on the others you know so that's why i include both the phrases endogenous or exogenous shots um you know being able to to watch out for those kinds of dynamics

then governing the commons.

That's sort of the benefit of if it can be done right, if it can be done successfully, integrating LLMs in a very thoroughly considered way that allows for accurate information accumulation and sharing and then using it for implementing policies.

You know, that's the aspect that kind of keeps things grounded and then gives this kind of natural language based way of dealing with an environment.

You know, this kind of just repeats many things I mentioned before, but we're synthesizing a lot of info from various stakeholders, various domains that are needed to handle the problem, repeatedly grounding agents and common goals from maintaining the environment against perturbation, capture, maintaining its Markov blanket.

and then nested collaboration potentially so just as users can practice oversight by running these models um agents themselves you know you might have agents who are at another layer so to speak who maintain their own oversight over other agents or coalitions of agents and

those kind of hierarchical agents might themselves act as something like an observer role.

They might recognize a broader issue going on in the firm that isn't being accounted for by specialized agents.

And so you can have those kinds of kind of hierarchical specialized structural roles

that allows for this kind of nestedness in how the environment is maintained.

And then a very interesting idea that I was gonna bring up later, but in the context of like looking at bioregional hubs or something along that line, I mean, if you were actually employing something like a biofirm to a real world scenario, then I don't see any reason why you couldn't have some way of interfacing between different bio firms, right?

Because if they're actually playing out,

in reality and building their own knowledge graphs you know say in real time in some kind of way then then you could very much have that kind of interfacing you could have you know structured prompts pulling from the knowledge graph of one bio firm you know sending that information to another bio firm and back uh you know so so that's you know i don't want to get ahead of ourselves but because the vast majority of the actual concrete work has been done on the bio firm

has only been over the course of frankly the past couple of weeks i mean longer than that for sure in terms of of john developing the ideas conversations we've had over the past couple months but whenever it comes to developing this stuff and actually realizing it it's still quite new so it's very exciting to report biofirm for users let me just do a wrap okay

um i'm gonna yeah yeah i'll speed this up a little bit um file from free users yeah uh one you know for for for a user's perspective first you'd submit your documents as i mentioned many times uh you know your own pdfs real world information uh you know if you want to submit textbooks you know regarding uh agricultural techniques and methods for taking care of farms you know including formulae that describe relationships or something

You have, from there, the direction is to have, there are two intended kind of potential paths you could take.

Right now, all the testing is being done with the first path, but the idea is to move it in the direction where you can have both available.

First one is for being able to explore counterfactuals.

So we generate synthetic data using domain knowledge or historical data.

That's what we're already doing.

And then second, direct application, that would be where you're actually able to send in data and have it be streamed and have this play out in real time.

This is very much, you know, kind of in the future, but that's sort of on the roadmap, so to speak.

And then once you followed one of those paths,

you could generate an environment for your agents.

And then also you could construct applicable agents, meaning agents to your design or ones that kind of fit the situation or be mapped to particular environmental variables, as we saw with the agent who takes care of the soil structure earlier, using AgentRanker, which is kind of an entire addition that Daniel kind of zoomed through, being able to incorporate, included, there's more information on that in the second live stream that was released.

From there, you would simulate your agent-based model, meaning there'd be an initial setup phase that takes in your initial documents and so on, constructs the agents, constructs the artificial environment, then runs your action perception loop.

It keeps things moving.

Data and LLM responses are encoded as observations for agents.

Agents take these observations, take care of their environment,

we already know homeostasis for the agent based on how it's set up is kind of hard-coded into its design and mapped to the homeostasis of its environment and finally there would be obviously we would want this analytics so having some kind of analytics suites that allows you to derive insights from different kinds of metrics that are being recorded over time there might be different kinds of like llm summary reports and recommendations that actually extracts the metrics and

you know, through a pre-structured prompt, as I've said a million times during the course of this, you know, using pre-structured prompts, you can input a lot of these results and then have an LLM kind of put it into a natural language, like just explain for you in everyday words, here's what happened, here's what's going on here, what the agents believe, and so on.

Bio-regional modeling, I want to look at this really quickly.

This is

a lot of Daniel's work on, I mean, this is just an initial step, but what's happened is that Daniel set particular kinds of structured prompts.

You can find more information on this in the first live stream, but basically asked about information in very particular ways.

to llns and and did it programmatically was able to extract incredible amount of information about these different regions different counties and uh he started with california and then moved to new england and then it's expanded from there and so we have like maine and massachusetts so for example uh massachusetts uh hampshire county

like you can uh he sent in like you know generate some kind of uh supply strategies statement um we have copies of these this is a json form meaning it's kind of uh set up in a way that can be read by the program

then here's just a more readable legible you know every day for a user reports like here is you know your supply chain strategist report on the ecosystem of that county uh references to what's going on in terms of its biodiversity how climate change is impacting it how land is used habitat fragmentation water quality so just a lot of the different things

that are going on specific to each of these counties.

You can do that from a market analyst's perspective as well to try and consider different kinds of opportunities.

What's the current economic landscape?

What kind of regulatory environment compliance requirements are there, state, federal, local?

Of course, these are currently very short descriptions.

Ideally, over time, you would build out your knowledge graph, your knowledge base, so you actually

you know kind of zoom into each of these send more follow-up prompts like tell me more about this you know cite your site the information you're pulling from um

then you could even consolidate that so this is the kind of a consolidated uh report you know very long for all these different market analyst perspective ecological perspective supply chain strategies and just kind of finding ways to to synthesize a lot of um information in one move this was a business case so i just wanted to show quickly like here are you know ideas of you know

you're defining out potential revenue streams, risk assessment mitigation strategies.

Five-year financial projections, of course, we could pull this kind of logic back into the program, have local computations done that actually comes up with much more

grounded realistic and precise information based on available resources that you have or that that at least artificially the agents have if you're doing more of an artificial setting um so yeah it's just it's making all these linkages so um you know that that becomes sort of the basis that the starting point for for our um for the bio firm so we

We then, again, we generated this artificial environment.

All we did was send in user documents, structured prompts through an LLM, give a structured response.

Let me quickly, yeah.

This allowed programming our environment, its dynamics, configuration, generating descriptions for each of the variables that get stored in this nice JSON structured file ecosystem config.

you you saw previously like i was looking at a github repository we like we're really building up the code in that in that base um and so anyway that sticking back to the artificial environment here this allows us to pull out yeah what are our variables or proxies for variables such as indices like it can be quite difficult to measure biodiversity so we make an index for that there might be other ways of doing that there might be more specific ways of doing that that fits your own use case

or otherwise we could, you know, tailor that.

Domain inspired quantitative relationships between the variables.

You can even, you know, with more manual control, you can set the noise levels for each of these.

Like if you assume that there might be some noisy process,

from a more mathematical or statistical perspective, which of these are actually controllable or not.

For those which are controllable, how much are they controllable?

Can you dramatically change biodiversity in one move?

Probably not.

you could and i don't want to follow that thought further biofirm development performance benchmarking uh this is another crucial part of the biofirm development being able to actually benchmark how our active inference agents uh perform relative to you know some kind of benchmark baseline such as having a more reactive or random sort of thermostat style like reactive um

control so basically on the left we don't have active inference agents it's just trying instead this is a more basic uh uh control strategy that kind of just reacts to the data it's seeing uh and of course the day is getting very out of hand i'm not saying it's not getting out of hand with the active inference agent but much more so that's what we're seeing from initial test case and

This active inference agent on the right is still that basic kernel homeostatic agent.

It is keeping its target within range 21% of the time relative to the random strategy, which is only seven.

The next steps would be to actually fine tune and modify the agents to better match the situation.

So far, it's very simplistic.

It's only looking at one variable.

tracking one variable has it's currently not taking in any information from an lm it's currently not taking in any information about the other environmental variables um this was really interesting to see though that already has this capacity for kind of outperforming the the reactive

model you know we've had some instances where the active inference agent doesn't uh you know it maybe matches the reactive model so it's just you know it's prime time to to step up the agent development game you know so that this was just step one it's like can we can we build it you know we yes we can so now let's improve it

um analytics suite we can look at you know the the variables themselves the correlations with one another we track the metrics about the environment over time and compare how the environment behaved in different simulation settings um you know that's what we want to do with agent-based modeling in the first place typically you want to be able to run simulations look at the outcomes based on different circumstances and then we can compare you know how this went using different agents um

And we could use LLM integration to translate this into a more readable report for users.

This is a whole other aspect I really wanted to emphasize.

I've made this kind of as an additional module so it can be edited

kind of after the fact, like outside of the main program, which also means that, you know, you could you can modify this bit as you want.

But basically, the idea is you try and measure the surprise or uncertainty or expected free energy or otherwise of the environment here.

um and and um basically look at how the actual variable diverges from where you want it to be you know the difference uh you know it's of course it's mathematical and it's drawing from active inference in other areas you know bayesian

statistics and mechanics but um the idea is you can measure how far away it is from where you wanted it to be in terms of surprisal uh you know actually capturing kind of this more rich or nuanced information about how that variable moves versus where you want it to be and then you can kind of flip that say oh well in a non-equilibrium steady state system where

more usually than not, the system is going to have a tendency towards generating more and more surprise.

That is, the complex relationships between the variables is going to lead to things typically going off course very quickly if you don't touch them.

Then in that case, any move at reducing surprise, meaning any move at combating that natural tendency towards kind of like chaos or entropy,

can be viewed as a form of generating value.

So this is the kind of additional metric we could add saying that there's a kind of value added to having agents minimizing surprise, meaning minimizing the free energy, meaning minimizing

the distance between homeostasis and where the data is now.

So we could see that kind of accumulation over time.

And again, it's modularized, meaning for a user that could potentially experiment as they like, they could change how this value added is computed.

I think that'd be very interesting for people who are interested in economic simulations or doing this differently or want to see how does this work.

and also shapley value analysis we have shapley values so um in the course of you know these simulations we have agents trying to solve or take care of their environment and so in the case of uh shapley value computation what it's doing is that we can compare how do certain coalitions of agents run through the simulation compared to one another uh in terms of uh

know these different metrics and so you can see like oh agent all five agents together uh you know did better at maintaining their environment in homeostasis uh they did better than only having um you know these this other particular coalition of agents but they actually did worse altogether versus when you only had these two agents working together right so it's a very kind of sophisticated way

at looking at how agents are actually solving the problems and actually carrying out actions that they need to carry out.

And we can do this for different metrics.

We can view that as that can be another way of saying this is how value is being generated by these agents.

Right.

So being able to make those kinds of analytics and then final

Yeah, these are the last couple slides.

GNN and AgentMaker.

This is very cool.

This kind of like, you know, we did so much rapid development at the bio firm over the past couple of weeks that this kind of like hit me.

I was like, oh, I don't, I didn't really think about that.

But Daniel has incorporated along with his co-authored work with Jack Smigel in the past.

They've worked on what's called general notation reputation.

And it's a way of translating active inference agents, meaning models or architectures, between different programming languages, including being able to compose them using natural language, and then also being able to visualize them graphically.

It's not quite there yet, the makings for being able to have this kind of translatability.

uh you know the ideal would be being able to write to an llm like hey make an agent for me that can do this and this and then by giving the ellen certain kinds of contacts that can fill in the matrices and and so on as you need it of course

we're still a distance from that, but it's kind of building the framework towards moving towards that.

And that way, you can make agents who are much more tailored to the situation.

And furthermore, ideally, you'd be able to translate them between programming, which is meaning, you know, being able to switch your applications and Python or Julia or otherwise, being able to have that done for you.

And then Daniel has basically forked the PI MDP repository, and then

built agent maker on top of it.

So right now it's kind of tailored to PI MDP.

But it could be used in other ways as well.

And so we already have some next steps on the kinds of agents we want to make.

For me, it's agents who should be able to observe the whole environment.

but only be able to control their own respective part.

To me, that's what would make the idea of having a shared common goal amongst heterogeneous specialized agents.

That's what would realize that.

So that's kind of the broader view of the bio firm.

You start with inputting your documents, your texts, your PDFs, data for fitting, not

excuse me, that's optional right now, it would be more ideal for a real world setting.

But for now, just generating an artificial environment, just the text is fine, we've been able to accomplish that so far.

And then from there, you could fit agents to the situation, or at least maybe customize them yourself using agent maker, you have the start of the your knowledge graph or base being developed.

And you have that ecosystem

all of its dynamics and configuration kind of spun up.

Then after that, you just run the simulation, looking at analytics.

We want to move towards having an interface in future, having dashboards.

We've already seen a lot of these graphs earlier.

And then we'd also want to have LLNs that could actually report the results.

And this is basically towards an adaptable kernel of a biofroom.

This is kind of the bigger picture.

have different kinds of design considerations a couple highlights we were we're looking at things like implementing potentially amortized inference we're kind of accelerating the inference process um using continuous state space uh predictions and models you know and then considering you know we we have an rx infer group that

at the Institute.

So we'd like to potentially move towards moving using a more reactive framework using Rx infer the Julia library as well as kind of complimentary library for environments.

The Rx environments library, plenty of different ways you might be able to use different kinds of API's or integrations with other tools, locally storing LLMs to mitigate the issue of having to necessarily like

uh use the open api open ai lm api

So, and then obviously integrating, you know, key aspects are like the resource management, right.

And that that's, that's actually, I would argue is going to be one of the easier parts of incorporating.

Cause it's like, just as we can currently read real data or generate, generate artificial data and have the agents manage that like soil structure.

We could do that similarly for different kinds of like, uh, you know, financial resource, uh, values or managing an account or something along those lines.


UNKNOWN:
Right.


SPEAKER_00:
So bringing it back, biofirm development testing is just a slide I showed earlier, but it's just to reinforce here some of the tools we use, some things we want to do in the future, some related institute projects related to, you know, we're working on projects that relate to mitigating future existential risk.

We've applied to a grant with Foresight.

There's the FLI grant project.

farmworks arts and fur group there's a new economic simulations related group some people are interested in looking at things like uh you know simulating currencies and and so on um in agent-based modeling and then those live stream updates um my computer is having uh repeatedly deciding to cover things up and that's it so uh

We, we went much further on time and send it.

There was a lot to, there was a lot to cover.

Um, really appreciate everyone's attention though.

Um, and you know, in these 11 minutes we have left, you know, if you guys want to give anything a shot, please go for it.

Oh, voice is going out.


SPEAKER_02:
Nope.


SPEAKER_01:
take some questions yeah anyone in the live chat can write a question John where where does this take you and or what will you plan to do tomorrow well I think what we tried to lay the foundation here in a very thorough way


SPEAKER_02:
is what is a biofirm and how is able to address many of the issues that you see in people trying to work out a bio

biofinancial to the bioregional economics and create the proper incentives.

And so I think what we're trying to do here is really show that there's a number of, when I started to say, here are some of the criteria that we're looking for in particular billing with Eleanor Osterman, the whole idea of how to govern a commons is a profound issue that,

They seem to be intractable to a lot of people.

I think this, in my view, this provides a framework for doing it, a scalable framework for doing it, a way involving people, a way of applying scientific methods and creating financial incentives and automating a lot of the data collection analysis task and how agents can build up their own competencies over a period of time.

So there's a lot here.

There's a lot to digest.

And it does provide a new metric of value that's based upon, you're not establishing value in the classic economics that our price arbitrage or information asymmetry arbitrage.

You're actually saying there's such a thing as a free energy principle, a scientific process, and we can show how the value is created and then how to allocate it to those people or those agents that are creating it.

So this is really sort of foundational.

So we want to take it to the next steps.

Um, and what we're going to be doing tomorrow is, is, um, actually there's a group of people who are experts in the sense that they work with, uh, how to take multi-agent architectures and apply them to, uh, in some cases to the, to the enterprise, uh, from Deloitte is doing that another group, uh, Simon

Uh, uh, Torrance is doing it in terms of looking at framework for agenda architectures.

Uh, there are others are looking at it and applying to the bio finance by regional finance issues.

Um, so we have that Susanna Troy.

We also have, uh.

uh we have matt maroney who's head of sustainable finance and metabolic and then there's uh john havens who's been involved with ieee establishing ethical frameworks for the application of geospatial models

So it's quite a combination of people.

But what we want them to look at is what's the impact of this when you're having fully autonomous biometric-based agents and what happens when you can apply them not just to the biometric, to the bioregional finance, but also the new notion of a firm.

And I think there's a real profound issue here.

What happens when you really have autonomous agents and you create institutions?

We have to create institutions that are supposed to be above people, but the problem is people populate institutions and they come captured.

But what happens if we start to put these agents and build institutions that are functioning this way, that are verifiable this way?

What does that happen to what we think about employment and jobs?

I mean, there's some really substantive issues here.

And how do you create this as a public resource, a public good that doesn't get captured by particularly small interests?

So it's a broad spectrum of questions we're putting out there.

I don't think that the public in general fully appreciates the capacity and the power of an active infants free energy principle as applied to economic and societal issues.

So we're going to try to tackle those issues and get people talking about it.

And this is a beginning, but we want to take it next steps and really get reference technologies out there that we can actually show and demonstrate and may be able to replicate.

that's that's the goal yeah and you're very helpful i gotta say you know yeah daniel kudos to you from all along i mean both you guys and daniel just you've done remarkable work and and and accelerating this process and we're very very grateful for that and what active inferences institute there's other things that the other sessions i saw here that seem to be very relevant to what we're doing so i'm really looking forward to collaborating

I'm quite fascinated with Thoughtforge, what Matt Brown is doing in doing homeostatic networks.

And it goes back to Ashby and the law of records of variety.

I think all of that's applicable to this too.

So there's this wonderful synthesis that you've enabled, you really have, and through a singular effort.

So again, extremely grateful for this.


SPEAKER_01:
Thank you.

It's been awesome.

lot more i could say but what what what can we're gonna we're gonna ask more of you i'm not done yet yeah you're not done no no we're gonna no the part that resonated with me in the presentation over the last two days of biofirm and how andrew laid out all these pieces is like

on one hand we have that researcher lens again about similarities and differences and about treating it like it's an engineering problem and then i think john you've brought in the deeper history of commons and cybernetics and these more open-ended problems about how do we connect any technical capacity

to the embodiment and the co-enactment of systems that are livable and regenerative and that's a little bit of a different kind of topic that's one though that that uh mass convening and awareness can can fertilize and make real

in a way where the technical understanding, it's like, you can skim the textbook, you can read it for 20 years.

It's like, it is just going to be the textbook.

It's just going to be something to jump off of.

and take into another way it's not like some of these questions about how different regions will work it's not it's not in the 2022 textbook so that's applying active inferences taking the high road and the low road and variational free energy expected free energy like all of those formalisms

and then like surprise surprise applying them to what matters and figuring out that project and operational part and there's just like there's so many pieces i was just like thinking of the yarn connecting the the the needs to what we do and don't have at different levels of readiness that takes us back to the tech tree and all these other ways that we've discussed coordinating

around that balanced way to think about regional and what is federated what is transferable how do we deal with all of these aspects and it's um been a great journey over the last few months also


SPEAKER_02:
Well, thank you.

What I find so reassuring is that you see applying the method and then it starts to explain and you keep applying it and it doesn't fade out.

It doesn't become ad hoc.

And so I feel like we're doing something really principled that is extensible.

And when I come from the

biofinance community and see all the things that they've been trying to do in the commons community say actually there's a wonderful fit here and getting these two worlds together is is is really important it's a challenge too um but i i people want to do that and and they're looking for something like this and so i really hope that we can be able to create that marriage as it works


SPEAKER_01:
To add one comment on that, I think when you opened today and said, well, we could make things fungible, we can connect, but that is also part of replicating, if not elaborating the problem, which is the fungibility of drinking water here and this calorie here and this unique species here, facilitating that fungibility

is facilitating the capture of the underlying resource so even if it were somehow trivial to make like assertions about impact monetized and the whole hyper certs and impact certification even if that were trivial and we can just assume that that we have full observability or something like that it could even accelerate financial capital

based cascades and so what what can be an escape grounding to that and that's where seem like whoa the models on through the paradigm of incentives are maximalist and reward oriented

can they be homeostatic or allostatic oriented to satisfy and be on paths that bounce prize rather than try to maximize but then as soon as we look around the world there's things that don't make sense we're not trying to maximize or minimize like the reservoir but rather within a range and so there's so many things where we can just connect

first principles of intelligence and research questions to just like the calm subtle sometimes ways that that the technology because to to move too fast also would again replicate fallacies right okay well

Good luck with the session tomorrow.

And I hope many people come join and engage in these fun, open source, meaningful projects.


SPEAKER_02:
This is just the beginning.


SPEAKER_01:
It's just a checkpoint.


SPEAKER_02:
It's a strong one, and I'm very pleased with it.


SPEAKER_01:
Cool.

All right.

Thank you, fellas.


SPEAKER_00:
See you later.

Okay, guys.
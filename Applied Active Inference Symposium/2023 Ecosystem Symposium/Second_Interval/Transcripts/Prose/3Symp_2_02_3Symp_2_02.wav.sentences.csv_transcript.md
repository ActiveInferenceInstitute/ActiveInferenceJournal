
00:08 _Daniel:_
[[start:8140][end:19570]] And it's a great segue from collective behavior in surprise minimizing agents to collective behavior in surprise minimizing agents, I guess.
[[start:19940][end:21200]] Welcome, Conor.

00:21 _Conor:_
[[start:21700][end:22112]] Hey.
[[start:22166][end:25984]] Sorry, I just turned on my audio, so I just heard the last thing you said.
[[start:26102][end:26432]] Yeah.

00:26 [[start:26486][end:27810]] Hey, welcome.
[[start:28420][end:29570]] Yes, thanks.

00:30 _Daniel:_
[[start:30060][end:33268]] Um, we're looking forward to your presentation.
[[start:33364][end:38788]] Multi agent active inference and multiscale alignment, current developments and challenges.
[[start:38884][end:43400]] So feel free to share your screen or proceed however you prefer.

00:44 _Conor:_
[[start:44060][end:44804]] Great, thanks.
[[start:44862][end:52056]] And before I start, because I've had issues with my voice saturating, like my microphone saturating, how's my audio?
[[start:52088][end:53036]] Is it clipping at all?
[[start:53058][end:54444]] Or does this sound okay?

00:54 _Daniel:_
[[start:54642][end:56704]] This is good and I'm watching it.

00:56 _Conor:_
[[start:56902][end:57888]] Okay, perfect.
[[start:57974][end:58850]] Thank you.

01:00 [[start:60980][end:62690]] I will share my screen.

01:06 [[start:66020][end:67328]] Share this one.

01:07 _Daniel:_
[[start:67494][end:69072]] The one thing I'll note oh, yes.

01:09 _Conor:_
[[start:69126][end:69500]] OK.

01:09 _Daniel:_
[[start:69590][end:75280]] It's just JF symbolic implementation does not use statistical distributions.
[[start:75440][end:80748]] It uses the symbolic and the logical inference.

01:20 [[start:80784][end:88004]] And now we're going to move back into the distributional space, and it will be awesome to see similarities and differences.
[[start:88132][end:91480]] So thank you, Conor, to you for the presentation.

01:32 _Conor:_
[[start:92860][end:101740]] And thank you, Daniel, for the introduction and for inviting me, as well as big thank you to the other organizers of the Third Applied Active Influence Symposium.
[[start:102560][end:105340]] Yeah, I'm really happy to be here to present.
[[start:105490][end:105804]] So.

01:45 [[start:105842][end:107144]] My name is Conor Heins.
[[start:107192][end:110940]] I'm a PhD student at the Max Planck Institute of Animal Behavior.
[[start:111100][end:115872]] And I'm also a researcher at the Versus AI research lab.
[[start:115926][end:117090]] R D lab.
[[start:117780][end:123300]] So I'm going to do something a little bit, I guess, unconventional for people in my position.

02:03 [[start:123370][end:126112]] Like, I'm a junior researcher coming to the end of my PhD.
[[start:126176][end:134648]] So usually when I give a talk, I would present on my own research, like what I've been up to for the last ten years working on.
[[start:134734][end:154764]] But instead of that, I'm actually going to talk about given the motivation of the symposium, I'm going to talk about something that's more of an overview or perspective on the current state of the field in multiscale active inference or multi agent active inference and what, in my opinion, we need to do to move forward as a field.

02:34 [[start:154882][end:161852]] I think that's very resonant with the kind of motivations and the title, indeed, of this symposium.
[[start:161996][end:167808]] So I'm going to give a general analysis of what multiscale active inference is, why it's important.

02:47 [[start:167974][end:184760]] I'm going to provide a brief analysis of its formal basis as it currently stands, and then what we need to develop in this kind of subdiscipline of active inference, multiscale active inference, to really make it rigorous and really to actually reap the benefits of what it promises.
[[start:186220][end:214050]] So generally, active inference has been used a lot to design agents that can solve problems, plan, and just generally emulate behavior that we deem intelligent, which includes things like risk sensitive decision making, intrinsic motivations to resolve uncertainty, and finally, from a more scientific standpoint, the ability to furnish a process theory about how biological brains actually might work.

03:35 [[start:215140][end:233348]] But in a lot of the theoretical work on active inference from the last ten years or ten plus years, really, there's also alongside all the kind of practical building adaptive agents, there's a claim that active inference is inherently or intrinsically multiscale from the very get go.
[[start:233434][end:235044]] It is a multiscale framework.
[[start:235092][end:237610]] It's not just about building single agents.

03:58 [[start:238380][end:249192]] So it's really whenever we write down a single active inference agent, what we're implicitly implying is also a nested hierarchy of active inference agents both below and above.
[[start:249336][end:264028]] So colloquially, you'll often see this in papers as the idea that there's Markov blankets all the way down Markov blanket is a statistical structure that's very kind of intrinsic to the definition of agents as they are defined under active inference.
[[start:264044][end:265904]] So I'm not going to get into defining that.
[[start:266022][end:273004]] I'm kind of assuming that there's a more disciplinarian audience there, but I'm sure other talks, for instance, can provide better clarity.
[[start:273052][end:277428]] So yeah, Markov blanket's active inference all the way down, all the way up.

04:37 [[start:277594][end:292100]] And at any given scale, crucially, the free energy minimizing dynamics, or the active inference dynamics are kind of claimed to be aligned with or parallel to the free energy minimizing gradients at the level below and above.
[[start:292260][end:305756]] So the claim is that as agents are doing their thing and doing active inference at one level, it both entails and is constrained by active inference processes of the macro agent that they're participating in.
[[start:305858][end:311440]] So I'm a cell that's part of a tissue as well as the micro agents that comprise them.
[[start:311510][end:317650]] I am a free energy minimizing cellular agent comprised of organelles that are also minimizing free energy.
[[start:318440][end:327920]] So this kind of constrained neat nested gradient descent on free energy is part of the story of multiscale active inference.

05:28 [[start:328000][end:334372]] And it also crucially, assumes that these dynamics are aligned, correlated cooperative across these different scales.
[[start:334516][end:342484]] So I should mention that there is a formal argument made more recently, I would say in the last five years, about how this is possible.

05:42 [[start:342622][end:347852]] And it relies on an apparatus from statistical physics called the Renormalization group.
[[start:347986][end:362188]] This basically allows you to analytically identify shared symmetries energy and conservation laws at different scales in a given system that's comprised of subsystems and subsystems, so on infinitum.
[[start:362284][end:380128]] So there's a formal argument specifically made in a free energy principle for a particular physics monograph in 2019 that applies the renormalization group apparatus to multivariate stochastic differential equations that are kind of the equivalent of agents.

06:20 [[start:380314][end:390628]] So you can apply that framework to certain sorts of coupled stochastic differential equations that exhibit Markov blanketed sparse coupling structure.
[[start:390724][end:401064]] And you can kind of prove analytically that there are going to be nested systems of Markov blankets and that they're all in some sense minimizing three energies at their own scales.
[[start:401112][end:411170]] So I'll get more into that argument later, but I just want to mention that as I define multiscale active inference that there is a formal argument that's related to it.

06:52 [[start:412180][end:418268]] So this slide I just put together to demonstrate the idea of nested free energy minimizing processes visually.
[[start:418364][end:430752]] So at a given scale, we can think of an agent as occupying some point in its free energy landscape indicated by this red orb, which represents, say, its configuration, its beliefs, and its actions.

07:10 [[start:430896][end:435272]] And it performs active inference and in doing so minimizes its free energy.
[[start:435326][end:438388]] So it changes the position of that ball on that landscape.
[[start:438484][end:440788]] And that is all we mean when we say active inference.
[[start:440884][end:449560]] That corresponds to the agent doing inference and doing action and kind of getting to the fixed point of its local free energy landscape.
[[start:449720][end:453544]] The multi agent case is simply when we add more of these processes.

07:33 [[start:453592][end:460848]] So there's other agents usually assumed to be similar agents, and the word similarity, let's put an asterisk on that.
[[start:460934][end:464880]] And they're all sitting at different points in their own free energy landscapes.
[[start:465540][end:469270]] The position of their local red ball is maybe in a different place.
[[start:470760][end:494232]] So the claim of multiscale active inference is that as we link these multiple active inference agents together, so they can actually exchange information like observations and actions with each other, what we automatically get is some kind of superagent that is also minimizing variation of free energy and is in some sense an emergent or Supervenient active inference agent.

08:14 [[start:494366][end:505240]] And I say the word, we automatically get a superagent with an asterisk because there may be some conditions on that mapping from local to global that have to be elucidated.

08:25 [[start:505400][end:507648]] So we'll come back to that in a bit.
[[start:507814][end:518540]] But in short, I think the definition of multiscale active inference is very eloquently put in this paper by Rafael Kaufman, Pranav Gupta, and Jacob Taylor.
[[start:518620][end:522516]] I think Rafael Kaufman is actually going to be on the panel later.
[[start:522698][end:528580]] And this line from their paper is a really nice, I think, just summary of what it is.
[[start:528730][end:530448]] So I'll just read it out loud.

08:50 [[start:530544][end:544536]] The upshot is that in theory, any active inference agent at one Spaciotemporal scale could be simultaneously composed of nested active inference agents at the scale below and constituent of a larger active inference agent at the scale above it.
[[start:544638][end:550108]] In effect, active inference allows you to pick a composite system or agent A that you want to understand.
[[start:550274][end:569316]] And it will be generally true that both that agent A is an approximate global minimizer of free energy at the scale at which that agent reliably persists, and that agent A is composed of subsystems A, sub I that are approximate local minimizers with free energy.

09:29 [[start:569418][end:583396]] So that is the claim as I'm going to continue evaluating in this talk, and I think it's just a great reference point to make, okay, that's what multiscale active inference is.
[[start:583498][end:584504]] Why is it important?

09:44 [[start:584622][end:586104]] Why do we actually care about that?
[[start:586142][end:590424]] That sounds philosophically nice and beautiful visually, but why is that important?
[[start:590542][end:596248]] So there's a ton of actually really important implications of this, both for the engineering and the natural sciences.
[[start:596424][end:608396]] First of all, the namesake of this symposium, I assume inspired by this recent paper by Karl Frison and all about enacting ecosystems of parenthetically shared intelligence.
[[start:608508][end:611404]] So this is the third applied active inference symposium.

10:11 [[start:611452][end:616684]] So to really make this resonate with the applied aspect, let's make this very concrete.
[[start:616812][end:634628]] If we can figure out this multiscale endeavor, then we can actually engineer distributed systems of multiaging intelligence where local agents, in doing their own little local active inference processes, are also cooperatively instantiating a global agent that's also performing active inference.

10:34 [[start:634724][end:650168]] This has huge computational potential, of course, compared to kind of the state of the art predominant methods for artificial intelligence, which are deep learning, which really is about propagating global information through an entire computation graph.
[[start:650264][end:658336]] So although you could argue back propagation is local in some sense, it's really not local in the way that multiscale active inference progresses to be local.
[[start:658438][end:669300]] So if we can figure out how to actually engineer multiscale active inference, it will have really tremendous implications for the study of artificial intelligence just from that pure engineering standpoint.

11:09 [[start:669640][end:670896]] It'll be cheaper.
[[start:671008][end:674320]] In one word, it'll be computationally energetically, memory wise.
[[start:674400][end:674864]] Cheaper.
[[start:674912][end:675940]] A lot cheaper.
[[start:676440][end:688340]] Secondly, from a kind of more natural sciences motivation, which is kind of where I'm coming from, I'm doing a PhD in biology, so I'm interested in questions about actual real systems in nature.

11:28 [[start:688500][end:699484]] Just the idea of being able to get super specific and rigorous about phrases like emergent intelligence, emergent computation, collective intelligence, superorganism that's often thrown around.

11:39 [[start:699522][end:701516]] We're talking about social insects, right?
[[start:701618][end:709852]] These are terms that you hear thrown around in many different scientific disciplines that deal with multi agent systems, network systems.
[[start:709996][end:714796]] But none of these terms, to my knowledge, have really rigorous or precise conditions.
[[start:714908][end:721712]] Multiscale active inference is a kind of framework that's in the position to provide those rigorous definitions and conditions.

12:01 [[start:721776][end:730396]] So from a scientific standpoint, it could really lend a lot of potential and usefulness for other scientific disciplines.
[[start:730528][end:732788]] And finally, another pragmatic motivation.
[[start:732884][end:745940]] There's loads of fields that are obsessed with designing and engineering systems where local, selfish individual behavior can lead when networked appropriately to some desired collective outcome.
[[start:746020][end:749480]] And these disciplines really want to figure out how to engineer that properly.
[[start:749560][end:757170]] So this goes from the design of financial markets and trading systems all the way down to how do you design a multiplayer video game?

12:38 [[start:758100][end:760364]] So that's kind of just motivating.

12:40 [[start:760412][end:763010]] Why is multiscale active inference even interesting?
[[start:763460][end:768436]] So then the question, of course, becomes, is the multiscale active inference claim actually true?
[[start:768538][end:776950]] Are all multi agent active inference systems comprised of and themselves comprise nested hierarchy of free energy minimization agents?
[[start:777320][end:789028]] A glance at a smattering of other scientific disciplines that specifically deal with multiple agents, multiple interests, collective phenomena like coordination, group behavior, collective intelligence.

13:09 [[start:789204][end:792988]] A glance at all those disciplines would naively suggest that the answer is no.
[[start:793074][end:807200]] So there's things like frustration in thermodynamic systems game theory, the very existence of zero sum games and mass equilibria bandwagon effects when we're talking about social networks and opinion dynamics.
[[start:807780][end:818480]] Sacrifices for the common good, which we see in different contexts in biology, like in the context of kin selection, but also in the context of arguably cell death in a tissue.
[[start:819300][end:828752]] These are all basically plenty of systems where local constraints and global constraints or desires or free energy gradients, whatever you want to call them, come into direct conflict.

13:48 [[start:828896][end:838356]] So the obvious example that I listed at the top of these bullets is the idea of geometric frustration that we see in Icing systems with very low temperature.

13:58 [[start:838468][end:849372]] So these Icing models basically describe lattices of Ferromagnets that are happy when they're pointing in the same direction as their neighboring Ferromagnets and they can be in an up or a down state.
[[start:849506][end:853068]] So basically the magnet can be pointing up or pointing down.
[[start:853154][end:860880]] And these global systems are defined by a global energy function and the whole system is in some sense trying to minimize that global energy function.
[[start:861030][end:869804]] But sometimes you'll find cases in these collective systems where this little spin in the middle cannot be happy because they're getting conflicting information from two neighbors.
[[start:869852][end:876676]] I want to be pointing up in blue like the agent on the left, but I also want to be pointing down like the agent on the right.

14:36 [[start:876778][end:891108]] So this is a system that's collectively finding some fixed point of its global free energy, but it's actually leading to a local conflict where this agent is not at a point where it can do anything to make itself happy or minimize its free energy further.

14:51 [[start:891204][end:905150]] So just from even the basic glance at Ferromagnetic lattices, we already see instances where local and global gradients or local and global optima are not aligned in the right way.
[[start:905600][end:910656]] So given all this, the burden of proof for multiscale active inference is still on us.
[[start:910678][end:917260]] So we need to show that collective active inference systems generically do align again across scales.
[[start:917340][end:928470]] And maybe if we can put an X across the word generically and it's not some automatic condition, if they don't, then at least we have to establish exactly the conditions in which they do.

15:30 [[start:930140][end:934040]] So anecdotally we do actually have some conditions.
[[start:934460][end:940632]] There seem to be some kind of basic ingredients to get collective active inference to work.
[[start:940766][end:947544]] So one is that we basically need agents to exchange actions and sensations across some kind of Markov blanket.
[[start:947672][end:949224]] This is not really a condition.
[[start:949272][end:952220]] This is almost more part of what it means to be an agent.

15:52 [[start:952370][end:963184]] So having Markov blanket separation between agents is just another way of saying we have multiple agents in our system rather than a single agent if you're violating the Markov blanket property.
[[start:963302][end:969904]] So internal states of one agent are not allowed to see the internal states or external states of another agent.
[[start:970022][end:973188]] Then you're kind of cheating because you're kind of saying it's actually one agent.
[[start:973274][end:977476]] And what you're doing is information sharing within the brain of a single agent.
[[start:977658][end:990120]] The second condition, which is something that's often hallucinated more anecdotally and not really formally, is this idea that agents need to have some kind of shared narrative or shared hidden states or censor space in their generative model.

16:30 [[start:990270][end:998764]] So I've worked a lot on collective active inference systems, just simulating agents and trying to get them to do interesting things together.
[[start:998882][end:1002556]] And my intuitions and experience do agree with this basic fact.
[[start:1002658][end:1011180]] If the agents don't have any similarity in what they're representing or trying to achieve, then it's kind of like trying to fit a square peg into a circular hole.
[[start:1011260][end:1027860]] So this is really nicely elucidated in one of the earliest cases in this paper, a Duet for One by first and Fritz in 2015, where they show that for two agents to really align, they kind of have to have a shared generative model and then you can get kind of this nice synchronized behavior.

17:08 [[start:1028280][end:1031284]] Again, though, these things like what does similarity mean?

17:11 [[start:1031322][end:1033136]] What does a shared narrative actually mean?
[[start:1033178][end:1036376]] Formally, mathematically, those things have not been initiated yet.
[[start:1036398][end:1056216]] So right now, a lot of the building of these collective systems is based on our intuitions and kind of engineering things using some vague guidelines like, oh, they should have a shared sensor space, but there's no mathematical conditions or guarantees about what degree of similarity is needed between two agents models to get the intended dynamics.
[[start:1056408][end:1068048]] And finally, we have to have at least some agreement between the generative model of each agent and the generative process, which is really the behavior of the other agents generating their data.
[[start:1068134][end:1072736]] So this is kind of related to the previous point about having shared generative models.

17:52 [[start:1072848][end:1086708]] But just to be very specific, the physics of the space that transfers your actions to my observations that physics can't be dramatically crazily different than how our generative models represent those physics.
[[start:1086804][end:1098456]] So if we took two fish with the same generative model of each other and they normally would school together in a fish tank, but we throw them in a volcano or shoot them out into outer space they won't.

18:18 [[start:1098488][end:1099100]] School together.
[[start:1099170][end:1107432]] Because then the generative process is so dramatically deviating from the way they are representing that physics, the way their generative model is constructed.
[[start:1107576][end:1111052]] So these are, again just ingredients, kind of guidelines or anecdotes.

18:31 [[start:1111116][end:1113836]] But there's nothing really rigorous behind these conditions.
[[start:1113948][end:1116080]] They're more like a list of best practices.
[[start:1116580][end:1119056]] So now let's get on to actual rigorous stuff.
[[start:1119158][end:1129184]] So, the first real rigorous attempt to show that multiscale active inference generally works is in one section of this free energy principle for a particular physics monograph from 2019.
[[start:1129312][end:1142884]] So it leverages this apparatus I mentioned earlier, the Renormalization group operator, to basically show how one can successively coarse grain multivariate stochastic differential equations that admit sparse coupling between their state variables.

19:03 [[start:1143012][end:1157516]] So the main result in my mind that connects these renormalization group results to multiscale active inference is the fact that the Lagrangian of the system at one scale can be expressed as a function of the Lagrangian at other scales.
[[start:1157548][end:1167852]] And that applies in a scale invariant fashion that is the main kind of output or the main benefit of using a renormalization group apparatus.

19:27 [[start:1167996][end:1171236]] So you can kind of think of Lagrangian like the generative model.
[[start:1171418][end:1178420]] It's a physics term, but it's related to the generative model of the agents that comprise the system and therefore also their free energy.
[[start:1178570][end:1192712]] So in terms of active inference, it means that this reasoning of the renormalization group can be used to smoothly move between the models of individual agents at one scale and the model of a collective or larger agent at a different scale or a smaller scale for that matter.

19:52 [[start:1192846][end:1203550]] And the nice thing about it is general for all kinds of dynamics and thus generative models, it doesn't depend heavily on the form of the stochastic differential equations that form your system.
[[start:1205120][end:1210796]] The issues with it is that there's not a global link to Bayesian mechanics and active inference.
[[start:1210828][end:1213804]] It's still all done in the traditional physics formalism.
[[start:1213852][end:1218684]] So we don't actually have an explicit link to local inference and global inference.
[[start:1218732][end:1224800]] Although if you know the connection between the Lagrangian and the generative model and the free energy, then you can make that connection.

20:24 [[start:1224880][end:1228848]] But it's not actually made explicitly for us in this part of the monograph.

20:28 [[start:1228944][end:1236852]] It also requires the assumption that the generative model and the generative process are identical at the local level that's related to how the Lagrangian is defined.
[[start:1236916][end:1238436]] That's also restrictive assumption.
[[start:1238468][end:1240532]] That's probably not realistic in my opinion.
[[start:1240676][end:1246036]] And then finally, there's something about kind of spatiotemporal segregation of scales.

20:46 [[start:1246148][end:1258172]] So we need to make assumptions about how fast random fluctuations are at one scale relative to another scale in order to justify kind of coarse grading or forgetting about certain states as you move between scales.
[[start:1258236][end:1260732]] And that's also something that you could argue.
[[start:1260796][end:1269670]] Current research into collective dynamics challenges that assumption about how fast noise is at one scale relative to the next.
[[start:1270680][end:1279632]] So now I'm going to discuss quickly another small contingent of active inference research that is attempted to address this mapping between local and global inference processes.
[[start:1279776][end:1291668]] So what I want to kind of just generally say with this presentation and to our community is that the kind of approach taken in these two papers, first of all, active inference model of collective intelligence and spin glass systems as collective active inference.

21:31 [[start:1291764][end:1298648]] This is one of the types of research I think we really need to move active inference, multiscale active inference forward.
[[start:1298814][end:1305112]] So I'm not trying to be too biased because I am the first author on the second of those papers, but I'll also be the first one to point out the limitation.
[[start:1305256][end:1315100]] But benefits wise, I think these approaches are really important because they formally relate a local generative model at one scale to a global generative model at a different scale.
[[start:1315180][end:1319524]] So really tie, how do the parameters of one model relate to a course grade model?
[[start:1319642][end:1329140]] And these are really good steps in the direction of a formal theory of collective intelligence that goes from local intelligences to global intelligence.

22:10 [[start:1330200][end:1331988]] However, there's still issues with these.
[[start:1332074][end:1337876]] One of them is that they only deal with issue with inference at the global level, not active inference.
[[start:1337988][end:1348588]] So both these papers concern with a bunch of local active inference agents that cooperate to form a global inference agent, like a passive baying agent, rather than an active inference agent.
[[start:1348674][end:1354472]] And it's also unclear whether the systems studying these papers are actually very generic.

22:34 [[start:1354536][end:1362924]] Like the results are generic to studying collective intelligence in general, or they're nice formal arguments, but they're only applicable to these specific systems.

22:42 [[start:1362972][end:1370772]] So we still don't have something that's even more kind of zoomed out and abstract than these, which tend to be a little bit case specific.
[[start:1370906][end:1377652]] And finally, the actual scale transcendence that we're doing in these papers is still relegated to really one step.
[[start:1377706][end:1385720]] We're not doing the full multiscale infinite scale regression that something like Renormalization group promises.
[[start:1386380][end:1392148]] So that's kind of the current overview of what I think are the most promising directions in multiscale active inference.
[[start:1392244][end:1394844]] And I'm aware on time, sorry, five minutes.

23:14 [[start:1394882][end:1413580]] So I'm going to quickly try to go through what I think are really promising directions to push in terms of multiscale active inference and kind of intuition pumps that I think will help us study these systems in a way that's different and also actually better informed by other research disciplines.
[[start:1413740][end:1420308]] So the general idea that I'd like to put forward is that misaligned gradients can actually be a good thing.

23:40 [[start:1420394][end:1426528]] So it's actually sometimes good when local free energy gradients are misaligned with global gradients.
[[start:1426624][end:1432748]] So sometimes the global system will actually do better if the local systems are performing worse.
[[start:1432864][end:1445260]] So this is something you could call multiscale conflict, where the free energy minimizing processes at one scale are actually doing bad, quote unquote, but it's because they're being driven by some higher scale process that is doing well.

24:05 [[start:1445410][end:1462160]] So rather than trying to always avoid constructing processes like this, I think this kind of frustration, to use the analogy from statistical physics, can actually be an inspiration for what we should investigate further because it actually might be key to facilitating optimality at different scales.

24:24 [[start:1464260][end:1480768]] The reason I put this forward is because there's loads of research, just recent research in the last several years that are suggesting that actually making local agents more frustrated or more unhappy might coincidentally or not coincidentally lead to better collective or global outcomes.

24:40 [[start:1480864][end:1484448]] So this is expressed in various forms in various bodies of work.
[[start:1484554][end:1497224]] One of the biggest patterns I've noticed is the study of collective behavioral systems over the last several decades is the idea that local noise and local Dysregulation can often facilitate global coherence or global coordination.
[[start:1497352][end:1507452]] And where multiscale active inference has something to say, in my opinion, is in framing this benefit of local frustration in terms of a misalignment of free energy gradients.
[[start:1507596][end:1520016]] So it may be that actually temporary misalignment local free energy gradients from global ones may facilitate the descent to fix points in the global free energy landscape that satisfy everyone at all scales.

25:20 [[start:1520128][end:1527888]] So I'm basically expressing an idea that's been known in various communities like Stochastic optimization and Stochastic resonance theory for decades.
[[start:1527984][end:1538520]] But I think we as active inference practitioners have a new and potentially useful perspective to shed on that, using the language of active inference and free energy minimization and Bayesian inference in general.
[[start:1538670][end:1558590]] So instead of thinking of accelerating optimization by just adding noise to the system, we can think of exactly how to design local generative models such that there's an optimal misalignment of local and global gradients or local and global generative models in a way that facilitates everyone in the long run actually facilitating or minimizing their free energy.

26:00 [[start:1560320][end:1563184]] So yeah, that's just kind of like something I'm putting out there.
[[start:1563222][end:1566496]] I'm investigating it now in my own work, but I have no real results on that.

26:06 [[start:1566518][end:1573350]] But I just wanted to put that out there in this venue because I think it maybe will inspire other people to think in a similar way.
[[start:1573960][end:1585160]] So just to conclude, now, multiscale active inference, I would say, is still largely based on theoretical or philosophical descriptions and illustrious simulations, but we're still lacking a formal theory.
[[start:1585740][end:1602510]] There's some theory in terms of the renormalization group arguments of the monograph, but they're still, in my opinion, a bit underdeveloped, a little under demonstrated and relying on some restrictive assumptions like the fast and slow fluctuations, the identity between generative model, generative process.
[[start:1603200][end:1618956]] There's a few more recent papers, those two by Kaufman at all and then by myself on spingglass systems that have attempted particular proofs of multi scale Bayesian inference systems in particular situations, but their generality is still not known and not proven.
[[start:1619068][end:1633828]] So what I'm kind of trying to conclude with is by saying we need to incorporate findings from other disciplines related to the role of noise, conflict and frustration in facilitating, not subverting collective intelligence or collective coordination.

27:14 [[start:1634004][end:1642916]] And I think we can really benefit by looking at these other disciplines to help us build a really powerful formal theory of multiscale active inference.
[[start:1643028][end:1648856]] And finally, I think we need to set the goalpost for what counts as a formal proof of multiscale active inference.
[[start:1648968][end:1664720]] And once we get there, once we're saying, okay, this counts as proof, this is satisfying, how can we use that to actually do the hardest part, in my opinion, which is engineering actual multiscale active inference systems that are intelligent and minimizing free energy at multiple scales.
[[start:1666020][end:1669824]] Yeah, so with that, I'm going to conclude looks like I'm just on time.
[[start:1669862][end:1679700]] So, yeah, thank you again for the invitation to present, and I'd like to thank a bunch of people who are listed here and beyond who have influenced my thinking and kind of my opinions.

28:00 [[start:1680600][end:1683050]] If there's time, I'm happy to take any questions.

28:06 _Daniel:_
[[start:1686700][end:1687450]] Awesome.
[[start:1687980][end:1690104]] Great talk.
[[start:1690222][end:1694430]] I'll just give a few seconds if anybody wants to type in a question.
[[start:1694800][end:1695932]] Also, it's really cool.
[[start:1695986][end:1707080]] Like, aswin in the previous session was highlighting Pymdp and just the way in which we enact the collective intelligence.

28:27 [[start:1707160][end:1718192]] Different people seeing a paper where an analytical formalization is introduced, and then there's still so much work to get it to the package, and then so much more work to take it to the last mile.
[[start:1718256][end:1722100]] And I think your presentation really checked a lot of those boxes.
[[start:1722920][end:1730556]] I'll just read a question and then that will just be an appetizer for our continued discussion.
[[start:1730688][end:1741320]] So Marco Lynn asked, do you expect the inferentially connected dynamics to exhibit behavior akin to theories of multi body systems?
[[start:1741660][end:1746350]] And to what extent can we transfer insights from that multi body of work?

29:07 [[start:1747280][end:1761372]] And then second question, just for our thinking and learning from Marco, have you explored integrating work on self organized criticality with multi scale active inference or other frameworks?
[[start:1761436][end:1766940]] Who can provide more flexible frameworks or assumptions for a generic notion of multiscale dynamics?
[[start:1767100][end:1768130]] Great questions.
[[start:1768820][end:1774812]] I hope that we can continue, have you back anytime, or just continue to collaborate in the ecosystem.
[[start:1774876][end:1778880]] So thank you for the epic talk, Conor, and good luck finishing your PhD.

29:39 _Conor:_
[[start:1779300][end:1780404]] Thanks a lot, Daniel.
[[start:1780452][end:1781720]] Yeah, I'll talk to you soon.

29:41 _Daniel:_
[[start:1781870][end:1782550]] Talk to you soon.

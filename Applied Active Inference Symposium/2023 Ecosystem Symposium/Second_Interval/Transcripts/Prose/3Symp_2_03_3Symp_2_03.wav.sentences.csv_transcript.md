
00:06 _Daniel:_
Greetings.

00:06 _Bert:_
All right, well, our next session. Hey, Bert.
Greetings.
Great.
Hey, how you doing?

00:13 Good, good.

00:13 _Dmitry:_
Very well.

00:14 _Daniel:_
Our next session is with Bert DeVries, Dmitri Bagaev, and Bart Van Erp.
It's going to be called towards User Friendly Design of Synthetic Active inference agents.
And I know a lot of people are super excited to see this really practical and cutting edge work.

00:31 So to you, Bert, and just let us know how we can support.

00:36 _Bert:_
Okay, great.
Is my audio good?

00:40 _Daniel:_
Yep, sounds good.

00:41 _Bert:_
Okay, then I'm going to share my screen.

00:46 I hope I picked the right one.
I don't work with zoom quite often.
Looks good.
Looks good.
Yeah.

00:52 All right.
Super.
Well, thanks a lot, Daniel, for hosting this symposium.
I've been watching some talks.
It's really amazing.

01:02 And we feel privileged to get a chance to present ourselves.
So we are also, just like a few others before us, interested in developing a toolbox for active inference.
And so this picture, or she kind of shows what we're about or what we are interested in.
So here's a lady on the left hand side, and I'm going to try to get a laser pointer.
And she has this idea about a rewarding behavior for a vacuum cleaning robot, right?

01:35 So she's writing down she has a textual expression.
Move around the apartment, apply suction until the floor is clean, do not touch objects, and when done, return to the dog.

01:45 So that's not so hard.
I'm going to rate that with one star out of three stars in terms of difficulty level to specify that.
But that's not enough to program this robot, right?

01:58 Because what she really needs to do now is to specify a generative model and there's effectors and actuators, right.
The robot has to move around, apply suction until the floor is clean.
So there are sensors, probably a camera.
Do not touch objects.
Or maybe there has to be object recognition.

02:21 This is a really difficult task to come up with this generative model here.
And on top of that, she has to specify this kind of rewarding behavior in terms now of probability distributions of this generative model.
So very difficult.
I'm going to rate that with two stars because the next thing she has to do for this model is to specify the inference procedure to do actually active inference and free energy minimization in real time for this complex model.

02:55 And really that's almost impossible, right?

02:57 Only a few specialists can really write a procedure for variational free energy minimization in some very difficult model.
So what we are about what we've been working on is to try to automate the inference task.
So get rid of the three stars.
And yes, she will still have to specify her model, but in the long term, we try to get away from that.
So in the long term, we hope we will get a toolbox.

03:25 And now we're talking 510 years, right, where a textual description would be enough to specify some initial model with an initial prior and everything else is just automated inference, learning of states, parameters, structural adaptation of the model, even maybe based on her feedback updating the prior.
So that's long term for now, we would be very happy if we could just automate the inference task.

03:58 So why is it so difficult to specify inference for an active inference agent?
Well, we have so many competing KPIs, right?
We want to do this for large model scopes, not just for ABCD models, but maybe there's also continuous variables and hierarchical models, right?

04:21 It must be very user friendly.
We really don't want her to worry about robustness of her code.
We don't want her to worry about whether two variables have conjugate relationships adaptivity.
We want to update states, parameters, maybe even the model.
The model structure has to be low power because these ancients often run on edge devices, so they run on their battery powered has to be in real time because you can't learn how to ride a bike if there's no real time reasoning.

04:58 And on top of that, you actually want to minimize variational frequency, right?

05:02 You want to do it at least as good or at least in a neighborhood of if you would do a manual derivation.
And some of these decidorata bite each other, right?
If you want to minimize variational free energy, but you have to do it in real time and on low power that kind of bites each other, right?
So these are difficult KPIs that they're all important.

05:29 You can't just take one out because then the whole system wouldn't work.
So when you read papers on active inference, you often also read and now we implement variation of minimization and that can be done by message passing on a graph.
And I want to clarify first why it has to be done by message passing on the graph.
I do that by giving a very short answer and then do an example.
The short answer is that Bayesian inference involves computing very large sum of products, like what you see here on the left hand side.

06:13 Here's a product AC, ADBC, and then we sum them AC plus ad and so forth.
This is a sum of products.
Now, we know by the Distributive law that this here on the left hand side can also be computed as on the right hand side.
If I multiply this out, I get a times C plus a times D and so forth.
This is a product of sums and they're exactly the same thing.

06:40 The only difference is that to compute the left hand side takes four additions sorry, four multiplications and three additions.
To compute the right hand side takes two additions and only one multiplication.
So on the right hand side is much cheaper to compute than the left hand side.
Normally when we write down marginalization and Beijing inference, we write things down as in the left hand side what message passing does on the graph, it will automatically convert that into much cheaper to evaluate product of sums.

07:18 And I'll give an example of that.

07:23 So here is an example model F of seven variables x one, x two through x seven.
And this model happens to be factorized FA of x one, FB, x two and so forth.
Now, we can draw this factorization as a graph.
And what we do, and this is called a Farney style factor graph, what we do is for each factor FA, we allocate a node.
So FB gets a node and FC gets a node.

07:54 And we associate the variables in our system with the edge.
And an edge is connected to a node if that variable is an argument of that function.
So FC is a function of x one, x two, x three.
And that means that FC connects to the edges x one, x two, x three, and FD is only a function of x four.

08:19 So FD only connects to the edge x four.

08:23 So what you can see in this graph is this graph is nothing but a visualization of the factorization assumptions that we have for this model.
Now, if I'm interested in a big marginalization task and I integrate out over all variables but x three, so x one, x two, x four and so forth through x seven, I'm interested in this.
Then taking advantage of this factorization, I can rewrite basically this sum of product into a product of sums as below here, what you will see here below this computes exactly the same thing.
But I've made use of this distributive law.
For instance, FC contains no x four, no x five.

09:20 So I moved it over the summation sign to the left.
And FB also doesn't contain x four, x five, x six, seven.
So I moved it all the way to the left.
And when you do that, you are left here with an expression where I only sum over two variables and here I have to sum over six variables and here over two, and here over two.
So you can imagine if each variable, let's say x one, x two, if each variable has ten interesting values that you need to sum over, then I have here the original marginalization problem.

10:02 I have ten to the power six, so a million terms, and here in red I have 100 terms and here I have 100 and here I have 100.
So here I have 300 terms and here I have 1 million terms.
So it's an enormous reduction in computational complexity when we make use of this distributive law.

10:29 Now, it turns out that if you write this out, you can associate these intermediate factors with messages on the graph.
It's just an interpretation, a visual interpretation.

10:42 It's as if FC receives a message from FA and FB receive or FC receives a message from FB and computes an outgoing message MU x three.
And the same thing for Fe.
So Fe receives a message from its neighboring factors, FD and FF and computes an outgoing message, x three.
So what you see here is that the entire marginalization process can be represented as basically computing a few messages on a graph and multiplying some of these messages with each other.
And that's how you can do Bayesian inference and also how you can do variational free energy minimization.

11:30 So this works in factorized models, but I would say even stronger if your model is not factorized and you have a lot of variables, there is just no way you can do proper inference.
So any serious model is factorized, like the brain is almost sparse, is almost empty.
We have, what is it, about 10 billion neurons, and each neuron connects to a few thousand other neurons.
So if I would draw the graph, that graph is almost empty, it is hugely sparse.
And so there is no other way to do inference in the brain than by message passing.

12:12 So that's why message passing, just because it's more effective than anything else.
Now then the issue is which message do you compute?
How do you compute messages?
Because there are different ways of doing it right.
And we also read in active inference papers, you can do this by variational message passing, or expectation maximization, or belief propagation and variational LaPlace and all these terms.

12:41 It turns out that there is an umbrella framework for all these methods passing frameworks.
And that umbrella framework is called Constraint better free energy minimization.
And I will try to illustrate it by this slide.
So here I have this graph.
This is just an example graph where my generative model is basically factorized in FA, FB, FC, FD and Fe.

13:16 And I've also written that here.
So this now is the variational free energy.
Now, I haven't made any assumption on Q of X.
So Q of X is still Q of x one, x two, x three.
It's just a joint overall variables and it doesn't have any factorization assumption.

13:35 It makes sense to also assume that the posterior kind of follows the factorization assumption of the prior, namely of the generative model.

13:45 So if we make that assumption and that means we're going to make the assumption that QX is also now a product of QAS of X of A, where QAS of X of A stands for beliefs over nodes.
What I mean by that is that Q of B is a posterior belief over this node, meaning it's a posterior belief over the edges that connect to this node.
Just like FB is a function of x one, x two, x four, that's if you will, the prior or the generative model, then Q of B, the variational posterior for this node will also depend on x one, x two, x four, and on no other factors.
If you just do that, then you will count some of the variables double because x one is part of the belief over FA, but also part of the belief over FB.

14:46 So we just have to discount that by dividing by beliefs over edges.
That means that I make now an assumption that my posterior beliefs is divided into local beliefs over notes and local beliefs over edges over variables.
This will make things a lot simpler.
In fact, if my graph is a tree and I did the tree here, and I would do message passing on that tree and I could suppose I could do that perfectly, everything is linear gaussian, then I get perfect Bayesian inference.
There is no approximation.

15:28 So this is a good assumption.
Sometimes it's still very hard to compute a message because even the single messages that come out of these nodes, they're still integrals or summations, and in particular the integrals may be a problem.
We may not have an analytical answer.

15:48 So what we sometimes do is add additional assumptions.
We'll say, well, the posterior belief over FD, I can't compute it in general, but I'm going to just assume now that it's a gaussian that makes it easier.

16:05 Or we can make an extra factorization assumption and say the posterior belief over FB, which is really a belief over the joint x one, x two, x four is going to be broken into independent belief over x one and belief over x two and x four.

16:30 These additional assumptions, if I impose them as well, this is what I recall.
Now, if I all substituted here in Q of x, I get what's called a constrained beth free energy.
This is the same Beth as in the Oppenheimer movie.
This is Hans Bethe, where it's named after.

16:53 We have a graph now that is highly factorized and we have local beliefs over notes and over and they're indicated with red and we have additional constraints in green.
They could be Gaussians or mean field constraints or other constraints.
And now we will assume constraints that make it possible to compute all the messages.
And now we can just automate this by making different assumptions.
We can turn this into expectation maximization or belief propagation or hybrid forms thereof.

17:34 We can turn it into any relevant message passing algorithm that you've heard of.
So this is a very nice umbrella framework that basically encompasses everything.
We've written a pretty large paper on this in the Entropy Journal where you can read all the math on how this works.

17:57 So we've talked about why message passing, namely because it's the most effective way of doing inference.
And we've talked about which messages to compute, namely we turn our variational free energy into something called a constraint, better free energy and then we can compute messages.

18:16 The only thing that's left is, well, when do we pass these messages?
What is the sequence of messages?
Which one comes first?
And this is where we see a lot of papers, right?
You have to write control flow, what's called control flow.

18:34 You have to say, okay, here is my algorithm for active inference.
First I specify a model.
Then let's do inference for every time step, collect a new observation, update the state, update the desired future, and so forth, compute expected free energy, select the policy, et cetera.
This kind of program.

18:57 The problem with active inferences is that there is nested for loops in here.

19:02 Here's a for loop, and here's another for loop.
And for each of these policies, I'm going to have to go into the future, so I'm going to have another time loop.
So it is for loops, in for loops, in for loops.
This will completely explode in terms of computational complexity.
So as a result, some very clever people have written very clever algorithms of doing this much faster.

19:27 Sophisticated inference, branching time active inference, dynamic programming EFE are recent proposals for doing this very clever.
In the end, all of these proposals come down to a particular just a message passing schedule.
Once we commit to message passing on the graph as our inference procedure, it's the only thing that's going on.
And all of this sophisticated inference and branching time active inference, all it does is it schedules the messages.

20:02 It says first this message, then this message, then this message.

20:06 I don't mean that as a slight to these algorithms.
They're very clever.
And as we've seen in the presentation by Aswin Paul, you get huge improvements if you go from regular inference to sophisticated inference.
But it's good to realize that these algorithms just specify in a graph which message comes after which message.
So here's an example of a graph and a message sequence.

20:38 Here's message one, then message two, and message three goes up, and then we go from FC to FF, and here's message five, and then we go to Fe.
And this could correspond this sequence to dynamic time programming EFE or sophisticated inference.
There are a couple of problems with this approach, which basically with having the user to specify a clever algorithm, first of all, you have to be a specialist to do it right.

21:08 Only these are very clever people.
That means that if we let it leave it to say to an engineer in a company, well, it's a high probability he's not going to get it right.

21:22 That's very unfortunate.
But there is another issue, and that is that in a sense, it's a global variable in the message passing schedule.
All nodes are visited, because if a node would not be visited, then we shouldn't have it in the graph.
And that means if one node crashes, basically the message passing schedule is invalid.
I have to reset my system.

21:48 And if you fly a drone, if it's deployed and it's out in the field and a node crashes, a transistor burns out, and I have to totally reset now my system, I have to compute a new message passing schedule.
Then you're not doing inference and your drone flies into the wall.
So this is not robust.
And it also for the same reason we may actually want to take out a node.

22:15 We may want to prune a node, we want to do structural adaptation.

22:20 And we can't do structural adaptation because we have to reset the system, recompute a message passing schedule.
So this procedural style where an engineer specifies which message comes after this message has some disadvantages.
It's not very robust.
And if you want to do it very clever, you have to be really a specialist.
So a better system is what we call reactive message passing.

22:51 And it's very related to what was in the first session called the actor model.
Keith Duggar had a nice presentation on the actor model.
So what we will do is we will say we will not have a global message passing schedule.
The engineer will not specify anything anymore.
The inference code that an engineer will have to write is just say, react to any free energy minimization opportunity.

23:22 In other words, there is no inference code.
It's completely automated.
And we will replace this global message passing schedule by local triggering inside the node.
So each node is now just an autonomous system that's interested in minimizing its free energy.
It can do so by sending out messages.

23:46 And when will it do so?
Well, it receives messages, and then when it looks at these messages and it feels like, oh, there is an opportunity for me to minimize free energy by or expected free energy energy by sending out a message.
Then we'll send out a message and each node will do so by itself asynchronously so you get parallel distributed processing, or concurrent processing, as Keith called it.
In principle, you could play this game on many computers at the same time, and so you get tremendous advantages.

24:25 First of all, you don't have to write difficult code.

24:28 Second of all, you can do multithreading or you can run it on multiple computers at the same time.
And there's also robustness advantages because if a node crashes, then there's nothing that stops the system from just finding another path, right?
If this node crashes, this path from here's message three, this path now doesn't work.
So I cannot send anything to Fe anymore from X.
Well, then I just sent a new message here.

25:04 Why not?
It's like when water falls down a mountain and it zigzags its way down into the value and you halfway put up an obstruction.
It just finds another path, not the preferred path.
This has to find, well, the second best path, because the first path has been obstructed, right?

25:26 And that's what's going to happen in this system as well, right?

25:29 That's just how nature works.
It tries to find the best path, the easiest path, and if that's not available, then we do the second best path.
And that's also what you can do with reactive message passing.
So you can prune nodes.
You can do structural adaptation, and it's far more robust.

25:48 And you can also do chance encounters with other drones, right?
Drones that get close can start communicating with each other, and when they fire away, they stop communicating with each other.
And this is no problem, because you can basically change who change nodes can change on the fly, who they communicate to and who they want to listen to.
That's the way nature works, and also how it works when we do reactive programming and reactive message passing.

26:29 So, in summary, we're interested in automating inference, in active inference agents, right?

26:39 Because it's an operation that's basically only for experts.
And this active inference technology is not going to be successful unless we get more people, let's say, democracies it, and we get competent engineers being able to develop good agents, right?
You shouldn't have to be a top specialist in the world to develop an active inference agent.
Now, in order to automate inference, you must do message passing, and I've talked about that for efficiency.
I've also talked about which messages to pass.

27:16 Not necessarily do you have to follow this framework, but constrained better Free energy framework is very convenient.
It's an umbrella framework that basically goes over all the interesting other message passing computations.

27:37 When message passing, reactive message passing, it's fully automated, so you don't have to write any code anymore.
In principle, you can do parallel distributed processing.
It's robust structural changes.

27:50 You can learn new inference pathways.
So lots of advantages here.
Now, how do we do it?
I like to introduce a toolbox that we've been working on called ARX infer.
And we do that with my lab here at the university.

28:09 I'm here in Eindhoven in the south of the Netherlands, and we have a lab.
The lab is called BIS lab.
Here are postdocs and assistant professors and PhD students, and we've been working on this for many years.
And some of these, like Albert and Ismail and Tyce, have written dissertations.
And our best work, we have consolidated that in a toolbox.

28:39 And the toolbox is called Arcs infer.
And if you want to have a look, you can go to the website, arcsinfer.
ML.
And Arctinfur works in the way that I've just discussed.
It does message passing.

28:54 It tries to minimize constraint, better free energy.
That means it can come up with all kinds of message passing algorithms.
It will do it in a reactive way, and it will try to do it in real time and low power and all the KPIs that we're talking about.
Now, it's, of course, not done, but it's functional and like to show some demos.
And I will leave it to Dimitri and Bart, who are two advanced PhD students in my lab to show the demos.

29:30 So I'm going to stop sharing.

29:37 _Daniel:_
Awesome.
Thank you, Bert.
Great talk.

29:40 _Bert:_
Sure.

29:42 _Dmitry:_
Can you hear?

29:43 _Bert:_
Yeah.
Yeah.

29:45 _Dmitry:_
Okay.
I will try to share my screen.

29:48 _Bert:_
Okay.

29:50 _Dmitry:_
So you should see it now.

29:52 _Daniel:_
Looks good.

29:53 _Dmitry:_
Okay, so, yeah, hello to everyone, I'm Dmitry
Bagaev. So I'm a PhD student in Bioslab in Einhoven University of Technology, and yes, I have a small presentation about actual software developments in so over the past few years, we have significantly improved our tools.
And basically my entire PhD was dedicated to implement this idea, which Beard was talking about, like implementing the variation of reactive message passing.

30:23 And in this presentation, I just want to show you what you can actually do using this theory under the hood.
Okay, so basically, in order to automate active inference, we need to automate Bayesian inference.
And we have already a lot of solutions for that, such as Pyro NumPy, which is funded by Google, Info.
Net is funded by Microsoft, turing is in July, PIMC, and many, many.
And basically these solutions are really good and they're really good at prototyping as well.

30:57 But our goal is eventually to be able to deploy these kind of systems, not just prototype.
And we are really focusing on these particular properties for this automated Bayesian inference.
So it must be low power, adaptive, real time scalable.
It also must be user friendly at the end and must support a large scope of models if we want it to be useful.
In Bioslab, we want to build such a software with such nice properties and it's always about trade offs, right?

31:33 So we do something better in one particular domain and maybe other software libraries, they might be better in a different domain, but we are really focusing on this particular property.
And so, yes, I will reiterate a little bit Bert's presentation.
So how do we achieve this?
So imagine we have an environment and we have an agent, and the agent takes some actions and the agent basically what he needs is to come up with some sort of good enough probabilistic model of its environment in order to do patient inference.

32:06 And in our framework, we encode the model as a factor graph, which not only models the observations, but also actions and desired future.

32:17 And this approach allows us to decompose these complex relationships between variables and hidden states into some kind of structure and local blocks.
And it's not a black box anymore.
And the model itself may have some sort of background motivation interpretation.
It may encode your prior knowledge about some particular physical system and the locality of these blocks basically allows you to scale to millions of variables and hidden states.
It allows you to pre optimize it maybe, or maybe use some sort of different approximation strategies in different places.

32:58 So it allows a lot of very nice properties as well.
And we use reactive message passing to run actual variational Bayesian inference.

33:09 It uses reactive programming under the hood to minimize the approximation to the variational free energy.
And yes, as Bert also mentioned, it's very much related to actor model.
And basically in Ariks and fur.

33:25 You can think of different nodes as actors themselves and they have basically one single purpose is to send a variational message that minimizes free energy.
This is a very short and very high level description, but it is essentially what is happening under lipid.
So we are not treating different agents which interact with each other as actors, but we also treat the actual components of the underlying model as actors themselves.
It's like a very hierarchical structure.
So this is the main central idea of this inference.

34:03 So here for example, first example, we can do an inference in a dynamical system.

34:10 This example, which is quite old already, I think it's like two years ago.
So we track a position of the object given some noisy measurements which are indicated by green dots, the actual real signal, we cannot observe it, but we just can plot.
It is shown as blue and the inferred signal is shown as red and the data set is infinite.
The inference end just reacts on it and does not assume any particular data size.

34:37 Simply reacts in your observation as soon as possible.
Yeah, I'm actually not sure how smoothly Zoom shares my screen.
Maybe you can see it's a bit lagging in the animations, I'm not sure because maybe zoom does not share it on a full frame rate.
And also on the right hand side you can see how we define models in our framework.
We use Julia as a programming language.

35:02 Basically, this is everything that you need to define this particular model and run inference on a data set.
And actually I literally spent more days to plot it instead of inference.

35:14 So inference was an easiest part for me, plotting was way much harder to relate to user friendliness.
And we actually have plans to improve our model specification language, make it even easier.
So now for technical reasons, we have some auxiliary statements in the model specification language, but we are working to improve that and make it even easier.

35:41 This is another example which is similar to the previous one, that uses much more complex and linear dynamical system of the double pendulum.
And the system is chaotic and we can observe only a small part of it with a lot of noise, also indicated as a green dots.
And nevertheless, given good enough model, you can infer the other hidden states with pretty much high precision and the code needed for that is also relatively short.

36:15 We also have examples with active inference agents that interact with their environment.
So the left up shows mountain car problem, very famous problem.
The left bottom side shows an active inference agent which tries to control the inverted pendulum from falling in the windy conditions it reacts in wind.
We also have a demo of an agent that controls a pendulum in an ever changing environment.
So on the right side you see a pendulum with an engine and engine has limited power and.

36:55 The agent itself needs to reach the goal and the goal is indicated as a red circle.

37:02 Basically, in this demo we can change the environment in real time and see how the agent reacts.
So we can change the mass of the pebble among its length or the amount of noise in the measurements, or we can change the goal, we can change maximum engine power, et cetera.

37:17 So the agent will still try to infer the best possible course of actions in order to reach its goal and it just never stops reacting.
It's also actually possible to restrict engine power such that it will not longer possible to reach the goal, right?
But the agent will still try.

37:40 We have other cool demos with smart navigation and collision avoidance which are still under active research and the code for them is not available publicly it will be soon available, but for example in this example we can define a set of agents with their boundaries and a set of their destinations.
And we can see how they try to resolve their routes altogether.

38:07 And we can have some static obstacles in the map.
We can see how agents can find their most optimal path in order to reach their goals and avoid any possible collision.

38:21 And it's also not necessary to have static obstacles, the obstacles themselves may move.
So on this demo we have hundreds of agents that navigate through a map of obstacles that move from bottom to top to the circles or obstacles.
And agents are depicted as small dots and they need to go from left to right, basically avoid any sort of collision.

38:47 And as I also mentioned, we want to perform efficient and real time inference, but we also to do it like low power, low performance on low performance devices such as Raspberry, Pi or Coolpy as an example.
And we have some results of successfully running the Bayesian audio source separation, for example, on coolpy.
So it is actually possible we try to run active inference agents also on Coolpy.
So as the aforementioned inverted pendulum, and as I mentioned, we also need to have a large model scope and basically RICS infer has not been designed to solve any of the aforementioned problems.

39:34 Specifically, we have a large set of different examples in our repository, different models, different data, different inference constraints.

39:44 We have examples for linear regression, hidden Markov model altogether, grace model hierarchy models, misheard models, Gaussian process and so, right?
So this approach is very versatile.
And for example, if you compare it with sort of a conventional software libraries where you let's say have a library that solves a common filter, might be a very great library, maybe super fast, have top performance works great and very reliable, super good.
But then you are constrained by this particular model common filtering, right?
And you can't really change it much.

40:24 In Ericsson Fur we are free to define our own models which we can pretty much easily define a model that essentially would act equivalently to common filtering equation.

40:37 And so basically in the demo that I showed before about object tracking, it was essentially a common filter but was written in a probabilistic model.
So yeah, that was my small addition to Bear's presentation.
So our software is free MIT license and it's open source available on GitHub.
Yeah, and we would be happy thanks to be able to present where we would be happy to answer.

41:06 Any, thanks.

41:14 _Daniel:_
Awesome.
All right, I'll just ask a quick question.
In the chat Marco asks sorry if I missed it are the collision avoidance demos real time adapting to other agents behavior or is it a collectively pre computed path?

41:33 _Dmitry:_
So basically they are not super real time, they're kind of fast to compute this path, like maybe 5 seconds or so.
But we are basically working to make it real time.
So we know what is the problem, we know where to improve and we will make it real time.
Yes, almost.

41:53 _Daniel:_
Next question.
Do you have some comparative data with other methods?
And just more generally, what kinds of benchmarks or when you're talking with industry in different settings, what are people looking for that killer app of active inference?
Or what are they looking for their key measures?

42:14 _Dmitry:_
So I personally have a big paper about comparison with sampling based methods like HMC and also in my PhD thesis, there will be a comparison with nuts, also other sampling based methods.

42:28 So, long story short, sampling based methods cannot really run this kind of sophisticated inference in real time.
They're very time consuming.
They do not really scale well to large problems which is really needed for active inference agents.
Because if you have a large environment, very complicated, you will have a lot of unknown variables in your model.

42:53 There is a paper that compares and basically we show that our approach scales much, much better.
So I personally run on just a regular MacBook laptop, I run the model with 2 million unknown variables and it was quite fast.

43:14 With sampling based methods you may find yourself in a model with like 100 variables and then you wait like 2 hours or something and then it turns out that your chain did not converge or something like that.

43:31 _Daniel:_
Cool.
Yeah, it's people commenting in the chat like how far message passing and factor graphs have come.
And so to Bias Lab and to Bert at all, we definitely appreciate this exciting line of research.
I mean, there's so much to learn there and sometimes looking at the equations, it can seem like it's like written in stone and just sort of the beginning and the end is, you know, variational free energy.
But then in your presentations you're really showing like no, we are hands on.

44:03 That's where we get the interpretability, the modularity, that's where it really is implemented.
And it's like an information logistics challenge.
It's not like an esoteric philosophy question at that point.

44:16 _Bert:_
No, indeed.
I should say it's taken us we are no geniuses, right.

44:24 So our lab exists more than eight years, and you see all the people in the lab, it's taken us many years with lots of wrong directions to get this to work to where it's now.
So it's a very long path.
But at this moment, I'm pretty confident that at some point in the future and we don't want to say in three months or in one year, but we will be able to write a toolbox that will allow people to design a generative model and just press a button and forget about the inference.
You don't have to worry about inference anymore.
It will be fast and automated, and that will happen, and it will happen within a few years.

45:08 And maybe somebody else will write an even better toolbox.
But I'm pretty confident that even our toolbox will be able to do that.

45:20 People talk about, why don't we have the success of deep learning and generative AI, right?
Well, they have the success because of big data availability of big data, big computers and toolboxes TensorFlow and all the successes.
We don't need big data because agents collect their own data in the field.

45:42 We don't need big computers, active influence agents, they manage their power resources.
But we need a really good toolbox because programming an active inference agent, programming the inference by hand is just not doable.
So we need a really good toolbox that really automates this.
We hope Arcs Infer will be one of the first toolboxes to do that.
I am sure that other people will also be working on it, and better toolboxes will come about.

46:18 But I think the optimistic message is that it will happen.

46:24 And once we have a toolbox like that, then we can actually a large community can start building agents, and we can actually show deployable agents in the field that they work.
And they work better than reinforcement learning agent or whatever is out there.
Right.
So I think that's a very positive and hopeful message.

46:48 _Daniel:_
It's what we expect.
It's what we prefer.

46:50 _Bert:_
Yeah.

46:52 _Daniel:_
Any last comments from either of you?

46:59 _Bert:_
Comments from us?
No, I'm just very happy to get the opportunity, and yeah, I want to everybody can download this toolbox.
I think at this moment, you still should be a programmer to work with the toolbox.
And I hope you're friendly, because it's not totally polished in the way that we want.
But it's coming, right?

47:28 It's coming.
In the next years, there will be a good toolbox for almost everybody to use, but people that are interested, even people that are interested to work.
Here at Biaslab, we have an open position for PhD students, so we're happy to receive emails from people that are interested to work with us.

47:52 _Dmitry:_
Thank you.

47:52 _Daniel:_
Dimitri, anything in closing?

47:56 _Dmitry:_
No, just that.
Thank you again for possibility to present.
Super nice to be here.

48:02 _Daniel:_
Cool.

48:03 _Dmitry:_
Yeah.

48:03 _Daniel:_
Well, later in the year, we will be discussing your two part recent work, and so we're going to be getting a lot into the details, and I hope that people in the institute and the ecosystem will be as excited as we all are.
Thank you.

48:20 _Bert:_
Thank you.

48:21 Bye.

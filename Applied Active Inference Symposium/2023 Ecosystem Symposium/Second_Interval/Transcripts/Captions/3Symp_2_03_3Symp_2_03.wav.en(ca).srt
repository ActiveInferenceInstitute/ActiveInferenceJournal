1
00:00:06,010 --> 00:00:08,734
Greetings. All right, well, our next

2
00:00:08,852 --> 00:00:11,898
session hey, Bert. Greetings.

3
00:00:12,074 --> 00:00:13,790
Great. Hey, how you doing? Good, good.

4
00:00:13,860 --> 00:00:16,746
Very well. Our next session is with Bert

5
00:00:16,778 --> 00:00:19,466
DeVries, Dmitri Baghev, and Bart Van

6
00:00:19,498 --> 00:00:21,694
ERP. It's going to be called towards

7
00:00:21,812 --> 00:00:24,106
User Friendly Design of Synthetic Active

8
00:00:24,138 --> 00:00:26,878
inference agents. And I know a lot of

9
00:00:26,884 --> 00:00:29,542
people are super excited to see this

10
00:00:29,596 --> 00:00:31,910
really practical and cutting edge work.

11
00:00:31,980 --> 00:00:34,486
So to you, Bert, and just let us know

12
00:00:34,508 --> 00:00:37,720
how we can support. Okay, great.

13
00:00:38,650 --> 00:00:41,434
Is my audio good? Yep, sounds good.

14
00:00:41,632 --> 00:00:44,300
Okay, then I'm going to share my screen.

15
00:00:46,430 --> 00:00:48,486
I hope I picked the right one. I don't

16
00:00:48,518 --> 00:00:51,866
work with zoom quite often. Looks good.

17
00:00:51,968 --> 00:00:54,320
Looks good. Yeah. All right. Super.

18
00:00:55,250 --> 00:00:57,994
Well, thanks a lot, Daniel, for hosting

19
00:00:58,042 --> 00:01:00,686
this symposium. I've been watching some

20
00:01:00,708 --> 00:01:04,094
talks. It's really amazing. And we feel

21
00:01:04,132 --> 00:01:06,106
privileged to get a chance to present

22
00:01:06,148 --> 00:01:08,978
ourselves. So we are also, just like a

23
00:01:08,984 --> 00:01:10,834
few others before us, interested in

24
00:01:10,872 --> 00:01:12,606
developing a toolbox for active

25
00:01:12,638 --> 00:01:15,726
inference. And so this picture,

26
00:01:15,838 --> 00:01:19,562
or she kind of shows what we're

27
00:01:19,646 --> 00:01:21,926
about or what we are interested in. So

28
00:01:21,948 --> 00:01:24,614
here's a lady on the left hand side,

29
00:01:24,812 --> 00:01:28,306
and I'm going to try to get a laser

30
00:01:28,338 --> 00:01:31,642
pointer. And she has this idea about a

31
00:01:31,776 --> 00:01:34,486
rewarding behavior for a vacuum cleaning

32
00:01:34,518 --> 00:01:36,906
robot, right? So she's writing down she

33
00:01:36,928 --> 00:01:39,274
has a textual expression. Move around

34
00:01:39,312 --> 00:01:41,578
the apartment, apply suction until the

35
00:01:41,584 --> 00:01:44,326
floor is clean, do not touch objects,

36
00:01:44,358 --> 00:01:46,366
and when done, return to the dog. So

37
00:01:46,468 --> 00:01:48,094
that's not so hard. I'm going to rate

38
00:01:48,132 --> 00:01:51,438
that with one star out of three stars in

39
00:01:51,444 --> 00:01:54,170
terms of difficulty level to specify

40
00:01:54,250 --> 00:01:57,570
that. But that's not enough to program

41
00:01:57,640 --> 00:01:59,394
this robot, right? Because what she

42
00:01:59,432 --> 00:02:02,626
really needs to do now is to specify a

43
00:02:02,648 --> 00:02:07,534
generative model and there's

44
00:02:07,662 --> 00:02:09,966
effectors and actuators, right. The

45
00:02:10,008 --> 00:02:13,106
robot has to move around, apply suction

46
00:02:13,298 --> 00:02:15,126
until the floor is clean. So there are

47
00:02:15,148 --> 00:02:18,278
sensors, probably a camera. Do not touch

48
00:02:18,364 --> 00:02:20,150
objects. Or maybe there has to be object

49
00:02:20,220 --> 00:02:23,114
recognition. This is a really difficult

50
00:02:23,312 --> 00:02:25,622
task to come up with this generative

51
00:02:25,766 --> 00:02:28,986
model here. And on top of that, she has

52
00:02:29,008 --> 00:02:32,134
to specify this kind of rewarding

53
00:02:32,182 --> 00:02:34,618
behavior in terms now of probability

54
00:02:34,794 --> 00:02:36,830
distributions of this generative model.

55
00:02:36,900 --> 00:02:39,294
So very difficult. I'm going to rate

56
00:02:39,332 --> 00:02:42,846
that with two stars because the

57
00:02:42,868 --> 00:02:45,290
next thing she has to do for this model

58
00:02:45,380 --> 00:02:47,746
is to specify the inference procedure to

59
00:02:47,768 --> 00:02:50,786
do actually active inference and free

60
00:02:50,808 --> 00:02:53,730
energy minimization in real time for

61
00:02:53,800 --> 00:02:56,142
this complex model. And really that's

62
00:02:56,206 --> 00:02:58,520
almost impossible, right? Only a few

63
00:02:59,450 --> 00:03:02,898
specialists can really write a procedure

64
00:03:02,994 --> 00:03:04,770
for variational free energy minimization

65
00:03:04,850 --> 00:03:08,102
in some very difficult model. So what

66
00:03:08,156 --> 00:03:10,582
we are about what we've been working on

67
00:03:10,716 --> 00:03:13,074
is to try to automate the inference

68
00:03:13,122 --> 00:03:15,302
task. So get rid of the three stars.

69
00:03:15,446 --> 00:03:17,734
And yes, she will still have to specify

70
00:03:17,782 --> 00:03:20,874
her model, but in the long term, we try

71
00:03:20,912 --> 00:03:23,274
to get away from that. So in the long

72
00:03:23,312 --> 00:03:25,786
term, we hope we will get a toolbox.

73
00:03:25,818 --> 00:03:28,400
And now we're talking 510 years, right,

74
00:03:29,010 --> 00:03:31,806
where a textual description would be

75
00:03:31,828 --> 00:03:34,158
enough to specify some initial model

76
00:03:34,324 --> 00:03:37,538
with an initial prior and everything

77
00:03:37,624 --> 00:03:39,534
else is just automated inference,

78
00:03:39,582 --> 00:03:41,570
learning of states, parameters,

79
00:03:42,070 --> 00:03:43,810
structural adaptation of the model,

80
00:03:43,880 --> 00:03:46,670
even maybe based on her feedback

81
00:03:46,830 --> 00:03:49,490
updating the prior.

82
00:03:49,650 --> 00:03:52,806
So that's long term for now,

83
00:03:52,988 --> 00:03:55,094
we would be very happy if we could just

84
00:03:55,132 --> 00:03:58,306
automate the inference task.

85
00:03:58,498 --> 00:04:02,230
So why is it so difficult to specify

86
00:04:02,310 --> 00:04:04,218
inference for an active inference agent?

87
00:04:04,304 --> 00:04:08,694
Well, we have so many competing KPIs,

88
00:04:08,822 --> 00:04:12,490
right? We want to do this for large

89
00:04:12,560 --> 00:04:15,434
model scopes, not just for ABCD models,

90
00:04:15,482 --> 00:04:17,642
but maybe there's also continuous

91
00:04:17,786 --> 00:04:21,034
variables and hierarchical models,

92
00:04:21,082 --> 00:04:23,450
right? It must be very user friendly.

93
00:04:23,610 --> 00:04:25,690
We really don't want her to worry about

94
00:04:25,780 --> 00:04:28,740
robustness of her code.

95
00:04:29,350 --> 00:04:31,540
We don't want her to worry about whether

96
00:04:32,630 --> 00:04:35,854
two variables have conjugate

97
00:04:35,902 --> 00:04:39,414
relationships adaptivity. We want to

98
00:04:39,452 --> 00:04:41,414
update states, parameters, maybe even

99
00:04:41,452 --> 00:04:44,246
the model. The model structure has to be

100
00:04:44,268 --> 00:04:46,470
low power because these ancients often

101
00:04:46,540 --> 00:04:50,246
run on edge devices, so they run on

102
00:04:50,268 --> 00:04:52,822
their battery powered has to be in real

103
00:04:52,876 --> 00:04:55,526
time because you can't learn how to ride

104
00:04:55,558 --> 00:04:57,322
a bike if there's no real time

105
00:04:57,376 --> 00:05:00,170
reasoning. And on top of that, you

106
00:05:00,240 --> 00:05:01,846
actually want to minimize variational

107
00:05:01,878 --> 00:05:03,466
frequency, right? You want to do it at

108
00:05:03,488 --> 00:05:04,958
least as good or at least in a

109
00:05:04,964 --> 00:05:07,194
neighborhood of if you would do a manual

110
00:05:07,242 --> 00:05:10,506
derivation. And some of these decidorata

111
00:05:10,538 --> 00:05:13,774
bite each other, right? If you want to

112
00:05:13,812 --> 00:05:15,986
minimize variational free energy, but

113
00:05:16,008 --> 00:05:18,066
you have to do it in real time and on

114
00:05:18,088 --> 00:05:20,642
low power that kind of bites each other,

115
00:05:20,696 --> 00:05:24,990
right? So these are difficult KPIs

116
00:05:25,070 --> 00:05:28,966
that they're all important.

117
00:05:29,068 --> 00:05:32,310
You can't just take one out because then

118
00:05:32,380 --> 00:05:37,846
the whole system wouldn't work. So when

119
00:05:37,868 --> 00:05:41,202
you read papers on active inference,

120
00:05:41,266 --> 00:05:44,162
you often also read and now we implement

121
00:05:44,226 --> 00:05:46,746
variation of minimization and that can

122
00:05:46,768 --> 00:05:50,358
be done by message passing on a graph.

123
00:05:50,454 --> 00:05:54,334
And I want to clarify first why it has

124
00:05:54,372 --> 00:05:56,046
to be done by message passing on the

125
00:05:56,068 --> 00:05:58,750
graph. I do that by giving a very short

126
00:05:58,820 --> 00:06:02,078
answer and then do an example. The short

127
00:06:02,164 --> 00:06:06,102
answer is that Bayesian inference

128
00:06:06,266 --> 00:06:09,906
involves computing very large

129
00:06:10,008 --> 00:06:12,274
sum of products, like what you see here

130
00:06:12,312 --> 00:06:14,546
on the left hand side. Here's a product

131
00:06:14,648 --> 00:06:18,482
AC, ADBC, and then we sum them

132
00:06:18,616 --> 00:06:20,946
AC plus ad and so forth. This is a sum

133
00:06:20,978 --> 00:06:23,766
of products. Now, we know by the

134
00:06:23,788 --> 00:06:26,838
Distributive law that this here on the

135
00:06:26,844 --> 00:06:29,574
left hand side can also be computed as

136
00:06:29,692 --> 00:06:32,306
on the right hand side. If I multiply

137
00:06:32,338 --> 00:06:34,842
this out, I get a times C plus a times D

138
00:06:34,896 --> 00:06:37,558
and so forth. This is a product of sums

139
00:06:37,654 --> 00:06:41,274
and they're exactly the same thing. The

140
00:06:41,312 --> 00:06:43,886
only difference is that to compute the

141
00:06:43,908 --> 00:06:46,618
left hand side takes four additions

142
00:06:46,714 --> 00:06:49,182
sorry, four multiplications and three

143
00:06:49,236 --> 00:06:51,374
additions. To compute the right hand

144
00:06:51,412 --> 00:06:54,174
side takes two additions and only one

145
00:06:54,212 --> 00:06:57,266
multiplication. So on the right hand

146
00:06:57,288 --> 00:06:59,826
side is much cheaper to compute than the

147
00:06:59,848 --> 00:07:02,722
left hand side. Normally when we write

148
00:07:02,776 --> 00:07:05,054
down marginalization and Beijing

149
00:07:05,102 --> 00:07:08,258
inference, we write things down as in

150
00:07:08,264 --> 00:07:11,054
the left hand side what message passing

151
00:07:11,102 --> 00:07:13,410
does on the graph, it will automatically

152
00:07:13,490 --> 00:07:16,742
convert that into much cheaper to

153
00:07:16,796 --> 00:07:19,846
evaluate product of sums. And I'll give

154
00:07:19,868 --> 00:07:21,420
an example of that.

155
00:07:23,470 --> 00:07:27,322
So here is an example model F

156
00:07:27,376 --> 00:07:29,834
of seven variables x one, x two through

157
00:07:29,872 --> 00:07:32,554
x seven. And this model happens to be

158
00:07:32,672 --> 00:07:36,894
factorized FA of x one, FB, x two and so

159
00:07:37,012 --> 00:07:39,374
forth. Now, we can draw this

160
00:07:39,412 --> 00:07:42,926
factorization as a graph. And what

161
00:07:42,948 --> 00:07:45,226
we do, and this is called a Farney style

162
00:07:45,258 --> 00:07:47,570
factor graph, what we do is for each

163
00:07:47,720 --> 00:07:51,086
factor FA, we allocate a node.

164
00:07:51,118 --> 00:07:54,526
So FB gets a node and FC gets a node.

165
00:07:54,718 --> 00:07:57,634
And we associate the variables in our

166
00:07:57,672 --> 00:08:00,454
system with the edge. And an edge is

167
00:08:00,492 --> 00:08:03,526
connected to a node if that variable is

168
00:08:03,548 --> 00:08:07,014
an argument of that function. So FC is

169
00:08:07,052 --> 00:08:11,142
a function of x one, x two, x three.

170
00:08:11,276 --> 00:08:13,594
And that means that FC connects to the

171
00:08:13,632 --> 00:08:17,558
edges x one, x two, x three, and FD

172
00:08:17,734 --> 00:08:20,698
is only a function of x four. So FD only

173
00:08:20,784 --> 00:08:24,654
connects to the edge x four. So what

174
00:08:24,692 --> 00:08:26,586
you can see in this graph is this graph

175
00:08:26,618 --> 00:08:30,254
is nothing but a visualization of

176
00:08:30,292 --> 00:08:32,942
the factorization assumptions that we

177
00:08:33,076 --> 00:08:36,414
have for this model. Now, if I'm

178
00:08:36,462 --> 00:08:40,430
interested in a big marginalization task

179
00:08:40,510 --> 00:08:43,534
and I integrate out over all variables

180
00:08:43,582 --> 00:08:46,514
but x three, so x one, x two,

181
00:08:46,552 --> 00:08:48,534
x four and so forth through x seven,

182
00:08:48,652 --> 00:08:54,310
I'm interested in this. Then taking

183
00:08:54,380 --> 00:08:56,982
advantage of this factorization, I can

184
00:08:57,036 --> 00:09:01,366
rewrite basically this sum

185
00:09:01,398 --> 00:09:05,750
of product into a product of sums

186
00:09:05,910 --> 00:09:09,114
as below here, what you will

187
00:09:09,152 --> 00:09:11,386
see here below this computes exactly the

188
00:09:11,408 --> 00:09:13,294
same thing. But I've made use of this

189
00:09:13,332 --> 00:09:15,790
distributive law. For instance,

190
00:09:16,450 --> 00:09:19,710
FC contains no x four,

191
00:09:19,780 --> 00:09:22,494
no x five. So I moved it over the

192
00:09:22,532 --> 00:09:26,110
summation sign to the left. And FB also

193
00:09:26,180 --> 00:09:29,346
doesn't contain x four, x five, x six,

194
00:09:29,448 --> 00:09:31,506
seven. So I moved it all the way to the

195
00:09:31,528 --> 00:09:35,090
left. And when you do that, you are left

196
00:09:35,160 --> 00:09:39,062
here with an expression where I

197
00:09:39,116 --> 00:09:42,866
only sum over two variables

198
00:09:42,898 --> 00:09:44,742
and here I have to sum over six

199
00:09:44,796 --> 00:09:46,662
variables and here over two, and here

200
00:09:46,716 --> 00:09:49,654
over two. So you can imagine if each

201
00:09:49,692 --> 00:09:52,794
variable, let's say x one, x two,

202
00:09:52,912 --> 00:09:55,274
if each variable has ten interesting

203
00:09:55,392 --> 00:09:58,474
values that you need to sum over, then I

204
00:09:58,512 --> 00:10:02,278
have here the original marginalization

205
00:10:02,374 --> 00:10:04,810
problem. I have ten to the power six,

206
00:10:04,960 --> 00:10:09,182
so a million terms, and here in

207
00:10:09,236 --> 00:10:12,478
red I have 100 terms and here I have 100

208
00:10:12,564 --> 00:10:15,450
and here I have 100. So here I have 300

209
00:10:15,540 --> 00:10:18,418
terms and here I have 1 million terms.

210
00:10:18,584 --> 00:10:21,586
So it's an enormous reduction in

211
00:10:21,608 --> 00:10:25,554
computational complexity when

212
00:10:25,672 --> 00:10:29,014
we make use of this distributive law.

213
00:10:29,132 --> 00:10:31,862
Now, it turns out that if you write this

214
00:10:31,916 --> 00:10:35,542
out, you can associate these

215
00:10:35,596 --> 00:10:38,246
intermediate factors with messages on

216
00:10:38,268 --> 00:10:40,130
the graph. It's just an interpretation,

217
00:10:40,210 --> 00:10:44,730
a visual interpretation. It's as if FC

218
00:10:45,710 --> 00:10:48,950
receives a message from FA and FB

219
00:10:49,030 --> 00:10:53,218
receive or FC receives a message from FB

220
00:10:53,334 --> 00:10:56,414
and computes an outgoing message MU x

221
00:10:56,452 --> 00:10:59,758
three. And the same thing for Fe. So Fe

222
00:10:59,924 --> 00:11:02,138
receives a message from its neighboring

223
00:11:02,234 --> 00:11:05,874
factors, FD and FF and computes an

224
00:11:05,912 --> 00:11:09,138
outgoing message, x three.

225
00:11:09,304 --> 00:11:12,658
So what you see here is that the entire

226
00:11:12,744 --> 00:11:15,394
marginalization process can be

227
00:11:15,432 --> 00:11:18,642
represented as basically computing a few

228
00:11:18,696 --> 00:11:22,086
messages on a graph and multiplying some

229
00:11:22,108 --> 00:11:24,966
of these messages with each other. And

230
00:11:24,988 --> 00:11:27,026
that's how you can do Bayesian inference

231
00:11:27,058 --> 00:11:29,174
and also how you can do variational free

232
00:11:29,212 --> 00:11:34,714
energy minimization. So this works in

233
00:11:34,832 --> 00:11:38,282
factorized models, but I would say even

234
00:11:38,336 --> 00:11:40,582
stronger if your model is not factorized

235
00:11:40,646 --> 00:11:44,014
and you have a lot of variables, there

236
00:11:44,052 --> 00:11:45,742
is just no way you can do proper

237
00:11:45,796 --> 00:11:48,606
inference. So any serious model is

238
00:11:48,628 --> 00:11:51,534
factorized, like the brain is almost

239
00:11:51,652 --> 00:11:54,766
sparse, is almost empty. We have, what

240
00:11:54,788 --> 00:11:57,730
is it, about 10 billion neurons, and

241
00:11:57,800 --> 00:11:59,618
each neuron connects to a few thousand

242
00:11:59,704 --> 00:12:02,226
other neurons. So if I would draw the

243
00:12:02,248 --> 00:12:04,530
graph, that graph is almost empty, it is

244
00:12:04,600 --> 00:12:08,534
hugely sparse. And so there is no other

245
00:12:08,572 --> 00:12:10,374
way to do inference in the brain than by

246
00:12:10,412 --> 00:12:13,590
message passing. So that's why message

247
00:12:13,660 --> 00:12:15,254
passing, just because it's more

248
00:12:15,292 --> 00:12:17,000
effective than anything else.

249
00:12:18,650 --> 00:12:22,098
Now then the issue is which message do

250
00:12:22,124 --> 00:12:23,894
you compute? How do you compute

251
00:12:23,942 --> 00:12:27,066
messages? Because there are

252
00:12:27,088 --> 00:12:29,226
different ways of doing it right. And we

253
00:12:29,248 --> 00:12:31,366
also read in active inference papers,

254
00:12:31,398 --> 00:12:33,062
you can do this by variational message

255
00:12:33,136 --> 00:12:36,346
passing, or expectation maximization,

256
00:12:36,458 --> 00:12:39,914
or belief propagation and variational

257
00:12:39,962 --> 00:12:42,666
LaPlace and all these terms. It turns

258
00:12:42,698 --> 00:12:46,382
out that there is an umbrella framework

259
00:12:46,446 --> 00:12:48,046
for all these methods passing

260
00:12:48,078 --> 00:12:50,670
frameworks. And that umbrella framework

261
00:12:50,830 --> 00:12:53,682
is called Constraint better free energy

262
00:12:53,736 --> 00:12:57,300
minimization. And I will try to

263
00:12:58,070 --> 00:13:01,430
illustrate it by this slide.

264
00:13:01,850 --> 00:13:04,886
So here I have this graph. This is just

265
00:13:04,908 --> 00:13:09,042
an example graph where my generative

266
00:13:09,106 --> 00:13:12,342
model is basically factorized

267
00:13:12,406 --> 00:13:15,754
in FA, FB, FC, FD and

268
00:13:15,792 --> 00:13:18,586
Fe. And I've also written that here. So

269
00:13:18,608 --> 00:13:20,940
this now is the variational free energy.

270
00:13:21,710 --> 00:13:24,966
Now, I haven't made any assumption on Q

271
00:13:25,008 --> 00:13:28,174
of X. So Q of X is still Q of x one, x

272
00:13:28,212 --> 00:13:30,234
two, x three. It's just a joint overall

273
00:13:30,282 --> 00:13:33,214
variables and it doesn't have any

274
00:13:33,252 --> 00:13:37,122
factorization assumption. It makes sense

275
00:13:37,256 --> 00:13:40,434
to also assume that the posterior kind

276
00:13:40,472 --> 00:13:42,974
of follows the factorization assumption

277
00:13:43,022 --> 00:13:44,990
of the prior, namely of the generative

278
00:13:45,070 --> 00:13:49,686
model. So if we make that assumption and

279
00:13:49,788 --> 00:13:51,606
that means we're going to make the

280
00:13:51,628 --> 00:13:55,318
assumption that QX is also now a product

281
00:13:55,404 --> 00:13:59,682
of QAS of X of A, where QAS

282
00:13:59,746 --> 00:14:02,854
of X of A stands for beliefs over nodes.

283
00:14:02,902 --> 00:14:07,146
What I mean by that is that Q of B is

284
00:14:07,168 --> 00:14:09,782
a posterior belief over this node,

285
00:14:09,846 --> 00:14:12,746
meaning it's a posterior belief over the

286
00:14:12,768 --> 00:14:15,422
edges that connect to this node. Just

287
00:14:15,476 --> 00:14:19,278
like FB is a function of x one, x two,

288
00:14:19,364 --> 00:14:22,718
x four, that's if you will, the prior or

289
00:14:22,724 --> 00:14:25,966
the generative model, then Q of B, the

290
00:14:25,988 --> 00:14:28,242
variational posterior for this node will

291
00:14:28,296 --> 00:14:30,914
also depend on x one, x two, x four,

292
00:14:30,952 --> 00:14:33,890
and on no other factors.

293
00:14:34,470 --> 00:14:36,978
If you just do that, then you will count

294
00:14:37,064 --> 00:14:40,262
some of the variables double because x

295
00:14:40,316 --> 00:14:43,458
one is part of the belief over FA,

296
00:14:43,554 --> 00:14:47,062
but also part of the belief over FB. So

297
00:14:47,116 --> 00:14:49,638
we just have to discount that by

298
00:14:49,724 --> 00:14:52,806
dividing by beliefs over edges.

299
00:14:52,998 --> 00:14:56,730
That means that I make now an assumption

300
00:14:57,070 --> 00:15:02,314
that my posterior beliefs is

301
00:15:02,352 --> 00:15:05,894
divided into local beliefs over notes

302
00:15:05,942 --> 00:15:07,854
and local beliefs over edges over

303
00:15:07,892 --> 00:15:11,502
variables. This will make things a lot

304
00:15:11,556 --> 00:15:15,434
simpler. In fact, if my graph is a tree

305
00:15:15,482 --> 00:15:18,418
and I did the tree here, and I would do

306
00:15:18,504 --> 00:15:20,562
message passing on that tree and I could

307
00:15:20,616 --> 00:15:22,302
suppose I could do that perfectly,

308
00:15:22,366 --> 00:15:24,546
everything is linear gaussian, then I

309
00:15:24,568 --> 00:15:27,506
get perfect Bayesian inference. There is

310
00:15:27,528 --> 00:15:29,414
no approximation. So this is a good

311
00:15:29,452 --> 00:15:32,342
assumption. Sometimes it's still very

312
00:15:32,396 --> 00:15:35,960
hard to compute a message because even

313
00:15:37,210 --> 00:15:39,798
the single messages that come out of

314
00:15:39,804 --> 00:15:41,914
these nodes, they're still integrals or

315
00:15:41,952 --> 00:15:44,426
summations, and in particular the

316
00:15:44,448 --> 00:15:46,634
integrals may be a problem. We may not

317
00:15:46,672 --> 00:15:49,594
have an analytical answer. So what we

318
00:15:49,712 --> 00:15:52,170
sometimes do is add additional

319
00:15:52,910 --> 00:15:55,934
assumptions. We'll say, well, the

320
00:15:55,972 --> 00:15:58,794
posterior belief over FD, I can't

321
00:15:58,842 --> 00:16:01,998
compute it in general, but I'm going to

322
00:16:02,004 --> 00:16:03,802
just assume now that it's a gaussian

323
00:16:03,866 --> 00:16:06,914
that makes it easier. Or we can make an

324
00:16:06,952 --> 00:16:10,114
extra factorization assumption and

325
00:16:10,152 --> 00:16:13,310
say the posterior belief over FB,

326
00:16:13,470 --> 00:16:16,174
which is really a belief over the joint

327
00:16:16,222 --> 00:16:18,694
x one, x two, x four is going to be

328
00:16:18,732 --> 00:16:22,674
broken into independent

329
00:16:22,722 --> 00:16:27,206
belief over x one and belief over x

330
00:16:27,228 --> 00:16:28,360
two and x four.

331
00:16:30,810 --> 00:16:34,022
These additional assumptions, if I

332
00:16:34,076 --> 00:16:36,326
impose them as well, this is what I

333
00:16:36,348 --> 00:16:39,754
recall. Now, if I all substituted here

334
00:16:39,792 --> 00:16:42,266
in Q of x, I get what's called a

335
00:16:42,288 --> 00:16:44,766
constrained beth free energy. This is

336
00:16:44,788 --> 00:16:47,098
the same Beth as in the Oppenheimer

337
00:16:47,114 --> 00:16:49,338
movie. This is Hans Bethe, where it's

338
00:16:49,434 --> 00:16:50,560
named after.

339
00:16:53,730 --> 00:16:57,726
We have a graph now that

340
00:16:57,748 --> 00:17:03,294
is highly factorized and we have local

341
00:17:03,412 --> 00:17:07,600
beliefs over notes and over

342
00:17:08,170 --> 00:17:10,886
and they're indicated with red and we

343
00:17:10,908 --> 00:17:13,238
have additional constraints in green.

344
00:17:13,324 --> 00:17:15,622
They could be Gaussians or mean field

345
00:17:15,676 --> 00:17:17,734
constraints or other constraints. And

346
00:17:17,772 --> 00:17:20,246
now we will assume constraints that make

347
00:17:20,268 --> 00:17:21,814
it possible to compute all the messages.

348
00:17:21,862 --> 00:17:24,074
And now we can just automate this by

349
00:17:24,112 --> 00:17:26,282
making different assumptions. We can

350
00:17:26,336 --> 00:17:28,646
turn this into expectation maximization

351
00:17:28,758 --> 00:17:33,354
or belief propagation or hybrid

352
00:17:33,402 --> 00:17:35,358
forms thereof. We can turn it into any

353
00:17:35,444 --> 00:17:37,566
relevant message passing algorithm that

354
00:17:37,588 --> 00:17:40,030
you've heard of. So this is a very nice

355
00:17:40,100 --> 00:17:42,638
umbrella framework that basically

356
00:17:42,724 --> 00:17:44,160
encompasses everything.

357
00:17:46,070 --> 00:17:49,522
We've written a pretty large paper on

358
00:17:49,576 --> 00:17:53,266
this in the Entropy Journal where

359
00:17:53,288 --> 00:17:55,314
you can read all the math on how this

360
00:17:55,352 --> 00:17:59,414
works. So we've talked about

361
00:17:59,452 --> 00:18:01,858
why message passing, namely because it's

362
00:18:01,954 --> 00:18:03,942
the most effective way of doing

363
00:18:04,076 --> 00:18:05,974
inference. And we've talked about which

364
00:18:06,012 --> 00:18:09,398
messages to compute, namely we turn

365
00:18:09,484 --> 00:18:12,218
our variational free energy into

366
00:18:12,304 --> 00:18:13,962
something called a constraint, better

367
00:18:14,016 --> 00:18:16,006
free energy and then we can compute

368
00:18:16,038 --> 00:18:19,578
messages. The only thing that's left is,

369
00:18:19,744 --> 00:18:22,102
well, when do we pass these messages?

370
00:18:22,246 --> 00:18:24,222
What is the sequence of messages? Which

371
00:18:24,276 --> 00:18:27,774
one comes first? And this is

372
00:18:27,812 --> 00:18:30,400
where we see a lot of papers, right?

373
00:18:31,730 --> 00:18:33,818
You have to write control flow, what's

374
00:18:33,834 --> 00:18:35,694
called control flow. You have to say,

375
00:18:35,892 --> 00:18:38,286
okay, here is my algorithm for active

376
00:18:38,318 --> 00:18:40,370
inference. First I specify a model.

377
00:18:40,520 --> 00:18:42,834
Then let's do inference for every time

378
00:18:42,872 --> 00:18:45,246
step, collect a new observation, update

379
00:18:45,278 --> 00:18:48,440
the state, update the desired future,

380
00:18:49,050 --> 00:18:51,414
and so forth, compute expected free

381
00:18:51,452 --> 00:18:53,910
energy, select the policy, et cetera.

382
00:18:55,610 --> 00:18:58,614
This kind of program. The problem with

383
00:18:58,652 --> 00:19:00,934
active inferences is that there is

384
00:19:00,972 --> 00:19:03,462
nested for loops in here. Here's a for

385
00:19:03,516 --> 00:19:06,154
loop, and here's another for loop. And

386
00:19:06,192 --> 00:19:08,858
for each of these policies, I'm going to

387
00:19:08,864 --> 00:19:11,258
have to go into the future, so I'm going

388
00:19:11,264 --> 00:19:13,214
to have another time loop. So it is for

389
00:19:13,252 --> 00:19:15,534
loops, in for loops, in for loops. This

390
00:19:15,572 --> 00:19:17,566
will completely explode in terms of

391
00:19:17,588 --> 00:19:20,186
computational complexity.

392
00:19:20,378 --> 00:19:23,950
So as a result, some very clever people

393
00:19:24,020 --> 00:19:26,594
have written very clever algorithms of

394
00:19:26,632 --> 00:19:28,590
doing this much faster. Sophisticated

395
00:19:28,670 --> 00:19:31,086
inference, branching time active

396
00:19:31,118 --> 00:19:33,682
inference, dynamic programming EFE are

397
00:19:33,736 --> 00:19:36,034
recent proposals for doing this very

398
00:19:36,072 --> 00:19:39,302
clever. In the end, all of these

399
00:19:39,356 --> 00:19:43,254
proposals come down to a particular

400
00:19:43,452 --> 00:19:46,150
just a message passing schedule.

401
00:19:47,530 --> 00:19:49,846
Once we commit to message passing on the

402
00:19:49,868 --> 00:19:52,006
graph as our inference procedure, it's

403
00:19:52,038 --> 00:19:54,906
the only thing that's going on. And all

404
00:19:54,928 --> 00:19:58,346
of this sophisticated inference and

405
00:19:58,368 --> 00:20:00,954
branching time active inference, all it

406
00:20:00,992 --> 00:20:03,146
does is it schedules the messages. It

407
00:20:03,168 --> 00:20:04,838
says first this message, then this

408
00:20:04,864 --> 00:20:08,222
message, then this message. I don't mean

409
00:20:08,276 --> 00:20:10,714
that as a slight to these algorithms.

410
00:20:10,762 --> 00:20:12,622
They're very clever. And as we've seen

411
00:20:12,676 --> 00:20:14,686
in the presentation by Aswin Paul, you

412
00:20:14,708 --> 00:20:16,546
get huge improvements if you go from

413
00:20:16,568 --> 00:20:18,510
regular inference to sophisticated

414
00:20:18,590 --> 00:20:22,020
inference. But it's good to realize that

415
00:20:22,710 --> 00:20:25,902
these algorithms just specify in a graph

416
00:20:25,966 --> 00:20:28,600
which message comes after which message.

417
00:20:29,850 --> 00:20:33,750
So here's an example of

418
00:20:33,820 --> 00:20:38,690
a graph and a message sequence.

419
00:20:38,770 --> 00:20:41,494
Here's message one, then message two,

420
00:20:41,532 --> 00:20:43,226
and message three goes up, and then we

421
00:20:43,248 --> 00:20:46,570
go from FC to FF, and here's message

422
00:20:46,640 --> 00:20:49,114
five, and then we go to Fe. And this

423
00:20:49,152 --> 00:20:51,530
could correspond this sequence to

424
00:20:51,680 --> 00:20:53,982
dynamic time programming EFE or

425
00:20:54,036 --> 00:20:57,326
sophisticated inference. There are a

426
00:20:57,348 --> 00:20:59,290
couple of problems with this approach,

427
00:20:59,370 --> 00:21:03,054
which basically with having the user to

428
00:21:03,172 --> 00:21:05,438
specify a clever algorithm, first of

429
00:21:05,444 --> 00:21:07,906
all, you have to be a specialist to do

430
00:21:07,928 --> 00:21:10,014
it right. Only these are very clever

431
00:21:10,062 --> 00:21:14,002
people. That means that if we let

432
00:21:14,056 --> 00:21:17,346
it leave it to say to

433
00:21:17,368 --> 00:21:20,438
an engineer in a company, well, it's a

434
00:21:20,444 --> 00:21:21,926
high probability he's not going to get

435
00:21:21,948 --> 00:21:24,886
it right. That's very unfortunate. But

436
00:21:24,908 --> 00:21:28,182
there is another issue, and that is that

437
00:21:28,316 --> 00:21:31,446
in a sense, it's a global variable in

438
00:21:31,468 --> 00:21:33,606
the message passing schedule. All nodes

439
00:21:33,638 --> 00:21:36,234
are visited, because if a node would not

440
00:21:36,272 --> 00:21:38,858
be visited, then we shouldn't have it in

441
00:21:38,864 --> 00:21:41,398
the graph. And that means if one node

442
00:21:41,494 --> 00:21:44,566
crashes, basically the message passing

443
00:21:44,598 --> 00:21:46,926
schedule is invalid. I have to reset my

444
00:21:46,948 --> 00:21:49,818
system. And if you fly a drone, if it's

445
00:21:49,834 --> 00:21:52,398
deployed and it's out in the field and a

446
00:21:52,404 --> 00:21:55,406
node crashes, a transistor burns out,

447
00:21:55,588 --> 00:21:57,586
and I have to totally reset now my

448
00:21:57,608 --> 00:21:59,234
system, I have to compute a new message

449
00:21:59,352 --> 00:22:02,354
passing schedule. Then you're not doing

450
00:22:02,392 --> 00:22:04,786
inference and your drone flies into the

451
00:22:04,808 --> 00:22:07,578
wall. So this is not robust.

452
00:22:07,774 --> 00:22:11,174
And it also for the same reason

453
00:22:11,292 --> 00:22:14,866
we may actually want to take out a node.

454
00:22:15,058 --> 00:22:17,926
We may want to prune a node, we want to

455
00:22:17,948 --> 00:22:21,946
do structural adaptation. And we

456
00:22:21,968 --> 00:22:23,754
can't do structural adaptation because

457
00:22:23,792 --> 00:22:26,666
we have to reset the system, recompute a

458
00:22:26,688 --> 00:22:29,530
message passing schedule. So this

459
00:22:29,600 --> 00:22:32,410
procedural style where an engineer

460
00:22:33,730 --> 00:22:35,982
specifies which message comes after this

461
00:22:36,036 --> 00:22:38,926
message has some disadvantages. It's not

462
00:22:38,948 --> 00:22:41,886
very robust. And if you want to do it

463
00:22:41,908 --> 00:22:43,646
very clever, you have to be really a

464
00:22:43,668 --> 00:22:47,474
specialist. So a better system

465
00:22:47,592 --> 00:22:50,370
is what we call reactive message

466
00:22:50,440 --> 00:22:54,238
passing. And it's very related

467
00:22:54,414 --> 00:22:58,338
to what was in the first session called

468
00:22:58,504 --> 00:23:01,622
the actor model. Keith Duggar had a nice

469
00:23:01,676 --> 00:23:05,014
presentation on the actor model. So what

470
00:23:05,052 --> 00:23:07,942
we will do is we will say we will not

471
00:23:07,996 --> 00:23:10,366
have a global message passing schedule.

472
00:23:10,498 --> 00:23:13,578
The engineer will not specify anything

473
00:23:13,664 --> 00:23:16,026
anymore. The inference code that an

474
00:23:16,048 --> 00:23:18,746
engineer will have to write is just say,

475
00:23:18,848 --> 00:23:21,430
react to any free energy minimization

476
00:23:21,510 --> 00:23:24,046
opportunity. In other words, there is no

477
00:23:24,148 --> 00:23:25,518
inference code. It's completely

478
00:23:25,604 --> 00:23:30,106
automated. And we will replace

479
00:23:30,298 --> 00:23:32,862
this global message passing schedule by

480
00:23:32,916 --> 00:23:35,762
local triggering inside the node. So

481
00:23:35,816 --> 00:23:38,814
each node is now just an autonomous

482
00:23:38,862 --> 00:23:41,246
system that's interested in minimizing

483
00:23:41,278 --> 00:23:44,766
its free energy. It can do so by sending

484
00:23:44,798 --> 00:23:48,482
out messages. And when will it do so?

485
00:23:48,536 --> 00:23:51,342
Well, it receives messages, and then

486
00:23:51,496 --> 00:23:53,238
when it looks at these messages and it

487
00:23:53,244 --> 00:23:54,806
feels like, oh, there is an opportunity

488
00:23:54,908 --> 00:23:57,974
for me to minimize free energy by or

489
00:23:58,012 --> 00:23:59,986
expected free energy energy by sending

490
00:24:00,018 --> 00:24:01,706
out a message. Then we'll send out a

491
00:24:01,728 --> 00:24:05,114
message and each node will do so by

492
00:24:05,152 --> 00:24:07,754
itself asynchronously so you get

493
00:24:07,952 --> 00:24:10,074
parallel distributed processing, or

494
00:24:10,112 --> 00:24:12,154
concurrent processing, as Keith called

495
00:24:12,192 --> 00:24:15,934
it. In principle, you could play this

496
00:24:15,972 --> 00:24:19,120
game on many computers at the same time,

497
00:24:19,810 --> 00:24:23,710
and so you get tremendous advantages.

498
00:24:25,510 --> 00:24:27,202
First of all, you don't have to write

499
00:24:27,256 --> 00:24:29,826
difficult code. Second of all, you can

500
00:24:29,848 --> 00:24:33,122
do multithreading or you can

501
00:24:33,256 --> 00:24:35,154
run it on multiple computers at the same

502
00:24:35,192 --> 00:24:38,840
time. And there's also

503
00:24:39,450 --> 00:24:44,130
robustness advantages

504
00:24:44,210 --> 00:24:47,826
because if a node crashes, then there's

505
00:24:47,858 --> 00:24:49,734
nothing that stops the system from just

506
00:24:49,772 --> 00:24:51,786
finding another path, right? If this

507
00:24:51,808 --> 00:24:55,206
node crashes, this path from here's

508
00:24:55,238 --> 00:24:57,446
message three, this path now doesn't

509
00:24:57,478 --> 00:24:59,978
work. So I cannot send anything to Fe

510
00:25:00,064 --> 00:25:03,418
anymore from X. Well, then I

511
00:25:03,424 --> 00:25:05,630
just sent a new message here. Why not?

512
00:25:05,780 --> 00:25:09,086
It's like when water falls down a

513
00:25:09,108 --> 00:25:11,774
mountain and it zigzags its way down

514
00:25:11,812 --> 00:25:14,606
into the value and you halfway put up an

515
00:25:14,628 --> 00:25:17,170
obstruction. It just finds another path,

516
00:25:18,310 --> 00:25:21,426
not the preferred path. This has

517
00:25:21,448 --> 00:25:24,494
to find, well, the second best path,

518
00:25:24,542 --> 00:25:25,826
because the first path has been

519
00:25:25,848 --> 00:25:27,966
obstructed, right? And that's what's

520
00:25:27,998 --> 00:25:29,714
going to happen in this system as well,

521
00:25:29,752 --> 00:25:32,562
right? That's just how nature works. It

522
00:25:32,616 --> 00:25:34,274
tries to find the best path, the easiest

523
00:25:34,322 --> 00:25:35,766
path, and if that's not available, then

524
00:25:35,788 --> 00:25:37,586
we do the second best path. And that's

525
00:25:37,618 --> 00:25:39,586
also what you can do with reactive

526
00:25:39,618 --> 00:25:42,406
message passing. So you can prune nodes.

527
00:25:42,438 --> 00:25:44,810
You can do structural adaptation,

528
00:25:45,630 --> 00:25:49,146
and it's far more robust. And you can

529
00:25:49,168 --> 00:25:53,942
also do chance encounters

530
00:25:54,006 --> 00:25:56,766
with other drones, right? Drones that

531
00:25:56,788 --> 00:25:58,734
get close can start communicating with

532
00:25:58,772 --> 00:26:01,422
each other, and when they fire away,

533
00:26:01,476 --> 00:26:03,262
they stop communicating with each other.

534
00:26:03,316 --> 00:26:05,746
And this is no problem, because you can

535
00:26:05,768 --> 00:26:10,002
basically change who

536
00:26:10,136 --> 00:26:13,966
change nodes

537
00:26:13,998 --> 00:26:15,794
can change on the fly, who they

538
00:26:15,832 --> 00:26:18,006
communicate to and who they want to

539
00:26:18,028 --> 00:26:21,526
listen to. That's the

540
00:26:21,548 --> 00:26:24,294
way nature works, and also how it works

541
00:26:24,332 --> 00:26:27,414
when we do reactive programming and

542
00:26:27,452 --> 00:26:30,370
reactive message passing. So,

543
00:26:30,540 --> 00:26:33,834
in summary, we're interested in

544
00:26:33,872 --> 00:26:37,046
automating inference, in active

545
00:26:37,078 --> 00:26:40,582
inference agents, right? Because it's

546
00:26:40,726 --> 00:26:43,982
an operation that's basically only for

547
00:26:44,036 --> 00:26:46,954
experts. And this active inference

548
00:26:47,002 --> 00:26:49,610
technology is not going to be successful

549
00:26:49,690 --> 00:26:52,766
unless we get more people, let's say,

550
00:26:52,788 --> 00:26:55,494
democracies it, and we get competent

551
00:26:55,642 --> 00:26:58,962
engineers being able to develop good

552
00:26:59,016 --> 00:27:00,834
agents, right? You shouldn't have to be

553
00:27:00,872 --> 00:27:03,602
a top specialist in the world to develop

554
00:27:03,656 --> 00:27:06,962
an active inference agent. Now, in order

555
00:27:07,016 --> 00:27:10,214
to automate inference, you must do

556
00:27:10,252 --> 00:27:11,846
message passing, and I've talked about

557
00:27:11,868 --> 00:27:14,294
that for efficiency. I've also talked

558
00:27:14,332 --> 00:27:17,334
about which messages to pass. Not

559
00:27:17,372 --> 00:27:19,654
necessarily do you have to follow this

560
00:27:19,692 --> 00:27:23,034
framework, but constrained better Free

561
00:27:23,072 --> 00:27:25,734
energy framework is very convenient.

562
00:27:25,782 --> 00:27:27,434
It's an umbrella framework that

563
00:27:27,552 --> 00:27:31,180
basically goes over

564
00:27:31,950 --> 00:27:35,262
all the interesting other message

565
00:27:35,396 --> 00:27:38,510
passing computations. When message

566
00:27:38,580 --> 00:27:40,666
passing, reactive message passing, it's

567
00:27:40,698 --> 00:27:42,606
fully automated, so you don't have to

568
00:27:42,628 --> 00:27:45,790
write any code anymore. In principle,

569
00:27:47,090 --> 00:27:48,714
you can do parallel distributed

570
00:27:48,762 --> 00:27:50,494
processing. It's robust structural

571
00:27:50,542 --> 00:27:52,254
changes. You can learn new inference

572
00:27:52,302 --> 00:27:55,860
pathways. So lots of advantages here.

573
00:27:56,230 --> 00:28:00,226
Now, how do we do it? I like

574
00:28:00,248 --> 00:28:02,594
to introduce a toolbox that we've been

575
00:28:02,632 --> 00:28:06,710
working on called ARX infer. And we

576
00:28:06,780 --> 00:28:09,206
do that with my lab here at the

577
00:28:09,228 --> 00:28:11,306
university. I'm here in Eindhoven in the

578
00:28:11,328 --> 00:28:12,986
south of the Netherlands, and we have a

579
00:28:13,008 --> 00:28:15,980
lab. The lab is called BIS lab. Here are

580
00:28:16,910 --> 00:28:19,740
postdocs and assistant professors and

581
00:28:20,190 --> 00:28:22,838
PhD students, and we've been working on

582
00:28:22,864 --> 00:28:25,760
this for many years. And some of these,

583
00:28:28,210 --> 00:28:30,990
like Albert and Ismail and Tyce,

584
00:28:31,490 --> 00:28:35,054
have written dissertations. And our

585
00:28:35,092 --> 00:28:37,278
best work, we have consolidated that in

586
00:28:37,284 --> 00:28:40,414
a toolbox. And the toolbox is called

587
00:28:40,452 --> 00:28:43,138
Arcs infer. And if you want to have a

588
00:28:43,144 --> 00:28:44,338
look, you can go to the website,

589
00:28:44,424 --> 00:28:49,802
arcsinfer. ML. And Arctinfur

590
00:28:49,886 --> 00:28:52,566
works in the way that I've just

591
00:28:52,588 --> 00:28:54,854
discussed. It does message passing. It

592
00:28:54,892 --> 00:28:56,742
tries to minimize constraint, better

593
00:28:56,796 --> 00:28:59,718
free energy. That means it can come up

594
00:28:59,724 --> 00:29:01,986
with all kinds of message passing

595
00:29:02,018 --> 00:29:05,626
algorithms. It will do it in

596
00:29:05,648 --> 00:29:09,162
a reactive way, and it will try to do it

597
00:29:09,216 --> 00:29:11,258
in real time and low power and all the

598
00:29:11,264 --> 00:29:12,886
KPIs that we're talking about. Now,

599
00:29:12,928 --> 00:29:15,306
it's, of course, not done, but it's

600
00:29:15,338 --> 00:29:18,894
functional and like to show some

601
00:29:18,932 --> 00:29:22,426
demos. And I will leave it to Dimitri

602
00:29:22,458 --> 00:29:24,554
and Bart, who are two advanced PhD

603
00:29:24,602 --> 00:29:28,610
students in my lab to show the demos.

604
00:29:30,710 --> 00:29:34,370
So I'm going to stop sharing.

605
00:29:37,350 --> 00:29:40,150
Awesome. Thank you, Bert. Great talk.

606
00:29:40,220 --> 00:29:43,558
Sure. Can you hear?

607
00:29:43,724 --> 00:29:46,854
Yeah. Yeah. Okay. I will try to

608
00:29:46,892 --> 00:29:48,920
share my screen. Okay.

609
00:29:50,010 --> 00:29:53,420
So you should see it now. Looks good.

610
00:29:53,950 --> 00:29:56,762
Okay, so, yeah, hello to everyone,

611
00:29:56,896 --> 00:29:59,206
I'm admitry. Baghaiv so I'm a PhD

612
00:29:59,238 --> 00:30:00,886
student in Bioslab in Einhoven

613
00:30:00,918 --> 00:30:03,646
University of Technology, and yes, I

614
00:30:03,668 --> 00:30:05,870
have a small presentation about actual

615
00:30:05,940 --> 00:30:09,166
software developments in so

616
00:30:09,268 --> 00:30:11,934
over the past few years, we have

617
00:30:11,972 --> 00:30:14,654
significantly improved our tools. And

618
00:30:14,692 --> 00:30:17,182
basically my entire PhD was dedicated to

619
00:30:17,236 --> 00:30:19,666
implement this idea, which Beard was

620
00:30:19,688 --> 00:30:22,066
talking about, like implementing the

621
00:30:22,088 --> 00:30:23,790
variation of reactive message passing.

622
00:30:23,870 --> 00:30:25,426
And in this presentation, I just want to

623
00:30:25,448 --> 00:30:28,742
show you what you can actually do using

624
00:30:28,796 --> 00:30:32,486
this theory under the hood. Okay,

625
00:30:32,588 --> 00:30:34,994
so basically, in order to automate

626
00:30:35,042 --> 00:30:36,834
active inference, we need to automate

627
00:30:36,882 --> 00:30:39,734
Bayesian inference. And we have already

628
00:30:39,772 --> 00:30:42,826
a lot of solutions for that, such as

629
00:30:43,008 --> 00:30:45,610
Pyro NumPy, which is funded by Google,

630
00:30:45,680 --> 00:30:47,750
Info. Net is funded by Microsoft,

631
00:30:47,830 --> 00:30:49,866
turing is in July, PIMC, and many,

632
00:30:49,888 --> 00:30:53,434
many. And basically these

633
00:30:53,472 --> 00:30:56,106
solutions are really good and they're

634
00:30:56,138 --> 00:30:58,302
really good at prototyping as well. But

635
00:30:58,356 --> 00:31:01,054
our goal is eventually to be able to

636
00:31:01,092 --> 00:31:03,582
deploy these kind of systems, not just

637
00:31:03,636 --> 00:31:06,146
prototype. And we are really focusing on

638
00:31:06,168 --> 00:31:07,954
these particular properties for this

639
00:31:07,992 --> 00:31:10,994
automated Bayesian inference. So it must

640
00:31:11,032 --> 00:31:13,906
be low power, adaptive, real time

641
00:31:14,008 --> 00:31:16,574
scalable. It also must be user friendly

642
00:31:16,622 --> 00:31:18,626
at the end and must support a large

643
00:31:18,728 --> 00:31:21,106
scope of models if we want it to be

644
00:31:21,128 --> 00:31:24,874
useful. In Bioslab,

645
00:31:24,942 --> 00:31:27,974
we want to build such a software with

646
00:31:28,012 --> 00:31:31,286
such nice properties and

647
00:31:31,308 --> 00:31:33,226
it's always about trade offs, right? So

648
00:31:33,248 --> 00:31:34,858
we do something better in one particular

649
00:31:34,944 --> 00:31:36,902
domain and maybe other software

650
00:31:36,966 --> 00:31:39,098
libraries, they might be better in a

651
00:31:39,104 --> 00:31:41,194
different domain, but we are really

652
00:31:41,232 --> 00:31:43,340
focusing on this particular property.

653
00:31:44,430 --> 00:31:46,686
And so, yes, I will reiterate a little

654
00:31:46,708 --> 00:31:48,494
bit Bert's presentation. So how do we

655
00:31:48,532 --> 00:31:50,526
achieve this? So imagine we have an

656
00:31:50,548 --> 00:31:53,422
environment and we have an agent, and

657
00:31:53,476 --> 00:31:55,658
the agent takes some actions and the

658
00:31:55,684 --> 00:31:58,754
agent basically what he needs is to come

659
00:31:58,792 --> 00:32:00,770
up with some sort of good enough

660
00:32:00,840 --> 00:32:03,090
probabilistic model of its environment

661
00:32:03,830 --> 00:32:06,210
in order to do patient inference.

662
00:32:06,870 --> 00:32:09,094
And in our framework, we encode the

663
00:32:09,132 --> 00:32:11,302
model as a factor graph, which not only

664
00:32:11,356 --> 00:32:14,102
models the observations, but also

665
00:32:14,156 --> 00:32:16,520
actions and desired future.

666
00:32:17,930 --> 00:32:21,334
And this approach allows us to decompose

667
00:32:21,462 --> 00:32:23,322
these complex relationships between

668
00:32:23,376 --> 00:32:26,474
variables and hidden states into some

669
00:32:26,512 --> 00:32:29,126
kind of structure and local blocks.

670
00:32:29,318 --> 00:32:32,490
And it's not a black box anymore.

671
00:32:33,250 --> 00:32:35,326
And the model itself may have some sort

672
00:32:35,348 --> 00:32:38,138
of background motivation interpretation.

673
00:32:38,234 --> 00:32:40,574
It may encode your prior knowledge about

674
00:32:40,692 --> 00:32:44,030
some particular physical system and

675
00:32:44,100 --> 00:32:46,670
the locality of these blocks basically

676
00:32:46,740 --> 00:32:49,346
allows you to scale to millions of

677
00:32:49,368 --> 00:32:51,518
variables and hidden states. It allows

678
00:32:51,534 --> 00:32:53,794
you to pre optimize it maybe, or maybe

679
00:32:53,832 --> 00:32:56,382
use some sort of different approximation

680
00:32:56,446 --> 00:32:59,686
strategies in different places. So it

681
00:32:59,708 --> 00:33:02,406
allows a lot of very nice properties as

682
00:33:02,428 --> 00:33:05,382
well. And we use reactive message

683
00:33:05,436 --> 00:33:07,746
passing to run actual variational

684
00:33:07,778 --> 00:33:10,494
Bayesian inference. It uses reactive

685
00:33:10,562 --> 00:33:12,614
programming under the hood to minimize

686
00:33:12,662 --> 00:33:14,166
the approximation to the variational

687
00:33:14,198 --> 00:33:16,954
free energy. And yes, as Bert also

688
00:33:16,992 --> 00:33:19,482
mentioned, it's very much related to

689
00:33:19,536 --> 00:33:23,754
actor model. And basically

690
00:33:23,872 --> 00:33:26,334
in Ariks and fur. You can think of

691
00:33:26,372 --> 00:33:30,654
different nodes as actors themselves and

692
00:33:30,692 --> 00:33:32,650
they have basically one single purpose

693
00:33:32,730 --> 00:33:34,734
is to send a variational message that

694
00:33:34,772 --> 00:33:37,442
minimizes free energy. This is a very

695
00:33:37,496 --> 00:33:39,454
short and very high level description,

696
00:33:39,502 --> 00:33:41,362
but it is essentially what is happening

697
00:33:41,416 --> 00:33:44,750
under lipid. So we are not treating

698
00:33:44,910 --> 00:33:47,474
different agents which interact with

699
00:33:47,512 --> 00:33:50,118
each other as actors, but we also treat

700
00:33:50,284 --> 00:33:52,466
the actual components of the underlying

701
00:33:52,498 --> 00:33:55,158
model as actors themselves. It's like a

702
00:33:55,164 --> 00:33:57,654
very hierarchical structure. So this is

703
00:33:57,692 --> 00:34:01,354
the main central idea of this

704
00:34:01,392 --> 00:34:05,066
inference. So here

705
00:34:05,168 --> 00:34:07,354
for example, first example, we can do an

706
00:34:07,392 --> 00:34:10,474
inference in a dynamical system. This

707
00:34:10,512 --> 00:34:12,506
example, which is quite old already, I

708
00:34:12,528 --> 00:34:15,374
think it's like two years ago. So we

709
00:34:15,412 --> 00:34:16,974
track a position of the object given

710
00:34:17,012 --> 00:34:19,262
some noisy measurements which are

711
00:34:19,316 --> 00:34:23,150
indicated by green dots, the actual real

712
00:34:23,220 --> 00:34:25,214
signal, we cannot observe it, but we

713
00:34:25,252 --> 00:34:28,226
just can plot. It is shown as blue and

714
00:34:28,248 --> 00:34:30,834
the inferred signal is shown as red and

715
00:34:30,872 --> 00:34:33,006
the data set is infinite. The inference

716
00:34:33,038 --> 00:34:34,626
end just reacts on it and does not

717
00:34:34,648 --> 00:34:37,886
assume any particular data size. Simply

718
00:34:37,918 --> 00:34:39,426
reacts in your observation as soon as

719
00:34:39,448 --> 00:34:42,054
possible. Yeah, I'm actually not sure

720
00:34:42,092 --> 00:34:44,760
how smoothly Zoom shares my screen.

721
00:34:45,370 --> 00:34:47,766
Maybe you can see it's a bit lagging in

722
00:34:47,788 --> 00:34:49,462
the animations, I'm not sure because

723
00:34:49,516 --> 00:34:51,274
maybe zoom does not share it on a full

724
00:34:51,312 --> 00:34:54,586
frame rate. And also on the right hand

725
00:34:54,608 --> 00:34:58,026
side you can see how we define models in

726
00:34:58,048 --> 00:34:59,898
our framework. We use Julia as a

727
00:34:59,904 --> 00:35:03,370
programming language. Basically,

728
00:35:03,440 --> 00:35:04,718
this is everything that you need to

729
00:35:04,724 --> 00:35:06,494
define this particular model and run

730
00:35:06,532 --> 00:35:10,126
inference on a data set. And actually I

731
00:35:10,228 --> 00:35:12,942
literally spent more days to plot it

732
00:35:12,996 --> 00:35:15,806
instead of inference. So inference was

733
00:35:15,828 --> 00:35:18,274
an easiest part for me, plotting was way

734
00:35:18,312 --> 00:35:20,734
much harder to relate to user

735
00:35:20,782 --> 00:35:24,034
friendliness. And we actually have

736
00:35:24,072 --> 00:35:27,054
plans to improve our model specification

737
00:35:27,102 --> 00:35:28,934
language, make it even easier. So now

738
00:35:28,972 --> 00:35:30,886
for technical reasons, we have some

739
00:35:31,068 --> 00:35:32,742
auxiliary statements in the model

740
00:35:32,796 --> 00:35:34,646
specification language, but we are

741
00:35:34,668 --> 00:35:38,102
working to improve that and make it even

742
00:35:38,156 --> 00:35:41,990
easier. This is another

743
00:35:42,060 --> 00:35:44,022
example which is similar to the previous

744
00:35:44,086 --> 00:35:46,346
one, that uses much more complex and

745
00:35:46,368 --> 00:35:48,214
linear dynamical system of the double

746
00:35:48,262 --> 00:35:51,882
pendulum. And the system is chaotic and

747
00:35:51,936 --> 00:35:54,270
we can observe only a small part of it

748
00:35:54,340 --> 00:35:56,526
with a lot of noise, also indicated as a

749
00:35:56,548 --> 00:35:59,630
green dots. And nevertheless,

750
00:36:00,930 --> 00:36:04,346
given good enough model, you can infer

751
00:36:04,538 --> 00:36:06,882
the other hidden states with pretty much

752
00:36:07,016 --> 00:36:10,066
high precision and the code needed for

753
00:36:10,088 --> 00:36:12,020
that is also relatively short.

754
00:36:15,510 --> 00:36:19,026
We also have examples with active

755
00:36:19,058 --> 00:36:21,174
inference agents that interact with

756
00:36:21,212 --> 00:36:25,734
their environment. So the

757
00:36:25,772 --> 00:36:29,234
left up shows mountain

758
00:36:29,282 --> 00:36:31,834
car problem, very famous problem. The

759
00:36:31,872 --> 00:36:33,926
left bottom side shows an active

760
00:36:33,958 --> 00:36:36,378
inference agent which tries to control

761
00:36:36,464 --> 00:36:38,986
the inverted pendulum from falling in

762
00:36:39,008 --> 00:36:41,820
the windy conditions it reacts in wind.

763
00:36:42,510 --> 00:36:44,494
We also have a demo of an agent that

764
00:36:44,532 --> 00:36:47,322
controls a pendulum in an ever changing

765
00:36:47,386 --> 00:36:50,062
environment. So on the right side you

766
00:36:50,116 --> 00:36:53,022
see a pendulum with an engine and engine

767
00:36:53,076 --> 00:36:56,494
has limited power and. The agent itself

768
00:36:56,612 --> 00:36:58,562
needs to reach the goal and the goal is

769
00:36:58,616 --> 00:37:00,450
indicated as a red circle.

770
00:37:02,470 --> 00:37:04,274
Basically, in this demo we can change

771
00:37:04,312 --> 00:37:06,226
the environment in real time and see how

772
00:37:06,248 --> 00:37:08,306
the agent reacts. So we can change the

773
00:37:08,328 --> 00:37:10,546
mass of the pebble among its length or

774
00:37:10,648 --> 00:37:13,410
the amount of noise in the measurements,

775
00:37:13,490 --> 00:37:15,654
or we can change the goal, we can change

776
00:37:15,692 --> 00:37:18,006
maximum engine power, et cetera. So the

777
00:37:18,028 --> 00:37:20,482
agent will still try to infer the best

778
00:37:20,556 --> 00:37:22,938
possible course of actions in order to

779
00:37:22,944 --> 00:37:25,494
reach its goal and it just never stops

780
00:37:25,542 --> 00:37:29,418
reacting. It's also actually possible

781
00:37:29,504 --> 00:37:33,454
to restrict engine power such that it

782
00:37:33,492 --> 00:37:35,646
will not longer possible to reach the

783
00:37:35,668 --> 00:37:37,486
goal, right? But the agent will still

784
00:37:37,508 --> 00:37:40,798
try. We have

785
00:37:40,884 --> 00:37:43,354
other cool demos with smart navigation

786
00:37:43,402 --> 00:37:46,846
and collision avoidance which are still

787
00:37:46,868 --> 00:37:48,834
under active research and the code for

788
00:37:48,872 --> 00:37:50,946
them is not available publicly it will

789
00:37:50,968 --> 00:37:53,106
be soon available, but for example in

790
00:37:53,128 --> 00:37:55,906
this example we can define a set of

791
00:37:55,928 --> 00:37:58,726
agents with their boundaries and a set

792
00:37:58,748 --> 00:38:01,894
of their destinations. And we can see

793
00:38:01,932 --> 00:38:03,874
how they try to resolve their routes

794
00:38:03,922 --> 00:38:04,710
altogether.

795
00:38:07,050 --> 00:38:09,766
And we can have some static obstacles in

796
00:38:09,788 --> 00:38:12,762
the map. We can see how agents can find

797
00:38:12,816 --> 00:38:16,394
their most optimal path in order

798
00:38:16,432 --> 00:38:19,274
to reach their goals and avoid any

799
00:38:19,312 --> 00:38:22,854
possible collision. And it's also not

800
00:38:22,912 --> 00:38:26,366
necessary to have static obstacles, the

801
00:38:26,388 --> 00:38:29,006
obstacles themselves may move. So on

802
00:38:29,028 --> 00:38:31,694
this demo we have hundreds of agents

803
00:38:31,812 --> 00:38:34,314
that navigate through a map of obstacles

804
00:38:34,362 --> 00:38:36,286
that move from bottom to top to the

805
00:38:36,308 --> 00:38:39,026
circles or obstacles. And agents are

806
00:38:39,128 --> 00:38:41,938
depicted as small dots and they need to

807
00:38:41,944 --> 00:38:44,254
go from left to right, basically avoid

808
00:38:44,302 --> 00:38:45,730
any sort of collision.

809
00:38:47,670 --> 00:38:50,546
And as I also mentioned, we want to

810
00:38:50,568 --> 00:38:52,514
perform efficient and real time

811
00:38:52,552 --> 00:38:54,982
inference, but we also to do it like low

812
00:38:55,036 --> 00:38:57,654
power, low performance on low

813
00:38:57,692 --> 00:38:59,666
performance devices such as Raspberry,

814
00:38:59,698 --> 00:39:02,794
Pi or Coolpy as an example. And we have

815
00:39:02,832 --> 00:39:05,466
some results of successfully running the

816
00:39:05,488 --> 00:39:07,178
Bayesian audio source separation, for

817
00:39:07,184 --> 00:39:09,850
example, on coolpy. So it is actually

818
00:39:09,920 --> 00:39:13,086
possible we try

819
00:39:13,108 --> 00:39:16,014
to run active inference agents also on

820
00:39:16,052 --> 00:39:19,114
Coolpy. So as the aforementioned

821
00:39:19,162 --> 00:39:23,326
inverted pendulum, and as

822
00:39:23,348 --> 00:39:27,090
I mentioned, we also need to

823
00:39:27,240 --> 00:39:30,082
have a large model scope and basically

824
00:39:30,136 --> 00:39:32,754
RICS infer has not been designed to

825
00:39:32,792 --> 00:39:34,334
solve any of the aforementioned

826
00:39:34,382 --> 00:39:37,602
problems. Specifically, we have a large

827
00:39:37,656 --> 00:39:39,206
set of different examples in our

828
00:39:39,228 --> 00:39:41,462
repository, different models, different

829
00:39:41,516 --> 00:39:43,590
data, different inference constraints.

830
00:39:44,250 --> 00:39:46,514
We have examples for linear regression,

831
00:39:46,562 --> 00:39:48,162
hidden Markov model altogether, grace

832
00:39:48,226 --> 00:39:51,186
model hierarchy models, misheard models,

833
00:39:51,378 --> 00:39:53,994
Gaussian process and so, right? So this

834
00:39:54,032 --> 00:39:55,930
approach is very versatile.

835
00:39:56,670 --> 00:40:00,506
And for example, if you

836
00:40:00,528 --> 00:40:03,094
compare it with sort of a conventional

837
00:40:03,142 --> 00:40:05,454
software libraries where you let's say

838
00:40:05,492 --> 00:40:07,230
have a library that solves a common

839
00:40:07,300 --> 00:40:09,322
filter, might be a very great library,

840
00:40:09,386 --> 00:40:12,058
maybe super fast, have top performance

841
00:40:12,154 --> 00:40:14,510
works great and very reliable,

842
00:40:15,830 --> 00:40:18,430
super good. But then you are constrained

843
00:40:18,590 --> 00:40:21,170
by this particular model common

844
00:40:21,240 --> 00:40:22,930
filtering, right? And you can't really

845
00:40:23,000 --> 00:40:26,674
change it much. In Ericsson Fur we

846
00:40:26,712 --> 00:40:30,182
are free to define our own models which

847
00:40:30,236 --> 00:40:32,342
we can pretty much easily define a model

848
00:40:32,396 --> 00:40:35,234
that essentially would act equivalently

849
00:40:35,282 --> 00:40:37,606
to common filtering equation. And so

850
00:40:37,628 --> 00:40:39,026
basically in the demo that I showed

851
00:40:39,058 --> 00:40:41,450
before about object tracking, it was

852
00:40:41,520 --> 00:40:44,682
essentially a common filter but was

853
00:40:44,736 --> 00:40:47,500
written in a probabilistic model.

854
00:40:49,470 --> 00:40:52,026
So yeah, that was my small addition to

855
00:40:52,048 --> 00:40:54,894
Bear's presentation. So our software is

856
00:40:54,932 --> 00:40:58,202
free MIT license and it's open source

857
00:40:58,266 --> 00:41:01,614
available on GitHub. Yeah, and we would

858
00:41:01,652 --> 00:41:04,238
be happy thanks to be able to present

859
00:41:04,324 --> 00:41:06,880
where we would be happy to answer. Any,

860
00:41:07,330 --> 00:41:08,080
thanks.

861
00:41:14,760 --> 00:41:17,124
Awesome. All right, I'll just ask a

862
00:41:17,162 --> 00:41:21,400
quick question. In the chat Marco asks

863
00:41:22,460 --> 00:41:24,740
sorry if I missed it are the collision

864
00:41:24,820 --> 00:41:27,336
avoidance demos real time adapting to

865
00:41:27,358 --> 00:41:29,016
other agents behavior or is it a

866
00:41:29,038 --> 00:41:31,000
collectively pre computed path?

867
00:41:33,020 --> 00:41:35,992
So basically they are not super real

868
00:41:36,046 --> 00:41:38,136
time, they're kind of fast to compute

869
00:41:38,168 --> 00:41:40,412
this path, like maybe 5 seconds or so.

870
00:41:40,546 --> 00:41:42,716
But we are basically working to make it

871
00:41:42,738 --> 00:41:44,556
real time. So we know what is the

872
00:41:44,578 --> 00:41:47,996
problem, we know where to improve and we

873
00:41:48,018 --> 00:41:51,090
will make it real time. Yes, almost.

874
00:41:53,220 --> 00:41:55,104
Next question. Do you have some

875
00:41:55,142 --> 00:41:57,404
comparative data with other methods?

876
00:41:57,452 --> 00:41:59,844
And just more generally, what kinds of

877
00:41:59,882 --> 00:42:02,724
benchmarks or when you're talking with

878
00:42:02,762 --> 00:42:04,516
industry in different settings, what are

879
00:42:04,538 --> 00:42:08,004
people looking for that killer app of

880
00:42:08,042 --> 00:42:09,364
active inference? Or what are they

881
00:42:09,402 --> 00:42:12,200
looking for their key measures?

882
00:42:14,140 --> 00:42:16,744
So I personally have a big paper about

883
00:42:16,862 --> 00:42:19,412
comparison with sampling based methods

884
00:42:19,476 --> 00:42:23,280
like HMC and also in my PhD thesis,

885
00:42:23,380 --> 00:42:25,208
there will be a comparison with nuts,

886
00:42:25,304 --> 00:42:27,500
also other sampling based methods.

887
00:42:28,240 --> 00:42:30,844
So, long story short, sampling based

888
00:42:30,882 --> 00:42:33,516
methods cannot really run this kind of

889
00:42:33,538 --> 00:42:35,790
sophisticated inference in real time.

890
00:42:36,240 --> 00:42:38,496
They're very time consuming. They do not

891
00:42:38,518 --> 00:42:40,464
really scale well to large problems

892
00:42:40,582 --> 00:42:42,236
which is really needed for active

893
00:42:42,268 --> 00:42:44,048
inference agents. Because if you have a

894
00:42:44,054 --> 00:42:45,920
large environment, very complicated,

895
00:42:47,060 --> 00:42:49,936
you will have a lot of unknown variables

896
00:42:49,968 --> 00:42:50,870
in your model.

897
00:42:53,480 --> 00:42:56,116
There is a paper that compares and

898
00:42:56,138 --> 00:42:58,144
basically we show that our approach

899
00:42:58,192 --> 00:43:01,144
scales much, much better. So I

900
00:43:01,182 --> 00:43:05,188
personally run on just a regular MacBook

901
00:43:05,364 --> 00:43:07,770
laptop, I run the model with 2 million

902
00:43:08,780 --> 00:43:12,824
unknown variables and it was quite

903
00:43:13,022 --> 00:43:16,608
fast. With sampling based methods

904
00:43:16,804 --> 00:43:19,244
you may find yourself in a model with

905
00:43:19,282 --> 00:43:21,052
like 100 variables and then you wait

906
00:43:21,106 --> 00:43:23,596
like 2 hours or something and then it

907
00:43:23,618 --> 00:43:25,312
turns out that your chain did not

908
00:43:25,366 --> 00:43:28,464
converge or something like

909
00:43:28,502 --> 00:43:29,090
that.

910
00:43:31,860 --> 00:43:34,256
Cool. Yeah, it's people commenting in

911
00:43:34,278 --> 00:43:37,276
the chat like how far message passing

912
00:43:37,308 --> 00:43:40,964
and factor graphs have come. And so to

913
00:43:41,002 --> 00:43:43,124
Bias Lab and to Bert at all, we

914
00:43:43,162 --> 00:43:46,164
definitely appreciate this exciting line

915
00:43:46,202 --> 00:43:47,956
of research. I mean, there's so much to

916
00:43:47,978 --> 00:43:50,116
learn there and sometimes looking at the

917
00:43:50,138 --> 00:43:52,208
equations, it can seem like it's like

918
00:43:52,234 --> 00:43:54,456
written in stone and just sort of the

919
00:43:54,478 --> 00:43:56,232
beginning and the end is, you know,

920
00:43:56,286 --> 00:43:58,776
variational free energy. But then in

921
00:43:58,798 --> 00:44:00,356
your presentations you're really showing

922
00:44:00,388 --> 00:44:03,676
like no, we are hands on. That's where

923
00:44:03,698 --> 00:44:05,164
we get the interpretability, the

924
00:44:05,202 --> 00:44:07,852
modularity, that's where it really is

925
00:44:07,906 --> 00:44:10,044
implemented. And it's like an

926
00:44:10,082 --> 00:44:12,488
information logistics challenge. It's

927
00:44:12,504 --> 00:44:14,572
not like an esoteric philosophy question

928
00:44:14,626 --> 00:44:17,440
at that point. No,

929
00:44:17,590 --> 00:44:21,308
indeed. I should say it's

930
00:44:21,324 --> 00:44:24,144
taken us we are no geniuses, right.

931
00:44:24,182 --> 00:44:27,328
So our lab exists more than eight years,

932
00:44:27,494 --> 00:44:29,252
and you see all the people in the lab,

933
00:44:29,306 --> 00:44:32,836
it's taken us many years with lots of

934
00:44:32,858 --> 00:44:35,476
wrong directions to get this to work to

935
00:44:35,498 --> 00:44:37,812
where it's now. So it's a very long

936
00:44:37,866 --> 00:44:40,996
path. But at this

937
00:44:41,018 --> 00:44:43,604
moment, I'm pretty confident that at

938
00:44:43,642 --> 00:44:47,008
some point in the future and we

939
00:44:47,034 --> 00:44:48,536
don't want to say in three months or in

940
00:44:48,558 --> 00:44:52,168
one year, but we will be able to write a

941
00:44:52,174 --> 00:44:54,612
toolbox that will allow people to design

942
00:44:54,686 --> 00:44:56,476
a generative model and just press a

943
00:44:56,498 --> 00:44:59,096
button and forget about the inference.

944
00:44:59,128 --> 00:45:00,616
You don't have to worry about inference

945
00:45:00,648 --> 00:45:03,736
anymore. It will be fast and automated,

946
00:45:03,848 --> 00:45:05,904
and that will happen, and it will happen

947
00:45:05,942 --> 00:45:09,584
within a few years. And maybe

948
00:45:09,622 --> 00:45:10,864
somebody else will write an even better

949
00:45:10,902 --> 00:45:13,296
toolbox. But I'm pretty confident that

950
00:45:13,318 --> 00:45:16,690
even our toolbox will be able to do

951
00:45:18,440 --> 00:45:21,990
that. People talk about,

952
00:45:23,560 --> 00:45:25,184
why don't we have the success of deep

953
00:45:25,232 --> 00:45:27,012
learning and generative AI, right?

954
00:45:27,066 --> 00:45:29,700
Well, they have the success because of

955
00:45:29,770 --> 00:45:32,584
big data availability of big data, big

956
00:45:32,622 --> 00:45:36,264
computers and toolboxes TensorFlow and

957
00:45:36,302 --> 00:45:38,632
all the successes. We don't need big

958
00:45:38,686 --> 00:45:41,352
data because agents collect their own

959
00:45:41,406 --> 00:45:44,344
data in the field. We don't need big

960
00:45:44,382 --> 00:45:46,750
computers, active influence agents,

961
00:45:48,400 --> 00:45:51,244
they manage their power resources. But

962
00:45:51,282 --> 00:45:54,190
we need a really good toolbox because

963
00:45:55,060 --> 00:45:57,008
programming an active inference agent,

964
00:45:57,094 --> 00:45:58,976
programming the inference by hand is

965
00:45:58,998 --> 00:46:02,304
just not doable. So we need a really

966
00:46:02,342 --> 00:46:05,570
good toolbox that really automates this.

967
00:46:06,260 --> 00:46:09,508
We hope Arcs Infer will be one of the

968
00:46:09,514 --> 00:46:11,508
first toolboxes to do that. I am sure

969
00:46:11,594 --> 00:46:14,804
that other people will also be working

970
00:46:14,842 --> 00:46:16,804
on it, and better toolboxes will come

971
00:46:16,842 --> 00:46:20,908
about. But I think the optimistic

972
00:46:21,104 --> 00:46:24,984
message is that it will happen. And once

973
00:46:25,022 --> 00:46:27,096
we have a toolbox like that, then we can

974
00:46:27,118 --> 00:46:29,704
actually a large community can start

975
00:46:29,742 --> 00:46:31,272
building agents, and we can actually

976
00:46:31,326 --> 00:46:35,208
show deployable agents in

977
00:46:35,214 --> 00:46:37,512
the field that they work. And they work

978
00:46:37,566 --> 00:46:39,740
better than reinforcement learning agent

979
00:46:39,810 --> 00:46:41,950
or whatever is out there. Right.

980
00:46:42,560 --> 00:46:45,792
So I think that's a very positive and

981
00:46:45,846 --> 00:46:48,992
hopeful message. It's what we expect.

982
00:46:49,046 --> 00:46:51,090
It's what we prefer. Yeah.

983
00:46:52,900 --> 00:46:55,730
Any last comments from either of you?

984
00:46:59,240 --> 00:47:01,524
Comments from us? No, I'm just very

985
00:47:01,562 --> 00:47:05,204
happy to get the opportunity, and yeah,

986
00:47:05,242 --> 00:47:10,392
I want to everybody

987
00:47:10,446 --> 00:47:14,152
can download this toolbox. I think

988
00:47:14,286 --> 00:47:16,696
at this moment, you still should be a

989
00:47:16,718 --> 00:47:19,240
programmer to work with the toolbox.

990
00:47:19,580 --> 00:47:22,596
And I hope you're friendly, because it's

991
00:47:22,628 --> 00:47:26,056
not totally polished in

992
00:47:26,078 --> 00:47:27,788
the way that we want. But it's coming,

993
00:47:27,874 --> 00:47:29,532
right? It's coming. In the next years,

994
00:47:29,586 --> 00:47:31,612
there will be a good toolbox for almost

995
00:47:31,666 --> 00:47:33,964
everybody to use, but people that are

996
00:47:34,002 --> 00:47:36,636
interested, even people that are

997
00:47:36,658 --> 00:47:38,816
interested to work. Here at Biaslab, we

998
00:47:38,838 --> 00:47:41,968
have an open position for PhD students,

999
00:47:42,054 --> 00:47:46,080
so we're happy to receive

1000
00:47:46,980 --> 00:47:48,608
emails from people that are interested

1001
00:47:48,694 --> 00:47:50,290
to work with us.

1002
00:47:52,420 --> 00:47:54,720
Thank you. Dimitri, anything in closing?

1003
00:47:56,420 --> 00:47:59,040
No, just that. Thank you again for

1004
00:47:59,110 --> 00:48:01,256
possibility to present. Super nice to be

1005
00:48:01,278 --> 00:48:04,456
here. Cool. Yeah. Well, later in the

1006
00:48:04,478 --> 00:48:06,632
year, we will be discussing your two

1007
00:48:06,686 --> 00:48:10,088
part recent work, and so we're going

1008
00:48:10,094 --> 00:48:12,436
to be getting a lot into the details,

1009
00:48:12,468 --> 00:48:15,396
and I hope that people in the institute

1010
00:48:15,428 --> 00:48:17,976
and the ecosystem will be as excited as

1011
00:48:17,998 --> 00:48:21,480
we all are. Thank you. Thank you.

1012
00:48:21,550 --> 00:48:21,796
Bye.



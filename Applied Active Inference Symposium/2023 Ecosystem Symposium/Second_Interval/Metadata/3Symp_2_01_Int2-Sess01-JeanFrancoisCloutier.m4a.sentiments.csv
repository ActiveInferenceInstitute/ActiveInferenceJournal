start	end	speaker	sentiment	confidence	text
4980	5530	A	0.546256959438324	You.
7500	9096	B	0.9103710651397705	Hello and welcome, everyone.
9198	10200	B	0.894907534122467	Welcome back.
10350	22010	B	0.9226104617118835	This is the second interval of the third applied active inference symposium at the active inference institute on August 22, 2023.
23100	35628	B	0.9708138108253479	This is going to be another packed and exciting interval, and we're kicking it off with Jean Francois Cloutier, a Collective of theorizers First Steps.
35724	40572	B	0.954369068145752	So, JF, thank you for joining and to you for this presentation.
40716	48116	B	0.7273027896881104	And if anybody has questions for this presentation or any of the others, please just put it in the live chat and I'll do it.
48138	48628	B	0.664699375629425	I can.
48714	49124	B	0.7464491128921509	So thanks.
49162	49536	B	0.5880610346794128	JF.
49568	50310	B	0.701943039894104	To you.
54900	56560	A	0.8880262970924377	Thank you, Daniel.
57800	65300	A	0.5320131182670593	I'm a software engineer at a company called Smart Rent, where I write software for smart home systems.
66120	73530	A	0.8372983932495117	I also work on a research project at the Active Inference Institute called the Robotics and Embodied Project.
74620	80650	A	0.7537313103675842	My current focus is unsupervised learning in active inference agents.
81840	100930	A	0.8259961009025574	In my presentation today, which is titled A Collective of theorizers First Steps, I'll first start with a brief recap of the project, then dive into recent progress, and then I'll conclude with what I see as the next steps of the project.
103780	111300	A	0.5200573205947876	Well, since 2017, I've been experimenting with Lego robots and models of cognition.
111880	121690	A	0.7548820972442627	I do this for my own education, because, like I think most of us, I understand something better when I build it.
122460	126776	A	0.8407978415489197	What you see here is my latest robot model.
126958	128276	A	0.7750126719474792	It's a rover.
128468	134780	A	0.8355658054351807	It has a whole bunch of sensors and a number of effectors actuators.
135360	142860	A	0.8313644528388977	As a matter of fact, those sensors and actuators can be understood as forming the marker blanket of my robot.
146330	156310	A	0.8587583899497986	Last year, I presented at the symposium about the history of the project, its then status, and its ambitions.
156730	164470	A	0.7114524245262146	I presented a cognitive model that implemented predictive processing from an active inference perspective.
164630	170006	A	0.6220465898513794	So there were generative models, there was predictions, there was prediction errors.
170198	187374	A	0.6661219596862793	And these generative models animated the robots so that they would roam, avoid obstacles, observe their companion, and also, in a very simplistic way, build a theory of mind of the other robot.
187502	207734	A	0.7272807955741882	They could from observations infer that the other robot had detected food, food being presented here as a sheet of paper on the floor, and then would kind of try to track the other robot to also get to the food and fun stuff like this.
207932	214006	A	0.727120578289032	The implementation combined multi processing and functional computing.
214038	218250	A	0.5074530839920044	There was nothing probabilistic in the implementation.
220830	223114	A	0.8202810287475586	I thought maybe we'd see them in action.
223162	224574	A	0.8745193481445312	That's what's presented last year.
224612	232698	A	0.8691520690917969	But just to get a sense of what they do, I have two robots here, one named Karl and Andy.
232794	236290	A	0.8123002648353577	I named them before I knew I was going to present at the conference.
237350	243090	A	0.7130743861198425	So they benefited from a number of training sessions.
243510	253000	A	0.6888447403907776	They learned how to find which policies would achieve their goals better than others.
253370	256150	A	0.8413662910461426	And one here on the left.
256300	288850	A	0.7718645930290222	Found the food rather right away but got closer to the pedestal where the beacon is that simulates the scent of food feared was getting into a collision and then backs off in a hurry as we'll see the other robot observes all this, sees the robot backing off as being in a panic and decides to share this emotion and backs up as well.
289000	291540	A	0.9712896347045898	So fun stuff.
298180	306390	A	0.7977799773216248	From the beginning I've implemented different cognition models but they all have in common the fact that they implement a society of mind.
307160	308852	A	0.74591463804245	So what is a society of mind?
308906	327710	A	0.7801429629325867	A society of mind is a concept by which the mind is not a monolithic structure but a composition of simple actors, independent actors that interact with each other in simple ways.
328480	334764	A	0.8624064922332764	That view of the mind was put forth by Marvin Minsky 50 years ago.
334802	336320	A	0.7627584934234619	So this is not recent.
340290	349434	A	0.861426830291748	Again, what I presented last year was a society of mind containing a hierarchy of, I call them cognition actors.
349562	359682	A	0.7819588780403137	Each cognition actor is an independent process, each one has a scope, a level of abstraction as well.
359736	381322	A	0.8864918947219849	So for example, you would have a cognition actor that's concerned with the location of food and it would have beliefs and perceptions about food location and these would feed into a higher level cognition actor, let's say food approach which is concerned about getting closer to the food and so forth and so on.
381376	385642	A	0.6659513115882874	These cognition actors communicate again in simple ways.
385696	398138	A	0.653266191482544	This is the society of mind and they communicate by emitting predictions, predictions about the beliefs of other cognition actors and they communicate by emitting prediction errors.
398234	422194	A	0.582653820514679	When predictions made about their beliefs by others are inaccurate that leads to connection actors processing these prediction errors and combining them with their own predictions to create an updated set of perceptions that are synthesized into beliefs.
422242	431770	A	0.7873309850692749	And these beliefs lead to actions in order to eliminate negative beliefs or validate positive beliefs.
432270	441390	A	0.775714099407196	Now it's from the interactions of all these kung fu actors that seemingly purposeful behaviors emerge.
442130	446190	A	0.6335895657539368	So that was successful but learning was very limited.
447170	465430	A	0.8292624354362488	This hierarchy of cognition actors was given, was predefined and the only learning that a robot would do was to discover which action policies tended to be more effective.
469950	475900	A	0.6318668127059937	So clearly my robots are not monolithic active inference agents.
477630	483978	A	0.6439568400382996	So the question is why would I use a society of mind architecture?
484154	494210	A	0.6027856469154358	Well, because I subscribe to the notion that all intelligence is collective intelligence.
495190	506854	A	0.8863571286201477	This paper makes the argument quite cogently and I'm going to cite a number of papers which were important to the evolution of my thinking.
506972	508280	A	0.5943736433982849	This is one of them.
510810	515898	A	0.7609864473342896	This paper sees intelligence as a process, not a property.
515984	523850	A	0.8773486614227295	It's a process enacted by the interacting parts as opposed to again, a property of individuals.
524750	540374	A	0.6856449842453003	So society of mind is basically that it's actually a number of interacting processes that together from which emerge apparently intelligent behaviors.
540522	549410	A	0.5302076935768127	Now my own personal current definition of intelligence is self sustaining, inactive sense making.
549560	564920	A	0.8393943309783936	Sense making is really important by autonomous agent system in a dissipative and dynamic environment and this guides all the work I do in this project.
567230	584080	A	0.8626464605331421	Now, if you want a description, a detailed description of where my project stood a year ago, there's a paper that was published that I published on Xenodo, which you can look up.
587380	599044	A	0.821362316608429	Now, over the last year, I wanted to move away from a pre built a given society of mind and toward a learned society of mind.
599082	610680	A	0.6930692195892334	I wanted to see if I could program an autonomous robot to evolve its society of mind through its interactions with its environment.
611580	622990	A	0.45453476905822754	Now, one might think that programming autonomy is an oxymoron would be a good point, but I don't think it is.
624400	654820	A	0.5492739677429199	If if the program I write and install my robot imparts constitutive autonomy, which is enable the robot to constitute its own identity, and if it imparts adaptivity, which enable the robot to modify itself from its interactions with the environment, then I think that the robot will be truly autonomous.
655820	664820	A	0.7665517926216125	Now, for autonomy to exist inherently in the robot, something in the robot must be at stake.
664980	670924	A	0.7693220973014832	And in this case, it's the survival of the robot society of mind.
671042	677880	A	0.7303063869476318	Now, the physical structure of the robot is not at stake.
677960	682028	A	0.6004384160041809	Its survival is not at stake, unless, of course, it falls off a shelf.
682204	699350	A	0.5563718676567078	But as I intend to have my robot develop its society of mind from experiences, I also expect that if it fails, that this society of mine will perish in its attempt to grow and sustain itself.
700040	710410	A	0.8071333765983582	So that's what's at stake going forward in this project is the survival of the robot society of mind.
711260	715000	A	0.8677300214767456	This survival would be an expression of being.
715070	723020	A	0.8550220727920532	By doing, the robot will need to act and interact in its environment in order to survive.
723360	730620	A	0.8905132412910461	Now, whatever sense it's going to make of its environment will then be grounded in this survival imperative.
730960	735950	A	0.8499899506568909	In essence, the robot society of mine will have skin in the game.
736560	741164	A	0.5321774482727051	If this weren't the case, then sense making would actually reside somewhere else.
741202	746864	A	0.8645284175872803	It would reside in in the mind of the programmer, in my mind as I observe the robots.
746912	753670	A	0.7901765704154968	But I would like the sense making to be grounded in the survival of the robot's mind itself.
754780	756010	A	0.6525441408157349	So that's key.
757500	763288	A	0.8233240246772766	Now, what do I mean by an evolving and growing society of mind?
763374	781920	A	0.8010207414627075	So instead of the given society of mind, which I showed earlier, I want the cognition actors to be created to connect with each other dynamically through interactions with the environment.
783060	790764	A	0.5665614008903503	So I want an active, self organizing, self optimizing collective of cognition actors.
790812	792076	A	0.8057406544685364	Now, is this feasible?
792188	793510	A	0.7160843014717102	Well, that's a big question.
795880	806084	A	0.8679417371749878	In trying to answer this question, I'll be also answering questions like what must be given a priori and what can be discovered?
806212	808920	A	0.8349944353103638	Well, I already have elements of an answer.
808990	821004	A	0.8539603352546692	I know that the sensors and the effectors and the primitive cognition actors that wrap them will be given.
821122	823708	A	0.6969653367996216	This will be like what you're born with, basically.
823874	835836	A	0.9101627469062805	And there might be a metacognition actor which role will be to oversee and guide the evolution of all other cognition actors.
835868	838160	A	0.8109029531478882	But that's my hypothesis.
840580	847376	A	0.7233353853225708	I can imagine a test environment for my robot that will test its ability to survive.
847488	859860	A	0.7012512683868408	I can imagine, for example, that as the robot moves around or computes, it uses a limited store of energy it's stimulated.
860200	865892	A	0.7689555883407593	And that store of energy is replenished when the robot consumes food.
865946	872780	A	0.8294566869735718	And that would be by being on top of pieces of paper, colored pieces of paper on the floor.
874000	877928	A	0.7874563336372375	And I'm going to make sure that it needs two sources of food in order to survive.
878024	881552	A	0.8698339462280273	That would be represented by a yellow paper and a green paper, for example.
881686	889312	A	0.7181512117385864	So that to prevent the robot from just simply finding one source of food and just stationing itself over it.
889446	892728	A	0.5960670113563538	So the environment will also contain obstacles.
892844	908292	A	0.8028392791748047	So the robot will need to learn how to navigate, avoid obstacles, locate different sources of food, get to them, and make sure that it alternate between various sources of food in order to survive.
908356	917530	A	0.7540897727012634	And the society of mind that will evolve, will hopefully evolve to successfully do this.
917840	926904	A	0.8529285192489624	Else if it doesn't, then it will shrink as resources disappear and essentially die.
927032	931520	A	0.49252206087112427	So that's what it's at stake for this robot.
933620	946980	A	0.5861086249351501	Now, this effort employs a number of frameworks, and by framework, I mean a useful system of concept and constraints that guide the implementation.
947560	953220	A	0.7847813367843628	Well, there's obviously the free energy principle and the Active inference framework.
955800	962740	A	0.8699110746383667	However, I see this, the Acactive Inference, as an as if framework.
962900	973740	A	0.8673040270805359	It describes the what what must be achieved in this case, reduction of the agents, variational, free energy.
973890	979368	A	0.5008233785629272	But it doesn't guide me as to the implementation, how to build the robot.
979464	983196	A	0.8487892746925354	For this, I need as is frameworks.
983388	993510	A	0.7195963859558105	And I'll be using two frameworks, one which I've been using since the beginning of the project, which is the Actor model.
994200	1010680	A	0.8550623655319214	The Actor model views computation as a diversity of processes, processes that are independent, have their own private internal state, and who communicate with one another strictly through messages.
1011900	1023788	A	0.774362325668335	Yesterday in Enterprise One, Keith Duggar presented on the Actor model and made the case that we should use the actor model to implement active inference agents.
1023954	1026350	A	0.8649824857711792	Well, I wholeheartedly agree with them.
1026800	1035824	A	0.8695501089096069	The other framework that I'm going to be using is a special case of symbolic AI called the App Perception Engine.
1035942	1042560	A	0.7426097393035889	And much of the presentation will be about the App Perception Engine and its implementation.
1044840	1061880	A	0.7711589932441711	So here's where we are is the project is located at the intersection of Active Inference as a domain, the Wet and Society of Mind as an architecture, and symbolic AI as a form of computing.
1062460	1066280	A	0.8371539115905762	So that's where the project is, at this intersection.
1069040	1070204	A	0.7698943614959717	So where to begin?
1070322	1077916	A	0.6665486693382263	So I want to dive into a more extensive form of learning.
1078018	1083660	A	0.7969242334365845	And the first step, logically, is to learn how to predict.
1083820	1091756	A	0.7154542207717896	So I want to enable a single cognition actor.
1091868	1099792	A	0.7712569236755371	We'll start with a single cognition actor to learn how to make sense of its local environment, its so called unvelt.
1099936	1109684	A	0.8087844252586365	And making sense implies, at a minimum, to be able to predict incoming sensations.
1109732	1112040	A	0.6894643902778625	So it needs to learn to predict.
1116650	1120966	A	0.8382256627082825	So what will be given to a cognition actor?
1120998	1131100	A	0.8289579153060913	Well, there will be a history of sensations broken into discrete units of time.
1131550	1138320	A	0.8707833886146545	So time n minus three and N minus two, n minus one time n, which is the present moment.
1140450	1143150	A	0.8306939005851746	So these would be remembered observations.
1144210	1157140	A	0.8651139140129089	And then what we want to get out of this is the ability to predict the next incoming set of sensations at time t equals n plus one, n plus two.
1157530	1169430	A	0.8634160161018372	And for this, to be able to predict future observations, we need some kind of predictor function that is learned from the remembered observation.
1170350	1175306	A	0.7259157299995422	Now, this predictive capability can be built in two very general ways.
1175488	1188506	A	0.7593050003051758	One is from statistics, so doing pattern analysis and being able to predict what's most probable, which is the standard current machine learning approach.
1188698	1212120	A	0.892710268497467	Or we could predict from an understanding of the observations by developing a causal model of what produced these sensations, and from this understanding, predict what rationally should be observed next.
1216070	1219620	A	0.7245798110961914	So this is all about sense making.
1219990	1223110	A	0.559876561164856	And now what is sense making?
1223180	1225000	A	0.4890304207801819	How do I understand sense making?
1227210	1232694	A	0.8200970888137817	Well, to rationally predict incoming sensory inputs, one must make sense of them.
1232732	1234760	A	0.7407384514808655	That's what making sense means to me.
1235150	1242154	A	0.8504021167755127	And to make sense of sensory inputs means to derive meaningful experiences from them.
1242192	1246346	A	0.772371232509613	It's not just data, it's not just pieces of data.
1246528	1248666	A	0.6694056987762451	There must be meaningful experiences.
1248858	1258862	A	0.8391502499580383	And by experience, I mean a conceptualization of the sensations and a unification of them in time and space.
1258996	1270110	A	0.7671052813529968	So making sense of these inputs will mean to produce meaningful experiences that are conceptualizations and unifications of the sensations.
1270270	1276840	A	0.7087432146072388	Now, an experience is meaningful if it is underwritten by a causal model.
1277450	1290330	A	0.8737285137176514	So the experience is perceived as the consequences of a latent generative model, generative process that we have modeled.
1291070	1294218	A	0.8337119817733765	And I want meaning to be inherent to the agent.
1294384	1303630	A	0.8321406245231628	And that only happens if the agent is truly autonomous and if this meaning is grounded in the survival imperative, as discussed earlier.
1306210	1308494	A	0.8062017560005188	So how does experiencing work?
1308692	1311540	A	0.5892404317855835	How can that be put into computer code?
1312150	1317090	A	0.6193446516990662	Surprisingly for this, we refer to the philosophy of Emmanuel Kant.
1317510	1326360	A	0.8719995617866516	Emmanuel Kant took a reverse engineering approach, asking himself what must entities do to achieve experience?
1327930	1337286	A	0.8446527719497681	This is akin to the free energy principles high Road, which can be paraphrased as what must organisms do to maintain their existence?
1337398	1355150	A	0.8064032793045044	So Emmanuel Kong tried to reverse engineer cognition, asking himself what's the minimal cognitive apparatus needed by an entity to have experiences?
1355970	1361666	A	0.7781596779823303	And that he documented in his critique of pure reason.
1361848	1364130	A	0.7430651187896729	Just a little parenthesis.
1365270	1370580	A	0.4962835907936096	The meaning of the title Critique of Pure Reason is not what I thought it was.
1371270	1375442	A	0.510532021522522	It actually translates more closely to the case for April.
1375506	1383074	A	0.8600507378578186	Cognition critique is a legal term is where you make your case and pure reason we translate nowadays.
1383122	1384710	A	0.7959368824958801	April cognition.
1385070	1406030	A	0.7754809260368347	So his work wanted to establish what must happen to create an experience that's coherent, that is unified in time and space, and to reverse engineer cognition as a system that is both complete and essential, that is minimal.
1411100	1427148	A	0.8549365997314453	Okay, so I'm going to try to give you the postage stamp version of Emmanuel Kant's theory focusing on the Synthetic Unity of Apperception.
1427324	1432050	A	0.7020367383956909	Well, first of all, there's the real world, which is outside of our direct experience.
1432740	1433884	A	0.5278511643409729	It's the pneumonia.
1433932	1446016	A	0.5281220078468323	It's forever hidden from us as an as is reality, but it impinges on our sensorium.
1446048	1462570	A	0.7915096879005432	And then so we have a number of intuitions, sight, sound, touch, smell that are initially separate, and then we need to network them, connect them both in time and in space.
1463200	1468750	A	0.6707596182823181	In space is sound.
1469520	1479970	A	0.8359761238098145	And the sight describing a single thing is one thing behind or inside another.
1481300	1486370	A	0.8936588168144226	And in time, is this happening before, after something else?
1487220	1504330	A	0.8499649167060852	And then at a higher level, meaning is given to these networked, intuitions, sensations via concepts and judgments, rules which are generalizations as to what can and cannot be.
1505980	1508090	A	0.6290931105613708	And this is what we experience.
1512720	1522690	A	0.7148253917694092	Now, it turns out that synthetic Unity of Apperception is a blueprint for automating sense making.
1525540	1534980	A	0.9239410758018494	It's kind of interesting, I think, that 18th century philosophy would be relevant to 21st century technology development.
1535480	1549290	A	0.7927134037017822	And this is what happened and was published by Richard Evans and all in the paper Making Sense of Sensory Input, where they developed the App Perception Engine.
1549740	1570656	A	0.9450049996376038	They took synthetic Unity of Apperception as software requirements and successfully implemented them into a piece of software, the App Perception Engine, and applied it to a number of exercises where they got a very good result.
1570838	1575360	A	0.7680919766426086	So the App Session Engine is in an instance of machine learning.
1575430	1586820	A	0.5037906169891357	It's unsupervised machine learning, and it operates on very small data sets and generates human readable generative models.
1587480	1592810	A	0.6117056012153625	When I read the paper, I realized, well, that's exactly what my robot needed.
1594700	1597848	A	0.8628381490707397	So what does an app perception engine do?
1598014	1615632	A	0.5736157894134521	Well, given a sequence of observed states, it finds a generative model that can recreate past states but most importantly, predict future states.
1615686	1624160	A	0.8184172511100769	And the state is defined as a set of simultaneous observations, sensations, intuitions.
1627740	1636264	A	0.8583555817604065	So an apperception engine searches for a causal theory that can recreate observations.
1636392	1642380	A	0.7002438902854919	I say searches because this causal theory is not determined by the observations.
1643380	1644512	A	0.7274588346481323	It has to be found.
1644566	1645980	A	0.7207533121109009	It has to be discovered.
1646140	1664390	A	0.7051603198051453	But once it is found, then it can be validated against the observations and see if it can recreate them and augment them into the future as well as into the past.
1667100	1669880	A	0.8164702653884888	So what is a causal theory?
1670780	1676920	A	0.8335025310516357	A causal theory is a logic program that has a number of components.
1679180	1686828	A	0.8258439302444458	In the causal theory, there will be the objects and the predicates from the observed relations.
1686914	1697356	A	0.9034342169761658	So from the observations, we can extract what objects were observed and what properties of these objects were observed and maybe what relationships these objects were observed.
1697548	1698720	A	0.7080346941947937	That's the start.
1698870	1702908	A	0.7905914783477783	Then we have latent object types, objects and predicates.
1703004	1717940	A	0.8584562540054321	So we may want to imagine causal theory, imagines hidden objects, maybe hidden types of objects and maybe hidden properties and relationships between objects, latent meaning unobserved.
1718020	1733608	A	0.8553455471992493	And given both the observed and unobserved objects and predicates, it derives rules, first of all constraints on those predicates, what's permissible.
1733704	1743712	A	0.5598363280296326	So for example, being in front of A cannot be in front of B and at the same time behind B.
1743766	1747632	A	0.6598963737487793	So an object cannot be in front of another and behind it.
1747686	1759316	A	0.8860626220703125	So there are constraints on predicates and then there are rules that apply to any simultaneous sets of observations, what they must conform to.
1759418	1771240	A	0.8860273361206055	Then there are rules that given a state, will infer the next state and then maybe some initial state from which we can run the causal theory.
1773020	1776520	A	0.8097315430641174	So what makes a causal theory unified?
1777180	1781000	A	0.808816134929657	Well, first of all it needs to be unified in order to make sense of the observation.
1781680	1783640	A	0.7704014182090759	There are various dimensions.
1783720	1789704	A	0.8223201632499695	So if a causal theory involves a number of objects, all these objects must be directly or indirectly related.
1789752	1796080	A	0.740558385848999	There's no object that just floats in space totally independent of the other objects, so they must all be related.
1796980	1799360	A	0.821599006652832	So they're spatially unified.
1799780	1820760	A	0.6513879299163818	All predicates that make up the causal theory, like on, off, behind, in front, they must be constrained so that, for example, in front cannot be at the same time as behind or that a light cannot be both turned on and turned off.
1820830	1828360	A	0.8040558695793152	So there's some restrictions on the predicates and that creates conceptual unity.
1828780	1839900	A	0.8321303725242615	And then there's static unity where all simultaneous relations must satisfy the static rules, and temporal unity, where all the states must be sequenced by causal rules.
1840320	1841820	A	0.8041380643844604	We'll see examples.
1843700	1852108	A	0.8832909464836121	So let's start with an example here of a set of observations.
1852204	1862436	A	0.8691893219947815	What we are observing are two lights and the lights can either be at any discrete moment in time, either on or off.
1862618	1871130	A	0.8651399612426758	So here we have a sequence of observations and one moment in time.
1871900	1874730	A	0.8226515054702759	The first light was off and the second light was on.
1875180	1884780	A	0.8228592872619629	Then the first light was on, the second light was off, then both were on, et cetera.
1885600	1891816	A	0.6453272104263306	And I put the gray bars there to show that maybe the observations are incomplete.
1891848	1899760	A	0.8153381943702698	So at one stage we can only see the second light or there may be other lights or other objects that we do not see, but that's what we observe.
1902870	1927322	A	0.8929122686386108	So you feed these observations in discrete time into the apperception engine and the perception engine searches for a causal theory that when applied to an initial condition, let's say that light A is off and light B is on.
1927456	1932350	A	0.8285091519355774	It will create a trace of recreated observations.
1932770	1940538	A	0.8718580007553101	That cover is a superset, matches the initial observations.
1940634	1945140	A	0.7070202231407166	And if this happens, then our causal theory is a good one.
1945910	1956100	A	0.8667720556259155	Now, the causal theory may infer the existence of hidden objects, hidden relations, and whatnot it may actually need to.
1958330	1966822	A	0.8606235384941101	So here's an example of a causal theory that is generated by my own implementation of the App perception engine.
1966956	1986400	A	0.813562273979187	Because I re implemented the App perception engine as described in the paper by Richard Evans and all, and I ran it on the set of observations about lights on two lights, one on, one off at any point in time.
1987490	1989918	A	0.7933940887451172	And it came up with it found a result.
1990084	1993098	A	0.7746531963348389	It found the result in 64 seconds.
1993274	1994980	A	0.9094927310943604	It was a perfect match.
1995430	2008200	A	0.7257859706878662	And it actually invented a relationship, which it called thread One, which we can let's imagine that it actually means connects to.
2009130	2012950	A	0.8266902565956116	And it found a static rule and a causal rule.
2013290	2017894	A	0.8560537695884705	It said that a light is on at any moment in time.
2017932	2022460	A	0.8326788544654846	A light is on if a light that connects to it is off.
2022990	2035280	A	0.8685418367385864	And it found a causal rule that said a light turns off if it connects to another light that was also off.
2035970	2041230	A	0.8534515500068665	So that's how the lights change over time, the status of on and off.
2041380	2056500	A	0.841346800327301	And it came up with initial conditions that said that, well, A connects to, first of all, that there's an object one, a light called object one that we don't see, but is there, we imagine is there, that A connects to it.
2056890	2061800	A	0.8253764510154724	The light object one connects to B, and the light B connects to object one.
2062410	2065190	A	0.8305599093437195	So that's the causal rule that it discovered.
2067770	2072170	A	0.8430129885673523	Now, if we run this causal rule, we produce a trace.
2073070	2076982	A	0.799640953540802	And as you can see, the trace matches the observation.
2077126	2079354	A	0.6786630749702454	It adds a new object.
2079552	2086910	A	0.9524658918380737	So the coverage being excellent, being perfect in this case, and our causal theory is a good one.
2086980	2088560	A	0.9012937545776367	It's actually a perfect one.
2089570	2091760	A	0.7749643921852112	It's not necessarily the only one, though.
2094230	2097646	A	0.8545184135437012	So is this causal theory unified?
2097838	2100050	A	0.8780462741851807	So going back to Dr.
2100120	2114710	A	0.5667629837989807	Kant's requirement of synthetic unity, of Apperception, not every causal theory will do, though it may predict correctly, it may not be meaningful unless it is unified.
2117560	2121844	A	0.8380070328712463	Well, we saw the four dimensions of unification.
2121892	2124484	A	0.8657327890396118	Is it spatially unified?
2124612	2131530	A	0.8479562401771545	Well, all our objects are connected directly or indirectly to each other.
2132480	2133630	A	0.9629592895507812	That's good.
2134720	2136940	A	0.7984639406204224	So we have spatial unification.
2137440	2139144	A	0.8941019177436829	Do we have conceptual unification?
2139192	2141192	A	0.7455692887306213	So we have this new predicate.
2141336	2142940	A	0.7626233100891113	We have two predicates, right?
2143090	2151584	A	0.8815737366676331	Pred One, which we translate to, connects to, and then the predicate that says whether the light is on or off.
2151782	2159908	A	0.7976672053337097	Well, we have a constraint that says that a light can only be connected to one other light.
2159994	2169840	A	0.6988629102706909	So pred one has a constraint on it that says it's exclusive.
2170000	2175572	A	0.5803812742233276	So an object cannot pred one to two objects cannot connect to two objects.
2175636	2179332	A	0.8856996297836304	That's a constraint that was discovered and part of the causal theory.
2179476	2192172	A	0.7653725743293762	And also implicitly, the on relation of the predicate has the value on or off, and it cannot be both at the same time.
2192226	2195180	A	0.7667472958564758	So it's conceptually unified.
2195700	2197680	A	0.8441943526268005	Is it statically unified?
2198020	2201292	A	0.7607068419456482	Are the static rules obeyed.
2201436	2212324	A	0.8820145130157471	Well, for example, the static rule would say that given that B connects to A, if B is off, then A must be on.
2212362	2217540	A	0.8915331363677979	So if you look at any place where B is off, a is going to be on.
2217610	2223716	A	0.8079487681388855	And you could do that for every other light and relationships between lights.
2223748	2226404	A	0.5828032493591309	So they all obey the static rule.
2226532	2236668	A	0.8473045229911804	And the causal rule says, for example, here, that if B connects to A, then if A was off, then B must turn off.
2236754	2241790	A	0.8309547901153564	So if you look at B, let's say B was off.
2245680	2259716	A	0.5467839241027832	Yeah, I'm sorry, if B connects to A, and yes, and if A was off, B must be off.
2259818	2264020	A	0.8290932774543762	So if A was off, B becomes off the next step.
2264090	2266310	A	0.5402731895446777	So that's correct as well.
2267240	2272730	A	0.559491753578186	So statistically we are true, and temporarily we are true.
2273500	2275060	A	0.7499180436134338	We are unified.
2275220	2279480	A	0.7691490054130554	And of course, that we get a thumbs up from Dr.
2279550	2283928	A	0.737260639667511	Kant, our causal theory is unified.
2284104	2289580	A	0.7708290219306946	Thus it makes sense of the observations of the sensory inputs.
2291120	2299308	A	0.6845136284828186	Now, it's no accident that Kant would be would figure in an active inference project.
2299474	2310980	A	0.6905230283737183	There is a link between active inference and Kant, and it runs through the celebrated 19th century German engineer Herman von Hemholz.
2311320	2319536	A	0.8815941214561462	He was a disciple of Kant and he developed the theory of visual perception that operationalized Kant's epistemology.
2319648	2322520	A	0.8312711119651794	And in fact, it anticipates predictive processing.
2322860	2329284	A	0.8919870257377625	In 1995, Peter Day and Jeff Hinton developed the Helmholtz machine.
2329332	2331000	A	0.6566539406776428	Name is in his hunter.
2332220	2338732	A	0.8603527545928955	It is a type of artificial neural network that's trained to create a generative model from an original set of data.
2338786	2341950	A	0.8635861277580261	And it can account for the hidden structure of the data.
2343040	2351410	A	0.9655314683914185	So as you see, there's a link which is discussed and elaborated in this paper, which is very interesting paper.
2352580	2356748	A	0.6190658807754517	All right, so close parentheses.
2356924	2361780	A	0.8336008787155151	So we've looked at the App Perception engine from the perspective of philosophy.
2362120	2366550	A	0.7336828708648682	So now let's look at it through the lens of machine learning.
2369660	2372136	A	0.861723005771637	The observations constitute a training set.
2372238	2373850	A	0.7285244464874268	It's a very small one.
2374220	2380500	A	0.8213613629341125	And the Appetition engine is the learning algorithm.
2380660	2384140	A	0.8263447880744934	And what is learned, the output is a causal theory.
2386000	2397904	A	0.8375148177146912	So the learning process is unsupervised logical inferencing, and the output is a human readable logic program.
2398102	2423450	A	0.6139687299728394	So we see here that there's some profound differences with the more popular form of machine learning in that the training set is really small, that the product of the learning is actually a human readable artifact, in this case, a logic program.
2425500	2427770	A	0.8600641489028931	So this is the training set.
2429340	2447520	A	0.7912314534187317	As inputted lights led, a turned off at time one, b turned on at time one, a turned on at time two, b turned off at time two, et cetera, et cetera.
2447940	2462992	A	0.8671156167984009	So that's the training set and you feed this into the perception engine's algorithm and out comes a causal theory.
2463136	2468676	A	0.9102301001548767	So in a little bit more details what the algorithm is and does.
2468858	2477684	A	0.8333927989006042	First, it extracts the observed object extent objects, the object types and predicates from the observations.
2477812	2481896	A	0.8447148203849792	So we have on, we have object A, object B.
2481998	2483692	A	0.7962432503700256	We have led as an object type.
2483746	2489020	A	0.9011567831039429	So that's all part of the observations and that becomes part of the extent vocabulary.
2489440	2503516	A	0.8399788737297058	Then the application engine imagines unobserved objects, types and predicates for the relationships and properties, and that becomes a latent vocabulary.
2503548	2506380	A	0.8114813566207886	So there's a step of imagination.
2506540	2530760	A	0.8704878687858582	Then, using the combined vocabulary, both the extent and latent vocabulary combined, it looks for a unified causal theory, a set of constraints, rules and initial conditions that obey the constraints of synthetic unity of apperception.
2532220	2539980	A	0.889640212059021	Once it has this causal theory and with initial conditions, it applies the causal theory to these conditions and produces a trace.
2540640	2546990	A	0.6117464303970337	It recreates observations if you want and augments them and extends them into the future.
2547680	2555952	A	0.8207736015319824	Now it looks at this trace and compares it with the initial observations for coverage and decides if this is a good causal theory or not.
2556086	2564784	A	0.858780026435852	Then it also looks at the causal theory complexity, how many rules, how complex are the rules, et cetera, and measures for complexity.
2564832	2578596	A	0.8093318939208984	So if we have a choice between two causal theories of equivalent coverage, the App Perception Engine will select the least complex one using Occam's Razor.
2578788	2587420	A	0.5863534212112427	Now, if you look at this algorithm, you'll see that the boxes in green are not deterministic.
2587840	2589308	A	0.808063805103302	This is where search happens.
2589394	2591976	A	0.7557850480079651	We can posit different kinds of objects.
2592088	2594888	A	0.8191100358963013	We can find different kinds of rules.
2594984	2598110	A	0.7611580491065979	So this is where search happens.
2599360	2604252	A	0.8600152134895325	Now, app perception is implemented using logic inference.
2604396	2608896	A	0.8813904523849487	Actually, it uses three forms of logic inference.
2609008	2617648	A	0.7677088379859924	There's the one that we're more familiar with, which is deduction, where given rules and causes, we infer the effects.
2617824	2619600	A	0.7755598425865173	Then there's induction.
2619680	2622324	A	0.7980542778968811	Where given causes and effect, we look for the rules.
2622372	2624728	A	0.6499711871147156	This is what science does, right?
2624814	2629108	A	0.8811314105987549	Looking for rules that would account for effects given the causes.
2629284	2636520	A	0.7352353930473328	Then there's abduction, where given the rules and given what we observe the effects, we're looking for the causes.
2636600	2642220	A	0.8696205615997314	In this case, we're looking for the latent objects, the latent relationships between these objects.
2642800	2654690	A	0.8363336324691772	And then you can combine both abduction and induction, where you're given effects, essentially observations, and you're looking for both causes and rules, which is what the App Perception Engine does.
2655300	2661524	A	0.5290951728820801	And this is where in the algorithm, these kinds of inferences are at play.
2661722	2671956	A	0.551758885383606	So positing latent objects, that's a form of abduction, imagining causes, finding the rules, well, that's clearly a form of induction.
2672148	2682916	A	0.8313073515892029	And then applying the rules of a causal theory to some initial conditions to create a trace, well, that's deduction.
2683028	2690072	A	0.8271713256835938	We have the causes, the initial conditions, we have the rules, causal theory, and then we produce a trace, the effects.
2690136	2691544	A	0.5886675715446472	So that's deduction.
2691592	2697260	A	0.860664427280426	So the App Perception Engine uses all forms of logical inference.
2698800	2707688	A	0.5577949285507202	Now, just a reminder that the output of the App Perception Engine, that is what is learned is actually human readable.
2707884	2720904	A	0.8270056843757629	You may want to compare that to a large array of floating points produced by traditional, the more popular form.
2720942	2722548	A	0.8189347982406616	Of machine learning nowadays.
2722644	2726392	A	0.8116307258605957	So here, this is what's actually produced by the app reception engine.
2726446	2735060	A	0.7922042012214661	As it runs on a set of observations, it produces a logic program that is human readable.
2735220	2741404	A	0.7417191863059998	When you look at it, the only thing you need to kind of guess is what is meant by pred one.
2741442	2752480	A	0.85893315076828	And if you think, well, maybe it means connects to maybe the lights are connected underneath a board out of sight of the observer.
2754580	2757668	A	0.5370554327964783	But finding a unified causal theory is hard.
2757754	2762436	A	0.8163212537765503	So we have to guess what the latent objects and predicates are.
2762618	2767968	A	0.9073289036750793	What are the hidden lights, what are the hidden relationships between lights?
2768064	2777160	A	0.8873630166053772	And we have to discover what constraints might apply on the predicates and what are the initial conditions from which we want to recreate a trace.
2777660	2782240	A	0.8928326964378357	What are the static rules that apply to simultaneous observations?
2782340	2792030	A	0.8857707977294922	And then what are the causal rules that given observations at time T will predict observations at time T plus one?
2792480	2793550	A	0.7388391494750977	This is hard.
2794080	2798384	A	0.6053121089935303	As a matter of fact, it's non polynomially hard.
2798582	2808080	A	0.6278270483016968	The search space grows exponentially with the size of the input, which is the size of the extent and latent vocabulary.
2809240	2816660	A	0.4977673292160034	So just like in chess, you can't predict to the end the consequence of a move because of common turbo explosion.
2817000	2826970	A	0.5650230050086975	With the app perception engine, you cannot systematically traverse the entire space of possible causal theories to find a good one because it's impossibly large.
2829740	2836140	A	0.47115248441696167	So the job of the apperception engine is to find a causal theory in a ridiculously large haystack.
2837520	2838830	A	0.8304287791252136	How to do this?
2839600	2856000	A	0.7888613939285278	In my implementation, I follow the recommendations and I follow also the implementation in Richard Evans'paper by breaking the search space into chunks.
2856500	2864020	A	0.8781269192695618	First there's a region and the region says so how many latent object types, objects and predicates will we allow?
2864090	2873364	A	0.8304996490478516	So what is the limit of imagination of the cognition actors that is trying to apperceive a causal theory?
2873492	2875428	A	0.8262396454811096	What are the limits of its imagination?
2875524	2891432	A	0.844439685344696	And within that region of bounded imagination, we carve it into templates where we say, okay, we're going to use these latent objects types, these latent objects.
2891576	2896000	A	0.8319894075393677	And so basically, what vocabulary, specific vocabulary we're going to be using?
2896150	2897392	A	0.8564460277557373	We're going to use object one.
2897446	2907884	A	0.9080162048339844	We're going to use object pred one on top of the observed on predicate and observed A and B lights.
2908012	2916688	A	0.8607242107391357	And we're going to set the maximum complexity on the rules and see if we can find causal theories that fit this template.
2916864	2930116	A	0.8178979158401489	So this is a carving up of the search space and having broken the search space into regions and templates, we have scopes in which to apply Heuristics.
2930228	2930936	A	0.5223946571350098	Now.
2931118	2932600	A	0.7364952564239502	Why heuristics?
2933120	2938172	A	0.6258179545402527	Because the systematic traversal cannot be done in reasonable time.
2938306	2944476	A	0.49725767970085144	There's just too many candidate causal theories to look at to find a good one.
2944578	2946320	A	0.8641567230224609	So we use Heuristics.
2948260	2954384	A	0.5937354564666748	We find ways of maybe getting to a good solution faster at the risk of missing it.
2954502	2961028	A	0.7204523086547852	But at least we'll have an answer or no answer in a reasonable amount of time.
2961114	2967396	A	0.7599544525146484	And there's a number of heuristics that I've implemented in my implementation of the app assetron engine.
2967578	2969040	A	0.7584396600723267	Well, there's time boxing.
2969120	2975240	A	0.5359671711921692	At some point, you'd spend no more than this amount of time looking into a region or into a template.
2976060	2977348	A	0.7202015519142151	There's multitasking.
2977444	2980724	A	0.8446490168571472	Well, the problem is actually, as they say, embarrassingly parallel.
2980852	2988830	A	0.8764312267303467	You can explore multiple regions and multiple templates in parallel and so make good use of a multicore computer.
2989680	2991612	A	0.6858328580856323	You want to make sure you don't repeat yourself.
2991666	2998080	A	0.5204271674156189	So you don't want to traverse the same region twice or look at the same causal theory twice.
2998500	3003520	A	0.5939826369285583	You want to satisfy maybe a good enough theory is just fine.
3003590	3006364	A	0.6304735541343689	We don't want to look for the perfect one necessarily.
3006412	3007730	A	0.6283333897590637	We may not have time.
3008740	3010044	A	0.7678931951522827	You want to fail early.
3010102	3021184	A	0.6265532970428467	If you're in a region where nothing good is found, you may want to leave it quite quickly at the risk of maybe not finding a good one that is just over the horizon.
3021232	3023220	A	0.5664073824882507	But you want to be impatient.
3023380	3024884	A	0.7768364548683167	You want to throw the dice.
3025012	3036350	A	0.7753918766975403	You may want to kind of mix it up so that every time you run the app Perception Engine on the same problem, you may find a different solution first.
3037840	3041372	A	0.7341771125793457	You want to go for the simpler solution first.
3041506	3044124	A	0.6569922566413879	You may not want to try everything, just sample some.
3044242	3052144	A	0.7105304598808289	You want to start with the easiest part of the search base first, be judicious and so forth and so on.
3052182	3054236	A	0.6499136090278625	And most importantly, be selective.
3054348	3061760	A	0.6547206044197083	So reject any causal theory that would fail the constraints of unity of apperception.
3062200	3071156	A	0.9659194946289062	With all these in place, my implementation of the app Perception Engine gives pretty good results.
3071268	3073080	A	0.8235154151916504	So here I did a run.
3073150	3074148	A	0.6023640036582947	This is not cherry picked.
3074164	3082244	A	0.8973373174667358	I decided to do one series of seven runs and collect the data and show it.
3082382	3093900	A	0.6028621196746826	And in this run, I set up the apparition engine to only accept a perfect causal theory, one that would produce a trace that totally covers the observation.
3094480	3097260	A	0.8186489939689636	And I did seven runs.
3097780	3100880	A	0.7448070049285889	The first one succeeded, found it in 4 seconds.
3101300	3103756	A	0.6447906494140625	The second one, it took 102 seconds.
3103788	3108672	A	0.8711404800415039	So there's some randomization in the order in which things have searched if luck is involved.
3108736	3113510	A	0.9812167882919312	As I said, the third one, 1 second, that was pretty cool.
3115400	3121784	A	0.7930853366851807	The fourth one, well, took 204 seconds, then 90, 612, 99.
3121822	3125496	A	0.9083219170570374	So quite a good distribution here.
3125678	3133960	A	0.7791888117790222	Now, I said, okay, I'm going to run the app Perception Engine again on the same training set that I showed earlier, those two lights.
3134120	3145076	A	0.5712316632270813	But this time I said, I'm going to accept the theory that has 85% or more coverage.
3145208	3153708	A	0.503499448299408	So it recreates the observations well enough, but not perfectly.
3153884	3156524	A	0.7979899644851685	And I time boxed it to 30 seconds.
3156572	3158276	A	0.7598577737808228	So you have 30 seconds to find it.
3158298	3158870	A	0.5352324843406677	Go.
3159640	3167760	A	0.5837092399597168	The first run, it find a causal theory with 75% accuracy immediately.
3167920	3170292	A	0.7261205911636353	Then the same accuracy, same coverage.
3170356	3171316	A	0.6584693193435669	10 seconds.
3171428	3172196	A	0.6043411493301392	11 seconds.
3172228	3174708	A	0.7798084020614624	It hit 29 seconds.
3174884	3177336	A	0.7864684462547302	It found a perfect one.
3177518	3180904	A	0.8052080869674683	Then eleven, it found 87% coverage and stopped right there.
3180942	3181688	A	0.9247530698776245	That's good enough.
3181774	3189416	A	0.6411927342414856	75 again, 100% is the first one it found above 85 in 18 seconds and 75% 0 second.
3189448	3193772	A	0.9330136179924011	So a good distribution again, so we're getting into reasonable times.
3193826	3196444	A	0.5865817070007324	We're not talking about hours here, we're talking about seconds.
3196572	3217536	A	0.6128314137458801	And I'm hoping to do further optimizations and bring it down to something even smaller so that a cognition actor can say, I want to make sense of these observations, query the apperception engine, and get an answer, a causal theory within maybe a couple of seconds.
3217568	3218630	A	0.7334836721420288	That's my hope.
3220440	3222356	A	0.7841250896453857	Now, something interesting here.
3222378	3231960	A	0.6990984082221985	It so happens that what makes it hard for the app perception engine to find a good causal theory is formally equivalent to what makes cognitive science as a whole hard.
3232030	3237020	A	0.8011713624000549	And this paper here makes the case and proves the case quite cogently.
3238640	3254372	A	0.8256799578666687	So cognizant science wants to find models, functions or algorithms that explain, account for situated behaviors.
3254436	3272450	A	0.8392727375030518	So you feed into the cognitive science machine pairs of situations and behaviors, and you want to come out of it a model, an explanation, a function, or an algorithm that accounts for it.
3273220	3289840	A	0.5118035674095154	Well, the paper makes the case that if the explanation is to be bounded in size, then the problem is computable, but it's not tractable in the sense meaning that it's combinatorially explosive.
3289920	3302936	A	0.493850976228714	But once you have a solution, it is computable and tractable to verify that the solution is good, that it accounts for the data that you're trying to understand.
3303118	3307850	A	0.6075372099876404	Well, this is equivalent, formally equivalent to what the app perception engine is doing.
3311200	3314300	A	0.8456886410713196	My implementation was done in prologue.
3314800	3316776	A	0.6918191909790039	I will not go into the details.
3316808	3318648	A	0.8307631015777588	It's about 1000 lines of prologue.
3318744	3328480	A	0.7563192248344421	I'll just say that prologue is a programming language that use deductive inference as its model of computation with backtracking.
3329460	3339430	A	0.7176455855369568	So essentially it searches for a solution and will backtrack if it took the wrong branch if you want.
3340280	3346570	A	0.8601038455963135	And we'll look for a different way of satisfying a line of the program.
3348220	3352170	A	0.6883667707443237	So let's just say that it makes traversing a search space.
3353340	3357316	A	0.6311318874359131	We get traversing a search brains for free when we program in prologue.
3357508	3360764	A	0.8910094499588013	I won't go into any more details, but you can see some prologue code here.
3360802	3371740	A	0.7281107306480408	And the fun thing is that a prologue program is akin to a logical description of the problem it's trying to solve.
3372340	3373890	A	0.9852017760276794	I think it's very cool.
3374420	3385344	A	0.8459566235542297	And prologue environment was augmented by something called constraint handling rules, which is an extension to prologue that adds abductive reasoning.
3385472	3401400	A	0.7362939119338989	So basically in the program, you can say, assume this is true until proven otherwise, and the CHR rules are there to verify if it can be proven otherwise.
3401980	3415080	A	0.7394641041755676	So, again, I'm not asking you to understand this code at all, but I want you to realize that this code is the code that actually executes a causal theory, both the static and causal rules to build traces.
3415240	3416540	A	0.6397572159767151	It is that small.
3416690	3417672	A	0.8977217078208923	It's very powerful.
3417736	3423564	A	0.9288024306297302	So combining prologue and CHR, I found extraordinarily powerful.
3423612	3425088	A	0.9906316995620728	I'm very excited about it.
3425174	3426400	A	0.6411510109901428	I'm a programmer.
3427300	3428480	A	0.640775740146637	Next steps.
3428900	3449800	A	0.6918900609016418	Well, next steps, now that we've solved individual learning by cognitive actors, well, I want to move to beliefs from sensations and to policies to validate or eliminate beliefs.
3451820	3454932	A	0.5073448419570923	A lot of these beliefs actually fall out of our perception.
3454996	3459656	A	0.8737854957580566	Latent objects and latent relationships and properties can be considered as beliefs.
3459688	3465500	A	0.8533554077148438	Then there are other kinds of beliefs that can be obtained from what's been perceived.
3466800	3491984	A	0.8069761991500854	There will be introspective beliefs that communicate how the cognition actor is doing in terms of competence, predictionary rates, how well its apperception is doing, and whether it is engaging with other cognition actors.
3492032	3493140	A	0.8409163355827332	Is it relevant?
3493880	3498804	A	0.8703216314315796	I will have feelings which will provide normativity to these beliefs.
3498852	3530880	A	0.8105090856552124	So if feelings are signals of risk to homeostasis, loss of resources, physical damage, too many prediction errors so that's anger, pain, and fear and feelings will taint beliefs over time and tainted beliefs, good beliefs, bad beliefs will want to be eliminated or validated through policies that will be synthesized by the cognition actor.
3531460	3538180	A	0.8119401335716248	And each cognition actor will make available to others its API.
3539320	3542896	A	0.8423054814338684	What predictions can be made about the beliefs of this cognition actor?
3542928	3551880	A	0.903081476688385	What actions are available to others to be asked of the cognition actor?
3552460	3577120	A	0.8800392150878906	And then as cognition actors connect to one another, as the conjunction actors form the umbelt of other cognition actors, then a conjunction actor will be able to predict the beliefs of others, will be able to compose policies made out of actions that are implemented by other cognition actors.
3577460	3586340	A	0.7352410554885864	And eventually, we'll have a society of mind, which is a bunch of intersecting boom belts.
3587480	3590870	A	0.5518264174461365	So that's it.
3591480	3599236	A	0.8301657438278198	So I see the society of mine is a complex system of collective theorizers.
3599428	3609790	A	0.6476938128471375	And I'm going to try going further with this project to answer the question if collective theorizers can self organize to actively sustain itself.
3610320	3620948	A	0.9810081720352173	So thank you to the Active Inference Institute for inviting me to present and for providing a home for this project and for the constant support and encouragement.
3621144	3623420	A	0.7616299986839294	I'll see you later on Discord.
3623580	3624450	A	0.8529649972915649	Thank you.
3625460	3626160	B	0.9184247851371765	Awesome.
3626310	3627756	B	0.874312698841095	Thank you, JF.
3627948	3638112	B	0.8286985754966736	Just to conclude the session, I'll read two questions and let's maybe address them in an upcoming Robotics and embodied meeting.
3638176	3646944	B	0.9450406432151794	So if you're excited about this project, certainly we all are and about symbolic active inference, join the Discord and participate in the Robotics and Embodied.
3646992	3656080	B	0.8535111546516418	But I'll drop these two questions from David Williams in the Chat, who wrote, one, how important is conducting this work in real world versus simulation?
3656160	3663132	B	0.6134129762649536	And two, what tools or components are missing in the robotics toolkit to make this research easier and better?
3663266	3668172	B	0.8123213648796082	I know those are things that you have a lot of thoughts on, so I'll look forward to discussing with you more.
3668306	3669580	B	0.874312698841095	Thank you, JF.
3671760	3672652	A	0.8529649972915649	Thank you.
3672786	3673580	B	0.48897257447242737	Peace.
3675920	3676910	A	0.4896698594093323	All right.
3679600	3680090	B	0.639849066734314	See you.

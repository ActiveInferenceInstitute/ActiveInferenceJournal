start	end	sentNum	speaker	confidence	text
8140	19570	1	A	0.99	And it's a great segue from collective behavior in surprise minimizing agents to collective behavior in surprise minimizing agents, I guess.
19940	21200	2	A	0.99997	Welcome, Conor.
21700	22112	3	B	0.99887	Hey.
22166	25984	4	B	0.63907	Sorry, I just turned on my audio, so I just heard the last thing you said.
26102	26432	5	B	0.89648	Yeah.
26486	27810	6	B	0.81443	Hey, welcome.
28420	29570	7	B	0.94186	Yes, thanks.
30060	33268	8	A	0.6376	Um, we're looking forward to your presentation.
33364	38788	9	A	0.82745	Multi agent active inference and multiscale alignment, current developments and challenges.
38884	43400	10	A	0.99975	So feel free to share your screen or proceed however you prefer.
44060	44804	11	B	0.99791	Great, thanks.
44862	52056	12	B	0.99	And before I start, because I've had issues with my voice saturating, like my microphone saturating, how's my audio?
52088	53036	13	B	0.99997	Is it clipping at all?
53058	54444	14	B	0.92015	Or does this sound okay?
54642	56704	15	A	0.99997	This is good and I'm watching it.
56902	57888	16	B	0.69427	Okay, perfect.
57974	58850	17	B	0.9998	Thank you.
60980	62690	18	B	0.98	I will share my screen.
66020	67328	19	B	0.81954	Share this one.
67494	69072	20	A	1	The one thing I'll note oh, yes.
69126	69500	21	B	0.29739	OK.
69590	75280	22	A	0.66708	It's just JF symbolic implementation does not use statistical distributions.
75440	80748	23	A	0.9994	It uses the symbolic and the logical inference.
80784	88004	24	A	1	And now we're going to move back into the distributional space, and it will be awesome to see similarities and differences.
88132	91480	25	A	0.97203	So thank you, Conor, to you for the presentation.
92860	101740	26	B	0.99	And thank you, Daniel, for the introduction and for inviting me, as well as big thank you to the other organizers of the Third Applied Active Influence Symposium.
102560	105340	27	B	0.99692	Yeah, I'm really happy to be here to present.
105490	105804	28	B	0.98998	So.
105842	107144	29	B	0.99998	My name is Conor Heins.
107192	110940	30	B	0.99879	I'm a PhD student at the Max Planck Institute of Animal Behavior.
111100	115872	31	B	0.99	And I'm also a researcher at the Versus AI research lab.
115926	117090	32	B	1	R D lab.
117780	123300	33	B	0.99867	So I'm going to do something a little bit, I guess, unconventional for people in my position.
123370	126112	34	B	0.99711	Like, I'm a junior researcher coming to the end of my PhD.
126176	134648	35	B	0.99999	So usually when I give a talk, I would present on my own research, like what I've been up to for the last ten years working on.
134734	154764	36	B	0.8294	But instead of that, I'm actually going to talk about given the motivation of the symposium, I'm going to talk about something that's more of an overview or perspective on the current state of the field in multiscale active inference or multi agent active inference and what, in my opinion, we need to do to move forward as a field.
154882	161852	37	B	1	I think that's very resonant with the kind of motivations and the title, indeed, of this symposium.
161996	167808	38	B	0.99981	So I'm going to give a general analysis of what multiscale active inference is, why it's important.
167974	184760	39	B	0.78962	I'm going to provide a brief analysis of its formal basis as it currently stands, and then what we need to develop in this kind of subdiscipline of active inference, multiscale active inference, to really make it rigorous and really to actually reap the benefits of what it promises.
186220	214050	40	B	0.99773	So generally, active inference has been used a lot to design agents that can solve problems, plan, and just generally emulate behavior that we deem intelligent, which includes things like risk sensitive decision making, intrinsic motivations to resolve uncertainty, and finally, from a more scientific standpoint, the ability to furnish a process theory about how biological brains actually might work.
215140	233348	41	B	0.88602	But in a lot of the theoretical work on active inference from the last ten years or ten plus years, really, there's also alongside all the kind of practical building adaptive agents, there's a claim that active inference is inherently or intrinsically multiscale from the very get go.
233434	235044	42	B	0.99999	It is a multiscale framework.
235092	237610	43	B	0.9998	It's not just about building single agents.
238380	249192	44	B	0.99102	So it's really whenever we write down a single active inference agent, what we're implicitly implying is also a nested hierarchy of active inference agents both below and above.
249336	264028	45	B	0.99658	So colloquially, you'll often see this in papers as the idea that there's Markov blankets all the way down Markov blanket is a statistical structure that's very kind of intrinsic to the definition of agents as they are defined under active inference.
264044	265904	46	B	0.98118	So I'm not going to get into defining that.
266022	273004	47	B	0.56212	I'm kind of assuming that there's a more disciplinarian audience there, but I'm sure other talks, for instance, can provide better clarity.
273052	277428	48	B	0.9758	So yeah, Markov blanket's active inference all the way down, all the way up.
277594	292100	49	B	0.65	And at any given scale, crucially, the free energy minimizing dynamics, or the active inference dynamics are kind of claimed to be aligned with or parallel to the free energy minimizing gradients at the level below and above.
292260	305756	50	B	0.99988	So the claim is that as agents are doing their thing and doing active inference at one level, it both entails and is constrained by active inference processes of the macro agent that they're participating in.
305858	311440	51	B	0.99998	So I'm a cell that's part of a tissue as well as the micro agents that comprise them.
311510	317650	52	B	1	I am a free energy minimizing cellular agent comprised of organelles that are also minimizing free energy.
318440	327920	53	B	0.99888	So this kind of constrained neat nested gradient descent on free energy is part of the story of multiscale active inference.
328000	334372	54	B	1	And it also crucially, assumes that these dynamics are aligned, correlated cooperative across these different scales.
334516	342484	55	B	0.99988	So I should mention that there is a formal argument made more recently, I would say in the last five years, about how this is possible.
342622	347852	56	B	1	And it relies on an apparatus from statistical physics called the Renormalization group.
347986	362188	57	B	0.99999	This basically allows you to analytically identify shared symmetries energy and conservation laws at different scales in a given system that's comprised of subsystems and subsystems, so on infinitum.
362284	380128	58	B	0.99995	So there's a formal argument specifically made in a free energy principle for a particular physics monograph in 2019 that applies the renormalization group apparatus to multivariate stochastic differential equations that are kind of the equivalent of agents.
380314	390628	59	B	0.96034	So you can apply that framework to certain sorts of coupled stochastic differential equations that exhibit Markov blanketed sparse coupling structure.
390724	401064	60	B	1	And you can kind of prove analytically that there are going to be nested systems of Markov blankets and that they're all in some sense minimizing three energies at their own scales.
401112	411170	61	B	0.55279	So I'll get more into that argument later, but I just want to mention that as I define multiscale active inference that there is a formal argument that's related to it.
412180	418268	62	B	0.99975	So this slide I just put together to demonstrate the idea of nested free energy minimizing processes visually.
418364	430752	63	B	0.99993	So at a given scale, we can think of an agent as occupying some point in its free energy landscape indicated by this red orb, which represents, say, its configuration, its beliefs, and its actions.
430896	435272	64	B	1	And it performs active inference and in doing so minimizes its free energy.
435326	438388	65	B	0.99998	So it changes the position of that ball on that landscape.
438484	440788	66	B	1	And that is all we mean when we say active inference.
440884	449560	67	B	0.9999	That corresponds to the agent doing inference and doing action and kind of getting to the fixed point of its local free energy landscape.
449720	453544	68	B	1	The multi agent case is simply when we add more of these processes.
453592	460848	69	B	0.99981	So there's other agents usually assumed to be similar agents, and the word similarity, let's put an asterisk on that.
460934	464880	70	B	0.82	And they're all sitting at different points in their own free energy landscapes.
465540	469270	71	B	1	The position of their local red ball is maybe in a different place.
470760	494232	72	B	0.99992	So the claim of multiscale active inference is that as we link these multiple active inference agents together, so they can actually exchange information like observations and actions with each other, what we automatically get is some kind of superagent that is also minimizing variation of free energy and is in some sense an emergent or Supervenient active inference agent.
494366	505240	73	B	1	And I say the word, we automatically get a superagent with an asterisk because there may be some conditions on that mapping from local to global that have to be elucidated.
505400	507648	74	B	0.99447	So we'll come back to that in a bit.
507814	518540	75	B	0.98942	But in short, I think the definition of multiscale active inference is very eloquently put in this paper by Rafael Kaufman, Pranav Gupta, and Jacob Taylor.
518620	522516	76	B	0.99	I think Rafael Kaufman is actually going to be on the panel later.
522698	528580	77	B	0.96	And this line from their paper is a really nice, I think, just summary of what it is.
528730	530448	78	B	0.98859	So I'll just read it out loud.
530544	544536	79	B	1	The upshot is that in theory, any active inference agent at one Spaciotemporal scale could be simultaneously composed of nested active inference agents at the scale below and constituent of a larger active inference agent at the scale above it.
544638	550108	80	B	0.75854	In effect, active inference allows you to pick a composite system or agent A that you want to understand.
550274	569316	81	B	1	And it will be generally true that both that agent A is an approximate global minimizer of free energy at the scale at which that agent reliably persists, and that agent A is composed of subsystems A, sub I that are approximate local minimizers with free energy.
569418	583396	82	B	0.99997	So that is the claim as I'm going to continue evaluating in this talk, and I think it's just a great reference point to make, okay, that's what multiscale active inference is.
583498	584504	83	B	0.99999	Why is it important?
584622	586104	84	B	0.71853	Why do we actually care about that?
586142	590424	85	B	0.992	That sounds philosophically nice and beautiful visually, but why is that important?
590542	596248	86	B	0.99271	So there's a ton of actually really important implications of this, both for the engineering and the natural sciences.
596424	608396	87	B	1	First of all, the namesake of this symposium, I assume inspired by this recent paper by Karl Frison and all about enacting ecosystems of parenthetically shared intelligence.
608508	611404	88	B	0.99967	So this is the third applied active inference symposium.
611452	616684	89	B	0.99985	So to really make this resonate with the applied aspect, let's make this very concrete.
616812	634628	90	B	0.99998	If we can figure out this multiscale endeavor, then we can actually engineer distributed systems of multiaging intelligence where local agents, in doing their own little local active inference processes, are also cooperatively instantiating a global agent that's also performing active inference.
634724	650168	91	B	0.78097	This has huge computational potential, of course, compared to kind of the state of the art predominant methods for artificial intelligence, which are deep learning, which really is about propagating global information through an entire computation graph.
650264	658336	92	B	0.99998	So although you could argue back propagation is local in some sense, it's really not local in the way that multiscale active inference progresses to be local.
658438	669300	93	B	0.99997	So if we can figure out how to actually engineer multiscale active inference, it will have really tremendous implications for the study of artificial intelligence just from that pure engineering standpoint.
669640	670896	94	B	0.99555	It'll be cheaper.
671008	674320	95	B	0.99969	In one word, it'll be computationally energetically, memory wise.
674400	674864	96	B	0.99954	Cheaper.
674912	675940	97	B	0.94	A lot cheaper.
676440	688340	98	B	0.99974	Secondly, from a kind of more natural sciences motivation, which is kind of where I'm coming from, I'm doing a PhD in biology, so I'm interested in questions about actual real systems in nature.
688500	699484	99	B	1	Just the idea of being able to get super specific and rigorous about phrases like emergent intelligence, emergent computation, collective intelligence, superorganism that's often thrown around.
699522	701516	100	B	0.95152	We're talking about social insects, right?
701618	709852	101	B	0.99998	These are terms that you hear thrown around in many different scientific disciplines that deal with multi agent systems, network systems.
709996	714796	102	B	0.99997	But none of these terms, to my knowledge, have really rigorous or precise conditions.
714908	721712	103	B	0.95038	Multiscale active inference is a kind of framework that's in the position to provide those rigorous definitions and conditions.
721776	730396	104	B	0.8466	So from a scientific standpoint, it could really lend a lot of potential and usefulness for other scientific disciplines.
730528	732788	105	B	1	And finally, another pragmatic motivation.
732884	745940	106	B	0.99995	There's loads of fields that are obsessed with designing and engineering systems where local, selfish individual behavior can lead when networked appropriately to some desired collective outcome.
746020	749480	107	B	1	And these disciplines really want to figure out how to engineer that properly.
749560	757170	108	B	0.99978	So this goes from the design of financial markets and trading systems all the way down to how do you design a multiplayer video game?
758100	760364	109	B	0.52162	So that's kind of just motivating.
760412	763010	110	B	0.99998	Why is multiscale active inference even interesting?
763460	768436	111	B	0.90738	So then the question, of course, becomes, is the multiscale active inference claim actually true?
768538	776950	112	B	0.98685	Are all multi agent active inference systems comprised of and themselves comprise nested hierarchy of free energy minimization agents?
777320	789028	113	B	1	A glance at a smattering of other scientific disciplines that specifically deal with multiple agents, multiple interests, collective phenomena like coordination, group behavior, collective intelligence.
789204	792988	114	B	0.65	A glance at all those disciplines would naively suggest that the answer is no.
793074	807200	115	B	0.99961	So there's things like frustration in thermodynamic systems game theory, the very existence of zero sum games and mass equilibria bandwagon effects when we're talking about social networks and opinion dynamics.
807780	818480	116	B	0.97383	Sacrifices for the common good, which we see in different contexts in biology, like in the context of kin selection, but also in the context of arguably cell death in a tissue.
819300	828752	117	B	0.84414	These are all basically plenty of systems where local constraints and global constraints or desires or free energy gradients, whatever you want to call them, come into direct conflict.
828896	838356	118	B	0.99946	So the obvious example that I listed at the top of these bullets is the idea of geometric frustration that we see in Icing systems with very low temperature.
838468	849372	119	B	0.99927	So these Icing models basically describe lattices of Ferromagnets that are happy when they're pointing in the same direction as their neighboring Ferromagnets and they can be in an up or a down state.
849506	853068	120	B	0.99972	So basically the magnet can be pointing up or pointing down.
853154	860880	121	B	1	And these global systems are defined by a global energy function and the whole system is in some sense trying to minimize that global energy function.
861030	869804	122	B	0.99999	But sometimes you'll find cases in these collective systems where this little spin in the middle cannot be happy because they're getting conflicting information from two neighbors.
869852	876676	123	B	1	I want to be pointing up in blue like the agent on the left, but I also want to be pointing down like the agent on the right.
876778	891108	124	B	0.67111	So this is a system that's collectively finding some fixed point of its global free energy, but it's actually leading to a local conflict where this agent is not at a point where it can do anything to make itself happy or minimize its free energy further.
891204	905150	125	B	0.99979	So just from even the basic glance at Ferromagnetic lattices, we already see instances where local and global gradients or local and global optima are not aligned in the right way.
905600	910656	126	B	0.99952	So given all this, the burden of proof for multiscale active inference is still on us.
910678	917260	127	B	0.6091	So we need to show that collective active inference systems generically do align again across scales.
917340	928470	128	B	1	And maybe if we can put an X across the word generically and it's not some automatic condition, if they don't, then at least we have to establish exactly the conditions in which they do.
930140	934040	129	B	0.99376	So anecdotally we do actually have some conditions.
934460	940632	130	B	0.99066	There seem to be some kind of basic ingredients to get collective active inference to work.
940766	947544	131	B	0.99189	So one is that we basically need agents to exchange actions and sensations across some kind of Markov blanket.
947672	949224	132	B	0.99976	This is not really a condition.
949272	952220	133	B	0.99992	This is almost more part of what it means to be an agent.
952370	963184	134	B	0.92398	So having Markov blanket separation between agents is just another way of saying we have multiple agents in our system rather than a single agent if you're violating the Markov blanket property.
963302	969904	135	B	0.99367	So internal states of one agent are not allowed to see the internal states or external states of another agent.
970022	973188	136	B	0.99985	Then you're kind of cheating because you're kind of saying it's actually one agent.
973274	977476	137	B	0.83	And what you're doing is information sharing within the brain of a single agent.
977658	990120	138	B	1	The second condition, which is something that's often hallucinated more anecdotally and not really formally, is this idea that agents need to have some kind of shared narrative or shared hidden states or censor space in their generative model.
990270	998764	139	B	0.99969	So I've worked a lot on collective active inference systems, just simulating agents and trying to get them to do interesting things together.
998882	1002556	140	B	1	And my intuitions and experience do agree with this basic fact.
1002658	1011180	141	B	0.99996	If the agents don't have any similarity in what they're representing or trying to achieve, then it's kind of like trying to fit a square peg into a circular hole.
1011260	1027860	142	B	0.99996	So this is really nicely elucidated in one of the earliest cases in this paper, a Duet for One by first and Fritz in 2015, where they show that for two agents to really align, they kind of have to have a shared generative model and then you can get kind of this nice synchronized behavior.
1028280	1031284	143	B	0.99996	Again, though, these things like what does similarity mean?
1031322	1033136	144	B	0.99999	What does a shared narrative actually mean?
1033178	1036376	145	B	0.77809	Formally, mathematically, those things have not been initiated yet.
1036398	1056216	146	B	0.99999	So right now, a lot of the building of these collective systems is based on our intuitions and kind of engineering things using some vague guidelines like, oh, they should have a shared sensor space, but there's no mathematical conditions or guarantees about what degree of similarity is needed between two agents models to get the intended dynamics.
1056408	1068048	147	B	1	And finally, we have to have at least some agreement between the generative model of each agent and the generative process, which is really the behavior of the other agents generating their data.
1068134	1072736	148	B	0.99996	So this is kind of related to the previous point about having shared generative models.
1072848	1086708	149	B	0.99998	But just to be very specific, the physics of the space that transfers your actions to my observations that physics can't be dramatically crazily different than how our generative models represent those physics.
1086804	1098456	150	B	0.99993	So if we took two fish with the same generative model of each other and they normally would school together in a fish tank, but we throw them in a volcano or shoot them out into outer space they won't.
1098488	1099100	151	B	0.96113	School together.
1099170	1107432	152	B	1	Because then the generative process is so dramatically deviating from the way they are representing that physics, the way their generative model is constructed.
1107576	1111052	153	B	0.99994	So these are, again just ingredients, kind of guidelines or anecdotes.
1111116	1113836	154	B	0.99999	But there's nothing really rigorous behind these conditions.
1113948	1116080	155	B	0.99964	They're more like a list of best practices.
1116580	1119056	156	B	0.99994	So now let's get on to actual rigorous stuff.
1119158	1129184	157	B	0.99993	So, the first real rigorous attempt to show that multiscale active inference generally works is in one section of this free energy principle for a particular physics monograph from 2019.
1129312	1142884	158	B	0.99234	So it leverages this apparatus I mentioned earlier, the Renormalization group operator, to basically show how one can successively coarse grain multivariate stochastic differential equations that admit sparse coupling between their state variables.
1143012	1157516	159	B	0.99807	So the main result in my mind that connects these renormalization group results to multiscale active inference is the fact that the Lagrangian of the system at one scale can be expressed as a function of the Lagrangian at other scales.
1157548	1167852	160	B	1	And that applies in a scale invariant fashion that is the main kind of output or the main benefit of using a renormalization group apparatus.
1167996	1171236	161	B	0.99729	So you can kind of think of Lagrangian like the generative model.
1171418	1178420	162	B	0.99974	It's a physics term, but it's related to the generative model of the agents that comprise the system and therefore also their free energy.
1178570	1192712	163	B	0.99086	So in terms of active inference, it means that this reasoning of the renormalization group can be used to smoothly move between the models of individual agents at one scale and the model of a collective or larger agent at a different scale or a smaller scale for that matter.
1192846	1203550	164	B	1	And the nice thing about it is general for all kinds of dynamics and thus generative models, it doesn't depend heavily on the form of the stochastic differential equations that form your system.
1205120	1210796	165	B	1	The issues with it is that there's not a global link to Bayesian mechanics and active inference.
1210828	1213804	166	B	0.98095	It's still all done in the traditional physics formalism.
1213852	1218684	167	B	0.8971	So we don't actually have an explicit link to local inference and global inference.
1218732	1224800	168	B	0.99996	Although if you know the connection between the Lagrangian and the generative model and the free energy, then you can make that connection.
1224880	1228848	169	B	0.99994	But it's not actually made explicitly for us in this part of the monograph.
1228944	1236852	170	B	0.99997	It also requires the assumption that the generative model and the generative process are identical at the local level that's related to how the Lagrangian is defined.
1236916	1238436	171	B	0.9995	That's also restrictive assumption.
1238468	1240532	172	B	0.89654	That's probably not realistic in my opinion.
1240676	1246036	173	B	1	And then finally, there's something about kind of spatiotemporal segregation of scales.
1246148	1258172	174	B	0.99816	So we need to make assumptions about how fast random fluctuations are at one scale relative to another scale in order to justify kind of coarse grading or forgetting about certain states as you move between scales.
1258236	1260732	175	B	1	And that's also something that you could argue.
1260796	1269670	176	B	0.99956	Current research into collective dynamics challenges that assumption about how fast noise is at one scale relative to the next.
1270680	1279632	177	B	0.99968	So now I'm going to discuss quickly another small contingent of active inference research that is attempted to address this mapping between local and global inference processes.
1279776	1291668	178	B	0.99938	So what I want to kind of just generally say with this presentation and to our community is that the kind of approach taken in these two papers, first of all, active inference model of collective intelligence and spin glass systems as collective active inference.
1291764	1298648	179	B	0.99993	This is one of the types of research I think we really need to move active inference, multiscale active inference forward.
1298814	1305112	180	B	0.99973	So I'm not trying to be too biased because I am the first author on the second of those papers, but I'll also be the first one to point out the limitation.
1305256	1315100	181	B	0.99985	But benefits wise, I think these approaches are really important because they formally relate a local generative model at one scale to a global generative model at a different scale.
1315180	1319524	182	B	0.9999	So really tie, how do the parameters of one model relate to a course grade model?
1319642	1329140	183	B	1	And these are really good steps in the direction of a formal theory of collective intelligence that goes from local intelligences to global intelligence.
1330200	1331988	184	B	1	However, there's still issues with these.
1332074	1337876	185	B	1	One of them is that they only deal with issue with inference at the global level, not active inference.
1337988	1348588	186	B	0.99967	So both these papers concern with a bunch of local active inference agents that cooperate to form a global inference agent, like a passive baying agent, rather than an active inference agent.
1348674	1354472	187	B	1	And it's also unclear whether the systems studying these papers are actually very generic.
1354536	1362924	188	B	0.9998	Like the results are generic to studying collective intelligence in general, or they're nice formal arguments, but they're only applicable to these specific systems.
1362972	1370772	189	B	0.99994	So we still don't have something that's even more kind of zoomed out and abstract than these, which tend to be a little bit case specific.
1370906	1377652	190	B	1	And finally, the actual scale transcendence that we're doing in these papers is still relegated to really one step.
1377706	1385720	191	B	0.99911	We're not doing the full multiscale infinite scale regression that something like Renormalization group promises.
1386380	1392148	192	B	0.99902	So that's kind of the current overview of what I think are the most promising directions in multiscale active inference.
1392244	1394844	193	B	1	And I'm aware on time, sorry, five minutes.
1394882	1413580	194	B	0.99988	So I'm going to quickly try to go through what I think are really promising directions to push in terms of multiscale active inference and kind of intuition pumps that I think will help us study these systems in a way that's different and also actually better informed by other research disciplines.
1413740	1420308	195	B	0.99985	So the general idea that I'd like to put forward is that misaligned gradients can actually be a good thing.
1420394	1426528	196	B	0.99996	So it's actually sometimes good when local free energy gradients are misaligned with global gradients.
1426624	1432748	197	B	0.99995	So sometimes the global system will actually do better if the local systems are performing worse.
1432864	1445260	198	B	0.99998	So this is something you could call multiscale conflict, where the free energy minimizing processes at one scale are actually doing bad, quote unquote, but it's because they're being driven by some higher scale process that is doing well.
1445410	1462160	199	B	0.99998	So rather than trying to always avoid constructing processes like this, I think this kind of frustration, to use the analogy from statistical physics, can actually be an inspiration for what we should investigate further because it actually might be key to facilitating optimality at different scales.
1464260	1480768	200	B	1	The reason I put this forward is because there's loads of research, just recent research in the last several years that are suggesting that actually making local agents more frustrated or more unhappy might coincidentally or not coincidentally lead to better collective or global outcomes.
1480864	1484448	201	B	0.99991	So this is expressed in various forms in various bodies of work.
1484554	1497224	202	B	1	One of the biggest patterns I've noticed is the study of collective behavioral systems over the last several decades is the idea that local noise and local Dysregulation can often facilitate global coherence or global coordination.
1497352	1507452	203	B	0.5	And where multiscale active inference has something to say, in my opinion, is in framing this benefit of local frustration in terms of a misalignment of free energy gradients.
1507596	1520016	204	B	0.50216	So it may be that actually temporary misalignment local free energy gradients from global ones may facilitate the descent to fix points in the global free energy landscape that satisfy everyone at all scales.
1520128	1527888	205	B	0.68915	So I'm basically expressing an idea that's been known in various communities like Stochastic optimization and Stochastic resonance theory for decades.
1527984	1538520	206	B	0.99994	But I think we as active inference practitioners have a new and potentially useful perspective to shed on that, using the language of active inference and free energy minimization and Bayesian inference in general.
1538670	1558590	207	B	0.99942	So instead of thinking of accelerating optimization by just adding noise to the system, we can think of exactly how to design local generative models such that there's an optimal misalignment of local and global gradients or local and global generative models in a way that facilitates everyone in the long run actually facilitating or minimizing their free energy.
1560320	1563184	208	B	0.99993	So yeah, that's just kind of like something I'm putting out there.
1563222	1566496	209	B	0.99935	I'm investigating it now in my own work, but I have no real results on that.
1566518	1573350	210	B	0.99999	But I just wanted to put that out there in this venue because I think it maybe will inspire other people to think in a similar way.
1573960	1585160	211	B	0.99976	So just to conclude, now, multiscale active inference, I would say, is still largely based on theoretical or philosophical descriptions and illustrious simulations, but we're still lacking a formal theory.
1585740	1602510	212	B	0.67211	There's some theory in terms of the renormalization group arguments of the monograph, but they're still, in my opinion, a bit underdeveloped, a little under demonstrated and relying on some restrictive assumptions like the fast and slow fluctuations, the identity between generative model, generative process.
1603200	1618956	213	B	0.962	There's a few more recent papers, those two by Kaufman at all and then by myself on spingglass systems that have attempted particular proofs of multi scale Bayesian inference systems in particular situations, but their generality is still not known and not proven.
1619068	1633828	214	B	0.97441	So what I'm kind of trying to conclude with is by saying we need to incorporate findings from other disciplines related to the role of noise, conflict and frustration in facilitating, not subverting collective intelligence or collective coordination.
1634004	1642916	215	B	1	And I think we can really benefit by looking at these other disciplines to help us build a really powerful formal theory of multiscale active inference.
1643028	1648856	216	B	1	And finally, I think we need to set the goalpost for what counts as a formal proof of multiscale active inference.
1648968	1664720	217	B	1	And once we get there, once we're saying, okay, this counts as proof, this is satisfying, how can we use that to actually do the hardest part, in my opinion, which is engineering actual multiscale active inference systems that are intelligent and minimizing free energy at multiple scales.
1666020	1669824	218	B	0.99858	Yeah, so with that, I'm going to conclude looks like I'm just on time.
1669862	1679700	219	B	0.99996	So, yeah, thank you again for the invitation to present, and I'd like to thank a bunch of people who are listed here and beyond who have influenced my thinking and kind of my opinions.
1680600	1683050	220	B	0.9999	If there's time, I'm happy to take any questions.
1686700	1687450	221	A	0.99594	Awesome.
1687980	1690104	222	A	0.99973	Great talk.
1690222	1694430	223	A	0.99951	I'll just give a few seconds if anybody wants to type in a question.
1694800	1695932	224	A	0.99986	Also, it's really cool.
1695986	1707080	225	A	0.95761	Like, aswin in the previous session was highlighting Pymdp and just the way in which we enact the collective intelligence.
1707160	1718192	226	A	0.99994	Different people seeing a paper where an analytical formalization is introduced, and then there's still so much work to get it to the package, and then so much more work to take it to the last mile.
1718256	1722100	227	A	1	And I think your presentation really checked a lot of those boxes.
1722920	1730556	228	A	0.99743	I'll just read a question and then that will just be an appetizer for our continued discussion.
1730688	1741320	229	A	0.99143	So Marco Lynn asked, do you expect the inferentially connected dynamics to exhibit behavior akin to theories of multi body systems?
1741660	1746350	230	A	1	And to what extent can we transfer insights from that multi body of work?
1747280	1761372	231	A	0.99	And then second question, just for our thinking and learning from Marco, have you explored integrating work on self organized criticality with multi scale active inference or other frameworks?
1761436	1766940	232	A	0.99965	Who can provide more flexible frameworks or assumptions for a generic notion of multiscale dynamics?
1767100	1768130	233	A	0.99972	Great questions.
1768820	1774812	234	A	1	I hope that we can continue, have you back anytime, or just continue to collaborate in the ecosystem.
1774876	1778880	235	A	0.99443	So thank you for the epic talk, Conor, and good luck finishing your PhD.
1779300	1780404	236	B	0.99947	Thanks a lot, Daniel.
1780452	1781720	237	B	0.96504	Yeah, I'll talk to you soon.
1781870	1782550	238	A	0.9994	Talk to you soon.

start	end	sentNum	speaker	confidence	text
7500	9096	2	B	0.78605	Hello and welcome, everyone.
9198	10200	3	B	0.99993	Welcome back.
10350	22010	4	B	0.99999	This is the second interval of the third applied active inference symposium at the active inference institute on August 22, 2023.
23100	35628	5	B	0.99995	This is going to be another packed and exciting interval, and we're kicking it off with Jean Francois Cloutier, a Collective of theorizers First Steps.
35724	40572	6	B	0.9969	So, JF, thank you for joining and to you for this presentation.
40716	48116	7	B	0.99	And if anybody has questions for this presentation or any of the others, please just put it in the live chat and I'll do it.
48138	48628	8	B	0.98	I can.
48714	49536	9	B	0.99288	So thanks, JF.
49568	50310	11	B	1	To you.
54900	56560	12	A	0.99961	Thank you, Daniel.
57800	65300	13	A	0.98627	I'm a software engineer at a company called Smart Rent, where I write software for smart home systems.
66120	73530	14	A	0.74	I also work on a research project at the Active Inference Institute called the Robotics and Embodied Project.
74620	80650	15	A	0.99962	My current focus is unsupervised learning in active inference agents.
81840	100930	16	A	0.95451	In my presentation today, which is titled A Collective of theorizers First Steps, I'll first start with a brief recap of the project, then dive into recent progress, and then I'll conclude with what I see as the next steps of the project.
103780	111300	17	A	0.99992	Well, since 2017, I've been experimenting with Lego robots and models of cognition.
111880	121690	18	A	1	I do this for my own education, because, like I think most of us, I understand something better when I build it.
122460	126776	19	A	0.99999	What you see here is my latest robot model.
126958	128276	20	A	0.75574	It's a rover.
128468	134780	21	A	0.99999	It has a whole bunch of sensors and a number of effectors actuators.
135360	142860	22	A	0.90671	As a matter of fact, those sensors and actuators can be understood as forming the marker blanket of my robot.
146330	156310	23	A	0.99888	Last year, I presented at the symposium about the history of the project, its then status, and its ambitions.
156730	164470	24	A	1	I presented a cognitive model that implemented predictive processing from an active inference perspective.
164630	170006	25	A	0.99984	So there were generative models, there was predictions, there was prediction errors.
170198	187374	26	A	1	And these generative models animated the robots so that they would roam, avoid obstacles, observe their companion, and also, in a very simplistic way, build a theory of mind of the other robot.
187502	207734	27	A	0.57432	They could from observations infer that the other robot had detected food, food being presented here as a sheet of paper on the floor, and then would kind of try to track the other robot to also get to the food and fun stuff like this.
207932	214006	28	A	0.96	The implementation combined multi processing and functional computing.
214038	218250	29	A	0.99795	There was nothing probabilistic in the implementation.
220830	223114	30	A	1	I thought maybe we'd see them in action.
223162	224574	31	A	0.46984	That's what's presented last year.
224612	232698	32	A	0.99951	But just to get a sense of what they do, I have two robots here, one named Karl and Andy.
232794	236290	33	A	1	I named them before I knew I was going to present at the conference.
237350	243090	34	A	0.99956	So they benefited from a number of training sessions.
243510	253000	35	A	0.99995	They learned how to find which policies would achieve their goals better than others.
253370	256150	36	A	1	And one here on the left.
256300	288850	37	A	0.99996	Found the food rather right away but got closer to the pedestal where the beacon is that simulates the scent of food feared was getting into a collision and then backs off in a hurry as we'll see the other robot observes all this, sees the robot backing off as being in a panic and decides to share this emotion and backs up as well.
289000	291540	38	A	0.99912	So fun stuff.
298180	306390	39	A	0.99993	From the beginning I've implemented different cognition models but they all have in common the fact that they implement a society of mind.
307160	308852	40	A	0.99993	So what is a society of mind?
308906	327710	41	A	0.98	A society of mind is a concept by which the mind is not a monolithic structure but a composition of simple actors, independent actors that interact with each other in simple ways.
328480	334764	42	A	1	That view of the mind was put forth by Marvin Minsky 50 years ago.
334802	336320	43	A	0.99991	So this is not recent.
340290	349434	44	A	0.99998	Again, what I presented last year was a society of mind containing a hierarchy of, I call them cognition actors.
349562	359682	45	A	1	Each cognition actor is an independent process, each one has a scope, a level of abstraction as well.
359736	381322	46	A	0.99386	So for example, you would have a cognition actor that's concerned with the location of food and it would have beliefs and perceptions about food location and these would feed into a higher level cognition actor, let's say food approach which is concerned about getting closer to the food and so forth and so on.
381376	385642	47	A	0.99996	These cognition actors communicate again in simple ways.
385696	398138	48	A	0.99997	This is the society of mind and they communicate by emitting predictions, predictions about the beliefs of other cognition actors and they communicate by emitting prediction errors.
398234	422194	49	A	0.99998	When predictions made about their beliefs by others are inaccurate that leads to connection actors processing these prediction errors and combining them with their own predictions to create an updated set of perceptions that are synthesized into beliefs.
422242	431770	50	A	0.84	And these beliefs lead to actions in order to eliminate negative beliefs or validate positive beliefs.
432270	441390	51	A	1	Now it's from the interactions of all these kung fu actors that seemingly purposeful behaviors emerge.
442130	446190	52	A	0.99998	So that was successful but learning was very limited.
447170	465430	53	A	0.86317	This hierarchy of cognition actors was given, was predefined and the only learning that a robot would do was to discover which action policies tended to be more effective.
469950	475900	54	A	0.74463	So clearly my robots are not monolithic active inference agents.
477630	483978	55	A	0.8456	So the question is why would I use a society of mind architecture?
484154	494210	56	A	0.99918	Well, because I subscribe to the notion that all intelligence is collective intelligence.
495190	506854	57	A	0.9999	This paper makes the argument quite cogently and I'm going to cite a number of papers which were important to the evolution of my thinking.
506972	508280	58	A	0.99999	This is one of them.
510810	515898	59	A	1	This paper sees intelligence as a process, not a property.
515984	523850	60	A	0.59878	It's a process enacted by the interacting parts as opposed to again, a property of individuals.
524750	540374	61	A	0.99987	So society of mind is basically that it's actually a number of interacting processes that together from which emerge apparently intelligent behaviors.
540522	549410	62	A	0.74	Now my own personal current definition of intelligence is self sustaining, inactive sense making.
549560	564920	63	A	0.99995	Sense making is really important by autonomous agent system in a dissipative and dynamic environment and this guides all the work I do in this project.
567230	584080	64	A	0.99	Now, if you want a description, a detailed description of where my project stood a year ago, there's a paper that was published that I published on Xenodo, which you can look up.
587380	599044	65	A	1	Now, over the last year, I wanted to move away from a pre built a given society of mind and toward a learned society of mind.
599082	610680	66	A	1	I wanted to see if I could program an autonomous robot to evolve its society of mind through its interactions with its environment.
611580	622990	67	A	0.62	Now, one might think that programming autonomy is an oxymoron would be a good point, but I don't think it is.
624400	654820	68	A	0.49978	If if the program I write and install my robot imparts constitutive autonomy, which is enable the robot to constitute its own identity, and if it imparts adaptivity, which enable the robot to modify itself from its interactions with the environment, then I think that the robot will be truly autonomous.
655820	664820	69	A	1	Now, for autonomy to exist inherently in the robot, something in the robot must be at stake.
664980	670924	70	A	0.64	And in this case, it's the survival of the robot society of mind.
671042	677880	71	A	1	Now, the physical structure of the robot is not at stake.
677960	682028	72	A	0.91016	Its survival is not at stake, unless, of course, it falls off a shelf.
682204	699350	73	A	0.99998	But as I intend to have my robot develop its society of mind from experiences, I also expect that if it fails, that this society of mine will perish in its attempt to grow and sustain itself.
700040	710410	74	A	0.99954	So that's what's at stake going forward in this project is the survival of the robot society of mind.
711260	715000	75	A	0.67458	This survival would be an expression of being.
715070	723020	76	A	0.99999	By doing, the robot will need to act and interact in its environment in order to survive.
723360	730620	77	A	1	Now, whatever sense it's going to make of its environment will then be grounded in this survival imperative.
730960	735950	78	A	1	In essence, the robot society of mine will have skin in the game.
736560	741164	79	A	0.99997	If this weren't the case, then sense making would actually reside somewhere else.
741202	746864	80	A	0.99999	It would reside in in the mind of the programmer, in my mind as I observe the robots.
746912	753670	81	A	1	But I would like the sense making to be grounded in the survival of the robot's mind itself.
754780	756010	82	A	0.9982	So that's key.
757500	763288	83	A	1	Now, what do I mean by an evolving and growing society of mind?
763374	781920	84	A	0.99997	So instead of the given society of mind, which I showed earlier, I want the cognition actors to be created to connect with each other dynamically through interactions with the environment.
783060	790764	85	A	0.99991	So I want an active, self organizing, self optimizing collective of cognition actors.
790812	792076	86	A	1	Now, is this feasible?
792188	793510	87	A	0.99793	Well, that's a big question.
795880	806084	88	A	0.84722	In trying to answer this question, I'll be also answering questions like what must be given a priori and what can be discovered?
806212	808920	89	A	0.99986	Well, I already have elements of an answer.
808990	821004	90	A	1	I know that the sensors and the effectors and the primitive cognition actors that wrap them will be given.
821122	823708	91	A	0.99998	This will be like what you're born with, basically.
823874	835836	92	A	0.58	And there might be a metacognition actor which role will be to oversee and guide the evolution of all other cognition actors.
835868	838160	93	A	0.9891	But that's my hypothesis.
840580	847376	94	A	0.99	I can imagine a test environment for my robot that will test its ability to survive.
847488	859860	95	A	0.67	I can imagine, for example, that as the robot moves around or computes, it uses a limited store of energy it's stimulated.
860200	865892	96	A	1	And that store of energy is replenished when the robot consumes food.
865946	872780	97	A	0.68	And that would be by being on top of pieces of paper, colored pieces of paper on the floor.
874000	877928	98	A	1	And I'm going to make sure that it needs two sources of food in order to survive.
878024	881552	99	A	1	That would be represented by a yellow paper and a green paper, for example.
881686	889312	100	A	0.99993	So that to prevent the robot from just simply finding one source of food and just stationing itself over it.
889446	892728	101	A	0.99043	So the environment will also contain obstacles.
892844	908292	102	A	0.91802	So the robot will need to learn how to navigate, avoid obstacles, locate different sources of food, get to them, and make sure that it alternate between various sources of food in order to survive.
908356	917530	103	A	1	And the society of mind that will evolve, will hopefully evolve to successfully do this.
917840	926904	104	A	0.99989	Else if it doesn't, then it will shrink as resources disappear and essentially die.
927032	931520	105	A	0.99999	So that's what it's at stake for this robot.
933620	946980	106	A	1	Now, this effort employs a number of frameworks, and by framework, I mean a useful system of concept and constraints that guide the implementation.
947560	953220	107	A	0.9753	Well, there's obviously the free energy principle and the Active inference framework.
955800	962740	108	A	0.99979	However, I see this, the Acactive Inference, as an as if framework.
962900	973740	109	A	0.99995	It describes the what what must be achieved in this case, reduction of the agents, variational, free energy.
973890	979368	110	A	0.99999	But it doesn't guide me as to the implementation, how to build the robot.
979464	983196	111	A	0.99999	For this, I need as is frameworks.
983388	993510	112	A	1	And I'll be using two frameworks, one which I've been using since the beginning of the project, which is the Actor model.
994200	1010680	113	A	1	The Actor model views computation as a diversity of processes, processes that are independent, have their own private internal state, and who communicate with one another strictly through messages.
1011900	1023788	114	A	0.78788	Yesterday in Enterprise One, Keith Duggar presented on the Actor model and made the case that we should use the actor model to implement active inference agents.
1023954	1026350	115	A	0.82344	Well, I wholeheartedly agree with them.
1026800	1035824	116	A	0.99	The other framework that I'm going to be using is a special case of symbolic AI called the App Perception Engine.
1035942	1042560	117	A	0.99	And much of the presentation will be about the App Perception Engine and its implementation.
1044840	1061880	118	A	0.99845	So here's where we are is the project is located at the intersection of Active Inference as a domain, the Wet and Society of Mind as an architecture, and symbolic AI as a form of computing.
1062460	1066280	119	A	0.99716	So that's where the project is, at this intersection.
1069040	1070204	120	A	0.73249	So where to begin?
1070322	1077916	121	A	0.99753	So I want to dive into a more extensive form of learning.
1078018	1083660	122	A	1	And the first step, logically, is to learn how to predict.
1083820	1091756	123	A	0.96437	So I want to enable a single cognition actor.
1091868	1099792	124	A	0.96745	We'll start with a single cognition actor to learn how to make sense of its local environment, its so called unvelt.
1099936	1109684	125	A	0.99	And making sense implies, at a minimum, to be able to predict incoming sensations.
1109732	1112040	126	A	0.86759	So it needs to learn to predict.
1116650	1120966	127	A	0.99873	So what will be given to a cognition actor?
1120998	1131100	128	A	0.8727	Well, there will be a history of sensations broken into discrete units of time.
1131550	1138320	129	A	0.99978	So time n minus three and N minus two, n minus one time n, which is the present moment.
1140450	1143150	130	A	0.99999	So these would be remembered observations.
1144210	1157140	131	A	1	And then what we want to get out of this is the ability to predict the next incoming set of sensations at time t equals n plus one, n plus two.
1157530	1169430	132	A	1	And for this, to be able to predict future observations, we need some kind of predictor function that is learned from the remembered observation.
1170350	1175306	133	A	1	Now, this predictive capability can be built in two very general ways.
1175488	1188506	134	A	1	One is from statistics, so doing pattern analysis and being able to predict what's most probable, which is the standard current machine learning approach.
1188698	1212120	135	A	1	Or we could predict from an understanding of the observations by developing a causal model of what produced these sensations, and from this understanding, predict what rationally should be observed next.
1216070	1219620	136	A	0.9999	So this is all about sense making.
1219990	1223110	137	A	0.83	And now what is sense making?
1223180	1225000	138	A	1	How do I understand sense making?
1227210	1232694	139	A	0.99997	Well, to rationally predict incoming sensory inputs, one must make sense of them.
1232732	1234760	140	A	0.99993	That's what making sense means to me.
1235150	1242154	141	A	0.86	And to make sense of sensory inputs means to derive meaningful experiences from them.
1242192	1246346	142	A	0.88415	It's not just data, it's not just pieces of data.
1246528	1248666	143	A	0.81442	There must be meaningful experiences.
1248858	1258862	144	A	1	And by experience, I mean a conceptualization of the sensations and a unification of them in time and space.
1258996	1270110	145	A	0.99986	So making sense of these inputs will mean to produce meaningful experiences that are conceptualizations and unifications of the sensations.
1270270	1276840	146	A	1	Now, an experience is meaningful if it is underwritten by a causal model.
1277450	1290330	147	A	0.91578	So the experience is perceived as the consequences of a latent generative model, generative process that we have modeled.
1291070	1294218	148	A	1	And I want meaning to be inherent to the agent.
1294384	1303630	149	A	1	And that only happens if the agent is truly autonomous and if this meaning is grounded in the survival imperative, as discussed earlier.
1306210	1308494	150	A	0.99584	So how does experiencing work?
1308692	1311540	151	A	1	How can that be put into computer code?
1312150	1317090	152	A	0.65501	Surprisingly for this, we refer to the philosophy of Emmanuel Kant.
1317510	1326360	153	A	0.56875	Emmanuel Kant took a reverse engineering approach, asking himself what must entities do to achieve experience?
1327930	1337286	154	A	0.75356	This is akin to the free energy principles high Road, which can be paraphrased as what must organisms do to maintain their existence?
1337398	1355150	155	A	0.9894	So Emmanuel Kong tried to reverse engineer cognition, asking himself what's the minimal cognitive apparatus needed by an entity to have experiences?
1355970	1361666	156	A	0.75	And that he documented in his critique of pure reason.
1361848	1364130	157	A	0.99994	Just a little parenthesis.
1365270	1370580	158	A	1	The meaning of the title Critique of Pure Reason is not what I thought it was.
1371270	1375442	159	A	0.99979	It actually translates more closely to the case for April.
1375506	1383074	160	A	0.63987	Cognition critique is a legal term is where you make your case and pure reason we translate nowadays.
1383122	1384710	161	A	0.64	April cognition.
1385070	1406030	162	A	0.99915	So his work wanted to establish what must happen to create an experience that's coherent, that is unified in time and space, and to reverse engineer cognition as a system that is both complete and essential, that is minimal.
1411100	1427148	163	A	0.65	Okay, so I'm going to try to give you the postage stamp version of Emmanuel Kant's theory focusing on the Synthetic Unity of Apperception.
1427324	1432050	164	A	0.87261	Well, first of all, there's the real world, which is outside of our direct experience.
1432740	1433884	165	A	0.80363	It's the pneumonia.
1433932	1446016	166	A	0.98529	It's forever hidden from us as an as is reality, but it impinges on our sensorium.
1446048	1462570	167	A	1	And then so we have a number of intuitions, sight, sound, touch, smell that are initially separate, and then we need to network them, connect them both in time and in space.
1463200	1468750	168	A	0.99979	In space is sound.
1469520	1479970	169	A	1	And the sight describing a single thing is one thing behind or inside another.
1481300	1486370	170	A	0.84	And in time, is this happening before, after something else?
1487220	1504330	171	A	1	And then at a higher level, meaning is given to these networked, intuitions, sensations via concepts and judgments, rules which are generalizations as to what can and cannot be.
1505980	1508090	172	A	1	And this is what we experience.
1512720	1522690	173	A	1	Now, it turns out that synthetic Unity of Apperception is a blueprint for automating sense making.
1525540	1534980	174	A	0.99981	It's kind of interesting, I think, that 18th century philosophy would be relevant to 21st century technology development.
1535480	1549290	175	A	1	And this is what happened and was published by Richard Evans and all in the paper Making Sense of Sensory Input, where they developed the App Perception Engine.
1549740	1570656	176	A	0.99841	They took synthetic Unity of Apperception as software requirements and successfully implemented them into a piece of software, the App Perception Engine, and applied it to a number of exercises where they got a very good result.
1570838	1575360	177	A	0.99892	So the App Session Engine is in an instance of machine learning.
1575430	1586820	178	A	0.99746	It's unsupervised machine learning, and it operates on very small data sets and generates human readable generative models.
1587480	1592810	179	A	1	When I read the paper, I realized, well, that's exactly what my robot needed.
1594700	1597848	180	A	0.99908	So what does an app perception engine do?
1598014	1615632	181	A	0.99948	Well, given a sequence of observed states, it finds a generative model that can recreate past states but most importantly, predict future states.
1615686	1624160	182	A	0.96	And the state is defined as a set of simultaneous observations, sensations, intuitions.
1627740	1636264	183	A	0.9978	So an apperception engine searches for a causal theory that can recreate observations.
1636392	1642380	184	A	1	I say searches because this causal theory is not determined by the observations.
1643380	1644512	185	A	0.99965	It has to be found.
1644566	1645980	186	A	0.99997	It has to be discovered.
1646140	1664390	187	A	0.99958	But once it is found, then it can be validated against the observations and see if it can recreate them and augment them into the future as well as into the past.
1667100	1669880	188	A	0.88709	So what is a causal theory?
1670780	1676920	189	A	0.76	A causal theory is a logic program that has a number of components.
1679180	1686828	190	A	0.99992	In the causal theory, there will be the objects and the predicates from the observed relations.
1686914	1697356	191	A	0.92207	So from the observations, we can extract what objects were observed and what properties of these objects were observed and maybe what relationships these objects were observed.
1697548	1698720	192	A	0.9996	That's the start.
1698870	1702908	193	A	1	Then we have latent object types, objects and predicates.
1703004	1717940	194	A	0.99906	So we may want to imagine causal theory, imagines hidden objects, maybe hidden types of objects and maybe hidden properties and relationships between objects, latent meaning unobserved.
1718020	1733608	195	A	0.68	And given both the observed and unobserved objects and predicates, it derives rules, first of all constraints on those predicates, what's permissible.
1733704	1743712	196	A	0.9998	So for example, being in front of A cannot be in front of B and at the same time behind B.
1743766	1747632	197	A	0.99998	So an object cannot be in front of another and behind it.
1747686	1759316	198	A	0.99997	So there are constraints on predicates and then there are rules that apply to any simultaneous sets of observations, what they must conform to.
1759418	1771240	199	A	1	Then there are rules that given a state, will infer the next state and then maybe some initial state from which we can run the causal theory.
1773020	1776520	200	A	0.99988	So what makes a causal theory unified?
1777180	1781000	201	A	0.99999	Well, first of all it needs to be unified in order to make sense of the observation.
1781680	1783640	202	A	0.96522	There are various dimensions.
1783720	1789704	203	A	0.99986	So if a causal theory involves a number of objects, all these objects must be directly or indirectly related.
1789752	1796080	204	A	0.99927	There's no object that just floats in space totally independent of the other objects, so they must all be related.
1796980	1799360	205	A	0.99982	So they're spatially unified.
1799780	1820760	206	A	1	All predicates that make up the causal theory, like on, off, behind, in front, they must be constrained so that, for example, in front cannot be at the same time as behind or that a light cannot be both turned on and turned off.
1820830	1828360	207	A	0.99993	So there's some restrictions on the predicates and that creates conceptual unity.
1828780	1839900	208	A	1	And then there's static unity where all simultaneous relations must satisfy the static rules, and temporal unity, where all the states must be sequenced by causal rules.
1840320	1841820	209	A	0.9971	We'll see examples.
1843700	1852108	210	A	0.99525	So let's start with an example here of a set of observations.
1852204	1862436	211	A	1	What we are observing are two lights and the lights can either be at any discrete moment in time, either on or off.
1862618	1871130	212	A	0.99989	So here we have a sequence of observations and one moment in time.
1871900	1874730	213	A	1	The first light was off and the second light was on.
1875180	1884780	214	A	1	Then the first light was on, the second light was off, then both were on, et cetera.
1885600	1891816	215	A	1	And I put the gray bars there to show that maybe the observations are incomplete.
1891848	1899760	216	A	0.99999	So at one stage we can only see the second light or there may be other lights or other objects that we do not see, but that's what we observe.
1902870	1927322	217	A	0.99884	So you feed these observations in discrete time into the apperception engine and the perception engine searches for a causal theory that when applied to an initial condition, let's say that light A is off and light B is on.
1927456	1932350	218	A	0.93522	It will create a trace of recreated observations.
1932770	1940538	219	A	0.93	That cover is a superset, matches the initial observations.
1940634	1945140	220	A	1	And if this happens, then our causal theory is a good one.
1945910	1956100	221	A	1	Now, the causal theory may infer the existence of hidden objects, hidden relations, and whatnot it may actually need to.
1958330	1966822	222	A	0.9398	So here's an example of a causal theory that is generated by my own implementation of the App perception engine.
1966956	1986400	223	A	0.48941	Because I re implemented the App perception engine as described in the paper by Richard Evans and all, and I ran it on the set of observations about lights on two lights, one on, one off at any point in time.
1987490	1989918	224	A	1	And it came up with it found a result.
1990084	1993098	225	A	0.99996	It found the result in 64 seconds.
1993274	1994980	226	A	0.78391	It was a perfect match.
1995430	2008200	227	A	1	And it actually invented a relationship, which it called thread One, which we can let's imagine that it actually means connects to.
2009130	2012950	228	A	1	And it found a static rule and a causal rule.
2013290	2017894	229	A	0.99971	It said that a light is on at any moment in time.
2017932	2022460	230	A	1	A light is on if a light that connects to it is off.
2022990	2035280	231	A	1	And it found a causal rule that said a light turns off if it connects to another light that was also off.
2035970	2041230	232	A	0.99997	So that's how the lights change over time, the status of on and off.
2041380	2056500	233	A	1	And it came up with initial conditions that said that, well, A connects to, first of all, that there's an object one, a light called object one that we don't see, but is there, we imagine is there, that A connects to it.
2056890	2061800	234	A	1	The light object one connects to B, and the light B connects to object one.
2062410	2065190	235	A	0.99998	So that's the causal rule that it discovered.
2067770	2072170	236	A	1	Now, if we run this causal rule, we produce a trace.
2073070	2076982	237	A	1	And as you can see, the trace matches the observation.
2077126	2079354	238	A	0.99996	It adds a new object.
2079552	2086910	239	A	0.99992	So the coverage being excellent, being perfect in this case, and our causal theory is a good one.
2086980	2088560	240	A	0.99325	It's actually a perfect one.
2089570	2091760	241	A	0.99917	It's not necessarily the only one, though.
2094230	2097646	242	A	0.95845	So is this causal theory unified?
2097838	2100050	243	A	0.99131	So going back to Dr.
2100120	2114710	244	A	0.74006	Kant's requirement of synthetic unity, of Apperception, not every causal theory will do, though it may predict correctly, it may not be meaningful unless it is unified.
2117560	2121844	245	A	0.99996	Well, we saw the four dimensions of unification.
2121892	2124484	246	A	0.99989	Is it spatially unified?
2124612	2131530	247	A	0.99982	Well, all our objects are connected directly or indirectly to each other.
2132480	2133630	248	A	0.89917	That's good.
2134720	2136940	249	A	0.97369	So we have spatial unification.
2137440	2139144	250	A	0.9998	Do we have conceptual unification?
2139192	2141192	251	A	0.99997	So we have this new predicate.
2141336	2142940	252	A	0.99998	We have two predicates, right?
2143090	2151584	253	A	0.99002	Pred One, which we translate to, connects to, and then the predicate that says whether the light is on or off.
2151782	2159908	254	A	0.99993	Well, we have a constraint that says that a light can only be connected to one other light.
2159994	2169840	255	A	0.99993	So pred one has a constraint on it that says it's exclusive.
2170000	2175572	256	A	0.99989	So an object cannot pred one to two objects cannot connect to two objects.
2175636	2179332	257	A	0.99958	That's a constraint that was discovered and part of the causal theory.
2179476	2192172	258	A	1	And also implicitly, the on relation of the predicate has the value on or off, and it cannot be both at the same time.
2192226	2195180	259	A	0.99339	So it's conceptually unified.
2195700	2197680	260	A	0.95892	Is it statically unified?
2198020	2201292	261	A	0.99	Are the static rules obeyed.
2201436	2212324	262	A	0.99988	Well, for example, the static rule would say that given that B connects to A, if B is off, then A must be on.
2212362	2217540	263	A	0.99923	So if you look at any place where B is off, a is going to be on.
2217610	2223716	264	A	1	And you could do that for every other light and relationships between lights.
2223748	2226404	265	A	0.99955	So they all obey the static rule.
2226532	2236668	266	A	1	And the causal rule says, for example, here, that if B connects to A, then if A was off, then B must turn off.
2236754	2241790	267	A	0.99927	So if you look at B, let's say B was off.
2245680	2259716	268	A	0.73	Yeah, I'm sorry, if B connects to A, and yes, and if A was off, B must be off.
2259818	2264020	269	A	0.98112	So if A was off, B becomes off the next step.
2264090	2266310	270	A	0.99082	So that's correct as well.
2267240	2272730	271	A	0.8266	So statistically we are true, and temporarily we are true.
2273500	2275060	272	A	0.99984	We are unified.
2275220	2279480	273	A	1	And of course, that we get a thumbs up from Dr.
2279550	2283928	274	A	0.79454	Kant, our causal theory is unified.
2284104	2289580	275	A	0.99656	Thus it makes sense of the observations of the sensory inputs.
2291120	2299308	276	A	1	Now, it's no accident that Kant would be would figure in an active inference project.
2299474	2310980	277	A	0.99999	There is a link between active inference and Kant, and it runs through the celebrated 19th century German engineer Herman von Hemholz.
2311320	2319536	278	A	0.99301	He was a disciple of Kant and he developed the theory of visual perception that operationalized Kant's epistemology.
2319648	2322520	279	A	1	And in fact, it anticipates predictive processing.
2322860	2329284	280	A	0.99993	In 1995, Peter Day and Jeff Hinton developed the Helmholtz machine.
2329332	2331000	281	A	1	Name is in his hunter.
2332220	2338732	282	A	0.99747	It is a type of artificial neural network that's trained to create a generative model from an original set of data.
2338786	2341950	283	A	1	And it can account for the hidden structure of the data.
2343040	2351410	284	A	0.99011	So as you see, there's a link which is discussed and elaborated in this paper, which is very interesting paper.
2352580	2356748	285	A	0.99	All right, so close parentheses.
2356924	2361780	286	A	0.99902	So we've looked at the App Perception engine from the perspective of philosophy.
2362120	2366550	287	A	0.83627	So now let's look at it through the lens of machine learning.
2369660	2372136	288	A	0.97	The observations constitute a training set.
2372238	2373850	289	A	0.8771	It's a very small one.
2374220	2380500	290	A	1	And the Appetition engine is the learning algorithm.
2380660	2384140	291	A	1	And what is learned, the output is a causal theory.
2386000	2397904	292	A	0.9999	So the learning process is unsupervised logical inferencing, and the output is a human readable logic program.
2398102	2423450	293	A	0.99995	So we see here that there's some profound differences with the more popular form of machine learning in that the training set is really small, that the product of the learning is actually a human readable artifact, in this case, a logic program.
2425500	2427770	294	A	0.99931	So this is the training set.
2429340	2447520	295	A	0.59298	As inputted lights led, a turned off at time one, b turned on at time one, a turned on at time two, b turned off at time two, et cetera, et cetera.
2447940	2462992	296	A	0.99981	So that's the training set and you feed this into the perception engine's algorithm and out comes a causal theory.
2463136	2468676	297	A	0.99989	So in a little bit more details what the algorithm is and does.
2468858	2477684	298	A	1	First, it extracts the observed object extent objects, the object types and predicates from the observations.
2477812	2481896	299	A	0.99988	So we have on, we have object A, object B.
2481998	2483692	300	A	0.99996	We have led as an object type.
2483746	2489020	301	A	0.99991	So that's all part of the observations and that becomes part of the extent vocabulary.
2489440	2503516	302	A	0.5	Then the application engine imagines unobserved objects, types and predicates for the relationships and properties, and that becomes a latent vocabulary.
2503548	2506380	303	A	0.68891	So there's a step of imagination.
2506540	2530760	304	A	1	Then, using the combined vocabulary, both the extent and latent vocabulary combined, it looks for a unified causal theory, a set of constraints, rules and initial conditions that obey the constraints of synthetic unity of apperception.
2532220	2539980	305	A	0.54761	Once it has this causal theory and with initial conditions, it applies the causal theory to these conditions and produces a trace.
2540640	2546990	306	A	0.99985	It recreates observations if you want and augments them and extends them into the future.
2547680	2555952	307	A	0.64	Now it looks at this trace and compares it with the initial observations for coverage and decides if this is a good causal theory or not.
2556086	2564784	308	A	1	Then it also looks at the causal theory complexity, how many rules, how complex are the rules, et cetera, and measures for complexity.
2564832	2578596	309	A	0.98386	So if we have a choice between two causal theories of equivalent coverage, the App Perception Engine will select the least complex one using Occam's Razor.
2578788	2587420	310	A	1	Now, if you look at this algorithm, you'll see that the boxes in green are not deterministic.
2587840	2589308	311	A	0.99998	This is where search happens.
2589394	2591976	312	A	0.99999	We can posit different kinds of objects.
2592088	2594888	313	A	0.99985	We can find different kinds of rules.
2594984	2598110	314	A	0.99918	So this is where search happens.
2599360	2604252	315	A	1	Now, app perception is implemented using logic inference.
2604396	2608896	316	A	0.99999	Actually, it uses three forms of logic inference.
2609008	2617648	317	A	0.90638	There's the one that we're more familiar with, which is deduction, where given rules and causes, we infer the effects.
2617824	2619600	318	A	0.77	Then there's induction.
2619680	2622324	319	A	0.99976	Where given causes and effect, we look for the rules.
2622372	2624728	320	A	0.99996	This is what science does, right?
2624814	2629108	321	A	0.99998	Looking for rules that would account for effects given the causes.
2629284	2636520	322	A	1	Then there's abduction, where given the rules and given what we observe the effects, we're looking for the causes.
2636600	2642220	323	A	0.99995	In this case, we're looking for the latent objects, the latent relationships between these objects.
2642800	2654690	324	A	1	And then you can combine both abduction and induction, where you're given effects, essentially observations, and you're looking for both causes and rules, which is what the App Perception Engine does.
2655300	2661524	325	A	1	And this is where in the algorithm, these kinds of inferences are at play.
2661722	2671956	326	A	0.99993	So positing latent objects, that's a form of abduction, imagining causes, finding the rules, well, that's clearly a form of induction.
2672148	2682916	327	A	1	And then applying the rules of a causal theory to some initial conditions to create a trace, well, that's deduction.
2683028	2690072	328	A	0.99984	We have the causes, the initial conditions, we have the rules, causal theory, and then we produce a trace, the effects.
2690136	2691544	329	A	0.99268	So that's deduction.
2691592	2697260	330	A	0.99682	So the App Perception Engine uses all forms of logical inference.
2698800	2707688	331	A	1	Now, just a reminder that the output of the App Perception Engine, that is what is learned is actually human readable.
2707884	2720904	332	A	1	You may want to compare that to a large array of floating points produced by traditional, the more popular form.
2720942	2722548	333	A	1	Of machine learning nowadays.
2722644	2726392	334	A	0.99895	So here, this is what's actually produced by the app reception engine.
2726446	2735060	335	A	0.99951	As it runs on a set of observations, it produces a logic program that is human readable.
2735220	2741404	336	A	0.99999	When you look at it, the only thing you need to kind of guess is what is meant by pred one.
2741442	2752480	337	A	0.99	And if you think, well, maybe it means connects to maybe the lights are connected underneath a board out of sight of the observer.
2754580	2757668	338	A	0.99999	But finding a unified causal theory is hard.
2757754	2762436	339	A	0.99989	So we have to guess what the latent objects and predicates are.
2762618	2767968	340	A	0.99961	What are the hidden lights, what are the hidden relationships between lights?
2768064	2777160	341	A	1	And we have to discover what constraints might apply on the predicates and what are the initial conditions from which we want to recreate a trace.
2777660	2782240	342	A	0.99993	What are the static rules that apply to simultaneous observations?
2782340	2792030	343	A	1	And then what are the causal rules that given observations at time T will predict observations at time T plus one?
2792480	2793550	344	A	0.99999	This is hard.
2794080	2798384	345	A	0.91578	As a matter of fact, it's non polynomially hard.
2798582	2808080	346	A	1	The search space grows exponentially with the size of the input, which is the size of the extent and latent vocabulary.
2809240	2816660	347	A	0.9983	So just like in chess, you can't predict to the end the consequence of a move because of common turbo explosion.
2817000	2826970	348	A	0.99999	With the app perception engine, you cannot systematically traverse the entire space of possible causal theories to find a good one because it's impossibly large.
2829740	2836140	349	A	0.99996	So the job of the apperception engine is to find a causal theory in a ridiculously large haystack.
2837520	2838830	350	A	1	How to do this?
2839600	2856000	351	A	0.9998	In my implementation, I follow the recommendations and I follow also the implementation in Richard Evans'paper by breaking the search space into chunks.
2856500	2864020	352	A	0.99	First there's a region and the region says so how many latent object types, objects and predicates will we allow?
2864090	2873364	353	A	0.99997	So what is the limit of imagination of the cognition actors that is trying to apperceive a causal theory?
2873492	2875428	354	A	0.61955	What are the limits of its imagination?
2875524	2891432	355	A	1	And within that region of bounded imagination, we carve it into templates where we say, okay, we're going to use these latent objects types, these latent objects.
2891576	2896000	356	A	0.86	And so basically, what vocabulary, specific vocabulary we're going to be using?
2896150	2897392	357	A	0.93258	We're going to use object one.
2897446	2907884	358	A	0.98814	We're going to use object pred one on top of the observed on predicate and observed A and B lights.
2908012	2916688	359	A	1	And we're going to set the maximum complexity on the rules and see if we can find causal theories that fit this template.
2916864	2930116	360	A	0.99973	So this is a carving up of the search space and having broken the search space into regions and templates, we have scopes in which to apply Heuristics.
2930228	2930936	361	A	1	Now.
2931118	2932600	362	A	0.74043	Why heuristics?
2933120	2938172	363	A	0.99972	Because the systematic traversal cannot be done in reasonable time.
2938306	2944476	364	A	0.56657	There's just too many candidate causal theories to look at to find a good one.
2944578	2946320	365	A	0.99997	So we use Heuristics.
2948260	2954384	366	A	0.99978	We find ways of maybe getting to a good solution faster at the risk of missing it.
2954502	2961028	367	A	0.99983	But at least we'll have an answer or no answer in a reasonable amount of time.
2961114	2967396	368	A	1	And there's a number of heuristics that I've implemented in my implementation of the app assetron engine.
2967578	2969040	369	A	0.96756	Well, there's time boxing.
2969120	2975240	370	A	1	At some point, you'd spend no more than this amount of time looking into a region or into a template.
2976060	2977348	371	A	0.91875	There's multitasking.
2977444	2980724	372	A	0.99543	Well, the problem is actually, as they say, embarrassingly parallel.
2980852	2988830	373	A	1	You can explore multiple regions and multiple templates in parallel and so make good use of a multicore computer.
2989680	2991612	374	A	1	You want to make sure you don't repeat yourself.
2991666	2998080	375	A	0.55906	So you don't want to traverse the same region twice or look at the same causal theory twice.
2998500	3003520	376	A	1	You want to satisfy maybe a good enough theory is just fine.
3003590	3006364	377	A	0.99991	We don't want to look for the perfect one necessarily.
3006412	3007730	378	A	0.87919	We may not have time.
3008740	3010044	379	A	1	You want to fail early.
3010102	3021184	380	A	0.99998	If you're in a region where nothing good is found, you may want to leave it quite quickly at the risk of maybe not finding a good one that is just over the horizon.
3021232	3023220	381	A	0.99901	But you want to be impatient.
3023380	3024884	382	A	1	You want to throw the dice.
3025012	3036350	383	A	1	You may want to kind of mix it up so that every time you run the app Perception Engine on the same problem, you may find a different solution first.
3037840	3041372	384	A	1	You want to go for the simpler solution first.
3041506	3044124	385	A	0.99	You may not want to try everything, just sample some.
3044242	3052144	386	A	1	You want to start with the easiest part of the search base first, be judicious and so forth and so on.
3052182	3054236	387	A	1	And most importantly, be selective.
3054348	3061760	388	A	0.99908	So reject any causal theory that would fail the constraints of unity of apperception.
3062200	3071156	389	A	0.99982	With all these in place, my implementation of the app Perception Engine gives pretty good results.
3071268	3073080	390	A	0.99973	So here I did a run.
3073150	3074148	391	A	0.99999	This is not cherry picked.
3074164	3082244	392	A	1	I decided to do one series of seven runs and collect the data and show it.
3082382	3093900	393	A	0.58	And in this run, I set up the apparition engine to only accept a perfect causal theory, one that would produce a trace that totally covers the observation.
3094480	3097260	394	A	0.98	And I did seven runs.
3097780	3100880	395	A	1	The first one succeeded, found it in 4 seconds.
3101300	3103756	396	A	1	The second one, it took 102 seconds.
3103788	3108672	397	A	0.99997	So there's some randomization in the order in which things have searched if luck is involved.
3108736	3113510	398	A	1	As I said, the third one, 1 second, that was pretty cool.
3115400	3121784	399	A	1	The fourth one, well, took 204 seconds, then 90, 612, 99.
3121822	3125496	400	A	0.99995	So quite a good distribution here.
3125678	3133960	401	A	0.95	Now, I said, okay, I'm going to run the app Perception Engine again on the same training set that I showed earlier, those two lights.
3134120	3145076	402	A	1	But this time I said, I'm going to accept the theory that has 85% or more coverage.
3145208	3153708	403	A	0.91451	So it recreates the observations well enough, but not perfectly.
3153884	3156524	404	A	1	And I time boxed it to 30 seconds.
3156572	3158276	405	A	0.99999	So you have 30 seconds to find it.
3158298	3158870	406	A	0.99938	Go.
3159640	3167760	407	A	1	The first run, it find a causal theory with 75% accuracy immediately.
3167920	3170292	408	A	0.82	Then the same accuracy, same coverage.
3170356	3171316	409	A	1	10 seconds.
3171428	3172196	410	A	0.96	11 seconds.
3172228	3174708	411	A	0.95391	It hit 29 seconds.
3174884	3177336	412	A	0.99974	It found a perfect one.
3177518	3180904	413	A	1	Then eleven, it found 87% coverage and stopped right there.
3180942	3181688	414	A	0.81789	That's good enough.
3181774	3189416	415	A	0.99	75 again, 100% is the first one it found above 85 in 18 seconds and 75% 0 second.
3189448	3193772	416	A	0.99986	So a good distribution again, so we're getting into reasonable times.
3193826	3196444	417	A	0.75483	We're not talking about hours here, we're talking about seconds.
3196572	3217536	418	A	1	And I'm hoping to do further optimizations and bring it down to something even smaller so that a cognition actor can say, I want to make sense of these observations, query the apperception engine, and get an answer, a causal theory within maybe a couple of seconds.
3217568	3218630	419	A	0.55147	That's my hope.
3220440	3222356	420	A	1	Now, something interesting here.
3222378	3231960	421	A	0.99517	It so happens that what makes it hard for the app perception engine to find a good causal theory is formally equivalent to what makes cognitive science as a whole hard.
3232030	3237020	422	A	1	And this paper here makes the case and proves the case quite cogently.
3238640	3254372	423	A	0.99634	So cognizant science wants to find models, functions or algorithms that explain, account for situated behaviors.
3254436	3272450	424	A	0.99987	So you feed into the cognitive science machine pairs of situations and behaviors, and you want to come out of it a model, an explanation, a function, or an algorithm that accounts for it.
3273220	3289840	425	A	0.99977	Well, the paper makes the case that if the explanation is to be bounded in size, then the problem is computable, but it's not tractable in the sense meaning that it's combinatorially explosive.
3289920	3302936	426	A	0.99984	But once you have a solution, it is computable and tractable to verify that the solution is good, that it accounts for the data that you're trying to understand.
3303118	3307850	427	A	0.99706	Well, this is equivalent, formally equivalent to what the app perception engine is doing.
3311200	3314300	428	A	0.99369	My implementation was done in prologue.
3314800	3316776	429	A	1	I will not go into the details.
3316808	3318648	430	A	0.97715	It's about 1000 lines of prologue.
3318744	3328480	431	A	0.84752	I'll just say that prologue is a programming language that use deductive inference as its model of computation with backtracking.
3329460	3339430	432	A	0.99982	So essentially it searches for a solution and will backtrack if it took the wrong branch if you want.
3340280	3346570	433	A	0.96	And we'll look for a different way of satisfying a line of the program.
3348220	3352170	434	A	0.99948	So let's just say that it makes traversing a search space.
3353340	3357316	435	A	0.96857	We get traversing a search brains for free when we program in prologue.
3357508	3360764	436	A	1	I won't go into any more details, but you can see some prologue code here.
3360802	3371740	437	A	1	And the fun thing is that a prologue program is akin to a logical description of the problem it's trying to solve.
3372340	3373890	438	A	1	I think it's very cool.
3374420	3385344	439	A	0.93	And prologue environment was augmented by something called constraint handling rules, which is an extension to prologue that adds abductive reasoning.
3385472	3401400	440	A	0.99631	So basically in the program, you can say, assume this is true until proven otherwise, and the CHR rules are there to verify if it can be proven otherwise.
3401980	3415080	441	A	0.97668	So, again, I'm not asking you to understand this code at all, but I want you to realize that this code is the code that actually executes a causal theory, both the static and causal rules to build traces.
3415240	3416540	442	A	0.99915	It is that small.
3416690	3417672	443	A	0.99993	It's very powerful.
3417736	3423564	444	A	0.99842	So combining prologue and CHR, I found extraordinarily powerful.
3423612	3425088	445	A	0.26197	I'm very excited about it.
3425174	3426400	446	A	0.97267	I'm a programmer.
3427300	3428480	447	A	0.9983	Next steps.
3428900	3449800	448	A	0.99959	Well, next steps, now that we've solved individual learning by cognitive actors, well, I want to move to beliefs from sensations and to policies to validate or eliminate beliefs.
3451820	3454932	449	A	1	A lot of these beliefs actually fall out of our perception.
3454996	3459656	450	A	0.99905	Latent objects and latent relationships and properties can be considered as beliefs.
3459688	3465500	451	A	0.81	Then there are other kinds of beliefs that can be obtained from what's been perceived.
3466800	3491984	452	A	0.99565	There will be introspective beliefs that communicate how the cognition actor is doing in terms of competence, predictionary rates, how well its apperception is doing, and whether it is engaging with other cognition actors.
3492032	3493140	453	A	0.99998	Is it relevant?
3493880	3498804	454	A	0.63	I will have feelings which will provide normativity to these beliefs.
3498852	3530880	455	A	0.99992	So if feelings are signals of risk to homeostasis, loss of resources, physical damage, too many prediction errors so that's anger, pain, and fear and feelings will taint beliefs over time and tainted beliefs, good beliefs, bad beliefs will want to be eliminated or validated through policies that will be synthesized by the cognition actor.
3531460	3538180	456	A	0.99	And each cognition actor will make available to others its API.
3539320	3542896	457	A	0.99956	What predictions can be made about the beliefs of this cognition actor?
3542928	3551880	458	A	0.57753	What actions are available to others to be asked of the cognition actor?
3552460	3577120	459	A	1	And then as cognition actors connect to one another, as the conjunction actors form the umbelt of other cognition actors, then a conjunction actor will be able to predict the beliefs of others, will be able to compose policies made out of actions that are implemented by other cognition actors.
3577460	3586340	460	A	1	And eventually, we'll have a society of mind, which is a bunch of intersecting boom belts.
3587480	3590870	461	A	0.99511	So that's it.
3591480	3599236	462	A	0.98102	So I see the society of mine is a complex system of collective theorizers.
3599428	3609790	463	A	0.99	And I'm going to try going further with this project to answer the question if collective theorizers can self organize to actively sustain itself.
3610320	3620948	464	A	0.98907	So thank you to the Active Inference Institute for inviting me to present and for providing a home for this project and for the constant support and encouragement.
3621144	3623420	465	A	0.71431	I'll see you later on Discord.
3623580	3624450	466	A	0.99959	Thank you.
3625460	3626160	467	B	0.99987	Awesome.
3626310	3627756	468	B	0.7196	Thank you, JF.
3627948	3638112	469	B	0.99994	Just to conclude the session, I'll read two questions and let's maybe address them in an upcoming Robotics and embodied meeting.
3638176	3646944	470	B	0.99207	So if you're excited about this project, certainly we all are and about symbolic active inference, join the Discord and participate in the Robotics and Embodied.
3646992	3656080	471	B	0.97422	But I'll drop these two questions from David Williams in the Chat, who wrote, one, how important is conducting this work in real world versus simulation?
3656160	3663132	472	B	0.98	And two, what tools or components are missing in the robotics toolkit to make this research easier and better?
3663266	3668172	473	B	1	I know those are things that you have a lot of thoughts on, so I'll look forward to discussing with you more.
3668306	3669580	474	B	0.99983	Thank you, JF.
3671760	3672652	475	A	0.99279	Thank you.
3672786	3673580	476	B	0.95177	Peace.
3675920	3676910	477	A	0.96	All right.
3679600	3680090	478	B	0.99	See you.

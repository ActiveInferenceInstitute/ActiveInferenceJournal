start	end	speaker	sentiment	confidence	text
6010	6570	A	0.8110875487327576	Greetings.
6650	10954	B	0.5262095332145691	All right, well, our next session hey, Bert.
11002	11898	B	0.8110875487327576	Greetings.
12074	12414	B	0.7671424746513367	Great.
12452	13102	B	0.7295911312103271	Hey, how you doing?
13156	13790	B	0.893919050693512	Good, good.
13860	14718	C	0.6945193409919739	Very well.
14884	20106	A	0.8822855949401855	Our next session is with Bert DeVries, Dmitri Baghev, and Bart Van ERP.
20218	25246	A	0.7511935234069824	It's going to be called towards User Friendly Design of Synthetic Active inference agents.
25428	31910	A	0.986501157283783	And I know a lot of people are super excited to see this really practical and cutting edge work.
31980	35560	A	0.566513180732727	So to you, Bert, and just let us know how we can support.
36570	37720	B	0.8942707777023315	Okay, great.
38650	39926	B	0.794789731502533	Is my audio good?
40028	41434	A	0.9580843448638916	Yep, sounds good.
41632	44300	B	0.8394524455070496	Okay, then I'm going to share my screen.
46430	48042	B	0.709541380405426	I hope I picked the right one.
48096	50826	B	0.48582324385643005	I don't work with zoom quite often.
51008	51866	B	0.9723016023635864	Looks good.
51968	52522	B	0.9723016023635864	Looks good.
52576	52854	B	0.5491447448730469	Yeah.
52912	53502	B	0.4896698594093323	All right.
53636	54320	B	0.6397510766983032	Super.
55250	59514	B	0.9841301441192627	Well, thanks a lot, Daniel, for hosting this symposium.
59562	60986	B	0.8580859303474426	I've been watching some talks.
61018	62094	B	0.9727526307106018	It's really amazing.
62292	66830	B	0.8066226243972778	And we feel privileged to get a chance to present ourselves.
66990	73410	B	0.6936759352684021	So we are also, just like a few others before us, interested in developing a toolbox for active inference.
73830	81574	B	0.827052652835846	And so this picture, or she kind of shows what we're about or what we are interested in.
81692	88898	B	0.7980170249938965	So here's a lady on the left hand side, and I'm going to try to get a laser pointer.
88994	95194	B	0.7786382436752319	And she has this idea about a rewarding behavior for a vacuum cleaning robot, right?
95232	98438	B	0.8761873245239258	So she's writing down she has a textual expression.
98614	105902	B	0.7812003493309021	Move around the apartment, apply suction until the floor is clean, do not touch objects, and when done, return to the dog.
105956	107422	B	0.485246866941452	So that's not so hard.
107476	114880	B	0.7646257281303406	I'm going to rate that with one star out of three stars in terms of difficulty level to specify that.
115670	118722	B	0.6380929350852966	But that's not enough to program this robot, right?
118776	129650	B	0.8397719264030457	Because what she really needs to do now is to specify a generative model and there's effectors and actuators, right.
129720	134582	B	0.8373996019363403	The robot has to move around, apply suction until the floor is clean.
134636	136870	B	0.8547326922416687	So there are sensors, probably a camera.
137370	138866	B	0.6408888697624207	Do not touch objects.
138898	141102	B	0.82550048828125	Or maybe there has to be object recognition.
141266	146666	B	0.6153849959373474	This is a really difficult task to come up with this generative model here.
146768	156830	B	0.7858704924583435	And on top of that, she has to specify this kind of rewarding behavior in terms now of probability distributions of this generative model.
156900	158494	B	0.798179566860199	So very difficult.
158612	174930	B	0.6392272114753723	I'm going to rate that with two stars because the next thing she has to do for this model is to specify the inference procedure to do actually active inference and free energy minimization in real time for this complex model.
175000	177442	B	0.6591723561286926	And really that's almost impossible, right?
177496	187078	B	0.5904716849327087	Only a few specialists can really write a procedure for variational free energy minimization in some very difficult model.
187244	193426	B	0.8117102980613708	So what we are about what we've been working on is to try to automate the inference task.
193458	195302	B	0.6326918601989746	So get rid of the three stars.
195446	202554	B	0.8415591716766357	And yes, she will still have to specify her model, but in the long term, we try to get away from that.
202592	205786	B	0.5949612855911255	So in the long term, we hope we will get a toolbox.
205818	229490	B	0.8235928416252136	And now we're talking 510 years, right, where a textual description would be enough to specify some initial model with an initial prior and everything else is just automated inference, learning of states, parameters, structural adaptation of the model, even maybe based on her feedback updating the prior.
229650	238306	B	0.8282971978187561	So that's long term for now, we would be very happy if we could just automate the inference task.
238498	244218	B	0.5614333152770996	So why is it so difficult to specify inference for an active inference agent?
244304	249500	B	0.6008886098861694	Well, we have so many competing KPIs, right?
250110	261390	B	0.5721572637557983	We want to do this for large model scopes, not just for ABCD models, but maybe there's also continuous variables and hierarchical models, right?
261460	263450	B	0.8743991851806641	It must be very user friendly.
263610	268740	B	0.5292453169822693	We really don't want her to worry about robustness of her code.
269350	278874	B	0.5518767833709717	We don't want her to worry about whether two variables have conjugate relationships adaptivity.
278942	282006	B	0.8502055406570435	We want to update states, parameters, maybe even the model.
282108	298134	B	0.5304209589958191	The model structure has to be low power because these ancients often run on edge devices, so they run on their battery powered has to be in real time because you can't learn how to ride a bike if there's no real time reasoning.
298262	302586	B	0.6497275829315186	And on top of that, you actually want to minimize variational frequency, right?
302608	307930	B	0.5066047310829163	You want to do it at least as good or at least in a neighborhood of if you would do a manual derivation.
308010	311694	B	0.6236024498939514	And some of these decidorata bite each other, right?
311732	320962	B	0.5895470976829529	If you want to minimize variational free energy, but you have to do it in real time and on low power that kind of bites each other, right?
321016	328966	B	0.5346351265907288	So these are difficult KPIs that they're all important.
329068	334440	B	0.8852148056030273	You can't just take one out because then the whole system wouldn't work.
335130	350358	B	0.8034046292304993	So when you read papers on active inference, you often also read and now we implement variation of minimization and that can be done by message passing on a graph.
350454	356670	B	0.8594207167625427	And I want to clarify first why it has to be done by message passing on the graph.
357010	361294	B	0.871584415435791	I do that by giving a very short answer and then do an example.
361412	373474	B	0.8158167600631714	The short answer is that Bayesian inference involves computing very large sum of products, like what you see here on the left hand side.
373592	380274	B	0.8898301720619202	Here's a product AC, ADBC, and then we sum them AC plus ad and so forth.
380322	381846	B	0.7691583633422852	This is a sum of products.
382028	391160	B	0.8931668400764465	Now, we know by the Distributive law that this here on the left hand side can also be computed as on the right hand side.
391610	395766	B	0.818035900592804	If I multiply this out, I get a times C plus a times D and so forth.
395878	400714	B	0.505914032459259	This is a product of sums and they're exactly the same thing.
400912	410026	B	0.6104285717010498	The only difference is that to compute the left hand side takes four additions sorry, four multiplications and three additions.
410138	415270	B	0.8681515455245972	To compute the right hand side takes two additions and only one multiplication.
415450	420994	B	0.6277415156364441	So on the right hand side is much cheaper to compute than the left hand side.
421192	438434	B	0.7174742221832275	Normally when we write down marginalization and Beijing inference, we write things down as in the left hand side what message passing does on the graph, it will automatically convert that into much cheaper to evaluate product of sums.
438562	441420	B	0.8002023100852966	And I'll give an example of that.
443470	450650	B	0.9052178859710693	So here is an example model F of seven variables x one, x two through x seven.
450800	457642	B	0.8904106020927429	And this model happens to be factorized FA of x one, FB, x two and so forth.
457786	462186	B	0.8394251465797424	Now, we can draw this factorization as a graph.
462378	471086	B	0.8934985995292664	And what we do, and this is called a Farney style factor graph, what we do is for each factor FA, we allocate a node.
471118	474526	B	0.8998468518257141	So FB gets a node and FC gets a node.
474718	479170	B	0.8057372570037842	And we associate the variables in our system with the edge.
479330	485158	B	0.8351836800575256	And an edge is connected to a node if that variable is an argument of that function.
485244	491142	B	0.8689432740211487	So FC is a function of x one, x two, x three.
491276	499530	B	0.8620993494987488	And that means that FC connects to the edges x one, x two, x three, and FD is only a function of x four.
499600	503510	B	0.837719738483429	So FD only connects to the edge x four.
503680	514400	B	0.609414279460907	So what you can see in this graph is this graph is nothing but a visualization of the factorization assumptions that we have for this model.
515350	530440	B	0.5404371023178101	Now, if I'm interested in a big marginalization task and I integrate out over all variables but x three, so x one, x two, x four and so forth through x seven, I'm interested in this.
531290	551834	B	0.7859174013137817	Then taking advantage of this factorization, I can rewrite basically this sum of product into a product of sums as below here, what you will see here below this computes exactly the same thing.
551872	554670	B	0.8381404280662537	But I've made use of this distributive law.
554820	560766	B	0.8358376026153564	For instance, FC contains no x four, no x five.
560868	564270	B	0.8773365616798401	So I moved it over the summation sign to the left.
564420	569858	B	0.5814797282218933	And FB also doesn't contain x four, x five, x six, seven.
569944	572100	B	0.8528501987457275	So I moved it all the way to the left.
572550	587462	B	0.7089031338691711	And when you do that, you are left here with an expression where I only sum over two variables and here I have to sum over six variables and here over two, and here over two.
587596	602778	B	0.7294852137565613	So you can imagine if each variable, let's say x one, x two, if each variable has ten interesting values that you need to sum over, then I have here the original marginalization problem.
602864	613902	B	0.7886287569999695	I have ten to the power six, so a million terms, and here in red I have 100 terms and here I have 100 and here I have 100.
613956	618418	B	0.5902297496795654	So here I have 300 terms and here I have 1 million terms.
618584	629014	B	0.566583514213562	So it's an enormous reduction in computational complexity when we make use of this distributive law.
629132	638722	B	0.8010550141334534	Now, it turns out that if you write this out, you can associate these intermediate factors with messages on the graph.
638786	641862	B	0.7730504274368286	It's just an interpretation, a visual interpretation.
642006	656894	B	0.8327398300170898	It's as if FC receives a message from FA and FB receive or FC receives a message from FB and computes an outgoing message MU x three.
657012	658686	B	0.8301475048065186	And the same thing for Fe.
658788	669138	B	0.9170573353767395	So Fe receives a message from its neighboring factors, FD and FF and computes an outgoing message, x three.
669304	683880	B	0.8448389768600464	So what you see here is that the entire marginalization process can be represented as basically computing a few messages on a graph and multiplying some of these messages with each other.
684650	690478	B	0.7463885545730591	And that's how you can do Bayesian inference and also how you can do variational free energy minimization.
690674	706410	B	0.48842138051986694	So this works in factorized models, but I would say even stronger if your model is not factorized and you have a lot of variables, there is just no way you can do proper inference.
706490	714042	B	0.5212287902832031	So any serious model is factorized, like the brain is almost sparse, is almost empty.
714106	720574	B	0.8585652112960815	We have, what is it, about 10 billion neurons, and each neuron connects to a few thousand other neurons.
720622	726366	B	0.5017889142036438	So if I would draw the graph, that graph is almost empty, it is hugely sparse.
726478	731590	B	0.6480638980865479	And so there is no other way to do inference in the brain than by message passing.
732170	737000	B	0.5524653792381287	So that's why message passing, just because it's more effective than anything else.
738650	742822	B	0.7781023383140564	Now then the issue is which message do you compute?
742886	744810	B	0.7707173824310303	How do you compute messages?
745550	748874	B	0.7524845600128174	Because there are different ways of doing it right.
748912	761774	B	0.8699557781219482	And we also read in active inference papers, you can do this by variational message passing, or expectation maximization, or belief propagation and variational LaPlace and all these terms.
761972	768830	B	0.5927324891090393	It turns out that there is an umbrella framework for all these methods passing frameworks.
768910	774686	B	0.67497718334198	And that umbrella framework is called Constraint better free energy minimization.
774878	781430	B	0.8646535873413086	And I will try to illustrate it by this slide.
781850	784050	B	0.79461669921875	So here I have this graph.
784130	796426	B	0.8556758761405945	This is just an example graph where my generative model is basically factorized in FA, FB, FC, FD and Fe.
796608	798362	B	0.849895179271698	And I've also written that here.
798416	800940	B	0.8595473766326904	So this now is the variational free energy.
801710	805422	B	0.7521781325340271	Now, I haven't made any assumption on Q of X.
805476	808894	B	0.8878402709960938	So Q of X is still Q of x one, x two, x three.
808932	814910	B	0.8408219218254089	It's just a joint overall variables and it doesn't have any factorization assumption.
815910	825602	B	0.7864218950271606	It makes sense to also assume that the posterior kind of follows the factorization assumption of the prior, namely of the generative model.
825736	842854	B	0.8323184251785278	So if we make that assumption and that means we're going to make the assumption that QX is also now a product of QAS of X of A, where QAS of X of A stands for beliefs over nodes.
842902	854986	B	0.8812447786331177	What I mean by that is that Q of B is a posterior belief over this node, meaning it's a posterior belief over the edges that connect to this node.
855098	873890	B	0.8869389295578003	Just like FB is a function of x one, x two, x four, that's if you will, the prior or the generative model, then Q of B, the variational posterior for this node will also depend on x one, x two, x four, and on no other factors.
874470	886498	B	0.8485201001167297	If you just do that, then you will count some of the variables double because x one is part of the belief over FA, but also part of the belief over FB.
886674	892806	B	0.5305708646774292	So we just have to discount that by dividing by beliefs over edges.
892998	908910	B	0.7453662157058716	That means that I make now an assumption that my posterior beliefs is divided into local beliefs over notes and local beliefs over edges over variables.
909570	911994	B	0.9367879033088684	This will make things a lot simpler.
912042	926450	B	0.681825578212738	In fact, if my graph is a tree and I did the tree here, and I would do message passing on that tree and I could suppose I could do that perfectly, everything is linear gaussian, then I get perfect Bayesian inference.
927030	928514	B	0.5338679552078247	There is no approximation.
928562	930386	B	0.7784498333930969	So this is a good assumption.
930578	946010	B	0.7020581960678101	Sometimes it's still very hard to compute a message because even the single messages that come out of these nodes, they're still integrals or summations, and in particular the integrals may be a problem.
946080	948266	B	0.5178475379943848	We may not have an analytical answer.
948448	953814	B	0.7120934128761292	So what we sometimes do is add additional assumptions.
953942	965270	B	0.5935525298118591	We'll say, well, the posterior belief over FD, I can't compute it in general, but I'm going to just assume now that it's a gaussian that makes it easier.
965450	988360	B	0.8363136649131775	Or we can make an extra factorization assumption and say the posterior belief over FB, which is really a belief over the joint x one, x two, x four is going to be broken into independent belief over x one and belief over x two and x four.
990810	996686	B	0.8677579164505005	These additional assumptions, if I impose them as well, this is what I recall.
996738	1004222	B	0.8781455159187317	Now, if I all substituted here in Q of x, I get what's called a constrained beth free energy.
1004356	1007386	B	0.7946063280105591	This is the same Beth as in the Oppenheimer movie.
1007418	1010560	B	0.8897320032119751	This is Hans Bethe, where it's named after.
1013730	1033238	B	0.8468059301376343	We have a graph now that is highly factorized and we have local beliefs over notes and over and they're indicated with red and we have additional constraints in green.
1033324	1037346	B	0.8595837354660034	They could be Gaussians or mean field constraints or other constraints.
1037458	1041814	B	0.7943889498710632	And now we will assume constraints that make it possible to compute all the messages.
1041862	1045750	B	0.5613463521003723	And now we can just automate this by making different assumptions.
1045830	1054106	B	0.8645338416099548	We can turn this into expectation maximization or belief propagation or hybrid forms thereof.
1054138	1058526	B	0.6995662450790405	We can turn it into any relevant message passing algorithm that you've heard of.
1058628	1064160	B	0.9845734238624573	So this is a very nice umbrella framework that basically encompasses everything.
1066070	1075940	B	0.5911004543304443	We've written a pretty large paper on this in the Entropy Journal where you can read all the math on how this works.
1077430	1084674	B	0.532288670539856	So we've talked about why message passing, namely because it's the most effective way of doing inference.
1084722	1096726	B	0.7112402319908142	And we've talked about which messages to compute, namely we turn our variational free energy into something called a constraint, better free energy and then we can compute messages.
1096838	1102102	B	0.6867533922195435	The only thing that's left is, well, when do we pass these messages?
1102246	1103946	B	0.8600783944129944	What is the sequence of messages?
1103978	1105520	B	0.8425489664077759	Which one comes first?
1105890	1110400	B	0.7728103995323181	And this is where we see a lot of papers, right?
1111730	1114586	B	0.8034799098968506	You have to write control flow, what's called control flow.
1114618	1118734	B	0.8478372097015381	You have to say, okay, here is my algorithm for active inference.
1118782	1120370	B	0.7662548422813416	First I specify a model.
1120520	1133910	B	0.8671891093254089	Then let's do inference for every time step, collect a new observation, update the state, update the desired future, and so forth, compute expected free energy, select the policy, et cetera.
1135610	1137510	B	0.5769879817962646	This kind of program.
1137660	1142598	B	0.8113980293273926	The problem with active inferences is that there is nested for loops in here.
1142684	1145894	B	0.7940585613250732	Here's a for loop, and here's another for loop.
1145942	1152486	B	0.7002043724060059	And for each of these policies, I'm going to have to go into the future, so I'm going to have another time loop.
1152518	1155178	B	0.6142606735229492	So it is for loops, in for loops, in for loops.
1155274	1160186	B	0.48987433314323425	This will completely explode in terms of computational complexity.
1160378	1167742	B	0.8947201371192932	So as a result, some very clever people have written very clever algorithms of doing this much faster.
1167806	1176770	B	0.8187636733055115	Sophisticated inference, branching time active inference, dynamic programming EFE are recent proposals for doing this very clever.
1177190	1186150	B	0.747484028339386	In the end, all of these proposals come down to a particular just a message passing schedule.
1187530	1193866	B	0.8021265864372253	Once we commit to message passing on the graph as our inference procedure, it's the only thing that's going on.
1194048	1202966	B	0.7452970743179321	And all of this sophisticated inference and branching time active inference, all it does is it schedules the messages.
1202998	1206270	B	0.7977231740951538	It says first this message, then this message, then this message.
1206420	1210714	B	0.6270232796669006	I don't mean that as a slight to these algorithms.
1210762	1211530	B	0.8434652090072632	They're very clever.
1211610	1219390	B	0.853114902973175	And as we've seen in the presentation by Aswin Paul, you get huge improvements if you go from regular inference to sophisticated inference.
1219550	1228600	B	0.6248460412025452	But it's good to realize that these algorithms just specify in a graph which message comes after which message.
1229850	1238690	B	0.8705670237541199	So here's an example of a graph and a message sequence.
1238770	1248586	B	0.8716229200363159	Here's message one, then message two, and message three goes up, and then we go from FC to FF, and here's message five, and then we go to Fe.
1248688	1255790	B	0.8787232637405396	And this could correspond this sequence to dynamic time programming EFE or sophisticated inference.
1256530	1268802	B	0.7131177186965942	There are a couple of problems with this approach, which basically with having the user to specify a clever algorithm, first of all, you have to be a specialist to do it right.
1268856	1270660	B	0.7731958031654358	Only these are very clever people.
1271030	1282470	B	0.6490933895111084	That means that if we let it leave it to say to an engineer in a company, well, it's a high probability he's not going to get it right.
1282540	1283990	B	0.9504074454307556	That's very unfortunate.
1284570	1292902	B	0.656349778175354	But there is another issue, and that is that in a sense, it's a global variable in the message passing schedule.
1292966	1299430	B	0.711046576499939	All nodes are visited, because if a node would not be visited, then we shouldn't have it in the graph.
1299510	1305754	B	0.8808546662330627	And that means if one node crashes, basically the message passing schedule is invalid.
1305802	1307520	B	0.6169372797012329	I have to reset my system.
1308290	1320690	B	0.7557501792907715	And if you fly a drone, if it's deployed and it's out in the field and a node crashes, a transistor burns out, and I have to totally reset now my system, I have to compute a new message passing schedule.
1321510	1325378	B	0.7649028301239014	Then you're not doing inference and your drone flies into the wall.
1325544	1327578	B	0.8074724674224854	So this is not robust.
1327774	1334866	B	0.8361436724662781	And it also for the same reason we may actually want to take out a node.
1335058	1340350	B	0.8857478499412537	We may want to prune a node, we want to do structural adaptation.
1340530	1348182	B	0.6492791175842285	And we can't do structural adaptation because we have to reset the system, recompute a message passing schedule.
1348326	1358378	B	0.6110280752182007	So this procedural style where an engineer specifies which message comes after this message has some disadvantages.
1358474	1359962	B	0.779079258441925	It's not very robust.
1360106	1365190	B	0.516167402267456	And if you want to do it very clever, you have to be really a specialist.
1365370	1371330	B	0.47686508297920227	So a better system is what we call reactive message passing.
1371910	1379830	B	0.8962143063545227	And it's very related to what was in the first session called the actor model.
1379980	1383686	B	0.9573380947113037	Keith Duggar had a nice presentation on the actor model.
1383868	1390366	B	0.6993296146392822	So what we will do is we will say we will not have a global message passing schedule.
1390498	1394374	B	0.5065006017684937	The engineer will not specify anything anymore.
1394502	1402234	B	0.8514211177825928	The inference code that an engineer will have to write is just say, react to any free energy minimization opportunity.
1402432	1404894	B	0.5096040368080139	In other words, there is no inference code.
1404932	1406510	B	0.5658071041107178	It's completely automated.
1407010	1415414	B	0.8646737933158875	And we will replace this global message passing schedule by local triggering inside the node.
1415482	1422340	B	0.7968621850013733	So each node is now just an autonomous system that's interested in minimizing its free energy.
1422950	1425966	B	0.8611018657684326	It can do so by sending out messages.
1426158	1428482	B	0.7855265140533447	And when will it do so?
1428536	1440602	B	0.7229891419410706	Well, it receives messages, and then when it looks at these messages and it feels like, oh, there is an opportunity for me to minimize free energy by or expected free energy energy by sending out a message.
1440656	1452780	B	0.8727195858955383	Then we'll send out a message and each node will do so by itself asynchronously so you get parallel distributed processing, or concurrent processing, as Keith called it.
1453490	1463710	B	0.9027928709983826	In principle, you could play this game on many computers at the same time, and so you get tremendous advantages.
1465510	1468114	B	0.6551743745803833	First of all, you don't have to write difficult code.
1468232	1475780	B	0.8306881785392761	Second of all, you can do multithreading or you can run it on multiple computers at the same time.
1477510	1491258	B	0.6262359023094177	And there's also robustness advantages because if a node crashes, then there's nothing that stops the system from just finding another path, right?
1491344	1497722	B	0.9022910594940186	If this node crashes, this path from here's message three, this path now doesn't work.
1497776	1502570	B	0.7805790901184082	So I cannot send anything to Fe anymore from X.
1502720	1504782	B	0.852368175983429	Well, then I just sent a new message here.
1504836	1505630	B	0.6866926550865173	Why not?
1505780	1515302	B	0.6137162446975708	It's like when water falls down a mountain and it zigzags its way down into the value and you halfway put up an obstruction.
1515386	1520126	B	0.6488556265830994	It just finds another path, not the preferred path.
1520318	1526722	B	0.4762933552265167	This has to find, well, the second best path, because the first path has been obstructed, right?
1526776	1529954	B	0.4971309304237366	And that's what's going to happen in this system as well, right?
1529992	1531700	B	0.5112062692642212	That's just how nature works.
1532150	1537074	B	0.6643199324607849	It tries to find the best path, the easiest path, and if that's not available, then we do the second best path.
1537122	1540638	B	0.8328669667243958	And that's also what you can do with reactive message passing.
1540754	1542406	B	0.8522870540618896	So you can prune nodes.
1542438	1548118	B	0.8283510208129883	You can do structural adaptation, and it's far more robust.
1548294	1556014	B	0.8844022154808044	And you can also do chance encounters with other drones, right?
1556132	1563262	B	0.5446698069572449	Drones that get close can start communicating with each other, and when they fire away, they stop communicating with each other.
1563316	1578920	B	0.6656273007392883	And this is no problem, because you can basically change who change nodes can change on the fly, who they communicate to and who they want to listen to.
1580890	1589490	B	0.5568851828575134	That's the way nature works, and also how it works when we do reactive programming and reactive message passing.
1589650	1598620	B	0.7464824914932251	So, in summary, we're interested in automating inference, in active inference agents, right?
1599150	1604522	B	0.542479395866394	Because it's an operation that's basically only for experts.
1604586	1619762	B	0.5448400378227234	And this active inference technology is not going to be successful unless we get more people, let's say, democracies it, and we get competent engineers being able to develop good agents, right?
1619816	1625154	B	0.6462158560752869	You shouldn't have to be a top specialist in the world to develop an active inference agent.
1625352	1633218	B	0.7959572672843933	Now, in order to automate inference, you must do message passing, and I've talked about that for efficiency.
1633394	1636280	B	0.8744580745697021	I've also talked about which messages to pass.
1636970	1645734	B	0.7699028849601746	Not necessarily do you have to follow this framework, but constrained better Free energy framework is very convenient.
1645782	1657546	B	0.6450229287147522	It's an umbrella framework that basically goes over all the interesting other message passing computations.
1657738	1664270	B	0.5025185942649841	When message passing, reactive message passing, it's fully automated, so you don't have to write any code anymore.
1664690	1669226	B	0.6304306983947754	In principle, you can do parallel distributed processing.
1669258	1670850	B	0.7932619452476501	It's robust structural changes.
1670920	1672782	B	0.5806919932365417	You can learn new inference pathways.
1672846	1675860	B	0.9080089330673218	So lots of advantages here.
1676230	1679140	B	0.6736648678779602	Now, how do we do it?
1679750	1685190	B	0.8529633283615112	I like to introduce a toolbox that we've been working on called ARX infer.
1685770	1689794	B	0.8608233332633972	And we do that with my lab here at the university.
1689842	1693434	B	0.8794191479682922	I'm here in Eindhoven in the south of the Netherlands, and we have a lab.
1693552	1695082	B	0.8565088510513306	The lab is called BIS lab.
1695136	1704126	B	0.7060860991477966	Here are postdocs and assistant professors and PhD students, and we've been working on this for many years.
1704308	1713390	B	0.9131572842597961	And some of these, like Albert and Ismail and Tyce, have written dissertations.
1714210	1718270	B	0.846399188041687	And our best work, we have consolidated that in a toolbox.
1719010	1721286	B	0.8145064115524292	And the toolbox is called Arcs infer.
1721338	1725198	B	0.8018295764923096	And if you want to have a look, you can go to the website, arcsinfer.
1725294	1726050	B	0.5522667169570923	ML.
1726470	1733042	B	0.9020072817802429	And Arctinfur works in the way that I've just discussed.
1733106	1734562	B	0.774913489818573	It does message passing.
1734626	1737638	B	0.5193867683410645	It tries to minimize constraint, better free energy.
1737804	1742950	B	0.6280487179756165	That means it can come up with all kinds of message passing algorithms.
1744590	1752634	B	0.8072953224182129	It will do it in a reactive way, and it will try to do it in real time and low power and all the KPIs that we're talking about.
1752672	1759386	B	0.5147032141685486	Now, it's, of course, not done, but it's functional and like to show some demos.
1759418	1768610	B	0.7194905877113342	And I will leave it to Dimitri and Bart, who are two advanced PhD students in my lab to show the demos.
1770710	1774370	B	0.6947088241577148	So I'm going to stop sharing.
1777350	1778100	A	0.9184247851371765	Awesome.
1778490	1779394	A	0.9118135571479797	Thank you, Bert.
1779442	1780150	A	0.9199342131614685	Great talk.
1780220	1780840	B	0.4753468334674835	Sure.
1782490	1783558	C	0.796157717704773	Can you hear?
1783724	1784102	B	0.5491447448730469	Yeah.
1784156	1784760	B	0.5491447448730469	Yeah.
1785290	1785750	C	0.584351658821106	Okay.
1785820	1788038	C	0.7296838760375977	I will try to share my screen.
1788204	1788920	B	0.584351658821106	Okay.
1790010	1791820	C	0.6958155632019043	So you should see it now.
1792430	1793420	A	0.9723016023635864	Looks good.
1793950	1797846	C	0.5447234511375427	Okay, so, yeah, hello to everyone, I'm admitry.
1797878	1814266	C	0.9064557552337646	Baghaiv so I'm a PhD student in Bioslab in Einhoven University of Technology, and yes, I have a small presentation about actual software developments in so over the past few years, we have significantly improved our tools.
1814378	1823790	C	0.8003792762756348	And basically my entire PhD was dedicated to implement this idea, which Beard was talking about, like implementing the variation of reactive message passing.
1823870	1830630	C	0.6390030384063721	And in this presentation, I just want to show you what you can actually do using this theory under the hood.
1831930	1837794	C	0.7836631536483765	Okay, so basically, in order to automate active inference, we need to automate Bayesian inference.
1837842	1846054	C	0.6616775989532471	And we have already a lot of solutions for that, such as Pyro NumPy, which is funded by Google, Info.
1846102	1850460	C	0.5619983077049255	Net is funded by Microsoft, turing is in July, PIMC, and many, many.
1851470	1857886	C	0.9807679057121277	And basically these solutions are really good and they're really good at prototyping as well.
1857988	1864474	C	0.5837598443031311	But our goal is eventually to be able to deploy these kind of systems, not just prototype.
1864602	1869810	C	0.6385690569877625	And we are really focusing on these particular properties for this automated Bayesian inference.
1870230	1874846	C	0.744691014289856	So it must be low power, adaptive, real time scalable.
1874958	1881810	C	0.7736056447029114	It also must be user friendly at the end and must support a large scope of models if we want it to be useful.
1883750	1893058	C	0.8412450551986694	In Bioslab, we want to build such a software with such nice properties and it's always about trade offs, right?
1893084	1903340	C	0.72841477394104	So we do something better in one particular domain and maybe other software libraries, they might be better in a different domain, but we are really focusing on this particular property.
1904430	1907754	C	0.6735718250274658	And so, yes, I will reiterate a little bit Bert's presentation.
1907802	1909102	C	0.8330914974212646	So how do we achieve this?
1909156	1926210	C	0.8356519937515259	So imagine we have an environment and we have an agent, and the agent takes some actions and the agent basically what he needs is to come up with some sort of good enough probabilistic model of its environment in order to do patient inference.
1926870	1936520	C	0.8092479109764099	And in our framework, we encode the model as a factor graph, which not only models the observations, but also actions and desired future.
1937930	1949126	C	0.6551200151443481	And this approach allows us to decompose these complex relationships between variables and hidden states into some kind of structure and local blocks.
1949318	1952490	C	0.7285537123680115	And it's not a black box anymore.
1953250	1958138	C	0.8818778991699219	And the model itself may have some sort of background motivation interpretation.
1958234	1970802	C	0.8405371904373169	It may encode your prior knowledge about some particular physical system and the locality of these blocks basically allows you to scale to millions of variables and hidden states.
1970936	1978450	C	0.7727735638618469	It allows you to pre optimize it maybe, or maybe use some sort of different approximation strategies in different places.
1978970	1983094	C	0.9522919058799744	So it allows a lot of very nice properties as well.
1983292	1988978	C	0.864403247833252	And we use reactive message passing to run actual variational Bayesian inference.
1989154	1995020	C	0.8364848494529724	It uses reactive programming under the hood to minimize the approximation to the variational free energy.
1995470	2000540	C	0.7655213475227356	And yes, as Bert also mentioned, it's very much related to actor model.
2002510	2005118	C	0.8693929314613342	And basically in Ariks and fur.
2005204	2016080	C	0.792761504650116	You can think of different nodes as actors themselves and they have basically one single purpose is to send a variational message that minimizes free energy.
2016610	2022366	C	0.6526923179626465	This is a very short and very high level description, but it is essentially what is happening under lipid.
2022478	2034246	C	0.8742496371269226	So we are not treating different agents which interact with each other as actors, but we also treat the actual components of the underlying model as actors themselves.
2034428	2036866	C	0.6890405416488647	It's like a very hierarchical structure.
2036978	2042250	C	0.7910269498825073	So this is the main central idea of this inference.
2043790	2049500	C	0.7500337958335876	So here for example, first example, we can do an inference in a dynamical system.
2050110	2054430	C	0.6774792671203613	This example, which is quite old already, I think it's like two years ago.
2054580	2065866	C	0.8260072469711304	So we track a position of the object given some noisy measurements which are indicated by green dots, the actual real signal, we cannot observe it, but we just can plot.
2065898	2072318	C	0.8018075227737427	It is shown as blue and the inferred signal is shown as red and the data set is infinite.
2072414	2076980	C	0.8443872332572937	The inference end just reacts on it and does not assume any particular data size.
2077430	2080020	C	0.8243603110313416	Simply reacts in your observation as soon as possible.
2080810	2084760	C	0.6311324834823608	Yeah, I'm actually not sure how smoothly Zoom shares my screen.
2085370	2092220	C	0.5671380162239075	Maybe you can see it's a bit lagging in the animations, I'm not sure because maybe zoom does not share it on a full frame rate.
2093070	2098982	C	0.891386866569519	And also on the right hand side you can see how we define models in our framework.
2099046	2101050	C	0.8368403315544128	We use Julia as a programming language.
2102910	2108206	C	0.8275495171546936	Basically, this is everything that you need to define this particular model and run inference on a data set.
2108388	2114250	C	0.638638436794281	And actually I literally spent more days to plot it instead of inference.
2114330	2121650	C	0.5007264614105225	So inference was an easiest part for me, plotting was way much harder to relate to user friendliness.
2123030	2128354	C	0.924200177192688	And we actually have plans to improve our model specification language, make it even easier.
2128402	2138870	C	0.7523450255393982	So now for technical reasons, we have some auxiliary statements in the model specification language, but we are working to improve that and make it even easier.
2141130	2149094	C	0.7318114042282104	This is another example which is similar to the previous one, that uses much more complex and linear dynamical system of the double pendulum.
2149222	2157470	C	0.6140180230140686	And the system is chaotic and we can observe only a small part of it with a lot of noise, also indicated as a green dots.
2158290	2172020	C	0.5416931509971619	And nevertheless, given good enough model, you can infer the other hidden states with pretty much high precision and the code needed for that is also relatively short.
2175510	2182390	C	0.8449147939682007	We also have examples with active inference agents that interact with their environment.
2183050	2191338	C	0.5718235373497009	So the left up shows mountain car problem, very famous problem.
2191504	2201820	C	0.8778167963027954	The left bottom side shows an active inference agent which tries to control the inverted pendulum from falling in the windy conditions it reacts in wind.
2202510	2208218	C	0.8437300324440002	We also have a demo of an agent that controls a pendulum in an ever changing environment.
2208394	2215342	C	0.7555586695671082	So on the right side you see a pendulum with an engine and engine has limited power and.
2215396	2220450	C	0.8982492685317993	The agent itself needs to reach the goal and the goal is indicated as a red circle.
2222470	2227326	C	0.8461052179336548	Basically, in this demo we can change the environment in real time and see how the agent reacts.
2227438	2237474	C	0.8603852391242981	So we can change the mass of the pebble among its length or the amount of noise in the measurements, or we can change the goal, we can change maximum engine power, et cetera.
2237602	2246410	C	0.737297534942627	So the agent will still try to infer the best possible course of actions in order to reach its goal and it just never stops reacting.
2248110	2256254	C	0.5429933071136475	It's also actually possible to restrict engine power such that it will not longer possible to reach the goal, right?
2256292	2258080	C	0.6561003923416138	But the agent will still try.
2260050	2280550	C	0.7964034676551819	We have other cool demos with smart navigation and collision avoidance which are still under active research and the code for them is not available publicly it will be soon available, but for example in this example we can define a set of agents with their boundaries and a set of their destinations.
2280890	2284710	C	0.8763651847839355	And we can see how they try to resolve their routes altogether.
2287050	2290558	C	0.7135785222053528	And we can have some static obstacles in the map.
2290674	2300570	C	0.531879723072052	We can see how agents can find their most optimal path in order to reach their goals and avoid any possible collision.
2301630	2308382	C	0.7439658641815186	And it's also not necessary to have static obstacles, the obstacles themselves may move.
2308516	2317614	C	0.8149006962776184	So on this demo we have hundreds of agents that navigate through a map of obstacles that move from bottom to top to the circles or obstacles.
2317742	2325730	C	0.7643728852272034	And agents are depicted as small dots and they need to go from left to right, basically avoid any sort of collision.
2327670	2341898	C	0.5207347869873047	And as I also mentioned, we want to perform efficient and real time inference, but we also to do it like low power, low performance on low performance devices such as Raspberry, Pi or Coolpy as an example.
2342064	2348630	C	0.8351302146911621	And we have some results of successfully running the Bayesian audio source separation, for example, on coolpy.
2348790	2356910	C	0.5492656230926514	So it is actually possible we try to run active inference agents also on Coolpy.
2357410	2374786	C	0.6636157631874084	So as the aforementioned inverted pendulum, and as I mentioned, we also need to have a large model scope and basically RICS infer has not been designed to solve any of the aforementioned problems.
2374888	2383590	C	0.8255698680877686	Specifically, we have a large set of different examples in our repository, different models, different data, different inference constraints.
2384250	2393506	C	0.7082077264785767	We have examples for linear regression, hidden Markov model altogether, grace model hierarchy models, misheard models, Gaussian process and so, right?
2393548	2395930	C	0.8880348205566406	So this approach is very versatile.
2396670	2416642	C	0.9350140690803528	And for example, if you compare it with sort of a conventional software libraries where you let's say have a library that solves a common filter, might be a very great library, maybe super fast, have top performance works great and very reliable, super good.
2416696	2422034	C	0.7273572087287903	But then you are constrained by this particular model common filtering, right?
2422072	2424020	C	0.6501640677452087	And you can't really change it much.
2424630	2437042	C	0.6335804462432861	In Ericsson Fur we are free to define our own models which we can pretty much easily define a model that essentially would act equivalently to common filtering equation.
2437186	2447500	C	0.8417759537696838	And so basically in the demo that I showed before about object tracking, it was essentially a common filter but was written in a probabilistic model.
2449470	2453282	C	0.5906230211257935	So yeah, that was my small addition to Bear's presentation.
2453446	2459550	C	0.8890244364738464	So our software is free MIT license and it's open source available on GitHub.
2460530	2466062	C	0.9007007479667664	Yeah, and we would be happy thanks to be able to present where we would be happy to answer.
2466196	2468080	C	0.6406362056732178	Any, thanks.
2474760	2475508	A	0.9184247851371765	Awesome.
2475674	2478100	A	0.8261963129043579	All right, I'll just ask a quick question.
2478170	2491000	A	0.7486708760261536	In the chat Marco asks sorry if I missed it are the collision avoidance demos real time adapting to other agents behavior or is it a collectively pre computed path?
2493020	2500412	C	0.7510186433792114	So basically they are not super real time, they're kind of fast to compute this path, like maybe 5 seconds or so.
2500546	2503292	C	0.7871881723403931	But we are basically working to make it real time.
2503346	2509228	C	0.6044572591781616	So we know what is the problem, we know where to improve and we will make it real time.
2509314	2511090	C	0.5153143405914307	Yes, almost.
2513220	2514224	A	0.6033201217651367	Next question.
2514342	2517404	A	0.9125931262969971	Do you have some comparative data with other methods?
2517452	2528736	A	0.664881706237793	And just more generally, what kinds of benchmarks or when you're talking with industry in different settings, what are people looking for that killer app of active inference?
2528768	2532200	A	0.8590136170387268	Or what are they looking for their key measures?
2534140	2547500	C	0.708166241645813	So I personally have a big paper about comparison with sampling based methods like HMC and also in my PhD thesis, there will be a comparison with nuts, also other sampling based methods.
2548240	2555790	C	0.7448104619979858	So, long story short, sampling based methods cannot really run this kind of sophisticated inference in real time.
2556240	2557852	C	0.8459552526473999	They're very time consuming.
2557916	2562992	C	0.7905351519584656	They do not really scale well to large problems which is really needed for active inference agents.
2563046	2570870	C	0.5464448928833008	Because if you have a large environment, very complicated, you will have a lot of unknown variables in your model.
2573480	2580230	C	0.9311981201171875	There is a paper that compares and basically we show that our approach scales much, much better.
2580540	2593770	C	0.8391392827033997	So I personally run on just a regular MacBook laptop, I run the model with 2 million unknown variables and it was quite fast.
2594620	2609090	C	0.5377633571624756	With sampling based methods you may find yourself in a model with like 100 variables and then you wait like 2 hours or something and then it turns out that your chain did not converge or something like that.
2611860	2612368	A	0.84200119972229	Cool.
2612454	2619712	A	0.7486594915390015	Yeah, it's people commenting in the chat like how far message passing and factor graphs have come.
2619766	2626884	A	0.9825451374053955	And so to Bias Lab and to Bert at all, we definitely appreciate this exciting line of research.
2627002	2637896	A	0.6992714405059814	I mean, there's so much to learn there and sometimes looking at the equations, it can seem like it's like written in stone and just sort of the beginning and the end is, you know, variational free energy.
2638078	2643012	A	0.4928343892097473	But then in your presentations you're really showing like no, we are hands on.
2643166	2648824	A	0.6576900482177734	That's where we get the interpretability, the modularity, that's where it really is implemented.
2648952	2652072	A	0.5630665421485901	And it's like an information logistics challenge.
2652216	2655730	A	0.6129429340362549	It's not like an esoteric philosophy question at that point.
2656740	2658400	B	0.5333142876625061	No, indeed.
2659620	2664144	B	0.49447348713874817	I should say it's taken us we are no geniuses, right.
2664182	2676356	B	0.578126847743988	So our lab exists more than eight years, and you see all the people in the lab, it's taken us many years with lots of wrong directions to get this to work to where it's now.
2676458	2678560	B	0.7608968019485474	So it's a very long path.
2678720	2699096	B	0.7782648205757141	But at this moment, I'm pretty confident that at some point in the future and we don't want to say in three months or in one year, but we will be able to write a toolbox that will allow people to design a generative model and just press a button and forget about the inference.
2699128	2700984	B	0.5746250152587891	You don't have to worry about inference anymore.
2701032	2707170	B	0.47870320081710815	It will be fast and automated, and that will happen, and it will happen within a few years.
2708980	2711644	B	0.5501841902732849	And maybe somebody else will write an even better toolbox.
2711772	2719190	B	0.8294409513473511	But I'm pretty confident that even our toolbox will be able to do that.
2720680	2727012	B	0.644275426864624	People talk about, why don't we have the success of deep learning and generative AI, right?
2727066	2737492	B	0.9204917550086975	Well, they have the success because of big data availability of big data, big computers and toolboxes TensorFlow and all the successes.
2737636	2742808	B	0.6955662965774536	We don't need big data because agents collect their own data in the field.
2742894	2750728	B	0.5903120636940002	We don't need big computers, active influence agents, they manage their power resources.
2750904	2760240	B	0.49210691452026367	But we need a really good toolbox because programming an active inference agent, programming the inference by hand is just not doable.
2760660	2765570	B	0.5967001914978027	So we need a really good toolbox that really automates this.
2766260	2770772	B	0.7531429529190063	We hope Arcs Infer will be one of the first toolboxes to do that.
2770826	2777430	B	0.820499837398529	I am sure that other people will also be working on it, and better toolboxes will come about.
2778200	2783210	B	0.7207674384117126	But I think the optimistic message is that it will happen.
2784300	2796904	B	0.8237706422805786	And once we have a toolbox like that, then we can actually a large community can start building agents, and we can actually show deployable agents in the field that they work.
2796942	2801292	B	0.5911147594451904	And they work better than reinforcement learning agent or whatever is out there.
2801346	2801950	B	0.5664746165275574	Right.
2802560	2806850	B	0.9778301119804382	So I think that's a very positive and hopeful message.
2808020	2808992	A	0.66706383228302	It's what we expect.
2809046	2810220	A	0.7113181948661804	It's what we prefer.
2810380	2811090	B	0.5491447448730469	Yeah.
2812900	2815730	A	0.8661284446716309	Any last comments from either of you?
2819240	2820084	B	0.8251933455467224	Comments from us?
2820122	2832120	B	0.9714505672454834	No, I'm just very happy to get the opportunity, and yeah, I want to everybody can download this toolbox.
2833340	2839240	B	0.725654125213623	I think at this moment, you still should be a programmer to work with the toolbox.
2839580	2846940	B	0.525435209274292	And I hope you're friendly, because it's not totally polished in the way that we want.
2847010	2848172	B	0.7509835958480835	But it's coming, right?
2848226	2848684	B	0.6987430453300476	It's coming.
2848722	2857484	B	0.9414293169975281	In the next years, there will be a good toolbox for almost everybody to use, but people that are interested, even people that are interested to work.
2857522	2870290	B	0.9746342897415161	Here at Biaslab, we have an open position for PhD students, so we're happy to receive emails from people that are interested to work with us.
2872420	2872896	C	0.8529649972915649	Thank you.
2872918	2874720	A	0.8530004024505615	Dimitri, anything in closing?
2876420	2877344	C	0.5952405333518982	No, just that.
2877382	2880248	C	0.965865433216095	Thank you again for possibility to present.
2880374	2881850	C	0.988166868686676	Super nice to be here.
2882860	2883368	A	0.84200119972229	Cool.
2883454	2883704	C	0.5491447448730469	Yeah.
2883742	2898890	A	0.9811347723007202	Well, later in the year, we will be discussing your two part recent work, and so we're going to be getting a lot into the details, and I hope that people in the institute and the ecosystem will be as excited as we all are.
2899420	2900410	A	0.8529649972915649	Thank you.
2900860	2901480	B	0.8529649972915649	Thank you.
2901550	2901796	B	0.5137447118759155	Bye.

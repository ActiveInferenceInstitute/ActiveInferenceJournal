start	end	speaker	sentiment	confidence	text
8140	19570	A	0.7531595230102539	And it's a great segue from collective behavior in surprise minimizing agents to collective behavior in surprise minimizing agents, I guess.
19940	21200	A	0.8490645289421082	Welcome, Connor.
21700	22112	B	0.5456083416938782	Hey.
22166	25984	B	0.4825323224067688	Sorry, I just turned on my audio, so I just heard the last thing you said.
26102	26432	B	0.5491447448730469	Yeah.
26486	27810	B	0.8753373026847839	Hey, welcome.
28420	29570	B	0.831592321395874	Yes, thanks.
30060	33268	A	0.9707918167114258	Um, we're looking forward to your presentation.
33364	38788	A	0.8473214507102966	Multi agent active inference and multiscale alignment, current developments and challenges.
38884	43400	A	0.7610098123550415	So feel free to share your screen or proceed however you prefer.
44060	44804	B	0.9534879922866821	Great, thanks.
44862	52056	B	0.7955566644668579	And before I start, because I've had issues with my voice saturating, like my microphone saturating, how's my audio?
52088	53036	B	0.8023521900177002	Is it clipping at all?
53058	54444	B	0.6941463947296143	Or does this sound okay?
54642	56704	A	0.9889017343521118	This is good and I'm watching it.
56902	57888	B	0.8497672080993652	Okay, perfect.
57974	58850	B	0.8529649972915649	Thank you.
60980	62690	B	0.818813145160675	I will share my screen.
66020	67328	B	0.7896571159362793	Share this one.
67494	69072	A	0.5245165824890137	The one thing I'll note oh, yes.
69126	69500	B	0.5175950527191162	OK.
69590	75280	A	0.49596932530403137	It's just JF symbolic implementation does not use statistical distributions.
75440	80748	A	0.8340345025062561	It uses the symbolic and the logical inference.
80784	88004	A	0.961107611656189	And now we're going to move back into the distributional space, and it will be awesome to see similarities and differences.
88132	91480	A	0.9830564856529236	So thank you, Connor, to you for the presentation.
92860	101740	B	0.9610488414764404	And thank you, Daniel, for the introduction and for inviting me, as well as big thank you to the other organizers of the Third Applied Active Influence Symposium.
102560	105340	B	0.9853770136833191	Yeah, I'm really happy to be here to present.
105490	105804	B	0.617027759552002	So.
105842	107144	B	0.7900664806365967	My name is Connor Hines.
107192	110940	B	0.867641806602478	I'm a PhD student at the Max Planck Institute of Animal Behavior.
111100	115872	B	0.8348464965820312	And I'm also a researcher at the Versus AI research lab.
115926	117090	B	0.7445245981216431	R D lab.
117780	123300	B	0.8383556008338928	So I'm going to do something a little bit, I guess, unconventional for people in my position.
123370	126112	B	0.856153130531311	Like, I'm a junior researcher coming to the end of my PhD.
126176	134648	B	0.8645133376121521	So usually when I give a talk, I would present on my own research, like what I've been up to for the last ten years working on.
134734	154764	B	0.8471502661705017	But instead of that, I'm actually going to talk about given the motivation of the symposium, I'm going to talk about something that's more of an overview or perspective on the current state of the field in multiscale active inference or multi agent active inference and what, in my opinion, we need to do to move forward as a field.
154882	161852	B	0.767768383026123	I think that's very resonant with the kind of motivations and the title, indeed, of this symposium.
161996	167808	B	0.821445643901825	So I'm going to give a general analysis of what multiscale active inference is, why it's important.
167974	184760	B	0.6280454397201538	I'm going to provide a brief analysis of its formal basis as it currently stands, and then what we need to develop in this kind of subdiscipline of active inference, multiscale active inference, to really make it rigorous and really to actually reap the benefits of what it promises.
186220	214050	B	0.613796591758728	So generally, active inference has been used a lot to design agents that can solve problems, plan, and just generally emulate behavior that we deem intelligent, which includes things like risk sensitive decision making, intrinsic motivations to resolve uncertainty, and finally, from a more scientific standpoint, the ability to furnish a process theory about how biological brains actually might work.
215140	233348	B	0.8048467040061951	But in a lot of the theoretical work on active inference from the last ten years or ten plus years, really, there's also alongside all the kind of practical building adaptive agents, there's a claim that active inference is inherently or intrinsically multiscale from the very get go.
233434	235044	B	0.7959004044532776	It is a multiscale framework.
235092	237610	B	0.7890404462814331	It's not just about building single agents.
238380	249192	B	0.8546656370162964	So it's really whenever we write down a single active inference agent, what we're implicitly implying is also a nested hierarchy of active inference agents both below and above.
249336	264028	B	0.7604349255561829	So colloquially, you'll often see this in papers as the idea that there's Markov blankets all the way down Markov blanket is a statistical structure that's very kind of intrinsic to the definition of agents as they are defined under active inference.
264044	265904	B	0.6805780529975891	So I'm not going to get into defining that.
266022	273004	B	0.6763921976089478	I'm kind of assuming that there's a more disciplinarian audience there, but I'm sure other talks, for instance, can provide better clarity.
273052	277428	B	0.8077352046966553	So yeah, Markov blanket's active inference all the way down, all the way up.
277594	292100	B	0.8836843371391296	And at any given scale, crucially, the free energy minimizing dynamics, or the active inference dynamics are kind of claimed to be aligned with or parallel to the free energy minimizing gradients at the level below and above.
292260	305756	B	0.8266288638114929	So the claim is that as agents are doing their thing and doing active inference at one level, it both entails and is constrained by active inference processes of the macro agent that they're participating in.
305858	311440	B	0.8773989677429199	So I'm a cell that's part of a tissue as well as the micro agents that comprise them.
311510	317650	B	0.6816554069519043	I am a free energy minimizing cellular agent comprised of organelles that are also minimizing free energy.
318440	327920	B	0.7608945369720459	So this kind of constrained neat nested gradient descent on free energy is part of the story of multiscale active inference.
328000	334372	B	0.6208159327507019	And it also crucially, assumes that these dynamics are aligned, correlated cooperative across these different scales.
334516	342484	B	0.8452343940734863	So I should mention that there is a formal argument made more recently, I would say in the last five years, about how this is possible.
342622	347852	B	0.8935861587524414	And it relies on an apparatus from statistical physics called the Renormalization group.
347986	362188	B	0.8192782998085022	This basically allows you to analytically identify shared symmetries energy and conservation laws at different scales in a given system that's comprised of subsystems and subsystems, so on infinitum.
362284	380128	B	0.8935187458992004	So there's a formal argument specifically made in a free energy principle for a particular physics monograph in 2019 that applies the renormalization group apparatus to multivariate stochastic differential equations that are kind of the equivalent of agents.
380314	390628	B	0.8986362814903259	So you can apply that framework to certain sorts of coupled stochastic differential equations that exhibit Markov blanketed sparse coupling structure.
390724	401064	B	0.8884187340736389	And you can kind of prove analytically that there are going to be nested systems of Markov blankets and that they're all in some sense minimizing three energies at their own scales.
401112	411170	B	0.8975687026977539	So I'll get more into that argument later, but I just want to mention that as I define multiscale active inference that there is a formal argument that's related to it.
412180	418268	B	0.829525351524353	So this slide I just put together to demonstrate the idea of nested free energy minimizing processes visually.
418364	430752	B	0.9143937230110168	So at a given scale, we can think of an agent as occupying some point in its free energy landscape indicated by this red orb, which represents, say, its configuration, its beliefs, and its actions.
430896	435272	B	0.6895281076431274	And it performs active inference and in doing so minimizes its free energy.
435326	438388	B	0.8455019593238831	So it changes the position of that ball on that landscape.
438484	440788	B	0.8449724912643433	And that is all we mean when we say active inference.
440884	449560	B	0.8901309967041016	That corresponds to the agent doing inference and doing action and kind of getting to the fixed point of its local free energy landscape.
449720	453544	B	0.8365190625190735	The multi agent case is simply when we add more of these processes.
453592	460848	B	0.793395459651947	So there's other agents usually assumed to be similar agents, and the word similarity, let's put an asterisk on that.
460934	464880	B	0.8361579775810242	And they're all sitting at different points in their own free energy landscapes.
465540	469270	B	0.8696950674057007	The position of their local red ball is maybe in a different place.
470760	494232	B	0.7343460917472839	So the claim of multiscale active inference is that as we link these multiple active inference agents together, so they can actually exchange information like observations and actions with each other, what we automatically get is some kind of superagent that is also minimizing variation of free energy and is in some sense an emergent or Supervenient active inference agent.
494366	505240	B	0.8040974140167236	And I say the word, we automatically get a superagent with an asterisk because there may be some conditions on that mapping from local to global that have to be elucidated.
505400	507648	B	0.8218311667442322	So we'll come back to that in a bit.
507814	518540	B	0.8562269806861877	But in short, I think the definition of multiscale active inference is very eloquently put in this paper by Rafael Kaufman, Pranav Gupta, and Jacob Taylor.
518620	522516	B	0.5316736102104187	I think Rafael Kaufman is actually going to be on the panel later.
522698	528580	B	0.958756148815155	And this line from their paper is a really nice, I think, just summary of what it is.
528730	530448	B	0.739564061164856	So I'll just read it out loud.
530544	544536	B	0.7975155115127563	The upshot is that in theory, any active inference agent at one Spaciotemporal scale could be simultaneously composed of nested active inference agents at the scale below and constituent of a larger active inference agent at the scale above it.
544638	550108	B	0.8266899585723877	In effect, active inference allows you to pick a composite system or agent A that you want to understand.
550274	569316	B	0.8286462426185608	And it will be generally true that both that agent A is an approximate global minimizer of free energy at the scale at which that agent reliably persists, and that agent A is composed of subsystems A, sub I that are approximate local minimizers with free energy.
569418	583396	B	0.8475021123886108	So that is the claim as I'm going to continue evaluating in this talk, and I think it's just a great reference point to make, okay, that's what multiscale active inference is.
583498	584504	B	0.7563615441322327	Why is it important?
584622	586104	B	0.7784913182258606	Why do we actually care about that?
586142	590424	B	0.5241076350212097	That sounds philosophically nice and beautiful visually, but why is that important?
590542	596248	B	0.7759105563163757	So there's a ton of actually really important implications of this, both for the engineering and the natural sciences.
596424	608396	B	0.8109971880912781	First of all, the namesake of this symposium, I assume inspired by this recent paper by Karl Frison and all about enacting ecosystems of parenthetically shared intelligence.
608508	611404	B	0.9079101085662842	So this is the third applied active inference symposium.
611452	616684	B	0.6449500918388367	So to really make this resonate with the applied aspect, let's make this very concrete.
616812	634628	B	0.5625563263893127	If we can figure out this multiscale endeavor, then we can actually engineer distributed systems of multiaging intelligence where local agents, in doing their own little local active inference processes, are also cooperatively instantiating a global agent that's also performing active inference.
634724	650168	B	0.7426000833511353	This has huge computational potential, of course, compared to kind of the state of the art predominant methods for artificial intelligence, which are deep learning, which really is about propagating global information through an entire computation graph.
650264	658336	B	0.6069263219833374	So although you could argue back propagation is local in some sense, it's really not local in the way that multiscale active inference progresses to be local.
658438	669300	B	0.7221258282661438	So if we can figure out how to actually engineer multiscale active inference, it will have really tremendous implications for the study of artificial intelligence just from that pure engineering standpoint.
669640	670896	B	0.6273645162582397	It'll be cheaper.
671008	674320	B	0.7360931038856506	In one word, it'll be computationally energetically, memory wise.
674400	674864	B	0.47216445207595825	Cheaper.
674912	675940	B	0.6422296166419983	A lot cheaper.
676440	688340	B	0.534037709236145	Secondly, from a kind of more natural sciences motivation, which is kind of where I'm coming from, I'm doing a PhD in biology, so I'm interested in questions about actual real systems in nature.
688500	699484	B	0.7557259798049927	Just the idea of being able to get super specific and rigorous about phrases like emergent intelligence, emergent computation, collective intelligence, superorganism that's often thrown around.
699522	701516	B	0.689584493637085	We're talking about social insects, right?
701618	709852	B	0.8353012204170227	These are terms that you hear thrown around in many different scientific disciplines that deal with multi agent systems, network systems.
709996	714796	B	0.6384848356246948	But none of these terms, to my knowledge, have really rigorous or precise conditions.
714908	721712	B	0.8295404314994812	Multiscale active inference is a kind of framework that's in the position to provide those rigorous definitions and conditions.
721776	730396	B	0.8930030465126038	So from a scientific standpoint, it could really lend a lot of potential and usefulness for other scientific disciplines.
730528	732788	B	0.6507686972618103	And finally, another pragmatic motivation.
732884	745940	B	0.6421210765838623	There's loads of fields that are obsessed with designing and engineering systems where local, selfish individual behavior can lead when networked appropriately to some desired collective outcome.
746020	749480	B	0.6582932472229004	And these disciplines really want to figure out how to engineer that properly.
749560	757170	B	0.7154479026794434	So this goes from the design of financial markets and trading systems all the way down to how do you design a multiplayer video game?
758100	760364	B	0.6951735615730286	So that's kind of just motivating.
760412	763010	B	0.5239648222923279	Why is multiscale active inference even interesting?
763460	768436	B	0.8336939215660095	So then the question, of course, becomes, is the multiscale active inference claim actually true?
768538	776950	B	0.9163806438446045	Are all multi agent active inference systems comprised of and themselves comprise nested hierarchy of free energy minimization agents?
777320	789028	B	0.9036806225776672	A glance at a smattering of other scientific disciplines that specifically deal with multiple agents, multiple interests, collective phenomena like coordination, group behavior, collective intelligence.
789204	792988	B	0.7056838274002075	A glance at all those disciplines would naively suggest that the answer is no.
793074	807200	B	0.513590395450592	So there's things like frustration in thermodynamic systems game theory, the very existence of zero sum games and mass equilibria bandwagon effects when we're talking about social networks and opinion dynamics.
807780	818480	B	0.7286791801452637	Sacrifices for the common good, which we see in different contexts in biology, like in the context of kin selection, but also in the context of arguably cell death in a tissue.
819300	828752	B	0.5317890644073486	These are all basically plenty of systems where local constraints and global constraints or desires or free energy gradients, whatever you want to call them, come into direct conflict.
828896	838356	B	0.5201895833015442	So the obvious example that I listed at the top of these bullets is the idea of geometric frustration that we see in Icing systems with very low temperature.
838468	849372	B	0.57794189453125	So these Icing models basically describe lattices of Ferromagnets that are happy when they're pointing in the same direction as their neighboring Ferromagnets and they can be in an up or a down state.
849506	853068	B	0.8472621440887451	So basically the magnet can be pointing up or pointing down.
853154	860880	B	0.5558366179466248	And these global systems are defined by a global energy function and the whole system is in some sense trying to minimize that global energy function.
861030	869804	B	0.7910416722297668	But sometimes you'll find cases in these collective systems where this little spin in the middle cannot be happy because they're getting conflicting information from two neighbors.
869852	876676	B	0.7195767760276794	I want to be pointing up in blue like the agent on the left, but I also want to be pointing down like the agent on the right.
876778	891108	B	0.6358480453491211	So this is a system that's collectively finding some fixed point of its global free energy, but it's actually leading to a local conflict where this agent is not at a point where it can do anything to make itself happy or minimize its free energy further.
891204	905150	B	0.6543805003166199	So just from even the basic glance at Ferromagnetic lattices, we already see instances where local and global gradients or local and global optima are not aligned in the right way.
905600	910656	B	0.8029320240020752	So given all this, the burden of proof for multiscale active inference is still on us.
910678	917260	B	0.6194933652877808	So we need to show that collective active inference systems generically do align again across scales.
917340	928470	B	0.7803460955619812	And maybe if we can put an X across the word generically and it's not some automatic condition, if they don't, then at least we have to establish exactly the conditions in which they do.
930140	934040	B	0.8782103061676025	So anecdotally we do actually have some conditions.
934460	940632	B	0.7977771162986755	There seem to be some kind of basic ingredients to get collective active inference to work.
940766	947544	B	0.9017433524131775	So one is that we basically need agents to exchange actions and sensations across some kind of Markov blanket.
947672	949224	B	0.5968604683876038	This is not really a condition.
949272	952220	B	0.8186523914337158	This is almost more part of what it means to be an agent.
952370	963184	B	0.5058585405349731	So having Markov blanket separation between agents is just another way of saying we have multiple agents in our system rather than a single agent if you're violating the Markov blanket property.
963302	969904	B	0.5990485548973083	So internal states of one agent are not allowed to see the internal states or external states of another agent.
970022	973188	B	0.6562101244926453	Then you're kind of cheating because you're kind of saying it's actually one agent.
973274	977476	B	0.8163021802902222	And what you're doing is information sharing within the brain of a single agent.
977658	990120	B	0.766845166683197	The second condition, which is something that's often hallucinated more anecdotally and not really formally, is this idea that agents need to have some kind of shared narrative or shared hidden states or censor space in their generative model.
990270	998764	B	0.7071589231491089	So I've worked a lot on collective active inference systems, just simulating agents and trying to get them to do interesting things together.
998882	1002556	B	0.6632118821144104	And my intuitions and experience do agree with this basic fact.
1002658	1011180	B	0.6679175496101379	If the agents don't have any similarity in what they're representing or trying to achieve, then it's kind of like trying to fit a square peg into a circular hole.
1011260	1027860	B	0.927185595035553	So this is really nicely elucidated in one of the earliest cases in this paper, a Duet for One by first and Fritz in 2015, where they show that for two agents to really align, they kind of have to have a shared generative model and then you can get kind of this nice synchronized behavior.
1028280	1031284	B	0.7265969514846802	Again, though, these things like what does similarity mean?
1031322	1033136	B	0.7778176665306091	What does a shared narrative actually mean?
1033178	1036376	B	0.6777622699737549	Formally, mathematically, those things have not been initiated yet.
1036398	1056216	B	0.6220574378967285	So right now, a lot of the building of these collective systems is based on our intuitions and kind of engineering things using some vague guidelines like, oh, they should have a shared sensor space, but there's no mathematical conditions or guarantees about what degree of similarity is needed between two agents models to get the intended dynamics.
1056408	1068048	B	0.7774642109870911	And finally, we have to have at least some agreement between the generative model of each agent and the generative process, which is really the behavior of the other agents generating their data.
1068134	1072736	B	0.8609243035316467	So this is kind of related to the previous point about having shared generative models.
1072848	1086708	B	0.6687588691711426	But just to be very specific, the physics of the space that transfers your actions to my observations that physics can't be dramatically crazily different than how our generative models represent those physics.
1086804	1098456	B	0.5549271106719971	So if we took two fish with the same generative model of each other and they normally would school together in a fish tank, but we throw them in a volcano or shoot them out into outer space they won't.
1098488	1099100	B	0.6718505024909973	School together.
1099170	1107432	B	0.625699520111084	Because then the generative process is so dramatically deviating from the way they are representing that physics, the way their generative model is constructed.
1107576	1111052	B	0.7591943740844727	So these are, again just ingredients, kind of guidelines or anecdotes.
1111116	1113836	B	0.5974752306938171	But there's nothing really rigorous behind these conditions.
1113948	1116080	B	0.706319272518158	They're more like a list of best practices.
1116580	1119056	B	0.7459447979927063	So now let's get on to actual rigorous stuff.
1119158	1129184	B	0.7639159560203552	So, the first real rigorous attempt to show that multiscale active inference generally works is in one section of this free energy principle for a particular physics monograph from 2019.
1129312	1142884	B	0.8235805034637451	So it leverages this apparatus I mentioned earlier, the Renormalization group operator, to basically show how one can successively coarse grain multivariate stochastic differential equations that admit sparse coupling between their state variables.
1143012	1157516	B	0.8427225947380066	So the main result in my mind that connects these renormalization group results to multiscale active inference is the fact that the Lagrangian of the system at one scale can be expressed as a function of the Lagrangian at other scales.
1157548	1167852	B	0.7362550497055054	And that applies in a scale invariant fashion that is the main kind of output or the main benefit of using a renormalization group apparatus.
1167996	1171236	B	0.8799011707305908	So you can kind of think of Lagrangian like the generative model.
1171418	1178420	B	0.8686016201972961	It's a physics term, but it's related to the generative model of the agents that comprise the system and therefore also their free energy.
1178570	1192712	B	0.7612070441246033	So in terms of active inference, it means that this reasoning of the renormalization group can be used to smoothly move between the models of individual agents at one scale and the model of a collective or larger agent at a different scale or a smaller scale for that matter.
1192846	1203550	B	0.7809266448020935	And the nice thing about it is general for all kinds of dynamics and thus generative models, it doesn't depend heavily on the form of the stochastic differential equations that form your system.
1205120	1210796	B	0.7112805247306824	The issues with it is that there's not a global link to Bayesian mechanics and active inference.
1210828	1213804	B	0.8653562664985657	It's still all done in the traditional physics formalism.
1213852	1218684	B	0.6626931428909302	So we don't actually have an explicit link to local inference and global inference.
1218732	1224800	B	0.7419418096542358	Although if you know the connection between the Lagrangian and the generative model and the free energy, then you can make that connection.
1224880	1228848	B	0.7313615679740906	But it's not actually made explicitly for us in this part of the monograph.
1228944	1236852	B	0.8495431542396545	It also requires the assumption that the generative model and the generative process are identical at the local level that's related to how the Lagrangian is defined.
1236916	1238436	B	0.6078879833221436	That's also restrictive assumption.
1238468	1240532	B	0.7831453084945679	That's probably not realistic in my opinion.
1240676	1246036	B	0.7478108406066895	And then finally, there's something about kind of spatiotemporal segregation of scales.
1246148	1258172	B	0.7518075108528137	So we need to make assumptions about how fast random fluctuations are at one scale relative to another scale in order to justify kind of coarse grading or forgetting about certain states as you move between scales.
1258236	1260732	B	0.8342949748039246	And that's also something that you could argue.
1260796	1269670	B	0.6457306146621704	Current research into collective dynamics challenges that assumption about how fast noise is at one scale relative to the next.
1270680	1279632	B	0.8013513088226318	So now I'm going to discuss quickly another small contingent of active inference research that is attempted to address this mapping between local and global inference processes.
1279776	1291668	B	0.8410059809684753	So what I want to kind of just generally say with this presentation and to our community is that the kind of approach taken in these two papers, first of all, active inference model of collective intelligence and spin glass systems as collective active inference.
1291764	1298648	B	0.5296817421913147	This is one of the types of research I think we really need to move active inference, multiscale active inference forward.
1298814	1305112	B	0.7915430068969727	So I'm not trying to be too biased because I am the first author on the second of those papers, but I'll also be the first one to point out the limitation.
1305256	1315100	B	0.925307035446167	But benefits wise, I think these approaches are really important because they formally relate a local generative model at one scale to a global generative model at a different scale.
1315180	1319524	B	0.7965617775917053	So really tie, how do the parameters of one model relate to a course grade model?
1319642	1329140	B	0.9432469010353088	And these are really good steps in the direction of a formal theory of collective intelligence that goes from local intelligences to global intelligence.
1330200	1331988	B	0.7538698315620422	However, there's still issues with these.
1332074	1337876	B	0.5052299499511719	One of them is that they only deal with issue with inference at the global level, not active inference.
1337988	1348588	B	0.8992578387260437	So both these papers concern with a bunch of local active inference agents that cooperate to form a global inference agent, like a passive baying agent, rather than an active inference agent.
1348674	1354472	B	0.612246036529541	And it's also unclear whether the systems studying these papers are actually very generic.
1354536	1362924	B	0.6924567222595215	Like the results are generic to studying collective intelligence in general, or they're nice formal arguments, but they're only applicable to these specific systems.
1362972	1370772	B	0.523925244808197	So we still don't have something that's even more kind of zoomed out and abstract than these, which tend to be a little bit case specific.
1370906	1377652	B	0.6826317310333252	And finally, the actual scale transcendence that we're doing in these papers is still relegated to really one step.
1377706	1385720	B	0.4855639636516571	We're not doing the full multiscale infinite scale regression that something like Renormalization group promises.
1386380	1392148	B	0.8017958402633667	So that's kind of the current overview of what I think are the most promising directions in multiscale active inference.
1392244	1394844	B	0.5856106281280518	And I'm aware on time, sorry, five minutes.
1394882	1413580	B	0.8859665989875793	So I'm going to quickly try to go through what I think are really promising directions to push in terms of multiscale active inference and kind of intuition pumps that I think will help us study these systems in a way that's different and also actually better informed by other research disciplines.
1413740	1420308	B	0.7142621278762817	So the general idea that I'd like to put forward is that misaligned gradients can actually be a good thing.
1420394	1426528	B	0.6845080852508545	So it's actually sometimes good when local free energy gradients are misaligned with global gradients.
1426624	1432748	B	0.5646964311599731	So sometimes the global system will actually do better if the local systems are performing worse.
1432864	1445260	B	0.5215033292770386	So this is something you could call multiscale conflict, where the free energy minimizing processes at one scale are actually doing bad, quote unquote, but it's because they're being driven by some higher scale process that is doing well.
1445410	1462160	B	0.5208345651626587	So rather than trying to always avoid constructing processes like this, I think this kind of frustration, to use the analogy from statistical physics, can actually be an inspiration for what we should investigate further because it actually might be key to facilitating optimality at different scales.
1464260	1480768	B	0.5508525371551514	The reason I put this forward is because there's loads of research, just recent research in the last several years that are suggesting that actually making local agents more frustrated or more unhappy might coincidentally or not coincidentally lead to better collective or global outcomes.
1480864	1484448	B	0.8968667984008789	So this is expressed in various forms in various bodies of work.
1484554	1497224	B	0.6044713854789734	One of the biggest patterns I've noticed is the study of collective behavioral systems over the last several decades is the idea that local noise and local Dysregulation can often facilitate global coherence or global coordination.
1497352	1507452	B	0.730257511138916	And where multiscale active inference has something to say, in my opinion, is in framing this benefit of local frustration in terms of a misalignment of free energy gradients.
1507596	1520016	B	0.7907612919807434	So it may be that actually temporary misalignment local free energy gradients from global ones may facilitate the descent to fix points in the global free energy landscape that satisfy everyone at all scales.
1520128	1527888	B	0.874875009059906	So I'm basically expressing an idea that's been known in various communities like Stochastic optimization and Stochastic resonance theory for decades.
1527984	1538520	B	0.7083300352096558	But I think we as active inference practitioners have a new and potentially useful perspective to shed on that, using the language of active inference and free energy minimization and Bayesian inference in general.
1538670	1558590	B	0.49011680483818054	So instead of thinking of accelerating optimization by just adding noise to the system, we can think of exactly how to design local generative models such that there's an optimal misalignment of local and global gradients or local and global generative models in a way that facilitates everyone in the long run actually facilitating or minimizing their free energy.
1560320	1563184	B	0.8300216794013977	So yeah, that's just kind of like something I'm putting out there.
1563222	1566496	B	0.592415452003479	I'm investigating it now in my own work, but I have no real results on that.
1566518	1573350	B	0.7963526248931885	But I just wanted to put that out there in this venue because I think it maybe will inspire other people to think in a similar way.
1573960	1585160	B	0.6404886245727539	So just to conclude, now, multiscale active inference, I would say, is still largely based on theoretical or philosophical descriptions and illustrious simulations, but we're still lacking a formal theory.
1585740	1602510	B	0.6250540614128113	There's some theory in terms of the renormalization group arguments of the monograph, but they're still, in my opinion, a bit underdeveloped, a little under demonstrated and relying on some restrictive assumptions like the fast and slow fluctuations, the identity between generative model, generative process.
1603200	1618956	B	0.7561755776405334	There's a few more recent papers, those two by Kaufman at all and then by myself on spingglass systems that have attempted particular proofs of multi scale Bayesian inference systems in particular situations, but their generality is still not known and not proven.
1619068	1633828	B	0.8154540657997131	So what I'm kind of trying to conclude with is by saying we need to incorporate findings from other disciplines related to the role of noise, conflict and frustration in facilitating, not subverting collective intelligence or collective coordination.
1634004	1642916	B	0.8711880445480347	And I think we can really benefit by looking at these other disciplines to help us build a really powerful formal theory of multiscale active inference.
1643028	1648856	B	0.8581567406654358	And finally, I think we need to set the goalpost for what counts as a formal proof of multiscale active inference.
1648968	1664720	B	0.5624316930770874	And once we get there, once we're saying, okay, this counts as proof, this is satisfying, how can we use that to actually do the hardest part, in my opinion, which is engineering actual multiscale active inference systems that are intelligent and minimizing free energy at multiple scales.
1666020	1669824	B	0.635395884513855	Yeah, so with that, I'm going to conclude looks like I'm just on time.
1669862	1679700	B	0.9409171938896179	So, yeah, thank you again for the invitation to present, and I'd like to thank a bunch of people who are listed here and beyond who have influenced my thinking and kind of my opinions.
1680600	1683050	B	0.8031215071678162	If there's time, I'm happy to take any questions.
1686700	1687450	A	0.9184247851371765	Awesome.
1687980	1690104	A	0.9199342131614685	Great talk.
1690222	1694430	A	0.8397275805473328	I'll just give a few seconds if anybody wants to type in a question.
1694800	1695932	A	0.9829391241073608	Also, it's really cool.
1695986	1707080	A	0.8386597037315369	Like, aswin in the previous session was highlighting Pymdp and just the way in which we enact the collective intelligence.
1707160	1718192	A	0.6352459788322449	Different people seeing a paper where an analytical formalization is introduced, and then there's still so much work to get it to the package, and then so much more work to take it to the last mile.
1718256	1722100	A	0.8260884284973145	And I think your presentation really checked a lot of those boxes.
1722920	1730556	A	0.6720552444458008	I'll just read a question and then that will just be an appetizer for our continued discussion.
1730688	1741320	A	0.9068472385406494	So Marco Lynn asked, do you expect the inferentially connected dynamics to exhibit behavior akin to theories of multi body systems?
1741660	1746350	A	0.897608757019043	And to what extent can we transfer insights from that multi body of work?
1747280	1761372	A	0.8548839688301086	And then second question, just for our thinking and learning from Marco, have you explored integrating work on self organized criticality with multi scale active inference or other frameworks?
1761436	1766940	A	0.82530277967453	Who can provide more flexible frameworks or assumptions for a generic notion of multiscale dynamics?
1767100	1768130	A	0.7468470931053162	Great questions.
1768820	1774812	A	0.9250901341438293	I hope that we can continue, have you back anytime, or just continue to collaborate in the ecosystem.
1774876	1778880	A	0.9889892935752869	So thank you for the epic talk, Connor, and good luck finishing your PhD.
1779300	1780404	B	0.9514397382736206	Thanks a lot, Daniel.
1780452	1781720	B	0.6551547646522522	Yeah, I'll talk to you soon.
1781870	1782550	A	0.6512061357498169	Talk to you soon.

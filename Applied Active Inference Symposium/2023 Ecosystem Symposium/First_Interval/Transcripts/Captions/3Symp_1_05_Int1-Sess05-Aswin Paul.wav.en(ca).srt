1
00:00:00,330 --> 00:00:04,078
And now welcome

2
00:00:04,164 --> 00:00:07,914
Aswin! How are you doing? Hi, Daniel.

3
00:00:07,962 --> 00:00:10,960
I'm good. Hanging out,

4
00:00:12,210 --> 00:00:13,406
looking forward. Can you hear me

5
00:00:13,428 --> 00:00:16,720
properly? Yeah, sounds good. And yes,

6
00:00:17,090 --> 00:00:19,582
looking forward to your workshop on

7
00:00:19,636 --> 00:00:22,766
sophisticated inference in pymdp. So it

8
00:00:22,788 --> 00:00:26,594
can be up to 90 minutes or it's totally

9
00:00:26,642 --> 00:00:28,358
okay if it's less and we can take a

10
00:00:28,364 --> 00:00:32,134
short break, but please take it away

11
00:00:32,172 --> 00:00:34,200
and just let me know however I can help.

12
00:00:34,570 --> 00:00:38,374
Great. Thank you so much. Maybe I'll

13
00:00:38,422 --> 00:00:41,260
share my screen and start. So,

14
00:00:42,270 --> 00:00:44,586
regarding the structure of what we are

15
00:00:44,608 --> 00:00:47,674
doing here, I'm assuming that people who

16
00:00:47,872 --> 00:00:50,234
see this later might want to try it

17
00:00:50,272 --> 00:00:52,526
hands on along with the tutorial or

18
00:00:52,548 --> 00:00:53,806
something. So that's the structure I

19
00:00:53,828 --> 00:00:57,178
kept in mind. And so I'll go slow.

20
00:00:57,354 --> 00:01:00,446
Please bear with me. So welcome all of

21
00:01:00,468 --> 00:01:02,554
you to this session on sophisticated

22
00:01:02,602 --> 00:01:04,946
inference in pymdp. So here we are going

23
00:01:04,968 --> 00:01:08,254
to attempt to model some sophisticated

24
00:01:08,302 --> 00:01:09,906
inference simulations, especially the

25
00:01:09,928 --> 00:01:12,786
one in the original paper using the

26
00:01:12,808 --> 00:01:15,266
pymdp module and how it is not part of

27
00:01:15,288 --> 00:01:17,126
pymdp right now. But we are on the

28
00:01:17,148 --> 00:01:19,122
process of adding sophisticated

29
00:01:19,186 --> 00:01:21,746
inference to the pymtp module. And I'm

30
00:01:21,778 --> 00:01:24,614
going to mainly talk about the code that

31
00:01:24,652 --> 00:01:27,994
I have developed to kind of add that

32
00:01:28,032 --> 00:01:30,598
functionality. Right. So I'm Ashwin

33
00:01:30,614 --> 00:01:34,106
Paul. I am finally a PhD candidate at

34
00:01:34,128 --> 00:01:36,554
Monash University. And mostly I work

35
00:01:36,592 --> 00:01:39,194
with active inference models and try to

36
00:01:39,232 --> 00:01:42,174
understand how to use them as an

37
00:01:42,212 --> 00:01:46,046
explainable model to basically

38
00:01:46,148 --> 00:01:47,706
understand emergence of intelligent

39
00:01:47,738 --> 00:01:50,206
behavior. Right, so let's dive into the

40
00:01:50,228 --> 00:01:53,374
material right away.

41
00:01:53,572 --> 00:01:56,082
So to give an intro of free energy

42
00:01:56,136 --> 00:01:58,674
principle, I'm sure all of you right now

43
00:01:58,712 --> 00:02:00,626
have an understanding of what it is.

44
00:02:00,728 --> 00:02:03,474
But the central idea is that an agent is

45
00:02:03,512 --> 00:02:07,298
always trying to minimize the entropy

46
00:02:07,314 --> 00:02:09,334
of its observations, right? So if an

47
00:02:09,372 --> 00:02:11,654
observation is having really low

48
00:02:11,692 --> 00:02:14,774
probability in your mind and that

49
00:02:14,812 --> 00:02:17,154
happens, then you are probably surprised

50
00:02:17,202 --> 00:02:20,118
and vice versa, right? And the entropy

51
00:02:20,134 --> 00:02:22,778
here is defined as the information

52
00:02:22,864 --> 00:02:25,818
theoretic entropy, where if you have a

53
00:02:25,824 --> 00:02:27,978
low probability, then automatically this

54
00:02:27,984 --> 00:02:29,686
is a high surprise or a high entropic

55
00:02:29,718 --> 00:02:33,066
observation. And as we all know, active

56
00:02:33,098 --> 00:02:36,254
inference also gives us a methodology to

57
00:02:36,292 --> 00:02:38,442
define what we call an agent environment

58
00:02:38,506 --> 00:02:41,726
loop. And this lets us define what is

59
00:02:41,748 --> 00:02:43,626
the agent that we are looking at and

60
00:02:43,668 --> 00:02:47,490
what is the behavior of the agent given

61
00:02:47,560 --> 00:02:50,210
the environment around it and so on.

62
00:02:50,280 --> 00:02:52,466
Right? So you're also familiar with the

63
00:02:52,488 --> 00:02:54,706
idea of Marco Blanket. And this is

64
00:02:54,728 --> 00:02:57,014
important because we always have to

65
00:02:57,052 --> 00:02:59,206
remember, I mean, have to remember the

66
00:02:59,228 --> 00:03:00,562
difference between the generative

67
00:03:00,626 --> 00:03:02,806
process and the generative model, which

68
00:03:02,828 --> 00:03:06,086
is quite a famous point of

69
00:03:06,108 --> 00:03:08,162
confusion in active inference literature

70
00:03:08,226 --> 00:03:10,058
and for the people who try to understand

71
00:03:10,144 --> 00:03:13,146
it in the beginning. Right? So the

72
00:03:13,168 --> 00:03:14,586
central question is that how does an

73
00:03:14,608 --> 00:03:18,074
agent minimize entropy? Because how

74
00:03:18,112 --> 00:03:20,074
does an agent know which observation is

75
00:03:20,112 --> 00:03:22,894
low or high probabilistic? Right? So

76
00:03:22,932 --> 00:03:24,394
that is by maintaining a generative

77
00:03:24,442 --> 00:03:26,814
model. And the generative model will

78
00:03:26,852 --> 00:03:29,194
tell you which is a high probabilistic

79
00:03:29,242 --> 00:03:31,166
observation and which is a low

80
00:03:31,188 --> 00:03:33,214
probabilistic observation. So the idea

81
00:03:33,252 --> 00:03:36,002
is that all the agent has access to is

82
00:03:36,056 --> 00:03:38,786
an observation that is coming from a

83
00:03:38,808 --> 00:03:40,370
generative process which the agent

84
00:03:40,440 --> 00:03:42,994
cannot directly observe. And an

85
00:03:43,032 --> 00:03:45,266
intelligent agent will try to build up a

86
00:03:45,288 --> 00:03:48,600
generative model in its mind, which is a

87
00:03:48,970 --> 00:03:51,366
model of the hidden states and the

88
00:03:51,388 --> 00:03:54,134
observation it has access to. And it can

89
00:03:54,172 --> 00:03:56,162
hope to kind of compute probabilities

90
00:03:56,226 --> 00:03:58,406
using this generative model, right? But

91
00:03:58,428 --> 00:04:00,826
there is a problem that in general it is

92
00:04:00,848 --> 00:04:03,306
an intractable problem to kind of

93
00:04:03,328 --> 00:04:05,146
marginalize the probability of

94
00:04:05,168 --> 00:04:06,858
observations from the generative model.

95
00:04:06,944 --> 00:04:08,906
And that is why we have to define an

96
00:04:08,928 --> 00:04:11,386
upper bound on the surprise that the

97
00:04:11,408 --> 00:04:13,446
agent is trying to minimize. And there

98
00:04:13,488 --> 00:04:15,054
comes the idea of free energy, right?

99
00:04:15,092 --> 00:04:16,910
So this upper bound the agent is

100
00:04:16,980 --> 00:04:19,502
supposedly minimizing is the free

101
00:04:19,556 --> 00:04:21,134
energy. And that's why it is the free

102
00:04:21,172 --> 00:04:23,454
energy principle. And here we have a new

103
00:04:23,492 --> 00:04:26,194
term called capital QS, which can be

104
00:04:26,232 --> 00:04:28,722
interpreted as the belief that the agent

105
00:04:28,776 --> 00:04:31,058
is maintaining about the hidden state in

106
00:04:31,064 --> 00:04:34,014
its generative model. And this quantity

107
00:04:34,062 --> 00:04:36,166
is the free energy. And traditionally we

108
00:04:36,188 --> 00:04:38,854
see this variational free energy being

109
00:04:38,892 --> 00:04:42,006
interpreted as in mostly the

110
00:04:42,028 --> 00:04:43,526
machine learning way where it is a

111
00:04:43,548 --> 00:04:45,506
balance between complexity and accuracy

112
00:04:45,538 --> 00:04:47,846
of the model. So when minimizing the

113
00:04:47,868 --> 00:04:50,006
free energy, the agent is trying to come

114
00:04:50,028 --> 00:04:52,186
up with a single model but at the same

115
00:04:52,208 --> 00:04:54,518
time an accurate model because here it's

116
00:04:54,534 --> 00:04:56,218
a minus sign with accuracy, right? It

117
00:04:56,224 --> 00:04:57,894
can also be interpreted in the physics

118
00:04:57,942 --> 00:05:01,806
way where the agent is always trying to

119
00:05:01,988 --> 00:05:05,198
minimize the energy of the model but at

120
00:05:05,204 --> 00:05:07,326
the same time maximizing the entropy of

121
00:05:07,348 --> 00:05:09,086
the model which goes in conjunction with

122
00:05:09,108 --> 00:05:11,934
the maximum entropy principle and so on

123
00:05:11,972 --> 00:05:14,702
from the classic literature. So, given

124
00:05:14,756 --> 00:05:17,266
that this idea of a generative model is

125
00:05:17,288 --> 00:05:20,178
so important in a software point of

126
00:05:20,184 --> 00:05:22,146
view, that's the first thing that you

127
00:05:22,168 --> 00:05:23,666
might want to do, right, to define a

128
00:05:23,688 --> 00:05:27,670
generative model for the agent which is

129
00:05:27,740 --> 00:05:30,194
informed or not informed depending upon

130
00:05:30,322 --> 00:05:32,086
the experiment that you're trying to

131
00:05:32,108 --> 00:05:34,514
model. So in classical active inference,

132
00:05:34,642 --> 00:05:37,478
usually decision making is defined in

133
00:05:37,484 --> 00:05:39,386
terms of policies. So, for example, if

134
00:05:39,408 --> 00:05:42,954
you are an agent in this environment so

135
00:05:42,992 --> 00:05:45,482
in the Mario game, Mario is the agent

136
00:05:45,536 --> 00:05:47,654
and everything else is the environment.

137
00:05:47,782 --> 00:05:51,838
And Mario has three available

138
00:05:51,924 --> 00:05:55,278
actions run, jump or stay in

139
00:05:55,284 --> 00:05:57,786
this environment. And the classic

140
00:05:57,818 --> 00:05:59,726
definition of policy is that it is a

141
00:05:59,748 --> 00:06:02,126
sequence of actions in time. So if you

142
00:06:02,148 --> 00:06:05,266
have a time horizon of capital T, then a

143
00:06:05,288 --> 00:06:07,234
policy is nothing but a series of

144
00:06:07,272 --> 00:06:09,266
actions you might take. So run, run,

145
00:06:09,368 --> 00:06:13,218
jump and so on in time. So this is the

146
00:06:13,384 --> 00:06:16,286
SuperScript is the action. So here it is

147
00:06:16,328 --> 00:06:18,674
jump, run and so on. And the subscript

148
00:06:18,722 --> 00:06:22,166
is the time. And then what you

149
00:06:22,188 --> 00:06:24,598
can have is a policy space, which is a

150
00:06:24,604 --> 00:06:26,310
collection of many such policies,

151
00:06:27,550 --> 00:06:30,746
small pi. And what you essentially do to

152
00:06:30,768 --> 00:06:32,858
take decisions in active inference is

153
00:06:33,024 --> 00:06:36,220
compute, not optimize. Compute the

154
00:06:36,590 --> 00:06:39,738
expected free energy of every policy in

155
00:06:39,744 --> 00:06:42,606
your policy space. And basically, that

156
00:06:42,628 --> 00:06:45,374
can be interpreted as a balance between

157
00:06:45,492 --> 00:06:47,150
risk and ambiguity.

158
00:06:50,770 --> 00:06:53,314
So when you compute this expected free

159
00:06:53,352 --> 00:06:55,298
energy, what you're trying to do is

160
00:06:55,464 --> 00:06:58,642
minimizing the risk. That is,

161
00:06:58,696 --> 00:07:01,922
how different is your belief about the

162
00:07:01,976 --> 00:07:04,942
observation from your prior preferences?

163
00:07:05,086 --> 00:07:07,618
Capital C. So this is also part of the

164
00:07:07,624 --> 00:07:08,998
generative model when you're trying to

165
00:07:09,004 --> 00:07:11,078
model control and at the same time you

166
00:07:11,084 --> 00:07:13,058
are trying to minimize the ambiguity

167
00:07:13,234 --> 00:07:14,966
when you are choosing a policy that has

168
00:07:14,988 --> 00:07:16,918
the minimum expected free energy,

169
00:07:17,004 --> 00:07:18,938
right? But this formulation has a

170
00:07:18,944 --> 00:07:21,162
problem that a policy space quickly

171
00:07:21,216 --> 00:07:23,850
becomes intractable, that there can be

172
00:07:24,000 --> 00:07:27,594
an enormous number of small pies or

173
00:07:27,632 --> 00:07:29,194
policies in your policy space and

174
00:07:29,232 --> 00:07:30,846
sitting and computing the expected free

175
00:07:30,868 --> 00:07:33,278
energy for all such policies even for a

176
00:07:33,284 --> 00:07:35,920
small time horizon, is not possible.

177
00:07:36,370 --> 00:07:39,786
But this is the classical structure

178
00:07:39,818 --> 00:07:41,738
that has been implemented in Pymdp

179
00:07:41,834 --> 00:07:45,282
nonetheless, where we have different

180
00:07:45,336 --> 00:07:48,580
modules in Pymdp that is meant to be

181
00:07:50,070 --> 00:07:51,746
implementing different aspects of

182
00:07:51,768 --> 00:07:54,970
behavior. So for example, for inference

183
00:07:55,070 --> 00:07:57,106
or perception we have belief

184
00:07:57,138 --> 00:07:59,154
propagation, fixed point iteration,

185
00:07:59,282 --> 00:08:01,480
marginal message passing and all that

186
00:08:03,210 --> 00:08:05,766
implemented in the inference module. In

187
00:08:05,788 --> 00:08:09,020
the control module we have different

188
00:08:09,790 --> 00:08:11,962
methods to evaluate expected free energy

189
00:08:12,016 --> 00:08:13,946
for policies, one depending upon the

190
00:08:13,968 --> 00:08:16,442
expected utility, the other one

191
00:08:16,576 --> 00:08:20,246
depending upon the classic method

192
00:08:20,278 --> 00:08:22,654
that I just explained. Then we have

193
00:08:22,772 --> 00:08:24,766
module for learning. So we learn the

194
00:08:24,788 --> 00:08:27,354
parameters in the form DP like capital

195
00:08:27,402 --> 00:08:29,674
A, capital B, so likelihood transition

196
00:08:29,722 --> 00:08:32,560
dynamics and so on. And then we have

197
00:08:33,330 --> 00:08:36,018
algorithms for implementing all this in

198
00:08:36,024 --> 00:08:39,314
the algorithms module. And then the most

199
00:08:39,352 --> 00:08:41,586
powerful thing in Pymdp right now is the

200
00:08:41,608 --> 00:08:44,226
agent class where it is easy for you to

201
00:08:44,248 --> 00:08:46,434
kind of define the agent environment

202
00:08:46,482 --> 00:08:49,526
loop and we are trying

203
00:08:49,548 --> 00:08:51,894
to build up. So today I'm going to talk

204
00:08:51,932 --> 00:08:53,618
about an agent class that implements

205
00:08:53,714 --> 00:08:56,326
sophisticated inference rather than the

206
00:08:56,348 --> 00:08:58,810
classical active inference that we just

207
00:08:58,880 --> 00:09:02,778
saw. So, as I mentioned,

208
00:09:02,944 --> 00:09:05,110
how many valid policies can be defined,

209
00:09:05,190 --> 00:09:07,114
say for a time horizon of 15 in

210
00:09:07,152 --> 00:09:09,562
classical active inference? Right? So

211
00:09:09,616 --> 00:09:12,086
the first policy of course, is a series

212
00:09:12,118 --> 00:09:15,726
of first action that is jumped, then you

213
00:09:15,748 --> 00:09:18,862
can have the last action changed and you

214
00:09:18,916 --> 00:09:21,406
already can see that there can be n

215
00:09:21,428 --> 00:09:23,534
number of combinations. And for this

216
00:09:23,572 --> 00:09:26,286
simple case, the policy space is as big

217
00:09:26,308 --> 00:09:28,338
as ten to the power 13. And in a

218
00:09:28,344 --> 00:09:30,466
stochastic problem setting there is no

219
00:09:30,488 --> 00:09:33,122
way to kind of come up with a small

220
00:09:33,176 --> 00:09:35,390
subset of this policy space so that you

221
00:09:35,400 --> 00:09:37,522
can tackle this problem of computational

222
00:09:37,586 --> 00:09:40,662
complexity. And yeah, as I mentioned,

223
00:09:40,716 --> 00:09:42,646
in a stochastic problem setting it is an

224
00:09:42,668 --> 00:09:45,462
intractable size policy space. There

225
00:09:45,516 --> 00:09:47,474
comes the idea of sophisticated

226
00:09:47,522 --> 00:09:50,902
inference where we are thinking about

227
00:09:50,956 --> 00:09:52,586
taking decisions in a different way,

228
00:09:52,608 --> 00:09:54,042
right? So rather than thinking about

229
00:09:54,096 --> 00:09:56,266
sequence of actions in time, we can

230
00:09:56,288 --> 00:09:58,314
directly think of what to do when we see

231
00:09:58,352 --> 00:10:00,566
something depending upon our beliefs

232
00:10:00,598 --> 00:10:02,210
about the current state and beliefs

233
00:10:02,230 --> 00:10:04,014
about the future, right? So if I see

234
00:10:04,052 --> 00:10:05,566
myself in a current situation, what

235
00:10:05,588 --> 00:10:07,886
should I do? And that's more like a

236
00:10:07,908 --> 00:10:11,134
straightforward thinking of how to take

237
00:10:11,172 --> 00:10:13,794
actions. And here the expected free

238
00:10:13,832 --> 00:10:15,726
energy. The structure of the expected

239
00:10:15,758 --> 00:10:18,690
free energy is the same but we are not

240
00:10:18,840 --> 00:10:20,994
evaluating expected free energy of

241
00:10:21,112 --> 00:10:24,306
policies, but expected free energy of

242
00:10:24,408 --> 00:10:27,126
observation action combinations. So if I

243
00:10:27,148 --> 00:10:28,946
see something and if I do this, what's

244
00:10:28,978 --> 00:10:30,786
the expected free energy? And that's

245
00:10:30,818 --> 00:10:33,174
what I'm trying to minimize. That's what

246
00:10:33,212 --> 00:10:37,218
I'm trying to optimize

247
00:10:37,394 --> 00:10:39,554
in this setting, right? So here again,

248
00:10:39,612 --> 00:10:41,546
we have the risk term where we are

249
00:10:41,568 --> 00:10:43,690
trying to minimize the deviation between

250
00:10:43,760 --> 00:10:45,622
the belief and the prior preferences.

251
00:10:45,766 --> 00:10:49,434
We also have the ambiguity term and this

252
00:10:49,472 --> 00:10:50,954
together makes up the expected free

253
00:10:50,992 --> 00:10:54,542
energy of this time point at time T.

254
00:10:54,676 --> 00:10:57,230
But we also have an expectation about

255
00:10:57,300 --> 00:10:58,926
what's the expected free energy in the

256
00:10:58,948 --> 00:11:01,726
next time step. And to evaluate the

257
00:11:01,748 --> 00:11:03,598
expected free energy of next time step,

258
00:11:03,684 --> 00:11:05,214
you will have to again compute this

259
00:11:05,252 --> 00:11:08,306
equation with OT plus two. And for that

260
00:11:08,328 --> 00:11:09,794
you will have to again compute this

261
00:11:09,832 --> 00:11:12,514
equation with OT plus three and so on.

262
00:11:12,552 --> 00:11:14,318
And this automatically becomes a tree

263
00:11:14,334 --> 00:11:16,846
search because of the recursive way this

264
00:11:16,888 --> 00:11:19,686
equation is defined and it comes with

265
00:11:19,708 --> 00:11:21,842
its own problems, but there are clever

266
00:11:21,906 --> 00:11:24,806
ways to get around them and we are going

267
00:11:24,828 --> 00:11:27,734
to discuss that in the code today. So,

268
00:11:27,772 --> 00:11:29,654
given the structure of sophisticated

269
00:11:29,702 --> 00:11:31,962
inference here, as I mentioned, the

270
00:11:32,016 --> 00:11:34,746
research replace the policy space that

271
00:11:34,768 --> 00:11:36,998
we saw for the traditional active

272
00:11:37,014 --> 00:11:39,820
inference we are used to. So in this

273
00:11:40,690 --> 00:11:43,646
workshop, what I am focusing on is how

274
00:11:43,668 --> 00:11:46,880
to kind of define a generative model

275
00:11:47,730 --> 00:11:50,894
and given an environment. So for

276
00:11:50,932 --> 00:11:53,680
example, this is the grid that is

277
00:11:54,770 --> 00:11:58,066
simulated in the original paper and we

278
00:11:58,088 --> 00:11:59,682
are going to talk about how to build up

279
00:11:59,736 --> 00:12:01,746
a generative model for this grid that

280
00:12:01,768 --> 00:12:03,134
can be used in the sophisticated

281
00:12:03,182 --> 00:12:05,670
inference in the PMDP module.

282
00:12:07,370 --> 00:12:09,094
So basically, what I'm trying to talk

283
00:12:09,132 --> 00:12:11,526
about is that the environment will have

284
00:12:11,548 --> 00:12:14,466
a step function that takes an action

285
00:12:14,578 --> 00:12:17,574
from the agent in Pymdp, and the agent

286
00:12:17,692 --> 00:12:19,914
will get an observation out of that

287
00:12:19,952 --> 00:12:22,074
action and we'll talk about this

288
00:12:22,112 --> 00:12:24,474
particular function, agent step and

289
00:12:24,512 --> 00:12:27,274
agent steps. Step will take up an

290
00:12:27,312 --> 00:12:29,466
observation and try to come up with an

291
00:12:29,488 --> 00:12:30,894
action for the next time. Step right.

292
00:12:30,932 --> 00:12:36,234
And this creates a loop and cleverly

293
00:12:36,282 --> 00:12:38,382
designing this loop will let you see

294
00:12:38,436 --> 00:12:40,766
emergence of purposeful behavior in the

295
00:12:40,788 --> 00:12:42,766
sophisticated inference setting. So for

296
00:12:42,788 --> 00:12:45,214
example, in this particular grid with

297
00:12:45,252 --> 00:12:46,658
sufficient planning horizon, you will be

298
00:12:46,664 --> 00:12:48,594
able to see that the agent is capable of

299
00:12:48,632 --> 00:12:50,738
navigating in this grid and so on. So

300
00:12:50,824 --> 00:12:52,802
this is the example that I'm going to

301
00:12:52,936 --> 00:12:55,860
focus on in this talk today.

302
00:12:59,370 --> 00:13:00,600
Excuse me,

303
00:13:05,280 --> 00:13:07,916
I want to right away jump in to the

304
00:13:07,938 --> 00:13:12,704
code. So we have the

305
00:13:12,742 --> 00:13:15,868
Pymdp home, which I'm

306
00:13:15,884 --> 00:13:18,112
hoping you are familiar with. So we have

307
00:13:18,166 --> 00:13:21,024
this GitHub repository where we have the

308
00:13:21,062 --> 00:13:24,980
pymdp module and inside pymdp module

309
00:13:25,640 --> 00:13:29,284
we have several parts for it.

310
00:13:29,322 --> 00:13:31,236
So here we have the agent in the

311
00:13:31,258 --> 00:13:33,440
original Pymdp module that implements

312
00:13:33,600 --> 00:13:35,376
the so called classical active

313
00:13:35,408 --> 00:13:38,276
inference. We have several environments

314
00:13:38,468 --> 00:13:40,536
and we have help of functions like

315
00:13:40,638 --> 00:13:43,224
learning inference, maths and so on. So

316
00:13:43,262 --> 00:13:47,064
this is the module of Pimedp. But in the

317
00:13:47,262 --> 00:13:50,056
parent folder we also have examples

318
00:13:50,248 --> 00:13:53,996
where there is tutorials about how

319
00:13:54,018 --> 00:13:56,428
to use the agent class, how to kind of

320
00:13:56,514 --> 00:13:58,444
deal with the environments and so on.

321
00:13:58,482 --> 00:14:01,916
So if you look at the pull requests so

322
00:14:01,938 --> 00:14:04,620
we are right now trying to merge

323
00:14:04,700 --> 00:14:06,464
sophisticated inference into the

324
00:14:06,502 --> 00:14:08,988
original Pymdp module. And today I'm

325
00:14:09,004 --> 00:14:10,464
going to talk about the code in this

326
00:14:10,502 --> 00:14:13,824
pull request. So if you want to try this

327
00:14:14,022 --> 00:14:16,644
hands on, you might want to go to this

328
00:14:16,682 --> 00:14:20,196
page where this pull request is there

329
00:14:20,378 --> 00:14:24,800
and it has the same structure of Pymdp.

330
00:14:24,880 --> 00:14:27,748
It's basically designed using Pymdp.

331
00:14:27,844 --> 00:14:30,408
And here what we have additionally is an

332
00:14:30,494 --> 00:14:33,524
agent si which is a sophisticated

333
00:14:33,572 --> 00:14:35,736
inference agent which does everything

334
00:14:35,838 --> 00:14:38,168
and also planning and decision making in

335
00:14:38,174 --> 00:14:40,316
the sophisticated inference way. And in

336
00:14:40,338 --> 00:14:42,204
the parent folder there is also an

337
00:14:42,242 --> 00:14:44,744
example folder for sophisticated

338
00:14:44,792 --> 00:14:47,180
inference demo. And what I am going to

339
00:14:47,250 --> 00:14:49,340
do today is to walk you through this

340
00:14:49,410 --> 00:14:51,540
tutorial of sophisticated inference.

341
00:14:51,640 --> 00:14:55,152
And on the way, I'm going to discuss and

342
00:14:55,206 --> 00:14:59,120
at points where I reference the

343
00:14:59,270 --> 00:15:01,104
helper codes, I'm going to go to that

344
00:15:01,142 --> 00:15:02,972
code and try to explain what's actually

345
00:15:03,046 --> 00:15:05,460
happening and how we complete that agent

346
00:15:05,530 --> 00:15:08,052
environment loop where we can see that

347
00:15:08,106 --> 00:15:09,910
purposeful behavior. Right?

348
00:15:11,800 --> 00:15:14,804
Yeah. So that's the Pymdp home. Then I

349
00:15:14,842 --> 00:15:17,412
also talked about the pull request. So

350
00:15:17,466 --> 00:15:19,924
let's right away go into the jupyter

351
00:15:19,972 --> 00:15:23,464
notebook. So this is my local copy of

352
00:15:23,502 --> 00:15:25,976
this repository, so it's easier for me

353
00:15:25,998 --> 00:15:27,800
to run it and show it in my personal

354
00:15:27,870 --> 00:15:30,264
computer. So this is the parent folder

355
00:15:30,312 --> 00:15:33,228
with Pymdp and examples and inside

356
00:15:33,314 --> 00:15:35,196
examples I have a demo folder for

357
00:15:35,218 --> 00:15:36,924
sophisticated inference and this is the

358
00:15:36,962 --> 00:15:39,036
notebook I'm talking about. Right? So

359
00:15:39,058 --> 00:15:42,096
here in this example, what we are trying

360
00:15:42,118 --> 00:15:46,076
to do is deal with this particular grid

361
00:15:46,108 --> 00:15:48,028
world task from the original

362
00:15:48,044 --> 00:15:51,104
sophisticated inference paper and make

363
00:15:51,142 --> 00:15:53,664
this agent or enable this agent to

364
00:15:53,702 --> 00:15:56,256
navigate to this red dot, which is

365
00:15:56,278 --> 00:15:57,844
supposedly the goal state in this

366
00:15:57,882 --> 00:16:00,480
particular task given a prior preference

367
00:16:00,560 --> 00:16:01,776
like this, right? So this prior

368
00:16:01,808 --> 00:16:04,196
preference is quite informative in the

369
00:16:04,218 --> 00:16:06,596
sense that we right away can see that

370
00:16:06,618 --> 00:16:08,356
this is the most preferred state, the

371
00:16:08,378 --> 00:16:10,312
white color and the surrounding states

372
00:16:10,366 --> 00:16:13,192
are kind of less preferred, but more

373
00:16:13,246 --> 00:16:15,384
preferred than the faraway ones. Right?

374
00:16:15,422 --> 00:16:17,656
So this is the grid world task that we

375
00:16:17,678 --> 00:16:21,870
are trying to use. So the first cell is

376
00:16:22,240 --> 00:16:25,020
importing all the necessary libraries

377
00:16:25,440 --> 00:16:28,604
and some useful libraries like NumPy and

378
00:16:28,642 --> 00:16:31,536
Matplotlib. And the most important one

379
00:16:31,558 --> 00:16:34,992
is Pymdp. So I'm actually now calling

380
00:16:35,046 --> 00:16:37,136
the local copy of my Pymdp with the

381
00:16:37,158 --> 00:16:38,924
sophisticated inference implementation,

382
00:16:39,052 --> 00:16:41,120
not the original one, which is not

383
00:16:41,190 --> 00:16:44,016
merged yet. And the first thing I want

384
00:16:44,038 --> 00:16:45,940
to talk about is the environment itself,

385
00:16:46,010 --> 00:16:48,692
right? So the environment step part

386
00:16:48,826 --> 00:16:51,284
where if I get some action, how does the

387
00:16:51,322 --> 00:16:54,580
environment work? And for that, inside

388
00:16:54,650 --> 00:16:58,164
this folder I have a file which is Grid

389
00:16:58,212 --> 00:17:02,040
environment Si PY and

390
00:17:02,110 --> 00:17:04,104
this is basically an environment class.

391
00:17:04,222 --> 00:17:06,024
So don't worry about how this

392
00:17:06,142 --> 00:17:08,096
environment is actually implemented.

393
00:17:08,228 --> 00:17:13,148
The only thing to worry about is this

394
00:17:13,314 --> 00:17:15,644
function that we are going to use which

395
00:17:15,682 --> 00:17:18,892
is environment step. So this function

396
00:17:18,946 --> 00:17:21,372
will take an action into it and

397
00:17:21,426 --> 00:17:23,776
depending upon the current state of the

398
00:17:23,798 --> 00:17:25,696
environment, it will calculate what is

399
00:17:25,718 --> 00:17:28,784
the most probable next state given this

400
00:17:28,822 --> 00:17:30,896
action from the agent. So that's the

401
00:17:30,918 --> 00:17:33,376
idea. And then it will also calculate a

402
00:17:33,398 --> 00:17:37,280
reward of some negligible negative value

403
00:17:37,430 --> 00:17:39,888
if it is not the Gold State, and if it

404
00:17:39,894 --> 00:17:43,216
is the Gold State, it will give a reward

405
00:17:43,248 --> 00:17:44,864
of ten. And that's how the environment

406
00:17:44,912 --> 00:17:47,316
is. Designed and it will update the

407
00:17:47,338 --> 00:17:49,216
current state to the new state. And

408
00:17:49,258 --> 00:17:51,224
basically what it will return is a new

409
00:17:51,262 --> 00:17:54,296
state, depending upon your action, the

410
00:17:54,318 --> 00:17:56,696
reward for that action and whether it is

411
00:17:56,718 --> 00:17:58,524
an end of the episode and so on. So this

412
00:17:58,562 --> 00:18:01,768
implementation is the standard OpenAI

413
00:18:01,864 --> 00:18:04,396
environment implementation and this is

414
00:18:04,418 --> 00:18:05,996
the environment step function, right?

415
00:18:06,018 --> 00:18:09,484
So in this grid, for example,

416
00:18:09,682 --> 00:18:12,188
if I am right now in this state and if I

417
00:18:12,194 --> 00:18:14,304
take an action up, so I have four

418
00:18:14,342 --> 00:18:17,424
available actions north, South, east and

419
00:18:17,462 --> 00:18:19,744
west. So if I go on north, then the

420
00:18:19,782 --> 00:18:22,096
environment step will make sure that I

421
00:18:22,118 --> 00:18:25,252
am in the state that is above the state.

422
00:18:25,306 --> 00:18:27,684
And if I go east or west, then I'll stay

423
00:18:27,722 --> 00:18:29,940
here. Or south. I'll stay here. So

424
00:18:30,090 --> 00:18:34,064
that's the idea. And there is an episode

425
00:18:34,112 --> 00:18:37,044
length limit that is eight here, that

426
00:18:37,082 --> 00:18:41,524
means that I am restricting the length

427
00:18:41,572 --> 00:18:43,416
of every episode to be eight, which is

428
00:18:43,438 --> 00:18:45,156
the ideal length of reaching this goal

429
00:18:45,188 --> 00:18:47,464
state just to avoid confusion. So in

430
00:18:47,502 --> 00:18:49,456
this environment, after eight actions,

431
00:18:49,508 --> 00:18:52,140
the environment will terminate. And if

432
00:18:52,210 --> 00:18:53,816
you have to kind of reach this goal

433
00:18:53,848 --> 00:18:56,156
state in the optimal time point so

434
00:18:56,178 --> 00:18:58,428
that's the idea how the environment is

435
00:18:58,514 --> 00:19:00,910
implemented. Okay,

436
00:19:01,860 --> 00:19:04,192
I hope that is clear. And there are many

437
00:19:04,246 --> 00:19:06,508
helpful functions in this environment,

438
00:19:06,604 --> 00:19:10,252
like rendering

439
00:19:10,396 --> 00:19:12,236
the environment, rendering a prior

440
00:19:12,268 --> 00:19:13,756
preference matrix in this environment.

441
00:19:13,788 --> 00:19:15,312
So if you design a prior preference,

442
00:19:15,376 --> 00:19:18,468
this environment can show that in a

443
00:19:18,474 --> 00:19:20,224
pictorial way how your prior preference

444
00:19:20,272 --> 00:19:22,464
is that you will see in the notebook

445
00:19:22,512 --> 00:19:26,068
below. So now it's time that we define

446
00:19:26,084 --> 00:19:27,924
a generative model for the sophisticated

447
00:19:27,972 --> 00:19:32,250
inference agent, right? Before that,

448
00:19:34,620 --> 00:19:36,328
let's define the structure of the

449
00:19:36,334 --> 00:19:37,944
generative model that we want the agent

450
00:19:37,982 --> 00:19:40,856
to have in its mind, which is tailor

451
00:19:40,888 --> 00:19:42,568
made for this particular environment,

452
00:19:42,664 --> 00:19:46,216
right? So here in this particular grid

453
00:19:46,248 --> 00:19:49,630
world task, we have 25 valid states

454
00:19:50,400 --> 00:19:53,010
starting from this state. All this black

455
00:19:53,620 --> 00:19:56,352
states in this path are valid states.

456
00:19:56,406 --> 00:19:59,216
So there are 25 valid states. Then there

457
00:19:59,238 --> 00:20:00,848
are four available actions for the

458
00:20:00,854 --> 00:20:04,096
agent, north, Southeast and west. So

459
00:20:04,118 --> 00:20:05,940
this is part of our generative model.

460
00:20:06,090 --> 00:20:09,716
This is also in alignment with

461
00:20:09,738 --> 00:20:12,356
the reality of the grid. But this is

462
00:20:12,378 --> 00:20:14,212
about what the agent has in its mind,

463
00:20:14,266 --> 00:20:17,736
right? And then the observation is just

464
00:20:17,758 --> 00:20:19,210
the state space.

465
00:20:20,780 --> 00:20:22,776
The problem is fully observable, so

466
00:20:22,798 --> 00:20:26,344
there is no ambiguity there. Then we

467
00:20:26,382 --> 00:20:28,330
define basically the number of states.

468
00:20:34,170 --> 00:20:36,646
Then we define basically the number of

469
00:20:36,668 --> 00:20:39,622
states, which is a list of your state

470
00:20:39,676 --> 00:20:42,614
space, number of factors, which is now

471
00:20:42,652 --> 00:20:44,386
one because you only have one hidden

472
00:20:44,418 --> 00:20:46,506
state factor here. Then number of

473
00:20:46,528 --> 00:20:48,620
controls, which is going to be four.

474
00:20:49,150 --> 00:20:51,802
That's four available actions and your

475
00:20:51,856 --> 00:20:54,186
observation space like that. So this is

476
00:20:54,208 --> 00:20:56,062
the structure of your generative model.

477
00:20:56,196 --> 00:20:59,854
And let's look at the structure of the

478
00:20:59,892 --> 00:21:02,846
parameters now inside a POMDP. So the

479
00:21:02,868 --> 00:21:05,280
first one is the likelihood function

480
00:21:07,090 --> 00:21:10,846
which is often denoted by capital A.

481
00:21:10,948 --> 00:21:13,074
And here it is a function of how many

482
00:21:13,112 --> 00:21:15,106
observational modalities I have and how

483
00:21:15,128 --> 00:21:17,506
many state modalities I have. Right? If

484
00:21:17,528 --> 00:21:20,900
I run this cell, yeah, I have to run the

485
00:21:21,270 --> 00:21:24,150
parent cell to make sure everything

486
00:21:24,220 --> 00:21:26,646
works. So I have rendered the

487
00:21:26,668 --> 00:21:29,398
environment the structure of the

488
00:21:29,404 --> 00:21:31,830
generative model. And here I have the

489
00:21:31,980 --> 00:21:34,754
capital A matrix which has a structure

490
00:21:34,802 --> 00:21:37,370
25 25. That means that I have 25 states

491
00:21:37,440 --> 00:21:39,914
and 25 observations. And here, because

492
00:21:39,952 --> 00:21:42,486
it's fully observable, I am initializing

493
00:21:42,518 --> 00:21:45,790
it as an identity matrix of size 25.

494
00:21:45,860 --> 00:21:49,610
So that's my likelihood matrix

495
00:21:49,690 --> 00:21:52,222
that I'm initializing for this

496
00:21:52,276 --> 00:21:55,086
particular grid world task. Then the

497
00:21:55,108 --> 00:21:57,630
second element is the transition matrix.

498
00:21:58,130 --> 00:22:00,686
So please note that I'm using all the

499
00:22:00,708 --> 00:22:03,966
existing Pymdp functionalities to define

500
00:22:03,998 --> 00:22:06,786
a random A matrix and then using an

501
00:22:06,808 --> 00:22:09,650
identity matrix on top of that.

502
00:22:09,800 --> 00:22:12,914
So, yeah, I'm not doing anything new.

503
00:22:12,952 --> 00:22:15,818
Here it is the existing Pymdp

504
00:22:15,854 --> 00:22:18,774
functionality. Then what I can do now is

505
00:22:18,892 --> 00:22:21,334
define the B matrix, which is also

506
00:22:21,372 --> 00:22:23,286
called transition matrix. So the

507
00:22:23,308 --> 00:22:25,794
transition matrix encodes transitions

508
00:22:25,842 --> 00:22:27,786
like where I'm going to end up in the

509
00:22:27,808 --> 00:22:29,578
future if I start from a particular

510
00:22:29,664 --> 00:22:32,186
state and take an action. So that's the

511
00:22:32,208 --> 00:22:35,146
idea where it depends upon the number of

512
00:22:35,168 --> 00:22:36,474
states, which is the hidden state

513
00:22:36,512 --> 00:22:39,386
modality and number of controls. So it

514
00:22:39,408 --> 00:22:42,350
has the structure of state action state

515
00:22:42,500 --> 00:22:44,126
where if I take an action from a

516
00:22:44,148 --> 00:22:45,614
particular state where I'm going to end

517
00:22:45,652 --> 00:22:47,358
up. So that is also a future state,

518
00:22:47,444 --> 00:22:51,778
right. So I'm going to initialize it as

519
00:22:51,944 --> 00:22:54,706
the true environment state. So now this

520
00:22:54,728 --> 00:22:56,206
is part of the environment that I've

521
00:22:56,238 --> 00:22:58,530
built. It will give out the B matrix.

522
00:22:59,190 --> 00:23:00,918
It might be worth it to look at the

523
00:23:00,924 --> 00:23:02,950
structure of this B matrix.

524
00:23:05,370 --> 00:23:08,854
So here we have 25, 25 four. So that

525
00:23:08,892 --> 00:23:12,646
means that if I take an action from a

526
00:23:12,668 --> 00:23:14,166
particular state where I'm going to

527
00:23:14,188 --> 00:23:18,054
enter and we have the true transition

528
00:23:18,102 --> 00:23:20,074
dynamics for this particular grid by

529
00:23:20,112 --> 00:23:22,474
design. So there is a function called

530
00:23:22,512 --> 00:23:25,066
get true B and that will give us the

531
00:23:25,088 --> 00:23:27,534
true b of the system which the agent can

532
00:23:27,572 --> 00:23:30,654
use. Okay? So ideally we would want

533
00:23:30,692 --> 00:23:32,382
the agent to learn this, but for the

534
00:23:32,436 --> 00:23:35,178
purpose of this demo we are assuming

535
00:23:35,194 --> 00:23:36,446
that the agent already knows the

536
00:23:36,468 --> 00:23:39,838
structure. And then comes the

537
00:23:39,844 --> 00:23:42,818
prior preference, which is interesting

538
00:23:42,904 --> 00:23:46,466
here in the sense that it is defined as

539
00:23:46,648 --> 00:23:48,978
how closer you are to the gold state.

540
00:23:49,064 --> 00:23:51,106
So if you are at the gold state, then

541
00:23:51,288 --> 00:23:53,814
clearly that is the most sought out

542
00:23:53,852 --> 00:23:58,006
state. You prefer that the most. And how

543
00:23:58,028 --> 00:23:59,382
do you prefer the neighboring states,

544
00:23:59,436 --> 00:24:01,254
right? So that is dependent upon the

545
00:24:01,292 --> 00:24:04,182
square root of the distance or basically

546
00:24:04,236 --> 00:24:06,958
the distance from that particular gold

547
00:24:06,994 --> 00:24:11,626
state. So you define a grid which

548
00:24:11,648 --> 00:24:14,154
is eight cross eight, the same size as

549
00:24:14,192 --> 00:24:17,054
this particular grid world task. And

550
00:24:17,172 --> 00:24:21,022
then we have a method to

551
00:24:21,076 --> 00:24:23,374
kind of add values which is the

552
00:24:23,412 --> 00:24:25,230
preference you have for every state.

553
00:24:25,380 --> 00:24:28,094
And if you render the particular C

554
00:24:28,132 --> 00:24:29,746
matrix you can see the structure which

555
00:24:29,768 --> 00:24:32,946
is the same where this gold state is

556
00:24:32,968 --> 00:24:35,006
more preferred and the surrounding

557
00:24:35,038 --> 00:24:38,114
states less preferred and so on. So now

558
00:24:38,152 --> 00:24:40,738
we have the C matrix also defined in the

559
00:24:40,744 --> 00:24:44,230
classic PMDP way. And then

560
00:24:44,380 --> 00:24:46,966
I initialize that C matrix as the C

561
00:24:46,988 --> 00:24:49,702
matrix we evaluated in the previous cell

562
00:24:49,756 --> 00:24:52,246
which is small C, this particular C

563
00:24:52,268 --> 00:24:55,330
matrix. Okay? And then lastly,

564
00:24:55,490 --> 00:24:57,394
for the generative model we have capital

565
00:24:57,442 --> 00:24:59,618
D which is your prior overhead and

566
00:24:59,644 --> 00:25:01,910
states. And for that I'm using a uniform

567
00:25:01,990 --> 00:25:04,726
object array. So that means that I don't

568
00:25:04,758 --> 00:25:07,086
have a prior of where I am starting. So

569
00:25:07,108 --> 00:25:10,350
let me run the pending cells.

570
00:25:12,530 --> 00:25:14,874
So here the D matrix is a uniform

571
00:25:14,922 --> 00:25:16,938
distribution over hidden states. I don't

572
00:25:16,954 --> 00:25:18,218
know where I'm going to start the

573
00:25:18,244 --> 00:25:20,466
simulations and so on. So this is the

574
00:25:20,648 --> 00:25:22,898
basic structure of the generative model.

575
00:25:23,064 --> 00:25:26,546
And then we have the agent class which I

576
00:25:26,568 --> 00:25:29,474
want to discuss separately, like the

577
00:25:29,512 --> 00:25:32,120
environment, right? So given these

578
00:25:32,650 --> 00:25:34,694
environment parameters, how would you

579
00:25:34,732 --> 00:25:37,414
expect the agent class to work? So where

580
00:25:37,452 --> 00:25:39,986
is the agent class? Inside this folder

581
00:25:40,018 --> 00:25:43,954
structure? Inside the Pymdp

582
00:25:44,002 --> 00:25:47,130
module folder we have an agent Si PY

583
00:25:48,350 --> 00:25:52,698
which is basically a class again and

584
00:25:52,784 --> 00:25:55,274
similar to the environment class here.

585
00:25:55,312 --> 00:25:58,254
Also we have a step function where this

586
00:25:58,292 --> 00:26:01,662
will take an observation to

587
00:26:01,716 --> 00:26:05,326
the function and also a flag whether or

588
00:26:05,348 --> 00:26:08,094
not to learn the environment, which is

589
00:26:08,132 --> 00:26:11,086
optional. So if you disable it, it won't

590
00:26:11,118 --> 00:26:13,394
learn the generative model. If you

591
00:26:13,432 --> 00:26:15,614
enable it, it will update the parameters

592
00:26:15,662 --> 00:26:17,554
of the generative model. And what

593
00:26:17,592 --> 00:26:19,394
basically it does is it will return an

594
00:26:19,432 --> 00:26:22,134
action which is to be taken at this

595
00:26:22,172 --> 00:26:24,646
point of time and the environment can

596
00:26:24,668 --> 00:26:27,126
basically use that action, right? So in

597
00:26:27,148 --> 00:26:28,886
this file we have the agent class which

598
00:26:28,908 --> 00:26:30,520
I will explain in detail.

599
00:26:31,850 --> 00:26:34,650
So I am basically importing that agent

600
00:26:34,720 --> 00:26:38,474
class in this cell and

601
00:26:38,512 --> 00:26:41,046
then we are going to try and reproduce

602
00:26:41,078 --> 00:26:43,798
this behavioral result from the original

603
00:26:43,814 --> 00:26:46,560
sophisticated inference paper. Okay?

604
00:26:47,010 --> 00:26:50,766
And for that. So what we expect is

605
00:26:50,788 --> 00:26:53,194
that given this prior preference

606
00:26:53,242 --> 00:26:55,838
structure, there are local maximas in

607
00:26:55,844 --> 00:26:57,358
this prior preference structure. So if

608
00:26:57,364 --> 00:26:59,310
you start from this particular point,

609
00:26:59,460 --> 00:27:02,306
if you do not plan deep enough, what you

610
00:27:02,328 --> 00:27:04,882
will end up is in one of these local

611
00:27:04,936 --> 00:27:07,634
maximas where you don't see that there

612
00:27:07,672 --> 00:27:09,842
is a highly preferred observation, say

613
00:27:09,896 --> 00:27:12,306
four steps down the line. So if you are

614
00:27:12,328 --> 00:27:14,886
in this particular state, what you will

615
00:27:14,908 --> 00:27:17,366
see is this local maxima and you will go

616
00:27:17,388 --> 00:27:18,946
and sit there because the neighboring

617
00:27:18,978 --> 00:27:21,286
states are less preferred and this state

618
00:27:21,388 --> 00:27:22,646
which is more preferred is not

619
00:27:22,668 --> 00:27:25,446
accessible because of the wall or the

620
00:27:25,468 --> 00:27:27,706
wall structure. So you have to take a

621
00:27:27,728 --> 00:27:29,798
turn and pass through less preferred

622
00:27:29,814 --> 00:27:32,298
states and you need deep planning in

623
00:27:32,304 --> 00:27:34,458
order to enable the agent to do that.

624
00:27:34,544 --> 00:27:36,126
The agent should be able to kind of

625
00:27:36,148 --> 00:27:39,822
simulate four time steps ahead in time

626
00:27:39,876 --> 00:27:42,266
to see that there is this highly

627
00:27:42,298 --> 00:27:44,494
rewarding observation coming to kind of

628
00:27:44,532 --> 00:27:47,934
do that actions. So that's the point

629
00:27:47,972 --> 00:27:50,866
that we are trying to see in this

630
00:27:50,888 --> 00:27:54,990
particular demo. So for a low planning

631
00:27:55,070 --> 00:27:57,586
depth it will basically get stuck in one

632
00:27:57,608 --> 00:27:59,934
of the local maximas but with sufficient

633
00:27:59,982 --> 00:28:02,374
planning depth it will navigate to the

634
00:28:02,412 --> 00:28:03,894
gold state. So that's what we are trying

635
00:28:03,932 --> 00:28:06,230
to see, right? So we have different

636
00:28:06,300 --> 00:28:09,846
planning horizons and what we are

637
00:28:09,868 --> 00:28:12,614
basically doing is give the agent a

638
00:28:12,652 --> 00:28:15,594
generative model which we right now

639
00:28:15,632 --> 00:28:18,214
defined the A matrix, B matrix, C matrix

640
00:28:18,262 --> 00:28:20,246
B matrix. Then we have the planning

641
00:28:20,278 --> 00:28:23,290
horizon of capital N. So here I am,

642
00:28:23,440 --> 00:28:26,026
iterating over planning depth. So N will

643
00:28:26,048 --> 00:28:28,410
be one, three and four for the loop.

644
00:28:28,570 --> 00:28:30,606
Then we have action precision, which is

645
00:28:30,628 --> 00:28:32,906
often denoted by alpha in active

646
00:28:32,938 --> 00:28:35,242
inference literature. So that determines

647
00:28:35,306 --> 00:28:37,914
which action is to be taken. So a highly

648
00:28:37,962 --> 00:28:40,994
precise action precision means that it

649
00:28:41,032 --> 00:28:43,330
will stick to the action with the lowest

650
00:28:43,910 --> 00:28:47,246
expected free energy. But a lower action

651
00:28:47,278 --> 00:28:49,074
precision is kind of probabilistic where

652
00:28:49,112 --> 00:28:51,374
it will also consider other actions.

653
00:28:51,502 --> 00:28:53,266
Then we have a planning precision which

654
00:28:53,288 --> 00:28:55,266
is part of the planning function we'll

655
00:28:55,298 --> 00:28:57,478
discuss, which is often denoted in the

656
00:28:57,484 --> 00:28:59,766
literature as gamma. Then we also have a

657
00:28:59,788 --> 00:29:01,426
search threshold which is extremely

658
00:29:01,458 --> 00:29:03,314
important for sophisticated inference

659
00:29:03,442 --> 00:29:05,974
because as we saw, sophisticated

660
00:29:06,022 --> 00:29:09,846
inference is a tree search and tree

661
00:29:09,878 --> 00:29:12,314
search is bad in the sense that it can

662
00:29:12,352 --> 00:29:14,666
have a lot of computations. But you have

663
00:29:14,688 --> 00:29:16,986
to define a threshold to kind of ignore

664
00:29:17,018 --> 00:29:19,566
many possibilities to make it work. And

665
00:29:19,588 --> 00:29:21,230
that's the idea that we will also

666
00:29:21,300 --> 00:29:24,894
discuss. So just a preview before we

667
00:29:24,932 --> 00:29:28,510
go to the agent class.

668
00:29:28,660 --> 00:29:31,954
What we are trying to do is in a loop we

669
00:29:31,992 --> 00:29:35,522
are going to call the agent step and

670
00:29:35,576 --> 00:29:38,242
environment step in series. So the agent

671
00:29:38,296 --> 00:29:40,546
will see an observation, it will take an

672
00:29:40,568 --> 00:29:43,334
action, that action will go into the

673
00:29:43,372 --> 00:29:45,334
environment, the environment will give

674
00:29:45,372 --> 00:29:47,634
it back new observations and this loop

675
00:29:47,682 --> 00:29:50,086
continues. And we want to see over time

676
00:29:50,188 --> 00:29:53,846
how this loop evolves into a purposeful

677
00:29:53,878 --> 00:29:56,906
behavior and if the agent at all is

678
00:29:56,928 --> 00:30:00,186
capable for that. So before I reveal the

679
00:30:00,208 --> 00:30:03,498
results, let's discuss the agent class.

680
00:30:03,664 --> 00:30:07,086
So in order to give an action, when an

681
00:30:07,108 --> 00:30:09,934
observation is given, the agent should

682
00:30:09,972 --> 00:30:12,862
have the planning and so on, right? So

683
00:30:12,916 --> 00:30:15,514
here is the agent, the sophisticated

684
00:30:15,562 --> 00:30:19,266
inference agent, where we are

685
00:30:19,288 --> 00:30:22,514
actually using the existing Pymdp agent

686
00:30:22,632 --> 00:30:25,826
for some functionalities. So in Pymdp we

687
00:30:25,848 --> 00:30:28,814
already have really well written

688
00:30:28,862 --> 00:30:33,222
functions for perception and

689
00:30:33,276 --> 00:30:35,542
learning. So the only thing we want to

690
00:30:35,596 --> 00:30:37,654
kind of replace is how the agent is

691
00:30:37,692 --> 00:30:39,494
doing planning and how the agent is

692
00:30:39,532 --> 00:30:42,690
taking decisions over policies, right?

693
00:30:42,860 --> 00:30:47,466
So here we

694
00:30:47,488 --> 00:30:49,930
are using that parent agent class.

695
00:30:50,080 --> 00:30:53,418
So from Pymdp agent we

696
00:30:53,424 --> 00:30:56,846
are importing that agent class which is

697
00:30:56,948 --> 00:30:59,886
sitting next to the Si agent that we are

698
00:30:59,908 --> 00:31:03,870
discussing now. And basically

699
00:31:03,940 --> 00:31:07,262
we are taking in the generative model

700
00:31:07,316 --> 00:31:09,694
structure from the main program for this

701
00:31:09,732 --> 00:31:12,994
class to work which is C and D and all

702
00:31:13,032 --> 00:31:15,346
the precisions and threshold parameter I

703
00:31:15,368 --> 00:31:18,500
mentioned, then it is kind of

704
00:31:19,030 --> 00:31:21,654
normalizing the prior preference that we

705
00:31:21,692 --> 00:31:24,760
mentioned in the main program. So here

706
00:31:25,610 --> 00:31:28,520
if I look at how C is,

707
00:31:30,490 --> 00:31:32,454
the structure of C is defined in terms

708
00:31:32,492 --> 00:31:35,226
of numbers and the prior preference is

709
00:31:35,248 --> 00:31:36,746
often interpreted or it should be a

710
00:31:36,768 --> 00:31:38,538
probabilistic distribution for the

711
00:31:38,544 --> 00:31:41,226
computations to work, right? So we are

712
00:31:41,248 --> 00:31:43,222
going to normalize it as a probabilistic

713
00:31:43,286 --> 00:31:47,894
distribution rather than having

714
00:31:47,952 --> 00:31:50,286
numbers that don't add up to one. So

715
00:31:50,308 --> 00:31:51,806
that's what's happening here. We are

716
00:31:51,828 --> 00:31:54,606
using Softmax to do that. Then what we

717
00:31:54,628 --> 00:31:57,262
are doing is we are initializing the

718
00:31:57,316 --> 00:32:00,082
existing PMDP agent with these

719
00:32:00,216 --> 00:32:02,946
generative model parameters and what we

720
00:32:02,968 --> 00:32:06,706
are intending to do is write a

721
00:32:06,728 --> 00:32:08,286
planning function for a given planning

722
00:32:08,318 --> 00:32:12,378
horizon and a given threshold for trees.

723
00:32:12,574 --> 00:32:15,766
Okay, so there are three functions in

724
00:32:15,788 --> 00:32:18,706
this agent class. One is a helper

725
00:32:18,738 --> 00:32:20,246
function for planning which we will

726
00:32:20,268 --> 00:32:22,770
discuss now. Then there is a planning

727
00:32:22,850 --> 00:32:25,674
function itself which is going to do

728
00:32:25,712 --> 00:32:28,346
planning using tree search. And then

729
00:32:28,448 --> 00:32:30,362
because it is a recursive tree search,

730
00:32:30,496 --> 00:32:32,070
we are going to need an additional

731
00:32:32,150 --> 00:32:34,998
function that implements that recursive

732
00:32:35,174 --> 00:32:38,462
evaluation where we are going to call

733
00:32:38,516 --> 00:32:40,640
this function called forward search

734
00:32:41,250 --> 00:32:44,014
inside the function itself. So we are

735
00:32:44,052 --> 00:32:45,614
calling this function inside this

736
00:32:45,652 --> 00:32:47,626
function. So that is to calculate

737
00:32:47,658 --> 00:32:49,154
expected free energy for the next step

738
00:32:49,192 --> 00:32:51,154
and it will call it again for the next

739
00:32:51,192 --> 00:32:53,826
step till our planning horizon. So

740
00:32:53,848 --> 00:32:57,778
that's the idea of recursive looping and

741
00:32:57,944 --> 00:33:00,466
finally it will return the expected free

742
00:33:00,488 --> 00:33:03,266
energy for all actions given an

743
00:33:03,288 --> 00:33:05,522
observations and then we just implement

744
00:33:05,586 --> 00:33:07,874
the step function where it is written

745
00:33:07,922 --> 00:33:09,558
sequentially what to do given an

746
00:33:09,564 --> 00:33:10,390
observation.

747
00:33:12,330 --> 00:33:15,580
So going back to the demo

748
00:33:18,190 --> 00:33:21,386
here we have this first

749
00:33:21,568 --> 00:33:25,386
idea where you get an observation and

750
00:33:25,488 --> 00:33:28,126
it gives out an action. So let's go to

751
00:33:28,148 --> 00:33:31,182
the agent step function and imagine what

752
00:33:31,236 --> 00:33:34,846
happens. So if it is time t equal to

753
00:33:34,868 --> 00:33:37,694
zero or in the beginning of the

754
00:33:37,732 --> 00:33:39,934
experiment, what it is ideally.

755
00:33:39,982 --> 00:33:41,570
Supposed to do in the first place is

756
00:33:41,640 --> 00:33:44,526
infer that state using its observation.

757
00:33:44,638 --> 00:33:47,106
So what we are giving it is an

758
00:33:47,128 --> 00:33:49,986
observation and using the modules for

759
00:33:50,008 --> 00:33:52,886
inference, it's going to come up with a

760
00:33:52,908 --> 00:33:56,406
belief QS, which is a

761
00:33:56,428 --> 00:33:59,026
belief about states. Okay, so self dot

762
00:33:59,058 --> 00:34:02,934
QS is the belief inside the agent and

763
00:34:02,972 --> 00:34:05,354
once it has a belief about where it is

764
00:34:05,392 --> 00:34:07,930
right now, it can implement plan

765
00:34:08,080 --> 00:34:11,034
research which is do planning for this

766
00:34:11,072 --> 00:34:13,994
particular belief of hidden state right

767
00:34:14,032 --> 00:34:16,606
now. And once it has done planning, it

768
00:34:16,628 --> 00:34:19,082
can take decision using the sample

769
00:34:19,146 --> 00:34:22,320
action function in Pymdp and basically

770
00:34:23,090 --> 00:34:26,734
return that action. And for every other

771
00:34:26,772 --> 00:34:28,866
time steps, the sequence remains the

772
00:34:28,888 --> 00:34:31,266
same. But it is also learning about the

773
00:34:31,288 --> 00:34:34,674
structure if you enable learning in your

774
00:34:34,712 --> 00:34:38,210
agent class. So that's the step

775
00:34:38,280 --> 00:34:42,642
function. But in order to do planning,

776
00:34:42,786 --> 00:34:45,586
what it does is it kind of reorganizes

777
00:34:45,618 --> 00:34:47,846
the generative model structure for any

778
00:34:47,868 --> 00:34:49,926
number of hidden state modalities and

779
00:34:49,948 --> 00:34:51,986
any number of observation modalities.

780
00:34:52,178 --> 00:34:55,418
So to discuss how melting works,

781
00:34:55,584 --> 00:34:58,746
I would like to talk about the new A

782
00:34:58,768 --> 00:35:01,738
matrices and B matrices it evaluates for

783
00:35:01,824 --> 00:35:05,442
implementing that planning and let's

784
00:35:05,526 --> 00:35:09,838
understand that. So let's go back to the

785
00:35:10,004 --> 00:35:13,854
original hidden state factors. So here

786
00:35:13,892 --> 00:35:16,266
we only have one hidden state factor.

787
00:35:16,458 --> 00:35:19,226
So that's why it is B zero, right? And B

788
00:35:19,268 --> 00:35:22,594
one does not exist because we only have

789
00:35:22,632 --> 00:35:25,054
one hidden state factor. But imagine

790
00:35:25,102 --> 00:35:27,330
that if I have two hidden state factors

791
00:35:28,230 --> 00:35:31,894
with the same size maybe. So here it

792
00:35:31,932 --> 00:35:34,630
could be a location and maybe something

793
00:35:34,700 --> 00:35:37,320
else inside the agent's mind.

794
00:35:37,850 --> 00:35:40,614
And we should also have controls for

795
00:35:40,652 --> 00:35:42,742
these two hidden state factors. Just

796
00:35:42,796 --> 00:35:46,442
like so

797
00:35:46,496 --> 00:35:48,854
there should be control for every hidden

798
00:35:48,902 --> 00:35:51,274
state factor if you're familiar with

799
00:35:51,392 --> 00:35:55,638
active inference idea and

800
00:35:55,744 --> 00:35:58,080
then maybe the observation space is also

801
00:35:58,450 --> 00:36:00,234
directly observing these two hidden

802
00:36:00,282 --> 00:36:03,470
state factors. Okay, so this is a new

803
00:36:03,540 --> 00:36:05,306
Generative Model structure with multiple

804
00:36:05,338 --> 00:36:08,006
hidden states and multiple observation

805
00:36:08,058 --> 00:36:10,594
modalities. And right away you can see

806
00:36:10,632 --> 00:36:12,994
that the dimensionalities of your

807
00:36:13,032 --> 00:36:15,954
parameters change. You have 25

808
00:36:15,992 --> 00:36:20,194
observations coming from 25

809
00:36:20,312 --> 00:36:23,206
times 25 hidden states. And if you look

810
00:36:23,228 --> 00:36:24,470
at the structure of the first

811
00:36:24,540 --> 00:36:26,550
observation modality, it's the same.

812
00:36:26,700 --> 00:36:29,414
But what we want is a new matrix where

813
00:36:29,452 --> 00:36:33,594
it's going to be 25 with not

814
00:36:33,632 --> 00:36:35,366
two hidden states but just one hidden

815
00:36:35,398 --> 00:36:38,874
state. It's only a reorganization of the

816
00:36:38,912 --> 00:36:42,150
generative model, but computations

817
00:36:42,230 --> 00:36:44,060
essentially remains the same.

818
00:36:44,910 --> 00:36:48,426
So that's what this helper

819
00:36:48,458 --> 00:36:51,006
function is trying to do, which will

820
00:36:51,028 --> 00:36:53,134
make things easier for us when we have

821
00:36:53,172 --> 00:36:58,494
multiple hidden state modalities. So if

822
00:36:58,532 --> 00:36:59,786
you have multiple hidden state

823
00:36:59,828 --> 00:37:02,514
modalities, we are going to compute how

824
00:37:02,552 --> 00:37:05,202
many total states you have, which is the

825
00:37:05,336 --> 00:37:07,486
multiplication of number of hidden

826
00:37:07,518 --> 00:37:09,586
states in each modality. Okay? So if you

827
00:37:09,608 --> 00:37:12,126
have 25 hidden states in one modality

828
00:37:12,158 --> 00:37:13,894
and 25 hidden states in the other

829
00:37:13,932 --> 00:37:17,142
modality, you're going to have 625 total

830
00:37:17,196 --> 00:37:19,894
number of states. And if you have four

831
00:37:19,932 --> 00:37:22,646
actions each in each modalities, then

832
00:37:22,668 --> 00:37:25,542
you have total of 16 actions which is

833
00:37:25,596 --> 00:37:27,430
nothing but the combination of these

834
00:37:27,500 --> 00:37:30,622
four actions each in the modalities.

835
00:37:30,706 --> 00:37:33,254
So you're going to have four times 416

836
00:37:33,302 --> 00:37:35,446
actions if you have two modalities. And

837
00:37:35,488 --> 00:37:38,798
it's basically going to build a

838
00:37:38,804 --> 00:37:43,662
generative model that has the same model

839
00:37:43,716 --> 00:37:45,710
parameters but just with a different

840
00:37:45,780 --> 00:37:47,958
dimension structure so that it's easier

841
00:37:47,994 --> 00:37:50,674
for us to calculate the expected. So now

842
00:37:50,712 --> 00:37:54,866
we have a new A and new B and

843
00:37:54,888 --> 00:37:58,366
a new belief which is nothing but tensor

844
00:37:58,398 --> 00:38:00,966
products of existing parameters and

845
00:38:00,988 --> 00:38:03,910
beliefs. It's nothing but a new big

846
00:38:04,060 --> 00:38:07,160
matrix and nothing else. Okay?

847
00:38:08,250 --> 00:38:10,006
It's not a change, it's just a

848
00:38:10,028 --> 00:38:12,790
transformation of structure. And given

849
00:38:12,860 --> 00:38:16,026
this A, B and Q, we are going

850
00:38:16,048 --> 00:38:18,266
to predict what's going to happen in the

851
00:38:18,288 --> 00:38:21,306
future and evaluate the expected free

852
00:38:21,328 --> 00:38:24,794
energies for them. So in order to do

853
00:38:24,912 --> 00:38:26,986
planning, so that's the second function.

854
00:38:27,088 --> 00:38:28,878
What we are going to do is first call

855
00:38:28,964 --> 00:38:31,118
the first function which will do the

856
00:38:31,124 --> 00:38:33,194
melting for us and set up the generative

857
00:38:33,242 --> 00:38:37,214
model in good dimensions, easy to

858
00:38:37,252 --> 00:38:39,554
compute. Then we have the expected free

859
00:38:39,592 --> 00:38:42,914
energy itself for all the actions and

860
00:38:42,952 --> 00:38:45,860
then we have the probability that

861
00:38:47,350 --> 00:38:49,202
depends upon this expected free energy

862
00:38:49,256 --> 00:38:51,366
for these actions. So why is it just the

863
00:38:51,388 --> 00:38:53,122
actions and not the observations?

864
00:38:53,266 --> 00:38:55,890
Because here we are going to evaluate

865
00:38:55,970 --> 00:39:00,086
expected free energy of actions for

866
00:39:00,108 --> 00:39:02,680
the given observations, right? So let me

867
00:39:03,550 --> 00:39:06,742
go back to the slides

868
00:39:06,806 --> 00:39:10,266
and discuss this pictorially to

869
00:39:10,288 --> 00:39:12,426
make things easier. So here we have the

870
00:39:12,448 --> 00:39:14,870
grid and we have the prior footprints.

871
00:39:14,950 --> 00:39:16,766
And what we are trying to implement is

872
00:39:16,788 --> 00:39:19,934
that if you observe some observation at

873
00:39:19,972 --> 00:39:22,078
time T, then you're going to consider

874
00:39:22,164 --> 00:39:25,662
the consequences of your actions given

875
00:39:25,716 --> 00:39:28,494
that observation, because you can

876
00:39:28,612 --> 00:39:30,094
predict what's going to happen. Because

877
00:39:30,132 --> 00:39:31,586
in your generative model you have the

878
00:39:31,608 --> 00:39:34,098
transition dynamics that will tell you,

879
00:39:34,184 --> 00:39:37,086
given this state, if I take this action,

880
00:39:37,118 --> 00:39:38,386
where I'm going to end up, right? So

881
00:39:38,408 --> 00:39:40,478
that's basically predicting what's going

882
00:39:40,504 --> 00:39:43,366
to happen in the future and you're right

883
00:39:43,388 --> 00:39:45,480
now considering the consequence of

884
00:39:46,570 --> 00:39:49,622
available actions in your arsenal. And

885
00:39:49,676 --> 00:39:52,326
then if you take an action, then you can

886
00:39:52,348 --> 00:39:54,166
predict what's going to happen in the

887
00:39:54,268 --> 00:39:56,438
next time step as a new observation,

888
00:39:56,534 --> 00:39:58,838
right? So you have a probability

889
00:39:58,934 --> 00:40:00,602
distribution that tells you that say

890
00:40:00,656 --> 00:40:03,754
this observation is the most likely and

891
00:40:03,872 --> 00:40:05,482
the other observations are not really

892
00:40:05,536 --> 00:40:08,286
likely. Then what you will do is you do

893
00:40:08,308 --> 00:40:10,666
this again, you consider the consequence

894
00:40:10,698 --> 00:40:12,302
of doing your actions from that

895
00:40:12,356 --> 00:40:15,214
particular observation and this goes on

896
00:40:15,252 --> 00:40:17,534
in your planning depth, right? So this

897
00:40:17,572 --> 00:40:19,758
can be thought of as maybe you want to

898
00:40:19,764 --> 00:40:22,020
go to the gym. Then you are going to

899
00:40:22,390 --> 00:40:24,610
consider all the consequences. What will

900
00:40:24,680 --> 00:40:26,766
happen if I wear my shoes, if I don't

901
00:40:26,798 --> 00:40:30,018
wear my shoes, if I go in my car, if I

902
00:40:30,024 --> 00:40:31,934
don't go in my car, then you realize

903
00:40:31,982 --> 00:40:33,774
that, okay, I have to wear my shoes.

904
00:40:33,902 --> 00:40:36,040
Then you consider the consequence that

905
00:40:36,650 --> 00:40:39,414
I'm now ready to go to the gym and me

906
00:40:39,452 --> 00:40:41,446
going to the gym will end up me being in

907
00:40:41,468 --> 00:40:43,686
the gym. So that's the idea. You

908
00:40:43,788 --> 00:40:45,574
consider the consequence of where you

909
00:40:45,612 --> 00:40:48,586
are right now and you can go as much as

910
00:40:48,608 --> 00:40:50,458
you want, right. You can predict. So in

911
00:40:50,464 --> 00:40:52,266
a game of chess, you might be in a

912
00:40:52,288 --> 00:40:54,074
particular state. You consider your

913
00:40:54,112 --> 00:40:56,254
consequences, you see the future, you

914
00:40:56,292 --> 00:40:58,462
consider consequences from that future,

915
00:40:58,516 --> 00:41:00,960
and you can go as deep as you want

916
00:41:03,650 --> 00:41:05,818
depending upon your computational

917
00:41:05,914 --> 00:41:07,706
abilities. Right. So that's what you're

918
00:41:07,738 --> 00:41:11,460
trying to implement in this agent class

919
00:41:11,910 --> 00:41:14,690
where we are considering consequences?

920
00:41:17,430 --> 00:41:21,410
Yeah. So for every modality

921
00:41:21,490 --> 00:41:23,506
we are going to consider the expected

922
00:41:23,538 --> 00:41:27,126
free energy for actions. And this

923
00:41:27,148 --> 00:41:29,510
will basically call the next function,

924
00:41:29,580 --> 00:41:31,670
which is forward search. So forward

925
00:41:31,740 --> 00:41:33,766
search is implementing the thing that I

926
00:41:33,788 --> 00:41:36,054
just mentioned, considering

927
00:41:36,102 --> 00:41:38,922
consequences. And in forward search,

928
00:41:38,976 --> 00:41:41,002
what you are basically doing is for

929
00:41:41,056 --> 00:41:43,994
every action. So in line 149,

930
00:41:44,032 --> 00:41:46,014
I have a loop that goes over every

931
00:41:46,052 --> 00:41:48,446
action. Then I'm going to consider the

932
00:41:48,468 --> 00:41:51,374
posterior or the consequences of all

933
00:41:51,412 --> 00:41:54,234
those actions. I use my transition

934
00:41:54,282 --> 00:41:56,414
probabilities to evaluate that

935
00:41:56,452 --> 00:41:58,926
consequences. Then I'm going to predict

936
00:41:58,958 --> 00:42:00,846
the observations, because my prior

937
00:42:00,878 --> 00:42:03,346
preferences are defined in terms of

938
00:42:03,368 --> 00:42:05,794
observations. I'm going to predict my

939
00:42:05,832 --> 00:42:08,066
observations and then evaluate the

940
00:42:08,088 --> 00:42:11,106
expected free energy, which is the sum

941
00:42:11,138 --> 00:42:13,190
of risk and ambiguity.

942
00:42:14,010 --> 00:42:16,614
Okay? I hope that makes sense. Like

943
00:42:16,732 --> 00:42:19,414
here, you have considered the

944
00:42:19,452 --> 00:42:22,486
consequence, which is consequence of

945
00:42:22,508 --> 00:42:25,434
future, which is post or posterior. And

946
00:42:25,472 --> 00:42:27,194
you're basically evaluating how good

947
00:42:27,232 --> 00:42:29,674
that posterior is depending upon your

948
00:42:29,712 --> 00:42:31,798
expected free energy. And that becomes

949
00:42:31,814 --> 00:42:33,354
the expected free energy for that

950
00:42:33,392 --> 00:42:35,674
particular action. And you do this for

951
00:42:35,712 --> 00:42:39,454
all the actions. Okay. And why

952
00:42:39,492 --> 00:42:41,902
this is powerful? It is because you can

953
00:42:41,956 --> 00:42:45,198
go as deep as you want. So here in the

954
00:42:45,204 --> 00:42:48,766
next step, you go to this loop where you

955
00:42:48,788 --> 00:42:52,430
will check if I am crossing my deep

956
00:42:52,510 --> 00:42:54,946
planning or the depth of planning. And

957
00:42:54,968 --> 00:42:56,578
then you are doing basically the same.

958
00:42:56,664 --> 00:42:59,186
Given that posterior, what is the

959
00:42:59,208 --> 00:43:01,094
consequence of the actions of that

960
00:43:01,132 --> 00:43:03,574
particular posterior. So here for

961
00:43:03,612 --> 00:43:05,558
considering that we are again calling

962
00:43:05,724 --> 00:43:08,102
the parent function. So the same

963
00:43:08,156 --> 00:43:10,438
function, forward search, to consider

964
00:43:10,524 --> 00:43:12,966
consequences of those combinations and

965
00:43:12,988 --> 00:43:14,826
it will basically come back and add up

966
00:43:14,848 --> 00:43:17,226
to your expected free energy. So what

967
00:43:17,328 --> 00:43:19,962
happens over this sequence is that you

968
00:43:20,016 --> 00:43:23,542
consider some or all future consequences

969
00:43:23,686 --> 00:43:27,394
and then all that values will trickle

970
00:43:27,462 --> 00:43:31,134
up your tree. And that sum of the

971
00:43:31,172 --> 00:43:33,134
expected free energy will tell you which

972
00:43:33,172 --> 00:43:35,166
action is good or which action is bad,

973
00:43:35,268 --> 00:43:38,434
which you can take to kind of see your

974
00:43:38,472 --> 00:43:40,914
preferred observations. So that's the

975
00:43:40,952 --> 00:43:44,466
idea of implementing tree search. And I

976
00:43:44,488 --> 00:43:46,546
will also talk about the importance of

977
00:43:46,648 --> 00:43:48,180
this threshold here,

978
00:43:50,650 --> 00:43:53,286
which makes this algorithm possible. So

979
00:43:53,308 --> 00:43:55,330
without this threshold, this algorithm

980
00:43:55,490 --> 00:43:58,534
will not work. I will explicitly talk

981
00:43:58,572 --> 00:44:01,320
about why that is the case. And then

982
00:44:01,850 --> 00:44:03,626
once you evaluate the expected free

983
00:44:03,648 --> 00:44:06,090
energy for all available actions, given

984
00:44:06,160 --> 00:44:08,618
the present state, you can basically

985
00:44:08,704 --> 00:44:11,686
compute what you call the action

986
00:44:11,718 --> 00:44:15,194
distribution. That is how probable is my

987
00:44:15,232 --> 00:44:17,550
action or how I should take my action.

988
00:44:18,850 --> 00:44:21,114
And we also have this action precision

989
00:44:21,162 --> 00:44:23,614
parameter alpha. So if alpha is very

990
00:44:23,652 --> 00:44:27,194
high then it basically is a highly

991
00:44:27,242 --> 00:44:28,766
skewed distribution where you will

992
00:44:28,788 --> 00:44:30,670
always choose the action that minimizes

993
00:44:30,750 --> 00:44:33,250
expected free energy. If alpha is really

994
00:44:33,320 --> 00:44:37,118
low then it's going to be a more sparse

995
00:44:37,134 --> 00:44:39,586
or spread out distribution. And then you

996
00:44:39,608 --> 00:44:42,934
can use this action distribution to

997
00:44:42,972 --> 00:44:46,566
sample actions in

998
00:44:46,588 --> 00:44:48,502
the agent environment loop. We just

999
00:44:48,556 --> 00:44:51,106
finished doing planning and computing

1000
00:44:51,138 --> 00:44:53,914
that action distribution. Then using

1001
00:44:53,952 --> 00:44:56,406
that action distribution, you can sample

1002
00:44:56,438 --> 00:44:58,666
an action from your policy space. So

1003
00:44:58,688 --> 00:45:00,474
let's look at the policy space in this

1004
00:45:00,512 --> 00:45:01,580
generative model.

1005
00:45:02,910 --> 00:45:05,618
So I'm switching back to the original

1006
00:45:05,654 --> 00:45:07,342
generative model with one hidden state

1007
00:45:07,396 --> 00:45:11,114
factor and let's

1008
00:45:11,162 --> 00:45:14,640
do planning and maybe

1009
00:45:15,650 --> 00:45:17,520
initialize this agent.

1010
00:45:18,310 --> 00:45:21,154
I just want to initialize this agent to

1011
00:45:21,192 --> 00:45:23,970
see the policy space, not run the loop.

1012
00:45:24,390 --> 00:45:30,004
So I

1013
00:45:30,042 --> 00:45:32,228
initialize this agent, say for planning

1014
00:45:32,244 --> 00:45:36,810
depth of one. And if I look at agent

1015
00:45:44,020 --> 00:45:47,024
policies, I can see that I have

1016
00:45:47,062 --> 00:45:49,844
basically four available actions which

1017
00:45:49,882 --> 00:45:52,916
is north, south, east and west. And if I

1018
00:45:52,938 --> 00:45:55,236
have an action distribution, it will

1019
00:45:55,258 --> 00:45:57,124
tell me how probable is to take that

1020
00:45:57,162 --> 00:46:04,540
action. So if I look at agent UPI,

1021
00:46:05,680 --> 00:46:07,756
okay, so this is not defined because I

1022
00:46:07,778 --> 00:46:10,684
have not done planning, but I can do

1023
00:46:10,722 --> 00:46:12,780
planning and then it will be defined.

1024
00:46:18,580 --> 00:46:21,364
Yeah, so I implemented planning with

1025
00:46:21,402 --> 00:46:23,024
research and then now I have an action

1026
00:46:23,072 --> 00:46:24,900
distribution. So for this particular

1027
00:46:24,970 --> 00:46:27,252
scenario, I am going to take my third

1028
00:46:27,306 --> 00:46:30,424
action the most which is zero point 99.

1029
00:46:30,462 --> 00:46:33,256
Basically that's the probability which

1030
00:46:33,278 --> 00:46:35,624
is north, south and east in this

1031
00:46:35,662 --> 00:46:37,176
particular case. I just wanted to kind

1032
00:46:37,198 --> 00:46:40,660
of familiarize you with the matrices,

1033
00:46:40,740 --> 00:46:43,964
but we are now going to see the

1034
00:46:44,002 --> 00:46:46,060
agent environment loop in action.

1035
00:46:48,880 --> 00:46:52,172
Now you can sample an action from the

1036
00:46:52,226 --> 00:46:55,004
sample action function and then

1037
00:46:55,042 --> 00:46:56,876
implement learning which is using the

1038
00:46:56,898 --> 00:46:59,404
standard Pymdp way where I will update

1039
00:46:59,452 --> 00:47:01,596
my transition dynamics and likelihood

1040
00:47:01,628 --> 00:47:03,744
dynamics depending upon what I see and

1041
00:47:03,782 --> 00:47:06,144
what's my belief and so on. So my

1042
00:47:06,262 --> 00:47:09,510
emphasis is on the decision making part.

1043
00:47:18,010 --> 00:47:20,694
So once you sample an action, then that

1044
00:47:20,732 --> 00:47:23,430
action basically goes back to the

1045
00:47:23,580 --> 00:47:26,200
environment. Okay, so now let us

1046
00:47:27,130 --> 00:47:29,626
implement this for a planning depth of

1047
00:47:29,648 --> 00:47:32,490
one and see how the agent behaves.

1048
00:47:32,990 --> 00:47:35,594
So here if it is a planning depth of

1049
00:47:35,632 --> 00:47:37,626
one, then that means that the agent is

1050
00:47:37,648 --> 00:47:39,434
only considering the consequence of one

1051
00:47:39,472 --> 00:47:41,262
time step ahead, just seeing the

1052
00:47:41,316 --> 00:47:43,690
immediate future for doing planning,

1053
00:47:43,770 --> 00:47:46,346
right? So I'm giving the planning depth

1054
00:47:46,378 --> 00:47:49,418
of one. I'm resetting the environment

1055
00:47:49,514 --> 00:47:51,146
where the agent is going to start from

1056
00:47:51,188 --> 00:47:53,300
that initial start state,

1057
00:47:56,150 --> 00:48:02,394
and in the loop, it's going to get

1058
00:48:02,432 --> 00:48:05,318
that observation, take that action,

1059
00:48:05,414 --> 00:48:07,758
give back an action, and we will look at

1060
00:48:07,764 --> 00:48:10,286
the action probabilities. And also we

1061
00:48:10,308 --> 00:48:12,046
will give that action back to the

1062
00:48:12,068 --> 00:48:13,434
environment, get back the observation,

1063
00:48:13,482 --> 00:48:15,706
and this loop continues till the episode

1064
00:48:15,738 --> 00:48:17,726
is terminated. And I have set the

1065
00:48:17,748 --> 00:48:19,854
episode length to be eight, just to see

1066
00:48:19,892 --> 00:48:24,034
the outcome of eight action. So when

1067
00:48:24,072 --> 00:48:27,042
we run this loop, these matrices are

1068
00:48:27,096 --> 00:48:29,394
nothing but the action distributions of

1069
00:48:29,432 --> 00:48:32,006
how likely each action is to be taken.

1070
00:48:32,188 --> 00:48:34,726
And this is where the agent end up in

1071
00:48:34,748 --> 00:48:37,638
the last time step. Okay? So let's maybe

1072
00:48:37,724 --> 00:48:40,658
kind of also enable this environment

1073
00:48:40,754 --> 00:48:43,642
render that will show us where the agent

1074
00:48:43,696 --> 00:48:47,466
is at every time step. So initially the

1075
00:48:47,488 --> 00:48:50,826
agent was at this location and we have

1076
00:48:50,848 --> 00:48:53,482
an action distribution of this here. So

1077
00:48:53,536 --> 00:48:55,306
north, south, east and west. So the

1078
00:48:55,328 --> 00:48:57,258
agent knows that it should go north.

1079
00:48:57,354 --> 00:48:59,546
Why? Because if I look at the prior

1080
00:48:59,578 --> 00:49:02,814
preference, this state is more

1081
00:49:02,852 --> 00:49:05,246
preferred than this state. Right? So the

1082
00:49:05,268 --> 00:49:07,086
agent successfully calculated the

1083
00:49:07,108 --> 00:49:09,902
expected free energy and inferred that,

1084
00:49:09,956 --> 00:49:12,914
okay, I should go to this state and not

1085
00:49:12,952 --> 00:49:14,802
stay in this state. And because it has

1086
00:49:14,856 --> 00:49:16,606
the generative model of transitions

1087
00:49:16,638 --> 00:49:18,866
available, it can infer that I should

1088
00:49:18,888 --> 00:49:21,494
take an action north to go to this

1089
00:49:21,532 --> 00:49:24,806
state. So that's good. And the agent

1090
00:49:24,908 --> 00:49:26,998
goes to north. And at this particular

1091
00:49:27,084 --> 00:49:29,014
state, the agent infers that it should

1092
00:49:29,052 --> 00:49:31,878
go to north southeast. So it will take

1093
00:49:31,964 --> 00:49:35,274
an action east and it will go here.

1094
00:49:35,392 --> 00:49:38,074
And at this point of time, I want your

1095
00:49:38,112 --> 00:49:40,374
attention where the action distribution

1096
00:49:40,422 --> 00:49:43,318
is equally probable for north and east.

1097
00:49:43,414 --> 00:49:45,686
Why is that? Because the agent is only

1098
00:49:45,728 --> 00:49:47,582
looking at the immediate future. Right.

1099
00:49:47,636 --> 00:49:51,162
So let's go back to the prior preference

1100
00:49:51,306 --> 00:49:54,240
where the agent is right now here,

1101
00:49:55,330 --> 00:49:57,374
or is it here? Yeah, it is right now

1102
00:49:57,412 --> 00:50:00,386
here in this particular state. And if

1103
00:50:00,408 --> 00:50:02,174
the agent is considering immediate

1104
00:50:02,222 --> 00:50:05,186
consequences of just one action, then

1105
00:50:05,368 --> 00:50:07,746
these two states are equally good for it

1106
00:50:07,768 --> 00:50:10,514
to be in the next state. So there is no

1107
00:50:10,552 --> 00:50:13,038
distinction between these two states if

1108
00:50:13,064 --> 00:50:14,546
it is only looking at the immediate

1109
00:50:14,578 --> 00:50:17,042
future. So that means that the expected

1110
00:50:17,106 --> 00:50:19,446
free energygies will conclude that I

1111
00:50:19,468 --> 00:50:21,426
should go to north or east. It doesn't

1112
00:50:21,458 --> 00:50:23,594
matter if I'm looking at just one time

1113
00:50:23,632 --> 00:50:26,860
step ahead. Okay? That's the idea.

1114
00:50:27,790 --> 00:50:31,690
And out of probability it is going

1115
00:50:31,760 --> 00:50:35,214
here. It took the action east and

1116
00:50:35,252 --> 00:50:38,494
from this state, when it's doing

1117
00:50:38,532 --> 00:50:40,174
inference, it's inferring that this

1118
00:50:40,212 --> 00:50:42,522
state is better and basically it's

1119
00:50:42,666 --> 00:50:46,480
ending up in this local maxima state,

1120
00:50:46,850 --> 00:50:50,414
which is this particular state where the

1121
00:50:50,452 --> 00:50:52,154
neighboring states are less preferred.

1122
00:50:52,202 --> 00:50:55,026
And this is Wall and you can't go there

1123
00:50:55,048 --> 00:50:57,794
because it is forbidden for the agent by

1124
00:50:57,832 --> 00:50:59,666
structure. So it's basically going to

1125
00:50:59,688 --> 00:51:02,126
sit there forever where it only sees

1126
00:51:02,158 --> 00:51:05,338
that local maximum of prior preference.

1127
00:51:05,534 --> 00:51:09,542
And let's look at what might

1128
00:51:09,596 --> 00:51:11,954
happen if you have higher planning

1129
00:51:12,002 --> 00:51:14,934
depth. So if I go to the planning depth

1130
00:51:14,982 --> 00:51:19,194
of three, then that means that

1131
00:51:19,392 --> 00:51:22,134
the agent is actually reaching the goal

1132
00:51:22,182 --> 00:51:24,966
state at the last time step. But still

1133
00:51:25,088 --> 00:51:30,494
in the third time point, it had two

1134
00:51:30,532 --> 00:51:33,018
probabilistic actions, north and east.

1135
00:51:33,114 --> 00:51:36,334
So here from this particular state, out

1136
00:51:36,372 --> 00:51:38,990
of probability, it took the action north

1137
00:51:39,330 --> 00:51:41,566
but it can all equally take the action

1138
00:51:41,598 --> 00:51:43,678
east and end up in this local maxima.

1139
00:51:43,694 --> 00:51:46,114
So let's run again and probably it will

1140
00:51:46,152 --> 00:51:48,626
end up in this local maxima, okay?

1141
00:51:48,808 --> 00:51:52,278
And only for the planning depth of four,

1142
00:51:52,364 --> 00:51:55,030
which is sufficient enough, which is

1143
00:51:55,100 --> 00:51:58,070
necessary for this particular grid,

1144
00:51:59,130 --> 00:52:01,590
the agent is fully sure of what to do.

1145
00:52:01,660 --> 00:52:04,522
So at every time point it is fully sure

1146
00:52:04,576 --> 00:52:08,150
of what to do, that it has to go north,

1147
00:52:08,230 --> 00:52:10,774
then east, north, north, north, east,

1148
00:52:10,822 --> 00:52:12,986
east and south and reach this particular

1149
00:52:13,088 --> 00:52:16,702
goal state. So only for time step or

1150
00:52:16,756 --> 00:52:19,054
planning depth n equal to four, it can

1151
00:52:19,092 --> 00:52:23,166
successfully navigate this grid with

1152
00:52:23,348 --> 00:52:26,720
100% certainty. So that's the idea.

1153
00:52:27,250 --> 00:52:29,874
That's the implementation that I hope

1154
00:52:29,912 --> 00:52:32,994
you got. So there is also this idea of

1155
00:52:33,032 --> 00:52:34,914
action precision. So here it's a high

1156
00:52:34,952 --> 00:52:37,426
action precision. That is why it is

1157
00:52:37,608 --> 00:52:40,006
taking the actions. That is from the

1158
00:52:40,028 --> 00:52:42,146
probability. If it is a low action

1159
00:52:42,178 --> 00:52:46,214
precision like one, then the

1160
00:52:46,412 --> 00:52:49,814
good actions are more probable. But that

1161
00:52:49,852 --> 00:52:53,082
doesn't mean that it will be taken right

1162
00:52:53,216 --> 00:52:56,842
here by luck, it is taking the right

1163
00:52:56,896 --> 00:52:59,162
actions and reaching the state. But here

1164
00:52:59,216 --> 00:53:01,466
probabilities are most passed. But you

1165
00:53:01,488 --> 00:53:04,486
will also see like exploration behavior

1166
00:53:04,678 --> 00:53:07,542
in more number of trials if you control

1167
00:53:07,616 --> 00:53:10,206
this action precision. So I set it to a

1168
00:53:10,228 --> 00:53:11,902
high value to make sure that the agent

1169
00:53:11,956 --> 00:53:14,798
reaches the goal for this particular

1170
00:53:14,884 --> 00:53:17,398
problem. But it's worth playing and it's

1171
00:53:17,434 --> 00:53:21,220
important, right? Okay,

1172
00:53:22,630 --> 00:53:26,142
yeah. So for different planning depths

1173
00:53:26,206 --> 00:53:27,746
like one, three and four in this

1174
00:53:27,768 --> 00:53:29,966
problem, this is the behavior that you

1175
00:53:30,008 --> 00:53:32,626
expect. For lower planning depths,

1176
00:53:32,658 --> 00:53:34,498
which is not sufficient, the agent ends

1177
00:53:34,514 --> 00:53:38,422
up in local maximas or

1178
00:53:38,476 --> 00:53:40,726
local minimas of expected free energy or

1179
00:53:40,748 --> 00:53:43,274
local maximas of prior preference. But

1180
00:53:43,312 --> 00:53:44,886
with sufficient planning depth, it's

1181
00:53:44,918 --> 00:53:48,746
able to navigate and reach the goal. So

1182
00:53:48,928 --> 00:53:52,170
that brings us to the last point in this

1183
00:53:52,240 --> 00:53:56,462
tutorial where why is it

1184
00:53:56,596 --> 00:54:01,280
important to have a threshold in

1185
00:54:02,850 --> 00:54:05,760
evaluating sophisticated inference? So

1186
00:54:06,290 --> 00:54:08,994
by threshold, what we mean is that we

1187
00:54:09,032 --> 00:54:11,714
can ignore future possibilities in two

1188
00:54:11,752 --> 00:54:15,342
levels, right? You can ignore not likely

1189
00:54:15,406 --> 00:54:17,586
actions or not likely observations in

1190
00:54:17,608 --> 00:54:19,714
this research. But if you consider the

1191
00:54:19,752 --> 00:54:22,798
consequences of all actions and

1192
00:54:22,824 --> 00:54:24,898
observations, that means that you'll

1193
00:54:24,914 --> 00:54:26,678
have to consider four consequences in

1194
00:54:26,684 --> 00:54:28,438
the first place. Then you will have to

1195
00:54:28,444 --> 00:54:31,046
consider four times the action states in

1196
00:54:31,068 --> 00:54:33,334
the next time step for the next time

1197
00:54:33,372 --> 00:54:35,626
step. Then all of that multiplied with

1198
00:54:35,648 --> 00:54:37,638
the number of actions and this tree

1199
00:54:37,654 --> 00:54:39,846
search becomes intractable and you'll

1200
00:54:39,878 --> 00:54:42,346
explode. And it's even worse than the

1201
00:54:42,368 --> 00:54:44,346
classical active inference policy space

1202
00:54:44,448 --> 00:54:47,774
problem. But by defining a threshold of

1203
00:54:47,812 --> 00:54:50,666
even a small value, we'll ignore

1204
00:54:50,698 --> 00:54:53,406
possibilities. So where is that

1205
00:54:53,508 --> 00:54:56,702
implemented? In the forward search

1206
00:54:56,756 --> 00:55:00,850
algorithm, we are considering actions

1207
00:55:01,270 --> 00:55:03,406
with only action probabilities greater

1208
00:55:03,438 --> 00:55:06,146
than the particular threshold. Here I am

1209
00:55:06,168 --> 00:55:09,186
defining it as one by 16. Also in the

1210
00:55:09,288 --> 00:55:13,414
parent paper, it's one by 16 the

1211
00:55:13,452 --> 00:55:16,920
action probability. Okay?

1212
00:55:17,290 --> 00:55:19,750
So if it is zero, then that means that

1213
00:55:19,820 --> 00:55:22,006
it will consider all the consequences of

1214
00:55:22,028 --> 00:55:25,866
future and that's intractable. So you

1215
00:55:25,888 --> 00:55:28,074
can ignore actions which is not

1216
00:55:28,112 --> 00:55:31,626
probable, and also ignore states which

1217
00:55:31,648 --> 00:55:34,780
is less likely, or only consider states

1218
00:55:35,250 --> 00:55:37,294
that has probability greater than this

1219
00:55:37,332 --> 00:55:39,422
particular threshold value, and that

1220
00:55:39,476 --> 00:55:41,354
significantly reduces the computational

1221
00:55:41,402 --> 00:55:43,518
complexity, where you will only consider

1222
00:55:43,604 --> 00:55:45,886
combinations that are probable in the

1223
00:55:45,908 --> 00:55:49,806
future. Tree and that lets you go deeper

1224
00:55:49,838 --> 00:55:52,290
in your planning horizon.

1225
00:55:52,870 --> 00:55:55,746
That's an important point here. And if

1226
00:55:55,768 --> 00:55:59,950
you compare the time that takes

1227
00:56:00,040 --> 00:56:02,854
for deeper planning for a search

1228
00:56:02,892 --> 00:56:06,600
threshold of zero, so a

1229
00:56:07,050 --> 00:56:09,846
search threshold of zero means that you

1230
00:56:09,868 --> 00:56:13,546
will consider all consequences. And the

1231
00:56:13,568 --> 00:56:16,874
more deep you plan, the more time

1232
00:56:16,912 --> 00:56:20,010
it takes. And if you see to consider

1233
00:56:20,080 --> 00:56:22,726
only the first future or the immediate

1234
00:56:22,758 --> 00:56:26,214
future, it takes 0.1 second. For

1235
00:56:26,272 --> 00:56:28,686
considering three possibilities into the

1236
00:56:28,708 --> 00:56:31,182
future, it takes 3 seconds, and for four

1237
00:56:31,236 --> 00:56:33,214
it takes 300 seconds. And you can see

1238
00:56:33,252 --> 00:56:34,830
that the computational time is

1239
00:56:34,900 --> 00:56:39,060
exponentially growing. But if you have a

1240
00:56:39,590 --> 00:56:41,890
very small search threshold,

1241
00:56:43,270 --> 00:56:47,220
you have that computational time that

1242
00:56:48,230 --> 00:56:52,050
makes sense for implementation in real

1243
00:56:52,120 --> 00:56:55,286
world, right? So here for N equal to

1244
00:56:55,308 --> 00:56:57,686
four, that is four times steps into the

1245
00:56:57,708 --> 00:57:00,386
future, it's only taking 0.1 second.

1246
00:57:00,418 --> 00:57:02,450
And that's okay. I can still do

1247
00:57:02,540 --> 00:57:05,290
simulations with this complexity, but

1248
00:57:05,360 --> 00:57:09,962
there is no way I can talk about how

1249
00:57:10,016 --> 00:57:11,722
less the computational complexity is.

1250
00:57:11,776 --> 00:57:14,426
It truly depends on the nature of your

1251
00:57:14,448 --> 00:57:16,186
prior preferences and environment in

1252
00:57:16,208 --> 00:57:18,314
action. But this search threshold

1253
00:57:18,362 --> 00:57:21,422
actually works in real life, and we just

1254
00:57:21,476 --> 00:57:24,622
saw that in our simulations, right? For

1255
00:57:24,676 --> 00:57:26,894
N equal to four, it took only like 0.3

1256
00:57:26,932 --> 00:57:29,986
seconds in our simulations. But if you

1257
00:57:30,008 --> 00:57:32,340
set the search threshold as zero,

1258
00:57:33,030 --> 00:57:35,826
it already is 300 seconds for doing full

1259
00:57:35,928 --> 00:57:39,646
depth planning. And if I set a planning

1260
00:57:39,678 --> 00:57:42,070
depth of five, then it will basically

1261
00:57:42,140 --> 00:57:44,646
run forever maybe, and I will not be

1262
00:57:44,668 --> 00:57:47,878
able to do simulation. So that's the

1263
00:57:47,964 --> 00:57:49,750
idea of search threshold.

1264
00:57:50,810 --> 00:57:54,746
So actually, that's it. I wanted to

1265
00:57:54,768 --> 00:57:57,340
explain the Agent class,

1266
00:57:58,270 --> 00:57:59,946
the environment class, and the

1267
00:57:59,968 --> 00:58:03,626
particular demo and yeah, maybe it's a

1268
00:58:03,648 --> 00:58:06,014
good time for questions if anybody was

1269
00:58:06,052 --> 00:58:10,494
listening. And I hope people get

1270
00:58:10,532 --> 00:58:14,286
to play with this code and look

1271
00:58:14,308 --> 00:58:17,118
at the tutorial and implement this and

1272
00:58:17,284 --> 00:58:19,474
build generative models like this. So

1273
00:58:19,512 --> 00:58:22,274
this particular example is how do you

1274
00:58:22,312 --> 00:58:24,126
build a generative model for this grid

1275
00:58:24,158 --> 00:58:27,106
world task and see how the Agent is able

1276
00:58:27,128 --> 00:58:31,154
to take meaningful actions? But here we

1277
00:58:31,192 --> 00:58:33,286
gave it that true structure of the

1278
00:58:33,308 --> 00:58:35,426
environment in the B matrix and A matrix

1279
00:58:35,458 --> 00:58:38,742
and so on. But you can also play around

1280
00:58:38,796 --> 00:58:42,418
with learning in

1281
00:58:42,444 --> 00:58:45,866
the sense that while defining the

1282
00:58:46,048 --> 00:58:50,154
Agent step, you can add a flag that

1283
00:58:50,192 --> 00:58:52,700
says learning equal to true.

1284
00:58:54,750 --> 00:58:57,786
And if you start with an uninformed AB

1285
00:58:57,888 --> 00:59:00,318
and so on, you can experiment on how the

1286
00:59:00,324 --> 00:59:01,962
Agent is learning that environment.

1287
00:59:02,026 --> 00:59:04,238
Here you can look at the B matrix in the

1288
00:59:04,244 --> 00:59:05,994
beginning, you can look at the B matrix

1289
00:59:06,042 --> 00:59:08,398
after, say, ten trials and see how the

1290
00:59:08,404 --> 00:59:10,834
learning is taking place here. It

1291
00:59:10,872 --> 00:59:12,686
doesn't matter because the agent knows

1292
00:59:12,718 --> 00:59:15,058
the structure and it won't learn much.

1293
00:59:15,144 --> 00:59:17,806
But if it starts from an unknown

1294
00:59:17,838 --> 00:59:19,474
structure, then there is scope of

1295
00:59:19,512 --> 00:59:21,414
learning also to be implemented. And

1296
00:59:21,532 --> 00:59:24,886
it's already implemented because we

1297
00:59:24,908 --> 00:59:28,470
are using existing Pymdp functionalities

1298
00:59:29,050 --> 00:59:31,174
for learning A and B and it's already

1299
00:59:31,212 --> 00:59:33,880
part of the step function. So I hope

1300
00:59:34,890 --> 00:59:37,490
step function is clear, which is the

1301
00:59:37,500 --> 00:59:39,078
only thing you need to know if you're

1302
00:59:39,094 --> 00:59:40,374
trying to implement sophisticated

1303
00:59:40,422 --> 00:59:42,554
inference and just the names of these

1304
00:59:42,592 --> 00:59:44,346
matrices if you want to probe them and

1305
00:59:44,368 --> 00:59:47,566
look at them. And yeah, I hope the

1306
00:59:47,588 --> 00:59:51,614
session was useful. So I thank my

1307
00:59:51,652 --> 00:59:54,762
collaborators and Connor, who maintains

1308
00:59:54,826 --> 00:59:57,150
Pimdp, and also Brendan who runs the

1309
00:59:57,220 --> 01:00:00,306
Pimdp Fellowship, which I was part of,

1310
01:00:00,328 --> 01:00:02,322
and that's where I worked on

1311
01:00:02,376 --> 01:00:03,986
implementing sophisticated inference in

1312
01:00:04,008 --> 01:00:07,346
Pimdp and it will be part of the

1313
01:00:07,368 --> 01:00:10,514
original Pymdp module soon. And I hope

1314
01:00:10,632 --> 01:00:12,806
people can start using this module to

1315
01:00:12,828 --> 01:00:14,354
simulate sophisticated inference

1316
01:00:14,402 --> 01:00:16,594
experiments and this basically becomes

1317
01:00:16,642 --> 01:00:18,886
useful. So maybe it's a good time to

1318
01:00:18,908 --> 01:00:21,078
discuss questions or clarifications on

1319
01:00:21,084 --> 01:00:23,126
the code or maybe it's a good time to

1320
01:00:23,148 --> 01:00:25,240
take a break, as Daniel was.

1321
01:00:27,870 --> 01:00:31,002
Thank you. That was awesome. Well,

1322
01:00:31,056 --> 01:00:33,194
I have a few different questions and

1323
01:00:33,232 --> 01:00:36,890
I'll read a few from the live chat.

1324
01:00:36,970 --> 01:00:39,146
So I'll first just go to the live chat

1325
01:00:39,178 --> 01:00:42,286
and then ask those and then ask some

1326
01:00:42,308 --> 01:00:45,358
other questions. So Dave asks, how do

1327
01:00:45,364 --> 01:00:46,842
you think about the neural

1328
01:00:46,906 --> 01:00:49,454
implementation of recursion? Brains

1329
01:00:49,502 --> 01:00:51,058
don't seem to implement computer

1330
01:00:51,144 --> 01:00:53,186
hardware style recursion deeper than a

1331
01:00:53,208 --> 01:00:55,502
stack depth of one. Aside from heavily

1332
01:00:55,566 --> 01:00:58,126
over learned tasks, we can confine

1333
01:00:58,158 --> 01:01:00,118
ourselves to asking about recursion for

1334
01:01:00,124 --> 01:01:02,514
the purposes of exploring temporally

1335
01:01:02,562 --> 01:01:04,614
deep state spaces, searching forward in

1336
01:01:04,652 --> 01:01:07,798
time. So how do we reconcile this

1337
01:01:07,964 --> 01:01:10,102
really beautiful and elegant and

1338
01:01:10,156 --> 01:01:14,130
computationally efficient full depth

1339
01:01:14,210 --> 01:01:18,234
tree search with the biological basis of

1340
01:01:18,352 --> 01:01:21,820
multiscale planning? Yeah,

1341
01:01:23,070 --> 01:01:25,482
so I'm not an expert in neural

1342
01:01:25,546 --> 01:01:28,014
computation, but the answer to that

1343
01:01:28,052 --> 01:01:31,822
would be basically you're doing only

1344
01:01:31,876 --> 01:01:34,142
one computation at a time, right?

1345
01:01:34,276 --> 01:01:37,586
And all you need is some

1346
01:01:37,608 --> 01:01:40,610
memory to store your beliefs and use

1347
01:01:40,680 --> 01:01:43,394
those beliefs to kind of do the same

1348
01:01:43,432 --> 01:01:46,722
computation again. So we are not talking

1349
01:01:46,776 --> 01:01:50,434
about this hardcore

1350
01:01:50,482 --> 01:01:52,614
recursive implementation. We are only

1351
01:01:52,652 --> 01:01:55,062
doing local computations at a time. And

1352
01:01:55,116 --> 01:01:56,886
just because of the structure of the

1353
01:01:56,908 --> 01:02:00,214
generative model and because we have

1354
01:02:00,252 --> 01:02:03,746
memory, this can be done. And I don't

1355
01:02:03,778 --> 01:02:07,514
see why brain can't do it, even though

1356
01:02:07,552 --> 01:02:09,386
individual neurons might not be able to

1357
01:02:09,408 --> 01:02:12,214
do it. The brain has memory. The brain

1358
01:02:12,262 --> 01:02:14,666
has the ability to store memory and the

1359
01:02:14,688 --> 01:02:16,286
ability to dream, the ability to

1360
01:02:16,308 --> 01:02:19,086
simulate. It knows the consequences of

1361
01:02:19,108 --> 01:02:21,306
actions. And you do this on a daily

1362
01:02:21,338 --> 01:02:24,750
basis where you plan your future

1363
01:02:24,820 --> 01:02:27,762
and decide what to do. Right? So on a

1364
01:02:27,816 --> 01:02:30,146
single neuron level, I'm not really sure

1365
01:02:30,328 --> 01:02:32,226
of how to answer that question, but I

1366
01:02:32,248 --> 01:02:34,146
don't really see why the brain can't do

1367
01:02:34,168 --> 01:02:36,050
it as an organism.

1368
01:02:38,710 --> 01:02:42,262
Cool. Okay, to the code.

1369
01:02:42,316 --> 01:02:44,806
I guess I have a few questions. Can we

1370
01:02:44,828 --> 01:02:46,310
go back to the maze?

1371
01:02:51,000 --> 01:02:52,820
And of course, if anyone else has

1372
01:02:52,890 --> 01:02:54,816
questions in the live chat, just go for

1373
01:02:54,858 --> 01:02:58,328
it. So in

1374
01:02:58,334 --> 01:03:02,424
the maze, how does the

1375
01:03:02,462 --> 01:03:05,752
moves that are possible, how is that

1376
01:03:05,806 --> 01:03:09,080
reflected? What stage is it updated,

1377
01:03:09,500 --> 01:03:12,264
for example, that it initially knows

1378
01:03:12,312 --> 01:03:13,996
that it can only go up and then it can

1379
01:03:14,018 --> 01:03:16,604
only go right or down? Where is that

1380
01:03:16,642 --> 01:03:20,208
reflected with the updating of kind

1381
01:03:20,214 --> 01:03:21,984
of that relational aspect of

1382
01:03:22,022 --> 01:03:23,936
affordances? What is even possible to

1383
01:03:23,958 --> 01:03:27,328
do? And then how does it evaluate in a

1384
01:03:27,334 --> 01:03:28,530
deep tree search?

1385
01:03:30,660 --> 01:03:34,388
Does it need to know what things

1386
01:03:34,474 --> 01:03:36,870
could or couldn't happen in the future?

1387
01:03:39,560 --> 01:03:41,140
So, if I understand the questions

1388
01:03:41,210 --> 01:03:44,912
correctly, there is an important

1389
01:03:44,986 --> 01:03:46,420
distinction between the generative

1390
01:03:46,500 --> 01:03:48,392
process and the generative model,

1391
01:03:48,526 --> 01:03:52,264
right? So in the grid world, which is

1392
01:03:52,302 --> 01:03:55,544
the generative process, we have

1393
01:03:55,582 --> 01:03:58,284
implemented all of that manually, where

1394
01:03:58,322 --> 01:04:01,870
we have this transition dynamics that

1395
01:04:02,640 --> 01:04:05,116
already knows what will happen if you

1396
01:04:05,138 --> 01:04:07,288
take an action from a state. So it's

1397
01:04:07,304 --> 01:04:09,356
like an environment that knows how to

1398
01:04:09,378 --> 01:04:12,416
work. So it's like a reality where there

1399
01:04:12,438 --> 01:04:14,316
are consequences of actions and it's

1400
01:04:14,348 --> 01:04:17,410
already there. But this information is

1401
01:04:17,940 --> 01:04:20,624
available to the agent to be part of its

1402
01:04:20,662 --> 01:04:23,608
generative model. And in the generative

1403
01:04:23,644 --> 01:04:26,132
model, what it basically does is use

1404
01:04:26,186 --> 01:04:28,388
that transition dynamics, which is given

1405
01:04:28,474 --> 01:04:31,204
or learned to simulate what might happen

1406
01:04:31,242 --> 01:04:34,404
in the future. Okay? So once I have that

1407
01:04:34,442 --> 01:04:37,750
transition dynamics, if I go to the

1408
01:04:38,120 --> 01:04:41,348
where's the if I go to the tree

1409
01:04:41,364 --> 01:04:43,896
search, what it is essentially doing is

1410
01:04:44,078 --> 01:04:47,064
evaluating the posterior given a

1411
01:04:47,102 --> 01:04:50,524
particular state and an action J. So

1412
01:04:50,642 --> 01:04:53,564
from my generative model, I know that if

1413
01:04:53,602 --> 01:04:55,676
I start from this state and if I take

1414
01:04:55,698 --> 01:04:58,380
this action, I go to this posterior and

1415
01:04:58,450 --> 01:05:01,730
I consider all the consequences. And if

1416
01:05:02,180 --> 01:05:04,588
in the generative model, it is unlikely

1417
01:05:04,684 --> 01:05:07,072
that if I go east, I don't go there and

1418
01:05:07,126 --> 01:05:10,636
so on, it's automatically reflected

1419
01:05:10,668 --> 01:05:12,610
in the expected free energy.

1420
01:05:14,360 --> 01:05:17,220
So I hope that answers the question. So

1421
01:05:17,370 --> 01:05:20,976
here, if I set the action

1422
01:05:21,008 --> 01:05:25,152
precision to be high and also

1423
01:05:25,306 --> 01:05:31,784
enable the so

1424
01:05:31,822 --> 01:05:34,170
initially I am in this particular state,

1425
01:05:34,540 --> 01:05:37,144
and with respect to the expected free

1426
01:05:37,182 --> 01:05:39,576
energy, what I'm doing is I'm using my

1427
01:05:39,598 --> 01:05:41,176
generative model to predict what will

1428
01:05:41,198 --> 01:05:44,444
happen if I take four actions. And my

1429
01:05:44,482 --> 01:05:46,124
predictions will say that if I take

1430
01:05:46,162 --> 01:05:48,556
North, I will go here. And if I take all

1431
01:05:48,578 --> 01:05:50,568
the other actions, I'll stay here. And

1432
01:05:50,594 --> 01:05:52,704
because in my prior preference, north is

1433
01:05:52,742 --> 01:05:55,084
more preferred, I can infer that north

1434
01:05:55,132 --> 01:05:58,640
is the action I should go for.

1435
01:05:58,790 --> 01:06:01,884
So that's the idea. So here, the grid

1436
01:06:01,932 --> 01:06:03,536
structure is given to the agent. And it

1437
01:06:03,558 --> 01:06:04,976
might be a little confusing, but the

1438
01:06:04,998 --> 01:06:06,736
agent can also learn this grid structure

1439
01:06:06,768 --> 01:06:09,268
and this will work. So once I know the

1440
01:06:09,274 --> 01:06:11,204
grid structure as an agent, I can

1441
01:06:11,322 --> 01:06:13,796
simulate the future and consider the

1442
01:06:13,818 --> 01:06:16,416
consequences of action, right? So that's

1443
01:06:16,448 --> 01:06:19,016
what's happening. And once I am in this

1444
01:06:19,038 --> 01:06:20,596
state, I will consider the consequence

1445
01:06:20,628 --> 01:06:23,076
of all four actions, and I will infer

1446
01:06:23,108 --> 01:06:26,024
that, okay, going east is better because

1447
01:06:26,062 --> 01:06:28,664
that will take me to this state. So

1448
01:06:28,702 --> 01:06:30,604
there is always a difference between or

1449
01:06:30,642 --> 01:06:32,552
you should keep in mind how generative

1450
01:06:32,616 --> 01:06:34,364
model and generative process are two

1451
01:06:34,402 --> 01:06:37,260
different things, and what the agent

1452
01:06:37,330 --> 01:06:39,484
knows might not be the reality or might

1453
01:06:39,522 --> 01:06:41,264
be the reality, depending upon what you

1454
01:06:41,302 --> 01:06:45,344
give it. Makes sense.

1455
01:06:45,542 --> 01:06:49,344
Cool. So if you were going to go

1456
01:06:49,382 --> 01:06:53,956
about making a new situation

1457
01:06:54,058 --> 01:06:55,344
that you wanted to do generative

1458
01:06:55,392 --> 01:06:59,412
modeling for, do you tend to start from

1459
01:06:59,466 --> 01:07:02,884
an existing working Pi MDP notebook and

1460
01:07:02,922 --> 01:07:05,572
start modifying state spaces, or do you

1461
01:07:05,626 --> 01:07:08,216
draw it out on a canvas? How would you

1462
01:07:08,238 --> 01:07:09,864
recommend somebody other than

1463
01:07:09,902 --> 01:07:12,708
replicating what is shown here? Let's

1464
01:07:12,724 --> 01:07:13,944
just say we're interested in something

1465
01:07:13,982 --> 01:07:16,824
that's not exactly just a maze. What do

1466
01:07:16,862 --> 01:07:20,892
we do to get our head

1467
01:07:20,946 --> 01:07:24,220
around how we should proceed?

1468
01:07:24,880 --> 01:07:28,396
Yeah, good question. So if you

1469
01:07:28,418 --> 01:07:30,584
are trying to simulate, say, a new

1470
01:07:30,642 --> 01:07:33,932
environment, you have the heavy lifting

1471
01:07:34,076 --> 01:07:36,812
to do, which is to define the generative

1472
01:07:36,876 --> 01:07:39,872
model for the agent. You can either

1473
01:07:39,926 --> 01:07:41,744
define a very sparse generative model,

1474
01:07:41,782 --> 01:07:44,196
which the agent can learn, but you have

1475
01:07:44,218 --> 01:07:45,856
to define the structure. That structure

1476
01:07:45,888 --> 01:07:49,044
should be there, and only using that

1477
01:07:49,082 --> 01:07:51,156
structure, the agent can learn the

1478
01:07:51,178 --> 01:07:54,084
generative model. So here you can make

1479
01:07:54,122 --> 01:07:56,308
use of this cell of code to understand

1480
01:07:56,394 --> 01:07:58,680
how I define the structure of the grid.

1481
01:07:59,020 --> 01:08:01,528
Okay, so I am defining a structure for

1482
01:08:01,534 --> 01:08:03,956
the agent for it to make use of. I'm

1483
01:08:03,988 --> 01:08:06,090
saying that there are 25 valid states

1484
01:08:06,620 --> 01:08:08,472
and there are four available actions.

1485
01:08:08,536 --> 01:08:11,496
And this is the standard way of defining

1486
01:08:11,528 --> 01:08:14,844
the state space in Pymdp. And you also

1487
01:08:14,882 --> 01:08:18,024
have to define the central

1488
01:08:18,072 --> 01:08:20,768
parameters ABC and D for the agent

1489
01:08:20,854 --> 01:08:24,560
simulations. So here I am defining A

1490
01:08:24,710 --> 01:08:27,004
using my state space and observation

1491
01:08:27,052 --> 01:08:30,464
space. But this step of giving it

1492
01:08:30,662 --> 01:08:32,604
or telling it that it's an identity

1493
01:08:32,652 --> 01:08:35,204
matrix is my decision choice in my

1494
01:08:35,242 --> 01:08:38,196
modeling. I don't have to do this for

1495
01:08:38,298 --> 01:08:41,492
simulations. I can see if the agent is

1496
01:08:41,546 --> 01:08:44,724
learning it from a random A matrix or

1497
01:08:44,762 --> 01:08:46,916
when it starts from a random A matrix.

1498
01:08:47,108 --> 01:08:50,392
And similarly for this B matrix, this

1499
01:08:50,446 --> 01:08:54,472
structure is defined. And there are

1500
01:08:54,606 --> 01:08:56,328
functions like this which can give you a

1501
01:08:56,334 --> 01:08:58,676
random B matrix. But this is a modeling

1502
01:08:58,708 --> 01:09:01,032
choice where if I want to give it the

1503
01:09:01,086 --> 01:09:02,776
grid structure or not give it the grid

1504
01:09:02,808 --> 01:09:05,116
structure, I can start from a random B

1505
01:09:05,138 --> 01:09:07,276
matrix, let the agent learn, and look at

1506
01:09:07,298 --> 01:09:09,516
that learned B metric. Just for the

1507
01:09:09,538 --> 01:09:11,864
purpose of the demo. I gave it the grid

1508
01:09:11,912 --> 01:09:13,696
structure to enable it to take the

1509
01:09:13,718 --> 01:09:16,624
actions. But it's not a mandatory thing.

1510
01:09:16,662 --> 01:09:18,864
So this notebook is useful in the sense

1511
01:09:18,902 --> 01:09:20,848
that you know what to do, but definitely

1512
01:09:20,934 --> 01:09:23,600
you should play around with steps that

1513
01:09:23,750 --> 01:09:26,356
may not be mandatory. Right, so if I

1514
01:09:26,378 --> 01:09:29,172
give a prior preference for this state

1515
01:09:29,226 --> 01:09:31,204
to be the maximum, then you can see

1516
01:09:31,242 --> 01:09:33,364
behavior that. The agent will try and go

1517
01:09:33,402 --> 01:09:37,024
there. Right. So this prior preference

1518
01:09:37,072 --> 01:09:39,896
is defined in conjunction that this is

1519
01:09:39,918 --> 01:09:41,768
the goal state, but this may not be the

1520
01:09:41,774 --> 01:09:44,136
goal state. And in a different ask,

1521
01:09:44,238 --> 01:09:46,152
what prior preference means is different

1522
01:09:46,206 --> 01:09:49,140
according to the environment. Right.

1523
01:09:49,310 --> 01:09:51,564
So that's also there and your prior. So

1524
01:09:51,602 --> 01:09:53,372
once you define that generative model,

1525
01:09:53,426 --> 01:09:56,524
which you have to do, you can't run from

1526
01:09:56,562 --> 01:09:59,004
it, then everything is kind of

1527
01:09:59,042 --> 01:10:02,672
automated. The agent only

1528
01:10:02,726 --> 01:10:06,080
have to use the agent step,

1529
01:10:06,230 --> 01:10:08,016
give it the observation from the

1530
01:10:08,118 --> 01:10:10,304
environment. The agent knows how to take

1531
01:10:10,342 --> 01:10:12,596
actions, and then everything that

1532
01:10:12,618 --> 01:10:14,036
happens inside the agent, you don't

1533
01:10:14,058 --> 01:10:15,140
really have to worry.

1534
01:10:17,480 --> 01:10:19,796
So this structure, I'm sure, will be

1535
01:10:19,818 --> 01:10:22,996
useful for your particular task that you

1536
01:10:23,018 --> 01:10:28,200
are trying to model in your hmm.

1537
01:10:31,660 --> 01:10:34,552
Yes, very interesting. And how would you

1538
01:10:34,606 --> 01:10:37,704
contrast that or point to any

1539
01:10:37,742 --> 01:10:39,484
similarities or differences with how

1540
01:10:39,522 --> 01:10:41,416
this would be pursued outside of active

1541
01:10:41,448 --> 01:10:43,644
inference? Like if somebody were just

1542
01:10:43,682 --> 01:10:46,076
going to use another kind of deep policy

1543
01:10:46,178 --> 01:10:50,028
agent in the maze example, what parts of

1544
01:10:50,034 --> 01:10:53,232
the process would be familiar and what

1545
01:10:53,286 --> 01:10:55,664
parts would be like a lot of work that

1546
01:10:55,702 --> 01:10:57,392
wasn't expected or skipping through

1547
01:10:57,446 --> 01:10:59,520
parts that were a lot of work otherwise.

1548
01:11:00,020 --> 01:11:03,076
Yeah. So the

1549
01:11:03,098 --> 01:11:04,884
general structure is very familiar to

1550
01:11:04,922 --> 01:11:08,004
somebody who does things like this in

1551
01:11:08,042 --> 01:11:11,636
reinforcement learning. So the idea that

1552
01:11:11,738 --> 01:11:14,292
it's an agent step and environment step,

1553
01:11:14,346 --> 01:11:17,960
so this is the standard OpenAI gym

1554
01:11:18,300 --> 01:11:20,936
way of writing an environment and a

1555
01:11:20,958 --> 01:11:23,544
standard OpenAI way of writing an agent.

1556
01:11:23,742 --> 01:11:26,680
Okay? So if you have a Q learning agent

1557
01:11:26,750 --> 01:11:29,332
who does the same trying to navigate,

1558
01:11:29,476 --> 01:11:32,184
then the way you have to define the

1559
01:11:32,222 --> 01:11:34,572
queue matrix is the heavy lifting there.

1560
01:11:34,626 --> 01:11:38,124
It's just a state action mapping. And in

1561
01:11:38,162 --> 01:11:40,376
contrast to that, in active inference,

1562
01:11:40,408 --> 01:11:41,804
you have to come up with a generative

1563
01:11:41,852 --> 01:11:44,268
model that you want to see. So in active

1564
01:11:44,284 --> 01:11:46,176
inference, generative model is the

1565
01:11:46,198 --> 01:11:48,736
central thing. Right. Without a

1566
01:11:48,758 --> 01:11:51,888
generative model, there is no meaning of

1567
01:11:52,054 --> 01:11:54,204
purposeful behavior in active inference.

1568
01:11:54,332 --> 01:11:57,828
So the only unfamiliar part for a

1569
01:11:57,834 --> 01:12:00,084
person who is coming from, say, a field

1570
01:12:00,122 --> 01:12:01,524
like reinforcement learning is the

1571
01:12:01,562 --> 01:12:03,524
structure of the generating model. But

1572
01:12:03,562 --> 01:12:05,124
there is no way, other than getting used

1573
01:12:05,162 --> 01:12:08,916
to it, where it's the palm DP structure

1574
01:12:08,948 --> 01:12:10,664
which dominates. But if you're doing

1575
01:12:10,702 --> 01:12:12,984
deep active inference, all of this is

1576
01:12:13,102 --> 01:12:15,684
going to be neural networks and palm DPS

1577
01:12:15,732 --> 01:12:18,700
are also not active inference things.

1578
01:12:18,770 --> 01:12:21,016
Right. It's an industrial engineering

1579
01:12:21,048 --> 01:12:24,056
thing. So palm DPS must also be familiar

1580
01:12:24,088 --> 01:12:25,308
for people who are coming from the

1581
01:12:25,314 --> 01:12:27,404
computer science background, just the

1582
01:12:27,442 --> 01:12:30,428
idea that what really happens in the

1583
01:12:30,434 --> 01:12:33,644
agent is the active inference part where

1584
01:12:33,682 --> 01:12:35,376
we have expected free energy s and

1585
01:12:35,398 --> 01:12:37,056
variation of free energy energies. And

1586
01:12:37,078 --> 01:12:38,960
if you want to learn about that, then

1587
01:12:39,110 --> 01:12:41,376
you have to go to the agents and see how

1588
01:12:41,398 --> 01:12:43,440
it works. Look at the matrices

1589
01:12:43,520 --> 01:12:45,876
numerically, see what's happening. But

1590
01:12:46,058 --> 01:12:48,324
in a level where you want to get

1591
01:12:48,362 --> 01:12:50,836
started, I don't see any problem. All of

1592
01:12:50,858 --> 01:12:54,816
this is standard frameworks

1593
01:12:54,848 --> 01:12:57,332
like palmdps and OpenAI gym

1594
01:12:57,396 --> 01:12:59,172
environments, agent environment loop.

1595
01:12:59,236 --> 01:13:02,536
All this is very deeply discussed in

1596
01:13:02,558 --> 01:13:08,750
computer science. It's not cool.

1597
01:13:10,560 --> 01:13:14,904
So what other motifs

1598
01:13:14,952 --> 01:13:17,400
or cognitive phenomena are you excited

1599
01:13:17,480 --> 01:13:21,052
or how do you see the Pi MDP development

1600
01:13:21,116 --> 01:13:24,544
trajectory continuing after your

1601
01:13:24,582 --> 01:13:27,090
sophisticated inference gets pulled in?

1602
01:13:27,540 --> 01:13:31,084
Yeah, so the Pimdp

1603
01:13:31,132 --> 01:13:34,724
had the original functionality and the

1604
01:13:34,762 --> 01:13:37,040
functionality to implement or simulate

1605
01:13:37,200 --> 01:13:39,236
general active inference agents with the

1606
01:13:39,258 --> 01:13:41,344
policy space and so on. And that enabled

1607
01:13:41,392 --> 01:13:47,016
a lot of people in the community who

1608
01:13:47,038 --> 01:13:51,620
are not familiar with complicated

1609
01:13:51,700 --> 01:13:53,752
coding and so on, who people who do

1610
01:13:53,806 --> 01:13:57,336
psychology, psychiatry, and all the

1611
01:13:57,358 --> 01:13:59,036
things. Right. So whoever want to come

1612
01:13:59,058 --> 01:14:02,200
up and try implement active inference,

1613
01:14:02,280 --> 01:14:06,236
pymdp enabled that. And I am hoping that

1614
01:14:06,338 --> 01:14:08,364
this module will enable people who want

1615
01:14:08,402 --> 01:14:10,572
to try out sophisticated inference

1616
01:14:10,716 --> 01:14:12,828
experiments in their particular domain.

1617
01:14:12,924 --> 01:14:15,664
So if you spend some time and get

1618
01:14:15,702 --> 01:14:17,436
familiarized yourself with the structure

1619
01:14:17,468 --> 01:14:20,864
of how Pymdp works, then everything else

1620
01:14:20,902 --> 01:14:24,340
is just writing a jupyter notebook with

1621
01:14:24,490 --> 01:14:26,756
minimal code, right, to simulate this.

1622
01:14:26,858 --> 01:14:29,764
So if you have a particular task in your

1623
01:14:29,802 --> 01:14:32,932
domain, I don't see a problem for a

1624
01:14:32,986 --> 01:14:35,624
beginner to kind of try and code it.

1625
01:14:35,662 --> 01:14:37,672
And what I'm very excited to see is

1626
01:14:37,806 --> 01:14:40,984
people using this module for variety of

1627
01:14:41,022 --> 01:14:42,808
experiments, just like how people

1628
01:14:42,894 --> 01:14:45,524
started using Pymdp and sophisticated

1629
01:14:45,572 --> 01:14:48,728
inference is taking off. And it's now

1630
01:14:48,894 --> 01:14:53,116
widely talked about how

1631
01:14:53,138 --> 01:14:55,992
it is the way of doing active inference.

1632
01:14:56,056 --> 01:14:58,124
And I'm really hoping that people in

1633
01:14:58,162 --> 01:15:00,664
various domains start using this module

1634
01:15:00,712 --> 01:15:02,704
and see their experiments, and I look

1635
01:15:02,742 --> 01:15:05,344
forward for the feedback. Yeah, so what

1636
01:15:05,382 --> 01:15:07,984
Pimdp did two years ago, I'm hoping this

1637
01:15:08,022 --> 01:15:09,664
module will do to people who are trying

1638
01:15:09,702 --> 01:15:12,508
to model active inference in the soap

1639
01:15:12,524 --> 01:15:17,140
state. So you mentioned the OpenAI

1640
01:15:17,640 --> 01:15:20,820
gym and the standardized format, and

1641
01:15:20,970 --> 01:15:24,710
what benchmarks do you use or

1642
01:15:25,580 --> 01:15:27,384
what kind of test suites are you

1643
01:15:27,422 --> 01:15:31,112
comparing, and how do we really know

1644
01:15:31,166 --> 01:15:33,850
when we've made a generative model that

1645
01:15:35,020 --> 01:15:38,396
really exceeds or excels in a way that

1646
01:15:38,498 --> 01:15:41,964
other techniques are just

1647
01:15:42,002 --> 01:15:46,140
not doing? Yeah, so if I may

1648
01:15:46,210 --> 01:15:48,990
go to the OpenAI gym website,

1649
01:15:49,860 --> 01:15:51,856
we have several experiments. There the

1650
01:15:51,878 --> 01:15:53,520
classical reinforcement learning

1651
01:15:53,590 --> 01:15:56,336
examples like the lunar lander that you

1652
01:15:56,358 --> 01:15:59,468
see in this screen right now. So active

1653
01:15:59,484 --> 01:16:02,332
inference from its inception has faced

1654
01:16:02,476 --> 01:16:07,296
problems of scaling to tasks. And that's

1655
01:16:07,328 --> 01:16:09,136
in itself a field of research in active

1656
01:16:09,168 --> 01:16:11,296
inference, scaling active inference.

1657
01:16:11,328 --> 01:16:13,296
And that's one of the reasons why deep

1658
01:16:13,328 --> 01:16:16,208
active inference took over dealing with

1659
01:16:16,234 --> 01:16:19,160
tasks like this. So there are benchmarks

1660
01:16:19,580 --> 01:16:22,036
even now where the sophisticated

1661
01:16:22,068 --> 01:16:23,784
inference may not be able to deal with

1662
01:16:23,822 --> 01:16:25,784
state spaces. And personally, that's my

1663
01:16:25,822 --> 01:16:28,092
research. In my PhD. I am actually

1664
01:16:28,146 --> 01:16:32,184
looking at optimizing computations

1665
01:16:32,232 --> 01:16:35,112
in sophisticated inference algorithms

1666
01:16:35,176 --> 01:16:37,720
that lets you scale up to environments

1667
01:16:37,800 --> 01:16:41,084
like that. But to get started, you will

1668
01:16:41,122 --> 01:16:43,436
have to kind of write code and see if it

1669
01:16:43,458 --> 01:16:46,464
works for an environment, then look at

1670
01:16:46,662 --> 01:16:49,088
if it's not working, then you have to

1671
01:16:49,094 --> 01:16:51,104
look at methods to scale it up and so

1672
01:16:51,142 --> 01:16:54,464
on. So if I am talking about benchmarks,

1673
01:16:54,512 --> 01:16:56,644
sophisticated inference is as good as

1674
01:16:56,682 --> 01:17:00,708
any RL algorithms for this state space.

1675
01:17:00,874 --> 01:17:02,704
So for small problems, sophisticated

1676
01:17:02,752 --> 01:17:04,372
inference will work and it's really

1677
01:17:04,426 --> 01:17:07,528
good. But for high dimensional problems

1678
01:17:07,614 --> 01:17:10,292
like this, the classical implementation

1679
01:17:10,356 --> 01:17:12,584
that I just showed might not work, but

1680
01:17:12,622 --> 01:17:14,964
it's good enough for any decent

1681
01:17:15,012 --> 01:17:16,776
experiment. But if you want to scale up,

1682
01:17:16,798 --> 01:17:19,268
then that's still open and it's a new

1683
01:17:19,294 --> 01:17:21,132
field of research and what you do might

1684
01:17:21,186 --> 01:17:24,284
become a next new important paper.

1685
01:17:24,402 --> 01:17:27,596
So that's all I can tell in that regard.

1686
01:17:27,778 --> 01:17:30,544
Cool. You have to work and see what.

1687
01:17:30,582 --> 01:17:32,480
Measures do you think you'd be looking

1688
01:17:32,550 --> 01:17:35,980
for, like computational

1689
01:17:36,060 --> 01:17:38,816
resources or what are the measures that

1690
01:17:38,838 --> 01:17:42,000
even make sense to juxtapose such

1691
01:17:42,070 --> 01:17:45,876
different methods? Yeah, so the

1692
01:17:46,058 --> 01:17:49,524
OpenAI Gym was designed for that,

1693
01:17:49,642 --> 01:17:52,544
to compare different algorithms.

1694
01:17:52,672 --> 01:17:55,716
So OpenAI Gym by definition is a

1695
01:17:55,738 --> 01:17:57,736
collection of many environments. So in

1696
01:17:57,758 --> 01:17:59,476
my demo, I was talking about the grid

1697
01:17:59,508 --> 01:18:02,136
environment. Openiigm is nothing but a

1698
01:18:02,158 --> 01:18:04,376
collection of many environments which

1699
01:18:04,398 --> 01:18:06,232
will let you interact with those

1700
01:18:06,286 --> 01:18:10,168
environments using the environment step

1701
01:18:10,254 --> 01:18:13,224
function. Yeah.

1702
01:18:13,342 --> 01:18:15,292
So here we have the environment step

1703
01:18:15,346 --> 01:18:17,228
function that will let you interact with

1704
01:18:17,234 --> 01:18:19,928
the lunar lander. And that particular

1705
01:18:20,034 --> 01:18:23,410
task will have matrixes that lets you

1706
01:18:24,020 --> 01:18:26,736
judge how good or bad your algorithm is.

1707
01:18:26,838 --> 01:18:29,840
So in this lunar lander problem, how

1708
01:18:29,910 --> 01:18:31,980
optimally can you land your rover

1709
01:18:32,060 --> 01:18:34,528
between these two flags by spending

1710
01:18:34,624 --> 01:18:37,684
minimizing the fuel and so on. So those

1711
01:18:37,722 --> 01:18:41,270
matrices are very tasks specific, and

1712
01:18:41,880 --> 01:18:43,732
that's one direction you have to take.

1713
01:18:43,786 --> 01:18:47,396
You can take try and compete with RL

1714
01:18:47,428 --> 01:18:51,432
algorithms in matrixes, but the right

1715
01:18:51,486 --> 01:18:53,416
potential or the potential I see in

1716
01:18:53,438 --> 01:18:55,956
sophisticated inference is modeling

1717
01:18:55,988 --> 01:18:58,856
intelligent behavior, where in RL the

1718
01:18:58,878 --> 01:19:01,084
focus is to get things done to make this

1719
01:19:01,122 --> 01:19:03,368
work. But it's not really explainable,

1720
01:19:03,464 --> 01:19:05,452
especially deep RL and deep learning

1721
01:19:05,506 --> 01:19:07,596
methods. But in active inference, if you

1722
01:19:07,618 --> 01:19:08,956
manage to scale it up, they are

1723
01:19:08,978 --> 01:19:12,304
explainable and that will

1724
01:19:12,342 --> 01:19:14,124
let you understand how intelligence

1725
01:19:14,172 --> 01:19:17,600
emerges with time. And I see that more

1726
01:19:17,670 --> 01:19:20,524
interesting than competing with RL,

1727
01:19:20,572 --> 01:19:23,012
because if your focus is getting things

1728
01:19:23,066 --> 01:19:25,108
done, then maybe engineering is the

1729
01:19:25,114 --> 01:19:27,860
right way and not active inference.

1730
01:19:29,880 --> 01:19:33,300
Awesome. Any other comments or thoughts?

1731
01:19:42,070 --> 01:19:44,006
Aswin, do you have any other comments or

1732
01:19:44,028 --> 01:19:47,494
thoughts? No, I'm pretty happy.

1733
01:19:47,612 --> 01:19:50,214
I hope I was clear explaining the code.

1734
01:19:50,252 --> 01:19:52,166
Maybe it was too complicated or simple,

1735
01:19:52,268 --> 01:19:55,286
depending upon your level, but I hope it

1736
01:19:55,308 --> 01:19:56,966
is useful to at least one person who

1737
01:19:56,988 --> 01:19:59,638
would start using this and write the

1738
01:19:59,644 --> 01:20:02,022
code. Thank you so much for your time.

1739
01:20:02,156 --> 01:20:03,366
Awesome. And thank you for the

1740
01:20:03,388 --> 01:20:06,634
opportunity. Thank you for joining.

1741
01:20:06,682 --> 01:20:09,646
Till next time. See you. Thank you so

1742
01:20:09,668 --> 01:20:10,046
much.



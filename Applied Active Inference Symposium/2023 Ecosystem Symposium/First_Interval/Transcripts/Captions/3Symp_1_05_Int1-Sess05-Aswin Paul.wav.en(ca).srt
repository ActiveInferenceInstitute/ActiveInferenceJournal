1
00:00:01,969 --> 00:00:06,265
Daniel: And now welcome aswin how are you

2
00:00:06,265 --> 00:00:07,999
doing?

3
00:00:07,181 --> 00:00:07,907
Aswin: Hi, Daniel.

4
00:00:07,955 --> 00:00:08,712
I'm good.

5
00:00:09,080 --> 00:00:12,375
Daniel: Hanging out, looking forward.

6
00:00:12,377 --> 00:00:13,493
Aswin: Can you hear me properly?

7
00:00:14,504 --> 00:00:15,648
Daniel: Yeah, sounds good.

8
00:00:15,654 --> 00:00:18,997
And yes, looking forward to your workshop

9
00:00:18,997 --> 00:00:22,306
on sophisticated inference in pymdp.

10
00:00:22,322 --> 00:00:25,600
So it can be up to 90 minutes or it's

11
00:00:25,600 --> 00:00:28,908
totally okay if it's less and we can take

12
00:00:28,908 --> 00:00:30,193
a short break, but please take it away

13
00:00:30,193 --> 00:00:34,516
and just let me know however I can help.

14
00:00:34,553 --> 00:00:35,633
Aswin: Great.

15
00:00:35,651 --> 00:00:36,772
Thank you so much.

16
00:00:37,867 --> 00:00:39,094
Maybe I'll share my screen and start.

17
00:00:40,147 --> 00:00:43,426
So, regarding the structure of what we

18
00:00:43,426 --> 00:00:46,720
are doing here, I'm assuming that people

19
00:00:46,720 --> 00:00:49,007
who see this later might want to try it

20
00:00:49,007 --> 00:00:51,265
hands on along with the tutorial or

21
00:00:51,265 --> 00:00:52,368
something.

22
00:00:52,372 --> 00:00:54,574
So that's the structure I kept in mind.

23
00:00:55,611 --> 00:00:57,812
And so I'll go slow.

24
00:00:57,829 --> 00:00:58,983
Please bear with me.

25
00:00:59,003 --> 00:01:01,666
So welcome all of you to this session on

26
00:01:01,666 --> 00:01:03,885
sophisticated inference in pymdp.

27
00:01:03,889 --> 00:01:06,151
So here we are going to attempt to model

28
00:01:06,151 --> 00:01:09,419
some sophisticated inference simulations,

29
00:01:09,419 --> 00:01:11,681
especially the one in the original paper

30
00:01:11,681 --> 00:01:14,943
using the pymdp module and how it is not

31
00:01:14,943 --> 00:01:16,127
part of pymdp right now.

32
00:01:16,131 --> 00:01:18,335
But we are on the process of adding

33
00:01:18,335 --> 00:01:20,546
sophisticated inference to the pymtp

34
00:01:20,546 --> 00:01:21,598
module.

35
00:01:21,614 --> 00:01:24,899
And I'm going to mainly talk about the

36
00:01:24,899 --> 00:01:27,206
code that I have developed to kind of add

37
00:01:27,206 --> 00:01:28,363
that functionality.

38
00:01:28,369 --> 00:01:28,392
Right.

39
00:01:28,396 --> 00:01:30,582
So I'm Ashwin Paul.

40
00:01:30,587 --> 00:01:34,921
I am finally a PhD candidate at Monash

41
00:01:34,921 --> 00:01:35,026
University.

42
00:01:35,041 --> 00:01:38,339
And mostly I work with active inference

43
00:01:38,339 --> 00:01:41,636
models and try to understand how to use

44
00:01:41,636 --> 00:01:44,950
them as an explainable model to basically

45
00:01:44,950 --> 00:01:47,021
understand emergence of intelligent

46
00:01:47,021 --> 00:01:48,031
behavior.

47
00:01:48,032 --> 00:01:52,071
Right, so let's dive into the material

48
00:01:52,071 --> 00:01:53,083
right away.

49
00:01:53,085 --> 00:01:55,106
So to give an intro of free energy

50
00:01:55,106 --> 00:01:58,131
principle, I'm sure all of you right now

51
00:01:58,131 --> 00:02:00,096
have an understanding of what it is.

52
00:02:00,097 --> 00:02:03,126
But the central idea is that an agent is

53
00:02:03,126 --> 00:02:06,156
always trying to minimize the entropy of

54
00:02:06,156 --> 00:02:08,175
its observations, right?

55
00:02:08,175 --> 00:02:11,204
So if an observation is having really low

56
00:02:11,204 --> 00:02:14,234
probability in your mind and that happens,

57
00:02:14,234 --> 00:02:17,260
, then you are probably surprised and

58
00:02:17,260 --> 00:02:18,273
vice versa, right?

59
00:02:18,274 --> 00:02:21,300
And the entropy here is defined as the

60
00:02:21,300 --> 00:02:23,326
information theoretic entropy, where if

61
00:02:23,326 --> 00:02:25,347
you have a low probability, then

62
00:02:25,347 --> 00:02:28,374
automatically this is a high surprise or

63
00:02:28,374 --> 00:02:30,395
a high entropic observation.

64
00:02:30,397 --> 00:02:33,426
And as we all know, active inference also

65
00:02:33,426 --> 00:02:36,455
gives us a methodology to define what we

66
00:02:36,455 --> 00:02:38,479
call an agent environment loop.

67
00:02:39,480 --> 00:02:42,511
And this lets us define what is the agent

68
00:02:42,511 --> 00:02:45,541
that we are looking at and what is the

69
00:02:45,541 --> 00:02:47,564
behavior of the agent given the

70
00:02:47,564 --> 00:02:50,591
environment around it and so on.

71
00:02:50,592 --> 00:02:50,595
Right?

72
00:02:50,595 --> 00:02:52,617
So you're also familiar with the idea of

73
00:02:52,617 --> 00:02:53,625
Marco Blanket.

74
00:02:53,626 --> 00:02:56,649
And this is important because we always

75
00:02:56,649 --> 00:02:58,670
have to remember, I mean, have to

76
00:02:58,670 --> 00:03:00,632
remember the difference between the

77
00:03:00,632 --> 00:03:02,654
generative process and the generative

78
00:03:02,654 --> 00:03:04,678
model, which is quite a famous point of

79
00:03:04,678 --> 00:03:07,703
confusion in active inference literature

80
00:03:07,703 --> 00:03:09,727
and for the people who try to understand

81
00:03:09,727 --> 00:03:11,744
it in the beginning.

82
00:03:11,745 --> 00:03:12,752
Right?

83
00:03:12,755 --> 00:03:14,779
So the central question is that how does

84
00:03:14,779 --> 00:03:16,795
an agent minimize entropy?

85
00:03:16,796 --> 00:03:19,820
Because how does an agent know which

86
00:03:19,820 --> 00:03:20,838
observation is low or high

87
00:03:20,838 --> 00:03:21,849
probabilistic?

88
00:03:22,850 --> 00:03:22,854
Right?

89
00:03:22,855 --> 00:03:24,874
So that is by maintaining a generative

90
00:03:24,874 --> 00:03:24,877
model.

91
00:03:24,878 --> 00:03:27,901
And the generative model will tell you

92
00:03:27,901 --> 00:03:29,926
which is a high probabilistic observation

93
00:03:29,926 --> 00:03:31,945
and which is a low probabilistic

94
00:03:31,945 --> 00:03:32,954
observation.

95
00:03:32,955 --> 00:03:34,978
So the idea is that all the agent has

96
00:03:34,978 --> 00:03:37,001
access to is an observation that is

97
00:03:37,001 --> 00:03:39,025
coming from a generative process which

98
00:03:39,025 --> 00:03:41,049
the agent cannot directly observe.

99
00:03:42,050 --> 00:03:44,076
And an intelligent agent will try to

100
00:03:44,076 --> 00:03:47,105
build up a generative model in its mind,

101
00:03:47,105 --> 00:03:50,134
which is a model of the hidden states and

102
00:03:50,134 --> 00:03:53,160
the observation it has access to.

103
00:03:53,161 --> 00:03:55,181
And it can hope to kind of compute

104
00:03:55,181 --> 00:03:57,207
probabilities using this generative model,

105
00:03:57,207 --> 00:03:58,211
, right?

106
00:03:58,212 --> 00:04:00,176
But there is a problem that in general it

107
00:04:00,176 --> 00:04:02,197
is an intractable problem to kind of

108
00:04:02,197 --> 00:04:04,214
marginalize the probability of

109
00:04:04,214 --> 00:04:06,238
observations from the generative model.

110
00:04:06,239 --> 00:04:09,259
And that is why we have to define an

111
00:04:09,259 --> 00:04:11,280
upper bound on the surprise that the

112
00:04:11,280 --> 00:04:12,297
agent is trying to minimize.

113
00:04:12,298 --> 00:04:14,317
And there comes the idea of free energy,

114
00:04:14,317 --> 00:04:15,320
right?

115
00:04:15,320 --> 00:04:17,341
So this upper bound the agent is

116
00:04:17,341 --> 00:04:19,367
supposedly minimizing is the free energy.

117
00:04:19,367 --> 00:04:19,368
.

118
00:04:19,369 --> 00:04:21,386
And that's why it is the free energy

119
00:04:21,386 --> 00:04:22,391
principle.

120
00:04:22,392 --> 00:04:24,413
And here we have a new term called

121
00:04:24,413 --> 00:04:26,437
capital QS, which can be interpreted as

122
00:04:26,437 --> 00:04:29,462
the belief that the agent is maintaining

123
00:04:29,462 --> 00:04:31,487
about the hidden state in its generative

124
00:04:31,487 --> 00:04:32,493
model.

125
00:04:32,495 --> 00:04:34,518
And this quantity is the free energy.

126
00:04:35,519 --> 00:04:37,546
And traditionally we see this variational

127
00:04:37,546 --> 00:04:39,568
free energy being interpreted as in

128
00:04:39,568 --> 00:04:42,594
mostly the machine learning way where it

129
00:04:42,594 --> 00:04:44,616
is a balance between complexity and

130
00:04:44,616 --> 00:04:46,633
accuracy of the model.

131
00:04:46,634 --> 00:04:48,655
So when minimizing the free energy, the

132
00:04:48,655 --> 00:04:50,677
agent is trying to come up with a single

133
00:04:50,677 --> 00:04:52,697
model but at the same time an accurate

134
00:04:52,697 --> 00:04:55,719
model because here it's a minus sign with

135
00:04:55,719 --> 00:04:56,730
accuracy, right?

136
00:04:56,730 --> 00:04:58,757
It can also be interpreted in the physics

137
00:04:58,757 --> 00:05:01,723
way where the agent is always trying to

138
00:05:01,723 --> 00:05:03,749
minimize the energy of the model but at

139
00:05:03,749 --> 00:05:06,775
the same time maximizing the entropy of

140
00:05:06,775 --> 00:05:09,801
the model which goes in conjunction with

141
00:05:09,801 --> 00:05:11,827
the maximum entropy principle and so on

142
00:05:11,827 --> 00:05:13,849
from the classic literature.

143
00:05:14,850 --> 00:05:16,876
So, given that this idea of a generative

144
00:05:16,876 --> 00:05:19,903
model is so important in a software point

145
00:05:19,903 --> 00:05:21,929
of view, that's the first thing that you

146
00:05:21,929 --> 00:05:24,952
might want to do, right, to define a

147
00:05:24,952 --> 00:05:26,977
generative model for the agent which is

148
00:05:26,977 --> 00:05:29,002
informed or not informed depending upon

149
00:05:29,002 --> 00:05:31,026
the experiment that you're trying to

150
00:05:31,026 --> 00:05:32,034
model.

151
00:05:32,035 --> 00:05:35,061
So in classical active inference, usually

152
00:05:35,061 --> 00:05:37,086
decision making is defined in terms of

153
00:05:37,086 --> 00:05:38,093
policies.

154
00:05:38,093 --> 00:05:41,119
So, for example, if you are an agent in

155
00:05:41,119 --> 00:05:43,145
this environment so in the Mario game,

156
00:05:43,145 --> 00:05:46,173
Mario is the agent and everything else is

157
00:05:46,173 --> 00:05:47,186
the environment.

158
00:05:47,187 --> 00:05:52,232
And Mario has three available actions run,

159
00:05:52,232 --> 00:05:55,269
, jump or stay in this environment.

160
00:05:56,269 --> 00:05:58,295
And the classic definition of policy is

161
00:05:58,295 --> 00:06:01,263
that it is a sequence of actions in time.

162
00:06:01,263 --> 00:06:01,264
.

163
00:06:01,265 --> 00:06:04,290
So if you have a time horizon of capital

164
00:06:04,290 --> 00:06:06,314
T, then a policy is nothing but a series

165
00:06:06,314 --> 00:06:08,331
of actions you might take.

166
00:06:08,332 --> 00:06:11,364
So run, run, jump and so on in time.

167
00:06:11,365 --> 00:06:15,402
So this is the SuperScript is the action.

168
00:06:15,402 --> 00:06:15,402
.

169
00:06:15,404 --> 00:06:17,427
So here it is jump, run and so on.

170
00:06:17,428 --> 00:06:19,447
And the subscript is the time.

171
00:06:19,449 --> 00:06:23,482
And then what you can have is a policy

172
00:06:23,482 --> 00:06:26,517
space, which is a collection of many such

173
00:06:26,517 --> 00:06:28,535
policies, small pi.

174
00:06:28,536 --> 00:06:31,561
And what you essentially do to take

175
00:06:31,561 --> 00:06:34,590
decisions in active inference is compute,

176
00:06:34,590 --> 00:06:35,601
not optimize.

177
00:06:35,601 --> 00:06:38,634
Compute the expected free energy of every

178
00:06:38,634 --> 00:06:40,657
policy in your policy space.

179
00:06:40,659 --> 00:06:44,691
And basically, that can be interpreted as

180
00:06:44,691 --> 00:06:47,721
a balance between risk and ambiguity.

181
00:06:50,757 --> 00:06:53,784
So when you compute this expected free

182
00:06:53,784 --> 00:06:55,809
energy, what you're trying to do is

183
00:06:55,809 --> 00:06:57,825
minimizing the risk.

184
00:06:57,829 --> 00:07:00,799
That is, how different is your belief

185
00:07:00,799 --> 00:07:03,828
about the observation from your prior

186
00:07:03,828 --> 00:07:04,839
preferences?

187
00:07:05,840 --> 00:07:06,850
Capital C.

188
00:07:06,851 --> 00:07:08,870
So this is also part of the generative

189
00:07:08,870 --> 00:07:10,891
model when you're trying to model control

190
00:07:10,891 --> 00:07:12,909
and at the same time you are trying to

191
00:07:12,909 --> 00:07:13,927
minimize the ambiguity when you are

192
00:07:13,927 --> 00:07:15,946
choosing a policy that has the minimum

193
00:07:15,946 --> 00:07:17,962
expected free energy, right?

194
00:07:17,963 --> 00:07:19,989
But this formulation has a problem that a

195
00:07:19,989 --> 00:07:22,016
policy space quickly becomes intractable,

196
00:07:22,016 --> 00:07:25,041
that there can be an enormous number of

197
00:07:25,041 --> 00:07:27,065
small pies or policies in your policy

198
00:07:27,065 --> 00:07:29,087
space and sitting and computing the

199
00:07:29,087 --> 00:07:31,108
expected free energy for all such

200
00:07:31,108 --> 00:07:34,133
policies even for a small time horizon,

201
00:07:34,133 --> 00:07:35,148
is not possible.

202
00:07:36,153 --> 00:07:39,187
But this is the classical structure that

203
00:07:39,187 --> 00:07:43,223
has been implemented in Pymdp nonetheless,

204
00:07:43,223 --> 00:07:46,253
, where we have different modules in

205
00:07:46,253 --> 00:07:49,285
Pymdp that is meant to be implementing

206
00:07:49,285 --> 00:07:52,313
different aspects of behavior.

207
00:07:52,314 --> 00:07:54,338
So for example, for inference or

208
00:07:54,338 --> 00:07:57,367
perception we have belief propagation,

209
00:07:57,367 --> 00:08:00,338
fixed point iteration, marginal message

210
00:08:00,338 --> 00:08:03,367
passing and all that implemented in the

211
00:08:03,367 --> 00:08:05,383
inference module.

212
00:08:05,385 --> 00:08:08,415
In the control module we have different

213
00:08:08,415 --> 00:08:11,446
methods to evaluate expected free energy

214
00:08:11,446 --> 00:08:14,474
for policies, one depending upon the

215
00:08:14,474 --> 00:08:17,506
expected utility, the other one depending

216
00:08:17,506 --> 00:08:20,533
upon the classic method that I just

217
00:08:20,533 --> 00:08:21,545
explained.

218
00:08:21,546 --> 00:08:23,567
Then we have module for learning.

219
00:08:23,568 --> 00:08:26,593
So we learn the parameters in the form DP

220
00:08:26,593 --> 00:08:28,617
like capital A, capital B, so likelihood

221
00:08:28,617 --> 00:08:30,637
transition dynamics and so on.

222
00:08:30,637 --> 00:08:33,663
And then we have algorithms for

223
00:08:33,663 --> 00:08:36,697
implementing all this in the algorithms

224
00:08:36,697 --> 00:08:37,704
module.

225
00:08:37,706 --> 00:08:40,736
And then the most powerful thing in Pymdp

226
00:08:40,736 --> 00:08:43,766
right now is the agent class where it is

227
00:08:43,766 --> 00:08:46,796
easy for you to kind of define the agent

228
00:08:46,796 --> 00:08:49,823
environment loop and we are trying to

229
00:08:49,823 --> 00:08:50,833
build up.

230
00:08:50,834 --> 00:08:52,859
So today I'm going to talk about an agent

231
00:08:52,859 --> 00:08:55,880
class that implements sophisticated

232
00:08:55,880 --> 00:08:57,901
inference rather than the classical

233
00:08:57,901 --> 00:08:59,924
active inference that we just saw.

234
00:09:00,871 --> 00:09:02,894
So, as I mentioned, how many valid

235
00:09:02,894 --> 00:09:05,921
policies can be defined, say for a time

236
00:09:05,921 --> 00:09:07,944
horizon of 15 in classical active

237
00:09:07,944 --> 00:09:08,953
inference?

238
00:09:08,954 --> 00:09:09,960
Right?

239
00:09:09,961 --> 00:09:11,986
So the first policy of course, is a

240
00:09:11,986 --> 00:09:14,013
series of first action that is jumped,

241
00:09:14,013 --> 00:09:17,043
then you can have the last action changed

242
00:09:17,043 --> 00:09:20,072
and you already can see that there can be

243
00:09:20,072 --> 00:09:22,093
n number of combinations.

244
00:09:22,094 --> 00:09:24,119
And for this simple case, the policy

245
00:09:24,119 --> 00:09:27,146
space is as big as ten to the power 13.

246
00:09:27,148 --> 00:09:30,172
And in a stochastic problem setting there

247
00:09:30,172 --> 00:09:32,197
is no way to kind of come up with a small

248
00:09:32,197 --> 00:09:35,221
subset of this policy space so that you

249
00:09:35,221 --> 00:09:37,245
can tackle this problem of computational

250
00:09:37,245 --> 00:09:38,254
complexity.

251
00:09:38,259 --> 00:09:41,283
And yeah, as I mentioned, in a stochastic

252
00:09:41,283 --> 00:09:43,306
problem setting it is an intractable size

253
00:09:43,306 --> 00:09:44,315
policy space.

254
00:09:45,320 --> 00:09:47,344
There comes the idea of sophisticated

255
00:09:47,344 --> 00:09:49,368
inference where we are thinking about

256
00:09:49,368 --> 00:09:52,391
taking decisions in a different way,

257
00:09:52,391 --> 00:09:52,397
right?

258
00:09:52,397 --> 00:09:54,418
So rather than thinking about sequence of

259
00:09:54,418 --> 00:09:57,439
actions in time, we can directly think of

260
00:09:57,439 --> 00:09:58,455
what to do when we see something

261
00:09:58,455 --> 00:10:00,414
depending upon our beliefs about the

262
00:10:00,414 --> 00:10:02,432
current state and beliefs about the

263
00:10:02,432 --> 00:10:03,442
future, right?

264
00:10:03,442 --> 00:10:05,464
So if I see myself in a current situation,

265
00:10:05,464 --> 00:10:06,473
, what should I do?

266
00:10:06,474 --> 00:10:09,503
And that's more like a straightforward

267
00:10:09,503 --> 00:10:11,527
thinking of how to take actions.

268
00:10:11,529 --> 00:10:14,553
And here the expected free energy.

269
00:10:14,554 --> 00:10:17,582
The structure of the expected free energy

270
00:10:17,582 --> 00:10:19,608
is the same but we are not evaluating

271
00:10:19,608 --> 00:10:22,633
expected free energy of policies, but

272
00:10:22,633 --> 00:10:24,657
expected free energy of observation

273
00:10:24,657 --> 00:10:26,674
action combinations.

274
00:10:26,675 --> 00:10:28,694
So if I see something and if I do this,

275
00:10:28,694 --> 00:10:30,710
what's the expected free energy?

276
00:10:30,711 --> 00:10:32,733
And that's what I'm trying to minimize.

277
00:10:32,734 --> 00:10:36,772
That's what I'm trying to optimize in

278
00:10:36,772 --> 00:10:38,793
this setting, right?

279
00:10:38,793 --> 00:10:40,813
So here again, we have the risk term

280
00:10:40,813 --> 00:10:42,833
where we are trying to minimize the

281
00:10:42,833 --> 00:10:44,853
deviation between the belief and the

282
00:10:44,853 --> 00:10:45,865
prior preferences.

283
00:10:45,867 --> 00:10:48,898
We also have the ambiguity term and this

284
00:10:48,898 --> 00:10:51,925
together makes up the expected free

285
00:10:51,925 --> 00:10:54,954
energy of this time point at time T.

286
00:10:54,956 --> 00:10:56,976
But we also have an expectation about

287
00:10:56,976 --> 00:10:58,996
what's the expected free energy in the

288
00:10:58,996 --> 00:10:59,006
next time step.

289
00:10:59,006 --> 00:11:02,972
And to evaluate the expected free energy

290
00:11:02,972 --> 00:11:04,998
of next time step, you will have to again

291
00:11:04,998 --> 00:11:07,024
compute this equation with OT plus two.

292
00:11:07,025 --> 00:11:09,045
And for that you will have to again

293
00:11:09,045 --> 00:11:11,068
compute this equation with OT plus three

294
00:11:11,068 --> 00:11:12,075
and so on.

295
00:11:12,075 --> 00:11:14,098
And this automatically becomes a tree

296
00:11:14,098 --> 00:11:17,124
search because of the recursive way this

297
00:11:17,124 --> 00:11:20,150
equation is defined and it comes with its

298
00:11:20,150 --> 00:11:22,175
own problems, but there are clever ways

299
00:11:22,175 --> 00:11:24,199
to get around them and we are going to

300
00:11:24,199 --> 00:11:27,222
discuss that in the code today.

301
00:11:27,223 --> 00:11:29,249
So, given the structure of sophisticated

302
00:11:29,249 --> 00:11:32,270
inference here, as I mentioned, the

303
00:11:32,270 --> 00:11:34,296
research replace the policy space that we

304
00:11:34,296 --> 00:11:37,321
saw for the traditional active inference

305
00:11:37,321 --> 00:11:38,333
we are used to.

306
00:11:38,335 --> 00:11:42,374
So in this workshop, what I am focusing

307
00:11:42,374 --> 00:11:46,414
on is how to kind of define a generative

308
00:11:46,414 --> 00:11:49,447
model and given an environment.

309
00:11:49,448 --> 00:11:52,477
So for example, this is the grid that is

310
00:11:52,477 --> 00:11:55,504
simulated in the original paper and we

311
00:11:55,504 --> 00:11:58,533
are going to talk about how to build up a

312
00:11:58,533 --> 00:12:01,502
generative model for this grid that can

313
00:12:01,502 --> 00:12:04,531
be used in the sophisticated inference in

314
00:12:04,531 --> 00:12:05,546
the PMDP module.

315
00:12:07,563 --> 00:12:09,588
So basically, what I'm trying to talk

316
00:12:09,588 --> 00:12:12,616
about is that the environment will have a

317
00:12:12,616 --> 00:12:15,642
step function that takes an action from

318
00:12:15,642 --> 00:12:17,667
the agent in Pymdp, and the agent will

319
00:12:17,667 --> 00:12:20,695
get an observation out of that action and

320
00:12:20,695 --> 00:12:23,723
we'll talk about this particular function,

321
00:12:23,723 --> 00:12:25,745
, agent step and agent steps.

322
00:12:25,747 --> 00:12:27,769
Step will take up an observation and try

323
00:12:27,769 --> 00:12:30,790
to come up with an action for the next

324
00:12:30,790 --> 00:12:30,793
time.

325
00:12:30,794 --> 00:12:30,798
Step right.

326
00:12:30,799 --> 00:12:33,827
And this creates a loop and cleverly

327
00:12:33,827 --> 00:12:36,855
designing this loop will let you see

328
00:12:36,855 --> 00:12:39,885
emergence of purposeful behavior in the

329
00:12:39,885 --> 00:12:42,912
sophisticated inference setting.

330
00:12:42,913 --> 00:12:44,932
So for example, in this particular grid

331
00:12:44,932 --> 00:12:46,951
with sufficient planning horizon, you

332
00:12:46,951 --> 00:12:47,969
will be able to see that the agent is

333
00:12:47,969 --> 00:12:50,989
capable of navigating in this grid and so

334
00:12:50,989 --> 00:12:50,993
on.

335
00:12:50,993 --> 00:12:53,025
So this is the example that I'm going to

336
00:12:53,025 --> 00:12:55,048
focus on in this talk today.

337
00:12:59,083 --> 00:13:06,092
Excuse me, I want to right away jump in

338
00:13:06,092 --> 00:13:08,115
to the code.

339
00:13:09,125 --> 00:13:13,167
So we have the Pymdp home, which I'm

340
00:13:13,167 --> 00:13:17,202
hoping you are familiar with.

341
00:13:17,203 --> 00:13:21,242
So we have this GitHub repository where

342
00:13:21,242 --> 00:13:25,284
we have the pymdp module and inside pymdp

343
00:13:25,284 --> 00:13:29,322
module we have several parts for it.

344
00:13:29,322 --> 00:13:31,347
So here we have the agent in the original

345
00:13:31,347 --> 00:13:33,368
Pymdp module that implements the so

346
00:13:33,368 --> 00:13:35,389
called classical active inference.

347
00:13:36,390 --> 00:13:38,417
We have several environments and we have

348
00:13:38,417 --> 00:13:41,446
help of functions like learning inference,

349
00:13:41,446 --> 00:13:42,458
, maths and so on.

350
00:13:42,459 --> 00:13:45,487
So this is the module of Pimedp.

351
00:13:45,488 --> 00:13:48,517
But in the parent folder we also have

352
00:13:48,517 --> 00:13:51,548
examples where there is tutorials about

353
00:13:51,548 --> 00:13:55,579
how to use the agent class, how to kind

354
00:13:55,579 --> 00:13:58,613
of deal with the environments and so on.

355
00:13:58,614 --> 00:14:01,586
So if you look at the pull requests so we

356
00:14:01,586 --> 00:14:03,608
are right now trying to merge

357
00:14:03,608 --> 00:14:06,639
sophisticated inference into the original

358
00:14:06,639 --> 00:14:08,651
Pymdp module.

359
00:14:08,652 --> 00:14:09,669
And today I'm going to talk about the

360
00:14:09,669 --> 00:14:11,681
code in this pull request.

361
00:14:11,681 --> 00:14:15,719
So if you want to try this hands on, you

362
00:14:15,719 --> 00:14:18,757
might want to go to this page where this

363
00:14:18,757 --> 00:14:22,796
pull request is there and it has the same

364
00:14:22,796 --> 00:14:24,817
structure of Pymdp.

365
00:14:24,818 --> 00:14:27,847
It's basically designed using Pymdp.

366
00:14:27,848 --> 00:14:30,873
And here what we have additionally is an

367
00:14:30,873 --> 00:14:32,894
agent si which is a sophisticated

368
00:14:32,894 --> 00:14:35,921
inference agent which does everything and

369
00:14:35,921 --> 00:14:37,946
also planning and decision making in the

370
00:14:37,946 --> 00:14:39,967
sophisticated inference way.

371
00:14:39,968 --> 00:14:42,994
And in the parent folder there is also an

372
00:14:42,994 --> 00:14:44,015
example folder for sophisticated

373
00:14:44,015 --> 00:14:45,026
inference demo.

374
00:14:45,027 --> 00:14:48,049
And what I am going to do today is to

375
00:14:48,049 --> 00:14:49,069
walk you through this tutorial of

376
00:14:49,069 --> 00:14:51,084
sophisticated inference.

377
00:14:51,085 --> 00:14:54,113
And on the way, I'm going to discuss and

378
00:14:54,113 --> 00:14:57,140
at points where I reference the helper

379
00:14:57,140 --> 00:14:59,167
codes, I'm going to go to that code and

380
00:14:59,167 --> 00:15:02,135
try to explain what's actually happening

381
00:15:02,135 --> 00:15:04,156
and how we complete that agent

382
00:15:04,156 --> 00:15:07,183
environment loop where we can see that

383
00:15:07,183 --> 00:15:09,201
purposeful behavior.

384
00:15:09,202 --> 00:15:09,209
Right?

385
00:15:11,227 --> 00:15:12,231
Yeah.

386
00:15:12,231 --> 00:15:14,251
So that's the Pymdp home.

387
00:15:14,252 --> 00:15:17,280
Then I also talked about the pull request.

388
00:15:17,280 --> 00:15:17,280
.

389
00:15:17,281 --> 00:15:19,307
So let's right away go into the jupyter

390
00:15:19,307 --> 00:15:20,314
notebook.

391
00:15:20,315 --> 00:15:22,337
So this is my local copy of this

392
00:15:22,337 --> 00:15:25,365
repository, so it's easier for me to run

393
00:15:25,365 --> 00:15:28,394
it and show it in my personal computer.

394
00:15:28,395 --> 00:15:30,419
So this is the parent folder with Pymdp

395
00:15:30,419 --> 00:15:33,443
and examples and inside examples I have a

396
00:15:33,443 --> 00:15:35,467
demo folder for sophisticated inference

397
00:15:35,467 --> 00:15:37,488
and this is the notebook I'm talking

398
00:15:37,488 --> 00:15:38,494
about.

399
00:15:38,495 --> 00:15:38,498
Right?

400
00:15:38,498 --> 00:15:41,524
So here in this example, what we are

401
00:15:41,524 --> 00:15:44,553
trying to do is deal with this particular

402
00:15:44,553 --> 00:15:46,576
grid world task from the original

403
00:15:46,576 --> 00:15:49,603
sophisticated inference paper and make

404
00:15:49,603 --> 00:15:51,627
this agent or enable this agent to

405
00:15:51,627 --> 00:15:54,652
navigate to this red dot, which is

406
00:15:54,652 --> 00:15:56,675
supposedly the goal state in this

407
00:15:56,675 --> 00:15:59,703
particular task given a prior preference

408
00:15:59,703 --> 00:16:01,662
like this, right?

409
00:16:01,662 --> 00:16:03,681
So this prior preference is quite

410
00:16:03,681 --> 00:16:05,704
informative in the sense that we right

411
00:16:05,704 --> 00:16:07,723
away can see that this is the most

412
00:16:07,723 --> 00:16:09,747
preferred state, the white color and the

413
00:16:09,747 --> 00:16:11,767
surrounding states are kind of less

414
00:16:11,767 --> 00:16:13,789
preferred, but more preferred than the

415
00:16:13,789 --> 00:16:15,800
faraway ones.

416
00:16:15,801 --> 00:16:15,803
Right?

417
00:16:15,804 --> 00:16:17,826
So this is the grid world task that we

418
00:16:17,826 --> 00:16:18,837
are trying to use.

419
00:16:18,839 --> 00:16:22,876
So the first cell is importing all the

420
00:16:22,876 --> 00:16:26,911
necessary libraries and some useful

421
00:16:26,911 --> 00:16:29,948
libraries like NumPy and Matplotlib.

422
00:16:30,950 --> 00:16:32,973
And the most important one is Pymdp.

423
00:16:32,974 --> 00:16:34,998
So I'm actually now calling the local

424
00:16:34,998 --> 00:16:37,023
copy of my Pymdp with the sophisticated

425
00:16:37,023 --> 00:16:39,044
inference implementation, not the

426
00:16:39,044 --> 00:16:42,070
original one, which is not merged yet.

427
00:16:42,072 --> 00:16:44,093
And the first thing I want to talk about

428
00:16:44,093 --> 00:16:46,112
is the environment itself, right?

429
00:16:46,113 --> 00:16:49,140
So the environment step part where if I

430
00:16:49,140 --> 00:16:51,168
get some action, how does the environment

431
00:16:51,168 --> 00:16:52,173
work?

432
00:16:52,175 --> 00:16:56,213
And for that, inside this folder I have a

433
00:16:56,213 --> 00:17:00,192
file which is Grid environment Si PY and

434
00:17:00,192 --> 00:17:04,231
this is basically an environment class.

435
00:17:04,232 --> 00:17:06,256
So don't worry about how this environment

436
00:17:06,256 --> 00:17:08,270
is actually implemented.

437
00:17:08,272 --> 00:17:11,307
The only thing to worry about is this

438
00:17:11,307 --> 00:17:15,345
function that we are going to use which

439
00:17:15,345 --> 00:17:17,366
is environment step.

440
00:17:17,367 --> 00:17:20,394
So this function will take an action into

441
00:17:20,394 --> 00:17:22,419
it and depending upon the current state

442
00:17:22,419 --> 00:17:25,442
of the environment, it will calculate

443
00:17:25,442 --> 00:17:27,465
what is the most probable next state

444
00:17:27,465 --> 00:17:30,489
given this action from the agent.

445
00:17:30,491 --> 00:17:31,503
So that's the idea.

446
00:17:31,504 --> 00:17:34,534
And then it will also calculate a reward

447
00:17:34,534 --> 00:17:37,564
of some negligible negative value if it

448
00:17:37,564 --> 00:17:40,593
is not the Gold State, and if it is the

449
00:17:40,593 --> 00:17:43,623
Gold State, it will give a reward of ten.

450
00:17:43,623 --> 00:17:43,626
.

451
00:17:43,626 --> 00:17:45,640
And that's how the environment is.

452
00:17:45,641 --> 00:17:47,664
Designed and it will update the current

453
00:17:47,664 --> 00:17:48,678
state to the new state.

454
00:17:48,679 --> 00:17:51,701
And basically what it will return is a

455
00:17:51,701 --> 00:17:53,724
new state, depending upon your action,

456
00:17:53,724 --> 00:17:55,748
the reward for that action and whether it

457
00:17:55,748 --> 00:17:58,770
is an end of the episode and so on.

458
00:17:58,771 --> 00:18:00,736
So this implementation is the standard

459
00:18:00,736 --> 00:18:02,759
OpenAI environment implementation and

460
00:18:02,759 --> 00:18:05,784
this is the environment step function,

461
00:18:05,784 --> 00:18:05,789
right?

462
00:18:06,790 --> 00:18:08,818
So in this grid, for example, if I am

463
00:18:08,818 --> 00:18:11,849
right now in this state and if I take an

464
00:18:11,849 --> 00:18:14,876
action up, so I have four available

465
00:18:14,876 --> 00:18:17,907
actions north, South, east and west.

466
00:18:17,907 --> 00:18:20,935
So if I go on north, then the environment

467
00:18:20,935 --> 00:18:23,960
step will make sure that I am in the

468
00:18:23,960 --> 00:18:25,982
state that is above the state.

469
00:18:25,982 --> 00:18:27,000
And if I go east or west, then I'll stay

470
00:18:27,000 --> 00:18:27,000
here.

471
00:18:28,000 --> 00:18:28,001
Or south.

472
00:18:28,001 --> 00:18:29,002
I'll stay here.

473
00:18:29,002 --> 00:18:31,004
So that's the idea.

474
00:18:32,005 --> 00:18:35,008
And there is an episode length limit that

475
00:18:35,008 --> 00:18:37,010
is eight here, that means that I am

476
00:18:37,010 --> 00:18:40,013
restricting the length of every episode

477
00:18:40,013 --> 00:18:43,016
to be eight, which is the ideal length of

478
00:18:43,016 --> 00:18:45,018
reaching this goal state just to avoid

479
00:18:45,018 --> 00:18:46,019
confusion.

480
00:18:47,019 --> 00:18:48,021
So in this environment, after eight

481
00:18:48,021 --> 00:18:51,024
actions, the environment will terminate.

482
00:18:51,024 --> 00:18:53,026
And if you have to kind of reach this

483
00:18:53,026 --> 00:18:56,029
goal state in the optimal time point so

484
00:18:56,029 --> 00:18:58,031
that's the idea how the environment is

485
00:18:58,031 --> 00:18:59,032
implemented.

486
00:19:00,027 --> 00:19:03,030
Okay, I hope that is clear.

487
00:19:03,030 --> 00:19:06,033
And there are many helpful functions in

488
00:19:06,033 --> 00:19:08,035
this environment, like rendering the

489
00:19:08,035 --> 00:19:11,038
environment, rendering a prior preference

490
00:19:11,038 --> 00:19:13,040
matrix in this environment.

491
00:19:13,040 --> 00:19:16,043
So if you design a prior preference, this

492
00:19:16,043 --> 00:19:18,045
environment can show that in a pictorial

493
00:19:18,045 --> 00:19:21,048
way how your prior preference is that you

494
00:19:21,048 --> 00:19:23,050
will see in the notebook below.

495
00:19:23,050 --> 00:19:25,052
So now it's time that we define a

496
00:19:25,052 --> 00:19:28,055
generative model for the sophisticated

497
00:19:28,055 --> 00:19:29,056
inference agent, right?

498
00:19:31,058 --> 00:19:33,060
Before that, let's define the structure

499
00:19:33,060 --> 00:19:36,063
of the generative model that we want the

500
00:19:36,063 --> 00:19:39,066
agent to have in its mind, which is

501
00:19:39,066 --> 00:19:41,068
tailor made for this particular

502
00:19:41,068 --> 00:19:43,070
environment, right?

503
00:19:43,070 --> 00:19:46,073
So here in this particular grid world

504
00:19:46,073 --> 00:19:50,076
task, we have 25 valid states starting

505
00:19:50,076 --> 00:19:51,078
from this state.

506
00:19:51,078 --> 00:19:55,082
All this black states in this path are

507
00:19:55,082 --> 00:19:56,083
valid states.

508
00:19:56,083 --> 00:19:58,085
So there are 25 valid states.

509
00:19:58,085 --> 00:20:01,082
Then there are four available actions for

510
00:20:01,082 --> 00:20:03,084
the agent, north, Southeast and west.

511
00:20:03,084 --> 00:20:05,086
So this is part of our generative model.

512
00:20:06,087 --> 00:20:09,090
This is also in alignment with the

513
00:20:09,090 --> 00:20:11,092
reality of the grid.

514
00:20:11,092 --> 00:20:13,094
But this is about what the agent has in

515
00:20:13,094 --> 00:20:14,095
its mind, right?

516
00:20:14,095 --> 00:20:18,099
And then the observation is just the

517
00:20:18,099 --> 00:20:19,100
state space.

518
00:20:20,101 --> 00:20:23,104
The problem is fully observable, so there

519
00:20:23,104 --> 00:20:24,105
is no ambiguity there.

520
00:20:24,105 --> 00:20:27,108
Then we define basically the number of

521
00:20:27,108 --> 00:20:28,109
states.

522
00:20:34,115 --> 00:20:36,117
Then we define basically the number of

523
00:20:36,117 --> 00:20:39,120
states, which is a list of your state

524
00:20:39,120 --> 00:20:41,122
space, number of factors, which is now

525
00:20:41,122 --> 00:20:44,124
one because you only have one hidden

526
00:20:44,124 --> 00:20:45,126
state factor here.

527
00:20:45,126 --> 00:20:47,128
Then number of controls, which is going

528
00:20:47,128 --> 00:20:48,129
to be four.

529
00:20:49,130 --> 00:20:51,132
That's four available actions and your

530
00:20:51,132 --> 00:20:53,134
observation space like that.

531
00:20:53,134 --> 00:20:55,136
So this is the structure of your

532
00:20:55,136 --> 00:20:56,137
generative model.

533
00:20:56,137 --> 00:20:59,140
And let's look at the structure of the

534
00:20:59,140 --> 00:21:02,137
parameters now inside a POMDP.

535
00:21:02,137 --> 00:21:06,141
So the first one is the likelihood

536
00:21:06,141 --> 00:21:09,144
function which is often denoted by

537
00:21:09,144 --> 00:21:10,145
capital A.

538
00:21:10,145 --> 00:21:12,147
And here it is a function of how many

539
00:21:12,147 --> 00:21:14,149
observational modalities I have and how

540
00:21:14,149 --> 00:21:16,151
many state modalities I have.

541
00:21:16,151 --> 00:21:17,152
Right?

542
00:21:17,152 --> 00:21:20,155
If I run this cell, yeah, I have to run

543
00:21:20,155 --> 00:21:24,159
the parent cell to make sure everything

544
00:21:24,159 --> 00:21:24,159
works.

545
00:21:25,160 --> 00:21:27,162
So I have rendered the environment the

546
00:21:27,162 --> 00:21:30,165
structure of the generative model.

547
00:21:30,165 --> 00:21:33,168
And here I have the capital A matrix

548
00:21:33,168 --> 00:21:35,170
which has a structure 25 25.

549
00:21:35,170 --> 00:21:38,173
That means that I have 25 states and 25

550
00:21:38,173 --> 00:21:38,173
observations.

551
00:21:39,173 --> 00:21:41,176
And here, because it's fully observable,

552
00:21:41,176 --> 00:21:44,179
I am initializing it as an identity

553
00:21:44,179 --> 00:21:45,180
matrix of size 25.

554
00:21:45,180 --> 00:21:49,184
So that's my likelihood matrix that I'm

555
00:21:49,184 --> 00:21:52,187
initializing for this particular grid

556
00:21:52,187 --> 00:21:53,188
world task.

557
00:21:54,189 --> 00:21:57,192
Then the second element is the transition

558
00:21:57,192 --> 00:21:57,192
matrix.

559
00:21:58,193 --> 00:22:01,190
So please note that I'm using all the

560
00:22:01,190 --> 00:22:04,193
existing Pymdp functionalities to define

561
00:22:04,193 --> 00:22:06,195
a random A matrix and then using an

562
00:22:06,195 --> 00:22:09,198
identity matrix on top of that.

563
00:22:09,198 --> 00:22:12,201
So, yeah, I'm not doing anything new.

564
00:22:12,201 --> 00:22:15,204
Here it is the existing Pymdp

565
00:22:15,204 --> 00:22:16,205
functionality.

566
00:22:16,205 --> 00:22:19,208
Then what I can do now is define the B

567
00:22:19,208 --> 00:22:22,211
matrix, which is also called transition

568
00:22:22,211 --> 00:22:22,211
matrix.

569
00:22:22,211 --> 00:22:24,213
So the transition matrix encodes

570
00:22:24,213 --> 00:22:27,216
transitions like where I'm going to end

571
00:22:27,216 --> 00:22:29,218
up in the future if I start from a

572
00:22:29,218 --> 00:22:31,220
particular state and take an action.

573
00:22:31,220 --> 00:22:33,222
So that's the idea where it depends upon

574
00:22:33,222 --> 00:22:36,225
the number of states, which is the hidden

575
00:22:36,225 --> 00:22:38,227
state modality and number of controls.

576
00:22:38,227 --> 00:22:41,230
So it has the structure of state action

577
00:22:41,230 --> 00:22:43,232
state where if I take an action from a

578
00:22:43,232 --> 00:22:45,234
particular state where I'm going to end

579
00:22:45,234 --> 00:22:45,234
up.

580
00:22:45,234 --> 00:22:48,237
So that is also a future state, right.

581
00:22:48,237 --> 00:22:52,241
So I'm going to initialize it as the true

582
00:22:52,241 --> 00:22:54,243
environment state.

583
00:22:54,243 --> 00:22:55,244
So now this is part of the environment

584
00:22:55,244 --> 00:22:56,245
that I've built.

585
00:22:56,245 --> 00:22:58,247
It will give out the B matrix.

586
00:22:59,248 --> 00:23:01,244
It might be worth it to look at the

587
00:23:01,244 --> 00:23:02,245
structure of this B matrix.

588
00:23:05,248 --> 00:23:08,251
So here we have 25, 25 four.

589
00:23:08,251 --> 00:23:11,254
So that means that if I take an action

590
00:23:11,254 --> 00:23:14,257
from a particular state where I'm going

591
00:23:14,257 --> 00:23:17,260
to enter and we have the true transition

592
00:23:17,260 --> 00:23:19,262
dynamics for this particular grid by

593
00:23:19,262 --> 00:23:20,263
design.

594
00:23:20,263 --> 00:23:23,266
So there is a function called get true B

595
00:23:23,266 --> 00:23:25,268
and that will give us the true b of the

596
00:23:25,268 --> 00:23:28,271
system which the agent can use.

597
00:23:28,271 --> 00:23:28,271
Okay?

598
00:23:28,271 --> 00:23:30,273
So ideally we would want the agent to

599
00:23:30,273 --> 00:23:33,276
learn this, but for the purpose of this

600
00:23:33,276 --> 00:23:35,278
demo we are assuming that the agent

601
00:23:35,278 --> 00:23:37,280
already knows the structure.

602
00:23:37,280 --> 00:23:40,283
And then comes the prior preference,

603
00:23:40,283 --> 00:23:43,286
which is interesting here in the sense

604
00:23:43,286 --> 00:23:47,290
that it is defined as how closer you are

605
00:23:47,290 --> 00:23:48,291
to the gold state.

606
00:23:49,292 --> 00:23:51,294
So if you are at the gold state, then

607
00:23:51,294 --> 00:23:54,297
clearly that is the most sought out state.

608
00:23:54,297 --> 00:23:54,297
.

609
00:23:54,297 --> 00:23:56,298
You prefer that the most.

610
00:23:56,299 --> 00:23:58,301
And how do you prefer the neighboring

611
00:23:58,301 --> 00:23:59,302
states, right?

612
00:23:59,302 --> 00:24:02,299
So that is dependent upon the square root

613
00:24:02,299 --> 00:24:05,302
of the distance or basically the distance

614
00:24:05,302 --> 00:24:07,304
from that particular gold state.

615
00:24:07,304 --> 00:24:11,308
So you define a grid which is eight cross

616
00:24:11,308 --> 00:24:14,311
eight, the same size as this particular

617
00:24:14,311 --> 00:24:16,313
grid world task.

618
00:24:16,313 --> 00:24:20,317
And then we have a method to kind of add

619
00:24:20,317 --> 00:24:23,320
values which is the preference you have

620
00:24:23,320 --> 00:24:25,322
for every state.

621
00:24:25,322 --> 00:24:28,325
And if you render the particular C matrix

622
00:24:28,325 --> 00:24:30,327
you can see the structure which is the

623
00:24:30,327 --> 00:24:33,330
same where this gold state is more

624
00:24:33,330 --> 00:24:35,332
preferred and the surrounding states less

625
00:24:35,332 --> 00:24:37,334
preferred and so on.

626
00:24:37,334 --> 00:24:40,337
So now we have the C matrix also defined

627
00:24:40,337 --> 00:24:42,339
in the classic PMDP way.

628
00:24:43,340 --> 00:24:46,343
And then I initialize that C matrix as

629
00:24:46,343 --> 00:24:49,346
the C matrix we evaluated in the previous

630
00:24:49,346 --> 00:24:51,348
cell which is small C, this particular C

631
00:24:51,348 --> 00:24:52,349
matrix.

632
00:24:52,349 --> 00:24:53,350
Okay?

633
00:24:53,350 --> 00:24:56,353
And then lastly, for the generative model

634
00:24:56,353 --> 00:24:58,355
we have capital D which is your prior

635
00:24:58,355 --> 00:24:59,356
overhead and states.

636
00:25:00,351 --> 00:25:02,353
And for that I'm using a uniform object

637
00:25:02,353 --> 00:25:02,353
array.

638
00:25:03,354 --> 00:25:05,356
So that means that I don't have a prior

639
00:25:05,356 --> 00:25:06,357
of where I am starting.

640
00:25:06,357 --> 00:25:10,361
So let me run the pending cells.

641
00:25:12,363 --> 00:25:14,365
So here the D matrix is a uniform

642
00:25:14,365 --> 00:25:16,367
distribution over hidden states.

643
00:25:16,367 --> 00:25:18,369
I don't know where I'm going to start the

644
00:25:18,369 --> 00:25:19,370
simulations and so on.

645
00:25:19,370 --> 00:25:21,372
So this is the basic structure of the

646
00:25:21,372 --> 00:25:22,373
generative model.

647
00:25:23,374 --> 00:25:26,377
And then we have the agent class which I

648
00:25:26,377 --> 00:25:28,379
want to discuss separately, like the

649
00:25:28,379 --> 00:25:30,381
environment, right?

650
00:25:30,381 --> 00:25:33,384
So given these environment parameters,

651
00:25:33,384 --> 00:25:36,387
how would you expect the agent class to

652
00:25:36,387 --> 00:25:36,387
work?

653
00:25:36,387 --> 00:25:38,389
So where is the agent class?

654
00:25:38,389 --> 00:25:40,391
Inside this folder structure?

655
00:25:41,392 --> 00:25:46,397
Inside the Pymdp module folder we have an

656
00:25:46,397 --> 00:25:50,401
agent Si PY which is basically a class

657
00:25:50,401 --> 00:25:53,404
again and similar to the environment

658
00:25:53,404 --> 00:25:55,406
class here.

659
00:25:55,406 --> 00:25:58,409
Also we have a step function where this

660
00:25:58,409 --> 00:26:02,407
will take an observation to the function

661
00:26:02,407 --> 00:26:05,410
and also a flag whether or not to learn

662
00:26:05,410 --> 00:26:08,413
the environment, which is optional.

663
00:26:09,414 --> 00:26:11,416
So if you disable it, it won't learn the

664
00:26:11,416 --> 00:26:12,417
generative model.

665
00:26:12,417 --> 00:26:14,419
If you enable it, it will update the

666
00:26:14,419 --> 00:26:16,421
parameters of the generative model.

667
00:26:17,421 --> 00:26:19,424
And what basically it does is it will

668
00:26:19,424 --> 00:26:21,426
return an action which is to be taken at

669
00:26:21,426 --> 00:26:23,428
this point of time and the environment

670
00:26:23,428 --> 00:26:26,431
can basically use that action, right?

671
00:26:26,431 --> 00:26:28,433
So in this file we have the agent class

672
00:26:28,433 --> 00:26:30,435
which I will explain in detail.

673
00:26:31,436 --> 00:26:34,439
So I am basically importing that agent

674
00:26:34,439 --> 00:26:38,442
class in this cell and then we are going

675
00:26:38,442 --> 00:26:40,445
to try and reproduce this behavioral

676
00:26:40,445 --> 00:26:43,448
result from the original sophisticated

677
00:26:43,448 --> 00:26:45,450
inference paper.

678
00:26:45,450 --> 00:26:46,451
Okay?

679
00:26:47,451 --> 00:26:48,453
And for that.

680
00:26:49,454 --> 00:26:51,456
So what we expect is that given this

681
00:26:51,456 --> 00:26:53,458
prior preference structure, there are

682
00:26:53,458 --> 00:26:56,461
local maximas in this prior preference

683
00:26:56,461 --> 00:26:57,461
structure.

684
00:26:57,462 --> 00:26:59,464
So if you start from this particular

685
00:26:59,464 --> 00:27:01,460
point, if you do not plan deep enough,

686
00:27:01,460 --> 00:27:04,463
what you will end up is in one of these

687
00:27:04,463 --> 00:27:06,465
local maximas where you don't see that

688
00:27:06,465 --> 00:27:09,468
there is a highly preferred observation,

689
00:27:09,468 --> 00:27:11,470
say four steps down the line.

690
00:27:11,470 --> 00:27:13,472
So if you are in this particular state,

691
00:27:13,472 --> 00:27:15,474
what you will see is this local maxima

692
00:27:15,474 --> 00:27:18,477
and you will go and sit there because the

693
00:27:18,477 --> 00:27:20,479
neighboring states are less preferred and

694
00:27:20,479 --> 00:27:23,482
this state which is more preferred is not

695
00:27:23,482 --> 00:27:25,484
accessible because of the wall or the

696
00:27:25,484 --> 00:27:26,485
wall structure.

697
00:27:26,485 --> 00:27:28,487
So you have to take a turn and pass

698
00:27:28,487 --> 00:27:30,489
through less preferred states and you

699
00:27:30,489 --> 00:27:33,492
need deep planning in order to enable the

700
00:27:33,492 --> 00:27:34,493
agent to do that.

701
00:27:34,493 --> 00:27:37,496
The agent should be able to kind of

702
00:27:37,496 --> 00:27:40,499
simulate four time steps ahead in time to

703
00:27:40,499 --> 00:27:43,502
see that there is this highly rewarding

704
00:27:43,502 --> 00:27:46,504
observation coming to kind of do that

705
00:27:46,504 --> 00:27:46,505
actions.

706
00:27:47,506 --> 00:27:49,508
So that's the point that we are trying to

707
00:27:49,508 --> 00:27:51,510
see in this particular demo.

708
00:27:51,510 --> 00:27:54,513
So for a low planning depth it will

709
00:27:54,513 --> 00:27:57,516
basically get stuck in one of the local

710
00:27:57,516 --> 00:27:59,518
maximas but with sufficient planning

711
00:27:59,518 --> 00:28:02,515
depth it will navigate to the gold state.

712
00:28:02,515 --> 00:28:02,515
.

713
00:28:03,516 --> 00:28:04,517
So that's what we are trying to see,

714
00:28:04,517 --> 00:28:05,518
right?

715
00:28:05,518 --> 00:28:08,521
So we have different planning horizons

716
00:28:08,521 --> 00:28:11,524
and what we are basically doing is give

717
00:28:11,524 --> 00:28:14,527
the agent a generative model which we

718
00:28:14,527 --> 00:28:17,530
right now defined the A matrix, B matrix,

719
00:28:17,530 --> 00:28:19,532
C matrix B matrix.

720
00:28:19,532 --> 00:28:21,534
Then we have the planning horizon of

721
00:28:21,534 --> 00:28:22,535
capital N.

722
00:28:22,535 --> 00:28:24,537
So here I am, iterating over planning

723
00:28:24,537 --> 00:28:25,538
depth.

724
00:28:25,538 --> 00:28:28,540
So N will be one, three and four for the

725
00:28:28,540 --> 00:28:28,541
loop.

726
00:28:28,541 --> 00:28:30,543
Then we have action precision, which is

727
00:28:30,543 --> 00:28:32,545
often denoted by alpha in active

728
00:28:32,545 --> 00:28:33,546
inference literature.

729
00:28:33,546 --> 00:28:36,549
So that determines which action is to be

730
00:28:36,549 --> 00:28:36,549
taken.

731
00:28:37,549 --> 00:28:39,552
So a highly precise action precision

732
00:28:39,552 --> 00:28:42,555
means that it will stick to the action

733
00:28:42,555 --> 00:28:45,558
with the lowest expected free energy.

734
00:28:45,558 --> 00:28:47,560
But a lower action precision is kind of

735
00:28:47,560 --> 00:28:50,563
probabilistic where it will also consider

736
00:28:50,563 --> 00:28:51,564
other actions.

737
00:28:51,564 --> 00:28:53,566
Then we have a planning precision which

738
00:28:53,566 --> 00:28:55,568
is part of the planning function we'll

739
00:28:55,568 --> 00:28:57,570
discuss, which is often denoted in the

740
00:28:57,570 --> 00:28:58,571
literature as gamma.

741
00:28:58,571 --> 00:29:01,568
Then we also have a search threshold

742
00:29:01,568 --> 00:29:03,570
which is extremely important for

743
00:29:03,570 --> 00:29:05,572
sophisticated inference because as we saw,

744
00:29:05,572 --> 00:29:08,575
, sophisticated inference is a tree

745
00:29:08,575 --> 00:29:10,577
search and tree search is bad in the

746
00:29:10,577 --> 00:29:12,579
sense that it can have a lot of

747
00:29:12,579 --> 00:29:13,580
computations.

748
00:29:13,580 --> 00:29:16,583
But you have to define a threshold to

749
00:29:16,583 --> 00:29:18,585
kind of ignore many possibilities to make

750
00:29:18,585 --> 00:29:19,586
it work.

751
00:29:19,586 --> 00:29:21,588
And that's the idea that we will also

752
00:29:21,588 --> 00:29:21,588
discuss.

753
00:29:22,589 --> 00:29:26,593
So just a preview before we go to the

754
00:29:26,593 --> 00:29:28,595
agent class.

755
00:29:28,595 --> 00:29:32,598
What we are trying to do is in a loop we

756
00:29:32,598 --> 00:29:35,602
are going to call the agent step and

757
00:29:35,602 --> 00:29:37,604
environment step in series.

758
00:29:37,604 --> 00:29:40,606
So the agent will see an observation, it

759
00:29:40,606 --> 00:29:42,609
will take an action, that action will go

760
00:29:42,609 --> 00:29:44,611
into the environment, the environment

761
00:29:44,611 --> 00:29:46,613
will give it back new observations and

762
00:29:46,613 --> 00:29:48,615
this loop continues.

763
00:29:48,615 --> 00:29:51,618
And we want to see over time how this

764
00:29:51,618 --> 00:29:54,621
loop evolves into a purposeful behavior

765
00:29:54,621 --> 00:29:57,624
and if the agent at all is capable for

766
00:29:57,624 --> 00:29:58,625
that.

767
00:29:58,625 --> 00:30:01,622
So before I reveal the results, let's

768
00:30:01,622 --> 00:30:03,624
discuss the agent class.

769
00:30:03,624 --> 00:30:06,627
So in order to give an action, when an

770
00:30:06,627 --> 00:30:09,630
observation is given, the agent should

771
00:30:09,630 --> 00:30:12,633
have the planning and so on, right?

772
00:30:12,633 --> 00:30:15,636
So here is the agent, the sophisticated

773
00:30:15,636 --> 00:30:19,640
inference agent, where we are actually

774
00:30:19,640 --> 00:30:22,643
using the existing Pymdp agent for some

775
00:30:22,643 --> 00:30:23,644
functionalities.

776
00:30:24,645 --> 00:30:28,649
So in Pymdp we already have really well

777
00:30:28,649 --> 00:30:32,653
written functions for perception and

778
00:30:32,653 --> 00:30:33,654
learning.

779
00:30:33,654 --> 00:30:36,657
So the only thing we want to kind of

780
00:30:36,657 --> 00:30:38,659
replace is how the agent is doing

781
00:30:38,659 --> 00:30:40,661
planning and how the agent is taking

782
00:30:40,661 --> 00:30:42,663
decisions over policies, right?

783
00:30:42,663 --> 00:30:48,669
So here we are using that parent agent

784
00:30:48,669 --> 00:30:49,670
class.

785
00:30:50,671 --> 00:30:53,674
So from Pymdp agent we are importing that

786
00:30:53,674 --> 00:30:57,678
agent class which is sitting next to the

787
00:30:57,678 --> 00:31:00,675
Si agent that we are discussing now.

788
00:31:01,676 --> 00:31:04,679
And basically we are taking in the

789
00:31:04,679 --> 00:31:07,682
generative model structure from the main

790
00:31:07,682 --> 00:31:10,685
program for this class to work which is C

791
00:31:10,685 --> 00:31:13,688
and D and all the precisions and

792
00:31:13,688 --> 00:31:16,691
threshold parameter I mentioned, then it

793
00:31:16,691 --> 00:31:19,694
is kind of normalizing the prior

794
00:31:19,694 --> 00:31:22,697
preference that we mentioned in the main

795
00:31:22,697 --> 00:31:23,698
program.

796
00:31:23,698 --> 00:31:26,701
So here if I look at how C is, the

797
00:31:26,701 --> 00:31:29,704
structure of C is defined in terms of

798
00:31:29,704 --> 00:31:32,707
numbers and the prior preference is often

799
00:31:32,707 --> 00:31:34,709
interpreted or it should be a

800
00:31:34,709 --> 00:31:37,712
probabilistic distribution for the

801
00:31:37,712 --> 00:31:39,714
computations to work, right?

802
00:31:39,714 --> 00:31:42,717
So we are going to normalize it as a

803
00:31:42,717 --> 00:31:46,721
probabilistic distribution rather than

804
00:31:46,721 --> 00:31:49,724
having numbers that don't add up to one.

805
00:31:50,724 --> 00:31:51,726
So that's what's happening here.

806
00:31:51,726 --> 00:31:53,728
We are using Softmax to do that.

807
00:31:53,728 --> 00:31:56,731
Then what we are doing is we are

808
00:31:56,731 --> 00:31:59,734
initializing the existing PMDP agent with

809
00:31:59,734 --> 00:32:02,731
these generative model parameters and

810
00:32:02,731 --> 00:32:05,734
what we are intending to do is write a

811
00:32:05,734 --> 00:32:08,737
planning function for a given planning

812
00:32:08,737 --> 00:32:12,741
horizon and a given threshold for trees.

813
00:32:12,741 --> 00:32:15,744
Okay, so there are three functions in

814
00:32:15,744 --> 00:32:17,746
this agent class.

815
00:32:17,746 --> 00:32:19,748
One is a helper function for planning

816
00:32:19,748 --> 00:32:21,750
which we will discuss now.

817
00:32:21,750 --> 00:32:24,753
Then there is a planning function itself

818
00:32:24,753 --> 00:32:26,755
which is going to do planning using tree

819
00:32:26,755 --> 00:32:27,756
search.

820
00:32:27,756 --> 00:32:30,759
And then because it is a recursive tree

821
00:32:30,759 --> 00:32:32,761
search, we are going to need an

822
00:32:32,761 --> 00:32:35,764
additional function that implements that

823
00:32:35,764 --> 00:32:37,766
recursive evaluation where we are going

824
00:32:37,766 --> 00:32:40,769
to call this function called forward

825
00:32:40,769 --> 00:32:42,771
search inside the function itself.

826
00:32:42,771 --> 00:32:45,774
So we are calling this function inside

827
00:32:45,774 --> 00:32:46,775
this function.

828
00:32:46,775 --> 00:32:48,777
So that is to calculate expected free

829
00:32:48,777 --> 00:32:50,779
energy for the next step and it will call

830
00:32:50,779 --> 00:32:52,781
it again for the next step till our

831
00:32:52,781 --> 00:32:53,782
planning horizon.

832
00:32:53,782 --> 00:32:56,785
So that's the idea of recursive looping

833
00:32:56,785 --> 00:32:59,788
and finally it will return the expected

834
00:32:59,788 --> 00:33:01,784
free energy for all actions given an

835
00:33:01,784 --> 00:33:04,787
observations and then we just implement

836
00:33:04,787 --> 00:33:06,789
the step function where it is written

837
00:33:06,789 --> 00:33:09,792
sequentially what to do given an

838
00:33:09,792 --> 00:33:10,793
observation.

839
00:33:12,795 --> 00:33:17,800
So going back to the demo here we have

840
00:33:17,800 --> 00:33:21,804
this first idea where you get an

841
00:33:21,804 --> 00:33:27,810
observation and it gives out an action.

842
00:33:27,810 --> 00:33:29,812
So let's go to the agent step function

843
00:33:29,812 --> 00:33:31,814
and imagine what happens.

844
00:33:32,815 --> 00:33:35,818
So if it is time t equal to zero or in

845
00:33:35,818 --> 00:33:38,821
the beginning of the experiment, what it

846
00:33:38,821 --> 00:33:39,822
is ideally.

847
00:33:39,822 --> 00:33:42,825
Supposed to do in the first place is

848
00:33:42,825 --> 00:33:44,827
infer that state using its observation.

849
00:33:44,827 --> 00:33:47,830
So what we are giving it is an

850
00:33:47,830 --> 00:33:50,833
observation and using the modules for

851
00:33:50,833 --> 00:33:53,836
inference, it's going to come up with a

852
00:33:53,836 --> 00:33:57,840
belief QS, which is a belief about states.

853
00:33:57,840 --> 00:33:57,840
.

854
00:33:57,840 --> 00:34:01,838
Okay, so self dot QS is the belief inside

855
00:34:01,838 --> 00:34:04,841
the agent and once it has a belief about

856
00:34:04,841 --> 00:34:07,844
where it is right now, it can implement

857
00:34:07,844 --> 00:34:10,847
plan research which is do planning for

858
00:34:10,847 --> 00:34:13,850
this particular belief of hidden state

859
00:34:13,850 --> 00:34:14,851
right now.

860
00:34:14,851 --> 00:34:17,854
And once it has done planning, it can

861
00:34:17,854 --> 00:34:20,857
take decision using the sample action

862
00:34:20,857 --> 00:34:23,860
function in Pymdp and basically return

863
00:34:23,860 --> 00:34:24,861
that action.

864
00:34:25,862 --> 00:34:27,864
And for every other time steps, the

865
00:34:27,864 --> 00:34:29,866
sequence remains the same.

866
00:34:29,866 --> 00:34:31,868
But it is also learning about the

867
00:34:31,868 --> 00:34:34,871
structure if you enable learning in your

868
00:34:34,871 --> 00:34:35,872
agent class.

869
00:34:36,873 --> 00:34:38,875
So that's the step function.

870
00:34:39,876 --> 00:34:42,879
But in order to do planning, what it does

871
00:34:42,879 --> 00:34:45,882
is it kind of reorganizes the generative

872
00:34:45,882 --> 00:34:47,884
model structure for any number of hidden

873
00:34:47,884 --> 00:34:50,887
state modalities and any number of

874
00:34:50,887 --> 00:34:51,888
observation modalities.

875
00:34:52,889 --> 00:34:55,892
So to discuss how melting works, I would

876
00:34:55,892 --> 00:34:59,896
like to talk about the new A matrices and

877
00:34:59,896 --> 00:35:02,893
B matrices it evaluates for implementing

878
00:35:02,893 --> 00:35:06,897
that planning and let's understand that.

879
00:35:07,898 --> 00:35:11,902
So let's go back to the original hidden

880
00:35:11,902 --> 00:35:13,904
state factors.

881
00:35:13,904 --> 00:35:15,906
So here we only have one hidden state

882
00:35:15,906 --> 00:35:16,907
factor.

883
00:35:16,907 --> 00:35:18,909
So that's why it is B zero, right?

884
00:35:18,909 --> 00:35:21,912
And B one does not exist because we only

885
00:35:21,912 --> 00:35:24,915
have one hidden state factor.

886
00:35:24,915 --> 00:35:27,918
But imagine that if I have two hidden

887
00:35:27,918 --> 00:35:30,920
state factors with the same size maybe.

888
00:35:30,921 --> 00:35:34,924
So here it could be a location and maybe

889
00:35:34,924 --> 00:35:37,928
something else inside the agent's mind.

890
00:35:37,928 --> 00:35:40,931
And we should also have controls for

891
00:35:40,931 --> 00:35:42,933
these two hidden state factors.

892
00:35:42,933 --> 00:35:46,937
Just like so there should be control for

893
00:35:46,937 --> 00:35:49,940
every hidden state factor if you're

894
00:35:49,940 --> 00:35:52,943
familiar with active inference idea and

895
00:35:52,943 --> 00:35:56,947
then maybe the observation space is also

896
00:35:56,947 --> 00:36:00,945
directly observing these two hidden state

897
00:36:00,945 --> 00:36:01,946
factors.

898
00:36:01,946 --> 00:36:04,949
Okay, so this is a new Generative Model

899
00:36:04,949 --> 00:36:06,951
structure with multiple hidden states and

900
00:36:06,951 --> 00:36:08,953
multiple observation modalities.

901
00:36:09,953 --> 00:36:11,956
And right away you can see that the

902
00:36:11,956 --> 00:36:13,958
dimensionalities of your parameters

903
00:36:13,958 --> 00:36:14,959
change.

904
00:36:14,959 --> 00:36:19,964
You have 25 observations coming from 25

905
00:36:19,964 --> 00:36:22,967
times 25 hidden states.

906
00:36:22,967 --> 00:36:24,969
And if you look at the structure of the

907
00:36:24,969 --> 00:36:26,971
first observation modality, it's the same.

908
00:36:26,971 --> 00:36:26,971
.

909
00:36:26,971 --> 00:36:30,975
But what we want is a new matrix where it'

910
00:36:30,975 --> 00:36:32,977
's going to be 25 with not two hidden

911
00:36:32,977 --> 00:36:35,980
states but just one hidden state.

912
00:36:35,980 --> 00:36:38,983
It's only a reorganization of the

913
00:36:38,983 --> 00:36:41,986
generative model, but computations

914
00:36:41,986 --> 00:36:44,989
essentially remains the same.

915
00:36:44,989 --> 00:36:47,992
So that's what this helper function is

916
00:36:47,992 --> 00:36:50,995
trying to do, which will make things

917
00:36:50,995 --> 00:36:53,998
easier for us when we have multiple

918
00:36:53,998 --> 00:36:55,000
hidden state modalities.

919
00:36:55,000 --> 00:36:58,003
So if you have multiple hidden state

920
00:36:58,003 --> 00:37:01,000
modalities, we are going to compute how

921
00:37:01,000 --> 00:37:04,003
many total states you have, which is the

922
00:37:04,003 --> 00:37:07,006
multiplication of number of hidden states

923
00:37:07,006 --> 00:37:08,007
in each modality.

924
00:37:08,007 --> 00:37:09,008
Okay?

925
00:37:09,008 --> 00:37:11,010
So if you have 25 hidden states in one

926
00:37:11,010 --> 00:37:13,012
modality and 25 hidden states in the

927
00:37:13,012 --> 00:37:16,015
other modality, you're going to have 625

928
00:37:16,015 --> 00:37:18,017
total number of states.

929
00:37:18,017 --> 00:37:21,020
And if you have four actions each in each

930
00:37:21,020 --> 00:37:24,023
modalities, then you have total of 16

931
00:37:24,023 --> 00:37:26,025
actions which is nothing but the

932
00:37:26,025 --> 00:37:29,028
combination of these four actions each in

933
00:37:29,028 --> 00:37:30,029
the modalities.

934
00:37:30,029 --> 00:37:32,031
So you're going to have four times 416

935
00:37:32,031 --> 00:37:35,033
actions if you have two modalities.

936
00:37:35,034 --> 00:37:37,036
And it's basically going to build a

937
00:37:37,036 --> 00:37:41,039
generative model that has the same model

938
00:37:41,039 --> 00:37:43,042
parameters but just with a different

939
00:37:43,042 --> 00:37:46,045
dimension structure so that it's easier

940
00:37:46,045 --> 00:37:49,048
for us to calculate the expected.

941
00:37:50,049 --> 00:37:53,052
So now we have a new A and new B and a

942
00:37:53,052 --> 00:37:57,056
new belief which is nothing but tensor

943
00:37:57,056 --> 00:38:00,053
products of existing parameters and

944
00:38:00,053 --> 00:38:01,054
beliefs.

945
00:38:01,054 --> 00:38:05,058
It's nothing but a new big matrix and

946
00:38:05,058 --> 00:38:06,059
nothing else.

947
00:38:06,059 --> 00:38:07,060
Okay?

948
00:38:08,061 --> 00:38:09,062
It's not a change, it's just a

949
00:38:09,062 --> 00:38:11,064
transformation of structure.

950
00:38:11,064 --> 00:38:14,067
And given this A, B and Q, we are going

951
00:38:14,067 --> 00:38:17,070
to predict what's going to happen in the

952
00:38:17,070 --> 00:38:20,073
future and evaluate the expected free

953
00:38:20,073 --> 00:38:22,075
energies for them.

954
00:38:22,075 --> 00:38:25,078
So in order to do planning, so that's the

955
00:38:25,078 --> 00:38:26,079
second function.

956
00:38:27,080 --> 00:38:29,082
What we are going to do is first call the

957
00:38:29,082 --> 00:38:32,085
first function which will do the melting

958
00:38:32,085 --> 00:38:35,088
for us and set up the generative model in

959
00:38:35,088 --> 00:38:37,090
good dimensions, easy to compute.

960
00:38:38,090 --> 00:38:40,093
Then we have the expected free energy

961
00:38:40,093 --> 00:38:43,096
itself for all the actions and then we

962
00:38:43,096 --> 00:38:46,099
have the probability that depends upon

963
00:38:46,099 --> 00:38:49,102
this expected free energy for these

964
00:38:49,102 --> 00:38:50,103
actions.

965
00:38:50,103 --> 00:38:52,105
So why is it just the actions and not the

966
00:38:52,105 --> 00:38:53,106
observations?

967
00:38:53,106 --> 00:38:56,109
Because here we are going to evaluate

968
00:38:56,109 --> 00:38:59,112
expected free energy of actions for the

969
00:38:59,112 --> 00:39:01,108
given observations, right?

970
00:39:01,108 --> 00:39:05,112
So let me go back to the slides and

971
00:39:05,112 --> 00:39:10,117
discuss this pictorially to make things

972
00:39:10,117 --> 00:39:11,118
easier.

973
00:39:11,118 --> 00:39:13,120
So here we have the grid and we have the

974
00:39:13,120 --> 00:39:14,121
prior footprints.

975
00:39:14,121 --> 00:39:17,124
And what we are trying to implement is

976
00:39:17,124 --> 00:39:20,127
that if you observe some observation at

977
00:39:20,127 --> 00:39:22,129
time T, then you're going to consider the

978
00:39:22,129 --> 00:39:25,132
consequences of your actions given that

979
00:39:25,132 --> 00:39:28,135
observation, because you can predict what'

980
00:39:28,135 --> 00:39:29,136
's going to happen.

981
00:39:29,136 --> 00:39:32,138
Because in your generative model you have

982
00:39:32,138 --> 00:39:34,140
the transition dynamics that will tell

983
00:39:34,140 --> 00:39:35,142
you, given this state, if I take this

984
00:39:35,142 --> 00:39:37,144
action, where I'm going to end up,

985
00:39:37,144 --> 00:39:38,145
right?

986
00:39:38,145 --> 00:39:40,147
So that's basically predicting what's

987
00:39:40,147 --> 00:39:43,150
going to happen in the future and you're

988
00:39:43,150 --> 00:39:46,153
right now considering the consequence of

989
00:39:46,153 --> 00:39:49,156
available actions in your arsenal.

990
00:39:49,156 --> 00:39:51,158
And then if you take an action, then you

991
00:39:51,158 --> 00:39:54,161
can predict what's going to happen in the

992
00:39:54,161 --> 00:39:56,163
next time step as a new observation,

993
00:39:56,163 --> 00:39:57,164
right?

994
00:39:57,164 --> 00:39:59,166
So you have a probability distribution

995
00:39:59,166 --> 00:40:02,163
that tells you that say this observation

996
00:40:02,163 --> 00:40:04,165
is the most likely and the other

997
00:40:04,165 --> 00:40:06,167
observations are not really likely.

998
00:40:06,167 --> 00:40:08,169
Then what you will do is you do this

999
00:40:08,169 --> 00:40:11,171
again, you consider the consequence of

1000
00:40:11,171 --> 00:40:13,174
doing your actions from that particular

1001
00:40:13,174 --> 00:40:15,176
observation and this goes on in your

1002
00:40:15,176 --> 00:40:17,178
planning depth, right?

1003
00:40:17,178 --> 00:40:19,180
So this can be thought of as maybe you

1004
00:40:19,180 --> 00:40:20,181
want to go to the gym.

1005
00:40:20,181 --> 00:40:23,184
Then you are going to consider all the

1006
00:40:23,184 --> 00:40:24,185
consequences.

1007
00:40:24,185 --> 00:40:26,187
What will happen if I wear my shoes, if I

1008
00:40:26,187 --> 00:40:28,189
don't wear my shoes, if I go in my car,

1009
00:40:28,189 --> 00:40:31,192
if I don't go in my car, then you realize

1010
00:40:31,192 --> 00:40:33,194
that, okay, I have to wear my shoes.

1011
00:40:33,194 --> 00:40:36,197
Then you consider the consequence that I'

1012
00:40:36,197 --> 00:40:38,199
'm now ready to go to the gym and me

1013
00:40:38,199 --> 00:40:41,202
going to the gym will end up me being in

1014
00:40:41,202 --> 00:40:41,202
the gym.

1015
00:40:41,202 --> 00:40:43,204
So that's the idea.

1016
00:40:43,204 --> 00:40:45,206
You consider the consequence of where you

1017
00:40:45,206 --> 00:40:48,209
are right now and you can go as much as

1018
00:40:48,209 --> 00:40:49,210
you want, right.

1019
00:40:49,210 --> 00:40:50,211
You can predict.

1020
00:40:50,211 --> 00:40:52,213
So in a game of chess, you might be in a

1021
00:40:52,213 --> 00:40:53,214
particular state.

1022
00:40:53,214 --> 00:40:56,217
You consider your consequences, you see

1023
00:40:56,217 --> 00:40:59,219
the future, you consider consequences

1024
00:40:59,219 --> 00:41:02,217
from that future, and you can go as deep

1025
00:41:02,217 --> 00:41:04,219
as you want depending upon your

1026
00:41:04,219 --> 00:41:06,221
computational abilities.

1027
00:41:06,221 --> 00:41:07,221
Right.

1028
00:41:07,222 --> 00:41:10,225
So that's what you're trying to implement

1029
00:41:10,225 --> 00:41:12,227
in this agent class where we are

1030
00:41:12,227 --> 00:41:14,229
considering consequences?

1031
00:41:17,232 --> 00:41:18,233
Yeah.

1032
00:41:18,233 --> 00:41:21,236
So for every modality we are going to

1033
00:41:21,236 --> 00:41:24,239
consider the expected free energy for

1034
00:41:24,239 --> 00:41:25,240
actions.

1035
00:41:25,240 --> 00:41:28,243
And this will basically call the next

1036
00:41:28,243 --> 00:41:30,245
function, which is forward search.

1037
00:41:30,245 --> 00:41:33,248
So forward search is implementing the

1038
00:41:33,248 --> 00:41:36,251
thing that I just mentioned, considering

1039
00:41:36,251 --> 00:41:37,252
consequences.

1040
00:41:37,252 --> 00:41:39,254
And in forward search, what you are

1041
00:41:39,254 --> 00:41:42,257
basically doing is for every action.

1042
00:41:42,257 --> 00:41:45,260
So in line 149, I have a loop that goes

1043
00:41:45,260 --> 00:41:46,261
over every action.

1044
00:41:47,262 --> 00:41:49,264
Then I'm going to consider the posterior

1045
00:41:49,264 --> 00:41:52,267
or the consequences of all those actions.

1046
00:41:52,267 --> 00:41:52,267
.

1047
00:41:52,267 --> 00:41:55,270
I use my transition probabilities to

1048
00:41:55,270 --> 00:41:57,272
evaluate that consequences.

1049
00:41:57,272 --> 00:41:59,274
Then I'm going to predict the

1050
00:41:59,274 --> 00:42:01,270
observations, because my prior

1051
00:42:01,270 --> 00:42:03,272
preferences are defined in terms of

1052
00:42:03,272 --> 00:42:04,273
observations.

1053
00:42:04,273 --> 00:42:07,276
I'm going to predict my observations and

1054
00:42:07,276 --> 00:42:10,279
then evaluate the expected free energy,

1055
00:42:10,279 --> 00:42:13,282
which is the sum of risk and ambiguity.

1056
00:42:14,282 --> 00:42:14,283
Okay?

1057
00:42:14,283 --> 00:42:16,285
I hope that makes sense.

1058
00:42:16,285 --> 00:42:18,287
Like here, you have considered the

1059
00:42:18,287 --> 00:42:21,290
consequence, which is consequence of

1060
00:42:21,290 --> 00:42:24,293
future, which is post or posterior.

1061
00:42:25,294 --> 00:42:27,296
And you're basically evaluating how good

1062
00:42:27,296 --> 00:42:29,298
that posterior is depending upon your

1063
00:42:29,298 --> 00:42:30,299
expected free energy.

1064
00:42:30,299 --> 00:42:33,301
And that becomes the expected free energy

1065
00:42:33,301 --> 00:42:34,303
for that particular action.

1066
00:42:34,303 --> 00:42:36,305
And you do this for all the actions.

1067
00:42:37,306 --> 00:42:38,306
Okay.

1068
00:42:38,307 --> 00:42:40,309
And why this is powerful?

1069
00:42:40,309 --> 00:42:43,312
It is because you can go as deep as you

1070
00:42:43,312 --> 00:42:43,312
want.

1071
00:42:43,312 --> 00:42:47,316
So here in the next step, you go to this

1072
00:42:47,316 --> 00:42:50,319
loop where you will check if I am

1073
00:42:50,319 --> 00:42:53,322
crossing my deep planning or the depth of

1074
00:42:53,322 --> 00:42:54,323
planning.

1075
00:42:54,323 --> 00:42:56,325
And then you are doing basically the same.

1076
00:42:56,325 --> 00:42:56,325
.

1077
00:42:56,325 --> 00:42:58,327
Given that posterior, what is the

1078
00:42:58,327 --> 00:43:00,323
consequence of the actions of that

1079
00:43:00,323 --> 00:43:02,325
particular posterior.

1080
00:43:02,325 --> 00:43:05,328
So here for considering that we are again

1081
00:43:05,328 --> 00:43:07,330
calling the parent function.

1082
00:43:07,330 --> 00:43:09,332
So the same function, forward search, to

1083
00:43:09,332 --> 00:43:11,334
consider consequences of those

1084
00:43:11,334 --> 00:43:13,336
combinations and it will basically come

1085
00:43:13,336 --> 00:43:15,338
back and add up to your expected free

1086
00:43:15,338 --> 00:43:16,339
energy.

1087
00:43:16,339 --> 00:43:19,342
So what happens over this sequence is

1088
00:43:19,342 --> 00:43:23,346
that you consider some or all future

1089
00:43:23,346 --> 00:43:26,349
consequences and then all that values

1090
00:43:26,349 --> 00:43:29,352
will trickle up your tree.

1091
00:43:29,352 --> 00:43:32,355
And that sum of the expected free energy

1092
00:43:32,355 --> 00:43:34,357
will tell you which action is good or

1093
00:43:34,357 --> 00:43:36,359
which action is bad, which you can take

1094
00:43:36,359 --> 00:43:38,361
to kind of see your preferred

1095
00:43:38,361 --> 00:43:39,362
observations.

1096
00:43:40,363 --> 00:43:42,365
So that's the idea of implementing tree

1097
00:43:42,365 --> 00:43:43,366
search.

1098
00:43:43,366 --> 00:43:47,370
And I will also talk about the importance

1099
00:43:47,370 --> 00:43:51,373
of this threshold here, which makes this

1100
00:43:51,373 --> 00:43:52,375
algorithm possible.

1101
00:43:53,376 --> 00:43:55,378
So without this threshold, this algorithm

1102
00:43:55,378 --> 00:43:56,379
will not work.

1103
00:43:56,379 --> 00:43:59,382
I will explicitly talk about why that is

1104
00:43:59,382 --> 00:44:00,377
the case.

1105
00:44:00,377 --> 00:44:03,380
And then once you evaluate the expected

1106
00:44:03,380 --> 00:44:06,382
free energy for all available actions,

1107
00:44:06,382 --> 00:44:08,385
given the present state, you can

1108
00:44:08,385 --> 00:44:10,387
basically compute what you call the

1109
00:44:10,387 --> 00:44:12,389
action distribution.

1110
00:44:12,389 --> 00:44:15,392
That is how probable is my action or how

1111
00:44:15,392 --> 00:44:17,394
I should take my action.

1112
00:44:18,395 --> 00:44:21,398
And we also have this action precision

1113
00:44:21,398 --> 00:44:22,399
parameter alpha.

1114
00:44:22,399 --> 00:44:24,401
So if alpha is very high then it

1115
00:44:24,401 --> 00:44:27,403
basically is a highly skewed distribution

1116
00:44:27,403 --> 00:44:29,406
where you will always choose the action

1117
00:44:29,406 --> 00:44:31,408
that minimizes expected free energy.

1118
00:44:32,409 --> 00:44:35,412
If alpha is really low then it's going to

1119
00:44:35,412 --> 00:44:37,414
be a more sparse or spread out

1120
00:44:37,414 --> 00:44:38,415
distribution.

1121
00:44:38,415 --> 00:44:41,418
And then you can use this action

1122
00:44:41,418 --> 00:44:45,422
distribution to sample actions in the

1123
00:44:45,422 --> 00:44:47,424
agent environment loop.

1124
00:44:48,424 --> 00:44:50,427
We just finished doing planning and

1125
00:44:50,427 --> 00:44:53,430
computing that action distribution.

1126
00:44:53,430 --> 00:44:55,432
Then using that action distribution, you

1127
00:44:55,432 --> 00:44:57,434
can sample an action from your policy

1128
00:44:57,434 --> 00:44:58,435
space.

1129
00:44:58,435 --> 00:45:00,431
So let's look at the policy space in this

1130
00:45:00,431 --> 00:45:01,432
generative model.

1131
00:45:02,433 --> 00:45:06,437
So I'm switching back to the original

1132
00:45:06,437 --> 00:45:10,441
generative model with one hidden state

1133
00:45:10,441 --> 00:45:14,445
factor and let's do planning and maybe

1134
00:45:14,445 --> 00:45:17,448
initialize this agent.

1135
00:45:18,449 --> 00:45:21,452
I just want to initialize this agent to

1136
00:45:21,452 --> 00:45:23,454
see the policy space, not run the loop.

1137
00:45:24,455 --> 00:45:29,460
So I initialize this agent, say for

1138
00:45:29,460 --> 00:45:33,464
planning depth of one.

1139
00:45:33,464 --> 00:45:39,469
And if I look at agent policies, I can

1140
00:45:39,469 --> 00:45:44,475
see that I have basically four available

1141
00:45:44,475 --> 00:45:50,481
actions which is north, south, east and

1142
00:45:50,481 --> 00:45:51,482
west.

1143
00:45:51,482 --> 00:45:54,485
And if I have an action distribution, it

1144
00:45:54,485 --> 00:45:56,487
will tell me how probable is to take that

1145
00:45:56,487 --> 00:45:57,488
action.

1146
00:45:57,488 --> 00:46:01,486
So if I look at agent UPI, okay, so this

1147
00:46:01,486 --> 00:46:06,491
is not defined because I have not done

1148
00:46:06,491 --> 00:46:10,495
planning, but I can do planning and then

1149
00:46:10,495 --> 00:46:12,497
it will be defined.

1150
00:46:18,503 --> 00:46:20,505
Yeah, so I implemented planning with

1151
00:46:20,505 --> 00:46:22,507
research and then now I have an action

1152
00:46:22,507 --> 00:46:23,508
distribution.

1153
00:46:23,508 --> 00:46:26,511
So for this particular scenario, I am

1154
00:46:26,511 --> 00:46:28,513
going to take my third action the most

1155
00:46:28,513 --> 00:46:30,515
which is zero point 99.

1156
00:46:30,515 --> 00:46:33,518
Basically that's the probability which is

1157
00:46:33,518 --> 00:46:35,520
north, south and east in this particular

1158
00:46:35,520 --> 00:46:36,521
case.

1159
00:46:36,521 --> 00:46:39,524
I just wanted to kind of familiarize you

1160
00:46:39,524 --> 00:46:42,527
with the matrices, but we are now going

1161
00:46:42,527 --> 00:46:45,530
to see the agent environment loop in

1162
00:46:45,530 --> 00:46:46,531
action.

1163
00:46:48,533 --> 00:46:51,536
Now you can sample an action from the

1164
00:46:51,536 --> 00:46:54,538
sample action function and then implement

1165
00:46:54,538 --> 00:46:56,541
learning which is using the standard

1166
00:46:56,541 --> 00:46:58,543
Pymdp way where I will update my

1167
00:46:58,543 --> 00:47:00,539
transition dynamics and likelihood

1168
00:47:00,539 --> 00:47:03,542
dynamics depending upon what I see and

1169
00:47:03,542 --> 00:47:05,544
what's my belief and so on.

1170
00:47:05,544 --> 00:47:08,547
So my emphasis is on the decision making

1171
00:47:08,547 --> 00:47:09,548
part.

1172
00:47:18,556 --> 00:47:20,559
So once you sample an action, then that

1173
00:47:20,559 --> 00:47:23,562
action basically goes back to the

1174
00:47:23,562 --> 00:47:24,563
environment.

1175
00:47:24,563 --> 00:47:27,566
Okay, so now let us implement this for a

1176
00:47:27,566 --> 00:47:31,570
planning depth of one and see how the

1177
00:47:31,570 --> 00:47:32,571
agent behaves.

1178
00:47:32,571 --> 00:47:35,574
So here if it is a planning depth of one,

1179
00:47:35,574 --> 00:47:37,576
then that means that the agent is only

1180
00:47:37,576 --> 00:47:39,578
considering the consequence of one time

1181
00:47:39,578 --> 00:47:42,581
step ahead, just seeing the immediate

1182
00:47:42,581 --> 00:47:44,583
future for doing planning, right?

1183
00:47:44,583 --> 00:47:47,586
So I'm giving the planning depth of one.

1184
00:47:47,586 --> 00:47:51,590
I'm resetting the environment where the

1185
00:47:51,590 --> 00:47:55,594
agent is going to start from that initial

1186
00:47:55,594 --> 00:47:58,597
start state, and in the loop, it's going

1187
00:47:58,597 --> 00:48:02,595
to get that observation, take that action,

1188
00:48:02,595 --> 00:48:06,599
, give back an action, and we will look

1189
00:48:06,599 --> 00:48:09,602
at the action probabilities.

1190
00:48:09,602 --> 00:48:11,604
And also we will give that action back to

1191
00:48:11,604 --> 00:48:13,606
the environment, get back the observation,

1192
00:48:13,606 --> 00:48:15,608
, and this loop continues till the

1193
00:48:15,608 --> 00:48:16,609
episode is terminated.

1194
00:48:16,609 --> 00:48:19,612
And I have set the episode length to be

1195
00:48:19,612 --> 00:48:21,614
eight, just to see the outcome of eight

1196
00:48:21,614 --> 00:48:22,615
action.

1197
00:48:22,615 --> 00:48:25,618
So when we run this loop, these matrices

1198
00:48:25,618 --> 00:48:28,621
are nothing but the action distributions

1199
00:48:28,621 --> 00:48:31,624
of how likely each action is to be taken.

1200
00:48:31,624 --> 00:48:32,624
.

1201
00:48:32,625 --> 00:48:34,627
And this is where the agent end up in the

1202
00:48:34,627 --> 00:48:35,628
last time step.

1203
00:48:35,628 --> 00:48:36,629
Okay?

1204
00:48:36,629 --> 00:48:39,632
So let's maybe kind of also enable this

1205
00:48:39,632 --> 00:48:42,635
environment render that will show us

1206
00:48:42,635 --> 00:48:45,638
where the agent is at every time step.

1207
00:48:45,638 --> 00:48:48,641
So initially the agent was at this

1208
00:48:48,641 --> 00:48:50,643
location and we have an action

1209
00:48:50,643 --> 00:48:53,646
distribution of this here.

1210
00:48:53,646 --> 00:48:54,647
So north, south, east and west.

1211
00:48:54,647 --> 00:48:56,649
So the agent knows that it should go

1212
00:48:56,649 --> 00:48:57,650
north.

1213
00:48:57,650 --> 00:48:57,650
Why?

1214
00:48:58,651 --> 00:49:01,648
Because if I look at the prior preference,

1215
00:49:01,648 --> 00:49:03,650
, this state is more preferred than this

1216
00:49:03,650 --> 00:49:04,651
state.

1217
00:49:04,651 --> 00:49:04,651
Right?

1218
00:49:04,651 --> 00:49:07,654
So the agent successfully calculated the

1219
00:49:07,654 --> 00:49:09,656
expected free energy and inferred that,

1220
00:49:09,656 --> 00:49:12,659
okay, I should go to this state and not

1221
00:49:12,659 --> 00:49:13,660
stay in this state.

1222
00:49:13,660 --> 00:49:16,663
And because it has the generative model

1223
00:49:16,663 --> 00:49:18,665
of transitions available, it can infer

1224
00:49:18,665 --> 00:49:21,668
that I should take an action north to go

1225
00:49:21,668 --> 00:49:22,669
to this state.

1226
00:49:22,669 --> 00:49:23,670
So that's good.

1227
00:49:23,670 --> 00:49:25,672
And the agent goes to north.

1228
00:49:25,672 --> 00:49:28,675
And at this particular state, the agent

1229
00:49:28,675 --> 00:49:29,676
infers that it should go to north

1230
00:49:29,676 --> 00:49:30,677
southeast.

1231
00:49:30,677 --> 00:49:33,680
So it will take an action east and it

1232
00:49:33,680 --> 00:49:35,682
will go here.

1233
00:49:35,682 --> 00:49:37,684
And at this point of time, I want your

1234
00:49:37,684 --> 00:49:40,687
attention where the action distribution

1235
00:49:40,687 --> 00:49:43,690
is equally probable for north and east.

1236
00:49:43,690 --> 00:49:44,691
Why is that?

1237
00:49:44,691 --> 00:49:46,693
Because the agent is only looking at the

1238
00:49:46,693 --> 00:49:47,694
immediate future.

1239
00:49:47,694 --> 00:49:47,694
Right.

1240
00:49:47,694 --> 00:49:51,698
So let's go back to the prior preference

1241
00:49:51,698 --> 00:49:55,702
where the agent is right now here, or is

1242
00:49:55,702 --> 00:49:56,703
it here?

1243
00:49:56,703 --> 00:49:58,705
Yeah, it is right now here in this

1244
00:49:58,705 --> 00:49:59,706
particular state.

1245
00:49:59,706 --> 00:50:02,703
And if the agent is considering immediate

1246
00:50:02,703 --> 00:50:05,706
consequences of just one action, then

1247
00:50:05,706 --> 00:50:07,708
these two states are equally good for it

1248
00:50:07,708 --> 00:50:09,710
to be in the next state.

1249
00:50:09,710 --> 00:50:11,712
So there is no distinction between these

1250
00:50:11,712 --> 00:50:13,714
two states if it is only looking at the

1251
00:50:13,714 --> 00:50:15,715
immediate future.

1252
00:50:15,716 --> 00:50:17,718
So that means that the expected free

1253
00:50:17,718 --> 00:50:19,720
energygies will conclude that I should go

1254
00:50:19,720 --> 00:50:21,722
to north or east.

1255
00:50:21,722 --> 00:50:23,724
It doesn't matter if I'm looking at just

1256
00:50:23,724 --> 00:50:24,725
one time step ahead.

1257
00:50:24,725 --> 00:50:25,726
Okay?

1258
00:50:25,726 --> 00:50:26,727
That's the idea.

1259
00:50:27,728 --> 00:50:32,733
And out of probability it is going here.

1260
00:50:33,734 --> 00:50:35,736
It took the action east and from this

1261
00:50:35,736 --> 00:50:38,739
state, when it's doing inference, it's

1262
00:50:38,739 --> 00:50:41,742
inferring that this state is better and

1263
00:50:41,742 --> 00:50:44,745
basically it's ending up in this local

1264
00:50:44,745 --> 00:50:47,748
maxima state, which is this particular

1265
00:50:47,748 --> 00:50:50,751
state where the neighboring states are

1266
00:50:50,751 --> 00:50:52,753
less preferred.

1267
00:50:52,753 --> 00:50:54,755
And this is Wall and you can't go there

1268
00:50:54,755 --> 00:50:57,758
because it is forbidden for the agent by

1269
00:50:57,758 --> 00:50:58,759
structure.

1270
00:50:58,759 --> 00:51:00,755
So it's basically going to sit there

1271
00:51:00,755 --> 00:51:03,758
forever where it only sees that local

1272
00:51:03,758 --> 00:51:05,760
maximum of prior preference.

1273
00:51:05,760 --> 00:51:09,764
And let's look at what might happen if

1274
00:51:09,764 --> 00:51:12,767
you have higher planning depth.

1275
00:51:12,767 --> 00:51:16,771
So if I go to the planning depth of three,

1276
00:51:16,771 --> 00:51:19,774
, then that means that the agent is

1277
00:51:19,774 --> 00:51:22,777
actually reaching the goal state at the

1278
00:51:22,777 --> 00:51:23,778
last time step.

1279
00:51:24,779 --> 00:51:28,783
But still in the third time point, it had

1280
00:51:28,783 --> 00:51:32,787
two probabilistic actions, north and east.

1281
00:51:32,787 --> 00:51:33,787
.

1282
00:51:33,788 --> 00:51:35,790
So here from this particular state, out

1283
00:51:35,790 --> 00:51:38,793
of probability, it took the action north

1284
00:51:38,793 --> 00:51:40,795
but it can all equally take the action

1285
00:51:40,795 --> 00:51:43,798
east and end up in this local maxima.

1286
00:51:43,798 --> 00:51:46,801
So let's run again and probably it will

1287
00:51:46,801 --> 00:51:48,803
end up in this local maxima, okay?

1288
00:51:48,803 --> 00:51:52,807
And only for the planning depth of four,

1289
00:51:52,807 --> 00:51:55,810
which is sufficient enough, which is

1290
00:51:55,810 --> 00:51:58,813
necessary for this particular grid, the

1291
00:51:58,813 --> 00:52:01,810
agent is fully sure of what to do.

1292
00:52:01,810 --> 00:52:04,813
So at every time point it is fully sure

1293
00:52:04,813 --> 00:52:07,816
of what to do, that it has to go north,

1294
00:52:07,816 --> 00:52:09,818
then east, north, north, north, east,

1295
00:52:09,818 --> 00:52:12,821
east and south and reach this particular

1296
00:52:12,821 --> 00:52:13,822
goal state.

1297
00:52:13,822 --> 00:52:17,826
So only for time step or planning depth n

1298
00:52:17,826 --> 00:52:21,830
equal to four, it can successfully

1299
00:52:21,830 --> 00:52:24,833
navigate this grid with 100% certainty.

1300
00:52:25,833 --> 00:52:26,835
So that's the idea.

1301
00:52:27,836 --> 00:52:30,839
That's the implementation that I hope you

1302
00:52:30,839 --> 00:52:30,839
got.

1303
00:52:30,839 --> 00:52:33,842
So there is also this idea of action

1304
00:52:33,842 --> 00:52:33,842
precision.

1305
00:52:33,842 --> 00:52:35,844
So here it's a high action precision.

1306
00:52:35,844 --> 00:52:38,847
That is why it is taking the actions.

1307
00:52:38,847 --> 00:52:40,849
That is from the probability.

1308
00:52:40,849 --> 00:52:44,853
If it is a low action precision like one,

1309
00:52:44,853 --> 00:52:49,857
then the good actions are more probable.

1310
00:52:49,858 --> 00:52:52,861
But that doesn't mean that it will be

1311
00:52:52,861 --> 00:52:55,864
taken right here by luck, it is taking

1312
00:52:55,864 --> 00:52:58,867
the right actions and reaching the state.

1313
00:52:58,867 --> 00:52:58,867
.

1314
00:52:58,867 --> 00:53:00,863
But here probabilities are most passed.

1315
00:53:00,863 --> 00:53:03,866
But you will also see like exploration

1316
00:53:03,866 --> 00:53:06,869
behavior in more number of trials if you

1317
00:53:06,869 --> 00:53:08,871
control this action precision.

1318
00:53:08,871 --> 00:53:11,874
So I set it to a high value to make sure

1319
00:53:11,874 --> 00:53:13,876
that the agent reaches the goal for this

1320
00:53:13,876 --> 00:53:15,878
particular problem.

1321
00:53:15,878 --> 00:53:18,881
But it's worth playing and it's important,

1322
00:53:18,881 --> 00:53:19,882
, right?

1323
00:53:20,883 --> 00:53:23,886
Okay, yeah.

1324
00:53:23,886 --> 00:53:25,888
So for different planning depths like one,

1325
00:53:25,888 --> 00:53:28,891
, three and four in this problem, this is

1326
00:53:28,891 --> 00:53:30,893
the behavior that you expect.

1327
00:53:30,893 --> 00:53:33,896
For lower planning depths, which is not

1328
00:53:33,896 --> 00:53:36,899
sufficient, the agent ends up in local

1329
00:53:36,899 --> 00:53:39,902
maximas or local minimas of expected free

1330
00:53:39,902 --> 00:53:41,904
energy or local maximas of prior

1331
00:53:41,904 --> 00:53:42,905
preference.

1332
00:53:42,905 --> 00:53:45,908
But with sufficient planning depth, it's

1333
00:53:45,908 --> 00:53:47,910
able to navigate and reach the goal.

1334
00:53:47,910 --> 00:53:52,915
So that brings us to the last point in

1335
00:53:52,915 --> 00:53:57,920
this tutorial where why is it important

1336
00:53:57,920 --> 00:54:01,918
to have a threshold in evaluating

1337
00:54:01,918 --> 00:54:04,921
sophisticated inference?

1338
00:54:05,922 --> 00:54:08,925
So by threshold, what we mean is that we

1339
00:54:08,925 --> 00:54:11,928
can ignore future possibilities in two

1340
00:54:11,928 --> 00:54:12,929
levels, right?

1341
00:54:12,929 --> 00:54:15,932
You can ignore not likely actions or not

1342
00:54:15,932 --> 00:54:18,935
likely observations in this research.

1343
00:54:18,935 --> 00:54:20,937
But if you consider the consequences of

1344
00:54:20,937 --> 00:54:23,940
all actions and observations, that means

1345
00:54:23,940 --> 00:54:25,942
that you'll have to consider four

1346
00:54:25,942 --> 00:54:27,944
consequences in the first place.

1347
00:54:27,944 --> 00:54:30,947
Then you will have to consider four times

1348
00:54:30,947 --> 00:54:32,949
the action states in the next time step

1349
00:54:32,949 --> 00:54:33,950
for the next time step.

1350
00:54:33,950 --> 00:54:35,952
Then all of that multiplied with the

1351
00:54:35,952 --> 00:54:37,954
number of actions and this tree search

1352
00:54:37,954 --> 00:54:40,957
becomes intractable and you'll explode.

1353
00:54:40,957 --> 00:54:42,959
And it's even worse than the classical

1354
00:54:42,959 --> 00:54:45,962
active inference policy space problem.

1355
00:54:45,962 --> 00:54:48,965
But by defining a threshold of even a

1356
00:54:48,965 --> 00:54:51,968
small value, we'll ignore possibilities.

1357
00:54:51,968 --> 00:54:54,971
So where is that implemented?

1358
00:54:55,972 --> 00:54:58,975
In the forward search algorithm, we are

1359
00:54:58,975 --> 00:55:01,972
considering actions with only action

1360
00:55:01,972 --> 00:55:04,975
probabilities greater than the particular

1361
00:55:04,975 --> 00:55:05,976
threshold.

1362
00:55:05,976 --> 00:55:08,979
Here I am defining it as one by 16.

1363
00:55:08,979 --> 00:55:12,983
Also in the parent paper, it's one by 16

1364
00:55:12,983 --> 00:55:14,985
the action probability.

1365
00:55:16,987 --> 00:55:16,987
Okay?

1366
00:55:17,988 --> 00:55:20,991
So if it is zero, then that means that it

1367
00:55:20,991 --> 00:55:22,993
will consider all the consequences of

1368
00:55:22,993 --> 00:55:24,995
future and that's intractable.

1369
00:55:24,995 --> 00:55:27,998
So you can ignore actions which is not

1370
00:55:27,998 --> 00:55:30,001
probable, and also ignore states which is

1371
00:55:30,001 --> 00:55:32,003
less likely, or only consider states that

1372
00:55:32,003 --> 00:55:35,006
has probability greater than this

1373
00:55:35,006 --> 00:55:37,008
particular threshold value, and that

1374
00:55:37,008 --> 00:55:40,011
significantly reduces the computational

1375
00:55:40,011 --> 00:55:42,013
complexity, where you will only consider

1376
00:55:42,013 --> 00:55:45,016
combinations that are probable in the

1377
00:55:45,016 --> 00:55:46,017
future.

1378
00:55:47,018 --> 00:55:50,021
Tree and that lets you go deeper in your

1379
00:55:50,021 --> 00:55:52,023
planning horizon.

1380
00:55:52,023 --> 00:55:54,025
That's an important point here.

1381
00:55:54,025 --> 00:55:58,029
And if you compare the time that takes

1382
00:55:58,029 --> 00:56:01,026
for deeper planning for a search

1383
00:56:01,026 --> 00:56:05,030
threshold of zero, so a search threshold

1384
00:56:05,030 --> 00:56:09,034
of zero means that you will consider all

1385
00:56:09,034 --> 00:56:11,036
consequences.

1386
00:56:11,036 --> 00:56:16,041
And the more deep you plan, the more time

1387
00:56:16,041 --> 00:56:17,042
it takes.

1388
00:56:17,042 --> 00:56:21,045
And if you see to consider only the first

1389
00:56:21,045 --> 00:56:24,049
future or the immediate future, it takes

1390
00:56:24,049 --> 00:56:25,050
0.1 second.

1391
00:56:25,050 --> 00:56:28,053
For considering three possibilities into

1392
00:56:28,053 --> 00:56:30,055
the future, it takes 3 seconds, and for

1393
00:56:30,055 --> 00:56:32,057
four it takes 300 seconds.

1394
00:56:32,057 --> 00:56:34,059
And you can see that the computational

1395
00:56:34,059 --> 00:56:36,061
time is exponentially growing.

1396
00:56:37,062 --> 00:56:41,066
But if you have a very small search

1397
00:56:41,066 --> 00:56:45,070
threshold, you have that computational

1398
00:56:45,070 --> 00:56:49,074
time that makes sense for implementation

1399
00:56:49,074 --> 00:56:52,077
in real world, right?

1400
00:56:52,077 --> 00:56:55,080
So here for N equal to four, that is four

1401
00:56:55,080 --> 00:56:58,083
times steps into the future, it's only

1402
00:56:58,083 --> 00:57:00,079
taking 0.1 second.

1403
00:57:00,079 --> 00:57:01,080
And that's okay.

1404
00:57:01,080 --> 00:57:04,083
I can still do simulations with this

1405
00:57:04,083 --> 00:57:07,086
complexity, but there is no way I can

1406
00:57:07,086 --> 00:57:10,089
talk about how less the computational

1407
00:57:10,089 --> 00:57:11,090
complexity is.

1408
00:57:11,090 --> 00:57:14,093
It truly depends on the nature of your

1409
00:57:14,093 --> 00:57:16,095
prior preferences and environment in

1410
00:57:16,095 --> 00:57:16,095
action.

1411
00:57:16,095 --> 00:57:19,098
But this search threshold actually works

1412
00:57:19,098 --> 00:57:22,101
in real life, and we just saw that in our

1413
00:57:22,101 --> 00:57:23,102
simulations, right?

1414
00:57:24,103 --> 00:57:26,105
For N equal to four, it took only like 0.

1415
00:57:26,105 --> 00:57:28,107
.3 seconds in our simulations.

1416
00:57:28,107 --> 00:57:31,110
But if you set the search threshold as

1417
00:57:31,110 --> 00:57:35,113
zero, it already is 300 seconds for doing

1418
00:57:35,113 --> 00:57:36,115
full depth planning.

1419
00:57:36,115 --> 00:57:39,118
And if I set a planning depth of five,

1420
00:57:39,118 --> 00:57:43,122
then it will basically run forever maybe,

1421
00:57:43,122 --> 00:57:46,125
and I will not be able to do simulation.

1422
00:57:46,125 --> 00:57:49,128
So that's the idea of search threshold.

1423
00:57:50,129 --> 00:57:52,131
So actually, that's it.

1424
00:57:53,132 --> 00:57:57,136
I wanted to explain the Agent class, the

1425
00:57:57,136 --> 00:58:00,133
environment class, and the particular

1426
00:58:00,133 --> 00:58:03,136
demo and yeah, maybe it's a good time for

1427
00:58:03,136 --> 00:58:06,139
questions if anybody was listening.

1428
00:58:06,139 --> 00:58:10,143
And I hope people get to play with this

1429
00:58:10,143 --> 00:58:13,146
code and look at the tutorial and

1430
00:58:13,146 --> 00:58:17,150
implement this and build generative

1431
00:58:17,150 --> 00:58:19,152
models like this.

1432
00:58:19,152 --> 00:58:22,154
So this particular example is how do you

1433
00:58:22,154 --> 00:58:24,157
build a generative model for this grid

1434
00:58:24,157 --> 00:58:27,160
world task and see how the Agent is able

1435
00:58:27,160 --> 00:58:29,162
to take meaningful actions?

1436
00:58:29,162 --> 00:58:32,165
But here we gave it that true structure

1437
00:58:32,165 --> 00:58:34,167
of the environment in the B matrix and A

1438
00:58:34,167 --> 00:58:36,169
matrix and so on.

1439
00:58:36,169 --> 00:58:40,173
But you can also play around with

1440
00:58:40,173 --> 00:58:44,177
learning in the sense that while defining

1441
00:58:44,177 --> 00:58:49,182
the Agent step, you can add a flag that

1442
00:58:49,182 --> 00:58:52,185
says learning equal to true.

1443
00:58:54,187 --> 00:58:57,190
And if you start with an uninformed AB

1444
00:58:57,190 --> 00:58:59,192
and so on, you can experiment on how the

1445
00:58:59,192 --> 00:59:01,188
Agent is learning that environment.

1446
00:59:02,189 --> 00:59:04,191
Here you can look at the B matrix in the

1447
00:59:04,191 --> 00:59:06,193
beginning, you can look at the B matrix

1448
00:59:06,193 --> 00:59:08,195
after, say, ten trials and see how the

1449
00:59:08,195 --> 00:59:10,197
learning is taking place here.

1450
00:59:10,197 --> 00:59:12,199
It doesn't matter because the agent knows

1451
00:59:12,199 --> 00:59:15,202
the structure and it won't learn much.

1452
00:59:15,202 --> 00:59:17,203
But if it starts from an unknown

1453
00:59:17,203 --> 00:59:18,205
structure, then there is scope of

1454
00:59:18,205 --> 00:59:20,207
learning also to be implemented.

1455
00:59:20,207 --> 00:59:24,211
And it's already implemented because we

1456
00:59:24,211 --> 00:59:27,214
are using existing Pymdp functionalities

1457
00:59:27,214 --> 00:59:30,217
for learning A and B and it's already

1458
00:59:30,217 --> 00:59:32,219
part of the step function.

1459
00:59:32,219 --> 00:59:35,222
So I hope step function is clear, which

1460
00:59:35,222 --> 00:59:37,224
is the only thing you need to know if you'

1461
00:59:37,224 --> 00:59:39,226
're trying to implement sophisticated

1462
00:59:39,226 --> 00:59:42,229
inference and just the names of these

1463
00:59:42,229 --> 00:59:44,231
matrices if you want to probe them and

1464
00:59:44,231 --> 00:59:45,232
look at them.

1465
00:59:45,232 --> 00:59:49,236
And yeah, I hope the session was useful.

1466
00:59:49,236 --> 00:59:52,239
So I thank my collaborators and Connor,

1467
00:59:52,239 --> 00:59:55,242
who maintains Pimdp, and also Brendan who

1468
00:59:55,242 --> 00:59:58,245
runs the Pimdp Fellowship, which I was

1469
00:59:58,245 --> 01:00:01,434
part of, and that's where I worked on

1470
01:00:01,434 --> 01:00:04,402
implementing sophisticated inference in

1471
01:00:04,402 --> 01:00:07,523
Pimdp and it will be part of the original

1472
01:00:07,523 --> 01:00:09,035
Pymdp module soon.

1473
01:00:09,048 --> 01:00:11,282
And I hope people can start using this

1474
01:00:11,282 --> 01:00:13,479
module to simulate sophisticated

1475
01:00:13,479 --> 01:00:16,725
inference experiments and this basically

1476
01:00:16,725 --> 01:00:17,836
becomes useful.

1477
01:00:17,853 --> 01:00:19,059
So maybe it's a good time to discuss

1478
01:00:19,059 --> 01:00:21,283
questions or clarifications on the code

1479
01:00:21,283 --> 01:00:24,523
or maybe it's a good time to take a break,

1480
01:00:24,523 --> 01:00:25,621
, as Daniel was.

1481
01:00:27,884 --> 01:00:28,954
Daniel: Thank you.

1482
01:00:28,961 --> 01:00:29,063
That was awesome.

1483
01:00:30,156 --> 01:00:33,462
Well, I have a few different questions

1484
01:00:33,462 --> 01:00:36,785
and I'll read a few from the live chat.

1485
01:00:36,793 --> 01:00:39,031
So I'll first just go to the live chat

1486
01:00:39,031 --> 01:00:41,257
and then ask those and then ask some

1487
01:00:41,257 --> 01:00:42,370
other questions.

1488
01:00:42,377 --> 01:00:45,692
So Dave asks, how do you think about the

1489
01:00:45,692 --> 01:00:48,976
neural implementation of recursion?

1490
01:00:48,989 --> 01:00:51,207
Brains don't seem to implement computer

1491
01:00:51,207 --> 01:00:53,419
hardware style recursion deeper than a

1492
01:00:53,419 --> 01:00:54,536
stack depth of one.

1493
01:00:54,545 --> 01:00:56,793
Aside from heavily over learned tasks, we

1494
01:00:56,793 --> 01:00:59,018
can confine ourselves to asking about

1495
01:00:59,018 --> 01:01:01,660
recursion for the purposes of exploring

1496
01:01:01,660 --> 01:01:03,897
temporally deep state spaces, searching

1497
01:01:03,897 --> 01:01:05,018
forward in time.

1498
01:01:05,033 --> 01:01:08,348
So how do we reconcile this really

1499
01:01:08,348 --> 01:01:12,728
beautiful and elegant and computationally

1500
01:01:12,728 --> 01:01:16,108
efficient full depth tree search with the

1501
01:01:16,108 --> 01:01:20,507
biological basis of multiscale planning?

1502
01:01:21,604 --> 01:01:24,918
Aswin: Yeah, so I'm not an expert in

1503
01:01:24,918 --> 01:01:27,240
neural computation, but the answer to

1504
01:01:27,240 --> 01:01:31,597
that would be basically you're doing only

1505
01:01:31,597 --> 01:01:34,910
one computation at a time, right?

1506
01:01:34,924 --> 01:01:37,275
And all you need is some memory to store

1507
01:01:37,275 --> 01:01:41,600
your beliefs and use those beliefs to

1508
01:01:41,600 --> 01:01:44,951
kind of do the same computation again.

1509
01:01:44,965 --> 01:01:49,040
So we are not talking about this hardcore

1510
01:01:49,040 --> 01:01:51,068
recursive implementation.

1511
01:01:52,069 --> 01:01:54,092
We are only doing local computations at a

1512
01:01:54,092 --> 01:01:54,095
time.

1513
01:01:54,096 --> 01:01:57,129
And just because of the structure of the

1514
01:01:57,129 --> 01:02:00,098
generative model and because we have

1515
01:02:00,098 --> 01:02:03,120
memory, this can be done.

1516
01:02:03,121 --> 01:02:05,147
And I don't see why brain can't do it,

1517
01:02:05,147 --> 01:02:08,174
even though individual neurons might not

1518
01:02:08,174 --> 01:02:09,187
be able to do it.

1519
01:02:09,187 --> 01:02:11,204
The brain has memory.

1520
01:02:11,206 --> 01:02:14,231
The brain has the ability to store memory

1521
01:02:14,231 --> 01:02:16,255
and the ability to dream, the ability to

1522
01:02:16,255 --> 01:02:17,262
simulate.

1523
01:02:17,264 --> 01:02:19,286
It knows the consequences of actions.

1524
01:02:19,287 --> 01:02:22,317
And you do this on a daily basis where

1525
01:02:22,317 --> 01:02:25,348
you plan your future and decide what to

1526
01:02:25,348 --> 01:02:26,352
do.

1527
01:02:26,352 --> 01:02:26,355
Right?

1528
01:02:26,355 --> 01:02:28,379
So on a single neuron level, I'm not

1529
01:02:28,379 --> 01:02:31,400
really sure of how to answer that

1530
01:02:31,400 --> 01:02:33,426
question, but I don't really see why the

1531
01:02:33,426 --> 01:02:36,450
brain can't do it as an organism.

1532
01:02:38,476 --> 01:02:39,484
Cool.

1533
01:02:40,498 --> 01:02:42,512
Daniel: Okay, to the code.

1534
01:02:42,512 --> 01:02:44,531
I guess I have a few questions.

1535
01:02:44,533 --> 01:02:46,552
Can we go back to the maze?

1536
01:02:51,599 --> 01:02:52,618
And of course, if anyone else has

1537
01:02:52,618 --> 01:02:55,641
questions in the live chat, just go for

1538
01:02:55,641 --> 01:02:55,643
it.

1539
01:02:57,664 --> 01:03:02,651
So in the maze, how does the moves that

1540
01:03:02,651 --> 01:03:06,695
are possible, how is that reflected?

1541
01:03:06,697 --> 01:03:09,724
What stage is it updated, for example,

1542
01:03:09,724 --> 01:03:12,753
that it initially knows that it can only

1543
01:03:12,753 --> 01:03:15,780
go up and then it can only go right or

1544
01:03:15,780 --> 01:03:15,786
down?

1545
01:03:15,788 --> 01:03:18,819
Where is that reflected with the updating

1546
01:03:18,819 --> 01:03:21,846
of kind of that relational aspect of

1547
01:03:21,846 --> 01:03:22,856
affordances?

1548
01:03:22,857 --> 01:03:24,875
What is even possible to do?

1549
01:03:24,878 --> 01:03:27,905
And then how does it evaluate in a deep

1550
01:03:27,905 --> 01:03:28,915
tree search?

1551
01:03:30,936 --> 01:03:34,971
Does it need to know what things could or

1552
01:03:34,971 --> 01:03:36,998
couldn't happen in the future?

1553
01:03:39,025 --> 01:03:42,050
Aswin: So, if I understand the questions

1554
01:03:42,050 --> 01:03:44,071
correctly, there is an important

1555
01:03:44,071 --> 01:03:46,092
distinction between the generative

1556
01:03:46,092 --> 01:03:49,120
process and the generative model, right?

1557
01:03:49,121 --> 01:03:52,149
So in the grid world, which is the

1558
01:03:52,149 --> 01:03:55,182
generative process, we have implemented

1559
01:03:55,182 --> 01:03:58,216
all of that manually, where we have this

1560
01:03:58,216 --> 01:04:01,189
transition dynamics that already knows

1561
01:04:01,189 --> 01:04:05,220
what will happen if you take an action

1562
01:04:05,220 --> 01:04:06,236
from a state.

1563
01:04:06,237 --> 01:04:09,261
So it's like an environment that knows

1564
01:04:09,261 --> 01:04:09,269
how to work.

1565
01:04:10,273 --> 01:04:12,295
So it's like a reality where there are

1566
01:04:12,295 --> 01:04:14,318
consequences of actions and it's already

1567
01:04:14,318 --> 01:04:15,323
there.

1568
01:04:15,325 --> 01:04:18,355
But this information is available to the

1569
01:04:18,355 --> 01:04:21,387
agent to be part of its generative model.

1570
01:04:21,387 --> 01:04:21,387
.

1571
01:04:21,389 --> 01:04:24,413
And in the generative model, what it

1572
01:04:24,413 --> 01:04:26,437
basically does is use that transition

1573
01:04:26,437 --> 01:04:29,462
dynamics, which is given or learned to

1574
01:04:29,462 --> 01:04:31,489
simulate what might happen in the future.

1575
01:04:31,489 --> 01:04:32,491
.

1576
01:04:32,493 --> 01:04:32,498
Okay?

1577
01:04:33,499 --> 01:04:36,533
So once I have that transition dynamics,

1578
01:04:36,533 --> 01:04:39,568
if I go to the where's the if I go to the

1579
01:04:39,568 --> 01:04:43,603
tree search, what it is essentially doing

1580
01:04:43,603 --> 01:04:46,633
is evaluating the posterior given a

1581
01:04:46,633 --> 01:04:49,664
particular state and an action J.

1582
01:04:49,668 --> 01:04:52,695
So from my generative model, I know that

1583
01:04:52,695 --> 01:04:55,723
if I start from this state and if I take

1584
01:04:55,723 --> 01:04:58,750
this action, I go to this posterior and I

1585
01:04:58,750 --> 01:05:00,714
consider all the consequences.

1586
01:05:00,715 --> 01:05:03,745
And if in the generative model, it is

1587
01:05:03,745 --> 01:05:06,775
unlikely that if I go east, I don't go

1588
01:05:06,775 --> 01:05:09,803
there and so on, it's automatically

1589
01:05:09,803 --> 01:05:12,835
reflected in the expected free energy.

1590
01:05:14,853 --> 01:05:16,875
So I hope that answers the question.

1591
01:05:16,876 --> 01:05:20,918
So here, if I set the action precision to

1592
01:05:20,918 --> 01:05:24,959
be high and also enable the so initially

1593
01:05:24,959 --> 01:05:28,999
I am in this particular state, and with

1594
01:05:28,999 --> 01:05:33,041
respect to the expected free energy, what

1595
01:05:33,041 --> 01:05:36,078
I'm doing is I'm using my generative

1596
01:05:36,078 --> 01:05:40,117
model to predict what will happen if I

1597
01:05:40,117 --> 01:05:43,142
take four actions.

1598
01:05:43,145 --> 01:05:45,167
And my predictions will say that if I

1599
01:05:45,167 --> 01:05:47,183
take North, I will go here.

1600
01:05:47,184 --> 01:05:49,205
And if I take all the other actions, I'll

1601
01:05:49,205 --> 01:05:50,211
stay here.

1602
01:05:50,212 --> 01:05:53,242
And because in my prior preference, north

1603
01:05:53,242 --> 01:05:56,272
is more preferred, I can infer that north

1604
01:05:56,272 --> 01:05:58,295
is the action I should go for.

1605
01:05:58,297 --> 01:06:00,251
So that's the idea.

1606
01:06:00,255 --> 01:06:02,275
So here, the grid structure is given to

1607
01:06:02,275 --> 01:06:03,281
the agent.

1608
01:06:03,282 --> 01:06:05,300
And it might be a little confusing, but

1609
01:06:05,300 --> 01:06:06,316
the agent can also learn this grid

1610
01:06:06,316 --> 01:06:08,331
structure and this will work.

1611
01:06:08,332 --> 01:06:10,356
So once I know the grid structure as an

1612
01:06:10,356 --> 01:06:12,378
agent, I can simulate the future and

1613
01:06:12,378 --> 01:06:15,401
consider the consequences of action,

1614
01:06:15,401 --> 01:06:15,406
right?

1615
01:06:15,408 --> 01:06:17,424
So that's what's happening.

1616
01:06:17,426 --> 01:06:19,446
And once I am in this state, I will

1617
01:06:19,446 --> 01:06:21,468
consider the consequence of all four

1618
01:06:21,468 --> 01:06:23,489
actions, and I will infer that, okay,

1619
01:06:23,489 --> 01:06:26,512
going east is better because that will

1620
01:06:26,512 --> 01:06:27,527
take me to this state.

1621
01:06:28,532 --> 01:06:30,554
So there is always a difference between

1622
01:06:30,554 --> 01:06:32,578
or you should keep in mind how generative

1623
01:06:32,578 --> 01:06:34,598
model and generative process are two

1624
01:06:34,598 --> 01:06:36,618
different things, and what the agent

1625
01:06:36,618 --> 01:06:39,640
knows might not be the reality or might

1626
01:06:39,640 --> 01:06:41,662
be the reality, depending upon what you

1627
01:06:41,662 --> 01:06:42,670
give it.

1628
01:06:44,692 --> 01:06:45,702
Makes sense.

1629
01:06:45,704 --> 01:06:46,712
Cool.

1630
01:06:46,716 --> 01:06:50,750
Daniel: So if you were going to go about

1631
01:06:50,750 --> 01:06:53,785
making a new situation that you wanted to

1632
01:06:53,785 --> 01:06:56,819
do generative modeling for, do you tend

1633
01:06:56,819 --> 01:07:00,794
to start from an existing working Pi MDP

1634
01:07:00,794 --> 01:07:04,830
notebook and start modifying state spaces,

1635
01:07:04,830 --> 01:07:07,863
, or do you draw it out on a canvas?

1636
01:07:07,865 --> 01:07:10,889
How would you recommend somebody other

1637
01:07:10,889 --> 01:07:12,913
than replicating what is shown here?

1638
01:07:12,914 --> 01:07:14,931
Let's just say we're interested in

1639
01:07:14,931 --> 01:07:16,951
something that's not exactly just a maze.

1640
01:07:16,951 --> 01:07:16,951
.

1641
01:07:16,953 --> 01:07:21,006
What do we do to get our head around how

1642
01:07:21,006 --> 01:07:24,031
we should proceed?

1643
01:07:24,038 --> 01:07:27,061
Aswin: Yeah, good question.

1644
01:07:27,062 --> 01:07:30,092
So if you are trying to simulate, say, a

1645
01:07:30,092 --> 01:07:32,118
new environment, you have the heavy

1646
01:07:32,118 --> 01:07:35,146
lifting to do, which is to define the

1647
01:07:35,146 --> 01:07:38,171
generative model for the agent.

1648
01:07:38,178 --> 01:07:40,197
You can either define a very sparse

1649
01:07:40,197 --> 01:07:42,217
generative model, which the agent can

1650
01:07:42,217 --> 01:07:44,234
learn, but you have to define the

1651
01:07:44,234 --> 01:07:45,241
structure.

1652
01:07:45,242 --> 01:07:47,267
That structure should be there, and only

1653
01:07:47,267 --> 01:07:50,293
using that structure, the agent can learn

1654
01:07:50,293 --> 01:07:51,307
the generative model.

1655
01:07:51,308 --> 01:07:54,335
So here you can make use of this cell of

1656
01:07:54,335 --> 01:07:57,359
code to understand how I define the

1657
01:07:57,359 --> 01:07:58,376
structure of the grid.

1658
01:07:59,379 --> 01:08:01,345
Okay, so I am defining a structure for

1659
01:08:01,345 --> 01:08:03,366
the agent for it to make use of.

1660
01:08:03,367 --> 01:08:06,391
I'm saying that there are 25 valid states

1661
01:08:06,391 --> 01:08:08,414
and there are four available actions.

1662
01:08:08,415 --> 01:08:11,448
And this is the standard way of defining

1663
01:08:11,448 --> 01:08:14,470
the state space in Pymdp.

1664
01:08:14,471 --> 01:08:17,505
And you also have to define the central

1665
01:08:17,505 --> 01:08:20,534
parameters ABC and D for the agent

1666
01:08:20,534 --> 01:08:21,546
simulations.

1667
01:08:21,547 --> 01:08:25,580
So here I am defining A using my state

1668
01:08:25,580 --> 01:08:27,605
space and observation space.

1669
01:08:27,606 --> 01:08:30,637
But this step of giving it or telling it

1670
01:08:30,637 --> 01:08:33,663
that it's an identity matrix is my

1671
01:08:33,663 --> 01:08:35,688
decision choice in my modeling.

1672
01:08:35,689 --> 01:08:39,721
I don't have to do this for simulations.

1673
01:08:39,722 --> 01:08:42,750
I can see if the agent is learning it

1674
01:08:42,750 --> 01:08:45,780
from a random A matrix or when it starts

1675
01:08:45,780 --> 01:08:46,798
from a random A matrix.

1676
01:08:47,800 --> 01:08:50,837
And similarly for this B matrix, this

1677
01:08:50,837 --> 01:08:52,858
structure is defined.

1678
01:08:52,859 --> 01:08:55,883
And there are functions like this which

1679
01:08:55,883 --> 01:08:57,903
can give you a random B matrix.

1680
01:08:57,904 --> 01:08:59,926
But this is a modeling choice where if I

1681
01:08:59,926 --> 01:09:02,890
want to give it the grid structure or not

1682
01:09:02,890 --> 01:09:04,911
give it the grid structure, I can start

1683
01:09:04,911 --> 01:09:06,932
from a random B matrix, let the agent

1684
01:09:06,932 --> 01:09:08,955
learn, and look at that learned B metric.

1685
01:09:08,955 --> 01:09:08,957
.

1686
01:09:08,959 --> 01:09:10,976
Just for the purpose of the demo.

1687
01:09:10,977 --> 01:09:13,001
I gave it the grid structure to enable it

1688
01:09:13,001 --> 01:09:14,013
to take the actions.

1689
01:09:14,014 --> 01:09:16,036
But it's not a mandatory thing.

1690
01:09:16,036 --> 01:09:19,060
So this notebook is useful in the sense

1691
01:09:19,060 --> 01:09:21,084
that you know what to do, but definitely

1692
01:09:21,084 --> 01:09:23,107
you should play around with steps that

1693
01:09:23,107 --> 01:09:25,121
may not be mandatory.

1694
01:09:25,122 --> 01:09:27,147
Right, so if I give a prior preference

1695
01:09:27,147 --> 01:09:30,171
for this state to be the maximum, then

1696
01:09:30,171 --> 01:09:32,189
you can see behavior that.

1697
01:09:32,190 --> 01:09:33,206
The agent will try and go there.

1698
01:09:33,206 --> 01:09:33,208
Right.

1699
01:09:33,209 --> 01:09:36,237
So this prior preference is defined in

1700
01:09:36,237 --> 01:09:39,266
conjunction that this is the goal state,

1701
01:09:39,266 --> 01:09:42,294
but this may not be the goal state.

1702
01:09:42,295 --> 01:09:44,314
And in a different ask, what prior

1703
01:09:44,314 --> 01:09:46,335
preference means is different according

1704
01:09:46,335 --> 01:09:47,347
to the environment.

1705
01:09:48,353 --> 01:09:49,360
Right.

1706
01:09:49,362 --> 01:09:51,382
So that's also there and your prior.

1707
01:09:51,383 --> 01:09:54,410
So once you define that generative model,

1708
01:09:54,410 --> 01:09:56,437
which you have to do, you can't run from

1709
01:09:56,437 --> 01:09:59,464
it, then everything is kind of automated.

1710
01:09:59,464 --> 01:09:59,466
.

1711
01:09:59,467 --> 01:10:04,451
The agent only have to use the agent step,

1712
01:10:04,451 --> 01:10:07,486
, give it the observation from the

1713
01:10:07,486 --> 01:10:08,498
environment.

1714
01:10:08,499 --> 01:10:11,520
The agent knows how to take actions, and

1715
01:10:11,520 --> 01:10:13,540
then everything that happens inside the

1716
01:10:13,540 --> 01:10:15,561
agent, you don't really have to worry.

1717
01:10:17,584 --> 01:10:20,619
So this structure, I'm sure, will be

1718
01:10:20,619 --> 01:10:24,658
useful for your particular task that you

1719
01:10:24,658 --> 01:10:28,691
are trying to model in your hmm.

1720
01:10:31,726 --> 01:10:33,744
Daniel: Yes, very interesting.

1721
01:10:33,746 --> 01:10:36,771
And how would you contrast that or point

1722
01:10:36,771 --> 01:10:38,796
to any similarities or differences with

1723
01:10:38,796 --> 01:10:40,819
how this would be pursued outside of

1724
01:10:40,819 --> 01:10:42,831
active inference?

1725
01:10:42,833 --> 01:10:44,858
Like if somebody were just going to use

1726
01:10:44,858 --> 01:10:47,884
another kind of deep policy agent in the

1727
01:10:47,884 --> 01:10:50,910
maze example, what parts of the process

1728
01:10:50,910 --> 01:10:52,936
would be familiar and what parts would be

1729
01:10:52,936 --> 01:10:55,962
like a lot of work that wasn't expected

1730
01:10:55,962 --> 01:10:57,988
or skipping through parts that were a lot

1731
01:10:57,988 --> 01:10:59,004
of work otherwise.

1732
01:11:00,950 --> 01:11:00,957
Aswin: Yeah.

1733
01:11:01,966 --> 01:11:04,995
So the general structure is very familiar

1734
01:11:04,995 --> 01:11:07,023
to somebody who does things like this in

1735
01:11:07,023 --> 01:11:09,040
reinforcement learning.

1736
01:11:09,041 --> 01:11:12,074
So the idea that it's an agent step and

1737
01:11:12,074 --> 01:11:15,109
environment step, so this is the standard

1738
01:11:15,109 --> 01:11:19,143
OpenAI gym way of writing an environment

1739
01:11:19,143 --> 01:11:22,176
and a standard OpenAI way of writing an

1740
01:11:22,176 --> 01:11:23,185
agent.

1741
01:11:23,187 --> 01:11:24,190
Okay?

1742
01:11:24,191 --> 01:11:26,217
So if you have a Q learning agent who

1743
01:11:26,217 --> 01:11:29,243
does the same trying to navigate, then

1744
01:11:29,243 --> 01:11:31,269
the way you have to define the queue

1745
01:11:31,269 --> 01:11:34,295
matrix is the heavy lifting there.

1746
01:11:34,295 --> 01:11:36,315
It's just a state action mapping.

1747
01:11:36,317 --> 01:11:38,336
And in contrast to that, in active

1748
01:11:38,336 --> 01:11:40,357
inference, you have to come up with a

1749
01:11:40,357 --> 01:11:43,380
generative model that you want to see.

1750
01:11:43,380 --> 01:11:45,404
So in active inference, generative model

1751
01:11:45,404 --> 01:11:46,417
is the central thing.

1752
01:11:46,417 --> 01:11:47,423
Right.

1753
01:11:47,428 --> 01:11:50,455
Without a generative model, there is no

1754
01:11:50,455 --> 01:11:53,483
meaning of purposeful behavior in active

1755
01:11:53,483 --> 01:11:54,491
inference.

1756
01:11:54,492 --> 01:11:56,517
So the only unfamiliar part for a person

1757
01:11:56,517 --> 01:11:59,540
who is coming from, say, a field like

1758
01:11:59,540 --> 01:12:01,504
reinforcement learning is the structure

1759
01:12:01,504 --> 01:12:03,521
of the generating model.

1760
01:12:03,522 --> 01:12:05,547
But there is no way, other than getting

1761
01:12:05,547 --> 01:12:07,569
used to it, where it's the palm DP

1762
01:12:07,569 --> 01:12:09,587
structure which dominates.

1763
01:12:09,588 --> 01:12:12,616
But if you're doing deep active inference,

1764
01:12:12,616 --> 01:12:14,638
, all of this is going to be neural

1765
01:12:14,638 --> 01:12:17,665
networks and palm DPS are also not active

1766
01:12:17,665 --> 01:12:18,676
inference things.

1767
01:12:18,677 --> 01:12:19,680
Right.

1768
01:12:19,680 --> 01:12:21,706
It's an industrial engineering thing.

1769
01:12:21,708 --> 01:12:24,729
So palm DPS must also be familiar for

1770
01:12:24,729 --> 01:12:26,752
people who are coming from the computer

1771
01:12:26,752 --> 01:12:28,774
science background, just the idea that

1772
01:12:28,774 --> 01:12:30,797
what really happens in the agent is the

1773
01:12:30,797 --> 01:12:32,818
active inference part where we have

1774
01:12:32,818 --> 01:12:35,840
expected free energy s and variation of

1775
01:12:35,840 --> 01:12:36,856
free energy energies.

1776
01:12:36,857 --> 01:12:39,881
And if you want to learn about that, then

1777
01:12:39,881 --> 01:12:41,904
you have to go to the agents and see how

1778
01:12:41,904 --> 01:12:42,911
it works.

1779
01:12:42,912 --> 01:12:44,931
Look at the matrices numerically, see

1780
01:12:44,931 --> 01:12:45,939
what's happening.

1781
01:12:45,941 --> 01:12:47,967
But in a level where you want to get

1782
01:12:47,967 --> 01:12:50,991
started, I don't see any problem.

1783
01:12:50,993 --> 01:12:53,027
All of this is standard frameworks like

1784
01:12:53,027 --> 01:12:56,059
palmdps and OpenAI gym environments,

1785
01:12:56,059 --> 01:12:59,081
agent environment loop.

1786
01:12:59,081 --> 01:13:02,050
All this is very deeply discussed in

1787
01:13:02,050 --> 01:13:03,064
computer science.

1788
01:13:03,065 --> 01:13:08,117
It's not cool.

1789
01:13:10,135 --> 01:13:14,172
Daniel: So what other motifs or cognitive

1790
01:13:14,172 --> 01:13:17,208
phenomena are you excited or how do you

1791
01:13:17,208 --> 01:13:21,242
see the Pi MDP development trajectory

1792
01:13:21,242 --> 01:13:24,274
continuing after your sophisticated

1793
01:13:24,274 --> 01:13:27,300
inference gets pulled in?

1794
01:13:27,305 --> 01:13:30,330
Aswin: Yeah, so the Pimdp had the

1795
01:13:30,330 --> 01:13:32,353
original functionality and the

1796
01:13:32,353 --> 01:13:35,382
functionality to implement or simulate

1797
01:13:35,382 --> 01:13:38,412
general active inference agents with the

1798
01:13:38,412 --> 01:13:40,433
policy space and so on.

1799
01:13:40,434 --> 01:13:44,474
And that enabled a lot of people in the

1800
01:13:44,474 --> 01:13:48,509
community who are not familiar with

1801
01:13:48,509 --> 01:13:52,550
complicated coding and so on, who people

1802
01:13:52,550 --> 01:13:56,589
who do psychology, psychiatry, and all

1803
01:13:56,589 --> 01:13:57,605
the things.

1804
01:13:57,605 --> 01:13:57,607
Right.

1805
01:13:57,607 --> 01:14:00,573
So whoever want to come up and try

1806
01:14:00,573 --> 01:14:03,604
implement active inference, pymdp enabled

1807
01:14:03,604 --> 01:14:03,609
that.

1808
01:14:04,610 --> 01:14:06,634
And I am hoping that this module will

1809
01:14:06,634 --> 01:14:08,656
enable people who want to try out

1810
01:14:08,656 --> 01:14:11,680
sophisticated inference experiments in

1811
01:14:11,680 --> 01:14:12,698
their particular domain.

1812
01:14:12,699 --> 01:14:15,722
So if you spend some time and get

1813
01:14:15,722 --> 01:14:18,751
familiarized yourself with the structure

1814
01:14:18,751 --> 01:14:20,779
of how Pymdp works, then everything else

1815
01:14:20,779 --> 01:14:23,807
is just writing a jupyter notebook with

1816
01:14:23,807 --> 01:14:26,837
minimal code, right, to simulate this.

1817
01:14:26,838 --> 01:14:29,869
So if you have a particular task in your

1818
01:14:29,869 --> 01:14:32,896
domain, I don't see a problem for a

1819
01:14:32,896 --> 01:14:35,925
beginner to kind of try and code it.

1820
01:14:35,926 --> 01:14:37,949
And what I'm very excited to see is

1821
01:14:37,949 --> 01:14:40,975
people using this module for variety of

1822
01:14:40,975 --> 01:14:43,002
experiments, just like how people started

1823
01:14:43,002 --> 01:14:45,028
using Pymdp and sophisticated inference

1824
01:14:45,028 --> 01:14:47,040
is taking off.

1825
01:14:47,041 --> 01:14:51,086
And it's now widely talked about how it

1826
01:14:51,086 --> 01:14:55,129
is the way of doing active inference.

1827
01:14:56,130 --> 01:14:58,150
And I'm really hoping that people in

1828
01:14:58,150 --> 01:15:00,113
various domains start using this module

1829
01:15:00,113 --> 01:15:02,134
and see their experiments, and I look

1830
01:15:02,134 --> 01:15:04,149
forward for the feedback.

1831
01:15:04,154 --> 01:15:06,179
Yeah, so what Pimdp did two years ago, I'

1832
01:15:06,179 --> 01:15:09,204
'm hoping this module will do to people

1833
01:15:09,204 --> 01:15:11,228
who are trying to model active inference

1834
01:15:11,228 --> 01:15:13,240
in the soap state.

1835
01:15:14,258 --> 01:15:18,296
Daniel: So you mentioned the OpenAI gym

1836
01:15:18,296 --> 01:15:22,332
and the standardized format, and what

1837
01:15:22,332 --> 01:15:25,368
benchmarks do you use or what kind of

1838
01:15:25,368 --> 01:15:29,408
test suites are you comparing, and how do

1839
01:15:29,408 --> 01:15:32,439
we really know when we've made a

1840
01:15:32,439 --> 01:15:36,477
generative model that really exceeds or

1841
01:15:36,477 --> 01:15:40,517
excels in a way that other techniques are

1842
01:15:40,517 --> 01:15:42,538
just not doing?

1843
01:15:43,544 --> 01:15:47,583
Aswin: Yeah, so if I may go to the OpenAI

1844
01:15:47,583 --> 01:15:51,621
gym website, we have several experiments.

1845
01:15:51,621 --> 01:15:51,622
.

1846
01:15:51,622 --> 01:15:53,644
There the classical reinforcement

1847
01:15:53,644 --> 01:15:56,670
learning examples like the lunar lander

1848
01:15:56,670 --> 01:15:58,697
that you see in this screen right now.

1849
01:15:58,697 --> 01:16:02,671
So active inference from its inception

1850
01:16:02,671 --> 01:16:05,706
has faced problems of scaling to tasks.

1851
01:16:05,707 --> 01:16:08,732
And that's in itself a field of research

1852
01:16:08,732 --> 01:16:10,755
in active inference, scaling active

1853
01:16:10,755 --> 01:16:11,762
inference.

1854
01:16:11,763 --> 01:16:13,786
And that's one of the reasons why deep

1855
01:16:13,786 --> 01:16:16,811
active inference took over dealing with

1856
01:16:16,811 --> 01:16:17,822
tasks like this.

1857
01:16:17,823 --> 01:16:19,848
So there are benchmarks even now where

1858
01:16:19,848 --> 01:16:22,873
the sophisticated inference may not be

1859
01:16:22,873 --> 01:16:24,895
able to deal with state spaces.

1860
01:16:24,895 --> 01:16:26,913
And personally, that's my research.

1861
01:16:26,914 --> 01:16:27,923
In my PhD.

1862
01:16:27,924 --> 01:16:30,952
I am actually looking at optimizing

1863
01:16:30,952 --> 01:16:33,984
computations in sophisticated inference

1864
01:16:33,984 --> 01:16:36,013
algorithms that lets you scale up to

1865
01:16:36,013 --> 01:16:38,034
environments like that.

1866
01:16:38,036 --> 01:16:41,063
But to get started, you will have to kind

1867
01:16:41,063 --> 01:16:44,090
of write code and see if it works for an

1868
01:16:44,090 --> 01:16:46,115
environment, then look at if it's not

1869
01:16:46,115 --> 01:16:49,143
working, then you have to look at methods

1870
01:16:49,143 --> 01:16:51,162
to scale it up and so on.

1871
01:16:51,163 --> 01:16:54,192
So if I am talking about benchmarks,

1872
01:16:54,192 --> 01:16:57,226
sophisticated inference is as good as any

1873
01:16:57,226 --> 01:17:00,197
RL algorithms for this state space.

1874
01:17:00,198 --> 01:17:02,217
So for small problems, sophisticated

1875
01:17:02,217 --> 01:17:04,238
inference will work and it's really good.

1876
01:17:04,238 --> 01:17:04,239
.

1877
01:17:05,240 --> 01:17:07,265
But for high dimensional problems like

1878
01:17:07,265 --> 01:17:10,293
this, the classical implementation that I

1879
01:17:10,293 --> 01:17:13,320
just showed might not work, but it's good

1880
01:17:13,320 --> 01:17:15,344
enough for any decent experiment.

1881
01:17:15,345 --> 01:17:18,370
But if you want to scale up, then that's

1882
01:17:18,370 --> 01:17:20,391
still open and it's a new field of

1883
01:17:20,391 --> 01:17:22,415
research and what you do might become a

1884
01:17:22,415 --> 01:17:24,432
next new important paper.

1885
01:17:24,433 --> 01:17:27,465
So that's all I can tell in that regard.

1886
01:17:27,467 --> 01:17:28,472
Cool.

1887
01:17:28,473 --> 01:17:30,495
You have to work and see what.

1888
01:17:30,495 --> 01:17:33,525
Daniel: Measures do you think you'd be

1889
01:17:33,525 --> 01:17:36,557
looking for, like computational resources

1890
01:17:36,557 --> 01:17:39,588
or what are the measures that even make

1891
01:17:39,588 --> 01:17:42,613
sense to juxtapose such different

1892
01:17:42,613 --> 01:17:43,623
methods?

1893
01:17:44,629 --> 01:17:47,663
Aswin: Yeah, so the OpenAI Gym was

1894
01:17:47,663 --> 01:17:51,702
designed for that, to compare different

1895
01:17:51,702 --> 01:17:52,714
algorithms.

1896
01:17:52,716 --> 01:17:54,738
So OpenAI Gym by definition is a

1897
01:17:54,738 --> 01:17:57,761
collection of many environments.

1898
01:17:57,762 --> 01:17:59,782
So in my demo, I was talking about the

1899
01:17:59,782 --> 01:18:00,733
grid environment.

1900
01:18:00,734 --> 01:18:03,762
Openiigm is nothing but a collection of

1901
01:18:03,762 --> 01:18:05,788
many environments which will let you

1902
01:18:05,788 --> 01:18:08,815
interact with those environments using

1903
01:18:08,815 --> 01:18:10,838
the environment step function.

1904
01:18:12,856 --> 01:18:13,862
Yeah.

1905
01:18:13,863 --> 01:18:15,882
So here we have the environment step

1906
01:18:15,882 --> 01:18:17,903
function that will let you interact with

1907
01:18:17,903 --> 01:18:18,914
the lunar lander.

1908
01:18:18,915 --> 01:18:21,943
And that particular task will have

1909
01:18:21,943 --> 01:18:24,977
matrixes that lets you judge how good or

1910
01:18:24,977 --> 01:18:26,997
bad your algorithm is.

1911
01:18:26,998 --> 01:18:29,002
So in this lunar lander problem, how

1912
01:18:29,002 --> 01:18:32,005
optimally can you land your rover between

1913
01:18:32,005 --> 01:18:34,007
these two flags by spending minimizing

1914
01:18:34,007 --> 01:18:36,009
the fuel and so on.

1915
01:18:36,009 --> 01:18:40,013
So those matrices are very tasks specific,

1916
01:18:40,013 --> 01:18:43,016
, and that's one direction you have to

1917
01:18:43,016 --> 01:18:43,016
take.

1918
01:18:43,016 --> 01:18:46,019
You can take try and compete with RL

1919
01:18:46,019 --> 01:18:49,022
algorithms in matrixes, but the right

1920
01:18:49,022 --> 01:18:52,025
potential or the potential I see in

1921
01:18:52,025 --> 01:18:54,027
sophisticated inference is modeling

1922
01:18:54,027 --> 01:18:57,030
intelligent behavior, where in RL the

1923
01:18:57,030 --> 01:19:00,027
focus is to get things done to make this

1924
01:19:00,027 --> 01:19:01,028
work.

1925
01:19:01,028 --> 01:19:03,030
But it's not really explainable,

1926
01:19:03,030 --> 01:19:05,032
especially deep RL and deep learning

1927
01:19:05,032 --> 01:19:06,033
methods.

1928
01:19:06,033 --> 01:19:08,035
But in active inference, if you manage to

1929
01:19:08,035 --> 01:19:11,038
scale it up, they are explainable and

1930
01:19:11,038 --> 01:19:13,040
that will let you understand how

1931
01:19:13,040 --> 01:19:15,042
intelligence emerges with time.

1932
01:19:15,042 --> 01:19:18,045
And I see that more interesting than

1933
01:19:18,045 --> 01:19:21,048
competing with RL, because if your focus

1934
01:19:21,048 --> 01:19:23,050
is getting things done, then maybe

1935
01:19:23,050 --> 01:19:26,053
engineering is the right way and not

1936
01:19:26,053 --> 01:19:27,054
active inference.

1937
01:19:29,056 --> 01:19:30,057
Awesome.

1938
01:19:30,057 --> 01:19:33,060
Daniel: Any other comments or thoughts?

1939
01:19:42,069 --> 01:19:44,071
Aswin, do you have any other comments or

1940
01:19:44,071 --> 01:19:44,071
thoughts?

1941
01:19:45,072 --> 01:19:47,074
Aswin: No, I'm pretty happy.

1942
01:19:47,074 --> 01:19:50,077
I hope I was clear explaining the code.

1943
01:19:50,077 --> 01:19:52,079
Maybe it was too complicated or simple,

1944
01:19:52,079 --> 01:19:55,082
depending upon your level, but I hope it

1945
01:19:55,082 --> 01:19:57,084
is useful to at least one person who

1946
01:19:57,084 --> 01:20:00,081
would start using this and write the code.

1947
01:20:00,081 --> 01:20:00,081
.

1948
01:20:00,081 --> 01:20:02,083
Thank you so much for your time.

1949
01:20:02,083 --> 01:20:02,083
Awesome.

1950
01:20:02,083 --> 01:20:03,084
And thank you for the opportunity.

1951
01:20:05,086 --> 01:20:06,087
Daniel: Thank you for joining.

1952
01:20:06,087 --> 01:20:07,088
Till next time.

1953
01:20:07,088 --> 01:20:08,089
See you.

1954
01:20:09,090 --> 01:20:10,091
Aswin: Thank you so much.

1955
01:20:10,091 --> 01:20:10,091
Bye.


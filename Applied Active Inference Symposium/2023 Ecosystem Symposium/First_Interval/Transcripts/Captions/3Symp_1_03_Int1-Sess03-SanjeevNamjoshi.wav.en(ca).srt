1
00:00:00,410 --> 00:00:03,428
Daniel: All right, the next presentation

2
00:00:03,428 --> 00:00:05,843
is by Sanjeev Namjoshi, and this

3
00:00:05,843 --> 00:00:08,408
presentation is going to be called

4
00:00:08,408 --> 00:00:11,212
developing next gen Active Inference

5
00:00:11,212 --> 00:00:13,453
Tools, broadening Accessibility,

6
00:00:13,453 --> 00:00:16,748
Educational Resources, and the Software

7
00:00:16,748 --> 00:00:17,869
Ecosystem.

8
00:00:22,341 --> 00:00:28,944
I'm going to start this talk right now.

9
00:00:32,368 --> 00:00:34,572
Sanjeev: Hello everyone, and thank you

10
00:00:34,572 --> 00:00:35,657
for being here.

11
00:00:35,671 --> 00:00:37,873
My name is Sanjeev Namjoshi.

12
00:00:37,882 --> 00:00:38,975
Daniel: I'm just going to restart it.

13
00:00:38,978 --> 00:00:40,109
Sanjeev: Machine learning engineer

14
00:00:40,109 --> 00:00:40,183
working at the AI.

15
00:00:40,186 --> 00:00:42,393
Daniel: Services firm just because it's a

16
00:00:42,393 --> 00:00:43,463
little quiet.

17
00:00:47,826 --> 00:00:49,045
All right, restarting the talk.

18
00:00:56,708 --> 00:00:58,907
Sanjeev: Hello everyone, and thank you

19
00:00:58,907 --> 00:00:58,991
for being here.

20
00:00:59,004 --> 00:01:01,663
My name is Sanjeev Namjoshi, and I'm a

21
00:01:01,663 --> 00:01:04,930
machine learning engineer working at the

22
00:01:04,930 --> 00:01:06,169
AI services firm Kung Fu AI, where I

23
00:01:06,169 --> 00:01:08,396
primarily focus on computer vision

24
00:01:08,396 --> 00:01:09,483
projects.

25
00:01:10,533 --> 00:01:12,741
Today I'm going to be talking to you

26
00:01:12,741 --> 00:01:14,937
about my progress toward providing

27
00:01:14,937 --> 00:01:16,157
greater accessibility, visibility, and

28
00:01:16,157 --> 00:01:18,371
knowledge of active inference and the

29
00:01:18,371 --> 00:01:20,522
free energy principle.

30
00:01:20,580 --> 00:01:22,770
I'm excited to be presenting this

31
00:01:22,770 --> 00:01:24,955
material at the active inference

32
00:01:24,955 --> 00:01:26,180
symposium this year because the idea of

33
00:01:26,180 --> 00:01:28,353
an enacted ecosystem of shared

34
00:01:28,353 --> 00:01:30,555
intelligence perfectly captures the

35
00:01:30,555 --> 00:01:32,792
philosophy that underlies the projects I'

36
00:01:32,792 --> 00:01:34,966
'm currently engaged with.

37
00:01:34,984 --> 00:01:37,201
I've spent the last seven months on

38
00:01:37,201 --> 00:01:39,455
sabbatical for my job to work exclusively

39
00:01:39,455 --> 00:01:41,671
on an active inference textbook and

40
00:01:41,671 --> 00:01:43,876
related tools, presenting chapter

41
00:01:43,876 --> 00:01:45,007
presentations, code reviews, and

42
00:01:45,007 --> 00:01:48,031
receiving feedback weekly at the active

43
00:01:48,031 --> 00:01:49,047
inference institute.

44
00:01:50,055 --> 00:01:52,077
The institute has provided a space where

45
00:01:52,077 --> 00:01:54,098
interdisciplinary research can flourish

46
00:01:54,098 --> 00:01:56,117
as the connections and influences of

47
00:01:56,117 --> 00:01:59,140
active inference spread to other fields.

48
00:01:59,141 --> 00:02:01,104
It has consistently fostered the spirit

49
00:02:01,104 --> 00:02:03,126
of collaboration and shared intelligence

50
00:02:03,126 --> 00:02:05,148
that I wish to embody in my own work.

51
00:02:05,149 --> 00:02:08,170
As part of this ecosystem, I intend to

52
00:02:08,170 --> 00:02:09,188
continue closely working with the

53
00:02:09,188 --> 00:02:12,210
Institute to provide materials that will

54
00:02:12,210 --> 00:02:14,231
bring active inference to a much wider

55
00:02:14,231 --> 00:02:14,239
readership.

56
00:02:15,244 --> 00:02:17,266
I originally chose this project when I

57
00:02:17,266 --> 00:02:19,288
saw the great potential in the active

58
00:02:19,288 --> 00:02:22,309
inference field and couldn't help but

59
00:02:22,309 --> 00:02:24,331
make a comparison to the state of deep

60
00:02:24,331 --> 00:02:26,354
learning in 2006, when neural networks

61
00:02:26,354 --> 00:02:28,375
were one of just many possible models

62
00:02:28,375 --> 00:02:30,395
rather than the dominant choice in

63
00:02:30,395 --> 00:02:32,414
academia and in industry.

64
00:02:33,426 --> 00:02:35,449
This, of course, all changed in 2006 when

65
00:02:35,449 --> 00:02:38,470
Hinton and his colleagues released the

66
00:02:38,470 --> 00:02:39,489
Deep Belief Network paper, which is

67
00:02:39,489 --> 00:02:42,512
generally understood as the start of deep

68
00:02:42,512 --> 00:02:44,533
learning as we understand it today.

69
00:02:45,539 --> 00:02:47,564
After some hardware innovations and the

70
00:02:47,564 --> 00:02:50,590
release of the Wellknown ImageNet Library,

71
00:02:50,590 --> 00:02:52,615
, we started to see coverage and success

72
00:02:52,615 --> 00:02:55,640
of AI in the news as well as in academic

73
00:02:55,640 --> 00:02:55,647
research.

74
00:02:56,652 --> 00:02:58,677
But in 2012, deep learning truly provided

75
00:02:58,677 --> 00:03:01,642
its value with AlexNet, and for the first

76
00:03:01,642 --> 00:03:03,666
time, deep learning achieved better than

77
00:03:03,666 --> 00:03:05,688
human performance on image detection

78
00:03:05,688 --> 00:03:06,694
tasks.

79
00:03:08,714 --> 00:03:09,728
But this was just the start.

80
00:03:10,730 --> 00:03:12,751
What followed was a proliferation of deep

81
00:03:12,751 --> 00:03:14,774
learning all across industry and research.

82
00:03:14,774 --> 00:03:14,774
.

83
00:03:14,776 --> 00:03:16,795
I've added in some Wellknown milestones

84
00:03:16,795 --> 00:03:18,812
just to highlight the explosion of

85
00:03:18,812 --> 00:03:20,830
progress in deep learning in the last

86
00:03:20,830 --> 00:03:22,850
decade, though there is so much more here

87
00:03:22,850 --> 00:03:23,863
that we could discuss.

88
00:03:23,868 --> 00:03:25,885
So what about the state of active

89
00:03:25,885 --> 00:03:26,896
inference as a field?

90
00:03:26,897 --> 00:03:28,917
From my perspective, active inference

91
00:03:28,917 --> 00:03:30,936
lies in the same position as deep

92
00:03:30,936 --> 00:03:31,942
learning.

93
00:03:31,942 --> 00:03:34,973
In 2006, influential and on the brink of

94
00:03:34,973 --> 00:03:37,004
exploding in popularity, this paper which

95
00:03:37,004 --> 00:03:40,030
is from contributors at the Active

96
00:03:40,030 --> 00:03:43,059
Inference Institute, shows the current

97
00:03:43,059 --> 00:03:45,089
growth of publications in the institute

98
00:03:45,089 --> 00:03:49,122
and its community in active inference.

99
00:03:49,125 --> 00:03:51,145
In the last three years, the active

100
00:03:51,145 --> 00:03:53,164
inference field has seen a number of

101
00:03:53,164 --> 00:03:54,177
important milestones.

102
00:03:55,185 --> 00:03:57,208
Here I show just a few that broaden the

103
00:03:57,208 --> 00:03:59,228
scope and attention to the field.

104
00:04:00,170 --> 00:04:02,194
We had the first International workshop

105
00:04:02,194 --> 00:04:04,211
on Active Inference in 2020.

106
00:04:04,213 --> 00:04:06,232
We had the first active inference

107
00:04:06,232 --> 00:04:08,256
symposium and the founding of the active

108
00:04:08,256 --> 00:04:09,269
inference institute.

109
00:04:10,270 --> 00:04:12,290
Then called the active inference lab.

110
00:04:12,296 --> 00:04:16,334
We had the release of the Par at all 2022

111
00:04:16,334 --> 00:04:20,370
textbook and the Pymdp Python package,

112
00:04:20,370 --> 00:04:23,407
and I see myself now as perfectly poised

113
00:04:23,407 --> 00:04:27,440
to bring active inference to greater

114
00:04:27,440 --> 00:04:29,467
visibility and attention.

115
00:04:30,471 --> 00:04:32,494
This is in part because of the current

116
00:04:32,494 --> 00:04:34,517
academic interest in deep reinforcement

117
00:04:34,517 --> 00:04:37,541
learning and generative modeling working

118
00:04:37,541 --> 00:04:39,562
alongside the institution and other

119
00:04:39,562 --> 00:04:40,573
organizations.

120
00:04:40,574 --> 00:04:42,595
My aim here is to provide some of the

121
00:04:42,595 --> 00:04:44,615
fundamental materials to capture the

122
00:04:44,615 --> 00:04:46,638
attention and notice of machine learning

123
00:04:46,638 --> 00:04:49,661
researchers and students to bridge this

124
00:04:49,661 --> 00:04:51,683
gap to bring active inference into its

125
00:04:51,683 --> 00:04:52,692
renaissance.

126
00:04:55,719 --> 00:04:57,743
To this end, I've been working for the

127
00:04:57,743 --> 00:04:59,768
past seven months on Sabbatical to finish

128
00:04:59,768 --> 00:05:02,731
work on a comprehensive textbook.

129
00:05:02,736 --> 00:05:04,758
The aim of the textbook is to provide the

130
00:05:04,758 --> 00:05:06,778
tools to bring active inference to a

131
00:05:06,778 --> 00:05:08,796
wider audience, primarily those in

132
00:05:08,796 --> 00:05:10,817
machine learning research and applied

133
00:05:10,817 --> 00:05:12,838
fields such as robotics, and to decrease

134
00:05:12,838 --> 00:05:14,859
the challenge in learning the material

135
00:05:14,859 --> 00:05:17,882
largely by separating it from much of the

136
00:05:17,882 --> 00:05:19,904
neuroscience background that is usually a

137
00:05:19,904 --> 00:05:20,916
prerequisite.

138
00:05:21,925 --> 00:05:23,947
This decrease in prerequisites means labs

139
00:05:23,947 --> 00:05:25,966
will have to spend less time helping

140
00:05:25,966 --> 00:05:27,985
students becoming acquainted with the

141
00:05:27,985 --> 00:05:29,003
field, and researchers outside of

142
00:05:29,003 --> 00:05:31,021
neuroscience will find this book an

143
00:05:31,021 --> 00:05:32,038
accessible entry point that uses

144
00:05:32,038 --> 00:05:34,059
terminology familiar to machine learning

145
00:05:34,059 --> 00:05:37,080
rather than neuroscience and fMRI image

146
00:05:37,080 --> 00:05:37,089
analysis.

147
00:05:38,098 --> 00:05:41,121
All derivations are in one place

148
00:05:41,121 --> 00:05:42,139
currently in the field.

149
00:05:43,140 --> 00:05:44,157
Many derivations are spread across

150
00:05:44,157 --> 00:05:46,177
different papers, even behavioral papers

151
00:05:46,177 --> 00:05:48,197
instead of just technical ones, and it's

152
00:05:48,197 --> 00:05:50,218
hard to know where to look if you want to

153
00:05:50,218 --> 00:05:52,236
understand a particular equation or

154
00:05:52,236 --> 00:05:53,242
concept.

155
00:05:54,255 --> 00:05:57,279
Part of the success of deep learning in

156
00:05:57,279 --> 00:05:59,302
the last decade has come directly from

157
00:05:59,302 --> 00:06:01,264
focusing on narrow improvements to

158
00:06:01,264 --> 00:06:03,286
specific aspects of the modeling.

159
00:06:04,289 --> 00:06:06,313
There are many open questions in areas of

160
00:06:06,313 --> 00:06:08,334
research, such as how to prune policy

161
00:06:08,334 --> 00:06:10,350
trees, exploring second order

162
00:06:10,350 --> 00:06:11,369
optimization rules for state and

163
00:06:11,369 --> 00:06:14,390
parameter updates, and scaling active

164
00:06:14,390 --> 00:06:14,398
inference.

165
00:06:15,406 --> 00:06:17,424
The increased accessibility for

166
00:06:17,424 --> 00:06:19,447
researchers would also lead to many new

167
00:06:19,447 --> 00:06:22,471
industry applications, such as autonomous

168
00:06:22,471 --> 00:06:24,493
vehicles, robotics, video game design,

169
00:06:24,493 --> 00:06:25,500
and AI.

170
00:06:25,508 --> 00:06:28,531
The textbook would also put a spotlight

171
00:06:28,531 --> 00:06:30,550
on Bayesian mechanics and invite

172
00:06:30,550 --> 00:06:32,570
contributors and contributions from

173
00:06:32,570 --> 00:06:34,591
researchers as this exciting nascent

174
00:06:34,591 --> 00:06:35,608
field grows and develops.

175
00:06:37,623 --> 00:06:39,648
Part Four is largely a literature review

176
00:06:39,648 --> 00:06:42,673
and can be very helpful to those writing

177
00:06:42,673 --> 00:06:44,698
about active inference from fields such

178
00:06:44,698 --> 00:06:47,723
as philosophy, psychology, sociology, and

179
00:06:47,723 --> 00:06:48,733
many others.

180
00:06:48,735 --> 00:06:50,758
And the historical context sections that

181
00:06:50,758 --> 00:06:53,780
are part of this book provide a lot of

182
00:06:53,780 --> 00:06:55,801
that context, as Active Inference is

183
00:06:55,801 --> 00:06:57,820
built upon decades of research in

184
00:06:57,820 --> 00:06:59,843
neuroscience, psychology, and many other

185
00:06:59,843 --> 00:07:01,806
fields, and also draws upon current work

186
00:07:01,806 --> 00:07:03,829
in many fields that have emerged in the

187
00:07:03,829 --> 00:07:05,844
last 25 to 30 years.

188
00:07:06,857 --> 00:07:09,881
Finally, LaTech reproducibility may offer

189
00:07:09,881 --> 00:07:11,904
interesting ways to rearrange the book

190
00:07:11,904 --> 00:07:13,926
and integrate it with the code for an

191
00:07:13,926 --> 00:07:15,941
online only experience.

192
00:07:19,989 --> 00:07:21,009
Now I'd like to share with you some of

193
00:07:21,009 --> 00:07:23,027
the progress of my textbook and the

194
00:07:23,027 --> 00:07:24,038
general structure.

195
00:07:25,040 --> 00:07:27,067
The textbook is divided into four parts.

196
00:07:27,068 --> 00:07:30,091
The first part introduces fundamental

197
00:07:30,091 --> 00:07:31,108
concepts to set the stage.

198
00:07:32,109 --> 00:07:34,132
In particular, I have focused here on

199
00:07:34,132 --> 00:07:36,156
presenting wellknown statistical ideas

200
00:07:36,156 --> 00:07:39,181
from the perspective of an agent modeling

201
00:07:39,181 --> 00:07:41,207
its environment, who states it must infer

202
00:07:41,207 --> 00:07:43,228
from an observed noisy signal.

203
00:07:44,236 --> 00:07:47,261
The second part focuses on continuous and

204
00:07:47,261 --> 00:07:49,282
discrete state space formulations of

205
00:07:49,282 --> 00:07:51,306
active inference, where the algorithm of

206
00:07:51,306 --> 00:07:53,328
focus for the continuous state space

207
00:07:53,328 --> 00:07:55,347
formulation is active generalized

208
00:07:55,347 --> 00:07:56,356
filtering.

209
00:07:57,360 --> 00:07:59,382
Part three, which I'll begin writing in a

210
00:07:59,382 --> 00:08:01,344
couple of months, focuses on a sketch of

211
00:08:01,344 --> 00:08:03,362
Bayesian mechanics and the required

212
00:08:03,362 --> 00:08:05,382
background designed with the knowledge

213
00:08:05,382 --> 00:08:07,404
this field is still dynamically, changing

214
00:08:07,404 --> 00:08:08,414
and evolving.

215
00:08:08,419 --> 00:08:10,437
Here I will focus on some of the

216
00:08:10,437 --> 00:08:13,460
fundamental concepts and ideas, as well

217
00:08:13,460 --> 00:08:15,483
as code simulations to allow readers to

218
00:08:15,483 --> 00:08:17,501
get a deeper and more intuitive

219
00:08:17,501 --> 00:08:18,519
understanding of some of these

220
00:08:18,519 --> 00:08:20,532
challenging ideas.

221
00:08:21,539 --> 00:08:22,557
Finally, part four is a systematic

222
00:08:22,557 --> 00:08:24,575
literature review that covers all the

223
00:08:24,575 --> 00:08:26,594
various applications and extensions to

224
00:08:26,594 --> 00:08:28,615
active inference that have been innovated

225
00:08:28,615 --> 00:08:30,633
in the last six to eight years.

226
00:08:30,634 --> 00:08:32,657
These applications include things like

227
00:08:32,657 --> 00:08:35,681
robotics, all the behavioral modeling and

228
00:08:35,681 --> 00:08:37,703
neuropsychiatry, and human and animal

229
00:08:37,703 --> 00:08:39,726
behavior, theory of mind, and so on.

230
00:08:40,736 --> 00:08:42,754
The extensions talk about how

231
00:08:42,754 --> 00:08:45,780
applications of active inference can be

232
00:08:45,780 --> 00:08:47,805
used to talk about dynamic systems more

233
00:08:47,805 --> 00:08:49,827
generally, and apply to things like

234
00:08:49,827 --> 00:08:52,851
ecosystems and to economies and other

235
00:08:52,851 --> 00:08:54,877
types of things like governance, and so

236
00:08:54,877 --> 00:08:55,882
on.

237
00:08:56,897 --> 00:08:59,921
As of this week, the rough drafts of part

238
00:08:59,921 --> 00:09:01,881
one and two are complete, and ten

239
00:09:01,881 --> 00:09:03,901
chapters have been presented to the

240
00:09:03,901 --> 00:09:04,919
Active Inference Institute.

241
00:09:05,927 --> 00:09:07,949
In support of this textbook are four

242
00:09:07,949 --> 00:09:10,972
separate tools that I will discuss over

243
00:09:10,972 --> 00:09:11,986
the next few slides.

244
00:09:13,002 --> 00:09:15,025
But before I do that, I would like to

245
00:09:15,025 --> 00:09:17,049
first highlight some special aspects and

246
00:09:17,049 --> 00:09:20,074
features of my approach to this textbook.

247
00:09:20,074 --> 00:09:20,076
.

248
00:09:24,113 --> 00:09:26,138
The major focus is on writing this book

249
00:09:26,138 --> 00:09:28,159
for a machine learning audience or

250
00:09:28,159 --> 00:09:31,183
students learning in this and adjacent

251
00:09:31,183 --> 00:09:31,189
areas.

252
00:09:32,190 --> 00:09:34,213
Neuroscience is out of scope for this

253
00:09:34,213 --> 00:09:34,217
book.

254
00:09:34,218 --> 00:09:37,242
Many of the recommended and even optional

255
00:09:37,242 --> 00:09:39,263
prerequisites that are shown here are

256
00:09:39,263 --> 00:09:41,287
typically known by undergraduate students

257
00:09:41,287 --> 00:09:44,310
in science and engineering and certainly

258
00:09:44,310 --> 00:09:46,334
by graduate students in these fields.

259
00:09:46,337 --> 00:09:49,361
Nonetheless, the book is written to be

260
00:09:49,361 --> 00:09:51,386
readable by those that desire to focus on

261
00:09:51,386 --> 00:09:54,409
everything that is, the math, the code,

262
00:09:54,409 --> 00:09:56,432
the concepts, those that just want to

263
00:09:56,432 --> 00:09:58,456
focus on the math but are not interested

264
00:09:58,456 --> 00:10:01,420
in implementation, and even those that

265
00:10:01,420 --> 00:10:03,443
may skim over the math and just try to

266
00:10:03,443 --> 00:10:05,467
understand the ideas intuitively.

267
00:10:09,505 --> 00:10:11,528
One thing that's very important to me in

268
00:10:11,528 --> 00:10:14,552
trying to express these ideas clearly is

269
00:10:14,552 --> 00:10:16,574
by spending a lot of time working on

270
00:10:16,574 --> 00:10:18,595
typesetting and style, which is very

271
00:10:18,595 --> 00:10:20,617
important to successful learning.

272
00:10:20,618 --> 00:10:23,643
So I would spend a lot of time attempting

273
00:10:23,643 --> 00:10:25,664
to make my work clear and readable.

274
00:10:26,674 --> 00:10:29,701
To this end, I have margins which collect

275
00:10:29,701 --> 00:10:31,727
specific key terms for references later,

276
00:10:31,727 --> 00:10:33,748
which you can see in some of these

277
00:10:33,748 --> 00:10:36,773
figures that are shown here, and these

278
00:10:36,773 --> 00:10:38,798
terms will eventually correspond to the

279
00:10:38,798 --> 00:10:41,822
ontology project ongoing at the Active

280
00:10:41,822 --> 00:10:43,839
inference institute.

281
00:10:44,851 --> 00:10:46,875
Margins also provide further explanation

282
00:10:46,875 --> 00:10:49,899
to accompany the text, and this will be

283
00:10:49,899 --> 00:10:51,922
useful to readers who want more detail

284
00:10:51,922 --> 00:10:52,934
and explanation.

285
00:10:54,950 --> 00:10:56,970
There's a large focus on building an

286
00:10:56,970 --> 00:10:58,992
intuitive understanding of the concepts.

287
00:10:58,994 --> 00:11:00,959
For example, importantly, all algorithms

288
00:11:00,959 --> 00:11:02,977
are explained from scratch.

289
00:11:02,978 --> 00:11:05,001
In this book, we typically start with a

290
00:11:05,001 --> 00:11:07,022
description of the agent environment

291
00:11:07,022 --> 00:11:08,033
modeling problem.

292
00:11:08,035 --> 00:11:11,060
We then start the book with a univariate

293
00:11:11,060 --> 00:11:13,084
case, extended the multivariate case,

294
00:11:13,084 --> 00:11:16,110
then we introduce variational inference.

295
00:11:16,111 --> 00:11:18,136
We add dynamics, generalized coordinates

296
00:11:18,136 --> 00:11:21,160
where applicable, hierarchical models,

297
00:11:21,160 --> 00:11:23,181
action and also learning and other

298
00:11:23,181 --> 00:11:25,207
modifications we might add to our models.

299
00:11:25,207 --> 00:11:25,209
.

300
00:11:27,227 --> 00:11:30,254
We have a very big focus on figures

301
00:11:30,254 --> 00:11:33,282
clearly walking the reader through the

302
00:11:33,282 --> 00:11:36,311
text and giving detailed visualizations

303
00:11:36,311 --> 00:11:39,342
of important concepts and in terms of how

304
00:11:39,342 --> 00:11:41,362
the textbook is set up.

305
00:11:41,363 --> 00:11:43,384
The early part of the book focuses on

306
00:11:43,384 --> 00:11:45,404
basic concepts such as hidden state

307
00:11:45,404 --> 00:11:47,423
estimation, that is, estimating the

308
00:11:47,423 --> 00:11:49,444
conditional distribution of a latent

309
00:11:49,444 --> 00:11:51,465
variable given some observed data.

310
00:11:52,476 --> 00:11:55,500
The aim here is to explain the modeling

311
00:11:55,500 --> 00:11:57,522
paradigm in the context of an agent

312
00:11:57,522 --> 00:11:59,545
attempting to infer the states of the

313
00:11:59,545 --> 00:12:01,508
environment, that is, the interaction

314
00:12:01,508 --> 00:12:03,528
between a generative model and a

315
00:12:03,528 --> 00:12:06,552
generative process, a perspective that

316
00:12:06,552 --> 00:12:08,577
differs from the Bayesian inference style

317
00:12:08,577 --> 00:12:11,601
that is normally taught in universities

318
00:12:11,601 --> 00:12:13,626
and many introductory textbooks.

319
00:12:15,643 --> 00:12:17,664
Typically, introductory textbooks on

320
00:12:17,664 --> 00:12:19,685
Bayesian inference focus on parameter

321
00:12:19,685 --> 00:12:21,706
estimation or learning, and part one

322
00:12:21,706 --> 00:12:23,729
introduces the expectation maximization

323
00:12:23,729 --> 00:12:25,748
algorithm as a way to explain the

324
00:12:25,748 --> 00:12:28,771
connection and separation between hidden

325
00:12:28,771 --> 00:12:30,792
state inference on the one hand and

326
00:12:30,792 --> 00:12:32,814
parameter learning on the other.

327
00:12:34,833 --> 00:12:36,852
Additionally, a large focus has been

328
00:12:36,852 --> 00:12:38,874
placed on variational inference, which is

329
00:12:38,874 --> 00:12:40,891
explained in detail, and the book

330
00:12:40,891 --> 00:12:42,911
features a catalog of all the different

331
00:12:42,911 --> 00:12:44,930
forms of variational free energy and

332
00:12:44,930 --> 00:12:46,950
expected free energy in the literature

333
00:12:46,950 --> 00:12:48,971
and how they can all be derived from one

334
00:12:48,971 --> 00:12:49,979
another.

335
00:12:51,006 --> 00:12:54,032
The book also covers Rau and Ballard

336
00:12:54,032 --> 00:12:57,060
style predictive coding terms and ideas

337
00:12:57,060 --> 00:12:59,085
such as key ideas such as prediction

338
00:12:59,085 --> 00:13:02,054
error minimization as well as clear and

339
00:13:02,054 --> 00:13:05,080
intuitive explanations of fundamental

340
00:13:05,080 --> 00:13:07,103
concepts such as surprisal.

341
00:13:08,112 --> 00:13:10,135
The textbook focuses heavily on building

342
00:13:10,135 --> 00:13:12,157
intuition through derivation and the

343
00:13:12,157 --> 00:13:15,180
general flow of most of the chapters is

344
00:13:15,180 --> 00:13:17,202
to set up the problem that needs to be

345
00:13:17,202 --> 00:13:19,225
solved which is defining an interaction

346
00:13:19,225 --> 00:13:22,251
between the agent and the environment.

347
00:13:22,253 --> 00:13:24,275
Showing the elements needed to solve it,

348
00:13:24,275 --> 00:13:26,297
which is usually random variables and

349
00:13:26,297 --> 00:13:29,320
parameters that form a joint distribution

350
00:13:29,320 --> 00:13:31,344
or generative model replacing probability

351
00:13:31,344 --> 00:13:33,363
distributions with their algebraic

352
00:13:33,363 --> 00:13:35,386
formulations and then moving through the

353
00:13:35,386 --> 00:13:37,408
algebra to a final analytic or gradient

354
00:13:37,408 --> 00:13:39,421
based equation.

355
00:13:39,429 --> 00:13:41,446
The readers should be able to recognize

356
00:13:41,446 --> 00:13:43,461
most equations in the literature.

357
00:13:43,462 --> 00:13:46,489
Upon reading this book, I'm also making

358
00:13:46,489 --> 00:13:48,517
extensive usage of Bayesian networks and

359
00:13:48,517 --> 00:13:51,541
other types and styles of graphical

360
00:13:51,541 --> 00:13:53,569
models such as factor graphs will appear

361
00:13:53,569 --> 00:13:55,580
in part four.

362
00:13:56,593 --> 00:13:58,618
There are hundreds of custom figures that

363
00:13:58,618 --> 00:14:01,584
have been created so far, and the figures

364
00:14:01,584 --> 00:14:03,607
are detailed to give the readers a deep

365
00:14:03,607 --> 00:14:06,631
understanding of the different types of

366
00:14:06,631 --> 00:14:08,655
content that is covered throughout the

367
00:14:08,655 --> 00:14:10,678
books, and also summarizes much of the

368
00:14:10,678 --> 00:14:12,699
information and equations that are

369
00:14:12,699 --> 00:14:15,724
pervasive throughout the active inference

370
00:14:15,724 --> 00:14:16,736
literature.

371
00:14:18,755 --> 00:14:20,775
Another important focus of the textbook

372
00:14:20,775 --> 00:14:22,793
is that many of the models that are

373
00:14:22,793 --> 00:14:24,812
presented are also shown in pseudocode,

374
00:14:24,812 --> 00:14:25,828
which should aid the reader in

375
00:14:25,828 --> 00:14:26,837
implementation.

376
00:14:27,847 --> 00:14:30,874
And finally, each chapter is filled with

377
00:14:30,874 --> 00:14:32,896
numerous what I call experiments.

378
00:14:32,897 --> 00:14:34,918
And these experiments correspond to the

379
00:14:34,918 --> 00:14:36,937
Jupyter notebook and try to show the

380
00:14:36,937 --> 00:14:38,957
application of a concept in a simulated

381
00:14:38,957 --> 00:14:39,965
environment.

382
00:14:39,967 --> 00:14:41,985
So a lot of these experiments start out

383
00:14:41,985 --> 00:14:42,995
by generating data.

384
00:14:42,996 --> 00:14:44,015
So we have some kind of generative

385
00:14:44,015 --> 00:14:46,038
process and then we have that data passed

386
00:14:46,038 --> 00:14:49,060
to a generative model or the agent which

387
00:14:49,060 --> 00:14:51,081
then attempts to either perceive and

388
00:14:51,081 --> 00:14:53,105
learn from that data and even act on it.

389
00:14:53,107 --> 00:14:56,132
And the example that's shown on the right

390
00:14:56,132 --> 00:14:58,155
margin here, this is just a perception

391
00:14:58,155 --> 00:15:00,119
problem on a continuous grid which has

392
00:15:00,119 --> 00:15:03,144
been divided into pieces for the purpose

393
00:15:03,144 --> 00:15:05,165
of the simulation, but represents a

394
00:15:05,165 --> 00:15:07,182
continuous state space.

395
00:15:07,184 --> 00:15:09,208
And the agent in the bottom left corner,

396
00:15:09,208 --> 00:15:12,230
shown as a mouse, has a prior belief

397
00:15:12,230 --> 00:15:14,253
about where some reward food is and its

398
00:15:14,253 --> 00:15:15,262
environment.

399
00:15:15,263 --> 00:15:17,286
But it then needs to perceive from

400
00:15:17,286 --> 00:15:20,312
sensory data that it observes the true

401
00:15:20,312 --> 00:15:22,339
location of that reward or food which is

402
00:15:22,339 --> 00:15:25,365
obscured or occluded in some way by the

403
00:15:25,365 --> 00:15:28,390
mist that's shown in that figure.

404
00:15:28,392 --> 00:15:30,414
So these types of experiments give the

405
00:15:30,414 --> 00:15:32,437
reader a better sense of how to apply

406
00:15:32,437 --> 00:15:35,460
these statistical ideas to a real world

407
00:15:35,460 --> 00:15:37,482
situation so we can understand how it

408
00:15:37,482 --> 00:15:39,505
might apply to some kind of autonomous

409
00:15:39,505 --> 00:15:40,512
agent.

410
00:15:42,538 --> 00:15:45,560
Next, I'd like to cover and shift my

411
00:15:45,560 --> 00:15:47,583
attention toward Jupyter notebooks and

412
00:15:47,583 --> 00:15:49,604
videos and like to note that upon

413
00:15:49,604 --> 00:15:52,629
publication of the textbook these Jupyter

414
00:15:52,629 --> 00:15:54,654
notebooks will be released on GitHub and

415
00:15:54,654 --> 00:15:57,679
should be fully reproducible using Docker

416
00:15:57,679 --> 00:15:59,703
and other version handling tools.

417
00:16:00,657 --> 00:16:03,683
One of the big emphasis on the Jupyter

418
00:16:03,683 --> 00:16:05,706
notebooks is it has to be a direct

419
00:16:05,706 --> 00:16:08,733
correspondence between the equations and

420
00:16:08,733 --> 00:16:11,761
explanations in the code and in the text.

421
00:16:11,761 --> 00:16:11,763
.

422
00:16:11,765 --> 00:16:13,787
This will build a direct understanding

423
00:16:13,787 --> 00:16:15,808
and show applications of the concepts

424
00:16:15,808 --> 00:16:16,814
explained.

425
00:16:18,833 --> 00:16:21,862
Notebooks are filled with simulations and

426
00:16:21,862 --> 00:16:24,890
visualizations many that appear in the

427
00:16:24,890 --> 00:16:24,898
main text.

428
00:16:27,919 --> 00:16:29,945
And in addition to that, I have also over

429
00:16:29,945 --> 00:16:31,969
the last seven months been presenting

430
00:16:31,969 --> 00:16:34,991
chapter presentations to the Active

431
00:16:34,991 --> 00:16:35,005
Inference Institute.

432
00:16:35,007 --> 00:16:38,033
So far, a draft version of the first ten

433
00:16:38,033 --> 00:16:40,058
chapters of the book have been recorded

434
00:16:40,058 --> 00:16:43,082
at the Active Inference Institute and

435
00:16:43,082 --> 00:16:45,108
these are just some sample slides that I'

436
00:16:45,108 --> 00:16:48,133
've prepared that try to explain these

437
00:16:48,133 --> 00:16:50,151
concepts in great detail.

438
00:16:51,160 --> 00:16:52,178
In the final stages of writing this

439
00:16:52,178 --> 00:16:54,196
textbook, video lectures will be re

440
00:16:54,196 --> 00:16:56,218
recorded and released alongside the book.

441
00:16:56,218 --> 00:16:56,219
.

442
00:16:57,221 --> 00:16:59,241
I also plan to create detailed code

443
00:16:59,241 --> 00:17:01,204
walkthrough videos that walk through the

444
00:17:01,204 --> 00:17:03,223
different examples in the Jupyter

445
00:17:03,223 --> 00:17:04,230
notebooks.

446
00:17:06,252 --> 00:17:08,278
Now I'd like to talk about a few planned

447
00:17:08,278 --> 00:17:11,300
future resources I'd like to work on

448
00:17:11,300 --> 00:17:13,326
after the book is complete, or toward the

449
00:17:13,326 --> 00:17:16,351
final stages of the book to have further

450
00:17:16,351 --> 00:17:18,373
support and educational tools.

451
00:17:19,389 --> 00:17:22,418
Some of these planned future resources

452
00:17:22,418 --> 00:17:25,446
include a software suite in Python to

453
00:17:25,446 --> 00:17:28,476
enable an alternative learning approach

454
00:17:28,476 --> 00:17:31,507
for those who do not wish to learn about

455
00:17:31,507 --> 00:17:34,535
the algorithms from scratch, and this

456
00:17:34,535 --> 00:17:37,563
will expand the possible landscape of

457
00:17:37,563 --> 00:17:40,595
engagement as Pymdp already exists.

458
00:17:40,596 --> 00:17:42,614
I will not be working on a discrete

459
00:17:42,614 --> 00:17:44,634
statespace Python package, but I'd like

460
00:17:44,634 --> 00:17:46,655
to fill the space for things like active

461
00:17:46,655 --> 00:17:48,675
generalized filtering, and also just an

462
00:17:48,675 --> 00:17:50,693
availability of different types of

463
00:17:50,693 --> 00:17:52,714
simulations of Bayesian mechanics as it's

464
00:17:52,714 --> 00:17:54,734
currently defined today, or at least the

465
00:17:54,734 --> 00:17:56,755
different versions and varieties of some

466
00:17:56,755 --> 00:17:58,771
of those key concepts.

467
00:17:59,783 --> 00:18:01,741
I'm also very much interested in

468
00:18:01,741 --> 00:18:03,764
interactive learning, and my aim would be

469
00:18:03,764 --> 00:18:05,786
to have these preset simulations concepts

470
00:18:05,786 --> 00:18:07,808
that are explained in text, with various

471
00:18:07,808 --> 00:18:09,829
simulations interspersed in the form of

472
00:18:09,829 --> 00:18:12,852
plots and demos and other visualizations.

473
00:18:12,852 --> 00:18:12,855
.

474
00:18:12,856 --> 00:18:14,877
And the idea would be that the user could

475
00:18:14,877 --> 00:18:16,895
manipulate sliders and knobs to tweak

476
00:18:16,895 --> 00:18:18,916
various parameters that would help aid in

477
00:18:18,916 --> 00:18:20,936
learning as they get a feel for how these

478
00:18:20,936 --> 00:18:22,955
systems behave, especially ones most of

479
00:18:22,955 --> 00:18:24,976
these systems we talk about are dynamics,

480
00:18:24,976 --> 00:18:26,997
so seeing how they change over time.

481
00:18:30,003 --> 00:18:32,005
The Active Inference Institute has made

482
00:18:32,005 --> 00:18:35,007
tremendous progress in the past couple of

483
00:18:35,007 --> 00:18:36,009
years to provide a collaborative

484
00:18:36,009 --> 00:18:38,011
environment for researchers and for

485
00:18:38,011 --> 00:18:40,013
students of active inference.

486
00:18:40,013 --> 00:18:43,016
I hope to be part of this ecosystem as I

487
00:18:43,016 --> 00:18:44,017
continue to support the spirit of

488
00:18:44,017 --> 00:18:47,020
accessibility and collaboration, and I'm

489
00:18:47,020 --> 00:18:49,022
excited to continue to contribute to this

490
00:18:49,022 --> 00:18:51,024
ecosystem of shared intelligence and look

491
00:18:51,024 --> 00:18:54,026
forward to what we can build together.

492
00:18:58,031 --> 00:19:00,027
I would like to thank the Active

493
00:19:00,027 --> 00:19:01,028
Inference Institute for hosting my

494
00:19:01,028 --> 00:19:03,030
presentations, code reviews, and feedback

495
00:19:03,030 --> 00:19:05,032
sessions and inviting me to present at

496
00:19:05,032 --> 00:19:08,035
the symposium, and also thank my employer,

497
00:19:08,035 --> 00:19:10,037
, Kung Fu AI, for letting me take time

498
00:19:10,037 --> 00:19:12,039
off to write for the past seven months.

499
00:19:13,040 --> 00:19:15,042
Please feel free to contact me at any

500
00:19:15,042 --> 00:19:15,042
time.

501
00:19:15,042 --> 00:19:17,044
Email is the easiest way, but I'm also

502
00:19:17,044 --> 00:19:20,047
available on the active inference

503
00:19:20,047 --> 00:19:21,048
institute discord.

504
00:19:21,048 --> 00:19:23,050
If you would like access to the textbook

505
00:19:23,050 --> 00:19:25,052
and related materials, please send an

506
00:19:25,052 --> 00:19:27,054
email requesting access and I can get you

507
00:19:27,054 --> 00:19:28,055
set up.

508
00:19:28,055 --> 00:19:29,056
And that's all I have for today.

509
00:19:29,056 --> 00:19:31,057
Thank you very much.

510
00:19:35,062 --> 00:19:35,062
Daniel: Awesome.

511
00:19:35,062 --> 00:19:36,063
All right.

512
00:19:36,063 --> 00:19:37,064
Thank you, Sanjeev.


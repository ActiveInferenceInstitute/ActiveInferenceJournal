1
00:00:01,209 --> 00:00:02,998
Daniel: All right.

2
00:00:08,072 --> 00:00:09,073
Greetings, Takuya.

3
00:00:10,163 --> 00:00:11,237
Takuya: Hi.

4
00:00:14,498 --> 00:00:16,755
Daniel: You can mute the live stream or

5
00:00:16,755 --> 00:00:19,025
turn off the other live stream, but yeah,

6
00:00:19,025 --> 00:00:20,184
thank you for joining.

7
00:00:22,305 --> 00:00:25,625
Takuya: Yes, thank you for inviting thank

8
00:00:25,625 --> 00:00:26,719
you for it.

9
00:00:26,726 --> 00:00:28,912
Yeah, it.

10
00:00:30,141 --> 00:00:35,623
Daniel: Well, we have a little bit 35 or

11
00:00:35,623 --> 00:00:39,070
38 minutes, so it would be awesome to

12
00:00:39,070 --> 00:00:42,371
have your presentation.

13
00:00:45,645 --> 00:00:46,720
Takuya: Great.

14
00:00:46,757 --> 00:00:59,057
So the moment well, can you see my.

15
00:01:01,675 --> 00:01:03,841
Daniel: I saw PowerPoint.

16
00:01:06,187 --> 00:01:07,262
Takuya: Okay.

17
00:01:09,483 --> 00:01:11,685
Can you see my screen?

18
00:01:12,730 --> 00:01:13,805
Daniel: Perfect.

19
00:01:14,922 --> 00:01:14,997
Takuya: Okay.

20
00:01:16,146 --> 00:01:18,328
So shall we start?

21
00:01:18,343 --> 00:01:18,389
Daniel: Yes.

22
00:01:18,397 --> 00:01:19,485
Thank you.

23
00:01:21,659 --> 00:01:22,734
Takuya: Then.

24
00:01:22,751 --> 00:01:27,214
Thank you for organizing this wonderful

25
00:01:27,214 --> 00:01:28,344
symposium.

26
00:01:28,356 --> 00:01:32,761
Today I'd like to talk about relationship

27
00:01:32,761 --> 00:01:36,117
between canonical neural network and

28
00:01:36,117 --> 00:01:40,502
active inference and possible extension

29
00:01:40,502 --> 00:01:43,799
for modeling social and shared

30
00:01:43,799 --> 00:01:46,014
intelligence using canonical neural

31
00:01:46,014 --> 00:01:47,027
network.

32
00:01:47,028 --> 00:01:49,046
So let's start.

33
00:01:56,111 --> 00:01:59,140
As you know, the free energy principle is

34
00:01:59,140 --> 00:02:01,108
proposed by Karl Friston that states that

35
00:02:01,108 --> 00:02:04,134
perception, learning and action of all

36
00:02:04,134 --> 00:02:07,160
biological organisms are determined to

37
00:02:07,160 --> 00:02:09,186
minimize variational free energy as a

38
00:02:09,186 --> 00:02:12,215
tractable proxy for surprise minimization.

39
00:02:12,215 --> 00:02:12,218
.

40
00:02:13,219 --> 00:02:17,263
And by this process organism can perform

41
00:02:17,263 --> 00:02:21,302
parational BJ inference or external

42
00:02:21,302 --> 00:02:23,320
mineral state.

43
00:02:23,326 --> 00:02:27,360
And this can't even just show one example

44
00:02:27,360 --> 00:02:30,390
typical set up under the free energy

45
00:02:30,390 --> 00:02:32,414
principle active inference.

46
00:02:32,415 --> 00:02:36,452
Here there is a hidden state in the

47
00:02:36,452 --> 00:02:40,492
external world and only a part of this

48
00:02:40,492 --> 00:02:43,519
state can be observable.

49
00:02:43,523 --> 00:02:47,560
For our agent, this doc and this

50
00:02:47,560 --> 00:02:51,606
transformation is done by genetic model

51
00:02:51,606 --> 00:02:56,651
parameterized by theta and to infer the

52
00:02:56,651 --> 00:03:00,637
hidden state, agent need to reconstruct

53
00:03:00,637 --> 00:03:05,681
the copy of the external state called

54
00:03:05,681 --> 00:03:07,706
posterior relief.

55
00:03:07,707 --> 00:03:10,734
And this optimization is done by

56
00:03:10,734 --> 00:03:13,764
minimizing variational free energy.

57
00:03:14,774 --> 00:03:17,802
And parameter is also optimized by

58
00:03:17,802 --> 00:03:20,834
minimizing free energy to obtain an apt

59
00:03:20,834 --> 00:03:23,862
generative model to represent this

60
00:03:23,862 --> 00:03:24,875
relationship.

61
00:03:25,880 --> 00:03:27,907
An interesting aspect of the free energy

62
00:03:27,907 --> 00:03:30,929
principle active inference is its

63
00:03:30,929 --> 00:03:32,958
application to the optimization of action.

64
00:03:32,958 --> 00:03:33,960
.

65
00:03:33,961 --> 00:03:37,008
Here our agent have some preference prior

66
00:03:37,008 --> 00:03:42,052
c and to obtain this observation in the

67
00:03:42,052 --> 00:03:46,098
future, agent may select the action that

68
00:03:46,098 --> 00:03:51,143
minimize the expected free energy in the

69
00:03:51,143 --> 00:03:55,186
future to obtain the most predictable

70
00:03:55,186 --> 00:03:57,200
outcome.

71
00:03:57,207 --> 00:04:01,182
And finally, by selecting actions that

72
00:04:01,182 --> 00:04:04,215
minimize the expect frequency, it can

73
00:04:04,215 --> 00:04:06,232
obtain the feed.

74
00:04:06,233 --> 00:04:09,263
So, this is a typical setup under active

75
00:04:09,263 --> 00:04:10,271
inference.

76
00:04:10,272 --> 00:04:12,298
But question is what is the neural

77
00:04:12,298 --> 00:04:15,324
neurosurge that can implement this

78
00:04:15,324 --> 00:04:16,332
process?

79
00:04:16,333 --> 00:04:20,371
So that is our interest and to address

80
00:04:20,371 --> 00:04:24,412
this issue we propose theory as follows.

81
00:04:24,413 --> 00:04:28,453
Here we consider that external world is

82
00:04:28,453 --> 00:04:32,493
parameterized characterized by a set of

83
00:04:32,493 --> 00:04:36,532
the variables like this here, this is a

84
00:04:36,532 --> 00:04:40,574
posterior expectation and the S indicates

85
00:04:40,574 --> 00:04:44,618
the hidden state in the external world.

86
00:04:44,619 --> 00:04:49,665
Delta indicate the action of the agent or

87
00:04:49,665 --> 00:04:53,701
decision and the theta indicate a

88
00:04:53,701 --> 00:04:57,747
parameter and lambda is a hyperparameter.

89
00:04:57,747 --> 00:04:58,749
.

90
00:04:58,751 --> 00:05:03,740
So those set of parameters or variables

91
00:05:03,740 --> 00:05:07,789
characterize a generative model and free

92
00:05:07,789 --> 00:05:10,815
energy variational.

93
00:05:10,815 --> 00:05:13,849
Free energy is defined as a function of

94
00:05:13,849 --> 00:05:17,884
the sequence of observation and external

95
00:05:17,884 --> 00:05:20,919
state and its minimization indicates the

96
00:05:20,919 --> 00:05:23,949
variational Bayesian inference.

97
00:05:25,964 --> 00:05:28,996
Similarly, we consider a dynamics of

98
00:05:28,996 --> 00:05:31,027
neural network and we consider that

99
00:05:31,027 --> 00:05:35,060
dynamics is characterized by a set of

100
00:05:35,060 --> 00:05:36,077
those variables.

101
00:05:36,078 --> 00:05:40,119
Here x indicates the neuroactivity firing

102
00:05:40,119 --> 00:05:44,152
rate at middle raya neurons and y

103
00:05:44,152 --> 00:05:48,189
indicate neuroactivity of output raya

104
00:05:48,189 --> 00:05:51,228
neurons and W here is a synaptic weight

105
00:05:51,228 --> 00:05:55,268
and phi is any other free parameter that

106
00:05:55,268 --> 00:05:59,302
characterize neural network.

107
00:05:59,303 --> 00:06:02,273
And we consider that neural network

108
00:06:02,273 --> 00:06:05,307
dynamics is characterized by minimization

109
00:06:05,307 --> 00:06:07,328
of some cost function l.

110
00:06:08,330 --> 00:06:11,363
This is a function of the O and Phi

111
00:06:11,363 --> 00:06:15,402
internal state of neural network and its

112
00:06:15,402 --> 00:06:19,440
minimization indicates the generation of

113
00:06:19,440 --> 00:06:22,477
neural network dynamics including both

114
00:06:22,477 --> 00:06:25,504
activity and plasticity.

115
00:06:25,507 --> 00:06:29,544
And our theory indicates equivalence

116
00:06:29,544 --> 00:06:32,573
between those two functions.

117
00:06:32,575 --> 00:06:35,608
Meaning that for any neural network that

118
00:06:35,608 --> 00:06:39,641
minimize some cost function error, there

119
00:06:39,641 --> 00:06:42,675
is a generative model that satisfy if we

120
00:06:42,675 --> 00:06:43,686
call error.

121
00:06:43,688 --> 00:06:48,730
Which means that the recapturation of

122
00:06:48,730 --> 00:06:52,777
external dynamics is an inherent feature

123
00:06:52,777 --> 00:06:57,819
of any neural system that follow such

124
00:06:57,819 --> 00:06:58,833
dynamics.

125
00:06:58,835 --> 00:07:02,815
So this is an interesting prediction but

126
00:07:02,815 --> 00:07:03,829
too abstract.

127
00:07:04,837 --> 00:07:07,867
So I would like to introduce some

128
00:07:07,867 --> 00:07:10,897
analytically tractable example to

129
00:07:10,897 --> 00:07:14,934
understand this relationship formally.

130
00:07:15,942 --> 00:07:18,973
So first we consider a very simple

131
00:07:18,973 --> 00:07:19,986
architecture.

132
00:07:19,987 --> 00:07:23,028
This is represented by POMDP without any

133
00:07:23,028 --> 00:07:25,046
state transition.

134
00:07:25,047 --> 00:07:31,103
So hidden state s is simply generated by

135
00:07:31,103 --> 00:07:36,158
priya distribution D and it is a binary

136
00:07:36,158 --> 00:07:37,169
state.

137
00:07:38,170 --> 00:07:42,215
But we consider a vector of binary state,

138
00:07:42,215 --> 00:07:46,252
so it is a factory of structure and

139
00:07:46,252 --> 00:07:50,294
observation is also a binary vector and

140
00:07:50,294 --> 00:07:54,330
the transformation from S to O is

141
00:07:54,330 --> 00:07:58,374
characterized by categorical distribution

142
00:07:58,374 --> 00:08:01,349
using likelihood of matrix A and

143
00:08:01,349 --> 00:08:06,392
variational Bayesian inference indicates

144
00:08:06,392 --> 00:08:10,435
the inversion of this generative process

145
00:08:10,435 --> 00:08:12,455
like this.

146
00:08:12,456 --> 00:08:16,498
So by solving the appropriate frequency

147
00:08:16,498 --> 00:08:20,536
functional we obtain those posterior

148
00:08:20,536 --> 00:08:24,576
brief that is optimal in the Bayesian

149
00:08:24,576 --> 00:08:25,585
sense.

150
00:08:26,598 --> 00:08:30,630
On the other hand, for neural network we

151
00:08:30,630 --> 00:08:33,662
consider such structure here upper part

152
00:08:33,662 --> 00:08:36,695
indicates the generation of signal in the

153
00:08:36,695 --> 00:08:39,725
external world and the neural network

154
00:08:39,725 --> 00:08:42,750
comprises only single layer.

155
00:08:42,752 --> 00:08:45,789
Here activity of output layer are

156
00:08:45,789 --> 00:08:49,829
generated by weighted sum of sensory

157
00:08:49,829 --> 00:08:54,874
input O weighted by synaptic matrix W.

158
00:08:54,875 --> 00:08:58,909
And we then characterize this neural

159
00:08:58,909 --> 00:09:00,875
network in the next slide.

160
00:09:01,880 --> 00:09:05,923
So to model neural network or neuron, we

161
00:09:05,923 --> 00:09:09,966
start from considering hodgeken hacksray

162
00:09:09,966 --> 00:09:12,998
equation which comprises four

163
00:09:12,998 --> 00:09:17,041
differential equation and this is a very

164
00:09:17,041 --> 00:09:21,084
complicated non linear equation although

165
00:09:21,084 --> 00:09:24,113
it is barely possible.

166
00:09:24,114 --> 00:09:27,148
So we consider some reduction of these

167
00:09:27,148 --> 00:09:28,157
equations.

168
00:09:28,159 --> 00:09:33,203
So a typical reduction method is like

169
00:09:33,203 --> 00:09:38,250
this for example, m here is much faster

170
00:09:38,250 --> 00:09:42,292
than other variables so this can be

171
00:09:42,292 --> 00:09:46,339
repressed with its fixed point or it is

172
00:09:46,339 --> 00:09:51,385
known that h and minus one sorry, it is

173
00:09:51,385 --> 00:09:55,426
known that h and one minus SN have

174
00:09:55,426 --> 00:09:58,454
similar dynamics.

175
00:09:58,454 --> 00:10:01,428
So we can consider a new effective

176
00:10:01,428 --> 00:10:05,463
variable U to characterize those two

177
00:10:05,463 --> 00:10:09,500
variables and then we obtain 2D hoskin

178
00:10:09,500 --> 00:10:12,532
hacks ray equation like this.

179
00:10:12,533 --> 00:10:16,577
So this is a famous cross of neural

180
00:10:16,577 --> 00:10:21,624
network model which includes the well

181
00:10:21,624 --> 00:10:26,670
known Fitzfuel Nagamo model or other

182
00:10:26,670 --> 00:10:30,718
continuous neural network model and we

183
00:10:30,718 --> 00:10:35,766
modify those model to derive canonical

184
00:10:35,766 --> 00:10:37,789
neural model.

185
00:10:38,791 --> 00:10:41,825
So this is a definition of canonical

186
00:10:41,825 --> 00:10:43,845
neural network model.

187
00:10:43,847 --> 00:10:47,884
Here leak current is characterized by

188
00:10:47,884 --> 00:10:51,924
imbass sigmoid function instead of cubic

189
00:10:51,924 --> 00:10:55,965
function which is adopted in the Fitzfuel

190
00:10:55,965 --> 00:10:57,980
Nagmo model.

191
00:10:57,981 --> 00:11:01,963
And we also consider connection synaptic

192
00:11:01,963 --> 00:11:04,998
connection and this part indicates

193
00:11:04,998 --> 00:11:09,040
synaptic input from sensory layer through

194
00:11:09,040 --> 00:11:13,082
weight matrices and 1 may consider that W

195
00:11:13,082 --> 00:11:17,120
one indicate excitatory synapse and W

196
00:11:17,120 --> 00:11:21,161
zero indicate inhibitory synapse and the

197
00:11:21,161 --> 00:11:25,200
threshold are adaptive thresholds that

198
00:11:25,200 --> 00:11:29,241
are function of W one and W zero.

199
00:11:29,247 --> 00:11:33,282
Interestingly, when we consider a fixed

200
00:11:33,282 --> 00:11:36,316
point of this differential equation we

201
00:11:36,316 --> 00:11:40,350
obtain well known red coding model with

202
00:11:40,350 --> 00:11:43,383
the Sigmoidal activation function.

203
00:11:45,401 --> 00:11:49,440
Which means that we can say that in some

204
00:11:49,440 --> 00:11:52,474
sense this canonical neural network

205
00:11:52,474 --> 00:11:56,513
network model is approximation of Hodgkin

206
00:11:56,513 --> 00:12:00,494
hatt's ray equation and its approximation

207
00:12:00,494 --> 00:12:03,528
level is, in some sense between the

208
00:12:03,528 --> 00:12:07,566
realistic model and the most simplified

209
00:12:07,566 --> 00:12:09,588
red coding model.

210
00:12:10,589 --> 00:12:13,625
So we basically consider this type of

211
00:12:13,625 --> 00:12:17,660
neural network model and in the next

212
00:12:17,660 --> 00:12:20,697
slide we consider what is the plausible

213
00:12:20,697 --> 00:12:24,733
cost function for this neural network

214
00:12:24,733 --> 00:12:25,742
model.

215
00:12:25,746 --> 00:12:29,789
So again we write the same equation for

216
00:12:29,789 --> 00:12:33,828
canonical neural network model which

217
00:12:33,828 --> 00:12:38,873
represents the activity of neurons vector

218
00:12:38,873 --> 00:12:42,917
of neurons and we consider cost function

219
00:12:42,917 --> 00:12:47,961
for this differential equation which can

220
00:12:47,961 --> 00:12:51,001
be obtained by simply calculating the

221
00:12:51,001 --> 00:12:55,039
integral of right hand side of this

222
00:12:55,039 --> 00:12:58,077
equation and get this type of cost

223
00:12:58,077 --> 00:13:02,057
function for neural network.

224
00:13:02,058 --> 00:13:05,084
This is biologically plausible cost

225
00:13:05,084 --> 00:13:08,115
function in the sense that its derivative

226
00:13:08,115 --> 00:13:11,145
derived neural network activity which has

227
00:13:11,145 --> 00:13:14,173
a certain biological plausibility.

228
00:13:14,178 --> 00:13:18,218
Moreover, if we consider derivative of

229
00:13:18,218 --> 00:13:22,255
this cost function with respect to

230
00:13:22,255 --> 00:13:25,287
synaptic weight W, we obtain a

231
00:13:25,287 --> 00:13:29,326
conventional synaptic plasticity rule

232
00:13:29,326 --> 00:13:32,357
which follows Hebian law.

233
00:13:34,373 --> 00:13:39,427
On the other hand, for Bayesian inference

234
00:13:39,427 --> 00:13:45,480
we first define generality model like the

235
00:13:45,480 --> 00:13:49,523
previous slide and we then derive

236
00:13:49,523 --> 00:13:54,576
variational free energy for given genetic

237
00:13:54,576 --> 00:13:56,589
model.

238
00:13:56,590 --> 00:13:59,627
So this variation of free energy is

239
00:13:59,627 --> 00:14:03,606
derived from the Pom DP model in the

240
00:14:03,606 --> 00:14:07,643
previous slide and its minimization

241
00:14:07,643 --> 00:14:11,680
indicates the Beijing inference and

242
00:14:11,680 --> 00:14:12,693
running.

243
00:14:12,698 --> 00:14:17,748
So we found formal correspondences

244
00:14:17,748 --> 00:14:23,800
between component of those two cost

245
00:14:23,800 --> 00:14:26,832
functions like this.

246
00:14:26,833 --> 00:14:30,876
So this broke vector formally correspond

247
00:14:30,876 --> 00:14:34,915
to this Brock vector that represent a

248
00:14:34,915 --> 00:14:38,957
posterior expectation and this logarithm

249
00:14:38,957 --> 00:14:43,001
correspond to this logarithm and actually

250
00:14:43,001 --> 00:14:47,042
this a matrix can be represented as the

251
00:14:47,042 --> 00:14:50,078
broke matrix like this and its dot

252
00:14:50,078 --> 00:14:55,126
product correspond to this computation.

253
00:14:55,127 --> 00:14:59,169
And finally, this phi naturally

254
00:14:59,169 --> 00:15:05,165
correspond to this log d logo state prior.

255
00:15:05,165 --> 00:15:05,167
.

256
00:15:06,173 --> 00:15:12,237
Which means that because the cost

257
00:15:12,237 --> 00:15:19,301
function are same, its derivative

258
00:15:19,301 --> 00:15:27,380
provides its derivative provides sorry.

259
00:15:27,387 --> 00:15:32,434
Because the cost function are formally

260
00:15:32,434 --> 00:15:36,476
equivalent, its derivative sorry.

261
00:15:37,479 --> 00:15:40,515
It's a result of derivative also

262
00:15:40,515 --> 00:15:43,543
corresponding each other.

263
00:15:44,555 --> 00:15:47,589
Which means that for any neural activity

264
00:15:47,589 --> 00:15:50,617
equation in this form, there is a

265
00:15:50,617 --> 00:15:54,653
corresponding Bayesian inference equation.

266
00:15:54,653 --> 00:15:54,655
.

267
00:15:54,656 --> 00:15:57,687
This is an equation that compute the

268
00:15:57,687 --> 00:16:01,662
posterior belief of the hidden state and

269
00:16:01,662 --> 00:16:04,697
this synaptic practice equation formally

270
00:16:04,697 --> 00:16:08,730
correspond to learning or parameter of

271
00:16:08,730 --> 00:16:09,746
genetic model.

272
00:16:09,747 --> 00:16:13,780
And moreover, by establishing this

273
00:16:13,780 --> 00:16:16,817
relationship we can consider the reverse

274
00:16:16,817 --> 00:16:20,855
engineering of the generality model from

275
00:16:20,855 --> 00:16:23,887
empirical data here this schematic

276
00:16:23,887 --> 00:16:26,918
summarize our approach to reverse

277
00:16:26,918 --> 00:16:30,954
engineer generative model and we first

278
00:16:30,954 --> 00:16:34,993
record the neural activity and assign the

279
00:16:34,993 --> 00:16:38,030
canonical neural network to explain this

280
00:16:38,030 --> 00:16:40,051
obtained data.

281
00:16:40,053 --> 00:16:43,083
And by computing the integral we obtained

282
00:16:43,083 --> 00:16:45,108
a cost function for this canonical

283
00:16:45,108 --> 00:16:46,116
network.

284
00:16:46,117 --> 00:16:50,150
And by the mathematical equivalence we

285
00:16:50,150 --> 00:16:52,179
established, we can automatically

286
00:16:52,179 --> 00:16:56,212
identify genetic model and variational

287
00:16:56,212 --> 00:16:59,242
free energy that correspond to this

288
00:16:59,242 --> 00:17:02,210
neural network architecture.

289
00:17:02,211 --> 00:17:06,254
So interestingly, this is a Bayesian

290
00:17:06,254 --> 00:17:10,296
agent which is a kind of artificial

291
00:17:10,296 --> 00:17:12,314
intelligence.

292
00:17:12,315 --> 00:17:15,340
But importantly, this artificial

293
00:17:15,340 --> 00:17:17,368
intelligence is formally derived from

294
00:17:17,368 --> 00:17:19,381
empirical data.

295
00:17:19,382 --> 00:17:22,417
So we can say that this agent is

296
00:17:22,417 --> 00:17:26,459
biomimetic artificial intelligence that

297
00:17:26,459 --> 00:17:31,503
follow the free energy principle and then

298
00:17:31,503 --> 00:17:35,546
its derivative with respect to parameter

299
00:17:35,546 --> 00:17:39,583
posterior derived synaptic plastic

300
00:17:39,583 --> 00:17:43,624
algorithms that follow the free energy

301
00:17:43,624 --> 00:17:47,665
minimization and its time integral can

302
00:17:47,665 --> 00:17:51,707
predict the running process of original

303
00:17:51,707 --> 00:17:54,737
neural network data.

304
00:17:55,743 --> 00:17:58,778
Which means that if the frequency

305
00:17:58,778 --> 00:18:02,751
principle is correct, then this

306
00:18:02,751 --> 00:18:06,793
prediction should work, should work and

307
00:18:06,793 --> 00:18:10,834
should be able to predict the result of

308
00:18:10,834 --> 00:18:14,876
this neural neta without referencing to

309
00:18:14,876 --> 00:18:16,898
the data itself.

310
00:18:17,900 --> 00:18:20,937
So our strategy is in summary, our

311
00:18:20,937 --> 00:18:24,972
strategy is that we reconstruct

312
00:18:24,972 --> 00:18:28,001
generative model only from the initial

313
00:18:28,001 --> 00:18:32,005
data, like initial data before running

314
00:18:32,005 --> 00:18:36,009
and then predict the running process or

315
00:18:36,009 --> 00:18:41,014
running curve that this neural system

316
00:18:41,014 --> 00:18:45,018
should follow by using the frequency

317
00:18:45,018 --> 00:18:49,022
principle and compare or examine whether

318
00:18:49,022 --> 00:18:53,026
this prediction is correct by comparing

319
00:18:53,026 --> 00:18:57,030
actual data after running and the

320
00:18:57,030 --> 00:19:03,029
prediction by the free energy principle.

321
00:19:03,030 --> 00:19:06,033
And if this prediction is correct, then

322
00:19:06,033 --> 00:19:09,036
it indicates the predictive validity of

323
00:19:09,036 --> 00:19:11,038
the free energy principle under setup

324
00:19:11,038 --> 00:19:12,039
considered.

325
00:19:14,041 --> 00:19:17,044
So we apply this strategy to the in vitro

326
00:19:17,044 --> 00:19:19,046
neural network.

327
00:19:19,046 --> 00:19:23,050
So here, this in vitro neural network are

328
00:19:23,050 --> 00:19:26,053
stimulated using the POMDP generative

329
00:19:26,053 --> 00:19:30,057
process defined in the previous slide.

330
00:19:30,057 --> 00:19:35,062
So there are two hidden sources that are

331
00:19:35,062 --> 00:19:40,067
binary signals and they are mixed to

332
00:19:40,067 --> 00:19:44,071
generate 32 sensory stimuli.

333
00:19:44,071 --> 00:19:47,074
Those are also binary.

334
00:19:48,075 --> 00:19:52,079
And this is an overview of experiment by

335
00:19:52,079 --> 00:19:56,083
stimulating in vitro neurons they

336
00:19:56,083 --> 00:20:00,081
generate spiking response and those rhyme

337
00:20:00,081 --> 00:20:05,086
indicate a high density spiking response.

338
00:20:05,086 --> 00:20:05,086
.

339
00:20:05,086 --> 00:20:12,093
So we observed that in some neuron the

340
00:20:12,093 --> 00:20:19,100
response specificity is so we found that

341
00:20:19,100 --> 00:20:25,106
for some neuron response was high to

342
00:20:25,106 --> 00:20:32,113
source one signal compared to source two

343
00:20:32,113 --> 00:20:34,115
signal.

344
00:20:34,115 --> 00:20:39,120
And if we see the transition of those

345
00:20:39,120 --> 00:20:44,125
neurons, we found that although we

346
00:20:44,125 --> 00:20:50,131
removed the offset at the first session

347
00:20:50,131 --> 00:20:56,137
to set those activities zero, but we see

348
00:20:56,137 --> 00:21:01,136
that those neuron self organized to

349
00:21:01,136 --> 00:21:07,142
respond high when source one is one, but

350
00:21:07,142 --> 00:21:13,148
those neurons response low level when the

351
00:21:13,148 --> 00:21:17,152
source one is zero.

352
00:21:18,153 --> 00:21:21,156
So which means that those neurons

353
00:21:21,156 --> 00:21:24,159
activity, those neurons response was

354
00:21:24,159 --> 00:21:27,162
consistent with our theoretical

355
00:21:27,162 --> 00:21:31,166
prediction that neuroactivity cell

356
00:21:31,166 --> 00:21:34,169
organized to encode the posterior

357
00:21:34,169 --> 00:21:37,172
expectation of healing state.

358
00:21:37,172 --> 00:21:40,175
And we also found that some other group

359
00:21:40,175 --> 00:21:43,178
of neuron responds preferentially to

360
00:21:43,178 --> 00:21:46,181
source two but not to source one.

361
00:21:48,183 --> 00:21:53,188
So then we ask okay, then we found that

362
00:21:53,188 --> 00:21:57,192
posterior expectation is encoded by

363
00:21:57,192 --> 00:22:02,191
neuroactivity and the next question is

364
00:22:02,191 --> 00:22:07,196
about other neuronal substrate and we

365
00:22:07,196 --> 00:22:12,201
then ask if the prior brief about hidden

366
00:22:12,201 --> 00:22:17,206
state is effectively equal to the firing

367
00:22:17,206 --> 00:22:21,210
threshold of neural activity model,

368
00:22:21,210 --> 00:22:25,214
neural network model.

369
00:22:26,215 --> 00:22:29,218
So if the theory is correct, this

370
00:22:29,218 --> 00:22:31,220
correspondence should exist.

371
00:22:32,221 --> 00:22:36,225
To check this, we first simulated a

372
00:22:36,225 --> 00:22:42,230
Bayern agent and when we varied the PRI

373
00:22:42,230 --> 00:22:47,236
of the Bayesian agent, the inference was

374
00:22:47,236 --> 00:22:50,239
attenuated as I expected.

375
00:22:51,239 --> 00:22:54,243
And we found that when we buried the

376
00:22:54,243 --> 00:22:57,246
excitatory level of in vitro neural

377
00:22:57,246 --> 00:23:01,244
network by using the pharmacological

378
00:23:01,244 --> 00:23:02,245
manipulation.

379
00:23:02,245 --> 00:23:07,250
We also found that the attenuation of

380
00:23:07,250 --> 00:23:12,255
inference which is consistent with our

381
00:23:12,255 --> 00:23:16,259
theoretical prediction that firing

382
00:23:16,259 --> 00:23:21,264
threshold encode prior belief or the

383
00:23:21,264 --> 00:23:23,266
hidden state.

384
00:23:24,267 --> 00:23:28,271
And next we consider whether the

385
00:23:28,271 --> 00:23:33,276
scientific plasticity followed the free

386
00:23:33,276 --> 00:23:38,281
energy principle by asking whether free

387
00:23:38,281 --> 00:23:42,285
energy principle can predict the

388
00:23:42,285 --> 00:23:47,290
qualitative self organization of a

389
00:23:47,290 --> 00:23:50,293
subsequent neural data.

390
00:23:52,295 --> 00:23:56,299
So here we model neural network like this,

391
00:23:56,299 --> 00:23:59,302
, there are two ensemble neurons that

392
00:23:59,302 --> 00:24:03,300
encode source one and source two.

393
00:24:03,300 --> 00:24:07,304
And we first compute the effective

394
00:24:07,304 --> 00:24:12,309
synaptic weight of those networks using a

395
00:24:12,309 --> 00:24:15,312
conventional connection strength

396
00:24:15,312 --> 00:24:20,317
estimation approach and plot those

397
00:24:20,317 --> 00:24:24,321
synaptic weights on the landscape of

398
00:24:24,321 --> 00:24:29,326
theoretically computed free energy.

399
00:24:31,328 --> 00:24:36,333
So this is a trajectory of empirically

400
00:24:36,333 --> 00:24:42,339
estimated synaptic weights effective sign

401
00:24:42,339 --> 00:24:47,344
optic connectivity and as predicted,

402
00:24:47,344 --> 00:24:53,349
those changes reduce the free energy.

403
00:24:54,351 --> 00:24:58,355
And here we computed this theoretically

404
00:24:58,355 --> 00:25:02,353
predicted free energy landscape only

405
00:25:02,353 --> 00:25:06,357
using the past ten sessions data.

406
00:25:06,357 --> 00:25:12,363
So that this indicates that indicates

407
00:25:12,363 --> 00:25:18,369
some prediction of the self organization.

408
00:25:18,369 --> 00:25:18,369
.

409
00:25:19,370 --> 00:25:23,374
And for more explicit prediction, we then

410
00:25:23,374 --> 00:25:27,378
simulated neuroactivity and plasticity

411
00:25:27,378 --> 00:25:30,381
using green as a principle.

412
00:25:30,381 --> 00:25:36,387
Here the brighter color indicates the

413
00:25:36,387 --> 00:25:41,392
prediction of data without reference to

414
00:25:41,392 --> 00:25:44,395
activity data.

415
00:25:44,395 --> 00:25:48,399
So those lines exactly follow this free

416
00:25:48,399 --> 00:25:50,401
energy gradient.

417
00:25:51,402 --> 00:25:55,406
And we found that this predicted

418
00:25:55,406 --> 00:26:00,405
trajectory is tightly correlated with

419
00:26:00,405 --> 00:26:05,410
this empirically estimated effective

420
00:26:05,410 --> 00:26:07,412
synaptic weights.

421
00:26:08,413 --> 00:26:12,417
And LR rate is like this.

422
00:26:12,417 --> 00:26:16,421
So it indicates that frequency principle

423
00:26:16,421 --> 00:26:19,424
can quantitatively predict the

424
00:26:19,424 --> 00:26:22,427
circumstance in this setup, so it

425
00:26:22,427 --> 00:26:26,431
indicates some predictive validity of the

426
00:26:26,431 --> 00:26:30,435
free energy principle under this setup.

427
00:26:30,435 --> 00:26:35,440
And then we also consider the modeling of

428
00:26:35,440 --> 00:26:39,444
the neuromodulation using active

429
00:26:39,444 --> 00:26:40,445
inference.

430
00:26:40,445 --> 00:26:44,449
It is well known that synaptic process is

431
00:26:44,449 --> 00:26:47,452
modified by various factors like Dopamine,

432
00:26:47,452 --> 00:26:50,455
, northern adrenaline, acetylcholine,

433
00:26:50,455 --> 00:26:52,457
serotonin, so on.

434
00:26:52,457 --> 00:26:57,461
And one interesting property of those

435
00:26:57,461 --> 00:27:01,460
moderation is that even though Dopamine

436
00:27:01,460 --> 00:27:05,464
was added after associative plasticity

437
00:27:05,464 --> 00:27:10,469
was established, it can change the result

438
00:27:10,469 --> 00:27:14,473
of plasticity in a post hoc or

439
00:27:14,473 --> 00:27:18,477
restorespective manner, so it implies

440
00:27:18,477 --> 00:27:22,481
some association to the reward and past

441
00:27:22,481 --> 00:27:24,483
decisions.

442
00:27:25,484 --> 00:27:28,486
So we model this process using canonical

443
00:27:28,486 --> 00:27:29,488
neural network.

444
00:27:29,488 --> 00:27:33,492
So here we model the post hoc modulation

445
00:27:33,492 --> 00:27:38,497
of HEBM plasticity using this type of

446
00:27:38,497 --> 00:27:42,501
plasticity equation and we also consider

447
00:27:42,501 --> 00:27:47,506
the recurrent neural network structure

448
00:27:47,506 --> 00:27:51,510
and output layer for this network.

449
00:27:51,510 --> 00:27:57,516
And we consider that modulation occur at

450
00:27:57,516 --> 00:28:01,514
this connectivity layer.

451
00:28:01,514 --> 00:28:05,518
And then we found cost function that can

452
00:28:05,518 --> 00:28:09,522
derive those differential equations and

453
00:28:09,522 --> 00:28:13,526
then found corresponding variational free

454
00:28:13,526 --> 00:28:17,530
energy and genetic model which means that

455
00:28:17,530 --> 00:28:21,533
this sort of neural network activity

456
00:28:21,533 --> 00:28:24,537
including moderation of sinus plasticity

457
00:28:24,537 --> 00:28:28,541
exactly follow the free energy principle

458
00:28:28,541 --> 00:28:32,545
under some type of homodp generative

459
00:28:32,545 --> 00:28:33,546
model.

460
00:28:34,547 --> 00:28:38,550
And by using this, we show that this

461
00:28:38,550 --> 00:28:42,555
biologically plausible neural network

462
00:28:42,555 --> 00:28:45,558
model with moderation of heavy and

463
00:28:45,558 --> 00:28:50,563
plasticity can solve some sort of delayed

464
00:28:50,563 --> 00:28:53,566
reward task like maze task.

465
00:28:54,567 --> 00:28:57,570
And then finally we would like to discuss

466
00:28:57,570 --> 00:29:01,568
a possible extension to this framework to

467
00:29:01,568 --> 00:29:03,570
the modeling social intelligence.

468
00:29:03,570 --> 00:29:08,575
So, to infer our con specifics, we need

469
00:29:08,575 --> 00:29:14,581
to select an appropriate generality model

470
00:29:14,581 --> 00:29:18,585
for our partners, depending on our

471
00:29:18,585 --> 00:29:20,587
partners.

472
00:29:20,587 --> 00:29:24,591
So this can be done by Beijing model

473
00:29:24,591 --> 00:29:28,595
selection scheme and we previously

474
00:29:28,595 --> 00:29:32,599
proposed a model that can predict the

475
00:29:32,599 --> 00:29:37,604
multiple biosomes using one big genetic

476
00:29:37,604 --> 00:29:41,608
model that comprises multiple genetic

477
00:29:41,608 --> 00:29:42,609
models.

478
00:29:43,610 --> 00:29:50,617
And this movie shows prediction of

479
00:29:50,617 --> 00:29:54,621
prediction of songs.

480
00:29:54,621 --> 00:29:58,625
So like this, this model nicely

481
00:29:58,625 --> 00:30:03,624
identified which genetic model is the

482
00:30:03,624 --> 00:30:08,629
best to explain a given sensory input.

483
00:30:08,629 --> 00:30:13,634
And this process indicates and the model

484
00:30:13,634 --> 00:30:18,639
can correctly infer the appropriate model

485
00:30:18,639 --> 00:30:23,644
and then imitate the song by its own

486
00:30:23,644 --> 00:30:24,645
action.

487
00:30:25,646 --> 00:30:29,650
So, although in the previous work we didn'

488
00:30:29,650 --> 00:30:33,654
't discuss a detailed neural neural

489
00:30:33,654 --> 00:30:36,657
substrate for this mixture generative

490
00:30:36,657 --> 00:30:40,661
model, we now be able to consider the

491
00:30:40,661 --> 00:30:44,665
corresponding circuit architecture.

492
00:30:44,665 --> 00:30:47,668
For example, if we consider the

493
00:30:47,668 --> 00:30:50,671
moderation of neural module by

494
00:30:50,671 --> 00:30:55,675
neuromodulator like Dopamine, it act as

495
00:30:55,675 --> 00:30:57,678
an attentional filter.

496
00:30:57,678 --> 00:31:01,676
And this attentional filter can be

497
00:31:01,676 --> 00:31:05,680
explained by three factor Hebian running

498
00:31:05,680 --> 00:31:10,685
rule introduced in the previous slide.

499
00:31:10,685 --> 00:31:16,691
So again, this modulation works as the

500
00:31:16,691 --> 00:31:22,697
post hoc moderation plasticity.

501
00:31:22,697 --> 00:31:26,701
And this modulation can optimize each

502
00:31:26,701 --> 00:31:31,706
model to represent one generative model,

503
00:31:31,706 --> 00:31:34,709
one generate one song in a mutually

504
00:31:34,709 --> 00:31:37,712
independent manner.

505
00:31:37,712 --> 00:31:40,715
So, through this process, it is possible

506
00:31:40,715 --> 00:31:44,719
to learn multiple generative model in a

507
00:31:44,719 --> 00:31:46,721
vertically plausible manner.

508
00:31:47,722 --> 00:31:49,724
So, in summary, we found that the

509
00:31:49,724 --> 00:31:53,728
dynamics of kinetic neural network that

510
00:31:53,728 --> 00:31:56,731
minimize a cost function can be read as

511
00:31:56,731 --> 00:31:59,734
minimization of variational free energy.

512
00:32:00,729 --> 00:32:03,732
It indicates free energy principle is an

513
00:32:03,732 --> 00:32:06,735
apt explanation for this type of neural

514
00:32:06,735 --> 00:32:07,736
network.

515
00:32:07,736 --> 00:32:11,740
And we also validate this prediction

516
00:32:11,740 --> 00:32:16,745
using some in vitro setup by showing that

517
00:32:16,745 --> 00:32:21,749
free energy principle can quantitatively

518
00:32:21,749 --> 00:32:24,753
predict the self organization of

519
00:32:24,753 --> 00:32:28,757
subsequent plasticity only using the

520
00:32:28,757 --> 00:32:30,759
initial data.

521
00:32:31,760 --> 00:32:35,764
And as the modeling, we can extend those

522
00:32:35,764 --> 00:32:40,769
canonical network modeling to the action

523
00:32:40,769 --> 00:32:44,773
generation as a planning via the delayed

524
00:32:44,773 --> 00:32:48,777
moderation of heavy emperor's DST.

525
00:32:49,778 --> 00:32:53,782
And finally, we discuss possibility to

526
00:32:53,782 --> 00:32:58,787
extend this canonical model to modeling

527
00:32:58,787 --> 00:33:03,786
the social or shared intelligence ah to

528
00:33:03,786 --> 00:33:07,790
interact with multiple partners.

529
00:33:07,790 --> 00:33:10,793
So, that's all of my talk.

530
00:33:10,793 --> 00:33:12,795
Thank you for listening.

531
00:33:12,795 --> 00:33:14,797
This is Acknowledgment.

532
00:33:14,797 --> 00:33:18,801
Our collaborator and fundings and our

533
00:33:18,801 --> 00:33:21,804
unit are now recruiting postdoc

534
00:33:21,804 --> 00:33:22,805
researchers.

535
00:33:22,805 --> 00:33:25,808
So if you're interested, please check.

536
00:33:28,811 --> 00:33:28,811
Daniel: Awesome.

537
00:33:29,812 --> 00:33:31,814
Thank you for the presentation, Takuya.

538
00:33:33,816 --> 00:33:35,818
I'll just ask a few quick questions from

539
00:33:35,818 --> 00:33:37,820
the live chat and a few other things that

540
00:33:37,820 --> 00:33:38,820
come up.

541
00:33:38,821 --> 00:33:42,825
So Dave says Takuya used the phrase after

542
00:33:42,825 --> 00:33:45,828
plasticity was established.

543
00:33:46,829 --> 00:33:49,832
Was his group able to modify e g increase

544
00:33:49,832 --> 00:33:50,833
plasticity?

545
00:33:51,833 --> 00:33:53,836
Or is he saying merely that it was shown

546
00:33:53,836 --> 00:33:55,838
that there is plasticity?

547
00:33:59,842 --> 00:34:03,840
Takuya: Yeah, I'm not sure if I

548
00:34:03,840 --> 00:34:08,845
understand correctly your question, but

549
00:34:08,845 --> 00:34:13,850
those groups show that Dopamine adding

550
00:34:13,850 --> 00:34:18,855
Dopamine after 2 seconds after the

551
00:34:18,855 --> 00:34:22,859
association occurred can change the

552
00:34:22,859 --> 00:34:26,863
magnitude of plasticity.

553
00:34:26,863 --> 00:34:31,868
So without how to say if Dopamine

554
00:34:31,868 --> 00:34:37,874
addition was before this association,

555
00:34:37,874 --> 00:34:42,879
then plasticity level is low.

556
00:34:42,879 --> 00:34:46,883
But after Dopamine addition after

557
00:34:46,883 --> 00:34:50,887
association can change this level can

558
00:34:50,887 --> 00:34:55,892
increase the plasticity like this or like

559
00:34:55,892 --> 00:34:59,896
this which indicates the post hook

560
00:34:59,896 --> 00:35:00,891
moderation.

561
00:35:03,894 --> 00:35:03,894
Cool.

562
00:35:03,894 --> 00:35:05,896
Daniel: I think that answers it.

563
00:35:07,898 --> 00:35:11,902
Well, what are you excited for?

564
00:35:11,902 --> 00:35:14,905
Or what are your hopes or feelings on

565
00:35:14,905 --> 00:35:17,908
where the active inference ecosystem is

566
00:35:17,908 --> 00:35:20,911
at and where we're headed in the coming

567
00:35:20,911 --> 00:35:22,912
months and years?

568
00:35:26,917 --> 00:35:30,921
Takuya: Sorry again chris, just what are

569
00:35:30,921 --> 00:35:32,923
you excited about?

570
00:35:32,923 --> 00:35:34,925
Daniel: Other than your own research

571
00:35:34,925 --> 00:35:35,926
directions?

572
00:35:35,926 --> 00:35:37,928
What are you excited about in the active

573
00:35:37,928 --> 00:35:38,929
inference ecosystem?

574
00:35:40,931 --> 00:35:40,931
Takuya: Yeah.

575
00:35:41,931 --> 00:35:45,936
So, of course, one direction is the

576
00:35:45,936 --> 00:35:50,941
modeling of social interaction, which is

577
00:35:50,941 --> 00:35:54,945
much rich architecture than the

578
00:35:54,945 --> 00:35:57,948
interaction between the static

579
00:35:57,948 --> 00:35:59,950
environment.

580
00:35:59,950 --> 00:36:03,948
So if both agents run with each other,

581
00:36:03,948 --> 00:36:06,951
then many interesting phenomena can be

582
00:36:06,951 --> 00:36:07,952
observed.

583
00:36:07,952 --> 00:36:11,956
So we are excited with modeling those

584
00:36:11,956 --> 00:36:16,961
phenomena using biologically plausible

585
00:36:16,961 --> 00:36:20,965
neural network model through this

586
00:36:20,965 --> 00:36:21,966
equivalence.

587
00:36:24,969 --> 00:36:25,970
Daniel: Awesome.

588
00:36:25,970 --> 00:36:27,972
Any last comments?

589
00:36:33,978 --> 00:36:34,979
Any other comments that you want to make,

590
00:36:34,979 --> 00:36:34,979
,

591
00:36:34,979 --> 00:36:35,980
Takuya?

592
00:36:43,988 --> 00:36:44,989
Awesome.

593
00:36:45,990 --> 00:36:47,992
Thanks again for the presentation.

594
00:36:47,992 --> 00:36:50,995
And people should check out Live Stream

595
00:36:50,995 --> 00:36:53,998
51, where you and I talked a few other

596
00:36:53,998 --> 00:36:55,000
times and went into some of the details

597
00:36:55,000 --> 00:36:56,001
on that work.

598
00:36:56,001 --> 00:36:57,002
There's a lot there.

599
00:36:57,002 --> 00:36:58,003
It's really exciting.

600
00:37:01,000 --> 00:37:02,001
Takuya: Thank you.

601
00:37:03,002 --> 00:37:05,004
Daniel: All right, thank you.

602
00:37:05,004 --> 00:37:06,005
See you next time.

603
00:37:07,006 --> 00:37:08,007
Takuya: See you next time.

604
00:37:08,007 --> 00:37:09,008
Daniel: Bye.


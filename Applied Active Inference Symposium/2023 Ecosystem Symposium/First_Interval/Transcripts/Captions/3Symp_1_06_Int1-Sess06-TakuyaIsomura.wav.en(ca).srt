1
00:00:01,210 --> 00:00:02,000
All right.

2
00:00:08,080 --> 00:00:11,390
Greetings, Takuya. Hi.

3
00:00:14,000 --> 00:00:17,404
You can mute the live

4
00:00:17,442 --> 00:00:18,796
stream or turn off the other live

5
00:00:18,818 --> 00:00:20,860
stream, but yeah, thank you for joining.

6
00:00:22,080 --> 00:00:25,708
Yes, thank you for inviting thank you

7
00:00:25,714 --> 00:00:28,150
for it. Yeah, it.

8
00:00:30,440 --> 00:00:35,108
Well, we have a

9
00:00:35,114 --> 00:00:38,324
little bit 35 or 38 minutes,

10
00:00:38,442 --> 00:00:41,832
so it would be awesome to have your

11
00:00:41,886 --> 00:00:42,760
presentation.

12
00:00:45,500 --> 00:00:50,264
Great. So the

13
00:00:50,302 --> 00:00:53,230
moment well,

14
00:00:57,040 --> 00:00:59,630
can you see my.

15
00:01:01,760 --> 00:01:03,420
I saw PowerPoint.

16
00:01:06,880 --> 00:01:10,364
Okay. Can you

17
00:01:10,402 --> 00:01:13,070
see my screen? Perfect.

18
00:01:14,240 --> 00:01:18,300
Okay. So shall we start?

19
00:01:18,450 --> 00:01:19,870
Yes. Thank you.

20
00:01:21,620 --> 00:01:25,484
Then. Thank you for organizing

21
00:01:25,532 --> 00:01:28,476
this wonderful symposium.

22
00:01:28,588 --> 00:01:31,330
Today I'd like to talk about

23
00:01:32,200 --> 00:01:34,416
relationship between canonical neural

24
00:01:34,448 --> 00:01:38,080
network and active inference

25
00:01:38,240 --> 00:01:41,476
and possible extension for

26
00:01:41,578 --> 00:01:45,376
modeling social and shared intelligence

27
00:01:45,488 --> 00:01:48,992
using canonical neural network. So let's

28
00:01:49,056 --> 00:01:49,670
start.

29
00:01:56,250 --> 00:01:58,402
As you know, the free energy principle

30
00:01:58,466 --> 00:02:02,010
is proposed by Karl Friston that states

31
00:02:02,080 --> 00:02:04,874
that perception, learning and action of

32
00:02:04,912 --> 00:02:07,526
all biological organisms are determined

33
00:02:07,558 --> 00:02:10,010
to minimize variational free energy as a

34
00:02:10,080 --> 00:02:12,114
tractable proxy for surprise

35
00:02:12,182 --> 00:02:15,534
minimization. And by this process

36
00:02:15,732 --> 00:02:20,074
organism can perform parational BJ

37
00:02:20,122 --> 00:02:23,040
inference or external mineral state.

38
00:02:23,650 --> 00:02:26,898
And this can't even just show one

39
00:02:26,984 --> 00:02:30,274
example typical set up under the free

40
00:02:30,312 --> 00:02:33,394
energy principle active inference. Here

41
00:02:33,592 --> 00:02:36,826
there is a hidden state in the external

42
00:02:36,878 --> 00:02:40,326
world and only a part of this state can

43
00:02:40,428 --> 00:02:43,030
be observable.

44
00:02:43,370 --> 00:02:46,790
For our agent, this doc

45
00:02:47,610 --> 00:02:50,666
and this transformation is done by

46
00:02:50,768 --> 00:02:54,650
genetic model parameterized by theta

47
00:02:55,310 --> 00:02:58,730
and to infer the hidden state,

48
00:02:58,800 --> 00:03:02,278
agent need to reconstruct the

49
00:03:02,464 --> 00:03:05,920
copy of the external state called

50
00:03:06,370 --> 00:03:09,482
posterior relief. And this optimization

51
00:03:09,626 --> 00:03:12,814
is done by minimizing variational free

52
00:03:12,852 --> 00:03:16,222
energy. And parameter is

53
00:03:16,276 --> 00:03:19,710
also optimized by minimizing free energy

54
00:03:19,860 --> 00:03:23,074
to obtain an apt generative model to

55
00:03:23,112 --> 00:03:25,490
represent this relationship. An

56
00:03:25,560 --> 00:03:27,702
interesting aspect of the free energy

57
00:03:27,756 --> 00:03:30,022
principle active inference is its

58
00:03:30,076 --> 00:03:32,374
application to the optimization of

59
00:03:32,412 --> 00:03:35,062
action. Here our agent have some

60
00:03:35,116 --> 00:03:39,160
preference prior c and to obtain this

61
00:03:39,470 --> 00:03:43,146
observation in the future, agent may

62
00:03:43,328 --> 00:03:46,422
select the action that minimize

63
00:03:46,486 --> 00:03:49,660
the expected free energy in the future

64
00:03:52,210 --> 00:03:57,150
to obtain the most predictable outcome.

65
00:03:57,810 --> 00:04:01,406
And finally, by selecting actions that

66
00:04:01,428 --> 00:04:04,266
minimize the expect frequency, it can

67
00:04:04,388 --> 00:04:07,950
obtain the feed. So, this is a typical

68
00:04:08,030 --> 00:04:10,482
setup under active inference. But

69
00:04:10,536 --> 00:04:12,686
question is what is the neural

70
00:04:12,718 --> 00:04:15,778
neurosurge that can implement this

71
00:04:15,864 --> 00:04:19,574
process? So that is our interest and

72
00:04:19,612 --> 00:04:22,946
to address this issue we propose theory

73
00:04:23,138 --> 00:04:26,200
as follows. Here we consider that

74
00:04:27,050 --> 00:04:29,262
external world is parameterized

75
00:04:29,346 --> 00:04:33,446
characterized by a set of the variables

76
00:04:33,558 --> 00:04:37,910
like this here, this is a posterior

77
00:04:37,990 --> 00:04:42,218
expectation and the S indicates the

78
00:04:42,304 --> 00:04:44,874
hidden state in the external world.

79
00:04:44,992 --> 00:04:48,590
Delta indicate the action of the agent

80
00:04:48,740 --> 00:04:53,114
or decision and the theta indicate

81
00:04:53,162 --> 00:04:56,434
a parameter and lambda is a

82
00:04:56,552 --> 00:05:00,690
hyperparameter. So those set

83
00:05:00,760 --> 00:05:04,830
of parameters or variables characterize

84
00:05:04,910 --> 00:05:08,934
a generative model and free

85
00:05:08,972 --> 00:05:11,206
energy variational. Free energy is

86
00:05:11,228 --> 00:05:14,662
defined as a function of the sequence of

87
00:05:14,716 --> 00:05:17,974
observation and external state and its

88
00:05:18,012 --> 00:05:22,470
minimization indicates the variational

89
00:05:22,550 --> 00:05:23,930
Bayesian inference.

90
00:05:25,470 --> 00:05:29,242
Similarly, we consider a dynamics of

91
00:05:29,296 --> 00:05:31,980
neural network and we consider that

92
00:05:32,690 --> 00:05:35,390
dynamics is characterized by a set of

93
00:05:35,460 --> 00:05:38,814
those variables. Here x indicates the

94
00:05:38,852 --> 00:05:42,910
neuroactivity firing rate at middle raya

95
00:05:43,570 --> 00:05:48,074
neurons and y indicate neuroactivity

96
00:05:48,122 --> 00:05:51,954
of output raya neurons and W here is a

97
00:05:51,992 --> 00:05:55,762
synaptic weight and phi is any other

98
00:05:55,816 --> 00:05:58,418
free parameter that characterize neural

99
00:05:58,434 --> 00:06:01,778
network. And we consider that neural

100
00:06:01,794 --> 00:06:04,822
network dynamics is characterized by

101
00:06:04,876 --> 00:06:07,894
minimization of some cost function l.

102
00:06:08,012 --> 00:06:11,702
This is a function of the O and Phi

103
00:06:11,766 --> 00:06:16,122
internal state of neural network and its

104
00:06:16,176 --> 00:06:19,530
minimization indicates the generation of

105
00:06:19,600 --> 00:06:22,574
neural network dynamics including both

106
00:06:22,772 --> 00:06:25,470
activity and plasticity.

107
00:06:25,810 --> 00:06:29,562
And our theory indicates equivalence

108
00:06:29,626 --> 00:06:32,378
between those two functions.

109
00:06:32,554 --> 00:06:36,482
Meaning that for any neural network that

110
00:06:36,536 --> 00:06:38,958
minimize some cost function error,

111
00:06:39,054 --> 00:06:41,598
there is a generative model that satisfy

112
00:06:41,694 --> 00:06:45,140
if we call error. Which means that

113
00:06:46,970 --> 00:06:50,674
the recapturation of external

114
00:06:50,722 --> 00:06:54,230
dynamics is an inherent feature of any

115
00:06:54,300 --> 00:06:58,418
neural system that follow such dynamics.

116
00:06:58,594 --> 00:07:02,438
So this is an interesting prediction but

117
00:07:02,604 --> 00:07:06,202
too abstract. So I would like

118
00:07:06,256 --> 00:07:09,830
to introduce some analytically tractable

119
00:07:09,910 --> 00:07:12,960
example to understand this relationship

120
00:07:13,330 --> 00:07:16,542
formally. So first we

121
00:07:16,596 --> 00:07:19,626
consider a very simple architecture.

122
00:07:19,818 --> 00:07:23,966
This is represented by POMDP without

123
00:07:24,068 --> 00:07:27,890
any state transition. So hidden state s

124
00:07:27,960 --> 00:07:31,694
is simply generated by priya

125
00:07:31,742 --> 00:07:35,346
distribution D and it

126
00:07:35,368 --> 00:07:37,958
is a binary state.

127
00:07:38,124 --> 00:07:41,170
But we consider a vector of binary

128
00:07:41,250 --> 00:07:44,674
state, so it is a factory

129
00:07:44,722 --> 00:07:48,482
of structure and observation

130
00:07:48,546 --> 00:07:52,362
is also a binary vector and

131
00:07:52,496 --> 00:07:55,882
the transformation from S to O is

132
00:07:56,016 --> 00:07:57,734
characterized by categorical

133
00:07:57,782 --> 00:08:01,574
distribution using likelihood

134
00:08:01,622 --> 00:08:05,514
of matrix A and variational

135
00:08:05,562 --> 00:08:07,934
Bayesian inference indicates the

136
00:08:07,972 --> 00:08:10,800
inversion of this generative process

137
00:08:11,570 --> 00:08:15,482
like this. So by solving the appropriate

138
00:08:15,546 --> 00:08:18,980
frequency functional we obtain those

139
00:08:19,350 --> 00:08:22,782
posterior brief that is optimal

140
00:08:22,846 --> 00:08:25,620
in the Bayesian sense.

141
00:08:26,890 --> 00:08:29,622
On the other hand, for neural network we

142
00:08:29,676 --> 00:08:33,718
consider such structure here upper part

143
00:08:33,804 --> 00:08:36,886
indicates the generation of signal in

144
00:08:36,908 --> 00:08:38,978
the external world and the neural

145
00:08:38,994 --> 00:08:42,134
network comprises only single layer.

146
00:08:42,262 --> 00:08:45,770
Here activity of output layer are

147
00:08:45,840 --> 00:08:50,054
generated by weighted sum of sensory

148
00:08:50,102 --> 00:08:53,274
input O weighted by synaptic

149
00:08:53,322 --> 00:08:56,894
matrix W. And we then characterize this

150
00:08:56,932 --> 00:09:00,590
neural network in the next slide.

151
00:09:01,010 --> 00:09:04,374
So to model neural network

152
00:09:04,522 --> 00:09:07,874
or neuron, we start from

153
00:09:07,992 --> 00:09:10,610
considering hodgeken hacksray equation

154
00:09:13,670 --> 00:09:16,030
which comprises four differential

155
00:09:16,110 --> 00:09:19,630
equation and this is a very complicated

156
00:09:19,710 --> 00:09:23,206
non linear equation although it

157
00:09:23,228 --> 00:09:26,102
is barely possible. So we consider some

158
00:09:26,236 --> 00:09:29,546
reduction of these equations. So a

159
00:09:29,568 --> 00:09:34,122
typical reduction method is

160
00:09:34,176 --> 00:09:37,898
like this for example, m here is

161
00:09:38,064 --> 00:09:41,194
much faster than other variables so this

162
00:09:41,232 --> 00:09:44,558
can be repressed with its fixed point

163
00:09:44,724 --> 00:09:48,442
or it is known that h and minus

164
00:09:48,586 --> 00:09:52,654
one sorry, it is known that h and

165
00:09:52,772 --> 00:09:56,722
one minus SN have

166
00:09:56,856 --> 00:10:00,738
similar dynamics. So we can consider

167
00:10:00,824 --> 00:10:03,906
a new effective variable U to

168
00:10:04,008 --> 00:10:07,974
characterize those two variables and

169
00:10:08,012 --> 00:10:10,978
then we obtain 2D hoskin hacks ray

170
00:10:10,994 --> 00:10:15,586
equation like this. So this is a famous

171
00:10:15,698 --> 00:10:19,222
cross of neural network model which

172
00:10:19,276 --> 00:10:23,334
includes the well known Fitzfuel Nagamo

173
00:10:23,382 --> 00:10:28,374
model or other continuous

174
00:10:28,422 --> 00:10:32,522
neural network model and we modify

175
00:10:32,666 --> 00:10:36,922
those model to derive canonical

176
00:10:36,986 --> 00:10:41,134
neural model. So this is a definition of

177
00:10:41,252 --> 00:10:44,480
canonical neural network model. Here

178
00:10:46,150 --> 00:10:49,102
leak current is characterized by imbass

179
00:10:49,166 --> 00:10:52,526
sigmoid function instead of cubic

180
00:10:52,558 --> 00:10:55,394
function which is adopted in the

181
00:10:55,432 --> 00:10:58,790
Fitzfuel Nagmo model. And we also

182
00:10:58,940 --> 00:11:02,930
consider connection synaptic connection

183
00:11:03,090 --> 00:11:07,042
and this part indicates synaptic input

184
00:11:07,106 --> 00:11:11,942
from sensory layer through weight

185
00:11:12,006 --> 00:11:15,658
matrices and 1 may consider

186
00:11:15,744 --> 00:11:18,918
that W one indicate excitatory synapse

187
00:11:18,934 --> 00:11:21,746
and W zero indicate inhibitory synapse

188
00:11:21,878 --> 00:11:25,034
and the threshold are adaptive

189
00:11:25,082 --> 00:11:28,014
thresholds that are function of W one

190
00:11:28,052 --> 00:11:30,986
and W zero. Interestingly,

191
00:11:31,098 --> 00:11:34,414
when we consider a fixed point of this

192
00:11:34,532 --> 00:11:38,106
differential equation we obtain well

193
00:11:38,148 --> 00:11:41,362
known red coding model with the

194
00:11:41,416 --> 00:11:43,380
Sigmoidal activation function.

195
00:11:45,190 --> 00:11:48,534
Which means that we can say that in some

196
00:11:48,572 --> 00:11:50,578
sense this canonical neural network

197
00:11:50,674 --> 00:11:53,906
network model is approximation

198
00:11:54,098 --> 00:11:57,720
of Hodgkin hatt's ray equation and its

199
00:11:58,250 --> 00:12:01,100
approximation level is,

200
00:12:02,750 --> 00:12:05,814
in some sense between the realistic

201
00:12:05,862 --> 00:12:09,286
model and the most simplified red coding

202
00:12:09,318 --> 00:12:12,782
model. So we basically consider this

203
00:12:12,836 --> 00:12:16,158
type of neural network model and in

204
00:12:16,164 --> 00:12:19,774
the next slide we consider what

205
00:12:19,812 --> 00:12:23,182
is the plausible cost function for

206
00:12:23,316 --> 00:12:26,818
this neural network model. So again

207
00:12:26,984 --> 00:12:30,878
we write the same equation

208
00:12:30,974 --> 00:12:34,466
for canonical neural network model which

209
00:12:34,648 --> 00:12:38,082
represents the activity of neurons

210
00:12:38,226 --> 00:12:41,622
vector of neurons and we

211
00:12:41,676 --> 00:12:44,422
consider cost function for this

212
00:12:44,476 --> 00:12:46,694
differential equation which can be

213
00:12:46,732 --> 00:12:50,538
obtained by simply calculating the

214
00:12:50,624 --> 00:12:53,914
integral of right hand

215
00:12:53,952 --> 00:12:58,460
side of this equation and get this

216
00:12:58,910 --> 00:13:02,018
type of cost function for neural

217
00:13:02,054 --> 00:13:04,666
network. This is biologically plausible

218
00:13:04,778 --> 00:13:07,054
cost function in the sense that its

219
00:13:07,092 --> 00:13:09,530
derivative derived neural network

220
00:13:09,610 --> 00:13:13,322
activity which has a certain biological

221
00:13:13,386 --> 00:13:15,730
plausibility. Moreover,

222
00:13:16,710 --> 00:13:20,370
if we consider derivative of this cost

223
00:13:20,440 --> 00:13:23,102
function with respect to synaptic weight

224
00:13:23,166 --> 00:13:27,258
W, we obtain a conventional

225
00:13:27,454 --> 00:13:30,898
synaptic plasticity rule which follows

226
00:13:31,074 --> 00:13:32,760
Hebian law.

227
00:13:34,410 --> 00:13:38,882
On the other hand, for Bayesian

228
00:13:38,946 --> 00:13:41,334
inference we first define generality

229
00:13:41,382 --> 00:13:44,470
model like the previous

230
00:13:44,550 --> 00:13:50,554
slide and we

231
00:13:50,592 --> 00:13:54,160
then derive variational free energy for

232
00:13:54,610 --> 00:13:57,114
given genetic model. So this variation

233
00:13:57,162 --> 00:14:01,278
of free energy is derived from the

234
00:14:01,444 --> 00:14:04,990
Pom DP model in the previous

235
00:14:05,070 --> 00:14:08,926
slide and its minimization indicates

236
00:14:09,118 --> 00:14:12,340
the Beijing inference and running.

237
00:14:12,870 --> 00:14:18,242
So we found formal

238
00:14:18,306 --> 00:14:22,018
correspondences between component

239
00:14:22,114 --> 00:14:25,702
of those two cost functions like

240
00:14:25,756 --> 00:14:28,914
this. So this broke

241
00:14:28,962 --> 00:14:32,238
vector formally correspond to this Brock

242
00:14:32,274 --> 00:14:34,614
vector that represent a posterior

243
00:14:34,742 --> 00:14:38,614
expectation and this logarithm

244
00:14:38,662 --> 00:14:41,702
correspond to this logarithm and

245
00:14:41,776 --> 00:14:45,614
actually this a matrix can

246
00:14:45,652 --> 00:14:48,782
be represented as the broke matrix like

247
00:14:48,836 --> 00:14:52,240
this and its dot product

248
00:14:52,850 --> 00:14:55,662
correspond to this computation.

249
00:14:55,806 --> 00:15:00,142
And finally, this phi naturally

250
00:15:00,206 --> 00:15:04,850
correspond to this log d logo state

251
00:15:04,920 --> 00:15:08,582
prior. Which means that because

252
00:15:08,716 --> 00:15:11,750
the cost function are same, its

253
00:15:11,820 --> 00:15:16,310
derivative provides

254
00:15:20,490 --> 00:15:27,080
its derivative provides sorry.

255
00:15:27,790 --> 00:15:31,958
Because the cost function are formally

256
00:15:32,134 --> 00:15:36,640
equivalent, its derivative sorry.

257
00:15:37,010 --> 00:15:40,602
It's a result of derivative

258
00:15:40,746 --> 00:15:43,440
also corresponding each other.

259
00:15:44,550 --> 00:15:48,382
Which means that for any neural activity

260
00:15:48,446 --> 00:15:51,634
equation in this form, there is a

261
00:15:51,672 --> 00:15:53,998
corresponding Bayesian inference

262
00:15:54,094 --> 00:15:57,006
equation. This is an equation that

263
00:15:57,128 --> 00:16:00,310
compute the posterior belief of the

264
00:16:00,380 --> 00:16:04,082
hidden state and this synaptic practice

265
00:16:04,146 --> 00:16:06,838
equation formally correspond to learning

266
00:16:06,924 --> 00:16:09,618
or parameter of genetic model.

267
00:16:09,804 --> 00:16:13,626
And moreover, by establishing this

268
00:16:13,728 --> 00:16:16,934
relationship we can consider the reverse

269
00:16:16,982 --> 00:16:19,882
engineering of the generality model from

270
00:16:19,936 --> 00:16:24,990
empirical data here this schematic

271
00:16:25,730 --> 00:16:29,738
summarize our approach to reverse

272
00:16:29,754 --> 00:16:32,654
engineer generative model and we first

273
00:16:32,692 --> 00:16:35,070
record the neural activity and assign

274
00:16:35,150 --> 00:16:38,194
the canonical neural network to explain

275
00:16:38,312 --> 00:16:42,082
this obtained data. And by computing the

276
00:16:42,136 --> 00:16:44,850
integral we obtained a cost function for

277
00:16:44,920 --> 00:16:48,374
this canonical network. And by the

278
00:16:48,412 --> 00:16:50,722
mathematical equivalence we established,

279
00:16:50,786 --> 00:16:54,578
we can automatically identify genetic

280
00:16:54,594 --> 00:16:57,558
model and variational free energy that

281
00:16:57,644 --> 00:17:00,902
correspond to this neural network

282
00:17:01,046 --> 00:17:04,522
architecture. So interestingly, this is

283
00:17:04,576 --> 00:17:09,274
a Bayesian agent which

284
00:17:09,312 --> 00:17:12,458
is a kind of artificial intelligence.

285
00:17:12,554 --> 00:17:15,002
But importantly, this artificial

286
00:17:15,066 --> 00:17:17,982
intelligence is formally derived from

287
00:17:18,116 --> 00:17:21,326
empirical data. So we can say that this

288
00:17:21,508 --> 00:17:25,134
agent is biomimetic artificial

289
00:17:25,182 --> 00:17:27,730
intelligence that follow the free energy

290
00:17:27,800 --> 00:17:32,130
principle and then its derivative with

291
00:17:32,200 --> 00:17:35,650
respect to parameter posterior

292
00:17:36,230 --> 00:17:39,386
derived synaptic plastic algorithms

293
00:17:39,438 --> 00:17:42,658
that follow the free energy minimization

294
00:17:42,834 --> 00:17:46,658
and its time integral can predict

295
00:17:46,754 --> 00:17:52,326
the running process of original neural

296
00:17:52,358 --> 00:17:56,074
network data. Which means

297
00:17:56,112 --> 00:17:59,722
that if the frequency principle is

298
00:17:59,776 --> 00:18:02,782
correct, then this prediction should

299
00:18:02,836 --> 00:18:06,206
work, should work and should

300
00:18:06,388 --> 00:18:10,766
be able to predict the result of this

301
00:18:10,868 --> 00:18:14,246
neural neta without referencing

302
00:18:14,378 --> 00:18:16,866
to the data itself.

303
00:18:17,048 --> 00:18:20,306
So our strategy is in

304
00:18:20,328 --> 00:18:23,380
summary, our strategy is that we

305
00:18:24,070 --> 00:18:27,686
reconstruct generative model only from

306
00:18:27,788 --> 00:18:29,400
the initial data,

307
00:18:30,570 --> 00:18:34,280
like initial data before running

308
00:18:34,650 --> 00:18:37,990
and then predict the running process or

309
00:18:38,060 --> 00:18:41,434
running curve that this

310
00:18:41,632 --> 00:18:45,914
neural system should follow by

311
00:18:46,112 --> 00:18:48,970
using the frequency principle and

312
00:18:49,040 --> 00:18:52,222
compare or examine whether this

313
00:18:52,276 --> 00:18:55,870
prediction is correct by comparing

314
00:18:56,530 --> 00:19:00,110
actual data after running and the

315
00:19:00,180 --> 00:19:03,002
prediction by the free energy principle.

316
00:19:03,146 --> 00:19:05,906
And if this prediction is correct, then

317
00:19:06,008 --> 00:19:08,754
it indicates the predictive validity of

318
00:19:08,792 --> 00:19:11,742
the free energy principle under setup

319
00:19:11,806 --> 00:19:15,298
considered. So we apply

320
00:19:15,384 --> 00:19:18,818
this strategy to the in vitro neural

321
00:19:18,834 --> 00:19:21,778
network. So here, this in vitro neural

322
00:19:21,794 --> 00:19:25,538
network are stimulated using the POMDP

323
00:19:25,714 --> 00:19:28,966
generative process defined in

324
00:19:28,988 --> 00:19:31,530
the previous slide. So there are two

325
00:19:31,600 --> 00:19:34,870
hidden sources that are binary signals

326
00:19:34,950 --> 00:19:38,266
and they are

327
00:19:38,288 --> 00:19:42,720
mixed to generate 32

328
00:19:43,490 --> 00:19:47,630
sensory stimuli. Those are also binary.

329
00:19:48,450 --> 00:19:52,110
And this is an overview of experiment

330
00:19:53,090 --> 00:19:56,238
by stimulating in vitro neurons they

331
00:19:56,324 --> 00:20:00,062
generate spiking response and those

332
00:20:00,116 --> 00:20:04,494
rhyme indicate a high density spiking

333
00:20:04,622 --> 00:20:10,678
response. So we observed that in

334
00:20:10,764 --> 00:20:14,582
some neuron the

335
00:20:14,636 --> 00:20:19,142
response specificity is so

336
00:20:19,276 --> 00:20:25,574
we found that for some neuron response

337
00:20:25,702 --> 00:20:29,660
was high to source one

338
00:20:30,590 --> 00:20:34,186
signal compared to source two signal.

339
00:20:34,378 --> 00:20:38,570
And if we see the transition

340
00:20:38,730 --> 00:20:42,222
of those neurons, we found

341
00:20:42,276 --> 00:20:46,390
that although we removed

342
00:20:46,410 --> 00:20:50,850
the offset at the first session

343
00:20:52,070 --> 00:20:55,634
to set those activities zero,

344
00:20:55,832 --> 00:20:59,782
but we see that those

345
00:20:59,836 --> 00:21:03,320
neuron self organized to respond high

346
00:21:03,850 --> 00:21:06,520
when source one is one,

347
00:21:08,650 --> 00:21:12,730
but those neurons response low

348
00:21:12,880 --> 00:21:16,250
level when the source one

349
00:21:16,400 --> 00:21:20,202
is zero. So which means that those

350
00:21:20,256 --> 00:21:24,110
neurons activity, those neurons response

351
00:21:24,770 --> 00:21:28,622
was consistent with

352
00:21:28,756 --> 00:21:31,134
our theoretical prediction that

353
00:21:31,252 --> 00:21:34,454
neuroactivity cell organized to encode

354
00:21:34,522 --> 00:21:37,214
the posterior expectation of healing

355
00:21:37,262 --> 00:21:40,162
state. And we also found that some other

356
00:21:40,216 --> 00:21:42,942
group of neuron responds preferentially

357
00:21:43,086 --> 00:21:46,740
to source two but not to source one.

358
00:21:48,090 --> 00:21:52,374
So then we ask okay,

359
00:21:52,492 --> 00:21:56,002
then we found that posterior

360
00:21:56,066 --> 00:21:58,322
expectation is encoded by neuroactivity

361
00:21:58,386 --> 00:22:02,086
and the next question is about other

362
00:22:02,188 --> 00:22:07,114
neuronal substrate and we

363
00:22:07,152 --> 00:22:12,162
then ask if the prior

364
00:22:12,246 --> 00:22:17,562
brief about hidden state is effectively

365
00:22:17,626 --> 00:22:21,326
equal to the firing threshold of

366
00:22:21,428 --> 00:22:24,862
neural activity model, neural network

367
00:22:24,926 --> 00:22:28,098
model. So if the theory is correct,

368
00:22:28,184 --> 00:22:31,810
this correspondence should exist.

369
00:22:32,390 --> 00:22:36,350
To check this, we first simulated

370
00:22:36,430 --> 00:22:39,846
a Bayern agent and when we

371
00:22:39,948 --> 00:22:44,070
varied the PRI of the

372
00:22:44,140 --> 00:22:47,782
Bayesian agent, the inference was

373
00:22:47,836 --> 00:22:50,838
attenuated as I expected.

374
00:22:51,014 --> 00:22:54,854
And we found that when we buried

375
00:22:54,982 --> 00:22:58,566
the excitatory level of in vitro neural

376
00:22:58,598 --> 00:23:01,762
network by using the pharmacological

377
00:23:01,846 --> 00:23:08,014
manipulation. We also found that the

378
00:23:08,052 --> 00:23:12,334
attenuation of inference which

379
00:23:12,372 --> 00:23:14,942
is consistent with our theoretical

380
00:23:15,006 --> 00:23:19,486
prediction that firing threshold encode

381
00:23:19,678 --> 00:23:23,220
prior belief or the hidden state.

382
00:23:24,710 --> 00:23:28,866
And next we consider whether the

383
00:23:28,968 --> 00:23:32,614
scientific plasticity followed the free

384
00:23:32,652 --> 00:23:36,520
energy principle by asking whether

385
00:23:37,610 --> 00:23:40,874
free energy principle can predict the

386
00:23:40,912 --> 00:23:44,380
qualitative self organization of a

387
00:23:47,950 --> 00:23:50,700
subsequent neural data.

388
00:23:52,130 --> 00:23:56,506
So here we model neural

389
00:23:56,538 --> 00:23:59,166
network like this, there are two

390
00:23:59,268 --> 00:24:01,854
ensemble neurons that encode source one

391
00:24:01,892 --> 00:24:06,190
and source two. And we first compute

392
00:24:06,270 --> 00:24:10,610
the effective synaptic weight of

393
00:24:10,760 --> 00:24:14,370
those networks using

394
00:24:14,440 --> 00:24:16,974
a conventional connection strength

395
00:24:17,022 --> 00:24:20,920
estimation approach and plot those

396
00:24:21,370 --> 00:24:26,034
synaptic weights on the landscape

397
00:24:26,082 --> 00:24:29,160
of theoretically computed free energy.

398
00:24:31,690 --> 00:24:35,830
So this is a trajectory of empirically

399
00:24:36,170 --> 00:24:40,470
estimated synaptic weights effective

400
00:24:41,370 --> 00:24:44,910
sign optic connectivity and as

401
00:24:44,980 --> 00:24:50,910
predicted, those changes reduce

402
00:24:51,730 --> 00:24:53,040
the free energy.

403
00:24:54,710 --> 00:24:58,226
And here we computed this

404
00:24:58,408 --> 00:25:01,410
theoretically predicted free energy

405
00:25:01,480 --> 00:25:05,202
landscape only using the past ten

406
00:25:05,256 --> 00:25:08,914
sessions data. So that this indicates

407
00:25:08,962 --> 00:25:13,750
that indicates some prediction

408
00:25:16,410 --> 00:25:18,550
of the self organization.

409
00:25:19,310 --> 00:25:23,718
And for more explicit prediction,

410
00:25:23,814 --> 00:25:27,654
we then simulated neuroactivity

411
00:25:27,702 --> 00:25:29,946
and plasticity using green as a

412
00:25:29,968 --> 00:25:35,274
principle. Here the

413
00:25:35,312 --> 00:25:38,362
brighter color indicates the prediction

414
00:25:38,506 --> 00:25:41,962
of data without reference

415
00:25:42,026 --> 00:25:45,842
to activity data. So those

416
00:25:45,896 --> 00:25:49,634
lines exactly follow this free

417
00:25:49,672 --> 00:25:52,802
energy gradient. And we found

418
00:25:52,856 --> 00:25:57,170
that this predicted trajectory

419
00:25:57,930 --> 00:26:01,586
is tightly correlated

420
00:26:01,698 --> 00:26:05,426
with this empirically estimated

421
00:26:05,538 --> 00:26:07,990
effective synaptic weights.

422
00:26:08,490 --> 00:26:12,300
And LR rate is like this.

423
00:26:12,670 --> 00:26:16,262
So it indicates that frequency

424
00:26:16,326 --> 00:26:20,742
principle can quantitatively predict

425
00:26:20,806 --> 00:26:23,934
the circumstance in this setup, so it

426
00:26:23,972 --> 00:26:27,214
indicates some predictive validity of

427
00:26:27,252 --> 00:26:29,534
the free energy principle under this

428
00:26:29,572 --> 00:26:33,486
setup. And then we also consider

429
00:26:33,588 --> 00:26:37,150
the modeling of the neuromodulation

430
00:26:38,550 --> 00:26:41,486
using active inference. It is well known

431
00:26:41,518 --> 00:26:45,282
that synaptic process is modified by

432
00:26:45,416 --> 00:26:48,914
various factors like Dopamine, northern

433
00:26:48,962 --> 00:26:51,378
adrenaline, acetylcholine, serotonin,

434
00:26:51,474 --> 00:26:55,574
so on. And one interesting property

435
00:26:55,692 --> 00:26:59,720
of those moderation is that even though

436
00:27:00,110 --> 00:27:03,260
Dopamine was added after

437
00:27:04,030 --> 00:27:07,850
associative plasticity was established,

438
00:27:08,590 --> 00:27:12,726
it can change the result of plasticity

439
00:27:12,838 --> 00:27:16,074
in a post hoc or restorespective manner,

440
00:27:16,202 --> 00:27:19,882
so it implies

441
00:27:19,946 --> 00:27:23,518
some association to the reward and

442
00:27:23,604 --> 00:27:27,042
past decisions. So we model this process

443
00:27:27,176 --> 00:27:30,418
using canonical neural network. So here

444
00:27:30,584 --> 00:27:33,842
we model the post

445
00:27:33,896 --> 00:27:37,482
hoc modulation of HEBM plasticity

446
00:27:37,646 --> 00:27:41,334
using this type of

447
00:27:41,532 --> 00:27:44,662
plasticity equation and we also consider

448
00:27:44,796 --> 00:27:47,346
the recurrent neural network structure

449
00:27:47,458 --> 00:27:51,414
and output layer for this network.

450
00:27:51,542 --> 00:27:55,082
And we consider that modulation occur at

451
00:27:55,216 --> 00:28:01,778
this connectivity layer.

452
00:28:01,974 --> 00:28:05,598
And then we found cost

453
00:28:05,684 --> 00:28:08,560
function that can derive those

454
00:28:10,370 --> 00:28:13,118
differential equations and then found

455
00:28:13,284 --> 00:28:15,790
corresponding variational free energy

456
00:28:15,860 --> 00:28:19,074
and genetic model which means that

457
00:28:19,192 --> 00:28:21,822
this sort of neural network activity

458
00:28:21,886 --> 00:28:24,654
including moderation of sinus plasticity

459
00:28:24,782 --> 00:28:28,478
exactly follow the free energy principle

460
00:28:28,574 --> 00:28:32,378
under some type of homodp

461
00:28:32,414 --> 00:28:35,910
generative model. And by using this,

462
00:28:35,980 --> 00:28:40,982
we show that this biologically

463
00:28:41,046 --> 00:28:44,410
plausible neural network model with

464
00:28:44,480 --> 00:28:46,922
moderation of heavy and plasticity can

465
00:28:46,976 --> 00:28:50,326
solve some sort of delayed

466
00:28:50,358 --> 00:28:53,610
reward task like maze task.

467
00:28:54,530 --> 00:28:56,654
And then finally we would like to

468
00:28:56,692 --> 00:28:59,166
discuss a possible extension to this

469
00:28:59,268 --> 00:29:02,718
framework to the modeling social

470
00:29:02,804 --> 00:29:04,640
intelligence. So,

471
00:29:05,750 --> 00:29:09,490
to infer our con specifics,

472
00:29:11,350 --> 00:29:13,982
we need to select an appropriate

473
00:29:14,046 --> 00:29:17,826
generality model for our partners,

474
00:29:17,938 --> 00:29:21,254
depending on our partners. So this can

475
00:29:21,292 --> 00:29:24,194
be done by Beijing model selection

476
00:29:24,242 --> 00:29:27,398
scheme and we previously proposed a

477
00:29:27,484 --> 00:29:32,386
model that can predict the multiple

478
00:29:32,498 --> 00:29:36,986
biosomes using one

479
00:29:37,088 --> 00:29:39,910
big genetic model that comprises

480
00:29:39,990 --> 00:29:44,158
multiple genetic models. And this

481
00:29:44,244 --> 00:29:48,000
movie shows prediction of

482
00:29:51,330 --> 00:29:54,378
prediction of songs.

483
00:29:54,554 --> 00:29:58,254
So like this, this model

484
00:29:58,372 --> 00:30:02,238
nicely identified which

485
00:30:02,404 --> 00:30:06,306
genetic model is the best to explain a

486
00:30:06,328 --> 00:30:10,198
given sensory input. And this process

487
00:30:10,284 --> 00:30:13,734
indicates and the

488
00:30:13,772 --> 00:30:16,886
model can correctly infer the

489
00:30:16,988 --> 00:30:20,486
appropriate model and then imitate

490
00:30:20,598 --> 00:30:24,730
the song by its own action.

491
00:30:25,550 --> 00:30:28,634
So, although in the previous work we

492
00:30:28,672 --> 00:30:32,006
didn't discuss a detailed neural

493
00:30:32,038 --> 00:30:35,274
neural substrate for this mixture

494
00:30:35,402 --> 00:30:38,590
generative model, we now be able to

495
00:30:38,660 --> 00:30:42,794
consider the corresponding

496
00:30:42,922 --> 00:30:45,954
circuit architecture. For example, if we

497
00:30:45,992 --> 00:30:49,934
consider the moderation of neural

498
00:30:49,982 --> 00:30:54,286
module by neuromodulator like Dopamine,

499
00:30:54,398 --> 00:30:57,650
it act as an attentional filter.

500
00:30:57,810 --> 00:31:00,854
And this attentional filter can be

501
00:31:00,892 --> 00:31:04,770
explained by three factor

502
00:31:04,930 --> 00:31:08,514
Hebian running rule introduced

503
00:31:08,562 --> 00:31:11,740
in the previous slide. So again,

504
00:31:12,110 --> 00:31:15,818
this modulation works

505
00:31:15,904 --> 00:31:22,450
as the post hoc moderation plasticity.

506
00:31:22,550 --> 00:31:26,250
And this modulation can optimize

507
00:31:26,330 --> 00:31:29,594
each model to represent one generative

508
00:31:29,642 --> 00:31:33,840
model, one generate one song in

509
00:31:34,150 --> 00:31:37,358
a mutually independent manner.

510
00:31:37,454 --> 00:31:40,706
So, through this process, it is possible

511
00:31:40,808 --> 00:31:44,466
to learn multiple generative model

512
00:31:44,648 --> 00:31:47,618
in a vertically plausible manner. So,

513
00:31:47,704 --> 00:31:50,814
in summary, we found that the dynamics

514
00:31:50,862 --> 00:31:53,314
of kinetic neural network that minimize

515
00:31:53,362 --> 00:31:57,160
a cost function can be read as

516
00:31:57,610 --> 00:31:59,894
minimization of variational free energy.

517
00:32:00,012 --> 00:32:03,242
It indicates free energy principle is an

518
00:32:03,296 --> 00:32:06,886
apt explanation for this type of neural

519
00:32:06,918 --> 00:32:10,806
network. And we also validate

520
00:32:10,998 --> 00:32:14,406
this prediction using some in vitro

521
00:32:14,518 --> 00:32:18,554
setup by

522
00:32:18,592 --> 00:32:20,880
showing that free energy principle can

523
00:32:21,730 --> 00:32:23,822
quantitatively predict the self

524
00:32:23,876 --> 00:32:27,650
organization of subsequent plasticity

525
00:32:27,990 --> 00:32:30,740
only using the initial data.

526
00:32:31,590 --> 00:32:35,490
And as the modeling, we can extend

527
00:32:35,910 --> 00:32:39,890
those canonical network modeling to

528
00:32:39,960 --> 00:32:43,650
the action generation as a planning

529
00:32:43,810 --> 00:32:47,346
via the delayed moderation of heavy

530
00:32:47,378 --> 00:32:51,160
emperor's DST. And finally, we discuss

531
00:32:51,930 --> 00:32:54,802
possibility to extend this canonical

532
00:32:54,866 --> 00:32:58,200
model to modeling the

533
00:32:58,570 --> 00:33:02,814
social or shared intelligence ah

534
00:33:02,932 --> 00:33:07,610
to interact with multiple partners.

535
00:33:07,770 --> 00:33:11,630
So, that's all of my talk. Thank you for

536
00:33:11,700 --> 00:33:14,942
listening. This is Acknowledgment. Our

537
00:33:14,996 --> 00:33:18,762
collaborator and fundings and our unit

538
00:33:18,826 --> 00:33:22,174
are now recruiting postdoc

539
00:33:22,222 --> 00:33:24,034
researchers. So if you're interested,

540
00:33:24,152 --> 00:33:25,220
please check.

541
00:33:28,120 --> 00:33:30,432
Awesome. Thank you for the presentation,

542
00:33:30,496 --> 00:33:34,036
Takuya. I'll just

543
00:33:34,138 --> 00:33:36,324
ask a few quick questions from the live

544
00:33:36,362 --> 00:33:37,796
chat and a few other things that come

545
00:33:37,818 --> 00:33:41,796
up. So Dave says Takuya

546
00:33:41,828 --> 00:33:44,344
used the phrase after plasticity was

547
00:33:44,382 --> 00:33:47,704
established. Was his group able to

548
00:33:47,742 --> 00:33:50,948
modify e g increase plasticity?

549
00:33:51,044 --> 00:33:53,336
Or is he saying merely that it was shown

550
00:33:53,368 --> 00:33:55,100
that there is plasticity?

551
00:33:59,600 --> 00:34:03,052
Yeah, I'm not sure if I understand

552
00:34:03,186 --> 00:34:09,392
correctly your question, but those

553
00:34:09,446 --> 00:34:13,132
groups show that Dopamine adding

554
00:34:13,196 --> 00:34:16,688
Dopamine after 2 seconds after

555
00:34:16,854 --> 00:34:20,740
the association occurred can

556
00:34:20,810 --> 00:34:24,564
change the

557
00:34:24,602 --> 00:34:30,464
magnitude of plasticity. So without how

558
00:34:30,522 --> 00:34:34,164
to say if Dopamine

559
00:34:34,292 --> 00:34:37,592
addition was before this

560
00:34:37,646 --> 00:34:41,320
association, then plasticity level is

561
00:34:41,470 --> 00:34:45,656
low. But after Dopamine

562
00:34:45,688 --> 00:34:49,276
addition after association can change

563
00:34:49,378 --> 00:34:53,500
this level can increase the plasticity

564
00:34:54,240 --> 00:34:58,000
like this or like this which

565
00:34:58,070 --> 00:35:00,800
indicates the post hook moderation.

566
00:35:03,140 --> 00:35:05,650
Cool. I think that answers it.

567
00:35:07,460 --> 00:35:10,752
Well, what are you excited

568
00:35:10,816 --> 00:35:15,030
for? Or what are your hopes or

569
00:35:15,400 --> 00:35:17,744
feelings on where the active inference

570
00:35:17,792 --> 00:35:20,176
ecosystem is at and where we're headed

571
00:35:20,208 --> 00:35:22,010
in the coming months and years?

572
00:35:26,790 --> 00:35:29,650
Sorry again chris,

573
00:35:31,110 --> 00:35:33,122
just what are you excited about? Other

574
00:35:33,176 --> 00:35:35,526
than your own research directions? What

575
00:35:35,548 --> 00:35:37,458
are you excited about in the active

576
00:35:37,474 --> 00:35:40,918
inference ecosystem? Yeah.

577
00:35:41,004 --> 00:35:44,760
So, of course, one direction is

578
00:35:45,610 --> 00:35:49,930
the modeling of social interaction,

579
00:35:50,590 --> 00:35:54,262
which is much rich architecture

580
00:35:54,326 --> 00:35:58,930
than the interaction between the static

581
00:35:59,030 --> 00:36:02,542
environment. So if both agents run with

582
00:36:02,596 --> 00:36:05,806
each other, then many interesting

583
00:36:05,908 --> 00:36:08,478
phenomena can be observed. So we are

584
00:36:08,564 --> 00:36:12,242
excited with modeling those

585
00:36:12,376 --> 00:36:15,010
phenomena using biologically plausible

586
00:36:16,790 --> 00:36:20,722
neural network model through this

587
00:36:20,776 --> 00:36:21,970
equivalence.

588
00:36:24,490 --> 00:36:27,910
Awesome. Any last comments?

589
00:36:33,130 --> 00:36:34,726
Any other comments that you want to

590
00:36:34,748 --> 00:36:35,910
make? Takuya.

591
00:36:43,600 --> 00:36:46,684
Awesome. Thanks again for the

592
00:36:46,722 --> 00:36:49,356
presentation. And people should check

593
00:36:49,378 --> 00:36:52,124
out Live Stream 51, where you and I

594
00:36:52,162 --> 00:36:54,572
talked a few other times and went into

595
00:36:54,626 --> 00:36:56,736
some of the details on that work.

596
00:36:56,918 --> 00:36:58,144
There's a lot there. It's really

597
00:36:58,182 --> 00:37:02,610
exciting. Thank you.

598
00:37:03,460 --> 00:37:06,690
All right, thank you. See you next time.

599
00:37:07,560 --> 00:37:09,780
See you next time. Bye.

600
00:37:39,250 --> 00:37:44,298
All right, the next session

601
00:37:44,394 --> 00:37:46,430
is going to be with Shannon Dobson.

602
00:37:47,090 --> 00:37:51,514
This section is going to be called Dark

603
00:37:51,642 --> 00:37:54,494
Imaginarium Shared Intelligence as an

604
00:37:54,532 --> 00:38:02,598
Infinity Curiosity type message.

605
00:38:02,684 --> 00:38:04,466
Shannon, make sure that everything's

606
00:38:04,498 --> 00:38:05,750
good with the audio.

607
00:38:10,330 --> 00:38:11,080
Wow.

608
00:38:14,490 --> 00:38:17,380
Great talk so far.



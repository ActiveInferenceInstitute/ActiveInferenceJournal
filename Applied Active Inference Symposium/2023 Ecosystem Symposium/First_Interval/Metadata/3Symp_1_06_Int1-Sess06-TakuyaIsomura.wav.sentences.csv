start	end	sentNum	speaker	confidence	text
1210	2000	1	A	0.6729	All right.
8080	9740	2	A	0.99525	Greetings, Takuya.
10640	11390	3	B	0.84997	Hi.
14000	20860	4	A	0.99989	You can mute the live stream or turn off the other live stream, but yeah, thank you for joining.
22080	26220	5	B	0.99254	Yes, thank you for inviting thank you for it.
26290	28150	6	B	0.84715	Yeah, it.
30440	42760	7	A	0.99968	Well, we have a little bit 35 or 38 minutes, so it would be awesome to have your presentation.
45500	46250	8	B	0.9821	Great.
46620	59630	9	B	0.87217	So the moment well, can you see my.
61760	63420	10	A	1	I saw PowerPoint.
66880	67630	11	B	0.99049	Okay.
69840	71870	12	B	0.99994	Can you see my screen?
72320	73070	13	A	0.99979	Perfect.
74240	74990	14	B	0.52491	Okay.
76480	78300	15	B	0.99791	So shall we start?
78450	78908	16	A	0.99993	Yes.
78994	79870	17	A	0.99999	Thank you.
81620	82368	18	B	0.89558	Then.
82534	88476	19	B	0.99996	Thank you for organizing this wonderful symposium.
88588	107792	20	B	0.99996	Today I'd like to talk about relationship between canonical neural network and active inference and possible extension for modeling social and shared intelligence using canonical neural network.
107936	109670	21	B	0.99928	So let's start.
116250	132874	22	B	0.99988	As you know, the free energy principle is proposed by Karl Friston that states that perception, learning and action of all biological organisms are determined to minimize variational free energy as a tractable proxy for surprise minimization.
133002	143040	23	B	1	And by this process organism can perform parational BJ inference or external mineral state.
143650	152494	24	B	0.99	And this can't even just show one example typical set up under the free energy principle active inference.
152622	163030	25	B	0.99997	Here there is a hidden state in the external world and only a part of this state can be observable.
163370	187690	26	B	0.99998	For our agent, this doc and this transformation is done by genetic model parameterized by theta and to infer the hidden state, agent need to reconstruct the copy of the external state called posterior relief.
187770	193440	27	B	1	And this optimization is done by minimizing variational free energy.
194450	204580	28	B	0.56	And parameter is also optimized by minimizing free energy to obtain an apt generative model to represent this relationship.
205030	213042	29	B	0.52023	An interesting aspect of the free energy principle active inference is its application to the optimization of action.
213186	237150	30	B	0.99995	Here our agent have some preference prior c and to obtain this observation in the future, agent may select the action that minimize the expected free energy in the future to obtain the most predictable outcome.
237810	246206	31	B	0.68	And finally, by selecting actions that minimize the expect frequency, it can obtain the feed.
246398	250206	32	B	0.99956	So, this is a typical setup under active inference.
250238	256258	33	B	0.99469	But question is what is the neural neurosurge that can implement this process?
256344	264306	34	B	0.99669	So that is our interest and to address this issue we propose theory as follows.
264418	284874	35	B	0.99999	Here we consider that external world is parameterized characterized by a set of the variables like this here, this is a posterior expectation and the S indicates the hidden state in the external world.
284992	298046	36	B	0.50257	Delta indicate the action of the agent or decision and the theta indicate a parameter and lambda is a hyperparameter.
298238	310546	37	B	0.99983	So those set of parameters or variables characterize a generative model and free energy variational.
310578	323930	38	B	0.99811	Free energy is defined as a function of the sequence of observation and external state and its minimization indicates the variational Bayesian inference.
325470	336778	39	B	0.78539	Similarly, we consider a dynamics of neural network and we consider that dynamics is characterized by a set of those variables.
336874	359266	40	B	0.99969	Here x indicates the neuroactivity firing rate at middle raya neurons and y indicate neuroactivity of output raya neurons and W here is a synaptic weight and phi is any other free parameter that characterize neural network.
359458	367894	41	B	1	And we consider that neural network dynamics is characterized by minimization of some cost function l.
368012	385470	42	B	0.98354	This is a function of the O and Phi internal state of neural network and its minimization indicates the generation of neural network dynamics including both activity and plasticity.
385810	392378	43	B	1	And our theory indicates equivalence between those two functions.
392554	403710	44	B	0.99989	Meaning that for any neural network that minimize some cost function error, there is a generative model that satisfy if we call error.
403870	418418	45	B	0.99999	Which means that the recapturation of external dynamics is an inherent feature of any neural system that follow such dynamics.
418594	423910	46	B	0.9999	So this is an interesting prediction but too abstract.
424730	434430	47	B	0.9999	So I would like to introduce some analytically tractable example to understand this relationship formally.
435250	439626	48	B	0.99432	So first we consider a very simple architecture.
439818	445710	49	B	0.89895	This is represented by POMDP without any state transition.
445790	457958	50	B	0.99043	So hidden state s is simply generated by priya distribution D and it is a binary state.
458124	492542	51	B	0.99998	But we consider a vector of binary state, so it is a factory of structure and observation is also a binary vector and the transformation from S to O is characterized by categorical distribution using likelihood of matrix A and variational Bayesian inference indicates the inversion of this generative process like this.
492676	505620	52	B	0.99869	So by solving the appropriate frequency functional we obtain those posterior brief that is optimal in the Bayesian sense.
506890	522134	53	B	0.99917	On the other hand, for neural network we consider such structure here upper part indicates the generation of signal in the external world and the neural network comprises only single layer.
522262	534478	54	B	0.99984	Here activity of output layer are generated by weighted sum of sensory input O weighted by synaptic matrix W.
534644	540590	55	B	0.99	And we then characterize this neural network in the next slide.
541010	564406	56	B	0.98833	So to model neural network or neuron, we start from considering hodgeken hacksray equation which comprises four differential equation and this is a very complicated non linear equation although it is barely possible.
564508	568818	57	B	0.99998	So we consider some reduction of these equations.
568994	598462	58	B	0.99957	So a typical reduction method is like this for example, m here is much faster than other variables so this can be repressed with its fixed point or it is known that h and minus one sorry, it is known that h and one minus SN have similar dynamics.
598526	612214	59	B	0.99942	So we can consider a new effective variable U to characterize those two variables and then we obtain 2D hoskin hacks ray equation like this.
612332	637998	60	B	0.9994	So this is a famous cross of neural network model which includes the well known Fitzfuel Nagamo model or other continuous neural network model and we modify those model to derive canonical neural model.
638164	643630	61	B	0.99368	So this is a definition of canonical neural network model.
643780	657102	62	B	0.98921	Here leak current is characterized by imbass sigmoid function instead of cubic function which is adopted in the Fitzfuel Nagmo model.
657256	689200	63	B	1	And we also consider connection synaptic connection and this part indicates synaptic input from sensory layer through weight matrices and 1 may consider that W one indicate excitatory synapse and W zero indicate inhibitory synapse and the threshold are adaptive thresholds that are function of W one and W zero.
689810	703380	64	B	0.94205	Interestingly, when we consider a fixed point of this differential equation we obtain well known red coding model with the Sigmoidal activation function.
705190	729850	65	B	0.99976	Which means that we can say that in some sense this canonical neural network network model is approximation of Hodgkin hatt's ray equation and its approximation level is, in some sense between the realistic model and the most simplified red coding model.
730000	745300	66	B	0.99995	So we basically consider this type of neural network model and in the next slide we consider what is the plausible cost function for this neural network model.
745670	782730	67	B	0.9997	So again we write the same equation for canonical neural network model which represents the activity of neurons vector of neurons and we consider cost function for this differential equation which can be obtained by simply calculating the integral of right hand side of this equation and get this type of cost function for neural network.
782890	794350	68	B	0.99999	This is biologically plausible cost function in the sense that its derivative derived neural network activity which has a certain biological plausibility.
794870	812760	69	B	0.6351	Moreover, if we consider derivative of this cost function with respect to synaptic weight W, we obtain a conventional synaptic plasticity rule which follows Hebian law.
814410	836030	70	B	0.99988	On the other hand, for Bayesian inference we first define generality model like the previous slide and we then derive variational free energy for given genetic model.
836100	852340	71	B	0.99319	So this variation of free energy is derived from the Pom DP model in the previous slide and its minimization indicates the Beijing inference and running.
852870	866262	72	B	0.99547	So we found formal correspondences between component of those two cost functions like this.
866396	895662	73	B	0.99935	So this broke vector formally correspond to this Brock vector that represent a posterior expectation and this logarithm correspond to this logarithm and actually this a matrix can be represented as the broke matrix like this and its dot product correspond to this computation.
895806	905730	74	B	1	And finally, this phi naturally correspond to this log d logo state prior.
906310	927080	75	B	0.99997	Which means that because the cost function are same, its derivative provides its derivative provides sorry.
927790	936640	76	B	0.99626	Because the cost function are formally equivalent, its derivative sorry.
937010	943440	77	B	0.86142	It's a result of derivative also corresponding each other.
944550	954590	78	B	0.99871	Which means that for any neural activity equation in this form, there is a corresponding Bayesian inference equation.
954670	969618	79	B	0.99987	This is an equation that compute the posterior belief of the hidden state and this synaptic practice equation formally correspond to learning or parameter of genetic model.
969804	1000210	80	B	1	And moreover, by establishing this relationship we can consider the reverse engineering of the generality model from empirical data here this schematic summarize our approach to reverse engineer generative model and we first record the neural activity and assign the canonical neural network to explain this obtained data.
1000360	1006650	81	B	1	And by computing the integral we obtained a cost function for this canonical network.
1006830	1022054	82	B	0.98	And by the mathematical equivalence we established, we can automatically identify genetic model and variational free energy that correspond to this neural network architecture.
1022182	1032458	83	B	0.95467	So interestingly, this is a Bayesian agent which is a kind of artificial intelligence.
1032554	1039198	84	B	0.99999	But importantly, this artificial intelligence is formally derived from empirical data.
1039284	1074780	85	B	0.99998	So we can say that this agent is biomimetic artificial intelligence that follow the free energy principle and then its derivative with respect to parameter posterior derived synaptic plastic algorithms that follow the free energy minimization and its time integral can predict the running process of original neural network data.
1075390	1096866	86	B	1	Which means that if the frequency principle is correct, then this prediction should work, should work and should be able to predict the result of this neural neta without referencing to the data itself.
1097048	1143002	87	B	0.99974	So our strategy is in summary, our strategy is that we reconstruct generative model only from the initial data, like initial data before running and then predict the running process or running curve that this neural system should follow by using the frequency principle and compare or examine whether this prediction is correct by comparing actual data after running and the prediction by the free energy principle.
1143146	1152850	88	B	1	And if this prediction is correct, then it indicates the predictive validity of the free energy principle under setup considered.
1154230	1159330	89	B	0.99236	So we apply this strategy to the in vitro neural network.
1159410	1170142	90	B	0.99155	So here, this in vitro neural network are stimulated using the POMDP generative process defined in the previous slide.
1170226	1184890	91	B	0.99985	So there are two hidden sources that are binary signals and they are mixed to generate 32 sensory stimuli.
1184970	1187630	92	B	0.99987	Those are also binary.
1188450	1205726	93	B	1	And this is an overview of experiment by stimulating in vitro neurons they generate spiking response and those rhyme indicate a high density spiking response.
1205918	1234186	94	B	0.99988	So we observed that in some neuron the response specificity is so we found that for some neuron response was high to source one signal compared to source two signal.
1234378	1277866	95	B	1	And if we see the transition of those neurons, we found that although we removed the offset at the first session to set those activities zero, but we see that those neuron self organized to respond high when source one is one, but those neurons response low level when the source one is zero.
1278048	1297714	96	B	0.99343	So which means that those neurons activity, those neurons response was consistent with our theoretical prediction that neuroactivity cell organized to encode the posterior expectation of healing state.
1297832	1306740	97	B	1	And we also found that some other group of neuron responds preferentially to source two but not to source one.
1308090	1345540	98	B	0.99427	So then we ask okay, then we found that posterior expectation is encoded by neuroactivity and the next question is about other neuronal substrate and we then ask if the prior brief about hidden state is effectively equal to the firing threshold of neural activity model, neural network model.
1346230	1351810	99	B	0.99756	So if the theory is correct, this correspondence should exist.
1352390	1370838	100	B	1	To check this, we first simulated a Bayern agent and when we varied the PRI of the Bayesian agent, the inference was attenuated as I expected.
1371014	1382714	101	B	1	And we found that when we buried the excitatory level of in vitro neural network by using the pharmacological manipulation.
1382842	1403220	102	B	0.99988	We also found that the attenuation of inference which is consistent with our theoretical prediction that firing threshold encode prior belief or the hidden state.
1404710	1430700	103	B	0.61	And next we consider whether the scientific plasticity followed the free energy principle by asking whether free energy principle can predict the qualitative self organization of a subsequent neural data.
1432130	1443022	104	B	0.9993	So here we model neural network like this, there are two ensemble neurons that encode source one and source two.
1443156	1469160	105	B	1	And we first compute the effective synaptic weight of those networks using a conventional connection strength estimation approach and plot those synaptic weights on the landscape of theoretically computed free energy.
1471690	1493040	106	B	0.98002	So this is a trajectory of empirically estimated synaptic weights effective sign optic connectivity and as predicted, those changes reduce the free energy.
1494710	1506638	107	B	0.72	And here we computed this theoretically predicted free energy landscape only using the past ten sessions data.
1506824	1518550	108	B	0.97308	So that this indicates that indicates some prediction of the self organization.
1519310	1530806	109	B	0.97	And for more explicit prediction, we then simulated neuroactivity and plasticity using green as a principle.
1530998	1544138	110	B	0.76368	Here the brighter color indicates the prediction of data without reference to activity data.
1544324	1550850	111	B	0.98246	So those lines exactly follow this free energy gradient.
1551750	1567990	112	B	1	And we found that this predicted trajectory is tightly correlated with this empirically estimated effective synaptic weights.
1568490	1572300	113	B	1	And LR rate is like this.
1572670	1590350	114	B	0.99349	So it indicates that frequency principle can quantitatively predict the circumstance in this setup, so it indicates some predictive validity of the free energy principle under this setup.
1590690	1600542	115	B	1	And then we also consider the modeling of the neuromodulation using active inference.
1600606	1612534	116	B	0.99993	It is well known that synaptic process is modified by various factors like Dopamine, northern adrenaline, acetylcholine, serotonin, so on.
1612732	1644950	117	B	1	And one interesting property of those moderation is that even though Dopamine was added after associative plasticity was established, it can change the result of plasticity in a post hoc or restorespective manner, so it implies some association to the reward and past decisions.
1645050	1649198	118	B	0.99948	So we model this process using canonical neural network.
1649374	1671414	119	B	0.99605	So here we model the post hoc modulation of HEBM plasticity using this type of plasticity equation and we also consider the recurrent neural network structure and output layer for this network.
1671542	1681778	120	B	0.75	And we consider that modulation occur at this connectivity layer.
1681974	1713560	121	B	1	And then we found cost function that can derive those differential equations and then found corresponding variational free energy and genetic model which means that this sort of neural network activity including moderation of sinus plasticity exactly follow the free energy principle under some type of homodp generative model.
1714090	1733610	122	B	1	And by using this, we show that this biologically plausible neural network model with moderation of heavy and plasticity can solve some sort of delayed reward task like maze task.
1734530	1743770	123	B	0.99	And then finally we would like to discuss a possible extension to this framework to the modeling social intelligence.
1743930	1760242	124	B	0.97433	So, to infer our con specifics, we need to select an appropriate generality model for our partners, depending on our partners.
1760306	1782970	125	B	0.99133	So this can be done by Beijing model selection scheme and we previously proposed a model that can predict the multiple biosomes using one big genetic model that comprises multiple genetic models.
1783130	1794378	126	B	1	And this movie shows prediction of prediction of songs.
1794554	1808338	127	B	0.9985	So like this, this model nicely identified which genetic model is the best to explain a given sensory input.
1808434	1824730	128	B	1	And this process indicates and the model can correctly infer the appropriate model and then imitate the song by its own action.
1825550	1844326	129	B	0.99558	So, although in the previous work we didn't discuss a detailed neural neural substrate for this mixture generative model, we now be able to consider the corresponding circuit architecture.
1844458	1857650	130	B	0.99807	For example, if we consider the moderation of neural module by neuromodulator like Dopamine, it act as an attentional filter.
1857810	1870118	131	B	1	And this attentional filter can be explained by three factor Hebian running rule introduced in the previous slide.
1870214	1882450	132	B	0.99982	So again, this modulation works as the post hoc moderation plasticity.
1882550	1897358	133	B	0.76	And this modulation can optimize each model to represent one generative model, one generate one song in a mutually independent manner.
1897454	1906990	134	B	0.99998	So, through this process, it is possible to learn multiple generative model in a vertically plausible manner.
1907150	1919894	135	B	0.99998	So, in summary, we found that the dynamics of kinetic neural network that minimize a cost function can be read as minimization of variational free energy.
1920012	1927590	136	B	0.99869	It indicates free energy principle is an apt explanation for this type of neural network.
1927750	1950740	137	B	0.99	And we also validate this prediction using some in vitro setup by showing that free energy principle can quantitatively predict the self organization of subsequent plasticity only using the initial data.
1951590	1968470	138	B	1	And as the modeling, we can extend those canonical network modeling to the action generation as a planning via the delayed moderation of heavy emperor's DST.
1969130	1987610	139	B	1	And finally, we discuss possibility to extend this canonical model to modeling the social or shared intelligence ah to interact with multiple partners.
1987770	1990510	140	B	0.9993	So, that's all of my talk.
1990580	1992510	141	B	0.52549	Thank you for listening.
1992930	1994474	142	B	0.99994	This is Acknowledgment.
1994602	2002814	143	B	0.99972	Our collaborator and fundings and our unit are now recruiting postdoc researchers.
2002862	2005220	144	B	0.99142	So if you're interested, please check.
2008120	2008916	145	A	0.992	Awesome.
2009098	2011540	146	A	0.99993	Thank you for the presentation, Takuya.
2013160	2018004	147	A	0.55392	I'll just ask a few quick questions from the live chat and a few other things that come up.
2018042	2025400	148	A	0.9705	So Dave says Takuya used the phrase after plasticity was established.
2026220	2030948	149	A	0.99989	Was his group able to modify e g increase plasticity?
2031044	2035100	150	A	0.99968	Or is he saying merely that it was shown that there is plasticity?
2039600	2066656	151	B	0.99761	Yeah, I'm not sure if I understand correctly your question, but those groups show that Dopamine adding Dopamine after 2 seconds after the association occurred can change the magnitude of plasticity.
2066848	2082120	152	B	0.98689	So without how to say if Dopamine addition was before this association, then plasticity level is low.
2082270	2100800	153	B	0.99954	But after Dopamine addition after association can change this level can increase the plasticity like this or like this which indicates the post hook moderation.
2103140	2103648	154	B	0.99989	Cool.
2103734	2105650	155	A	0.97	I think that answers it.
2107460	2111140	156	A	0.99722	Well, what are you excited for?
2111210	2122010	157	A	0.9993	Or what are your hopes or feelings on where the active inference ecosystem is at and where we're headed in the coming months and years?
2126790	2132770	158	B	0.93673	Sorry again chris, just what are you excited about?
2132840	2135314	159	A	0.7023	Other than your own research directions?
2135362	2138870	160	A	1	What are you excited about in the active inference ecosystem?
2140410	2140918	161	B	0.99831	Yeah.
2141004	2159802	162	B	0.98559	So, of course, one direction is the modeling of social interaction, which is much rich architecture than the interaction between the static environment.
2159946	2167322	163	B	0.99919	So if both agents run with each other, then many interesting phenomena can be observed.
2167386	2181970	164	B	0.99982	So we are excited with modeling those phenomena using biologically plausible neural network model through this equivalence.
2184490	2185240	165	A	0.99978	Awesome.
2185930	2187910	166	A	0.99999	Any last comments?
2193130	2194886	167	A	0.99999	Any other comments that you want to make,
2194908	2195910	168	A	0.30439	Takuya?
2203600	2204350	169	A	0.99983	Awesome.
2205600	2207580	170	A	0.99998	Thanks again for the presentation.
2207920	2216736	171	A	1	And people should check out Live Stream 51, where you and I talked a few other times and went into some of the details on that work.
2216918	2217744	172	A	0.99994	There's a lot there.
2217782	2218880	173	A	0.96451	It's really exciting.
2221620	2222610	174	B	0.97961	Thank you.
2223460	2225136	175	A	0.68092	All right, thank you.
2225318	2226690	176	A	0.99998	See you next time.
2227560	2228852	177	B	0.91739	See you next time.
2228986	2229780	178	A	0.89697	Bye.

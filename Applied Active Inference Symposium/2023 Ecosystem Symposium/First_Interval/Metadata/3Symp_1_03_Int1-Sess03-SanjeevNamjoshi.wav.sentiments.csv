start	end	speaker	sentiment	confidence	text
410	17710	A	0.5222861766815186	All right, the next presentation is by Sanjeev Namjoshi, and this presentation is going to be called developing next gen Active Inference Tools, broadening Accessibility, Educational Resources, and the Software Ecosystem.
22440	28470	A	0.875560462474823	I'm going to start this talk right now.
32720	35612	B	0.9273515343666077	Hello everyone, and thank you for being here.
35746	37768	B	0.8525656461715698	My name is Sanjeev Namjoshi.
37864	38796	A	0.7514979839324951	I'm just going to restart it.
38818	40876	B	0.8320296406745911	Machine learning engineer working at the AI.
40908	43680	A	0.7818538546562195	Services firm just because it's a little quiet.
47310	49500	A	0.7752902507781982	All right, restarting the talk.
56140	58972	B	0.9273515343666077	Hello everyone, and thank you for being here.
59106	69840	B	0.8399949669837952	My name is Sanjeev Namjoshi, and I'm a machine learning engineer working at the AI services firm Kung Fu AI, where I primarily focus on computer vision projects.
70340	80240	B	0.7265142202377319	Today I'm going to be talking to you about my progress toward providing greater accessibility, visibility, and knowledge of active inference and the free energy principle.
80820	94696	B	0.9869605302810669	I'm excited to be presenting this material at the active inference symposium this year because the idea of an enacted ecosystem of shared intelligence perfectly captures the philosophy that underlies the projects I'm currently engaged with.
94878	109820	B	0.8432770371437073	I've spent the last seven months on sabbatical for my job to work exclusively on an active inference textbook and related tools, presenting chapter presentations, code reviews, and receiving feedback weekly at the active inference institute.
110640	119096	B	0.8991369605064392	The institute has provided a space where interdisciplinary research can flourish as the connections and influences of active inference spread to other fields.
119208	125872	B	0.9428818821907043	It has consistently fostered the spirit of collaboration and shared intelligence that I wish to embody in my own work.
125926	134980	B	0.8888147473335266	As part of this ecosystem, I intend to continue closely working with the Institute to provide materials that will bring active inference to a much wider readership.
135480	152440	B	0.8178743124008179	I originally chose this project when I saw the great potential in the active inference field and couldn't help but make a comparison to the state of deep learning in 2006, when neural networks were one of just many possible models rather than the dominant choice in academia and in industry.
153660	164430	B	0.785463809967041	This, of course, all changed in 2006 when Hinton and his colleagues released the Deep Belief Network paper, which is generally understood as the start of deep learning as we understand it today.
165040	175810	B	0.8373978734016418	After some hardware innovations and the release of the Wellknown ImageNet Library, we started to see coverage and success of AI in the news as well as in academic research.
176260	186500	B	0.9527628421783447	But in 2012, deep learning truly provided its value with AlexNet, and for the first time, deep learning achieved better than human performance on image detection tasks.
188440	189892	B	0.7131805419921875	But this was just the start.
190026	194468	B	0.4976162612438202	What followed was a proliferation of deep learning all across industry and research.
194634	203370	B	0.7739657759666443	I've added in some Wellknown milestones just to highlight the explosion of progress in deep learning in the last decade, though there is so much more here that we could discuss.
203900	206644	B	0.8440487384796143	So what about the state of active inference as a field?
206782	211260	B	0.7959603071212769	From my perspective, active inference lies in the same position as deep learning.
211330	229280	B	0.7864804863929749	In 2006, influential and on the brink of exploding in popularity, this paper which is from contributors at the Active Inference Institute, shows the current growth of publications in the institute and its community in active inference.
229620	234820	B	0.8271676898002625	In the last three years, the active inference field has seen a number of important milestones.
235560	239876	B	0.605228066444397	Here I show just a few that broaden the scope and attention to the field.
240058	244196	B	0.8133383393287659	We had the first International workshop on Active Inference in 2020.
244378	249924	B	0.8068884015083313	We had the first active inference symposium and the founding of the active inference institute.
250052	252090	B	0.8633327484130859	Then called the active inference lab.
252700	269760	B	0.8824763894081116	We had the release of the Par at all 2022 textbook and the Pymdp Python package, and I see myself now as perfectly poised to bring active inference to greater visibility and attention.
270180	280348	B	0.535956859588623	This is in part because of the current academic interest in deep reinforcement learning and generative modeling working alongside the institution and other organizations.
280444	292340	B	0.661355197429657	My aim here is to provide some of the fundamental materials to capture the attention and notice of machine learning researchers and students to bridge this gap to bring active inference into its renaissance.
295000	302120	B	0.8764572143554688	To this end, I've been working for the past seven months on Sabbatical to finish work on a comprehensive textbook.
302620	320620	B	0.5377286672592163	The aim of the textbook is to provide the tools to bring active inference to a wider audience, primarily those in machine learning research and applied fields such as robotics, and to decrease the challenge in learning the material largely by separating it from much of the neuroscience background that is usually a prerequisite.
321600	337940	B	0.5266478061676025	This decrease in prerequisites means labs will have to spend less time helping students becoming acquainted with the field, and researchers outside of neuroscience will find this book an accessible entry point that uses terminology familiar to machine learning rather than neuroscience and fMRI image analysis.
338920	342980	B	0.8641305565834045	All derivations are in one place currently in the field.
343050	353320	B	0.5550370812416077	Many derivations are spread across different papers, even behavioral papers instead of just technical ones, and it's hard to know where to look if you want to understand a particular equation or concept.
354620	363660	B	0.9041063785552979	Part of the success of deep learning in the last decade has come directly from focusing on narrow improvements to specific aspects of the modeling.
364000	374880	B	0.652823269367218	There are many open questions in areas of research, such as how to prune policy trees, exploring second order optimization rules for state and parameter updates, and scaling active inference.
375620	385060	B	0.850212037563324	The increased accessibility for researchers would also lead to many new industry applications, such as autonomous vehicles, robotics, video game design, and AI.
385880	395880	B	0.9103096127510071	The textbook would also put a spotlight on Bayesian mechanics and invite contributors and contributions from researchers as this exciting nascent field grows and develops.
397420	408424	B	0.8557211756706238	Part Four is largely a literature review and can be very helpful to those writing about active inference from fields such as philosophy, psychology, sociology, and many others.
408622	425490	B	0.8197594285011292	And the historical context sections that are part of this book provide a lot of that context, as Active Inference is built upon decades of research in neuroscience, psychology, and many other fields, and also draws upon current work in many fields that have emerged in the last 25 to 30 years.
426740	435190	B	0.8750163912773132	Finally, LaTech reproducibility may offer interesting ways to rearrange the book and integrate it with the code for an online only experience.
439960	444868	B	0.8374886512756348	Now I'd like to share with you some of the progress of my textbook and the general structure.
445044	447748	B	0.8211641311645508	The textbook is divided into four parts.
447924	451860	B	0.8646130561828613	The first part introduces fundamental concepts to set the stage.
452020	463900	B	0.8032820224761963	In particular, I have focused here on presenting wellknown statistical ideas from the perspective of an agent modeling its environment, who states it must infer from an observed noisy signal.
464720	476720	B	0.8984531164169312	The second part focuses on continuous and discrete state space formulations of active inference, where the algorithm of focus for the continuous state space formulation is active generalized filtering.
477140	488420	B	0.8773727416992188	Part three, which I'll begin writing in a couple of months, focuses on a sketch of Bayesian mechanics and the required background designed with the knowledge this field is still dynamically, changing and evolving.
488920	500280	B	0.5201658606529236	Here I will focus on some of the fundamental concepts and ideas, as well as code simulations to allow readers to get a deeper and more intuitive understanding of some of these challenging ideas.
501020	510348	B	0.632402777671814	Finally, part four is a systematic literature review that covers all the various applications and extensions to active inference that have been innovated in the last six to eight years.
510514	519650	B	0.8289594054222107	These applications include things like robotics, all the behavioral modeling and neuropsychiatry, and human and animal behavior, theory of mind, and so on.
520660	535350	B	0.7158980369567871	The extensions talk about how applications of active inference can be used to talk about dynamic systems more generally, and apply to things like ecosystems and to economies and other types of things like governance, and so on.
536840	544920	B	0.7455153465270996	As of this week, the rough drafts of part one and two are complete, and ten chapters have been presented to the Active Inference Institute.
545740	551640	B	0.5475754141807556	In support of this textbook are four separate tools that I will discuss over the next few slides.
553260	560620	B	0.827618420124054	But before I do that, I would like to first highlight some special aspects and features of my approach to this textbook.
564400	571948	B	0.5271240472793579	The major focus is on writing this book for a machine learning audience or students learning in this and adjacent areas.
572124	574736	B	0.7002095580101013	Neuroscience is out of scope for this book.
574918	586500	B	0.8196462392807007	Many of the recommended and even optional prerequisites that are shown here are typically known by undergraduate students in science and engineering and certainly by graduate students in these fields.
586840	605800	B	0.6098453998565674	Nonetheless, the book is written to be readable by those that desire to focus on everything that is, the math, the code, the concepts, those that just want to focus on the math but are not interested in implementation, and even those that may skim over the math and just try to understand the ideas intuitively.
609520	620816	B	0.9124466180801392	One thing that's very important to me in trying to express these ideas clearly is by spending a lot of time working on typesetting and style, which is very important to successful learning.
620918	625460	B	0.7114996314048767	So I would spend a lot of time attempting to make my work clear and readable.
626520	643000	B	0.8548520803451538	To this end, I have margins which collect specific key terms for references later, which you can see in some of these figures that are shown here, and these terms will eventually correspond to the ontology project ongoing at the Active inference institute.
644220	652460	B	0.7561722993850708	Margins also provide further explanation to accompany the text, and this will be useful to readers who want more detail and explanation.
654080	658324	B	0.5236552357673645	There's a large focus on building an intuitive understanding of the concepts.
658472	662732	B	0.5521819591522217	For example, importantly, all algorithms are explained from scratch.
662876	668336	B	0.9015054106712341	In this book, we typically start with a description of the agent environment modeling problem.
668518	676096	B	0.874305009841919	We then start the book with a univariate case, extended the multivariate case, then we introduce variational inference.
676208	685960	B	0.8344694972038269	We add dynamics, generalized coordinates where applicable, hierarchical models, action and also learning and other modifications we might add to our models.
687820	701324	B	0.606360673904419	We have a very big focus on figures clearly walking the reader through the text and giving detailed visualizations of important concepts and in terms of how the textbook is set up.
701362	711650	B	0.8956813216209412	The early part of the book focuses on basic concepts such as hidden state estimation, that is, estimating the conditional distribution of a latent variable given some observed data.
712660	733700	B	0.8199644088745117	The aim here is to explain the modeling paradigm in the context of an agent attempting to infer the states of the environment, that is, the interaction between a generative model and a generative process, a perspective that differs from the Bayesian inference style that is normally taught in universities and many introductory textbooks.
735340	752510	B	0.8649721741676331	Typically, introductory textbooks on Bayesian inference focus on parameter estimation or learning, and part one introduces the expectation maximization algorithm as a way to explain the connection and separation between hidden state inference on the one hand and parameter learning on the other.
754400	769010	B	0.7014482021331787	Additionally, a large focus has been placed on variational inference, which is explained in detail, and the book features a catalog of all the different forms of variational free energy and expected free energy in the literature and how they can all be derived from one another.
771720	787320	B	0.6378911733627319	The book also covers Rau and Ballard style predictive coding terms and ideas such as key ideas such as prediction error minimization as well as clear and intuitive explanations of fundamental concepts such as surprisal.
788220	802136	B	0.8182058334350586	The textbook focuses heavily on building intuition through derivation and the general flow of most of the chapters is to set up the problem that needs to be solved which is defining an interaction between the agent and the environment.
802328	819200	B	0.8978615999221802	Showing the elements needed to solve it, which is usually random variables and parameters that form a joint distribution or generative model replacing probability distributions with their algebraic formulations and then moving through the algebra to a final analytic or gradient based equation.
819940	823216	B	0.7840411067008972	The readers should be able to recognize most equations in the literature.
823328	835130	B	0.5011811852455139	Upon reading this book, I'm also making extensive usage of Bayesian networks and other types and styles of graphical models such as factor graphs will appear in part four.
836380	856620	B	0.6880427002906799	There are hundreds of custom figures that have been created so far, and the figures are detailed to give the readers a deep understanding of the different types of content that is covered throughout the books, and also summarizes much of the information and equations that are pervasive throughout the active inference literature.
858560	866800	B	0.6615439057350159	Another important focus of the textbook is that many of the models that are presented are also shown in pseudocode, which should aid the reader in implementation.
867780	872716	B	0.6447433829307556	And finally, each chapter is filled with numerous what I call experiments.
872828	879600	B	0.8690475225448608	And these experiments correspond to the Jupyter notebook and try to show the application of a concept in a simulated environment.
879760	882596	B	0.8116268515586853	So a lot of these experiments start out by generating data.
882698	893624	B	0.895240843296051	So we have some kind of generative process and then we have that data passed to a generative model or the agent which then attempts to either perceive and learn from that data and even act on it.
893822	907276	B	0.5877660512924194	And the example that's shown on the right margin here, this is just a perception problem on a continuous grid which has been divided into pieces for the purpose of the simulation, but represents a continuous state space.
907458	915308	B	0.907215416431427	And the agent in the bottom left corner, shown as a mouse, has a prior belief about where some reward food is and its environment.
915404	928068	B	0.8170903325080872	But it then needs to perceive from sensory data that it observes the true location of that reward or food which is obscured or occluded in some way by the mist that's shown in that figure.
928234	940250	B	0.6110636591911316	So these types of experiments give the reader a better sense of how to apply these statistical ideas to a real world situation so we can understand how it might apply to some kind of autonomous agent.
942860	959440	B	0.8099252581596375	Next, I'd like to cover and shift my attention toward Jupyter notebooks and videos and like to note that upon publication of the textbook these Jupyter notebooks will be released on GitHub and should be fully reproducible using Docker and other version handling tools.
960740	971364	B	0.8515744805335999	One of the big emphasis on the Jupyter notebooks is it has to be a direct correspondence between the equations and explanations in the code and in the text.
971562	976500	B	0.6452833414077759	This will build a direct understanding and show applications of the concepts explained.
978360	984890	B	0.8284017443656921	Notebooks are filled with simulations and visualizations many that appear in the main text.
987020	995588	B	0.8533950448036194	And in addition to that, I have also over the last seven months been presenting chapter presentations to the Active Inference Institute.
995764	1010210	B	0.6137033104896545	So far, a draft version of the first ten chapters of the book have been recorded at the Active Inference Institute and these are just some sample slides that I've prepared that try to explain these concepts in great detail.
1011060	1016976	B	0.8423005938529968	In the final stages of writing this textbook, video lectures will be re recorded and released alongside the book.
1017158	1024100	B	0.6960534453392029	I also plan to create detailed code walkthrough videos that walk through the different examples in the Jupyter notebooks.
1026280	1038360	B	0.531697690486908	Now I'd like to talk about a few planned future resources I'd like to work on after the book is complete, or toward the final stages of the book to have further support and educational tools.
1039980	1060568	B	0.5155341029167175	Some of these planned future resources include a software suite in Python to enable an alternative learning approach for those who do not wish to learn about the algorithms from scratch, and this will expand the possible landscape of engagement as Pymdp already exists.
1060664	1078180	B	0.6024950742721558	I will not be working on a discrete statespace Python package, but I'd like to fill the space for things like active generalized filtering, and also just an availability of different types of simulations of Bayesian mechanics as it's currently defined today, or at least the different versions and varieties of some of those key concepts.
1079400	1092516	B	0.8521408438682556	I'm also very much interested in interactive learning, and my aim would be to have these preset simulations concepts that are explained in text, with various simulations interspersed in the form of plots and demos and other visualizations.
1092708	1106750	B	0.7290366291999817	And the idea would be that the user could manipulate sliders and knobs to tweak various parameters that would help aid in learning as they get a feel for how these systems behave, especially ones most of these systems we talk about are dynamics, so seeing how they change over time.
1110420	1120748	B	0.965912938117981	The Active Inference Institute has made tremendous progress in the past couple of years to provide a collaborative environment for researchers and for students of active inference.
1120924	1134010	B	0.9871283769607544	I hope to be part of this ecosystem as I continue to support the spirit of accessibility and collaboration, and I'm excited to continue to contribute to this ecosystem of shared intelligence and look forward to what we can build together.
1138380	1152250	B	0.9517173171043396	I would like to thank the Active Inference Institute for hosting my presentations, code reviews, and feedback sessions and inviting me to present at the symposium, and also thank my employer, Kung Fu AI, for letting me take time off to write for the past seven months.
1153100	1155352	B	0.6189103722572327	Please feel free to contact me at any time.
1155486	1161436	B	0.7226464748382568	Email is the easiest way, but I'm also available on the active inference institute discord.
1161628	1168288	B	0.9027235507965088	If you would like access to the textbook and related materials, please send an email requesting access and I can get you set up.
1168454	1169904	B	0.4973934292793274	And that's all I have for today.
1169942	1171010	B	0.9599631428718567	Thank you very much.
1175080	1175828	A	0.9184247851371765	Awesome.
1175994	1176532	A	0.48966991901397705	All right.
1176586	1177440	A	0.8692706823348999	Thank you, Sanjeev.

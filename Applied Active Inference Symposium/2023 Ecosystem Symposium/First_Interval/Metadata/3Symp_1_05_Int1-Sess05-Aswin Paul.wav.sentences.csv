start	end	sentNum	speaker	confidence	text
1970	7006	2	B	0.95	And now welcome aswin how are you doing?
7188	7914	3	A	0.99802	Hi, Daniel.
7962	8720	4	A	0.99805	I'm good.
9810	12766	5	B	0.3971	Hanging out, looking forward.
12788	13946	6	A	0.74559	Can you hear me properly?
14058	15502	7	B	0.99529	Yeah, sounds good.
15556	22090	8	B	1	And yes, looking forward to your workshop on sophisticated inference in pymdp.
22250	34200	9	B	1	So it can be up to 90 minutes or it's totally okay if it's less and we can take a short break, but please take it away and just let me know however I can help.
34570	35366	10	A	0.99993	Great.
35548	36760	11	A	0.99997	Thank you so much.
37710	39980	12	A	0.90568	Maybe I'll share my screen and start.
40510	52734	13	A	0.99	So, regarding the structure of what we are doing here, I'm assuming that people who see this later might want to try it hands on along with the tutorial or something.
52772	54800	14	A	1	So that's the structure I kept in mind.
55170	57178	15	A	0.99	And so I'll go slow.
57354	58894	16	A	0.99997	Please bear with me.
59092	63854	17	A	0.98	So welcome all of you to this session on sophisticated inference in pymdp.
63902	76286	18	A	0.99	So here we are going to attempt to model some sophisticated inference simulations, especially the one in the original paper using the pymdp module and how it is not part of pymdp right now.
76328	81010	19	A	1	But we are on the process of adding sophisticated inference to the pymtp module.
81170	88662	20	A	0.99	And I'm going to mainly talk about the code that I have developed to kind of add that functionality.
88726	88954	21	A	0.60192	Right.
88992	90854	22	A	0.95	So I'm Ashwin Paul.
90902	95302	23	A	1	I am finally a PhD candidate at Monash University.
95446	108218	24	A	1	And mostly I work with active inference models and try to understand how to use them as an explainable model to basically understand emergence of intelligent behavior.
108314	113374	25	A	0.98667	Right, so let's dive into the material right away.
113572	120626	26	A	1	So to give an intro of free energy principle, I'm sure all of you right now have an understanding of what it is.
120728	128534	27	A	1	But the central idea is that an agent is always trying to minimize the entropy of its observations, right?
128572	138410	28	A	1	So if an observation is having really low probability in your mind and that happens, then you are probably surprised and vice versa, right?
138480	150598	29	A	0.88	And the entropy here is defined as the information theoretic entropy, where if you have a low probability, then automatically this is a high surprise or a high entropic observation.
150774	158986	30	A	1	And as we all know, active inference also gives us a methodology to define what we call an agent environment loop.
159098	170210	31	A	0.96	And this lets us define what is the agent that we are looking at and what is the behavior of the agent given the environment around it and so on.
170280	170562	32	A	0.57591	Right?
170616	173614	33	A	0.96	So you're also familiar with the idea of Marco Blanket.
173662	191414	34	A	0.96	And this is important because we always have to remember, I mean, have to remember the difference between the generative process and the generative model, which is quite a famous point of confusion in active inference literature and for the people who try to understand it in the beginning.
191542	192220	35	A	0.92704	Right?
192590	196582	36	A	0.99	So the central question is that how does an agent minimize entropy?
196646	201994	37	A	0.99948	Because how does an agent know which observation is low or high probabilistic?
202042	202494	38	A	0.99771	Right?
202612	204798	39	A	0.97	So that is by maintaining a generative model.
204884	212458	40	A	1	And the generative model will tell you which is a high probabilistic observation and which is a low probabilistic observation.
212554	221982	41	A	1	So the idea is that all the agent has access to is an observation that is coming from a generative process which the agent cannot directly observe.
222126	233094	42	A	1	And an intelligent agent will try to build up a generative model in its mind, which is a model of the hidden states and the observation it has access to.
233212	238246	43	A	1	And it can hope to kind of compute probabilities using this generative model, right?
238268	246858	44	A	1	But there is a problem that in general it is an intractable problem to kind of marginalize the probability of observations from the generative model.
246944	252774	45	A	1	And that is why we have to define an upper bound on the surprise that the agent is trying to minimize.
252822	255054	46	A	1	And there comes the idea of free energy, right?
255092	259870	47	A	1	So this upper bound the agent is supposedly minimizing is the free energy.
259940	262154	48	A	1	And that's why it is the free energy principle.
262282	272386	49	A	1	And here we have a new term called capital QS, which can be interpreted as the belief that the agent is maintaining about the hidden state in its generative model.
272568	274930	50	A	1	And this quantity is the free energy.
275000	286390	51	A	1	And traditionally we see this variational free energy being interpreted as in mostly the machine learning way where it is a balance between complexity and accuracy of the model.
286540	296074	52	A	1	So when minimizing the free energy, the agent is trying to come up with a single model but at the same time an accurate model because here it's a minus sign with accuracy, right?
296112	313962	53	A	1	It can also be interpreted in the physics way where the agent is always trying to minimize the energy of the model but at the same time maximizing the entropy of the model which goes in conjunction with the maximum entropy principle and so on from the classic literature.
314106	332486	54	A	1	So, given that this idea of a generative model is so important in a software point of view, that's the first thing that you might want to do, right, to define a generative model for the agent which is informed or not informed depending upon the experiment that you're trying to model.
332588	338342	55	A	0.99	So in classical active inference, usually decision making is defined in terms of policies.
338406	347654	56	A	0.98	So, for example, if you are an agent in this environment so in the Mario game, Mario is the agent and everything else is the environment.
347782	355994	57	A	0.82	And Mario has three available actions run, jump or stay in this environment.
356042	361454	58	A	1	And the classic definition of policy is that it is a sequence of actions in time.
361572	368194	59	A	1	So if you have a time horizon of capital T, then a policy is nothing but a series of actions you might take.
368232	371490	60	A	1	So run, run, jump and so on in time.
371560	375278	61	A	1	So this is the SuperScript is the action.
375454	377814	62	A	1	So here it is jump, run and so on.
377852	379734	63	A	1	And the subscript is the time.
379932	388598	64	A	0.99	And then what you can have is a policy space, which is a collection of many such policies, small pi.
388694	395142	65	A	1	And what you essentially do to take decisions in active inference is compute, not optimize.
395206	400830	66	A	0.99964	Compute the expected free energy of every policy in your policy space.
400980	407150	67	A	1	And basically, that can be interpreted as a balance between risk and ambiguity.
410770	417570	68	A	0.79	So when you compute this expected free energy, what you're trying to do is minimizing the risk.
417990	424942	69	A	0.9999	That is, how different is your belief about the observation from your prior preferences?
425086	426066	70	A	0.81521	Capital C.
426168	437302	71	A	1	So this is also part of the generative model when you're trying to model control and at the same time you are trying to minimize the ambiguity when you are choosing a policy that has the minimum expected free energy, right?
437356	455920	72	A	0.9	But this formulation has a problem that a policy space quickly becomes intractable, that there can be an enormous number of small pies or policies in your policy space and sitting and computing the expected free energy for all such policies even for a small time horizon, is not possible.
456370	472366	73	A	0.83	But this is the classical structure that has been implemented in Pymdp nonetheless, where we have different modules in Pymdp that is meant to be implementing different aspects of behavior.
472478	485394	74	A	0.99	So for example, for inference or perception we have belief propagation, fixed point iteration, marginal message passing and all that implemented in the inference module.
485522	501542	75	A	1	In the control module we have different methods to evaluate expected free energy for policies, one depending upon the expected utility, the other one depending upon the classic method that I just explained.
501686	503790	76	A	0.99988	Then we have module for learning.
503860	510734	77	A	0.99	So we learn the parameters in the form DP like capital A, capital B, so likelihood transition dynamics and so on.
510772	517534	78	A	0.96	And then we have algorithms for implementing all this in the algorithms module.
517662	530406	79	A	0.97	And then the most powerful thing in Pymdp right now is the agent class where it is easy for you to kind of define the agent environment loop and we are trying to build up.
530508	539500	80	A	0.61	So today I'm going to talk about an agent class that implements sophisticated inference rather than the classical active inference that we just saw.
540110	548374	81	A	1	So, as I mentioned, how many valid policies can be defined, say for a time horizon of 15 in classical active inference?
548422	549018	82	A	0.99949	Right?
549184	562362	83	A	1	So the first policy of course, is a series of first action that is jumped, then you can have the last action changed and you already can see that there can be n number of combinations.
562426	567726	84	A	1	And for this simple case, the policy space is as big as ten to the power 13.
567838	578470	85	A	1	And in a stochastic problem setting there is no way to kind of come up with a small subset of this policy space so that you can tackle this problem of computational complexity.
578970	584600	86	A	1	And yeah, as I mentioned, in a stochastic problem setting it is an intractable size policy space.
585050	592794	87	A	0.67346	There comes the idea of sophisticated inference where we are thinking about taking decisions in a different way, right?
592832	603214	88	A	1	So rather than thinking about sequence of actions in time, we can directly think of what to do when we see something depending upon our beliefs about the current state and beliefs about the future, right?
603252	606334	89	A	1	So if I see myself in a current situation, what should I do?
606452	611802	90	A	1	And that's more like a straightforward thinking of how to take actions.
611946	614322	91	A	1	And here the expected free energy.
614456	626434	92	A	1	The structure of the expected free energy is the same but we are not evaluating expected free energy of policies, but expected free energy of observation action combinations.
626562	630070	93	A	1	So if I see something and if I do this, what's the expected free energy?
630140	632338	94	A	1	And that's what I'm trying to minimize.
632514	638374	95	A	0.50403	That's what I'm trying to optimize in this setting, right?
638412	645622	96	A	1	So here again, we have the risk term where we are trying to minimize the deviation between the belief and the prior preferences.
645766	654542	97	A	1	We also have the ambiguity term and this together makes up the expected free energy of this time point at time T.
654676	659662	98	A	1	But we also have an expectation about what's the expected free energy in the next time step.
659716	667442	99	A	1	And to evaluate the expected free energy of next time step, you will have to again compute this equation with OT plus two.
667576	672514	100	A	0.93	And for that you will have to again compute this equation with OT plus three and so on.
672552	687270	101	A	1	And this automatically becomes a tree search because of the recursive way this equation is defined and it comes with its own problems, but there are clever ways to get around them and we are going to discuss that in the code today.
687420	698410	102	A	1	So, given the structure of sophisticated inference here, as I mentioned, the research replace the policy space that we saw for the traditional active inference we are used to.
698560	709786	103	A	1	So in this workshop, what I am focusing on is how to kind of define a generative model and given an environment.
709898	725670	104	A	0.96	So for example, this is the grid that is simulated in the original paper and we are going to talk about how to build up a generative model for this grid that can be used in the sophisticated inference in the PMDP module.
727370	745622	105	A	0.84	So basically, what I'm trying to talk about is that the environment will have a step function that takes an action from the agent in Pymdp, and the agent will get an observation out of that action and we'll talk about this particular function, agent step and agent steps.
745766	750414	106	A	0.99321	Step will take up an observation and try to come up with an action for the next time.
750452	750894	107	A	0.98899	Step right.
750932	762298	108	A	0.99	And this creates a loop and cleverly designing this loop will let you see emergence of purposeful behavior in the sophisticated inference setting.
762394	770354	109	A	0.98	So for example, in this particular grid with sufficient planning horizon, you will be able to see that the agent is capable of navigating in this grid and so on.
770392	775860	110	A	0.98	So this is the example that I'm going to focus on in this talk today.
779370	788510	111	A	0.85956	Excuse me, I want to right away jump in to the code.
789600	797264	112	A	0.98	So we have the Pymdp home, which I'm hoping you are familiar with.
797382	809284	113	A	0.99	So we have this GitHub repository where we have the pymdp module and inside pymdp module we have several parts for it.
809322	815980	114	A	1	So here we have the agent in the original Pymdp module that implements the so called classical active inference.
816080	822888	115	A	0.99995	We have several environments and we have help of functions like learning inference, maths and so on.
822974	825764	116	A	0.99	So this is the module of Pimedp.
825892	838444	117	A	1	But in the parent folder we also have examples where there is tutorials about how to use the agent class, how to kind of deal with the environments and so on.
838482	848156	118	A	0.79	So if you look at the pull requests so we are right now trying to merge sophisticated inference into the original Pymdp module.
848268	851132	119	A	0.99	And today I'm going to talk about the code in this pull request.
851196	864800	120	A	0.97	So if you want to try this hands on, you might want to go to this page where this pull request is there and it has the same structure of Pymdp.
864880	867748	121	A	0.94911	It's basically designed using Pymdp.
867844	879752	122	A	0.99	And here what we have additionally is an agent si which is a sophisticated inference agent which does everything and also planning and decision making in the sophisticated inference way.
879886	885692	123	A	0.99	And in the parent folder there is also an example folder for sophisticated inference demo.
885826	891540	124	A	1	And what I am going to do today is to walk you through this tutorial of sophisticated inference.
891640	909136	125	A	1	And on the way, I'm going to discuss and at points where I reference the helper codes, I'm going to go to that code and try to explain what's actually happening and how we complete that agent environment loop where we can see that purposeful behavior.
909248	909910	126	A	0.70963	Right?
911800	912164	127	A	0.80139	Yeah.
912202	914116	128	A	1	So that's the Pymdp home.
914218	917040	129	A	0.57683	Then I also talked about the pull request.
917120	920500	130	A	0.99	So let's right away go into the jupyter notebook.
920580	928440	131	A	0.96	So this is my local copy of this repository, so it's easier for me to run it and show it in my personal computer.
928590	938508	132	A	0.99	So this is the parent folder with Pymdp and examples and inside examples I have a demo folder for sophisticated inference and this is the notebook I'm talking about.
938594	938844	133	A	0.70685	Right?
938882	961236	134	A	0.99	So here in this example, what we are trying to do is deal with this particular grid world task from the original sophisticated inference paper and make this agent or enable this agent to navigate to this red dot, which is supposedly the goal state in this particular task given a prior preference like this, right?
961258	975112	135	A	1	So this prior preference is quite informative in the sense that we right away can see that this is the most preferred state, the white color and the surrounding states are kind of less preferred, but more preferred than the faraway ones.
975166	975384	136	A	0.89577	Right?
975422	978796	137	A	0.97	So this is the grid world task that we are trying to use.
978978	989896	138	A	0.99	So the first cell is importing all the necessary libraries and some useful libraries like NumPy and Matplotlib.
990088	992412	139	A	0.99	And the most important one is Pymdp.
992476	1002112	140	A	0.94	So I'm actually now calling the local copy of my Pymdp with the sophisticated inference implementation, not the original one, which is not merged yet.
1002246	1006292	141	A	0.93	And the first thing I want to talk about is the environment itself, right?
1006346	1012388	142	A	1	So the environment step part where if I get some action, how does the environment work?
1012554	1024104	143	A	0.89	And for that, inside this folder I have a file which is Grid environment Si PY and this is basically an environment class.
1024222	1028096	144	A	1	So don't worry about how this environment is actually implemented.
1028228	1037660	145	A	1	The only thing to worry about is this function that we are going to use which is environment step.
1037810	1050000	146	A	1	So this function will take an action into it and depending upon the current state of the environment, it will calculate what is the most probable next state given this action from the agent.
1050150	1051392	147	A	0.66	So that's the idea.
1051526	1063652	148	A	1	And then it will also calculate a reward of some negligible negative value if it is not the Gold State, and if it is the Gold State, it will give a reward of ten.
1063706	1065124	149	A	1	And that's how the environment is.
1065162	1068900	150	A	0.99934	Designed and it will update the current state to the new state.
1068970	1078148	151	A	1	And basically what it will return is a new state, depending upon your action, the reward for that action and whether it is an end of the episode and so on.
1078174	1085996	152	A	0.99	So this implementation is the standard OpenAI environment implementation and this is the environment step function, right?
1086018	1097724	153	A	0.99	So in this grid, for example, if I am right now in this state and if I take an action up, so I have four available actions north, South, east and west.
1097772	1105252	154	A	1	So if I go on north, then the environment step will make sure that I am in the state that is above the state.
1105306	1107972	155	A	1	And if I go east or west, then I'll stay here.
1108026	1108544	156	A	0.99999	Or south.
1108592	1109332	157	A	0.82997	I'll stay here.
1109386	1111270	158	A	1	So that's the idea.
1112440	1126916	159	A	0.92	And there is an episode length limit that is eight here, that means that I am restricting the length of every episode to be eight, which is the ideal length of reaching this goal state just to avoid confusion.
1127028	1131208	160	A	1	So in this environment, after eight actions, the environment will terminate.
1131384	1139500	161	A	0.53	And if you have to kind of reach this goal state in the optimal time point so that's the idea how the environment is implemented.
1140160	1143120	162	A	0.99661	Okay, I hope that is clear.
1143190	1153756	163	A	1	And there are many helpful functions in this environment, like rendering the environment, rendering a prior preference matrix in this environment.
1153788	1163110	164	A	1	So if you design a prior preference, this environment can show that in a pictorial way how your prior preference is that you will see in the notebook below.
1163960	1169610	165	A	1	So now it's time that we define a generative model for the sophisticated inference agent, right?
1171180	1183164	166	A	0.99996	Before that, let's define the structure of the generative model that we want the agent to have in its mind, which is tailor made for this particular environment, right?
1183282	1191648	167	A	1	So here in this particular grid world task, we have 25 valid states starting from this state.
1191734	1196352	168	A	0.97148	All this black states in this path are valid states.
1196406	1198384	169	A	1	So there are 25 valid states.
1198582	1203756	170	A	0.77314	Then there are four available actions for the agent, north, Southeast and west.
1203868	1205940	171	A	1	So this is part of our generative model.
1206090	1211328	172	A	0.99993	This is also in alignment with the reality of the grid.
1211504	1214772	173	A	1	But this is about what the agent has in its mind, right?
1214906	1219210	174	A	1	And then the observation is just the state space.
1220780	1224490	175	A	1	The problem is fully observable, so there is no ambiguity there.
1224860	1228330	176	A	0.99968	Then we define basically the number of states.
1234170	1245466	177	A	0.62978	Then we define basically the number of states, which is a list of your state space, number of factors, which is now one because you only have one hidden state factor here.
1245568	1248620	178	A	0.99874	Then number of controls, which is going to be four.
1249150	1253562	179	A	0.7888	That's four available actions and your observation space like that.
1253616	1256062	180	A	1	So this is the structure of your generative model.
1256196	1262378	181	A	1	And let's look at the structure of the parameters now inside a POMDP.
1262474	1270846	182	A	1	So the first one is the likelihood function which is often denoted by capital A.
1270948	1276594	183	A	1	And here it is a function of how many observational modalities I have and how many state modalities I have.
1276632	1277122	184	A	0.98589	Right?
1277256	1284840	185	A	0.99977	If I run this cell, yeah, I have to run the parent cell to make sure everything works.
1285450	1290246	186	A	1	So I have rendered the environment the structure of the generative model.
1290348	1295802	187	A	0.83	And here I have the capital A matrix which has a structure 25 25.
1295856	1298870	188	A	0.99999	That means that I have 25 states and 25 observations.
1299030	1305790	189	A	1	And here, because it's fully observable, I am initializing it as an identity matrix of size 25.
1305860	1313978	190	A	0.92	So that's my likelihood matrix that I'm initializing for this particular grid world task.
1314154	1317630	191	A	0.99639	Then the second element is the transition matrix.
1318130	1329650	192	A	0.99	So please note that I'm using all the existing Pymdp functionalities to define a random A matrix and then using an identity matrix on top of that.
1329800	1332914	193	A	0.97	So, yeah, I'm not doing anything new.
1332952	1336690	194	A	0.99879	Here it is the existing Pymdp functionality.
1336850	1342786	195	A	0.99992	Then what I can do now is define the B matrix, which is also called transition matrix.
1342898	1351334	196	A	1	So the transition matrix encodes transitions like where I'm going to end up in the future if I start from a particular state and take an action.
1351462	1358822	197	A	1	So that's the idea where it depends upon the number of states, which is the hidden state modality and number of controls.
1358886	1365806	198	A	1	So it has the structure of state action state where if I take an action from a particular state where I'm going to end up.
1365828	1368078	199	A	1	So that is also a future state, right.
1368244	1374082	200	A	1	So I'm going to initialize it as the true environment state.
1374136	1376482	201	A	1	So now this is part of the environment that I've built.
1376536	1378530	202	A	1	It will give out the B matrix.
1379190	1382950	203	A	1	It might be worth it to look at the structure of this B matrix.
1385370	1388390	204	A	1	So here we have 25, 25 four.
1388460	1400602	205	A	1	So that means that if I take an action from a particular state where I'm going to enter and we have the true transition dynamics for this particular grid by design.
1400736	1408062	206	A	1	So there is a function called get true B and that will give us the true b of the system which the agent can use.
1408196	1408542	207	A	0.98982	Okay?
1408596	1417150	208	A	1	So ideally we would want the agent to learn this, but for the purpose of this demo we are assuming that the agent already knows the structure.
1417890	1428978	209	A	1	And then comes the prior preference, which is interesting here in the sense that it is defined as how closer you are to the gold state.
1429064	1434198	210	A	1	So if you are at the gold state, then clearly that is the most sought out state.
1434284	1436006	211	A	0.69	You prefer that the most.
1436188	1439606	212	A	1	And how do you prefer the neighboring states, right?
1439628	1447626	213	A	1	So that is dependent upon the square root of the distance or basically the distance from that particular gold state.
1447808	1456258	214	A	1	So you define a grid which is eight cross eight, the same size as this particular grid world task.
1456454	1465230	215	A	1	And then we have a method to kind of add values which is the preference you have for every state.
1465380	1477458	216	A	1	And if you render the particular C matrix you can see the structure which is the same where this gold state is more preferred and the surrounding states less preferred and so on.
1477624	1482440	217	A	1	So now we have the C matrix also defined in the classic PMDP way.
1483290	1492722	218	A	1	And then I initialize that C matrix as the C matrix we evaluated in the previous cell which is small C, this particular C matrix.
1492786	1493446	219	A	0.6539	Okay?
1493628	1499930	220	A	0.62	And then lastly, for the generative model we have capital D which is your prior overhead and states.
1500000	1502918	221	A	1	And for that I'm using a uniform object array.
1503014	1506702	222	A	1	So that means that I don't have a prior of where I am starting.
1506836	1510350	223	A	1	So let me run the pending cells.
1512530	1516622	224	A	1	So here the D matrix is a uniform distribution over hidden states.
1516676	1519346	225	A	1	I don't know where I'm going to start the simulations and so on.
1519368	1522898	226	A	1	So this is the basic structure of the generative model.
1523064	1530574	227	A	1	And then we have the agent class which I want to discuss separately, like the environment, right?
1530632	1536550	228	A	0.97	So given these environment parameters, how would you expect the agent class to work?
1536700	1538662	229	A	1	So where is the agent class?
1538796	1540790	230	A	0.99994	Inside this folder structure?
1541850	1555274	231	A	0.99989	Inside the Pymdp module folder we have an agent Si PY which is basically a class again and similar to the environment class here.
1555312	1568938	232	A	0.99993	Also we have a step function where this will take an observation to the function and also a flag whether or not to learn the environment, which is optional.
1569114	1572898	233	A	1	So if you disable it, it won't learn the generative model.
1572984	1576850	234	A	0.99999	If you enable it, it will update the parameters of the generative model.
1577000	1586294	235	A	1	And what basically it does is it will return an action which is to be taken at this point of time and the environment can basically use that action, right?
1586332	1590520	236	A	1	So in this file we have the agent class which I will explain in detail.
1591850	1605420	237	A	1	So I am basically importing that agent class in this cell and then we are going to try and reproduce this behavioral result from the original sophisticated inference paper.
1605810	1606560	238	A	0.99872	Okay?
1607010	1608720	239	A	0.99	And for that.
1609490	1617034	240	A	0.69	So what we expect is that given this prior preference structure, there are local maximas in this prior preference structure.
1617082	1631362	241	A	1	So if you start from this particular point, if you do not plan deep enough, what you will end up is in one of these local maximas where you don't see that there is a highly preferred observation, say four steps down the line.
1631496	1646434	242	A	1	So if you are in this particular state, what you will see is this local maxima and you will go and sit there because the neighboring states are less preferred and this state which is more preferred is not accessible because of the wall or the wall structure.
1646562	1654458	243	A	1	So you have to take a turn and pass through less preferred states and you need deep planning in order to enable the agent to do that.
1654544	1666954	244	A	1	The agent should be able to kind of simulate four time steps ahead in time to see that there is this highly rewarding observation coming to kind of do that actions.
1667082	1671794	245	A	1	So that's the point that we are trying to see in this particular demo.
1671912	1682982	246	A	1	So for a low planning depth it will basically get stuck in one of the local maximas but with sufficient planning depth it will navigate to the gold state.
1683036	1685142	247	A	1	So that's what we are trying to see, right?
1685276	1699094	248	A	1	So we have different planning horizons and what we are basically doing is give the agent a generative model which we right now defined the A matrix, B matrix, C matrix B matrix.
1699222	1702026	249	A	0.99998	Then we have the planning horizon of capital N.
1702128	1705238	250	A	1	So here I am, iterating over planning depth.
1705334	1708410	251	A	1	So N will be one, three and four for the loop.
1708570	1713722	252	A	0.53656	Then we have action precision, which is often denoted by alpha in active inference literature.
1713786	1716894	253	A	1	So that determines which action is to be taken.
1717012	1725394	254	A	1	So a highly precise action precision means that it will stick to the action with the lowest expected free energy.
1725592	1731374	255	A	1	But a lower action precision is kind of probabilistic where it will also consider other actions.
1731502	1738658	256	A	0.99993	Then we have a planning precision which is part of the planning function we'll discuss, which is often denoted in the literature as gamma.
1738754	1753734	257	A	0.99995	Then we also have a search threshold which is extremely important for sophisticated inference because as we saw, sophisticated inference is a tree search and tree search is bad in the sense that it can have a lot of computations.
1753862	1759310	258	A	1	But you have to define a threshold to kind of ignore many possibilities to make it work.
1759380	1761920	259	A	1	And that's the idea that we will also discuss.
1762610	1768510	260	A	1	So just a preview before we go to the agent class.
1768660	1777486	261	A	0.99998	What we are trying to do is in a loop we are going to call the agent step and environment step in series.
1777598	1788418	262	A	1	So the agent will see an observation, it will take an action, that action will go into the environment, the environment will give it back new observations and this loop continues.
1788514	1798442	263	A	1	And we want to see over time how this loop evolves into a purposeful behavior and if the agent at all is capable for that.
1798496	1803498	264	A	1	So before I reveal the results, let's discuss the agent class.
1803664	1812382	265	A	1	So in order to give an action, when an observation is given, the agent should have the planning and so on, right?
1812516	1823966	266	A	0.99	So here is the agent, the sophisticated inference agent, where we are actually using the existing Pymdp agent for some functionalities.
1824078	1833782	267	A	0.95	So in Pymdp we already have really well written functions for perception and learning.
1833916	1842690	268	A	1	So the only thing we want to kind of replace is how the agent is doing planning and how the agent is taking decisions over policies, right?
1842860	1849930	269	A	0.88	So here we are using that parent agent class.
1850080	1860960	270	A	0.92	So from Pymdp agent we are importing that agent class which is sitting next to the Si agent that we are discussing now.
1861810	1883526	271	A	0.92	And basically we are taking in the generative model structure from the main program for this class to work which is C and D and all the precisions and threshold parameter I mentioned, then it is kind of normalizing the prior preference that we mentioned in the main program.
1883708	1899754	272	A	1	So here if I look at how C is, the structure of C is defined in terms of numbers and the prior preference is often interpreted or it should be a probabilistic distribution for the computations to work, right?
1899792	1909902	273	A	0.99	So we are going to normalize it as a probabilistic distribution rather than having numbers that don't add up to one.
1910036	1911454	274	A	0.99	So that's what's happening here.
1911492	1913742	275	A	0.63611	We are using Softmax to do that.
1913876	1932378	276	A	0.6023	Then what we are doing is we are initializing the existing PMDP agent with these generative model parameters and what we are intending to do is write a planning function for a given planning horizon and a given threshold for trees.
1932574	1937080	277	A	0.97933	Okay, so there are three functions in this agent class.
1937690	1941190	278	A	1	One is a helper function for planning which we will discuss now.
1941340	1947354	279	A	0.99993	Then there is a planning function itself which is going to do planning using tree search.
1947552	1962830	280	A	1	And then because it is a recursive tree search, we are going to need an additional function that implements that recursive evaluation where we are going to call this function called forward search inside the function itself.
1962980	1966142	281	A	1	So we are calling this function inside this function.
1966276	1973646	282	A	0.61	So that is to calculate expected free energy for the next step and it will call it again for the next step till our planning horizon.
1973678	1990390	283	A	0.84	So that's the idea of recursive looping and finally it will return the expected free energy for all actions given an observations and then we just implement the step function where it is written sequentially what to do given an observation.
1992330	2007050	284	A	1	So going back to the demo here we have this first idea where you get an observation and it gives out an action.
2007130	2011934	285	A	1	So let's go to the agent step function and imagine what happens.
2012132	2019934	286	A	1	So if it is time t equal to zero or in the beginning of the experiment, what it is ideally.
2019982	2024526	287	A	0.99998	Supposed to do in the first place is infer that state using its observation.
2024638	2037638	288	A	0.99	So what we are giving it is an observation and using the modules for inference, it's going to come up with a belief QS, which is a belief about states.
2037804	2054474	289	A	0.79405	Okay, so self dot QS is the belief inside the agent and once it has a belief about where it is right now, it can implement plan research which is do planning for this particular belief of hidden state right now.
2054592	2064590	290	A	0.99	And once it has done planning, it can take decision using the sample action function in Pymdp and basically return that action.
2065410	2069362	291	A	0.85	And for every other time steps, the sequence remains the same.
2069496	2075620	292	A	1	But it is also learning about the structure if you enable learning in your agent class.
2076470	2078900	293	A	1	So that's the step function.
2079590	2091986	294	A	0.99	But in order to do planning, what it does is it kind of reorganizes the generative model structure for any number of hidden state modalities and any number of observation modalities.
2092178	2106560	295	A	1	So to discuss how melting works, I would like to talk about the new A matrices and B matrices it evaluates for implementing that planning and let's understand that.
2107650	2113274	296	A	1	So let's go back to the original hidden state factors.
2113402	2116266	297	A	1	So here we only have one hidden state factor.
2116458	2118622	298	A	0.58	So that's why it is B zero, right?
2118676	2124238	299	A	0.64	And B one does not exist because we only have one hidden state factor.
2124334	2130020	300	A	0.8	But imagine that if I have two hidden state factors with the same size maybe.
2130730	2137320	301	A	1	So here it could be a location and maybe something else inside the agent's mind.
2137850	2142306	302	A	1	And we should also have controls for these two hidden state factors.
2142418	2161390	303	A	0.99497	Just like so there should be control for every hidden state factor if you're familiar with active inference idea and then maybe the observation space is also directly observing these two hidden state factors.
2161810	2168862	304	A	0.99946	Okay, so this is a new Generative Model structure with multiple hidden states and multiple observation modalities.
2169006	2174114	305	A	0.55	And right away you can see that the dimensionalities of your parameters change.
2174232	2182178	306	A	1	You have 25 observations coming from 25 times 25 hidden states.
2182344	2186550	307	A	1	And if you look at the structure of the first observation modality, it's the same.
2186700	2195594	308	A	1	But what we want is a new matrix where it's going to be 25 with not two hidden states but just one hidden state.
2195632	2204060	309	A	0.99991	It's only a reorganization of the generative model, but computations essentially remains the same.
2204910	2215498	310	A	1	So that's what this helper function is trying to do, which will make things easier for us when we have multiple hidden state modalities.
2215674	2228718	311	A	1	So if you have multiple hidden state modalities, we are going to compute how many total states you have, which is the multiplication of number of hidden states in each modality.
2228814	2229074	312	A	0.99278	Okay?
2229112	2238120	313	A	1	So if you have 25 hidden states in one modality and 25 hidden states in the other modality, you're going to have 625 total number of states.
2238570	2250622	314	A	1	And if you have four actions each in each modalities, then you have total of 16 actions which is nothing but the combination of these four actions each in the modalities.
2250706	2255014	315	A	1	So you're going to have four times 416 actions if you have two modalities.
2255142	2269810	316	A	0.76	And it's basically going to build a generative model that has the same model parameters but just with a different dimension structure so that it's easier for us to calculate the expected.
2270150	2281586	317	A	1	So now we have a new A and new B and a new belief which is nothing but tensor products of existing parameters and beliefs.
2281698	2286470	318	A	0.99955	It's nothing but a new big matrix and nothing else.
2286540	2287160	319	A	0.97518	Okay?
2288250	2291570	320	A	0.99815	It's not a change, it's just a transformation of structure.
2291730	2302346	321	A	1	And given this A, B and Q, we are going to predict what's going to happen in the future and evaluate the expected free energies for them.
2302448	2306986	322	A	0.96	So in order to do planning, so that's the second function.
2307088	2317910	323	A	0.99987	What we are going to do is first call the first function which will do the melting for us and set up the generative model in good dimensions, easy to compute.
2318010	2330110	324	A	0.99158	Then we have the expected free energy itself for all the actions and then we have the probability that depends upon this expected free energy for these actions.
2330190	2333122	325	A	1	So why is it just the actions and not the observations?
2333266	2341606	326	A	0.99999	Because here we are going to evaluate expected free energy of actions for the given observations, right?
2341628	2351286	327	A	0.99	So let me go back to the slides and discuss this pictorially to make things easier.
2351398	2354870	328	A	1	So here we have the grid and we have the prior footprints.
2354950	2369854	329	A	0.96	And what we are trying to implement is that if you observe some observation at time T, then you're going to consider the consequences of your actions given that observation, because you can predict what's going to happen.
2369892	2378226	330	A	1	Because in your generative model you have the transition dynamics that will tell you, given this state, if I take this action, where I'm going to end up, right?
2378248	2389154	331	A	1	So that's basically predicting what's going to happen in the future and you're right now considering the consequence of available actions in your arsenal.
2389282	2397130	332	A	1	And then if you take an action, then you can predict what's going to happen in the next time step as a new observation, right?
2397280	2406406	333	A	1	So you have a probability distribution that tells you that say this observation is the most likely and the other observations are not really likely.
2406598	2417022	334	A	0.99999	Then what you will do is you do this again, you consider the consequence of doing your actions from that particular observation and this goes on in your planning depth, right?
2417076	2420666	335	A	1	So this can be thought of as maybe you want to go to the gym.
2420778	2424030	336	A	0.99996	Then you are going to consider all the consequences.
2424110	2433774	337	A	0.99999	What will happen if I wear my shoes, if I don't wear my shoes, if I go in my car, if I don't go in my car, then you realize that, okay, I have to wear my shoes.
2433902	2441842	338	A	0.99999	Then you consider the consequence that I'm now ready to go to the gym and me going to the gym will end up me being in the gym.
2441906	2443094	339	A	1	So that's the idea.
2443212	2449322	340	A	0.98	You consider the consequence of where you are right now and you can go as much as you want, right.
2449376	2450054	341	A	1	You can predict.
2450102	2453114	342	A	1	So in a game of chess, you might be in a particular state.
2453232	2466634	343	A	1	You consider your consequences, you see the future, you consider consequences from that future, and you can go as deep as you want depending upon your computational abilities.
2466762	2467006	344	A	0.68572	Right.
2467028	2474690	345	A	1	So that's what you're trying to implement in this agent class where we are considering consequences?
2477430	2478180	346	A	0.60782	Yeah.
2478790	2485030	347	A	1	So for every modality we are going to consider the expected free energy for actions.
2485450	2490886	348	A	1	And this will basically call the next function, which is forward search.
2490988	2497078	349	A	1	So forward search is implementing the thing that I just mentioned, considering consequences.
2497254	2502150	350	A	1	And in forward search, what you are basically doing is for every action.
2502310	2506750	351	A	1	So in line 149, I have a loop that goes over every action.
2507090	2512282	352	A	0.99998	Then I'm going to consider the posterior or the consequences of all those actions.
2512426	2517390	353	A	0.94	I use my transition probabilities to evaluate that consequences.
2517730	2524210	354	A	0.95185	Then I'm going to predict the observations, because my prior preferences are defined in terms of observations.
2524630	2533190	355	A	0.98967	I'm going to predict my observations and then evaluate the expected free energy, which is the sum of risk and ambiguity.
2534010	2534758	356	A	0.99899	Okay?
2534924	2536102	357	A	1	I hope that makes sense.
2536156	2544910	358	A	0.99636	Like here, you have considered the consequence, which is consequence of future, which is post or posterior.
2545090	2550794	359	A	1	And you're basically evaluating how good that posterior is depending upon your expected free energy.
2550912	2554454	360	A	0.69	And that becomes the expected free energy for that particular action.
2554582	2556670	361	A	1	And you do this for all the actions.
2557330	2558030	362	A	0.99862	Okay.
2558180	2560554	363	A	1	And why this is powerful?
2560682	2563742	364	A	0.85	It is because you can go as deep as you want.
2563876	2574574	365	A	1	So here in the next step, you go to this loop where you will check if I am crossing my deep planning or the depth of planning.
2574702	2576578	366	A	1	And then you are doing basically the same.
2576664	2582130	367	A	0.99964	Given that posterior, what is the consequence of the actions of that particular posterior.
2582210	2587078	368	A	1	So here for considering that we are again calling the parent function.
2587244	2596330	369	A	1	So the same function, forward search, to consider consequences of those combinations and it will basically come back and add up to your expected free energy.
2596480	2609482	370	A	1	So what happens over this sequence is that you consider some or all future consequences and then all that values will trickle up your tree.
2609546	2619650	371	A	1	And that sum of the expected free energy will tell you which action is good or which action is bad, which you can take to kind of see your preferred observations.
2620150	2623298	372	A	1	So that's the idea of implementing tree search.
2623464	2632998	373	A	0.72	And I will also talk about the importance of this threshold here, which makes this algorithm possible.
2633084	2636630	374	A	1	So without this threshold, this algorithm will not work.
2636780	2640230	375	A	1	I will explicitly talk about why that is the case.
2640380	2652646	376	A	1	And then once you evaluate the expected free energy for all available actions, given the present state, you can basically compute what you call the action distribution.
2652838	2657550	377	A	0.99999	That is how probable is my action or how I should take my action.
2658850	2662298	378	A	0.97	And we also have this action precision parameter alpha.
2662394	2671922	379	A	1	So if alpha is very high then it basically is a highly skewed distribution where you will always choose the action that minimizes expected free energy.
2672056	2678542	380	A	0.99999	If alpha is really low then it's going to be a more sparse or spread out distribution.
2678686	2687938	381	A	1	And then you can use this action distribution to sample actions in the agent environment loop.
2688034	2693106	382	A	1	We just finished doing planning and computing that action distribution.
2693298	2698250	383	A	0.99998	Then using that action distribution, you can sample an action from your policy space.
2698400	2701580	384	A	1	So let's look at the policy space in this generative model.
2702910	2717520	385	A	0.6	So I'm switching back to the original generative model with one hidden state factor and let's do planning and maybe initialize this agent.
2718310	2723970	386	A	0.85	I just want to initialize this agent to see the policy space, not run the loop.
2724390	2733288	387	A	1	So I initialize this agent, say for planning depth of one.
2733454	2751808	388	A	1	And if I look at agent policies, I can see that I have basically four available actions which is north, south, east and west.
2751904	2757552	389	A	0.93	And if I have an action distribution, it will tell me how probable is to take that action.
2757616	2772780	390	A	0.99	So if I look at agent UPI, okay, so this is not defined because I have not done planning, but I can do planning and then it will be defined.
2778580	2783776	391	A	0.97022	Yeah, so I implemented planning with research and then now I have an action distribution.
2783888	2790424	392	A	1	So for this particular scenario, I am going to take my third action the most which is zero point 99.
2790462	2796184	393	A	0.99994	Basically that's the probability which is north, south and east in this particular case.
2796222	2806060	394	A	1	I just wanted to kind of familiarize you with the matrices, but we are now going to see the agent environment loop in action.
2808880	2825424	395	A	0.99	Now you can sample an action from the sample action function and then implement learning which is using the standard Pymdp way where I will update my transition dynamics and likelihood dynamics depending upon what I see and what's my belief and so on.
2825462	2829510	396	A	0.99	So my emphasis is on the decision making part.
2838010	2844466	397	A	0.85	So once you sample an action, then that action basically goes back to the environment.
2844578	2852490	398	A	0.99708	Okay, so now let us implement this for a planning depth of one and see how the agent behaves.
2852990	2864302	399	A	1	So here if it is a planning depth of one, then that means that the agent is only considering the consequence of one time step ahead, just seeing the immediate future for doing planning, right?
2864436	2867120	400	A	1	So I'm giving the planning depth of one.
2867730	2889150	401	A	0.54498	I'm resetting the environment where the agent is going to start from that initial start state, and in the loop, it's going to get that observation, take that action, give back an action, and we will look at the action probabilities.
2889490	2896490	402	A	1	And also we will give that action back to the environment, get back the observation, and this loop continues till the episode is terminated.
2896570	2902110	403	A	1	And I have set the episode length to be eight, just to see the outcome of eight action.
2902870	2912006	404	A	1	So when we run this loop, these matrices are nothing but the action distributions of how likely each action is to be taken.
2912188	2915846	405	A	0.81	And this is where the agent end up in the last time step.
2915948	2916502	406	A	0.94714	Okay?
2916636	2925100	407	A	1	So let's maybe kind of also enable this environment render that will show us where the agent is at every time step.
2925870	2933098	408	A	1	So initially the agent was at this location and we have an action distribution of this here.
2933184	2934934	409	A	1	So north, south, east and west.
2934982	2937258	410	A	1	So the agent knows that it should go north.
2937354	2937950	411	A	0.51968	Why?
2938100	2944414	412	A	0.99999	Because if I look at the prior preference, this state is more preferred than this state.
2944532	2944862	413	A	0.9951	Right?
2944916	2953714	414	A	1	So the agent successfully calculated the expected free energy and inferred that, okay, I should go to this state and not stay in this state.
2953752	2962166	415	A	1	And because it has the generative model of transitions available, it can infer that I should take an action north to go to this state.
2962348	2963686	416	A	1	So that's good.
2963868	2965858	417	A	1	And the agent goes to north.
2965954	2970530	418	A	1	And at this particular state, the agent infers that it should go to north southeast.
2970610	2975274	419	A	1	So it will take an action east and it will go here.
2975392	2983318	420	A	1	And at this point of time, I want your attention where the action distribution is equally probable for north and east.
2983414	2984362	421	A	0.99992	Why is that?
2984496	2987166	422	A	1	Because the agent is only looking at the immediate future.
2987268	2987582	423	A	0.99981	Right.
2987636	2996254	424	A	1	So let's go back to the prior preference where the agent is right now here, or is it here?
2996292	2999714	425	A	0.99946	Yeah, it is right now here in this particular state.
2999832	3009538	426	A	1	And if the agent is considering immediate consequences of just one action, then these two states are equally good for it to be in the next state.
3009624	3015014	427	A	1	So there is no distinction between these two states if it is only looking at the immediate future.
3015132	3021026	428	A	1	So that means that the expected free energygies will conclude that I should go to north or east.
3021058	3024538	429	A	1	It doesn't matter if I'm looking at just one time step ahead.
3024704	3025418	430	A	0.99243	Okay?
3025584	3026860	431	A	0.63554	That's the idea.
3027790	3032380	432	A	1	And out of probability it is going here.
3033150	3052154	433	A	1	It took the action east and from this state, when it's doing inference, it's inferring that this state is better and basically it's ending up in this local maxima state, which is this particular state where the neighboring states are less preferred.
3052202	3058302	434	A	1	And this is Wall and you can't go there because it is forbidden for the agent by structure.
3058366	3065338	435	A	1	So it's basically going to sit there forever where it only sees that local maximum of prior preference.
3065534	3072526	436	A	1	And let's look at what might happen if you have higher planning depth.
3072658	3083866	437	A	0.97	So if I go to the planning depth of three, then that means that the agent is actually reaching the goal state at the last time step.
3084048	3093018	438	A	1	But still in the third time point, it had two probabilistic actions, north and east.
3093114	3103678	439	A	1	So here from this particular state, out of probability, it took the action north but it can all equally take the action east and end up in this local maxima.
3103694	3108626	440	A	1	So let's run again and probably it will end up in this local maxima, okay?
3108808	3121590	441	A	1	And only for the planning depth of four, which is sufficient enough, which is necessary for this particular grid, the agent is fully sure of what to do.
3121660	3133834	442	A	1	So at every time point it is fully sure of what to do, that it has to go north, then east, north, north, north, east, east and south and reach this particular goal state.
3133952	3144906	443	A	0.91	So only for time step or planning depth n equal to four, it can successfully navigate this grid with 100% certainty.
3145018	3146720	444	A	1	So that's the idea.
3147250	3150786	445	A	0.90285	That's the implementation that I hope you got.
3150968	3153774	446	A	1	So there is also this idea of action precision.
3153822	3155886	447	A	1	So here it's a high action precision.
3155998	3158574	448	A	0.99999	That is why it is taking the actions.
3158622	3160690	449	A	0.99956	That is from the probability.
3160850	3169026	450	A	0.99999	If it is a low action precision like one, then the good actions are more probable.
3169218	3178442	451	A	1	But that doesn't mean that it will be taken right here by luck, it is taking the right actions and reaching the state.
3178576	3180566	452	A	1	But here probabilities are most passed.
3180598	3188794	453	A	1	But you will also see like exploration behavior in more number of trials if you control this action precision.
3188842	3195230	454	A	1	So I set it to a high value to make sure that the agent reaches the goal for this particular problem.
3195300	3199460	455	A	1	But it's worth playing and it's important, right?
3200470	3203042	456	A	0.90056	Okay, yeah.
3203096	3210694	457	A	1	So for different planning depths like one, three and four in this problem, this is the behavior that you expect.
3210892	3222846	458	A	0.74976	For lower planning depths, which is not sufficient, the agent ends up in local maximas or local minimas of expected free energy or local maximas of prior preference.
3222978	3227290	459	A	1	But with sufficient planning depth, it's able to navigate and reach the goal.
3227950	3244922	460	A	1	So that brings us to the last point in this tutorial where why is it important to have a threshold in evaluating sophisticated inference?
3245066	3252354	461	A	1	So by threshold, what we mean is that we can ignore future possibilities in two levels, right?
3252392	3258370	462	A	1	You can ignore not likely actions or not likely observations in this research.
3258520	3267542	463	A	1	But if you consider the consequences of all actions and observations, that means that you'll have to consider four consequences in the first place.
3267676	3273730	464	A	0.99999	Then you will have to consider four times the action states in the next time step for the next time step.
3273820	3280294	465	A	0.71035	Then all of that multiplied with the number of actions and this tree search becomes intractable and you'll explode.
3280342	3285100	466	A	1	And it's even worse than the classical active inference policy space problem.
3285410	3291630	467	A	1	But by defining a threshold of even a small value, we'll ignore possibilities.
3291970	3294510	468	A	1	So where is that implemented?
3295410	3305246	469	A	0.99997	In the forward search algorithm, we are considering actions with only action probabilities greater than the particular threshold.
3305438	3308030	470	A	0.9184	Here I am defining it as one by 16.
3308190	3314630	471	A	1	Also in the parent paper, it's one by 16 the action probability.
3316170	3316920	472	A	0.91376	Okay?
3317290	3324686	473	A	0.62	So if it is zero, then that means that it will consider all the consequences of future and that's intractable.
3324818	3346480	474	A	1	So you can ignore actions which is not probable, and also ignore states which is less likely, or only consider states that has probability greater than this particular threshold value, and that significantly reduces the computational complexity, where you will only consider combinations that are probable in the future.
3347190	3352290	475	A	0.44129	Tree and that lets you go deeper in your planning horizon.
3352870	3354674	476	A	0.99964	That's an important point here.
3354792	3371618	477	A	0.99	And if you compare the time that takes for deeper planning for a search threshold of zero, so a search threshold of zero means that you will consider all consequences.
3371794	3377498	478	A	1	And the more deep you plan, the more time it takes.
3377584	3385450	479	A	1	And if you see to consider only the first future or the immediate future, it takes 0.1 second.
3385790	3392474	480	A	0.99991	For considering three possibilities into the future, it takes 3 seconds, and for four it takes 300 seconds.
3392522	3396430	481	A	1	And you can see that the computational time is exponentially growing.
3397190	3412694	482	A	0.39	But if you have a very small search threshold, you have that computational time that makes sense for implementation in real world, right?
3412732	3420386	483	A	1	So here for N equal to four, that is four times steps into the future, it's only taking 0.1 second.
3420418	3421302	484	A	1	And that's okay.
3421436	3431722	485	A	1	I can still do simulations with this complexity, but there is no way I can talk about how less the computational complexity is.
3431776	3436678	486	A	1	It truly depends on the nature of your prior preferences and environment in action.
3436774	3443840	487	A	1	But this search threshold actually works in real life, and we just saw that in our simulations, right?
3444210	3448350	488	A	0.99993	For N equal to four, it took only like 0.3 seconds in our simulations.
3448510	3456830	489	A	1	But if you set the search threshold as zero, it already is 300 seconds for doing full depth planning.
3456910	3466470	490	A	1	And if I set a planning depth of five, then it will basically run forever maybe, and I will not be able to do simulation.
3466810	3469750	491	A	1	So that's the idea of search threshold.
3470810	3472940	492	A	1	So actually, that's it.
3473790	3486762	493	A	0.99	I wanted to explain the Agent class, the environment class, and the particular demo and yeah, maybe it's a good time for questions if anybody was listening.
3486906	3499138	494	A	1	And I hope people get to play with this code and look at the tutorial and implement this and build generative models like this.
3499224	3509518	495	A	1	So this particular example is how do you build a generative model for this grid world task and see how the Agent is able to take meaningful actions?
3509694	3516262	496	A	1	But here we gave it that true structure of the environment in the B matrix and A matrix and so on.
3516396	3532700	497	A	1	But you can also play around with learning in the sense that while defining the Agent step, you can add a flag that says learning equal to true.
3534750	3541962	498	A	1	And if you start with an uninformed AB and so on, you can experiment on how the Agent is learning that environment.
3542026	3550530	499	A	0.99991	Here you can look at the B matrix in the beginning, you can look at the B matrix after, say, ten trials and see how the learning is taking place here.
3550600	3555058	500	A	1	It doesn't matter because the agent knows the structure and it won't learn much.
3555144	3560914	501	A	1	But if it starts from an unknown structure, then there is scope of learning also to be implemented.
3560962	3572646	502	A	0.97	And it's already implemented because we are using existing Pymdp functionalities for learning A and B and it's already part of the step function.
3572828	3585260	503	A	0.98	So I hope step function is clear, which is the only thing you need to know if you're trying to implement sophisticated inference and just the names of these matrices if you want to probe them and look at them.
3585630	3589482	504	A	0.63	And yeah, I hope the session was useful.
3589626	3609362	505	A	1	So I thank my collaborators and Connor, who maintains Pimdp, and also Brendan who runs the Pimdp Fellowship, which I was part of, and that's where I worked on implementing sophisticated inference in Pimdp and it will be part of the original Pymdp module soon.
3609496	3617378	506	A	0.76	And I hope people can start using this module to simulate sophisticated inference experiments and this basically becomes useful.
3617554	3625240	507	A	1	So maybe it's a good time to discuss questions or clarifications on the code or maybe it's a good time to take a break, as Daniel was.
3627870	3628570	508	B	0.99908	Thank you.
3628640	3629660	509	B	0.99997	That was awesome.
3630590	3636890	510	B	0.99899	Well, I have a few different questions and I'll read a few from the live chat.
3636970	3642750	511	B	1	So I'll first just go to the live chat and then ask those and then ask some other questions.
3642820	3648814	512	B	0.97	So Dave asks, how do you think about the neural implementation of recursion?
3648942	3654418	513	B	0.99996	Brains don't seem to implement computer hardware style recursion deeper than a stack depth of one.
3654504	3665190	514	B	0.99999	Aside from heavily over learned tasks, we can confine ourselves to asking about recursion for the purposes of exploring temporally deep state spaces, searching forward in time.
3665340	3680090	515	B	1	So how do we reconcile this really beautiful and elegant and computationally efficient full depth tree search with the biological basis of multiscale planning?
3681070	3694142	516	A	0.99987	Yeah, so I'm not an expert in neural computation, but the answer to that would be basically you're doing only one computation at a time, right?
3694276	3704562	517	A	0.99	And all you need is some memory to store your beliefs and use those beliefs to kind of do the same computation again.
3704696	3711906	518	A	1	So we are not talking about this hardcore recursive implementation.
3712018	3714646	519	A	0.9998	We are only doing local computations at a time.
3714748	3723014	520	A	1	And just because of the structure of the generative model and because we have memory, this can be done.
3723132	3729754	521	A	1	And I don't see why brain can't do it, even though individual neurons might not be able to do it.
3729792	3731494	522	A	1	The brain has memory.
3731622	3737226	523	A	1	The brain has the ability to store memory and the ability to dream, the ability to simulate.
3737418	3739674	524	A	0.99	It knows the consequences of actions.
3739802	3746286	525	A	1	And you do this on a daily basis where you plan your future and decide what to do.
3746308	3746542	526	A	0.99915	Right?
3746596	3756050	527	A	1	So on a single neuron level, I'm not really sure of how to answer that question, but I don't really see why the brain can't do it as an organism.
3758710	3759460	528	A	0.99326	Cool.
3760870	3762262	529	B	0.78117	Okay, to the code.
3762316	3764230	530	B	0.98	I guess I have a few questions.
3764380	3766310	531	B	0.72987	Can we go back to the maze?
3771000	3775450	532	B	1	And of course, if anyone else has questions in the live chat, just go for it.
3777500	3786596	533	B	1	So in the maze, how does the moves that are possible, how is that reflected?
3786708	3795644	534	B	0.9999	What stage is it updated, for example, that it initially knows that it can only go up and then it can only go right or down?
3795842	3802684	535	B	0.99985	Where is that reflected with the updating of kind of that relational aspect of affordances?
3802732	3804530	536	B	0.99999	What is even possible to do?
3804900	3808530	537	B	1	And then how does it evaluate in a deep tree search?
3810660	3816870	538	B	0.99994	Does it need to know what things could or couldn't happen in the future?
3819560	3829064	539	A	1	So, if I understand the questions correctly, there is an important distinction between the generative process and the generative model, right?
3829182	3846620	540	A	1	So in the grid world, which is the generative process, we have implemented all of that manually, where we have this transition dynamics that already knows what will happen if you take an action from a state.
3846770	3849950	541	A	1	So it's like an environment that knows how to work.
3850320	3855376	542	A	1	So it's like a reality where there are consequences of actions and it's already there.
3855558	3861808	543	A	0.51	But this information is available to the agent to be part of its generative model.
3861974	3872164	544	A	1	And in the generative model, what it basically does is use that transition dynamics, which is given or learned to simulate what might happen in the future.
3872362	3872916	545	A	0.99397	Okay?
3873018	3889530	546	A	1	So once I have that transition dynamics, if I go to the where's the if I go to the tree search, what it is essentially doing is evaluating the posterior given a particular state and an action J.
3889920	3900404	547	A	1	So from my generative model, I know that if I start from this state and if I take this action, I go to this posterior and I consider all the consequences.
3900552	3912610	548	A	1	And if in the generative model, it is unlikely that if I go east, I don't go there and so on, it's automatically reflected in the expected free energy.
3914360	3916548	549	A	1	So I hope that answers the question.
3916634	3943260	550	A	1	So here, if I set the action precision to be high and also enable the so initially I am in this particular state, and with respect to the expected free energy, what I'm doing is I'm using my generative model to predict what will happen if I take four actions.
3943600	3947436	551	A	1	And my predictions will say that if I take North, I will go here.
3947538	3950204	552	A	1	And if I take all the other actions, I'll stay here.
3950322	3958640	553	A	1	And because in my prior preference, north is more preferred, I can infer that north is the action I should go for.
3958790	3960130	554	A	1	So that's the idea.
3960500	3963152	555	A	1	So here, the grid structure is given to the agent.
3963206	3968164	556	A	1	And it might be a little confusing, but the agent can also learn this grid structure and this will work.
3968282	3975684	557	A	1	So once I know the grid structure as an agent, I can simulate the future and consider the consequences of action, right?
3975882	3977444	558	A	1	So that's what's happening.
3977642	3987770	559	A	1	And once I am in this state, I will consider the consequence of all four actions, and I will infer that, okay, going east is better because that will take me to this state.
3988300	4002130	560	A	1	So there is always a difference between or you should keep in mind how generative model and generative process are two different things, and what the agent knows might not be the reality or might be the reality, depending upon what you give it.
4004260	4005344	561	A	0.52676	Makes sense.
4005542	4006290	562	A	0.99994	Cool.
4006660	4027380	563	B	0.98	So if you were going to go about making a new situation that you wanted to do generative modeling for, do you tend to start from an existing working Pi MDP notebook and start modifying state spaces, or do you draw it out on a canvas?
4027540	4032408	564	B	0.66156	How would you recommend somebody other than replicating what is shown here?
4032494	4036180	565	B	0.99458	Let's just say we're interested in something that's not exactly just a maze.
4036340	4044220	566	B	0.90536	What do we do to get our head around how we should proceed?
4044880	4047180	567	A	0.99923	Yeah, good question.
4047250	4058210	568	A	1	So if you are trying to simulate, say, a new environment, you have the heavy lifting to do, which is to define the generative model for the agent.
4058900	4065232	569	A	1	You can either define a very sparse generative model, which the agent can learn, but you have to define the structure.
4065296	4071812	570	A	0.99999	That structure should be there, and only using that structure, the agent can learn the generative model.
4071866	4078680	571	A	1	So here you can make use of this cell of code to understand how I define the structure of the grid.
4079020	4083672	572	A	0.9948	Okay, so I am defining a structure for the agent for it to make use of.
4083726	4088472	573	A	0.98892	I'm saying that there are 25 valid states and there are four available actions.
4088536	4094056	574	A	0.92	And this is the standard way of defining the state space in Pymdp.
4094168	4101644	575	A	1	And you also have to define the central parameters ABC and D for the agent simulations.
4101772	4107552	576	A	1	So here I am defining A using my state space and observation space.
4107686	4115856	577	A	1	But this step of giving it or telling it that it's an identity matrix is my decision choice in my modeling.
4115968	4119152	578	A	0.51	I don't have to do this for simulations.
4119296	4126916	579	A	1	I can see if the agent is learning it from a random A matrix or when it starts from a random A matrix.
4127108	4132932	580	A	1	And similarly for this B matrix, this structure is defined.
4132996	4137428	581	A	1	And there are functions like this which can give you a random B matrix.
4137524	4148760	582	A	0.77	But this is a modeling choice where if I want to give it the grid structure or not give it the grid structure, I can start from a random B matrix, let the agent learn, and look at that learned B metric.
4148920	4150620	583	A	0.99999	Just for the purpose of the demo.
4150770	4154332	584	A	1	I gave it the grid structure to enable it to take the actions.
4154476	4156624	585	A	1	But it's not a mandatory thing.
4156662	4165208	586	A	1	So this notebook is useful in the sense that you know what to do, but definitely you should play around with steps that may not be mandatory.
4165324	4172020	587	A	0.97668	Right, so if I give a prior preference for this state to be the maximum, then you can see behavior that.
4172090	4173652	588	A	1	The agent will try and go there.
4173706	4173924	589	A	0.99884	Right.
4173962	4182456	590	A	0.88	So this prior preference is defined in conjunction that this is the goal state, but this may not be the goal state.
4182558	4187800	591	A	1	And in a different ask, what prior preference means is different according to the environment.
4188380	4189140	592	A	0.99778	Right.
4189310	4191304	593	A	1	So that's also there and your prior.
4191352	4199668	594	A	1	So once you define that generative model, which you have to do, you can't run from it, then everything is kind of automated.
4199784	4208876	595	A	0.95	The agent only have to use the agent step, give it the observation from the environment.
4208988	4215140	596	A	1	The agent knows how to take actions, and then everything that happens inside the agent, you don't really have to worry.
4217480	4228200	597	A	0.97	So this structure, I'm sure, will be useful for your particular task that you are trying to model in your hmm.
4231660	4233512	598	B	0.99749	Yes, very interesting.
4233646	4242232	599	B	1	And how would you contrast that or point to any similarities or differences with how this would be pursued outside of active inference?
4242376	4259520	600	B	0.99898	Like if somebody were just going to use another kind of deep policy agent in the maze example, what parts of the process would be familiar and what parts would be like a lot of work that wasn't expected or skipping through parts that were a lot of work otherwise.
4260020	4260770	601	A	0.9983	Yeah.
4261700	4269076	602	A	1	So the general structure is very familiar to somebody who does things like this in reinforcement learning.
4269178	4283544	603	A	0.96	So the idea that it's an agent step and environment step, so this is the standard OpenAI gym way of writing an environment and a standard OpenAI way of writing an agent.
4283742	4284104	604	A	0.57429	Okay?
4284142	4294572	605	A	1	So if you have a Q learning agent who does the same trying to navigate, then the way you have to define the queue matrix is the heavy lifting there.
4294626	4296632	606	A	0.802	It's just a state action mapping.
4296776	4303072	607	A	1	And in contrast to that, in active inference, you have to come up with a generative model that you want to see.
4303126	4306784	608	A	1	So in active inference, generative model is the central thing.
4306822	4307410	609	A	0.99827	Right.
4307940	4314204	610	A	0.99991	Without a generative model, there is no meaning of purposeful behavior in active inference.
4314332	4323124	611	A	1	So the only unfamiliar part for a person who is coming from, say, a field like reinforcement learning is the structure of the generating model.
4323242	4329796	612	A	1	But there is no way, other than getting used to it, where it's the palm DP structure which dominates.
4329908	4338700	613	A	0.52	But if you're doing deep active inference, all of this is going to be neural networks and palm DPS are also not active inference things.
4338770	4339052	614	A	0.89864	Right.
4339106	4341676	615	A	0.98584	It's an industrial engineering thing.
4341858	4356716	616	A	1	So palm DPS must also be familiar for people who are coming from the computer science background, just the idea that what really happens in the agent is the active inference part where we have expected free energy s and variation of free energy energies.
4356828	4362172	617	A	1	And if you want to learn about that, then you have to go to the agents and see how it works.
4362326	4365044	618	A	0.99797	Look at the matrices numerically, see what's happening.
4365162	4370228	619	A	1	But in a level where you want to get started, I don't see any problem.
4370394	4379172	620	A	0.99969	All of this is standard frameworks like palmdps and OpenAI gym environments, agent environment loop.
4379236	4383444	621	A	0.99972	All this is very deeply discussed in computer science.
4383572	4388750	622	A	0.94466	It's not cool.
4390560	4407090	623	B	0.99	So what other motifs or cognitive phenomena are you excited or how do you see the Pi MDP development trajectory continuing after your sophisticated inference gets pulled in?
4407540	4420404	624	A	0.93861	Yeah, so the Pimdp had the original functionality and the functionality to implement or simulate general active inference agents with the policy space and so on.
4420442	4437592	625	A	1	And that enabled a lot of people in the community who are not familiar with complicated coding and so on, who people who do psychology, psychiatry, and all the things.
4437646	4437828	626	A	0.69446	Right.
4437854	4443932	627	A	0.99	So whoever want to come up and try implement active inference, pymdp enabled that.
4444066	4452828	628	A	0.99	And I am hoping that this module will enable people who want to try out sophisticated inference experiments in their particular domain.
4452924	4466756	629	A	0.96	So if you spend some time and get familiarized yourself with the structure of how Pymdp works, then everything else is just writing a jupyter notebook with minimal code, right, to simulate this.
4466858	4475624	630	A	0.98	So if you have a particular task in your domain, I don't see a problem for a beginner to kind of try and code it.
4475662	4487048	631	A	1	And what I'm very excited to see is people using this module for variety of experiments, just like how people started using Pymdp and sophisticated inference is taking off.
4487214	4495992	632	A	1	And it's now widely talked about how it is the way of doing active inference.
4496056	4504000	633	A	0.99	And I'm really hoping that people in various domains start using this module and see their experiments, and I look forward for the feedback.
4504420	4513090	634	A	0.60393	Yeah, so what Pimdp did two years ago, I'm hoping this module will do to people who are trying to model active inference in the soap state.
4514840	4542910	635	B	0.98	So you mentioned the OpenAI gym and the standardized format, and what benchmarks do you use or what kind of test suites are you comparing, and how do we really know when we've made a generative model that really exceeds or excels in a way that other techniques are just not doing?
4543520	4551276	636	A	0.94255	Yeah, so if I may go to the OpenAI gym website, we have several experiments.
4551308	4558768	637	A	0.99966	There the classical reinforcement learning examples like the lunar lander that you see in this screen right now.
4558854	4565680	638	A	1	So active inference from its inception has faced problems of scaling to tasks.
4565760	4571296	639	A	1	And that's in itself a field of research in active inference, scaling active inference.
4571328	4577256	640	A	1	And that's one of the reasons why deep active inference took over dealing with tasks like this.
4577358	4584564	641	A	1	So there are benchmarks even now where the sophisticated inference may not be able to deal with state spaces.
4584612	4586360	642	A	1	And personally, that's my research.
4586510	4587408	643	A	0.99999	In my PhD.
4587444	4598524	644	A	1	I am actually looking at optimizing computations in sophisticated inference algorithms that lets you scale up to environments like that.
4598642	4611344	645	A	1	But to get started, you will have to kind of write code and see if it works for an environment, then look at if it's not working, then you have to look at methods to scale it up and so on.
4611382	4620708	646	A	0.94	So if I am talking about benchmarks, sophisticated inference is as good as any RL algorithms for this state space.
4620874	4624932	647	A	1	So for small problems, sophisticated inference will work and it's really good.
4625066	4635492	648	A	1	But for high dimensional problems like this, the classical implementation that I just showed might not work, but it's good enough for any decent experiment.
4635556	4644284	649	A	1	But if you want to scale up, then that's still open and it's a new field of research and what you do might become a next new important paper.
4644402	4647596	650	A	1	So that's all I can tell in that regard.
4647778	4648268	651	A	0.80127	Cool.
4648354	4650544	652	A	1	You have to work and see what.
4650582	4663380	653	B	0.9998	Measures do you think you'd be looking for, like computational resources or what are the measures that even make sense to juxtapose such different methods?
4664040	4672544	654	A	0.60134	Yeah, so the OpenAI Gym was designed for that, to compare different algorithms.
4672672	4677188	655	A	0.97	So OpenAI Gym by definition is a collection of many environments.
4677284	4680308	656	A	1	So in my demo, I was talking about the grid environment.
4680484	4690890	657	A	0.17387	Openiigm is nothing but a collection of many environments which will let you interact with those environments using the environment step function.
4692620	4693224	658	A	0.76837	Yeah.
4693342	4698424	659	A	1	So here we have the environment step function that will let you interact with the lunar lander.
4698552	4706736	660	A	1	And that particular task will have matrixes that lets you judge how good or bad your algorithm is.
4706838	4716484	661	A	1	So in this lunar lander problem, how optimally can you land your rover between these two flags by spending minimizing the fuel and so on.
4716522	4723732	662	A	0.99	So those matrices are very tasks specific, and that's one direction you have to take.
4723786	4741564	663	A	1	You can take try and compete with RL algorithms in matrixes, but the right potential or the potential I see in sophisticated inference is modeling intelligent behavior, where in RL the focus is to get things done to make this work.
4741682	4746072	664	A	1	But it's not really explainable, especially deep RL and deep learning methods.
4746136	4755392	665	A	1	But in active inference, if you manage to scale it up, they are explainable and that will let you understand how intelligence emerges with time.
4755526	4767860	666	A	0.99	And I see that more interesting than competing with RL, because if your focus is getting things done, then maybe engineering is the right way and not active inference.
4769880	4770532	667	A	0.93665	Awesome.
4770666	4773300	668	B	0.99998	Any other comments or thoughts?
4782070	4784710	669	B	0.71361	Aswin, do you have any other comments or thoughts?
4785290	4787494	670	A	0.99941	No, I'm pretty happy.
4787612	4790214	671	A	1	I hope I was clear explaining the code.
4790252	4800246	672	A	0.99985	Maybe it was too complicated or simple, depending upon your level, but I hope it is useful to at least one person who would start using this and write the code.
4800428	4802022	673	A	0.9998	Thank you so much for your time.
4802156	4802550	674	A	0.96529	Awesome.
4802620	4803960	675	A	0.73	And thank you for the opportunity.
4805450	4806634	676	B	0.99866	Thank you for joining.
4806682	4807550	677	B	0.6327	Till next time.
4807620	4808400	678	B	0.99994	See you.
4809010	4810046	679	A	0.99657	Thank you so much.
4810148	4810410	680	A	0.9825	Bye.

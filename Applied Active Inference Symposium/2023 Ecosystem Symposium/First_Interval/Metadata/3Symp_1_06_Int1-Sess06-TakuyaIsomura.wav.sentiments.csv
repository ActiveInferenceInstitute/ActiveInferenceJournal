start	end	speaker	sentiment	confidence	text
1210	2000	A	0.4896697998046875	All right.
8080	9740	A	0.7365932464599609	Greetings, Takuya.
10640	11390	B	0.5325649380683899	Hi.
14000	20860	A	0.7323071956634521	You can mute the live stream or turn off the other live stream, but yeah, thank you for joining.
22080	26220	B	0.97087562084198	Yes, thank you for inviting thank you for it.
26290	28150	B	0.5547363758087158	Yeah, it.
30440	42760	A	0.9596810936927795	Well, we have a little bit 35 or 38 minutes, so it would be awesome to have your presentation.
45500	46250	B	0.7671424746513367	Great.
46620	59630	B	0.6071699857711792	So the moment well, can you see my.
61760	63420	A	0.7338313460350037	I saw PowerPoint.
66880	67630	B	0.584351658821106	Okay.
69840	71870	B	0.8383948802947998	Can you see my screen?
72320	73070	A	0.7672419548034668	Perfect.
74240	74990	B	0.584351658821106	Okay.
76480	78300	B	0.8704578876495361	So shall we start?
78450	78908	A	0.46103888750076294	Yes.
78994	79870	A	0.8529649376869202	Thank you.
81620	82368	B	0.550727128982544	Then.
82534	88476	B	0.9883362650871277	Thank you for organizing this wonderful symposium.
88588	107792	B	0.7422940731048584	Today I'd like to talk about relationship between canonical neural network and active inference and possible extension for modeling social and shared intelligence using canonical neural network.
107936	109670	B	0.6439459323883057	So let's start.
116250	132874	B	0.8598160743713379	As you know, the free energy principle is proposed by Karl Friston that states that perception, learning and action of all biological organisms are determined to minimize variational free energy as a tractable proxy for surprise minimization.
133002	143040	B	0.8919145464897156	And by this process organism can perform parational BJ inference or external mineral state.
143650	152494	B	0.5165812969207764	And this can't even just show one example typical set up under the free energy principle active inference.
152622	163030	B	0.7168523073196411	Here there is a hidden state in the external world and only a part of this state can be observable.
163370	187690	B	0.9001387357711792	For our agent, this doc and this transformation is done by genetic model parameterized by theta and to infer the hidden state, agent need to reconstruct the copy of the external state called posterior relief.
187770	193440	B	0.6835027933120728	And this optimization is done by minimizing variational free energy.
194450	204580	B	0.7590532302856445	And parameter is also optimized by minimizing free energy to obtain an apt generative model to represent this relationship.
205030	213042	B	0.8647361993789673	An interesting aspect of the free energy principle active inference is its application to the optimization of action.
213186	237150	B	0.8736971020698547	Here our agent have some preference prior c and to obtain this observation in the future, agent may select the action that minimize the expected free energy in the future to obtain the most predictable outcome.
237810	246206	B	0.8585207462310791	And finally, by selecting actions that minimize the expect frequency, it can obtain the feed.
246398	250206	B	0.7608489990234375	So, this is a typical setup under active inference.
250238	256258	B	0.8324025869369507	But question is what is the neural neurosurge that can implement this process?
256344	264306	B	0.821295976638794	So that is our interest and to address this issue we propose theory as follows.
264418	284874	B	0.8838886618614197	Here we consider that external world is parameterized characterized by a set of the variables like this here, this is a posterior expectation and the S indicates the hidden state in the external world.
284992	298046	B	0.836780309677124	Delta indicate the action of the agent or decision and the theta indicate a parameter and lambda is a hyperparameter.
298238	310546	B	0.8693985342979431	So those set of parameters or variables characterize a generative model and free energy variational.
310578	323930	B	0.8721493482589722	Free energy is defined as a function of the sequence of observation and external state and its minimization indicates the variational Bayesian inference.
325470	336778	B	0.8833863139152527	Similarly, we consider a dynamics of neural network and we consider that dynamics is characterized by a set of those variables.
336874	359266	B	0.906951367855072	Here x indicates the neuroactivity firing rate at middle raya neurons and y indicate neuroactivity of output raya neurons and W here is a synaptic weight and phi is any other free parameter that characterize neural network.
359458	367894	B	0.903353750705719	And we consider that neural network dynamics is characterized by minimization of some cost function l.
368012	385470	B	0.8712749481201172	This is a function of the O and Phi internal state of neural network and its minimization indicates the generation of neural network dynamics including both activity and plasticity.
385810	392378	B	0.7767881751060486	And our theory indicates equivalence between those two functions.
392554	403710	B	0.763981819152832	Meaning that for any neural network that minimize some cost function error, there is a generative model that satisfy if we call error.
403870	418418	B	0.8747554421424866	Which means that the recapturation of external dynamics is an inherent feature of any neural system that follow such dynamics.
418594	423910	B	0.5431608557701111	So this is an interesting prediction but too abstract.
424730	434430	B	0.7423320412635803	So I would like to introduce some analytically tractable example to understand this relationship formally.
435250	439626	B	0.7770624160766602	So first we consider a very simple architecture.
439818	445710	B	0.7447676658630371	This is represented by POMDP without any state transition.
445790	457958	B	0.7790403366088867	So hidden state s is simply generated by priya distribution D and it is a binary state.
458124	492542	B	0.8194535374641418	But we consider a vector of binary state, so it is a factory of structure and observation is also a binary vector and the transformation from S to O is characterized by categorical distribution using likelihood of matrix A and variational Bayesian inference indicates the inversion of this generative process like this.
492676	505620	B	0.6579654812812805	So by solving the appropriate frequency functional we obtain those posterior brief that is optimal in the Bayesian sense.
506890	522134	B	0.8568440079689026	On the other hand, for neural network we consider such structure here upper part indicates the generation of signal in the external world and the neural network comprises only single layer.
522262	534478	B	0.9065243005752563	Here activity of output layer are generated by weighted sum of sensory input O weighted by synaptic matrix W.
534644	540590	B	0.9102900624275208	And we then characterize this neural network in the next slide.
541010	564406	B	0.4864237308502197	So to model neural network or neuron, we start from considering hodgeken hacksray equation which comprises four differential equation and this is a very complicated non linear equation although it is barely possible.
564508	568818	B	0.8926141262054443	So we consider some reduction of these equations.
568994	598462	B	0.8385975360870361	So a typical reduction method is like this for example, m here is much faster than other variables so this can be repressed with its fixed point or it is known that h and minus one sorry, it is known that h and one minus SN have similar dynamics.
598526	612214	B	0.9027146100997925	So we can consider a new effective variable U to characterize those two variables and then we obtain 2D hoskin hacks ray equation like this.
612332	637998	B	0.7020681500434875	So this is a famous cross of neural network model which includes the well known Fitzfuel Nagamo model or other continuous neural network model and we modify those model to derive canonical neural model.
638164	643630	B	0.8490958213806152	So this is a definition of canonical neural network model.
643780	657102	B	0.8509779572486877	Here leak current is characterized by imbass sigmoid function instead of cubic function which is adopted in the Fitzfuel Nagmo model.
657256	689200	B	0.906740665435791	And we also consider connection synaptic connection and this part indicates synaptic input from sensory layer through weight matrices and 1 may consider that W one indicate excitatory synapse and W zero indicate inhibitory synapse and the threshold are adaptive thresholds that are function of W one and W zero.
689810	703380	B	0.7122476100921631	Interestingly, when we consider a fixed point of this differential equation we obtain well known red coding model with the Sigmoidal activation function.
705190	729850	B	0.8203588128089905	Which means that we can say that in some sense this canonical neural network network model is approximation of Hodgkin hatt's ray equation and its approximation level is, in some sense between the realistic model and the most simplified red coding model.
730000	745300	B	0.9248127341270447	So we basically consider this type of neural network model and in the next slide we consider what is the plausible cost function for this neural network model.
745670	782730	B	0.902536928653717	So again we write the same equation for canonical neural network model which represents the activity of neurons vector of neurons and we consider cost function for this differential equation which can be obtained by simply calculating the integral of right hand side of this equation and get this type of cost function for neural network.
782890	794350	B	0.76518714427948	This is biologically plausible cost function in the sense that its derivative derived neural network activity which has a certain biological plausibility.
794870	812760	B	0.9154329895973206	Moreover, if we consider derivative of this cost function with respect to synaptic weight W, we obtain a conventional synaptic plasticity rule which follows Hebian law.
814410	836030	B	0.8957512974739075	On the other hand, for Bayesian inference we first define generality model like the previous slide and we then derive variational free energy for given genetic model.
836100	852340	B	0.9054951667785645	So this variation of free energy is derived from the Pom DP model in the previous slide and its minimization indicates the Beijing inference and running.
852870	866262	B	0.9022368788719177	So we found formal correspondences between component of those two cost functions like this.
866396	895662	B	0.8848913311958313	So this broke vector formally correspond to this Brock vector that represent a posterior expectation and this logarithm correspond to this logarithm and actually this a matrix can be represented as the broke matrix like this and its dot product correspond to this computation.
895806	905730	B	0.8752419948577881	And finally, this phi naturally correspond to this log d logo state prior.
906310	927080	B	0.6753466725349426	Which means that because the cost function are same, its derivative provides its derivative provides sorry.
927790	936640	B	0.682974636554718	Because the cost function are formally equivalent, its derivative sorry.
937010	943440	B	0.8321841955184937	It's a result of derivative also corresponding each other.
944550	954590	B	0.9015665650367737	Which means that for any neural activity equation in this form, there is a corresponding Bayesian inference equation.
954670	969618	B	0.9072101712226868	This is an equation that compute the posterior belief of the hidden state and this synaptic practice equation formally correspond to learning or parameter of genetic model.
969804	1000210	B	0.8568035364151001	And moreover, by establishing this relationship we can consider the reverse engineering of the generality model from empirical data here this schematic summarize our approach to reverse engineer generative model and we first record the neural activity and assign the canonical neural network to explain this obtained data.
1000360	1006650	B	0.8794693946838379	And by computing the integral we obtained a cost function for this canonical network.
1006830	1022054	B	0.7530145645141602	And by the mathematical equivalence we established, we can automatically identify genetic model and variational free energy that correspond to this neural network architecture.
1022182	1032458	B	0.7404395937919617	So interestingly, this is a Bayesian agent which is a kind of artificial intelligence.
1032554	1039198	B	0.7679734826087952	But importantly, this artificial intelligence is formally derived from empirical data.
1039284	1074780	B	0.8556535243988037	So we can say that this agent is biomimetic artificial intelligence that follow the free energy principle and then its derivative with respect to parameter posterior derived synaptic plastic algorithms that follow the free energy minimization and its time integral can predict the running process of original neural network data.
1075390	1096866	B	0.5023409128189087	Which means that if the frequency principle is correct, then this prediction should work, should work and should be able to predict the result of this neural neta without referencing to the data itself.
1097048	1143002	B	0.8795253038406372	So our strategy is in summary, our strategy is that we reconstruct generative model only from the initial data, like initial data before running and then predict the running process or running curve that this neural system should follow by using the frequency principle and compare or examine whether this prediction is correct by comparing actual data after running and the prediction by the free energy principle.
1143146	1152850	B	0.6567350625991821	And if this prediction is correct, then it indicates the predictive validity of the free energy principle under setup considered.
1154230	1159330	B	0.8786440491676331	So we apply this strategy to the in vitro neural network.
1159410	1170142	B	0.9266384840011597	So here, this in vitro neural network are stimulated using the POMDP generative process defined in the previous slide.
1170226	1184890	B	0.8236085772514343	So there are two hidden sources that are binary signals and they are mixed to generate 32 sensory stimuli.
1184970	1187630	B	0.6363953351974487	Those are also binary.
1188450	1205726	B	0.8274991512298584	And this is an overview of experiment by stimulating in vitro neurons they generate spiking response and those rhyme indicate a high density spiking response.
1205918	1234186	B	0.8264501094818115	So we observed that in some neuron the response specificity is so we found that for some neuron response was high to source one signal compared to source two signal.
1234378	1277866	B	0.8156988024711609	And if we see the transition of those neurons, we found that although we removed the offset at the first session to set those activities zero, but we see that those neuron self organized to respond high when source one is one, but those neurons response low level when the source one is zero.
1278048	1297714	B	0.7664511203765869	So which means that those neurons activity, those neurons response was consistent with our theoretical prediction that neuroactivity cell organized to encode the posterior expectation of healing state.
1297832	1306740	B	0.8600553274154663	And we also found that some other group of neuron responds preferentially to source two but not to source one.
1308090	1345540	B	0.8934553861618042	So then we ask okay, then we found that posterior expectation is encoded by neuroactivity and the next question is about other neuronal substrate and we then ask if the prior brief about hidden state is effectively equal to the firing threshold of neural activity model, neural network model.
1346230	1351810	B	0.7752359509468079	So if the theory is correct, this correspondence should exist.
1352390	1370838	B	0.7797365188598633	To check this, we first simulated a Bayern agent and when we varied the PRI of the Bayesian agent, the inference was attenuated as I expected.
1371014	1382714	B	0.8276477456092834	And we found that when we buried the excitatory level of in vitro neural network by using the pharmacological manipulation.
1382842	1403220	B	0.7825686931610107	We also found that the attenuation of inference which is consistent with our theoretical prediction that firing threshold encode prior belief or the hidden state.
1404710	1430700	B	0.8974912762641907	And next we consider whether the scientific plasticity followed the free energy principle by asking whether free energy principle can predict the qualitative self organization of a subsequent neural data.
1432130	1443022	B	0.8647153377532959	So here we model neural network like this, there are two ensemble neurons that encode source one and source two.
1443156	1469160	B	0.9225054979324341	And we first compute the effective synaptic weight of those networks using a conventional connection strength estimation approach and plot those synaptic weights on the landscape of theoretically computed free energy.
1471690	1493040	B	0.8550365567207336	So this is a trajectory of empirically estimated synaptic weights effective sign optic connectivity and as predicted, those changes reduce the free energy.
1494710	1506638	B	0.8451546430587769	And here we computed this theoretically predicted free energy landscape only using the past ten sessions data.
1506824	1518550	B	0.9083371758460999	So that this indicates that indicates some prediction of the self organization.
1519310	1530806	B	0.8569596409797668	And for more explicit prediction, we then simulated neuroactivity and plasticity using green as a principle.
1530998	1544138	B	0.8088247179985046	Here the brighter color indicates the prediction of data without reference to activity data.
1544324	1550850	B	0.8747862577438354	So those lines exactly follow this free energy gradient.
1551750	1567990	B	0.7670286893844604	And we found that this predicted trajectory is tightly correlated with this empirically estimated effective synaptic weights.
1568490	1572300	B	0.5718077421188354	And LR rate is like this.
1572670	1590350	B	0.7192309498786926	So it indicates that frequency principle can quantitatively predict the circumstance in this setup, so it indicates some predictive validity of the free energy principle under this setup.
1590690	1600542	B	0.8903440833091736	And then we also consider the modeling of the neuromodulation using active inference.
1600606	1612534	B	0.8366261124610901	It is well known that synaptic process is modified by various factors like Dopamine, northern adrenaline, acetylcholine, serotonin, so on.
1612732	1644950	B	0.7585286498069763	And one interesting property of those moderation is that even though Dopamine was added after associative plasticity was established, it can change the result of plasticity in a post hoc or restorespective manner, so it implies some association to the reward and past decisions.
1645050	1649198	B	0.88336181640625	So we model this process using canonical neural network.
1649374	1671414	B	0.9204846620559692	So here we model the post hoc modulation of HEBM plasticity using this type of plasticity equation and we also consider the recurrent neural network structure and output layer for this network.
1671542	1681778	B	0.8936604261398315	And we consider that modulation occur at this connectivity layer.
1681974	1713560	B	0.82728511095047	And then we found cost function that can derive those differential equations and then found corresponding variational free energy and genetic model which means that this sort of neural network activity including moderation of sinus plasticity exactly follow the free energy principle under some type of homodp generative model.
1714090	1733610	B	0.6587386727333069	And by using this, we show that this biologically plausible neural network model with moderation of heavy and plasticity can solve some sort of delayed reward task like maze task.
1734530	1743770	B	0.7837294936180115	And then finally we would like to discuss a possible extension to this framework to the modeling social intelligence.
1743930	1760242	B	0.8853868842124939	So, to infer our con specifics, we need to select an appropriate generality model for our partners, depending on our partners.
1760306	1782970	B	0.6804929375648499	So this can be done by Beijing model selection scheme and we previously proposed a model that can predict the multiple biosomes using one big genetic model that comprises multiple genetic models.
1783130	1794378	B	0.8109886050224304	And this movie shows prediction of prediction of songs.
1794554	1808338	B	0.6525439620018005	So like this, this model nicely identified which genetic model is the best to explain a given sensory input.
1808434	1824730	B	0.6861579418182373	And this process indicates and the model can correctly infer the appropriate model and then imitate the song by its own action.
1825550	1844326	B	0.8166481852531433	So, although in the previous work we didn't discuss a detailed neural neural substrate for this mixture generative model, we now be able to consider the corresponding circuit architecture.
1844458	1857650	B	0.8889420032501221	For example, if we consider the moderation of neural module by neuromodulator like Dopamine, it act as an attentional filter.
1857810	1870118	B	0.815501868724823	And this attentional filter can be explained by three factor Hebian running rule introduced in the previous slide.
1870214	1882450	B	0.8348632454872131	So again, this modulation works as the post hoc moderation plasticity.
1882550	1897358	B	0.7141045928001404	And this modulation can optimize each model to represent one generative model, one generate one song in a mutually independent manner.
1897454	1906990	B	0.7001765370368958	So, through this process, it is possible to learn multiple generative model in a vertically plausible manner.
1907150	1919894	B	0.7880404591560364	So, in summary, we found that the dynamics of kinetic neural network that minimize a cost function can be read as minimization of variational free energy.
1920012	1927590	B	0.5793163180351257	It indicates free energy principle is an apt explanation for this type of neural network.
1927750	1950740	B	0.5865697860717773	And we also validate this prediction using some in vitro setup by showing that free energy principle can quantitatively predict the self organization of subsequent plasticity only using the initial data.
1951590	1968470	B	0.8459897041320801	And as the modeling, we can extend those canonical network modeling to the action generation as a planning via the delayed moderation of heavy emperor's DST.
1969130	1987610	B	0.7357572317123413	And finally, we discuss possibility to extend this canonical model to modeling the social or shared intelligence ah to interact with multiple partners.
1987770	1990510	B	0.7745591998100281	So, that's all of my talk.
1990580	1992510	B	0.9618316292762756	Thank you for listening.
1992930	1994474	B	0.5891779065132141	This is Acknowledgment.
1994602	2002814	B	0.8232986330986023	Our collaborator and fundings and our unit are now recruiting postdoc researchers.
2002862	2005220	B	0.7092253565788269	So if you're interested, please check.
2008120	2008916	A	0.9184247851371765	Awesome.
2009098	2011540	A	0.975576639175415	Thank you for the presentation, Takuya.
2013160	2018004	A	0.8847981691360474	I'll just ask a few quick questions from the live chat and a few other things that come up.
2018042	2025400	A	0.8239175081253052	So Dave says Takuya used the phrase after plasticity was established.
2026220	2030948	A	0.8398151397705078	Was his group able to modify e g increase plasticity?
2031044	2035100	A	0.5588241219520569	Or is he saying merely that it was shown that there is plasticity?
2039600	2066656	B	0.6551424860954285	Yeah, I'm not sure if I understand correctly your question, but those groups show that Dopamine adding Dopamine after 2 seconds after the association occurred can change the magnitude of plasticity.
2066848	2082120	B	0.6306747198104858	So without how to say if Dopamine addition was before this association, then plasticity level is low.
2082270	2100800	B	0.8568180799484253	But after Dopamine addition after association can change this level can increase the plasticity like this or like this which indicates the post hook moderation.
2103140	2103648	B	0.84200119972229	Cool.
2103734	2105650	A	0.6061187982559204	I think that answers it.
2107460	2111140	A	0.5437098741531372	Well, what are you excited for?
2111210	2122010	A	0.9217638969421387	Or what are your hopes or feelings on where the active inference ecosystem is at and where we're headed in the coming months and years?
2126790	2132770	B	0.6157411336898804	Sorry again chris, just what are you excited about?
2132840	2135314	A	0.7202929258346558	Other than your own research directions?
2135362	2138870	A	0.5644592046737671	What are you excited about in the active inference ecosystem?
2140410	2140918	B	0.5491447448730469	Yeah.
2141004	2159802	B	0.614137589931488	So, of course, one direction is the modeling of social interaction, which is much rich architecture than the interaction between the static environment.
2159946	2167322	B	0.8299610614776611	So if both agents run with each other, then many interesting phenomena can be observed.
2167386	2181970	B	0.9529569149017334	So we are excited with modeling those phenomena using biologically plausible neural network model through this equivalence.
2184490	2185240	A	0.9184247851371765	Awesome.
2185930	2187910	A	0.8024787306785583	Any last comments?
2193130	2194886	A	0.8803504109382629	Any other comments that you want to make?
2194908	2195910	A	0.6083254814147949	Takuya.
2203600	2204350	A	0.9184247851371765	Awesome.
2205600	2207580	A	0.9770573377609253	Thanks again for the presentation.
2207920	2216736	A	0.5170981287956238	And people should check out Live Stream 51, where you and I talked a few other times and went into some of the details on that work.
2216918	2217744	A	0.7669020891189575	There's a lot there.
2217782	2218880	A	0.9853336811065674	It's really exciting.
2221620	2222610	B	0.8529649376869202	Thank you.
2223460	2225136	A	0.8092430830001831	All right, thank you.
2225318	2226690	A	0.6020334959030151	See you next time.
2227560	2228852	B	0.6020334959030151	See you next time.
2228986	2229780	A	0.5137446522712708	Bye.
2259250	2266430	A	0.7646216154098511	All right, the next session is going to be with Shannon Dobson.
2267090	2282598	A	0.9375799298286438	This section is going to be called Dark Imaginarium Shared Intelligence as an Infinity Curiosity type message.
2282684	2285750	A	0.6520479321479797	Shannon, make sure that everything's good with the audio.
2290330	2291080	A	0.7093686461448669	Wow.
2294490	2297380	A	0.9577347636222839	Great talk so far.

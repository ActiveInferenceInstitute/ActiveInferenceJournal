start	end	speaker	sentiment	confidence	text
330	880	A	0.546256959438324	You.
1970	7006	B	0.8122658133506775	And now welcome aswin how are you doing?
7188	7914	A	0.6762325167655945	Hi, Daniel.
7962	8720	A	0.9117157459259033	I'm good.
9810	12766	B	0.806130588054657	Hanging out, looking forward.
12788	13946	A	0.7189454436302185	Can you hear me properly?
14058	15502	B	0.9498600363731384	Yeah, sounds good.
15556	22090	B	0.9546405673027039	And yes, looking forward to your workshop on sophisticated inference in pymdp.
22250	34200	B	0.6034063100814819	So it can be up to 90 minutes or it's totally okay if it's less and we can take a short break, but please take it away and just let me know however I can help.
34570	35366	A	0.7671424746513367	Great.
35548	36760	A	0.9793016910552979	Thank you so much.
37710	39980	A	0.8362265825271606	Maybe I'll share my screen and start.
40510	52734	A	0.8740900158882141	So, regarding the structure of what we are doing here, I'm assuming that people who see this later might want to try it hands on along with the tutorial or something.
52772	54800	A	0.8689452409744263	So that's the structure I kept in mind.
55170	57178	A	0.6453473567962646	And so I'll go slow.
57354	58894	A	0.7575491070747375	Please bear with me.
59092	63854	A	0.9717873334884644	So welcome all of you to this session on sophisticated inference in pymdp.
63902	76286	A	0.8408743739128113	So here we are going to attempt to model some sophisticated inference simulations, especially the one in the original paper using the pymdp module and how it is not part of pymdp right now.
76328	81010	A	0.8402337431907654	But we are on the process of adding sophisticated inference to the pymtp module.
81170	88662	A	0.7417822480201721	And I'm going to mainly talk about the code that I have developed to kind of add that functionality.
88726	88954	A	0.5664746165275574	Right.
88992	90854	A	0.8874531388282776	So I'm Ashwin Paul.
90902	95302	A	0.6749657392501831	I am finally a PhD candidate at Monash University.
95446	108218	A	0.7434468269348145	And mostly I work with active inference models and try to understand how to use them as an explainable model to basically understand emergence of intelligent behavior.
108314	113374	A	0.7055745720863342	Right, so let's dive into the material right away.
113572	120626	A	0.756130039691925	So to give an intro of free energy principle, I'm sure all of you right now have an understanding of what it is.
120728	128534	A	0.7135979533195496	But the central idea is that an agent is always trying to minimize the entropy of its observations, right?
128572	138410	A	0.6267848014831543	So if an observation is having really low probability in your mind and that happens, then you are probably surprised and vice versa, right?
138480	150598	A	0.6964723467826843	And the entropy here is defined as the information theoretic entropy, where if you have a low probability, then automatically this is a high surprise or a high entropic observation.
150774	158986	A	0.8380339741706848	And as we all know, active inference also gives us a methodology to define what we call an agent environment loop.
159098	170210	A	0.8799266219139099	And this lets us define what is the agent that we are looking at and what is the behavior of the agent given the environment around it and so on.
170280	170562	A	0.7360090017318726	Right?
170616	173614	A	0.8742364048957825	So you're also familiar with the idea of Marco Blanket.
173662	191414	A	0.6345372796058655	And this is important because we always have to remember, I mean, have to remember the difference between the generative process and the generative model, which is quite a famous point of confusion in active inference literature and for the people who try to understand it in the beginning.
191542	192220	A	0.7360090017318726	Right?
192590	196582	A	0.8013162612915039	So the central question is that how does an agent minimize entropy?
196646	201994	A	0.6958804726600647	Because how does an agent know which observation is low or high probabilistic?
202042	202494	A	0.7360090017318726	Right?
202612	204798	A	0.8145608305931091	So that is by maintaining a generative model.
204884	212458	A	0.806228518486023	And the generative model will tell you which is a high probabilistic observation and which is a low probabilistic observation.
212554	221982	A	0.7766016125679016	So the idea is that all the agent has access to is an observation that is coming from a generative process which the agent cannot directly observe.
222126	233094	A	0.7413041591644287	And an intelligent agent will try to build up a generative model in its mind, which is a model of the hidden states and the observation it has access to.
233212	238246	A	0.7689242362976074	And it can hope to kind of compute probabilities using this generative model, right?
238268	246858	A	0.6666292548179626	But there is a problem that in general it is an intractable problem to kind of marginalize the probability of observations from the generative model.
246944	252774	A	0.79206383228302	And that is why we have to define an upper bound on the surprise that the agent is trying to minimize.
252822	255054	A	0.8008697628974915	And there comes the idea of free energy, right?
255092	259870	A	0.8737470507621765	So this upper bound the agent is supposedly minimizing is the free energy.
259940	262154	A	0.6501067280769348	And that's why it is the free energy principle.
262282	272386	A	0.8887730240821838	And here we have a new term called capital QS, which can be interpreted as the belief that the agent is maintaining about the hidden state in its generative model.
272568	274930	A	0.7925347685813904	And this quantity is the free energy.
275000	286390	A	0.7892997860908508	And traditionally we see this variational free energy being interpreted as in mostly the machine learning way where it is a balance between complexity and accuracy of the model.
286540	296074	A	0.7334993481636047	So when minimizing the free energy, the agent is trying to come up with a single model but at the same time an accurate model because here it's a minus sign with accuracy, right?
296112	313962	A	0.8383772969245911	It can also be interpreted in the physics way where the agent is always trying to minimize the energy of the model but at the same time maximizing the entropy of the model which goes in conjunction with the maximum entropy principle and so on from the classic literature.
314106	332486	A	0.6834945678710938	So, given that this idea of a generative model is so important in a software point of view, that's the first thing that you might want to do, right, to define a generative model for the agent which is informed or not informed depending upon the experiment that you're trying to model.
332588	338342	A	0.8808046579360962	So in classical active inference, usually decision making is defined in terms of policies.
338406	347654	A	0.8123975992202759	So, for example, if you are an agent in this environment so in the Mario game, Mario is the agent and everything else is the environment.
347782	355994	A	0.7847238779067993	And Mario has three available actions run, jump or stay in this environment.
356042	361454	A	0.8283615112304688	And the classic definition of policy is that it is a sequence of actions in time.
361572	368194	A	0.6950111389160156	So if you have a time horizon of capital T, then a policy is nothing but a series of actions you might take.
368232	371490	A	0.7419177889823914	So run, run, jump and so on in time.
371560	375278	A	0.8628978729248047	So this is the SuperScript is the action.
375454	377814	A	0.7914459705352783	So here it is jump, run and so on.
377852	379734	A	0.6836281418800354	And the subscript is the time.
379932	388598	A	0.8162171244621277	And then what you can have is a policy space, which is a collection of many such policies, small pi.
388694	395142	A	0.5887215733528137	And what you essentially do to take decisions in active inference is compute, not optimize.
395206	400830	A	0.7967257499694824	Compute the expected free energy of every policy in your policy space.
400980	407150	A	0.8560143709182739	And basically, that can be interpreted as a balance between risk and ambiguity.
410770	417570	A	0.7840136289596558	So when you compute this expected free energy, what you're trying to do is minimizing the risk.
417990	424942	A	0.8664847612380981	That is, how different is your belief about the observation from your prior preferences?
425086	426066	A	0.6601583957672119	Capital C.
426168	437302	A	0.8230603337287903	So this is also part of the generative model when you're trying to model control and at the same time you are trying to minimize the ambiguity when you are choosing a policy that has the minimum expected free energy, right?
437356	455920	A	0.6777933239936829	But this formulation has a problem that a policy space quickly becomes intractable, that there can be an enormous number of small pies or policies in your policy space and sitting and computing the expected free energy for all such policies even for a small time horizon, is not possible.
456370	472366	A	0.8964852690696716	But this is the classical structure that has been implemented in Pymdp nonetheless, where we have different modules in Pymdp that is meant to be implementing different aspects of behavior.
472478	485394	A	0.8880211114883423	So for example, for inference or perception we have belief propagation, fixed point iteration, marginal message passing and all that implemented in the inference module.
485522	501542	A	0.9033867120742798	In the control module we have different methods to evaluate expected free energy for policies, one depending upon the expected utility, the other one depending upon the classic method that I just explained.
501686	503790	A	0.8035035133361816	Then we have module for learning.
503860	510734	A	0.9013999104499817	So we learn the parameters in the form DP like capital A, capital B, so likelihood transition dynamics and so on.
510772	517534	A	0.8492960333824158	And then we have algorithms for implementing all this in the algorithms module.
517662	530406	A	0.9289743900299072	And then the most powerful thing in Pymdp right now is the agent class where it is easy for you to kind of define the agent environment loop and we are trying to build up.
530508	539500	A	0.847241222858429	So today I'm going to talk about an agent class that implements sophisticated inference rather than the classical active inference that we just saw.
540110	548374	A	0.8518918752670288	So, as I mentioned, how many valid policies can be defined, say for a time horizon of 15 in classical active inference?
548422	549018	A	0.7360090017318726	Right?
549184	562362	A	0.8253123164176941	So the first policy of course, is a series of first action that is jumped, then you can have the last action changed and you already can see that there can be n number of combinations.
562426	567726	A	0.7868592143058777	And for this simple case, the policy space is as big as ten to the power 13.
567838	578470	A	0.5134372711181641	And in a stochastic problem setting there is no way to kind of come up with a small subset of this policy space so that you can tackle this problem of computational complexity.
578970	584600	A	0.594726026058197	And yeah, as I mentioned, in a stochastic problem setting it is an intractable size policy space.
585050	592794	A	0.8549959659576416	There comes the idea of sophisticated inference where we are thinking about taking decisions in a different way, right?
592832	603214	A	0.8551225066184998	So rather than thinking about sequence of actions in time, we can directly think of what to do when we see something depending upon our beliefs about the current state and beliefs about the future, right?
603252	606334	A	0.8145303726196289	So if I see myself in a current situation, what should I do?
606452	611802	A	0.8091327548027039	And that's more like a straightforward thinking of how to take actions.
611946	614322	A	0.6921086311340332	And here the expected free energy.
614456	626434	A	0.8212184906005859	The structure of the expected free energy is the same but we are not evaluating expected free energy of policies, but expected free energy of observation action combinations.
626562	630070	A	0.83890300989151	So if I see something and if I do this, what's the expected free energy?
630140	632338	A	0.7445014715194702	And that's what I'm trying to minimize.
632514	638374	A	0.8170201182365417	That's what I'm trying to optimize in this setting, right?
638412	645622	A	0.7428028583526611	So here again, we have the risk term where we are trying to minimize the deviation between the belief and the prior preferences.
645766	654542	A	0.8497303128242493	We also have the ambiguity term and this together makes up the expected free energy of this time point at time T.
654676	659662	A	0.8656213283538818	But we also have an expectation about what's the expected free energy in the next time step.
659716	667442	A	0.8879376649856567	And to evaluate the expected free energy of next time step, you will have to again compute this equation with OT plus two.
667576	672514	A	0.8329636454582214	And for that you will have to again compute this equation with OT plus three and so on.
672552	687270	A	0.5865595936775208	And this automatically becomes a tree search because of the recursive way this equation is defined and it comes with its own problems, but there are clever ways to get around them and we are going to discuss that in the code today.
687420	698410	A	0.8858192563056946	So, given the structure of sophisticated inference here, as I mentioned, the research replace the policy space that we saw for the traditional active inference we are used to.
698560	709786	A	0.810709536075592	So in this workshop, what I am focusing on is how to kind of define a generative model and given an environment.
709898	725670	A	0.7537280917167664	So for example, this is the grid that is simulated in the original paper and we are going to talk about how to build up a generative model for this grid that can be used in the sophisticated inference in the PMDP module.
727370	745622	A	0.9210651516914368	So basically, what I'm trying to talk about is that the environment will have a step function that takes an action from the agent in Pymdp, and the agent will get an observation out of that action and we'll talk about this particular function, agent step and agent steps.
745766	750414	A	0.8594465255737305	Step will take up an observation and try to come up with an action for the next time.
750452	750894	A	0.6146336197853088	Step right.
750932	762298	A	0.6306355595588684	And this creates a loop and cleverly designing this loop will let you see emergence of purposeful behavior in the sophisticated inference setting.
762394	770354	A	0.7655384540557861	So for example, in this particular grid with sufficient planning horizon, you will be able to see that the agent is capable of navigating in this grid and so on.
770392	775860	A	0.5426909327507019	So this is the example that I'm going to focus on in this talk today.
779370	788510	A	0.6660231947898865	Excuse me, I want to right away jump in to the code.
789600	797264	A	0.5786247849464417	So we have the Pymdp home, which I'm hoping you are familiar with.
797382	809284	A	0.8414694666862488	So we have this GitHub repository where we have the pymdp module and inside pymdp module we have several parts for it.
809322	815980	A	0.8962814211845398	So here we have the agent in the original Pymdp module that implements the so called classical active inference.
816080	822888	A	0.8019351363182068	We have several environments and we have help of functions like learning inference, maths and so on.
822974	825764	A	0.873950719833374	So this is the module of Pimedp.
825892	838444	A	0.8685365915298462	But in the parent folder we also have examples where there is tutorials about how to use the agent class, how to kind of deal with the environments and so on.
838482	848156	A	0.9007370471954346	So if you look at the pull requests so we are right now trying to merge sophisticated inference into the original Pymdp module.
848268	851132	A	0.9032531976699829	And today I'm going to talk about the code in this pull request.
851196	864800	A	0.892817497253418	So if you want to try this hands on, you might want to go to this page where this pull request is there and it has the same structure of Pymdp.
864880	867748	A	0.803879976272583	It's basically designed using Pymdp.
867844	879752	A	0.8689553141593933	And here what we have additionally is an agent si which is a sophisticated inference agent which does everything and also planning and decision making in the sophisticated inference way.
879886	885692	A	0.8533372282981873	And in the parent folder there is also an example folder for sophisticated inference demo.
885826	891540	A	0.8083291053771973	And what I am going to do today is to walk you through this tutorial of sophisticated inference.
891640	909136	A	0.8742061853408813	And on the way, I'm going to discuss and at points where I reference the helper codes, I'm going to go to that code and try to explain what's actually happening and how we complete that agent environment loop where we can see that purposeful behavior.
909248	909910	A	0.7360090017318726	Right?
911800	912164	A	0.5491447448730469	Yeah.
912202	914116	A	0.860278844833374	So that's the Pymdp home.
914218	917040	A	0.9056982398033142	Then I also talked about the pull request.
917120	920500	A	0.8446199893951416	So let's right away go into the jupyter notebook.
920580	928440	A	0.5905554890632629	So this is my local copy of this repository, so it's easier for me to run it and show it in my personal computer.
928590	938508	A	0.8713755011558533	So this is the parent folder with Pymdp and examples and inside examples I have a demo folder for sophisticated inference and this is the notebook I'm talking about.
938594	938844	A	0.7360090017318726	Right?
938882	961236	A	0.8618931770324707	So here in this example, what we are trying to do is deal with this particular grid world task from the original sophisticated inference paper and make this agent or enable this agent to navigate to this red dot, which is supposedly the goal state in this particular task given a prior preference like this, right?
961258	975112	A	0.7813787460327148	So this prior preference is quite informative in the sense that we right away can see that this is the most preferred state, the white color and the surrounding states are kind of less preferred, but more preferred than the faraway ones.
975166	975384	A	0.7360090017318726	Right?
975422	978796	A	0.8629792332649231	So this is the grid world task that we are trying to use.
978978	989896	A	0.582476019859314	So the first cell is importing all the necessary libraries and some useful libraries like NumPy and Matplotlib.
990088	992412	A	0.7921684384346008	And the most important one is Pymdp.
992476	1002112	A	0.7004504203796387	So I'm actually now calling the local copy of my Pymdp with the sophisticated inference implementation, not the original one, which is not merged yet.
1002246	1006292	A	0.743177056312561	And the first thing I want to talk about is the environment itself, right?
1006346	1012388	A	0.8706367611885071	So the environment step part where if I get some action, how does the environment work?
1012554	1024104	A	0.7978835701942444	And for that, inside this folder I have a file which is Grid environment Si PY and this is basically an environment class.
1024222	1028096	A	0.6781187653541565	So don't worry about how this environment is actually implemented.
1028228	1037660	A	0.5136287808418274	The only thing to worry about is this function that we are going to use which is environment step.
1037810	1050000	A	0.908758282661438	So this function will take an action into it and depending upon the current state of the environment, it will calculate what is the most probable next state given this action from the agent.
1050150	1051392	A	0.6967025399208069	So that's the idea.
1051526	1063652	A	0.8526800870895386	And then it will also calculate a reward of some negligible negative value if it is not the Gold State, and if it is the Gold State, it will give a reward of ten.
1063706	1065124	A	0.565912127494812	And that's how the environment is.
1065162	1068900	A	0.902165949344635	Designed and it will update the current state to the new state.
1068970	1078148	A	0.8833816647529602	And basically what it will return is a new state, depending upon your action, the reward for that action and whether it is an end of the episode and so on.
1078174	1085996	A	0.851702868938446	So this implementation is the standard OpenAI environment implementation and this is the environment step function, right?
1086018	1097724	A	0.9194169044494629	So in this grid, for example, if I am right now in this state and if I take an action up, so I have four available actions north, South, east and west.
1097772	1105252	A	0.850009024143219	So if I go on north, then the environment step will make sure that I am in the state that is above the state.
1105306	1107972	A	0.8593823313713074	And if I go east or west, then I'll stay here.
1108026	1108544	A	0.7264188528060913	Or south.
1108592	1109332	A	0.7298474907875061	I'll stay here.
1109386	1111270	A	0.6967025399208069	So that's the idea.
1112440	1126916	A	0.7271966934204102	And there is an episode length limit that is eight here, that means that I am restricting the length of every episode to be eight, which is the ideal length of reaching this goal state just to avoid confusion.
1127028	1131208	A	0.5370615124702454	So in this environment, after eight actions, the environment will terminate.
1131384	1139500	A	0.8329627513885498	And if you have to kind of reach this goal state in the optimal time point so that's the idea how the environment is implemented.
1140160	1143120	A	0.6162233948707581	Okay, I hope that is clear.
1143190	1153756	A	0.8484894633293152	And there are many helpful functions in this environment, like rendering the environment, rendering a prior preference matrix in this environment.
1153788	1163110	A	0.8783542513847351	So if you design a prior preference, this environment can show that in a pictorial way how your prior preference is that you will see in the notebook below.
1163960	1169610	A	0.8315830230712891	So now it's time that we define a generative model for the sophisticated inference agent, right?
1171180	1183164	A	0.8762599229812622	Before that, let's define the structure of the generative model that we want the agent to have in its mind, which is tailor made for this particular environment, right?
1183282	1191648	A	0.5020496249198914	So here in this particular grid world task, we have 25 valid states starting from this state.
1191734	1196352	A	0.548147439956665	All this black states in this path are valid states.
1196406	1198384	A	0.49611416459083557	So there are 25 valid states.
1198582	1203756	A	0.9170013070106506	Then there are four available actions for the agent, north, Southeast and west.
1203868	1205940	A	0.8104245662689209	So this is part of our generative model.
1206090	1211328	A	0.5539944171905518	This is also in alignment with the reality of the grid.
1211504	1214772	A	0.7513941526412964	But this is about what the agent has in its mind, right?
1214906	1219210	A	0.7230443358421326	And then the observation is just the state space.
1220780	1224490	A	0.7057780623435974	The problem is fully observable, so there is no ambiguity there.
1224860	1228330	A	0.8152483701705933	Then we define basically the number of states.
1234170	1245466	A	0.8374188542366028	Then we define basically the number of states, which is a list of your state space, number of factors, which is now one because you only have one hidden state factor here.
1245568	1248620	A	0.8651852607727051	Then number of controls, which is going to be four.
1249150	1253562	A	0.8633998036384583	That's four available actions and your observation space like that.
1253616	1256062	A	0.8756700158119202	So this is the structure of your generative model.
1256196	1262378	A	0.9033966660499573	And let's look at the structure of the parameters now inside a POMDP.
1262474	1270846	A	0.9002265334129333	So the first one is the likelihood function which is often denoted by capital A.
1270948	1276594	A	0.843589723110199	And here it is a function of how many observational modalities I have and how many state modalities I have.
1276632	1277122	A	0.7360090017318726	Right?
1277256	1284840	A	0.7861483693122864	If I run this cell, yeah, I have to run the parent cell to make sure everything works.
1285450	1290246	A	0.8837990760803223	So I have rendered the environment the structure of the generative model.
1290348	1295802	A	0.9000873565673828	And here I have the capital A matrix which has a structure 25 25.
1295856	1298870	A	0.8679887056350708	That means that I have 25 states and 25 observations.
1299030	1305790	A	0.8319653868675232	And here, because it's fully observable, I am initializing it as an identity matrix of size 25.
1305860	1313978	A	0.9133585095405579	So that's my likelihood matrix that I'm initializing for this particular grid world task.
1314154	1317630	A	0.8201972842216492	Then the second element is the transition matrix.
1318130	1329650	A	0.9096094965934753	So please note that I'm using all the existing Pymdp functionalities to define a random A matrix and then using an identity matrix on top of that.
1329800	1332914	A	0.649739146232605	So, yeah, I'm not doing anything new.
1332952	1336690	A	0.8683273196220398	Here it is the existing Pymdp functionality.
1336850	1342786	A	0.9170975685119629	Then what I can do now is define the B matrix, which is also called transition matrix.
1342898	1351334	A	0.9033348560333252	So the transition matrix encodes transitions like where I'm going to end up in the future if I start from a particular state and take an action.
1351462	1358822	A	0.8454935550689697	So that's the idea where it depends upon the number of states, which is the hidden state modality and number of controls.
1358886	1365806	A	0.8802385330200195	So it has the structure of state action state where if I take an action from a particular state where I'm going to end up.
1365828	1368078	A	0.8273007869720459	So that is also a future state, right.
1368244	1374082	A	0.8240495920181274	So I'm going to initialize it as the true environment state.
1374136	1376482	A	0.6520035862922668	So now this is part of the environment that I've built.
1376536	1378530	A	0.8235820531845093	It will give out the B matrix.
1379190	1382950	A	0.7879815697669983	It might be worth it to look at the structure of this B matrix.
1385370	1388390	A	0.8492634296417236	So here we have 25, 25 four.
1388460	1400602	A	0.8950589299201965	So that means that if I take an action from a particular state where I'm going to enter and we have the true transition dynamics for this particular grid by design.
1400736	1408062	A	0.8834695816040039	So there is a function called get true B and that will give us the true b of the system which the agent can use.
1408196	1408542	A	0.7123860716819763	Okay?
1408596	1417150	A	0.8047388792037964	So ideally we would want the agent to learn this, but for the purpose of this demo we are assuming that the agent already knows the structure.
1417890	1428978	A	0.5986461639404297	And then comes the prior preference, which is interesting here in the sense that it is defined as how closer you are to the gold state.
1429064	1434198	A	0.5037310123443604	So if you are at the gold state, then clearly that is the most sought out state.
1434284	1436006	A	0.5578821301460266	You prefer that the most.
1436188	1439606	A	0.843780517578125	And how do you prefer the neighboring states, right?
1439628	1447626	A	0.8914452195167542	So that is dependent upon the square root of the distance or basically the distance from that particular gold state.
1447808	1456258	A	0.8881059288978577	So you define a grid which is eight cross eight, the same size as this particular grid world task.
1456454	1465230	A	0.8829594850540161	And then we have a method to kind of add values which is the preference you have for every state.
1465380	1477458	A	0.7405284643173218	And if you render the particular C matrix you can see the structure which is the same where this gold state is more preferred and the surrounding states less preferred and so on.
1477624	1482440	A	0.8748835325241089	So now we have the C matrix also defined in the classic PMDP way.
1483290	1492722	A	0.9002532958984375	And then I initialize that C matrix as the C matrix we evaluated in the previous cell which is small C, this particular C matrix.
1492786	1493446	A	0.7123860716819763	Okay?
1493628	1499930	A	0.8783233165740967	And then lastly, for the generative model we have capital D which is your prior overhead and states.
1500000	1502918	A	0.8327118754386902	And for that I'm using a uniform object array.
1503014	1506702	A	0.5200818181037903	So that means that I don't have a prior of where I am starting.
1506836	1510350	A	0.830758273601532	So let me run the pending cells.
1512530	1516622	A	0.8145632743835449	So here the D matrix is a uniform distribution over hidden states.
1516676	1519346	A	0.5939886569976807	I don't know where I'm going to start the simulations and so on.
1519368	1522898	A	0.860273540019989	So this is the basic structure of the generative model.
1523064	1530574	A	0.873063862323761	And then we have the agent class which I want to discuss separately, like the environment, right?
1530632	1536550	A	0.8577070236206055	So given these environment parameters, how would you expect the agent class to work?
1536700	1538662	A	0.7523485422134399	So where is the agent class?
1538796	1540790	A	0.8503240942955017	Inside this folder structure?
1541850	1555274	A	0.9280076026916504	Inside the Pymdp module folder we have an agent Si PY which is basically a class again and similar to the environment class here.
1555312	1568938	A	0.8882872462272644	Also we have a step function where this will take an observation to the function and also a flag whether or not to learn the environment, which is optional.
1569114	1572898	A	0.5923241972923279	So if you disable it, it won't learn the generative model.
1572984	1576850	A	0.7465574145317078	If you enable it, it will update the parameters of the generative model.
1577000	1586294	A	0.8655881881713867	And what basically it does is it will return an action which is to be taken at this point of time and the environment can basically use that action, right?
1586332	1590520	A	0.921805202960968	So in this file we have the agent class which I will explain in detail.
1591850	1605420	A	0.915730893611908	So I am basically importing that agent class in this cell and then we are going to try and reproduce this behavioral result from the original sophisticated inference paper.
1605810	1606560	A	0.7123860716819763	Okay?
1607010	1608720	A	0.6318534016609192	And for that.
1609490	1617034	A	0.8825666308403015	So what we expect is that given this prior preference structure, there are local maximas in this prior preference structure.
1617082	1631362	A	0.577627956867218	So if you start from this particular point, if you do not plan deep enough, what you will end up is in one of these local maximas where you don't see that there is a highly preferred observation, say four steps down the line.
1631496	1646434	A	0.48507368564605713	So if you are in this particular state, what you will see is this local maxima and you will go and sit there because the neighboring states are less preferred and this state which is more preferred is not accessible because of the wall or the wall structure.
1646562	1654458	A	0.7922536134719849	So you have to take a turn and pass through less preferred states and you need deep planning in order to enable the agent to do that.
1654544	1666954	A	0.7508687973022461	The agent should be able to kind of simulate four time steps ahead in time to see that there is this highly rewarding observation coming to kind of do that actions.
1667082	1671794	A	0.8927933573722839	So that's the point that we are trying to see in this particular demo.
1671912	1682982	A	0.7697589993476868	So for a low planning depth it will basically get stuck in one of the local maximas but with sufficient planning depth it will navigate to the gold state.
1683036	1685142	A	0.8246265649795532	So that's what we are trying to see, right?
1685276	1699094	A	0.8976278901100159	So we have different planning horizons and what we are basically doing is give the agent a generative model which we right now defined the A matrix, B matrix, C matrix B matrix.
1699222	1702026	A	0.8695834279060364	Then we have the planning horizon of capital N.
1702128	1705238	A	0.7890107035636902	So here I am, iterating over planning depth.
1705334	1708410	A	0.8535065650939941	So N will be one, three and four for the loop.
1708570	1713722	A	0.8866184949874878	Then we have action precision, which is often denoted by alpha in active inference literature.
1713786	1716894	A	0.7960551381111145	So that determines which action is to be taken.
1717012	1725394	A	0.8001797795295715	So a highly precise action precision means that it will stick to the action with the lowest expected free energy.
1725592	1731374	A	0.8688277006149292	But a lower action precision is kind of probabilistic where it will also consider other actions.
1731502	1738658	A	0.9012333750724792	Then we have a planning precision which is part of the planning function we'll discuss, which is often denoted in the literature as gamma.
1738754	1753734	A	0.4749319851398468	Then we also have a search threshold which is extremely important for sophisticated inference because as we saw, sophisticated inference is a tree search and tree search is bad in the sense that it can have a lot of computations.
1753862	1759310	A	0.6277911067008972	But you have to define a threshold to kind of ignore many possibilities to make it work.
1759380	1761920	A	0.8654260039329529	And that's the idea that we will also discuss.
1762610	1768510	A	0.9163243174552917	So just a preview before we go to the agent class.
1768660	1777486	A	0.9089441895484924	What we are trying to do is in a loop we are going to call the agent step and environment step in series.
1777598	1788418	A	0.8473353385925293	So the agent will see an observation, it will take an action, that action will go into the environment, the environment will give it back new observations and this loop continues.
1788514	1798442	A	0.8911335468292236	And we want to see over time how this loop evolves into a purposeful behavior and if the agent at all is capable for that.
1798496	1803498	A	0.8973247408866882	So before I reveal the results, let's discuss the agent class.
1803664	1812382	A	0.8519318103790283	So in order to give an action, when an observation is given, the agent should have the planning and so on, right?
1812516	1823966	A	0.8656414151191711	So here is the agent, the sophisticated inference agent, where we are actually using the existing Pymdp agent for some functionalities.
1824078	1833782	A	0.8492221832275391	So in Pymdp we already have really well written functions for perception and learning.
1833916	1842690	A	0.6878489255905151	So the only thing we want to kind of replace is how the agent is doing planning and how the agent is taking decisions over policies, right?
1842860	1849930	A	0.7722480893135071	So here we are using that parent agent class.
1850080	1860960	A	0.943930983543396	So from Pymdp agent we are importing that agent class which is sitting next to the Si agent that we are discussing now.
1861810	1883526	A	0.8779380917549133	And basically we are taking in the generative model structure from the main program for this class to work which is C and D and all the precisions and threshold parameter I mentioned, then it is kind of normalizing the prior preference that we mentioned in the main program.
1883708	1899754	A	0.8729605674743652	So here if I look at how C is, the structure of C is defined in terms of numbers and the prior preference is often interpreted or it should be a probabilistic distribution for the computations to work, right?
1899792	1909902	A	0.5858302116394043	So we are going to normalize it as a probabilistic distribution rather than having numbers that don't add up to one.
1910036	1911454	A	0.8100988864898682	So that's what's happening here.
1911492	1913742	A	0.8092836737632751	We are using Softmax to do that.
1913876	1932378	A	0.9005705118179321	Then what we are doing is we are initializing the existing PMDP agent with these generative model parameters and what we are intending to do is write a planning function for a given planning horizon and a given threshold for trees.
1932574	1937080	A	0.8702021837234497	Okay, so there are three functions in this agent class.
1937690	1941190	A	0.8819551467895508	One is a helper function for planning which we will discuss now.
1941340	1947354	A	0.889434278011322	Then there is a planning function itself which is going to do planning using tree search.
1947552	1962830	A	0.8734524250030518	And then because it is a recursive tree search, we are going to need an additional function that implements that recursive evaluation where we are going to call this function called forward search inside the function itself.
1962980	1966142	A	0.8843052387237549	So we are calling this function inside this function.
1966276	1973646	A	0.881912112236023	So that is to calculate expected free energy for the next step and it will call it again for the next step till our planning horizon.
1973678	1990390	A	0.8249097466468811	So that's the idea of recursive looping and finally it will return the expected free energy for all actions given an observations and then we just implement the step function where it is written sequentially what to do given an observation.
1992330	2007050	A	0.8839088678359985	So going back to the demo here we have this first idea where you get an observation and it gives out an action.
2007130	2011934	A	0.8828023672103882	So let's go to the agent step function and imagine what happens.
2012132	2019934	A	0.8344711661338806	So if it is time t equal to zero or in the beginning of the experiment, what it is ideally.
2019982	2024526	A	0.8031398057937622	Supposed to do in the first place is infer that state using its observation.
2024638	2037638	A	0.9000471830368042	So what we are giving it is an observation and using the modules for inference, it's going to come up with a belief QS, which is a belief about states.
2037804	2054474	A	0.8988175988197327	Okay, so self dot QS is the belief inside the agent and once it has a belief about where it is right now, it can implement plan research which is do planning for this particular belief of hidden state right now.
2054592	2064590	A	0.9014477729797363	And once it has done planning, it can take decision using the sample action function in Pymdp and basically return that action.
2065410	2069362	A	0.8155842423439026	And for every other time steps, the sequence remains the same.
2069496	2075620	A	0.7089051604270935	But it is also learning about the structure if you enable learning in your agent class.
2076470	2078900	A	0.7757495641708374	So that's the step function.
2079590	2091986	A	0.8786150813102722	But in order to do planning, what it does is it kind of reorganizes the generative model structure for any number of hidden state modalities and any number of observation modalities.
2092178	2106560	A	0.8579520583152771	So to discuss how melting works, I would like to talk about the new A matrices and B matrices it evaluates for implementing that planning and let's understand that.
2107650	2113274	A	0.8556461930274963	So let's go back to the original hidden state factors.
2113402	2116266	A	0.615896999835968	So here we only have one hidden state factor.
2116458	2118622	A	0.7231533527374268	So that's why it is B zero, right?
2118676	2124238	A	0.5353000164031982	And B one does not exist because we only have one hidden state factor.
2124334	2130020	A	0.8063071966171265	But imagine that if I have two hidden state factors with the same size maybe.
2130730	2137320	A	0.8452427387237549	So here it could be a location and maybe something else inside the agent's mind.
2137850	2142306	A	0.829451322555542	And we should also have controls for these two hidden state factors.
2142418	2161390	A	0.8679286241531372	Just like so there should be control for every hidden state factor if you're familiar with active inference idea and then maybe the observation space is also directly observing these two hidden state factors.
2161810	2168862	A	0.8168413639068604	Okay, so this is a new Generative Model structure with multiple hidden states and multiple observation modalities.
2169006	2174114	A	0.8160837292671204	And right away you can see that the dimensionalities of your parameters change.
2174232	2182178	A	0.8572866320610046	You have 25 observations coming from 25 times 25 hidden states.
2182344	2186550	A	0.836638331413269	And if you look at the structure of the first observation modality, it's the same.
2186700	2195594	A	0.7336230278015137	But what we want is a new matrix where it's going to be 25 with not two hidden states but just one hidden state.
2195632	2204060	A	0.8528873920440674	It's only a reorganization of the generative model, but computations essentially remains the same.
2204910	2215498	A	0.788051962852478	So that's what this helper function is trying to do, which will make things easier for us when we have multiple hidden state modalities.
2215674	2228718	A	0.8654137849807739	So if you have multiple hidden state modalities, we are going to compute how many total states you have, which is the multiplication of number of hidden states in each modality.
2228814	2229074	A	0.7123860716819763	Okay?
2229112	2238120	A	0.6987287998199463	So if you have 25 hidden states in one modality and 25 hidden states in the other modality, you're going to have 625 total number of states.
2238570	2250622	A	0.8279666900634766	And if you have four actions each in each modalities, then you have total of 16 actions which is nothing but the combination of these four actions each in the modalities.
2250706	2255014	A	0.8434860706329346	So you're going to have four times 416 actions if you have two modalities.
2255142	2269810	A	0.6131119132041931	And it's basically going to build a generative model that has the same model parameters but just with a different dimension structure so that it's easier for us to calculate the expected.
2270150	2281586	A	0.656069815158844	So now we have a new A and new B and a new belief which is nothing but tensor products of existing parameters and beliefs.
2281698	2286470	A	0.6060550808906555	It's nothing but a new big matrix and nothing else.
2286540	2287160	A	0.7123860716819763	Okay?
2288250	2291570	A	0.8478268980979919	It's not a change, it's just a transformation of structure.
2291730	2302346	A	0.8938649296760559	And given this A, B and Q, we are going to predict what's going to happen in the future and evaluate the expected free energies for them.
2302448	2306986	A	0.8755771517753601	So in order to do planning, so that's the second function.
2307088	2317910	A	0.7188459038734436	What we are going to do is first call the first function which will do the melting for us and set up the generative model in good dimensions, easy to compute.
2318010	2330110	A	0.879587709903717	Then we have the expected free energy itself for all the actions and then we have the probability that depends upon this expected free energy for these actions.
2330190	2333122	A	0.5716620683670044	So why is it just the actions and not the observations?
2333266	2341606	A	0.8849704265594482	Because here we are going to evaluate expected free energy of actions for the given observations, right?
2341628	2351286	A	0.715767502784729	So let me go back to the slides and discuss this pictorially to make things easier.
2351398	2354870	A	0.83550626039505	So here we have the grid and we have the prior footprints.
2354950	2369854	A	0.8750746250152588	And what we are trying to implement is that if you observe some observation at time T, then you're going to consider the consequences of your actions given that observation, because you can predict what's going to happen.
2369892	2378226	A	0.8801158666610718	Because in your generative model you have the transition dynamics that will tell you, given this state, if I take this action, where I'm going to end up, right?
2378248	2389154	A	0.8125483989715576	So that's basically predicting what's going to happen in the future and you're right now considering the consequence of available actions in your arsenal.
2389282	2397130	A	0.8956650495529175	And then if you take an action, then you can predict what's going to happen in the next time step as a new observation, right?
2397280	2406406	A	0.7632622718811035	So you have a probability distribution that tells you that say this observation is the most likely and the other observations are not really likely.
2406598	2417022	A	0.8843782544136047	Then what you will do is you do this again, you consider the consequence of doing your actions from that particular observation and this goes on in your planning depth, right?
2417076	2420666	A	0.7901868224143982	So this can be thought of as maybe you want to go to the gym.
2420778	2424030	A	0.5568094849586487	Then you are going to consider all the consequences.
2424110	2433774	A	0.6321272253990173	What will happen if I wear my shoes, if I don't wear my shoes, if I go in my car, if I don't go in my car, then you realize that, okay, I have to wear my shoes.
2433902	2441842	A	0.7768846750259399	Then you consider the consequence that I'm now ready to go to the gym and me going to the gym will end up me being in the gym.
2441906	2443094	A	0.6967025399208069	So that's the idea.
2443212	2449322	A	0.8134140372276306	You consider the consequence of where you are right now and you can go as much as you want, right.
2449376	2450054	A	0.6437391042709351	You can predict.
2450102	2453114	A	0.8428608775138855	So in a game of chess, you might be in a particular state.
2453232	2466634	A	0.764868438243866	You consider your consequences, you see the future, you consider consequences from that future, and you can go as deep as you want depending upon your computational abilities.
2466762	2467006	A	0.5664746165275574	Right.
2467028	2474690	A	0.7570379972457886	So that's what you're trying to implement in this agent class where we are considering consequences?
2477430	2478180	A	0.5491447448730469	Yeah.
2478790	2485030	A	0.8680983781814575	So for every modality we are going to consider the expected free energy for actions.
2485450	2490886	A	0.8819983601570129	And this will basically call the next function, which is forward search.
2490988	2497078	A	0.8686045408248901	So forward search is implementing the thing that I just mentioned, considering consequences.
2497254	2502150	A	0.8653836250305176	And in forward search, what you are basically doing is for every action.
2502310	2506750	A	0.8855438232421875	So in line 149, I have a loop that goes over every action.
2507090	2512282	A	0.7681993842124939	Then I'm going to consider the posterior or the consequences of all those actions.
2512426	2517390	A	0.8711127042770386	I use my transition probabilities to evaluate that consequences.
2517730	2524210	A	0.8740352392196655	Then I'm going to predict the observations, because my prior preferences are defined in terms of observations.
2524630	2533190	A	0.8525945544242859	I'm going to predict my observations and then evaluate the expected free energy, which is the sum of risk and ambiguity.
2534010	2534758	A	0.7123860716819763	Okay?
2534924	2536102	A	0.6199934482574463	I hope that makes sense.
2536156	2544910	A	0.8505088686943054	Like here, you have considered the consequence, which is consequence of future, which is post or posterior.
2545090	2550794	A	0.789627730846405	And you're basically evaluating how good that posterior is depending upon your expected free energy.
2550912	2554454	A	0.865850031375885	And that becomes the expected free energy for that particular action.
2554582	2556670	A	0.7123473882675171	And you do this for all the actions.
2557330	2558030	A	0.584351658821106	Okay.
2558180	2560554	A	0.5353693962097168	And why this is powerful?
2560682	2563742	A	0.5450935363769531	It is because you can go as deep as you want.
2563876	2574574	A	0.9031283259391785	So here in the next step, you go to this loop where you will check if I am crossing my deep planning or the depth of planning.
2574702	2576578	A	0.5436723828315735	And then you are doing basically the same.
2576664	2582130	A	0.8409659266471863	Given that posterior, what is the consequence of the actions of that particular posterior.
2582210	2587078	A	0.8271456956863403	So here for considering that we are again calling the parent function.
2587244	2596330	A	0.8814693689346313	So the same function, forward search, to consider consequences of those combinations and it will basically come back and add up to your expected free energy.
2596480	2609482	A	0.8507313132286072	So what happens over this sequence is that you consider some or all future consequences and then all that values will trickle up your tree.
2609546	2619650	A	0.8023439049720764	And that sum of the expected free energy will tell you which action is good or which action is bad, which you can take to kind of see your preferred observations.
2620150	2623298	A	0.8756105899810791	So that's the idea of implementing tree search.
2623464	2632998	A	0.5787374377250671	And I will also talk about the importance of this threshold here, which makes this algorithm possible.
2633084	2636630	A	0.80443274974823	So without this threshold, this algorithm will not work.
2636780	2640230	A	0.8992953300476074	I will explicitly talk about why that is the case.
2640380	2652646	A	0.883884072303772	And then once you evaluate the expected free energy for all available actions, given the present state, you can basically compute what you call the action distribution.
2652838	2657550	A	0.879069983959198	That is how probable is my action or how I should take my action.
2658850	2662298	A	0.8873356580734253	And we also have this action precision parameter alpha.
2662394	2671922	A	0.6577874422073364	So if alpha is very high then it basically is a highly skewed distribution where you will always choose the action that minimizes expected free energy.
2672056	2678542	A	0.5256631374359131	If alpha is really low then it's going to be a more sparse or spread out distribution.
2678686	2687938	A	0.8848633170127869	And then you can use this action distribution to sample actions in the agent environment loop.
2688034	2693106	A	0.884762167930603	We just finished doing planning and computing that action distribution.
2693298	2698250	A	0.8948450684547424	Then using that action distribution, you can sample an action from your policy space.
2698400	2701580	A	0.8867884278297424	So let's look at the policy space in this generative model.
2702910	2717520	A	0.8416052460670471	So I'm switching back to the original generative model with one hidden state factor and let's do planning and maybe initialize this agent.
2718310	2723970	A	0.6918349862098694	I just want to initialize this agent to see the policy space, not run the loop.
2724390	2733288	A	0.8634430170059204	So I initialize this agent, say for planning depth of one.
2733454	2751808	A	0.8807042837142944	And if I look at agent policies, I can see that I have basically four available actions which is north, south, east and west.
2751904	2757552	A	0.8869923949241638	And if I have an action distribution, it will tell me how probable is to take that action.
2757616	2772780	A	0.775048017501831	So if I look at agent UPI, okay, so this is not defined because I have not done planning, but I can do planning and then it will be defined.
2778580	2783776	A	0.8279293179512024	Yeah, so I implemented planning with research and then now I have an action distribution.
2783888	2790424	A	0.8484766483306885	So for this particular scenario, I am going to take my third action the most which is zero point 99.
2790462	2796184	A	0.8821821212768555	Basically that's the probability which is north, south and east in this particular case.
2796222	2806060	A	0.8843967318534851	I just wanted to kind of familiarize you with the matrices, but we are now going to see the agent environment loop in action.
2808880	2825424	A	0.8307639360427856	Now you can sample an action from the sample action function and then implement learning which is using the standard Pymdp way where I will update my transition dynamics and likelihood dynamics depending upon what I see and what's my belief and so on.
2825462	2829510	A	0.8464061617851257	So my emphasis is on the decision making part.
2838010	2844466	A	0.8668325543403625	So once you sample an action, then that action basically goes back to the environment.
2844578	2852490	A	0.8765120506286621	Okay, so now let us implement this for a planning depth of one and see how the agent behaves.
2852990	2864302	A	0.8160045146942139	So here if it is a planning depth of one, then that means that the agent is only considering the consequence of one time step ahead, just seeing the immediate future for doing planning, right?
2864436	2867120	A	0.7922802567481995	So I'm giving the planning depth of one.
2867730	2889150	A	0.9060479998588562	I'm resetting the environment where the agent is going to start from that initial start state, and in the loop, it's going to get that observation, take that action, give back an action, and we will look at the action probabilities.
2889490	2896490	A	0.8016165494918823	And also we will give that action back to the environment, get back the observation, and this loop continues till the episode is terminated.
2896570	2902110	A	0.9038197994232178	And I have set the episode length to be eight, just to see the outcome of eight action.
2902870	2912006	A	0.7672631740570068	So when we run this loop, these matrices are nothing but the action distributions of how likely each action is to be taken.
2912188	2915846	A	0.7381312251091003	And this is where the agent end up in the last time step.
2915948	2916502	A	0.7123860716819763	Okay?
2916636	2925100	A	0.779595673084259	So let's maybe kind of also enable this environment render that will show us where the agent is at every time step.
2925870	2933098	A	0.9162542223930359	So initially the agent was at this location and we have an action distribution of this here.
2933184	2934934	A	0.8460072875022888	So north, south, east and west.
2934982	2937258	A	0.8718559145927429	So the agent knows that it should go north.
2937354	2937950	A	0.6120101809501648	Why?
2938100	2944414	A	0.5276419520378113	Because if I look at the prior preference, this state is more preferred than this state.
2944532	2944862	A	0.7360090017318726	Right?
2944916	2953714	A	0.6970624923706055	So the agent successfully calculated the expected free energy and inferred that, okay, I should go to this state and not stay in this state.
2953752	2962166	A	0.8742031455039978	And because it has the generative model of transitions available, it can infer that I should take an action north to go to this state.
2962348	2963686	A	0.9544609189033508	So that's good.
2963868	2965858	A	0.8004688620567322	And the agent goes to north.
2965954	2970530	A	0.9163079857826233	And at this particular state, the agent infers that it should go to north southeast.
2970610	2975274	A	0.8975555300712585	So it will take an action east and it will go here.
2975392	2983318	A	0.812056839466095	And at this point of time, I want your attention where the action distribution is equally probable for north and east.
2983414	2984362	A	0.6291940212249756	Why is that?
2984496	2987166	A	0.6051695942878723	Because the agent is only looking at the immediate future.
2987268	2987582	A	0.5664746165275574	Right.
2987636	2996254	A	0.9083850383758545	So let's go back to the prior preference where the agent is right now here, or is it here?
2996292	2999714	A	0.7356870174407959	Yeah, it is right now here in this particular state.
2999832	3009538	A	0.5781965851783752	And if the agent is considering immediate consequences of just one action, then these two states are equally good for it to be in the next state.
3009624	3015014	A	0.6525540351867676	So there is no distinction between these two states if it is only looking at the immediate future.
3015132	3021026	A	0.856171190738678	So that means that the expected free energygies will conclude that I should go to north or east.
3021058	3024538	A	0.6260384917259216	It doesn't matter if I'm looking at just one time step ahead.
3024704	3025418	A	0.7123860716819763	Okay?
3025584	3026860	A	0.723736584186554	That's the idea.
3027790	3032380	A	0.8140259385108948	And out of probability it is going here.
3033150	3052154	A	0.6147039532661438	It took the action east and from this state, when it's doing inference, it's inferring that this state is better and basically it's ending up in this local maxima state, which is this particular state where the neighboring states are less preferred.
3052202	3058302	A	0.7550489902496338	And this is Wall and you can't go there because it is forbidden for the agent by structure.
3058366	3065338	A	0.7278656363487244	So it's basically going to sit there forever where it only sees that local maximum of prior preference.
3065534	3072526	A	0.6990927457809448	And let's look at what might happen if you have higher planning depth.
3072658	3083866	A	0.7909149527549744	So if I go to the planning depth of three, then that means that the agent is actually reaching the goal state at the last time step.
3084048	3093018	A	0.8787577152252197	But still in the third time point, it had two probabilistic actions, north and east.
3093114	3103678	A	0.9013921022415161	So here from this particular state, out of probability, it took the action north but it can all equally take the action east and end up in this local maxima.
3103694	3108626	A	0.6619277596473694	So let's run again and probably it will end up in this local maxima, okay?
3108808	3121590	A	0.6856473684310913	And only for the planning depth of four, which is sufficient enough, which is necessary for this particular grid, the agent is fully sure of what to do.
3121660	3133834	A	0.7910818457603455	So at every time point it is fully sure of what to do, that it has to go north, then east, north, north, north, east, east and south and reach this particular goal state.
3133952	3144906	A	0.7040266990661621	So only for time step or planning depth n equal to four, it can successfully navigate this grid with 100% certainty.
3145018	3146720	A	0.6967025399208069	So that's the idea.
3147250	3150786	A	0.5145702958106995	That's the implementation that I hope you got.
3150968	3153774	A	0.8790529370307922	So there is also this idea of action precision.
3153822	3155886	A	0.847603976726532	So here it's a high action precision.
3155998	3158574	A	0.8522013425827026	That is why it is taking the actions.
3158622	3160690	A	0.7989975214004517	That is from the probability.
3160850	3169026	A	0.5874457359313965	If it is a low action precision like one, then the good actions are more probable.
3169218	3178442	A	0.764392614364624	But that doesn't mean that it will be taken right here by luck, it is taking the right actions and reaching the state.
3178576	3180566	A	0.6581175327301025	But here probabilities are most passed.
3180598	3188794	A	0.8208627104759216	But you will also see like exploration behavior in more number of trials if you control this action precision.
3188842	3195230	A	0.7821882367134094	So I set it to a high value to make sure that the agent reaches the goal for this particular problem.
3195300	3199460	A	0.7504494786262512	But it's worth playing and it's important, right?
3200470	3203042	A	0.6095885634422302	Okay, yeah.
3203096	3210694	A	0.7858789563179016	So for different planning depths like one, three and four in this problem, this is the behavior that you expect.
3210892	3222846	A	0.5638107061386108	For lower planning depths, which is not sufficient, the agent ends up in local maximas or local minimas of expected free energy or local maximas of prior preference.
3222978	3227290	A	0.5022327303886414	But with sufficient planning depth, it's able to navigate and reach the goal.
3227950	3244922	A	0.6452893614768982	So that brings us to the last point in this tutorial where why is it important to have a threshold in evaluating sophisticated inference?
3245066	3252354	A	0.5471600294113159	So by threshold, what we mean is that we can ignore future possibilities in two levels, right?
3252392	3258370	A	0.5284098982810974	You can ignore not likely actions or not likely observations in this research.
3258520	3267542	A	0.6514984369277954	But if you consider the consequences of all actions and observations, that means that you'll have to consider four consequences in the first place.
3267676	3273730	A	0.8733775615692139	Then you will have to consider four times the action states in the next time step for the next time step.
3273820	3280294	A	0.7019378542900085	Then all of that multiplied with the number of actions and this tree search becomes intractable and you'll explode.
3280342	3285100	A	0.9359527230262756	And it's even worse than the classical active inference policy space problem.
3285410	3291630	A	0.5380495190620422	But by defining a threshold of even a small value, we'll ignore possibilities.
3291970	3294510	A	0.7789276838302612	So where is that implemented?
3295410	3305246	A	0.8678586483001709	In the forward search algorithm, we are considering actions with only action probabilities greater than the particular threshold.
3305438	3308030	A	0.9168930649757385	Here I am defining it as one by 16.
3308190	3314630	A	0.9184184074401855	Also in the parent paper, it's one by 16 the action probability.
3316170	3316920	A	0.7123860716819763	Okay?
3317290	3324686	A	0.6855329871177673	So if it is zero, then that means that it will consider all the consequences of future and that's intractable.
3324818	3346480	A	0.6723645329475403	So you can ignore actions which is not probable, and also ignore states which is less likely, or only consider states that has probability greater than this particular threshold value, and that significantly reduces the computational complexity, where you will only consider combinations that are probable in the future.
3347190	3352290	A	0.715043306350708	Tree and that lets you go deeper in your planning horizon.
3352870	3354674	A	0.6120420694351196	That's an important point here.
3354792	3371618	A	0.6644166707992554	And if you compare the time that takes for deeper planning for a search threshold of zero, so a search threshold of zero means that you will consider all consequences.
3371794	3377498	A	0.7121214866638184	And the more deep you plan, the more time it takes.
3377584	3385450	A	0.7525148391723633	And if you see to consider only the first future or the immediate future, it takes 0.1 second.
3385790	3392474	A	0.6736621260643005	For considering three possibilities into the future, it takes 3 seconds, and for four it takes 300 seconds.
3392522	3396430	A	0.48614856600761414	And you can see that the computational time is exponentially growing.
3397190	3412694	A	0.7185654640197754	But if you have a very small search threshold, you have that computational time that makes sense for implementation in real world, right?
3412732	3420386	A	0.7218294143676758	So here for N equal to four, that is four times steps into the future, it's only taking 0.1 second.
3420418	3421302	A	0.5445113778114319	And that's okay.
3421436	3431722	A	0.5187219381332397	I can still do simulations with this complexity, but there is no way I can talk about how less the computational complexity is.
3431776	3436678	A	0.8713469505310059	It truly depends on the nature of your prior preferences and environment in action.
3436774	3443840	A	0.7625848054885864	But this search threshold actually works in real life, and we just saw that in our simulations, right?
3444210	3448350	A	0.7440844178199768	For N equal to four, it took only like 0.3 seconds in our simulations.
3448510	3456830	A	0.6621888279914856	But if you set the search threshold as zero, it already is 300 seconds for doing full depth planning.
3456910	3466470	A	0.6048794388771057	And if I set a planning depth of five, then it will basically run forever maybe, and I will not be able to do simulation.
3466810	3469750	A	0.7972550988197327	So that's the idea of search threshold.
3470810	3472940	A	0.6505111455917358	So actually, that's it.
3473790	3486762	A	0.6913132667541504	I wanted to explain the Agent class, the environment class, and the particular demo and yeah, maybe it's a good time for questions if anybody was listening.
3486906	3499138	A	0.7215836048126221	And I hope people get to play with this code and look at the tutorial and implement this and build generative models like this.
3499224	3509518	A	0.9056951403617859	So this particular example is how do you build a generative model for this grid world task and see how the Agent is able to take meaningful actions?
3509694	3516262	A	0.8706234693527222	But here we gave it that true structure of the environment in the B matrix and A matrix and so on.
3516396	3532700	A	0.8330847024917603	But you can also play around with learning in the sense that while defining the Agent step, you can add a flag that says learning equal to true.
3534750	3541962	A	0.8477998375892639	And if you start with an uninformed AB and so on, you can experiment on how the Agent is learning that environment.
3542026	3550530	A	0.904979407787323	Here you can look at the B matrix in the beginning, you can look at the B matrix after, say, ten trials and see how the learning is taking place here.
3550600	3555058	A	0.5224291086196899	It doesn't matter because the agent knows the structure and it won't learn much.
3555144	3560914	A	0.7973945140838623	But if it starts from an unknown structure, then there is scope of learning also to be implemented.
3560962	3572646	A	0.6262183785438538	And it's already implemented because we are using existing Pymdp functionalities for learning A and B and it's already part of the step function.
3572828	3585260	A	0.710503101348877	So I hope step function is clear, which is the only thing you need to know if you're trying to implement sophisticated inference and just the names of these matrices if you want to probe them and look at them.
3585630	3589482	A	0.8945133686065674	And yeah, I hope the session was useful.
3589626	3609362	A	0.9342092871665955	So I thank my collaborators and Connor, who maintains Pimdp, and also Brendan who runs the Pimdp Fellowship, which I was part of, and that's where I worked on implementing sophisticated inference in Pimdp and it will be part of the original Pymdp module soon.
3609496	3617378	A	0.898826539516449	And I hope people can start using this module to simulate sophisticated inference experiments and this basically becomes useful.
3617554	3625240	A	0.6403735280036926	So maybe it's a good time to discuss questions or clarifications on the code or maybe it's a good time to take a break, as Daniel was.
3627870	3628570	B	0.8529649972915649	Thank you.
3628640	3629660	B	0.9809349179267883	That was awesome.
3630590	3636890	B	0.8790817856788635	Well, I have a few different questions and I'll read a few from the live chat.
3636970	3642750	B	0.8966062068939209	So I'll first just go to the live chat and then ask those and then ask some other questions.
3642820	3648814	B	0.9141011834144592	So Dave asks, how do you think about the neural implementation of recursion?
3648942	3654418	B	0.5242508053779602	Brains don't seem to implement computer hardware style recursion deeper than a stack depth of one.
3654504	3665190	B	0.8429134488105774	Aside from heavily over learned tasks, we can confine ourselves to asking about recursion for the purposes of exploring temporally deep state spaces, searching forward in time.
3665340	3680090	B	0.8141845464706421	So how do we reconcile this really beautiful and elegant and computationally efficient full depth tree search with the biological basis of multiscale planning?
3681070	3694142	A	0.6627551913261414	Yeah, so I'm not an expert in neural computation, but the answer to that would be basically you're doing only one computation at a time, right?
3694276	3704562	A	0.8421289324760437	And all you need is some memory to store your beliefs and use those beliefs to kind of do the same computation again.
3704696	3711906	A	0.5036703944206238	So we are not talking about this hardcore recursive implementation.
3712018	3714646	A	0.7678983807563782	We are only doing local computations at a time.
3714748	3723014	A	0.5504958033561707	And just because of the structure of the generative model and because we have memory, this can be done.
3723132	3729754	A	0.6749406456947327	And I don't see why brain can't do it, even though individual neurons might not be able to do it.
3729792	3731494	A	0.7750606536865234	The brain has memory.
3731622	3737226	A	0.7824311852455139	The brain has the ability to store memory and the ability to dream, the ability to simulate.
3737418	3739674	A	0.6213085651397705	It knows the consequences of actions.
3739802	3746286	A	0.8469079732894897	And you do this on a daily basis where you plan your future and decide what to do.
3746308	3746542	A	0.7360090017318726	Right?
3746596	3756050	A	0.4964020550251007	So on a single neuron level, I'm not really sure of how to answer that question, but I don't really see why the brain can't do it as an organism.
3758710	3759460	A	0.84200119972229	Cool.
3760870	3762262	B	0.7005574703216553	Okay, to the code.
3762316	3764230	B	0.7500346302986145	I guess I have a few questions.
3764380	3766310	B	0.8686008453369141	Can we go back to the maze?
3771000	3775450	B	0.7147151231765747	And of course, if anyone else has questions in the live chat, just go for it.
3777500	3786596	B	0.8644314408302307	So in the maze, how does the moves that are possible, how is that reflected?
3786708	3795644	B	0.7709322571754456	What stage is it updated, for example, that it initially knows that it can only go up and then it can only go right or down?
3795842	3802684	B	0.8797861933708191	Where is that reflected with the updating of kind of that relational aspect of affordances?
3802732	3804530	B	0.560184121131897	What is even possible to do?
3804900	3808530	B	0.8350542187690735	And then how does it evaluate in a deep tree search?
3810660	3816870	B	0.7442856431007385	Does it need to know what things could or couldn't happen in the future?
3819560	3829064	A	0.8360745906829834	So, if I understand the questions correctly, there is an important distinction between the generative process and the generative model, right?
3829182	3846620	A	0.8517594337463379	So in the grid world, which is the generative process, we have implemented all of that manually, where we have this transition dynamics that already knows what will happen if you take an action from a state.
3846770	3849950	A	0.6583179831504822	So it's like an environment that knows how to work.
3850320	3855376	A	0.547706663608551	So it's like a reality where there are consequences of actions and it's already there.
3855558	3861808	A	0.8731182217597961	But this information is available to the agent to be part of its generative model.
3861974	3872164	A	0.8758171200752258	And in the generative model, what it basically does is use that transition dynamics, which is given or learned to simulate what might happen in the future.
3872362	3872916	A	0.7123860716819763	Okay?
3873018	3889530	A	0.9014164209365845	So once I have that transition dynamics, if I go to the where's the if I go to the tree search, what it is essentially doing is evaluating the posterior given a particular state and an action J.
3889920	3900404	A	0.8400581479072571	So from my generative model, I know that if I start from this state and if I take this action, I go to this posterior and I consider all the consequences.
3900552	3912610	A	0.6540278196334839	And if in the generative model, it is unlikely that if I go east, I don't go there and so on, it's automatically reflected in the expected free energy.
3914360	3916548	A	0.6391335129737854	So I hope that answers the question.
3916634	3943260	A	0.8472340703010559	So here, if I set the action precision to be high and also enable the so initially I am in this particular state, and with respect to the expected free energy, what I'm doing is I'm using my generative model to predict what will happen if I take four actions.
3943600	3947436	A	0.8645835518836975	And my predictions will say that if I take North, I will go here.
3947538	3950204	A	0.843174397945404	And if I take all the other actions, I'll stay here.
3950322	3958640	A	0.6573973894119263	And because in my prior preference, north is more preferred, I can infer that north is the action I should go for.
3958790	3960130	A	0.6967025399208069	So that's the idea.
3960500	3963152	A	0.8749191164970398	So here, the grid structure is given to the agent.
3963206	3968164	A	0.7184808850288391	And it might be a little confusing, but the agent can also learn this grid structure and this will work.
3968282	3975684	A	0.8756069540977478	So once I know the grid structure as an agent, I can simulate the future and consider the consequences of action, right?
3975882	3977444	A	0.8103461265563965	So that's what's happening.
3977642	3987770	A	0.6968345642089844	And once I am in this state, I will consider the consequence of all four actions, and I will infer that, okay, going east is better because that will take me to this state.
3988300	4002130	A	0.7813884019851685	So there is always a difference between or you should keep in mind how generative model and generative process are two different things, and what the agent knows might not be the reality or might be the reality, depending upon what you give it.
4004260	4005344	A	0.5342625379562378	Makes sense.
4005542	4006290	A	0.84200119972229	Cool.
4006660	4027380	B	0.907401442527771	So if you were going to go about making a new situation that you wanted to do generative modeling for, do you tend to start from an existing working Pi MDP notebook and start modifying state spaces, or do you draw it out on a canvas?
4027540	4032408	B	0.6944501996040344	How would you recommend somebody other than replicating what is shown here?
4032494	4036180	B	0.5281324982643127	Let's just say we're interested in something that's not exactly just a maze.
4036340	4044220	B	0.8506565093994141	What do we do to get our head around how we should proceed?
4044880	4047180	A	0.6520525813102722	Yeah, good question.
4047250	4058210	A	0.8000215291976929	So if you are trying to simulate, say, a new environment, you have the heavy lifting to do, which is to define the generative model for the agent.
4058900	4065232	A	0.8114581108093262	You can either define a very sparse generative model, which the agent can learn, but you have to define the structure.
4065296	4071812	A	0.744963526725769	That structure should be there, and only using that structure, the agent can learn the generative model.
4071866	4078680	A	0.864125669002533	So here you can make use of this cell of code to understand how I define the structure of the grid.
4079020	4083672	A	0.8775932788848877	Okay, so I am defining a structure for the agent for it to make use of.
4083726	4088472	A	0.6110801696777344	I'm saying that there are 25 valid states and there are four available actions.
4088536	4094056	A	0.8006324172019958	And this is the standard way of defining the state space in Pymdp.
4094168	4101644	A	0.8497850298881531	And you also have to define the central parameters ABC and D for the agent simulations.
4101772	4107552	A	0.9073071479797363	So here I am defining A using my state space and observation space.
4107686	4115856	A	0.893433153629303	But this step of giving it or telling it that it's an identity matrix is my decision choice in my modeling.
4115968	4119152	A	0.5099159479141235	I don't have to do this for simulations.
4119296	4126916	A	0.8689635992050171	I can see if the agent is learning it from a random A matrix or when it starts from a random A matrix.
4127108	4132932	A	0.85700523853302	And similarly for this B matrix, this structure is defined.
4132996	4137428	A	0.8114582896232605	And there are functions like this which can give you a random B matrix.
4137524	4148760	A	0.8797747492790222	But this is a modeling choice where if I want to give it the grid structure or not give it the grid structure, I can start from a random B matrix, let the agent learn, and look at that learned B metric.
4148920	4150620	A	0.8263282179832458	Just for the purpose of the demo.
4150770	4154332	A	0.8783644437789917	I gave it the grid structure to enable it to take the actions.
4154476	4156624	A	0.6912866830825806	But it's not a mandatory thing.
4156662	4165208	A	0.6334202885627747	So this notebook is useful in the sense that you know what to do, but definitely you should play around with steps that may not be mandatory.
4165324	4172020	A	0.8669394254684448	Right, so if I give a prior preference for this state to be the maximum, then you can see behavior that.
4172090	4173652	A	0.8504994511604309	The agent will try and go there.
4173706	4173924	A	0.5664746165275574	Right.
4173962	4182456	A	0.7092130780220032	So this prior preference is defined in conjunction that this is the goal state, but this may not be the goal state.
4182558	4187800	A	0.8556116223335266	And in a different ask, what prior preference means is different according to the environment.
4188380	4189140	A	0.5664746165275574	Right.
4189310	4191304	A	0.8193355202674866	So that's also there and your prior.
4191352	4199668	A	0.6931948661804199	So once you define that generative model, which you have to do, you can't run from it, then everything is kind of automated.
4199784	4208876	A	0.8606173992156982	The agent only have to use the agent step, give it the observation from the environment.
4208988	4215140	A	0.7970625758171082	The agent knows how to take actions, and then everything that happens inside the agent, you don't really have to worry.
4217480	4228200	A	0.7232815623283386	So this structure, I'm sure, will be useful for your particular task that you are trying to model in your hmm.
4231660	4233512	B	0.9295148253440857	Yes, very interesting.
4233646	4242232	B	0.8939741253852844	And how would you contrast that or point to any similarities or differences with how this would be pursued outside of active inference?
4242376	4259520	B	0.6463377475738525	Like if somebody were just going to use another kind of deep policy agent in the maze example, what parts of the process would be familiar and what parts would be like a lot of work that wasn't expected or skipping through parts that were a lot of work otherwise.
4260020	4260770	A	0.5491447448730469	Yeah.
4261700	4269076	A	0.6741432547569275	So the general structure is very familiar to somebody who does things like this in reinforcement learning.
4269178	4283544	A	0.8447132110595703	So the idea that it's an agent step and environment step, so this is the standard OpenAI gym way of writing an environment and a standard OpenAI way of writing an agent.
4283742	4284104	A	0.7123860716819763	Okay?
4284142	4294572	A	0.803885817527771	So if you have a Q learning agent who does the same trying to navigate, then the way you have to define the queue matrix is the heavy lifting there.
4294626	4296632	A	0.8205919861793518	It's just a state action mapping.
4296776	4303072	A	0.7252950072288513	And in contrast to that, in active inference, you have to come up with a generative model that you want to see.
4303126	4306784	A	0.7748671770095825	So in active inference, generative model is the central thing.
4306822	4307410	A	0.5664746165275574	Right.
4307940	4314204	A	0.6197040677070618	Without a generative model, there is no meaning of purposeful behavior in active inference.
4314332	4323124	A	0.7992819547653198	So the only unfamiliar part for a person who is coming from, say, a field like reinforcement learning is the structure of the generating model.
4323242	4329796	A	0.5873228907585144	But there is no way, other than getting used to it, where it's the palm DP structure which dominates.
4329908	4338700	A	0.6282204985618591	But if you're doing deep active inference, all of this is going to be neural networks and palm DPS are also not active inference things.
4338770	4339052	A	0.5664746165275574	Right.
4339106	4341676	A	0.7977426648139954	It's an industrial engineering thing.
4341858	4356716	A	0.8594034910202026	So palm DPS must also be familiar for people who are coming from the computer science background, just the idea that what really happens in the agent is the active inference part where we have expected free energy s and variation of free energy energies.
4356828	4362172	A	0.8417012095451355	And if you want to learn about that, then you have to go to the agents and see how it works.
4362326	4365044	A	0.861022412776947	Look at the matrices numerically, see what's happening.
4365162	4370228	A	0.6292497515678406	But in a level where you want to get started, I don't see any problem.
4370394	4379172	A	0.7712876796722412	All of this is standard frameworks like palmdps and OpenAI gym environments, agent environment loop.
4379236	4383444	A	0.7869052290916443	All this is very deeply discussed in computer science.
4383572	4388750	A	0.9639177322387695	It's not cool.
4390560	4407090	B	0.8668830394744873	So what other motifs or cognitive phenomena are you excited or how do you see the Pi MDP development trajectory continuing after your sophisticated inference gets pulled in?
4407540	4420404	A	0.8718008399009705	Yeah, so the Pimdp had the original functionality and the functionality to implement or simulate general active inference agents with the policy space and so on.
4420442	4437592	A	0.5749936103820801	And that enabled a lot of people in the community who are not familiar with complicated coding and so on, who people who do psychology, psychiatry, and all the things.
4437646	4437828	A	0.5664746165275574	Right.
4437854	4443932	A	0.8035202026367188	So whoever want to come up and try implement active inference, pymdp enabled that.
4444066	4452828	A	0.8501270413398743	And I am hoping that this module will enable people who want to try out sophisticated inference experiments in their particular domain.
4452924	4466756	A	0.7569639086723328	So if you spend some time and get familiarized yourself with the structure of how Pymdp works, then everything else is just writing a jupyter notebook with minimal code, right, to simulate this.
4466858	4475624	A	0.7322050333023071	So if you have a particular task in your domain, I don't see a problem for a beginner to kind of try and code it.
4475662	4487048	A	0.9838863611221313	And what I'm very excited to see is people using this module for variety of experiments, just like how people started using Pymdp and sophisticated inference is taking off.
4487214	4495992	A	0.8441568613052368	And it's now widely talked about how it is the way of doing active inference.
4496056	4504000	A	0.9587722420692444	And I'm really hoping that people in various domains start using this module and see their experiments, and I look forward for the feedback.
4504420	4513090	A	0.5252590179443359	Yeah, so what Pimdp did two years ago, I'm hoping this module will do to people who are trying to model active inference in the soap state.
4514840	4542910	B	0.7331828474998474	So you mentioned the OpenAI gym and the standardized format, and what benchmarks do you use or what kind of test suites are you comparing, and how do we really know when we've made a generative model that really exceeds or excels in a way that other techniques are just not doing?
4543520	4551276	A	0.5227068066596985	Yeah, so if I may go to the OpenAI gym website, we have several experiments.
4551308	4558768	A	0.8945363759994507	There the classical reinforcement learning examples like the lunar lander that you see in this screen right now.
4558854	4565680	A	0.5017215013504028	So active inference from its inception has faced problems of scaling to tasks.
4565760	4571296	A	0.7833577394485474	And that's in itself a field of research in active inference, scaling active inference.
4571328	4577256	A	0.7590183019638062	And that's one of the reasons why deep active inference took over dealing with tasks like this.
4577358	4584564	A	0.650753915309906	So there are benchmarks even now where the sophisticated inference may not be able to deal with state spaces.
4584612	4586360	A	0.8358684778213501	And personally, that's my research.
4586510	4587408	A	0.7556644082069397	In my PhD.
4587444	4598524	A	0.5415791869163513	I am actually looking at optimizing computations in sophisticated inference algorithms that lets you scale up to environments like that.
4598642	4611344	A	0.7093877196311951	But to get started, you will have to kind of write code and see if it works for an environment, then look at if it's not working, then you have to look at methods to scale it up and so on.
4611382	4620708	A	0.7839275002479553	So if I am talking about benchmarks, sophisticated inference is as good as any RL algorithms for this state space.
4620874	4624932	A	0.9433615803718567	So for small problems, sophisticated inference will work and it's really good.
4625066	4635492	A	0.4931679964065552	But for high dimensional problems like this, the classical implementation that I just showed might not work, but it's good enough for any decent experiment.
4635556	4644284	A	0.7411588430404663	But if you want to scale up, then that's still open and it's a new field of research and what you do might become a next new important paper.
4644402	4647596	A	0.8305165767669678	So that's all I can tell in that regard.
4647778	4648268	A	0.84200119972229	Cool.
4648354	4650544	A	0.6885477304458618	You have to work and see what.
4650582	4663380	B	0.8416628837585449	Measures do you think you'd be looking for, like computational resources or what are the measures that even make sense to juxtapose such different methods?
4664040	4672544	A	0.8064594864845276	Yeah, so the OpenAI Gym was designed for that, to compare different algorithms.
4672672	4677188	A	0.823692798614502	So OpenAI Gym by definition is a collection of many environments.
4677284	4680308	A	0.8858309984207153	So in my demo, I was talking about the grid environment.
4680484	4690890	A	0.5364482998847961	Openiigm is nothing but a collection of many environments which will let you interact with those environments using the environment step function.
4692620	4693224	A	0.5491447448730469	Yeah.
4693342	4698424	A	0.7351751327514648	So here we have the environment step function that will let you interact with the lunar lander.
4698552	4706736	A	0.8063665628433228	And that particular task will have matrixes that lets you judge how good or bad your algorithm is.
4706838	4716484	A	0.8044427037239075	So in this lunar lander problem, how optimally can you land your rover between these two flags by spending minimizing the fuel and so on.
4716522	4723732	A	0.8653275966644287	So those matrices are very tasks specific, and that's one direction you have to take.
4723786	4741564	A	0.5396076440811157	You can take try and compete with RL algorithms in matrixes, but the right potential or the potential I see in sophisticated inference is modeling intelligent behavior, where in RL the focus is to get things done to make this work.
4741682	4746072	A	0.5969836711883545	But it's not really explainable, especially deep RL and deep learning methods.
4746136	4755392	A	0.514451265335083	But in active inference, if you manage to scale it up, they are explainable and that will let you understand how intelligence emerges with time.
4755526	4767860	A	0.6169130206108093	And I see that more interesting than competing with RL, because if your focus is getting things done, then maybe engineering is the right way and not active inference.
4769880	4770532	A	0.9184247851371765	Awesome.
4770666	4773300	B	0.8044846057891846	Any other comments or thoughts?
4782070	4784710	B	0.8711403012275696	Aswin, do you have any other comments or thoughts?
4785290	4787494	A	0.8969833254814148	No, I'm pretty happy.
4787612	4790214	A	0.6933611631393433	I hope I was clear explaining the code.
4790252	4800246	A	0.7315627336502075	Maybe it was too complicated or simple, depending upon your level, but I hope it is useful to at least one person who would start using this and write the code.
4800428	4802022	A	0.9892724752426147	Thank you so much for your time.
4802156	4802550	A	0.9184247851371765	Awesome.
4802620	4803960	A	0.96186763048172	And thank you for the opportunity.
4805450	4806634	B	0.9637842178344727	Thank you for joining.
4806682	4807550	B	0.6996942758560181	Till next time.
4807620	4808400	B	0.639849066734314	See you.
4809010	4810046	A	0.9793016910552979	Thank you so much.
4810148	4810410	A	0.5137447118759155	Bye.

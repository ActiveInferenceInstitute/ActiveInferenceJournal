SPEAKER_00:
Welcome back, everyone.

This is our third session of the Applied Active Inference Symposium with Professor Carl Fristen, hosted by the Active Inference Lab, and it's June 21st, 2021.

We're here representing the DOT Tools Organizational Unit of the lab, the third organizational unit in the lab.

And the goals of DOT Tools is to enable

Effective tool and instrument use for all active inference lab processes.

So that's just using the digital tools affordances that we have better, as well as exploring and designing affordances for our niche, modifying our niche, resulting in effective action as well as innovations in tool development.

As with the other groups, we've been meeting weekly in Tools and having a lot of awesome insights related to where Active Inference might come into play, and that's what we're excited to talk to you about.

Some of the core insights from the work in this unit relates to learning by doing, the recognition that modern systems are cyber-physical, everything is really intercalated with the digital,

And also we found it really refreshing kind of like a two-stroke engine to be sidestepping or complimenting or augmenting some of these philosophical discussions with technical clarifications.

And two ways in which we've seen that play out on the left here is a quote from you during a 2019 Dropbox blog post.

when you wrote that technology is the natural extension of active inference beyond the single person, which of course brings technology far from being something artificial into the realm of extended and embedded cognition in our niche.

And then on the right side is a slide from a very recent talk by Bert de Vrij on Beyond Deep Learning, Natural AI Systems, speaking to several applications in hardware and software of active inference, for example, gesture recognition, robotic navigation, and also audiometry for hearing aids.

And one effort that we're starting up now is a NetHack challenge.

It's kind of a video game played in text characters, and we're assembling a team with already multiple interested participants to get an active inference agent on the playing field, so to speak, and have people

maybe update their generative model when they see that it doesn't have to be a 3 billion parameter neural network trained for six months on the GPU, but what if it's enough to just be curious and to want to succeed?

Those are the kinds of things that motivate us in .tools.

So we can start right off the bat with asking, how can we use Active Inference to structure the process of innovation and tool development?

And how can Active Inference concepts help us design for complex agents that are interacting in complex niches?

For example, thinking about niche modification, extension of affordances, reduction of uncertainty or structuring of communications.


SPEAKER_02:
Again, great questions.

So the use of active inference to structure the process of innovation and tool development, that is in itself an entertaining notion.

in the sense that you are a realization of active inference.

I'm mindful that your nice use of the combination of, well, the emphasis on curiosity as the imperative that drives most of our behavior is exactly the imperative that as a scientist drives me and most of the people I know.

And in a sense, I would imagine, also drives your initiative and your laboratory.

So all the questions you are asking are really, how do I make the next move in order to resolve uncertainty about your particular project?

model of how, say, artificial intelligence or human communication is going to evolve.

So in that light, I think there are two levels to the answer.

The first one is just to celebrate and acknowledge that you are engaging in the scientific process as formulated by active inference, that you are on a journey of trying to satisfy curiosity that will be never ending.

And that speaks to one of your themes in the previous slide about learning is doing.

The only way you're going to resolve or sate that curiosity is to go out there and see what happens.

And that is exactly the right thing to do.

A more practical level answer, though, I think speaks to the tool development.

Because one of the, one of the fundaments of active inference is the appreciation that if you just want to maximize the likelihood that this, your kind of inference

world model or generative model that entails the way that you exchange with and interrogate and ping a world is the right world that is articulated out there in the sense of extended cognition for example

in terms of the software tools or the educational tools that you're making available, then all of this is still subject to the imperative to minimise complexity.

So in maximising the likelihood that these tools will be out there,

And in a sense, you're saying this model, this way of narrating the way that the world works provides an accurate description that is as simple as possible.

So you cannot escape the complexity.

I'm speaking like JÃ¼rgen Schmidhuber now, which is a good thing in this instance.

So that means you've got to find the simplest tools.

And it's interesting that you highlighted Bert DeVries' contribution because, you know, again, just practically thinking, what's the game here?

The game here is to write down

the or find the best hypothesis, the best explanation for my lived world and my me could be active inference labs and the lived world is everything that you have to engage with in terms of educational or commercial or academic partners.

So you've got to write down, you've got to explore the model space in terms to find the right model, generative model of the way that you're

your system or your organization works.

The first steps in writing down the generative model are basically to define its structure in terms of the sort of hidden factors and latent factors and their interactions and all that good stuff.

But it has to be done in the simplest way possible.

So what's the simplest way of writing down a generative model?

Well, it's to write down a Bayesian graphical model.

What does that mean for the actual coding practically?

and the software schemes and implementation that you would either offer to people or pre-package in terms of user interfaces, that it's going to be message passing on those graphs.

And I'm trying to get back to Burt De Vries' Factor Lab, Forney Lab formulation.

So to my mind, that's the simplest

most generic bit of computer science that you would come across in the service of finding the right software tools to build absolutely everything, because absolutely everything can be written down as a generative model.

If there's a generative model there, there's a Bayesian dependency graph.

If there's a Bayesian dependent graph, you know there's a factor graph.

If there's a factor graph, then you know there's a message passing scheme.

What is that message passing scheme?

It's just a free energy, variational free energy minimizing message passing scheme.

I would imagine that as tool development increases that there will be a move towards a common language that will look very much like BERT's phony style message passing and within that

You know, you've got very limited choices, which is a good thing because that again speaks to this minimization of complexity and just coarse-graining the world and your world at its coarsest level that will sustain an accurate account or a precise account of what you want to achieve.

So I'm thinking here, the tools just have to come in two flavors.

They have to deal with continuous state-space generative models

uh to interface with you know of the kind you need for robotics um but also uh the other flavor would be discrete state space your latent states um discrete late states uh models uh that you need to do say computational linguistics or you know modeling um the climate in in various states um

and we know that the um we know all the message passing schemes that would be entailed by a commitment to one of those two kinds of models um in the sense of sort of generalized bayesian filtering for the continuous state space um and by generalized you know is you know i include uh generalized courts of motion uh which generalize things like calvin filtering

And on the discrete state space side, you're talking about either belief propagation or variational message passing.

So when you just think about it, what you have to do in providing tools of a software kind or simulation kind, happily, there aren't many choices you have to worry about.

So, you know,

You know, in that sense, all you need to do is to make sure that your tools accommodate both generalized Bayesian filtering and bleep propagation and or variational message passing.

And then you're using off the shelf technology, which brings us back to, well, what's the real problem then?

Well, the real problem is writing down the generative model.

what sort of problems, how would you unpack those problems in terms of innovation and tool development?

Well, it's solving the model selection problem.

So sometimes I think when describing the space of problems that are faced, say, with generalized AI or AGI, you can unpack them at different spatial temporal scales into the inference problem.

into the learning problem and into the selection problem by which I mean using Bayesian model selection to get the right structure you know do I use six or twelve layers in my deep neural network do I use a convolutional model or do you use a transformer you know these are basically problems of

that are solved if you have a mechanics that can score the structure, enable you to select the right form.

So that I think is going to be a focus of innovation in the, it already is, but certainly in the near future in terms of development

And in the sense that I think the inference and learning problems, they're solved problems that you can just go to BERT and get your favorite message passing scheme, or you can keep at the level of educational or academic message passing and use the MATLAB schemes that we generate here in London for toy problems.

And what is not, I think, a solved problem and will require an innovative solution is the structural learning problem or the selection problem.

exploring not the right hypothesis, but in the principled way, exploring the space of generative models you might want to bring to the table.

And that has many, many different issues and things that come to mind, of course, that you could do it in

in a bottom up way by trying out new hypotheses.

Where do you get those from?

You get them from experts in the field because effectively they are bootstrapping themselves on the basis of our prior beliefs or our knowledge about how something works.

You can do it in a top down approach by

having over-parameterized, over-expressive models, but with very weak, imprecise parameterizations, and then use Bayesian model reduction to solve the selection problem.

These are ways that people are thinking at the moment.

this thinking is innovative because I don't think there are any clear answers.

So how would you use active inference to solve the structure learning problem when in a sense it's already being used in the sense of Bayesian model selection as natural selection but you really want to speed that up and make it work within your commercial academic lifetimes but I would imagine the exact same principles would be brought to bear there.

And, you know, that almost answers the next one.

How can active inference concepts help design complex agents interacting in complex niches?

You just have to build these things as proof of principle and hypothesis testing.

And the nice thing is, you know, all the

the machinery and the tools that would be requisite in building these things right from the variational message passing using, say, Forney Lab through to now you've got the right fitness function when it comes to using, say, a genetic algorithm to explore a structure space.

And what is that fitness function?

It's the evidence lower bound or the variational free energy.

So you've got all the maths in place.

This is a question that I think of simulating these things and providing proof of principle.

How you would translate that into the real world, I don't know at this stage, I'm afraid.

I think a challenging first step would be to actually use robotics in silico or hardware.

or possibly a lot of excitement at the moment of using soft robotics.

And actually, you design your niche and see what happens and then turn your attention to niche construction, where you now acknowledge that the niche itself is also succumbing to the principles, not of active inference in and of itself,

in the sense that niches don't plan.

But certainly in the FEP sort of vanilla free energy minimizing approach.

So yeah, I haven't actually thought about that before, but that's an interesting asymmetry when it comes to simulating multi-agent interactions in the context of niche construction, where often it is the case that the niche is just the other agents in an ensemble.

But if you now actually include the environment as part of the niche that is playing host to all the denizens that are the ensemble of active inference agents,

There is this distinction between the ability to plan the consequences of action that would entail optimization of the expected free energy versus simply reflexively minimizing surprisal by minimizing free energy as an evidence bound.

And put that more simply, more intuitively, you're either with generative models that support planning or not.

So there's nothing fundamentally different between these approaches.

It's just if you've got a generative model, that is a model of the paths into the future, consequent upon how you act upon the world.

That's a much richer, deeper generative model than the kinds of generative models that would be applicable to a thermostat or an environment.

And it's likely that the environment,

I have in mind here, which is a warehouse that you've got a sentient robot going around trying to get the right things.

So the robot can plan, but the environment, the niche can't.

It'll still conform to the principles, the free energy principle.

There will still be particles and things that are conserved and they will still fall and behave in a predictable way.

There may even be a thermostat controlling the temperature.

None of these things are planning.

So there's an interesting asymmetry that gets into the game when you're talking about complex agents interacting in complex niches.

Part of that complexity has to be a specification of whether the complexity entails planning or not.

And it just creates different problem spaces, certainly in the context of multi-agent simulations.

So that's how I would

sort of carve up the problem spaces in terms of implicitly problem spaces that will only be explored by doing.

And by doing, I just mean actually realizing physically these processes in the kind of situations that you think are going to be useful for the future.


SPEAKER_00:
Thanks for the answer.

And it's really fascinating about using simulation

that selection can happen within the generation of for example a startup rather than between generations because of course we can let organisms or startups proliferate and then let pruning occur at the generational scale or there could be ways to design so that that selection occurs within a generation more like learning and development rather than intergenerational selection so awesome points there

This could be a broad question, but we're curious what areas of applied active inference you think just might be exciting, promising, or important?


SPEAKER_02:
So my personal usual response to this comes in two flavors.

The first is from the point of view of a

theoretical biologist and a psychiatrist so if you know if you can understand how a normal sentient artifact or person behaves then that creates a space in which you can think about

false inference and false learning, or certainly suboptimal from the point of view of minimizing surprise or free energy.

So that's a fancy way of saying understanding the computational basis of psychopathology.

So there's a whole literature

on using active inference as a, if you like, a normative framework within which to provide an ontology of false inference or failures or aberrant active inference.

And why would you want to do that?

Well, if it can all be reduced just to the good belief updating and the good message passing,

we actually have quite a comprehensive understanding of neuronal message passing and all its physiology and all the roles of various neurotransmitters and microcircuits and neuroanatomy that underwrite that kind of neuronal message passing.

And implicitly, we also then have a fairly

a fairly fine-grained understanding of the role of neurotransmitters and the consequences of pharmacological interventions in the context of experience-dependent learning and inference of the kind we've been talking about.

So from a translational perspective,

literally translating the formalism on offer from active inference into the clinical domain, that would be one motivation for developing this theoretical framework.

The other one is more in the line of technology and artificial general intelligence.

So then the question is, well, if I now want to build sentient artefacts

not only build them but build brothers and sisters so they are complex and interact and learn to love each other in a complex environment that could include me.

Then you've got a clear offer from Active Inference as to the design principles you might want to use to actually build these artefacts.

And then there are interesting questions about

what kind of artefact do you want to build?

And we've already discussed the difference between a thermostat and a sentient robot going around collecting your next sort of home delivery.

There are different kinds of generative models.

So now you ask the question, OK, what are the exciting and promising

kinds of artifacts as defined by their generative models that one might expect to see in the future.

And then we get into the world of generative models that support planning.

So we're talking about deep generative models where they have a temporal depth.

What are the next stages that you might be looking at?

Well, there's also a sort of hierarchical depth.

that would at some point, first of all, include the capacity to deploy precision.

And why is that important?

Well, as soon as you have the deploying the precision,

as a process of inference, you have now a normative theory for this kind of mental action or covert action.

So one example of this would be, I don't know the technology, but I can be assured that I know

I know what it's trying to do, but thinking about transformer networks and the way that attentional selection operates in this context, what you're saying is you can actually optimize the attentional selection as an inference process using active inference or an evidence lower bound.

where you're now predicting what things to attend to and what particular ways to switch on and which ways to switch off.

And at that point, you can understand that as mental action.

So when the transformers or variational autoencoders start to now

optimize their estimates of the posterior precision at lower layers say in an autoencoder it's now acquired the capacity for mental action and it now will pay attention to various representations and possibly even various data sources that's not magical we do that every day in the sort of mdp active inference

and use it to explain a lot of the attentional mechanisms implemented in the brain.

If you can migrate that technology into deep learning, you would have taken one baby step towards true sentience, which is mental action.

The next step would be, OK, so how can I now minimize the complexity

of my generative model, where my generative model now actually includes this meta-inference in the sense I'm now providing predictions about my inference because I'm controlling the precision of hierarchically subordinate message passing.

And at that point you start to think, well, perhaps one simplifying, one way of simplifying the computational complexity or the complexity part of the inference would be to carve up different states of intentional deployment in exactly the same way we're talking about carving up people into Biden versus Trump voters.

you know, a simple, stable, complexity-minimizing carving up, which suddenly suggests to you that you can now equip an artifact with states of mind, so that they can be in four states of mind.

They can be happy, they can be sad, they can be confident, they can be unsure, and they will have to infer, given all the evidence at hand, including the message passing through in the hierarchy, what state of mind it is in.

And if you now include in terms of the sensory evidence, you know, the voltage on their batteries or some measurement of their interception, you now have something that's getting very, very close to, say, Ryan's notion of emotions.

So now you've got a part of the generative model is now inferring

what state of mind am I in as the best explanation for all these interceptive embodied sensations, not just the proprioceptive state of my actuators, but also are they getting a bit sticky?

Is there some wear and tear?

Are my batteries charged?

All of these things come together as part evidence in conjunction with all the usual visual, the radar, auditory, sort of acoustic inputs to actually supply evidence for a post-trip leave.

I'm in this state of mind.

I'm anxious.

My battery's running out.

This immediately creates different prior preferences, cost functions, if you like, that would be applied to your policies because you've got a deep change in model that plans into the future.

So now you've got an artifact that not only has the capacity for mental action, it's now got the capacity to be in different emotional states.

The next step is to say, hang on, so there are these different states.

Can I now equip it with a minimum selfhood?

Can the hypothesis that I am actually an artifact

provide empirical priors that reduce the complexity of my message passing at subordinate levels, that is generating, that is inferring the state of mind that I am in, that in turn optimizes the postures of the precisions of various likelihood mappings or preferences over policies.

So at this point, you're starting to get to artifacts that could have minimal self-awareness.

The next stage would be that's only going to be ever, that's only going to be, I think, useful when you consider dialect interactions again, because the only rationale for having self-awareness is to disambiguate self from other, which means that there must be some confusion or some uncertainty

at hand in order to justify the resolution and certainly justify that complexity of the model, which means that you have to be interacting with or exchanging with things that are sufficiently like you to license the inclusion

in your generative moral of a self versus other or that you are like me or not like me so we actually come now back full circle to what we're talking about before in terms of inferring who I am I talking to so this I think there's you know structurally something quite fundamental about this influence problem

are you a creature like me or not or are you like one of those are you a pet are you a plant you know just being able to carve up this world in a way that is self-referential

necessarily entails a minimal selfhood in the inferences of these that speaks to the importance of getting the necessary evidence from the environment that would be, if you like, license that degree of complexity and the only kinds of environments that can supply that degree, that license that degree of complexity

are when that environment, that eco-niche, actually comprises other agents like me that make it, if you like, worthwhile me inferring, oh, it's me, not you doing that.

So I would imagine then

most promising applications of active inference in constructing sentient artifacts pets and you know carers or people things that you can converse with and would be to grow them certainly with themselves but more importantly with you there so they can learn by their doing with you there so they

they want to they're curious about you and you're curious about them at that point one could argue that's the only scenario which you're going to have any empathetic interaction with these artifacts so I'm sure there are other applications in terms of climate change or commerce or whatever but in terms of imagining what you could produce you could sell

I would imagine that a mindful robot that actually is curious, genuinely curious about you, because that will teach it something about itself.


SPEAKER_00:
thanks for that answer um the idea of tools for attention and of design and engineering for regimes of attention to use an active inference term is really essential and what you were talking about there with the phone first off before the internet when there weren't other devices of similar kind there was no need to communicate out and what we've seen is that as there's more and more devices of similar or interoperable kinds new levels of organization have to emerge

And then I thought about the anxiety that a person might feel when their phone is running low on battery.

Right now, that sensor reading is getting emotionally offloaded to the human.

So we could have that anxiety on device.

So let's have a more relaxing relationship with our phone.

And then as you pointed out, it would be the incipient steps of selfhood, or perhaps what they could even call a self phone, if I'm allowed one pun per symposium.

The next question is, what kinds of tools have been most helpful in your work in research, which includes many areas such as SPM and DCM that a lot of people who are just learning about active inference might not be very familiar with?

And what kinds of tools don't exist yet, but might be helpful for active inference work?


SPEAKER_02:
So the mathematical tools, and I'm often asked this question of students, do I need to be able to do maths to contribute to this field?

And if so, what kind of maths?

I won't tell you what my answer is, but what I have found useful is certainly mathematics, but not necessarily very high end.

This is always Wikipedia level mathematics, and in particular,

Dynamical systems theory, information theory and linear algebra are probably all you need to do everything really.

And indeed, you could read most of quantum electrodynamics as basically linear algebra with a bit of probability theory underneath it.

So that has been the mainstay, that is, you know,

if there is one tool, that it would be the tool and the language of maths and relatively simple maths.

And the second thing is to, the learning is doing, you know, see one, do one, teach one ethos, I think applies very pragmatically in this context.

which means it's actually very useful if you can get students to actually build their own little simulated artefacts and even more useful when they can actually code it up themselves, which means you need access to a high level, at least third generation programming language that a student can get fluent with should they want to.

uh not only to use the existing tools but you know try and write it down themselves without having to spend years training as a computer scientist so i found MATLAB very useful in that respect not because it's terribly efficient although i have to say actually some of the

matrix operators and under the hood tensor operators are actually much more efficient than people give them credit for because it actually came from X-ray crystallography.

However, what's really useful about it is it uses the same syntax that you would find in a book on linear algebra.

which didactically or educationally is really quite important when it comes to writing and reading the code.

So we have deliberately stuck with MATLAB, not because it's computationally efficient or that it's open source, it should be, I don't think it is, but simply because it's configured in a way that people reading standard texts, 101 texts in the new algebra and the like,

would be able to see how it transcribes into a computer language so that's been a really useful a really useful tool and looking ahead I imagine that you know one's going to need open access and possibly more it's I don't know it could go either way I'm just thinking about first of all people like Burt and Forney Lab in terms of very generic

very high end specifications of message passing in computer science.

It may be that that's the level you want people to actually compose their generative models and their artifacts, and they don't even need to know about linear algebra and even less information theory.

What they need to know is the language of relational, the object relations and how to specify just different classes of

exponential probability distributions and, you know, is it categorical?

Is it continuous?

Is it always positive or can it be positive and negative?

And that may be quite sufficient to write down a factor graph or a generative model and then everything else is just off the shelf and it can write itself.

So that would certainly be possibly helpful in the future.

Sorry, I'm moving on to what kinds of tools don't exist at the moment.

So I'm thinking of a

I never used it, but what I imagine would be BERT's phony lab facilities offered as an application or a user interface that allowed you to compose generative models and then just hit, compose a generative model, compose a generative process, the actual world that's going to be modeled, and then just click run and see what happens.

That would be really useful, I think.

Having said that, the other side to future scoping here is, I repeat, leveraging more specialised or other fields, amortising certain parts of inference, learning to infer, or indeed inferring to learn, or learning to plan, or learning to infer how you plan.

so or starting to sort of see what parts of the inference process are so conserved that they could actually be amortized and learned and certainly that looks as how as though that's what the brain has done for example there are people who think that the cerebellum

has basically learned how the motor cortex does its online KL control of Kalman filtering and therefore lends a fluency and a computational efficiency to the message passing

which in its absence doesn't mean you can't do something.

It just means you can't do it as fluently and as gracefully and as quickly as you could with a cerebellum.

Indeed, when you have a cerebellar lesion, all that really happens is you become a bit clumsy and slow.

Those kinds of tools, a quick and cheerful integration or importing various amortization and deep learning technology into a Forney-style message-passing scheme that could support any kind of generative model, I think would be really, really useful.


SPEAKER_00:
Awesome.

Thank you.

Approaching this nexus from another angle, what kinds of tools and platforms could inform transdisciplinary, highly contextual, and engaged teams that are working with these approaches?

we hope to be working with others to be developing the active inference curriculum and body of knowledge more broadly.

But when teams are actually using these kinds of approaches, what kinds of platforms might exist to enable their work?


SPEAKER_02:
Again, I have a strong suspicion that you know the answer to this much better than I do.

So I'm trying to guess at the answer that you know is the right answer, and I'm not doing very well here.

So, you know,

I think that we've already talked and certainly implicitly in the way that you presented the ambitions and implicitly some of the questions and all the answers are there, whether that's trying to engage through education, whether it's trying to engage through insight using say embodied experience, illustrations of the basic principles, whether it's supplying

games or user interfaces, graphical user interfaces to facilitate the designing and enacting and playing with genitive models and active inference.

I think these are all obvious and laudable ways of leveraging

what Active Influence has to offer.

Participatory.

I mean, the learning is doing thing and the see one, teach one, do one keeps coming back to mind.

And the course completely licenses the participatory aspect.

What kind of participation did you have in mind?

Are you talking about hackathons?

Are you talking about playing games with active inference computers that start to hate you or love you?


SPEAKER_00:
What level of participation were you... Yes, Stephen, do you want to give a quick thought on a few kinds of participation or what does that mean to you?


SPEAKER_01:
yeah one area is quite interesting is in psychodrama they use action methods like action sociometry or spatial activities to look at how people relate to their experience in a dynamic way so physically

So we've been looking at ways that spatial participatory approaches can unpack people's relationships to different niches or different workplaces or different types of embodied experience.

And then that could be visible to be put into active inference type geometries.


SPEAKER_02:
I see.

Okay, right.

There's a great example.

So two things that I've come across before are the architectural design and the importance of

not just sort of pragmatic affordances, you know, can I walk up there, can I sit there, but also the epistemic affordances, you know, if I look over there, what would I learn about the space around me if I go around that corner?

So there is, you know, embryonic interest in my world from the architectural sciences and architecture

in and of itself that could in principle be motivated.

It's an odd discipline because it's half like art and half like science, but some of their ideas are very much aligned with certainly Gibsonian notions of affordance and also the affordances, the dual aspect affordances brought by expected free energy under active influence.

So it's not just, you know,

Am I the kind of creature that can sit on this particular chair?

But also, what will I learn if I so do?

And so things become epistemically attractive to engage with.

The other domain is in entertainment and in music and in particular,

the joy of synchronization and mutual predictability or minimizing free energy through mutual prediction when singing or dancing together or indeed interacting with a slightly greater asymmetry in terms of being a member of an audience watching a band for example so you know

one of the key things that comes out of that kind of research in ways of measuring the implicit generalized synchrony that you get from having this information geometry that I was talking about before

that rests upon there being um a synchronization manifold between the inside and the outside but if the outside is another inside from another person's point of view what you now have is something called a synchronization manifold so there's a mathematical image or space um to actually talk about mutual inference and mutual active inference and engagement and communication

singing together, for example, or diachronically exchanging messages that does actually translate mathematically into movement and belief updating on a synchronization manifold.

And that has real-world correlates.

You can measure that using kinematic measurements.

You're putting LEDs on people who are dancing together, for example,

measuring heart rate variability or galvanic skin responses or doing eye tracking or indeed EEG and start to so there's quite a lot of work in in things like hyperscanning and in in it you know sort of ethology and dance disciplines where in the arts in the life sciences

where they do use a lot of these techniques to quantify the degree of generalized synchrony, what it would be nice to do is actually try and model that synchrony or understand that synchrony in terms of movement on the synchronization manifold, which is sort of the mutual belief updating.

And one thing which comes out of that just in discussion, if no further, is the reciprocal, the circular causality that is necessary

to maintain that generalized synchrony.

The particular synchronization manifold we're talking about from the point of view of active infants, of course, is mediated across the Markov blanket, so the active and sensory states.

But in general,

you need to have reciprocal coupling in order to get synchronisation, so directed coupling doesn't work.

And if that's true, what that means is that engaging as an audience, for example, or participating as a spectator,

will only really work in terms of establishing that generalized synchrony that you are chasing.

And why you're chasing it?

Well, as soon as you have a generalized synchrony, you've got predictability for free for all.

And that's a good thing because that minimizes free energy.

The more predictable you can make the world, the better it is from the point of view of free energy.

But you can only do that if, as a member of the audience or a witness to something, you can actually actively intervene on it.

So that brings to mind, how could you get, for example, discussing this with friends of Maxwell, if you wanted to promote virtual concerts online, for example, during the pandemic?

What you don't have online, which is what glues things together, things like mosh pits in sort of carnivals and festivals, is you don't have the audience participation, the applaud, the roars, the lighter waving or the light waving.

So how would you get that back into a virtual experience?

Because that would be absolutely essential.

you know i think to actually engage people otherwise you know you you'll be you'll be looking at a pop concert on television so you know more than just if you like um revealing the underlying correlates of that generalized synchrony in terms of the eeg traces of the the dancers or um doing some sensory mapping from um

their motion to auditory input, just making the sensory evidence that supports the mutual inference more precise and more available just by having it displayed, say, by putting motion in sound or sound in motion.

or EEG, electroencephalographic measures of performance or the audience visualizing that.

And that has been done by people like Paul Boucher in Barcelona.

More than that, to actually enable the audience to change what the performers are doing.

You have to make, you know, or perhaps what other members of the audience are doing.

So you have to empower them to close that that circular causality to get that dynamical coupling place.

So you get you get the right kind of generalized synchrony so that, you know,

That sort of dynamical systems perspective on synchronization and free energy minimization certainly speaks to a particular kind of participation and engagement that does indeed rest upon action-oriented approaches.

But crucially, it's the action of the audience on the performers, not the performer's action on the audience that is usually what you need to pay more attention to.

Was that the kind of thing you were thinking about?


SPEAKER_01:
Yeah, that's really a useful answer, actually.

Yeah, we were thinking about that and some participatory immersive theatre type events and other participation in collective meaning making.

So that's the type of thing that we're looking at.


SPEAKER_00:
And it reminds me of the livestream affordance, which is relatively novel, but allows people to be asking questions and it enables not just efficient production of material in a one-shot approach, but it allows the feedback.

And I can't help but add that it's that affordance for participation, for example, Speak Now or Forever Hold Your Peace,

that expands the wedding into the community because there is the opportunity for feedback.

It's not just a breakaway click.

It's actually something that remains integrated through the affordance for participation.

So I'll turn to the last question for this section.

How might future modeling involve large scale patterns in social data sets and working backwards to infer their hidden causes?

For example, in the case of pandemic modeling, governance, economic, other situations.


SPEAKER_02:
Well, this is a very practical and very pressing question because, of course, a lot of people are asking themselves that now, specifically with respect to pandemic models, but also the people who are exercised and have the interventional clout when it comes to COVID are generally also the people who are invested in climate change problems as well.

So there's a lot of noise out there at the moment about

how we can harness the data simulation and modeling advances made during COVID-19 and keep the momentum up to tackle climate change.

Not just climate, but the economic structures and financial structures

and informational structures that are deeply interwoven in terms of climate change.

My answer is going to be somewhat deflationary.

And I've had this kind of conversation before, again, with Maxwell and John Klippinger and related friends.

And I'm due actually to have another conversation with them on certainly the open world

in the near future.

There's a temptation to take all the high church of the free energy principle and active inference and epistemic foraging and all of that good stuff we were just talking about and say, oh, well, now let's make it work in terms of understanding, say, the pandemic.

and you don't need to do that all you need to do is to apply the good scientific principles that things like active inference appeal to to the problem at hand and it all comes back to the generative model so you know all you're saying here is how might future modeling involve large-scale patterns and social data um to infer that the um hidden causes is just a statement of we need

the right generative models to make proper sense of the big data at hand.

And in saying the right generative models, we need the equipment both to invert those models in the sense of inferring

the parameters, the interactions, using the simple tools that we've just talked about.

They will just be lifting it from the laboratory or continuing to use MATLAB.

But the bigger problem is one we talked about, which is the selection of the structure learning problem.

So this goes beyond just

How many layers do I have in my deep network?

Much more important, I think, is the factorization.

It's knowing how many conditionally independent factors do I need to minimize the complexity to get the right granularity, the right way of carving up.

the latent causes behind all the data that is available to me.

So I think the pandemic modeling is a beautiful example of this because the factors that determine whether I infect you

It can certainly be written down in terms of virology and the ACE receptors, ACE2 receptors, and basically production numbers and transmission strengths and transmission risks and the spike proteins.

But that's only half the story.

The other half of the story is how likely are you to be at work or at home when I'm at work?

Are you likely to be wearing a face mask?

Are you going to, are we going to be one or two meters apart?

So all these behavioral aspects start to become really important factors.

And even beyond that, when it comes to making sense of the model, the likelihood part of the model that actually generates the data can become extremely difficult to optimize when you start to think about

what kind of data is at hand.

For example, just notification rates of new cases per day of coronavirus.

Now, you might think, oh, that's really great data.

It's really difficult data to handle because the different kinds of tests not only have differential false positive and false negative rates, but there are different ways in which they are deployed.

really confounds that in terms of the selection bias.

So are you testing people who are symptomatic?

What's the probability of being infected if you're symptomatic?

Are you not?

Are you doing survey testing?

Are you doing the same amount of testing this week as you were doing last week?

All of these, what would be, if you like, from an epidemiological or a behavioural science perspective, really uninteresting factors, suddenly now become the most important

factors in making sense of those data.

But you only know that when you start to do the model comparison and the structure learning, when you actually commit to writing down the congenital models.

And that's certainly what I've learned over the past year, now a year and a half.

The future of modeling is, first of all, it's obvious what the future is.

It's basically writing down the right kind of dynamical state space models that account for data.

But the future is really dealing with the problems of structural learning and model selection for any data, but in particular for the big data.

at hand in terms of pandemics or trafficking on the web, you know, or climate change.

So it's a really exciting opportunity.

Why do people want to do it?

Well, once you've got the most evidence, i.e.

the minimum free energy model at hand, and you've got posteriors over all the model parameters and all the right interactions,

Then you can do all sorts of stuff in terms of reducing people's uncertainty about the future because you've quantified the uncertainty and explained to them things that were once uncertain about and what isn't uncertain about.

That has enormous implications for mental health and well-being.

and possibly even feeding back into finance because you always hear, well, the biggest determinant in terms of the markets is the market confidence.

It's all about the uncertainty.

So if you can do uncertainty quantification in a principled way using this kind of modeling, you've done a big thing already.

But then you come to monitoring putative interventions.

You've now got a direct handle posterior estimate on the latent states you actually want to make decisions on.

So it's not the notification rates or the number of new cases in California today.

It's a number of new people that have become infected today.

And that's a very difficult thing to

infer given all of these complicated aspects of the generative model.

And then, of course, once you've established the validity of this model in terms of its construct and predictive validity, then you could intervene on it.

Then you can say, well, what would happen if I changed this?

What would happen if I changed that?

um and what would happen now what would happen in the future so that you know you're suddenly in a world of quantitative modeling um um where you can start to um ask some very powerful questions and also um share with everybody who matters and the products of your inference so you can now start to think about having supplementing the weather forecast with the

an epidemic forecast you know the virus in your area and tomorrow we expect you know you can also do that to the markets and these kinds of things I think are going to be more important when people or when the current generation

get to your generation, I guess, and start to wrestle more with climate change because they're going to want to not just know whether it's going to rain tomorrow.

They're going to want to know at the level, not just the weather, but the climate.

What are the indicators?

Because those indicators really contextualize and

inform their generative models about their place in the world at that global scale.

But to provide that kind of weather forecasting, that meteorology beyond the weather, you're going to need to have these state space models properly optimised in a first principle way in relation to the marginal likelihood or the

their evidence bounds.

I've just read governance here because governance is just policy decision-making based upon counter-patron outcomes.

So that is always underwritten by these Bayesian beliefs, but you can't get the Bayesian beliefs unless you've got a generative model that has the consequences of action in the future.

There would be also interventions either politically or financially or otherwise.


SPEAKER_00:
Thank you so much again for joining this symposium.

It was really a special moment for the lab and we look forward to continued interaction.

So much appreciated and we will see you all soon.

Thanks for everyone who's watching and we hope that you participate in ACT-INF Lab.

So thanks everyone.

Bye.
SPEAKER_05:
hello and welcome to the active inference lab to our first ever applied active inference symposium today it's june 21st 2021 and we're very honored to be here with professor carl friston and many of our lab participants

Just as a way of quick introduction, the Active Inference Lab is a nonprofit organization that is a participatory open science laboratory.

And we're working to curate and develop applications related to the Active Inference framework, something that hopefully we'll be going into a lot more in detail today.

And this is a screenshot of our website.

As far as the overview of the symposium, there are three organizational units in the lab, .edu, education, .coms, communication, and .tools.

And each of these units are going to facilitate a 45-minute or so session, and we'll have a short break in between sessions.

So in our weekly meetings over the past weeks for each organizational unit, we've been developing questions and getting excited about things that we wanted to talk to you about.

As far as a few overarching themes that were kind of spoken to,

really through the whole journey of our lab, but also across organizational units.

The first theme is applying active inference across systems.

Again, something that will come up probably in all sections.

The idea of research debt, the idea that we don't want to be developing

Research frameworks that have a huge burden on those who are learning and applying, and that especially early in the formalization of frameworks, it's extremely valuable to increase the accessibility so that we don't end up with major headaches and incompatibilities later on.

Collective intelligence and the ways in which that is manifest across different systems.

transdisciplinary teams, projects, and communities, which are kind of like nested levels of organization, but transdisciplinarity is something that is necessary for the type of work that we're all interested in.

And also just modern challenges and opportunities for research and all that that means related to online and everything else and

of course, anything else that you have tumbling around and wanted to bring to the table thematically.

So there we are with our sort of lab overview and introduction.

Let's go to our first organizationalunit.edu.

The goal of .edu is to scaffold and create a participatory and dynamic active inference body of knowledge, which we'll talk more about in a second.

And our progress and actions this year

have been to release a terms list v1 which benefited greatly from your feedback and also we're now updating the terms list to version 2 which now includes five complete language translations and many references and citations for the terms

way that we're approaching the development of the terms is by using approaches that place ontology and progressively more formalized versions of ontologies as kind of the backbone of an educational body of knowledge so we started on the left side here with a terms list in the first quarter of 2021 and the ontology working group is like a train that's pushing to the right as they're learning ontology by doing

and developing progressively stronger and stronger ways of relating the terms and the concepts that are essential for understanding active inference.

And this will help us develop principled educational material that's also able to be translated rapidly.

Alex, do you want to give a quick thought on where knowledge engineering comes into play?


SPEAKER_03:
Yeah, thanks.

At this slide we are showing this work with ontology with system engineering approach which we are also using in the lab and considering possible deliverables of working on educational materials and creating them.

We should have at some point of time textbooks and educational courses.

And actually, maybe this lab is started from the idea that textbook for active inference should be created.

Also, we see some connection that can be applied to organizational management for creating translations to make it multi-language from the beginning.

And also we should see for some domain-specific use cases that we can understand in terms of that ontology that we are going to create.


SPEAKER_05:
Thanks, Alex.

So on to the questions section.

We're going to start off pretty broad here in the .edu.

How do we go about determining the core ideas and terms for active inference?

This will be the format of the question slides, Carl, so feel free to jump in.


SPEAKER_01:
Right.

I guess it will be structured around the key ideas and essential ingredients that underwrite the free energy principle and how that translates into active influence.

So without thinking about it too deeply,

my mind just goes to what are the things, what are the basic ingredients you need to explain to somebody what active inference is and why it works.

And it normally starts off with the notion of a generative model.

And then from that,

you spin off all the appropriate mathematical ideas and constructs and descriptions that would attend that.

I mean, it may be best to reflect the question back to you.

So this idea of having an ontology.

And it's certainly my experience that people are entertained by sometimes the poetic use of phrases and descriptions such as epistemic affordance when trying to grapple with what are the fundamental ideas behind active inference.

Some of them are fundamental and some of them are not.

So it certainly is an interesting idea to try and tie down the ontology.

But let me ask you,

This ontology just means what it says in the sense that you're trying to define the essential concepts and how they relate to each other.

Is that the basic idea?


SPEAKER_05:
Yep.

Going back to this slide here, we want to have a continuum from a list of terms potentially that could be developed into coherent and again, principled course material and competencies, but also develop a logic.

And we're developing within the SUMO ontology development framework, which defines not just relational edges, but actually a actual logic

And so we hope to be able to ask, like, is this a complete active inference model?

Have we really checked off all the boxes and used those kinds of logical tools that are accessible to the well-developed ontological frameworks?


SPEAKER_01:
Okay.

Well, that's very compelling and very clear.

It strikes me then that it would be useful to link that operational ontology to the underlying maths.

So much of the...

much of the conceptual steps, both in understanding and implementing active inference, usually in terms of simulating interesting behaviour or using it as an observation model to explain some empirical data from a study, much of it

can be developed in terms of a series of moods that usually, or in fact almost universally, are framed in terms of

either information theory or linear algebra or differential equations and you can just build the story from that so if you're looking for that degree of formal and useful detail then it would be

one principle you might refer to is basically where does one equality assertion or description or variable or object, where does it come from in terms of inheriting from the more basic formalism?

So what I'm thinking of here is where does active inference start and how do you get to

the calculus and the Bayesian mechanics that you would associate with active inference.

And my guess is, given the structure or the way that you have approached the ontology, you've probably actually done that already or are in the process of doing that.

Are you going to go through some examples that would sort of highlight the strategy and the problems, which are usually more illuminating than the solutions that you've encountered so far?


SPEAKER_05:
Sure, I'll switch here to this screenshot of the current state of what it looks like.

And we're starting just in tabular form by compiling up to five references and citable definitions.

First, just looking for exact cases where a term is used.

and then we'll go from how the term has been used towards synthetic definitions that capture different senses of the term and then along with the concise narrative of the field and also ontology experts who are here with us we're going to then be working to

make the actual logical underpinnings elucidated in terms of specifiable code rather than just concise English definitions.

And then from that sort of generator of the formal relationships, we'll be able to descend into mathematical formalisms or other natural human languages.

We'll keep you posted on this project though.

For sure.

Let's go to this next question and imagine that we had that set of terms in development.

It's going to be a work in progress our whole lives.

How would we go from core terms and ideas to an interactive and enlivening education that speaks to people from many different backgrounds?


SPEAKER_01:
So I'm going to answer this question from the point of view of my experience as a supervisor, which is probably a little bit of a narrow remit from your more general ambition.

And I imagine that this is related to this notion of research debt.

I can't remember now, but this notion that you don't want to put too much

pressure on people when becoming acquainted with the utility and application of either the code or the ideas.

So in my experience in an academic setting,

just having toy simulations is usually the best way to give people a feel for what this approach does and how it can be used.

So it's enormously potent in terms of demystifying and also illustrating the functionality at hand or that can be accessed.

It also...

having a sort of, you know, a working, or at least a toy model, sort of provides a proof of principle that can strip away the magic as well.

And I think your ambition to try and make this accessible to people who are not necessarily fluent in the underlying

information theory or dynamical systems is very laudable and perfectly feasible.

So again in my experience some of the most creative applications of active inference can be by people who don't really necessarily wonder too much what's underneath the hood.

It all comes back again to the design of the generative model.

So if you get the generative model right

and it's apt to describe the thing that you want to understand or to simulate, then usually everything else follows suit.

And I mean that in the sense that you can just take off the shelf software, which I presume that your ultimate ambition is to make available.

and make it work in the service of sort of saying well what would this agent or this synthetic creature or person do in exchange with her environment if this was the generative model and this was the generative process.

So a lot of this

A lot of this really, I imagine in terms of answering your question, how do we go from core terms to interactive and enlivening education, is just establishing a language, a lexicon that allows you to talk through somebody in constructing their own simulations that speak to the issues that engage them.

either academically or beyond academia.

So clearly then the core terms play the role of literally a language in terms of communication, which brings us back again to the importance of the ontology.

and having the terms linked in a formal way to the mathematical expressions and also procedures and processes.

So I guess that

a precondition to use the core terms in an interactive and enlivening educative sense will rest upon getting that ontology right.

In my experience, the best way to get the ontology right in the sense of it being

enabling is just to talk about the terms until everybody, until there's some consensus and everybody understands them in terms of either their teleology, well, sorry, no, both in terms of their teleology, but also in terms of where they come from, from the point of view of the code and ultimately the maths that underwrites all this.

Is that the sort of answer you're looking for here or thinking along, are you thinking along the same lines?


SPEAKER_05:
It sounds great.

There's so many dimensions there.

Just to provide a summary or just jump in at one place, what is Active Inference and what does Active Inference do?


SPEAKER_01:
Right.

So that's perfect because I was just thinking it would be really useful just to go down the terms that you had in the

in the previous but one slide highlighted in green, because I think all the heavy lifting here is really just shouting about what are the core aspects and claims or the core things that you're trying to communicate with any one of those terms.

So for me, active inference would

be a description of a process that can be seen as something that arises from the free energy principle.

So you can either

and tell that story from the point of view of a physicist and say the active inference is a teleological description of processes that systems that self-organize must possess.

Or you could tell the story or define active inference from the point of view of neurobiology and ethology.

from the point of view of, say, predictive processing, and describes what it entails.

And I've used the word basing mechanics before, because from the point of view of the physics definition, it would be a teleological description of a basing mechanics that necessarily arises with certain assumptions from any self-organizing system.

key thing about active inference which um i think would be important to put in the definition in the ontology i'm not sure it's it's already there but you know if you're in charge of sculpting that ontology um then you're in the position to make sure it's there um is it it's about it's beyond predictive um processing it's beyond sentience and it emphasizes um

or reflects the pragmatic term at the beginning of this century, really, sort of epitomized by the sort of the four E's, you know, embodied, embedded, extended and the like.

to make it clear that sentience is active and that you are talking about the circular causality of engagement of any particle person or plant with whatever is out there.

So that would be certainly one thing to emphasize in terms of what active inference means.

The inference is interesting in the sense that it does imply a process and a process with purpose, which is to infer, which is why I keep using the word, you know, a theological description of something that's actually underneath the hood from the point of view of physics.

One final point here is

There's an easy confusion, I think, between, first of all, active inference and passive inference.

So that's certainly something which probably needs resolving, certainly in the philosophical literature.

So I often come across the philosophers who say, well, there's passive inference or perceptual inference, which is just basically inferring states of affairs in the world.

on the basis of some sensory evidence.

And then there's the extra bit, which is the active bit, which is now you're in charge of gathering that sensory evidence upon which you are now going to prosecute your perceptual inference.

That's an interesting dichotomy, which I'm not sure is, you know,

is a correct dichotomy.

If it's not right, I'm not sure that it is not right in the sense that it is a useful distinction, but certainly is not what active inference was originally termed to mean.

By conjoining active and inference, there were a number of motivations.

First, it was a generalization of David McKay's active learning.

But probably more importantly, it was a nod to the notion of active sensing and active perception, that perception is in and of itself an active process, a constructive process that you have to put

policies, plans and action into the game.

So that, I think, would be one important aspect of active inference to define.

And I don't know that it has been defined so far.

So, you know, perhaps it's your job to define that.

The other thing which is...

important I think in terms of emphasizing what active inference entails actually comes from that inactive perspective which is inference about the consequences of action and that has an important but really simple concomitant that the consequences of action are in the future

And that means you now have to think, if you're thinking about active inference in terms of teleology or as a normative theory of behaviour, of sentient behaviour, you have to now think about

I'm sorry, and I should say qualify.

When I say normative, I mean it can be operationally defined in terms of as an optimization process that in turn requires you to define the objective function or functional

And that's important practically, because if you're now thinking about sentient behaviour, active inference, and it's inference about things that haven't yet happened because you haven't yet acted, then you're necessarily talking about objective functions or functionals.

that are about states of affairs in the future.

And that is an important move and something that active inference embraces, which goes beyond predictive coding.

So much of the literature in the 20th, in the 1990s and subsequent, much of the literature

inspired that sort of inactive perception or active sensing take situated cognition take on on sentience originated in you know in things like predictive coding but predictive coding is not what is meant by active inference you can do predictive coding just by if you're a statistician just minimizing variational free energy that that that's only

half the game, once you move into the world of active inference, from a teleological perspective, you have to do that, you have to form beliefs about hidden states of affairs in the world, using the perceptual side of perceptual inference, but that is only in the service of rolling out into the future.

and deciding what the best thing is to do next.

And that running out into the future and deciding clearly calls for an objective function.

So in active inference, that would be the expected free energy, which may or may not be unfortunately named, but that's what it is.

and therefore active inference sort of implies that you are committed to optimizing an expected free energy and implicitly it's all about choosing the next thing to do

So for me, those would be two would be two sort of cardinal things that should be embraced by definition of active inference that transcend other normative approaches.

So, for example, reinforcement learning in behavioral psychology would be all about what the good things are to do.

And you commit to a loss of functionality.

a value function of states um you know if that was the your um the kind of behavior that you're trying to describe if on the other hand you were all about the psychophysics of perception or just building base or digital terrorist recognition systems where you weren't in charge of gathering those data then your objective functions will be would be very very different um but what active infant says well you can't you can't

carve up the two problem domains because they're just both sides of the same coin and thereby you're now facing the problem of defining an objective function that is fit for purpose, that does both

the belief updating about latent states or hidden states generating the data and also the best way to solicit or cause those data or outcomes under some prior preferences or some goal-directed constraints.

Is that a good long-winded answer?


SPEAKER_05:
Thank you for the comprehensive answer.

It leads directly to our next questions, which are what is the free energy principle and especially what is the relationship between active inference and the free energy principle?


SPEAKER_01:
Well, that's I think a slightly easier question to answer.

I mean, the free energy principle is just a variational principle of least action.

Why is it special or not formally identical to all the other variational principles that we use?

If you look under the hood, right from quantum through statistical and stochastic to classical mechanics.

Well, the only thing that differentiates really

variational principle of least action that is the free energy principle is that you're paying careful attention to the separation of states to which you apply that principle the separation of states into the states of an agent or a particle or a person and the outside states so technically

If you were in statistical thermodynamics, for example, you'd normally assume that separation in terms of some idealized gas that was contained within the container or heat reservoir or a heat bath without really worrying about where the heat bath or the heat reservoir came from.

But the free energy principle says, well, no, you can't really do that.

You've really got to attend very carefully to that.

to what licenses a separation of different kinds of states that you can assign to the inside of something and the outside of something, and the states that mediate the exchange between the inside and the outside, and then you get into the Markov blanket and Markov boundary literature.

So just to summarize, the free energy principle is just a principle of least action.

by which I mean that there is a description of dynamics in terms of the most likely paths any system will take, that is the special provenance of a partitioning or a separation of the states of some universe into the states that are owned by an agent or a particle and those that are not, and the states that mediate the exchange between them.

So that would be the free energy principle.

Active inference, as I say, is a sort of teleological spinoff from the free energy principle in the sense that in the same sense that you have now at hand

a principle of least action that allows you to identify, simulate, define the paths or trajectories or the narratives that a system, the most likely paths, trajectories or narratives that a system will pursue under certain conditions, and those conditions are just that there is an attracting set of states

to which that system will converge to or will look as if it's attracted to.

So what I was working towards was the notion of an attracting set as a metaphor for equipping that

physics with a teleology, and that teleology is nicely illustrated by the notion of attraction.

So, you know, when mathematicians talk about attractors in the particular case in the free energy principle, these are sort of pullback attractors or the kind of attractors that you get in random dynamical systems.

There's a proper and natural tendency to think that these particular states of the attracting set literally attract in the sense of gravitational attraction or any other kind of attraction.

They pull states towards them.

So that, to me, would be a theological interpretation, which I think is much closer to active inference, that you're saying that inference is a process that has a purpose and the underlying free energy principle looks, allows you to say, the way it looks as if self-organizing systems

uh show these certain properties they're attracted to certain states they show um you know they're attracted to certain paths and we can describe those uh in terms of the theological ontology and that would be active inference and one practical difference between active inference and the free energy principle is that the free energy principle is just a principle it's neither right nor wrong it's just like um

Barrow-Millage has noted it's like sort of Noether's theorem or Hamilton's principle of least action.

But as soon as you start to say, well, I think that this principle applies to this population or person or particle,

that suddenly commits you or requires you to define the attracting set of states, a pullback attractor, in another jargon, the equivalent would be a generative model.

And as soon as you commit to a generative model to explain the teenology of this system or this agent or this person,

then you've moved into the world of non-falsifiable principles into falsifiable hypotheses because you could have chosen the wrong generative model.

And thereby there will be evidence for choosing this generative model or that generative model.

So the relationship between active inference and the free energy principle is operationally quite simple.

Active inference is the application of the free energy principle to a particular system.

But in that application, you're bringing a lot of teleology to the table.

And more specifically, you're having to commit to a particular generative model.

And as soon as you do that, that becomes your theory or your hypothesis about what is an apt description for this system.

So a number of, I think, sort of interesting distinctions in terms of the relationship between active inference and free energy principle that I imagine your ontology has already addressed or is certainly addressing.


SPEAKER_05:
Well, we'll get there.

Thank you for that excellent answer.

For the next question, Lorena, please read it out.


SPEAKER_04:
Oh, hi.

So still in the spirit of broad questions and broad terms, and that, I think, comes in line with what came before.

So how and where does the idea of information play a role in the pre-energy principle, and how does it relate

or is active inference in the sense, what is something to keep in mind when thinking about information dynamics in active inference?


SPEAKER_01:
Right, well, these are great questions.

I'm getting the hang of this now.

You just want me to talk.

You'll present a question to me, not if I'm talking, which I'm very happy to do.

Are you sure you want me to do that?

Or should this be a conversation?

Perhaps this will turn into a conversation at some point.

Anyway, so information.

So it plays a dual role in the sense that information

Theoretic formulations underpin most of the derivations behind that principle of least action.

And it can be no other way in the sense that all mechanics from physics is really articulated in terms of probability densities of distributions.

And as soon as you

have a mechanics that is or a calculus or probability distributions, you're effectively in the world of information theory.

And you see that at many different levels.

So one nice example of this is the central quantity that we often use to score the likelihood of being in a particular state.

If you're a statistician, that would be the marginal likelihood.

If you were fluent with an FEP ontology, it would be surprise or almost simply surprise.

That is just basically the self-information.

If you're a physicist, you'd look at this as a potential.

It's a negative log probability.

So you start, really, when thinking about the physics, with this central concept of self-information, which for the IAP can be read as a potential function or a surprisal function, or surprise, and it is the thing that the variational free energy is a bound approximation to.

So at that level and then everything else, every other move you make mathematically in terms of the expected self-information being the entropy and why that is important as a characterization of various probability distributions in the setting of self-organization.

testify to the fact that information theory is absolutely central to all the maths that underlies the physics of the sentience that emerges from having a distinction between the states of the system and states that are not of the system namely the Markov blanket.

Having said that,

Information to most people's minds usually means more, certainly in the folk psychology context.

It's really information about something.

And the FEP active inference has, I think, something quite special to bring to the table here that goes beyond information theoretic treatments that you get in communication and signal processing and rate distortion theorems.

All of that kind of information is just your extensions of information theory that inherit from self-information or the implausibility of a particular event or message.

Or in more abstract domains such as sentience and consciousness, you would go to something like integrated information theory.

But that is all about this Shannon-esque kind of information.

The other kind of information is information about something.

So what I wanted to try and put on the table

is the very fact that you've got this Markov blanket or separation of states on the inside and states on the outside means that now you can equip the states on the inside with the role of encoding posterior conditional Bayesian beliefs about states on the outside.

And that introduces technically a different kind of information geometry, a different kind of information theory.

where crucially now you can read the internal dynamics as containing or having information about what's going on on the outside and this is a really important move equipping your neural dynamics or variational message passing or belief propagation in a computer

with an information geometry that now allows you to read off the state of the computer or the state of the neural activity in terms of what it is encoding or the information it contains about the outside.

And so that sort of dual aspect information geometry has been celebrated to a minor extent in the philosophy literature by Vanya Weiss.

asking the question is, you know, is this really the maths of sentience where you now have information about things?

And in a sense that, you know, that really is the heart of the free energy principle in the sense, or active inference anyway, in the sense that it equips that information geometry.

I mean, technically what you are saying is that any particular internal state of a computer or a person or a brain is

now can be read as encoding a Bayesian or a posterior belief about other states, namely hidden or latent causes outside the Markov blanket.

And that defines technically something called a statistical manifold.

And as soon as there is a statistical manifold, there's an information geometry.

And any movement on that manifold necessarily implies a change in your Bayesian beliefs, namely Bayesian belief updating, which means now there's an interpretation of neuronal dynamics, movements on a statistical manifold on the inside in terms of belief updating.

So the notion of active inference as the process of belief updating

really rests upon this fundamental notion that there's information about stuff going on, about stuff that is encoded or parameterised by the internal machinations and the mechanics and the dynamics of the inside.

So I think it might be quite important to

if you're trying to describe or educate people in terms of how they should understand information as playing a central role in sentience.

I think it'd be important to differentiate between the mathematical notions of Shannon information and self-information

and the calculus of probability and mutual information, for example, and say that this is not the kind of information that is implicit in information geometry and the sentience that is afforded by active inference, when now you're understanding neuronal dynamics or message passing in a computer on some sort of factor graph,

Because in this instance, each of those messages or those neural dynamics can now be read as belief updating, namely changing your mind about other things so that the stuff on the inside has information about stuff on the outside.


SPEAKER_05:
Thanks for this important answer and we're going to pass over a few questions and go to 18 with Stephen reading the question to continue on this theme about the separation of the inside and the outside, so thank you Stephen and please read off 18.


SPEAKER_00:
Thank you.

I was going to ask, how can the integrity of the active inference process theory be maintained when blanket states and generative models are being interpreted in novel ways?

So we were thinking about what do you think of discussions around Markov or Perl, Friston blankets, et cetera?


SPEAKER_01:
That's an excellent question.

I have quite a technical answer, so if it's getting too technical, tell me.

I'll try and get back to what you were really trying to unearth.

So this is not a fast moving field, but certainly been a delicate and important area of discussion over the past few years.

So in the original introduction of Markov blankets, there was

an explicit nod to Pearl's construction of Markov blankets and how Markov blankets are used practically in terms of simplifying message passing in computer science.

However,

that may have been something of an oversimplification.

Because from the point of view of the free energy principle, it is the kind of causality that the free energy principle deals with is not the kind of causality that people, particularly people like Pearl, but also people dealing with things like Granger causality,

deal with.

So we're not, from the point of view of the free energy principle, that starts with a stochastic differential equation or a random dynamical system written as a random dynamical equation, OU processes being simple examples.

In physics, these would be Langevin-like equations.

Common to all of these starting points is time and evolution and dynamics.

Now, there is nothing in Pearl's formulations, well, certainly there's nothing in Pearl's book on causality that deals with time.

And I know that because before the days of PDFs and being able to go and search particular words, I had to go through and find out there's one paragraph

that mentions dynamics.

So, you know, if you were in statistics, computer science, you know, this will be the world of dynamic Bayesian nets.

That is their take on something which is actually much more universal, which is basically the universe as a Markovian dynamical process.

So just stepping back, the challenge now is to articulate

independences that underwrite Markov blankets in the sense of Perl in terms of dynamics.

So you've now got to link two quite distinct fields, which is basically the fields of dynamics and long-term processes, things that have paths of least action.

to the world of statistics and Pearl-esque independences and causality cast as interventions that have observable consequences.

The problem in doing that linking is that you have to really abandon the notion of causality in the world of Granger causality and Pearl, because causality is baked into, is inherent in writing down any

differential equation, be it stochastic or random or deterministic, in the sense that states cause motion.

So the causality in this context would be a more control theoretic causality.

So that means that you can't then use the causality concept later on, but it does mean that you've now got to derive from

a dynamical Markovian calculus, the necessary conditions that would lead to the conditional independences that are necessary to define Markov boundaries.

Just to slip in here, the Markov blanket

is composed of minimal blankets, mainly boundaries in the sense of pearl.

And on most recent analyses, it looks as if the blanket is actually two Markov boundaries in the sense of pearl.

But to get to the sense of pearl,

you've got to think very carefully about what are the constraints that lead to the conditional independences, where those constraints are specified in terms of equations of motion and things like the amplitude of random fluctuations.

So once you've seen that that is the link that needs to be made,

That actually simplifies things in the sense there's no real attitude for interpretation.

So I'm going back to the part of your question, you know, blankets and generative models are being interpreted in novel ways.

I don't think there's any latitude for any novel interpretation other than the

Sorry, if in novel ways you mean the best way or the correct way and we just haven't got there yet, then I would, you know, I'd concur entirely with that.

If you think that there is some latitude, there's some library of insightful reinterpretations and redefinitions, all of which have an equal veracity, then I would suggest that's not the case.

There's only one way, there's only one Markov blanket, or there's only one particular partition that will give you, that can be articulated in terms of Markov blankets.

And

the only novelness there is really in tying down very precisely and defensively how you get from a Langevin formulation to a Markov blanket.

At the moment, the novel way of doing that looks as if it's a

that the conditional independences arise from sparse dynamical coupling or causal coupling.

So if you read the causality as the influence that a state has on the motion of itself or any other state in this sort of minimal Langevin-like description of the universe, then it is the sparsity of influence or the sparsity of coupling

that leads to conditional independencies.

And if a system has a sufficiently rich sparsity of conditional independencies and implicitly coupling, then it will have a particular petition.

And if it has that particular petition, then the free energy principle holds.

So I think the discussions around Markov, Pearl, Friston and Blankets are

essential, they're fascinating.

The conclusions of those discussions that I think I'm going to probably have to refer back to the underlying maths and that maths is all about connecting Langevin formulations of physics to the kind of calculus that Pearl has established in a more statistical sense.


SPEAKER_05:
Thank you for the educational answer.

This brings us almost to the end of the .edu section.

So I will pass to the final question to be read by Dean, who had several excellent points and questions.

So Dean, feel free to ask however you would like.


SPEAKER_02:
Good morning.

So the question is, what's the difference between a subject matter expert

and a prediction matter expert, and how does this relate to your mode of interaction?


SPEAKER_01:
You're going to have to unpack what subject and prediction matter experts means for me.


SPEAKER_02:
Yeah.

For me, interesting, you become subject matter expert by gaining a certain amount of concentration in a particular field or area, and you become prediction matter expert when you are able to think more distributively, more dispersively.

And so I think what, when I read some of the things and listen to some of the stuff that I've heard you talk about, you've brought these two worlds together.

And so I'm kind of interested in hearing what you think in terms of introducing some of the ideas and principles that you brought into a world where traditionally we focused on concentrating, whether it's materializing something from an engineering perspective or deciding what's in and what's out.

You've brought in another aspect to look at, and I'm just curious what you think of that.


SPEAKER_01:
Okay, that's a fascinating distinction.

I'm not sure it's terribly important when I think about it, because clearly you're the expert on this, but it certainly would be fascinating to consider the conditions under which you

were able to simulate the emergence of a subject matter versus a prediction matter expert in silica, for example.

This is a proof of principle that these are both effectively Bayes optimal ways of responding to a particular environment.

And my guess is that you would be able to do that relatively easily

by appealing to the ideas that you find in applying some of active inference notions to structure learning and development where the basic idea is if you've got a

very volatile environment by which I mean that there's lots of uncertainty in the contingencies or possibly there are lots of random fluctuations that are irreducible in terms of your ability to predict the outcome of the trajectory of latent states of the world in which you are becoming an expert.

then when you parameterize your uncertainty, usually formally in terms of the precision of various likelihood mappings or probability transition matrices in a sort of discrete state space generative models, when you parameterize your beliefs about that uncertainty, irreducible uncertainty and volatility, then

agents that believe or have inferred that they are in a very volatile, changeable, capricious world usually become better at the prediction side of things in the sense that they rely less upon deep past experience

and assign more precision or more potency to the more recent evidence.

So they have a different style of evidence accumulation that enables, and also they have the right level of uncertainty about what will happen next.

So it looks as if in their predictive engagement and epistemic foraging in that world,

it looks as if they are better at predicting changes because they're not committed to a particular explanation or understanding of how their world works.

On the other hand, if you create a world which is incredibly predictable and learnable, then over time, the natural pressure to minimize

free energy translates into a pressure to minimize complexity, namely a way of modeling your world and your exchange with it in the simplest way possible, in accordance with Occam's principle.

And what that then leads to is somebody, it sounds as if it's somebody who becomes a subject matter expert.

And so the subject matter is their lived world.

it has now become so predictable that they do not entertain all possible other outcomes because they have precise beliefs about the way that things will unfold and they can make very wise very parsimonious or using parsimonious

degrees of freedom, they can make moves and become very expert in the way that this particular non-volatile, predictable, i.e.

precise world works.

And the link with aging here is that

If you allow for the fact that we create our own environments, at many levels, active inference will permit or is a way of framing our eco-niche construction.

The story people tend to tell is that as you get older, you basically make your world more predictable and you become a subject matter expert in your own lived world.

So I like the example, I no longer go bungee jumping or go to discos.

because my world is very, very predictable.

You're very much an expert in it, because my world is basically my conservatory and my study and my bedroom.

So I'm a complete subject expert on that.

You take me out to a disco and I will not,

I will not be able to predict what's going to happen next because I'm old.

Whereas, you know, adolescents and children and certainly, you know, newborn infants or newborn artifacts discovering their world, which is full of uncertainty,

They are not yet subject experts and the epistemic pressures or motivation for them to learn about what happens if I do that and what can I control and what can't I control, that will make them very quickly into prediction experts until they become experts.

until they become sufficiently fluent that they can now engineer their world to make it non-volatile, and then they presumably will become subject matter experts.

So I'm sure that would be fairly simple to simulate using all the sort of toy active inference schemes that we currently use, and it would be really interesting if these two different kinds of

synthetic agents did develop some cognitive styles and confidence in what they were doing that looked exactly like the distinction you're talking about.

I'm not sure it would work, but if it does, that would be, you know, I think an illuminating proof of principle.


SPEAKER_05:
Thanks for this answer and for this session from the lab and .edu.

That last answer really spoke to the importance also of intergenerational learning.

At this point, we will take a five-minute break and we will return for .coms.

So thanks again, and we'll see everybody in five minutes right here.
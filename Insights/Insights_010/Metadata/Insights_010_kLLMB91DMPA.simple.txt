SPEAKER_00:
Hello, everyone, and welcome back to Active Inference Insights.

As always, I'm your host, Darius Parvizi-Wayne.

And today I have the great pleasure of speaking to Mr. Although very soon it will be Dr. Lance da Costa.

Lance is a PhD student with Greg Pavliotis and Carl Friston jointly at Imperial College London and UCL.

And his work lies at the intersection of mathematics, cognitive science and machine learning.

Indeed, he is at the very cutting edge of the technical formulations underlying active inference.

And so I'm personally looking forward to learning from him today.

And I hope you are too.

Ladies and gentlemen, I bring you Lance da Costa.

Lance, thank you so much.

I genuinely meant what I don't really like doing the introduction because it feels sort of a bit stagnant and rigid, but I genuinely meant what I wrote there, which is that I'm really looking forward to learning.

Thank you for having me.

I appreciate the introduction.

Yeah.

When do you expect to become a doctor?

When's that title?

happening?


SPEAKER_01:
My defense is at the end of February.

So it's in a month.


SPEAKER_00:
Yeah.

Wow, fingers crossed.

I'm sure you'll smash it.


SPEAKER_01:
Thanks.


SPEAKER_00:
So before we start getting into the maths, this is very exciting for me, because you're the first I would say, pretty much pure mathematician we've had on that computer scientists, we've had cognitive scientists, but no one really delving into the nether regions of the maths.

You know, when I was researching what you're interested in, I came across the fact that you're interested in world models.

I'm also very interested in world models.

So I wanted to start there as like a teaser before we get into the maths.

And I wanted to ask whether you could explain what you mean by a world model and how you distinguish that from a self model.

And the reason why I asked that question is kind of twofold.

One is that I think the word world models might be slightly misleading.

because it kind of implies that we're modeling the world.

And in some sense we are, but we're definitely not modeling the entire world.

So it might be called something like an eco niche model.

Um, although that's less catchy.

And then the other reason is I think there's still an open question, at least to my eyes in active inference about whether we are modeling the world and ourselves, or whether we're just modeling the world and we fit into that in some way.

Right.

It would be great to unpick that if possible.


SPEAKER_01:
Yeah, that's actually a cutting edge question, but I really like it.

Great.

So let me start by world model.

What I mean, what I personally mean by world model is that you, as an individual, you have always some sensory data coming in, olfactive, visual, auditive, and so forth.

And you're going to build a model for what's going on outside that causes your data.

Now, the model that you build is a Jonathan model.

Which means that you can generate counterfactual data about what might happen in the future, what might happen if I do this and that.

It's a probabilistic model because you're always uncertain about what's going on around in the world.

And it's also a causal model.

This means that you're not, you're not exploiting the, well, you're not exploiting just the correlations in the data that come in, but you're actually explaining the causes of the data.

Something like more powerful than, than a statistical model.

Now, I think you're right in calling it an eco niche model, because I mean, the data that we have coming in at any point in time is just from around us.

And so that's ultimately what we want to model.

When we talk about the free energy principle, we say that the brain optimizes its model to maximize accuracy and minimize complexity.

And that entails having a model that's as simple as possible.

And therefore you don't want to model things that are like too far away from you, that are not important from you.

And so this is why having an eco niche model as a terminology makes more sense.

So when you talked about world model and self model, and this is where it really gets cutting edge.

So in the like simple applications of active inference and AI,

You just care about modeling the world.

But now, as humans, we don't only do that.

So for instance, if I take a particular, if I make a particular decision or take a particular course of action, and you ask me, why did you do that?

I'm not going to be able to answer in saying, oh, this neuron did this, and this neuron did that, and this happened.

But I'm going to give you a high level overview using natural language of why I did this.

is the idea of metacognition, which means of having a model of the self in addition of having the model of the world.

So you have a model of yourself and you have a model of your own cognition.

And so this is kind of like a higher form of cognition and active inference, which has not so far been really implemented out there.

I believe at least in machine learning, it's kind of come out in papers on modeling meditation with active inference.


SPEAKER_00:
lars's work yeah yeah yeah yeah very nice cool that's really so so you so this is yeah this is a question i've been entertaining when i wrote this first paper on flow that i've now mentioned far too many times so i shan't mention it again um i was thinking exactly what are we talking about when we're talking about something like an epistemic agent model and uh i remember one commenter commentator said oh is it just metacognition so that's an interesting alignment there um

because in a sense, yes, you're right, we have to reflect on our own capacities.

And model that as an inference.

My only question there is, you know, in laws is paper, you have these really beautiful hierarchical Bayes graphs.

Do you think there's a there has to be an upper bound on the computational complexity of that metacognitive?

And what?

And is that just the limitation of the computation in and of itself?

Or does it mean that there might be something ontologically?

So actually, there?


SPEAKER_01:
Right.

What do you mean by ontological exactly?


SPEAKER_00:
Let's use a word.

Well, I'm not going to invoke something like a Cartesian soul, but I guess is there anything unique beyond the algorithm, beyond the probabilistic model?


SPEAKER_01:
Right.

So what I would say to that, I mean, what we're personally investigating with Lars is what does metacognition actually allow us to do?

that's important for survival that an agent that does not have metacognition is not able to do.

So the idea is if metacognition arose at some point is because it increases our chances of survival, our fitness, it minimizes our free energy.

How exactly?

I mean, how is metacognition useful beyond what we just talked about?

It's currently unknown, at least for me.

Cool.


SPEAKER_00:
Yeah.

I think it's a complex issue because it's entangled with common folk psychology notions of what it is to be a self.

So most people will think of themselves, probably if you ask them, they would say, yes, I am this substantial thing somewhat behind my eyes that persists through time.

So somewhat of some kind of Cartesian reified ego.

And I actually found, I find that active influence is a lot easier to pass once we take that out of the equation.

when you just have the physics or when you just have the statistical mechanics, it becomes a lot easier to say, okay, maybe this is maybe the self is just a further inference.

That's kind of where I was pushing you in terms of the ontological thing, because I think people listening, if they haven't really, you know, thought about the, the Buddhism and the metacognition and the,

Derek Parfait-related philosophy might think, well, there's just something there that's doing it, right?

There's something there that's a homunculus behind the eyes that's actually doing the calculations, that's doing the variational Bayesian inference.

But I assume you can't, you know, there's no notion of that in your models.

So you mean like, kind of like an agent inside the agent?

Indeed.

And obviously, this leads to a homuncular infinite regress, because who's watching the screen and so, you know, and so on.

But

I think people have an issue with just the raw notion that there's just algorithmic computation going on because they're going to ask, wait, what, who is doing it?

And that leads us back to some kind of active agent that's actually there, not just an emergent phenomenon.


SPEAKER_01:
Right.

So I have a few things to say about that.

So when we engage in meditation, computationally it looks like we're building a higher level in our giant model, which is kind of like another agent inside that makes inferences about our cognition.

And so when you're a trained meditator, you not only think about the world or experience the world, but you experience your own cognitive processes and you train yourself in doing that.

Now how that looks like computationally is that you have a hierarchical giant model with many levels of abstraction.

But when you translate that into math and physics, it's kind of this idea of the Russian doll, where you have an outside agent and some more agents inside hierarchically stacked that just infer each other.


SPEAKER_00:
Yes.

So they're a Markov blanket in some sense.

Yeah, nested Markov blankets.

So, but that, but you know, that analogy implies that there is some outer shell that might be right.

Only that might be, you know, yeah.

This makes me think of Maxwell Ramsey, the mouths paper on consciousness that there's this right.

Only layer that is contextualizing, but not contextualized.

Um, but that's kind of the other way around, right?

That's like, that's the.

That's kind of like the prime mover unmoved.

Whereas I feel like your argument is saying, well, everything is contextualizing everything else.

And actually what we might call the outer shell, the autobiographical self is just the aggregation of all of these other selves.

Does that sound about right?


SPEAKER_01:
Yeah, I feel it's kind of hard for me to describe it.

I'm also thinking along these lines, but

I would say, I mean, the 300 principle, everything is dependent on the blanket that you select.

Right.

And it's the idea that we have multiple nested blankets.

Now, the story that you tell depends on which blanket you decide to talk about.


SPEAKER_00:
I think people have an intuitive folk psychology notion that, of course, there's a point at which

the modeling of me as a discrete identity needs to stop.

So we could keep, so, so most people would probably think about the surface of the skin, the epithelium and say, well, that that that's me.

Right.

And then we can get into conversations about extended mind hypothesis, but I actually don't think it's that relevant here.

I guess the point is, um, it seems like that the point at which we stop the Markov blanket is actually, it's not arbitrary, but it's an open question.

it's philosophically vague, so to speak, which, which, which negates the kind of folk psychology notion that there is something like me, which has these strict bounds.


SPEAKER_01:
Yeah, so, so I completely agree with kind of like the dissolution of those strict bounds, once you start looking at it very carefully, intuitively, I think that my, you know, my hands are mine, because I can control them in a size way.

And I think this is where that comes from.

Okay.

it is the preciseness at which you can act on something or infer that's nice yeah when you when you do a lot of meditation or when you when you look at it like very carefully you start realizing that there's so many things in your body that you cannot control it's kind of obvious when when you think about it and you start realizing that you have very little agency over many things that are happening in your body so

I think every one of us has things that they're satisfied and dissatisfied about themselves.

When you think about it, where maybe I'm dissatisfied with something about myself, but then this is not really my fault of me being that way.

So here's an example of something that I cannot control.

Once you reflect on that more and more, you start realizing that, you know, we are a self-organizing systems.

Just like a tornado is a self organizing system.

And we have a lot of things that are going on inside our bodies, inside our minds that you cannot just cannot control.


SPEAKER_00:
So, yeah, that's interesting.

I guess, I guess my rebuttal to that would be arguably the literature says that, or suggests there are processes that underlie something like a minimal self, like interoceptive inference.

So that can be done reflexively or through kind of allostatic control.

So let's say my glucose level drops beyond a certain level.

I either, you know, secrete the necessary hormones or go out there and eat.

But let's just say I'm secreting those hormones.

I'm not, you know, my autobiographical self doesn't have the, doesn't have the sense it's doing anything.

Yeah.

That mechanism of prediction error suppression is supposedly at the heart of the sense of mindness and presence.

So how do we resolve that if we're casting selfhood as that, which is within my control?


SPEAKER_01:
Um, well, I don't know, to be honest, but, uh, it's a, it's a difficult question, but, uh, yeah, I mean, the, the more, the more I think about all of this and the more I read, the more I realize that we have control over very little.

Yeah.

So in, in the.

meditation books.

So I keep coming to that because they're a source of inspiration for me for describing high level cognition.

There is this, like fundamental truth, quote, unquote, which there is no self.

The more you start like, investigating the mind through self inquiry, the more you realize that there is no self.

Yeah, yeah.

So maybe it speaks kind of to the things that we're just discussing.


SPEAKER_00:
sure yes this is yeah this is an interesting point um i guess you one would have to ask very dedicated meditators uh whether when they're doing let's say vipassana and they break that kind of duality um because you know the subject objectuality yielding to oneness is kind of a seminal phenomenon that occurs whether that sense of mindness and presence is also lost

So there's an, you know, uh, John Viveky, who's one of my favorite cognitive scientists.

And it was on the podcast speaks about adverbial qualia.

And what he means by that is there are these so-called pure consciousness events.

And even in those, the sense of presence and perhaps mindless, although we kind of weren't sure, um, might still be present, which I, which to my.

Is sounds like some computational mechanism, like interoceptive inference still being a play.

So I guess this is calling for a denial of the strict distinction between the body and the mind that the interceptive inference can still underlie or bodily inference can still underlie very important phenomenology.


SPEAKER_01:
Yeah.

Well, active inference is definitely taking this philosophical position that cognition is embodied.

So yeah, it speaks exactly to what you said.

Now, I just want to add something else.

So we talked about the model of the self, and I would say there's actually two models of the self.

So there's the model of the body and this has a clear evolutionary reason for being there.

So for example, if something in my body hurts, I need to figure out what to do about it for it not to hurt.

So it's about preserving your homeostasis and fulfilling preferences.

Now there's this other level that we've also been discussing, which is like the model of our own cognition.

These are two different things.


SPEAKER_00:
Yeah.

That's interesting.

Yeah.

Cool.

There's lots of, there's lots to chew on that.

I'm glad that I'm glad that this is cutting edge question.


SPEAKER_01:
These are really things that I know a lot of people are thinking about.


SPEAKER_00:
Yeah.

I think, I think I'm gonna, I'm going to have laws on at some point.

Yeah, he owes me that to be honest.

He keeps promising.

Okay, let's to the best you know, this is this self modeling philosophical conversation is more my bag.

But let's take that deep plunge into the mathematics.

So as a as a kind of forewarning or a precursory warning to the audience, I'm not a mathematician.

So everything I'm doing when I'm reading the maths, I'm trying to ground in intuitive

philosophically robust concepts.

So just, just for the audience and yourself, please do excuse me if I cock up.


SPEAKER_01:
Um, you won't, I mean, this is a, I think it's very important to be able to talk about these things without the math.

It's literally, literally impossible to talk in math.

So yes.

Yeah.

Yeah.

Yeah.

There's this about this in a, in a natural language that's hopefully as simple as possible.


SPEAKER_00:
Please.

Well, yeah, I guess that's how it has to be taught is through metaphor.

So yeah, life through metaphor for George Lakoff kind of stuff.

So starting right at the basis in active inference, the world is described as a random dynamical system defined or described by a stochastic differential equation, like a Langevin equation.

Why is it worthwhile starting there?

Why is that the starting point?


SPEAKER_01:
Right.

Should I explain briefly what a Langevin equation is perhaps?

Sure.

It would be better you doing it than me.

Okay.

Sure.

Well, so here's the thing.

We want a fundamental theory of the brain and cognition, and we want it to be grounded in math and physics.

Now, if you look at the fundamental theories that exist in physics, these would be general relativity, classical mechanics, statistical mechanics, quantum physics.

they are all based on stochastic differential equations or Langevin equations at some level.

Now, a stochastic differential equation is a very fundamental and also basic object.

And I'll explain why it's basic.

It's basic in the sense that when you're modeling a system, there are the known knowns and there are the known unknowns.

Stochastic differential equation brings the two together.

There may be the known knowns about the system, which could be forces that act on different components of the system.

This is going to drive the system deterministically along a predetermined path.

And then there are known unknowns.

These could be random fluctuations.

For example, if you have a drone, this could be random fluctuations due to wind.

If you have a boat, random fluctuations due to waves.

And these are things that you possibly cannot accurately capture.

So you're going to summarize them in a random term.

And then together, so you have this deterministic term that gives you a predetermined trajectory, and you have this random term that sort of perturbs the trajectory in a random way.

When you put the two together, you get kind of like a random evolution of your system with some structure.

And this is basically what the Langevin equation or stochastic differential equation encodes.

So this is to say it's a very general starting point and it's also the starting point that's consistent with the rest of physics.

And that's why we start there when we when we think about the principle.

Excellent.


SPEAKER_00:
Yeah.

So there's a couple of things I wanted to pick up.

Am I right to say that it's about the rate of change of

So of some X, let's say of some states.

So that's kind of what we're looking at here.

Okay.

And then the second thing is this notion of random noise.

If you speak to certain philosophers, for example, they might say, well, it depends on who you ask, I guess, because for a universal

for some demon that knows the entire unfoldings of the world in a single glance.

There's no randomness.

Sure.

And yet to us, the modelers, well, it's just computationally impossible to know every single element of the wind, let's say, or the waves.

So what are we talking about?

Are we talking about randomness in and of itself in the actual system that's being modeled or for the modeler?


SPEAKER_01:
This is a great question.

Here, it's agnostic in the sense that you could imagine you can model the system that you're trying to model in all in exquisite detail.

So you know absolutely everything that's going on.

In that case, you would have no randomness.

In that case, you still have a stochastic differential equation with no stochastic term.

which is an ordinary differential equation, but it's still a stochastic differential equation, so the same math applied.

You just have a random term that's like zero.

Maybe there are things that you cannot model accurately in the system, so you put them in some noise and the same math applies.

So I would say it's noise from the perspective of the modeler or not, and we don't actually care.

The point is that

Once you start from the assumption that you can model the system in that way, then things carry through.


SPEAKER_00:
And I make the presumption, I'm presuming here that that noise is considered Gaussian.

It's considered a normal distribution generally.


SPEAKER_01:
Yeah.

So generally for you to be able to do mathematics, you take Gaussian noise.

Is that problematic?

Is that an issue?

Well, it depends on who you ask.

Like

Uh, most math and physics is based on Gaussian noise because it represents the mode, the world most accurately.

You have something called the central limit theorem that if you have a lot of like fluctuations, they kind of average away in a pile of sand.

Yeah.

Yeah.

So your noise turns out to be oftentimes very well, uh, by Gaussian process.

Uh, but then there, there are instances where Gaussian noise is not accurate.

So for example, if you're talking to people who model the financial market, they do not use Gaussian noise.

They use noise with like jumps to model like a very sharp, um, yeah, jumps in the market.

So Gaussian noise is a good place to start.

I think ultimately, as we work on the 300 principle mathematically, we want to be much more inclusive.


SPEAKER_00:
Sure.

Isn't that, I mean, just as a side, isn't that remarkable fact that like stochastic

processes tend to all has been Gaussian.

It's just like, I think it's awesome.


SPEAKER_01:
Yeah.

I mean, you have all these remarkable properties of the Gaussian distribution and it's also like the only one distribution that we actually know how to work with very well.

So it's a good thing.

Yeah, it's cool.


SPEAKER_00:
And then I guess my other question about the Langevin equation is

you have sort of f of x as the deterministic force.

So it's a function of the states of x, the flow of x. Is that right?


SPEAKER_01:
Yeah, that's right, yeah.


SPEAKER_00:
But that doesn't have a kind of comma t next to it.

Why is that not a function of time as well?


SPEAKER_01:
So it could be in its most general form, f, the flow would be also a function of time.

Now, if you think of it, let's say you have a state space and you have time, you can just incorporate time as another element, as another state.

So you can set your state space to be space time and then you're back at F of X. So I think that's why oftentimes people just go with F of X. Cool.


SPEAKER_00:
Okay, excellent.

Now let's jump to the next step.

We have density dynamics.

and the Fokker-Planck equation and the path integral formulation.


SPEAKER_01:
Right.


SPEAKER_00:
Very fundamental question.

What is the difference between those two?

There's no difference.


SPEAKER_01:
There's just different ways of articulating the same thing.


SPEAKER_00:
Okay.


SPEAKER_01:
So the Langevin equation gives you like the rate of change as a function of the flow and a random component, random noise.

Then when you sort of solve that,

you sort of get, this is a probability that the system takes this trajectory, and this is a probability that gets this other trajectory and so forth.

This information is what's carried in the path integral formulation.

The path integral formulation gives you the negative log probability of a path.

And then the Fokker-Planck equation, which says at any point in time, this is the distribution that encloses the system.

So you might in a predefined place that, you know, I mean, the system might be, uh, might start in a particular location, safe space, maybe because of the flow and the random fluctuations, uh, at time T plus one system will be, it could be here.

It could be there.

It could be there with different probability.

And then there's a probability distribution that summarizes this.

And this is what's included in the Fokker-Planck equation.


SPEAKER_00:
They both come to the same conclusion.

You run the maths both ways and they all converge.

The answer.


SPEAKER_01:
It's exactly the same thing.

It's different ways of looking at the same thing.

And depending on the question that you want to answer, taking one formulism or another is easier.

But the point is, you can always go from one to the other and back.


SPEAKER_00:
Nice.

Okay, that's cool.

Okay, let's jump into the Fokker Planck equation because I've been doing some reading and I might be wrong, but I'm excited about it.

It's excited me.

Yeah.

So for what I've learned, it's a partial differential equation that relates the rate of change of the probability density function.

So that's that P with the little dot and X comma T.

with respect to time, drift, and diffusion.

Right.

How's that sounding?


SPEAKER_01:
Yeah, it's a great summary.

I can elaborate on that a little bit too.


SPEAKER_00:
Oh, please.

Well, people have no idea what I'm talking about.

Okay.


SPEAKER_01:
Well, you'll see that you got it perfectly right.


SPEAKER_00:
Oh, thank God for that.

All right.

Please explain.


SPEAKER_01:
In a large amount equation, you have the rate of change of the state, the flow,

and the random term.

Yes.

Now, this is what physicists call it.

But if you talk to a mathematician, he would say you have the rate of change of states, the drift, and the random term, which they call the diffusion.

Yeah, it's nice.

Which is characterized by the diffusion.

So then the Fokker-Planck equation is very similar.

You have, instead of having the rate of change of the state, you have the rate of change of the probability density that describes the state.


SPEAKER_00:
then this is given as a function of the drift so the flow and the diffusion so the random term okay cool so so um if we were going to cast this in like in terms of uh just cartesian coordinates just x and y axes would it be right to say that we could have x as like the position let's say of because i read there's a lot here in sort of brownian particles so i had my particles in some you know doing brownian motion the x would be the position of some particle

And the Y would be the probability of finding it there.


SPEAKER_01:
When you talk about the Fokker-Planck equation.

Yeah.

So the Fokker-Planck equation would be that.

So at each point in time, the solution to the Fokker-Planck equation is the density of the system that evolves over time.

And so you could plot this on the X, Y axis, as you said, on the X axis, you would have the position of the system and the Y axis, you would have the probability distribution.

And so imagine things are simple.

So if we imagine the system is always Gaussian, distributed according to Gaussian distribution, then if you solve the Fokker-Planck equation, you would have a Gaussian that evolves over time.

So the mean would be shifting and the variance would be shifting also potentially.


SPEAKER_00:
Nice.

Okay, cool.

That's really useful.

So how would we then integrate time?

Is that X and Y axis considered like a snapshot in time?

Yeah.

Okay.

Excellent.

Yes.

Okay, cool.

That's actually really useful because I think people might got confused by that.

Yeah.

Now let's get to.

So this gives us density dynamics.

But when we talk about active, we talk about steady state densities.

People, I think, have this intuitive notion from whether they're coming from dynamical systems theory or something else of a kind of basin and attract to say, I mean, I think of it kind of like a magnet, just metaphorically.


SPEAKER_01:
what is how do we get from density dynamics to a steady state density um so i should caveat this by saying that the latest formulations of the free energy principle are not based on uh having a steady state density that's kind of like a new thing and i think for like intuition it's best to think about the steady state case so um kind of like the intuition is that if you're modeling an agent

It has a preferred set of states that it likes to come back to, or does everything it can to be back at those states.

You could think of homeostatic states, for instance.

You can also think about the fact that if you're modeling a system that does not dissipate in time, so let's say not a gas, for instance, then the system should also have a steady state, at least over a short period of time, because it doesn't dissipate.

So now we're going to try to model, I mean, the 300 principle is about modeling or analyzing those systems that have a steady state that has these sort of preferred set of states.

And so you ask if a system has a preferred set of states and it's also decomposed into an organism, its boundary on what's not the organism.

And this could be like a living organism.

It could be a stone.

It could be anything that has a boundary.

Then you basically look at what the internal states do.

What does the boundary states do?

How did they evolve?

And what do the external states do?

And then the conclusion is that in general, the internal states look as if, I mean, you can describe them as inferring the external states, basically blanket states.


SPEAKER_00:
Yeah, yeah, yeah.

Yes, that's an interesting point.

formulation and one we could come back to which is this kind of uh yeah tracking in some variational bayesian inference way um we've spoken about this sort of looking like very important two words in the free energy principle uh a number of times in this in this podcast but i was curious more about in terms of the fokker plank equation i don't know why i've turned into a master um

So this is the way that I would think about it, and please feel free to, from what I've learned in a very shallow way, is that we had this probability density function.

So let's say, as you say, we have our x, we have our position y, we have our probability distribution.

Let's just say it's Gaussian.

And per organism, per attractor set, it's going to be different, right?

So mine might be really close to this size, yours might be whatever.

What I kind of understood as a steady state is over time.

So let's say now we put all of those images of the X and Y axis on top of each other.


SPEAKER_01:
Right.


SPEAKER_00:
And let's say they were somewhat translucent.

If it's steady state, you expect that probability distribution to stay on top of itself.

If it's not, you'll expect it to shift from side to side.

Yeah.

Or become completely flat.

Okay.

Or become completely flat.

I guess reincarnation would be kind of Gaussian flat Gaussian reincarnation.

Well, let's say it was, it was a tongue in cheek comment, but like I have a Gaussian distribution of being in a certain state.


SPEAKER_01:
Right.


SPEAKER_00:
I die.

I enter into the ether.

I flatten out and then I reappear as a frog and I enter into a new Gaussian distribution.


SPEAKER_01:
Yeah.

That's a good way to put it.


SPEAKER_00:
All right.

Nice.

Okay.

So is that, is that, how does that sound mathematically?

What does that look like mathematically?


SPEAKER_01:
Okay.

So, um, I think one, yeah, I think there's kind of like two scenarios.

Uh, one, one is that there's no steady state and the system dissipates.

Um, and therefore you're, you're like snapshots of, uh,

of like the density at each point in time will converge to a flat distribution.

Okay.

And the other case is that there is a steady state and the snapshots converge onto like a clear, well-defined distribution.

Yeah.

Which would be like a Gaussian or something else.

The point of a steady state is that if you start the system in that distribution, it will remain in that distribution.

Sure.


SPEAKER_00:
Although, okay, okay, fine.

And that's strict as in you will stay within that distribution or is there permission, let's say in sentient creatures like our own for that distribution to change somewhat.


SPEAKER_01:
Um, so that's a good question.

Uh, there's kind of like two answers to that.

You being at a pre-specified distribution does not mean that you're not doing interesting things.


SPEAKER_00:
Yeah.


SPEAKER_01:
So if we just think about a convection cell, uh, you basically like, or a tornado, you have like this circling around, uh, the probability distribution of where the gas is or where the tornado is just stays constant.

It's kind of like this ranging, uh, but you still have some interesting motion inside.

Um, right.


SPEAKER_00:
So I might not always.

So it doesn't mean that state.

Right.

Because I might just, you know, over time it will look like I'm converging on that state, but I might not always be there.


SPEAKER_01:
Yeah, you're probably it's kind of like hard to wrap your head.


SPEAKER_00:
Yeah.

If you think about like homeostasis, right, if you model, if you like, let's say temperature, if you model temperature, you can imagine it will have a Gaussian distribution.

So with the peak at 37.9 degrees centigrade.

But you are not going to spend your entire life at 37.9 degrees centigrade.

Exactly.


SPEAKER_01:
You're going to move around and will be according to this Gaussian distribution.


SPEAKER_00:
Right.

Right.

Okay, cool.

Okay, good.

I want to jump to that notion of the internal states tracking the external states, because I think people struggle with that.

And I definitely did for a while.

So the way that I read internal and external is tracking and tracked.

That for me has been a really parsimonious and useful way.

I think I got that from Maxwell, that that's a really nice way of thinking about it because the internal external is such a philosophically troublesome distinction.

So tracking and tracked.

And then I think of something like the equation for variational free energy.

And I think about the KL divergence between the recognition density and the true posterior and the maximization of model evidence.

Now in that, I guess I'm not,

I don't have to necessarily make this strict distinction between the internal states, my states, and the worldly states.

Because all I'm doing there is I'm saying, given some observation, I have a probability distribution of some state, whether they're mine or whether they're the world's.

Because for me, that distinction is not that strong.

Because for me, I think you could pose that actually all that we are are just this aggregation of worldly states.

I am one temperature distribution.

I am one glucose distribution.

That's not mine.

That's the world's.

So that's the first thing.

So we're doing this X given Y, this recognition density.

And then you have the maximization of the model evidence, which is the probability of Y given your model.

But again, there's nothing like,

internal that's going on there.

It's just statistics.

So do you think that it's somewhat troublesome to cast, like put this little homunculus or this little ego in the internal states and being like he's doing sufficient statistics?

Is that do you see that as slightly troublesome?


SPEAKER_01:
I think it's a nice picture for intuition, but it's certainly it can be misleading for the reasons that you pointed out.

Internal and external states can be interchanged

Yeah.

And again, it's all about the blanket.

Yeah.

The external, the external states could be part of your own body and the internal states could be part of your brain and the blanket somewhere in between.

Or the internal states could be like a school of fish and the external states could be like a predator.


SPEAKER_00:
Right.

Yeah.

I just never thought this is no attack on anyone.

I don't know who came up with it.

Probably Carl.

Sorry, Carl.

I never thought that was a particularly good formulation of the free energy principle.

I always thought that, you know, things that seem to persist look like they're maximizing model evidence for themselves is way better.

But this is something to take out with him.


SPEAKER_01:
Yeah.

But it's kind of like two things we're touching on here because tracking and the model evidence, they're kind of like two things that are going on.

Because when you think about it, the free energy is this KL divergence plus the surprise.

And we use this tracking to minimize the free energy.


SPEAKER_00:
Right.

So what I mean by that is the maximization of model evidence or

I think it's the minimization of log model evidence, but it's the same thing.

That in and of itself, the way I'm trying to frame it is that that entails some form of tracking.

Because you've got that probability of why, but it's sort of given your model.

So yeah, I just think for people, this is again, it's more of an education point about active inference, not about the maths or anything.

I just sense that for me, it's always been more intuitive to think about

That, although again, it's hardly intuitive, is it?

I mean, it takes a bit of time to really wrap your head around.


SPEAKER_01:
Yeah.

There's another important point that came out in the research when we were talking about the fact that you can, I mean, a lot of people say that you can always interchange internal and external states and that one is tracking and the other is tracked.

But it turns out that this is not always the case.

Because for the internal states to track the external states, they need to have enough like representational capacity, make inferences about all the internal states that are out there.

They don't have enough representational capacity.

Maybe they might just infer some or infer linear combinations of external states.

In any case, they're just going to have like part of the picture.


SPEAKER_00:
Okay.


SPEAKER_01:
That's interesting.

which means that for external states to track internal states, they also need to have like enough representational capacity and a sufficiently high quality coupling with their own sensory states with the boundary to make inferences about the internal states.

So you always have this synchronization and this inference going on, but you can't necessarily say that the,

internal states will be tracking all the internal states all the time or vice versa.

Right.

Yeah.

It depends on the nitty gritty of the coupling and how much representational capacity these states have.


SPEAKER_00:
Well, again, that's why for me, this maximization of model evidence just seems a little bit simpler because whether you're a drop of oil or a rock or a human, we can sort of, not arbitrarily, but we can generously give out models for each one of them and say that they look like they're maximizing model evidence.

Yeah.

like how long something lasts is a different question.

So the salt that I put in water maximizes model evidence for itself being salt for a very short amount of time.

So you could cast that as a surprise or being very high, but at least for a while it's salt, it's maximizing model evidence.

And then I guess it's maximizing model evidence within a Markov blanket that's the water salt solution.

Anyway, it's an interesting foray, but that's a good point.

Yeah.

We're now going to jump to the next part of this.

So people want to follow along, by the way, a lot of this is from the free energy principle made simple, but not too simple.

Or I think that's the name of the title.


SPEAKER_01:
That's right.

Yeah.


SPEAKER_00:
That's a great, this is a, well, I wouldn't say it's necessarily a perfect starting point for someone who hasn't done maths or physics.

I would recommend the textbook.

So our pet Sulu and Friston recommendations actually.


SPEAKER_01:
Yeah.

There's also a Maxwell's paper on a basin mechanics.


SPEAKER_00:
Yep.

That's very good.


SPEAKER_01:
I enough beliefs, and this is really cutting edge in describing the math, but it's also meant to be a much more approachable.


SPEAKER_00:
Yes.

Okay.

Yep.

So Maxwell's 2023 paper, which is on the preprint server, um, the textbook.

And I've also been reading and we'll come to this.

I've been reading Sanjeev Namjoshi's draft of his book.

It's being sort of distributed as a group in the Active Inference Institute where you can read it.

And that's been really useful.

So there's stuff out there and this podcast.

So yeah, but if you want to follow along with this, this is that free energy principle made simple but not too simple.

So the next thing that you come to there is solenoidal and gradient flows.

Right.

So maybe we can talk about, maybe we should start by just explaining what those two flows mean and how they're distinct from one another.


SPEAKER_01:
Okay.

Um, so this is a decomposition of the flow in the Langevin equation.

Yeah.

And so, uh, to put it simple, to put it very simply, the, um, um, well, just to give an analogy in vector calculus, you have this.

fundamental theorem about, uh, that's known as the Hemholtz decomposition, which says that if you have a vector field, so think about a bunch of vectors sitting on a table, uh, you can decompose them onto, uh, one that's kind of like circling around an origin to put it simply, and one that's going inside or outside.

Okay.

So mathematically, this is what we're doing here to decompose, uh, the flow in the larger equation.

Now, what this really means in practice is that you have, uh, the solenoidal flow, which is the interesting part of the flow and the, how did you call the other one?


SPEAKER_00:
Uh, what do I call it?

Gradient flow.


SPEAKER_01:
Right.

And there's the gradient flow.

That's the uninteresting one.

If you are, um, if you are a living system, you have by definition solenoidal flow.

And why is that?

Solenoidal flow is related to time irreversibility and entropy production.

Okay.

If you don't have solenoidal flow, it means that the system, if you play it in time, as time goes forward, and then you reverse the movie, statistically the system will look the same.

So it's essentially a dead system.

It could be like a stone, for instance.


SPEAKER_00:
Yeah.


SPEAKER_01:
On the other hand, if I, you know, you played a podcast for 20 seconds as time goes forward and then you played backward for 20 seconds, he will look super weird and very different.

Right.

The proof that there is time irreversibility time has a lot, there's a direction.

Um, and this is yeah.

Characteristic of the fact that we are alive.


SPEAKER_00:
Interesting.

Okay.

So let me just get some, some more, you know, some simple things.

outlined here so you said the helmholtzian the helmholtz decomposition is of the flow is that if i'm looking at the longevin equation that's not that doesn't include the random noise that's just f of x f yeah it's a decomposition of f of x but also done in relation to the noise term so is the noise term so would that

i'm asking here is when i'm splitting it into solenoidal and gradient is that splitting just the f of x or the f of x and the thing yeah okay cool so okay so there's a there's a dissipative aspect in all of this so the analogy that i thought of was um

If you create a whirlpool, let's say in your sink or in a cup of coffee, you get this solenoidal flow, right?

It's conservative, but due to, I presume just friction, you get this dissipative, uh, this, this specific aspect where the swirling motion fades away.


SPEAKER_01:
Yeah, totally.


SPEAKER_00:
Right.

Is that, is that the gradient flow then sort of infiltrating a purely solenoidal flow?


SPEAKER_01:
Yeah, that's absolutely right.

Another example is if you put milk in your coffee, there will be like the standard mixing of things, which will be the gradient flow.

And if you start stirring the cup of coffee with a spoon, then you're adding, say, a little flow.


SPEAKER_00:
And is it where they meet that things become, like we can say things are dissipative, because for me, philosophically, dissipative means that you had order and now you've got less order.

But if everything is, if there's no solenoidal flows, then arguably there's no, is there any order to begin with?


SPEAKER_01:
It's hard to say.

I have trouble relating these notions to order.

But I think there's an indirect way of relating them, which is solenoidal flow is always something that circles around something.

You think about

the circadian rhythm, us waking up and going to sleep, all these are cycles.

They are examples of solenoidal flow.

Solenoidal flow produces entropy.

So I think that's in some sense related to order and disorder.

If you want to maintain as a living being, if you want to maintain solenoidal flow or just as a physical system, you need to be using energy and dissipating entropy.


SPEAKER_00:
So that's kind of the idea.

And that's just a consequence of the mathematics and the physics.


SPEAKER_01:
Yeah.

Yeah.

Basically.

Okay.

The fact that we have so in order flow within us, like rhythms and cycles means that we need to eat and, uh, and secret.


SPEAKER_00:
Right.

Right.

Okay.

Okay.

Because in some sense, the w that's a form of order, which is neg and tropic.

Yeah.

Yeah.

Okay.

Now, OK, so what is this?

The other thing that comes up is the term divergence.


SPEAKER_01:
Right.


SPEAKER_00:
So people say that solenoidal flows are zero divergent.

Yeah.

What does that what does that mean?


SPEAKER_01:
Right.

It's kind of that's just kind of a mathematical term.

Uh, well, uh, uh, because if you take the divergence of the vector field, it's zero getting to that, I'll give you a picture.

If we, if we are at our steady state distribution, so let's say, um, our system is two dimensional and it's distributed according to Gaussian.

That's sitting on my table here.

Um, the flow is a motion around the contours of this Gaussian distribution.

Hmm.

And the gradient flow is motion towards the peak or away from the peak.


SPEAKER_00:
Interesting.


SPEAKER_01:
And so we have our overall motion, which is given by the flow in the Langevin equation, which could be in any direction.

And then the fact is you can decompose it on motion towards the peak.

So towards states that are more expected or preferred and motion away from the peak.

strictly away from the peak these two are divergence free flow you can also the other part of the decomposition is motion that just goes around the contours of the distribution which means that it does not approach the peak or go away from the peak is it if i'm thinking about it diagrammatically does it matter where that circulation is going does it so so it could be at the base of the gaussian which means it would in some like again just


SPEAKER_00:
diagrammatically means it would be wider or it could be at the top of the Gaussian, which means it's occupying these high probability states.

Does it matter?


SPEAKER_01:
Um, I think it matters as a signature of the system.

Probably.

I mean, this idea of certain auto flow is one of the most important like signatures of, uh, of being alive and what your states are doing, what kind of cycles you have in your system or in yourself.

So it's a very important descriptor of what's going on.


SPEAKER_00:
Okay, cool.

And the other thing was, and again, you can tell me when I make a terrible faux pas, is that solenoidal flows break what's called detailed balance.

Yeah.

And what I've learned about this is that detailed balance is when the rate of transitions of any two states is equal in both directions.


SPEAKER_01:
That's right.


SPEAKER_00:
So you can think of it in terms of, I thought of it in terms of a crowd going in and out of a,

like a stadium or in and out of a room, that rate is going to be the same in and out.

And that, I believe, and again, you can tell me if I'm wrong, there's something to do with Boltzmann's H theorem, which says that that detailed balance implies zero net entropy.


SPEAKER_01:
Right.

So two parts.

It's absolutely right that detailed balance means that there is flow

Um, in and out, say, uh, of a stadium at the same rate, it means that if you play the flow in and out of the stadium forward or backward, it will look the same.

Yeah.

There's no production of entropy and there's no sort of flow and the system is time reversible.


SPEAKER_00:
Yeah.

Yeah.

Yeah.

Okay.

So that kind of explains why it's sort of net net zero entropy, right?

If entropy is foundational to the passage of time.

yeah yeah yeah basically yeah as soon as you have so no flow time uh starts taking place you start having a narrow time okay so if if we took so yeah okay so this is interesting so let's let's kind of this is an odd exercise but we can kind of build a human from scratch right so let's try right um

So the gradient flows are going up or down the probability distribution.

So we take a human as just a Gaussian distribution.

It's kind of going up or down.

But let's say without any solenoidal flows, we have this detailed balance.

Yeah.

So it's going up and down now.

Does that not mean that because it's doing it equally, you're going to end up at that peak?

No, because you have equal flow towards the peak or away from the peak.

So how does that average out?

What does that look like in terms of where the crowd might be?

Or they're just everywhere?

Are we talking about the human or the crowd going in and out?

Let's say the crowd.

Let's say because we've got detailed balance.

So let's say the crowd.

Okay.

I'm going to say, okay, uh, I have a random friend X and that they belong to this crowd.

That's, uh, behaving in terms of detailed balance.

And I want to find them.

Is there a probability distribution that could help me to find your friend?

Yes.

Who's going in and, you know, it's part of that tide.


SPEAKER_01:
Um, yeah, I'm not sure about that.

I will just say is like,

If you have one exit in your stadium with people going in and out at the same rate.

Yeah.

And they're kind of like mixing with each other.

Who is who you have detailed balance.

You just start having one entrance and one exit.

Then you do not have detailed balance because you have people coming in one way and going out the other way.

So you start having a cycle.


SPEAKER_00:
Ah, okay.

Now that's, that's okay.

That's really useful.

Nice.

So let's talk about the human in a sense.

Cause the human is a combination of, we're kind of what Carl would say we're in this Goldilocks zone.

So we're not orbital planets with just solenoidal flows.

So we, so we're not just this secular circular thing, but we're also not just not neg entropy, non entropy one exit.


SPEAKER_01:
Right.


SPEAKER_00:
So we have both.

So we have both.

Yeah.


SPEAKER_01:
Yeah.

It's a very interesting sweet spot.

If you just consider systems that have just the gradient flow, they're dead.

And, you know, they might look like gases and stones and stuff like that.


SPEAKER_00:
Yeah, yeah, yeah.


SPEAKER_01:
If you just look at things that have solenoidal flow, well, I would say they're probably dead as well.

It could be like planets orbiting around the sun, like they're going one direction and not the other.

And we are kind of like in between where we have both going on.


SPEAKER_00:
So I guess this is where my question was going, which is, okay, let's take a rock.

The rock has just gradient flows, so it's dead.

But it seems to me that the rock is doing a pretty good job at self-evidencing.

So does in some ways the solenoidal flow take us away from, yes, it gives us animacy and whatever, but does it take us away from just being able to rigidly self-evidence, rigidly maximize model evidence?


SPEAKER_01:
No.

No, no, it does not.

And so the beauty of the mathematics here is that we're, we're taking the most general sort of equation that can describe the world.

So the principle applies if there is some note of flow and no dissipative flow, there is dissipative flow and flow.


SPEAKER_00:
I mean, it applies if there's both.

Yes, but what I mean by this is it's probably worth mentioning that I'm kind of thinking about this over time.

So my plastic bottle is going to outlive me.

And because things over time have to abide by the free energy principle, they have to look as if they're maximizing model evidence or doing some variational Bayesian inference.

You could say if we had some normative sense, which again, I'm quite skeptical, but you could say like the water bottle is doing better self-evidence over a period of time.

Uh, yeah, over, over a long period of time, certainly.

And that's because, well, that seems to me to map on quite nicely to the solenoidal gradient flow distinction, because like if rocks and, uh, I dunno, particles are not solenoidal.

Where am I?

I feel like, I feel like there's something I'm missing.


SPEAKER_01:
Yeah.

That's, uh, I mean, I think it's very interesting.

You point out that things.

that do not have serinoidal flow sort of endure over a longer period of time.

I never thought about that.


SPEAKER_00:
Okay.

Okay.

Fine.

Fine.

Fine.

Fine.

I won't push it then.

Okay.

That's really okay.

Yeah.


SPEAKER_01:
It's interesting.

I need to think about it.


SPEAKER_00:
Yeah.

Yeah.

Yeah.

Well, God, don't know what kind of worms I've just opened up.

I really don't want to get shouted out on the comments.

Okay.

So something I do understand a little bit better is just KL divergence.

That's more up my street because I feel like it's a little bit more.

cognitive sciency.

So there's callback Leibler divergence.

And as I said, this can be cast as a distinction if we're doing Bayesian inference between a true posterior and what's called a recognition density, which is like your version of a posterior.

Now, there was something you brought up in that, or Carl brought up or whoever wrote it in that simplifying the free energy principle paper, which is called a Fisher information metric tensor.

which measures changes in the KL divergence.

It might be so people, I think view the variational for energy as a kind of snapshot in time.

So given like this perceptual inference, how big is my KL divergence given my action, how much surprises there, but this seems to me to integrate change over time.

So what is the Fisher information metric tensor and how can we integrate that into a more accurate or more faithful model of the world or model of ourselves?


SPEAKER_01:
Right.

So once you accept that you have a recognition density about the world and that this is minimizing free energy, you can ask how much does your recognition density or your beliefs about the world have to change in order to minimize free energy?

And so if you want to quantify how much your beliefs move in a certain amount of time, you need what's called an information metric.


SPEAKER_00:
information metric in the sense that it quantifies how much information changes over time the information and the information the fisher information metric is what allows you to do that nice okay cool are you are you familiar with this notion that's been coming out of maybe like the more philosophical or cognitive science side of active inference so people like mark miller and julian keverstein of prediction error dynamics um


SPEAKER_01:
I mean, intuitively, I can see what that would mean.


SPEAKER_00:
Yeah.

The idea is that we have a higher order prior that we will track how good we're doing at minimizing free energy over time.

And this in terms of psychology maps onto something like positive or negative affect because we're doing well.

We're doing well.

We're finding these slopes of variational energy that we can descend down.

And it's kind of an explanation for why we don't seek out dark rooms.

So there's one other solution to the dark room problem.

It sounds quite similar to what you're saying, which is, except what you're saying seems strictly, I don't know, in terms of the KL divergence tied to perceptual inference, but we can say that a system is becoming better at perceptual inference because over time, those KL divergences are going down.

Is that

Reasonable to say?


SPEAKER_01:
Well, yes and no, because if, if the data stream sort of like stayed constant, or if you stop the data stream, then your KL divergence would go down because you would kind of like get to the free energy minimum.


SPEAKER_00:
Yeah.


SPEAKER_01:
But you always have new data coming in.

So you have two forces at play.

You have you, uh, who are like predicting the data and minimizing free energy, but then you have data that's potentially increasing the free energy.

Right.


SPEAKER_00:
But it's the world, but going back to our Langevin equation, the world is the world is modelable in some sense.

It's predictable in certain senses.

So I, I may be wrong, but it seems to me that the more data you get over time, the better your model is going to be.

Right.

It's, it's just gonna become more powered, right.

To use a statistical term.

what what's the kind of, so that seems to me that like your information metric should be pointing that, you know, your KL divergences are going down, even if you're getting more data, because that data is just, it's got a similar distribution to the data that came before.


SPEAKER_01:
Right.

Okay.

So, so I see, I see where, where the confusion lies.

Um, I agree with you that as we get more data, our model gets better and our free energy, um,

in general tends to be lower so that's one truth the other truth is that even though even even though i mean that's a fact you're the data that's coming in keeps changing uh which means that the posterior density the true the truth keeps changing so your recognition dynamics always have to keep tracking this procedure right right right now the fish information looks at how much these um


SPEAKER_00:
recognition dynamics have to travel in a certain period of time to match it to posterior is there an equation that governs that because obviously i've become obsessed with equations um is there is there how much your beliefs change no not just how much your beliefs change but kind of how the data changes how the data changes that's the launch of my equation that's the launch of an equation but it's so that's the function of the x being the worldly states

Got it.

Got it.

Okay.

Okay.

Yes.

Okay.

I see where my confusion was.

I guess I was viewing the world as kind of static.

I mean, you can even like take away the random noise, right?

If your F of X is changing, your agent who's modeling that is going to have to keep updating.

Yeah.

Nice.

Nice.

Okay, cool.

That's very useful.

Okay.

Excellent.

Okay, have I got any more maths?

I got a bit more maths.

I guess something I wanted to ask about is that this is a broader question.

So in active inference, we have discrete state spaces and continuous state spaces.

And it's kind of about how we carve up time in some sense.

So continuous state spaces is where it doesn't really make sense to put time into separate chunks or sequential chunks.

So, you know, if you're thinking about attention or perception or perception, let's say, I know perception includes some action, but let's just take perception.

It's better to kind of model that in terms of a continuous state space.

If you're thinking about decision-making, you would think about that in terms of discrete state spaces, because an action policy is a discrete thing.

In discrete state space, we've seen this kind of proliferation of so-called POMDP schema.

Um, so these are these partially observable Markov decision blank processes.

I've always think about Markov blankets.

Um, is there an equivalent of that in continuous state spaces?


SPEAKER_01:
Uh, so you can definitely have continuous, uh, partially observed Markov decision processes.

Um, so, so let, let me take a step back.

There's two things at play.

There's a.

whether the state space is discrete or continuous, and whether time is discrete or continuous.

Now in active inference implementation so far, you basically have either continuous space, continuous time, or discrete space, discrete time, or both.

But people haven't really been playing with continuous space, discrete time, continuous time, discrete space.

But there's no reason why, in principle, you would not be able to do that.


SPEAKER_00:
Okay, that's cool.

This is, again, my very much layman knowledge of physics.

Do we not take time and space to be somewhat interwoven?

Yeah, this is a bit beyond me.

I guess a way around this is to say that we're modeling, right?

So in our modeling, we can take time as continuous and space as discrete and vice versa.

Yeah, this is this is only about modeling about how you represent the world.


SPEAKER_01:
Yeah, there is a deep, a deeper question, which is how do we ultimately like represent the world in physics, right?

It's some series, you know, consider time and space as jointly but for us humans, yeah, we don't need to get into that for active modeling.


SPEAKER_00:
Okay, so we've we've kind of clarified that.

So

when so where would the pomdp schemes be able to be used if it's not just discrete time discrete space can they applicable to continuous space continuous time so so where would you actually use them so i've seen for example just to give some background i've kind of seen pomdp schemes used in mainly just aspect like context of decision making so um for example yeah going back to the flow paper flow involves selecting action policies

Meditation involves mental action.

Attention, right?

Like intentional attention, endogenous attention is a decision.

These can all be modeled quite nicely.

But to my eyes, everyone who talks about perception will use something like a predictive coding scheme.

I'm just curious about if there is an alternative to that in the literature.


SPEAKER_01:
In the literature, no.


SPEAKER_00:
Okay.


SPEAKER_01:
I was talking to Ryan Smith recently, and he was interested in continuous state POMDP with discrete time.

And so the idea here is that you can model a continuous representation of the world and the sequential decision-making at the same time.

So it opens up a lot of possibilities.


SPEAKER_00:
Okay, cool.

I'm speaking to Ryan fairly soon.

So that will be useful.

That'll be cool.

That's a good thing to bring up.

Yeah, that's really interesting.

I don't know.

This brings to mind, there's this incredible paper by a guy called Juan Diego Bogota.

And I think his name is Zachariah Jabara.

And it's sort of synthesizing subjective time and objective time for a Bayesian model.

And I'm going to tell you, I stared at that graph for about six hours.

Um, and it still didn't make sense.

So that's, you know, if anyone wants to read that and try and think about how maybe not space and time, but like two different types of time can be coalesced, that's a good place to start.


SPEAKER_01:
So I'll give you another theory of mine.

Uh, I did this wonderful watch workshop on computational neuro phenomenology, uh, last year.

And I had a wonderful conversation with, uh, annual Seth, and we've kind of converged onto the hypothesis that subjective time

is the amount of distance that your beliefs travel over some period of objective time okay cool here's the thing like when you when your beliefs move a lot yeah you can quantify that with the fisher information metric you would have a perception that a lot of time has elapsed okay so that you've now got me on this new pet hobby of mine because uh i'm sure you haven't seen it but literally yesterday i published a pre-print


SPEAKER_00:
called distrusting the policy, how inference over action shapes our perception of time.

That's cool.

So please, I would love some feedback.

Basically, I wrote it.

So this starts off as being trying to explain temporality and flow states.

And I thought, no, actually, this works.

I think this works more broadly.

So I'm not going to spoil it.

Well, I will spoil it.

But basically, my idea.

So I'm building off what Jacob Howie called distrusting the present.

So this is very similar to what you're saying.

So Jacob Howie has this idea that the flow of time is contingent on the rate at which we update our perceptual hypotheses about the world.

So if I'm in a volatile environment, the state estimation I have at time t is not going to work for t plus 1.

And so what happens is the moment I find t, I immediately start discounting it, which you can consider sort of precision weighting.

And immediately I have to update that and he calls that now.

I, I don't think that that totally maps onto the phenomenology because I'm really curious about what you think about this.

And again, I will try and do it quickly cause I don't want to bore you, but we have, um, so I say, so he was kind of, it seemed to me that he was rooting, uh, flow of time in predictability, but the problem is.

you're going to cast it only in terms of perception or only in terms of the a matrix then you've got a slight problem because there are two different types of predictability you've got one which i call static predictability which is basically when you're staring at a wall now when you're staring at a wall your perceptual hypothesis is going to be pretty good let's say over a prolonged period of time and so jacob's theory says

Time will slow down.

And it does, right?

You stare at a wall for an hour.

It's going to feel like a day.

Yeah.

But then the other problem is something called dynamic predictability.

So let's say you play the video game level once.

It's very exciting.

Time seems to flow quickly.

Now, let's say I make you play that same video game 100 times.

Now, perceptually, the world is changing.

You got to do all of that state, like that state estimation, that combination of the D matrix and the A matrix needs to keep happening at the same rate.

And I want to flesh this out experimentally, but I have a hunch that time is going to move way slower.

So what I've now said is it's about the frequency with which you update your action policies.

And the reason why I said that is because with each action policy updating, you also have to update your beliefs in the C matrix.

And the C matrix is your expected, uh, your preferred sensory observations.

And that I think accounts for the perceptual aspect of the flow of time.


SPEAKER_01:
That's a really interesting hypothesis.

Yeah.


SPEAKER_00:
Yeah.

So I, so it's Friday the 19th of January on Tuesday, people are not going to be able to, you know, this is not, people can't go back in time, but anyway, I'm doing a talk about it at the, at the Institute on Tuesday.

Um,

So I think we're converging on similar points here.

I think it's kind of where is the source of your information change?

So for me, it's the C matrix, really, because it can't be the B matrix, because in dynamic, predictable B matrix of state transitions in dynamically predictable situations, like playing the same video game 100 times, I'm going to have the same number of B matrices compared to when I played it the first time.

So I don't think it's the B matrix.

But because you have to keep doing this expected free energy calculation for each action policy and the C matrix feeds into that, my hunch is that it might be rooted in that.


SPEAKER_01:
Right.

You could also think about the fact that if you replay the video game like many times, your beliefs still have to move to adapt to the current situation in the video game.

But there will be less and less novelty.

Yeah.

Because you adapted your like big world model to like predict the video.

So how that translates to in practice is that you have low level beliefs that move a lot to adapt to a situation, but high level beliefs that don't move.


SPEAKER_00:
Okay, nice.

Yeah.

Again, I think I'm trying to get this to be done experimentally to see if this theory maps onto anything.


SPEAKER_01:
It would be really nice to test.

But that's a very interesting... Yeah, I'm always in favor of all of the empirical experiments.

I feel as a community, we're lacking that.


SPEAKER_00:
Right.

And I think it shouldn't be that hard to do.

I think you just have to give people statically predictable context, dynamically predictable context, and flow context.


SPEAKER_01:
Yeah.

As soon as you can map a POMDP onto the data that's coming in, it'll be very easy to model everything that's going on.


SPEAKER_00:
Well, I may reach out to your laws on that one.

Um, okay, cool.

So that, that was a fun little foray.

Um, okay.

Actually let's, let's, um, we, we spoke, we very briefly touched upon attention.

Um, and I have a question about attention.

If I may, I've, I've been, I've been, I've been grappling with attention for quite a long time.

Um, so attention is kind of construed as the optimization of the likelihood distribution and active inference.

So the a matrix in these Palm BP schema.

Now, there are these really lovely papers by, his name is Mirza, so Mirza et al, 2016, 2018, and 2019, I think.

And he shows, he gives this example of like searching for a red pen.

And he says that when you're searching for a red pen, what you do is you downweight the sensory precision of stimuli that are irrelevant to that context.

And so

Basically you end up doing salient behavior, which is, we can view as epistemically salient because it yields information gain, but the information that you want, um, now given the, um, yeah, so that makes sense because task relevant stimuli are downweighted.

So only task relevant stimuli involved in this maximum information gain.

Okay, fine.

But pragmatically, so, so that episode, that information gain is like, I didn't know where the red pen was.

Now I know where the red pen is.

Yeah.

But in terms of pragmatic, in terms of pragmatics, we know that action policies in active inference are driven by a kind of almost a false belief, a false preference.

So I might say, I have a red pen and I'm going to go out there to confirm that belief because without it, there's lots of prediction error or variational free energy.

This seems to be a kind of issue because on one hand,

I'm gaining information about the world.

So I kind of knew that I didn't have a pen and now I know where the pen is.

And so I have this KL divergence.

But then I also have this oddly pragmatic belief that I do have a red pen because without that belief, I wouldn't be motivated to go and get the red pen.

So I'm trying to grapple with how to resolve that clash.

And I'm curious if you can help.


SPEAKER_01:
Yeah, not sure.

what i'm thinking is uh i always think about attention as control of the a matrix like in large largest paper on meditation uh you make some some uh sensory modalities more precise and others less precise yeah and it feeds into into your computations yes so


SPEAKER_00:
Yeah, see, I've always found it quite useful to distinguish precision from precision weighting.

So precision I just view as the kind of inverse entropy of a distribution.

But precision weighting is actually what I do.

I weight precision.

And that's not all going to be down.

Weirdly, precision weighting is not all down to precision.

I think it often is.

It often is in terms of

So they talk about how attention is basically, one definition of it is the capacity to kind of sift through the data that you currently have for the most informationally reliable or precise data, right?

So the one that's going to say, okay, given this, I know exactly what caused this.

But clearly, like, for example, in my searching of the red pen, there's going to be loads of things in the environment that are

that have a very strict relationship mapping between the observation and the state.

So while I'm scanning, I might see something and go, oh, obviously that's my phone, right?

It's not ambiguous whether it's my phone or not.

But the whole point of this Mirza paper is I don't care.

So I'm down weighting that stimuli.

But that has nothing to do with how well it maps onto some state.


SPEAKER_01:
Yeah.

Does that make sense?

Okay, so if I understand correctly, this is because you want to conform to your prior belief.

And therefore, if your prior belief is very precise, you will discount whatever sensory information comes in.


SPEAKER_00:
Right.

So my prior belief is something like, I have a red pen.

And so that's very precise.

And yeah, so that leads you to discount things that are, because you're, you're, you're, yeah, you're only going to pay attention to things that are task relevant.

Yeah.

But that's kind of the point is that kind of a belief saying I have a pen, but then there still is this epistemic game, which is at the heart of salience, which is I didn't have a pen or I didn't know where the pen was.

And now I do know where the pen is.

And so you're starting with, I don't know where the pen is, but for your act to trigger action, you have the belief.

I know where the pen is.


SPEAKER_01:
To, to trigger action, to look for the pen.

I'm getting, I'm getting a little bit lost.


SPEAKER_00:
Okay.

So, so, um, preferences underlie action, right?

So I can see matrix.

So I have the preference about the sensory information that I expect to receive.

So let's say preferences can be cast as just beliefs.

So I believe, so for example, in some very sub-personal way, I believe that I'm going to be at 37.9 degrees.

And so I act, so I remain around that.

Now, I want to find my red pen.

We can cast that volition as a preference, as a belief.

I know where my red pen is.

And the reason why you would do that is because, well, the world is not giving you that information.

And so you've got prediction error and you want to minimize that prediction error.

So that triggers action to confirm the belief that you have a red pen.

Okay.

That's on one side.

But on the other side, what captures attention is that which is most salient.

And what salience can be cast as is epistemic gain, information gain.

And that, for me, in this red pen example, looks like, well, I didn't know where the red pen was, and now I do know where the red pen is.

But what you've got on one hand is on the epistemic side, you start with the belief, I don't know where the pen is, because if I didn't know where the pen is, finding the pen would not be very useful, right?

So that's inhibition of return.

That kind of explains inhibition of return.

That's on the epistemic side.

On the pragmatic side, I have a pen.

So I'm just asking how those two things come together.

I'm so sorry, Darius.

I think my brain is fried.

That's all right.

I'm sure someone in the comments will have an answer.

I kind of feel that Carl will just have an answer.

I mean, I'm sure.

Maybe not.

Maybe not.


SPEAKER_01:
Maybe a research paper.


SPEAKER_00:
Yeah, maybe.

I know that one's going to take me down some wormholes.

so okay last thing i guess i want to talk about is you went on the machine learning street talk podcast and you spoke a lot about core knowledge which i thought was really interesting i really didn't expect that to come up um but i know it's one of your interests yeah what are the kind of priors that need that are inbuilt that facilitate intelligence sentient behavior so i guess

What are your current thoughts on how, so it's interesting, cause I actually come from a linguistics background.

So I told you before we started and at least, you know, at Oxford where I was there, they love Chomsky.

So Chomsky has this notion of universal grammar.

There's something built into the genetic code that facilitates language learning.

So I'm very familiar with these arguments, poverty of stimulus and these arguments in terms of nativism.

What are your thoughts on how fleshed out those priors are, um,

in terms of being innate?


SPEAKER_01:
Well, I think there's definitely innate priors when we're born.

This is all studied.

I think one of the best people is Elizabeth Schpelke.

Her whole career describing and characterizing those priors.

These seem to be incredibly precise, much like we cannot change them.

They seem to be super general in the sense that they would apply to any kind of a naturalistic or physical environment that you could ever be bored in.

So these are things like object permanence.

Two objects cannot interact at a distance, but two people can interact at a distance.

So two agents can.

I think there's a core knowledge system for numbers, learning numbers.

There might also be a core knowledge system for language.

They might also be one for vision.

So there's six of them and she wrote a book about it, which is what babies know that I absolutely recommend with all of that research.

So it's a big research program to actually being able to understand those and reveal those coronary systems.

It's another one to be able to reverse engineer them.

Interesting.


SPEAKER_00:
Yeah, I think one of the issues, I guess, is at least

the history of linguistics one of the great problems was how rich are these priors so this is what later got known as darwin's problem uh because chomsky has this idea that well in the 80s he had this principles and parameters idea which is that what ug is is a bunch of parameters that can be switched on you know given sensory data those parameters get switched on or off and then the problem is you have this massive proliferation of parameters

So there were 500 parameters and finish people.

There was one only for finish, but obviously we all had it, but only unless you were exposed to finish did you ever turn it on.

And it just became completely inexplicable given evolution.

Because how on earth did one mutation or a couple of mutations trigger that?

So he now thinks that there's something a lot more pared down, something like recursion.

And I think why that's interesting is because recursion is not

It's not what we call propositional knowledge.

It's not knowledge that something.

So Chomsky might say, oh, okay.

In the 60s, he might have said, okay, I know that the baby knows that after this comes this or something.

But now this is just actually a mechanism.

So what we might call procedural knowledge.

When you think about core knowledge for maths or core knowledge for vision, or I guess vision really won't be propositional, or core knowledge for language, are you construing these as propositional or procedural?


SPEAKER_01:
Um, so well, these would be priors in the giant of model, which could be expressed, um, in a number of ways.

It could be, uh, sort of like, um, like a grammar of, uh, like core components that you already have in your giant of model.

Well, let me put it in a different way.

Like when we're born, we have a very simple genitive model and gradually we learn about the world around us and we make it more complex.

Now what we have at the beginning is this core knowledge.

It's kind of like a starting point genitive model that allows you to structure learning, learn about everything else.

Now the structure of that is a very debated.

among computational people.

So for example, Josh Tenenbaum is pursuing this idea of probabilistic programs, which are based on symbolic grammars and probabilistic rules between different entities.

Then there's other people who might model these things with neural networks.

So in those cases, you don't have symbols, you don't have that grammar going on.

It's very much an open area of research.


SPEAKER_00:
Cool.

Yeah.

I mean, I guess all these neural networks still have some priors, um, in, in a lot of cases.

Yeah.

Awesome.

Yeah.

Yeah.

I used to love these debates between Chomsky and people like, um, Jeffrey Elman sort of mid 1990s, proper hardcore stuff.

Cool.

And yeah, I, I, it's interesting.

You mentioned vision.

Cause I always thought that for me, the one glaring one where there needs to be priors is vision.

Vision is perception is well, visual perception is such an amazing capacity.


SPEAKER_01:
Yeah.


SPEAKER_00:
To be able to do right.

Yeah.


SPEAKER_01:
There's a startup called, uh, common sense machines that has pushed the core knowledge of vision really to a state of the art level where they have a world model for vision, and then they do inference on that world model.

And they're able to reconstruct 3d scenes from 2d images in a pretty amazing way.

So.


SPEAKER_00:
Yeah.

Have you seen these things?

This is slightly different, but you have these neural nets that are trained on brain imaging data.

And they can take an MRI of the brain and say that this person was thinking about a dolphin or something.

They can have an image of what that person is thinking about.

Obviously, we're given the training that there is this, I'm not going to say causation correlation, I won't go into it, but that when this brain data is like this, this person is thinking this, and so you have the testimonies of the people.

But it just shows the power of those and the kind of nuance of those large language models that they can be able to do that.

But actually, I think it's,

arguably less exciting than people think it is because they forget about the training process that underlies it.


SPEAKER_01:
Yeah.

And it doesn't allow you to understand.

It just allows you to predict.


SPEAKER_00:
Exactly.

Exactly.

And that's where, that's where our, our favorite topic of active inference might, might come in maybe and yield some artificial understanding.

Hopefully.

Hopefully.

Well, people are getting very excited.

They're getting very excited.

Lance, that was, that was awesome.

I feel like I've learned.

I feel like I've gone to university again.

Yeah, me too.

No, this was, this was so much fun.

I knew it would be fun.

Thank you for being gentle with me and kind with the maths.

Well, thanks for having me.

No, it was an absolute pleasure.

Um, I really want to see what comes out of this stuff, especially with annual about the Fisher information metric and time.


SPEAKER_01:
Uh, it's going to come out.


SPEAKER_00:
Really cool.

Cool.

It sounds like we're converging potentially on a similar idea.


SPEAKER_01:
So it would be really nice to talk.

I need to flesh out these ideas and then, uh, yeah, we can talk more.


SPEAKER_00:
Splendid.

Okay.

Well, thank you from Langevin and all of this stuff.

It's been an absolute pleasure.

So thank you for myself and the Institute.


SPEAKER_01:
It's been a great pleasure for me too.

Thanks.
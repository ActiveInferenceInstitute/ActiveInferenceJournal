SPEAKER_02:
Hello, everyone, and welcome back to Active Inference Insights.

I'm your host, Darius Parvizi-Wayne, and today I'm delighted to be speaking to Vanya Visa.

Vanya is a postdoctoral fellow at Johannes Gutenberg University Mainz in the department of... Royal University Bochum, sorry.

The internet has lied to me.

The internet has lied to me.

I don't like this introduction bit already.

But I think this is true nonetheless.

His research centers on consciousness, mental representation, philosophy of cognitive science, philosophy of mind, and of course, active inference.

We also both share a love of self-modeling, attention, autopoetic intentionality, and much, much more.

And I'm sure these are just some of the topics that we will discuss today.

Vayner, welcome to the show.

Thank you so much for joining me.


SPEAKER_01:
Thank you for having me.

It's a pleasure.


SPEAKER_02:
No, this is really exciting.

Yeah, as I said in the intro, we share a lot of interests that might appear quite niche to people, but actually, as far as I can tell, pertain to their lived experience at every moment.

So it's worthwhile dissecting.

Funnily enough, the first time I think I'd heard of you was actually from Jacob Hovey in the conversation I had with him.

And he said that you know Thomas Metzinger's work perhaps better than Thomas and certainly better than anyone else.

So I wonder whether we could start by outlining some of these crucial concepts that sometimes seem a bit counter-intuitive to people.

So for example, when I say we both share a love of self-modelling or phenomenal self-modelling, what does that mean?

What does this sort of acronym PSM, the phenomenal self-model mean?


SPEAKER_01:
Yeah, good.

So maybe as a disclaimer, of course, Thomas knows more about this than me.

invented the term self-models and has developed the self-model theory of subjectivity over many many years and actually I think I can say that there's probably no living philosopher who has read as much relevant literature about this topic as Thomas because he's devoted his entire career to this and

works a lot and I would never, even if I tried, be in a position to know as much about this as he does.

And I think it might be a good idea even to invite him to this podcast.


SPEAKER_02:
You've been invited.

He has said yes.

that was a while ago, I tried to push it.

So Thomas, if you're listening, which I think he might be, you know, you never know.

Please do come on.


SPEAKER_01:
Yeah, so actually, it's quite likely that some of the things that I'll be saying might not be 100% true.

So Thomas, if you're listening, now, you may want to correct some of the misrepresentations of your work.

But anyway, yeah, what is a self model?

So for me, it's, I

think easiest to first think about bodily self models.

So just imagine some creature or a robot that has a body and has to control the body.

And as we all know, in many cases, control can be facilitated

or improved if you have a model of the thing that you're controlling.

And if you want to control your body and the interaction with the environment, it can be highly useful to have a model of your body and a model of your environment.

And this is actually then already a form of self model, which is part of the world model.

So the world model would be just be a representation of

the world or the relevant part of the world, the parts that are relevant to you and the, your environment and your body is part of this world.

So it's the self model is part of the world model, but of course that's just a form of self model.

And it's quite primitive probably compared to the self models that we

human beings have.

And one thing that distinguishes ourselves models is that we don't have only unconscious self self models, but actually we have, we also consciously experienced the contents of our self models.

So this brings us into the notion of a phenomenal self model.

where phenomenal refers to phenomenal experience, phenomenal consciousness.

So if you have not just an unconscious self model, but a phenomenal self model, you actually experience your body as your own body.

And another form of selfhood that you then might experience is experiencing your own actions as your action.

So this is often called a sense of agency that you have over your action and a sense of ownership that you have over your body.

And both can also get lost or be affected in various ways.

And so these are some

concepts that are relevant here.

There's one thing that I wanted to add, or maybe an example that Thomas also likes to use, I think, is this robot Starfish that was developed by Josh Bongard and colleagues.

And in order to make it flexible and able to adapt to changes to the environment and also to its own

So for instance, if one of its limbs is damaged or is cut off, you want the robot to still be able to control its body and control movements.

And so it can be useful to have a model of the body, a self model.

But of course, in that robot, it's not a phenomenal self model.

It's not a conscious robot.

So that would be an example of an unconscious bodily self model.


SPEAKER_02:
Yeah, let's start with the body.

I think that's a good place to start.

Because as you say, it's kind of the low hanging fruit, the lowest level of what people think about in terms of the phenomenal self model.

I think a potential point of confusion for people is the fact that the body has what I guess Merleau-Ponty called a dual aspect, which is both that it's a lived body, but it is also a body that can be observed and can be objectified.

And so

I guess the question here is why do we, is there a difference between saying I am my body, I live through my body, or versus I have a body?

And what are those different aspects of selfhood?

Because on one part, I feel like you're getting

what we will be called perspective illness.

I am my body.

I live through my body.

I live.

Yeah, I mean, literally through it as a vehicle for which I can be in the world versus possession.

I am some in this case, it seems like people would feel like a disembodied ego who has a body that they own.

Is there a way of coalescing these kind of two canonical features of selfhood one being possession and one being perspective illness or association with the lived body?


SPEAKER_01:
Yeah, that's a great question.

So I think these different perspectives are, to some extent, compatible, whereas it depends on what you mean.

So one could, for instance, wonder whether a non-human animal, which maybe does not have a form of consciousness that is comparable to human conscious experience, and that doesn't

reflect about its own body nevertheless lives through its body.

So there's a live body, but that doesn't mean that we don't have that we, I would say it's just that we can also have different forms of experience, or richer forms of bodily experience, and reflect about

our own experience in different ways.

And so having a body versus living through a body.

So it seems to me that the lived body is that that's more characteristic of experiences or situations in which we're not so much aware of having a body at all.

So maybe when we're engaging in

some physical activity, playing sports and just not thinking about where our limbs are, but just moving and interacting and in the sense living through the body.

I don't know if that captures what is usually meant by that expression.


SPEAKER_02:
Yes.

I wonder whether it's a good question.

We wrote this paper, not you and I, me, Carl, Lars, and others wrote this paper on flow states.

And flow states are exactly this, playing sports.

And we tried to ground it in a

self-modelling architecture.

And actually, the revisions have been really useful for that because I've been able to get a little bit more concrete on what I'm talking about.

And the kind of dichotomy that we're saying is present there, I think I can say this because hopefully it'll be published, well, as the pre-print's out, is the distinction really between a minimal phenomenal self, which is like these really basal fundamental aspects of selfhood, mindness, presentness, perspectivalness, versus epistemic agency or strong epistemic agency.

And by that, I mean, you know, when we let's say plan or when I'm talking to you and I have to think about my next sentence, there's a very strong sense of self-guiding that process where I can use, let's say, former resources, former knowledge to apply to deep temporal planning.

And we're saying that that's attenuated in flow.

and what we're also saying therefore is that metacognition because i think it's really important here to distinguish between pre-reflective self-awareness and reflective self-awareness and i'm treating reflective self-awareness as a metacognitive act your self is reflecting on itself because um reflective self-consciousness is itself a kind of part a form of epistemic foraging that too is attenuated in flow states but i think the interesting question there is if i was in flow would i purely live through my body or could i be

perceptually aware of having a body nonetheless?

Could I have both?

And these are very difficult questions to tease out because, you know, they're only done retrospectively and so you only get qualitative reports.

That's an interesting dimension that I haven't thought about.

I think, okay, so I emailed you, however, you know, a month ago, two months ago, and I said, so I've been trying to get my, I've been trying to think about how Metzinger's phenomenal self-model can be coalesced with active inference.

And this is the Active Inference Insights podcast.

So I messaged you, I said, active inference takes cognition to be fundamentally inferential.

Are the processes within or the processes that generate the phenomenal self model inferential?

Are the processes that are themselves being modeled by the phenomenal self inferential or both?

because I think you would do a great service to everyone if you could explain that what Thomas is talking about is kind of the cognitive system recapitulating its own data structures into phenomenal space.

And I think that's an interesting idea.

He talks about this in sort of his 2003 book and 2008 book and some other works that the phenomenal self model is in an implicitly reflective way is obviously a model of itself, whether that is a, you know,

giving relevant features into consciousness or coarse-grained features into consciousness, whatever it is, but it's a reflective process where the data structures in the body and brain are being transposed into phenomenal space.

So maybe we can start there.

I may have butchered it and I'm sure Thomas is pulling his hair out.

But maybe we can start there and then feed an inference and see where it might exist within those cycles.


SPEAKER_01:
Yeah, so I think this is actually quite complex topic because when we're talking about self models, it seems to presuppose that there's already an experienced I, right?

That I have a body, I am moving my hand and so on.

but I don't think that's necessarily already present in any, in the most basic forms of phenomenal self models.

And what it's, at least the way I think about it is that having a, at least a bodily self model is mainly about just making a distinction between one between inner and outer as it were.

So I'm just trying to avoid the,

with self and non self.

So a distinction between two types of processes.

One is the environment and the other is here where I am.

And I think that actually suffices for having a bodily self model or even a phenomenal self model.

But of course, when we

think about our experience and the way we the different forms of self that we experience there's there's more to it there's there's something like an an eye and maybe at this point it's useful to um bring another aspect of the the self model theory on the table which is the

phenomenal model of the intentionality relation.

So quite exactly.

And so the idea is that there's in a complex self model, phenomenal self model in which there's not just a distinction between this belongs to me.

This does not belong to me.

This is my body.

This is not my body.

And so on.

Well, this is my,

my action or this is an involuntary movement and so on.

In more complex self models, you also have this perspectivalness and subjectivity in the sense that you not just make the distinction between what is self and non-self or what is

outside and what is inside, but also how you are related to the environment, how you are related to your body and how your thoughts are related to the world and so on.

So this brings a reflexive component to it and makes it more complex.

I find it a bit difficult to put this into words.


SPEAKER_02:
Yeah, it is difficult.

I mean, so I can hopefully assist.

I have some sort of quotes here.

um great so this is the way that i've understood it at least and so we'll start with the quote so thomas metzinger 2008 so the two paper the ego tunnel i think is 2009 so this is a really actually very useful paper um he wrote in 2008 called something like the empirical approach to phenomenal self-modeling it's really it's actually very lucid he says that

The subjectively experienced content of the phenomenal self is the representational content of a currently active dynamic data structure in the system's central nervous system.

And that is, it's a representational entity whose content is determined by the system's very own properties.

So what I mean by that is it's standing in for something that is lower level to it, namely these data structures, right?

And we obviously here are taking as axiomatic that there are representational structures and maybe that's problematic and we can get there.

And the way that I've always read the PMIR is that the cognitive system can pay attention to the fact that it itself is directed at other things.

And therefore, what happens is that it...

well, I'm not going to say it makes the inference, but it projects into phenomenal space this notion of itself and something else.

I guess my question here is, that's taking as a presupposition that there is some division that the self model can model, i.e.

it can reflect on the fact that its own attention can be directed at so-called external worldly objects or things in the world model.

My question here would be, how does the brain know what in the sense is external to it and what is internal to it?

In the sense that I can introspectively pay attention to parts of my own lived experience, but have a sense of mindness.

and not feel that kind of duality or that distinction.

But if in principle I pay attention to the chair next to me, that feels like a separate object to me.

So if the brain is recapitulating some distinctions that are in its self-model, I guess my real question is, are we not, as you say, already invoking an a priori distinction between the thing that's paying attention and the attended object?


SPEAKER_01:
Yeah, so if I understand your question correctly,

then the answer would be yes.

And so you mentioned knowledge.

So how does the brain know?

And of course, knowledge requires truth.

So you're assuming that there is effect to the matter, whether something is really inside or outside, or something is really I or mine.

or self or not self.

And I don't think we have to make this assumption that these are truth claims and that the brain has knowledge about this.

But it does have to make some prior assumptions about that.

And these can be delusions.

So in that sense,

I think the question how does the brain know can be dissolved, but of course when the question is where do these priors come from and why are they relatively immune to revision and what happens if they do break down or the sense of self dissolves and so on.


SPEAKER_02:
So might it be feasible to say that at the very bottom level, all we have is attention in some way?

I've recognized that I almost, well, I did commit this kind of dualistic fallacy when I said there is the attender and the attended object.

It seems to me that actually that might be an inference too far.

Rather, there just is the attention onto these kinds of objects in the

these data structures.

Again, Metzinger puts this far better than I do.

He says that when he talks about the PMIR, the phenomenal model of the intentionality relation, he says that you mentally simulate yourself as currently being directed at a target object or goal state.

And that's a really interesting notion, right?

This mental simulation of yourself.

Maybe he's committing a linguistic fallacy there that there's a self there in the first place.

So

So we can pay attention, let's say, to the fact that we can be targeted at certain areas of our data state space.

But then we can also pay attention to the fact that we're paying attention.

And that seems to me, I don't want you to think about this proposal, that seems to me to be maybe the heart of reflective self-consciousness.

That I can pay attention to the fact that there seems to be something that is paying attention to that which is

that which is where attention is going to.

There's a kind of triple hierarchy.

One, does that sound like a feasible underlying explanation of metacognitive self-conceptualization or reflective self-consciousness?

And two, how deep do those recursive patterns go?

What I mean by that is like, how many times can I reflect on my own attentional processes?

And I know Lars Sandberg-Smith and others have been doing this in terms of so-called meta-awareness, but I'm thinking more in terms of actually the underlying phenomenal reality of being a self.

Is there a point when which the very notion just becomes, uh, you know, irreconcilable with a certain degree of recursive operations?


SPEAKER_01:
Yeah.

Um,

Great question.

So regarding this triple distinction, I think that's already quite difficult to imagine.

So maybe we can just try to clarify this and just restate this again together.

But before that, there's something that came to my mind.

Because when we think about how does the brain know

um what's inside um and what's in uh outside and when it comes to i mean there's some to some extent there there might be truth to to these um to claims about this and um so when it comes to attention we can just make a this really broad distinction between

um attention attention to processes that are driven by external influences so when some salient object or process processor now noise and the environment suddenly captures my intention and we can contrast that with cases in which i'm controlling my my attention and so the changes in attention are generated

or more caused by internal processes than by external processes.

And the brain can make inferences about that and can be more or less accurate about whether some changes in attention have been more caused by external or internal processes.

And it can make sense to model attention to improve these inferences and improve the control of attention.

and so on.

And maybe we can use this to just re-describe this triple distinction that you mentioned, because I'm not sure I fully got what you are.


SPEAKER_02:
Okay.

Yeah.

I'm aware that there are lots of layers to this, as is the problem when we talk about hierarchies.

So I can actually read, I was thinking maybe it's best for me to just read what I've, so this is, so if people go and look at that flow paper right now on the preprint server, they won't see this.

Hopefully, I'm not spoiling anyone's party.

By the time this comes out, this is at the very end of a review process, so hopefully it's fine.

Anyway, people can shout at me if they want.

I wrote, so it's my words.

I said, in planning, so we're talking about epistemic agency.

In planning, the organism not only knows that it is intentionally directed at something else, yielding a weak form of epistemic agency and perspectivalness, but it is also in the act of knowing about its own knowledge, which encodes as multiple counterfactual representations of what might unfold as a result of my actions in the section of my current eco niche that I seem to control.

In this way, planning involves strong epistemic agency, since the cognizer can epistemically model itself as epistemically agentive, i.e.

it can come to know its own capacity to control itself and its eco-niche over a protracted period of time through stored knowledge.

And then I go on to say that that self-modelling process is transparent in the sense that the agent often does not know that it knows its intentional objects.

Rather, there just is the sense of epistemic agency, which permeates a pre-reflexive self-awareness.

This is where the triple thing comes in.

When the agent does enter a state of knowing the fact that it knows its epistemic capacities and evolves into a phenomenologically metacognitive, metamodelling, or perhaps meta-epistemic organism, which reflects on its being.

Yeah.

So that, I think the issue that I had when writing this was you have to start from the original.

If you're using the Metzinger foundation, you have to start with the original basis, which is that the self model itself is a simulation.

It's a recapitulation of a data structure, but it's a recapitulation of data structure, which in itself being reflected on.

And that reflects that can also be reflected on.

So that's where I'm building in this nature of hierarchical recursion.


SPEAKER_00:
Mm hmm.


SPEAKER_02:
How does that sound to your ears?

And again, what what?

What does it mean for us to continue to go up these recursive ranks?

Like, what happens, for example, if I like, is there a point where I just really concretize my sense of self?

Or does it actually almost become dispersive?

Because all I realize that's going on is that I'm paying attention and I'm just paying attention to different levels of the hierarchy.

So I'm kind of curious about what you think might be the phenomenological consequences of going up this hierarchy of reflective recursion.


SPEAKER_01:
Yeah, so I don't think I can give a full answer to this question because there might be multiple things going on.

when I'm not just planning my actions or planning how to interact with the environment or whatever, but also reflecting upon my planning, becoming aware that I'm planning.

I might be evaluating this and becoming aware of the fact that I'm

uncertain about different options, or I might become aware of the fact that I really want to do one thing.

And so that this might then lead to further experience experience of certainty or maybe also, or not uncertainty, right?

Or

confidence that I'm making a good decision or doubtfulness about the decision.

And I think we can then repeat this process and reflect upon this already metacognitive process.

But the resulting experience that we have might not add


SPEAKER_02:
too much so i don't know if if it makes sense to say yeah i'm confident about the confidence that i have in my confidence in my confidence and so yes i mean it sounds you know on one hand one could read that as a genuinely vertical recursion on one hand you can also read it as a kind of uh like the way people talk about the fact that a triple bluff is the same as a single bluff you know we might actually just turn our you know turn our heads onto our tails so to speak and end up actually


SPEAKER_01:
experiencing something it's a it's a curious inquiry um someone needs to model that maybe i need to model um yeah just just um what i wanted to say is that i i think it makes a lot of sense for three levels or or um three recursions and then

above and beyond that i i really don't have a clear intuition and it's probably an empirical question also to what extent it can improve control and um as i said just from the point point of view of experience we we can mentally build further models for the iterations but they might not have a real functional significance for the

for the lower order processes.


SPEAKER_02:
Good.

Okay.

We will park the self models.

I had to do it because it's been a bugbear of mine, but we'll get Thomas on and we can all chat out together and see where we end up.

There's a broader question here about the so called generative passage.

which is this idea that is really nicely articulated in Maxwell Ramsey's 2022 paper and on colleagues, of course, which is this idea that, okay, we kind of have two things that we are trying to balance.

One is experiential reality, lived experience, and the other are, in some sense, the tools of science.

That's one thing.

The other thing could also be, let's say, neurological systems.

And Ramseth and colleagues present three forms of what they call the generative passage or this kind of ways of tying those things together.

One is ontological.

And so that's not necessarily a subscription to reductionism.

It's not necessarily saying that phenomenal reality is ontological.

neural structures.

But that's one option, whether it's super big, that's that whole conversation.

One is epistemological, which is that the kind of tools that we use in our science can be applied fruitfully, or can be used to describe and explain

phenomenal reality, lived experience.

And the last one is methodological naturalization, which I think I've been using a little bit more in my own writing, which doesn't really have a commitment.

It's quite agnostic about ontological epistemological claims and just says, well, we can do some good explanatory work by thinking about Pondy P schema or predictive processing or whatever it is.

But I'm not going to say anything about whether consciousness is these architectures or neurons.

I was curious about, I think Thomas, I don't want to speak on his behalf and you'll be able to say better than I do.

I feel like he's quite materialist or reductive about this.

And at least this is something that came to my attention from Julian Kiverstein when I spoke to Julian on this podcast, who was very keen to reject that reductionism and forefront

intentional experience and forefront lived experience and say that that's irreducible.

So I was just curious about where you stand on this generative passage.

And I guess the sauciest question is the ontological passage.


SPEAKER_01:
Yeah.

Deep question.

So first of all, I think different views on this all make sense and are useful.

There's a lot of uncertainty about how different levels of reality relate to each other.

And so to some extent, we just have to follow some assumptions that we make.

And then there will be different views that are compatible with the models and data.

that we have.

So I'm not claiming that we can say, well, we can, obviously we can reduce conscious experience to brain activity or something like that.

And it's to some extent also an open question for me, what role computational models actually play in explaining conscious experience.


SPEAKER_02:
And well, do they play?


SPEAKER_01:
So I think there's, so the, maybe the simple answer, one simple answer is to say, well, consciousness is so puzzling.

It's so difficult to grasp its different features and talk about what we are experiencing from moment to moment.

It's difficult to put that into words.

And computational models can help to make certain ideas a bit more precise.

And this might then just be, in a very simple way, a use of computational models.

It's not even an explanation.

trying to provide a better description of that which we want to explain.

What are the properties, the different features of conscious experience that we need to explain?

And computational models, I think, can very legitimately be used to provide such more detailed descriptions.

And once you have that, you can, of course, then try to make predictions about the implementation in the brain or make predictions about how conscious experience will change in certain situations.

Or you can compare to

descriptions of conscious experiences that we have to phenomenological reports from persons in altered states of conscious experience and so on.

And then at some point you may wonder, well, maybe these computational models provide more than just a metaphor for this, that is useful to describe complex and puzzling features of experience.

Maybe they also,

point to the mechanisms and go some way towards explaining how conscious experience arises or why it has the features that it has, even if we cannot maybe explain why there is conscious experience in the first place.

And to the extent that the computational models that are used also neurobiologically

at least not implausible.

And we might be able to find some empirical confirmation or to the extent that empirical neuro biological and data from cognitive and cognitive neuroscience are compatible with these models.

We might be inclined to make stronger claims about the relationship between

conscious experience and computational models and neuro biological processes.

And of course, I'm so my I ideally, we'd have very detailed computational models that can be used to describe, predict, and maybe even control many features of conscious experience.

And ideally, we know how they might be implemented in the brain, and therefore have some really have something like a computational explanation or certain features of experience.

But I can also understand if some some people might be more hesitant and saying that, though, now that I'm thinking about it, I'm curious, actually, I have to, I guess I have to

watch the episode with Julian Kivisty because... It's a long one.

Yeah, okay.

Yeah, I'd be really interested in what exactly his view on these matters is.


SPEAKER_02:
Yeah, we go pretty deep actually.

yes yeah he was very much striking down the line of the uh irreducibility of conscious experience which i don't think is necessarily exact controversial um but he was very you know he was he was being stubborn about that point i don't think he'd be upset if i said that um

Yes, yes.

Okay, good.

We can remain agnostic.

The hard problem is the hard problem.

And unfortunately, it's the most frequent question that I ask.

And no one has solved it for me yet.

And I don't think they will.

So I'm going to ask a kind of what charmers might say is a less hard problem or an easier problem, which is about the features of consciousness, especially as it's lived by us.

Is all consciousness, self consciousness?


SPEAKER_01:
I would say no, it's not.

So I'm not an expert on this topic and I haven't read Thomas's new book yet.

But I intend to and hopefully then I will know a bit more about this.

But I take reports from people who claim to have experienced states of

non-subjective experience seriously, or I think we should not reject these reports.

And I'm definitely open to the possibility and also find the strategy of trying to look at the simplest form of conscious experience, which may not be a form of self-consciousness.

I find that quite promising.


SPEAKER_02:
Yes, so do I. Does this map onto something that I think you've written about?

Indeed, you have.

Subject unity and phenomenal unity.

Is there a link there?

Well, maybe we can start with what's the difference between subject unity and phenomenal unity?


SPEAKER_01:
Yeah, so that can become quite complicated.

So first of all, subject unity can mean different things.

Some people just mean, well, different experiences that are had by a subject are subject unified if they're had by the same subject of experience.

Now, what is a subject of experience?

And some people just identify a subject with an organism.

So in that sense, subject unity would not...

entail subjective experience or having a, having some form of self-consciousness would just mean subject unity.

What would that just mean that there's a conscious organism that has different experiences at the same time.

So, and these are all subject unified in the sense that they are had by the same subject.

But of course, if you have a formal self-consciousness, you might say that this,

different form of subject unity, maybe one in which your you have different self conscious experience or different different conscious experiences that are all forms of self consciousness and somehow refer to the same same self.

Something along these lines.


SPEAKER_02:
Yeah, so what, going back to the kind of notion that all consciousness is self consciousness.

It goes back to actually what Mr. Kiverstein said.

So he's got this paper.

I think it's a 2018 paper.

I'll double check that.

No, 2020.

The Free Energy and the Self, an Ecological Inactive Interpretation.

He says very clearly, he says, in line with the arguments of the phenomenologists, I will claim that every feeling must be felt by someone.

It must have mind that's built into it if it is to feel a particular way.

such a it's such a strong claim and again i haven't read thomas's new book but i do kind of understand this perspective because at least for example let's talk about if one was imagining depersonalization it's a depersonalization experience for who right or if you think about someone who's experiencing a psychedelic experience it isn't so much that perspectivalness itself is eliminated

It's more that one maybe has this more oceanic sense of self, which is dispersive.

But that still arguably is a qualia for the self, right?

Or for the space of awareness that is endowed with perspectivalness.

So I'm just curious about whether you can think, and again, this is a tricky question.

It's a big question.

And there probably isn't, we can't probably conceive of this at all, but sort of, well, I guess actually the question I'll start with is,

When I learned about consciousness, for example, I learned that there were three fundamental tenets to a conscious experience, qualia, intentionality, and subjectivity.

This is what the British schooling system teaches you.

When I hear Julian speak about, well, you're always going to have that perspectiveness or intentionality, it's always something for you.

Can we, you know, is there a way that we could say, okay, some features, at least all those three that I've just picked up are more fundamental than others and that we can have this very, very minimal conscious experience or minimal phenomenal experience, the MPE, another acronym.

And what would you say are the defining characteristics of a minimal phenomenal experience therefore?


SPEAKER_01:
yeah i can only say it's an empirical question so we can speculate about what the minimal form of experience might be but it seems that there's a lot going on in ordinary waking experiences that can get lost in altered states of consciousness but what exactly will remain i i i don't know so thomas um can say more about that


SPEAKER_02:
Is there a problem with relying on phenomenological reports to inform such questions?


SPEAKER_01:
Yeah, so to some extent, there's of course the challenge that these reports are given in retrospect and, what do you say, retrospect?

Yeah, yeah, yeah.

then some people say, well, it's a bit paradoxical if you say you have had an experience in which you were not present or something like that.

And one, of course, the mere fact that you can remember such experiences doesn't mean that you

you must have had a form of self-consciousness or that there must have been subjectivity in the experience.

And also when people say, well, I realized that I had not been there, that there was no sense of self or no form of self consciousness.


SPEAKER_00:
I realized that when I, when I transitioned, transitioned into, um, the,


SPEAKER_01:
ordinary form of conscious experience.

So while, as it were, the self was returning, I realized, hey, there's something additional coming to my conscious experience.

So it could not have been there before.


SPEAKER_02:
Yeah, that's, that's really interesting.

yeah i guess it makes me think you know um to even have those reports whether that's in the internal speech or external reports requires propositional language and in my conceptualization of the scale of phenomenal self-modeling propositional language is super high up in the epistemic agent it's the way that we plan it's the way that we reflect on our own being and so on and it sometimes makes me think well

Is there a bias, therefore, when we don't have the capacity for propositional claims, either internally or externally, we're biased to think that that constitutes some form of self-annihilation?

And the reason why I think that is because testimonies, for example, about flow states,

they'll say at least the testimonies won't be as concrete as this but the researchers interpretations which are problematic will say the self disappeared or there was there was a reduced self-awareness that's probably a weaker form of that claim well i think the all that we're really coming to here is that you just need to get a lot more particular about what part of the self model you're talking about and how it's been attenuated or accentuated um

And actually, I think hopefully for the listeners, what they're kind of gathering is the self is not this kind of single monolithic thing that we take it to be.

And maybe that's the influence of Descartes and the tradition that followed from him.

But rather, it's a complex hierarchical appearance in many ways, but also potentially an underlying data structure that will oscillate and modulate and change given different contexts.

Good.


SPEAKER_01:
Let's take that a little bit more now into just sort of... Maybe just one thing to make it slightly more concrete.

So what is, I think, very plausible is to assume that a sense of temporal self-location can get lost.

So you can have conscious experiences in which there's no time, no temporal experience.

You don't experience

any now as it were or maybe only now but not as distinguished from future and past and then experiences in which you're not identifying yourself with some place in space so there's no spatial self-location and if i understand

Thomas correctly, he would suggest that in such minimal phenomenal experience, there certain some potential for certain control processes is still experienced.

So I mean, maybe we can try to

illustrate this with attention.

So you can either be in the process of controlling or changing your attention, and have the experience of being in control, having an experience of attentional agency.

And you but you can also have a sense of control of the potential for changing your focus of attention.


UNKNOWN:
So


SPEAKER_01:
you know now you're fixating focusing your attention on something or maybe on my voice but you are aware of the fact that you could shift your focus of attention and this is so this potential aspect might be something that in one form or the other could still be present in a minimal phenomenal experience and

Thomas can probably describe this more eloquently and in more detail.


SPEAKER_02:
It also makes me think, what are the prerequisites for attention?

It makes me think that something like temporality or just some dynamic needs to be there, right?

Because how are you going to go from one thing to the other without it being over time?

Which also makes me think there might have to be also over space.

Well,

guess we also have covert you know mental attention or covert attention mental attention whether that's going through some space is kind of depends on how you define space that's a really interesting question i guess yeah there's a there's a nice link there you could say okay i've got a function that remains in the minimal phenomenal experience but let me think also about what the necessary features of the experience or the world would have to be for me to have that function

Um, yeah, it makes me think of these, um, so-called pure consciousness events, uh, which I came to know about through John Viveky.

And he sort of describes how in these pure conscious events, um, so-called adjectival qualia, redness, blueness, sweetness disappear, um, equals adverbial qualia remain.

And by that, he means now on us here in us.

presentness, whatever, you know, perspectivalness.

And maybe that's a nice road in.

But yeah, I will probe that with Mr. Metzinger.

Well, I've read your papers before and I read them again because I think they're so wonderful and interesting.

You have these two with Carl, which I love.

And in one of them, you talk about sort of life-mind continuity, which I've also been getting really into because I've been reading my Evan Thompson book.

And in addition to sort of life and mind distinction, you also have a distinction between basic minds and non-basic minds, which I think is important as a prior thing to understand before we get into a sort of life-mind continuity.

So at just a very basic level, what would you say is the distinction between a basic mind and a non-basic mind?


SPEAKER_01:
Yeah, so if I remember correctly, I think we mainly make this

distinction in terms of representation.

So a basic mind would be one that does not have representations and a non basic mind is one that has representations.


SPEAKER_02:
Good.

And how does that feed?

So how does that fit?

So when Evan Thompson was talking about mind and life continuity, he was kind of seeking underlying

functions, structures, processes that are involved both in the constitution of mind and life.

So for example, autopoiesis, self-organization, or what else would he have had?

Operational closure.

These are all things that were coming out of the dynamical systems theory and Varela's work.

And now we've got active inference.

So I was wondering whether, how does active inference, because I think at one point you say in that paper, I won't misspeak, I think I've got the quote here, all systems possessing a Markov blanket have properties that are relevant for understanding the mind and consciousness.

If such systems have mental properties, then they can have them partly by virtue of possessing a Markov blanket, and hence your Markovian monism.

This is, I think, this is the 2020 paper, so not the Life Mind one.

But it calls to mind a sort of life-mind continuity because active inference postulates anything that exists has a Markov blanket, and it can be seen as if it's parametricizing Bayesian beliefs about the external state.

So everything has a Markov blanket, anything that we can call a thing has a Markov blanket.

And as you say, anything having a Markov blanket properties are relevant.

And I think you've been very careful with your words here for understanding mind.

Why would that be the case?

And also, how is that the case without us resorting to a sort of panpsychism?


SPEAKER_01:
Yeah, that's a good question and really a fair question because I think what we say in that paper can easily be misunderstood and maybe we should have found a better way of conveying what

Or I should have found a better way of conveying what I had in mind.

So the idea is not that as soon as you have a Markov blanket, you have a mind and it just becomes more complex.

The idea is more that as soon as you have the Markov blanket of your self-organizing system, which conforms to the free energy principle.

So there's a Markov blanket.

And then you can interpret internal states as encoding probabilistic beliefs about external states given blanket states.

And this is the feature that is also relevant to understanding, I would claim, the mind.

It does not mean that once you have that, you already have a mind in an interesting sense.

But it's just that if you have a mind, then you can describe, you can understand, I would say, mental processes in terms of minimizing variation-free energy and understand them in terms of

in terms of computational processes that involve these probabilistic beliefs about external states given blanket states.

And so what I really wanted to say is that this is not something that comes at a later stage when we have really complex systems.

But according to the free energy principle, we already have that for very simple self-organizing systems.

so to contrast this with um inactive approaches that would say well we have these simple systems and they are still there they can be quite autonomous and so on and remarkable and so remarkable in fact that even higher even non-basic minds and in a different sense or maybe i shouldn't say non-basically but so even more complex

systems, complex organisms like us that have minds, many of their properties can be explained, many of their mental properties can be explained by reference to the same dynamics and features that are already present in simple autonomous systems.

And maybe you don't even need to invoke representations to explain these more complex organisms.

And I'm suggesting, well, no, because you already have some form of as if representationality or intentionality and even simple self-organizing systems.

And this as if representationality

can then, or you can, making some further assumption, you can argue that in more complex systems, it's not just a useful way of describing them as if they had representations, but they actually have representations.

And so there's something in simple self-organizing systems that's already there, that's also there

in more complex organisms that have a mind and consciousness.

And in order to understand the mind, to the extent that the free energy principle and active inference are useful to understanding the mind, we have to refer to these internal states and how they encode beliefs about external states given blanket states.

And that's something we already have in

very simple self-organizing systems.

So that's just the idea, which I think, if described in the way in which we did, can be really misleading.

And I hope this also suggests how it avoids panpsychism.

But if not, I'm happy to elaborate on it.


SPEAKER_02:
No, I think so.

And to be fair, you make it actually pretty clear.

You say that you sort of propose some rhetorical questions.

Does it follow that all systems with a mark of blank have a mind?

Are such systems conscious?

This formalism itself does not answer these questions.

So I think you're fine.

I think...

uh maybe what people might be slightly confused over is this notion of an information geometry if they've ever read this paper so the way that i understood it was you have sort of two informational geometries let's say two ways of uh describing a um

a system that has a Markov blanket internal and external states, right?

That coupled system.

And you have the so-called intrinsic geometry, which actually just describes the behavior of the internal state if it, you know,

I'll ask whether that includes the particular states, i.e.

the blanket states.

And then you have the so-called extrinsic geometry, which are the beliefs that the internal states actually encode about the external states.

So that's a very rough draw after the distinction between the intrinsic geometry and the extrinsic geometry.

It would be great if you could flesh that out a little bit more.

But how does that pertain here to...

mind, because I think you also make it quite clear that that property of the Markov blanketed system itself is relevant to the question of mind.


SPEAKER_01:
Yes, so it is in a sense relevant, but I would say in a rather trivial way.

So yeah, I think this can be really confusing.

I'm not sure how useful it is to

even refer to information geometry to get at these more general conceptual points.

But so an information geometry is just, you have a space in which every point corresponds to a probability distribution, and then you can, um, you can have different shapes and, um, many falls in, in this,

space, or for example, surfaces, and compute distances between points, distances between probability distributions.

And these distances will not be Euclidean distances.

So it's just a different kind of space.

And so the internal states, if you interpret them

probabilistic beliefs about external states given blanket states, you can identify them with points in the information geometry.

And then the way in which these beliefs are updated and which they change can then be described as a trajectory through that space.

And you also have a different space of probability distributions

for what one could call the physical dynamics of the system.

So if you're not referring to probability distributions encoded by internal states, but about the probabilities of internal states, or maybe paths, then you can also identify them with points in such a space.

space of probability distributions, but it's a different, these are different probability distributions.

And the thing just is that you can then describe trajectories through both of these information geometries.

And in a way they are two sides of two different sides of the same coin.

So the idea is that you can describe the,

physical dynamics of these self-organizing systems equivalently by describing them in terms of the dynamics, the belief dynamics of Bayesian mechanics in terms of changes of probability distributions encoded by internal states rather than in terms of probability distributions over states of the system.


SPEAKER_02:
Yeah, yeah.

Okay.

So I think the reason why this might be a bit confusing for people is because then people have heard that all that we're talking about, when we talk about internal states, is this parametrized, if that's the word, beliefs about external states, that's all the internal states are, right?

They just are probably distributions over external states.

Okay, so that might

be a commitment, an ontological commitment to say that, but let's take that as a kind of starting point.

That's what we mean by internal states.

And then we have an intrinsic geometry, which is mapping the evolution of those beliefs, right?

And that's over time.

But given that those beliefs just are over the external states, is there not an isomorphism between the intrinsic and the extrinsic insofar as

When the intrinsic changes, i.e.

when the path of the state of the internal states changes, by necessity, the extrinsic information geometry would have changed because the internal states just are the probabilistic distributions that the extrinsic geometry defines.

So for me, I can't right now see how there's a... I say they're isomorphic.

I almost think, well, I almost can't see how they're not one and the same.

But I might be missing something.

I probably am.

I'm always missing something.


SPEAKER_01:
No, I think you're on the right track.

So there, yeah, essentially, just two different ways of describing the system.


SPEAKER_02:
But OK, so what is the utility of that, I guess, is the question.

Because to me, it seems like just a very philosophy of science point, the utility of two different descriptions of the same phenomenon

it has to have some utility right like if we're talking about heat let's talk about how it feels to be hot or i can talk about the velocity of particles within a heat bath or within whatever but that's useful because one i can attend to phenomenological facts and the other one i can tend to facts about physics whereas this it just seems like prima facie there are two distinct descriptions but

under the blanket, because the internal states really just are probability distributions about external states, there's nothing over and above their evolution beyond that which can be described by the intrinsic information geometry.

So is there something more to these two types of descriptions which make them distinct?

Or is it just

Semantics?

I don't want to say that because that always sounds very derogatory, and I don't mean to be derogatory.


SPEAKER_01:
Yeah, so on the one hand, one could say, yeah, and to some extent, it's just semantics.

But that doesn't mean that it's not useful to use one instead of the other description.

So think about describing a system in terms of representations.

So you can, of course, just describe the vehicles without referring to the content.

And so you're dispensing with the representational description and just focusing on the physical material realizers of the representations.

And that's fine.

But you typically get some benefit

going to a different level of description.

So when you're describing a system in terms of representations you have, you can refer to contents, provide a normative description, so the system can have true or false representations, or they can be more or less accurate.

And you also have that

for the Bayesian mechanics.

So in a way, it's just a different way of describing the system.

And you don't need that.

You can describe the system.

You can stick to your purely physical description and don't need to refer to probabilistic beliefs.

But this gives you some benefits.

On the one hand, it reduces complexity because when you're referring to internal states as encoding probabilistic beliefs, you're actually coarse-graining the system and not just talking about, say,

billions of neurons, but about neural populations that encode different beliefs.

And so you can reduce the complexity of your description.

But still, given that, if the free energy principle is correct, you have an equivalent description, it can still be as accurate or almost as accurate

but without having to deal with all the unnecessary details.

So that's one benefit that you get, just a reduction of complexity.


SPEAKER_02:
Yes.

I sense that that's a reduction of complexity for the scientist.

I guess really you very much put it nicely in terms of the distinction between the vehicle and the content, which certainly pertains when we're talking about representational systems.

I guess my point really here is that

I arguably think that there is no vehicle, the internal state is not a vehicle of anything.

And I think this really comes down to actually how you describe it, because the way you describe it is that they encode probabilistic beliefs.

Whereas I think if you're abandoning the vehicle, you can just say they are probabilistic beliefs.


SPEAKER_01:
And if I think if you have the... Sorry, so then it seems like you're dispensing with the physical, purely physical description, and just

happy to say, well, we only need the Bayesian mechanics, right?


SPEAKER_02:
I would just say that we are sometimes misled to think that active inference entails a representational approach.

And yeah, you can do the descriptions in terms of the Bayesian mechanics.

That itself will map onto some physical organisation.

But I don't think it necessarily dispenses with it.

It just says that when we as scientists try to carve up these spaces, we are making, well, we are making these very strong distinctions.

And this is something that I've been tackling with literally ever since I discovered active inference and learned about the free energy principle, which is what is unique about the internal states?

Is there something over and above the parametrization of beliefs about external states that are in the internal states that make the thing the thing it is?

I.e., is all that we are preferred external states?

Or are we a thing that is trying to get to preferred external states, if you can see the difference there?

So I think that, again, comes back to whether we embody a generative model or have a generative model.

And I think I've been leaning in terms of the, just in terms of the physics, towards the idea that we embody a generative model.

All that means is that we are just a bundle of external states.

And you can describe the direction, the information travel of those external states in terms of Bayesian inference.

But the internal external distinction is somewhat illusory, because

we just are the external states.

Might sound very sort of Buddhist, actually.

But yeah, I think maybe that's where we don't necessarily, we're not disagreeing, but maybe where I don't really see the strength of this distinction, because I don't really see the strength of the distinction between the internal and the external.

And arguably, I just see it as an instrumental tool.

I don't know if that answers your question.


SPEAKER_01:
Yeah, no, no.

That's a

Good point also speaks to what we talked about in the beginning, right?

To what extent there's a factor, the matter whether something is internal or external.


SPEAKER_02:
Yeah, I just think a big mistake that people well, I think a big stumbling block that people have

although it may also paradoxically be a very useful tool for them, is to think of active inference, and this is more like a meta conversation about how we get educated on these things, think of active inference as a theory that needs a homunculus.

And I'd argue that it's not, but then you have to be very concrete on the fact that the free energy principle and active inference is a theory of physics.

And it's just describing...

physical manifolds, well, not even physical manifolds, statistical manifolds, whether they're physical, mental, so on.

And that, I think, gets you away from the kind of more homunculized predictive coding, predictive processing approach.

And so again, I mean, ultimately, I think it comes down to one's aims.

And I think as far as I'm aware, the math stands up on both fronts.

And maybe it's a question I need to speak to Karl about, which is,

So the free energy principle rests upon the Markov blanket distinction.

Why do we not just have one unified state space where different things are tending towards different manifolds?

Why do we need them to be tending towards those manifolds?

over the parametricization of external states.

And I think here the really important point is, and we've come back to it over and over again on this podcast, the free energy principle tells you what things look like they're doing, not necessarily what they are doing.

And maybe that deflates, to use Carl's favorite word, it deflates some of our worries about really making a strong ontological claim, right?

It's just a descriptive thing.

Then that makes me think, well, what's the point?

But I don't think what's the point, because I think it's important.

And I think you do get some good stuff out of it.


SPEAKER_01:
so yeah that's just one thing i would like to add so um that i think there's one perspective one from which we what one can argue that it's not completely arbitrary whether we say we interpret or we describe internal states as

probabilistic beliefs and identify them with these beliefs and just focus on this or whether we also think about the other perspective or the way of describing the dynamics of the system.

Because the same Bayesian mechanics, the same dynamics of belief can be realized in different ways.

So you can simulate this on a computer or you can implement it in a living organism.

And I think when we consider the possibility of artificial consciousness, this actually may make a difference.

Because if we're just focusing on the, if we're just

seeing internal states as probabilistic beliefs and nothing else, then we cannot make this distinction between different types, different kinds of realizations of variation of free energy minimization.

And I think it's an open question and very important question to what extent there can be a conscious experience in a

computer and a digital computer with a classical architecture or to what extent it needs a biological body or maybe a robot that can interact with the environment and for for this reason i think it's important to keep this to yeah not not to


SPEAKER_02:
um ignore this distinction between two ways of looking at the system yeah fair enough i guess uh ultimately as well something i do think is important to say is that it depends on how you define beliefs um and that's something that we all like not me and you necessarily but all of us equivocate on the whole time because we're not clear where we're talking about folk psychology beliefs or bayesian beliefs

And I guess when I'm talking about the internal states just being manifestations of external states, I'm talking about Bayesian beliefs in terms of the beliefs that those internal states have.

When I'm talking about the rejection of the homunculized view of the internal states, in many ways I'm rejecting, I'm not rejecting the Bayesian beliefs that they have or embody or entail.

So I'm rejecting the folk psychology, homuncularized, representational beliefs

propositional beliefs that we talk about in philosophy.

So I think that's an important thing as well for people to bear in mind that there's often risk in active inference that we equivocate and talk past one another.

Again, not you and I generally, but everyone, because we don't get concrete on what we're talking about with respect to beliefs.

But that's a nice, interesting segue because you wrote this paper in 2022 with Carl on AI ethics.

in computational psychiatry and it actually takes us nicely full circle to mr metzinger professor metzinger's me um because he actually was the first person i ever read in the ego tunnel talking quite explicitly about the ethics of generating artificial intelligence like re in a very

philosophically robust way.

Obviously, people have been talking about what were the ethics in terms of job opportunities in terms of, you know, trolley problems when my car might run over a grandmother or a child and so on.

But I think what Thomas said that was really impactful for me was, we might be actually creating conscious life.

So it's not just the, you know, it's not just the ethical effects on us.

it's actually the generation of sentience, feeling, consciousness into the universe, an uptick in that, whereby those things start to actually have moral value

which are arguably equivalent or perhaps given their greater complexity and depending on how you define your morality, more important than us.

So you can throw out the window your kind of classic Sam Harris worry that AI is just going to stomp on us like we're ants.

That's one ethical issue about us.

But also, and again, I don't mean that this should happen or will happen, but

arguably you know those ai creatures are gonna have sentience and feelings and consciousness just like us and we have to take their so you know rights or values if they have them if we give them that into account so it's not just a single-sided relationship that's going to emerge

I'm curious about what you think about that.

Is it a concern that we really have to take seriously that if, for example, we end up making AGI and very importantly, that AGI is conscious, because I think there's a really important point that there's a difference here between intelligence and consciousness.

What concerns do we have to have not only about ourselves, but also about what we are bringing into the world?


SPEAKER_01:
Yeah, I think there are many different and to some extent related questions that are relevant here.

And one question is, of course, what about the risk of actually creating artificial consciousness?

What consequences would that have?

And should we do it or should we refrain from even attempting to create artificial consciousness?

And things become more complicated when we also consider the fact that

there will likely be a lot of uncertainty about whether a given artificial system is conscious or not.

So Eric Schwitzgabel has done a lot of very relevant and interesting work on this.

And he describes the problem roughly as follows.

I'll try to summarize the main ideas, but there's

People who are interested in this can easily find his publications and podcast interviews with Eric Schwitzgabel.

I highly recommend looking these things up.

The basic problem is this.

There might be situations in which there are artificial systems that may

seem to be conscious and we might in addition also have no way of being certain that they are not conscious so there might be no clear evidence as to whether they are conscious or not even from a scientific point of view given that we don't have a consensus in consciousness science we don't really know what consciousness is there's no

generally agreed upon theory of consciousness and so on, and many different metaphysical views about consciousness that all have some currency.

So there's a lot of uncertainty about which systems could be conscious.

And as artificial systems become more and more capable and have more and more impressive capabilities,

cognitive capacities, there will likely be points at which they will seem conscious and also fulfill some indicators of consciousness that we might have.

But they might still be quite different from human beings, conscious human beings or other conscious animals in many respects.

So that there will be a lot of uncertainty and there are two

types of errors that one could make one is believing that such systems are conscious and maybe also believing that they therefore deserve a moral status maybe because they can feel pain and pleasure um so assigning a moral status to such systems believing that they're conscious although in fact there's nothing going on they're completely unconscious and then there might be

situations in which we have to make trade offs between the interests of human beings, conscious human beings, and the seemingly conscious artificial systems, which are in fact unconscious.

So just, if we just think about, uh, um, I mean, just, just, uh, as a slightly silly example, trolley case in which we, we, um,

can save five artificial systems that we believe are conscious, or one conscious human being.

And of course, if these systems actually are unconscious, but we just think, well, we should be better safe than sorry, we should not, we should just err on the side of being a bit too liberal, then we might say it would be a good idea to save the

the artificial systems, or in other situations in general, just put more weight on their interests.

And in fact, they're completely unconscious.

And yeah, so that would be, I think, a- Yeah, that's a terrifying idea.

Yeah.

And of course, the other case is that the artificial systems that don't seem to be conscious,

and we firmly believe or we um maybe there's also some uncertainty but um for whatever reason we decide no um these systems are probably not conscious maybe because they don't seem to be conscious but actually they are conscious and then in certain situations we might without thinking much about it sacrifice them or

destroy them or whatever, and in fact maybe induce a lot of suffering in these artificial systems, but without knowing that.

So these are two scenarios that we, I think, would like to avoid.

And Schwitzgeber certainly argues for this, that we should avoid such situations, namely by avoiding to create systems for which there's no certainty about whether they're conscious or not.

And so this is not, it's a bit different from Metzinger's position.

And the question, of course, is,

is this really feasible or is it realistic that we will in the future only create systems that are either clearly non-conscious or that are clearly conscious and i think in order to get there it would be really useful um to have a handle on some necessary conditions for consciousness that might not be

fulfilled by certain classes of artificial systems.

And if, for instance, having a body is required for being conscious, then we could rule out that there's consciousness and a computer simulation.

So I'm not claiming that I have a good argument for this.

But it's, I think it's would be extremely desirable to have a

justified account of necessary conditions for consciousness that are not fulfilled by many artificial systems.

And so that's the line of research that I'm trying to pursue of finding good reasons to believe that there are certain necessary conditions for consciousness.

Of course, the other approach which I must say

from from a certain point of view seems a lot more compelling is to say well we don't really know what is necessary for consciousness or not we cannot rule out that certain systems are conscious if they don't fulfill certain conditions but we can look at theories of consciousness we can look at work on animal consciousness and

derive indicators for consciousness.

And that's this preprint from last year by Patrick Butlin, Robert Long, and a whole bunch of other authors.

Eric Schwitz is also one of the co-authors.

And they do exactly that.

They say, well, under the assumption of computational functionalism, we can derive

a certain number of indicators from theories of consciousness and can think about how they could be implemented in artificial systems and AIs.

We can also assess to what extent they are already fulfilled by current AIs.

But based on this, it's highly unlikely that current AIs are conscious.

But if future AIs fulfill many of these indicators, it becomes more likely that they're conscious.

Each time an artificial system fulfills one of these indicators, the probability that it's conscious rises a little.

And of course, that's a very elegant approach because you don't have to make any strong metaphysical commitments.

You can just say, well, we don't really know what consciousness is, but we can say what features make it a bit more likely that a system is conscious.

and as soon as we have systems that fulfill many of these indicators or many markers of consciousness we might reasonably say well it's highly likely that the system is conscious and we should um assign a moral status to the system but of course then the question is what what happens in the meantime when we have many systems that fulfill some indicators or many but not

maybe not so many indicators that we can really be certain that these systems are conscious.

And how can we avoid these situations?


SPEAKER_02:
Yes.

It's fascinating.

It's fascinating.

We don't have time for me to put on my philosopher's skeptical hat and, uh,

and point out all the problems with trying to find behavioral indicators of consciousness, which are so obvious that I don't really need to say them.

But again, that's the philosopher's point.

And this more of a pragmatic point, which is, we are probably gonna have to make that decision at some point.

And so why it's actually kind of interesting, we're setting up a sort of Bayesian process with which to come up with a posterior that a given object is conscious or not.

So it's kind of fun that we're inverting the Bayesian process to figure out.

But as you know,

your model is only as good as its priors.

And if those behavioral priors or behavioral indicators are wrong, then you are really barking down the wrong tree.

You might be Bayes optimal, but you're Bayes optimal for a dysfunctional purpose.

Cool.

That's really, yeah, that's fascinating.

I mean, I'd love to go further, but I'm aware that we are running a bit low on time.

So you mentioned that this is kind of your main focus of research at the moment.

What else can people expect from you coming up next couple of months this year?

And also how can people get in touch with you if they want to ask you questions or just chat?


SPEAKER_01:
Yeah, sure.

Um, so I'm, I have a blue sky account.

And I think my blue sky handle is Vanya.visa.

Probably should check.

But I also have a Twitter account.


SPEAKER_02:
I don't really understand what it is.

I like Twitter.


SPEAKER_01:
Yeah.

So I also have a Twitter account.

And then my handle is W-A-W-I-E-S-E, I think.


SPEAKER_02:
Excellent.

And, um, And what can people look forward to reading or, or hearing from you about, um, and also maybe plug the workshop that you're at now, because that's a really cool, um, thing that you guys are doing.


SPEAKER_01:
Yeah.

So I'm currently at, um, the IK in Germany.

Um, IK is, um, the interdisciplinary college.

It's a.

spring school for cognitive science.

It's an entire week with a lot of fascinating courses from AI to neuroscience, neurotechnology.

We also have some more philosophical courses, some more theoretical, some more practical.

It's a really great mixture.

and many fascinating people over here that you can talk to so i highly recommend coming to the ik and guna and um maybe you have show notes or something like that for the podcast maybe you can provide a link to the website everything goes in the description i'll put your blue sky in the description as well i'll make sure i do that thank you

Cool.

Maybe, maybe one thing more people can expect.

Um, so I signed a contract with MIT press, um, for a book on artificial consciousness working on.

And also, um, I think I should remind myself that I should be busy working on the manuscript.


SPEAKER_02:
Fantastic.

Well, congratulations.

Do people know this already or is this a big reveal?

Have we done an exclusive break on Active Inference Insights or do people know about this book already?


SPEAKER_01:
I think most people don't know about it.

I've just told a few persons about it.

But on the other hand, most people don't know me.

Yeah, true.


SPEAKER_02:
No one knows us.

Well, I forget that.

I was just trying to sort of embody a speculative paparazzi-esque person.

I didn't do it very well.

Vanya, this was super fun.

I feel like there's still a lot to get into.

Um, so maybe when Thomas comes, it would be great to have you on even if, you know, partially for a bit, so we can get deeper into these ideas.

And, um, thank you for placating me and listening to my ramblings about recursive properties and operations.


SPEAKER_01:
Thank you.

Thank you very much for having me.

And then this was really a pleasure.

So, um, I think we had a nice flow on the conversation.


SPEAKER_02:
We did.

We did.

All right.

Thank you everyone for watching as well.

Take care.
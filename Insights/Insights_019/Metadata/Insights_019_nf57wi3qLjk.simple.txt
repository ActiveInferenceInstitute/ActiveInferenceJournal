SPEAKER_00:
Hello, everyone, and welcome back to Active Inference Insights.

I'm your host, Darius Parvizi-Wayne, and today I'm thrilled to be speaking with Connor Hines.

Connor is a PhD student, although he just told me he just finished his PhD at the Max Planck Institute of Animal Behavior, as well as a researcher at Versus AI.

His work focuses on the apparent teleology of complex systems and especially how they can be described as conducting variational Bayesian inference over their environments.

Thus, Conor leans heavily on active inference and the free energy principle in his analysis of collective behavioral systems.

So he's the perfect guest for today's episode.

Conor, thank you so much for being here.

I'm super excited to learn from you.

I need to put my technical hat on because your papers are technical, but I think there's a lot that non-technical audiences will be able to get out of them.

I wanted to start... No, no, it's genuinely, it's absolutely my pleasure.

I feel like psychology or cognitive science likes taking the unit of analysis as the individual and then saying, okay, how can we extrapolate from the individual to describe group dynamics?

So I wanted to start very broadly with a kind of axiomatic question about when you study collective systems.

Is there a difference between collective behavior in and of itself, like what you talk about in terms of collective behavior, and then the behavior of an aggregation of individuals who we can say belong to a group?

So is there something over and above the behavior of individuals when we talk about collective behavior?


SPEAKER_01:
Yeah.

I mean, this question is one of the motivations for why I ended up studying this.

And I think it ties into what I would say is a larger unifying theme of complex systems in general, complex system science.

I think if you could sum it up in one phrase would be more is different, the whole is greater or different than the sum of its parts.

And that is really evident, I think, in the study of collective behavior.

So yeah, I would say that there's something absolutely different about kind of blurring your eyes, so to speak, on the individual components of a system and treating the system as a thing, as an emergent thing.

And there's a lot of theories about whether that's like an epiphenomenal thing.

Is that just something that exists on a descriptive level?

Or does this larger thing, like a flock of birds or a school of fish or a society or an economy,

Does it in some way have like causal power and influence on these like micro level individuals, but I like the larger if the last like whatever 30 years of complex system science has shown us that yes, there is something qualitatively different about the bigger picture than just an aggregation of individual components.


SPEAKER_00:
Yeah, I think in terms of, let's say, socio cultural groups, the top down influence is quite transparent.

I mean, in terms of an economy, the economy that you function in, presumably has a kind of circular causality with the agents in the economy.

I'm wondering whether in terms of more mechanical complex systems.

So even in let's say, simulation work, is there something that emerges ontologically, in terms of the actual dynamics of the group as a whole, that

can't be reduced to, let's say, just like a summation of those individual behaviors.

And I guess, in that, even if there is, can we distinguish that from top down causality?

So as you say, you might have something which like is ontologically different, you can actually say, Okay, this is a different phenomenon.

But where do you see the top down causality coming in, before we get to, let's say, languages or broader cultures or economies?


SPEAKER_01:
Yeah, no, that's a hotly debated topic, effectively.

So there's a lot of work that's kind of trying to actually quantify the amount of top-down causation that there is.

Because, of course, we can always define some set of macroscopic or coarse-grained variables from an ensemble of microscopic variables.

Say I have a bunch of particles or a bunch of gas particles.

I can take averages like...

and come up with quantities like the temperature or the average mass or the average velocity.

The question is, is that coarse-grained variable actually doing anything to the lower level?

Does it feed back down?

And my intuition is even in simulation, yes, there is.

And people have tried to quantify this.

There's people like Lionel Barnett and Anil Seth at University of Sussex who have come up with quantities like dynamical independence.

So there's an idea where these coarse grain variables don't just act as descriptors or aggregators of lower level constituents, but they end up having their own causal power.

And intuitively, when you hear that, it's quite hard to imagine how that's possible.

Um, but it's, I think it's one of the most fascinating and kind of magical things about complex systems is that basically these slower, like both like spatially and temporally coarse grain variables end up kind of, uh, accumulating a life of their own and they can in fact influence and in some ways be more predictive of the lower variables, um, than just taking the whole like ensemble of lower variables as predictors themselves.

Um, I don't.

to be quite frank although i'm quite confident this exists i don't have a great intuition for how that's possible because it kind of flies in the face of the last you know 200 years of reductionist science which is basically saying that the only causal direction is up everything starts like you know with gluons and quarks and then you kind of build the universe up from there

But all of this research is suggesting that there is some degree of downward causal influence.

And ontologically, it's actually quite philosophically difficult to reckon with that.

I don't know why that is.


SPEAKER_00:
Yes.

I mean, I wonder whether it's just a slightly different type of causation from the kind of gluons or quarks up in the sense that going all the way back to Aristotle, people talk about constraint versus causation, and it might be just

macrosystems constrain the organization of microsystems.

But yeah, it's a massive question.

And yeah, as you said, it's very philosophically rich.

I mean, strong emergence, weak emergence, all of this kind of stuff is really philosophical.

Excellent.

I'm going to sort of humans or are in silico simulations of humans or cognitive agents.

Um,

when you put when you sort of run these in silico simulations, you equip, you know, I read one of your papers, you equip them with social information, right social priors.

So in one of yours, you add distance to the net their neighbors, or you would have the rates of the change of the distance.

I'm wondering whether

Would it be possible?

Do you know, do individual agents need to be equipped with the social priors to become social agents?

More?

Is there a way that actually just by acting, and let's say having a shared goal, but without actual reference to the presence of other agents, we have this kind of gestalt, or we have this kind of emergence of a overall pattern?


SPEAKER_01:
yeah definitely i i think so basically there's different ways you can effectively introduce correlations or coupling between agents um the way that we think i mean there's a lot of evidence that animals and agents in society do correlate each other's activity is through

either directly or indirectly perceiving aspects of the other.

Like we can see each other, schooling fish can sense their neighbors, bacteria can sense each other in kind of like all sorts of interesting ways.

but there's nothing qualitatively saying that you can't couple a system of particles with some like third court like driving or common source.

So for instance, if I have a bunch of particles trying to navigate towards a food source on some like media, like they're all attracted by biochemical gradients towards a food source, of course that will induce correlations in their activity.

So it will seem like they're all moving as one,

where as in fact, there's nothing that's actually letting them sense each other, but they're just some kind of common source that's giving the appearance of correlated activity.

And this is a classic thing that comes up in neuroscience too, when we're looking at large recordings of neurons.

oftentimes we're not recording the whole brain, right?

We're just looking at some small subsection of neural activity.

And you see all these correlations between neural firing patterns.

And you say, oh, look, they're connected.

And people use all kinds of correlational techniques to infer the nature of those interactions.

Oh, this neuron is driving that neuron.

But you actually never know if there's some third causal driving force that's actually responsible for correlating their activity.

And this is actually one of the nice things about studying collective animal behavior,

Like in the lab of Ian Cousin, where I did my PhD, they have the ability to kind of track with high resolution a bunch of individuals, like every individual in an ensemble, in a collective.

And they can even reconstruct kind of, to some extent, the visual fields and the sensory fields of the individuals in the group.

And in that way, they can kind of decouple these potentially third correlating or confounding factors from the actual use of social information that the individuals might be.

That's cool.


SPEAKER_00:
That's interesting.

Yeah, because I can imagine without that, it can be very confounded.

And it makes me think,

To what degree here is collective behaviour genuinely advantageous for these organisms?

And I guess it comes to a kind of evolutionary question.

There is an evolutionary debate about what is the transmitted thing over epochs?

Is it the actual gene, the species, collective and so on?

Exactly.

In some sense, if we just say, okay, well, there are these aggregations of individuals which have similar goals and therefore look as if they are acting in accordance with one another.

And obviously, when you start recognizing others, well, then you have to start taking them into account.

Your work, have you got any intuition as to whether the presence of others with shared goals is actually beneficial for the organism who's pursuing that individual goal or whether it'd be better if we all siloed off and an island onto ourselves?


SPEAKER_01:
yeah yeah that's it that's like a really um perennial question the study of in not only collective behavior but like socio-biology kind of in its original formulation by like eo wilson is you know is there something inherently advantageous to group living and um if so it's clearly not a be-all end-all because we do have

the presence of species that are like solo predators, like wild cats, right?

They're not living in herds, but then there's other animals that are actually living in groups.

And so there's clearly some trade off there.

So I think one answer that probably biologists, because I have to admit, like I often theorize very high level with concepts from physics and mathematics, but there are clear

you know, individual fitness and more selfish gene related motivations for group living.

Like a classic one that has been used to motivate flocking and schooling fish is like predation risk.

If you're living in a group, you're basically lowering the chance that any particular individual gets predated because, and there's also interesting psychological effects that flocking has on predators.

Like there's a crowding effect.

So if there's a bunch of individuals in my visual field, it's harder for me to focus on a single one.

So that collectively lowers the predation risk of everyone in that collective.

So that's one answer to that.

There's another interesting answer, which I don't know.

This is maybe a little controversial.

And I think this is where it really ties into the free energy principle, which is once you have a collective system,

regardless of whether it's actually beneficial to the individuals that partake in it if the system has dynamics that kind of allow it to survive it's kind of created now its own selection pressure on it in of itself so the system

um simply by existing and not dissipating is kind of like self-evidencing or selecting itself at perhaps like a higher dynamical scale and this is where i think concepts like the free energy principle which is really in the game of defining why things persist in the first place and what it means to be a thing um has something to say about about collective behavior so i would actually argue that there's certain

kind of group dynamics that exist, sometimes even at the expense of the individual fitness.

But once you've entered into this macro scale, the macro scale is kind of living and trying to find evidence for its own existence, even if that goes back and punishes the individual actors that actually originally cooperated to create it.

Um, that's a little bit more controversial, but I think it's a really good, maybe a framework for understanding kind of like maladaptive group dynamics, things like misinformation or virality, social networks, things like that.


SPEAKER_00:
Well, the Marxists will be screaming that you've got a great theory there of capitalists, you know, capitalist realism.

Um, yeah, that's, that's really interesting.

I mean, I guess as a, you know, more philosophical bent on this one things, well, where's the kind of locus of control here?

Because I think intuitively, we don't think that the group dynamics have control or agency, like the overall arc of a culture has the agency of its constituents individuals.

But this idea is like, again, I think it comes down to a kind of constraint causality distinction, there are certain things that say that you can do within

capitalist America versus what you could do in communist Russia.

And those actors constraints without there being some homunculus.

I mean, obviously, there are homunculus is in the insofar as there are political figures.

No, that's a that's a really interesting point.

I think what I've been thinking about is, and this is a even deeper question is,

and I'm wondering whether you have to think about this when you talk about collective behavior and aggregating individuals.

Do you ever think, at what point does one individual end and what?

And when does it?

But when can we distinguish one individual from another?

And I know that sounds stupid, because people will just say, Well, it's the body and you just have it, you just see them.

And obviously, when we're talking about high level evolved systems, like the visual system, and it probably is based on

you know, v1, picking out lines, which distinguish it, you know, individual organisms.

I'm curious, in terms of the actual Bayesian mechanics and the free energy principle, is there anything that gives us individuated thingness beyond taking that as a kind of axiom, like, oh, no, there, it just is a thing that is self evidencing?


SPEAKER_01:
No, yeah, I don't think it does.

I think that's one of the challenges of maybe what the general problem would be called is like system identification or Markov blanket identification.

How do I like look at some high dimensional system and actually pick out what are the individuals or the components of that system?

And even within like biological systems, it's often not that clear.

Like you and me might look at,

hill of termites or ants, colony of ants, and say, okay, yeah, those individual ants are the individuals.

But in a system where they all share the same genetic material, it becomes a little bit unclear.

And especially when you have these kind of self-immolating sacrifices for the good of the collective.

it's you start thinking about things like the extended phenotype like are these individuals actually individuals or are they kind of just manifestations of some common gene pool they're kind of like the the tools and the fingers that the gene pool is using to act in the world um so i don't think that bayesian mechanics has

any strict definition about what a thing has to be.

I mean, people will disagree with me on this.

For instance, the recent work from people like Carl Friston on particular things, strange kinds, this line of papers actually makes particular arguments about what

constitutes a thing and it has to do with a particular set of dynamical constraints between the different subsets of the thing like there's active states and sensory states and there's certain conditional independence relations that have to exist um so so the proposals have been made but i don't think anything about basing mechanics actually tells you how to do the partitioning of the state space it's like given a state space we can then start interpreting the dynamics of that thing

as doing some sort of Bayesian inference, but where the actual state space comes from, that's a much deeper question that I think is like an active, active area of inquiry.


SPEAKER_00:
That's really Yeah, that's fascinating.

Because I guess one way of doing it would be that you were you out like scientists in some way, and you go, Well, if these things are behaving differently, let's say you got to answer behaving differently, they're diverging in their movements, then you would say, Well, we can make the assumption that the active and sensory states are different, their Markov blanket is different.

But if they're replicating one another, then even that decomposition of the system into sensory states and active states kind of...

kind of informs you to say that that's one overarching thing.

And again, I mean, I like recruiting kind of a philosophical vagueness here, which is that, like, from a philosophical perspective, it's probably just an empty question.

And there, you know, it's well, what point does a pile of sand become a heap or, you know, and so on.

And it's, it's empty, if not, if not interesting, and maybe beneficial when we're talking about

i think there are phenomenological effects about you know to what degree do i recognize myself as a kind of siloed off individual versus part of a group and they're probably very aberrant ends to that spectrum like social isolation versus schizophrenia where i'm kind of melting into the world versus being completely alone so i think there are there's import but yeah i recognize that it's


SPEAKER_01:
Yeah, I do think, yeah, I totally agree on a philosophical level.

There is no hard line in the sand.

There's not going to be some be-all, end-all definition.

Eventually, what will be our deciding factor is the utility or, I guess, the more pragmatist sort of stances.

The partition we choose for a particular system, is it going to help us do something?

So for instance, in a lot of my interest in collective animal behavior, I've just been mesmerized by like videos of schooling fish, right?

And these schools are not like cells in the sense that

know the the i mean cells potentially violate this as well but the position and the role that individuals play within the school are not fixed right so fish can like go to different sides of this area some fish can kind of act as the eyes and ears of the school while others are acting more like the leaders so you have like spontaneous change in roles and leadership and that already is basically telling you that at the higher level at least

the the kind of sensory states are not assigned like unchanging individual identities so they're so already the partition we use if we want to start studying the school as an individual that partition has to kind of supersede individual identity has to be some spatiotemporal coarse graining of like the movements and identities of different fish or particles over time


SPEAKER_00:
good which partition we use should be a function of like what question or what hypothesis we're trying to answer yeah um so it comes down to this teleological or pragmatic motivation have you got this is a seems like a much more banal question but i'm just curious because i don't know that much about collective behavior in biology have you got a particular favorite of collective behavior you mentioned schools of fish is there anything else that kind of stands out as impressive


SPEAKER_01:
Yeah, so schools of fish is probably my top one just because they're visually so mesmerizing.

I mean, starling flocks are quite interesting as well, particularly because, so it often happens that in many realistic schooling scenarios,

although you'll have three-dimensional schools this is something i learned from my my advisor ian in practice a lot of the schooling actually is happening in two dimensions so even three-dimensional schools are actually made up of more like 2d manifolds whereas if you look at starling flocks they have very interesting three-dimensional structure so you've probably seen these like um what they call murmurations like these kind of shimmering and oh they almost look like chaotic like strange attractors the way they hold over on themselves

And even those have like quasi two-dimensional structure, but those two-dimensional planes are moving in 3D space.

And so stuff like that is just visually very interesting.

And there's a lot of really interesting research like trying to do high resolution recording in 3D of all the birds in a flock and trying to track them.

So I think that's a really a beautiful one.

But again, it comes back to your earlier questions.

What is the actual function of that?

Sometimes these murmurations are very beautiful and impressive, but people still don't really know what the function of such crazy displays of collective coordination are.


SPEAKER_00:
Yeah.

It brings to mind what you mentioned about the fact that there's actually a conversation I had with Carl because we were right at the first episode of this podcast.

We were talking about what is the imperative

Cole, you know, originally spoke about an existential imperative underlying the free energy principle.

But it's a slightly misleading term because it doesn't say that, you know, we have to do anything, right?

I could just...

disintegrate right now.

It's saying that if I do persist, then I look as if I am doing variational based inferences on.

So I said to myself, well, but is there an underlying telos to you know, is there an evolutionary pressure to survive?

Right?

This is just a kind of a biological self truth.

And he went, Yeah, and you kind of see that because there are not many animals that kill themselves, right?

Like there does seem to be some underlying pressure, or some self evidencing survival in a way like survival with self evidencing.

But it does bring to mind this really interesting fact that there are some animals who will self-immolate.

And yeah, I think that is so interesting.

Is there any kind of idea about beyond, you know, we mentioned ants and termites, and I think that's kind of altruism for the group.


UNKNOWN:
Yeah.


SPEAKER_00:
Are there any other examples?

Well, I'm wondering if there's any examples that might just be very tricky to explain under a kind of Bayesian mechanics or under an active inference.

I mean, we can expand that question more broadly and just say, is there any animal collective behaviour that you just find staggeringly odd under like an embedded Markov blanket system?


SPEAKER_01:
Yeah, I mean, I think like, so one of the main reasons I went into the collective motion in particular, like the schooling fish and kind of flocking birds was because I found them to be exactly those examples that violate

the kind of classic image we have of Markov blankets where you have like kind of layers of states that are not really changing.

Like you have the particles on the edge of the blanket and then you have the ones that are a little more inside.

And then finally you have like the kind of hard nucleus of states.

So I find that even like basic collective motion kind of evades the

proper understanding under the current iterations of Bayesian mechanics.

I mean, this is part of the reason the paper you mentioned that kind of studies like frames individual elements of like a school as like active inference agents.

That paper actually didn't try to formulate the whole school as a collective.

That was one of the other objectives I had during my PhD.

And it's something I'd still like to study if I have time.

But I find that actually doing proper Bayesian mechanics on the whole school as if the school is the unit of analysis was quite difficult precisely because collective motion is an example where the roles that individuals play is changing dynamically.

So if we are to find like a stable Markov blanket description that satisfies all the tenets of the free energy principle, we will have to

kind of come up with a more dynamic version of system identification that allows you to kind of dynamically redraw who is a sensory state, what parts of your high dimensional state space are playing the role of active versus sensory versus internal dynamically over time.

So I think even like the most kind of basic examples, ant colonies as well, right?

They're a dynamic mixing system.

Although the roles are more genetically predetermined for the social insects,

Though all these examples, I think, are a bit of a challenge to the classic formulation of Bayesian mechanics.


SPEAKER_00:
I guess one of the earlier critiques of active inference was that it's a kind of state-based formalism, which doesn't allow for dynamic change, as you say, and over-timeness.

i recognize i think this is something that actually came up in a odd twitter discussion between maxwell and kate nave about uh path integrals and the path dynamics it might be you know you're right at the forefront of this research maybe it's worth just for the audience unpicking

what is the difference between a kind of state-based or static active inference versus a path-based active inference, which allows for this kind of, well, I think of metamorphosizing Markov blankets or something like that.


SPEAKER_01:
Yeah, yeah, for sure.

So the classic variant of deriving the free energy principle,

rests on specifying what's called a steady state distribution.

So it's actually just a probability distribution over the states of the system that should not change.

And this can generally be applied to many systems, at least on some time scale.

So it seems like the distribution of particles in a rock or in me and you, for example, doesn't seem to change on quite so long of a time scale.

Of course, the rub is in how do I define what a state is.

Is a state a...

particular cell, because cells are turning over all the time, or is it maybe a tissue, things like that.

But anyway, the basic free energy principle as it was, I mean, I guess as it was pedagogically described, this is, I'm referring to papers like the free energy principle for a particular physics, the monograph from 2019.

So the idea is you have this stationary distribution over states, and it's like very trivially a multivariate system.

So there's a potential for some of those states to be active states, sensory states, internal states.

You've kind of partitioned the system, but what's important is that

over long times if i measure the system the probability distribution of being in this part of state space or this part of state space that probability is fixed it doesn't change over time so it still allows for particles to move around if i look at the individual trajectories like things can change there can be fluctuations but the stationary density the probabilities are are constant yeah

That's in contrast with the more recent path integral or path formulation, where it's not the distribution over the states themselves, like finding my position in this part of the room over time on average, but it's more probably the distribution over paths or trajectories themselves.

So the point of this is that...

There's also a stationary measure or a stationary distribution, but it's not over particular points in state space, but it's rather over sequences of points.

And that allows you to have a lot more flexibility in the dynamics, where you could have things that appear non-stationary in the sense of if I look at just the positions of the variables, that will be changing over time.

But it's because there's actually a more abstract stationary measure, which is operating on the paths or the sequences of variables of the system.

So yeah, this is what allows, I mean, apparently, this is, I think, what's going to be upcoming as we see this formulation unpacked more, especially in terms of simulations.

We're going to see versions of this Bayesian mechanics applied to systems that are moving in time and not stationary.

And this is only something you can get with the path-based formulation.


SPEAKER_00:
So when you talk about the states, so we're either talking about the sort of path that states take or the kind of position that they have at a given time, what part of the Markov blanket are we talking here?

Because some people talk about the internal states, parametricizing beliefs over external states and their self-evidencing.

And then some people talk about the actual blanket states or the particular blanket states reducing VFE or some people talk about the particular states, which is the blanket states plus the internal states.

So when we're talking about states tending towards an attracting set, what are the commitments that we're making here in terms of which states those are?


SPEAKER_01:
So yeah, people will disagree on this.

I mean, in the original formulation of the free energy principle, it should be all of those things.

So blanket, sensory, active, and internal all have a stationary distribution defined over them.

So it's basically the joint distribution of all the subsets of states.

That's the thing that should be unchanging.


SPEAKER_00:
Would that include the external states, or are we allowing those to have some chaotic element to them?


SPEAKER_01:
Yeah, I mean, for the mathematics in the original formulation to work, you actually also need the external states to be unchanging, which is one of the critiques that the free entry principle had got is like, you're not really defining a stationary system to be the system per se, like the agent, but you're actually including the whole world.

the agent is embedded in as part of your stationary measure.

And you could make arguments that, oh, well, it's referring to just the eco niche, like the behaviorally relevant part of the agent's world that it's trying to control.

So it's kind of like the agent is forcing its external states to obey the stationary measure.

But I think the path-based formulation allows you to get around that because all these things now become non-stationary if considered in terms of just raw states.

But yeah, originally it does include all the subsets of states.


SPEAKER_00:
It sounds like even when we're talking about the change of the path of states, there still seems to be some kind of predetermined characteristics or predetermined attracting set.

that path is tending towards a certain probability.

For sure.

You allow for some what Carl would call itinerancy or some deviation from that ideal, but you've still got some pre characteristic dynamics.

I'm just wondering, you'll know a lot more about physics than I do.

How much does that really square with kind of fundamental physics or understanding of the world?

Is it actually feasible to say,

even if we allow for some itinerancy and change over time, there are there's like a mathematical way that you could still describe that.


SPEAKER_01:
Yeah, I think that actually that assumption is quite consistent with physics because I mean, there's a lot of like equivalency between these different things.

So as soon as I say, oh, these stochastic differential equations tend toward some attracting set.


UNKNOWN:
Mm hmm.


SPEAKER_01:
Another way of saying that is I've just written down a particular form of their flow, like the way that they change over time, some particular equations of motion.

And if those equations of motion meet a few basic constraints, like one of them is that they don't diverge into positive or negative infinity.

which is, I think, a pretty safe assumption.

I mean, if we were assuming all of our equations diverges to infinity, we wouldn't be able to do much with them.

So as long as you say that this equation doesn't diverge to infinity, another way of saying that is there is some measure that this belongs to.

It basically will tend towards something with constant probabilities.

And those constant probabilities

in certain systems will be better described in terms of paths rather than states.

So the paths of the system will have constant probability.

So although that sounds like restricting, it's actually just a statement of writing down any differential equation that doesn't have some crazy kind of impossible form.


SPEAKER_00:
Right.

OK.

final question on the physics, because you did a reply to this, how particular is the physics of the free energy principle made by Aguilera and colleagues.

And I just was really curious, because it's very much pertains to what I was saying about what states are we talking about the particular states, the internal states of blanket states.

And you say in that paper, and I can quote, I expressed general agreement with the target article statement that the marginal flow of internal states does not point along

variational free energy gradients evaluated the most likely internal state.

However, in this commentary, I focus on the flow of particular states, internal brackets, internal and blanket states, and their variational for energy gradients.

And the average flow of these systems do point along variational free energy gradients.

And in my notes, I've literally just written, what does this mean?


SPEAKER_01:
Yeah, no, thanks for bringing that up because I wrote another commentary, I think on the same article that was a lot more like, it was more about this sparse coupling conjecture.

And that one actually didn't get much attention.

But so yeah, that effectively, this is a very interesting point because one of the main hypotheses of the free energy principle, and this manifests in terms of like particular process theories, like predictive coding, is that if you look at the internal states of some system,

they're pointing directly towards the free energy minimum.

So either on average, or if it's deterministic for a particular path, the system looks like it's minimizing free energy very nicely.

So in the same way we train deep neural networks to exactly maximize the ability to predict the test data, we train predictive coding networks

to exactly minimize free energy.

So they're taking the direction of steepest descent towards the free energy minimum.

And at that minimum is where your inference is best.

At the local minimum of the free energy, your inference over external states is the best it can be, given your approximate posterior.

in your generative model so what aguilera at all's um paper found is that if you look at like very simple stochastic differential equations these are like in this class of linear systems so they're very easy to analyze and they're probably not the best model of the more complex systems we're actually interested in in biology but they're a really good starting point what they found is that sorry that's where i live outside yeah i've got to walk um

uh that they found that even for these simple linear systems the average internal states are actually not pointing towards these free energy minimum and that was kind of posed as like a challenge to the free energy principle and the mathematics behind that derivation in my opinion were uh totally sound so it is true that actually even in these uh

very simple linear diffusion systems you don't get this kind of nice interpretation as internal states walking in the steepest direction towards the best inference they're actually taking a more circuitous route towards the minimum which entails the best inference however if you extend your um your kind of your your

definition of internal states to include also the active states and the sensory states.

So the so-called particular states, the things that differentiate the particle from the external environment, then those things actually are pointing down free energy gradients.

And this kind of is in line with this general

idea that i think is a lot more in line with like the idea of sensory motor coupling it's not just the internal states that are doing inference but the internal states are kind of cooperating or conspiring with your active states in order to collaboratively get to that free energy minimum at which point inference is exact or as exact as it can be given your approximations so it's saying okay i don't need to go the the steepest way to

the free energy minimum, I can take like a kind of curvy circuitous route.

But I'm at the same time, I'm pushing my active and sensory states in such a way that will actually minimize the time it takes us to get to the free energy minimum.

And that was kind of the point of that, of that article, or one of the points.


SPEAKER_00:
No, that's really useful.

I'm just wondering, so when we talk about

So we're talking here about the sort of self-evidencing or the gradient descent of particular states.

Could we say that actually it might, what would it mean to say, what would it look like if I said we're doing gradient descent just on blanket states?

So let's ignore, silo off the internal states for a second and just say our blanket states

absent the internal states also, they also point down variational free energy gradients.


SPEAKER_01:
Yeah, I don't think that is guaranteed, actually.

I don't think blanket states are necessarily pointing down free energy gradients necessarily without taking into account sensory and, or sorry, internal states.

So the internal states are kind of the things that are driving the active states to jointly minimize the free energy.

And the free energy, recall, is defined on both the blanket and the internal states because it's defined on the observations, which are like the blanket states.

And it's also defined on your beliefs over external states, which in this case are parameterized by internal states.

So if you define the free energy just on the blanket states, I don't think it's guaranteed to, it will be minimized in the long run, but it's not guaranteed that the dynamics are pointing in the direction of steepest descent.


SPEAKER_00:
How do you, when people, so I think people, this is kind of coming to the kind of education of active inference, I guess, and part of this podcast is learning

me learning and the audience learning alongside with me when people when people like you talk about internal states i understand what it means to sort of parameterize beliefs over external states and in some ways you're doing vfe because you want to return to like a set of external states which is viable for your existence so fish are not going to be found outside water i'm not going to be found in an ice cube so on my freezer i don't know um

Now, my sort of a persistent question that I've always had, and I don't think anyone has really answered, and I'm not sure there is an answer is, is that we say that the internal states are kind of the vehicle for these beliefs.

But are they could you also just say they are these beliefs?

Like, is there anything over and above the internal states beyond these Bayesian beliefs?


SPEAKER_01:
Yeah, no, that's a really good question.

So this gets into a bigger question also about identifiability and uniqueness of the free energy principle and Bayesian mechanics.

So in short, the internal states are not just simply the beliefs.

because they are whatever physically they are.

Like if we're looking at a school of fish and we're saying these five fish are playing the role of the internal states or looking at a cell, this part of DNA is playing the role of the internal states.

It's coding the...

production of proteins.

That itself, that internal state doesn't have anything intrinsically.

It's not parametrizing parameters of probability distributions over external states.

So what's always

at play when you're saying that the internal states are about or in somehow representing external states is there's always a function there, which in the classic free energy literature is called the synchronization map.

And what this is, it's a function that takes your internal states and it maps them into the parameters of probability distributions.

So it's basically something that takes the raw, dirty physics of the system, however it is.

the metals and the ions and the things that actually consist of your system, and it's mapping them into a space of probability distribution.

So if we wanted to say that my neurons are encoding beliefs about the position of objects in the room,

that that function that synchronization map would be doing is it would like take the firing rate of a neuron which is like an actual physical neurophysiological property that i can measure from the neuron and it's mapping it to something like the mean of a gaussian distribution or the variance of a gaussian distribution so it i mean i guess in that sense it's like do you think that the firing rates of the neurons simply are the belief i would say that

actually this this kind of hints at at something i i um saw you talking about alex kiefer on on one of your earlier podcasts is like this kind of identity problem like are the neural firing rates the belief the representation themselves or is there some map that takes the raw physics of the system and then maps it into a probability distribution into a representation so i think you always do need that function that actually takes the system and maps it to a belief

But the problem with identifiability is it seems like there may be many candidate functions that take the system states and map them to a probability distribution over beliefs.

So it's very hard to just look at a system and say, oh, yeah, it's forming beliefs about the average distance to neighbors or the light refraction in the room or the contrast of some

some image out there we we usually come to the table with very strong priors about for instance neural coding we think that neurons either code things with the timing of spikes or they use like the firing rate and that's how they do the encoding and that's basically saying we come to the table with priors about what that function is but in theory there's nothing about the free energy principle that tells you what that function is so it's a very hard like identifiability problem in general


SPEAKER_00:
Okay, that's interesting.

Yeah.

I mean, another thing that kind of sticks in my mind when I think about this is where does the weather like,

where do the beliefs stop?

So what point is something internal to the system?

And what point is something external to the system?

So like, for example, let's say my eye, right, there's something like me, let's take as a kind of presupposition, I am conducting variational Bayesian inference, like, over the external states, but the external states in some strange way are also myself, right?

Because I also, let's say, the observations that come with respect to my glucose levels,

now that's because i need to have some beliefs right some uh probability distribution that my glucose levels are going to tend towards a certain point in that like there is some internal belief about an external state but that external state is actually still within me now we kind of get around this through this notion of embedded markup blankets right like i'm not actually paramatrializing those beliefs but actually my cells are but i guess the point here is

It comes back to a question that I've had loads of times and people have had different answers to it.

Is this just a useful structuring tool here where I can just say, well, I don't really know why or how or when this Markov blanket ends and becomes part of the bigger order Markov blanket and so on until you get something that it's like to be me, but it's useful and we can do some useful maths or useful physics based on it.

Because I just think like, again, it's philosophically vague as to does it makes like, is the cell is part of the, you know, is there just a straight elision really, between an internal state and an external state from the perspective of a cell, because it seems that it's also doing Bayesian beliefs on its own location, its own status, its own, right, like, so

I've always struggled with that with the free energy principle.

Yeah, it makes sense for me that the internal states I have to parameterizing beliefs about where I am in the world or who I'm with and so on.

But what about myself?

I've got to be about my glucose levels, my temperature, my well being and all the way down to my very granular physics level.

Where does it stop?


SPEAKER_01:
Yeah, exactly.

I think the generic answer to this, I think is basically what you meant by embedded Markov blankets, which is if I look at like a coarse grained level, like more psychological level of explanation, like you have beliefs about who's in the other room or what's behind your door.

So that itself could be drawn out as like a Bayesian network where you have some sensory states, which are like your sensory organs, like your eyes and ears and nose.

And then you have your internal stage, which is like your psychological state or something or some coarse grained representation of what your brain is thinking.

And then you have that external state, which is like the presence or absence of another person.

Each of those nodes in our Bayesian graph, we can kind of double click on and zoom in.

And that itself is a thing that has internal blanket sensory.

So that's like the generic thing.

answer that that um people like carl friston will talk about with the renormalization group like you have markov blankets at different scales and of course the work of maxwell ramsted and people like that who are literally like writing papers about how exactly this happens you have this nested markup blanket so i think that is the the general answer is that

every time we write down a Bayesian network and a graph, we're like saying, okay, that's the blankets, that's the internal, that's the external.

Every time we do that, we're kind of admitting, well, actually I can always like take what I consider a single state and just rip it up and like start partitioning that.

And then I think the ultimate conclusion of that exercise is basically your conclusion, which is it all becomes a matter of why are we doing this?

Like, why am I at this level of zoomed outness

And it's probably for some practical utility as a modeler, as a neuroscientist, as a physicist.


SPEAKER_00:
Yeah.

Yeah.

So if we take the sort of psychological perspective and let's say, you know, so let's say you actually will use your preprint.

So the paper that I was mentioning before was from the I believe it's collective behavior from surprise minimization.

So that's a really great preprint that people can check out is obviously quite technical, but there's, again, lots of things that people can get out of it.

let's say, you know, we embed these, we're doing these Bayesian beliefs over the distance that I am from another person and the rate of change of that distance.

I guess my question here is, am I also conducting Bayesian beliefs about where I am?

Do I get where I am out of just like, do I get where I am just out of the beliefs about where other people are?

Even at a very psychological level, you know, there's someone else in the other room.

Do I also need to be doing

Bayesian beliefs about myself in relation to those external world?

And does that mean that I am also in some ways, like I've got a dual perspective, I've got like this dual monism where I am doing the Bayesian belief about myself as if I was part of the external world?


SPEAKER_01:
Yeah, kind of like doing theory of mind on your own self, like having to simulate your own inference process.

Yeah, I mean, so I think the argument made in that collective behavior from surprise minimization paper is that you can get away without that variable.

Like you can get away with a lot of coordinated behavior,

where agents are tuning their own position and their velocity in space just by operating on very low level reflex arcs, basically just trying to minimize sensory prediction error.

And so those agents in that paper don't have any representation of their own position.

That's not like a latent variable in their generative model.

all they do is they have some velocity vector and that induces prediction errors and what they can do is they can just change that velocity to basically reduce prediction errors so you can get a get quite a lot of sophisticated behavior without that representation however i think there are plenty of good examples of systems and maybe this gets more into like what you would call metacognition although it doesn't even have to be that yeah but there are systems where you would like to have

a representation of a random variable in your system that represents something like the self position.

And you would like to do inference over that variable.

And that basically is what active inference is.

If I have a variable that corresponds to some latent that can control my own body position or my speech or whatever, and doing inference over that latent variable is basically what ends up being some kind of agency or control.

And I think the cases when you would want to have that extra variable are ones where the dynamics of yourself are above and beyond what can just be determined from your sensory inputs.

So for like these reflexive fish, I can get away without having a self variable because their behavior is pretty much completely a deterministic function of their sensory states.

There is a little bit of like memory in there because they have the belief from the last time step of belief updating.

But it's very simple.

It's not like a very complex dependency.

When you have individual agents that have a lot of autonomy, I guess, which I would just parse into more long-term nonlinear dynamics, then it makes sense to introduce another latent variable in your model that corresponds to something like myself or my position or my actions.

And at that point, you're going to get more of, I would say, a tension between externally driven behaviors that are driven by sensory stimuli only

and internal, more like endogenous or autonomous behaviors.

And those are the cases when you would want to put that latent variable in there.


SPEAKER_00:
Yeah, I've been I've been thinking about this myself.

I'm, we're just finishing up these edits for this flow states paper, and trying to explain sort of with Lars and Carl and others and try to explain sort of what it means for the you know, this is the self is the so called or hidden, like one of just many hidden causes, according to active inference is hypothesizing cause, and trying to explain kind of the difference between an internal cause and an external cause for something that's already presumably internal, right, like me,

And yeah, this is cool, though.


SPEAKER_01:
I think I remember hearing you're probably more up to date with this, but I remember hearing Carl talk about the idea that the very fact that we introduced something like a self was basically a new structure in a generative model in order to explain disparate input.

So it's like I'm hearing a bunch of speech signals.

and if i don't if i only have one latent variable in my model which is like there is somebody speaking right then it's not going to be a good enough model of like the speech signals and when you get enough of these like social signals it actually becomes the free energy minimizing solution to break that node into two and there's one that i can control and there's one that's someone else that i can't control exactly that's the best way to like explain the data


SPEAKER_00:
Well, yeah, and presumably this is happening actually in developmental psychology and developmental biology whereby a child doesn't really know, have a self-other distinction.

And I always relate this back to, I think action is very important.

So this is going back to the work of someone like Chris Frith and the comparator model whereby I can make predictions about the consequences of my action.

And the very fact that I can do that and witness the sensory outcomes of my action means I kind of get, well, I get this sensory motor arc

And then I can have a higher order inference that there is something that that internal to my generative model that is driving that action.

I guess the question here is what's happening in something like a psychopathology, like thought insertion and schizophrenia, where people don't trust the thoughts that I mean, I think this is actually kind of true.

And I think the Buddhists actually fleshed this out quite well, though, obviously, if you actually meditate, you'll see that while the thoughts are not yours, like they're not cut, they're not you're not making the force.

But there's a classic thing in schizophrenia of like,

people who have schizophrenia will often think that the thoughts that they're thinking aren't theirs.

They're being beamed in by an alien or a government surveillance operation.

How you feed in something like a predictive model into that, unless you take thought as an action and then say, well, I'm predicting the outcomes of my thought and so on.

that because I guess we think of predictions as thoughts in some realist way.

And so you end up with this circular loop.

But yeah, it's fascinating.

I think it's Yeah, it is fascinating.

I think it's a I think it's a I think this idea of the self as an inferred cause is reasonable.

I think the only problem is, is that it engenders some kind of potential homunculus, which is sort of what's doing the inference in the first place.

Yeah, exactly.

But these are questions for people.


SPEAKER_01:
Yeah, that brings you back to having to define internal states.

Like, what if the internal states are the self and you're inferring that that even exists?

you need to define the inference process on some other subset of states like that.


SPEAKER_00:
How did you get off ground in the first place?


SPEAKER_01:
Yeah, exactly.


SPEAKER_00:
Yeah.

Cool.

Well, you've just given me another paper idea, and maybe a PhD.

So, okay, awesome.

I wanted to ask about this scene construction paper, because I've been really interested in scene construction.

And I was, I was happy, you know, I saw you work with Bette Mitzer, and I think his work is really amazing.

And I just wrote this paper on attention and it's super inspired by what he was doing with Carl and this, especially their selective attention paper, their 2019 paper.


SPEAKER_01:
Yeah, definitely.


SPEAKER_00:
Maybe just for the audience who aren't, who probably have an idea and intuition what scene construction is, and they probably think of it as visual, although I really like the sort of language example here.

Maybe you could explain just at a very sort of high level what scene construction is.


SPEAKER_01:
Yeah, no, I like that you brought up the language example, because that's actually what originally compelled me.

Even when I saw those papers on scene construction, I actually immediately thought of it in terms of like parsing and language.

A scene construction is just kind of a type of hidden variable inference.

So I have a bunch of sparsely sampled data points.

And then given that sequence of data points, like little sensory samples, I suddenly kind of

infer something that's explaining all of them at once like oh this this is the latent cause this is the latent exp explanation that kind of unifies all these disparate um these disparate data points and i guess the construction aspect comes from two ways one is the fact that usually the way we get that sequence of sparse data is through some kind of sampling so like as i read a sentence

I'm tip in like Western languages, you're typically reading left to right.

And as my eyes go right across the page, I'm gathering more evidence.

and I'm constructing the scene as in my posterior probability distribution over these different explanations is kind of shifting until at some point, and often we see with human language and visual understanding, there's kind of this like nonlinear feedback where, oh, it kind of snaps into place.

Oh, this is what's going on.

And it's very interesting seeing this in language too, like in different languages, like German, for instance, the verb will come at the end of the sentence.

So you kind of have to wait until the end of the sentence where that meaning kind of snaps into place.

so that that's generally what i would describe as scene construction and then in the visual case often it's um you might be looking at a a a painting or something like an ambiguous painting let's say some a modernist maybe abstract thing with not very clear figures and you're sparsely sampling it with your eyes trying to figure out like what's actually going on here and at some point when sampling you you get that gestalt impression of oh there's like a figure in a cloak or something

And that moment of gestalt recognition is like the act of constructing the scene, of inferring the latent explanation for all the sensory data that I've gathered.


SPEAKER_00:
Excellent.

That's really useful.

I guess something that Beck's work really drove home to me and impelled me to write this paper was the importance of context.

So if you're doing a sort of Yabba's task where you have a picture, exactly what you just said, you have a picture and then someone says, well, how rich are the family in this picture?

And then you scan all of the furniture and you look at their clothes and then they ask something else.

I don't actually know what it is, but let's say, how well do they get along with each other?

And you look at how they're sitting and you look at whether they're looking at each other.

So context is clearly driving a kind of overall, the kind of derivation of a latent courts in some ways, because you're going, Okay, well, I'm going to pick out this evidence.

And that's going to drive my inference.

So are we, would you say that we're always integrating a kind of higher order context?

when we're doing scene construction such that, you know, there is no such thing as the kind of neutral scene.

It's always, you know, the inference is always being driven by our recognition of where we are, what we are, and so on.


SPEAKER_01:
Totally.

Yeah, I think that's that that's one of the most interesting aspects of Bayesian modeling in general is like,

The ability for people to very rapidly infer a context, which to map it back to the kind of abstraction I used to describe scene construction in the beginning is like, you're inferring which latent explanations you're even testing.


SPEAKER_00:
Yeah, exactly.


SPEAKER_01:
In the case of, do you get the sense that the family in the painting is rich?

You're suddenly...

that's inference of a rapid context that then induces which latent explanations am I interested in learning about?

Like rich versus not rich.

Whereas if the cue or the prompt was, is the man wearing a red shirt?

Then the different latent explanations that you've suddenly inferred you have to test is a different set.

And I think that's like one of the most amazing and kind of underexplored abilities that humans and other animals have is they can infer,

can use context to basically quickly infer a generative model so like the context kind of selects the generative model that then you use to do something like active inference yeah well yeah i've been thinking about this um because clearly


SPEAKER_00:
clearly it's just not computationally feasible to contemplate or assess every possible state.

I mean, this is the whole basis of variational Bayesian inferences that kind of P of Y is just, it's in almost infinitely diverse in its state space.

And so I've been kind of proposing that there's a higher order action policy, which does the kind of pruning to use reinforcement language, the kind of pruning of your potential state space.

And I've been thinking about that a lot in terms of action, because even action, I don't consider doing absolutely everything at any given time.

Have you got any other further intuitions about how that might work?

Because I think the slight tricky thing here is that we have a kind of higher order policy, let's say, to prune our state space, but that needs itself to be driven by observations.

right?

I need to know, okay, I'm going to prune accordingly, because I've observed something in the world, right?

Like, so I see that it's raining, for example, in London right now.

And so, like, I'm just immediately pruning swimming trunks and sunglasses and so on.

Yeah, I needed to do the observation in the first place.

So it sounds like I got to have a higher order action policy to observe the world to drive my context.

I'm just curious about how much how deeply recursive does that go?


SPEAKER_01:
Yeah, I think like the the overall

My Amin.

I would like to think that there's a generic solution where you basically can start with no structure and build everything from scratch.

But I think there's basically no free lunch here.

And I agree with you that maybe the best way to explain human behavior is that you're inferring a context or inferring something that then allows you to prune or select a model that then you can use in practice.

But the process by which you do that also assumes some kind of model.

What's the model by which you infer

I need to act to do this.

Where those models come from, these deep hyper priors about the spaces of models that I can select at a lower level, those are probably things that are developmentally inborn.

That's probably the space that evolution actually works on, on the space of hyper priors or deep hyper models that babies are born with in their brain.

And these things are probably things that you and I couldn't elucidate in

like in human cognitive neuroscience is like a probability of the dots moving right versus left.

Like that's a likelihood we can write down.

These kinds of models that are like the models about how do I select which structures to use for a given task?

Those are probably very hard to put into words, but those are the sorts of things that I expect developmental psychologists have somewhat of a hold on.

Like these, they probably have to do with like intuitive folk physics, like things like

There are objects, there are agents, there are things with like spatial temporal correlations over time, like very deep physical constructs.

But I do think that at a more mechanistic level, what you're saying makes a lot of sense.

I think like one way you could formulate this is in terms of mixture models.

So mixture models is like I have some assignment variable that's very high up that tells me which model am I using right now.

And so the mixing variable is what you infer.

And then given the inference of the mixing variable, that effectively selects one of five or ten or a thousand different alternative explanations for the data.

And usually those are models of the same form, like a Gaussian mixture model is a mixture of Gaussians.

But I think in what you're discussing, what you're mixing are totally alternative structures.

One structure that I'm inferring, one context I'm inferring I'm in is like I'm driving a race car.

Another context is that I'm talking to a friend or a colleague.

And those are totally different models.

I think one place where the mixture model falls apart is

With mixture models, you would assume that when you infer that higher level assignment, sometimes that inference can kind of be wide.

So you're not sure what context you're in.

But when you look at human behavior, people tend not to mix context.

There's probably some experimental paradigms where you get that.

But oftentimes people infer like they kind of select like I'm definitely going to make these assumptions going forward.

I'm definitely going to make these assumptions.

So there might be something like a better model of structure selection than mixture models.

But that would be one idea, I think, like mathematically where to get started would to be looking at like mixtures of structures effectively.


SPEAKER_00:
Yeah, nice.

I spoke to Sanjeev now Joshi about this on the podcast, who's a versus with you.

And I spoke to him about it.

And we went back and forth.

And I think we ended up grounding out and evolution.

So I think absolutely, you're absolutely right.

I think there are a couple of things I want to say to that one is, it's always I tell everyone this on this podcast.

And personally, I think it's always a bit dangerous to just take one slice of the generative model at one slice of time and take that to be what's actually happening.

it's always dynamic the observations are always feeding into the upper levels which are feeding back down i guess that's the beauty of a kind of predictive coding or a deep parametric modeling system is that like

this is just an idealization.

It's constantly happening.

Personally, I think the other like point of complexity here is that not only do we need to, let's say, derive a context, we also have to derive the salience of the context or like the relevance.

So you know, I've been Yeah, I wrote this paper where I'm kind of talking about a circular causality between preferences and attention.

And something that I kind of realized was, well, well, I knew it, obviously, because it's just for my life that if I'm hungry, like if I'm truly starving, I'm not good.

I don't care about anything else that's on like, that seems fundamentally peripheral at that time, whether that's finishing a paper or, you know,

being on my phone and so on, which means that like, there's almost not only is our higher order policy that I prune my state space, according to the fact that I'm now hungry, but also that now trumps any other states possible state space, like on my phone, or I'm writing my essay, which implies that there's like an even higher level precision weighting policy that says, Well, not only do you have to prune accordingly, but you also have to prune in a hierarchical fashion.


SPEAKER_01:
Yeah.


SPEAKER_00:
And I don't know, again, I don't know how deep this goes because as Carl will tell us, anatomically, the brain is very ridgy and very recursive.


SPEAKER_01:
Yeah, that's true.

I mean, that also reminds me, I think, of this paper of Burke's that you mentioned from 2019, right?

Because if I remember correctly, that version of selective attention actually depended on the preferences.

Is that right?

So yeah, that's something like other than that work by Burke, and I guess the paper you're discussing now, I don't know of much work that explicitly uses the preferences

to then select what generative model is being used for inference.

Because you're absolutely right.

It's not just, oh, I have some observations that I infer the context, which then tells me how to act.

But that inference also depends on what your goals are, like what is important right now.

And usually we get importance in the kind of classic model based reinforcement learning ways.

We roll out a given policy and we see how much does that policy overlap with what I want, like a reward function.

We basically evaluate policies in terms of reward functions.

But what you're suggesting

maybe cannot be accommodated in that way, where the actual inference itself is biased by, it's not just policy selection, but even state selection or state inference is biased by what's important, what's relevant.

And I think there are ways you can get that.

Like there's this generalized free energy paper by Thomas Parr and Carl Friston from 2019, where they show that in that particular formulation of

variational free energy called the generalized free energy you actually do get this interesting influence on state inference from preferences so your preferences will kind of optimistically bias your state inference so that you'll only be doing inference about things that kind of matter for your actual selection so that might be one one place to also call it pointed me in the direction of that paper because you read it you read my paper and then obviously you didn't tell me about this one that sounds absolutely right yeah because i mean my way of coming at it was thinking well


SPEAKER_00:
in principle, you know, there's near infinite things that I can optimize my free energy minimization over, right?

Like, like, and there are ways out of this.

I mean, some people will talk about something like, you know, Julian Kibberstein, or Mark Miller, or Mark Anderson was all about aerodynamics.

So it's not enough to minimize free energy, there needs to be a constantly improving slope of prediction error minimization.

I still don't think that does it because in principle, I could just get way better at tidying my room, but like that's just it or something extremely trivial.

Yeah, that's gonna cut it ultimately, because I might just end up starving.

So it's Yeah, it is exactly as you said, I still think this can be couched in terms of something like an expected free energy

function it's just it's just far it's just higher order and everything trickles down from it such that as you say like now we have like a selected generative model in truth i haven't thought too much about how this could be implemented mathematically because yeah you know


SPEAKER_01:
That's the place that... Yeah, that's totally true.

You could actually say... Because the typical way we do the expected free energy is in this brute force enumeration.

Let's say I did this.

I did X versus Y. If I did X, what would my expected free energy look like?

It would be a combination of the utility, which is the preferences, and then this information gain.

If you were to evaluate that brute force for model selection...

if I was to select this model and then go through a whole round of active inference with this model, it just becomes very hard to compute the expected free energy, I think.

So maybe there's just a better algorithm or implementation that would allow you to kind of heuristically evaluate these counterfactual consequences.

Like if I selected this model, would that make me less hungry?

Would that like save me?

But you probably have to come up with some heuristic or some clever way to get around the enumeration.


SPEAKER_00:
Well, I wonder whether it could just be belief free, whether you might just like, we might just have contextually queued model selection.

You know, the moment that I have the slight, you know, the slightest drop in my glucose, then the whole thing cascades down, such that I have the prioritization of seeking food, and I, I don't even have to run the expected free energy of, I could go here, I could go there, I could go and what am I, you know, how well am I going to do?

Yeah, rather

I just have this what now I just at the very bottom of all that process, I just have this one preference, which is, I think, you know, something like that might hijack or shortcut the whole process leading to like, quite low compute, but actually a pretty robust way of like getting adaptation, because presumably, this is something similar to what a child is doing, right?

Like, yeah.

And I think that kind of works in terms of like,

I don't know, like the simplicity of children in some ways that they sort of cry over anything, which kind of implies that there is just like a very simple stimulus response mapping.

And I wonder whether that could just be expanded, you know, just more broadly to model selection.


SPEAKER_01:
Absolutely.

Yeah.

I think that ultimately is probably what's really happening in biology because this also extends across the biological spectrum.

Animals are very context or basically needs driven.

Things like attention, all these basic cognitive faculties are really enslaved to

effectively the basic things that the lower brain is telling the body it needs to do, like breathing, eating, thirst, sleeping.

So I think ultimately it is some fast stimulus response mapping.

I think the only issue with that is it does kind of

it seems less principled, right?

It seems like, oh, we just have to hack in a short circuit.

And it's not just like evolving through a neat process of free energy minimization.

Maybe it is, but it's just happening on a slower timescale.

Like it's some slow accumulation of experience evolutionarily that has trained this short circuit inference to say, yeah.


SPEAKER_00:
There might have been some EFE calculation on the part of evolution to select for evolution.

you know, stimulus response mappings for something very like fundamental, like food or sleep or breathing and so on.


SPEAKER_01:
Yeah, exactly.


SPEAKER_00:
Yeah.

Okay, cool.

Well, that that's given me lots of ideas I need to think about.

Final sort of topic I want to touch on is, you know, just before we went live, we spoke a little bit about you being versus full time.

And I want to sort of ask again, I know, it's been the Sanjeev and others, everything is very under the wraps.

And I respect them.

I don't want Maxwell to come and

shake me.

But so I was curious, just a very sort of high level, you've gone from biology.

So thinking about biological feasibility, to modeling and artificial intelligence and trying to ground something in, you know, trying to take biotic cognition and say, Well, what makes that intelligent?

And then can we replicate that in silico or, you know, in a robot?

I was just wondering, how biologically feasible are certain assumptions made by active inference modelers?

For example, active inference modelers talk about, again, not exactly my bag, but a mean field approximation, for example.

where presumably the hidden states or the different tensors have conditional independence.

How you can take that one, you can take any other assumptions more broadly.

Does this seem to you actually biologically feasible or is it just like an interesting modeling tool?


SPEAKER_01:
No.

Yeah.

Those are probably the most interesting and important examples of the assumptions we make.

So I think the biggest one in one word is factorization.

The models, both the generative model that animals have, as well as their cue, their approximate posterior, which they optimize using free energy minimization, those almost necessarily have to be simplified, in my opinion.

There are some people who believe that the brain is doing sampling-based inference, in which case they can actually be doing full posteriors, in which they don't need any simplifications in their model of the world.

They're doing full posterior inference on their model.

I don't think that really holds up because it's way too slow to be...

useful for the timescale that biology needs to work on.

So I think those assumptions like factorization in the generative model, those are quite realistic.

And factorization in the posterior, in this approximate posterior.

I think that's actually one of the coolest things about variational inference is it seems like something that brains can do.

Because once you make these factorization assumptions, what it gives you

are updates to your beliefs that are beautifully local like they they only need nearby information and this is kind of the whole um

rallying cry of people like Bert DeVries who work on these message passing and factor graph schemes.

What's beautiful about that is it's distributed and local and that locality inherits from the fact that we're making factorizations or we're making assumptions of certain variables in my model or in my beliefs are independent of each other.

A specific question is, is the mean field factorization appropriate?

That one assumes that all the different state factors are conditionally independent, actually just fully independent.

That one, I think, is an open question.

And so there's basically a knob we can turn from fully factorized where everything's super cheap and local

but you're introducing approximation error.

As soon as you start making these assumptions, your beliefs are not going to be as good at actually explaining the data and minimizing free energy.

On the other side, we have fully enumerated, combinatoric, entangled beliefs and explanations for the world, which are maybe the most accurate, but doing inference on them is going to be expensive and take a long time.

So the open question in my mind for biology

if we were trying to really map biological processes to like variational Bayesian inference is like where on that sliding scale is biology or our particular systems.

And so a lot of the work that we're trying to do at Versys is basically find good settings of that knob.

Like we want to figure out what is a setting of factorization of complexity we can build into these models that they're still sufficiently expressive and generalizable.

They generalize across contexts, across tasks.

but they're not computationally insane.

Like we need to make things cheap and we need to also, we would ideally like to make them cheap enough that we could imagine

neural wetware implementing them with like local computation.

Awesome.


SPEAKER_00:
Yeah, yeah, that's, yes.

I mean, I guess that is the kind of, well, that's always the accuracy complexity trade off, right?

I mean, it's built into the very, I started in my introduction by asking about what I mentioned, in my introduction, apparent teleology.

Actually, this was probably the first question I ever had in when I discovered active inference, what

a year and a bit ago, which is

Well, what is free energy?

Why?

Like, is there a little sensor in the brain that is like cued into this?

Like, again, this was me being very naive.

Is this like little thing and it's sensing free energy and it's trying to get it down to this minimum, which again, now I think about it obviously implies some kind of action policy that is descending down free energy gradients.

So it is a naive question, but in some ways it's useful, I think, because it feeds into an intuition, which is

human, you know, there's this long standing, there was a long standing debate about like, what life is, right?

I mean, like the Alain Vittal, is there like some special source that we put into humans that just like, means that we self organize?

And I guess this is just a perennial problem, which is, well, why are we doing this in the first place?

Right?

Carl tells us that if we are doing, you know, if we are, if I can say that you're Connor today, and you're calling tomorrow, I can kind of describe what you've done.

But why you do that is a kind of mystery.

And I think some people intuitively think, well, maybe there's this adaptive sensor in the brain which tracks this kind of special thing called free energy, which is probably not happening.

Let's be honest.

and it was obviously an information Lee theoretic term that makes things compute tractable from a modeling perspective.

But when you're talking about putting that into an artificial agent,

the question that comes to my mind is, is there anything else that you need to do?

Is there a higher order prior, for example, you need to put into it, that means that it actually will act to minimize variational free energy, that actually will self organize?

Because there's like, it's kind of like, why does a computer work?

Like what I've always just like, what's the kind of fundamental thing and maybe in the case of computer or humans, it's electricity?

Is there anything like that you need to kind of put in that means that it's just doesn't just sit there and

is built with these beautiful priors, but doesn't really act in accordance with them.


SPEAKER_01:
Yeah.

Yeah.

No, that's a really deep question.

I mean, so at a fundamental level, I totally agree with you that there's no secret sauce.

There's no special signal.

As soon as I've written down any equation of motion that doesn't explode any equation of motion for systems that just generally keep existing.

Another way of saying that is I've written down a free, I can write the dynamics of that system with some free energy functional.

So as designers of artificial systems, we have the luxury of not having to write down an equation of motion in such a particular way that we know if we backed out the Bayesian mechanics of it, oh, look, it's actually minimizing free energy.

We can start with the free energy.

So as artificial engineers, we can say, no, we're going to actually have these things integrate equations of motion that minimize free energy.

So that's what we do.

But systems like self organizing biological systems in the same way

on the Savannah, the drafts necks are not getting longer to reach the trees because they want to optimize fitness.

That's a post hoc, like apparently teleological description for the dynamics of systems.

And then we put a sauce on it, a descriptive sauce called fitness or called natural selection.

That's the exact same thing with free energy.

There's just dynamics.

And then post hoc, we dress it up with, well, these dynamics look like they're actually doing approximate Bayesian inference.

And that's a cool,

useful, in my opinion, framing in the same way natural selection is a cool, useful framing for statistical filtering of phenotypes over generations.

But as artificers, as builders and engineers, we can actually write systems that optimize fitness.

We can write an algorithm

We could write a genetic algorithm that does it automatically.

We can also write an algorithm that directly surfs a fitness landscape.

And in the same way, we can directly write algorithms that minimize free energy.

The problem is we now have the burden of proof of having to write down a general model and to figure out what the best free energy functional for a given task is.

I think a more almost complex systems

way of designing a system would be to actually just design a bunch of particles in a vat and basically design physics, design a particular sandbox of physics that emergently gives rise to agents that minimize free energy without actually plugging that in at the level.

But that's hard though.


SPEAKER_00:
Yeah.

That's kind of where I was leaning, right?


SPEAKER_01:
I see.


UNKNOWN:
Yeah.


SPEAKER_00:
I guess it does bring into mind just fundamental questions of teleology and makes you think, well, why is physics doing this in the first place?

And as you say, it probably would be in silico.

Can we get these kind of particles that just do it?


SPEAKER_01:
is there a way that we could get to the bottom of that rung without invoking another axiom or another miracle or another process and maybe not you know it's um yeah that's yeah is your thinking that if we did it that way so we started by just like writing down very simple physics of like particles and of that and then we we get the amazing mechanics emergent for free is because i've thought about this as well in your thinking is that like is there an advantage to doing it that way rather than top down


SPEAKER_00:
Well, I think there's an advantage in the sense that to my kind of eyes, there's a slight difference between natural selection and active inference.

And that's because I think with natural selection, you get both the principle and the process.

Now, there are obviously some axioms and miracles.

Now, I'm not a geneticist, so don't shoot the messenger.

But the basic idea, obviously, is that you get the mutations.

And mutations are what drive natural selection, because it leads to kind of differentiation within the pool of genes, if you take a very sort of gene focused form of Darwinian natural selection.

But that gives you both the principle, okay, over time, given mutations, there will be optimization for fitness just for free, because you just get, obviously, you get the things that are successful in, you know, the genes that permit reproducing themselves being reproduced, and so on.

But with variational free energy and active inference and the free energy principle, you get the principle.

What you don't necessarily get is the process.

Namely, what Carl tells you is if things persist, this is what they have to, we can describe them using these mechanics.

It doesn't give you why things persist.

So that's kind of what I mean.

And I do think that if you had a kind of completely neutral system, because the reason why some of these queries come to mind is because I think in the older papers, the older active influence papers, they would very subtly squeeze in this thing where it would say, well, there's also the higher order prior or higher order action policy to minimize free energy.

And then that, I think, is a very philosophically fraught thing.

Because then you go, well, is there a higher order policy for that policy?

And so, right.

And evolution obviously ground some of this out.

But it makes me think like, sure, it's a really lovely scientific tool.

But the power is really in the process theories, whether that's decoding, active inference, variational message passing, POMDP schema, whatever you want.

because you don't get the process for free from the free energy principle.

And I think in that there's a slight difference with evolution.


SPEAKER_01:
Totally.

Yeah, I see exactly what you mean.

Yeah.

I mean, I guess one

So it depends on how you define evolution.

I would potentially argue that natural selection is even happening in non-biological systems.

It's just the process theory that provides the equivalent of mutation is maybe slightly different.

Right.

But I guess you could still say that, well, there's still a mutation.

Like it's just... It's a change.

Yeah, it's a change, exactly.


SPEAKER_00:
Yeah, yeah, yeah.


SPEAKER_01:
I think my hunch is that something like that could also be identified for...

a variational free energy minimization, but I think it would be something that maybe would not be satisfying

Like you need, which is another way of saying change.

You need some function that maps a state to another state.

And that if you iteratively apply that function, it doesn't diverge to infinity.


SPEAKER_00:
Yeah.


SPEAKER_01:
And I think that would be like the kind of process, theoretic equivalent of mutation in, in natural selection.


SPEAKER_00:
Yeah.

I'm sure someone's gonna do it.

It probably won't be me.

Probably will be cold.

Let's be honest.

Yeah, it comes back, I think, to this idea that Kate was talking about, and I haven't read her book, but I know it's out, which is that maybe a slight concern with active inference and the free energy principle is that at its very core, it rests on some kind of determined characteristics.


SPEAKER_01:
Mm hmm.


SPEAKER_00:
maybe I mean all theories have their axioms right I mean all theories have their miracles and natural selection definitely has its miracles but it's very neat and clever and it all wraps up quite nicely and yeah I think you're absolutely right that maybe there's a little bit more work to do to really get the process of like why things don't just fall apart I mean this is Schrodinger's big question right I mean yeah and Karl's big question but

Yeah, I think it's very important for an audience to realize that when we talk about active inference being the overarching theory of everything, that's probably never, never really made those claims.


SPEAKER_01:
Yeah, I totally agree.

We need, we don't have the physics of why things don't fall apart yet.

We've assumed that that physics works and is there.

And then we do the free energy.


SPEAKER_00:
Yeah, yeah, yeah, yeah, yeah.

And which makes one think, and again, this is no disrespect to Carl or you or Maxwell, anyone who's deeply invested in this, like

Exactly how much are we getting out of the free energy principle?

Because I think Carl himself takes a very deflationary or tautologist count of the free energy principle, which is that it just says, if things persist, they act and perceive like things that would persist do.

Things act in accordance with themselves if they are to exist as themselves.


UNKNOWN:
Yeah.


SPEAKER_00:
But as I said, how things do that, right?

You have to do that.

If I make the fundamental presupposition that you are the same thing today as tomorrow, like you have to at least be obeying by that principle, how you do that.

Well, I guess we might know.

Colin, this was really, really fun.

These are wonderful, deep questions.

And I'm actually just very glad.

I think something that people need to be more of is epistemically humble.

and say where they don't know the answer.

And I'm very much in admiration of the fact that you're willing to say that there are plenty of things that this community still doesn't know.

So and yeah, the insights are just wonderful.

I mean, I my final question actually is just a curious curiosity, which is you were a neuroscientist, right?

So master's neuroscience, PhD neuroscience?

How on earth did you learn all the maths and the physics?

I'm just out of curiosity.


SPEAKER_01:
No, that's a great question.

Yeah, so I did my bachelor's in kind of more cognitive neuroscience, doing a lot of EEG, psycholinguistic stuff, so not really math heavy at all.

And then in my last year of bachelor's, I took one computational neuroscience class at University of Pennsylvania.

It was Vijay Balasubramanian.

He was the teacher.

And it just really turned my mind not only to comp neuro, but then all the methods that you need to know to do proper computational neuroscience.

And that kind of just opened the Pandora's box a little bit.

And then I did a two-year internship at the National Institute on Drug Abuse, and we were doing

again like translational pre-clinical neuroscience working with rats and mice and just during the evenings during that that two-year stint i was taking all these like machine learning and computational neuroscience classes that at the time in like 2015 were still like free on coursera so like andrew ing's course jeff hinton's class

And I, yeah, I just like had to teach myself a bunch of math.

And it was annoying and hard, but I think it helps me catch up a lot to a point where I could then go and read Carl's papers and actually like kind of understand what's going on.

Yeah.

But it was definitely a really long and like self-taught journey.

And because of that, I do have a lot of like foundational gaps in math that people like Lance DeCosto, like you can clearly tell by talking to him, he doesn't have those gaps because he has that proper education.


SPEAKER_00:
Well, I'll tell you, there's one man with the biggest gap of them all, and that's me.

So the PhD will be fun.

It'll be great.

We'll be bootstrapping our way to some kind of mathematical adequacy.

Definitely wonderful.

Wonderful.

So much to think about.

So many wonderful insights and great intuitions.

And yeah, it's just been an absolute pleasure on my part, and I'm sure everyone will benefit accordingly.


SPEAKER_01:
So yeah, it's been so nice.

Thanks for inviting me in the first place.

And it was really a pleasure talking to you.


SPEAKER_00:
It was wonderful.

Connor, thank you so, so much again.

Thank you.
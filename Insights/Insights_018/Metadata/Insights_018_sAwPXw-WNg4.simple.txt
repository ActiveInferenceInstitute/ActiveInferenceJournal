SPEAKER_01:
Hello, everyone, and welcome back to Active Inference Insights.

As always, I'm your host, Darius Parvizi-Wayne.

Can't even say my name anymore.

And today I'm joined by Dr. Sanjeev Namjoshi.

Sanjeev is an expert in data science, computer vision, machine learning, and bioinformatics with strong teaching experience in bioinformatics, biochemistry, and computational neuroscience.

He is also what I would say is a world expert in active inference.

In fact, he's currently putting together a textbook which explains the fundamentals of the theory, which will act as a wonderful addition to the education resources we as a community already have.

Sorry for butchering that intro, but Sanjeev, welcome to the show.

We've got past my least favorite part of the show, which is the introduction, so we're here.

Thank you again for joining me.


SPEAKER_00:
Yeah, thank you so much for inviting me.

I really appreciate it.


SPEAKER_01:
No, it really is a pleasure because I have been, you know, it's the least I could do.

I've been benefiting a lot from your textbook, which is not out.

So people, if they're sort of scrambling on the internet to try and find your textbook, they may only find the Pa Pizzolo and Friston one.

So you may want to start just by saying sort of, well, I think today's episode is going to be slightly different.

Normally we sort of do some deep dives into people's papers, but I think today we're

We're going to start at least by talking about the kind of education side of active inference.

So nothing too technical necessarily, but just sort of thinking about how these ideas are permeating academic culture, intellectual culture, and then sort of broader culture.

But maybe where we could start, I sort of gave a very brief introduction there, is with you just kind of outlining how you found active inference, because people might not be aware of your work.


SPEAKER_00:
Sure.

Yeah.

So my path to Active Inference was really unusual.

I think what was really unique about it was I had been at a point in my career where I was starting to figure out where all my interests were, but I didn't know that Active Inference actually existed.

So that was the journey toward that was that moment of finding out that all of my research interests were colliding in one place.

And if we go back really far to just the beginning, when I was doing my undergraduate research, I was working in neurobiology.

I also worked in genetics.

I did a lot of work in biochemistry.

And a lot of my PhD work was actually molecular neuroscience, but I had wanted to move in a more technical direction.

So I did do bioinformatics work in my PhD, which introduced me more to some of the techniques and skills that I would need in computational neuroscience, which is when my first postdoc was in that area.

And it was at the end of my postdoc when I was getting close to leaving academia that I discovered active inference while just browsing papers, writing a grant.

The work I was doing was modeling human behavior with reinforcement learning.

So there was definitely an overlap in relationship to active inference.

but um my advisor has said yeah yeah we know about active inference we don't really use it in the lab but it's something we're interested in and so that kind of just led me in that direction and reading some of carl's original papers uh that just got me off on that path and i became really excited about learning more about it and that's kind of where i stumbled into the original problems of there's just no resources out there this was around 2018.


SPEAKER_01:
Yeah, 2018, there were definitely no resources out there.

We're doing a bit better now.

We have the Institute, we have the textbook by Pa Petsula and Friston, which people can buy, and then hopefully your one, which is coming out.

I sort of want to go back, it's interesting, you said sort of the skills that you learned in doing bioinformatics were, you know, really contributed to your capacity to sort of pick up active inference quite intuitively.

What skills are they?

And what sort of do you feel has benefited you most?


SPEAKER_00:
Yeah, so the skills that I'm kind of talking about here are very broad in general.

So the strategy that I've taken with the textbook is really to break it down into the language that everyone would understand if they're in any technical field like engineering or computer science or statistics.

And that language is probability theory.

If you strip away the neuroscience and a lot of the other themes and ideas in there, you're left with just a mechanism of Bayesian inference.

And a lot of the bioinformatics methodology that I was using in my PhD, when you look at it, they boil down to some probabilistic formation.

so i had some of that background i had enough of the background in the neurosciences and enough in the probabilistic reasoning side to be able to translate that into the active inference domain there's all these other layers on top of it of course but at the core that's kind of what i see is the baseline and that's common to all types of statistical modeling methods you can motivate it in a probabilistic formulation sure and when people hear the word probabilistic maybe they their mind does


SPEAKER_01:
jump to base, but maybe it doesn't.

So active inference obviously leans very heavily on Bayesian inference, variational Bayesian inference.

You know, to your eyes, would that be the only path that needed to pursue?

Could there be any other mathematical formulations that might flesh out the claims of active inference and self organization that isn't necessarily Bayesian?


SPEAKER_00:
I definitely think there could be multiple ways we can frame the active inference problem because we're ultimately computational neuroscientists or modelers.

They're trying to find the most convenient way to model a system, not just for the purpose of making predictions, but also something that's intuitive and easy to communicate and that makes sense to people when we try to formulate these more broader claims and bigger hypotheses about the brain.

i think it's just the convenience of the bayesian formulation that things neatly fit into this picture um but there are of course instances where sometimes you have to kind of force it into the picture sometimes that's you know if we try to modeling something um there may be other ways to do it and um the you know their probabilistic reasoning i think is just really

convenient way to being able to model dependencies among variables.

And when you pair that with this dynamic formulation, you talk about dynamical systems in a probabilistic setting, all the tools that you get out of it, I think are really important for being able to model the brain, but it's probably not the only way we could, we could conceive of.


SPEAKER_01:
Yeah.

I feel like, I mean, again, I'm coming into this very new because, you know, I discovered this during my master's, which was, you know, a year ago.

So I'm,

pre-doc, I'm very sort of babyish into this field.

But when you read the kind of history of, I guess, computational neuroscience or computational psychology, it feels like there was this transition between dynamical systems into active inference.

So we haven't really touched actually on dynamical systems theory on this podcast, in part because it's an active inference podcast, but in part because it's technical and I don't really have the sort of chops to get into that.

but I've been reading a little bit of Scott Kelso's work and getting into sort of meta stability.

Maybe you could very briefly, it doesn't need to be super deep, but talk about how we get, you know, what does that transition path look like from dynamical systems?

What are the, what are the key insights from dynamical systems?

Because Carl was working in, you know, in dynamical systems in the late nineties and the early two thousands.

And what does that add?

Or what does that, what are the convergence points between that and an active inference?


SPEAKER_00:
The biggest thing about it is that when you look at the history of Active Inference and you go back to the 90s, a lot of the work that was intersecting with Carl's work was the work that was going on at the Gatsby Computation Unit at UCL.

A lot of what you would call the

figures, the grand figures in unsupervised learning techniques and Bayesian networks were being developed.

Uh, people like Hinton, Harmony and, um,

David McKay, and there are many others as well, were developing those techniques in the 1990s.

And because of Friston working at UCL as well, there was a lot of overlap in knowledge.

Of course, he was attending a lot of the same meetings and conferences and things like that, and adapted that into SPM.

If I'm not mistaken, I think SPM2, which was the 2002 version, I believe, switched into the Bayesian world from the more frequentist version of SPM in earlier editions.

And so you have this unsupervised learning idea, which I think is really deeply powerful because a classic problem in dynamic systems is the idea of filtering.

You have some observed data that links to some unobserved process and you have to figure out what can we learn about this process that generated that data from just these noisy samples alone.

And there is a really great paper, Gaharmani, and I think I may be pronouncing this wrong, but Rowis, it's R-O-W-E-I-S, I believe.

I think it's 1999 paper.

They reformulate all the unsupervised learning problems and these ideas of the expectation maximization algorithm, variational Bayes.

They kind of put it all together and show you how you can model state space equations in a Bayesian setting.

And I think that's kind of the start of this move in that direction.

And there's a great thesis, Beale, 2003 PhD thesis, which then kind of puts all those ideas together.

And I think the basic way of looking at it is that you're doing Bayesian inference, but you can look at it as Bayesian inference over time.

And with all of the work in the 90s with dynamic models of the brain, like you mentioned, Kelso, and there are others as well, looking at the brain as a dynamic system, which changes in a way that settles towards some kind of equilibrium.

Those ideas, I think when you combine with the Bayesian inference and all of these unsupervised learning ideas coming together under dynamic systems umbrella, it's a natural fit to then say, okay, how can we model the brain with certain equations where there are these unknown states trying to be estimated and the equilibrium solution is the most likely estimate of those states given the data.


SPEAKER_01:
Cool.

Yeah.

I want to get into expectation maximization because I was just very excited when I

feel like I learned what it was, although I'm probably missing something, but I thought it was incredible.

But just to sort of finish up on the dynamical systems theory and the sort of chronology.

I guess there's two questions.

One is a very broad question, which is in some ways is active inference, a kind of apotheosis or the endpoint of dynamical systems very can it be couched within that overarching frame?

And then second point, which is a little bit more specific is, in terms of meta stability,

So this idea that the, let's say the neuronal dynamics are jumping from, as you say, like an attracting set or what people talk about wells, but it never really settles in one place.

That will sound very similar, you know, very familiar to people who are familiar with active inference.

Uh, so I guess is my question is what's interesting though, I guess, is the kind of, as you say, over timeness, the diachronicity of meta stability.

Where does it diverge from active inference?

What is its kind of convergence points?

And then again, how does that contribute to the difference between dynamical systems theory and active inference?


SPEAKER_00:
Yeah.

So that's, of course, is a huge, I think so much could be written and talked about on this.

So I'll kind of give the high level answer, my interpretation of that, which is that

Active inference, I think, is what's really interesting about it.

If you look at the equations of active inference, this paper is Hierarchical Models in the Brain, Carl's 2008, I want to say, paper.

It's a great paper because he spends a lot of time digging back through that old literature from the 1990s and then saying,

look, if we add all these other extra elements onto it, which is what some of those papers were doing for in your imaging setting, we get all of these different techniques that fall out of it.

So it's kind of like active, even if you just take out the active part, just the perception alone, perception of both hidden states, learning model parameters, and also learning the precision of sensory data or your priors.

all of that information in one algorithm.

If you start taking all these special cases, you can get all the neural network and supervised learning literature.

You can get all these unsupervised learning problems, filtering problems, whether it's dynamic or static.

So I see that as kind of like active inference is sort of a global overarching method that you could apply to any kind of stochastic time series type of analysis.

If you take out the action part, then you just have purely a perception part looking for patterns in data.

But if you add action in there, then you kind of recover this idea of control systems and the ability to control stochastic dynamical systems in a particular setting.

So that's the way I view it is kind of like you, you get all these other things that kind of fall out of it.

Um, so as far as I, I can see it as it's kind of the broadest generalization one could imagine of these kinds of Bayesian motivated statistical systems.

Sure.


SPEAKER_01:
I'm curious again, before we get into the sort of nitty gritty, I mean, I'm, I'm not, I'm not here to sell active inference, but I'm always curious, you know, about scope about what the next 10 years is going to the next 20 years.

you say in the 90s i i had chris friff on and he was sort of telling me just how exciting it was in the 90s uh because you had the gatsby you had all the psychological work the kind of pure psychological work that he was doing and then the kind of middle ground the car was occupying which was a sort of foot in both uh you know lands and as you say they kind of coalesced in the late 2000s and that in the 2010s i'm wondering whether you can see any other sort of not a rival theory

But something that could, if we're talking about dialectics and we end up with some synthesis, what are the kind of dialectical counterparts to active inference in the 2020s and the 2030s?

Is there anything that it's going to synthesize with to give us something even bigger and broader and more overarching?

Or is this, I don't know why I've gone very Hegelian, but is this the kind of end of history in some ways?


SPEAKER_00:
Yeah, that's a really good question.

And I think it's, you know,

It's really hard to know.

I think at the moment right now, if you think about just if you're talking in purely computational terms, everyone is talking about deep learning.

And that's what, of course, is an industry.

It's all over what research programs are focusing so much on deep learning systems.

And that's kind of the same thing is claims have been made about that as well.

The deep learning is the end game.

And so everything will fall.

It'll kind of become the main methodology.

Um,

I mean, there are not like a lot of other things that I think have the same broad scope trying to encompass so many different areas.

I think that's kind of the biggest advantage of Active Inference is the unification.

But there's something to be said about the fact that Active Inference uses one objective function to do everything.

How far can we get with that?

Is it better to have tailor-made systems where for different problems you have really specific types of objective functions?

To me, I would say the biggest contender or biggest rival to Active Inference isn't one specific field, but more an approach of just saying, how can we engineer problems in a really specific way?

with very heavily tailored objective functions that satisfy a particular problem domain or something like that.

Because I'm wondering if you'd run into a problem with this sort of generalist specialist issue where active inference is kind of great at everything, but maybe can be outperformed by a specialist system that's really tailored designed for that kind of data.


SPEAKER_01:
Right.

I think that's exactly right.

I mean, the university that I work at for my day job, which is not super active inference, it's a little active inference here, Royal Holloway.

There's a lot of RL modelers.

And when I speak to them, that is the kind of, I mean, they're very, obviously very respectful and very interested in active inference modeling.

But I think that is in many ways, the critique, which is that sure, the free energy principle might be applicable to everything.

But as you say, in the very sort of particular domain, you've got to do the model comparison.

And if your RL, like if, if your RL

model works better than so be it.

I think the active inference response to that would always be, well, you can just encompass RL with active inference.

So whatever claims you're making about RL will ultimately be taken up by active inference.

I guess the one difference, and you'll have to tell me about this because you're saying you're starting in RL and then found active inference, is that active inference is very strong on the as-if teleology

as if it's doing variational Bayesian inference.

Does RL have an equivalent?

So does RL say that individuals look as if they're maximizing a reward function?

Or is there a kind of more ontological claim that no, they actually are doing that and you can see that in neurons?

And then maybe in that case, just in terms of the philosophical strength of these positions, if RL was making more solid ontological claims, then maybe your RL researchers are going to get excited over that.


SPEAKER_00:
So there's two different angles that I would kind of take this question from.

And it's, you know, when we talk about active inference or reinforcement learning, are we talking about it from the perspective of what can we do and build with these systems?

Or are we talking about, you know, the as if nature as in like, we're trying to model specifically the brain and we want to go for biological plausibility because, you know, ultimately if we're just trying to build, you know, an AI, like, you know, when I say AI, I mean an intelligence that mimics animal and human behavior.

Maybe we don't need to model it perfectly after the brain.

We can get as if the as if part can get us, you know, it may not be exactly the same as biological intelligence.

Yeah.

But it's it's a version of intelligence that does the same things.

And as far as anyone looking from the outside, it effectively functions in the same way.

So that's sort of like, I think, with both reinforcement learning and active inference, you kind of see the split.

Reinforcement learning started as looking at models of human behavior and human and animal behavior, and then got taken off into the computer science realm, where then all these new techniques that have been developed after it may not necessarily resemble the brain anymore.

So when we talk about RL, it's hard to, you know, what does that mean?

Because it's so many different things now.

And the same thing is kind of happening with active inference, right?

Where we started in that realm and now we're in a place where we're applying it to robots and other things in ways that may not be necessarily biologically plausible.

So I guess the direct answer to your question is...

I think we need to define what RL and active inference mean specifically to be able to kind of characterize the as if-ness of it.

And I think the other issue is that it's really hard to test these kinds of things.

Like how do you test that neurons are representing things in a certain way?

You're always kind of locked into some kind of philosophical backbone whenever you do these experiments too.

And so to some degree, I feel like as if is...

maybe the best you can do.

And it's all about predictability.

Like, are we predicting the activity of neurons and what they're doing?

And that's kind of the best thing you can get to.

I think that the ontological part of it, that's hard to close that gap.

And I don't know if we'll get there at least in the next decade or so, maybe a lot longer to get to that point.


SPEAKER_01:
Yeah.

I think as a naive psychology slash philosophy student, I sort of came in and thought, well, we'll just get to the construct.

But you realize that in, well, in psychology, especially, you know, if I'm studying, if I've got an attention paradigm, I just take for granted that I know what attention is.

But like, frankly, there's, well, there's this good paper, actually, I think it was Hommel.

The title of it was no one knows what attention is.

And I kind of think you could have that for every construct.

I mean, is anyone really, and in some ways that's a shame, but as you say, it's predictability.

I think the difference where, well, my stance is that I'm not an out and out modeler.

So my interest is in saying, okay, well, let's take a cognitive function like attention.

How close can we get to, well, let's just say there's a general agreement we have over what attention is.

And fundamentally that's a phenomenological thing because we only really know what attention is by paying attention to our own attention in some strange way.

And I guess this brings me to ask, do you think in some ways, like, you know, you saying we've got active inference in robotics and you've got someone like me who's trying to do.

phenomenology and active inference, and you've got people applying it to atoms, and, you know, the very fundamental physics, and so on.

Do you think active inference is risking becoming too diffuse, that it's just this kind of one size fits all and let's just take any phenomena phenomenon and see what we can say about it in terms of variational Bayesian inference?

And if so,

Is that a kind of big problem?

What's the kind of narrowing down or the refinement of the theory and the community that might need to happen?


SPEAKER_00:
So this speaks to a problem that I've already run into when people are reading drafts of my chapters.

I'm already seeing that everyone kind of has their own vision and understanding of what active inference is.

And there's sort of this local knowledge problem, right?

Everyone comes in it from a different angle.

And the way that they've interacted with it means that they see active inference in one light or another.

And that is a big challenge because there is this prediction error.

People read the book, they're like, well, I never thought of active inference that way.

Here's what it means to me.

They're expecting it to be a certain way.

And I think this becomes especially true the further out you go from the actual mathematics.

Because the actual mathematical models, of course, there's always interpretation of what the math means, but they're a lot easier to say like, hey, I made this simulation, here's what it's doing.

Okay, it converged, it didn't converge.

We have these tools and metrics and things that we can say what's happening when we run a model.

But when you start getting to the outer edges of that, where there's a lot of the philosophy, there's active inference and ecosystems and weather and active inference, talking about plants and how plants...

can be predicting.

I mean, that's kind of the draw for Bayesian mechanics is that it's, well, it should apply to any dynamical systems that are trying to predict one another within certain set of constraints, which should cover a very broad spectrum of types of systems.

I definitely think that's both hurt and helped active inference.

I think there's a tendency

In any field, when the classic, when all you have is a hammer, everything looks like a nail.

If your proposal is vague enough, you can probably massage it into the active inference framework.

But then when you start going down further and further, the actual math of it.

Okay, so what is the math of ecosystems and communities and economics?

Like huge, all these agents that are all their own generative models, all interacting.

in a big group, what does that look like mathematically?

Does it really scale to that level?

Or is there some other generalization we're missing?

I think it's great to speculate.

Personally, as someone, I just love the idea of taking into these new areas.

I think that's what spurs creativity and allows people to then explore new ideas.

I think we should distinguish carefully between active inference itself and applications of active inference in a theoretical sense, philosophical applications of it.


SPEAKER_01:
Yeah, yeah.

I mean, this has been my personal experience, which actually is that my life becomes a lot easier when I'm writing papers to stick to the maths, as in not that I'm writing mathematical papers.

But that when I have the very, just very simply, if I have the variational free energy formula, the expected free energy formula in front of me, I actually can't go too wrong, in some ways.

Not that I run the derivations.

And obviously, there is an appeal to authority that those derivations hold.

But you know, I remember my introduction, you know, when I started thinking about active inference and reading the kind of seminal philosophy, philosophical pieces by Andy Clark and Jacob hobby, and these are, you know, classics and very important.

But you then go, Well, am I learning about predictive coding?

Okay, what the hell are all of these equations doing?

What's the variational mess?

And then that's really a mess for a lot of people.

And so I think for a lot, I don't know, my experience has been, when people ask me, Okay, what, what is active inference, I literally just go, Well,

if you have just the variational free energy formula, like, let's just start there.

Um, a question that comes to mind, actually, when it comes to sort of talking to people about active inference is that people will say, Okay, well, what every theories guys, miracles, every theories guys, axioms, what are the axioms of active inference?

What is it presuppositions?

And I say, Well, I kind of think there are two, but I think you'll be able to tell me there are more.

One, I say you have to kind of accept that there's thingness, that there are individuated things.

And I think that's an actually really interesting

development that could go which is like, what is an individuated thing?

Like, how do we even get there?

How do we start there?

But that's the kind of axiom and then the other one I think is time that you have that you have this kind of movement back to the attracting set over time.

Is there anything more that we should add to that kind of set of presuppositions active inference rest on?

I know there's gonna be stuff like ergodicity, which is a little bit more technical, but is there anything more that


SPEAKER_00:
i should be telling people about when they ask me a question like that so i think the the biggest motivating motivating factors here uh what you said i think is those are definitely um really important assumptions and i think there are others that can be paired with it or um maybe are broader categories and one is that when you talk about thingness and existence right like

we're defining what existence means like existence is a philosophical concept that has been explored for you know thousands of years so there's many different ways of looking at existence i think this is more of a pragmatically motivated way of saying that you know when we when something exists it exists meaning that we can measure its properties over an appreciable amount of time

So thingness, of course, there is that problem, like how do you define thingness?

How do you define what is how one or two things are identical to one another?

There are all these kind of philosophical questions and you get into like muriology about how parts and wholes come together and what is a part.

And there's all kinds of topics one can go into here that have rich literature, right?

That's one part of it is that we have to talk about what is it we're actually talking about.

I think that's one assumption that you mentioned.

And along with the time assumption, it's the idea that we're taking a physics perspective, which physics has a particular reductionist way of looking at the world.

And here we're saying that everything we're modeling can come down to stochastic differential equations.

um because everything in active inference ultimately is a form formulation of a stochastic differential equation or you have two different systems that are themselves modeled that way and you look at their interaction

So, I mean, there are some other things too, like, you know, you talk about like sparse coupling and Markov blankets.

There's other like little bits and pieces that are in here that are more the philosophical foundations.

But then when you go into the mathematics, there are a lot of little simplifying assumptions here and there that you could toss in.

One example is like the Laplace approximation, which is used in active inference.

It makes it simpler.

There is some biological reasons to believe that that assumption is true.

Same with the mean field approximation.

There are reasons to believe that the brain has these sort of independent separations, but it becomes especially true when you're getting to the actual mathematics.

There are all these little pieces, Gaussian assumptions, for example, that you start to add on there.


SPEAKER_01:
Yeah.

Excellent.

Okay.

So yeah, let's Okay, so yeah, so the one thing that I've been thinking about is what actions I tell people, then the other thing that people ask me is, well, what is it in the first place?

And your explanation, at least in the first chapter, I think of the textbook, which is the kind of this notion that the brain is sort of trapped in this cranium and has no access to the world and receives it through this, you know, the sensory data channels was incredibly, I mean, one hears about these in, you know, these examples in philosophy, but I think that was really on the money.

So maybe

you could do a better job than me at just saying, well, what is the fun?

Like, if we're just talking about human, well, cognitive creatures, creatures that are trying to exert some control, some active sensing movement, or even perception, you know, within its environment and trying to survive, let's say without invoking any strong teleology, what is the problem that they face?


SPEAKER_00:
Yeah, the biggest problem is absolutely that the physical systems that make up the world are hugely complex.

So by complex, I mean there are many, many variables that change.

Obviously, there's a lot of structure in the world that we can leverage.

moment to moment um things can move and be very volatile um and so that's i think this is getting a bit into other topics but that's the reason why we formulate you know things like culture and society and structure into our world we make the world more predictable by by having these elements in there that just make there's less overhead computational overhead and i think that's why

We look at the evolutionary history of humans, social groups, smaller social groups, and the ability to see others as part of your whole, like you're part of a community of people where you all collectively care about each other.

I think these are all survival techniques that come into play.

Um, and they're a necessity for making the world more certain because things can change at any moment.

You know, even just, you know, you drive to work one day, um, and the roads closed, right?

Like their decisions, even in an irregular world, there are always things that come up all the time in your personal lives.

You get sick, um, you know, things come up, meetings get canceled and you're, you have to go, you know, change your schedules, things like that.

Those are all part of our everyday lives.

And.

Ultimately, I think the idea is that when we think about active inference, it's the idea of if we're trying to deal as these biologically, from the biological perspective, we're looking at humans trying to make sense of their world and quickly identify what's going on and quickly make a decision.

the purpose of survival which is the whole point it's not to represent the world perfectly and all these other things we might want to do it's literally to be able to survive and pass on our genes to the next generation then what are the minimum requirements to be efficient to do that and also make those decisions in a really quick time knowing that the computational complexity of the world is enormous and we're dealing with like you know fractal levels of systems of

nested hierarchies of things that are changing dynamically, how can we leverage the data that we have to quickly make decisions?

And I think if we're talking in the most broad spectrum of thinking about active inference, it's looking at how the brain has solved this problem and attempting to write that down mathematically, or at least something that behaves like how we perceive the brain to be behaving.

Cool.


SPEAKER_01:
Yeah.

So the way that I kind of see it at its very core is that you have, as you say, a coupled system, the environment, the agent in the arena, the environment and the organism.

And the organism hasn't got direct access to the environment, but it has direct access to sensory observations.

And so it develops

a so called generative model, which is trying to, in some ways mimic or replicate the so called generative process.

generative process is the fact that there are so called hidden causes or latent causes out there in the world, which actually generate those sensory data.

And we're trying to have a model of the model of the world or be a model of the world, whatever you want, which maps onto that, albeit not perfectly.

And I think this is a really important point, because

Well, then we would just be the world and we don't want to be the world.

We want to be us.

And so we have our preferences and whatnot.

Now let's, I want to go to sort of expectation maximization, because I think it's a really interesting way into this problem.

So if we were, let's, let's not even think about genetic priors or phenotypic priors, because in some ways that hacks its way to an answer.

If I was John Locke and I thought the brain was really a tabula rasa, like I literally give it nothing but the capacity to take in sensory observations.

What's my way of building a generative model?

I have my sensory observations and that's about it.

And I want to build a model with hidden causes and parameters that relate the hidden causes to those sensory observations.

What is the kind of technical way of doing that?


SPEAKER_00:
So if we're talking in kind of like you said, like a tabula rasa kind of thing where it's just- Yeah, yeah, yeah.

I saw that with them.

You can think of maybe like a uniform prior was the way that invasion inference you would think about.

You don't necessarily have any particular belief about what the state of the world is like de novo, and you would accumulate over time.

So I think that this is, when we do active inference simulations,

We're essentially mimicking that because you might have some very basic simulations.

You can put a uniform distribution over your model, and you're kind of just saying all states are kind of flat.

I don't have any one belief that one state is more likely than any other in my environment, and I have to just learn what is out there.

with just this really simple model.

I think it's really hard to not have some assumptions because even in a model like that, you still have a Gaussian assumption about likelihood and prior.

And you also have some kind of generating equation where you have some mapping that's there about, okay, what is the relationship between environment states and the sensations that I as this agent experience?

Those are kind of built into the model.

So I think there's always I think it'd be very hard to start from nothing other than doing kind of like a sort of like an evolutionary kind of algorithm where you just have random connections forming over time.

And the ones that survive are the ones that make good connections.

And that's probably how you get off the ground in the first place is like in a limited environment.

There's just random selection going on and you just have you could have just a big set of random connections in a likelihood and even maybe just choose from a random set of family distributions from the exponential family and then just kind of see run it and see what you get out of that, which ones survive.

And the one that survives, you propagate it to the next generation and you continue to run more experiments in a dynamic environment and see which one comes out of that.

And what you'll probably be left with at a certain point is something like those kind of priors that we talked about, like where you were saying not to get too much into like the genetic priors and stuff like that.

They probably come from that kind of a process is that you have supervised learning.

That's kind of what evolution is.

It's like many, many years of this sort of training with the environment and the ones that succeed are the ones that pass to the next generation.


SPEAKER_01:
Yeah, so I think this is a really interesting question, actually, because I've never really thought about it.

But I don't want to get too philosophical, because it's it's an unanswerable question.

But as you say, if there was no, so we might have some prior, we can have like a Gaussian assumption.

But let's say we just actually had no prior expectations over latent causes.

It seems to me that the, this, the space

of potential latent causes is near infinite, at least in its gradations.

I could go from every single decimal point all the way down, which makes me think that at the very inception of life, let's say, how do we even get off the ground at all, given that every single potential hidden cause is as likely as another?

And once I have one or two, then sure, I can bootstrap off those and get to some descriptive viability in terms of my model.

But if you literally had something that had none, maybe some assumptions like a Gaussian assumption, but in terms of actually expected states, none.

How do you even get off the ground on randomness?

Because can't you just go in infinite directions?


SPEAKER_00:
So are you familiar with Terence Deakins?

Terence Deakins is a philosopher.

He has something called the Autogen in his 2011 book.


SPEAKER_01:
I'm faintly familiar with him, but not enough to really say much about him.


SPEAKER_00:
Okay, so I'm going to appeal to this because I think it's a really interesting way of kind of motivating this question.

And this is getting to more in the broader topics about like Autopoies' self-organizing and self-maintaining systems.

I would say that his example of the Autogen is an example of, and I'll explain this in a second, of how could something get off the ground from basically nothing?

And so when he proposes the autogen, the very brief idea is that he says, suppose you just have some molecules that are just in a liquid, some kind of solvent, and they're just these molecules floating around.

And when they're in close enough proximity to each other,

they're able to form a little catalytic reaction.

So imagine four molecules, one makes the other in a chain such that A makes B, B makes C, C makes D, but D makes A. So now you have this little self-sustaining propagating chain just because of pure chemical dynamics.

But if there's not enough raw materials to make those proteins, it'll eventually fall apart.

But suppose that an enclosed membrane depends on some of those proteins.

So then you get this positive feedback loop where the existence of those proteins encloses it in a small space.

And so it perpetuates that reaction.

And also the raw materials are outside of that membrane so that whenever the membrane deteriorates, because there's lack of components, the raw materials just float back in and they resustain the system again, which then rebuilds that cell wall.

Or you can think of it as just, it could be just molecular components that are just housing that reaction in a small space.

So in that small space, you effectively limited the state space based a very small number of things.

And the breakdown of that system is a negative, that negative feedback cycle ends up causing the system to then regrow again.

Right.

So you can motivate that probabilistically.

Like I'm, you can make a simulation where that, you know, those are all in probability distributions and

we're not assigning agency to that it's not like it knows what it's doing but one can see how you could scale that up into more and more complex systems and arguably that's exactly what's happening in the brain if you think about it in terms of self-organization you're just looking at another set of positive and negative feedback loops in a huge scale that we describe in the language of probability theory um so that's another way yeah i think


SPEAKER_01:
Well, it's very, obviously, it's very reminiscent of autopoiesis as first introduced by Varela and Maturana, and then sort of taken up in the 90s by people like Evan Thompson.

But I don't, I'm sort of stretching into my past reading here.

And I'm not 100% sure that something like a classic autopoetic system needs to break down its, you know, needs a breakdown of its membrane in order to build up its membrane.

I think you can invoke something like a partially permeable membrane where you just have the constant flooding of raw materials, which allow the metabolism to happen that still recreates that.

Because obviously, the partially permeable membrane itself is going to dissolve or break down because of entropy.

And so you allow it to build back up.

It's more or less the same, I guess, to be a philosopher, an annoying philosopher, you continue to ask, well, how do we get the kind of what would be operational organisational closure, whereby you have a

A instigating B, which instigates C, which instigates D, but then that also instigates A. How does that loop get off the ground in the first place?

Because I feel like autopoiesis is a very good description of things as they are.

And the classic example is the cell, the cell which creates itself from within.

But I guess the question is, how does that circularity, how does that circular causality, as they say, get going in the first place?

And I think that's probably, at this moment in time, an unanswerable question.


SPEAKER_00:
I definitely agree.

So first of all, just to say that, yeah, this is not the only kind of system we could imagine.

This is just one example that I think just captures the basic ideas.

And there may be many other ways of doing this.

This is getting a little bit out of my, this is definitely out of my field of expertise, but just from when I was in graduate school, there were labs, friends of mine that worked in labs in synthetic biology.

Um, and they were building that you were using DNA and other types of other molecules like proteins.

And they were doing experiments exactly like these where, um, autocatalytic reactions would just form, uh, given certain set of conditions being met.

Um, and so they would just, you know, put particular compounds in, cause you don't always know which ones will work, but you can toss them in and under specific conditions of heat and pressure and whatever, uh, salts and whatever compounds are needed, um, metals you end up getting, um,

autocatalytic loops forming, very simple ones that of course break down over time.

I don't think anything like the autogen as far as I know has been replicated in a lab.

But at least like the getting off the ground part, I think just from, you're probably familiar with Stuart Kaufman's term like order for free.

um you you do have uh like especially when your state space is constrained when there's only a certain set of possibilities you have less possibilities and then you kind of get this idea where one system starts forming you constrain the state space further and you get this kind of building domino reaction where eventually you end up getting something that seems convergent because of small little chains of random decisions that were kind of made along the way

So that's kind of one way I look at it.

I don't know too much about synthetic biology, so that's not my area, but yeah.


SPEAKER_01:
Well, I had Mike Levin on and I should have asked him.


SPEAKER_00:
My fault.


SPEAKER_01:
So let's then build in something... So evolution has bestowed on us these genetic priors, which

I don't think we need to necessarily get into what is the essence of them.

Because the nativist position or really is a kind of prior or prior knowledge is still quite fuzzy.

I feel like whether it's a mechanism or an actual piece of knowledge, and this goes all the way back to Chomsky.

But what's the kind of so I want to I want to get to expect expectation maximization.

Whether that's deriving a prior from

parameters and an observation, or whether that's deriving parameters from a prior and not an observation.

What's this kind of, it's just very, there's a very beautiful, again, it's kind of like a circular causality whereby you can derive one from, and it's a kind of work in progress, trial and error process, but you'll be far better explaining it to me.

So for the audience who don't know what expectation maximization is,

What is it and what does it give us?

How can we bootstrap our way to intelligence with it?


SPEAKER_00:
So expectation maximization is algorithm that was developed in a 1978 paper.

I think it's Dempster is the first author on that paper.

And what was essentially happened in the 90s was that algorithm was kind of put into the unsupervised learning foundations that were going on in that time, specifically what's called linear Gaussian systems.

That's where it was being understood in that particular context.

And you can see that variational inference is kind of like a more broader perspective on expectation maximization.

So from the perspective you're asking about, the EM, expectation maximization algorithm, is a way of essentially, there's kind of a bootstrap going on here.

So on the one hand, you have some kind of a model, and I'm speaking more from the perspective

predictive coding type of perspective on or unsupervised learning perspective, I should say, is a bit more accurate.

Yeah, there's many different ways you can interpret it.

So this is a particular way of looking at it in the systems where you have latent variables.

So you have something unknown and all you have is sensory data of some kind.

And so

the idea is well the problem is that you're trying to figure this out what you're trying to do is you're saying okay we know that there are some states out there in the world and they're generating data and there's some function that relates that has parameters that relates to that relationship so for example in a linear setting um the relationship between hidden states and sensory data will just form a line just a linear relationship maybe it's just uh you know just as an example it could just be 3x plus one

So you take any hidden state value, multiply by three, you add one, and that's what your data is.

Well, there are, there could be parameters that we might add in there.

So other little, other things that could change the nature of that relationship.

The problem is you don't know that.

So you don't know what those parameters actually are, but you also don't know the hidden state.

So you basically have two things that you're missing.

Parameters in this sense are not random variables.

They're treated in the EM algorithm.

They're just point estimates.

They're single values.

But then you have this probabilistic system on the other side here generating that information.

Your hidden states have noise on them.

So now you have the situation, all you have here is just some data.

So can you take that data and from that data, learn about the other parts of the system that you don't know?

And the reason this works is because the data is conditioned on those things.

It's affected by it.

It's not like in isolation.

In some sense, you can think of it like they lie behind the data itself and they're part of that process in some way.

The problem is it's all mixed together.

So this is sometimes motivated in like a signal,

a blind source separation problem or signal unmixing in

in signal processing literature.

And so the EM algorithm is a very clever way of getting off the ground, so to speak.

You described it kind of like the circular causality or something like that.

It's kind of a bootstrapping perspective where you say, well, I don't know what that hidden state is out there, but I can start from a random guess and get my expectation of what I think it is.

What's the most likely value?

It probably is given what I currently believe.

then from that hidden state you do maximum likelihood estimation to say well here's what from if this state is this way here's what i think the parameter is most likely to be um given that i've just estimated what the state is and then you you go back to the next round you say okay usually you can motivate this in terms of a like a gradient descent or there's many ways to look at this problem but basically uh usually use the log likelihood as your as your objective function

or negative log likelihood, and you say, okay, well, I'm gonna make another adjustment down the log likelihood for a new guess at what the hidden states are, what my expectation is.

Now that I have those new parameters I just estimated.

And then you go back another round again and you keep going in this circle until you hit convergence, which I will say that EM natively often has trouble converging.

So you may have to add other constraints to make that system work.

But ultimately you get off the ground with just sensory data alone.

You are able to estimate other things that you don't know.

And that's the essence of also variational Bayes is very similar to that as well.


SPEAKER_01:
That was what I thought the answer to my tabular as a question was, but we got to auto gens, which was, which was cool.

Um, yeah, there's, I guess, yeah, I guess, uh, you mentioned, I was going to ask, um, you mentioned maximum likelihood estimation.

So I think it's what people might not know what that means.

Um, what, what is Mac maximum likelihood estimation and how does it feed into this picture?


SPEAKER_00:
So maximum likelihood estimation is a really well-known statistical technique that has its origin in many different... It's used in a lot of frequentist statistics as well as Bayesian.

There are many ways to kind of formulate it, but the basic idea that the way that I like to think about it in terms of this sort of predictive coding or active inference based perspective,

just because that's kind of the common language we're using right now, is to say we have some unknown variable of some kind.

So you have something that's latent in this case.

Can we estimate the most likely value of that particular unknown variable instead of the whole distribution?

So you could get back the whole distribution and do Bayesian inference, or you could turn it more into this restricted problem where you get an estimator, which tells you the mode or mean of that distribution instead.

And you need some kind of a likelihood model to do that.

And you make some assumption about, okay, I have P of Y given X, that's your likelihood model.

You have some guess about how,

observations and states are related, and you invert that relationship by just finding the mode only.

So the most likely value instead of the whole distribution.

And it can be an easier, less computationally expensive problem that you can get a simple update equation for given what distribution you're using.

But the problem is that it may be a biased estimator.

It may not be exactly right given

the other assumptions that are in your model um so it's just a convenient way to estimate what parameters are that's kind of the most common way of using it is to say i have some you can use it in linear regression for example what are the most likely parameters that are give rise to this particular model that i have interesting you know it's funny there's a couple of things that come to mind one is that people will be you know people who did


SPEAKER_01:
what you would call high school maths, what I would call senior school maths, will be very familiar with these terms, right?

Mean, mode, whatever.

And yet, it sounds really complicated.

So I'm kind of bringing it back to this education point.

Is it really complicated?

I had a question ran down, which I haven't asked, but you know, people think that the maths is really complicated.

Is it really complicated?

Or does it seem or does it just seem really complicated?


SPEAKER_00:
I have to admit that the perspective that I'm going to give you now might have been different than my perspective five years ago.

So it's hard to tell sometimes because you're so deep in something, right?

Is it now appearing easy because you've learned all the material?

Because I remember how I felt when I first encountered all this stuff.

So I'll say that Bayesian mechanics, I think that side of things is a lot harder to explain and a lot more technical.

You can motivate everything in there as well, probably within probability theory, I would say.

And that's kind of the approach I'm going to take in the second book is going to be in that direction.

But if we're just restricting to active inference, I would personally say that it's not as complicated as it looks.

It all can be motivated in terms of simple distributions.

Gaussian's are easy to deal with, means and modes.

I think what gets complicated is when people start dealing with linear algebra where you just have multiple dimensions in your system, then the notation gets kind of very abstract and compact.

in the univariate case um everything has a really strict and very simple interpretation that you can you can think about in terms of like equations how you're deriving things um personally i think it's almost there are times when i'm writing and i look back at the equations and i'm like wow there's so few things in here really um when you think about what things you need to tune um and obviously when you're building like huge you know like

If you're looking for replicating all aspects of human and animal behavior, yes, it's going to get complicated.

But the core actual methodology I would argue is something you could teach to high schoolers.

You may not use all the technical terms, but you could get them to run a simulation and see what's going on and play around with variables.

And they'll have a pretty intuitive understanding of what's happening, for example.


SPEAKER_01:
Right.

And so how would you recommend, so people, you know, people might not have access to your textbook or they won't.

And you know, if I'm in the public and the pulpits and Friston textbook, although it's very good, does, I think for a lot of people feel like they've been thrown in the deep end at certain, you know, certain chapters I think are relatively taxing.

what do you recommend?

How do you know?

How do you recommend people go about this?

If they're not, you know, if they're like me, for example, I'm trying to pick it up and think I'm doing okay, and learning step by step?

What would you recommend people do?

Is that are there certain prerequisites?

You know, did you do a statistics course?

Should you do a linear algebra course?

You do a calculus course?

What are the key fundamentals that people need to do?

And do you have any recommendations?

I mean, this is also asking for myself about how to really

bootstrap your own learning experience?


SPEAKER_00:
Well, of course, that's my aim for the book is to provide this perspective.

But in terms of prerequisites, when I think about what active inference at its real core, having a solid understanding of introductory probability theory.

So we're talking about

Something you could learn, some people learn it in high school, this kind of introductory probability, just conditionals, joint distributions, and having a good intuitive sense of that.

But it would probably be, for most people, it'd be some undergraduate course.

The second would be calculus.

If you want to just learn the concepts, single variable calculus would be enough.

So if you know how to take a derivative and you know what an integral is, that's sufficient for most things.

And then the third, when you start getting into linear algebra, that's useful when you start talking about more useful systems that are multidimensional.

But if you're just staying in univariate, everything you can graph, you can visualize, all the equations are just numbers, there's no matrices or vectors.

That core idea, which you can get, you know, the scaling of the percentages of active inference from zero to 100% understanding is definitely not linear.

I think it gets, you know, the distance between 95 and 96 is so much bigger than like zero to 50, right?

So there's levels of understanding, but I think you could get with that alone, you could get 75% of the way there of understanding what's going on.

Now there isn't a lot of resources on that.

I think ultimately,

What you need to do is have some very basic Python programming background and being able to make a very, very simple simulation with the most minimum materials.


SPEAKER_01:
This is what I was going to ask.

A lot of the advice that I've got is, well, I'm going to be doing a PhD.

They just say, well, you'll do it during your PhD.

The more you do it.

the more the easier it gets.

And I get I actually think that's the case of a lot of things not own.

I mean, there are certain things where it's not the case, actually, which I think is quite interesting.

So for example, philosophy,

I would argue that in certain things, actually, sometimes your earlier intuitions are a little bit better because sometimes you end up down such a rabbit hole that you are a bit blinkered and you know, I'm writing stuff on like self models.

Well, there's just so many recursive elements to it.

And so you get a little bit stuck, but I found that for example, uh, coding.

So like, oh, I use art because in my day job, I do statistics.

you really just learn by doing it.

And then it becomes like a language in many ways.

So would that be, yeah, I want to hear your thoughts on that learning by doing.

It's not necessarily what people have been trained to do because we're used to going to school and reading textbooks, but it sounds like that's a really strong recommendation on your part.


SPEAKER_00:
Yes.

And actually for me, I love reading textbooks.

That for me is how I've always learned.

I've always been in situations where I didn't have a teacher and that's same with active inference.

I didn't do research in it first.

I taught myself active inference and also I taught it while I was writing it.

I mean, there, there isn't any resource out there.

There are a lot of little tricks that are in people's heads.

So even when I have all the model in front of me, just tuning it with like learning rates and stuff, there's all kinds of behavior.

I have no idea what's going on.

It just comes from trial and error.

So I think,

yeah i have the perspective of yes i love learning from textbooks but i learned the most teaching was one way teaching it to people but also through just messing around with the models and the crazy thing about the active inference models that's really really nice about them the simple ones is that um if you're just trying to just do a very basic simulation everything is analytic update rules so if you can just take a derivative and you can do a bit of addition and subtraction um

Knowing how all the pieces fit together is a challenge, but the actual code for a really simple active inference model is just a bit of for loops, addition and subtraction, and some like updating of some data structures, just so you can keep track of the history of how the system evolves over time.

The actual core of it is really simple to implement.

And that's my experience is the, what I try to do in the book, especially in the revisions I'm doing right now, I went through doing revisions at the moment,

having pseudo code that is nearly exact to what you would put into Python or R. So you start out, you initialize these things, then you make a loop and then you update these things in a loop.

And then you interact with the environment, get a new data point, or you act on it, depending on what your first perception or action, and you just continue that loop.

And all of the equations are right there.

So there may be a couple of different ways you could put it into code, but ultimately the sequence is really clear.

And I think that part of it, with a bit of guidance, someone who doesn't have a lot of experience in probability could get a feel for what's going on without even knowing all of the requisite background.

and all the details that go into there.

And that's the fun part.

I really want to, that's kind of something for the future is to have like sliders and stuff like on a website where you can just play around with sliders and learn by just interacting with a graph that has a pre-made model.

You can get so much intuition from just doing that alone.


SPEAKER_01:
Yeah, I think that would be great.

I also wonder whether a thing that's been really useful for me, and really this is just because of the generosity of people who

you know, are being far too kind.

Someone like Carl walking me through a soft max function.

Um, just cause he's nice and generous.

I wonder how, how much did you use?

No, you're saying you're teaching yourself active inference.

How much did you get out of corresponding with other people?

And is there anyone that sort of comes to mind that the sort of has, you know, formalized intuitions or has been a good sort of a discussion partner?


SPEAKER_00:
So possibly to my own detriment, I seem to have a habit of having to do everything myself, which on the one hand is really good because I get to learn things really deeply and I have to go into it and really understand it.

But on the other, it's been a big challenge.

And the honest answer to that question is there are very few people that I've actually interacted with personally.

Now that I've been working at Diversys now since December and

There, I definitely have now started to for a lot of the technical questions.

Magnus Quedal is one person who's been very, very helpful on the factor graph side.

Lance DaCosta as well.

Recently, we spoke and he was very helpful helping me out with generalized coordinates of motion, which are still very, very tricky for me to understand.

They're a pretty big challenge in active inference models.

um and i would say that for me it's been more brute force so there are two or three really good reviews um luckily at all 2017 it's very dense um also rafael um bocax i hope i'm not pronouncing that yeah correctly that's another uh review and there are a couple others too like the uh ryan smith's paper with with carl um and christopher white um that has the uh discrete time tutorial

Essentially, it's what I did was I took those papers, and then I scoured the internet for code examples and little snippets, and put all that together.

And that took me five years to get to that point.

But that's what I did is just that and then constantly reading papers and being confused.

And so maybe it could have been easier, but there was no Active Inference Institute.

I didn't know people.

That was the problem.

I was working in isolation.

Which is a challenge.


SPEAKER_01:
It always is.

Well, good.

I mean, it's amazing.

I mean, it's super cool.

Yeah, I point people in the direction of Smith and colleagues, but there are other tutorials because I've spoken to Ryan as well, and he's great.

think that's the easiest one although to be honest i always i always go into it and then like get halfway through and i'm like ah i've got some philosophy to do and then i said yes really long and i think um yeah i think you're right you need to have the coding up with you while you're doing it and and you're sort of figuring things out on the go and that's for my phd so i'm not going to do that yet it's an interesting point about coding because i feel like um my

intuition here is that it's all been happening on MATLAB.

This is a very nerdy technical point, but people might find it interesting because they might, as you say, go now inspired by this and try and build their own active inference simulations.

So, you know, Carl's famous MATLAB that you can download where, you know, you said that I think there is a Python, there is a Python, you know, whatever you would call it.

Packaging that you can package that you can.

Thank you.

Would you say one is superior than the other?

I mean, most psychologists are used to R. What's the kind of good starting point?

You mentioned Python.

Would you recommend Python for a starting point?


SPEAKER_00:
so i i so the book is i'm using python as the main language in the book um and uh i so i i've used r for many as the first real language i used in grad school was r for most of my phd work so i'm familiar with r as well matlab i only have a passing familiarity with i used a little bit in my one of my postdocs but um

Like anything, like any tool, it depends what you're trying to do with it.

Personally, I think that you can make some really quick data processing in R, especially with how it becomes tidyverse and stuff.

You can do some quick processing and stuff.

You can do a lot of quick stuff in R. That's my life.

I love tidyverse.

It's great.

And it made a little language.

Some people might even argue it's not even R anymore.

It's a new thing on top of it or a framework.

But there are ways that you can make a quick and dirty script in R that does active inference super easily.

You can do the same in MATLAB.

I find MATLAB to be really confusing, personally.

And Python, I'm only picking because it's kind of the de facto language for machine learning.

If you're trying to build big systems with it, you might get into trouble if you don't know what you're doing because things get very complicated.

But if you're just doing simple scripts, there isn't a lot of stuff that stands in the way.

So I like Python because it's kind of clean.

R is fairly clean.

I think people are unfair to R. I think it actually is a great language.

I would say that there's nothing wrong with using any of the three languages.

It's just whatever you're comfortable with.

And I would say that the minimum hello world of active inference would probably look pretty similar in all three languages.

It really just comes down to something like slicing notation and really basic things might differ.

But the actual code, there isn't anything super specialized if you're just doing a really simple discrete or continuous time active inference model.

I'm not sure that answered your question.


SPEAKER_01:
no no that answers my question that's my question and yeah i think people as you say hopefully over time we'll find more and more resources that kind of guide them through the process we'll get there um i want to get to verses at some point next point but

we had a good back and forth about priors and i i feel like it would be remiss of me not to mention priors um i also had this chat uh with ronnie sladky who had written this paper the amygdala complex um from an active inference perspective which i think i mean i read a lot of active inference papers as you can imagine not only for this but for my own work and

That was really, I think the first place where I really felt that the notion of priors had been nicely disambiguated because they are so, it's so loosely used.

Yes.

So people will talk about, and I think, I think until you're familiar with something like the POMDP scheme, it's, it's very confusing because at least in the POMDP schemes, for example, you have D matrix, you know, your initial state prior, you have your C matrix, the preference prize, and that begins to,

separate things in a way that's possible.

You have a really nice diagram, which I wish I could show, but I'm not going to share my screen so it won't work.

But, you know, we have different priors in active inference, although we lump them under this kind of general term prior.

So we talk about preference priors.

We talk about, and that kind of builds in stuff like phenotypic priors or genetic priors.

And then we also have state priors.

And I think the main difference that is, you know, intuitive to me and hopefully intuitive to other people is, you know, in all aspects of life, both are going to be at play because you need to work out where you are more or less or what's going on and

So basically what the state you're in.

But then you're also always going to have a preferred version of that.

I mean, it's a terrible way to put it, but you're going to have a preferred outcome.

You're going to have a preferred state that you want to be in.

And there might be a divergence there.

Is there any other kind of type of prior that you might want to build into that picture?

And how problematic just more generally is this idea that we just have priors, posterior?

People don't even talk about likelihood distributions, right?

They just talk about prediction error.

or sensory data, which is confusing once you start looking at like an A matrix.

So yeah, let's just kind of get into the nitty gritty.

How many different types of priors are there?

Will they end up all being the same?

What's the kind of situation there?


SPEAKER_00:
i think the distinction comes down to how we're talking about priors um like you said right they're used in so many different ways so i would say like in the most general non-technical sense when people use the word prior they typically mean assumptions that are held by the agent in some way some kind of assumption and assumption can even just mean like um you know my actions have consequences you know i take an action i get a sensory out you know it could even be something like um

just built-in assumptions about the structure of the world itself.

So if you're talking- Right.

So if you're talking very broadly, yeah, a likelihood you can call it a prior if you're saying this really, really vague sense of just anything that's your assumption about whatever's going on, about how you think the world operates or how your own actions operate, right?

Now that's, of course, I think where the confusion comes from is that prior just kind of means like presuppositions or assumptions that are sort of just there in the model.

When we think about prior in the Bayesian sense, then it has to take on a really specific meaning.

So I think that's where the modeling, when you talk about an actual POMDP or you're looking at a Bayesian network, that to me is where you can be a lot more particular about what kind of prior you're dealing with.

So just to be clear here, a prior in that sense means you have a variable whose value is uncertain.

So you can have a distribution over the values that that variable is most likely to take.

You don't know which one is the one that's gonna actually take, but you have learned from some interaction through data or possibly through evolution or through teaching.

People learn about how the world works by parents teach us certain priors about how using prior in a bit of kind of both senses of like,

you could motivate it in an evasion way but it'd be like prior is about here is how the world works and those could be part of your values right you can you can think of it in such so many broad ways so this thing is the problem is that we take these models that are really simple and we're trying to describe them attribute them to really hugely complicated things like culture and things like that and it's a lot harder than what does a prior innovation network look like when you're talking about culture

I mean, you could think of it in 10 different ways.

I mean, I'm sure you could make a model for what those priors are like and how they work into a model of human behavior, for example.

But there is no right way to do that.

I think it depends how you're kind of structuring the research problem.

So getting more to your question, you're asking like, what other kind of priors are there?

I mean, I think there are.

Like you said, there are priors about the agent's expectations about how the environment behaves, its preferred states about what states it would like to be in the environment, but also all the sort of priors that come down for how its body works, how movement works, the assumptions that you just have about the nature of the world and the body, the nature of your own model that come down to personality, preferences about how you view the world,

There's so many ways to look at it.

So maybe a better answer would, if we narrow the scope, unless that answered your question, I'm not sure.


SPEAKER_01:
No, no, no.

That's useful.

I mean, I think something you said that was really interesting there is that the prior is kind of the variable encoded in the prior is a random variable.

such that it's the kind of the function of other possible outcomes as a probability distribution.

But I guess people don't think about them like that, right?

Like intuitively, you don't think about that.

You go, well, I want to be, well, let's say like a body prior.

I want to be 37.8 degrees Celsius.

That's always a useful one.

But it's not like that, right?

It's over a probability distribution.

And maybe that's worth unpicking

Maybe not.

Maybe it actually just confounds or muddies the water somewhat.

But how should people sort of tackle that intuition?

No, I just have like so-called preferred states and they're one thing.

They don't have, there's no scope for them.

There's no distribution of them.

I just want to be here.

How do you get around that problem building in this notion of like a probability distribution or tending towards a probability distribution?


SPEAKER_00:
Yeah, so the answer to that question I think comes down to precision.

So each of the probability distributions that we can, you know, we talk about a prior over a variable, we're saying that that variable

I think because the non-intuitive part is that everything is couched in probabilistic language.

So one thing that I like to kind of say is like desire is expectation.

So metaphorically, when we think about desire, it's not what we want.

It's what you re-encode that is what do you expect?

And that's a really hard thing to do because that's not how we think about things.

We don't think about like, I want to eat means I expect to have food in my body.

That sounds weird, right?

Like it's just, no, it's naturally, I want to eat is just a feeling that I have that I know that if I have food, it'll be satisfied.

And that's kind of how we think about it.

So when I mean precision, I mean that when you, for example, you talked about body temperature.

So yes, I prefer my body temperature to be at this particular 98 degrees, let's say Fahrenheit.

But actually the way to say it is I expect it to be that way because that's what your brain is predicting.

And with a prior, really the truth is that there's actually a range, right?

Because more than likely you can push that to about 104 degrees with a fever.

You don't wanna do that very often, but you could.

And you could push it the other way.

So now you have a distribution that's centered on 98 degrees.

Now, it could be very precise.

When I said precision, it means how precise are we talking?

Is it just a really tall point right around 98 degrees?

And for other priors, it may be more spread out, like you can tolerate a range.

So the priors are really useful because then you can encode this notion of ranges of acceptable values with a preferred value in the middle.

And if it's something where you're so certain about it, like you have this preference, this is the exact state I want to be in, nothing else, then the probabilistic notion still works.

You have a random variable, you increase the precision till it hits a point mass.

And then it becomes a single value and you only entertain that value for that prior.


SPEAKER_01:
So I presume that mathematically that,


SPEAKER_00:
that that sort of uh that limit is basically infinite right in in the in the actual maths and you never actually reach it in terms of biotic limited creatures but that aside it's kind of that's kind of yeah i mean in mathematically like if you look at like factor graphs and stuff we use delta functions all the time in there and um but they're kind of yeah like from the mathematical perspective i think mathematicians would probably get upset with the way they're used it's more of a practical way of just saying like

Hey, we're turning a probability distribution into a single value.

So it just becomes a number, not a scalar rather than yeah.


SPEAKER_01:
This is the so-called Dirac delta function that again, I got very excited over.

I get very excited over very odd things, but I guess the philosophers would also get annoyed about the infinity thing as well.

So it wouldn't just be the mathematicians.

Yeah.

This is all really useful for me and really fascinating.

So I guess.

Well, I guess people might think, okay, so I have these probability distributions that kind of makes sense for me in terms of like my C matrix, my preference priors.

But then how am I going to have probability distributions over something like a state prior?

What I guess the question here is what triggers that probability distribution?

Because it sounds like, you know,

sure, we have observations, and we have a likelihood distribution.

And that allows us to say, Okay, I can do state transitions, because I have this constant interaction of observations and states.

But just for my initial state, what's the kind is, I mean, I guess the question here is, is there such a thing as an initial state?

Because I think it's always a mistake to take the generative model and slice it up in terms of really, you know, discrete time, obviously, you have to for modeling purposes.

But

Yeah, I mean, I'm going to go around in circles here, but my question is, yeah, does it make sense to really think of a completely isolated initial state, a D matrix, really?

And if so, how do you get, what's the context that drives that initial probability?

Because it sounds like without any context, every state is equally as likely as each other.


SPEAKER_00:
So if I understand your question correctly, like, you know, you take the D matrix at the beginning of the discrete time POMDP active inference problem, right?

You're saying that, say that there's only four possible states in the environment that it could be.

And here is my belief about the probability or expectation about which one is most likely, right?

So you're asking like, but, you know, any agent that exists, I mean, you know, you can say like,

I don't know if it's birth or when you're a clump of cells at a certain point, whatever, you know, what is the starting point?

Like, is that, was that agent really starting with just like, okay, here's my initial state.

Now I'm in my simulation begins.

Right.

That's how we do it.

We model.

Right.

But is that what you're kind of asking?

Like, what about there it's unbounded, right?

Cause there's infinite number of states.

Effective.

Yes.


SPEAKER_01:
I guess the issue that I think people have, and maybe I had until I sort of saw this written out is that when you have like a tensor or a matrix, there's not one thing in that.

right and i think that's so you have the big letter this is something actually recently i've sort of properly got my head around you have the big letter which is the tensor yeah and then you have the small letter um which is the is that the beliefs encoded in the tensor am i getting this right or is it

Those are the parameters of the tensor.


SPEAKER_00:
In this case, it's the same thing because the tensors themselves are the parameters.

That's how we do them.

But the question is, where do those values come from?

So lowercase d would be, you have these states with those probabilities.

How do you learn what those probabilities are in the first place?

Assuming there's only four, for example, right?

We're already restricting it.

Maybe you can call that a prior, right?

The agent believes there's only four states in the environment.

But then the question is, how do we learn what those particular probabilities are?

That's what the lowercase d comes from.


SPEAKER_01:
I'm just sort of curious.

I guess I'm just curious about what does that even look like?

When I say I have four potential states that might be encoded in my d tensor, I feel like when a

when someone like me who's interested in computational phenomenology, but learning it and it's coming from the philosophy and the phenomenology, one understands what a D tensor is and the B tensor and an A tensor, because

It's like, oh, it's that.

But then the fact, what does it mean for something to be built into that tensor such that let's say you have multiple states in a D tensor with multiple different distributions?

What does that actually look like?

Is that purely a mathematical modeling thing, or is there a way that we can intuitively think about that?


SPEAKER_00:
So if I think I'm understanding your question correctly, I think

the issue you're describing here is something that I think is what I've wondered, like, this is a model ultimately, right?

Like it's a model of a very simple scenarios.

Cause that's originally where Active Inference was, it came from was, can we model simple human and animal behaviors in a very restricted environment?

So if you restrict your environment to a T maze where, you know, a mouse is at one end of a T maze and it can only move in four places, it can only exist in those states.

Okay, then you can, you know, in that it's an abstraction of a real physical, real world scenario.

In reality, you know, these are, it's actually all continuous when you get down to it, right?

Like, I think the discrete state space formulation is kind of an abstraction to think about high level categories, which themselves are encoded in neural dynamics.

The question is like, let's say you really made a real agent that you were gonna put into the world, say it's a robot and it's gonna interact with anything, right?

Well, what does its D matrix look like?

Is it an infinite vector, right?

I think that's kind of the open problem is like, how do you actually do that for a real agent?

I mean, there are states of the world that I don't even know exist, right?

So if I have a D vector, right, like it's not there yet.

And so what I think would probably need to happen is you have a D vector that has the ability to grow as one way you could treat the problem.

So if it encounters a new kind of state, that's just something it has never explored before, you add in a new slot to the D vector and have to recalculate, recalibrate all your probabilities relative to that.

Something like that, I think that's getting to what you're asking.


SPEAKER_01:
Yeah.

I mean, my intuition here is that it's really important to always also curtail the state space or the belief space in these tenses.

So I've just written this paper.

I've just written the preprint.

It's a more sort of cognitive science philosophy paper, but it's basically about how underactive inference for my eyes, something like attention is always constrained by preferences.

So it doesn't really make sense to talk about like, I'm trying to collapse this classic endogenous exogenous distinction that exists in the intentional literature, because under active inference, I'm saying, well, think about what you're paying attention to.

It's always that which is most salient to you, and that's always governed by your prior preferences.

But then what kind of came to mind as well?

It's not every single prior preference is exerting pressure on your attentional schema at any given time.

I am not simultaneously paying attention to how hungry I am, how tired I am, and so on.

There's got to be some context driving mechanism that means that when I get hungry, I start paying attention to food.

When I recognize my child is in the middle of the road, I start paying more attention to the cars and the prospect of them getting hit and so on and so forth.

which again implies some kind of paradoxical circular causality, because I need to be paying attention to that in the first place to drive my constraints, constraints, state space, such that those preferences continue to drive the attentional specification.

And that's kind of similar to what I'm talking about with this D matrix, right?

I can't entertain every single possible probable state computationally intractable, but I also, but for me to know which states are going to at least be feasible,

I need to pay, I need to be observing the world, right?

Like, I don't know if I'm in Iceland or if I'm in the Sahara and so on.

So this is kind of what I meant by, is there such a thing as the detensor because an unbounded detensor so-called prior to observation to my more philosophically leaning mind sounds like it just could consider absolutely everything.

which it clearly doesn't.

So it makes me think, oh, is the detensor just the kind of B tensor in disguise?

Sorry, I rambled.

I rambled.


SPEAKER_00:
No, but I mean, to answer your question, that is exactly what the detensor is.

Because if you slice a B tensor with an action, you get a vector out of it.

because that's the whole idea is that it's like assuming whatever ended up happening, this is now the state that you're in.

You can kind of think of it that way because whenever you slice the B matrix, you get back that slice only and whatever slice happened to be

your starting slice, there's no action that preceded it because you're starting at the beginning of the simulation.

But that's why it's a vector is because it's actually what ends up happening when you take an action is you do slice the B tensor into a vector and say, OK, I'm transitioning.

And now that becomes my prior that we use for the next round of inference.

um so uh that's that's just just wanted to comment on that uh your question that you're asking is a really deep question that i don't i don't know if there's a good answer to that yet like what does it look like when you you know is there are detensors floating around and being computed in the brain somewhere like what do they look like is that what you're kind of asking like they're just this unbounded entities like what i'm thinking is that there's got to be some top-down constraints which is context


SPEAKER_01:
related.

Because, you know, we can't just a very basic level, we can't consider, let's say, in terms of action, we can't consider every possible action, right?

Because there's infinite things in principle, every infinite gradations of things, at least that I can do in any given context.

And so this is, I'm not the first to think this, right?

There's Carl's sophisticated active inference paper with Lance and others where they talk about

probably do need a higher, higher order action policy, which constrains the possible actions that you might take in a given context.

And when I was thinking about this attention paper, I was thinking, well, so people talk about, for example, hunger has really high precision.

I'm like, okay.

And then people say, well, precision is attention.

And I'm like, well, no, because I'm not always thinking about how hungry I am.

And so what I do is I try to disambiguate precision and precision waiting and attention and all of these different categories.

But what I'm saying here is there has to be a, there has to be a thing that says, well, when I'm hungry, because I need, I need to eat when I'm starving over everything.

Right.

I need to stop by my essay.

I need to stop speaking to people.

I need to eat.

It needs to be some prioritization mechanism that says when your glucose drops below this point, everything gets focused on food.

Now on one hand, yeah, that makes sense because presumably my set point, my homeostatic set point is very deep in my generative model.

So it trickles down all of the predictions at the same time.

It's still not clear unless you have that higher order action policy, why you would even prioritize hunger over playing a video game.

So I think I wonder where the acts of inference needs to go in this direction and say, well,

what is constraining these spaces in the first place?

It's all well and good having them, but that's really my fundamental question here is, do we need to invoke something like high order action policy just to constrain the possible beliefs that we can have in a generative model?

I guess the answer is yes.


SPEAKER_00:
Yes, yeah, definitely.

And I think this area is something that I've actually been reading just the last couple of days for one of the chapters I'm writing.

And I still don't have a great grasp of the literature, but here's just kind of a sketch of my understanding to answer or just at least address my speculation on what you're asking.

So there is the idea of attention in the active inference formulation.

attention is also active right like you you can choose what you're attending to at one time so attention is usually considered like the newsworthiness of something so how salient or how interesting or informative is something at one point in time or another uh which in the active inference model like you say there's a precision waiting on the signals the prediction error signals um for

either sensations or state transitions and things like that um so but it's context dependent there's a paper recently about social cues in context so just as an example of a word i'll say that paper later i think you might find it interesting but it's like even the example of a word right a word in isolation has no particular it may have many many different meanings so now there's uncertainty you have a signal someone says a word right

That's not enough because there's uncertainty in the actual content of that signal.

It could mean so many different things, but the context might narrow that scope and tell you what it actually has, what the actual meaning of that word is in the sentence that you're using it.

Right.

So the context would be the sentence in this case.


SPEAKER_01:
Um, yeah, I was very inspired by that, that Mitz's work.

Um,

he's got all of 2016, 2017, especially 2019 paper with Carl on selective attention, which I think if you haven't read it, you would really, really like.

It shows that like context.

So for example, if I'm doing a Yarbus task, which is a classic attentional paradigm where someone says, you know, show a picture of a family, let's say, and they say, well, how much do you think this family is worth?

And then all they do is look at

the furniture and their clothes.

And then they say, I don't know, do this.

I don't know actually what the second condition is.

This might be something like, well, do you think this family get along and all they look at the eyes and they look at how they're looking at one another.

And what that shows is like what's salient.

I actually prefer the term relevant, but that's for technical attentional reasons, but it doesn't really matter.

What's relevant is always relative to context.

Right.

It's always contingent on the goals of the agent at the time.

But then you get I think the issue that I kind of skirted and may need to think about is you potentially end up in an infinite regress because you say, okay, I have a higher order context or a higher order action policy that is saying, for example, when I'm hungry, prioritize hunger over the fact that I want to finish this video game level.


SPEAKER_00:
Okay.


SPEAKER_01:
then what then you but then someone might come along and say well why is that higher order action policy relevant right maybe that's just a hidden state for another higher order action policy and so you may end up with this infinite regress of saying well where does the relevance kind of stop and maybe you just get something like embedded priors genetically embedded priors that get you out of that hole um because i can't really see any other way out of it but all of that to say that

I think, I think context is really important in terms of guiding the precision waiting of beliefs.

I don't like the term.

I don't like the, although I don't like the illusion of precision and precision waiting because precision for me is very mathematical term and precision waiting is a much richer term, but you're absolutely right.

Attention is not only.

pulled towards something it's also you know oftentimes seemingly purposefully directed the point i was trying to make in this paper is that it's fundamentally goal directed even at the very lowest level right like even in terms of like color discrimination you could you know you can describe it as if your visual neurons have these prior preferences and so on anyway


SPEAKER_00:
So actually, it'd be worth saying about this, like, you know, I'm trying to I'm trying to work through your question, like probabilistically, so we don't get into the like, you know, philosophically, it's very hard to talk about these things, because like, but I'm trying to look at like actual model, right.

So let's just say that, you know, hunger is a random variable, right, like your state of being hungry, which is dependent on blood glucose levels.

Right.

So your body is actively sensing your bloodstream.

Hormones are sent out.

Those levels come back as sensory information.

And that's a sensory signal you get from your body that your brain will then say, okay, blood glucose is a thing I'm measuring here.

It's gone down to a certain range.

And that there's a prior, literally in the Bayesian sense, there's a prior on blood glucose that has distribution of its acceptable range.

And so now that the state has changed, conditioning on that distribution makes hunger now more likely or more expected in the model because blood glucose has now fallen.

So I'm thinking of it through conditioning, right?

These effects are all conditioned variables where the state of hunger becomes more likely because it's conditioned on some other thing that's being measured.

So it's kind of,

I don't know if it's like an operating system, right?

Where there's like tasks and you're sending something up to the top is like process this first, but it feels like this queuing kind of happens through distributions is kind of what I'm saying.


SPEAKER_01:
But that's the question, right?

Is now that the body recognizes it's hungry, why does it say, okay, I'm going to stop petting my dog or playing on my phone and I'm going to go get food unless it's starving.

And that's all it can think about.

It's all it's paying attention to.

what is governing that?

Okay.

Well, you go, there's a higher order policy because it has to be, which says, okay, given this hidden state now, so the hidden state of hunger has become an observation for a higher order action policy, which says, given this observation, go find food.

My point with the infinite regresses

is there an action policy, I've got to try and make sure I guess, right, I didn't write this in the paper.

So this is actually very influenced by john Verveke, his work on relevance realisation.

If anyone's curious about this argument, he puts it in his relevance realisation paper.

So I'm butchering it.

Sorry, john, john might be watching.

Sorry, john.

But the idea would be, why?

What, you know, is there an action policy guiding that action policy?

Like, where does the limit, where does it stop?

Because you might say, well, why would I, when I'm hungry, go and find food?

And that's because you have a higher order action policy, which says that given your observation that there is this action policy and so on and so forth.

So I think there's an important question here, which might be remaining in the activist literature, which is where does the cap of context stop?

Like, is there a point at which it just stops?

And where does that come from?

And again, I think the answer probably has to be this evolutionary, but that's boring.


SPEAKER_00:
I don't know if you would call it a cop out, but that's kind of what you end up having to do at some point, right?

Like when you have this regress, if you were saying like the boring answer is, well, we have these

DNA is compressed source code of all these other things that we've experienced and it overrides other things, right?

I guess, yeah, I don't know if I have a good answer to your question other than just saying like, if you're talking in a purely Bayesian sense, like if you just had Bayes' theorem and you increase the, because the precision, right?

The precision is also learned from the data.

That's also part of, right?

Like you learn how much to modulate that precision

If you have even a three-way network where you have your data, you have hunger, and you have play games, and those two things are being integrated together to have this distribution over like, what do I do?

Do I eat or do I play games?

What you're inferring from the data.

Well, if you're learning from the data, the precision is much higher on the distribution for hunger.

then it's waiting as much more higher invasion inference than the gaming playing games.

And so your posterior will be weighted much more highly toward the state will be more likely is getting food.

So I don't know if you even need to evoke that infinite regress other than I'm not sure I'm thinking about this through this.

Maybe we're not understanding.

I'm not understanding your question, but no, I think you are.


SPEAKER_01:
When I spoke to John about it, I have said, I think you get out of the infinite regress through evolution.

or you get out of it through, well, yeah, you could say learning, right?

Whether it's kind of this supervised learning over epochs or whether it's in real time.

I think the problem with in real time learning is like,

why if it's not evolutionary and it is in real time, why would the system prioritize the precision weighting of those signals over others?

And then maybe you end up in the infinite regress.


SPEAKER_00:
I do think at a certain point you have to bottom out somewhere.

You're right.

Because I mean, why is it in the first place that food is?

Where did that distribution come from in the first place?


SPEAKER_01:
Exactly.

That's the point.

Yeah, and I'm just really curious about what these... What's the drive?

Because I invoke this kind of notion, again, of a circular causality between attention and preferences, whereby for me to prioritize a preference in my precision weighting, for me to go, okay, the prior for hunger is now more heavy in precision weighting than the prior to play video games.

Well, I need to pay attention.

As I said before, I need to pay attention to the observations that I'm making so as to drive that

which in turn drives my attention towards that.

And you get these kinds of positive feedback loops, which I think actually also end up explaining something like addiction.

And maybe I know John would argue that the opposite of that would be something like agapic love where you're one with the universe.

But anyway, we don't have to go there.

If I can, I will find what John says about relevance realization, because I think

It's his response in some ways to the frame problem.

So the frame problem being, well, how do you know, like if you're an artificial intelligent being, how do you know which information to zero in on?

Okay, because you have a, well, how do you end up with a policy to zero in on that information where you have a higher order policy?

How do you have that?

And so that's the infinite regress is the infinite regress of the frame problem.

But I guess when you bottom up, we're just going around, but when you bottom out with evolution, you seemingly resolve that problem.


SPEAKER_00:
I think so.

And that's just as an example.

And I think I'm not sure this is true, but I believe it's true that I read somewhere that like infants will, you know, talking like very small infants, few months old, they will.

I don't know what the exact age is, so I can't give a great.

answer on this, but they will prefer, they will be selective about the kind of liquids they will ingest.

So like sweet things is one example, but like bitter or salty, those could be poisons.

And there are many other things too, like suckling behavior, other instinctual things that infants within the first few weeks have the awareness of or ability to do these sorts of very, very basic behaviors.

Where does that come from?

Because if you're...

there aren't any where are those priors coming from i think you have to bottom out on some level of evolution in some way eventually um but along that way in the development right i think there's so many other things that happen in there and a lot of it is also cultural things you know you were taught

Even in the early reinforcement learning or active inference paper, 2009 with Carl's paper with a few other authors on there, they actually use supervised learning to teach the model it's like priors first to solve the mountain car problem.

And they argue like, well, you learn from other people like your caregivers, people around you teach you some things.

So I think that's the other option.

It's either evolutionary or it's learned very, very early on as something that you learn during development that becomes encoded as this highly precise thing.

But that's just speculation.

I'm not quite sure.

No, it's fascinating.


SPEAKER_01:
I mean, it goes back.

It's really amazing.

I mean, I think active inference in...

giving us these kinds of phenotypic priors answers so many questions that were longstanding in cognitive science.

I mean, like this argument of kind of foundational concepts goes back to someone like Jerry Fodor and Fodor saying, well, we kind of need these innate concepts, you know, because how would I ever be able, you know, it's almost platonic.

It's actually, it is a Plato argument.

You know, how could I ever bracket something into the concept of red rather than blue without already having the concept of red?

And Plato argues that you have the form of red.

And Karl will say, well, no, you have a sort of proto-foundational prior that allows you to do this assortment.

So thank God for Mr. Friston.

Um, let's, um, let's finish up just by talking about the versus.

So people are probably aware of what versus is, but if they're not, um, maybe you can give just like sort of a brief overview of what versus is, what the ambition is.

Um, and also what you do that.

Cause I'm sure people are very curious.


SPEAKER_00:
Sure.

So I've only started at Versus in December.

So Versus in any ways is still very new to me.

So this is just to give an overview for Versus.

Versus in, I think it's almost two years now, Carl and many of the researchers that were working with him joined the Versus R&D group.

So versus AI company, they're working on active inference models and applying them and scaling them and learning how to use active inference models in more realistic settings.

So kind of the scope of when we talk about versus we look at how we have natural intelligence on one hand and artificial intelligence.

So that's kind of the way we're reframing it.

We're thinking about how do biological systems intelligently do things, biologically inspired intelligence.

And how can we engineer those things using active inference?

So in one sense, like the R&D group is continuing the work that Carl was doing with many of his colleagues.

But now we're trying to apply them in specific settings.

So I can't talk a lot about the actual research itself.

Yeah, yeah, yeah.

Of course.

I'm not, you know, but so I'm giving more of a very, very high level explanation.

But the major contribution is, you know, we have definitely open source papers that have been published.

A couple like on federated active inference is one example.

The inductive inference paper.

There's a couple others on structure learning that came out recently.

So there is some insight into some of the work that's being done at Versys there through those papers.

But more generally, Versys is focused on the vision of the spatial web.

um which uh there's a book uh the ceo of of versus uh gabriel ney as well as uh dan mapes they wrote co-wrote a book together the spatial web which is uh looking at the future of what the internet could be so looking at the isolated technologies that are that make up web 3 when you look at things like blockchain and ar vr technologies internet of things and smart cities and things like that digital twins

they all have their own little research area.

They're all kind of being different companies in isolation are studying these things.

So the idea is to put them together into one umbrella where we have the new version of the internet where you don't really have web pages anymore, but you have people, places, and things.

So there's so much to unpack in that umbrella that we can talk about that the book goes into.

And I'm still myself learning and getting my hands on all of the details of that.

But it is the new web standards that would come out of this.

We registered with IEEE, so you can attend, look at the documents and things like that that are part of the development of the spatial web.

And Active Inference is playing the core AI component of what the spatial web will entail.

So it's kind of functioning as the...


SPEAKER_01:
broader scale of it how can we take it out of the neurosciences and scale it up into something that can be used in these kinds of technologies really cool really really cool and a lovely explanation um i was curious just when i heard you you know you one does hear the versus people talk about mimicking or replicating biotic intelligence in artificial uh entities

how much do you guys again this is a very high level question because i don't want maxwell to shout at me but how much do you guys run into like the as if problem that we've already discussed namely maybe in terms of like the ground truth of active inference is that things look as if they are conducting variational bayesian inference over their external states but how they're doing that whether they're actually doing that right these are still kind of live debates

wondering whether when you translate that over to an artificial intelligence agent it leaves you a little bit well up in the air about okay exactly we we can say okay they have to be abiding by the free energy principle they have to look as if they're minimizing variational free energy but is there a problem where you go well how on earth are we how on what's the process theory how are they actually going to be looking as if they're doing that so what's the kind of yeah i'm curious about how you how you skirt that problem


SPEAKER_00:
So I think these are the kinds of things that will probably look different.

These discussions look different to different people, depending on your philosophical presuppositions.

So different researchers at Versus and outside of Versus all have different takes on this.

So my personal take is that the as if problem is not as important, I don't think, in an engineering setting.

Because ultimately, it's sort of the scientific process about

okay, I'm making this model that I want to do something in this world and it needs to predict in a certain way, it needs to do these certain things.

Is it doing that?

If it's not, then what is it missing that we need to add to make it make it?

And at the end of the day, I mean, I don't think it really matters if something is totally mimicking human-like intelligence in a way that for all intents and purposes, maybe like kind of the grand Turing test of, you know, in general of any kind of skill or a thing it can reason about,

If it's doing that effectively, then I feel like the as if question doesn't really matter for this kind of a circumstance.


SPEAKER_01:
Yeah.

Until you start thinking about consciousness and the ethics of conscious AI and whether something like embodied cognition and active inference yields consciousness.

Again, these are deep philosophical questions.


SPEAKER_00:
Yeah.

I definitely agree.


SPEAKER_01:
I just wonder whether when I think about active inference and the more time I spend with it, the less...

concrete, in a Socratic way, about what I can say exactly it's claiming.

Because sure, you have the mathematical derivations, but

know going all the way back to just basic analytical philosophy all the maths just turns in on itself right it's an analytic it's an analytic truth it's kind of tautological it doesn't necessarily tell you anything about the world apart from the fact that things have to look as if they're abiding by this principle like things look like they're abiding by hamiltonian's law at least the principle of least action so i'm just wondering whether in an engineering sense right so in a very pragmatic sense what and again

Not revealing any secrets because NDAs, Maxwell, scary.

Joking.

How do you get from, okay, you have the VFE equation or the EFE equation, and then you have these kinds of offshoots like predictive coding or PonDP schema, variational message passing and so on.

How do you kind of choose, in a way, which one's the best option?

And also, then we have to build in this issue of okay, we've got discrete state space, we've got continuous state space, in reality, everything's hybrid.

So I'm just wondering whether how you can get from like a super fundamental mathematical axiom, like the variational free energy formula to something that can actually be programmed and is applicable.

And I guess this comes to a question of how does someone decide what kind of simulation to run in the first place?


SPEAKER_00:
Yeah, I mean, this is a big problem that will come from scaling any technology.

But Active Inference, if you even just look at the robotics papers that have come out the last five years or so, there's been a lot more exploration in that area.

A lot of them have to make practical considerations and tricks to make it work in the robotics setting.

So whether that's, you know, further approximations of certain signals that are just taken for granted and the equations, you know, it's like, well, there's latency issues.

There's all these other things that, you know, you have to deal with.

And these robotics papers often start with the core of active inference and then throw a bunch of other things on top of it.

So the most common trick is the like amortized inference, which would be like, you know,

You, you know, it's taken for granted if you have a small, tiny little, you know, toy simulation, my generative model is this linear equation or this quadratic or whatever it is.

How do you actually, you know, you already have learned those variables.

Okay, well, how do you do that in a setting where it's this huge multidimensional thing?

Well, you can use deep learning, use some universal function approximator that learns the parameters of all these distributions.

So the broad answer to your question is like, you come up, you hit a wall with a lot of these sorts of things when you look at like the robotics literature and you're trying to engineer these sorts of systems where you have to start including tricks that were not necessarily in the original formulations.

And you think, okay, well, I have to prune my policy tree.

There's so many different options here.

How do I figure out what's the best action to take?

Because I have to do it engineering setting.

And so there's a paper recently on Reactive

uh decision trees uh reactive uh behavioral trees in uh in robotics that came out a couple years ago another perspective looking at branching time active inference for example so these are all like little modifications on the original methodology the core is still the same but you can add these extra layers on there so you make better predictions and you can actually function and build systems in the real world

And I think it's sort of like a guess and check, like you build a simulation or you build a robot and you realize, okay, there's a major limitation here.

What do we need to add onto the system mathematically for it to actually deal with that problem?

And I see this as an iterative process.

Like you don't know starting out, like you start out with just the basic model and then you realize, wait, that doesn't work very well.

And then you keep adding components on there until you get a more holistic model that can make better predictions.


SPEAKER_01:
You guys are very pragmatic.

very good it's very good it's very yeah i like all engineering is right yeah yeah yeah yeah no it's um yes uh yeah yeah well let me know let me know how you guys get about with this pruning via context problem because i think that's a problem and i'm curious about how that's getting done probably again with these uh phenotypic priors um i did actually get up this viveki thing so i'll mention it because you might be interested because it actually comes from artificial intelligence

So this is actually from a really nice paper, which was in Phenomenology in the Cognitive Sciences by Brett Anderson, Mark Miller, and John Vervaeke called Predictive Processing and Relevance Realization, Exploring Convergent Solutions to the Frame Problem.

And therein they say, and I will get it up.

In the middle of the 20th century, artificial intelligence researchers discovered what Dennett, 1987, called a new deep epistemological problem that came to be known as the frame problem.

In its original form, the problem consisted of how to program an artificially intelligent agent to intelligently take into account the side effects of its actions.

For example, if a robot needs to retrieve a battery, but the battery is on a wagon that also has a bomb on it,

The robot must be able to realize that retrieving the battery will bring the bomb along with it, thus resulting in its own destruction.

This may seem like a relatively simple issue, but it turned out to be incredibly complicated.

The problem is that there are a near infinite number of potential side effects to any particular action.

How can we program the robot to take into account the relevant side effects while intelligently ignoring the irrelevant one?

We cannot program the robot to systematically assess all of the possibilities because there are far too many.

And so I guess that idea is, okay, you give it a prior which says, you know, don't pick up wagons with bombs on them.

But then there's a potential side effect that whams if there's a child underneath it and you want to pick up the wagon with the bomb on it and you want to resolve the child.

And what you end up with, I think-

what you're getting on that scenario, right?

Yeah, you get massive overfitting.

Yeah.

And so what I think active inference brings in really nicely to this, again, and this comes out in my paper, I guess, is the notion of a hierarchy.

And I guess that feeds in, right?

Like, if there's a child on, you know, if there's a child at risk, you do it.

And I guess the question of the frame problem is, well, what's at the very top of your hierarchy of values?

And maybe those are evolutionary priors.


SPEAKER_00:
That's a really good question.

I don't, you know, of course I don't think anyone knows the answer to that question, but I will point out when I'm thinking about this, what you, you know, you brought up that up that, you know you know, children,

me children are like that robot like really like you know toddlers they don't know the context differences between things right like but you're taught it over time if you put them in the world yeah they're probably going to take that bomb and they won't know the difference but we tell them by saying no don't do that don't do that they learn over time uh to make those those contexts and i don't know exactly like maybe some of it eventually bottoms out at evolution but i think that's what the learning stage is for you know young children they have to go through this sort of

you know, trial and error where they're doing all kinds of things that could kill them.

And they don't know until a parent tells them and explains, you know, like, this is dangerous.

This is why, you know, and there's still in that, in that question is like, how do you then, you know, how are all those things ranked?

Where, like you said, like, what if there's a child, like it doesn't completely solve the problem.

But I'm just pointing out that like perhaps a robot needs to go through like extensive training where, you know, like if there was a way to encode those priors, maybe we could build them in.

But it's probably so complex that the only way is to make like an infant robot and have it go up through stages of development where we start teaching it things and we, you know, instruct it and it starts to build in those those pieces in there.

That's just speculation.


SPEAKER_01:
No, I think that's absolutely right.

Again, the philosopher in me is saying, well, why

And this is the infinite frame problem.

Why would the child listen to the parent and try and survive in the first place?

And then maybe you do get grounded out in evolution.


SPEAKER_00:
I mean, there's definitely the instinct to, you know, your caregivers are the, you know, why is there an instinct to cry?

And why, you know, there are all those sorts of things.

Like there are animals that don't cry out in pain are often ones that are not.

They're ones that don't have a mother.

Like there's no...

There's no one going to help you.

Crying out in pain is a social reaction because you know that others will take care of you.

Eventually, it's going to be built into social customs, which again, then bottom out into evolution, is my guess.

Eventually.


SPEAKER_01:
All of this is some kind of foundationalism.

And I have been friends give us that foundation.

So we took it took me it took me an hour to get there.

But we got there.

So I'm very sorry for everyone who had to end use.

I mean, these problems are like that.

An hour is like, yeah, I knew the fucking answer already.

I already spoke to john about him for some reason.

Anyway, we got that it's all evolution was not all evolution.

It's a hierarchy.

Final question.

There's a lot of hype about deep learning and

large language models and neural networks.

And we haven't even touched upon this podcasts most frequently touched upon topic, which is consciousness.

And this is gonna be a very flippant, facetious question.

And I want it to be answered in a flippant, facetious, facetious manner.

Are any large language models that exist right now conscious?


SPEAKER_00:
My instinct tells me no.

But

I will say that large language models continuously surprise me with the kinds of things they can do.

There's all this research coming out right now about like, well, what appears to be immersion is really not, now that we kind of look at it a bit more closely.

I think that it's worth considering, not that they're conscious, but there may be some level of

I'm trying to think of the right word here, because self-awareness, all those things are not quite there.

But there's always this argument like, oh, well, something is mimicking it.

It's not actually the same thing as that, like a parrot kind of thing.

He doesn't understand the meaning of the words.

Where does that boundary get crossed?


SPEAKER_01:
there's eventually a point right where the mimicry is the same as the actual thing right and i don't know it's a very deep philosophical question yeah um which should only be answered in facetious ways because it's it's hundreds of years of thinking oh yeah well seoul's chinese rumors

Well, you just classic.


SPEAKER_00:
You don't.


SPEAKER_01:
Yeah.

You just don't.

Right.

Like, you do the mimicry.

And actually, well, there might be some people who say that Chinese room is conscious.

That's kind of in some ways.


SPEAKER_00:
You're guided by intuition.

you don't have a definition of what conscious is which we don't it's really hard i think we have you only can do is you can kind of poke holes in things and be like well we know we're conscious at least we think we are and here's what we do and this thing is not doing that thing like you know yeah the whole whole the chinese room thing is a whole lookup table right like well yeah yeah yeah we're not doing a lookup table but there's some compression going on like we have a compressed representation of something is that so is compression sufficient


SPEAKER_01:
And maybe, you know, or relevance.

Yes.


SPEAKER_00:
Yeah.


SPEAKER_01:
I'm thinking about these ideas.

I'm not going to, I'm not going to give any teasers, but I'm thinking about this.

Um, okay.

And final question is will large language models get us there?

And by get us there, I mean, um, well, actually you can pick, you can either have consciousness or you can have what people throw around as you know, with the term general artificial intelligence or sorry, artificial general intelligence, AGI.

God knows what that means, but you may have your own definition.

So will it get us either to Consciousness or AGI?

Take your pick.

Please be facetious.


SPEAKER_00:
Personally, I find LLMs really boring.

I just don't find them very interesting, and there's been a lot of hype around them and what they can do.

You know, I'm the kind of person, I don't have a lot of very strong opinions about these kinds of things because I feel like we just don't know enough, but I can have, I have hunches, right?

I have a hunch that it's not gonna get us far enough.

And I don't think if you throw enough compute power at an LLM, eventually you'll have something out of it.

And I think the big reason is because there's so much, when you look at like active inference models, they're not nearly on the scale of, you know, they're not LLMs, right?

there's causality there's a sense of causality and structure in the data generation process that i think you're not completely recapitulating with an llm this is more of an intuitive feeling i don't have data to back that up but when i look at an llm i don't see it as i see it as clever memorization with some level of compression um in there and you if if it's possible to do

you know, you take maybe LLMs is too much to look at, but like a simpler deep learning model.

Can you take that deep learning model and do it better with active inference with less parameters, less training time, less data?

Can you do a lot more with a lot less better generalization?

If you can, then my hunch would be that the active inference model, which is obviously where my position tends toward, is compressing or capturing something in the data much better than a deep learning system is.

Now, I don't know what that difference is and what it's doing, but if I had to make a bet, I would make it on that active inference is doing it better and more compressed in a way that's probably more scalable and more likely to bring about

so-called AGI, which for me means mimicking human and animal behavior on reasoning tasks is a very vague, like what we see in each other as being intelligent behavior, we recognize it in the systems that we build and doing it as well as human performance.

That's kind of what I would consider as a vague definition of what AGI might be.


SPEAKER_01:
That's good.

That's a nice, it's a weak version of AGI.

It's good.

It's good.

It's good.

We're not going to get anyone too angry with that one.

don't want to commit too hard to it so no no no well you never know who listens you never know who listens i might get you know an email from jeffrey hinton or someone you never know i'm not i i'm just going to watch um but if versus new philosophers then well actually i know there are philosophers and it sounds like maxwell for example but yeah there's others too yeah and so on well sanji i think all of that means that you're absolutely on the cutting edge of well of

the future of intelligence.

And that's exciting.

I think, for my side, you're, you're a real inspiration.

And I say that a lot to people.

And that's because I mean, because I'm really, really very inspired and touched by all the people who have helped me.

But you have helped me not only with this textbook, but also, you know, privately in messages, and you've put up with some terrible questions, probably, but they're all questions.

Thank you.

I was waiting for that.

But

Yeah, I think I this, hopefully, I mean, this podcast, I've really, really loved because I think it shows people that something that can seem incredibly complicated.

Well, in some sense is but can be, you know, it's tractable to use a sort of Bayesian term, just given a lot of hard work, determination and kind of motivation.

So

you're definitely a kind of shining light, although people may have not, you know, people may not know you because you're not sort of one of these kind of, you know, you haven't got millions of papers out there.

But nonetheless, it's, it's a great pleasure to speak to you.

And thank you for the work you do.

I always ask people before they finish just to let people let the public and the listening public know where they can be found.

So, you know, if anyone does have any curious questions about the textbook,

And maybe you want to speak about the textbook.

You can give yourself just the kind of 30 second plug, but yeah, you know, email, Twitter, whatever, whatever's best.


SPEAKER_00:
Yeah, email is the best way to reach me.

It's Sanjeev, S-A-N-J-E-E-V.namjoshi, N-A-M-J-O-S-H-I at gmail.com.

You can also find me on the Active Inference Institute Discord channel if you want to DM me over there.

My website is snamjoshi.github.io.

I know this hasn't been picked up by search engines yet, so try to work on it.


SPEAKER_01:
I will put it in the video description.


SPEAKER_00:
Yeah, it's hard to find, but I just have a little bit more about my papers and research and things.

Yeah, I'm open for people taking a look at the textbook.

Basically, if you send me an email just saying you want to look at it, I can give you a link to...

the CODA page that's hosted by the Active Inference Institute, or their chapter drafts and just video lectures that I've given.

Still a work in progress.

I'm in second revision right now.

There's two books.

The first volume is due on June 1st.

Wow.

It's coming up.

Amazing.

I've been pretty slammed.

And then the next one is Bayesian Mechanics.

It'll be much later.

if you're willing to look through any drafts and you're very curious about anything else um you're welcome to send me an email and i can give you access and also that's just yeah feel free to send me an email and i'm happy to talk awesome sanjeev this was awesome really really fun really informative um thank you so much great thank you very much i really appreciate it
SPEAKER_00:
Hello everyone and welcome back to Active Inference Insights.

Today I have the great pleasure of speaking to Alex Kiefer.

Alex is currently the Senior Research and Design Engineer at Versys but was previously a lecturer at Monash University in Philosophy.

His work is deeply exciting and unique and focuses on the convergence of machine learning, computational neuroscience and philosophy of mind.

Alex, I'm sure you know some of my favorite topics.

The latter one I feel a little bit more well versed on, but I can't wait to learn more from you about the two prior.

Firstly, thank you so much for joining me.

This is really exciting.

I think reading over your work, both prior to the kind of research that I did for this and during,

I found it to be incredibly, as I say, unique and special because I feel like you're one of a kind in so far as not many people are really able to merge the mathematical side and the philosophical side.

So I'm actually just curious more generally about how you got into this convergence point.

Was it through maths and machine learning or was it more through philosophy and then you had to add the maths on top of that?


SPEAKER_01:
yeah well first of all thanks so much for uh for talking with me like i'm looking forward to just discussing with you and it's nice that it also happens to be on a public uh podcast absolutely

um so so yeah that's a good question uh so i i came from a philosophy actually originally came from an art a fine arts background so i'm i have a tendency to go from one thing to another but i got into all of this through philosophy so i would say as a philosophy grad graduate student i was thinking about cognitive architecture you know in a naive way i was starting to think about these things uh and this was

um you know this was this was some time ago so i'd say the landscape was a bit different so not every philosophy graduate student would necessarily have been exposed to like deep learning connectionism um indeed when i was working my phd deep learning was just starting to sort of be a thing

So I started to find references to like connectionist models for things like natural language processing.

And I got interested in those things.

And I'd say, I mean, as is the case for many people in my field, Andy Clark's BBS article, whatever next big watershed moment where I had been thinking about, for example,

higher order Bayesian models, signal detection theory of consciousness, for example, by people like Hak-Wen Lau.

So I've been thinking in these terms of generative models already, but then that article really just sort of caused some things to click into place for me.

And I'd had sort of a,

I don't know, amateur or casual interest in software development, coding on the side, and mathematics.

So essentially, I got into this through philosophy.

I picked up as much mathematics as I could on the street, as it were, to do the things I wanted to do.

And I just thought it was really essential to be able to build some of these models to understand really what the hell was going on, right?

So not to get too far into this topic yet,

I mean, part of my philosophical point of view is that really a naive sort of atomistic symbolic approach to content, the content of mental representations or representations in general is not really

is not really the way to go, although it's sort of maybe predominant, I'd say, in a lot of the philosophical literature.

So to really understand something, I think you have to understand its functional profile.

And so I thought that there were a lot of discussions in philosophy about cognitive architecture that were not sufficiently grounded in

um a kind of knowledge of the the way that these representations might work which i think to some extent i mean there's always um sort of the low road and the high road right there's always top-down understanding and more bottom-up understanding but i think some bottom-up understanding with um i felt was necessary to to understand this this stuff and then of course once you start to work with models like this they um sort of become interesting to work on their in their own right so to some extent i was pulled away from the more purely theoretical questions into

wanting to build better models of cognition?


SPEAKER_00:
Sure.

Well, I mean, it's in the most praise, you know, praising way possible that you wouldn't be able to tell.

Hence why I had to ask the question.

I mean, again, I'm not a mathematician, so I can't, you know, I can't say, you know, I can't be an authority on your mathematical chops.

But honestly, it's, I've always, I've always sort of considered you a philosopher.

And then I, you know, reread the papers.

I was like, wow,

So this is like super technical.

Uh, I was really, really impressed.

So no, I, I find it very, I find it very admirable when philosophers, cause I guess it's a path that I'm also, you know, to be candid also trying to pursue, which is supplementing or combining theoretical thinking with, um, you know, with mathematical modeling or mathematical formalisms.

So it's, it's kind of nice to you and people like Maxwell pave that pave that way.

So thanks in a weird kind of indirect way.

Um, I'm so glad you brought up content and representation because I was actually exactly what I wanted to start.

Um, I kind of thought we would start somewhat broadly.

So I guess, um, it's funny, you know, doing a psychology masters.

You probably hear the word representation quite a lot, and I did, but no one ever really goes deeper than that.

It's like, okay, there are representations, and we all kind of have a hunch about what that means, and we think about paintings or photographs, and they stand for something.

However, just doing this podcast and doing more reading, you kind of see that it's actually an incredibly contentious issue, what a representation is, do we have representations in the brain, mind, and so on.

I was, I feel like I'd come across notions of similarity and resemblance in the representational literature, but maybe not as sort of directly explicated by yourself.

You sort of speak about a form of Bayesian structural representationism.

I thought this was a really, really interesting idea, because as you know, and as you recognise, in philosophy, resemblance or similarity and representation are often divorced.

And people will say, well, you can't get representation out of resemblance, something akin to that.

How do you manage to get representationism out of sort of structural similarity?

And what does your position entail more broadly?


SPEAKER_01:
Yeah.

So indeed, I think representation is something that it's a term that a lot of people across many fields use and

I think they're justified in using it.

It's a fairly easy language game to pick up in a way, to start talking about representations.

And I think the term has just proven its usefulness, just in the fruitfulness of all the various research programs that employ this as a central term.

And so I guess to start with your direct question,

I think one, right, so how do you get past the usual objections to basing an account of representation, a philosophical theory of representation and resemblance?

I guess maybe the first objection that you might encounter is, well, resemblance is all over the place.

It's very cheap.

Whereas representation, especially in the sense that's relevant to psychological

capacities and theorizing about those capacities should be something more special.

So I'd say the first move to make is to say that representation is also cheap.

That is to argue that, so I actually have a,

I'll probably say this a lot.

I say it in any case enough in conversations.

I have a paper on this.

I haven't published.

It's sort of against inflationary concepts of mental representation.

So I think there are some, not dangerous, there are some waters you get into here flirting with things like panpsychism that maybe we can touch on at some point.

Here's my view.

If you can come up with a serviceable notion of representation that does the explanatory work it should, both with respect to cognitive science and also in common sense terms with respect to capturing the manifest image as well as the scientific image.

You can understand how this thing that you're calling representation does those jobs.

And if it turns out that it's everywhere, well, then it's everywhere, right?

So I don't think there's any need to front load the assumption that representation needs to be rare.

So that's, that's what now I don't want to say that every resemblance relation is a representation relation.

But I think you can part of the answer to that objection is to bite the bullet and say, you know, individual neurons probably represent things, for example.

in the same sense, the same fundamental sense as you or I do at the personal level, maybe with different features within that category.


SPEAKER_00:
But we still get the extra step, right?

So we still get the extra step from pure resemblance to, in your work at least, I mean, some people will just say they're at least in basic minds,

there are no representations.

And here I'm thinking about the sort of radical and activists who say it's only present in these kind of scaffolded, non-basic minds or basic forms of mind.

But you do want the positive representations, as you said.

And you do make it clear that we have to go beyond resemblance.

So maybe what I'm kind of leading you here is towards the kind of teleosemantics or kind of more functional semantics.

I really enjoy, I mean, I personally really, really like this idea that in some ways a representation means something to the organism, I guess, because it, for me, it feeds into ideas of sensemaking or affordances or whatever you want to call that.

So how do we go from pure

objective, in quote marks, resemblances to a representation that is for or realized within an organism.


SPEAKER_01:
Yeah, so all right, so I'm going to try to spell this out.

This is difficult.

To me, this is like, in a way, it's closest to the core, the very core philosophical problems that I think remain sort of unresolved or without a uncontentious resolution for good reason, because it relates to basically the way in which the sort of subject relates to the object or the mind relates to the world.

So the caveat, this is still ongoing.

I don't feel settled about some of these questions.

What Jacob and I said in our papers on this is that following more or less the account that had been in the literature is that you need not just structural resemblance in order to have representation, but exploitable structural resemblance.

um but i do want to i do want to emphasize that um i and i can to some degree speak speak for jacob since we've collaborated on this but you know he has his own views um so i'll just speak for myself for now um i i don't see either the resemblance relation or the exploitability relation as necessarily um in fact relations to concrete particulars external to the organism uh let's start with resemblance um so and this this is uh this is in the papers that we wrote

although it could stand to be made more explicit, I think.

Fundamentally, the kind of resemblance I think that the account rests on is resemblance between the structure of your internal model and some hypothetical world.

and it needn't be the actual world.

So what you represent in the first instance, and this sort of resonates with some of the things that were said early on in the Bayesian brain as the Bayesian brain idea started to break across philosophy and cognitive science about

know your representation of the world being something like an internal simulation or perception is like a vertical hallucination or uh online controlled hallucination um so so the resemblance in the first instance i would say is between the internal cognitive structures and a hypothetical world which may or may not uh match the actual world and then i would say a similar thing about exploitability um

Exploitability, you might try to catch that out in very obviously naturalistic terms in terms of like, as you said, teleology, what is the function of this cognitive structure?

What does it tend to allow me to achieve or what evolved to allow me to achieve, for example?

I am definitely not a teleosemanticist actually.

I think there's been a lot of good efforts to integrate active inference and teleosemantics in the literature.

I think we can get into this if you'd like, but I think it's a misstep.

I don't think you should have to look

At least if you interpret teleology in terms of something historical, I think it's a misstep.

I don't think you need to look to any kind of evolutionary history or beyond the current functioning of the organism in order to say what the contents of its states are.

That was already a mouthful.

I can always go on.


SPEAKER_00:
There's plenty of that.

So I guess the first thing I want to just touch on is

So yeah, you do speak about the sort of structural resemblance between, and this is actually, as you say, quite, at least in my eyes, well-established within the literature, the structural resemblance between the Bayesian, the generative model and the generative process.

I really like your addition that it doesn't have to be the real world because you've got imagination, memory, dreams, and so on.

And I have had conversations with people who say that I think, you know, the perception action cycle itself needs to be maybe buttressed by, uh,

either retrospective or prospective ways of thinking.

And that's actually kind of in the phenomenology as well, right?

Hustle distinguishing between the live moment and then also retrospection and prospection more formally.

So that's a really nice addition.

What is maybe slightly more contentious is if we fold truth conditions in or satisfaction conditions.

And I feel like

Again, I don't want to speak for you.

You can tell me that if the generative model is meant to accurately map onto the generative process in the way that minimizes that KL divergence, there has to be some element of truth tracking or, uh, we can't be completely making it up on the spin because then our models are basically useless over time.

So how much do you then fold not only, uh,

the the real-time exploitation but also the fact that there's got there's some truth relation between uh world and model yeah so so i think on the fundamental um again uh


SPEAKER_01:
if you just want to characterize what about this thing makes it a representation, I think it does.

I tend to think in terms of truth conditions, I actually think truth conditions are a nice fit with a Bayesian approach because what are distributions over like events, right?

And you can think of these as events and their descriptions of states of affairs.

It's not like your distribution is over dog, right?

It's over there as a dog.

So it's a nice fit.

And of course you can think of,

there are nice relationships between logic as usually defined over propositions with truth values and probability theory so i think that all works out very nicely um so the i guess the question is to what i mean practically speaking your your model and this is sort of a very free energy principle type statement i mean your model your model will not exist if it doesn't track the truth to some degree right and actually i know there's there's definitely a pushback in the in

um free energy principle and active inference literature against the idea that these models have to be veridical because they're used for control of course as well um but i think i think that the clear truth is that the you're of course so perfect for radicality um at least at the sort of uh

highest possible like um resolution or fidelity of representation is is not needed and it's probably would be counterproductive right you don't want the map to be as rich as the territory usually right but i also think you can't you absolutely need to to track the truth to some degree and the nice thing about structural resemblance approaches is that you can uh you can

quantify various ways of doing it, but you can try to quantify the degree of structural resemblance and you don't have to talk about just like truth or falsity.

This is something that can come in degrees.


SPEAKER_00:
Sure.

Okay.

So we're kind of in agreement that there's only a slight it's not a caveat.

It's a kind of red herring.

I'm sure that we can skirt.

It's actually something I raised with Carl in our first conversation.

They haven't raised since because you kind of get integrated into the Bayesian game and then you forget all of your sort of actually quite useful naive questions.

know i asked him whams if there is just like no noumenon right ones if there just are no hidden states if i just take a very kind of george barkley idealist position there's no kind of all there are ideas i mean i guess you could possibly ideas as being the latent states and and their appearance as being the sensory uh you know input but it's

It's a kind of classical philosophical question, which is, whams if we're a brain in the vat, do we still get truth conditions out of that?

Can we say that we've got hierarchies of observation and what they track even within a kind of illusory space?


SPEAKER_01:
Yeah, I love that question.

Okay, I mean, there's different layers of this.

There's different ways of thinking about these skeptical scenarios.

I would say that you still have truth conditions for sure, to the extent that

I mean, to some degree, I'm just like an intuitive internalist or not solipsist, but internalist.

I start from the internal Cartesian kind of point of view.

And I do think there's something sort of esoteric you might say about the way we represent the world.

It seems as though to me that I perceive things as being thus and so.

And the condition for that being true, I think that that representation has a truth condition.

whether or not, of course, whether or not it's true or could be true, whether or not the structure of the universe is such as to be true, even possibly.

I do think with respect to any skeptical, there might be versions of skepticism where you say there's literally just nothing out there.

and um i'm not quite sure what to say about that but i have thought more about um so david chalmers has this great paper called i think it's called the matrix as metaphysics where he goes into some of these issues and

I don't know if I'll be able to fully justify this at the moment, but I know that where I've landed on this, thinking about this stuff in the past, is that from the point of view of trying to model the generative process, so you figure every agent has its own sort of subjective ontology, which is an aspect of this internal simulation that you develop over time through sensory feedback.

Now, whatever the nature of the noumenon is,

It's something that, through interacting with it, has allowed you to generate these structures.

And I think those structures

those structures, I guess, is a form of structural realism, right?

So the structure exists regardless of whether, even if we're in a simulation, there's a sense in which the internal representations that I have, there's truth conditions would be satisfied unless, right?

So it could be part of your overall model that you say, and this isn't a computer simulation or something, right?

So these very high level sort of,

beliefs about the kind of world you live in, your metaphysical beliefs might might falsify your, your overall model or not.

But to be agnostic on those points.


SPEAKER_00:
Sure.

Yeah, I mean, I think Jacob actually has a really nice contribution to his how to end train your evil demon paper, which is that the demon is giving us pretty reliable, you know, pretty reliable signals, whether it's tricking us or not.

I mean, we there's kind of beautiful sensory motor

loop whereby we move and what we sense is changed by what we how we move and how we move is changed by what we sense and we seem to have quite good control over our evil demon i guess like as just um a kind of budding philosopher you always run that argument to the ground which is that maybe there is no you know maybe there is no simulation there just is pure awareness right um or that's just you or that's just you uh or well not you but you know your mind and you're a solipsist and in that case

there is nothing to track.

It's not like a computer simulation.

You mentioned David Chalmers, he's written this work on virtual reality, which I'm sure you've checked out.

And, you know, he doesn't think there's such he doesn't really matters, right?

Like the same structures, the same philosophical relations still hold whether it's carbon or silicon or whatever.

But there's still this kind of binary distinction, okay, you've got what is mapping or the generative process, the generative model and the generative process that which is being mapped.

And

I just think there is at least philosophically, I might be wrong here, a skeptical position where you just go, well, maybe there just is the sense of mapping, but there's actually nothing behind that.

All there is is the map.

There is no territory.


SPEAKER_01:
that I'd say, yeah, so even the sort of thin sort of realism you get out of this David Chalmers perspective might be sunk if it's like, I forget what this Russell called it, but basically the hypothesis that the world sprang into existence a second ago, right?

And all your memories with the all your memories intact.


SPEAKER_00:
Yeah, but it's Bertrand Russell, right?

Did he?

I believe so.


SPEAKER_01:
Yeah.

So I don't have an answer to that.

In general, though, I think so skepticism is something that, indeed, you have to kind of struggle with it.

I guess if you study philosophy in that way.

But I think to me, the outcome of that struggle is just a realization

that that's the way things are and uh and that's okay um yeah it's not really okay right because if you actually found out that one of these scenarios were true and you suddenly woke up and you're in a completely different world okay that would be it'd be quite traumatic um traumatizing but


SPEAKER_00:
Well, so would be being alone in the universe.

That would be quite traumatic as well.


SPEAKER_01:
Honestly, yes.

In most moods, it's easy to dismiss.

But if you take it seriously, honestly, that's one of the most terrifying prospects, I think.

But let's acknowledge it rather than trying to dismiss it via a superficial argument.


SPEAKER_00:
Sure, sure.

I think in part this is, we don't have to go into this, but this is in part a result of philosophical education, at least in the Anglo-Saxon world, which is they start you basically on skepticism.

They start you on Descartes.

And then you read

stuff in active inference about perception.

And they're all saying, well, we kind of take direct realism as, you know, for granted, you're like, what?

Like, at school, we said that was rubbish.

I just wonder, and and then, you know, you start hearing all the externalist attacks on internalism.

So I'm kind of curious about your experience

openly, and it sounds like it's a taboo, and it really ought not to be and I don't think it is, but openly disc or promulgating or proposing a internalist position.

It seems to me at least that from the outside, a fairly rare position to occupy and the kind of hot thing is to be as externalist as possible, albeit there is some pushback, of course, against radical and activism.

So I'm just curious about sort of sociologically how you found that.


SPEAKER_01:
Yeah, absolutely.

It's been

it's definitely sort of a unpopular position within sort of this, well, I'd say in general, as you said, in general these days.

So, right, so before I ever got into the active infant stuff, I sort of was already fighting the tide against, I'd say, externalism.

To me, it's just,

It's a perfectly legitimate set of motivations that motivate the externalists.

I want to understand the role that representation and related notions play in explanations and cognitive science.

And that's a sort of third person sort of operation.

And so on what basis would I posit contents?

Well, I'm trying to explain

you know, how this mouse manages to navigate in its environment or whatever.

And there's no reason not to use sort of externalist, externally available evidence in doing that.

So that's fine.

That's about content assumption.

But I think, I don't know, I think, so that,

movement i'm not sure exactly where the roots begin they probably go back as far as anything but um there were these moments in i guess the 70s the in the 80s people like tyler burge were really pushing anti-individualism uh in the philosophy of psychology um and i think this is just one of those cases where the sort of the baby got thrown out with the bath water or things right it's like you know a pendulum and i think the pendulum swung very far in the direction of externalism but i just i just think that leaves out um

some very, very basic points, like the fact that what we're trying to explain, I would think, and maybe this is too tied to consciousness.

I'm curious what you think about that.

Trying to explain an organism's point of view on the world.

So I think you want to limit yourself to that.

And if you start to legislate about what the organism's point of view on the world is by reading in information about what you know is in their environment,

but you don't know how they're parsing it or how they're processing the information into cognitive internal structures.

I think you're in danger of, um, um, sort of just biasing your account of what, of what their point of view by your point of view.

And, um, that's a sort of naive way of putting it, but I think you can make some of that more, more precise.


SPEAKER_00:
I, well, yeah, I think I, yeah, I don't know how much we need to, I think we're kind of, we were seeing from the same hymn sheet here.

I think, um, I mean, I think I'm in quite a privileged position actually as a,

relative layman because I have no skin in the game, right?

I just see things as I see it in terms of my intuitions.

And I think also starting from consciousness and privileging consciousness, privileging lived experience, at least biases you towards some form of internalism.

I mean, I had this conversation with Inez, Inez Ippolito in one of the earliest episodes of this series.

And

Yeah, I'm exactly like you.

I mean, I read Evan Thompson and I read Varela and I love it.

I actually think it's really beautiful.

And I love this idea of the sensory motor loop.

And Evan Thompson actually in mind and life talks about how the mind body problem can be become a body body problem.

It's the lived body versus the living body.

But even if you read that chapter on consciousness, and Evan, I'm sure, would say this, and I'm sure he does say it in the actual book, you still don't get the phenomenal reality.

You still have that distance.

And it's the same thing I say with Inez, which is that, well, I'm very happy to say that affordances are socially structured by our language, by our culture, and so on.

But there's still something it is like for something to afford itself to me.

And I just, I have a real hard time saying, that way of seeing the world is relative.

It's in some Wittgensteinian way, the qualia is a kind of old fashioned idea.

And let's throw it, it just doesn't do the job for me.

I've had a bit of a very small chat with Jakob about this.

And I think we're both on the same page, which is that not well, actually, and to be fair, I just don't think anyone's going to be a really true, strong illusionist about the hard problem and about

consciousness.

Now, you can get into your Keith Frankish and Dan Dennett and say, well, it's about qualia and postulating these private states.

And again, I'm not really well versed enough to understand that.

But I think you're absolutely right.

Starting from philosophy of mind, analytic philosophy of mind, and the tradition makes one very, very skeptical, at least, of a reductive account.

Which I guess is actually also, you know, sorry for rambling, it's also kind of weird because then you see some very strong externalists also push back against reductivism.

As I was talking to Julian Kiverstein, and Julian is very unhappy with the idea that neural states are you.

And Evan says this as well.

But they think, well, actually, you can just expand that notion into your sensory motor capacities and your affordances with the environment.

No, you're still missing something.

like you're missing phenomenal reality.

So I'm kind of curious about what you think about that.

Like, where is it?

Do we get out of the problem by saying, you know, neurons aren't you?

But actually, if you talk about your wider socio cultural, you know, context and your history, we get you?

Or do we need to fold in phenomenality?


SPEAKER_01:
Yeah.

So I think there are a couple of issues here.

Um, so, so one strand of this leads right into some other work I've done that the psychophysical identity stuff where, so I think, so the, the sort of, um, representationalist versus an activist access is one thing.

And certainly I fall on the representationalist end of that.

And I have to admit, I probably need to give an activist they're doing and,

try to really immerse myself in the core literature there.

But to me, it's a non-starter.

I don't see how you can explain just perceptual errors and things like that without appealing to something that says that the world is this way, even though it's not.

And to me, that's already sufficient to be a representation.

And I don't encounter many super radical and activists who think that we don't need representations.

once you have representations in play then that's that's all i've ever ever assumed um so i don't worry about too much about that but then you're also talking about sort of the i guess the issue of whether the phenomenal can be reduced to the physical um things like that and so in in the psychophysical identity paper i

I'm not sure by the way, of course, as a Bayesian and a philosopher, I don't have settled views quite generally.

But the view that I explore in that paper and that I do still want to pursue as a serious possibility is that you can have something like a mind-brain identity thesis.

whatever it is, it has to do justice to phenomenology.

So even if it's that you're just your neurons, which I don't think that's quite the story.

I think probably the scientific account coming from the life sciences and physics and so on is not finished yet.

And I think

wherever we end up with that, I think it'll make it sound less absurd, whatever it is.

It's not going to be that we're a collection of neurons necessarily.

It's probably some form of self-organization that has a sort of natural unity.

We can get into that.

But essentially, even if that's true, I think a satisfying account has to be able to explain in completely compelling terms

how that could be true, right?

So how could my current experience of the world while looking at my screen right now and talking to you, how the hell could that be the same thing as a bunch of neuronal activity and so on?

And I don't personally think that applying to my wider

ecological niche or anything like that is going to help elucidate that.

I think it's more like we need to elucidate the connection between the physical mechanisms.

The physical mechanism is actually stacking the deck too much.

The physical processes and my conscious experience.

And I think there's some hope that we can do that, but it cuts along very different lines from the inactivist kind of solution.


SPEAKER_00:
Yeah.

Well, again,

I just personally think that that concession is made at least within a lot of inactivism.

Even sort of like Alvin Noe's sensory motor theory talks about how sort of neural signatures will tell you maybe why you're seeing red rather than blue or why you're hearing rather than seeing, but it doesn't tell you why you're hearing.

It doesn't tell you why you're seeing red.

Yeah.

I like your optimism.

I don't know if I share it about the relationship between the physical and the phenomenal, but I have a soft appreciation, let's say, for more esoteric theories of consciousness, whether that be a kind of conscious realism a la Donald Hoffman or maybe even panpsychism.

But again, I've got no commitments.

We'll loop back, but I wanted to just quickly touch upon this notion of teleosemantics.

which was new to me until a couple of months ago.

And then I go into, I was reading about teleosemantics and teleosemiotics and the difference between those.

But if we just stick with teleosemantics, the idea being that sort of from a telos purpose, a biological function, let's say you can get representation, you can get some meaning and you want to push back on that in a way because of the historicity.

So the content itself is not necessarily determined by your background.

It's interesting because I kind of see it differently, although I don't necessarily have a good way of articulating it, only because we take as an assumption that the prior preferences we have in our active inference models are at least constituted in part by evolutionary priors.

And I presume that my representation, and then also coming from the affordance literature, my representation of a table is very different from an ant's representation of a table because the affordance is the difference.

So what do you mean in terms of siphoning off the evolutionary history from some form of teleosemantics?


SPEAKER_01:
Yeah.

Oh, great.

So, I mean, first I should say, I think Tileo Semantics is an amazing research program.

Like I've read a lot of Ruth Milliken's work, which is foundational to it, and it's fantastic work, absolutely foundational to where we are today.

But I think, I mean, the big picture is that I just think, okay, I'll answer the question you asked, which is what do I mean by siphoning off the evolutionary history?

Just quickly to motivate why I do that, I think you have to bite some bullets.

So I'm willing to bite the representation is everywhere bullet.

But one thing that Milliken explicitly says is, so if you have, you know, there's these thought experiments, like I think it's called Swamp Man or something, where some creature appears suddenly out of some quantum fluctuation or something.

and is physically identical to a human being who would have evolved over millions of years, et cetera, to have the same structure.

What Milligan says is that basically the internal structures of that creature, such as the heart, brain, whatever, shouldn't really even be called that because those terms refer to things that have a certain function, and that function comes with selection history.

So I guess the idea is, well, this thing just appeared.

You don't really know what its parts are for.

You don't really know what it's for.

And I just completely reject that intuition.

I think it's clear that if you

You might have to spend a couple of minutes analyzing it or talking to it, as the case may be.

But you'll see, you'll be able to figure out what these things are for.

It's like, well, the purpose of its heart is to pump blood in the sense that it's doing that now and it's keeping it alive.

And I don't think you need to look further back.

And so I guess that does start to get into...

i mean by by separating these things off so i think if you have if you have a rich enough account of the current functional organization of something that you should be able to figure out there's a sense in which you can cash out uh the the purpose of the structures within that thing without knowing about the history so maybe i'm relying here on an overly um complacent distinction between like

constitutive and causal relationships.

So like certainly the history is necessary to understand how this structure got to exist.

I just think that's a different question from like,

what is it doing now, possibly what's its purpose now, certainly what's its content.

So I want to add one more remark here, which is even if you had a nice way of cashing out biological function, let's say you could even do it in a way that doesn't appeal to history.

I still don't think that that's even the same thing.

I don't think that's what you want to explain semantic normativity.

I just think it's a different kind of question.


SPEAKER_00:
Yes.

Yes.

And in fact, that's interestingly the very point that Hato and Mayan make in rejecting teleosemantics, which is like, how are you getting content out of biological necessity?

And I think Jerry Fodor had a similar stance.

Yeah.

I also wonder whether we need to make the distinction between what I can say about the swamp creature and what it is like to be the swamp creature in terms of maybe it's not necessarily for me to know the entire evolutionary

past of the swamp creature in some deep epistemological ways to account or to make a rough approximation of what's going on.

But as you say, in terms of the actual ontological status of its internal states, and by that, I mean, I buy the thing about heart and lungs, because you can get those by analogy.

But I guess the idea of the kind of private mental states

you probably do get some way by accounting for its evolutionary past, right?

In the same way that I get some way from distinguishing myself from an ant, because, well, we have very different paths.

And that evolution is some way I'm not saying entirely, but in some ways guiding that which you know, the affordances which the landscape presents to us.

But no, I so if we then go back to actually what you do say, you're so positive account, and give a quote here, I thought this was really great.

So

So you've got sort of your structural resemblance and how we're going to get representation out of that.

And you say that this problem can be solved by requiring that the structural resemblance can be used or exploited by a cognitive system in order to make cognitive functioning effective.

This captures the intuitive idea that a representation, mental or otherwise, serves as a proxy or standard for what is represented.

And I certainly see that.

in the sense of how you then get representations, because in a sense, it's meaning for the organism.

Some evolutionary biologists might go, well, what makes something is effective is its evolutionary payoffs.

I'm still struggling, at least from that, to see the strong divorce from teleofunctionalism or teleosemantics.

Is it the addition of the kind of truth conditions that get us away from a purely teleosemantic or functional semantics?


SPEAKER_01:
Well, okay, so I guess I don't see a deep distinction between an account in terms of truth conditions and a teleosemantic account necessarily.

I thought at least that what people like Millikan were trying to do is provide an account of how you get from very naturalistically understandable things like

states carrying information about other states, and things like natural selection, to an account of representational content.

And the main ingredient that's missing there is the possibility of error.

And I think of those as sort of one and the same.

So the truth condition of a representation is what would have to be the case for it to be accurate or true.

And so if you've provided an account of error, then in a sense, you've provided an account of something that can function as a truth condition.

But I think...

I guess I just don't see it.

So I think you do need some account of the possibility of error.

So we did discuss some ways that you might get error out in that paper that you're quoting from, I think.

So you could have an accurate generative model that its structure resembles the structure of the generative process in relevant ways.

But it could be misregistered.

So you could, you know, you could

infer that you're in some state that you're not either because of misapplication of the model or just because, hey, it's a probabilistic model and sometimes what's probable isn't what's actual.

So that's one way of explaining it.

In general, I think you can also just look at the broader, even just, it's a bit of a cartoon to say completely synchronously because any process is somewhat temporally extended.

So I would say history matters only to the extent that you're trying to give an account of a process that might exist in some time scale.

and so from the point of view of a smaller time scale that might be a historical matter but i don't think you need you should for example you shouldn't have to go deep or at all into evolutionary history i think to give an account of my current perceptual ability um that is to give an account of the fact that i can perceive things and that i have mental states with contents you do have to appeal to evolution to explain or it's at least a very good theory if you want to explain like well how did i get into the

the state in which I have this capacity.

But I just think those are two different questions.

So I don't think I've really answered your question yet.

So I think if you just look at the roughly synchronous present-day organization of the organism, you should be able to tell what's now.

This is sort of idealizing.

I don't think you can actually do this, practically speaking, in every case.

If you could take a snapshot of my physical organization right now, and it might take months or years of analyzing it to figure out what the actual functional profile

of all of the components of me as a physical system is or are.

If you could do that, I think you could tell a story about, well, what is the current functional role of this thing?

Say it's a belief that there's a fence across the street.

And you could give an account of the content in those terms in terms of, well, which aspect of the generative process does this aspect of the generative model map onto?

Um, and yet it could still be the case.

You could independently investigate whether there is in fact a fence across the street and you can say, well, this representation has that content, but he's wrong about that aspect of the world.


SPEAKER_00:
Right.

Yeah.

Yeah, yeah, yeah.

No, you're absolutely right.

Of course you, uh, you, you certainly make a point of misrepresentation.

This is, um, for those wondering, this is representation in the prediction error minimization framework by yourself and Jacob.

Um.

Yeah, I think another point that you make there, which is important is that pure functional semantics don't require this structural mapping.

I think that's important as well, that you don't necessarily need this relationship between something like a generative process and a generative model in traditional teleosemantics.

Another thing that you add that I thought was really interesting, I'd love to learn about and sort of peruse is this idea of content holism.

that's a kind of additional part to your proposal and you say here the content of one part of the overall representation depends on the structure of the whole and thus cannot be determined without simultaneously fixing the content of the other parts and you also i should point out in a footnote i believe allow for that to become recursive so that's not you know the the part is not just the part it might also be the whole for sub components

What is Continentalism?

How did you come to that idea?

And what does it add to your structural representationism?


SPEAKER_01:
Right.

So I recently was talking with some philosophers who are co-graduates from my philosophy PhD program at the CUNY Graduate Center.

And I tend to just like assume that everyone thinks in terms of, you know, content holism and stuff.

And they were like, no, if you're going to submit a paper on this stuff, you really have to explain that because, like, so I realized, OK, that's something that has to be unpacked.


UNKNOWN:
So


SPEAKER_01:
This is something that I got, I'd say, primarily from Quine, from the philosopher W.V.

Quine.

My PhD supervisor, David Rosenthal, was heavily influenced by Quine and Sellers, and they're both theorists who think along these lines.

I think this is one of the great divides in philosophical discussions about semantics.

in the past century.

I would just say that I don't see holism as something that I'm adding to the package.

It's not something that I'm tacking onto the view.

I think pretty much any functional role type of semantics, which I take structural resemblance theories to be a species of,

has to have this feature.

So if the view is that your generative model gets its content in virtue of resembling the generative process.

And by the way, just to make this clear, because this also has come up in some discussions with people, at least when I think about that, I think of the generative model as something that has dynamics and itself unfolds over time.

Right.

So, so you have a sort of four dimensional, if you will, um, thing that resembles a four dimensional external thing, or that might happen to be externally real as well.

If you're lucky, if your model's accurate, um, and if, and if we're not solipsists and if yes, and if anything exists, sorry.

Yeah, go ahead.

Yeah.

Yeah.

So, um, um, right.

So the.

So the structural, right, so if you want to understand how anything in that model, so obviously generative models and generative processes are complex things.

If you want to understand the content of any particular mental state, say, and let's say it's realized by some pattern of activity in prefrontal cortex or whatever,

you need to understand what role does that play in my overall model, this four-dimensional object, and then what analogous role does something play in the system I'm representing?

And to me, that seems like from the ground up, it's going to be a holistic account because the content of this structure really depends on the role it plays in the whole.


SPEAKER_00:
Right.

I guess where I'm

Where I probably need clarification is the idea that part part relations, as you say themselves are not resemblance relations.

And then folding that into the footnote, which is that the parts can also function as structural representation in these hierarchically organized systems.

I guess I don't.

So if the content is fixed, according to its representation, the structural representation as a whole.

And thus it doesn't get its content just purely by the part that it plays, but actually, well, you know, the individual part of the place, but actually the fact that it's integrated in a whole, and yet it can also constitute a whole within, let's say a smaller Markov blanket, if you will.

Um, those two, can we hold both of those up at the same, at the same time?


SPEAKER_01:
um i think so i'm glad you're talking about the footnotes by the way i love my footnotes i would have footnotes within footnotes if i could footnotes are very important people people ignore them i love footnotes yeah they're terrible for readers but anyway they're useful for writers yeah yeah yeah so i guess i don't see any

any conflict here.

So for example, say some cortical mini column plays a particular role with respect to one's entire brain, where it really represents, I don't know, the location or the pose of some object.

And when I say represents that, I mean the role that this mini column plays in the overall system is analogous to the role that object plays

in the relationships among things in the represented system.

So that's the basic structural resemblance relation.

Then that mini column might also have its internal structure in virtue of which it represents something.

But I think we're talking about two different levels of description, possibly two different... In a multi-scale active inference picture, those might be two different agents.

And whether I really want to attribute agency to mini columns and cortical mini columns, I don't know, but like the point is it would be, uh, this, this subsystem that would have its own, um, structural representationally grounded content would be a distinct thing that would just be a part of this larger system, which is me.

And only in so far as that thing played the role it does in my generative model, would it have content?

Would it constitute part of my point of view in the world?


SPEAKER_00:
Yes.

So, okay.

So maybe I can articulate a bit better.

I guess my question here is, well, one is a kind of more pragmatic question is about like, so where do we cap off the structured representation as a whole?

Like that, you know, where do I end in some sense, but we can leave, we can park that for one second.

I guess my point is you say structured representation as a whole gets its content by resemblance.

The same need not be true of its parts.

These get their content via the overall structural resemblance, but the part-part relations themselves are not resemblance relations.

So let's say the content of the component part gets that content because of the overall structural resemblance.

But my problem here is, okay, of course I can go out and do that and that works for me.

But the problem would be then embedding that in a hierarchy where you say,

that that part itself constitutes a holistic structure.

Because you say, we do not mean to rule out that parts may also be structured and function themselves as structural representations.

Right?

Yeah.

So but then that has its own holistic identity.

Yeah.

because it's by which its component parts form their identity according to this model.

So I guess I'm just, I can imagine there is a way out, but I guess I'm slightly confused about how something can't have its content fixed

unless it is part of a structured whole and yet is providing a kind of structured whole for subparts, unless those subparts in some ways are also contingent on the overall structured whole and we just have a hierarchy of identity or meaning.


SPEAKER_01:
So, yeah, so my current understanding of this, I think it would be great if...

It would be really cool if you could cash out the content.

So let's say I have a generative model, whatever constitutes it, some internal structures, I would say.

if we could sort of show what the internal relationships are between that model and lower scale models of individual neurons, for example, such that the contents, let's say experience, look, I'm not attributing consciousness to neurons necessarily.

I think you can divorce consciousness from representational content to some extent, even if you're an internalist, right?

I still think you could talk about

Maybe this is part of my deep idiosyncrasies of the way I was trained in philosophy, but I'm comfortable thinking about unconscious thoughts at the personal level and such.

I think you can talk about the content of my states, divorcing it somewhat from consciousness.

It would be really cool if the content of my current thought had some intrinsic relationship to the content of the thoughts of the neurons, right?

But I don't see that that has to be the case.

And that's my answer to you is just whatever lower level structure it is.

And so, yeah, again, this structure could play a role in the overall structure.

Let's just say it's my central nervous system that's the relevant vehicle.

I don't know if that's right, but say it's that.

It would be in virtue of this thing would be basically an atom in a sense with respect to that level of description.

All that would matter would be, and this is definitely consistent with the hierarchical active inference point of view, what would matter would be its Markov blanket.

Its internal states can't matter functionally to the system at that level.

Because any internal states that produce the same input output profile, the same interface would do just as well.

Nevertheless, that thing might be a whole whose parts map onto the parts of some external things.

It's just that that would be a different system.

That would be like a system at a lower level.

I guess one upshot of holism, it's a bit of a hermeneutic type of picture where the content is sort of relative to the, or it's associated with the system that you're describing.

And so the fact that this thing is a holistic vehicle for content,

isn't inconsistent with its being just a part at the higher level, because those are two different sort of content descriptions, right?

Content sub Alex and content sub critical column or whatever.

You seem very unsatisfied with that.


SPEAKER_00:
No, I'm not.

I'm not.

I'm not unsatisfied.

It's just a broader point.

It's kind of difficult to do these things in real time in conversation.

You need to sit down and draw something or have some time to think about it.

We can park that we can come back to it, we can we can park it.

It's a it's a fascinating idea.

And I think it's also really important for active inference generally, and predictive coding.

Because you can have this kind of very coarse grained idea of a hierarchy, whereby the top parts contextualize, constrain, determine the lower parts.

But that kind of once you get into the granular level, and the philosophically rather vague point, which is

you know, priority, you know, significance hierarchy priority is a spectrum and what you know, what point we can cause these strict divisions, then I think these questions start to matter, which is about these part of relations, which actually leads me very nicely into predictive coding.

So, yeah, predictive coding in some ways, comes along with active inference and the Bayesian brain hypothesis, I think people have

started to be a bit more strict about distinguishing sort of continuous state space, discrete state space, and POMDP schema and predictive processing.

But predictive coding is still a very important part of the literature.

I really, really loved your paper where I got a complete history lesson in predictive coding, Helmholtz machines, Jeff Hinton.

I just thought it was great.

And people hear these

words and terms thrown around like Helmholtz machine, generative model, discriminative model, recognition model, and they probably don't know what it means.

And in a sense, they they're relying on an authority like you to hold the meaning.

So I thought we could kind of unpick that not only for the audience, but also for myself.

Because I take some of these things just as historical fact.

So perhaps we can start with the distinction between a generative model

a discriminative model and a recognition model?


SPEAKER_01:
Sure.

So with the caveat that I am also, this is also for me because refreshing one's own memory is always a good thing.

Sure.

So I'd say let's start with generative versus discriminative model.

So this is within, I'm already assuming that we're talking about probabilistic models here.

So a generative model is just basically a joint probability density over some variables of interest, right?

A discriminative model, if you were going to try to cash that out in terms of probability distributions, would be a conditional distribution.

So if you have some input x and some label y,

this model would be a model of the probability of y given x, right?

And so you can use this to, you know, these things are typically used in, I guess, discriminative types of tasks like labeling images or something, classification.

Generative models are just much nicer and more powerful in many ways.

I think there's all sorts of... I don't need to rehearse all the arguments in favor of them, but... No, don't.

You don't need to.

Don't worry.

From a generative perspective, instead of... So a discriminative model used for image classification might give you, yeah, what's the probability of the label cat given this input image?

Whereas you could instead set up a generative model that jointly models what's the joint probability of labels x and images y. And it could be based on some conditional, some latent variable.

And then you would just invert that model, given you can.

And it's a nice thing about paradigms like this.

And predictive coding architectures are very good at this.

You can supply an image and invert the model and predict a label.

Or you can supply a label and predict an image.


SPEAKER_00:
And this takes us into the recognition model.

People might be familiar with the idea of a recognition density, but maybe not a recognition model as the inversion of a generative model.


SPEAKER_01:
Yeah, so this is something where I need to, if I play too loose and fast with this distinction, then people like Karl and Maxwell will take me to task for it.

This is one thing that has come more clearly in focus for me over the years, which is that, so the Helmholtz machine, this was my introduction to all this stuff computationally was just coding up one of those and trying it out and learning about its properties.

And so in that model, you have an amortized, a learned frozen recognition model, which

basically it's your trained model for inverting the generative model.

And so by inverting the generative model, of course, I just mean given some input, you want to infer some latent state that represents the hidden state that would have caused that input.

So obviously doing that in general is intractable using naive Bayesian inference.

So you want some

You want to use, for example, variational inference.

Great.

So the Q distribution, which is your approximate posterior.

And the recognition model in the Helmholtz machine is just a trained amortized.

parameterization of the recognition density.

So you give it some input, you have a set of bottom-up weights that can be distinct from the generative weights that just map into the latent space.

So that's your recognition model because it's used for recognition.


SPEAKER_00:
Yes.


SPEAKER_01:
Yeah.

And so from a broader predictive coding cognitive scientific perspective, you might want to say, well,

Clearly, organisms like us implement some form of recognition model.

Let's on the Bayesian brain hypothesis, right?

Let's say that we have generative models that go from like things like concepts to arrays of simulated sensory data.

You need some way of getting from the actual data you receive to the concept back to the concepts.

And think of that as in a broad sense of recognition model.

The reason I said I have to be careful here is that you don't need a

amortized mapping in order to do this in predictive coding architectures.

You can just run the thing and do iterative inference, try to minimize the free energy over all the nodes in the network given an input.

And so you have a recognition process without necessarily having a pre-trained recognition model.


SPEAKER_00:
Right.

Because the Helmholtz machine, its learning happens in this kind of so-called wake-sleep cycle, right?

which I think some people have linked to human sleep, but I won't go there.

I guess a point of confusion for me, now I'm not a computer scientist, a coder, et cetera, so you are going to have to be patient with me, is the generative model is talking about these probability densities, let's say just over two variables.

Now, that seems to me that it doesn't necessarily prioritize one variable over the other like a discriminative model would.

Which sounds to me that then inverting it doesn't really make sense because you haven't got a starting version.

So is the generative model, when we talk about a generative model in predict, uh, predictive coding or active inference, the probability, let's say the probability of a state, uh, or what the probability of an observation given a state, let's say that sounds more like that very, you know, the line and basis formula sounds like given your explanation, like a discriminative model.


SPEAKER_01:
Yeah, so thanks for the opportunity to clarify this.

So in a discriminative model, my paradigm for this is some feed-forward neural network where you've got image layer as input and layers processing and label layer as output.

If I were to try to train a Helmholtz machine, for example, to do that, instead I would have

image and label sort of concatenated or you could think of them as two distinct modalities whatever you'd have these on the bottom layer and you'd build whatever however many layers you want on top of that and so when i talk about inverting a generative model so i kind of implicitly assume that the generative model uh has latent variables in it

And so the latent variable is often called z or h or whatever.

So you supply your input.

You do some inference onto this latent variable.

And that's your internal representation.

That's your internal states in an active inference idiom that would represent the external states, the hidden external states.

So instead of a mapping from the one, let's think of labels and images as two observation modalities, right?

Instead of constructing a mapping directly from one observation modality to another, in this kind of model, you would have a mapping jointly from the observation modalities to this hidden latent variable.

And inverting the generative model is inferring the latent variable given the observations.


SPEAKER_00:
I see.

So it's kind of embedded in a way.

It's kind of, you're discriminating something, but within that itself is overarching, the generative relation.

Does that make sense?


SPEAKER_01:
Yeah, yes, exactly.

So you could do discrimination, right?

You can always get these conditional distributions from the joint distribution.

So this is a way of... That's what's great about generative models.

They do more.

I mean, of course, they're harder to train.

It's more information that you need to model.

But if you have one, then you can do discrimination by clamping a label, say, inferring a latent state, and then generating the other modality.


SPEAKER_00:
Cool.

Excellent.

Yes.

Okay, perfect.

So the Helmholtz machine is what, 1995, 1996.

And then Friston's predictive coding, I think was introduced first in 2005.

But there were probably hints of it earlier than that.

What are the differences between the two?

Actually, I'm not going to pit.

Mr. Diane over Mr. Friston.

So I'm going to leave it.

I'm not going to try and ask for a comparison.

Let's just ask for the differences between the two because people probably hear them aligned but don't know how they're distinguished.


SPEAKER_01:
Yeah.

I mean, I think a lot of this stuff, I think Carl and others were working on these ideas and I think a lot 90s were just there's amazing stuff going on.

So one difference I already mentioned is that in the Helmholtz machine,

you explicitly are training a bottom-up recognition model and as you said this is usually well originally the first proposal for this was awake sleep algorithm where basically you would you'd present an input propagate the activities given the current weights and then train the generative weights to be more likely to generate that then you would generate a fantasy top down and retrain the train the recognition weights to

to produce that when you get an input.

And it was a way of bootstrapping two models at once, so the generative model and the recognition model.

Later on, and I don't know as much about this, but in discussing this with colleagues, it's come to light that

Something that worked better was to just always use the bottom-up activities to train both models.

So this is called the reweighted wake-sleep algorithm.

So that's a tangent.

Don't need to get into that.

But the Helmholtz machine didn't work all that amazingly.

It was an amazing proof of concept.

I always thought of it as like a minimal mind because it had all the basic ingredients you need, sort of imagination, perception, and so on.

Maybe not action, but we can .

So that's one big difference.

Whereas in a predictive coding network, you've got some vector of current hidden state activities.

You get an input.

You make a prediction, or you send a signal up to the hidden layer.

This is the prediction error.

And the prediction error is computed with respect to the current activity.

you update the activities so as to minimize the energy term, which is just additive across all the nodes.

So there's no recognition model.

You can just use the generative weights.

You just use the inverse of those to map to the hidden state.

And so you're not training two distinct models.

If you're training a model like this, you would just train the generative model by minimizing prediction error.

Other differences.

So in the Helmholtz machine, there isn't really any

there's a certain abstraction at work in that architecture.

So I always intuitively thought of this as modeling the fact that we know from neuroscience that the same neural hardware is used for perception and imagination, at least largely.

overlap.

And so the Helmholtz machine models that because you use the same units, the same artificial neurons, to do the bottom-up sweep as well as the top-down sweep.

But those two information flows never interact.

You've got a wake phase and a sleep phase.

And when you're running the model in generative mode, you only use the top-down information.

And so the authors, Hinton and Diane and all, pointed out that this is not really ideal as a model of perception because we know that there are top-down effects during perception.

So you don't really want just a frozen, amortized mapping that propagates forward.

You want to use the top-down information during perception.

And so predictive coding has that feature.

So if you could just clamp a bottom-level feature vector, whatever the input is, and the predictive coding algorithm

will be integrating information from the priors like sort of online in order to perform the bottom of inference in effect, right?

So you've got, oh, you've always got predictions coming down from the top and prediction error coming up from the bottom.

And these two sources of information are integrated online during perception.

So it's a much better model in that respect.


SPEAKER_00:
Maybe we can also feed in this idea of, um, horizontality in predictive coding.

Um,

I actually came to this through Andy Clark's paper as well, whatever next, because he makes quite an explicit point that it's not just verticality, it's horizontality.

And almost that horizontality, if I'm not wrong, is really kind of where the inference is happening, right?

It's kind of the selection of the best hypothesis.

So maybe you could do a better job than me and just explain, you know, what,

What does horizontality introduce to this picture?


SPEAKER_01:
So I don't know if I am sure how Andrew Clarke used that word.

I assume you mean like lateral connections?

Yes, lateral.

No, no, it's fine.

So, right.

So definitely there's... Well, so whether or not you have...

lateral connections in your model right there's other ways of achieving that you can have sort of diagonal influences like you you know use the representations of this layer propagate information upward and then there's a there's a sort of recurring feedback loop where this layer gets modulated again but you in any case you get this sort of

I don't know if it's always winner-take-all, but sort of winner-take-all behavior where one of the hypotheses at this level is selected.

And some of the earliest predictive coding models were literally, yeah, were not even hierarchical models.

They were just like to model, I guess,

efficient information processing and retinal ganglion cells, I think, where you'd have like, the receptive, so you'd have a neuron here, and it's input, it's output, it's spiking activity would be modulated, sort of dampened by what was going on in its vicinity.


SPEAKER_00:
Right?

Yeah, yeah, yeah.

You make Yeah, you made that point, I think, in the paper where you Yes, you talk about the kind of center.

I'm butchering.

But yeah, that's like the early 80s, I think.


SPEAKER_01:
Yeah, yeah.

So that was one of the first applications of predictive coding was instead of prior information, possibly at a very deep level, right, from like, whatever evolutionary priors or cognitive priors, it's just like, well, we sort of embody the prior that this cell only needs to fire if what it's doing is different from what would be expected given its neighbors.


SPEAKER_00:
Yes.


SPEAKER_01:
So, yeah.

And so, and certainly, and one of, I think, Carl's earliest paper on this, or one of the earlier ones I'm familiar with, it's called something like a theory of... Cortical responses.

Cortical responses.

Thank you.


SPEAKER_00:
I've memorized them all.

Not that I've read them, but I've memorized them.


SPEAKER_01:
That's what you got to do.

I need to up my game on that.

No, no, no.

So yeah, he discusses the role of lateral connections.

And so it's sort of a mechanism for competition in a way.

It's another way of reducing sparsity, right?

So I've played around with...

versions of a Helmholtz machine.

I don't know what you call this, but I just added some lateral connections to the Helmholtz machine because it doesn't have those.

I mean, well, you break certain theoretical guarantees if you do that.

But anyway, and what happens is you get an increasingly sparse representation because of this sort of competitive potentially behavior among the units at one level.


SPEAKER_00:
Yeah, that makes sense.

I guess...

I feel like an issue that I've always had with predictive coding at a very, very general level, and this is definitely more indicative of my intellectual limitations than the actual theory's limitations, is it's very difficult to be able to say at a snapshot in time what's going on.

I think over a diachronic, diachronically, it kind of makes sense, right?

Like I get these bottom, you know, you get these bottom up prediction errors, which in some ways lead to model changes at very, you know, much higher levels, but still those higher levels constrain the expectations at the lower levels.

But I guess it's kind of like where it's difficult to picture as happening all at the same time, because it's,

prediction error trickling up, but the predictions that are going to deal with those prediction errors already coming down.

So why are those prediction errors coming up in the first place, right?

If they should be getting suppressed by the things that are coming down.

And maybe some people skirt this problem just by adopting some kind of linear time sequence in their minds.

I understand it's all happening at the same time, but they go,

oh, like this, this level can't deal with it.

So it goes up and eventually you get the prediction and it suppresses it.

I've always just found that to be very problematic, because who is saying at which level the prediction error stops, right?

It just seems like it's a kind of miracle.

So maybe you can help me.

Or maybe you can't help me.

Is this an intellectual?

Is this an intellectual limitation of my own?

Or is this something that's come to mind before?


SPEAKER_01:
I think that can help you to some extent.

I don't know that it's an intellectual limitation.

I don't see any problem.

I tend to think about this stuff sometimes in a quite visual way.

I just imagine this stuff working and then also I try it out in code and it works.

I would like to give you a more discursive answer though.

um so yes indeed if you try to so and i've definitely built models um there's david heger has a paper also on it's a similar title to carl's paper it's a theory of cortical something uh where he has a very similar model and

Yeah, so in implementing this, it's like, well, what do you update first?

First of all, I'm like, how do you, right?

So I think the solution to all that stuff, part of it is, and I thought a bit about how to build predictive coding architectures is just do the updating asynchronously, which if you want a little bit of...

background to that, it's basically like each neuron, each model neuron would just be its own sort of process.

So we can go deeper into the computer science stuff if you want.

But basically, each neuron would be running in its own sort of processor thread.

And it would take inputs and operate them as it gets them and then produce outputs as it has them to produce.

And all of this would be going on simultaneously.

The idea that you need to first get all of the input to a layer, the prediction error, and integrate it with the top-down predictions and figure out how much information to pass on, in practice, they're just not issues.

And even without going into the implementation of this stuff in terms of parallel distributed processing, you can just think about the energy function.

So the free energy function that you're trying to function on, that you're trying to minimize,

it's it's additive across the nodes so each node computes its free energy so whenever it gets an input whatever its current state happens to be um right you just you just modulate its output based on that um so i guess and i guess i imagine this happening so it's sort of chaotic i suppose right it's um you would just basically each each each neuron could locally process its own

its own input and produce an output.

And, and then, you know, if you're simulating this on a serial system, you have to choose when to do what.

But that's not terribly central, I think, to the architecture.


SPEAKER_00:
Yes, I guess I guess the problem there is, if every neurons just doing its own thing, and over time, you know, over time over the sequence of neuronal activity, prediction error just gets sliced off, right.

And so you get this additive minimization of free energy.

can run a philosophical thought experiment and just go, well, what happens if the total amalgamation or accumulation of all the neurons relevant to this activity, they just can't get rid of all that prediction error.

But we're siphoning off learning, right?

So we're siphoning off parametric updating because we're saying that's happened asynchronously.

I'm just left with a bundle of prediction error.

And you always hear, well, that's just going to end up being minimized.


SPEAKER_01:
So your prediction error is never minimized, right?

If it were minimized in the sense of life would be over, you would be dead.

Yeah.

It's an interesting question to bring up.

What if there's just prediction error that can't be handled?

Is it just going to be this oscillating

chaotic system where the prediction error just bounces around?

Yes.

That's what would happen.

You just shunt around the prediction error.

And I've seen this happen in models where it just doesn't converge.

It's not a stable state.

It's maybe a metastable state or something.

um there's or there's oscillatory behavior or something um which Carl would say is is important right on to life on larger time scales yeah um yeah yeah so everything is always sort of smooth so it's it's yeah well if you ever get a probability of a one or zero um in these models and I'd say in real life as well yeah yeah yeah exceptions like there might be some

we can get this, we go straight into like crazy amounts of mysticism.

Maybe there's some kind of fire of one of like existence or something, but like when it comes to... I hope so.

I think so, actually.

But yeah, in terms of the probabilities that are represented by your model, like it's never going to be no prediction error.


SPEAKER_00:
No, I think that's a valid point.

And I guess once you fold in the learning, whether that's parametric learning or structure learning...

that prediction error that was so grave at one point is eventually going to get resolved one way or another, whether that's through... I find sometimes when I run into this problem, just anecdotally, I find myself going towards a variational free energy equation, just being like, there's my safety.

Because through either action or perception, we're minimizing

you know, we're going down those descents of free energy.

Okay, you've, you've, you've, you've quashed my anxiety, somewhat.

One other sticking point I have with active inference.

And this really came to light actually, a couple of weeks ago, and I spoke to Ronald sladky, who's a neuroscientist at University of Vienna, and he's written, he's written, it's not what he's written about the amygdala from an active inference perspective.

And he does this really nice thing of distinguishing between

kind of, I think it calls them preference priors.

You can call them phenotypic priors, evolutionary priors, whatever you call them, but prior preferences.

And then also what I've been calling state priors, right?

So literally what state do I expect to be in now, right?

And you can have that as your D matrix or your B matrix if we're doing POMDP schema.

And then also, and that obviously feeds into perceptual updating just on like some, if we have just like naive bays, I know it's not happening like that, but let's just say.

But then you also have these preference powers.

In many ways, as Carl has made very explicit, that too is shaping your perception, hence why we have visual illusions.

I just find in the literature there hasn't been too much work gone into distinguishing those and also talking about that interplay.

For you, does that just come out in a hierarchical system?

Do those things converge on being the same thing?

We just call them a prior?

Or is it useful to continue to distinguish them?


SPEAKER_01:
Yeah, that's a great question.

I mean, ideally, for me, it's the former.

It's that they come out to be the same thing.

So if you're doing POMDP modeling, of course, yes, you'll define a C vector, which is your preference.

And then you might also have a D vector and so on.

But I think, ideally, in a hierarchical system, it should be that you just have your priors.

And to me, what the phenotypic

distribution is or the deep priors that represent desires and homeostatic requirements.

To me, that's just when your whole hierarchy links into something that's at the root of this graph.

So you have a basic need for food, let's say.

that's not going to change, right?

You can't really rewrite that or learn your way out of it.

And then what you're modeling at the lower levels could very well be a situation that's not consistent with that prior being satisfied.

But they're still just priors, right?

And really what you want is that the priors at that deep level, which are constitutive of and defining of the kind of thing you are, you want those to

smoothly propagate to the periphery for everything to work out well.

But of course, what you're currently representing at the lower levels of a hierarchy, internal hierarchy, is going to be the result of variational inference, right?

So I think of, I mean, there's some talk about this in the later sections of that paper that Maxwell and Ryan Smith and I wrote on beliefs and desires, where

basically the Q distribution is sort of your beliefs in the bare bones version of the picture.

Yeah, that's your estimate of the current state of the world.

It might be arbitrarily different in principle from your desires, but still, yeah, and the priors are just the generative model itself, the joint distribution that defines this kind of thing that you are.

So I think that...

said a lot.

But I think basically, the distinction between two parameters C and, and D in a hierarchical model, which is a way that you might set some of these these priors, I think that that's more of a modeling convenience than anything.

I'm not sure that everyone would agree with me on that.


SPEAKER_00:
But it's really interesting.

And it's really useful explanation.

I mean, yeah, once you have them in a kind of smooth hierarchy, you can start to kind of visually or mathematically see how it might all sync up.

I guess the issue is in folk psychology, there's an intuition always that beliefs, preferences, let's say, sorry, desires, preferences are distinct from my beliefs about what's going on in the world right now.

And I guess that's held in some sense because there's a hierarchy.

It is, yeah.


SPEAKER_01:
I wouldn't say that this eradicates the distinction.

I understand the pull towards that in some of Andy Clark's work where he's... I think it's not clear that he wants to preserve this sort of classical distinction between beliefs and desires in a way.

So I understand the pull towards that, but I do think even though your desires are these priors, which are belief-like in a way,

Because to me, they're really not, right?

So the priors are certainly, they represent something, right?

And so if their desire is then, in the classical philosophy of my way of putting it, they have satisfaction conditions, which are a lot like truth conditions.

It's just that it's a different propositional attitude.

know they're in the same the representation wise they're in the same format it's a different attitude to the same kind of content yes and then the result of posterior inference is a belief and so the same

structure cognitive or otherwise i send i tend to think that there's a very transparent relationship between cognitive and physical sort of implementation structures anyway the structure can as considered as opposed to the results of posterior inference it's a belief um but then considered uh in terms of its downstream effects in a top-down generative model it sort of functions as a desire so yeah things in the system yeah yeah i mean i wonder whether how


SPEAKER_00:
Yes, the hierarchy is a, is a really useful way in which that, I guess, again, philosophically vague distinction can be made concrete.

And obviously people also talk about the temporality aspect of the generative hierarchy.

And so far as the higher levels tracks, slower fluctuations in the external dynamics and the lower levels, the faster dynamics.

It makes one think whether that necessarily maps onto desires, preferences, and so on, only being at the higher level there, i.e.

being the result of slower fluctuations, right?

Like I had ice cream.

I've had ice cream a thousand times.

I like ice cream versus a belief that there is ice cream or that there isn't ice cream.

It's an interesting question.

It's an interesting question whether beliefs, sorry, well, desires, let's say, necessarily need to originate at the higher level and whether sort of more classical folk psychology beliefs are lower level.


SPEAKER_01:
Oh, can I jump in on that?


SPEAKER_00:
No, no, no.

I wasn't just spitballing.


SPEAKER_01:
Please, yeah, yeah, yeah, go.

I really like that.

So I think Andy Clark also talks about this in one of his recent papers, more recent papers.

So

there's a sense in which you really can talk about sort of like false desire and versus more genuine desire that really stems from your being in that I think sort of genuine sorts of desires

are from the deepest levels of the model.

But then in cases of like addiction, what gets happens is yes, right.

So basically, there's a bottom up sort of an attractor sort of manages to establish itself in your system on the basis of bottom up input that creates a sharp expectation over a particular outcome that might be

not at all a good fit with your deeper priors, but that just gets self-confirmed so much that it becomes a rigid feature of the model.

And so that's, I think that's a perfectly possible active inference account of addiction.

And there's a desire there that will, in that it motivates your behavior and you will go for the thing, but it's definitely deeply, obviously, it impoverishes the rest of the model.


SPEAKER_00:
Right.

Well, that's really nice.

Yeah, that's really, really nice.

John Verveke is a

very you know big hero of mine talks about reciprocal narrowing and reciprocal opening with respect to addiction i think this isn't his direct work i can't remember whose it is but the idea being that as you say addiction is a kind of downward spiral such that and you can just think about in terms of dopamine um such that a singular source becomes your kind of

source of pleasure in the very folk psychology way, but also like your singular prior and everything becomes determined by that.

And maybe, well, he, you know, he will use the word like agape, but you know, you have this kind of opening, maybe Heidegger would call it ala fea, where you have this kind of disclosure and that actually opens yourself up to the world.

And I guess, yeah, maybe it is a bit mystical, but I wonder whether in doing that, we're uncovering something which is actually, was already there in a way.


SPEAKER_01:
I mean, absolutely.

Sure, there's a mystical take on that, but it's also just absolutely the case that you want as much variability in your states as is consistent with preserving the phenotype.

And so that's something that, for example, we've written at Mao and Maxwell and others have written on that resilience, the work on resilience.


SPEAKER_00:
Yes, and Mark Miller.


SPEAKER_01:
Yes, thank you.

I've been doing a lot of things, so I haven't thought about that paper in a while, but, uh, but yes, like, and, and that's just comes out of just very, I think just variational inference, even like you just, you just wanna maximize the, the entropy of your distribution subject to the constraints from your model.


SPEAKER_00:
And yeah, Ryan, who you mentioned earlier, he sort of has this idea of, um, structure learning aiding emotional granularity and that being really useful because you don't get trapped into thinking that you're furious if you're actually maybe just a bit frustrated or, and, and, and so on.

It's all, uh,

complexity minus accuracy at the end of the day.

Yeah, I don't know what I've had this idea.

And I'm thinking maybe writing a paper on it.

But I there's, I think a really nice addition to this whole picture is the idea of selective attention.

So Beck Mitz, who I think was in Carl's lab back in the day, he wrote a bunch of papers with Carl about attention.

And he has this 2019 paper about selective attention.

I've spoken about it on the podcast before.

And the idea being that you're

priors, your preferences constrain your attention.

And that kind of make that's very sort of in sync with traditional bias competition theory, right?

And it's actually stuff I wrote about during my masters.

The idea being that, you know, what is elevated in terms of gain control aligns with is actually in line with your goals.

So if I'm looking for a ginger person, then all the templates of things that are orange or ginger stand out to me.

And I've been having this idea that we can actually route that selective attention to the very deepest level of our belief hierarchy, such that we kind of do have, it's very Jordan Peterson, actually, because it's very sort of narrative-y, but we do have these kinds of deep, like what, like we have basically a guiding principle.

And that could be,

I tweeted that thing that you liked, which was Simone Vale, attention is prayer.

Influenced by Ian McGilchrist, attention is a moral act.

And I think that's really powerful because attention is basically a phenomenological indicator of that which you are guided by.


UNKNOWN:
And I think that's really powerful.


SPEAKER_00:
So I'm thinking maybe again, it's a, I'm putting this out there, so no one steals it, but I've got this, I'm trying to think about how to formalize this idea that like the very deep.

Temporarily invariant parts of our being might actually be the guiding force of our attentional schema.


SPEAKER_01:
That's, that's wonderful.

I hope you write that paper.

Um, I, I haven't done so much research on attention as such, uh, in, in the psychological literature and such.

So it's, it's, it's really cool to hear that what you've researched confirms this kind of perspective.


SPEAKER_00:
Well, hopefully, hopefully someone doesn't come into the comment section, you know, all those common sections, but people always have things to say.

Um, this might be a slightly selfish question, uh, but I don't think it is because I think people also benefit from this.

You say that I wanted to kind of go back to your.

journey as an academic, if you will, and starting off as philosophy and then Andy Clark, and then sort of finding modeling.

So what really helped was actually modeling a Helmholtz machine, for example.

And you hear this a lot, you know, if you want to learn how to do POMDP stuff, know how to do the MATLAB work.

The question I've always had, like, I haven't done any of this coding.

What does it look like?

I mean, when you say I code a Helmholtz machine.

What do you get on your screen?

I know that sounds really stupid, but I think people are kind of confused about what that looks like, especially when Carl talks about, oh, you get these little agents who are doing things.

What does that actually look like?


SPEAKER_01:
Yeah, no, it's not a stupid question.

It's one of the problems you have to solve if you want to do this stuff, right?

So what I did, I mean, this was in the relatively early days of this stuff in broader academia in the sense that unless you were doing machine learning, you probably didn't usually care about this stuff.

So what I did was I started writing it in JavaScript.

I used dynamic CSS to render this as a webpage.

I'll have to send you some link.

I'll post it on my Twitter or something.

Basically, I had this very early.

The reason I never shared it so much with the world at the time is that it didn't work very well at all.

But I basically had a Helmholtz machine visualized in a browser.

And it was like, you just have some squares for each neuron.

In the hidden layers, you'd have these sort of static-y looking patterns, as you'd expect.

And then you'd visualize the output layer as an image.

And I sort of overlaid the...

the input, the ground truth input image with the prediction using like red and blue or something.

I got those color codes come from like Jeff Fenton's work and a lot of people have used them or red and green maybe.

But anyway, so to me, I really needed this concrete visualization to understand what's not.

If you run Carl's MATLAB code, it's much more like you just get the output.

I mean, that's a fine way of working.

It's worked well for many people.

You see the belief distribution change over time just plotted on an axis based on various conditions.

But so, yeah.

So in any case, I mean, you, I think you, you need to set up a visualization that makes this stuff intelligible.

I feel like, I don't know if that was the heart of your question, but no, no, no, no.


SPEAKER_00:
I'm yes.

I'm just kind of, I guess people always ask like, what, what are these, like, what are, you know, when we talk about in silico agents, right?

Is that a thing like really, or is that just a metaphor?


SPEAKER_01:
I mean,

I kind of hope it's a metaphor because I just, you know, kill thousands of them a year if they're real, you know.

Oh, yeah, yeah, yeah, yeah.

My browser window and it's just gone.

But no, I mean, I think to the extent that you can model to the extent that you think these types of models are models of agency.

Then it's real.


SPEAKER_00:
In your browser window, what do you actually get?

You know what I mean?

What do you see?

When you say agent, I think people naturally think of little gummy bears or little teddy bear looking things.


SPEAKER_01:
Well, so the Helmholtz machine then is a bad example because that's really just doing sort of it was really just showing online learning of like,

generative model and it doesn't have an action component to be interesting.

But I've seen tons of visualizations of active inference models and other similar models.

So if you look at some of the machine learning work on learning to play video games as an example, so that's a strange case, but if you have a model that's trained to play Super Mario Brothers, then

I mean, you see the effects of the agent anyway.

You can just visualize.

It depends on your problem, right?

There's plenty of sort of grid world type, simple proof of concept type active inference models that get made where you can see the location of an agent on a little map or something.

I love that.

Yeah.

But typically, honestly, I mean, just as a, as a matter of like everyday sort of, um, working reality, it's like, usually you'll get a series of plots of the police belief updates over time.

Um, and, and the action that was chosen.

Right.

And that works.

Yeah.

And it's an ongoing struggle to find better, more powerful visualizations for this stuff.

Sure.


SPEAKER_00:
Sure.

I imagine it is.

Okay.

Sort of just getting, you know, wrapping things up.

You've gone from philosophy to.

computational science, let's say.

And the kind of contentious bridge there, let's say between phenomenal structures and mechanical explanations or mechanical formalisms is often called the generative passage.

Started, I guess, by Varela and then formula or that term instituted by Maxwell and his team in that 2022 paper.

I think I get this from a lot of philosophers who start off as philosophers is the kind of non-reducibility of intentionality, the non-reducibility of consciousness.

What do you think of the generative passage?

And what do you think is the sort of most, I won't say likely, but the most adaptive or effective manifestation of it?

And by that, I mean, some people talk about ontological naturalism, which is we could

basically positive, reductive approach, and epistemological naturalism, which is that the way that we come the way we think about or describe, let's say, in this case, Bayesian dynamics is going to be helpful, or actually, in that case, it's not helpful, it actually does describe

the structures, the intentional structures, or a kind of procedural or more, you know, mechanical way where it's actually just useful, right?

It's kind of instrumental.

But actually might not be telling us anything too concrete about the phenomenal reality.

Now, since you kind of flirted in both camps, where, where do your allegiances lie?

What do you think?


SPEAKER_01:
Yeah, no.

So as far as the narrowly the term generative passage, I stopped me if you think I'm misusing that term.

I haven't been deeply involved in thinking in that term.

Sure.


SPEAKER_00:
I think you're talking about the relationship between phenomenology and in this case, Bayesian dynamics, let's say or you Well, I mean, people talk about neuro phenomenology, right?

So it could be neuronal dynamics.


SPEAKER_01:
Yeah, yeah, great.

So yeah, so my overall, so there's a lot of things rolled into that question.

Epistemologically, I kind of see things as, and it's good that you brought this up.

I wanted to comment on this earlier.

So another paper that I haven't published is on this topic.

So I would be a transcendental idealist, except that the evidence that I've seen for that position is empirical.

So probably someone said something similar.

But I want to write a paper on almost empirically informed Kantianism, where it's like, well, what we've learned about how these systems work

um provides evidence for the conclusion that the our experience of reality is is mostly based on uh prior scriptures in the brain right um so i think that's really interesting epistemological it's different from sort of classical transcendental idealism where you just had kantian categories right um because you're

You're sort of presuming to say something.

There's a sense in which the naturalistically studyable reality comes first there because you're saying it's like, well, the way neurons work, we probably encode these generative models in our brain, right?

And that's why we conclude that we have subjective ontologies and such that we learn in order to parse out the sensory data.

So overall, that's my sort of view on the epistemology of it.

In terms of the relationship between phenomenology and cognitive structures and physical structures, in my work, I try to argue for the most minimal account of that relationship where transparency is the ideal across all those goals.

So of course, that's not going to work perfectly.

If I see a dog on the street, it doesn't mean there's a dog in my head.

It doesn't mean that it's something

it does mean that there's something whose dynamics reflect the dynamics of that thing out there um if there's a thing out there as we said earlier um but but let's for a second let's just think about the relationship between the phenomenology and the cognitive structures right um um

so yeah in the psychophysical identity paper i also i cite some work by the gestalt psychologists who really assumed that there was a strong isomorphism there um that you really in a sense if you if you looked closely enough at the um well i i sorry i was i was shifting to the physical level of description again but certainly that there you should be able to sort of re understand the structure of phenomenology in terms of um

terms of the structure of these Bayesian prior structures, basically, and they get updated by sensory experience.

And so I totally agree with Maxwell at all on that.

One reason I haven't been more gung-ho about computational phenomenology is just that I already assume something further, which is that... So their research program, as I understand it, is let's sort of step back and just try to build a generative model for phenomenology and understand

how you can understand phenomenology in terms of generative modeling, I think you can just shoot straight through the cognitive structure and just talk about isomorphism between the phenomenological structures and the cognitive structures and the implementation basis for those.

Interesting.

So I guess it's good.

It's epistemically humble, and it's a good project to try to do that.

I just find that it's more effective for me to be constrained by more

Usually when I think about this stuff, I'm constrained by thinking about neuronal implementation level, as well as cognitive level, as well as phenomenological.


SPEAKER_00:
I was hoping you were going to be looser.

Oh, well.

I was hoping you guys, there might be some link.

You're entitled to, of course you're entitled.

I mean, I'm personally, I mean, I've written about this a little bit publicly.

Ontologically speaking, I'm quite agnostic.

I have maybe a slightly romantic constraint, which means that I just don't want to do the reductionism.

I just don't.

But actually, I should clarify.

I've actually got a paper in front of me.

Maxwell and colleagues make it clear that actually the so-called ontological naturalization doesn't necessarily entail reductionism.

Only that, and I can quote here, some theory or framework means that one reformulates all the properties, entities, and processes postulated by that theory in terms of those posited in the ontology of the empirical sciences.

And how that gets then fleshed out in terms of epistemological naturalization is that you would take the core principles and forms of your explanation from the natural sciences, and then you would just have methodological naturalization, what I call procedural naturalization before,

which is bringing your sort of methodology into continuity with the empirical sciences.

So as you say, kind of constraining phenomenology, you know, according to the principles of psychology and neuroscience without saying that they are those principles or without saying that we should only understand them by those principles.

Yeah, it's a really, it's a really nice trifecta that and I guess there isn't an answer.

But you know, you have some people who are very adamant that

perhaps even the methodological naturalization is a step too far.

And we should just be as humble as possible with respect to the nature of consciousness and say, well, we just have no idea in what form it's grounded or in what form it relates to the physical universe.

But I'm probably not that loose.

But no, it's interesting.


SPEAKER_01:
Can I follow up with one more question?

Please.

Because I think it's a really important point.

So I don't think in reductive terms about consciousness.

So even if it turns out that there's a perfect isomorphism between phenomenal cognitive and physical structures, I don't think that means that everything would be reduced to physical structures.

So I see more, I hope that these lines of inquiry would converge

So that you'd end up seeing that these are different ways of talking about the same thing, but it doesn't mean that one of those ways is fundamental.

Um, so yeah, I don't know if that is any different than what you just said, but, um, yeah, no, that's absolutely great.


SPEAKER_00:
Um, okay.

Final question.

Uh, actually penultimate question.

We can rush through these, uh, pan psychism.

Is it, is it, is it bullshit or is it.

Brilliant.

Sorry.

Only because you mentioned it earlier.


SPEAKER_01:
No, it's certainly not bullshit.

I don't think it's brilliant to just rush headlong into it without thinking about it.

But no, I think it depends what you mean.

Right.

But you assume we're talking about phenomenal consciousness.

I'm quite comfortable with the idea that any physical structure

There's some edge cases, like rocks, where it's like, well, it's kind of like a don't care case, as Quine would say.

It's an edge case where there's no structure to speak of.

I guess, to me, the natural view is that the kinds of contents you can represent consciously depend on the complexity of your structure.

But the base ground existence of sort of a field in which these things are represented, I don't see why that should be different from the basis for physical existence.

Good.


SPEAKER_00:
Um, final question.


SPEAKER_01:
Yeah.


SPEAKER_00:
You can only have one for the rest of I'm being trivial now, but you can only have one for the rest of your life.

Philosophy, computer science, which one are you keeping?


SPEAKER_01:
Uh, that's rough.

Um,

I was going to say computer science, because I think that the best computer science contains good philosophy.

But I think in terms of the intention rather than the extension of these things, I think philosophy.

Because you'd end up inventing computer science if you did philosophy well enough in the long term.

Fantastic.

I'm so glad you said that.


SPEAKER_00:
Which one's harder?


SPEAKER_01:
Computer science is way harder.

Is it?

Fuck.

I mean, sorry, if hard means like it's hard to do, it's like a struggle, then yes.

But if you mean like... Cognitively, theoretically...

Oh, then philosophy.


SPEAKER_00:
Yeah, we'll have to discuss more about about all of this stuff.

This was so much fun.

Alex, thank you.

Thank you for dealing with my layman thoughts and terrible attempts to try to understand things that are way beyond my pay grade.

But this was awesome.


SPEAKER_01:
Likewise.

Thanks, Darius.

This was this is great.


SPEAKER_00:
Cool.

All right.

Thank you everyone for tuning in as well.

Okay.
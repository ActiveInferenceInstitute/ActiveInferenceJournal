SPEAKER_01:
Hello everyone and welcome back to Active Inference Insights.

Today I have the real pleasure of speaking to Ryan Smith.

Ryan is a principal investigator at the Laurier Institute for Brain Research and he's the research associate professor at Oxley College of Health Sciences at the University of Tulsa.

His work focuses on the neurocomputational mechanisms underlying emotion-cognition interactions and how they relate to brain-body interactions with special focus on planning, information gain and interoceptive inference.

is also the primary author of probably the best known and most highly rated tutorial on active inference writing it alongside Carl Friston and Christopher White.

Ryan, thank you for giving me your time.

Welcome to the show.

Thanks for thanks for having me excited.

It's an absolute pleasure.

It's really very exciting.

As I said, I mean, obviously, I'd come across your work kind of inevitably for me to have invited you but it was actually a real treat to sort of go back and

go through what feels like you've done loads and work at this incredible velocity.

So it's been really fun to dive in and explore this, I guess what felt like a more empirical side, because I'm spending most of my time reading philosophy papers.

So it's great to see some traditional statistics and some proper empirical work.

So it's been great fun on my part.

I wanted to start at kind of a pretty broad level and speak about subjective wellbeing.

So you wrote this paper in 2022, sort of grounding subjective wellbeing in the parameters and mechanisms of active inference.

You alongside Kasper Hesp have come up with this really nice way of grounding wellbeing in expected free energy.

But as you write in that paper, you say,

While variational free energy can be understood to track basic biological fitness, this is neither necessary nor sufficient for an individual to report high subjective wellbeing.

So maybe where we could start for the audience is just explaining why the minimization of variational free energy is not sort of the simple solution to obtaining high subjective wellbeing.


SPEAKER_00:
Sure.

Yeah.

And I mean, it's definitely a very kind of complicated and complex topic to jump into.

And it honestly depends a little bit about the exact kind of formalism that you're kind of working in.

I mean, me, I almost exclusively work within the sort of like POMDP formalism associated with decision making and planning processes that...

that is the focus of our tutorial.

And the focus of the tutorial was very much like trying to get people to a point of being able to do true kind of experimental empirical applications.

I mean, that's the final section is all kind of meant to transition into that just because that's primarily what I do.

So I mean, with respect to the kind of

Yeah, your question here is that within the POMDP scheme, you know, variational free energy is primarily used as a means of optimizing posterior beliefs about states, right?

It's a state inference sort of process, whereas expected free energy is the thing where you're trying to say, okay, what do I expect the kind of rewardingness of the outcomes to be and how much information do I expect to gain, you know, conditional on choosing X versus Y?

So there is a true kind of valency thing in the expected free energy because you have this expected divergence or lack thereof between the things you want and the things you expect under a policy.

Whereas the variational free energy is just a function of the observations you're getting and how you update beliefs about the most appropriate approximate posterior over those states.

Right.

So in other words, you know, if I perceive that, you know, the observation I'm getting is that, you know, someone's like stabbing me or something.

Right.

Then the process I go through to minimize variation for energy, that that is what I use to infer that the best interpretation of what's happening to me is that I'm currently being stabbed.

Right.

So if the idea with variational free energy, as it is, at least in the kind of standard POMDP formalism, is a way of just getting at the best posterior belief about the state you're in, then that state could be something you really like or something you really don't like, right?

Sure.

So just when you see it that way, then there is this kind of big distinction between trying to

trying to maximize the consistency between your preferred observations and the observations you think you're going to get, that's just, that's valency and could relate to well-being in a much more direct way than just trying to optimize your beliefs about the current states you're in.


SPEAKER_01:
Right.

It's interesting that, yeah, you focus more on the sort of perceptual side of variational free energy, because at least the way that I've spoken about it and I've seen written and I've written is that it's kind of dual-wieldy in the sense that

variational free energy being this kind of upper bound on self informational surprise, or you can also minimize it by taking action.

But then that said, when we talk about action, we always frame it within expected free energy terms.

So I've been kind of thinking just, again, this is just a completely sort of nascent, not really fleshed out for is that action seems to do can do variational for energy and expected free energy minimization at the same time.

in the sense that if your sort of maximization of model evidence term, the minus log of the probability of Y, if you're doing that and also you're maximizing pragmatic value, it seems like you're doing both at the same time.

Is it kind of an erroneous way of thinking to align or combine expected free energy and variational free energy in this way?


SPEAKER_00:
Um, yeah, I mean, like I said, it's a little bit complex because I mean, um, so, so, okay, let's start with this.

Um, certainly, um, in the, uh, you know, what the, the accuracy of the expected free energy calculation that you do, right.

depends on whether you have good posterior beliefs over states.

And it depends on whether you're updating your actual generative model appropriately based on the patterns of observations you're getting over time, which would correspond to what we typically call learning as opposed to perception.

So obviously, you're expected for energy.

Estimates will not be good if your model is really bad.

And so there's an indirect dependence on our ability to maximize being in the states that we want to be in.

There's a dependence between that and minimizing variational free energy, at least in a way that, of course, that depends on whether you're minimizing variational free energy with respect to a model that's in fact accurate, right?

I mean, you can get an approximate posterior belief that minimizes variational free energy,

with respect to a model, but if your model is very mismatched with the world, then that approximate posterior is still not going to be a good description of the world.

And therefore you might still end up getting observations you really don't want when you choose policies based on that.

Right.

So, um, you know, that being said, I mean, you know, it's a little bit the way you formalize it because, um, the, you know, often when it's kind of talked about more generically, especially in the philosophy literature,

Um, yeah, definitely variational for energy has the kind of dual right perception action aspect, but, um, but there, there isn't really like a, I mean, the, the tricky thing is, is that action, at least when action is formed in the kind of explicit planning decision-making sense, you still do require, um, some kind of cost function, right.

You still have to have some mechanism that says I expect action a to be better than action B. Sure.

Um,

And free energy is a function of current observations, right?

Whereas the planning and decision making kind of has to be with respect to future observations.

And that's kind of the distinction between variational and expected free energy.

Now, I say that in addition, however, I mean, so you mentioned the thing that me and Casper had done with, you know, deeply felt affect paper.

And in there, there is this kind of further term that

can be added into the active inference formalism if you want to.

It doesn't always kind of have to be there.

It depends on the context of the sort of task or cognitive process you care about or that you're trying to model.

But you are effectively comparing the expected free energy to the variational free energy with the new observation that you get.

Right.

And so there either will be a match or a mismatch there.

So if it's the case, for example, and under with that example, if it's the case that the, um, the expected fringes gets a little mathy, but, but the expected for energy vector over, over policies, if that kind of geometrically, if, if that vector points in a direction that is consistent with, um, the variational for energy of the observation that you get, right.

then you can kind of think of that as being a little bit kind of like a positive reward prediction error.

It's basically saying that what you got was consistent with and even sort of better than expected compared to G. So it's kind of like F matches with G or is consistent with G or maybe even a little better.

um nice and and that in the in the deeply felt affect formalism the idea was that that that uh sort of positive prediction that positive reward prediction error effectively can itself be treated as an observation that can kind of feed into a higher level model where it's kind of like evidence for a positive affective state and then that can kind of put a prior back down on what that what the kind of expected value is for um the parameter associated with that which is just this

beta hyper prior thing.

And the function of that though is interesting, right?

Because in the reverse direction, you can also have F disagree with G, right?

Which would just mean that

Right.

You're expected free energy was based on a bad model, essentially.

And so and so, you know, in that case, you would expect that there'd be a negative affective response.

And so that would act for act as evidence for this higher level model that would say, OK, I'm in it.

That's evidence for a negative affective state.

But what that what that beta term ends up doing.

Right.

So when you negative affective state corresponds to that beta value getting bigger, which ends up being what it what it does is a little bit is a little bit nuanced as well, though, because if you don't assume that there's any kind of initial habit like prior beliefs over policies, then

Effectively, all that beta does is just add noise to choice.

Um, you could think about it kind of like a random exploration term in, um, like in, like in reinforcement learning, but it's just kind of saying like, trust you're expected for energy less, therefore, therefore select actions a little bit more randomly from your posterior distribution over policies.

Um, but if you instead happen to have precise, uh, habits, right.

So you've just the last 50 times, you know, you were in this situation, you chose option two.

and um and it's kind of assumed implicitly that that must have worked out well right if you kept doing it over and over again then then if beta gets big that it ends up essentially favoring the influence of habits over um what the expected free energy would say um so in that case it kind of implies that uh when you're in a negative affective state you'll become more kind of habit driven um and so that's that's a little bit the way that that um the dynamics end up working okay excellent yeah yeah so maybe just myself and the audience i'm going to try and do a


SPEAKER_01:
sort of paraphrasing slash just trying to align what's going up and what's going up and what's going down.

Yeah.

So I'm familiar with the gamma parameter.

Yep.

The gamma parameter being sort of a scale of expected free energy estimates.

So it kind of tracks in a way how confident the system is in its action model.

That's the way that I think about it.

Beta is just the hyper parameter on that.

It's just the inverse of that.

One over beta is gamma.

Okay.

Okay.

So that's really useful.

So the way that I thought about it is that if you've got lower values of gamma,

That means the system has lower confidence in sort of its capacity to do expected free energy.

And so it's going to lean towards habits.

Exactly.

That means that it's high beta.

Yeah.

Got it.

Okay.

So have you seen this new George Dean paper that came out?

The preprint came out, what, last week?

something like this wonderful paper it's about sort of canalized psychopathological behavior so very much in line with what you're writing about okay and exactly what they say you so you can sort of treat gamma itself as a hidden state

And you can maximize your mental act, you can optimize your mental actions.

So as to kind of optimize gamma in a way.

So it kind of looks like, okay, I'm going to avoid thinking about certain memories.

Or I'm going to avoid thinking about expected, you know, a potential future.

Because

I know that will lead to reduced gamma, a lower estimate of my capacity to do expected free energy.

I may be butchering this.


SPEAKER_00:
Yeah, I feel a little limited just because I haven't read the paper.

So yeah, I might not be able to comment confidently.


SPEAKER_01:
Sure, sure, sure.

Well, I just wanted to mention it in any case because it's a nice addition to this picture.


SPEAKER_00:
Sure.

Yeah.

And I mean, for, on our end, yeah, I can, all I can really kind of say is, is that, yeah, for, for us, or at least the way that we've been, we thought about it previously is, is, is that gamma, gamma itself is, is, is not like sort of directly hidden state itself, but it is like an observation that can act as, um, it's a, it's a thing that can be treated as an observation to inform inference over hidden states at a higher level.

Nice.

So you have this kind of higher level that has a posterior of, okay, I'm in a positive affective state or I'm in a negative affective state.

And the current and the change in gamma after a new observation is one piece of evidence for changing the positive or negative affective state.

Two things that I think are kind of important there, if we're talking about it that way.

Again, that's just my idea.

But if we're talking about it that way, one is that

crucially that kind of implies that it's not the only thing that can act as evidence for for the emotional state here yeah right so you could have lots of other things besides just this kind of positive or negative prediction error thing that influences right like i mean i would think that things about your interoceptive state right like are probably important for that um among among other things right so so i just i

I wouldn't I just personally wouldn't want to commit myself to saying that like it's gamma and only gamma is directly right.

That's this thing.

Because the because in that formalism, what ends up happening is just whatever your current belief is at the higher level about your affective state ends up putting a prior back on what gamma is.

Right.

So when I'm in a negative affective state going forward on the next trial or the next time I have to make a choice, that thing is pushing gamma down.

Right.

Now, something that I do probably want to highlight, though, is that I doubt that that's the complete story for at least a couple of reasons.

One big one is that empirically,

Negative affective states, while they are consistent with what would be kind of suggested here, negative affective states,

track something like lack of confidence in a model.

That being said, typically what that tends to look like in a lot of other sorts of modeling frameworks and some empirical results is that the main effect that that has isn't really to make decision making noisier, but actually to like

turn up information seeking and to turn up learning rate.

It makes sense that if I don't think my model is very good, then I have to kind of turn up my learning rate.

So I update my beliefs faster from the next sequence of events that I experience because I shouldn't be trusting my model.

So it's kind of like, okay, I got to hurry up and change my model to make it accurate again.

And kind of influence of learning rate doesn't directly fall from anything about Gamma.


SPEAKER_01:
Yeah, yeah, yeah, yeah, yeah.

So that's more, I've been thinking about it a little bit as well in terms of canalization.

That's more kind of this negotiation between the B matrix and the A matrix.


SPEAKER_00:
Potentially.


SPEAKER_01:
That might be one part of it, right?

If I've got particularly imprecise B matrices, let's say, because I have some dopaminergic synaptic misfiring, and so I have hyper precision over A,

then I'm gonna mistrust these state transitions more than the norm, let's say.


SPEAKER_00:
Yeah, I guess the thing that probably matters here is that depending on the context that you're trying to model, there's very different things that you might need to be, there's very different parts of your model that you might either want to be or not wanting to be updating.

right so so so if so maybe right what i'm not confident in is something about my beliefs about state transitions in that case i want to be effectively or directly updating my learning rate about the transition probability right i should really pay attention to the coincidences between state and next state over and over again to kind of like re-optimize that whereas if if the thing that i'm um

not confident about is something more about my likelihood function, right, more about the mapping between states observations, then I ought to be changing the kind of rate at which I change my beliefs about that likelihood function.

Now, you know, how you do that itself is tricky.

Like I said, I think I think more kind of extended formalism would need to be

put together if we were going to try to turn gamma into something that also affected that.

But it also, I don't know, one kind of tricky thing, I guess, is that

In a lot of the kind of practical modeling applications right now, the way that learning rate is adjusted is just via a kind of fixed parameter that's essentially just a scaler on the updates to the counts and the counts that are added to the Dirichlet distributions.

for A or B or whatever one you happen to be modeling learning at a given time.

But it's similar to including these sorts of constants the way you do in a standard kind of reinforcement learning model, but it's not quite the same as the way you might

do it if you were being kind of like trying to be fully true to Bayesian learning, because because in if you were kind of going through Bayesian learning completely and you didn't want to have a kind of like ad hoc constant, you know, that just kind of scales the updates, then it ought to be something that follows from the relative precisions of the your prior and your and the signal coming from the right.

Yeah.

And

You can do that, and if you assume a kind of constant sensory input and a constant likelihood, then you would learn more from that signal, given that you had flatter versus sharper priors over states, which would follow from your transition beliefs and therefore your B matrix, quote unquote.

And that actually kind of when we've tried to do that in the past, it works, but it's hard to make it work without making a model hierarchical or at least kind of making it kind of implicitly hierarchical.

Yes.

It's a little tricky, but it formally the reason for that is, is that you in the current schemes, observations are kind of treated as things that are

kind of fully like one hot vector, like it's a, you know, a one and a bunch of zeros, basically.

There's no uncertainty about the observation where the uncertainty comes in.

It's just what state you should believe you're in given that observation, but to, to get the kind of learning that I'm talking about, you need to actually have the, the, the observer to do it right.

In some ways you have to actually have the observation vector itself, have some noise in it.


SPEAKER_01:
Yeah, yeah, yeah.


SPEAKER_00:
Because it does.

Right.

And so that happens naturally in a hierarchical model because the first level states do have noise in them and they act as observations for the second level.

Oh, I see.

But also for practical applications, you can just kind of pretend your model is effectively hierarchical by just having it be single level, but make the observations noisy as opposed to being one-hot vectors.

And it does the same thing.


SPEAKER_01:
Yeah.

Well, I was thinking of the other way that to control these learning rates, you would need something like a metacognitive aspect.


SPEAKER_00:
Yeah, you could do something like that, too.

You could have like a higher level, like a second level state that sort of conditional on that, whatever that is, it modulates the learning rate at the level below.


SPEAKER_01:
Let me go back to this, this George Dean paper, just because again, I'm, I'm quite concerned for some reason on well, not for some reason, it's for good intellectual honesty that I don't misspeak.

And I don't, you know,

So I'm sort of curious again about what you think.

So I've just got it up here.

They say, um, duh, duh, duh, duh, duh.

Yeah.

So ineffective inference, the system not only possesses an estimate of action model confidence described as implicitly metacognitive, but another layer of the generative model can be added so that the confidence estimate itself becomes a hidden state that the system infers using the same mechanics of perceptual inference.

So that's kind of similar to, in a sense, what I was saying.

The Gamma itself is becoming the hidden state.

But I think the point that they're making there is that you're not just doing perceptual updating, you're also doing planning as inference.

So you can do in terms of mental action, attentional schemes, you can deploy certain mental actions, such that the observations you make are going to be either, you know, increasing the likelihood of being in that hidden state or not.

And that's what you know, that's where whether that's thinking about something that's bad, that's gonna happen in the future, or something bad that happened in the past.

you're more likely to avoid thinking those things because you're aware that probabilistically you might end up in low gamma.


SPEAKER_00:
Yeah, that's interesting.

I guess there's a few different things here where I'm just, I don't know.

I mean, my initial go-to when we start talking about all of this is kind of like how I would like,

know write it down you know yeah yeah yeah it kind of you know work formally and so there's kind of like a few different things that kind of mix together in there um so i mean like one one thing i could totally see which i thought was what you were talking about a second ago was was like um instead of the gamma based kind of hierarchical updating thing that we were talking about before like that kind of second level state part of its likelihood back down to the first level could also be something about like the the learning rate

hmm right so it could directly through this kind of additional connection in the second level likelihood you know when i'm in a negative affective state turn up the turn off the learning rate at the lower level um which isn't so like so much intentional or kind of like controlled right it's just something that kind of like happens automatically as a function of whatever you infer your affective state is um that's just a little different i suppose from a few built-in

a policy itself that controlled something like the like transitions to like attentional states yes so we've done that in some more like emotion inference models right where you can kind of you could kind of selectively choose okay now i'm going to pay attention to my current valence all right now i'm going to pay attention to my arousal level um where you can kind of use a second hidden state factor to kind of like pull out slices of the

likelihood tensor that encode the precision of the observations with respect to the mapping to a different hidden state factor.

But but so there's there's that kind of thing.

You don't actually need explicit hierarchy for that.

You just kind of need separate orthogonal state factors like

in practice, but yeah, so you could do that.

And then the third thing I guess it makes me think of is something that I played around with a little bit that I think is actually a really cool idea but haven't done anything really with is that you can try to account for what can still be a kind of Bayesian form of motivated cognition that would look kind of biased on the outside.

you know, because if in a second level model, you're, you're treating the first level states as observations for the second level, then that means like, you can have a prior preference distribution over Yes, first level states, right?

So then effectively, like, you actually just like want to believe some things and not others, right?

You want to believe some things and not others, then like the optimal policy will could be something that looks super anti Bayesian from a fully like epistemic perspective.


UNKNOWN:
Sure.


SPEAKER_01:
but probably Bayes optimal under the actual scheme that you've got in place.


SPEAKER_00:
Yeah, exactly.

Right.

But externally, right?


SPEAKER_01:
Yes.

I mean, this is exactly the point they're coming to.

They speak of sort of avoidant mental action or motivated inattention.

Right.

And this idea is that you're kind of performing the second order inference over the model precision, as I said.

But you have this kind of predictive model with model precision as the hidden state such that, I mean, the way they talk about it, how will I feel if I do that, but with respect to mental action?

Sure.

Which I think is very, I don't know, I just thought it was very cool and very much up your...


SPEAKER_00:
yeah absolutely i mean yeah when it comes to the affective stuff i totally agree like i said i'm super interested in this i mean it uh i mean in part it makes me like like i both think it's really cool and it also makes me slightly like empirically uncomfortable and the reason for that is like i'm pretty sure it makes like

like actually testing, like whether a model like this is kind of like the the best model of human behavior.

Basically, I think it makes it kind of unfalsifiable.

Right, right, right.

Well, account for basically anything.


SPEAKER_01:
But let's jump into that, then.

I mean, you when we actually spoke on email, you were keen to sort of mention that you're not just doing pure simulation work.

And you're doing imperial what you call him?

Well, I think what is called empirical computational psychiatry.

whereby you fit models to behavioral data in clinical populations, which is super cool, but I'm not a mathematician.

So and I assume a lot of the audience aren't mathematicians either.

So perhaps you could just in sort of a broad brush approach, describe the distinction between pure simulation work and empirical computational psychiatry.


SPEAKER_00:
yeah i mean i guess i think the the cool part about doing simulation stuff is is that you're like pretty under constrained right like there's a there's a kind of like creativity about it and you can kind of get really complicated and you know it allows you to kind of simulate some cool mechanisms um

But a lot of times, like the concern, I suppose, is that they become a lot of like just so stories in a sense.

Right.

Because, you know, I could come up with probably like ten different generative models that would, you know, that would hypothesize somewhat different mechanisms, but they would produce all the kind of like behavioral signatures of the sort of thing we're trying to explain.

Right.

right so so i think it's important to be able to find ways to actually you know test you know which of these models are actually good you know like accurate descriptions of what's actually going on in the brain um and um and so you know for that right you need to get these models to have direct contact with data somehow

Um, and, um, and so usually, you know, so the sort of thing that, you know, we will do and why should say, and another thing is, is that, um, you know, like how good a model fits data, um, is always a relative comparison.

Right.

I mean, you can't, there are absolute metrics of model fit, but usually you want to kind of say, okay, here's a space of possible generative models.

Um, you know, that, that hypothesize different sorts of algorithms that are being used or, or, or different just model structures, but that assume, you know, underlying the same underlying algorithm.

Um, and then you kind of want to, and you want to do like Bayesian model comparison, right.

You just want to say out of all these models, which model does the data provide the most evidence for?

Um,

And, you know, like that actually allows you to be able to test whether or not like an active inference model or what an active inference model would, a given active inference model would predict, you know, actually reproduces the behavior that people actually do, right?

And so I just don't, yeah, I mean, to be scientific about it, I just wouldn't want, or I think it's important that for people to take

for people outside of just the kind of like active inference community proper, right?

To actually kind of take these things seriously and think that they're moving science forward that, you know, we need to be able to actually show that these things are improving, right?

How well we can predict what brain and behavior actually look like compared to lots of other simpler models that have been around for, you know, much longer than active inference.

Right.

So so I mean, you know, computational psychiatry is a fairly it's fairly new field, but you know, there's a lot of there's more and more people working in it, but most of them are not coming from like an active inference background.

You know, they more tend to focus on simpler reinforcement learning models that

you know, at the end of the day, don't tend to be really too different in what they predict from from active inference in a lot of cases.

So, you know, the trick is kind of, you know, finding the right kinds of decision making or behavioral or perceptual tasks to have people to have, you know, participants come in and do whether they be healthy or whether they be patient groups to try to

know to see in other words you have to find tasks where it's clear that a particular active inference model would actually make different predictions than a simpler um reinforcement learning model and um you know we've started to do quite a bit more of that i am you know there's a you know for anyone for anyone who wants to come little plug uh the um one of my uh current grad students um is uh

presenting, well, she's just about to submit for a talk or a poster for the Computational Psychiatry Conference this summer in Minneapolis in July.

And it's, and this has been kind of our most thorough attempt at this lately is we just have a fairly simple like explore exploit, sort of like three armed bandit task that involves trade offs between information seeking and reward seeking.

And so it

does kind of match right where where active inference is supposed to kind of show show its strengths with the directed exploration um in this case primarily with respect to the uh novelty term right so this would be like parameter exploration not state exploration um but um but so more or less we identified i think like this is like table essentially of um something like

30 different possible RL models and 30 different equivalent complexity active inference models, something like that.

It's a really massive model space.

And fit all these models to behavioral data and a couple hundred people.

and both a both a taiwanese sample actually and an american sample um and um and just done model comparison across these um and um and the the models that tend to win are more complicated models um there's a the the two the two kind of best models in what we found so far one is

One is an active inference model that assumes that people have one forgetting rate and two separate learning rates for wins and losses separately.

So basically update your beliefs more or less when you get a win versus when you get a loss.

And then a kind of like dynamic decision noise sort of term that is based on the beta thing, actually based on the updates of the beta gamma.

term that we were talking about, where effectively what that ends up meaning is that when people get an outcome that's unexpected, right, then gamma goes down.

And so the posterior verpolysis flattens out.

And so essentially their choices become more exploratory in the randomness sense when they get outcomes that they weren't expecting if they were negative.

If they get outcomes that are better than expected and it's positive, then they become sort of more deterministic and going with what the model would say is the best choice.

And so that's the best active inference model that tends to fit.

So it's very complex.

It has like five or six parameters, something like that.

And the winning RL model is slightly different.

It assumes dynamic learning as opposed to dynamic forgetting, or I mean, as opposed to dynamic decision noise.

But the point is that the main difference between these models ends up not really so much being differences in exploratory behavior so much as whether or not they include Bayesian forms of learning.

the main thing that the main thing that active inference does that's really different when you match it to kind of an equivalent complexity rl model is um is that active inference models are kind of with the with the dirichlet distributions again the details of that don't matter for non-mathematical people it just basically means that in an rl model you're just kind of updating your beliefs about the expected reward probabilities whereas an active inference you're keeping track of those reward probabilities but also you're keeping track of your confidence

in the beliefs about the reward probabilities um and um and so that's what and the the kind of take home with it for for what for what she's done is um mainly that the um the ability for the two models to um predict behavior is actually at the best two models um it's actually fairly similar but Bayesian model comparison does um select the active inference model over the

over the RL model.

And because the differences in accuracy between the models aren't amazingly different, ends up relating probably a little bit more to like complexity cost differences and kind of evaluation of the independent, like how independent each parameter is in explaining the behavior, things like that.

So anyway, I mean, so a lot of what we've been doing is kind of attempting to say, okay, well, you know, how much better, you know, does active inference actually do, right?

And explaining behavior.

Is it the kind of most parsimonious, accurate explanation for behavior?

um how people actually do make decisions in information seeking contexts um so that's a little bit you know where you know kind of the approach that we take so that and then using those things um by also having people in patient groups with like depression or anxiety or substance use disorders um do these same sorts of tasks and then see whether um

you know, we can explain clinically relevant differences in these, you know, between patients and healthy comparisons through differences in the actual parameter estimates, right?

So maybe it's the case that, so one thing that we have found, right, is that pretty consistently substance users in active inference models tends to show much slower learning rates from negative outcomes than positive outcomes.


SPEAKER_01:
they kind of it takes a lot more times of them choosing a supplemental thing before they kind of change what they do um i found that i found that an interesting result uh your 2023 paper elevated decision uncertainty and reduced avoidance drives in depression anxiety and substance use disorders i may be that's a different one but yeah this might be slightly different but i found maybe maybe i misinterpreted what you said there but this idea that um

individuals at least with substance use disorders show a reduced sensitivity to unpleasant stimuli.


SPEAKER_00:
Yeah.

Okay.

So, so it's, yeah, so there's a, yeah, so that's a, that's a, with a different task.

Um, it's actually the same, the same sample, but it's a different task.


SPEAKER_01:
I guess the prior one is learning rates, right?


SPEAKER_00:
The first one.

Yeah.

So it's kind of depends, right.

The learning rate one, again, that's with like a bandit task.

And so it's, it is about kind of explore, exploit, updating beliefs about reward probabilities, et cetera, where it's the approach avoidance conflict task is a little different.

It's not a learning task.

It's purely just kind of a planning forward looking task.

And in that one, it's more or less, there's a different kind of number of, you know, points that, you know, people can win, but they have to also choose to go through some pretty unpleasant affect of seeing some pretty, seeing and hearing some pretty unpleasant affect of stimuli at the same time.

gonna have to plan whether okay do i prefer to kind of move toward the side of this runway that means the probability is higher i'll kind of get this safe neutral outcome but no points or whether i'll whether i expect it to be kind of worth it to me right to like under a certain point value try to act in a way that will increase the probability that i'll get the points but also see the super unpleasant stuff right and and in that case there's um

I mean, this is sort of task that's been used in non-computational modeling ways several times in the past.

But the main kind of metric, if you don't do it the computational way, is more or less just this general kind of unidimensional approach avoidance thing.

Whereas the modeling version is a little bit more helpful because we can estimate the beta parameter for these people.

And then we can also estimate certain things about the shape of their preference distribution.

and what that shows.

So we can kind of use that to dissociate two different aspects of approach avoidance conflict.

One is this kind of like uncertainty about decisions and the other is this actual kind of approach avoidance drive.

And yeah, the results for that are actually kind of surprising, because like in the depression and anxiety and in the substance users, they tend to show

um greater just the patients tend to show greater decision uncertainty so they have higher beta values but they um they actually show um more precise preferences for the for the points which basically means that they're means that they're kind of more willing to go through the negative affect stuff i was very surprised by that


SPEAKER_01:
yeah so they're actually less avoidant right so they're more uncertain but they're less avoidant and that's we've now shown that in four studies you know so it's a very consistent result across multiple samples i read that and i had to read it twice uh because i was really like i really expected the other way around because we're used to hearing that avoidance is the kind of number one driver of a psychological disorder like generalized anxiety disorder so what

What do you imagine is going on here?

I mean, I think one, just talking in terms of very layman non computational terms.

Yeah.

There's, there's the overexposure of people with psychological disorders to negative stimuli in the first place.

So what's negative for a healthy control is what's all relative, right?

And if you've been exposed to overtly negative things, like your own emotions over and over again,

maybe you have a reduced sensitivity?


SPEAKER_00:
Yeah.

I mean, there's, you know, I mean, we don't, we don't really have like, unfortunately, I mean, our, our hypothesis going in was absolutely what you said originally, right.

That we would get more avoidance than the patient groups.

Um, but, uh, you know, we were just hoping that the, the modeling would help us kind of distinguish these couple different kind of dimensions, right.

Of, of, of approach avoidance conflict.

Um, but, uh, but definitely we didn't expect the, the more approachy stuff in, uh, in the, in the patient groups.

Um,

That being said, yeah, so because of that, we don't have kind of a bunch of other, we don't have all the measures that we probably would have gotten to try to dissociate different possible explanations for that kind of unexpected avoidance or unexpected lack of avoidance.

um so it becomes a little speculative but um one thing is absolutely i mean that's one thing you might you might suspect is what you said that um you know if if people uh if people just in general are in this kind of heightened negative affective state then you know you might expect that makes them more sensitive to other negative affective things but you also might make that also might think they're a little bit habituated to it yes exactly what would that look like in terms of a kind of palm dp scheme because i guess we associate habits


SPEAKER_01:
with action with the prior value or prior beliefs over action when we're talking about habituated uh responses to stimuli I mean I guess it is it's in the sense action but what what shape does that take


SPEAKER_00:
Um, yeah, I mean, so, so like I said, in, in the official model that we were fitting, it doesn't really occlude any include, uh, the dynamics of habituation.

We're just kind of estimating the, what the shape of their preference distribution is right there to explain their behavior.

Um, and so it, so then the explanation just ends up being that they have a more precise preference for, for winning the points or, or, uh, or, uh, less the, the, the magnitude of the.

the probability of the negative outcomes in the preference distribution, which, again, doesn't really mean probability, it just means they, they disprefer the negative stuff less, right?

Is, is, you know, that ends up explaining it.

But, but, but what would it look like if you included some sort of actual

learning dynamics for, um, for habituation.

Um, I mean, so this kind of habituation is, is really not so much more about habitual action as it is just the level of sensitivity you have to stimulate.

Right.

So, so it's a little more as this flavor of, okay, if I see something over and over again, then my response to it each time is less.

Right.


SPEAKER_01:
So, so, um, I think you could cost that in terms of action.

Um, it, it,

Well, I mean, it's okay.

So I'm thinking in terms of traditional therapeutic responses, right?

Like in terms of exposure and response prevention therapy, for example, you at least have an element of it, which said that a dysfunctional avoidance response.

Again, this is a non active inference way of framing it, but just in a layman sense, it entrenches the severity or the perception of the severity of the stimulus.


SPEAKER_00:
Yeah.

So, I mean, we have, you know, and I, I mean, I wouldn't necessarily have expected you to seen it, but if you happen to, I mean, we do have this paper we published a couple of years ago as a simulation paper specifically on mechanisms of, uh, possible mechanisms of CBT in a POMDP.

So.

We actually did simulate exposure therapy in particular.

But it was the crucial kind of thing we were looking for there was whether or not we could kind of capture, make predictions about the ways that cognitive interventions like reappraisal would interact with behavioral interventions like exposure therapy.

Um, and, um, and I mean, some of, some of, some of that, we ended up being kind of, kind of excited about it seemed consistent with results or with, you know, when I talk to clinicians, cause I work with a lot of clinicians, um, you know what they say, but where, where with that, I mean, the, the kind of, you know, habituation was much more about like, so if a person has a strong

If a person has a strong expectation that like if they choose a certain action, then the things they are going to observe are things that they really don't like.

Right.

So things that have a very low preference level, then, you know, it follows all being equal.

They're not going to they're going to take actions that don't lead them to those things.

make those observations right but the problem is is that when those when those um when those uh uh beliefs about the expected outcomes of actions are are wrong right then if they just avoid it every time right then they can never update their beliefs right so i mean exposure is essentially kind of this kind of like uh clinician facilitated directed exploration right right you know they kind of

push you to go take this action that's going to make you see, okay, what actually happens when I'm in this state that I've been avoiding?

And so the kind of habituation in that just ends up being once they start actually seeing what observations they really get when they're avoiding, then they just

You know, it just updates their likelihood function.

Right.

Right.

Right.

Right.

Right.

Back under that state.

But but the interesting thing, at least in that paper, was a lot more of how that interacts with your cognitive beliefs.

So because because you can also and this is often what happens, right, is this is that when when people go through exposure therapy,

um they will they can um even after those response the negative responses kind of go away and they start to say okay now now i believe like in the context of spider phobia for example like no okay actually you know i believe the spider is safe now right and so their responses their affective responses like died down but

know you bring them back in a couple days later they see the spider and they have the negative responses again sure right so it doesn't kind of like stick around it doesn't generalize um you know so why is that right because under a simple under a simple kind of like unlearning story um they would have just overwritten their old likelihood function right so they shouldn't right their generative models just be different they shouldn't have those responses again

Um, you know, so what explains that?

Well, what explains that has to be that they, um, that they don't take evidence.

They don't take the evidence from the unexpected outcomes when they see a spider, um, as evidence that their likelihood function was wrong.

They take it instead as evidence that they've actually transitioned into a new hidden, hidden state context.

So that now, so now they believe, okay, I am in the safe context.

and then they just update their likelihood under the safe context.

Ah, that's interesting.

Right.

So then all you have to do is say, okay, well, if there's any uncertainty in the transition function, then even though they believe they're in the safe context now, come back in a couple of days later, there's more uncertainty over what context they're in.

So their responses come back because the likelihood function under the danger context still says, right, bad things will happen if spider.


SPEAKER_01:
So how would you resolve that?


SPEAKER_00:
Yeah, so one thing you want to do then is you want to try your best as a therapist in some way to not use cognitive approaches to try to make people believe, oh, actually, this is really safe, right?

Yeah, yeah, yeah.

You're going to go in this room, you're going to see this spider, but it is safe, right?

Right.

like you have to have them actually maintain some kind of uncertainty about what context they're in right because only in that case if there's like like in the simulations more or less like we showed is like if you keep the posterior distribution that they have about the context there and if you kind of keep that imprecise right if you maintain the kind of cognitive uncertainty

and when they get the unexpected observations that bad things don't happen then it does overwrite the likelihood under both states right um under both of both in context and then it does generalize right um and that's um that's actually uh consistent with this kind of cool study that um sam gershman um

has written some stuff about back in 2015-ish.

It's been a little while, but where you can do more or less the same thing with rats with just classical conditioning experiments where you play a tone, they get shocked over and over again with some probability.

So now

You play the tone and they kind of jump or freeze or do something implying that they'll shock.

And then the standard thing to do during extinction is you just present the tone over and over again.

You don't show the shock at all or you don't get the shock at all.

And so then the responses slowly die.

But similar to with exposure therapy, you bring the rats back a day later.

you play the tone again and they do have this kind of spontaneous recovery is what it's called where they start to have the fear response again um which means they're doing basically the same thing right the rats too are just inferring okay i'm not sure if i'm still in the safe context so so what they showed though is that if you do the extinction um if you do the extinction

time course a little differently then you can actually prevent this spontaneous recovery and the way they did it is instead of during extinction just abruptly completely stopping the the association between the tone and the shock

What they did is they just said, okay, for a little while, tone is associated with shock 90% of the time.

Okay, now for a while, tone is associated with shock 80% of the time.

Okay, now 70% of the time.

They taper them off.

Yeah, and what that does, you can kind of think about it, is the prediction errors are never that big, right?

And if the prediction errors are never that big, then that's not good evidence for the shift in context.

Right.

So it does cause this slow overriding of the original likelihood function as opposed to promoting the state inference thing.

Um, and when they did it that way and then they tested their rats later, they didn't have the spontaneous recovery.

Like the, the fear responses didn't come back.


SPEAKER_01:
Um, and that is fascinating.

It's very, it's, it's quite counterintuitive though.

Yeah.

I think you would be, one would be under the presumption that if you just kept providing evidence, you know, if you kept tying an observation to a different state, that would just become entrenched, but it,

it does make sense that that itself becomes this kind of crystallized thing.

And then once you take them out and put them back in, then the previous one still has that precision.

That's really fascinating.

You also said something very interesting there, which was, um, it was kind of under the radar, but the difference between having a strong preference to be somewhere or to have something or to be something or whatever versus, uh,

had like having a strong dis preference not to be somewhere be something have something so a positive versus a double negative i've often found this in in active inference i don't think it's been particularly well codified what the difference is between those so people say oh um i want to be um 37.8 right in some sub-personal way i want to be 37.8 degrees celsius and so i take action to restore that

But you could well cast as I don't want to be 40 degrees Celsius.


SPEAKER_00:
Yeah.


SPEAKER_01:
Is there a technical difference between those?


SPEAKER_00:
So, I mean, one thing that I guess is kind of unique about the way that active inference encodes preferences compared to, you know, more kind of like traditional formalisms like reinforcement learning that cast things explicitly in terms of reward function is that

Strictly speaking, an active inference agent isn't really trying to maximize reward.

They're trying to minimize divergence from a target distribution.

And that actually has certain interesting implications.

One thing to consider with that, though, is that unlike in things like reinforcement learning, all of your preferences kind of constrain each other in the sense that the whole distribution has to add up to one, right?

So I think that you will have to kind of, relatively speaking,

basically you will only be able to specify relative preferences.

So I think that if you,

if you have a distribute a preference distribution where the kind of highest mass is over, you know, like 38 ish degrees, right.

Then, then the fact that it kind of tails off on both sides simultaneously guarantees that you disprefer being above or below that.

Um, so, so I don't know explicitly whether or not there is, um,

whether or not there's any way to encode those in a way that's different.

There are interesting things you can do by putting kind of like negative preferences for something and positive preferences for another thing.

If there's like an observation you can lock as kind of like a baseline value of like zero.

When the thing gets normalized,

to turn it back into an actual probability distribution, then it does still end up being kind of a precise preference for one thing and dispreference for the other.

And they do still end up being relative to each other.

And the main thing that we tend to see when we do simulation stuff and those sorts of settings is it just tends to...

lead to this kind of like anti exploration behavior.

When when, you know, one thing's really negative, where it's kind of like, the, the agent is just kind of this, like, you know, better safe than sorry, you know, behavior.

But I, but I think there are, there should definitely be ways of encoding those in terms of all just kind of relative, relative positive values that end up normalizing to the same distribution.


SPEAKER_01:
Cool.

Yeah, I think part of the reason why I asked that is because I think it's a point that you make in the papers that you've written on interceptive inference, which is that when we speak of sorts of so called aberrant interceptive inference, a lot of papers are not very clear whether they're talking about the underweighting of priors or the overweighting of the likelihood distribution.


SPEAKER_00:
Sure.


SPEAKER_01:
Um, I think part of the problem is people think of, uh, precision waiting as like a necessary zero sum game such that you can't have both.

So if I have, uh, reduced precision over my likelihood.

by necessity, that will lead to increased precision over my prior, if I've decreased precision of my prior by necessity, at least to an uptake in the precision of the likelihood, or you show at least in this 2020 paper, the one that I read from PLOS, we did the interoceptive perturbation condition isn't the case.

So

Where do you think that kind of false intuition comes from?


SPEAKER_00:
It comes from the fact that people usually in the way that they, at least in these more kind of conceptual papers, comes in part from the fact that these are under, they're talking about models that have Gaussian assumptions with continuous distributions.

Um, cause if you have, if you just have on along a single dimension, right.

If you have like a distribution that defines the likelihood function and a distribution that defines the, um, prior belief, then, then there are right.

When I turn up one or turn down the other, then there's a bunch of different ways that I can kind of combine those things to lead to the same posterior.

Um, so it does become hard to distinguish.

Um, so it depends on the dimensionality and depends on whether you were talking about continuous or categorical distributions.

Um, so, so in, in the, in the case of what we're doing, these are discrete distributions, right?

So in, in discrete models with the dimensionality, the way that we have it set up, um, it makes, uh, that makes clearly distinct behavioral predictions, but, um, but in the, uh, in the kind of unidimensional Gaussian, um, set up, um, then you, you do get this more, um, only the relative weighting matters.


SPEAKER_01:
Yeah.

Yeah, yeah, yeah, yeah.

That makes sense.

Okay, so thus far, I know we haven't got too much time.

But thus far, I know, we've kind of been speaking generally about perceptual updating and parameter updating, or parameter learning and belief updating.

But we also have structure learning and active inference.

And I think you make a really nice point to say that sort of something like introducing a concept to distinguish between, um, or, or to yes, to let's say split up a concept that you had before.

So if I had anger now, maybe I can turn that into rage and fury.

Sure.

Um, I think, I believe you call that emotional granularity.


SPEAKER_00:
Yeah.

And, uh, and, uh,

Yeah.

And the emotion and the emotional state inference context.

Yeah.

We would talk about that in terms of granularity or, uh, yeah, that's usually the, probably the best way to talk about it.


SPEAKER_01:
Yeah.

It's a really fascinating concept because it, it, it, I think on, on first glance, it seems like, uh, it seems like a sort of just linear benefit, the more emotional granularity I have the better.


SPEAKER_00:
yeah i mean what that what the direct relationship would be between like the uh the number of you know states in a state space and uh and something to do with gamma i think is is going to be fairly indirect um but for a number of reasons but um i mean i think with most things right i mean the general answer is going to be that um it's not so much about complexity per se as more complexity than necessary yeah right so so um right you

you know, complexity, adding complexity is good if it increases accuracy, right?

So what it would mean, right, to be more complex than necessary would be that there's a mismatch to the actual generative process.

I think that if you're overfitting to the generative process, if you're inventing more categories than there actually are distinctions to make, then obviously that's going to be problematic because you're going to think you have more information than actually you do.

But so long as you're carving up your state space in a more fine-grained way, and that actually does add to the informativeness of the information that your generative model is able to kind of pick up and then use to inform policy selection, then I would expect it would be the more the better, right?

It's just the more... So it's the more accurate information you have, the more informed policy selection will be, right?

So as long as...

you have you're good at inferring the right you know posterior over what emotional state you're in and each emotional state that you kind of have have acquired into your generative model um if each of those does in fact make unique um um predictions about patterns of current and future observations under that state then that yeah i mean it'll be better just because it'll should right on average just make policy selection more informed right right right but um yeah


SPEAKER_01:
Thinking more broadly about the distinction between structure learning and parameter learning.

Do you see these as bright dividing lines or do you see them as being kind of philosophically vague as to when parameter learning becomes structure learning?


SPEAKER_00:
Um, you know, like, I guess it's just, it's just tricky.

I mean, like you get in the weeds with some of this stuff because people use structural learning, the term structural learning to mean several

Structural learning itself is a superordinate category with a bunch of different exemplar things within it.

I don't know if you saw this one paper on structural learning specifically that we put out a few years ago where it was just a dumb example of people trying to learn coarse-grained or fine-grained different types of birds and fish and stuff.

Right.

And, you know, so that it was like, you could just learn like these broad categories of bird and fish, or you could kind of like, learn to carve it up into specific types of birds, specific types of fish, and they just kind of each of those does it was defined with respect to specific like patterns of covariance among certain sorts of observable features, right, like wings versus gills and

size and color and things like that but um you know in that particular case the way that like structure learning was posed was in relation to kind of like inferring how many states need to be in your state space right to account for account for data

Um, and so, and so, um, that has to do with kind of the setup of the structure of the generative model proper, right?

Like how many, how many types of states, how many states, how many, how many, how many state factors, how many levels of states with any state factor, et cetera.

So in the general POMDP scheme, that is distinguishable from parameter learning in the sense that parameters are usually kind of the values of the entries into the matrices within things like the likelihood function, transition function.

and et cetera, um, that you update, but you update those under the assumption that like, you know, number of number of columns in the, in each matrix are, are kind of fixed.

Right.

Right.

So, so, so if you're talking about kind of this granularity kind of fine graining of, of your, of the concepts that you have, um, then, then there is a clear distinction between, um, between, uh,

those examples of parameter learning and structural learning.

But like I said, there's other types of structural learning and the actual implementation

of how you do structural learning also matters because you know in our example um you know we we use this kind of like little trick right where we just kind of said you know we're we're actually gonna you know stick the matrices in as though there's a bunch of these kind of extra unused columns you know we didn't have to kind of like explicitly add states to the state space

we just kind of had a bunch of states that were just kind of like sitting there silently unused and then the way that and then the way that the thing kind of worked was just that you know what they meant to be unused was the distributions were just completely flat right so they didn't predict really anything unique about the observations um and so and so the way that that would work is just that um

More or less that if you got some pattern of observations that was consistent with one of the kind of informative likelihood functions under a kind of state that you already had, then the posterior will be precise over that state.

And, you know, you would have a basically zero posterior over all the unused columns.

And so you wouldn't have any parameter updating under those columns.

But if you had some pattern of observations and they were really inconsistent with any of the informative mappings that you had, then effectively what that would do is it would mean that the flat mappings would better explain the data.

Right, right, right, right, right.

And you know, and as soon as like the flat mapping better explains the data, then you end up with a precise posterior over one of these unused columns, right, whichever one happens to explain it best.

And then you just start learning the parameters under that new column.

Right.

So so and then you, that's just a way of learning a new informative mapping.

So it's kind of like effectively now,

you have a new column, you've expanded your state space, right?

And you could, you could do that a little bit more explicitly than we did by writing in some additional function that like explicitly, you know, just like added new columns to the state space or something, as opposed to just having these unused ones sitting around.

But my point is, is that the relationship between structural learning and parameter learning is very, they're very tightly

intertwined in that case, right?

Because when you start learning the parameters, the thing that leads you to start learning parameters is the exact thing that is causing you to do structural learning.

Right, right, right, right, right.

Yes, exactly.

But again, that's one implementation, right?

So I'm saying that that won't be true.

That doesn't need to be true generically, but that will be true in some cases.


SPEAKER_01:
yeah yeah yeah yeah well it seems like they all to yeah they all lean on one another in some kind of recursive manner where you can't really have structure learning without learning the parameters in you know of that structure in the first place nor the belief updating that allows you to


SPEAKER_00:
kind of get there in the first place right in a dynamic system in a dynamically changing system yeah i mean i think i think it's just and again i can't i can't really say with confidence how well this will generalize to other cases of structural learning or or even just other kind of

generative model setups and things like that.

But yeah, at least in this case, you have to effectively infer that the current informative parameters and parameter based mappings that you have are not doing a good job of explaining something and then end up with a very low posterior distribution.

over or a posterior that assigns a very low probability to any of the states you currently have in your state space and then yeah and then so it's like you get the right posterior based on you know the based on the fact that your current parameters are not good expla you know don't provide good explanations for the data and then

by starting to update different set of parameters, you end up leading to doing something that is effectively structural learning.

So it's, yeah, they're just very, yeah, it is all just very kind of tightly connected.

Yeah.


SPEAKER_01:
Yeah.

Yeah.

Well, it'd be interesting to see if you could sort of disambiguate them, you know, in a model, like whether you could just have, I guess there's a,


SPEAKER_00:
Yeah, conceptually, though, I think that's tricky, right?

Because you somehow have to say I need structural learning only because well, exactly.

Well, that's why I'm kind of by recursive nature.


SPEAKER_01:
I don't know if recursion is even the right way of talking about it might be sort of nesting.

In some sense that belief updating drives parameter updating, which like ultimately feeds into structural learning.


SPEAKER_00:
Yeah.


SPEAKER_01:
Yeah.

Cool.

So final question.

Big question.

You've got this paper coming out, the so-called empirical status of predictive coding.


SPEAKER_00:
And active inference.


SPEAKER_01:
And active inference.

Yes.

Is that the whole name of the paper?

Yeah.

It's a sort of classic critique leverage that active inference that it's self-contained and it's an echo chamber and it's a lot of philosophers and mathematicians kind of spitballing these fancy ideas, but it doesn't

This is a common critique.

I'm not saying it's true.


SPEAKER_00:
Oh, okay.


SPEAKER_01:
Yeah.

Yeah, yeah, yeah.

Oh, well, it would be odd if I were to promulgate that position as the host of this podcast.


SPEAKER_00:
Oh, no, no, no.

Sorry.

I thought you were saying that that was the thrust of our paper.


SPEAKER_01:
No, no, no, no, no.

I'm saying it's a common critique from those on the outset.


SPEAKER_00:
I don't remember saying all of that.


SPEAKER_01:
No, no, no.

I know you didn't.

But it's a common critique, at least.

I mean, that might be oversimplifying the critique, but that, you know, this theory of everything...

is overgeneralizing and it doesn't have an evidential basis.

Having done this sort of deep dive into the empirical status of active inference and predictive coding, what are the conclusions that you guys have drawn?


SPEAKER_00:
I mean, I just think I kind of think at the end of the day, I mean, it amounts to be something kind of similar to what I think you'll see in a lot of review papers for most things.

Right.

This is just that there's a lot of stuff where more work needs to be done to be for it to be definitive.

I mean, that's kind of like the very oversimplified conclusion.

I mean, I would say that I mean, one thing that I hope that we tried to make clear is just.

It's more that like there's just lots of different levels of claims with respect to both predictive coding and active inference.

And they all kind of need to be tested in a way that's a little bit, at least partially independent of each other.

Right.

So, I mean, especially in the philosophy of literature, people will even talk about really broad like categories like, you know, just like predictive processing, you know, quote unquote.

But like, I mean, if you want to move from that down to stuff that is actually testable requires.

a bunch of, there's a bunch of kind of layers that you have to kind of work through to get there.

So you kind of say, okay, well, predictive processing, it's very generic idea that in some way,

of the main things the brain does is predict stuff i guess right that's like okay well you know two of the most kind of like prominent but certainly not the only possible categories for that are like predictive coding and perception and active inference and action but then even from there right i mean like predictive coding makes particular um explain or makes particular predictions if what you're talking about is um something about the uh the the the

the predictive coding algorithm right as defined right which is which doesn't necessarily need to make predictions at the level of the kind of like neurobiological implementation right so

And same thing, same thing with active inference, right?

Active inference or say, okay, well, what type of active inference are we talking about?

Are we talking about like the, you know, continuous state spaces?

Are we talking about discrete state spaces?

And, you know, like, are we talking about, um, you know, commitments related to, you know, some of these like claims about, you know, the gamma stuff and dopamine, or, you know, are we just talking about things about categorical state inference and that, and that, that depends on.

minimizing variational for energy through like a variational message passing scheme, right?

And okay, are we actually hypothesizing variational message passing?

Are we hypothesizing marginal message passing?

Or are we hypothesizing that the implementation

draws or that the algorithm is doing some sort of belief propagation thing.

And then even there, that's still all just at the algorithmic level.

And then you have to say, okay, well, if I commit to some algorithm, if I commit to POMDP discrete state space, this particular generative model structure for this task,

you know with this you know with this message passing scheme you know as i said then all right there's like probably at least like 20 ways that you could set up a neural network right to solve that right so then you also have to have a hypothesis about okay what am i what what implementation you know am i am i testing right and what are the predictions of that implementation

Right.

So if I'm doing something at the neuroscience level, then I'm going to say, okay, it's kind of this very kind of hierarchical thing where it's like, you know, this particular neural prediction that falls from this neural implement hypothesis, neural implementation that falls from this specific narrow algorithmic commitment, you know, like from this larger kind of framework, you know what I'm saying?

So it's like, you start making exclusions.

Yeah, so testing these things, what it even means to be testing these things is kind of complicated because by the time you can actually get down to something that's empirically testable, you've narrowed it down a ton.

And ruling out one implementation doesn't falsify the objective processing, nor does it falsify active inference, right?

It just means you ruled out that a particular implementation of a particular algorithm within the active inference framework, et cetera.

So it's like, okay, well now I'm testing implementation

hypothesis too um so it's a you know so that that's tough right so then um but a lot of you know but a lot of what's been done is much more um well I should say with with predictive coding a lot of stuff's been done at the at the neural level with active inference it's very minimal has been done in terms like neuroscience um neuroscientific studies like Philip Schortenbeck had a cool study in like 2015 where they did um where they did uh some of the um some of the uh

like midbrain dopamine results in relation to some of the beta updating stuff.

I mean, it's called alpha in that paper, so like slightly older symbolism, but same basic idea.

But so, you know, so there's that, and then, you know, there's kind of like with predictive coding, for example, you know, I think many people would say that one, you know, one specific commitment, at least to kind of like vanilla forms of, you know, hypothesis predictive coding architectures is like, okay, well,

prediction errors get passed upward, predictions get passed downward.

There's kind of these separate populations of prediction units and prediction error units.

Typically, it's like, okay, well, the prediction error units are in cortical layer two, three, and the deep prediction units are in pyramidal cells in layer five and six.

But then when you actually look at some of these more complicated canonical microcircuit models and things like that, that really explore in detail how you would do predictive coding in a way that would kind of match up to the actual cortical columnar architectures that we know about from micro anatomical actual brain studies.

I mean, it becomes way more complicated.

You have specific roles of granule cells, and then you have the sorts of predictions and prediction errors that are hypothesized to be passed around through different inner neurons, and then how those pass signals up.

Prediction error signals need to be inhibitory, but the between column connections are all glutamatergic, and so they're excitatory.

So you need to basically have any prediction error signal be excitatory, but then pass through some inhibitory inner neuron

at the receiving end to then turn it into an inhibitory thing.

And then as opposed to the simple kind of vanilla predictive coding architectures or algorithm, most of the canonical microservice circuit stuff has been pitched in terms of these more kind of like

temporally extended versions of predictive coding where you have like at each level, you're not just representing a hidden state, but you're also representing what they call like a hidden cause.

And the distinction between a hidden state and a hidden causes is that like,

One of them actually predicts the dynamics of the states at the level below.

Um, so it's kind of like hidden states to say like hidden state is, or now I'm mixing up which one it is.

One of them either hidden state or hidden cause.

Uh, I mean, this is just very jargony, just arbitrary choice of terminology to a certain extent, but, but.

Well, I'll say it the way that's a little more intuitive to me.

But so if Hidden State is like birds singing a song at one level, then I just have some representation about probability of birds singing song or something like that, fairly abstract.

But then that then sends a signal to represent this hidden cause thing.

And what the hidden cause thing does is send predictions down to the level below about like some series of notes, right?

Like in some sequence to the level where the hidden state at that level is like representing what note is happening at a unit at some time period.

So even there, right, those are both predictive coding, but those are two different predictive coding theories.

So anyway, I mean, so just to point out fairly long-windedly that there's just a lot of complexity in specifying what you're in fact even testing.

Um, but the, um, but the main, the main thing, I mean, the main kind of broad strokes result was just that, um, in predictive coding, I mean, there's certainly nothing that rules it out.

Right.

I mean, there's, um, there's some cool, you know, results probably like the strongest results that, um, would suggest something like that has to be going on.

is um like findings related to like omission responses right where brain activity increases when you expect to get us to get a stimulus and the stimulus is absent yeah right so like if the stimulus is stimulus comes right it should trigger some activity under a feed forward sort of hypothesis but the absence of something should never trigger yeah yeah let's just go classic schultz and montague

Yeah, but emission responses and other things like error-related negativity and a lot of the kind of more standard stuff that looks like prediction and prediction error responses that are consistent with predictive coding, they certainly don't provide really precise evidence.

you know, particular layer two, three neurons that are representing prediction errors and five, six ones that are going to predictions.

There are a couple of cool studies that have come out like super recently though, that do look like with like laminar, um, fMRI and things like that, that do are starting to support these kind of like distinct populations of error neurons and predict neurons.

So that stuff's exciting.


SPEAKER_01:
Um, it's all very, well, you know, it's a niche, but at least it's, it's incorporating it.

Well, at least it sounds like,

from your investigation, it's beginning to, well, I'd like to think that it's beginning to hone down on some specific ways that we can get more precise hypotheses and ways of testing those hypotheses.

I mean, it's, it's, it's, it's right, you know, if, you know, the free, the main papers 2009 2010.

Fair enough that it's taken a little while to get it's, you know, it to get to stop

reaching that kind of finer precision, that level of granularity, that level of, I think, to my eyes, some of that philosophical work needed to be done just to lay out exactly the parameters, if you pardon the pun, of the explorandum of what we're trying to find out.

So I don't know, it sounds promising to me.

It sounds exciting.


SPEAKER_00:
Yeah, no, I mean, I think, yeah, I mean, certainly, I think, I think we need to hold things to a given, I mean, as you say, right, given that these, you know, these particular theories are, you know, much more recent, right, than other, like, you know, computational neuroscience theories that have had a lot more time, right, to be tested.

So, so yeah, I mean, I think we need to be fair, given the amount of time that

that um you know these things have been uh these things have even been around to test and the you know the number of groups that have been motivated to you know put together studies and get them funded right to do these sorts of tests um absolutely um you know i mean and and with um and with respect to uh you know active inference i mean that's even more true right i mean the the actual kind of like empirically testable like behavioral modeling you know sorts of you

you know, sufficient, sufficiently precise, right.

Formalisms for, um, you know, like the kind of stuff that we do, you know, what's, that's been around like, well, it's since like 2017, you know, and it's, you know, been kind of evolving from there.

And, and so, um, and so, you know, much, much less time even.

And, um,

You know, I mean, I think there's a lot of things that, you know, I mean, we still need to do, I mean, along the approach that we're taking, right?

So, I mean, primarily, you know, what we've, you know, the stuff that we review that, you know, that we've done with, you know, like trying to find empirical support for active inference probably hasn't really up until now done a good enough job of doing model comparison against competing models.

You know, our focus has been largely kind of like computational psychiatry focused.

So the idea has been more, okay, let's define a set of active inference models

fit those models to behavior and see where we can find differences in those models between clinical groups and healthy groups right so we can show that like in an absolute sense right like active inference models could explain data preview you know reasonably well right like on a on a model on a task where like chance action probability would be like one-third you know like these models can explain behavior with you know assigning probabilities around like

you know, between like 0.65 and 0.7 something, right?

So, I mean, it's, you know, much, much better than chance and not too different than what you'd see in other computational neuroscience studies.

But, you know, that doesn't directly compare active inference models to, you know, other classes of models, right, that have already been around, right?

And so that's a little bit more the kind of thing we've been trying to do.


SPEAKER_01:
recently as to more definitively kind of say you know how much you know can we find evidence for active inference models that is not equal evidence for other models that have been around for longer yeah yeah yeah no that sounds absolutely worthwhile and yeah i think you're i think some people are you know they don't they almost don't want to do that because if it turns out that an rl model does better than an active inference model then like

Let's just throw in the towel.


SPEAKER_00:
Yeah.

Even that, I think, needs to be kind of understood with a little bit of a grain of salt, though, because, well, you can worry that there are tasks where the tasks are simple enough that an RL model will do a perfectly good job explaining what's going on.

and an active inference model would basically predict the exact same thing right as a reinforcement learning model and in that case you know maybe the active inference model is more complicated but you know i mean that's just because in those contexts they're you know they have a certain isomorphism you know yeah to them so i think i think you have to you have to

make these comparisons in tasks where um like a simpler model certainly could explain behavior equally well but but there's there's reason to think other models that there could be the um patterns of dynamics in the in the choice behavior that um that could differentiate right active inference models from reinforcement I like it Ryan I like it you're staying true to the to the field I like it it's good that loyalty is much needed but no I I I really do


SPEAKER_01:
admire the intellectual rigor and the academic rigor.

It's much needed.


SPEAKER_00:
And I certainly wouldn't want to say our stuff and only our stuff.

I mean, there are other people that are doing very recently that have been doing really cool stuff.

Like there's this

like the first office lesson, like Gibson or something like that.

I can't remember Sam Gibson.

Maybe I don't call me on that, but, but they did this really cool, um, um, study where they took like these two, um, a couple, or maybe it was like four.

I think it was, yeah, four different like publicly available data sets, um, from, uh, on the two-step task.

Um, two-step tap is a very kind of classic, um, RL task that like basically was designed specifically to estimate how much people are doing.

more kind of like model-based versus model-free sorts of planning.

And, I mean, super widely used.

I mean, people have shown really cool stuff with, you know, like developmentally, you know, like kids as they get older start to look more model-based.

Right, right, right.

You know, basically it means they actually start to use an actual transition function.

Yeah, yeah.

Right?

And, but, you know, so what they did is they took models

Right.

Like the classic, you know, widely used model based model free kind of combo model that typically gets used for for that task.

And then they also fit that same data to to like a competing active inference model.

And in two of the four data sets, model comparison was kind of equivocal.

Um, and in the other two data sets, active inference models did win in model comparison.

Um, for that, and these are pretty large data sets.

And again, but that's a very, it's another, you know, very kind of classic task.

So the fact that, uh, the fact that active inference did a better job of explaining two-step tax behavior, I thought, you know, I thought that was actually like incredible.

Yeah.

No.

Well, yeah.

I mean, when it starts infiltrating better than what we've done in terms of actually testing active inference models.


SPEAKER_01:
Yeah.

when it starts entering into the space where others should dominate that's you know that's when you know it's onto something ryan i know you're you're the world's busiest man uh filling out forms and and all sorts uh and so i'm incredibly appreciative we've just hit the hour and a half mark um well thank you so much for for giving me your time and educating me i feel i mean every time i have people on whether they're philosophers neuroscientists mathematicians

and so on.

It's always such a pleasure to, to learn more about the work they're doing, in terms of what you've got coming up of it.

So this paper that is coming out in the future.

That's out, people can read it, although it supposedly out in the future.

What else?

What else have you got coming up?

Where can people find your work?

How can people reach out to you if they're interested in asking questions or collaborating or whatever it might be?


SPEAKER_00:
Oh yeah.

I mean, you know, any, anything on our, you know, our, uh, lab website, uh, you know, or email me, you know, like our, our smith at Laurier Institute.org, uh, you know, feel free.

Uh, yeah, we'll, we'll link the Twitter.

I mean, you know, the typical routes.

All the, all the standards, the standard stuff.

I mean, my email is also typically the one that's like the corresponding author email on most papers.

So, um, you know, and there's certainly lots, uh, lots of ways to get in contact with you.

So desire, but great.


SPEAKER_01:
Well, we'll make sure we link everything below.

Uh, so thank you once again from me and the Institute.

It's been really, really fun.

Sure.

No, thank you very much for having me.
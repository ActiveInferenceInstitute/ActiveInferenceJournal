SPEAKER_00:
Excellent.

And we're away.

Okay.

So welcome everybody to the first episode of Active Inference Insights, brought to you by the Active Inference Institute.

I am your host, Darius Parvizi-Wayne, and today I have the privilege of speaking to Professor Carl Friston, one of the most revolutionary thinkers in the fields of neuroscience, cognitive science, theoretical biology, and dare I say, physics and philosophy.

A leading authority in neuroimaging, he is the inventor of statistical parametric modeling and voxel-based morphometry.

I hope I pronounced that correctly.

More recently, he has posited the free energy principle, which is technically a normative account of self-organization in terms of optimal Bayesian design.

or in other words, a description of how things continue to be things.

As such, his groundbreaking ideas hold profound implications, not only for biology and psychology, but also the very nature of reality itself.

Ladies and gentlemen, it is my honor to introduce Professor Carl Friston.

Professor Friston, thank you so much for joining us.


SPEAKER_01:
Well, thank you.

Thank you for that lovely introduction.

What you can say is I just invented lots of three-letter acronyms.

That's much simpler.


SPEAKER_00:
So you have.

And I sense a lot of three-letter acronyms have also spawned from a lot of your work.

Excellent.

Well...

this podcast, this is the first episode.

So as I was saying, sort of off camera, this is very exciting for me, hopefully very exciting for everyone tuning in.

The kind of the reason why it's come about is that we wanted the Active Inference Institute wanted a way to introduce the ideas of active inference, and the free energy principle, perhaps to an audience who were not

PhD holders, scholars in the area, but had a kind of latent interest in psychology, neuroscience, maths, biology, whatever it is, and wanted to explore these ideas a little bit further.

So it's a non-specialized introduction to the ideas of active inference and the free energy principle.

And as the author, inventor, discoverer of the free energy principle, we can have an interesting philosophical conversation about what that is.

it's it's an absolute delight to have you.

So I guess my first question that I need to ask for everyone listening is, could you give us a very brief, or not very brief, but a very accessible definition of the free energy principle?

What free energy is?

And how you know what this what this principle says about reality and ourselves?


SPEAKER_01:
Yes, I'm not very good at giving brief answers.


SPEAKER_00:
We have a while, so go ahead.


SPEAKER_01:
So just to put things in context, the free energy principle is a principle as read by a physicist or engineer.

That means it's a method.

much like Hamilton's principle of least action that you can apply.

So one might ask, what are the most important applications of the free energy principle?

I guess that would be active inference.

So I will answer your question and I will say what the free energy principle is, but that may have to be qualified by looking specifically at the application, which is active inference and what that means.

theologically, heuristically, what it brings to the table when you do apply the free energy principle.

But the free energy principle itself is just a description of things that persist or exist, and it rests upon a careful specification of what you mean by a thing.

It starts with exactly the same assumptions that all of physics starts with, basically a random dynamical system.

Some people describe that in terms of stochastic differential equations.

If you're not familiar with that kind of description, it's just saying

that you can describe the universe in terms of the way that states change as a function of themselves.

So it's a very generic description of any given universe that just commits to the notion that there is a state of the universe and it could be very high dimensional and that it has some dynamics.

That's where the free energy principle starts.

But the special thing about the free energy principle that distinguishes it from other formulations in physics, for example, quantum, thermodynamics, statistical and classical mechanics, all of which can be, if you like, derived from this basic description of the world as a random dynamical system.

The thing that separates the free energy principle

um or gives it its particular flavor is a careful distinction between the states of something and everything else and the way that we do that is by inducing a separation you know technically a partition of all possible states into three kinds the states of something that are internal to the thing

everything else external to the thing and then crucially a set of states that separate the internal from the external states that allow and couple the inside to the outside and that's referred to as the blanket states or a Markov blanket or boundary.

So a lot of this rests upon this notion of a boundary that on the one hand

separates the inside from the outside of something, and by a thing I mean anything from a small particle to a priest or a person.

Anything that can be individuated and possesses characteristic states constitutes something.

And with tongue in cheek, you therefore have a theory of every space thing, not everything.

So this Markov boundary or Markov blanket is a sort of crucial device in the sense that it allows you to individuate something from everything else, but at the same time,

it couples the internal to the external states.

So we're now talking about a special kind of physics, and probably a more ubiquitous kind of physics, which is the physics of open systems.

Because the inside is now vicariously open to, in exchange with,

the outside across the Markov blanket.

And just for interest, because this will be relevant later on for active inference, we usually divide the blanket states into active and sensory states that can be regarded very simply as the inputs and the outputs.

Technically, the inputs of the sensory states are defined in the sense that the internal states don't influence the sensory states and conversely,

the active states are defined such that the external states don't influence the active states.

But more simply, what we're saying is that something can be defined in terms of the influences upon it and the influences that it exerts through the blanket states on the outside world.

So just given that assumption,

that entails existence in the sense that to define this Markov blanket, you need for the system to have some characteristic states, technically an attracting set of states that you'd find the thing in.

Just by assuming the existence of this attracting set of this random dynamical system that possesses this distinction between the inside and the outside,

everything else follows and everything else simply put is you can read the internal and active states sometimes referred to as the autonomous states of a particle or a priest or a person as

Either, if you're a physicist, conforming to a particular principle of least action, and that principle of least action is that it minimizes the path integral of the variational free energy.

If you want to think of this more intuitively, because that free energy also stands in for, or is an approximation to, a bound upon model evidence, you can also say that it must be the case

that the autonomous states of anything will look as if they are trying to maximize the evidence for what we would now consider the internal states to be a model of what's going on on the outside.

Some people refer to that as self-evidencing,

So if I just repeat that from the physicist's point of view, given the assumption that you've got this random dynamical system with an attracting set that has this partition that allows you to individuate something within this system, it has to be the case that the dynamics of the autonomous states of that system perform a gradient flow on a quantity called variation-free energy.

That's it.

Is that interesting?

Well, it becomes interesting if you realize that this variation of free energy has a particular meaning and a particular interpretation that gives you a teleology for this kind of dynamic.

And that teleology is often referred to in terms of Bayesian mechanics.

Why?

Well, because the free energy, the variation of free energy

is the same quantity that measures the evidence for a generative model of the causes of some data if you're a statistician and in this instance the data that we're talking about are the impressions of the outside state on the on the markov blanket so what you're now in a position where you are now in a position you are now licensed to talk about anything that exists

in some simple elemental sense, performing inference and possessing this, or evincing this kind of Bayesian mechanics.

And in philosophy, people like Jacob Howey nicely summarizes as self-evidencing.

So you can imagine that it looks as if anything that exists

is basically moving in a way, sensing and acting upon its world in a way that's garnering evidence for its own existence.

So this is, if you like, a slight cheat in the sense of what we're doing is describing

the dynamics of something that exists and then we're saying but it now looks as if in virtue of its existence it is now it can be described theologically as acting to gather evidence for its own existence its own its own structure and that structure entails a generative model of what's going on on the outside so how does that help well

It doesn't really help in any fundamental way other than it's very pretty and it's nice to talk about Bayesian mechanics as being a sort of complement or another kind of physics that comes from the same stable as quantum mechanics and statistical mechanics and I repeat classical or Lagrangian mechanics.

However, there is a nice application at hand

once you realize that this uh once you interpret um the variation free energy in terms of a um technically a a the logarithm of some modern heavens what that means is that you can now build things you can build things and you can simulate those things

Why?

Well, because you know that their dynamics must be conforming to this principle of least action.

Another way of thinking about this is that it does a gradient descent or a hill climbing on a particular functional, which is this variation of free energy or the negative variation of free energy, which is the log evidence.

So because this log evidence is a function of a model that just is a description of the characteristic states things occupy or things are found in, you can now write down the generative model, which is just a probability distribution over the characteristic states or defining the characteristic states you want your thing to be attracted to.

This is the attracting set.

And once you've written down your generative model, you can then work out the gradients of the free energy functional of that generative model.

And then you can simply integrate or mathematically solve for the dynamics, the flow, the motion, the movement of this thing.

So I know this isn't terribly accessible, but it's the way I think of these things that you are.

Now in a position, instead of just looking at something and saying, oh, I wonder what generative model would be fit for purpose to describe this particular thing.

You can now turn that on its head and say, okay,

I want this kind of thing, and I'm going to describe and define this kind of thing with a particular generative model.

I just have to write down the probability distribution over the causes and consequences from the point of view of the thing in question, namely the external states and its impressions upon the Markov blanket.

And once I've written that down, I can now simulate the autonomous states.

I can now create a little autonomous artifact, a little autonomous particle,

that behaves in exactly the same way as something that existed that had that kind of generative model so it's a very practical if you like um it's a principle that can be applied in a very practical way to simulate things and if you can then simulate things you can start to build things you can start to um

do computational neuroscience you can start to sort of naturalize cognitive neuroscience and possibly even sort of psychology in terms of the physics that is inherent in this basic mechanics that comes from the free energy principle and you can also start to ask questions about the beliefs of another person why well part of this generative model or

The generative model can always be sort of split into two things, sort of priors before you see some data and the likelihood, the likelihood of that data given the causes of that data.

This would be the external state.

So you have prior belief over the causes, the external states, and you have a likelihood that maps the causes to the consequences, which are your observations, your sensations.

And therefore, you can now fiddle with, say, a synthetic subject or person who is being simulated in the computer

until their behavior their actions possibly their decisions matches that of any given cohort or any given subject or animal and then having adjusted the priors part of the generative model and possibly the likelihood you've now basically created a digital twin

that emulates the same kind of belief updating the same kind of basic mechanics that you can now infer is possessed by the thing in question so this is a um sort of quite a high-end application which which is um you know one of the the um the motivations for um for

developing the free energy principle and active influences and application that enables you to essentially phenotype another thing another person for example in terms of this belief updating in terms of this basing mechanics and that can be quite useful in context when you want to understand how people work and particularly how their belief updating works this becomes particularly interesting in the context of computational psychiatry

where you want to be able to explain people's behavior in terms of what do they believe about the world out there?

How are they sense-making?

How are they assimilating evidence for their world model, which you can read as the geriatric model?

and under what priors are they doing this sort of Bayesian assimilation or evidence accumulation that underwrites their decisions.

So that's one particular application.

There'll be other applications if you want to make

intelligent artifacts or you want to understand the mechanisms of belief updating at different temporal scales, belief sharing between two things that are in communication or transacting across their shared Markov blanket or indeed looking at a shared world and then one can start to get into the world of distributed cognition and federated inference.

There are all sorts of things you can do once you start to apply active inference.


SPEAKER_00:
Yeah, that was fantastic.

I mean, we have a lot to get our teeth into and probably some concepts there that are new to our audience.

So it might be worth sort of backtracking certain things and trying to find a kind of let's sort of dissect some of these concepts together.

The first thing I want to speak about is this notion that the internal states in some sense embody the kind of desirable model given their attractor state, the desirable model of the world.

I think one of the questions that one has when one enters into the world of active inference is whether an organism is a model or whether an organism has a model.

And here we sort of have a backtrack to cybernetics and the good regulator theorem that all good regulatory systems have a model of their environment if we think about thermostats.

anything alike.

So maybe it's worth unpacking that because I think a lot of people might have this intuitive sense that there's a homunculus in the brain, which has a model of the world and it's very internalist.

But maybe from a more physics perspective, we can adopt a more externalist position where there isn't this little thing which has a sort of representative picture of the world, but actually is just enacting

some model of itself, which in many ways is just some version of its external dynamics.

So let's start there, because I think that will feed nicely into maybe a conversation about internal and external dynamics and exactly what that looks like in the free energy principle.


SPEAKER_01:
Yeah, that's a great question.

And some deep issues unearthed by that.

And it's nice you've introduced cybernetics, the notion of the good regulator theorem, because in one

On one view, I think you can look at the free energy principle as just a 21st century version of that kind of cybernetic thinking.

There are other things that the free energy principle inherits from James's maximum principle, lots of ideas in psychology.

perception as hypothesis testing.

There are lots of different roads that converge upon this formulation that is on offer in terms of the free energy principle.

But that particular question is really interesting.

I have to say that I usually try to elude that question by saying that the internal states entail a generative model.

However, I am

If one was pressed upon it, then I would say very clearly that the internal states, the thing is a generative model.

It's not, if you like,

a picture that you painted in terms of that homunculus looking at its virtual reality inside its head and trying to predict what's going on.

It is instantiated, it is realized in its physics, in its existence, in its dynamics.

The reason I use the word entailed

is that the generative model is technically just a probability distribution.

So you ask yourself, well, does a probability distribution exist in some physical sense?

And the answer is, well, no.

A probability distribution is what it is.

It's a mathematical description.

However, that probability distribution will have sufficient statistics that could exist.

So what I'm saying is that, for example, let's take a Gaussian distribution.

I can express mathematically, I can write down the notation for a Gaussian, a bell-shaped distribution over some variable that goes from plus to minus infinity.

Or I can say, I can characterize

or specify this Gaussian distribution in terms of its mean and its variance and those are two numbers that are not random variables they are sufficient statistics and it may be and in fact it is the case that the free energy principle associates the

physical internal states with the sufficient statistics of these Bayesian beliefs and there's a lovely story you know in mathematics that would take you down the path of information geometries and statistical manifolds and

belief updatings as movements in these geometries and trying to understand the metrics in these spaces and how that relates to uncertainty and precision.

That's a lovely story.

We don't need to really pursue that at the moment, but the key point being made here

The generative model is not physically instantiated other than in terms of some sufficient statistics in any setting.

However, in the free energy principle, the generative model is in and of itself never actually instantiated in terms or realized in terms of its

sufficient statistics.

And that's because of something that we were talking about before, which is the dynamics.

So the dynamics is a gradient flow, which means that all we need are the gradients of the free energy.

So we don't need the generative model.

All we need is the gradients of this free energy function, or technically a function of a function of the generative model.

So if I was building an artifact, I could certainly prescribe the right dynamics by writing down a generative model.

And then I could evaluate the gradients of that free energy function of the generative model.

And then those gradients would actually drive the dynamics.

I could simulate self-organization.

I could simulate active inference.

I could simulate belief sharing, whatever I wanted to do.

But in that simulation, all I really needed were the gradients of the free energy.

And we only need the generative model because the free energy is a function of the generative model.

So in that sense, the generative model is really a teleological description that you would bring to the table to understand your sense-making of another thing.

The thing in and of itself just needs the free energy gradients.

It just needs to self-evidence by moving in the direction of maximizing, on average, the log evidence for some abstraction, which is a generative model.

But it doesn't need to know that.

just so we don't confuse people.

So moving up those log evidence gradients very much like paradoxically moving up concentration gradients to sort of clump together to keep in this attracting set and to resist the dispersion of the random fluctuations inherent in these random dynamical systems is just the same as moving down the negative log evidence, which is the

the uh the free energy itself it's it's it's it's the same thing so what does that mean well it um what it means is that the germative model is something that you bring to the table to endow the behavior of this thing with an explainable teleology

um of the kind it looks as if this thing is acting in a way to maximize the evidence or gather the evidence for its generative model and this is what i think its generative model is but notice the thinking of itself doesn't go through this process um it could of course so if i and now people things like you and me

we do have a model, and we communicate it and we talk about it, but most things would not possess that deep structure.

So you can invoke homunculi, you can invoke self-modelling, you can invoke

the kind of, not homunculus in the sense that you would normally end up with an infinite regress, but the kind of metacognitive aspect that comes from any deeply structured hierarchical generative model.

You can get that sort of homunculus-like detachment and metacognitive

perspective on things.

But strictly speaking, the generative model is only entailed by the dynamics.

And I should say that, you know,

This is probably best to think about applications of the free energy principle as being limited to simulating and modeling and understanding other things as opposed to oneself, unless one's doing a lot of self-modeling.

So in that sense, you are ascribing

a teleology, a purpose, that would explain the behavior of something that you're observing.

But notice, because you are observing it from its point of view, you are an external state, and from your point of view, all you can see is the Markov blanket.

You cannot see what's going on on the inside.

So it would be mathematically impossible for you to ever know

whether what's going on on the inside is indeed describable as even a gradient flow on a free energy functional, let alone did it have its own internal model, did it have its own beliefs.

It just looks like that from your point of view, because you'll never ever know.

By definition, if you knew, then you would, by definition, have breached the Markov blanket and that thing would cease to exist as the kind of thing that it was before.

I mean, it may sound obvious or it may sound trivial, but I don't think it is if you just look at people like me.

who spend our entire lives trying to peer through the Markov blanket using brain imaging, for example, or people doing comparative anatomy by dissecting dead things or by inventing new techniques to try and get, you can never get beneath the Markov blanket, but you can certainly sort of have certain perspectives on the Markov blanket that are closer and closer to the internal space, but you can never actually get in there.

I imagine a whole philosophy on this.

You couldn't see your own visual processing.

You couldn't hear your own cochlear dynamics.

You can't ever have access to the inside very much in the spirit that internalists would say you can never have direct access to the outside.

So I think that also applies to these stories about generative models and the Bayesian mechanic interpretation that comes along with the free energy principle.

Does that help in some way?

It's a little bit subtle.

And I can just imagine a lot of my friends tearing their hair out because they have very particular views about this.

Some people think it's a model of a model.

Some people think it's taken very internally.

Some people are externalists.

But mathematically, it's not the model which drives us.

It is the dynamics which drives us.

And that dynamics can be interpreted

in relation to a generative model, should you want to.


SPEAKER_00:
Excellent.

Well, I'm going to follow that philosophical line of thinking.

And what I want to ask is, this is an interesting point that the notion that we can get to a closer approximation of what the internal states of ourselves are, for example, but it's always going to be an approximation because piercing the Markov blanket would constitute self disintegration under the under the rules of all the principle for energy principle per se.

How, in a sense, therefore, do we know, or how much can we even make the claim, therefore, that there is such a thing as an external dynamic that goes beyond anything, goes beyond our sensory inputs?

It may seem like a deeply non-scientific question, but the way you were speaking, I couldn't help but think of people like George Berkeley and even Kant and this phenomenal noumenal distinction.

If we don't ever have direct access to so-called external dynamics, because that is part and parcel of the physical game that we're in, why can't we just say that there are no hidden states and all we get are probability distributions over sensory appearances, but we don't need something like a likelihood mapping to something that's latent?


SPEAKER_01:
um well you you you could say that and a lot of my friends do say that um and i think in the sense that um the the notion of direct access means that um you will never know in some sort of heuristic sense um what the external states are if they exist at all and um on the other hand you could also argue

that there is profound access and existentially meaningful access to the external states.

It's just organized in such a way that it has to be transacted by the Markov blanket.

I haven't had this conversation for several years

I used to have lots of pat answers, depending on who I was talking to.

But the way that you phrased the question, which was very clever, just made me think, well, what is the internalist argument?

Does it mean that because I don't have direct access, that the external states don't directly influence me, that means that they don't exist?

Is that the argument that an internalist would make?


SPEAKER_00:
Well, might it be the case that we don't need to invoke a kind of further realm, a further causal realm, an underlying ontological realm, if all we have are just the streams of sensory input and seemingly some regularities, some patterned regularities in them?

Why, at least in terms of doing the Bayesian modeling, do we need to invoke a hidden state, an underlying cause?

For example, if we just take that, if we look at the low road to active inference, we talk about how human, we talk about how cognitive creatures like ourselves predict the world, let's say a very simple level.

A question that I've always had is why does the claims of predictive processing or predictive coding more generally, why does it make claims about hidden states?

Why doesn't it make claims just about regularities within streams of appearances?

That's the way that I would pose that question.


SPEAKER_01:
Right.

I'm just mindful you've introduced predictive coding and

Just for those people who may not know the intimate relationship between all these things.

So another way of looking at free energy is the amount of prediction error.

And more specifically, if you're talking to some people, it would be the total amount of precision weighted prediction error, which means that there's another reading of minimizing free energy, which is not the maximizing the

model evidence, but minimizing surprise where surprise is the implausibility of some sensory data given your model of that data.

So in exactly the same way that if I go around gathering data that I find continuously surprising,

then that's basically evidence I haven't got the right kind of model to be to be able to explain ie predict those those data so again we have one of these sort of

completely equivalent descriptions but you know they have very different flavors that to maximize model evidence is to minimize surprise which is to minimize a prediction error which is to maximize predictability which is to minimize variation free energy and so and so forth they're all the same they're all the same thing so when you're talking about

productive coding as a particular algorithm for minimizing free energy, technically a variational scheme, which is known in engineering as a Kalman filter.

What you're saying is I'm making certain assumptions

about my generative model that means I can write down these free energy gradients as prediction errors.

So literally the prediction error, my apologies, the precision weighted prediction error literally is mathematically exactly the gradient that we were talking about before.

So just to join the dots.

So to answer your question, why is it that I need to invoke latent states in order to make sense of data?

I would argue that the sense-making is just explaining data in terms of latent states.

Those latent states are often referred to, you know,

in the technical literature as hidden states.

And what do you mean by that?

Well, it just means they're unobservable from the point of view of the phoenix, but they are hidden behind the Markov blanket.

So I would say that detecting patterns simply is invoking latent states that organize and structure those patterns.

I don't think there's anything magical about latent states.

So the latent states that are, if you like, entertained by your internal dynamics and your sense-making are not the external states.

They are descriptions that are only relevant to the generative model that you need in order to define the prediction errors or the free energy gradients in order to simulate or describe

your sense making in terms of your neuronal dynamics or your internal dynamics or the operations of say a thermostat.

But there's a twist here.

There's a reason it's called active inference because it's got a big active at the front.

So it's not good enough just to say, oh, I can understand psychology just as making lots of sense of my sensory impressions.

That's not good enough because you are coupled to the world reciprocally, which means that you actually are not only are you exposed to the world, but the world is exposed to you in an exactly symmetrical way.

And what would that mean?

how could one describe that?

And then we come back to where we started with this partition of blanket states into sensory and active states, into the inputs and the outputs, which means that sense-making is quintessentially active.

So one way of phrasing that is that, yes, I'm making sense of all this sensory data and all the patterns, but at the same time, I'm actually in charge

of actively soliciting those data, causing those patterns.

So I think once you bring that sort of inactive aspect to the table, so I notice that you treated predictive coding as a sort of generalization of predictive processing.

it might be easier if you did the other way around because then you have the grace to um accommodate action in predictive processing its most general sense and if you don't then predictive processing you know i think is an incomplete story and you have to have um as part of the processing the active solicitation the um

the garnering and the structuring and queering the world in a way that reciprocates with the right kind of information that allows you to do the good sense-making in terms of the latent states that we're talking about.

I mean, formally, and this takes us into another part of the story, which we didn't previously unpack about active inference.

So somebody might ask, well, okay, so we've got this

application of a free entry principle that a physicist might be quite comfortable with to behavior that entails some kind of sense-making cheekily with you know I like to call this sentient behavior then I get told off for that but so so how does this differ from reinforcement learning you know how your behavioral psychology

It differs in a quite fundamental way because the big thing that active inference brings to the table is that, first of all, it says you are in charge.

You are actively sensing.

This is active vision, active perception, active inference in this most general sense, which means now what are the imperatives for generating your own data?

Now, you could, if you were doing behavioral psychology of a behaviorist sort in the 20th century, you could say, well, I just want to generate those sensations that I find rewarding.

And so I've got some privileged sensory channels that I'm going to label as reward, and I'm just going to act in a way that maximizes the reward that comes in.

From the point of view of the free energy principle, that's not what happens.

What happens is I want to find those data that resolve the expected surprise or the expected free energy consequent on acting, soliciting those data.

So what's expected surprise?

Well, technically it's an entropy, it's just a mathematical measure of uncertainty.

So what that means is that the description of things that persist or exist over some particular period of time reduces now to a kind of Bayesian mechanics in which the behavior will look as if it is information seeking and uncertainty resolving.

um it will look as if it is responding to these epistemic affordances under constraints and it is those constraints that um um if you like would be the homologue of the reward um they are literally the constraints endowed by the priors on the generator model the kind of um sensory states that constitute this attracting set you know to which i am attracted

But the vast majority of the drives for good behaviour, or behaviour that things that exist would evince, is this resolution of uncertainty, this information seeking.

And indeed, that leads you into considerations, well, how would you describe that kind of information, that optimal information-seeking behaviour, if you were a scientist or if you were an engineer?

And if you were a scientist, you'd be looking at the principle of optimal Bayesian design, articulated by people like Dennis Lindley in the middle of the last century.

where you measure the quality of an action in terms of the information gain afforded by the data that you secure by acting in this way.

Exactly the same idea emerged in the context of active learning in machine learning.

This is the problem.

If getting data is costly,

What data point would I need to resolve the greatest uncertainty about my beliefs about the latent causes of those data?

So active learning, again, notice active's an explicit part of this process.

So in answer to your question about would it be sufficient just to understand

sentience in its most basic or elemental form.

as discovering patterns in data i would say no um because that denies the openness of any given system immersed in her world um in two directions you know the world influencing you and you influencing the world and so you put that into the mix i think it's very difficult to conceive of any construct um

Possibly not in philosophy, but certainly any construct in computer science or physics that would permit a complete description of sentient behaviour just in terms of sensing patterns.


SPEAKER_00:
Does that make sense?

Yeah, yeah, that makes sense.

That makes sense.

Makes sense to me.

Yes, I want to stay on action for a second, because I think the notion of the minimization of free energy or the maximization of model 11s makes some could make some intuitive sense for an agent like ourselves, in the sense that we

We have our sensory inputs which are divergent from our priors and we act to change the world to make those sensory outcomes more in line with our expectations, our preferences.

However, at a more fundamental level, one definition that we spoke about, the free energy principle, is that the internal and external states look like they're tracking one another across a Markov blanket.

Now, a question that comes to my mind when I see a definition like that is,

which is, that makes sense for me, for a sentient, conscious human.

But what would a stone be tracking?

Or what would a drop of oil be tracking, to use an example that you've used before?

I think going to something a bit more less animate, and I presume the answer lies in animacy, I think would be a really interesting avenue to go down because

It gets to these fundamental questions of how does something as minimal as a particle retain its particle-ness?

And what does that really look like when we presume that they're not conscious?

Unless we're panpsychists, but we won't go just down that route just yet.


SPEAKER_01:
Well, that's where you were taking us, which isn't...

A fun place to be.

How does one elude panpsychism in the free energy principle?

Well, I think you've just answered your own question there, so perhaps we will just unpack that a little bit.

What's the difference between a stone and a thermostat and me?

And I think what you're now asking is, are there what I will call natural kinds, but I wasn't allowed to because that has a lot of philosophical baggage.

But for the purpose of this conversation, I'll just say, are there natural kinds of things?

that display different kinds of behaviour that can be equipped with different kinds of teleology.

And I would say absolutely.

And you've just rehearsed some of the key ones.

So as you noted, to be animate is a big thing.

in the sense that the stone is not animate, in the sense that it moves itself.

If stones started wandering uphill or flying around, that would be much more interesting.

But they don't have that animacy that is characteristic of biotic self-organization.

So what does that mean in a deflationary sense of the free energy principle?

It just means that the active states are an empty set.

And usually for stones, you might also argue the internal states are empty.

We don't have to.

The stone can be making sense of its world in terms of its latent states or can acquire that into interpretations.

Perhaps that's a nice example of what we were trying to drill down on earlier on in terms of is there a generative model?

I mean, one could argue that a stone,

has a generative model of external milieu in terms of the temperature.

And the latent variable, which is a hidden state, it's a latent state, as well known in statistical thermodynamics, which is a sufficient statistic of the distribution of things.

And the temperature could well be inferred by the interior of a stone.

and therefore it is making sense of the pattern of its sensory exchanges with the world on the surface of the stone simply because it's warming and cooling and therefore tracking the ambient environmental temperature so that's perfectly free energy principle consistent um it's a very boring kind of uh of sense making but you know and it is that boring kind of sense making that i had in mind when talking about if you preclude the active part

active inference and just think about some perceptual inference then you're sort of missing the point really but I think a stone is a good example of that now would you say the stone has a generative model you could you could argue that it but you know it's you trying to if I explain the behavior of a stone but the stone that doesn't have very much behavior but you could you could start to think well now I can test a hypothesis it is actually registering a

the temperature on the inside.

And again, just to rehearse the argument, the fundamental argument we were talking about before, you will never know, though, until you break the stone, because once you break the stone, it's no longer a stone.

It's just a broken stone.

So exactly the same problems or issues confront you, even with the stone.

But focusing on that sort of big move from things with and without active states, I think then you're talking about things that are animate.

things that have animacy and act upon the world.

Is that sufficient to get the kind of behaviour that we've been talking about, which is the kernel of active inference in terms of what active inference brings to the table in terms of information seeking and having a variety of different affordances, particularly epistemic affordances,

as an explanation for behaviour, you could argue no.

A thermostat,

can act upon the world.

It can switch heating elements on or off, and it senses things by its thermoreceptors.

You could say like a Watts governor.

It now has active states.

It has inputs and outputs, and it has internal dynamics.

And in one sense, the Watts governor is a good regulator and therefore conforms to the cybernetic view of a generative model.

And it's acting in the right kind of way.

So does this kind of system have the curiosity that would be associated with information seeking and resolving uncertainty?

And you would argue, well, no, it doesn't.

So now you've got another kind of natural kind which does.

So what's the difference between what's governor or a thermostat or possibly a virus and me?

I think the key difference is a particular kind of self-modelling that just comes from being very big and a bit structurally complicated in the sense that there are certain parts inside my body and inside my brain that are so distant

from the active states that are actually moving my arms, my actuators, or are engaging my autonomic reflexes, that I cannot see them directly.

And if that is the case, the only way that I now can sense the consequences of my action is vicariously through the sensory states.

Now this introduces a really interesting distinction between me and a virus.

It means that I now will start to treat my own action as a hidden or latent cause of my sensations.

And that brings to the table, okay, well, if now my own action is being inferred as a random variable, then that was a separation between what I think I'm doing

or what my active inference would allow me to talk about in terms of what I'm planning to do or what I'm inferring to do in the spirit of planning as inference and what I'm actually doing.

So this brings you to an even more different kind of thing, which would be something like me, that now has beliefs about its own action, which is distinct from the actual real variables that constitute the active states of the Markov blanket.

And as soon as I have beliefs about my own action, I have to ask myself, well, as a physicist, I would ask, well, what are the probability distributions over my active states, those states that actually change the causes or the external states generating my sensations?

And when you write that down,

what you get is exactly what we were talking about before, which is this mixture of expected information gain, basically expected free energy, that can be decomposed into this information-seeking part and the constraints afforded by the prior part of your generative model, the things that you find very surprising.

Plus, just to unpack that intuitively,

Let's take the predictive processing surprise minimization view of self-organization and an active inference.

And that means that I am the kind of creature that exists.

Therefore, I must be minimizing my surprise.

Furthermore, I now have beliefs about my actions.

I have prior beliefs.

I must have prior beliefs about my actions.

Of course, this is all sub-personal.

So what kind of priors would I have about my own actions?

Well, I'm going to, because I exist and I'm a free energy surprise minimizing kind of thing because I exist, then I must choose those actions that minimize the surprise I expect following that action.

So that means I'm going to minimize my expected surprise.

Now, there are two ways in which I can do that.

And we've already spoken explicitly about the two ways.

One way is just to notice that the average surprise, the expected surprise is just uncertainty.

So I can reduce my expected surprise or my anticipated or my average surprise.

by getting to grips and knowing what would happen if I did that with greater precision.

So this is information seeking.

This is the sort of the curiosity.

It's the novelty seeking part.

It is exactly the part that underwrites the principles of optimum Bayesian design, getting the data that minimize my uncertainty because uncertainty just is expected surprise.

But there's another way of doing minimising expected surprise.

If I know what is very surprising, by being at very, very low temperatures or being very poor or being snubbed socially, everything that is, if you like, not characteristic of me,

given that I exist in a particular way, then any deviations from that basically can be thought of as surprising.

So if I stray beyond

very much in the spirit of homeostasis, although now we're talking about anastasis because we're talking about the future, then I'm going to choose those actions that don't just do the information-seeking, responding to the epistemic affordances, but also will elude those kinds of surprising states that are very uncharacteristic of me, being very poor, missing out on that opportunity, being embarrassed, being in pain, although pain is probably a bad example,

We're talking about the sensations that we want to avoid.

So that would speak to the prior cost or the prior surprise, the negative, which was the preferences you mentioned before.

So you can look at these constraints afforded by the priors, the pragmatic part, if you like, of these affordances.

Some people call them instrumental.

So you've got epistemic and instrumental affordances that just are the expected information gain and the negative expected cost, which would be the manifestation of the constraints.

You can look at the complement of constraints in terms of where I don't go in my sensory state space.

as where I do go, which are my preferences, my preferred characteristic attracting sets.

So just to remind myself and you and people who are listening still at this stage, why are we going through all this?

Well, because this is what comes out of a consideration of beliefs, prior beliefs about the way active state should unfold.

and that becomes very pertinent when now I have to actually embody those beliefs in my generative model.

So suddenly now I become something that is very distinct from a thermostat or a virus because now I have a generative model of the consequences of my action

and that generative model is very simple it just says a priori I will a priori think that the action that maximizes information gain and maximizes this instrumental value are going to be the most likely policies that and this you know could

could be construed as a high-end kind of planning as inference it's not as trivial or simple as a sort of KL control because we've got this epistemic part of it but it still has a spirit of planning of inference so what I'm saying quite simply is there are certain things that plan and there are certain kinds of things that don't plan

virus doesn't plan, the weather doesn't plan, evolution doesn't plan, the stone doesn't plan, and you could argue that many smaller insects don't plan.

But as you get bigger things, then they start to plan.

And I think you move from non-planning to planning, at which point, by definition,

You are ascribing to these bigger things like you and me, ascribing a teleology to our behavior that rests upon a generative model that includes the consequences of its own action, which crucially equips it with a temporal depth because the consequences are in the future.

So unlike the thermostat that doesn't need to look very far into the future, things that plan are so big that they lose contact with their actual actuators.

Now, look as if they have this sort of temporal depth, this temporal thickness to their generative models, specifically enabling them to plan their actions into the future.

So that's how I would get out of the panpsychism argument.


SPEAKER_00:
Okay, I've never heard that argument before about the distance from the actuators.

That's very interesting.

Because it seems in that sense that the temporal depth is downstream on size, in a sense.

Is that a correct reading?

And is there a reading of your argument that you've just laid out there

in which you can have deep temporal modeling, planning as inference, and allostasis as well as a sense of retrospective inference without being a large thing?

Or is this notion of being far from your actuators so that you have to start disambiguating what is your action and what is the action of the world or other people, is that fundamental?

Yeah, I mean, this is a really interesting notion, because I've actually never heard it really be elucidated like that.

So I'd love to know whether there is a way that a virus could be doing deep temporal modeling despite its minimal size.


SPEAKER_01:
I mean, to be honest, I don't know, but an intuitive answer would be no, size really does matter.

It really does matter.

So by size, I'm implicitly talking about the

the sparse coupling that underwrites the conditional dependencies in modern dynamical systems that give rise to Markov blankets and blankets and blankets.

So I'm not literally talking about how wide something is in terms of millimetres, I'm talking about the sort of the hierarchical depth of conditional dependencies and being secluded behind Markov blanket after Markov blanket after Markov blanket.

So if you're comfortable associating physical size with that hierarchical depth in terms of what a thing is, in terms of having this internal hierarchical structure, internal Markov blankets, then I think the answer is very clear.

No, you have to be sufficiently big.

So I'd be extremely surprised.

So, for example,

said this before but you know i would imagine very very small insects can't plan whereas something the size of a bee might be able to plant so i'd imagine that a drosophila couldn't plan but i i would imagine a bee might just be able to get there and it and it it starts um you know first of all that that that immediately speaks as a philosophical vagueness in terms of that temporal thickness which which is interesting i mean

Is it categorical?

Can I say this kind of artefact can be explained in terms of a generative model that does not cover its own action because it has direct access to its actuators because it's sufficiently simple and small and there are no Markov blankets that intervene between effectively the sense-making part

For example, sending predictions in the predictive processing spirit down to the actuators.

Whereas this thing is so big that there are inevitably, with probability one, so many Markov blankets that intervene, there is no now direct access to action upon the world.

That might be a bright line.

It might be a qualitative distinction between things that do and do not have a representation of their own actions.

On the other hand, the counter argument, I think, would be this more vague notion that even a thermostat

could be construed as having a little glimpse of the future simply because it could be um cast as say a pid controller which basically um invokes now rates of change and as soon as you invoke rates of change you've got sort of you know a mathematical image of a little trajectory into the future so it could be a great it could be a graded thing

I've lost the importance of your question.

I'll ask you a question again.


SPEAKER_00:
It was probably unimportant.

but this is really sorry i'll go again this is a very interesting line i'd love to i'd love to dive down into so what so my interpretation of this my my reading of this is that in a sense having bundles of markov blankets embedded in markov blankets leads to the inevitable inference that

There is something like me which is doing something and there is something that's not like me which is also causing sensory data that I'm receiving.

That makes sense.

Now, let's tie in temporality to this.

Does this mean that in a sense, temporality is a corollary of selfhood, in the sense that the selfhood is almost more fundamental than the temporality because it comes from

the dimensionality of the things that are mark of blankets upon mark of blankets and how can we integrate temporality and temporal thickness or depth into this picture right that's absolutely right and um you could tell the story in a number of different ways you know if you're talking to um


SPEAKER_01:
for example, Chris Fields or Maxwell Ramstad, they might start talking about irreducible Markov blankets that necessarily require the notion of nested Markov blankets and that there is some

one or more core irreducible Markov blankets that would have this aspect of looking at one's own inference processes and acting internally and you get the notion of mental action and you can then start to work towards qualitative experiences in terms of self-modelling and what that might look like.

The other way that you could tell this story is to invoke the renormalization group if you were a physicist and just know that when you're talking about any hierarchical generative model,

there is inevitably a coarse graining that includes time in that depth, which means very simply that the deeper parts of my generative model and indeed of viruses are

on a free energy principle view, encoding beliefs about things that change more and more slowly as you go deeper and deeper.

So this is separation temporal scales.

So a commonsensical view of this is that the neuronal representations down near my primary auditory cortex or indeed in the brain stem,

are representing very, very fast fluctuations in pressure and frequencies and sort of scenographic-like representations that are changing possibly even in fractions of a millisecond if you're an owl.

um uh but certainly over over 100 milliseconds then i move up to sort of you know uh primary auditory cortex and i may be in the realm of some um hundreds of milliseconds and sort of frequency glides that defined phonemes and i move up to secondary auditory cortices and then through the auditory hierarchy getting to the level of

words right up to say you know some parts of prefrontal cortex where we've got entire semantics and syntax and narratives and stories start to emerge and as every time we go deeper into the hierarchy we slow down or we extend the temporal compass thereby providing a context

for the faster influences and fluctuations at the level below.

So again we come to this notion of size entailing a certain kind of depth, in this instance a hierarchical depth in the generative model, that, as you say, necessarily goes hand in hand with a temporal depth and a separation of temporal scales.

So I think you're absolutely right.

There has to be a temporal aspect to deep generative models.

And by deep, I just mean that there are nested Markov blankets.

So to define a hierarchy is only defined in terms of the conditional independences and implicitly the Markov blankets of one level in the hierarchy that

defines it as a level in the hierarchy.

Without the Markov blanket the hierarchy would not be there.

So that's another example of this sort of

being open but being closed and being able to individuate something from something else and this is the thing is just the level of a hierarchical generative model that we're ascribing to the dynamics of some internal states.

I think there's a third story you could tell which is possibly where you're going which is how it relates to consciousness.

In the sense, and this is a very simple story, that if I am forming beliefs and doing basing belief updating as prescribed by active inference,

about the consequences of my actions then that has to be in the future as we've just said and perhaps certain things think deeper into the future than other things but notice what we've done here it's about my actions so not only have you committed to temporality and temporal depth and temporal thickness

as an attribute of things that plan and are more like you and me.

But you've also committed to agency.

So now you've created from something that was previously not an agent into an agent.

When I'm using agency in the sense that one would not say that a thermostat had agency, but you would say that I have agency in the sense that I have beliefs about the consequences of my action.

And I will or can be described as using those beliefs to select a particular action.

And coming back to predictive coding formulations, that selection would basically, I'm going to predict the state of my motor plant or my autonomic nervous system and provide the right set points and then the action.

the active stage will actually reflexively fulfill those prophecies, those predictions, and everything is working properly, I'll be a good allostat, or in Roshbi's terms, a good homestat.

what what you've done there is is bring agency into the game um and i think that again this speaks to the inactive part that you know you now because you've got beliefs about the future and specifically the future that you have caused that you have authored you have now become an agent

So we have, I think, got this stage to consciousness and self-awareness, but I think we've got the necessary foundation that there are only certain things that have to be agents, I think, to be conscious of themselves as agents, as doing things, as things that exist and exist.

in a world in the sense that their existence is manifest in terms of the consequences of the way that they act upon that world.

If it wasn't, you wouldn't need any of this.

You wouldn't need this temporal thickness.

You'd just need the apparatus of a stone or a thermostat to do your sense-making.

but as soon as you've got agency in the game i think then you've got you've got something which takes a step closer to um having a very deep and possibly it's this irreducible mark off blanket um um there of the generative model or level of a deep generative model which actually now starts to entertain the hypothesis i am an agent um

you don't need but you could and you have to ask yourself why would you ever want to entertain that hypothesis i am an agent um so this would be minimal selfhood just a representation um that i can be in different states of mind when it uh that and those states of mind

are something that underwrite or I can condition on in terms of what I'm going to do.

And that doing could be internal.

It could be how I deploy attention in terms of precision weighting in predictive coding.

So it could be covert action or it could be overt action.

I could now select certain plans at a lower level that would be then enacted by my motor system or my autonomic system if I was that kind of creature.

Does that make sense?


SPEAKER_00:
It makes sense.

And it's opened up a can of worms in my head.

Well, inevitably, because we brought in lots of concepts here and I want to try and unpick them for people at home and for myself, but it's fascinating.

So I'm loving it.

So let's keep going along this, along this road.

We've got agency, consciousness, and temporality as these three things I want to have a little look at.

Starting with the first thing you mentioned as a response to my question about temporality.

that we have a deep hierarchy in which deeper levels of the hierarchy are tracking slower fluctuations in the external dynamics and therefore contextualize and constrain the higher order faster dynamics.

Are we, in this model,

we invoking time as a fundamental aspect of those external dynamics?

Or is it an inference that the organism is making about the nature of reality that it's experiencing?


SPEAKER_01:
Well, that's again an excellent question.

It certainly does not address time perception in terms of the psychophysics of time perception.

Does it address time in a more fundamental sense?

I don't think it does in a sort of folk psychological sense.

So the answer to that question normally is framed in terms of relative movements in a belief space.

And so if you think of time as part of some metric space time, then what you're talking about is time as something that can be measured.

time that has a distance say milliseconds or seconds up and the like um could me um could me as a generative model um

ever have access to or build hypotheses of time as a metric.

Yes, we certainly could.

However, it would be very difficult, you know, ignoring sort of internal clocks and the like.

Well, naturally, no, you have to ignore internal clocks.

You then ask, how do you do it?

And normally what the story is,

It's the number of moves you make at one level relative to another level of a deep generative model.

And the number of moves is quite crucial here in terms of tying that down mathematically.

And my favorite way of thinking about the number of moves is basically something called the information length.

And the information length refers back to something we've mentioned before, which is information geometry.

So as if you think of our so one picture of the free energy principle is quite a technical picture, but I think it's quite useful.

is that our internal, say, brain states or the internal states of a thermostat encode a probability distribution, a Bayesian belief, a conditional belief about some latent states, which means that for every state of the brain, there is, if you like, a point in the state space of all the physical states of the brain.

But each point in that space now stands in for a probability distribution.

And that means that this is a special kind of space because it is equipped with a metric.

What is that metric?

Well, it's the KL divergence.

Well, the path integral of infinitesimally small KL divergences as I move through this space.

So this is what defines information geometry.

The information geometry is a very particular bit of mathematics or theorizing.

that applies to very very special state spaces or manifolds where every point in that on that manifold corresponds to a belief or a probability distribution so now what if the metric

is the KL divergence or the information length which is related to the accumulated KL divergences as I move from one point to another.

What we are saying is that the number of moves I make, the number of units as measured with this kind of metric, this information geometric measure, which actually technically is this Fisher information,

is basically a description of the precision by which I measure whether I've changed my mind.

So if we say that I've made n moves, that means I've changed my mind n times.

And I know that because I can measure the information length and I've actually moved by a measure that is large simply because it is large in relation to the precision that provides the distance between any two points on this.

So what that means, to put it very simply, is that time,

can be measured by the number of times I change my mind at level n in my hierarchy, divided by the number of times I change my mind at the lower level.

So now you've got a relational, you can gauge time by the relative degrees of belief updating, the information rate, which is the

confusingly the rate of change of the length with time but where time is now contextualized by the level above or by the level below so you have this very relativistic view of time and you could have different levels of you know I would imagine if you you know attempt different levels of your hierarchy you could get a very different kind of time perception indeed we know that

attentional set and in conditions where say taking psychedelics or in things like Parkinson's disease where you get changes in the neurobiological encoding of uncertainty and therefore this Fisher information metric you can get distortions and different senses of time so what I'm saying is there were probably as many different

senses of time as there are levels in a hierarchical model.

And it can only really be measured in a relative sense in terms of how many moves do I make at one level for any given move at another level.

So what we've done now is, if you like, take temporality off the table to the extent now that we're now just talking about movements in an information space or a belief space that has this kind of information geometry.

these are really fascinating issues uh i think yet to be fully explored um my my friend zaf um um has done probably some of the best work in in in this area um but you know it is it is to my mind um a really interesting issue because you know it really forces you to think about the um

what you mean by separation temporal scales and what kind of course grading over time is implicit in this kind of move from one level to the next level.


SPEAKER_00:
Does that address what you were... Yeah, I mean, as I was saying, this is all really interesting.

And again, something I hadn't thought of before in terms of

in terms of this in terms of this contextualization process being about moves, which implies to me that there has to be like this sense of time is contingent on these moves that are being done at the different levels of the hierarchy.

does that process itself not imply some temporal dynamics?

How can one distinguish a move of 10 relative to a move of five without having segmented moves in some kind of space-time geometry?


SPEAKER_01:
Yes, and it is exactly that segmentation that appeals to the information geometry.

So how would you actually do that segmentation?

How would you measure the amount of belief updating

in universal or clock time at one level of a hierarchy.

And this is where the information geometry comes in.

It's basically the number of different brain states that you have entertained during your movement on this statistical manifold, which is also, I think, very nicely articulated in terms of belief updating.

So you can literally think of this as the space that would be traced out if I were to plot the activity of all my neurons on every axis of some very high dimensional state space.

That would be a statistical manifold.

My belief updating, which is a continuous process, my gradient flow,

um is literally moving on this manifold the distance i move is the information length that literally scores the degree to which i change my mind it's also mathematically the information gain um as measured well closely related to the information gain as measured by the kl divergence between my prior and my posterior after i do my belief updating i'm moving on this manifold and the question is you know how far have i moved

And can I segment that into units?

And that's basically what the information length allows you to do.

So I love this picture because if you just think about what kind of flows on this manifold would be prescribed by the free energy principle.

Because the free energy is a functional of the beliefs encoded at each point, then the free energy is a landscape on the statistical manifold.

So we are perpetually falling downhill on this manifold.

The free energy gradients are themselves and the free energy is itself changing because my beliefs are changing.

So I've got this lovely itinerant model or picture in mind where I've got a statistical manifold.

It is now where I have gradient flows, where I'm flowing on this manifold using a gradient flow on free energy, where the free energy now provides a Waddington-like landscape.

chasing minima all the time and of course that chasing reflects the fact that the free energy itself changes as you move around the manifold because your beliefs are changing and the free energy is a function of belief and then you introduce this uh this notion which i think is really important um the notion that the higher level of a deep model um provides the context

and what would the context look like in terms of this free energy landscape well it means that the landscape would change so you know i've got this gradient flow which is um um from um on a fast time scale just flowing down the landscape down the landscape but at the slow level the landscape is itself now changing all the time

So I'm perpetually chasing minima and moving minima because the gradients themselves are contextualized and are changing slowly.

So you get this deep structure to the temporal dynamics, which has this very non-Markovian aspect.

So now we've got this interesting picture.

where things like you and me that are big, where big just means that there are lots of Markov blankets that constitute my deep generative model that enable this separation of time scales.

You've got this picture where each level is now providing the landscape for the level below.

with very, very fast changes, but of course these are now changing, the landscape is itself changing, so that now you've got a non-Markovian model of a universe that we know is Markovian.

So we know because we started, if you remember right at the beginning with this notion of a random dynamical system,

that is usually associated with things like a Langevin equation or a Markovian process.

So we're starting from a Markovian universe with effectively no memory.

And now we've found things inside this universe that look as if they found a non-Markovian explanation, a coarse-grained

sensible and sense-making explanation for their sensory impressions that is quintessentially non-Markovian, that has this deep temporal structure of the kind you find in language.

The whole point of, say, for example, hierarchical Dirichlet process models of natural language right through to the current focus on transformer architectures in large language models

They're all about breaking the Markovian.

They're all about finding non-Markovian explanations for what, at the end of the day, must be a Markovian process.

And you may ask, where does that come from?

Well, if we spend most of our time trying to model input

where that input is generated by things like us and we are complex deep things that generate non-Markovian actions and in fact our universe now becomes non-Markovian simply because we have other things in it that are sufficiently big to have these deep generative models.

And then you get into language and communication and the itinerancy, which is, I think, quite unique to us.

I don't think you'll find this kind of dynamics if you get too big.

And certainly, as we've already discussed, it can't exist at a very microscopic scale.

So even in your head, your macromolecules in the 16th dendrite in the CA1 field of your left hippocampus, that doesn't do any planning.

It's too small.

uh it doesn't have that requisite depth of or nesting of markov blankets um uh so small stuff can't do can't have this separation temporal scales and this particular deep itinerancy and um sort of um you know markovian breaking like property but interestingly when you get too big

you also can't do it either.

So the motion of the heavenly bodies, the moon and the sun and the like, they don't plan.

And it's interesting to ask, well, why not?

Well, because they don't have...

because they're so big, all the random fluctuations in their world have gone away, and you now are left with classical mechanics.

So you could sort of get Thomistic chaos in n-body problems, but there wouldn't be a description

um that would um be usefully interpreted in terms of planning as inference or active inference in the way that we've been talking about so i think there's a golden locks regime you've got to be big enough but not too big uh to do this to do this kind of planning wow amazing yeah i mean it's um


SPEAKER_00:
it's bringing up all of these existential, potentially fluffy questions in my mind, which I would like to sort of ask at least one, which is, we could ask why, let's take the macro structures, the celestial bodies,

black hole universe itself, we could ask why in a mechanistic mathematical sense do they not have this kind of non-Markovian temporal horizon?

So there's a mathematical answer to that, or a physical in the sense of being explained by physicists, there's an answer to that.

There isn't necessarily an existential answer to that.

And I guess I do not expect a concrete answer because I'm sure there isn't one, but millennia has been dedicated to the question of the human condition and why maybe that itinerancy is so fundamentally defining of our nature, which is that we can't seem to kind of sit still.

We are open and the world is affecting us and we're affecting the world, as you said, and we're in this constantly dynamic coupling.

I guess I would love to just hear intuitive thoughts you might have on why the universe would unfold in such a way that you do have a Goldilocks zone.

Why is it the case that things like us have this wandering openness, rather than just being either in the soup, or being

firmly rigid forever, if you have any thoughts on that.


SPEAKER_01:
Not well-formed thoughts, but I mean it is a fascinating thing to reflect upon.

I think the answers that someone like me would give you would inherit from a sort of a classical view of

physics as opposed to a quantum information theoretic view so you may get a very different answer if you ask somebody like Carlo Rovelli or Stephen Wolfram or you know my friend Chris Fields but if you're happy with a sort of

101 physicists' response to that.

The way that I would look at that is through the lens of the renormalization group, which just says that things exist at different scales and there is a

you are looking for explanations of that scale-free behavior in terms of laws that are dynamics or Lagrangians, technically, that are conserved over different levels.

And for the free energy principle, that is just the dynamics, the gradient flow on the free energy defined at each level or each scale.

and that leads you to then a view of

the same kind of things sorry the same kind of processes and the same principles applying at each and every scale of the universe so that you would add a very very fine time scale and a very very fine um spatial scale say the quantum scale you would have lots of very very fast hot stones moving around where the random fluctuations predominate

and there is very little of this kind of long-term itinerancy.

Now, technically, that results from a certain aspect of the way that you can always decompose any dynamics, any gradient flow.

So I've spoken about the dynamics exclusively in terms of gradient flows flowing down

the flowing down the free energy landscape, say on a cisco manifold, being my favourite picture of that.

That denies another really important aspect of the flow, which is the flow orthogonal to that gradient flow, known as solenoidal flow or divergence free flow.

So the way that I look at this through the lens of a sort of classical physicist that would predicate everything on this random dynamical system or Langevin approach is that you've got a continuum between the very small and the very big

where now we're not talking so much about the degree of nesting of Markov blankets, but simply the contribution of random fluctuations to the dynamics.

And the important thing to bear in mind is that any dynamics, any flow, has this solenoidal part, this conservative part, and this dissipative part that inherits from the random fluctuations.

If you're very, very small,

then all the random fluctuations have not yet been averaged away and the dissipative part predominates over the conservative part the solenoidal part which means that at the very very small scales you are effectively in the world of thermodynamics where you're effectively ignoring the solenoidal part

So everything is dissipative.

You get fluctuation dissipation theorems, integral fluctuation theorems, and then you can drive generalizations in the second law and all of that good stuff.

And then as we get bigger and bigger and bigger,

The coarse graining implicit in what's known as an RG operator in the renormalization group, I look at it as a grouping and dimension reduction.

A coarse graining entails an averaging, and as you average, the random fluctuations average out to zero.

So now the dissipative part disappears, and you're just left with this solenoidal flow

which is just the conservative dynamics of Newtonian mechanics and Lagrangian mechanics.

And the solenoidal aspect is just a description of like the Moon going around the Earth and the Earth going around the Sun.

It's things that have orbits, stable orbits or quasi-periodic orbits that are approaching deterministic simply because they are very, very big.

So the interesting thing happens in the middle, where you've got a mixture of dissipative dynamics and conservative dynamics,

What would the conservative dynamics look like?

Well, they would look like this sort of itinerant oscillation.

It would look like the same kind of classical conservative oscillation that the moon does, but it will be much more itinerant.

It'll look like a biorhythm.

It will look like a life cycle.

It will have that property that we associate with living movement.

So, we talked before about the fact to be autonomous is to move, it is to have active states.

One would also argue that movement in and of itself is not sufficient.

The moon moves, the oceans move.

Are they biotic in their self-organization?

I think you'd be looking now

for a special kind of movement which emphasizes this mixture of solenoidal and dissipative dynamics.

So sandwiched in between the very big and the very small is a Goldilocks regime.

where indeed you do have this opportunity for the biotic kind of self-organization that characterizes our existence, that is not only manifest in terms of delicately crafted solenoidal diversion 3 dynamics, but also nested at different time scales.

so just think about all the different time scales that your body entertains solenoidal dynamics rotational dynamics from the fast gamma oscillations again in that uh hippocampal neuron through to um your um cardiac cycle through to your respiratory cycle through to the your

diurnal cycles through to your slower cycles right through to your life cycle and so on and so forth and then you take it up to any scale you want but the key thing the point being made here

is that not only do you have this admixture of dissipative gradient flows that underwrite self-evidencing this is in the context of this solenoidal flow at multiple scales at nested scales so again we come back to this deep nesting so on that view there will be a golden locks regime

and of course it is only at that in that regime will you ever get systems that start to self-model and communicate and co-construct their their niches will you get cultural niche construction will you get the kind of will you get language will you have conversations like this it's not going to happen at the level of macromolecules or cells and it's not going to happen at the level of heavenly bodies but it will happen

at our level.

And clearly has happened, if I can believe my senses.


SPEAKER_00:
Yeah, I guess the Yes, I guess the I mean, my follow up my my inevitable follow up would be why are there these different sizes, but I don't I don't want to get into that because it's just turtles all the way down about the


SPEAKER_01:
why i don't know i mean we can bring it to close very quickly but that's it it is turtles all the way down that's i i think a fundamental insight uh and once you once you commit to that everything starts to make a lot more sense so um you know just in terms of um

know how long will my markov blanket last well it will last you know for a long time at my temporal scale but the um the scale above um will last much longer so i have an environment and a niche that lasts longer than i do and my body lasts longer than anyone's cell and my cell less lasts longer than any one intercellular component and you know that component will last longer than any particular macromolecular

configuration.

So it is all the way down and you need it at each level.

You have to have the context being a thing, having its own Markov blanket at its scale in order to provide the context for the faster coming and going of Markov blankets at the lower scale.

But in the same sense, because the larger scale inherits from the dynamics of the lower scale,

There's a circular causality, which people like George Ellis would cast in terms of top-down and bottom-up causation.

Put simply, what that means is I cannot exist.

Well, let's put it another way.

My hippocampal cell cannot exist unless my brain is in good shape.

My brain cannot be in good shape unless I'm doing good active inference and relating correctly within my family and my conspecifics.

My conspecifics cannot work, exist, unless there is a biosphere that sustains that kind of life.

The biosphere cannot exist, and so on and so forth, all the way up and all the way down.

At every level there are things, and those things have to have Markov blankets.

and at every level they can be construed as doing some kind of elemental inference or self-evidencing um the story we're telling though is the special kind of self-evidencing that comes out at a particular scale but the existence of that scale requires exactly the same um i repeat from the perspective of the renormalization group it requires exactly the same dynamics or lagrangian or functional form of the dynamics um at you know at every level and you know

on this view because the explanatory target of the free energy principle is basically relational it's trying to understand self-organization of a thing that can be individuated in this context and in terms of what it is contextualizing it's a very myopic ambition it doesn't describe you know

cosmology or the universe.

You're not going to get electromagnetism from the free energy principle.

And so it doesn't really have to worry about what is the largest scale or what is the smallest scale.

You can keep on going all the way down.


SPEAKER_00:
if you had to hedge your bets because i guess it is a hedge better bet hedger rather um does it stop does does the does is it or is it just it just goes on mark of blankets on mark of blankets


SPEAKER_01:
Yes.

Mathematically, yes.

It's a little bit like you asked me, where's the edge of the earth?

When does the earth stop?

If you're committed to the renormalization group, now it just keeps on going forever.

I'm quite comfortable with that because

it emerges from a question, which is, what is a state?

So we've been talking glibly about state spaces, statistical manifolds, and states of affairs, and hidden states and latent states.

To have a state space, and specifically to have a state space that can be interpreted or construed as a statistical manifold,

You have to have states, but where do states come from?

And the argument, sorry, I shouldn't ask you.

The argument is that they are the states of things.

Where do things come from?

We just said it's a partition of state space.

Where do the states come from?

If you try to answer that question,

then all you have to do is basically determine the mapping between things or particles and states.

And the answer is quite simple.

It is basically one way of looking at this sort of, or using the apparatus of the renormalization group, that the state of a thing

is the state of its Markov blanket states.

And then that state itself now has a Markovian or a partition into Markov blankets.

So you now get blankets and blankets.

And so that you can resolve the paradox or the catch-22.

What's the state of something?

Well, what's the thing?

Well, a thing is determined by conditional independence among the states.

You can resolve that infinite regress just by saying it is just a recursion.

States at one level inherit the averages of the Markov blanket states of things at the next level.

But at every level, there are things.

So you have this, you just keep on going down.

I should say,

There are people who believe that there is a smaller scale or it is sufficient.

I'm not absolutely sure about this, but if you ever get the chance in later life, if you can talk to, again, people like Carlo Rovelli or Stephen Wolfram, from the point of view of quantum loop gravity and from the point of view of the Rulliad,

then there is this notion that there is some irreducible smallness scale beyond which you can't go, and then the game is how do you get a continuous world out of this sort of quantum, discrete, very, very small description of things.


SPEAKER_00:
Yes.

Yeah, yeah.

Strange loops makes me think of Hofstadter and strange loops also makes me think slightly of the holographic principle.

Well, you just mentioned there about projections onto the screen.

This notion of reading off the Markov blanket.

You said something that I said, we had three things I wanted to talk about.

And we've covered temporality, albeit over a long temporal scale.

Once very quickly ask, you mentioned

in passing about celestial bodies being nearly deterministic and my ears pricked up.

On an internalist model of the free energy principle, I sense that there's actually a kind of licensing for free will because we have this internal model and we act upon the world as to confirm the model that we wish to observe.

an externalist perspective where we really are no different from another physical entity except for the fact that we have the possibility of moving things and and acting to to self-evidence there isn't so much space for free will but my ears picked up when you spoke about agency and when you spoke about the celestial bodies being nearly deterministic because it makes me think

It seems to imply, at least heuristically, that

being open to the environment, being an itinerant being licenses agents agency, but I want to make sure this is true metaphysical agency in the sense of not hard determinism, um, or whether this is agency in the sense of self conception as a free agent, because those are two different things.

Um,

it's we won't be denying that we perceive ourselves to be free.

I guess the fun, the more fundamental question there is whether that's an illusion or not.

And so I was wondering whether free energy principle has anything to say about a kind of libertarian versus deterministic perspective on free will.


SPEAKER_01:
Um,

Well, I mean, you could certainly use the free energy principle to tell stories that speak to those arguments.

And I think you've picked up on some of the key

parts of those stories um so you know deterministic chaos that you get with n-body problems when thinking about so massive bodies dancing around each other um you know there is still chaos in mix in the mix um so there's an unpredictability and sensitivity to initial conditions in in the good old-fashioned way of um non-linear dynamical system theory of a deterministic sort

But once you get to things like you and me, you're really in the world of stochastic chaos, so the stochastic bit inheriting from the random fluctuations, where you've got two kinds of, if you like, two aspects of uncertainty, and implicitly then in modeling that uncertainty,

the opportunity to talk about free will and selection.

One interesting twist here is that if you want to apply the free energy principle as a principle of least action to things like you and me, what you have to imagine is that we are deterministic things in a stochastic and chaotic world.

So in making sense of things, we only do so at a particular scale.

We're talking before about the averaging away, when I'm sufficiently big to average away my random fluctuations.

Under the free energy principle, that literally means you have to take the average of lots of neurons.

So the kind of sense making, the kind of dynamics that you would simulate under the free energy principle applies to and only to

the ensemble behavior where you can average away any of the stochastic aspects so you will not get you will not be able to deduce evidence or um solicit or find evidence for the free energy principle by looking at single neuron or single synaptic it has to be at the ensemble level

where there are all sorts of arguments why that has to be the case.

But from our point of view, it's when the random fluctuations are averaged away.

So I'm disqualifying what I'm saying.

I'm not saying that the brain is stochastic or shows stochastic chaos.

What I'm saying is that the brain has to provide an apt explanation or approximation

to explain stochastic chaos.

And crucially, the stochastic chaos associated with our own behaviour, with the consequences of our own actions.

So you've got a deterministic system trying to now model a non-deterministic stochastic chaotic system.

And in so doing, well, that's easy to do because, of course, we're talking about

that modeling being in terms of belief or probability distribution.

So that's absolutely no problem.

So on that view, I think you could easily motivate free will of a certain kind in the sense that when you're planning, you have effectively to select

amongst different models of the future, technically based on model selection.

And selection is an act.

And so again, we have a different kind of agency here, but it's a kind of agency inherent in the selection amongst competing plans or policies, narratives, paths into the future that I think has all the space to say that this, and it's you doing the choosing.

Remember, you can't

There's nothing on the outside that can influence directly that choice on the inside.

So from your point of view, you certainly have free will.

Would it look like you have free will from the outside?

I think that's a more vexed question.

Of course, you'll never know, but I think you'd probably be in the game of, again, practically how I spend most of my day job, is inferring to what extent

is this artifact or this person doing good planning as inference, good planning.

For example, if somebody is in a coma or asleep or has a psychiatric disorder, you need to know

to what extent they are and in what way they are self-modelling.

This is a slightly sort of clinical and abstract example, but it illustrates the point that you will never know whether something has free will.

All you can do is infer

that they are behaving as if they had a generative model that entailed a self-model and thereby a model of the consequences of action and therefore as if they were a true agent and possibly they may know they're a true agent they may not you know they may be again you know that would require another level of of the Markov blanket is that what you had in mind


SPEAKER_00:
Yes, I mean, I think, I guess the kind of hardline determinist response would always be pushing you to say whether that organism or particle could have done otherwise.

So it may have this reflexive quality of okay, I'm doing Bayesian model selection, to my sense of self, and we will come to consciousness, because I think that's the kind of crux of this issue is why is any of this happening online anyway.

But

it might have this self reflexive notion of being a free agent, but the hard line determinists from the kind of external position of physics might always say, well, could that bundle of atoms ever have done otherwise, in terms of its Bayesian model selection?

But it's, again, it might just be one of these sort of loops that just keep spinning.

Yeah, it's all fascinating.

I had a question that I've just popped out of my head when I was talking about free will, which was, I will come back to it.

It will come back to me.

Consciousness.

I don't know if we can call it a red herring yet in the academic world, because it's become a very hot topic and people are getting very heated about it.

And there's no need to bring up particular examples of when people have got heated about it, but people are very invested in their theories.

And I guess rightly so.

It seems to be this one thing that people can't seem to couch within their theories of everything.

if they have one?

Starting with a very simple question, do you consider it to be a real problem?

Do you take something like the hard problem seriously?

Or for you, is it more of a meta question?

Or does a deflationary account just do the job in terms of the hard problem?


SPEAKER_01:
It really depends who I'm talking to.

as my friend Jeff Beck would say, it depends which pants I'm wearing.

I think very much it's a question of a conversation and you have to infer how much investment the person you're talking about has in this issue and what position they're taking.

And I mean that just in terms of being polite and engaging with integrity in scientific debate or indeed philosophical debate.

but I also made it in a slightly more fundamental way that consciousness is an illusion.

And I think illusions are fantastic.

They're literally fantasies.

And I think that's what makes our brains fantastic organs.

I think they're hypotheses.

I think that the illusion is the thing that we test.

It is the thing that generates the predictions in predictive processing.

So I think consciousness is another illusion.

very much along the lines that one reading of Andy Clark's basing qualia paper that even qualitative experience qualia are just illusions they're just things they're just constructs we bring to the table including selfhood they're just hypothesis illusions that are particularly apt at explaining everything that we sense and making good sense of it as a parsimonious hypothesis

Consciousness itself, I think, is exactly that, because it only applies, the hypothesis, is this thing conscious or not, only applies because there are other things in my universe that I have to model, like you.

So the question is not whether I am conscious, the question is whether you are conscious, and of course we've already said that is unanswerable because you have a Markov blanket.

So it's an unanswerable question,

but crucially it's a question about the things I observe that I'm trying to make sense of.

It only becomes, I think,

only in the spirit of the meta-hard problem or the meta-problem.

It only becomes vexed when you make the mistake of asking, am I conscious?

I don't think that's what the question is there for.

The illusion is not for me, it's for you.

The illusion is the consciousness is a construct, a hypothesis, and a very plausible one, and all the evidence that I have at hand would suggest that you are conscious, and indeed,

literally because there is that evidence it is there in the sense of an evidence maximizing illusion hypothesis or component of my generative model so I go through that just to make it clear that you know the answer depends upon who you're talking to but also who you're trying to ascribe consciousness to so if I was talking to

Well, if I was talking to people like Jacob Hoey, the first thing he'd say is that the free energy principle is not a theory of consciousness.

And in fact, he likes saying that at every opportunity.

So there's nothing in the free energy principle that has anything explicit to say about consciousness.

On the other hand, people have used it in different ways to address the hard problem.

You could take the view that Andy Clark takes, that there isn't a hard problem, and perhaps even with David Chalmers' work recently on the meta-hard problem, that's probably a more interesting problem.

Or you could grasp the nettle and start to talk about it.

If you were talking to my colleagues, Chris Frith and Adam Saffron and Maxwell Ramstead, I think inspired by something that you mentioned earlier on, which was the

holographic principle in quantum information theory where the holographic screen now plays a role of the Markov blanket that separates the inside and the outside in terms of bulk reading and writing to the holographic screen.

If you're talking to him, then he would start to talk about these minimal screens, these minimal Markov blankets, and develop quite a nice argument that if you can't reduce, in the sense that we're talking about nested Markov blankets inside a Markov blanket, if there are no more reductions at hand, then the only way that the internal states of that irreducible Markov blanket

can know about their own existences by acting upon lower levels and therefore there's something unique, there exists a unique Markov blanket that maybe have the attribute of consciousness.

If you're talking to somebody like Vanier Weiss, he would tackle this in terms of

trying to resolve a dualistic position by offering a dual aspect monism of a Markovian sort and that rests very heavily upon this information geometry.

So if you remember before we're talking about sort of a state space equipped with free energy functionals on it that plays a role of a syscal manifold.

But there are two statistical manifolds that jointly sit in the same space of, say, neural activity.

One is that for any given point, there is a probability of being there from a point of view of neuronal activity and the thermodynamics.

So this would be a thermodynamic kind of information geometry.

And then the other one is what the free energy principle uses, which is using that point to encode beliefs about external states.

So you've got two information geometries, belief structures, if you like, that supervene on exactly the same material process, one of which pertains to probability distributions or beliefs about the internal states.

and the other one is probability distributions or beliefs encoded by the internal states about the external states.

So you could construe one as being the mental and one as being the material.

And then you've got this dual aspect monism where you're now asking questions about, well, what are the lawful relationships between the thermodynamic free energy and the variational free energy that pertains to beliefs about things?

So you've got now a mechanics that talks about beliefs, and that may well be one way of repairing that sort of dualistic approach.

There's another perspective which you would get if you were talking to people like Giulio Tononi and Cyril, well, my colleagues involved in one of the Templeton Foundation Abusera research collaborations, where you're trying to find bright lines between the free energy principle and integrated information theory.

um which is not necessarily the best thing to do because a lot of integrated information theory is exactly consilient with the free energy principle however if you are going to get funded by trying to tease them apart then the thing that distinguishes those

approaches to consciousness is really the active part that we're talking about.

So boiling this down in a way that somebody like, I repeat, Jacob Howie might articulate this is, it's the difference between seeing and looking.

To be conscious, I have to look.

It's not enough to see.

To be conscious, I have to listen.

It's not enough to hear.

So you're putting action into games.

There's active sensing.

There's sort of closing the loop between the inside and the outside.

may be the thing that is necessary for consciousness.

And simply because you're saying that consciousness is an attribute of agents, and agents have to act, what do they act on?

Their sensations.

What does that mean?

They do active sensing.

So to be an agent is to be an active sensor.

Yes, to do active inference.

If you're saying that things that aren't agents can be conscious, then you wouldn't worry about that.

But if you are committed to the notion that to be conscious, to have a minimal selfhood,

then you have to have the notion of self.

If you have to have the notion of self, then you have to be an agent.

If you have to be an agent, you have to have generative models about the consequences of your agency, your actions.

And then we go through all of those nice links that we were talking about before with temporality and the like.

And indeed, the hierarchical structure takes us back to the Crisfields irreducible idea

holographic screen or um or markov blanket so that would be the way that i would um converse with that crowd uh uh i've run out yes i've run out of people that i might talk about it so i have to find out what you like and then i'd i'd tell a story that you like or let you tell a story that uh that you like well i guess my question to you would be


SPEAKER_00:
Can you imagine a world of self evidencing?

Can you imagine?

Okay, let's put it another way.

Can you imagine a perfect replica world like ours, where the lights are turned off entirely, there is no consciousness?

Or is it Do you imagine it's a necessary

consequence of the way that creatures like us are structured, that to do deep temporal planning, to do planning as inference, whatever it is, or self-hooding, we need consciousness.

Could there have been another way that the universe would have unfolded without us being online?


SPEAKER_01:
I suspect not.

Sorry, I'm torn between being taken by you down sort of your philosophical zombie routes versus another story that comes out in the Free Energy Principle about consciousness, which is the...

the importance of self-modelling in relation to other modelling and the fact that we are actually living in a world of things like us.

One story I didn't cover, which is a story you would tell if you were talking to my friend Chris Frith or people who are much more interested in the social neuroscience

or, say, attachment theory and neurodevelopmental issues that are induced by evolutionary psychology view or niche construction, for example.

All of these views of consciousness means that consciousness is just there, which is where I started, to infer the state, the disposition, the intentions, state of mind of somebody else.

Why and how can you do that?

Well, you can only do that if that thing is sufficiently like you to be able to use your generative model of, say, your physical actions as a model of their physical actions to infer their intentions and the like.

And when one takes that to its limit, then often what the real problem is is not in terms of inferring the content of some communication or some exchange of semiotic cues.

It really is inference about agency again.

It's did you do that or did I do that?

So that ambiguity now requires you

to have a model of self versus other.

So it's a much more relational notion of self.

So, you know, sometimes I think people talk about minimal selfhood as something which is monolithic.

I don't think that's necessary as part of a gender model.

It certainly would be important to have different kinds of self.

You know, am I embarrassed?

Am I in love?

You know, as we were talking about, you know, in the...

TMB meeting a few days ago.

That would be an expressive and very necessary part of the generative model where I have to contextualise my own emotional and pro-social and autonomic and moteric behaviours.

But there's no one self, it's a self.

Self like this, like this and like this.

A more fundamental relationship, relational aspect of selfhood is in relation to otherhood.

You know, versus me, and am I mum?

Or is mum a thing?

Am I a thing?

I'm the same thing as mum.

You know, he's going to sort of Kleinian sort of arguments there.

Put simply, what that means is it may well be only in universes or worlds that are populated by ensembles of conspecifics that are sufficiently similar would you need selfhood, simply to distinguish self from other.

In a universe where there's only me, I wouldn't need selfhood.

So it would not pay its way in terms of model complexity or model evidence to have the notion of me being a self if there's only me.

It only pays its way if I now have to entertain the hypothesis that either I caused that or you caused that.

So the question now is, is it inevitable that you get a universe with lots of similar things emerging

all disambiguating themselves or attributing agency to themselves in the golden in the golden zone i think it probably is i think that would be i think if you simulated

sufficiently accurately artificial life.

I think you would get the emergence of populations of similar things and they would niche construct and they would thereby drive their own increasingly nested and structurally complicated

or sparsely structured organisations in terms of their Markov blankets.

And this at some point would inevitably invoke modelling of others and therefore almost by default,

of self of selfhood, I think that would be emergent, you know, but it would require being in the Goldilocks regime for a sufficient amount of time for this kind of niche construction, cultural niche construction to actually play out.

So, you know, you'd have to have this very slow timescale, providing the right kind of context for this, this kind of thing to think to emerge.


SPEAKER_00:
Yeah.

Yeah, I think I have some sympathies to the argument that Chalmers did lay out in 95 in his original hard problem paper, which is that

I think I can still ask the further question, which is, can systems like me make the functional distinction between myself and others without there being a sense of what it's like to be me?

Can that just be an offline-ness, which is done purely in some algorithmic manner, which need not involve some qualia of myself-ness?

So yeah, there's a point that there's an element in me where I don't want to say that that in a sense resolves a hard problem.

Because at least I can hypothetically envision that being done offline.

I wonder whether you've given much thought to Mark Soames his idea, which is that consciousness might be rooted in affect.

Because this seems to me to be a bit more of a self-confirming idea, which is that feelings must be felt.

Now, I think there's still an open question there, which is affect arises at the point of uncertainty and drives us to resolve uncertainty.

And when we do so, we feel good about ourselves.

Now, again, I still think when I was reading his book, I still had this open question.

Why would affect even emerge in that place?

Why wouldn't we just have this drive

to resolve that uncertainty without there being the onlineness.

So at least for me, although I may be wrong, and I'm very happy to be wrong, there is still no simple parsimonious explanation which can get around this point that Chalmers makes, which is you can give me the function, but you can't give me the explanatory power why it needs to be online.

And I wonder if there ever will be some mechanism which is sufficiently

which isn't just purely tautological or invokes consciousness as some fundamental reality, which makes me wonder whether you've heard of Donald Hoffman, whether you're familiar with his work.

Because I sense that in some ways it has some interesting implications maybe for the free energy principle in the sense of space and time not being fundamental units of reality.

we've been speaking a little bit about how, where do space and time come from?

How much do space and time overlap in the sense of generating, like we need expansive creatures like us with embedded Markov blankets inevitably have a sense of temporal modeling, and there's these interesting interactions.

Does the idea of space and time being

what he would say applications on a desktop.

Does that have any real do you sense has any real ontological implications for free energy principle?

Or is it a red herring?


SPEAKER_01:
Well, you've introduced two important things.

I forgot to mention Mark Soames is another friend.

I would say it's all about this irreducible Markov blanket that acts upon the rest of the brain.

Where is that action?

It's going to be in the neuromodulator systems that set the precision.

The precision is exactly that Fisher metric, which tells you about how much you're updating your beliefs.

It is the way that you do mental action.

It is, as you intimated, feeling in the sense that it is quintessentially balanced by increasing or decreasing the precision or uncertainty.

So that's how I would...

tie in Mark's formulation, which I think is internally consistent with, say, Chris Field's formulation or Vanier's formulation.

So Mark focuses on, I think, a key aspect of active inference, which is, if you're a psychologist, it will be attention.

It's basically acting in a way on the inside

to get the estimate and deploy the right confidence or the right precision in mathematically the curvature of the free energy in and of itself.

Getting that right and that has a lovely connection to

attention it means that you can't divorce um consciousness from attention you can't divorce it from agency in the sense of mental action or internal action you now have a deep connection to um thomas metzinger's um the distinction between your um opaque and phenomenal transparency

And the way that Mark would tell the story would be that this is just feeling, and feeling is at the heart of any qualitative experience.

So, yeah, thank you for reminding me he was one person I forgot to cover.

The second thing, you know, the space and time being illusions.

I mean, yeah, I sort of start off there, so I'm not quite sure that it...

So if you're saying that Donald's, and I should say he's a friend, I think, I've never met him, but I think he's a friend of Chris Field and Mike Levin.

So, you know, there's a small world here.

But if you're just saying that space and time are illusions in the sense that we would talk about illusions as being fantastic and the thing that makes us into fantastic little beings and have

beings and our brain into a fantastic organ.

Yeah, absolutely.

I mean, the question for me is why on earth would you, what, what,

what must be the case for us to have this illusion of space-time and that we are living in a metric space.

And it's practically very important.

I mean, a lot of people spend a lot of time in machine learning embedding things in metric spaces.

And sometimes it's very difficult to see what licenses is there to do that.

You can certainly see how it would work in terms of projective geometries in metric spaces.

um but the deep question is why you know you know i can't um i can't imagine a um you know uh an amoeba really having having a notion of of uh space um i may be wrong um and so knows um um you know if you listen to people like um um stephen wolfram um you know

the notion of space-time itself may, the illusion of space-time itself may be just an artifact of the fact that our

sensory organs respond very very slowly in relation to the speed of light so if light traveled very very slowly we imagine a world in which um we saw with sound and sound was very very very slow um would that would that be would that give you the illusion of space um you know a distance as an action of the kind that you know we sort of implicitly assume when we're talking about sort of visual objects

So I think he has this nice notion that in this kind of world, with these kinds of sense organs, when something moves from here to here, is it the same thing anymore?

Because that's what you're saying when you have a metric space in which objects move.

Certain symmetries or invariances inherit from the notion of movement in a metric space.

But that may well just be a fantasy.

We've gone off quality of experience and the hard problem.


SPEAKER_00:
Yeah.

Well, it all ties back.

It's a beautiful, it's a very good question, which is why would this, why would space and time be the interface for us?

I guess that is the deep question.

And yet it appears that at least it is something, at least in conscious awareness.

I did remember my question and it actually feeds into the last couple of questions I will have.

which is you mentioned that you spend a lot of your time inferring in the technical and layman sense, whether other people have, I think you said good generative models.

There's an interesting question here, I think, which I may have had intuitively when I started reading about active inference, which is, is there any such thing as good self-evidencing in the sense that

I may, so a lot of, of course, you know, as you wrote a lot of the papers, a lot of psychiatric disorders and psychological disorders have been rooted in disruptions to precision weighting mechanisms, for example.

Can we consider, are the words good, bad, optimal, suboptimal, are these useful terms in this context?

Or is it that that individual has a set of priors

which are different from you and I and everyone else, and we all actually have a sense of, we all have kind of different priors, so we're all self-evidencing in unique ways.

And yet philosophical vagueness allows us to clump us together into a group.

Is there such a thing as what it is to be a good self-evidencer as a human, beyond just a trivial point that we're not just dissipating into the heat bath?


SPEAKER_01:
Yeah, that's an excellent question.

So I think the answer to that is yes.

But as you correctly pointed out, it should be qualified by the complete class theorem.

So you're clearly referred to the complete class theorem.

So it might be just wise to...

define what that means and the implications for discussions of good and bad generative models.

So the complete class theorem, the way I like to summarize it, is that for any given pair of behaviors and loss functions, there exists some priors that render your behavior Bayes optimal.

which means that there is always a description of every behaviour as Bayes' optimal under some prize.

So in that sense, you can't be good or bad where the thing is basically your decisions, because this inherits from Bayes' decision theory, so it's not quite...

full as active inference and accommodating information gain.

But within the confines of basic decision theory, you can't be good or bad.

You just have different priors.

So my favorite example is that if we all went to live on Mars or a much heavier planet, people with Parkinson's disease would probably be much better, have better priors

sub-personal prize in terms of dopamine deficits because they move more slowly and more cautiously.

So I think from the point of view of the complete cluster, I think it's important to bear that in mind.

There are just different prizes.

But I do think, though, that you can sort of just say, well, hang on a second.

What about, well, a good and bad prize?

And I think then we get into this, again, this hierarchical story, because, of course, as soon as you have a hierarchical generative model, there are no prize.

Everything's an empirical prize.

Everything can be contextualized.

Everything is conditioned for everything else, including your prize.

And then you're just back into the game of self-evidencing.

What is good self-evidencing?

It's achieving a higher marginal likelihood on model evidence.

How would you do that?

By finding the right prize.

How would you do that?

By learning.

How would you do that?

By changing your model with basic model selection or structural learning.

How would you do that at an evolutionary scale?

You do basic model selection with natural selection.

So yeah, on that view, the log evidence, as bounded by the free energy, is just adaptive fitness, is how well your priors are adapted to this world you are trying to explain.

So I think, yes, absolutely, you can be a good self-evidencer, you can be a bad self-evidencer.

If you're natural selection, that matters because if you're a bad self evidencer, that means you won't be around in the next generation.

So the marginal likelihood of you existing at an evolutionary time scale is low.

So your evidence is also the probability that you will be found, you will exist.

So it's quite important

to be a good self-evidencer because that's your marginal likelihood of existing.

And I've now slipped in another synonym for free energy beyond prediction weighted prediction errors or beyond surprise or surprise or self-information, beyond log evidence.

It's also, it's just, it's called the log marginal likelihood.

It's the likelihood of your sensory exchanges with your world, your eco-niche, having marginalized out or averaged away all the unknowns, which in this context are all the states out there.

the hidden causes or the external states which you model in terms of hidden causes.

So yeah, I think it's very important to be good at evidencing.

The measure of goodness and badness is just the marginal likelihood of the model evidence of the free energy.

There is nothing else.

If you're saying, does that speak to some creatures that will be able to actually assess

how good they are at self-evidencing then i would say absolutely um people like uh matthias joffily looked at this specifically from the point of view of emotion and developed a whole taxonomy of emotion in terms of um rates of change of free energy and you get into stories of slope chasing uh um

looking for gradients and gradients and gradients of uh self-evidencing on the measurement of the goodness of self-evidence which is uh which is which is the free energy um more refined stories more recent stories um would basically be looking at certain kinds of uncertainty

being encoded explicitly in your generative model as underlying affect, particularly the certainty about what you're going to do next, for example.

Then we get back to the story that Mark Soames would like to tell, which is the precision of your beliefs

is the most important attribute that defines its valence that you're feeling at this sort of minimal effective level.


SPEAKER_00:
Yeah.

The complete class theorem is an interesting one because I think it leads one into these potential strange conclusions.

It makes me think of animals that commit suicide, oddly.

So I think sort of aphids might explode themselves in the context of a predator.

It ultimately comes back down to your model evidence, I guess.

So I am the type of creature that will commit suicide if I'm in the presence of a ladybug, of course.

But then those kinds of examples I sense for me show that

the words good or bad, I'm just pushing a little bit back just intuitively.

The words good or back in the form of continued existence slightly put the cart before the horse, because who said that that's the imperative?

So if I existed in the world and everyone's imperative was to no longer exist, God forbid, then I would be doing bad self-evidencing on that standard if I didn't.

if I continue to persist.

So is there a point at which even the, yeah, like, do you see how, well, do you see that argument holding any water?

Or is it just now we're just, sorry, dissolving into semantics?


SPEAKER_01:
Well, no, no, I think it's an interesting thing to contemplate.

You're basically, I think, undermining that particular argument by a proof abductor absurdum.

So if you think you are the kind of thing that will self-destruct, auto-vitiate or commit suicide, then you can't exist because you've killed yourself.


UNKNOWN:
Right.


SPEAKER_01:
So I would be extremely surprised if there were any common natural subhuman artefacts that kill themselves.

I didn't know about the aphid example.

I mean, obviously lemmings is the sort of popular culture one, but I think they would be extremely rare.

And then you have to understand the selection at an evolutionary or transgenerational level.

in order to apply the free energy principle.

I think you're absolutely right.

I'm very mindful that a couple of weeks ago I was asked to review a paper applying active influence to suicide and it was a very artful paper.

I do remember thinking, I'm not sure whether the point was made implicitly or explicitly in the paper,

It made me think in exactly the way that you were talking about the imperatives to survive and are they the right way to think about the causes of our behaviour?

I would say absolutely not.

This is not about wanting to survive.

This is simply wanting to avoid uncharacteristic sensations.

and then we put a post hoc hypothesis oh that's because i want to survive then right and that's a fantasy that's another illusion uh so you know we do not know what it is like to be dead so yes it cannot be hey but it can be surprising it certainly can't be the kind of surprise that you'd register in terms of a free energy so it's not that we're frightened of being dead um we are going to avoid by these

pragmatic constraints um states that we've have inherited that um for portend um cessation and dissipation and death definitely in all itself is not at all frightening we go to sleep every every um suspension of course we go to sleep every night um so you know i i i would i would um

sorry yeah and the final point i think is i think what you were doing though you were probably um creating a slightly vexed and self-defeating argument by saying uh by assuming that um we can understand things in terms of imperatives the whole point of the free energy principle is that there aren't any imperatives this is where it starts as things that exist must behave like this and part of that behaving like this means that you can interpret them

as optimizing something and complying with imperatives.

But that's just an interpretation that you'll bring to the table when you apply it to something else.

If you then apply it to yourself, then things might get interesting.

You might have hard problems.

But there are no imperatives.

In fact, even minimizing free energy is not an imperative.

You can have the free energy principle applying to things that have very dissipated attracting sets that have very, very high entropy.

All it's saying is that there has to be a certain gradient flow that counters the random fluctuations.

Then you ask yourself, well, hang on a second, so why does it look as if

were always trying to attain some very precise homeostasis.

And the free energy principle would turn that on its head and say, well, look, okay, in some universes, there may be very precise things.

And these precise things, what would they look like?

Oh, it would look as if they were trying, they had aspirations to evincing a very precise homeostasis and indeed hemostasis.

So you get this notion of another kind of natural thing, which are precise things.

And so if precise things exist, it would look as if they are compelled to comply to very specific imperatives.

But from a free agent's point of view, it's not saying they exist because they comply with the imperative.

It's saying existence implies that it looks as if they have this imperative.

Now, one final point.

I'm aware that we're now approaching three hours.

Well, final point, though.

You introduced the word optimisation.

I think that was the right and important thing to introduce because, you know, it's only when you bring free energy into the game as a bound on marginal likelihood or the negative lock marginal likelihood with the self-informational surprise or surprisal

do you convert an existential description into an optimization problem?

So that's exactly the move that Richard Feynman made when he introduced variation-free energy.

He had this impossible marginalization problem to actually evaluate the marginal likelihood of paths.

So exactly the same kind of problem that we're talking about in terms of what are my priors over policies and plans and paths into the future.

You can't evaluate that exhaustively.

It's analytically and practically, physically not realizable.

So the marginalization problem, the true inference problem, if you like,

the true existential problem, if there is one, cannot be solved.

But what you can do is introduce this bound and turn it into an optimization problem.

So just by using free energy, just by appealing to this teleology that comes from predicating everything on this evidence-lower bound, you are actually saying that it looks as if

creatures that exist are optimizing something.

What are they optimizing?

It looks as if they're optimizing a bound on their marginal likelihood or their adaptive fitness.

What does that look like?

Well, it looks exactly like Bayesian inference.

Or if you talk to somebody in machine learning, it would look as if you're optimising your negative variation of energy, which is known as an ELBO.

So this is an evidence lower bound.

It is exactly the objective function used in high end neural networks like variation autoencoders.

So I think the distinction between just being and optimizing is quite crucial.

And you're absolutely right that once you make the move of framing things in terms of variational bounds, using variational calculus or variational Bayes, then you have actually committed to an optimization narrative.

You didn't have to.

Just to exist, you could...


SPEAKER_00:
yes yeah i was gonna have this funky philosophical argument about needing to exist to predict but i won't go there i won't go there um because we are going to wrap up all of this philosophical mathematical thoughts as and consciousness thoughts has brought to my mind thomas nagel thomas nagel has this idea that death is bad because life is inherently good

we've been talking about life and being and what it is to be and what one needs to do to be and persist to be if we're going to wrap everything up is there a sense in which that

kind of related to what we were just talking about in terms of imperatives of course there is no imperative to be in any sense but you said it was an interesting thing you said there was it looks like we are all being anyway right even if we didn't have to it looks like we are there's no god granted plan for us to be but we seem to be doing it anyway does any of nagel's

romantic i philosophical idea that life is fundamentally good being is good does any of that resonate with you um yes it does because it's it's it's you know it's nice isn't it um


SPEAKER_01:
You'd probably sort of license that with phrases of the kind that, you know, to exist is to be curious.

So this speaks to this sort of, you know, the ability to respond to epistemic affordances.

And certainly, you know, to exist as curious creatures is obviously the good way to exist in any given world and would be mandated.

I can't resist saying, though, of course, that this only really applies to you.

It doesn't apply to me.

So you are good because you are curious and you exist.

I wish I could say the same for me, but I can't, strictly speaking.

But you could if you wanted to.


SPEAKER_00:
I could abstract to you in some degree.

I could infer that there is a you.

Great.

And then I guess we can wrap it up there with one more question.

We've just spoken about life being good.

I'll flip the question.

I don't know if I've ever heard you talk about death, although we briefly touched upon it.

I won't make it too personal, but

If life is kind of good in some sense because of the concomitant parts of living as we do as... I don't know if it's good to be a stone, but it's nice to be me having these conversations.

Would that make death bad?

You said we fear dying because we fear, in an affective sense, the states we don't expect to embody.

And in some sense, we don't expect to embody death because it doesn't fit in with our priors.

But beyond that free energy principle perspective, is there a broader position in which just returning back to the fabric of the universe is bad?

Is there something to say about reentering that Markov blanket?

Do my Markov blankets dissipate?

What does that process look like?

And does that strike fear in you or strike awe?


SPEAKER_01:
Well, more or, but I don't want to overstate it.

It is beautiful from a pure mathematical point of view.

um so i i have a very sanguine view of this and we've already said you know you've never been dead i've never been dead so no one can tell me it's an unpleasant experience um so i i'm not particularly i i'm not gonna um avoid being dead i certainly think all of all the paths to being dead are probably probably very surprising and unpleasant so i'm going to avoid those

Death in and of itself I don't think is quintessentially bad in the same way that being curious and being engaged with the world is good while you exist.

I think that's perfectly okay.

But I think death is, you know, the life cycle, let's call it a life cycle, that it's slightly less imbued with morbidity.

The life cycle is quite important because if you just look at, if you just take this multi-scale view of self-organisation cast in terms of this basic mechanics,

that you know for example natural selection being basic model selection then if you put that together with what we're talking about before about ecosystems and niche construction constantly changing the lived world then what you need to do is to do constant basic model selection

in order to get the right structures, very much in the spirit that we talked about before, where there are good priors and there are bad priors in the sense of the marginal likelihood or adaptive fitness.

To learn those good priors, you sometimes have to start from scratch again with a different structure.

And of course, this is exactly what Bayesian model selection does via structure learning at an evolutionary time scale will entail death.

So that picture basically says that we have to keep refreshing, sometimes cast in terms of basal forgetting.

So the fact that we die

could be construed as a base optimal forgetting where we are the environment's memory

of what is good for this eco-niche but the environment on this natural selection knows that the environment is changing so what was good last generation will not necessarily be good for the subsequent generation so it has to forget in the same way that you learn by forgetting you have insights by dispensing with redundant information so in a sense death

for in that life cycle is an essential part of the self-evidencing of the species.

So it may be bad for you, I suppose, in a Nagel-esque sense, in that it's denying the opportunity to be curious and engaged with your lived world, but it's certainly good for your children and your children's children.


SPEAKER_00:
Right, right, right.

I'm a I'm a full warning for them.

That's good.

I'm glad to I'm glad to be that porn in that game.

And I kind of cheery notes on which to end will be a slightly more morbid question.

Carl, this was an absolute pleasure.

Despite the occasional Wi Fi hiccup.

I think it's, it's gone.

And, you know, I've been incredibly happy with it.

Thank you so much for being our first guest.

And answering my suite of sometimes bizarre questions.

It was a real joy on my part.


SPEAKER_01:
right well it's been a real pleasure talking to you your questions were excellent i have no idea how long you're going to have to spend editing all this but i i thank you again for that opportunity to uh you know to talk about these wonderful issues and hopefully we'll do it again excellent thank you
1
00:00:01,260 --> 00:00:03,240
okay

2
00:00:03,240 --> 00:00:08,580
welcome thanks it's uh March 15 2023

3
00:00:08,580 --> 00:00:11,460
we're in meeting 20 of

4
00:00:11,460 --> 00:00:13,200
cohort two

5
00:00:13,200 --> 00:00:17,699
in our first discussion on chapter nine

6
00:00:17,699 --> 00:00:19,199
so

7
00:00:19,199 --> 00:00:24,119
as we begin this discussion of nine

8
00:00:24,119 --> 00:00:26,160
is there any

9
00:00:26,160 --> 00:00:29,340
General comment or question about nine

10
00:00:29,340 --> 00:00:33,260
that anyone wants to bring up

11
00:00:49,800 --> 00:00:53,280
one one part on nine is it at least

12
00:00:53,280 --> 00:00:56,579
points the direction towards

13
00:00:56,579 --> 00:00:58,800
an Avenue that many people who are

14
00:00:58,800 --> 00:01:01,440
interested in learning and applying

15
00:01:01,440 --> 00:01:04,440
active inference want to go down which

16
00:01:04,440 --> 00:01:05,580
is

17
00:01:05,580 --> 00:01:08,400
to make generative models

18
00:01:08,400 --> 00:01:11,580
yes that help us understand different

19
00:01:11,580 --> 00:01:13,380
cognitive systems

20
00:01:13,380 --> 00:01:16,920
and also have models that can accept

21
00:01:16,920 --> 00:01:18,960
empirical data

22
00:01:18,960 --> 00:01:21,900
so that we can enact the tale of two

23
00:01:21,900 --> 00:01:23,040
densities

24
00:01:23,040 --> 00:01:24,900
so we can use the model in the

25
00:01:24,900 --> 00:01:26,880
generative Direction

26
00:01:26,880 --> 00:01:29,580
to go from

27
00:01:29,580 --> 00:01:32,280
specified parameters to

28
00:01:32,280 --> 00:01:35,460
output data sets of synthetic data kind

29
00:01:35,460 --> 00:01:38,759
of like generative AI but then also to

30
00:01:38,759 --> 00:01:41,040
recognize

31
00:01:41,040 --> 00:01:43,740
outcomes in the world or data

32
00:01:43,740 --> 00:01:46,799
and then update or parameterize our

33
00:01:46,799 --> 00:01:48,659
generative models

34
00:01:48,659 --> 00:01:51,000
from those data

35
00:01:51,000 --> 00:01:53,939
ends it's that Tale of Two densities

36
00:01:53,939 --> 00:01:56,040
the generative density and the

37
00:01:56,040 --> 00:01:59,040
recognition density the ability to run

38
00:01:59,040 --> 00:02:01,380
these models both ways

39
00:02:01,380 --> 00:02:06,000
to generate valid synthetic data from a

40
00:02:06,000 --> 00:02:07,920
generative model

41
00:02:07,920 --> 00:02:11,760
and if you design your experiment so

42
00:02:11,760 --> 00:02:14,340
that those data which are generated

43
00:02:14,340 --> 00:02:16,680
are also actually being generated in the

44
00:02:16,680 --> 00:02:18,660
real world then you can just instantly

45
00:02:18,660 --> 00:02:21,239
plug it back in

46
00:02:21,239 --> 00:02:23,360
um

47
00:02:23,520 --> 00:02:26,340
and we'll I'm sure unpack and explore

48
00:02:26,340 --> 00:02:30,000
more so we'll go through the chapter but

49
00:02:30,000 --> 00:02:32,340
is there any just general comment or

50
00:02:32,340 --> 00:02:36,020
general thoughts on chapter nine first

51
00:02:38,459 --> 00:02:40,700
foreign

52
00:02:43,920 --> 00:02:46,860
so let's look at one previous question

53
00:02:46,860 --> 00:02:48,420
so how did chapter four on the

54
00:02:48,420 --> 00:02:51,420
generative model relate to chapter nine

55
00:02:51,420 --> 00:02:53,640
as we're getting close to the end of the

56
00:02:53,640 --> 00:02:55,800
book chapter six was a recipe it was

57
00:02:55,800 --> 00:02:59,280
about applying active inference

58
00:02:59,280 --> 00:03:01,800
chapter four we were learning the

59
00:03:01,800 --> 00:03:04,200
necessary prerequisites in chapter one

60
00:03:04,200 --> 00:03:07,080
and two and three were the roads to

61
00:03:07,080 --> 00:03:09,000
chapter four

62
00:03:09,000 --> 00:03:11,159
then chapter five we got the

63
00:03:11,159 --> 00:03:13,019
neurobiology and some examples chapter

64
00:03:13,019 --> 00:03:15,060
six we got the recipe

65
00:03:15,060 --> 00:03:18,780
chapter 7 and 8 was the discrete time

66
00:03:18,780 --> 00:03:20,400
and continuous time

67
00:03:20,400 --> 00:03:22,739
generative models

68
00:03:22,739 --> 00:03:26,959
and now we're in chapter nine

69
00:03:28,319 --> 00:03:30,080
where we're learning how to solve

70
00:03:30,080 --> 00:03:34,319
problems and it's about getting one's

71
00:03:34,319 --> 00:03:36,420
hands dirty with case studies and

72
00:03:36,420 --> 00:03:39,260
problem solving

73
00:03:46,260 --> 00:03:49,260
we'll see this in the figure

74
00:03:49,260 --> 00:03:51,959
upcoming

75
00:03:51,959 --> 00:03:55,799
a key distinction is between the

76
00:03:55,799 --> 00:03:58,140
subjective model

77
00:03:58,140 --> 00:04:02,119
and the objective model

78
00:04:02,580 --> 00:04:04,019
there's different readings of the first

79
00:04:04,019 --> 00:04:05,940
sentence and in some readings it could

80
00:04:05,940 --> 00:04:08,280
be seen as playing fast and loose with

81
00:04:08,280 --> 00:04:11,040
instrumentalism and realism

82
00:04:11,040 --> 00:04:14,819
so the subjective model though is that

83
00:04:14,819 --> 00:04:17,100
which the subject

84
00:04:17,100 --> 00:04:20,060
is that that which is subject specific

85
00:04:20,060 --> 00:04:23,699
from the subject's perspective now it

86
00:04:23,699 --> 00:04:26,100
turns out that it is a map that we're

87
00:04:26,100 --> 00:04:28,919
making from the subjects perspective

88
00:04:28,919 --> 00:04:32,220
so it isn't the actual territory

89
00:04:32,220 --> 00:04:34,560
of the subjectivity

90
00:04:34,560 --> 00:04:36,600
but it is

91
00:04:36,600 --> 00:04:39,720
from the subjects perspective

92
00:04:39,720 --> 00:04:43,620
and then to meet that and make the full

93
00:04:43,620 --> 00:04:45,060
model

94
00:04:45,060 --> 00:04:47,540
we are going to have an objective model

95
00:04:47,540 --> 00:04:50,160
which doesn't mean that it's the one and

96
00:04:50,160 --> 00:04:52,440
only it means that it's our view from

97
00:04:52,440 --> 00:04:53,600
the outside

98
00:04:53,600 --> 00:04:59,180
seeing that thing as an object

99
00:05:03,620 --> 00:05:06,360
this entire setup

100
00:05:06,360 --> 00:05:08,639
with a tale of two densities and the the

101
00:05:08,639 --> 00:05:10,740
metabasian

102
00:05:10,740 --> 00:05:12,419
enables

103
00:05:12,419 --> 00:05:16,139
us to make Bayesian cognitive models and

104
00:05:16,139 --> 00:05:18,000
think about experimentation in a

105
00:05:18,000 --> 00:05:20,580
Bayesian context

106
00:05:20,580 --> 00:05:23,400
which lets us talk about Bayes optimal

107
00:05:23,400 --> 00:05:25,620
cognition

108
00:05:25,620 --> 00:05:28,020
in different systems

109
00:05:28,020 --> 00:05:29,940
and one area that that has been

110
00:05:29,940 --> 00:05:33,660
especially relevant is in computational

111
00:05:33,660 --> 00:05:36,660
phenotyping

112
00:05:38,039 --> 00:05:40,620
this chapter deals with a utility

113
00:05:40,620 --> 00:05:42,479
of active inference formulations in

114
00:05:42,479 --> 00:05:44,340
analyzing data from behavioral

115
00:05:44,340 --> 00:05:46,020
experiments

116
00:05:46,020 --> 00:05:47,699
it's going beyond the proof of principle

117
00:05:47,699 --> 00:05:49,919
simulation so in the proof of principle

118
00:05:49,919 --> 00:05:54,240
domain we have the Jumping Frog in the

119
00:05:54,240 --> 00:05:57,240
hands we have the rat in the tmas and so

120
00:05:57,240 --> 00:05:58,320
on

121
00:05:58,320 --> 00:06:00,840
and here active inference is going to be

122
00:06:00,840 --> 00:06:05,758
used to answer scientific questions

123
00:06:07,080 --> 00:06:10,919
since the subjective GM

124
00:06:10,919 --> 00:06:14,160
is the generator of behavior in active

125
00:06:14,160 --> 00:06:16,199
inference

126
00:06:16,199 --> 00:06:17,639
then

127
00:06:17,639 --> 00:06:20,039
the scientific hypotheses that we're

128
00:06:20,039 --> 00:06:21,720
going to entertain

129
00:06:21,720 --> 00:06:24,419
about the causes and consequences of

130
00:06:24,419 --> 00:06:25,979
phenotype

131
00:06:25,979 --> 00:06:29,460
must be in terms of hypotheses about

132
00:06:29,460 --> 00:06:33,500
alternative generative models

133
00:06:34,740 --> 00:06:35,940
so

134
00:06:35,940 --> 00:06:38,460
the challenge is to

135
00:06:38,460 --> 00:06:42,479
just like an abductive logic challenge

136
00:06:42,479 --> 00:06:46,380
generate a portfolio or spaces

137
00:06:46,380 --> 00:06:49,199
of alternative generative models

138
00:06:49,199 --> 00:06:51,120
cognitive models

139
00:06:51,120 --> 00:06:52,620
and then

140
00:06:52,620 --> 00:06:57,240
ask for a given string of observations

141
00:06:57,240 --> 00:07:01,080
what the evidence is for difference

142
00:07:01,080 --> 00:07:03,900
alternative hypotheses provided

143
00:07:03,900 --> 00:07:05,759
and then at that point one could take a

144
00:07:05,759 --> 00:07:08,759
more classical or frequentist approach

145
00:07:08,759 --> 00:07:10,500
they could also take a more pure

146
00:07:10,500 --> 00:07:12,360
Bayesian approach and ask about like the

147
00:07:12,360 --> 00:07:15,120
Bayes factor and the relative support of

148
00:07:15,120 --> 00:07:16,620
different models

149
00:07:16,620 --> 00:07:19,620
but the Upstream

150
00:07:19,620 --> 00:07:22,199
part is to generate alternative

151
00:07:22,199 --> 00:07:24,120
generative models

152
00:07:24,120 --> 00:07:26,819
explicitly or implicitly by specifying

153
00:07:26,819 --> 00:07:29,099
State spaces or possibilities

154
00:07:29,099 --> 00:07:31,919
and then chapter 9 is when we're

155
00:07:31,919 --> 00:07:34,440
plugging in empirical data and then

156
00:07:34,440 --> 00:07:37,560
going to ask about which of those

157
00:07:37,560 --> 00:07:40,620
generative models or spaces within the

158
00:07:40,620 --> 00:07:43,500
possibilities of the GMS

159
00:07:43,500 --> 00:07:45,720
have what evidence with respect to the

160
00:07:45,720 --> 00:07:48,440
observables

161
00:07:48,479 --> 00:07:50,280
if you have any thought or questions

162
00:07:50,280 --> 00:07:52,259
just totally go for it otherwise we'll

163
00:07:52,259 --> 00:07:54,539
just continue to like walk through it on

164
00:07:54,539 --> 00:07:57,360
the first discussion

165
00:07:57,360 --> 00:08:00,300
Michael go for it

166
00:08:00,300 --> 00:08:03,120
yeah I'm uh I mean I guess you'll still

167
00:08:03,120 --> 00:08:04,979
come to it but I'm struggling with this

168
00:08:04,979 --> 00:08:08,340
mathematics uh very much and last time

169
00:08:08,340 --> 00:08:10,139
we had this interesting discussion about

170
00:08:10,139 --> 00:08:12,680
temporal

171
00:08:12,680 --> 00:08:15,599
models the difference between chapter 7

172
00:08:15,599 --> 00:08:18,539
and chapter and chapter eight and I was

173
00:08:18,539 --> 00:08:20,759
thinking that for my data that I have

174
00:08:20,759 --> 00:08:22,680
with this gay State and the keystrokes

175
00:08:22,680 --> 00:08:25,319
and all this these temporal models would

176
00:08:25,319 --> 00:08:26,220
be

177
00:08:26,220 --> 00:08:29,660
perhaps much better Suited

178
00:08:29,660 --> 00:08:33,179
but then we discussed about these

179
00:08:33,179 --> 00:08:34,760
different uh

180
00:08:34,760 --> 00:08:38,458
kinds of derivations and I'm really

181
00:08:38,458 --> 00:08:42,839
Unsure how what that implies whether I

182
00:08:42,839 --> 00:08:43,620
should

183
00:08:43,620 --> 00:08:44,640
um

184
00:08:44,640 --> 00:08:48,060
find a way to express my data in kinds

185
00:08:48,060 --> 00:08:52,399
of waves so that one can compute somehow

186
00:08:52,399 --> 00:08:55,860
derivations from this or

187
00:08:55,860 --> 00:08:58,140
um so we did discussed about The

188
00:08:58,140 --> 00:09:02,760
Disappearance of of time in these models

189
00:09:02,760 --> 00:09:06,360
and I I'm really struggling to

190
00:09:06,360 --> 00:09:10,800
understand what that means in my data

191
00:09:10,800 --> 00:09:13,580
that I have but I don't know whether

192
00:09:13,580 --> 00:09:16,380
this really relates to what you were

193
00:09:16,380 --> 00:09:18,000
saying before

194
00:09:18,000 --> 00:09:20,820
yeah it was it's definitely it's it's um

195
00:09:20,820 --> 00:09:23,880
it's a it's a experience and a challenge

196
00:09:23,880 --> 00:09:27,540
when you have an actual uh data set and

197
00:09:27,540 --> 00:09:29,040
for people who are already

198
00:09:29,040 --> 00:09:31,019
experimentalists

199
00:09:31,019 --> 00:09:34,140
it's not like we can just dream up the

200
00:09:34,140 --> 00:09:36,360
most creative generative model and then

201
00:09:36,360 --> 00:09:39,660
build a lab to record those data in many

202
00:09:39,660 --> 00:09:42,899
cases were actually constrained in what

203
00:09:42,899 --> 00:09:45,300
we can do or we already have the data

204
00:09:45,300 --> 00:09:46,080
set

205
00:09:46,080 --> 00:09:48,300
and then we're going to work backwards

206
00:09:48,300 --> 00:09:52,200
from the data set to generative models

207
00:09:52,200 --> 00:09:55,680
yep that's the case in my case yes yeah

208
00:09:55,680 --> 00:09:57,959
yeah so I mean one

209
00:09:57,959 --> 00:09:59,279
um

210
00:09:59,279 --> 00:10:03,180
um uh Beware all who pass this point is

211
00:10:03,180 --> 00:10:05,760
everything just like everything that led

212
00:10:05,760 --> 00:10:07,740
to the collection of your data was an

213
00:10:07,740 --> 00:10:10,140
experimentalist's choice you could have

214
00:10:10,140 --> 00:10:12,060
collected more or different

215
00:10:12,060 --> 00:10:13,800
um participants you could have done this

216
00:10:13,800 --> 00:10:16,320
or that in the data collection

217
00:10:16,320 --> 00:10:18,600
and now let's just say that we're in the

218
00:10:18,600 --> 00:10:20,100
analysis phase

219
00:10:20,100 --> 00:10:22,500
everything Downstream everything about

220
00:10:22,500 --> 00:10:24,120
the structure of the generative model

221
00:10:24,120 --> 00:10:27,360
and so on is all going to be modeler's

222
00:10:27,360 --> 00:10:29,279
degrees of freedom

223
00:10:29,279 --> 00:10:31,040
so

224
00:10:31,040 --> 00:10:34,440
we are looking for different spaces to

225
00:10:34,440 --> 00:10:35,700
specify

226
00:10:35,700 --> 00:10:37,800
and then different portfolios of

227
00:10:37,800 --> 00:10:40,200
generative models to test against and

228
00:10:40,200 --> 00:10:41,640
one of the great things about Bayesian

229
00:10:41,640 --> 00:10:42,959
statistics

230
00:10:42,959 --> 00:10:46,700
is that wildly disparate

231
00:10:46,700 --> 00:10:49,500
models architecturally

232
00:10:49,500 --> 00:10:52,320
can be compared through the base factor

233
00:10:52,320 --> 00:10:54,660
and the Bayesian information Criterion

234
00:10:54,660 --> 00:10:57,240
so just a explain why that's relevant

235
00:10:57,240 --> 00:10:59,880
relative to the um frequency statistics

236
00:10:59,880 --> 00:11:03,420
When comparing frequentist statistical

237
00:11:03,420 --> 00:11:05,459
models

238
00:11:05,459 --> 00:11:08,399
one of the only principled ways to

239
00:11:08,399 --> 00:11:11,040
actually test for which model is better

240
00:11:11,040 --> 00:11:14,279
is the hierarchical likelihood ratio

241
00:11:14,279 --> 00:11:15,240
test

242
00:11:15,240 --> 00:11:17,220
so you can say I have two strictly

243
00:11:17,220 --> 00:11:20,040
nested models I have factor a and Factor

244
00:11:20,040 --> 00:11:21,839
B and now I'm going to test for an

245
00:11:21,839 --> 00:11:24,300
interaction effect of a x b

246
00:11:24,300 --> 00:11:26,459
and then you can ask whether there's a

247
00:11:26,459 --> 00:11:28,200
statistically significant Improvement by

248
00:11:28,200 --> 00:11:32,220
taking on that interaction effect but if

249
00:11:32,220 --> 00:11:34,920
you said I have a model with a XB and

250
00:11:34,920 --> 00:11:36,420
then I have a model with B and C and C

251
00:11:36,420 --> 00:11:40,079
and D and DNA and X and Y those models

252
00:11:40,079 --> 00:11:43,200
cannot be directly compared because

253
00:11:43,200 --> 00:11:45,240
they're

254
00:11:45,240 --> 00:11:46,019
um

255
00:11:46,019 --> 00:11:48,300
likelihoods

256
00:11:48,300 --> 00:11:51,000
are not comparable

257
00:11:51,000 --> 00:11:53,160
in contrast

258
00:11:53,160 --> 00:11:56,640
Bayesian models can be compared in

259
00:11:56,640 --> 00:11:58,260
various ways

260
00:11:58,260 --> 00:12:01,320
the base Factor the Bic accuracy minus

261
00:12:01,320 --> 00:12:02,760
complexity

262
00:12:02,760 --> 00:12:04,320
free energy

263
00:12:04,320 --> 00:12:08,220
even when there's a very very different

264
00:12:08,220 --> 00:12:10,560
model anatomies

265
00:12:10,560 --> 00:12:13,140
so it's not even like you have to decide

266
00:12:13,140 --> 00:12:15,360
whether you want to do only a discrete

267
00:12:15,360 --> 00:12:17,339
or only a continuous time generative

268
00:12:17,339 --> 00:12:18,600
model

269
00:12:18,600 --> 00:12:21,800
and hopefully as the software pipelines

270
00:12:21,800 --> 00:12:24,779
become more developed

271
00:12:24,779 --> 00:12:28,140
it'll be like you'll specify all the

272
00:12:28,140 --> 00:12:29,820
different channels that your data can

273
00:12:29,820 --> 00:12:31,680
flow through

274
00:12:31,680 --> 00:12:35,160
and then see how diff wildly different

275
00:12:35,160 --> 00:12:37,740
models

276
00:12:37,740 --> 00:12:40,620
match up with the data

277
00:12:40,620 --> 00:12:43,019
and look at their relative evidence or

278
00:12:43,019 --> 00:12:47,300
the areas that they fit well or don't in

279
00:12:47,639 --> 00:12:49,079
and you still might have to prune that

280
00:12:49,079 --> 00:12:50,579
down

281
00:12:50,579 --> 00:12:53,820
in some talk or paper

282
00:12:53,820 --> 00:12:55,440
but also maybe it'll be possible to

283
00:12:55,440 --> 00:12:57,779
share those computational resources also

284
00:12:57,779 --> 00:13:00,839
with that kind of high bandwidth

285
00:13:00,839 --> 00:13:03,779
um but yeah we'll we'll return to it so

286
00:13:03,779 --> 00:13:06,600
two reasons why we wanna

287
00:13:06,600 --> 00:13:10,920
fit data like why chapter nine

288
00:13:10,920 --> 00:13:14,639
the first is to estimate parameters of

289
00:13:14,639 --> 00:13:18,560
interest from a group

290
00:13:19,440 --> 00:13:22,560
those kinds of computational phenotypes

291
00:13:22,560 --> 00:13:27,540
might be useful for some other setting

292
00:13:27,839 --> 00:13:29,760
the second reason is to compare

293
00:13:29,760 --> 00:13:32,880
alternative hypotheses

294
00:13:32,880 --> 00:13:36,959
so here we have in the simplest case two

295
00:13:36,959 --> 00:13:37,980
groups

296
00:13:37,980 --> 00:13:39,600
and we want to ask if there is a

297
00:13:39,600 --> 00:13:41,240
difference

298
00:13:41,240 --> 00:13:46,500
in some parameter in the cognitive model

299
00:13:46,500 --> 00:13:48,480
a related

300
00:13:48,480 --> 00:13:51,839
reason to use computational phenotyping

301
00:13:51,839 --> 00:13:54,240
is potentially even for one person or

302
00:13:54,240 --> 00:13:56,820
one group or it could be for two groups

303
00:13:56,820 --> 00:14:01,860
is to compare two different models

304
00:14:01,860 --> 00:14:04,320
against one another and ask which one

305
00:14:04,320 --> 00:14:06,839
has different

306
00:14:06,839 --> 00:14:08,459
um evidence

307
00:14:08,459 --> 00:14:11,579
and so these are the two agendas

308
00:14:11,579 --> 00:14:14,760
estimating parameters given a cognitive

309
00:14:14,760 --> 00:14:15,959
model

310
00:14:15,959 --> 00:14:17,519
and

311
00:14:17,519 --> 00:14:23,720
evaluation among models given data

312
00:14:27,680 --> 00:14:30,420
that's the same hypothesis

313
00:14:30,420 --> 00:14:32,459
you have the same hypothesis you just

314
00:14:32,459 --> 00:14:35,239
replace the model

315
00:14:36,200 --> 00:14:41,760
same data so like uh like uh let's just

316
00:14:41,760 --> 00:14:44,220
say in the first case what this would

317
00:14:44,220 --> 00:14:45,720
look like would be people who have

318
00:14:45,720 --> 00:14:48,300
condition X or not

319
00:14:48,300 --> 00:14:50,779
I'm going to measure their attention

320
00:14:50,779 --> 00:14:53,760
with with this given attention model I'm

321
00:14:53,760 --> 00:14:55,320
going to test if there's a significant

322
00:14:55,320 --> 00:14:58,199
difference in this attention parameter

323
00:14:58,199 --> 00:15:00,480
this one would say I have two attention

324
00:15:00,480 --> 00:15:03,180
models I have one where attention is a

325
00:15:03,180 --> 00:15:04,680
single layer model and I have one where

326
00:15:04,680 --> 00:15:07,139
there's like a you know a nested model

327
00:15:07,139 --> 00:15:08,579
on attention

328
00:15:08,579 --> 00:15:11,339
and now on the whole data set or on just

329
00:15:11,339 --> 00:15:13,740
this one person I'm going to ask which

330
00:15:13,740 --> 00:15:16,199
of these two cognitive models is more

331
00:15:16,199 --> 00:15:19,819
consistent with the data that I see

332
00:15:20,519 --> 00:15:22,980
and and this

333
00:15:22,980 --> 00:15:24,600
um these two agendas

334
00:15:24,600 --> 00:15:27,300
are perfectly compatible

335
00:15:27,300 --> 00:15:30,899
with different terms Within

336
00:15:30,899 --> 00:15:33,120
Bayes theorem

337
00:15:33,120 --> 00:15:34,920
which

338
00:15:34,920 --> 00:15:38,899
is a tight

339
00:15:39,300 --> 00:15:41,100
synthesis

340
00:15:41,100 --> 00:15:42,959
of the

341
00:15:42,959 --> 00:15:46,800
kind of Foley Bayesian epistemology

342
00:15:46,800 --> 00:15:49,639
it's like we're doing Bayesian science

343
00:15:49,639 --> 00:15:53,579
with Bayesian statistics

344
00:15:53,579 --> 00:15:55,620
we're approaching

345
00:15:55,620 --> 00:15:59,699
our data collection and Analysis

346
00:15:59,699 --> 00:16:04,279
in an optimal information setting

347
00:16:08,279 --> 00:16:11,940
okay now we get to figure nine one with

348
00:16:11,940 --> 00:16:14,699
the objective and the subjective so does

349
00:16:14,699 --> 00:16:16,740
anyone want to give a thought on figure

350
00:16:16,740 --> 00:16:20,459
9.1 what they see here or what they see

351
00:16:20,459 --> 00:16:23,040
this figure doing in the context of the

352
00:16:23,040 --> 00:16:25,099
book

353
00:16:31,920 --> 00:16:33,779
okay just to give a first thought then

354
00:16:33,779 --> 00:16:35,279
again just raise your hander or write in

355
00:16:35,279 --> 00:16:37,199
the chat or anything

356
00:16:37,199 --> 00:16:39,959
that we saw in

357
00:16:39,959 --> 00:16:42,060
chapters four

358
00:16:42,060 --> 00:16:44,040
five

359
00:16:44,040 --> 00:16:47,519
seven eight all those generative models

360
00:16:47,519 --> 00:16:49,380
all those base graphs

361
00:16:49,380 --> 00:16:52,380
those are in the center in the dashed

362
00:16:52,380 --> 00:16:53,160
line

363
00:16:53,160 --> 00:16:55,320
and so it's a map

364
00:16:55,320 --> 00:16:57,240
but it's a map that we're constructing

365
00:16:57,240 --> 00:17:01,380
as if it were a view from the inside

366
00:17:01,380 --> 00:17:04,980
so here's the mouse in the T Maze and

367
00:17:04,980 --> 00:17:07,980
here's the pomdp in this case it's it's

368
00:17:07,980 --> 00:17:10,679
a discrete time you can tell because it

369
00:17:10,679 --> 00:17:12,199
has the past present and future

370
00:17:12,199 --> 00:17:15,540
transition Matrix and so on

371
00:17:15,540 --> 00:17:17,160
and so we've made a discrete time model

372
00:17:17,160 --> 00:17:19,199
of the mouse

373
00:17:19,199 --> 00:17:22,339
in the teammates so it's the as if

374
00:17:22,339 --> 00:17:25,380
Mouse's View From the Inside

375
00:17:25,380 --> 00:17:27,299
and and that's what makes sense when

376
00:17:27,299 --> 00:17:29,700
we're doing computational phenotyping we

377
00:17:29,700 --> 00:17:32,400
don't want to do model selection on

378
00:17:32,400 --> 00:17:34,799
um omniscient mice

379
00:17:34,799 --> 00:17:37,020
because we don't think that that

380
00:17:37,020 --> 00:17:38,880
exists in the world so the kinds of

381
00:17:38,880 --> 00:17:41,520
constraints that we think actually are

382
00:17:41,520 --> 00:17:45,120
uh justifiable in the world like the

383
00:17:45,120 --> 00:17:46,679
only input is sensory and the only

384
00:17:46,679 --> 00:17:48,720
outputs are actions

385
00:17:48,720 --> 00:17:51,720
those constraints are embodied within

386
00:17:51,720 --> 00:17:55,320
the GM that we make for the subject

387
00:17:55,320 --> 00:17:58,440
but the subject is also the object of

388
00:17:58,440 --> 00:18:00,000
our study

389
00:18:00,000 --> 00:18:02,100
the object of our study or the system of

390
00:18:02,100 --> 00:18:04,740
interest of our study is that

391
00:18:04,740 --> 00:18:07,020
Mouse in the teammates

392
00:18:07,020 --> 00:18:09,419
and so with respect to how we designed

393
00:18:09,419 --> 00:18:11,640
that teamaze experiment

394
00:18:11,640 --> 00:18:14,100
and whether we do it with one trial on

395
00:18:14,100 --> 00:18:15,660
one day or we do it with this other

396
00:18:15,660 --> 00:18:17,940
block design or we have it replicated

397
00:18:17,940 --> 00:18:20,100
across Laboratories

398
00:18:20,100 --> 00:18:23,100
those experimental parameters

399
00:18:23,100 --> 00:18:26,700
that are action selections from us

400
00:18:26,700 --> 00:18:29,880
can be understood as being drawn

401
00:18:29,880 --> 00:18:32,700
from distributions of possible

402
00:18:32,700 --> 00:18:35,539
experiments

403
00:18:35,580 --> 00:18:37,320
and

404
00:18:37,320 --> 00:18:40,080
what are action selections by the

405
00:18:40,080 --> 00:18:41,640
experimenter

406
00:18:41,640 --> 00:18:44,280
are being passed into the subjects model

407
00:18:44,280 --> 00:18:47,600
as an observation

408
00:18:48,240 --> 00:18:51,360
what is passed out of the subject

409
00:18:51,360 --> 00:18:53,520
through action

410
00:18:53,520 --> 00:18:55,620
is the input

411
00:18:55,620 --> 00:18:58,639
for the Observer

412
00:18:58,740 --> 00:19:02,640
so we're kind of wrapping

413
00:19:02,640 --> 00:19:04,860
the generative models that we saw

414
00:19:04,860 --> 00:19:07,799
in chapter four Etc

415
00:19:07,799 --> 00:19:10,919
calling those subjective models

416
00:19:10,919 --> 00:19:12,960
not to mean that they're willy-nilly but

417
00:19:12,960 --> 00:19:15,120
just to say that they are as if from the

418
00:19:15,120 --> 00:19:17,039
subject's perspective

419
00:19:17,039 --> 00:19:19,200
and then wrapping that

420
00:19:19,200 --> 00:19:22,260
as a behavioral researcher in an

421
00:19:22,260 --> 00:19:24,600
objective model not to mean that it's

422
00:19:24,600 --> 00:19:26,640
the one only true model but it's the one

423
00:19:26,640 --> 00:19:30,059
from the outside with the subject as

424
00:19:30,059 --> 00:19:32,160
object

425
00:19:32,160 --> 00:19:37,100
and this is called meta Bayesian

426
00:19:39,059 --> 00:19:41,280
so in some ways this is a

427
00:19:41,280 --> 00:19:43,500
formalization

428
00:19:43,500 --> 00:19:44,760
of

429
00:19:44,760 --> 00:19:47,460
ethology

430
00:19:47,460 --> 00:19:49,620
with the cognitive modeling twist

431
00:19:49,620 --> 00:19:51,960
because we're explicitly saying that

432
00:19:51,960 --> 00:19:55,500
we're interested in this cognitive or

433
00:19:55,500 --> 00:19:59,179
internal map Michael

434
00:19:59,720 --> 00:20:03,380
has has this to do with um

435
00:20:03,380 --> 00:20:06,900
self-evidencing experimental that they

436
00:20:06,900 --> 00:20:08,880
put in the data and they get the results

437
00:20:08,880 --> 00:20:10,919
and then they mother

438
00:20:10,919 --> 00:20:15,780
the subject in between in such a way

439
00:20:15,780 --> 00:20:18,660
that the expectations become true that

440
00:20:18,660 --> 00:20:23,220
the input output relations and so is it

441
00:20:23,220 --> 00:20:27,960
has it to do with itself evidencing

442
00:20:27,960 --> 00:20:30,320
ideas

443
00:20:30,960 --> 00:20:34,200
yeah the experimental

444
00:20:34,200 --> 00:20:37,140
yeah it's very interesting

445
00:20:37,140 --> 00:20:40,799
um like our subjective model

446
00:20:40,799 --> 00:20:44,039
we can see why that is set up in a way

447
00:20:44,039 --> 00:20:45,900
to be self-evidising

448
00:20:45,900 --> 00:20:47,760
we can say that the mouse is moving

449
00:20:47,760 --> 00:20:49,559
around in the team A's

450
00:20:49,559 --> 00:20:52,320
so that it reduces Divergence so that it

451
00:20:52,320 --> 00:20:54,900
can be the kind of mouse that it expects

452
00:20:54,900 --> 00:20:57,539
and prefers to be we can understand its

453
00:20:57,539 --> 00:20:59,580
diverse behavioral Selections in light

454
00:20:59,580 --> 00:21:02,460
of self-evidizing

455
00:21:02,460 --> 00:21:05,280
if we're going to consider ourselves in

456
00:21:05,280 --> 00:21:07,380
a Bayesian light as well

457
00:21:07,380 --> 00:21:09,419
then also

458
00:21:09,419 --> 00:21:11,360
that entails a certain kind of

459
00:21:11,360 --> 00:21:14,160
epistemological self-evidencing

460
00:21:14,160 --> 00:21:18,660
now in the extreme or in one simple

461
00:21:18,660 --> 00:21:21,299
reading self-evidencing would be like

462
00:21:21,299 --> 00:21:23,400
you write down the number four oh I

463
00:21:23,400 --> 00:21:25,440
wonder what number four great write down

464
00:21:25,440 --> 00:21:27,120
the number four look at the number it's

465
00:21:27,120 --> 00:21:28,200
four

466
00:21:28,200 --> 00:21:30,720
well that isn't what self-evidising

467
00:21:30,720 --> 00:21:31,679
means

468
00:21:31,679 --> 00:21:34,980
anymore than for the organism it means

469
00:21:34,980 --> 00:21:37,799
to stay in the dark room

470
00:21:37,799 --> 00:21:39,780
which has been a point of philosophical

471
00:21:39,780 --> 00:21:41,640
contention for like literally 10 years

472
00:21:41,640 --> 00:21:44,580
people say well if organisms are acting

473
00:21:44,580 --> 00:21:47,159
to reduce surprise or to self-evidence

474
00:21:47,159 --> 00:21:49,200
why don't they just stay in the dark

475
00:21:49,200 --> 00:21:50,159
room

476
00:21:50,159 --> 00:21:52,020
dark room challenge

477
00:21:52,020 --> 00:21:53,700
you know Throw Down The Gauntlet and

478
00:21:53,700 --> 00:21:54,539
it's like

479
00:21:54,539 --> 00:21:56,640
right if the generative model didn't

480
00:21:56,640 --> 00:21:59,580
have survival requirements and its only

481
00:21:59,580 --> 00:22:01,679
imperative was to have the reduced

482
00:22:01,679 --> 00:22:04,620
Divergence with a photon distribution

483
00:22:04,620 --> 00:22:07,140
you would and you do see that behavior

484
00:22:07,140 --> 00:22:09,780
but that's not what organisms do

485
00:22:09,780 --> 00:22:11,880
and Maps aren't territories so if you

486
00:22:11,880 --> 00:22:13,440
actually built a realistic generative

487
00:22:13,440 --> 00:22:14,460
model

488
00:22:14,460 --> 00:22:17,100
you would find it self-evidencing

489
00:22:17,100 --> 00:22:19,559
doesn't just mean staying in the dark

490
00:22:19,559 --> 00:22:21,780
room it also means self-evidencing for

491
00:22:21,780 --> 00:22:23,280
temperature and thirst and all these

492
00:22:23,280 --> 00:22:24,480
other drives

493
00:22:24,480 --> 00:22:29,240
so self-evidencing by the researcher

494
00:22:29,900 --> 00:22:32,580
plays out

495
00:22:32,580 --> 00:22:34,799
in a way that would need its own

496
00:22:34,799 --> 00:22:37,980
generative model to be specific about

497
00:22:37,980 --> 00:22:40,140
but for example if the researcher says

498
00:22:40,140 --> 00:22:42,960
my commitment is to maximum information

499
00:22:42,960 --> 00:22:45,000
experimentation

500
00:22:45,000 --> 00:22:48,000
I'm not trying to Simply confirm what my

501
00:22:48,000 --> 00:22:49,080
friend told me

502
00:22:49,080 --> 00:22:51,840
my commitments and expectations

503
00:22:51,840 --> 00:22:55,799
preference is for maximally informative

504
00:22:55,799 --> 00:22:57,659
experiment design

505
00:22:57,659 --> 00:23:00,900
then the experiment that they design

506
00:23:00,900 --> 00:23:04,500
can be understood as self-evidising

507
00:23:04,500 --> 00:23:07,260
even if the data points are maximally

508
00:23:07,260 --> 00:23:09,539
surprising

509
00:23:09,539 --> 00:23:12,620
that would be the point

510
00:23:20,460 --> 00:23:23,059
Ali

511
00:23:24,260 --> 00:23:26,960
and also about

512
00:23:26,960 --> 00:23:29,000
self-evidencing

513
00:23:29,000 --> 00:23:31,980
is in recent literature

514
00:23:31,980 --> 00:23:32,760
um

515
00:23:32,760 --> 00:23:36,080
especially since 2019

516
00:23:36,080 --> 00:23:39,720
there's two distinct way of formulating

517
00:23:39,720 --> 00:23:43,200
uh self-evident self-evidencing one is

518
00:23:43,200 --> 00:23:45,480
in terms of Bayesian model evidence

519
00:23:45,480 --> 00:23:48,840
which is the approach taken uh in this

520
00:23:48,840 --> 00:23:52,140
chapter as well and one is in the the

521
00:23:52,140 --> 00:23:55,500
terms of lagrangian of the autonomous

522
00:23:55,500 --> 00:24:00,380
Paths of active particles so these

523
00:24:00,380 --> 00:24:05,460
autonomous paths are actually or uh in

524
00:24:05,460 --> 00:24:07,740
other words these lagrangians are

525
00:24:07,740 --> 00:24:10,380
actually the expected green energy which

526
00:24:10,380 --> 00:24:14,460
can by themselves decomposed into two

527
00:24:14,460 --> 00:24:17,840
separate terms corresponding with

528
00:24:17,840 --> 00:24:21,059
expected costs on one hand and expected

529
00:24:21,059 --> 00:24:22,640
Information Gain

530
00:24:22,640 --> 00:24:26,580
so uh in this kind of formulation the

531
00:24:26,580 --> 00:24:29,760
expected cost is the surprisal of

532
00:24:29,760 --> 00:24:33,299
sensory paths which is also what's

533
00:24:33,299 --> 00:24:37,400
referred to as uh the sensory

534
00:24:37,400 --> 00:24:41,159
surprisal or the self self evidencing of

535
00:24:41,159 --> 00:24:45,720
the self-organization systems so uh

536
00:24:45,720 --> 00:24:49,260
there are slight uh or there's some

537
00:24:49,260 --> 00:24:51,120
subtle differences between these two

538
00:24:51,120 --> 00:24:53,600
formulations but ultimately they

539
00:24:53,600 --> 00:24:59,059
converge onto same results providing

540
00:24:59,059 --> 00:25:03,360
I mean a description of the behavior of

541
00:25:03,360 --> 00:25:06,780
these systems in uh or in particular

542
00:25:06,780 --> 00:25:10,500
active particles uh in the environment

543
00:25:10,500 --> 00:25:13,440
in terms of minimizing their expected

544
00:25:13,440 --> 00:25:15,980
free energy

545
00:25:16,200 --> 00:25:18,980
thank you

546
00:25:22,620 --> 00:25:24,059
um

547
00:25:24,059 --> 00:25:26,279
when we set up

548
00:25:26,279 --> 00:25:31,039
the ethological moment in this way

549
00:25:31,200 --> 00:25:34,020
it's an active inference problem

550
00:25:34,020 --> 00:25:37,440
so it can be approached using all of the

551
00:25:37,440 --> 00:25:39,419
amazing tools

552
00:25:39,419 --> 00:25:42,980
that are being developed

553
00:25:44,340 --> 00:25:47,039
our focus is on active and specifically

554
00:25:47,039 --> 00:25:49,200
discrete time

555
00:25:49,200 --> 00:25:51,539
however the generic methods may be used

556
00:25:51,539 --> 00:25:55,700
with other likelihood functions

557
00:25:57,600 --> 00:26:01,260
so other normative models of behavior

558
00:26:01,260 --> 00:26:04,080
in active inference our normativity is

559
00:26:04,080 --> 00:26:07,020
around self-evidencing

560
00:26:07,020 --> 00:26:11,760
things should persistent things should

561
00:26:11,760 --> 00:26:14,460
minimize or bound their surprise but

562
00:26:14,460 --> 00:26:16,559
there's no issue with having a

563
00:26:16,559 --> 00:26:18,000
subjective model with a different

564
00:26:18,000 --> 00:26:19,919
normativity

565
00:26:19,919 --> 00:26:23,460
that it should maximize reward as per a

566
00:26:23,460 --> 00:26:26,720
utility function defined

567
00:26:29,600 --> 00:26:32,340
now there will be

568
00:26:32,340 --> 00:26:34,679
a generic inference scheme that can be

569
00:26:34,679 --> 00:26:38,640
used for metabasian inference

570
00:26:38,640 --> 00:26:40,919
then there's going to be a recipe and

571
00:26:40,919 --> 00:26:43,940
then a key example

572
00:26:44,100 --> 00:26:46,500
readers uninterested in certain details

573
00:26:46,500 --> 00:26:49,799
are invited to skip 9.39.4 okay we'll

574
00:26:49,799 --> 00:26:52,460
keep that in mind

575
00:26:56,720 --> 00:27:00,240
parametric empirical base

576
00:27:00,240 --> 00:27:04,919
is a technique that's used to jump start

577
00:27:04,919 --> 00:27:07,320
a Bayesian analysis

578
00:27:07,320 --> 00:27:09,720
with empirical data

579
00:27:09,720 --> 00:27:13,620
so a common question by adherence and

580
00:27:13,620 --> 00:27:16,440
critics in Bayesian statistics is like

581
00:27:16,440 --> 00:27:19,200
well what priors should I choose

582
00:27:19,200 --> 00:27:21,240
because there's no such thing as a

583
00:27:21,240 --> 00:27:24,240
unbiased or a non-biased prior

584
00:27:24,240 --> 00:27:27,539
there's the uniform distribution like

585
00:27:27,539 --> 00:27:30,360
the flat probability distribution but

586
00:27:30,360 --> 00:27:31,980
even that

587
00:27:31,980 --> 00:27:35,100
is over a certain interval

588
00:27:35,100 --> 00:27:38,220
and it still embodies

589
00:27:38,220 --> 00:27:39,840
a prior

590
00:27:39,840 --> 00:27:42,900
albeit one that is uniform across the

591
00:27:42,900 --> 00:27:45,200
range

592
00:27:45,240 --> 00:27:46,740
but there's other situations where the

593
00:27:46,740 --> 00:27:49,200
uniform distribution is not even a

594
00:27:49,200 --> 00:27:51,980
possible selection

595
00:27:52,260 --> 00:27:54,059
for example you can't have a uniform

596
00:27:54,059 --> 00:27:56,220
distribution across all possible Heights

597
00:27:56,220 --> 00:27:58,500
of trees at some point you just got to

598
00:27:58,500 --> 00:28:01,080
be like you know objects have this range

599
00:28:01,080 --> 00:28:02,940
of sizes

600
00:28:02,940 --> 00:28:05,640
and so there's always this question

601
00:28:05,640 --> 00:28:08,340
about selection of priors and Hyper

602
00:28:08,340 --> 00:28:10,440
priors and and so on

603
00:28:10,440 --> 00:28:14,520
now in many cases in practice

604
00:28:14,520 --> 00:28:16,260
it can be shown that it doesn't matter

605
00:28:16,260 --> 00:28:18,419
that much for example somebody will show

606
00:28:18,419 --> 00:28:22,080
that with two wildly Divergent prior

607
00:28:22,080 --> 00:28:24,840
distributions there's so much signal in

608
00:28:24,840 --> 00:28:26,580
the data that it converges to the same

609
00:28:26,580 --> 00:28:29,039
posterior so that's often used as

610
00:28:29,039 --> 00:28:30,720
evidence like hey we tried different

611
00:28:30,720 --> 00:28:33,600
priors and we always got the same

612
00:28:33,600 --> 00:28:35,100
conversions

613
00:28:35,100 --> 00:28:37,220
even with different families of priors

614
00:28:37,220 --> 00:28:41,760
so we feel like this signal is being

615
00:28:41,760 --> 00:28:44,460
driven by patterns in the data and it's

616
00:28:44,460 --> 00:28:47,039
not an aberration because of our choice

617
00:28:47,039 --> 00:28:49,740
of like a very opinionated prior

618
00:28:49,740 --> 00:28:52,020
so that's one strategy people take

619
00:28:52,020 --> 00:28:54,419
presentation of alternative priors

620
00:28:54,419 --> 00:28:55,679
another

621
00:28:55,679 --> 00:28:58,140
approach that can be taken

622
00:28:58,140 --> 00:28:59,820
and it's not exclusive to what was

623
00:28:59,820 --> 00:29:02,460
previously mentioned is parametric

624
00:29:02,460 --> 00:29:06,720
empirical base and so in this situation

625
00:29:06,720 --> 00:29:11,880
empirical data are used to kick start

626
00:29:11,880 --> 00:29:13,740
setting the priors

627
00:29:13,740 --> 00:29:15,539
so we're going to be measuring the

628
00:29:15,539 --> 00:29:17,760
height of children in the whole school

629
00:29:17,760 --> 00:29:20,460
first we're going to measure one

630
00:29:20,460 --> 00:29:22,260
classroom and then we're going to use

631
00:29:22,260 --> 00:29:24,059
the mean and the variance of that

632
00:29:24,059 --> 00:29:25,620
classroom

633
00:29:25,620 --> 00:29:28,740
as our prior moving forward for the

634
00:29:28,740 --> 00:29:31,580
other classrooms

635
00:29:32,580 --> 00:29:34,620
so it's just a way to take empirical

636
00:29:34,620 --> 00:29:35,760
data

637
00:29:35,760 --> 00:29:38,940
and use the empirical data summary

638
00:29:38,940 --> 00:29:40,320
statistics

639
00:29:40,320 --> 00:29:43,500
like mean invariance for gaussian

640
00:29:43,500 --> 00:29:46,380
to jump start subsequent Bayesian

641
00:29:46,380 --> 00:29:48,899
analyzes including of the data that

642
00:29:48,899 --> 00:29:51,419
generated that prior now that analysis

643
00:29:51,419 --> 00:29:52,919
by itself won't be super informative

644
00:29:52,919 --> 00:29:55,020
because you kind of already described

645
00:29:55,020 --> 00:29:57,360
the maximum likelihood solution for that

646
00:29:57,360 --> 00:29:58,620
one data set

647
00:29:58,620 --> 00:30:01,140
but with sub sampling and so on there's

648
00:30:01,140 --> 00:30:04,980
some ways to make PEB really useful

649
00:30:04,980 --> 00:30:07,500
all right so 9.3 9.4 it was like

650
00:30:07,500 --> 00:30:10,620
reminded that they're not essential so

651
00:30:10,620 --> 00:30:11,820
we'll move a little faster through them

652
00:30:11,820 --> 00:30:13,740
but we can return to it

653
00:30:13,740 --> 00:30:14,520
um

654
00:30:14,520 --> 00:30:17,299
next time

655
00:30:18,960 --> 00:30:20,100
okay

656
00:30:20,100 --> 00:30:22,500
here we have the variables that are

657
00:30:22,500 --> 00:30:25,799
being used in the metabasian image in 9

658
00:30:25,799 --> 00:30:27,480
figure 9.1

659
00:30:27,480 --> 00:30:31,140
and there's a focus on the likelihood

660
00:30:31,140 --> 00:30:32,399
function

661
00:30:32,399 --> 00:30:34,200
and um

662
00:30:34,200 --> 00:30:37,200
Bayesian model evidence

663
00:30:37,200 --> 00:30:38,580
is

664
00:30:38,580 --> 00:30:41,159
closely linked to surprise or

665
00:30:41,159 --> 00:30:43,740
self-evidising because the model with

666
00:30:43,740 --> 00:30:45,600
the most evidence

667
00:30:45,600 --> 00:30:48,480
is the one that is least surprised by

668
00:30:48,480 --> 00:30:50,880
data

669
00:30:50,880 --> 00:30:53,580
so sometimes we're talking more about

670
00:30:53,580 --> 00:30:56,220
maximizing likelihood other times

671
00:30:56,220 --> 00:30:58,140
minimizing the surprise

672
00:30:58,140 --> 00:31:01,380
but the tuning of the parameters

673
00:31:01,380 --> 00:31:03,360
is not going to

674
00:31:03,360 --> 00:31:07,939
clash between those approaches

675
00:31:10,860 --> 00:31:13,440
pi

676
00:31:13,440 --> 00:31:16,679
are those policy selections

677
00:31:16,679 --> 00:31:19,200
and so pi

678
00:31:19,200 --> 00:31:23,700
is going to be based upon a minimization

679
00:31:23,700 --> 00:31:27,720
of free energy over all the pies

680
00:31:27,720 --> 00:31:31,260
so this is like active

681
00:31:31,260 --> 00:31:33,480
free energy minimization of the

682
00:31:33,480 --> 00:31:36,299
generative model not of the territory of

683
00:31:36,299 --> 00:31:38,039
the GM

684
00:31:38,039 --> 00:31:41,580
is going to be used as the imperative

685
00:31:41,580 --> 00:31:43,500
for Action selection which is what we

686
00:31:43,500 --> 00:31:45,960
observed from The Mouse and the

687
00:31:45,960 --> 00:31:48,380
teammates

688
00:31:48,419 --> 00:31:51,080
so if we have the model that does best

689
00:31:51,080 --> 00:31:55,620
at predicting their behavior

690
00:31:55,620 --> 00:31:59,580
then we've done a good compromised job

691
00:31:59,580 --> 00:32:02,340
on the subjective

692
00:32:02,340 --> 00:32:06,379
perception cognition action

693
00:32:06,600 --> 00:32:08,640
and there's just a mention of this

694
00:32:08,640 --> 00:32:12,600
temperature parameter shaky handedness

695
00:32:12,600 --> 00:32:14,460
which is just kind of fun to like think

696
00:32:14,460 --> 00:32:16,620
about even just from an intuition

697
00:32:16,620 --> 00:32:18,120
perspective

698
00:32:18,120 --> 00:32:21,419
where in the shaky hand limit the high

699
00:32:21,419 --> 00:32:24,419
temperature everything is shaking a lot

700
00:32:24,419 --> 00:32:26,940
it eradicates

701
00:32:26,940 --> 00:32:29,700
differences in the likelihood of taking

702
00:32:29,700 --> 00:32:31,500
different actions

703
00:32:31,500 --> 00:32:33,480
like if something is just vibrating it

704
00:32:33,480 --> 00:32:36,120
just ends up making all action selection

705
00:32:36,120 --> 00:32:38,340
choices equally likely

706
00:32:38,340 --> 00:32:41,220
whereas like in the sort of

707
00:32:41,220 --> 00:32:44,580
um short-handed limit

708
00:32:44,580 --> 00:32:46,080
then

709
00:32:46,080 --> 00:32:50,220
you follow either your habits or your

710
00:32:50,220 --> 00:32:52,980
free energy minimization

711
00:32:52,980 --> 00:32:56,840
without further hindrance

712
00:32:58,020 --> 00:32:59,820
so

713
00:32:59,820 --> 00:33:02,340
sure handed

714
00:33:02,340 --> 00:33:05,580
optimization of policy

715
00:33:05,580 --> 00:33:08,600
shaky handed

716
00:33:08,760 --> 00:33:11,700
if and if it's soft Max is one

717
00:33:11,700 --> 00:33:14,640
then you're sampling from the posterior

718
00:33:14,640 --> 00:33:16,019
so there's two actions and your

719
00:33:16,019 --> 00:33:19,980
posterior evaluation is 60 40. if this

720
00:33:19,980 --> 00:33:22,440
parameter is one you you select those

721
00:33:22,440 --> 00:33:24,539
policies sixty and forty percent of the

722
00:33:24,539 --> 00:33:26,039
time respectively

723
00:33:26,039 --> 00:33:27,960
if you turn up the temperature it

724
00:33:27,960 --> 00:33:30,539
converges back to 50 50 and if you turn

725
00:33:30,539 --> 00:33:32,100
down the temperature you're always going

726
00:33:32,100 --> 00:33:34,679
to select the higher

727
00:33:34,679 --> 00:33:37,519
um likelihood one

728
00:33:39,659 --> 00:33:43,500
there are some details on the LaPlace

729
00:33:43,500 --> 00:33:45,059
um assumption

730
00:33:45,059 --> 00:33:46,500
there's probably a lot of detail that

731
00:33:46,500 --> 00:33:48,659
could be gone into but

732
00:33:48,659 --> 00:33:52,679
LaPlace puts a quadratic function

733
00:33:52,679 --> 00:33:55,799
that is censored at the maximum

734
00:33:55,799 --> 00:33:59,039
likelihood solution to a distribution so

735
00:33:59,039 --> 00:34:01,860
if it's bimodal there's two humps to the

736
00:34:01,860 --> 00:34:04,740
distribution LaPlace will get stuck at

737
00:34:04,740 --> 00:34:06,059
one of them

738
00:34:06,059 --> 00:34:08,159
or you'll get other kinds of issues like

739
00:34:08,159 --> 00:34:10,020
it'll be stuck at one but very over

740
00:34:10,020 --> 00:34:12,179
dispersed and so on so this isn't

741
00:34:12,179 --> 00:34:14,760
approximating the distribution

742
00:34:14,760 --> 00:34:17,399
completely but it turns out it's a super

743
00:34:17,399 --> 00:34:19,020
computable way

744
00:34:19,020 --> 00:34:21,719
that in many reasonable situations does

745
00:34:21,719 --> 00:34:24,199
really well

746
00:34:24,300 --> 00:34:27,659
now we return to the PEB

747
00:34:27,659 --> 00:34:28,679
so

748
00:34:28,679 --> 00:34:33,119
we want to ask regular Anova type

749
00:34:33,119 --> 00:34:36,619
statistical modeling questions

750
00:34:38,099 --> 00:34:39,899
here

751
00:34:39,899 --> 00:34:43,580
we have a linear model

752
00:34:44,639 --> 00:34:47,760
this is kind of like SPM Style

753
00:34:47,760 --> 00:34:51,679
where X is a design Matrix

754
00:34:54,179 --> 00:34:56,280
it's also used in linear

755
00:34:56,280 --> 00:34:58,500
regressor models won't go into this in

756
00:34:58,500 --> 00:35:00,300
too much more detail but this is kind of

757
00:35:00,300 --> 00:35:01,920
like saying

758
00:35:01,920 --> 00:35:03,300
just like you could do a linear

759
00:35:03,300 --> 00:35:06,800
regression with a design Matrix on

760
00:35:06,800 --> 00:35:08,760
height

761
00:35:08,760 --> 00:35:10,859
you can do a linear regression with a

762
00:35:10,859 --> 00:35:13,500
design Matrix on generative model

763
00:35:13,500 --> 00:35:15,720
parameters

764
00:35:15,720 --> 00:35:18,540
and still test for an effect of age by

765
00:35:18,540 --> 00:35:20,700
height on attention or something like

766
00:35:20,700 --> 00:35:22,819
that

767
00:35:24,660 --> 00:35:26,040
9.5

768
00:35:26,040 --> 00:35:28,020
instructions for model based analysis

769
00:35:28,020 --> 00:35:30,960
Ollie go for it

770
00:35:30,960 --> 00:35:33,660
yeah I just wanted to point out

771
00:35:33,660 --> 00:35:36,720
um the relation between the uh LaPlace

772
00:35:36,720 --> 00:35:40,320
assumption and the form of the

773
00:35:40,320 --> 00:35:43,200
recognition density because

774
00:35:43,200 --> 00:35:47,040
um very briefly uh assuming I mean

775
00:35:47,040 --> 00:35:50,160
LaPlace assumption uh mathematically

776
00:35:50,160 --> 00:35:54,060
leads to this assumption that that the

777
00:35:54,060 --> 00:35:56,640
shape of the recognition density would

778
00:35:56,640 --> 00:35:59,820
be necessarily a gaussian density so in

779
00:35:59,820 --> 00:36:02,839
that case the variation of free energy

780
00:36:02,839 --> 00:36:06,240
would be equivalent to Gibbs energy but

781
00:36:06,240 --> 00:36:10,500
without that LaPlace assumption then we

782
00:36:10,500 --> 00:36:12,720
would have a variational free energy

783
00:36:12,720 --> 00:36:16,079
equals Gibbs energy minus the entropy of

784
00:36:16,079 --> 00:36:18,599
the recognition density and by

785
00:36:18,599 --> 00:36:20,460
recognition density I mean the

786
00:36:20,460 --> 00:36:22,260
probability that the approximate

787
00:36:22,260 --> 00:36:27,720
probability density for hidden inputs

788
00:36:27,720 --> 00:36:31,320
um hidden input variables under a

789
00:36:31,320 --> 00:36:34,460
certain generative model

790
00:36:34,740 --> 00:36:36,180
thank you

791
00:36:36,180 --> 00:36:39,618
many times

792
00:36:40,440 --> 00:36:44,880
getting to a gaussian or a quadratic

793
00:36:44,880 --> 00:36:46,260
is

794
00:36:46,260 --> 00:36:47,940
simplifying

795
00:36:47,940 --> 00:36:50,400
it's symmetric

796
00:36:50,400 --> 00:36:52,560
it has the minimum number of parameters

797
00:36:52,560 --> 00:36:55,980
too to describe like the mean and the

798
00:36:55,980 --> 00:36:57,420
scaling

799
00:36:57,420 --> 00:37:00,500
the mean the variance

800
00:37:01,020 --> 00:37:02,520
but that doesn't mean distributions in

801
00:37:02,520 --> 00:37:04,440
the world are like that

802
00:37:04,440 --> 00:37:07,260
however for ones that are even roughly

803
00:37:07,260 --> 00:37:09,060
shaped

804
00:37:09,060 --> 00:37:10,560
like that like they have a central

805
00:37:10,560 --> 00:37:13,160
tendency

806
00:37:13,160 --> 00:37:17,160
these kinds of symmetric models can be

807
00:37:17,160 --> 00:37:19,619
like super super effective because they

808
00:37:19,619 --> 00:37:21,900
have very very well established methods

809
00:37:21,900 --> 00:37:24,560
for solving

810
00:37:24,900 --> 00:37:27,380
okay

811
00:37:27,420 --> 00:37:28,920
now we're going to get to instructions

812
00:37:28,920 --> 00:37:31,079
for the model based analysis so

813
00:37:31,079 --> 00:37:34,800
definitely contrast slash juxtapose this

814
00:37:34,800 --> 00:37:39,420
with the recipe from chapter six so in

815
00:37:39,420 --> 00:37:41,579
six we were kind of coming to the system

816
00:37:41,579 --> 00:37:44,660
what are we modeling why are we doing it

817
00:37:44,660 --> 00:37:47,400
what structurally

818
00:37:47,400 --> 00:37:50,579
are we looking to include in our GM

819
00:37:50,579 --> 00:37:52,740
how are we going to specifically set up

820
00:37:52,740 --> 00:37:54,240
that GM

821
00:37:54,240 --> 00:37:55,920
and then how are we going to set up the

822
00:37:55,920 --> 00:37:57,960
generative process okay doesn't have to

823
00:37:57,960 --> 00:37:59,640
be in order but these are what we'll

824
00:37:59,640 --> 00:38:04,440
need to figure out now in chapter 9

825
00:38:04,440 --> 00:38:06,960
we have some instructions for how to go

826
00:38:06,960 --> 00:38:09,240
from not just building a GM like we saw

827
00:38:09,240 --> 00:38:11,160
in six but to

828
00:38:11,160 --> 00:38:15,119
using the GM for GM based analysis

829
00:38:15,119 --> 00:38:16,619
so

830
00:38:16,619 --> 00:38:19,320
collecting behavioral data including

831
00:38:19,320 --> 00:38:23,579
metadata or demographic data

832
00:38:23,579 --> 00:38:24,839
two

833
00:38:24,839 --> 00:38:27,240
formulate a pomdp

834
00:38:27,240 --> 00:38:29,220
I I think here they could have said

835
00:38:29,220 --> 00:38:33,720
construct a GM as per chapter 6.

836
00:38:33,720 --> 00:38:36,000
but they're focusing like they mentioned

837
00:38:36,000 --> 00:38:39,300
above on the discrete time case

838
00:38:39,300 --> 00:38:42,000
so that's why they're focusing on the

839
00:38:42,000 --> 00:38:44,660
pomdp

840
00:38:46,200 --> 00:38:50,240
specify a likelihood function

841
00:38:51,240 --> 00:38:53,940
that is going to simulate behavior and

842
00:38:53,940 --> 00:38:55,920
quantify the likelihood of observed

843
00:38:55,920 --> 00:38:57,540
actions

844
00:38:57,540 --> 00:38:58,800
so

845
00:38:58,800 --> 00:39:01,079
we started with data

846
00:39:01,079 --> 00:39:06,180
then we formed a pomdp basically like it

847
00:39:06,180 --> 00:39:08,820
was laid out in chapter six

848
00:39:08,820 --> 00:39:10,560
it turns out we're doing a pmdp like

849
00:39:10,560 --> 00:39:12,560
chapter seven but it's basically like

850
00:39:12,560 --> 00:39:14,579
formulating the GM

851
00:39:14,579 --> 00:39:16,079
and now we're going to actually be using

852
00:39:16,079 --> 00:39:17,280
that GM

853
00:39:17,280 --> 00:39:20,220
for doing the kinds of papers that

854
00:39:20,220 --> 00:39:23,040
people write about where this age group

855
00:39:23,040 --> 00:39:25,920
had with this statistical confidence

856
00:39:25,920 --> 00:39:28,680
level a higher likelihood of this or

857
00:39:28,680 --> 00:39:30,839
that

858
00:39:30,839 --> 00:39:32,520
um

859
00:39:32,520 --> 00:39:34,079
how should the model be used to

860
00:39:34,079 --> 00:39:36,920
calculate a likelihood

861
00:39:36,920 --> 00:39:39,900
specification of priors

862
00:39:39,900 --> 00:39:41,940
that's where

863
00:39:41,940 --> 00:39:42,900
um

864
00:39:42,900 --> 00:39:45,900
you can take a zero centered

865
00:39:45,900 --> 00:39:46,980
um

866
00:39:46,980 --> 00:39:48,839
approach or again one could use

867
00:39:48,839 --> 00:39:51,180
parametric empirical base

868
00:39:51,180 --> 00:39:54,480
to just say oh previous studies found

869
00:39:54,480 --> 00:39:56,940
this distribution so that's going to be

870
00:39:56,940 --> 00:40:00,380
my prior distribution

871
00:40:00,599 --> 00:40:02,720
um

872
00:40:03,300 --> 00:40:05,940
here's our prior probability

873
00:40:05,940 --> 00:40:08,400
and our likelihood

874
00:40:08,400 --> 00:40:11,160
what we need now is the posterior

875
00:40:11,160 --> 00:40:14,420
probability in the model evidence

876
00:40:18,420 --> 00:40:20,640
likelihood function

877
00:40:20,640 --> 00:40:22,320
prior

878
00:40:22,320 --> 00:40:25,800
solving for now with data

879
00:40:25,800 --> 00:40:30,619
posterior distributions and evidence

880
00:40:32,760 --> 00:40:34,200
so

881
00:40:34,200 --> 00:40:36,180
we put information in on the front half

882
00:40:36,180 --> 00:40:38,520
of Bayes equation now we're solving for

883
00:40:38,520 --> 00:40:40,800
the second half of Base equation and

884
00:40:40,800 --> 00:40:43,859
there are routines in SPM in Matlab that

885
00:40:43,859 --> 00:40:46,339
help with this

886
00:40:46,920 --> 00:40:51,240
lastly perform the group level analysis

887
00:40:51,240 --> 00:40:53,760
this is also an opportunity for using

888
00:40:53,760 --> 00:40:56,540
PEB

889
00:40:57,300 --> 00:41:01,640
or you can do any other kind of truly

890
00:41:01,640 --> 00:41:05,099
wide variety of statistical methods

891
00:41:05,099 --> 00:41:06,780
you could

892
00:41:06,780 --> 00:41:09,300
fall back to frequentism and just do a

893
00:41:09,300 --> 00:41:10,500
t-test

894
00:41:10,500 --> 00:41:12,300
just say all right I had 17 in this

895
00:41:12,300 --> 00:41:14,280
group and 35 in this group and we got a

896
00:41:14,280 --> 00:41:15,900
t-test to see if this attention

897
00:41:15,900 --> 00:41:18,060
parameter was significantly different

898
00:41:18,060 --> 00:41:20,160
you could use a canonical variate

899
00:41:20,160 --> 00:41:21,720
analysis

900
00:41:21,720 --> 00:41:25,140
or an independent contrast analysis or

901
00:41:25,140 --> 00:41:27,900
principal component analysis

902
00:41:27,900 --> 00:41:31,560
any statistical method that you would

903
00:41:31,560 --> 00:41:34,920
see in any paper

904
00:41:34,920 --> 00:41:39,000
this is just doing statistics at this

905
00:41:39,000 --> 00:41:41,240
point

906
00:41:44,880 --> 00:41:48,260
a little bit more detail

907
00:41:49,260 --> 00:41:52,380
and figure 9.2

908
00:41:52,380 --> 00:41:54,960
is showing the six step inversion

909
00:41:54,960 --> 00:41:57,560
procedure

910
00:41:57,980 --> 00:42:01,320
collect the data

911
00:42:01,320 --> 00:42:04,020
make the model

912
00:42:04,020 --> 00:42:08,480
specify the likelihood function

913
00:42:09,480 --> 00:42:13,079
P of utilda remember back to 91 util

914
00:42:13,079 --> 00:42:15,960
does the observed Behavior

915
00:42:15,960 --> 00:42:18,780
so we're doing behavioral modeling

916
00:42:18,780 --> 00:42:20,220
what we're interested in is the

917
00:42:20,220 --> 00:42:21,900
distribution

918
00:42:21,900 --> 00:42:27,260
of probabilities of of behaviors given

919
00:42:27,260 --> 00:42:30,660
Theta o and m

920
00:42:30,660 --> 00:42:35,720
Theta o and m

921
00:42:36,420 --> 00:42:39,480
so now we've kind of like

922
00:42:39,480 --> 00:42:42,359
specified the pumdp for the mouse in the

923
00:42:42,359 --> 00:42:44,339
in the maze

924
00:42:44,339 --> 00:42:47,160
but we've pulled back our likelihood

925
00:42:47,160 --> 00:42:48,960
function

926
00:42:48,960 --> 00:42:52,920
to in a way abstract or insulate from

927
00:42:52,920 --> 00:42:57,480
all of those subjective model parameters

928
00:42:57,480 --> 00:43:00,180
such that our statistical analysis is

929
00:43:00,180 --> 00:43:02,119
only about

930
00:43:02,119 --> 00:43:04,859
objective model parameters

931
00:43:04,859 --> 00:43:06,720
so I'd be like if we were making a

932
00:43:06,720 --> 00:43:09,839
cognitive model of a decision to take X

933
00:43:09,839 --> 00:43:11,940
or Y Behavior

934
00:43:11,940 --> 00:43:13,560
we'd want to do that then collect the

935
00:43:13,560 --> 00:43:14,940
data on the behavior

936
00:43:14,940 --> 00:43:17,520
and then do a demographic analysis on

937
00:43:17,520 --> 00:43:21,720
how X or Y is related with this or that

938
00:43:21,720 --> 00:43:25,380
and those statistics are not an appeal

939
00:43:25,380 --> 00:43:27,540
to the cognitive model of the decision

940
00:43:27,540 --> 00:43:29,160
maker

941
00:43:29,160 --> 00:43:31,319
they're actually a layer of descriptive

942
00:43:31,319 --> 00:43:32,460
statistics

943
00:43:32,460 --> 00:43:35,280
that will be absolutely familiar to

944
00:43:35,280 --> 00:43:37,380
anyone who's done behavioral modeling in

945
00:43:37,380 --> 00:43:38,880
the lab it's just a slightly different

946
00:43:38,880 --> 00:43:40,500
way of looking at them

947
00:43:40,500 --> 00:43:43,140
but it can be seen as doing model

948
00:43:43,140 --> 00:43:47,339
evaluation and estimation

949
00:43:47,339 --> 00:43:49,200
using statistics

950
00:43:49,200 --> 00:43:51,660
in a way that doesn't mention any of the

951
00:43:51,660 --> 00:43:55,819
inner cognitive model parameters

952
00:43:56,599 --> 00:43:59,880
because things have been framed in this

953
00:43:59,880 --> 00:44:03,420
nice tan metabasian way

954
00:44:03,420 --> 00:44:07,380
we can do the tale of two densities

955
00:44:07,380 --> 00:44:09,780
and here is kind of a visualization of

956
00:44:09,780 --> 00:44:11,640
what the um

957
00:44:11,640 --> 00:44:15,060
t-test would look like those those error

958
00:44:15,060 --> 00:44:16,440
bars could be bootstrapped et cetera et

959
00:44:16,440 --> 00:44:17,760
cetera but let's just imagine that

960
00:44:17,760 --> 00:44:19,740
they're from a t-test so here's five

961
00:44:19,740 --> 00:44:21,060
groups

962
00:44:21,060 --> 00:44:24,060
and here we have some parameter that we

963
00:44:24,060 --> 00:44:25,440
modeled

964
00:44:25,440 --> 00:44:29,339
their novelty seeking something or some

965
00:44:29,339 --> 00:44:32,339
other specific model inside

966
00:44:32,339 --> 00:44:34,380
and then we can ask like

967
00:44:34,380 --> 00:44:37,099
is there more variance within or among

968
00:44:37,099 --> 00:44:39,119
These Bars

969
00:44:39,119 --> 00:44:41,579
and maybe you see you know a bar with an

970
00:44:41,579 --> 00:44:42,900
asterisk here these two are

971
00:44:42,900 --> 00:44:44,160
indistinguishable but they're both

972
00:44:44,160 --> 00:44:46,680
significantly different than these three

973
00:44:46,680 --> 00:44:48,660
and you know this one is significantly

974
00:44:48,660 --> 00:44:50,400
different from that one but not that so

975
00:44:50,400 --> 00:44:52,319
you see those kinds of like regular

976
00:44:52,319 --> 00:44:56,060
Anova type statistics

977
00:44:56,240 --> 00:45:01,098
here in a Bayesian setup

978
00:45:02,640 --> 00:45:06,240
so specification of the GM chapter 6 and

979
00:45:06,240 --> 00:45:07,859
all that

980
00:45:07,859 --> 00:45:11,940
allows us to do statistics on it

981
00:45:11,940 --> 00:45:14,339
just if you had some Advanced model of

982
00:45:14,339 --> 00:45:17,280
materials properties

983
00:45:17,280 --> 00:45:19,140
and then we were just testing which one

984
00:45:19,140 --> 00:45:22,879
was like harder or softer

985
00:45:27,359 --> 00:45:28,260
um

986
00:45:28,260 --> 00:45:30,420
before analyzing actual data we may want

987
00:45:30,420 --> 00:45:32,160
to check face validity

988
00:45:32,160 --> 00:45:34,560
by generating fictive data

989
00:45:34,560 --> 00:45:37,440
so taking just a look at it visually

990
00:45:37,440 --> 00:45:41,099
and also doing parameter recovery

991
00:45:41,099 --> 00:45:43,319
so that means can you identify or

992
00:45:43,319 --> 00:45:44,700
recover

993
00:45:44,700 --> 00:45:48,780
parameters or do you have issues with

994
00:45:48,780 --> 00:45:52,380
your model like collinearity

995
00:45:52,380 --> 00:45:54,180
where there's two parameters that are

996
00:45:54,180 --> 00:45:56,160
like they're so covariate with each

997
00:45:56,160 --> 00:45:56,940
other

998
00:45:56,940 --> 00:46:00,780
that sometimes you'll run the model and

999
00:46:00,780 --> 00:46:02,339
it will explain the data equally well

1000
00:46:02,339 --> 00:46:04,980
with wildly different combinations of

1001
00:46:04,980 --> 00:46:09,200
those two parameters just for example

1002
00:46:09,660 --> 00:46:13,800
um and again classical SPM like angle

1003
00:46:13,800 --> 00:46:17,240
with a design Matrix

1004
00:46:18,180 --> 00:46:20,160
it's kind of amazing how it works but

1005
00:46:20,160 --> 00:46:22,740
the design Matrix encodes all of the

1006
00:46:22,740 --> 00:46:24,119
experimental

1007
00:46:24,119 --> 00:46:26,640
metadata

1008
00:46:26,640 --> 00:46:28,920
and then you have like a matrix of

1009
00:46:28,920 --> 00:46:31,520
regressors

1010
00:46:31,619 --> 00:46:34,079
and you just smash the design Matrix

1011
00:46:34,079 --> 00:46:36,900
into this beta

1012
00:46:36,900 --> 00:46:39,900
and somehow the dimensions work out and

1013
00:46:39,900 --> 00:46:43,280
it's like what you'd want to know

1014
00:46:43,560 --> 00:46:46,440
like in SPM it'll be one Matrix with

1015
00:46:46,440 --> 00:46:50,460
this is the um the age and so on of the

1016
00:46:50,460 --> 00:46:51,960
of the patients

1017
00:46:51,960 --> 00:46:54,119
and here's a big Matrix with all of the

1018
00:46:54,119 --> 00:46:56,160
fmri data

1019
00:46:56,160 --> 00:46:57,839
and you just smash them together and it

1020
00:46:57,839 --> 00:47:01,140
and then it sets up this

1021
00:47:01,140 --> 00:47:04,879
if it's all set up properly

1022
00:47:07,380 --> 00:47:09,720
okay and then just we'll look at these

1023
00:47:09,720 --> 00:47:11,760
upcoming sections but then talk more

1024
00:47:11,760 --> 00:47:14,640
with open questions and and look at any

1025
00:47:14,640 --> 00:47:16,079
other details

1026
00:47:16,079 --> 00:47:17,760
there's going to be an analysis of the

1027
00:47:17,760 --> 00:47:22,940
Mirza at all socket eye movement

1028
00:47:23,280 --> 00:47:24,660
and

1029
00:47:24,660 --> 00:47:28,020
smooth pursuit of eye movement atoms

1030
00:47:28,020 --> 00:47:30,980
and mersa

1031
00:47:31,140 --> 00:47:34,680
smooth tracking of eye movement

1032
00:47:34,680 --> 00:47:37,920
on the left atoms

1033
00:47:37,920 --> 00:47:43,319
discrete tracking of eye movements mersa

1034
00:47:47,339 --> 00:47:49,680
not much more said than that

1035
00:47:49,680 --> 00:47:51,540
obviously there's papers to go back to

1036
00:47:51,540 --> 00:47:53,040
and those are even generative models

1037
00:47:53,040 --> 00:47:54,599
that we can use

1038
00:47:54,599 --> 00:47:58,339
replicate them and modify them

1039
00:47:59,880 --> 00:48:03,240
models of false inference

1040
00:48:03,240 --> 00:48:05,400
so this is going to be approaching that

1041
00:48:05,400 --> 00:48:08,880
kind of challenging Nexus of well if

1042
00:48:08,880 --> 00:48:10,920
it's Bayes optimal then how is their

1043
00:48:10,920 --> 00:48:13,020
maladaptive Behavior

1044
00:48:13,020 --> 00:48:16,319
how can people have delusions

1045
00:48:16,319 --> 00:48:20,760
How can there be the accommodation of

1046
00:48:20,760 --> 00:48:23,400
chemical changes

1047
00:48:23,400 --> 00:48:27,060
that that just fundamentally alter

1048
00:48:27,060 --> 00:48:31,099
the substrate of cognition

1049
00:48:32,700 --> 00:48:38,460
table 9.1 has a sample of examples

1050
00:48:38,460 --> 00:48:40,619
that um

1051
00:48:40,619 --> 00:48:42,720
our papers

1052
00:48:42,720 --> 00:48:45,540
exploring computational pathology in

1053
00:48:45,540 --> 00:48:47,460
primarily humans

1054
00:48:47,460 --> 00:48:52,040
but almost surely vertebrates

1055
00:48:52,800 --> 00:48:54,839
addiction impulsivity compulsivity

1056
00:48:54,839 --> 00:48:57,000
delusions hallucination

1057
00:48:57,000 --> 00:48:59,160
interpersonal and personality disorder

1058
00:48:59,160 --> 00:49:01,520
ocular motor syndrome

1059
00:49:01,520 --> 00:49:03,060
pharmacotherapy

1060
00:49:03,060 --> 00:49:06,020
prefrontal syndromes visual neglect

1061
00:49:06,020 --> 00:49:10,200
disorders of interceptive inference

1062
00:49:10,200 --> 00:49:12,540
so just

1063
00:49:12,540 --> 00:49:15,839
giving us some examples

1064
00:49:15,839 --> 00:49:18,300
and the notes

1065
00:49:18,300 --> 00:49:20,700
tend to be defining what that pathology

1066
00:49:20,700 --> 00:49:23,220
was

1067
00:49:23,220 --> 00:49:24,140
um

1068
00:49:24,140 --> 00:49:26,220
operationalized as

1069
00:49:26,220 --> 00:49:28,440
by those studies

1070
00:49:28,440 --> 00:49:31,079
and then briefly

1071
00:49:31,079 --> 00:49:35,099
what kind of generative models were used

1072
00:49:35,099 --> 00:49:38,180
to support what new explanations

1073
00:49:38,180 --> 00:49:41,819
predictions or control opportunities are

1074
00:49:41,819 --> 00:49:45,240
presented when we have for example an

1075
00:49:45,240 --> 00:49:48,060
active model of oculomotor syndrome as

1076
00:49:48,060 --> 00:49:50,339
opposed to some other framework for

1077
00:49:50,339 --> 00:49:53,420
describing that behavior

1078
00:49:55,440 --> 00:49:57,119
um

1079
00:49:57,119 --> 00:49:59,280
a useful way of thinking about causes of

1080
00:49:59,280 --> 00:50:00,660
pathological behaviors to think about

1081
00:50:00,660 --> 00:50:03,660
the prior belief used for policies

1082
00:50:03,660 --> 00:50:05,040
and how each part of this may be

1083
00:50:05,040 --> 00:50:06,780
disrupted to give rise to abnormal

1084
00:50:06,780 --> 00:50:08,700
policy selection

1085
00:50:08,700 --> 00:50:11,520
so one could look at a figure and just

1086
00:50:11,520 --> 00:50:12,839
imagine that we're dealing with a very

1087
00:50:12,839 --> 00:50:14,339
simple

1088
00:50:14,339 --> 00:50:17,220
uh mouse-like patient

1089
00:50:17,220 --> 00:50:19,500
on the top here

1090
00:50:19,500 --> 00:50:20,220
um

1091
00:50:20,220 --> 00:50:22,939
or um

1092
00:50:28,740 --> 00:50:31,020
just looking at it here

1093
00:50:31,020 --> 00:50:33,260
if we're interested in that behavioral

1094
00:50:33,260 --> 00:50:37,220
policy selection component

1095
00:50:37,319 --> 00:50:40,619
that is uh conditioned on

1096
00:50:40,619 --> 00:50:43,980
preferences and affordances or habits

1097
00:50:43,980 --> 00:50:46,980
so that's why they mentioned c and e

1098
00:50:46,980 --> 00:50:49,020
because maybe there's a group of people

1099
00:50:49,020 --> 00:50:51,000
who are repeatedly or compulsively

1100
00:50:51,000 --> 00:50:53,099
taking a given Behavior

1101
00:50:53,099 --> 00:50:55,440
but again just totally simplifying

1102
00:50:55,440 --> 00:50:56,880
making this up

1103
00:50:56,880 --> 00:51:00,000
there might be some who prefer that

1104
00:51:00,000 --> 00:51:02,520
behavior and understand the consequences

1105
00:51:02,520 --> 00:51:05,040
of their action so they engage in the

1106
00:51:05,040 --> 00:51:06,960
selection of the behavior because they

1107
00:51:06,960 --> 00:51:10,319
expect slash prefer it

1108
00:51:10,319 --> 00:51:13,200
in another case it actually might not be

1109
00:51:13,200 --> 00:51:16,260
aligned with the preferences but it has

1110
00:51:16,260 --> 00:51:18,300
become habitual

1111
00:51:18,300 --> 00:51:21,540
in which case there's a difference

1112
00:51:21,540 --> 00:51:24,359
cognitive model parameterization

1113
00:51:24,359 --> 00:51:26,579
and potentially different consequences

1114
00:51:26,579 --> 00:51:29,520
or treatment opportunities

1115
00:51:29,520 --> 00:51:30,900
however

1116
00:51:30,900 --> 00:51:33,059
the experiment needs to be carefully

1117
00:51:33,059 --> 00:51:35,660
designed

1118
00:51:36,180 --> 00:51:39,960
to resolve those possibilities

1119
00:51:39,960 --> 00:51:42,540
which is why a great

1120
00:51:42,540 --> 00:51:44,099
advance

1121
00:51:44,099 --> 00:51:46,079
is to be able to basically not just

1122
00:51:46,079 --> 00:51:48,900
pre-register what you're going to do

1123
00:51:48,900 --> 00:51:51,960
but to do the statistical power analysis

1124
00:51:51,960 --> 00:51:54,000
and ask which regions of experimental

1125
00:51:54,000 --> 00:51:56,040
State space are going to be Bayesian

1126
00:51:56,040 --> 00:51:59,220
optimal for for example distinguishing

1127
00:51:59,220 --> 00:52:01,440
two groups of people

1128
00:52:01,440 --> 00:52:03,780
some of whom have a habitual bias and

1129
00:52:03,780 --> 00:52:05,520
others have a strong prior preference

1130
00:52:05,520 --> 00:52:07,319
for something

1131
00:52:07,319 --> 00:52:09,720
but it's not just like any given set of

1132
00:52:09,720 --> 00:52:11,339
experimental data are going to resolve

1133
00:52:11,339 --> 00:52:12,240
this

1134
00:52:12,240 --> 00:52:14,220
because these are not measurable things

1135
00:52:14,220 --> 00:52:16,020
in the world

1136
00:52:16,020 --> 00:52:17,940
they're inferred parameters in our

1137
00:52:17,940 --> 00:52:19,319
cognitive map

1138
00:52:19,319 --> 00:52:22,140
so it's not just going to happen because

1139
00:52:22,140 --> 00:52:24,540
we tried

1140
00:52:24,540 --> 00:52:28,200
that being said we can make the GM ahead

1141
00:52:28,200 --> 00:52:30,900
of time and do the simulations do that

1142
00:52:30,900 --> 00:52:34,020
parameter identification and Recovery

1143
00:52:34,020 --> 00:52:36,839
to determine what kinds of experimental

1144
00:52:36,839 --> 00:52:39,720
paradigms could resolve that if that was

1145
00:52:39,720 --> 00:52:42,859
what we were trying to resolve

1146
00:52:44,460 --> 00:52:46,680
um then there's a few more uh

1147
00:52:46,680 --> 00:52:49,559
interesting points here

1148
00:52:49,559 --> 00:52:52,339
on everything from salience and

1149
00:52:52,339 --> 00:52:54,540
neuromodulators and

1150
00:52:54,540 --> 00:52:57,740
different pathologies

1151
00:52:58,260 --> 00:53:01,140
and they summarize

1152
00:53:01,140 --> 00:53:03,000
they outlined an approach to use the

1153
00:53:03,000 --> 00:53:04,260
models

1154
00:53:04,260 --> 00:53:06,240
previously described to pose questions

1155
00:53:06,240 --> 00:53:08,400
to empirical data so chapter 6 was like

1156
00:53:08,400 --> 00:53:10,859
the recipe for making the GM and then

1157
00:53:10,859 --> 00:53:13,260
here we saw like the kind of second

1158
00:53:13,260 --> 00:53:16,339
stage of the recipe

1159
00:53:16,440 --> 00:53:18,300
the instructions for model based

1160
00:53:18,300 --> 00:53:20,640
analysis

1161
00:53:20,640 --> 00:53:23,579
that helped us see like not just the

1162
00:53:23,579 --> 00:53:25,140
building of the model which got us to

1163
00:53:25,140 --> 00:53:26,339
here

1164
00:53:26,339 --> 00:53:28,859
but now we see the whole six stages that

1165
00:53:28,859 --> 00:53:30,780
gets us like to the visualization and

1166
00:53:30,780 --> 00:53:32,579
statistical analysis that we want in the

1167
00:53:32,579 --> 00:53:34,700
paper

1168
00:53:40,200 --> 00:53:41,760
um they also highlighted that you can

1169
00:53:41,760 --> 00:53:44,520
use these methods to distinguish groups

1170
00:53:44,520 --> 00:53:48,059
for example with a given cognitive model

1171
00:53:48,059 --> 00:53:49,980
or you can use it to distinguish

1172
00:53:49,980 --> 00:53:53,819
alternative model hypotheses given data

1173
00:53:53,819 --> 00:53:56,779
from one or more group

1174
00:53:57,660 --> 00:53:59,880
they gave a bunch of examples in table

1175
00:53:59,880 --> 00:54:01,200
nine one

1176
00:54:01,200 --> 00:54:03,359
and they focused on the examples of

1177
00:54:03,359 --> 00:54:07,500
atoms and Mirza at all in eye tracking

1178
00:54:07,500 --> 00:54:10,140
Pursuit giving just a total clear

1179
00:54:10,140 --> 00:54:12,059
example of how even for the very same

1180
00:54:12,059 --> 00:54:13,260
data

1181
00:54:13,260 --> 00:54:15,000
you can model it with continuous or

1182
00:54:15,000 --> 00:54:17,540
discrete time

1183
00:54:21,960 --> 00:54:24,839
it's a generic method for experimental

1184
00:54:24,839 --> 00:54:26,960
design

1185
00:54:27,480 --> 00:54:29,640
that offers an opportunity to answer

1186
00:54:29,640 --> 00:54:31,260
questions about the function of the

1187
00:54:31,260 --> 00:54:34,700
nervous system and health and disease

1188
00:54:36,180 --> 00:54:39,180
okay

1189
00:54:40,500 --> 00:54:42,420
that's good for our first discussion on

1190
00:54:42,420 --> 00:54:44,940
nine we'll come back next week

1191
00:54:44,940 --> 00:54:47,940
and check any

1192
00:54:47,940 --> 00:54:50,880
questions that people have added

1193
00:54:50,880 --> 00:54:54,839
and uh yeah thanks we'll come back next

1194
00:54:54,839 --> 00:54:57,020
week


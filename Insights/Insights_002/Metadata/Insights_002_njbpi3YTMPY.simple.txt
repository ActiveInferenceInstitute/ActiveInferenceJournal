SPEAKER_00:
Hello, everybody, and welcome back to the Active Inference Insights podcast, brought to you by the Active Inference Institute.

I'm your host, Darius Parvizi-Wayne.

And today I have the pleasure of chatting to Mal Alboracin.

Mal is a cognitive computing PhD candidate at the University of Quebec in Montreal, as well as the director of product innovations and research at Versys, a company which leads the way in applying the active inference framework to AI.

With expertise in computational neuroscience, sexology, consciousness, and much more, Mal's work is broad, innovative, and deeply exciting, which makes me rather excited for this conversation.

Mal, welcome to the show.

Thank you so much for joining us.

As I said, it's an absolute pleasure.

Well, thanks for having me.

So, yeah, as I sort of mentioned in the introduction, your work covers a very broad range of topics and areas of research.

But as you know, this podcast is exploring the themes of active inference and the free energy principle.

So I want to mainly focus on that, even though we can see about how these different themes tie in with that overarching framework.

If one goes on your Google Scholar and sees your work on active inference, again, they'll see it's diverse.

Where I want to start is the paper that you wrote on epistemic communities.

I guess the first question our audience might have, just as a reminder for those listening, this podcast is very much meant as a gentle introduction to active inference, as gentle as an introduction to active inference can be.

So just for our audience, my first question is very simple, and it's just what is an epistemic community?


SPEAKER_01:
So it has many definitions.

It's a loosely used term in a variety of fields.

But the way we've used it is essentially a community of people that share very specific types of beliefs that lead them to sample reality in the same way.

So if you think about, like,

your norms, they usually are tied to a specific way of thinking.

Like, for instance, the fact of going to work.

The fact of going to work 9 to 5 is tied to an ideology that values a certain type of productivity, consumerism, etc., and a certain type of relationships where we are together in the morning, together in the evening, and we are with different people during the day, etc., etc., etc.

your life becomes shaped by these ideas that give rise to norms.


SPEAKER_00:
Cool.

And let's sort of tie in the active inference framework then to this idea that we have a community which is tied together through sampling similar norms, similar sensory observations.

Perhaps you could start just by giving us a

brief or relatively accessible notion of what active inference is.

I mean, Carl did a very good job last week at explicating this, maybe just as a reminder to people, and then how that framework and the truths that the free energy principle discloses can be applied to the formation of an epistemic community.


SPEAKER_01:
Yeah, so active inference as a principle is rather simple.

It's really just the idea that we try to minimize how much we are likely to dissolve.

We don't want to die.

So in order to do so, we're going to try to map the world and understand what might be coming such that we can avoid moments where we might be dissolving or moments that are dangerous for us, more surprising.

We try to create evidence of ourself.

We try to predict ourself

existing in the future.

And to do this, we make a prediction, act in the world, try to change something, see if it changes the way we thought it was going to change, and then update our model of the world.

So you've got this sensory active loop where the agent is embedded in the world rather than just outside of it, mapping it.

So it's a pretty crucial idea that the agent necessarily has a perspective

which is defined by how they are separate and yet embedded in the world.

They have a boundary.

We generally refer to that as a Markov blanket.

This boundary is both defining and defined by the outside world and the agent themselves.

So if you understand that the agent necessarily has to map the world in order to predict what's likely to happen,

but that the agent is only capable of dealing with their perspective and how much they can compute from their perspective, you understand the agent is fundamentally limited.

It has to maximize how much information it has about the world to communicate with potentially other agents, which will also be gathering information about the world, not just from each other, but from their own experience within the world.

Now, what we posited when we were doing our research is that

it would be more costly to just flat out change your entire model of the world if you are wrong than try to seek out agents who might validate your view of the world and therefore also share the same model.

This is where we bring in the notion of niche construction.

When agents exist in the world, as I said, they evidence their own existence.

To do so, they shape the world.

So anytime I walk on the grass, I make a little dent and I create a little path.

And so if other agents like me, same agents with the same model are going to take the same path, eventually that path is going to be pretty obvious.

It's going to pull other agents to that path.

So I and other agents like me are shaping the environment to resemble us.

We therefore thought that confirmation bias here was a way for agents to avoid having to constantly change their model and were more likely to be with people like them, create a world that resembles them, and eventually create something that resembles an echo chamber, at least on the belief path.

What we also posit is the outcome of that is you're not just sharing beliefs.

You're creating the world around you, such that eventually some communities will in fact create some degree of the beliefs they are sharing in the world around them, which is no longer at all the same world as some other community, which may be sharing together another model.

And it becomes practically incommunicable.

Not only are they not sharing the same model, they're not sharing the same world.

They're not sampling it the same way.

And so communicating here is essentially impossible.


SPEAKER_00:
Amazing.

That's super interesting.

And I think definitely maps onto people's experiences if they've ever found themselves down the rabbit hole of an echo chamber, whether that's on Twitter or Instagram or wherever.

I guess a question that comes to my mind here is that

maybe a heuristic idea would be that truth exists as some kind of prior within a generative model that an agent like us has, like, it's important that we tie ourselves in some way to veracity.

Because if not, then we're going to be constantly surprised every time something false happens.

Or, or that false true distinction is a bit blurry, but but you see where I'm going.

So a lot of echo chambers are reinforcing their model of the world, but it doesn't necessarily need to subscribe to any notion of external truth.

To what degree does that ever butt up against a kind of inherent prior, which is that eventually the falsity of your claims is going to come home?

Or can the echo chamber, the entrenchment just keep going, keep digging as far down as possible?


SPEAKER_01:
Yeah, so this is a super interesting idea and we first have to define what we mean by truth and what level of truth.

What do we mean by truth?

Do we mean something that I can observe?

Do we mean something that is verifiable by other agents?

And then what level of truth?

Do we mean the truth of the event?

the social truth of the meaning of the event, the fact that to some extent, no matter what happens, this element will be considered true.

Do you see what I mean?

Like, there's so many ways in which this can break down.

And what this leads us to is, so this is a theory.

This is something I'm trying to develop.

Obviously, it's not...

verified it.

So it's still something to take with a few grain of salt.

But it's a theory that I think naturally falls from what we've done.

Depending on the level of truth you're talking about and your definition of truth.

The fact that something, say, happened or not, can be irrelevant.

What matters about truth is whether it's actionable to some extent.

So let's take an example.

If I decide I can fly, but I never act on it, it has no impact on my life.

My sensory active loop isn't going to change.

My model is not going to change.

So I can carry this belief without ever having any consequence at that level.

I'm not going to die.

But now I can tell other people that I can fly.

And if none of us act on it, but we all squarely believe we can fly.

And it serves a social purpose of identifying each other.

We know who we are.

And then we congregate to each other.

We create a reality together.

We create purposes.

What happens then?

Where does the buck fall?

Where does that belief become a belief that potentially harms us?

I think that's a true question.

So think of things that are more contentious, like does God exist?

Technically, there is no way to prove it.

Literally, part of the definition is that believing God has to be faith-based, or at least, you know, for Judeo-Christian beliefs, it has to be faith-based, which means by definition, you shouldn't need evidence.

You should believe without evidence.

You should take a leap of faith.

Okay, then by virtue of that, we can't verify it, we can't change it.

Your belief is kind of impervious.

So then what are the outcomes of that belief?

How does it allow a community to survive?

How does it allow people to survive?

And there's a really interesting phenomenon that happens there.

Some people will start manufacturing evidence of God.

they will start thinking, okay, well, yes, it's true, I don't need evidence, but I can read certain kinds of things as evidence, and they will select the kinds of things that count as evidence, i.e., things that they can't ascribe a clear cause to.

So, for instance, they won't ask God to feed their dog, because if they did, and the dog doesn't get fed, the dog dies, and now they're like, well,

God didn't do it.

There's a clear proof here.

It didn't happen.

But something more nebulous, like, in the future, I will get a job.

I mean, you know, like, it's highly likely you're gonna, but can you really ascribe the cause of that?

Like, you might have sent some resumes, but there's no guarantee anyone will pick up your resume.

Like, it's practically luck at this point.

So if it's luck, it's God.

because luck is not something we can ascribe a clear cause to.

So then you get these communities that are functional because that belief never really comes into play in their actions.

And so you can have weird types of layered beliefs for certain people.

Like you could have a flat-earther be your manager.

It probably won't change much.

It's not compatible with your beliefs, but in terms of the layer that you share,

The functional layer that you share, it doesn't come into play.

But if I believe I can fly and therefore you can fly and I tell you, go, go, do it now, fly, jump off the building.

Now we're going to have a problem and therefore that kind of belief is less likely to be carried because functionally, it's likely to disappear.

In terms of self-organization, it just is likely to disappear.


SPEAKER_00:
Right?

Yeah, that's interesting.

I guess it's absolutely right to unpick these different layers.

And the fact that I guess one way of looking at it is that truth can have this kind of instrumentalist or pragmatic aspect to it, but also this something that you know, philosophers of language would think about a little bit more in terms of conditional statements and whether something's actually true in the world.

I wonder whether

Therefore, if we look at the active inference framework and we try to apply active inference to this, a question that comes up in my head is, does for an agent to consider their belief to genuinely be true, because I think what we can say here is, if I had the belief that I could fly, perhaps I occupy that truth at an instrumentalist level because I'm surrounded by other people who believe that they can fly and it actually gives me a sense of purpose.

And so I almost delude myself into thinking that but really a push came to shove and someone had me at the top of a cliff edge, I probably wouldn't do it because I don't think there's actually a state of the world out there which says Darius can fly.

Does the active inference framework require for an agent to believe that there is genuinely solid epistemic grounds to believe that something is ontologically the case so that I can actually fly that if I took off,

that I would need to be able to sample or have sampled evidence for that?

Or is there a mechanism that might be peripheral to that, which could give me that trust in a sufficient trust in my model that I maybe I would jump off the cliff?


SPEAKER_01:
So I think we have to look at how life is realised and how you plan yourself in the future, right?


UNKNOWN:
So


SPEAKER_01:
let's assume you really think you can fly and your life just doesn't lend itself to it.

Like, that's not what you do.

You take the car, you hang around, you have coffee, you work, whatever.

It just never comes into play.

You could hold that belief with high certainty because it never gets tested and there's never a risk.

Now, if you were to start having

real risk like that it actually like this that state is not highly unlikely it's like it's over here and you now have to start testing your own predictions of yourself into the world and you're gonna start seeing spikes of free energy like this like you've never seen anyone fly you sure you can but you've never seen anyone fly and the risk associated with not flying is really really really high

I think that's when you start seeing the armor break.

Like I think the difference becomes between someone who has lost touch with actual reality.

Like they genuinely believe they can fly and it becomes an actual thing they need to do now.

They probably will do it, but they will also probably die, which means that, um, these kinds of beliefs, which we generally consider to be psychotic, right?

Like it's, it's, that's the definition.

It's a loss of touch with reality, no matter what that means.

And so when we start couching these ideas in this way, we start deep pathologizing a little bit.

We're like, well, it's not, they're not, they're not sick per se.

There's nothing intrinsically wrong with them, but their beliefs are no longer aligned with the ecosystem within which we exist.

If they were on say the moon for a minute, maybe, you know, they could fly.

Like it wouldn't be an impossible belief.

It's just not a belief here.

that works.

So I think that's why most of the people who hold beliefs that are not related to what we would consider something that happened, like it's not verifiable, it's not something that we can ensure happened or can happen.

Generally, these beliefs don't relate to things that are possibly

harmful to them.

For instance, believing in Trump.

Believe in what Trump says.

It's easy to debunk what he says.

A lot of people have done it.

In fact, it's actually it takes more time to debunk everything he says than just to listen to him.

But it has no impact on your life if you do believe him.

Pragmatically, it won't hurt you.

It's a very useful tool to lie about things that are

completely unverifiable to you.

Therefore, why not believe them?


SPEAKER_00:
Yeah, I think what you spoke about in terms of deep apologizing is very interesting.

When I was speaking to Carl, for those who are listening, they might recall we sort of spoke about the fact that

the complete class theorem states that actually any behavior can be cast as Bayes optimal under some prior beliefs.

So I think that's a nice, it lends itself to a humane approach to psychopathology or confirmation bias, more mundane instances of circular reasoning or convincing yourself that something is true when it's clearly not.

Going back to your paper, I thought an interesting thing was, so for those who don't know, Mao and Maxwell and others sort of created a computational model in addition to the theorizing, right?

So there is the theory, and then you would put it in silico.

you endowed your silico agents with an a priori hypothesis that or preference that they would be more likely to sample agents who are similar to them, which is in a sense, the kind of prior preference, which underwrites confirmation bias.

My question here is,

In that, are you stipulating that we too have those prior preferences?

Because on one hand, yes, I mean, that manifests in the behaviour and it makes sense in terms of confirming your own model.

At the same time, humans can also be incredibly flexible creatures who do change their model on the basis of evidence.

So firstly,

Is the claim there that that is an actual feature of human cognition and our generative model?

And two, if so, would you couch that in terms of natural selection and evolutionary pressures?

Or is it even more fundamental to self-organization to sample information from those who are more similar to you?


SPEAKER_01:
Right, so confirmation bias was a parameter.

It was something we tested with and without or on a continuum and we saw what the effects of it were relative to the formation of echo chamber or to the change in beliefs.

And then we saw that it had a magnifying effect, but it wasn't as simple as just you turn it off and it's gone and you turn it on and it's there, right?

There was still the possibility to form communities without

uh the notion of confirmation bias it just wasn't as strong and it also depended on the network dynamics uh different networks for propagate information differently so um we also had the notion of uh habit formation so if you have uh the need to uh to have habits basically you're going to slowly start sampling the same thing over and over and over again so um

There were a lot of parameters that we used, and we found that confirmation bias basically biased the agent's likelihood model to acquire more evidence about their environment if the information confirms their pre-existing beliefs.

It was that simple, basically.

So basically, it just made it so that they would still sample people around them, but they were more likely to resample somebody

that said something that agreed with their model, which is extremely compatible with active inference, right?

You are trying to self-evidence.

So you're going to try and sample things that seem to agree with your model.

So confirmation bias was just a formalization of that specific parameter.

What we see is it's actually adaptive for you to be with people like you.

When you are with people like you, not only are you likely to understand people better and therefore share information more efficiently, you're also likely to be in an environment that is beneficial for you, not just because you craft the environment around you, but because if you find yourself with people like you, you're probably finding yourself in an environment that benefits those people.

So, you know, it's just adaptive now.

It's adaptive to an extent.

If you're only with people with exactly your same model, then you're also very likely to have the same issues and to be vulnerable to the same kind of volatility.

It's like monoculture.

If you have one type of banana, if there's one disease banana, they're all going to get it and you no longer have bananas.

It's the same idea.

You want to continuously grow and change and gather more error and increase your fitness landscape such that you never get to a point where there's an error that none of you saw coming and it just wipes everything out like a black swan event.

So there's like a nice middle ground.

It's like a super stimulus.

If you find yourself in an echo chamber, it feels really, really good.

for a time.

But there is a part of your model, the part that wants exploration, that eventually understands that you're stagnating and something needs to change, something needs to happen.

So it's not because we try to confirm our model that we also just want the model to be stationary.

We want to minimize free energy and we want to know that we are minimizing free energy, right?

It's layered and metacognitive.

So in that sense, you wanna see yourself going down.

So if you're in a plateau, you're no longer going down, right?

So that's why I think to some extent, even when you have a group that seems to agree, that seems to have the same idea, it'll eventually start disagreeing inside.

Like there'll be people that'll push, that'll keep polarizing such that now there's,

little bit of a difference like you're still some people like well no okay that's that's too far I don't think all of the world is flat maybe you know a little bit but not all of it or I don't think the entire universe is just the planet that's silly do you see what I mean like it's the same belief it's the same kind of moving away from the trajectory that the others are going but you can't always constantly absolutely agree or you cease

to learn information.

So you'll start again, sampling a little bit outside.

It's just that if you're stuck in an echo chamber and the echo chamber has, you know, drifted, drifted, drifted, drifted, drifted so far away from anybody you could ever talk to such that there is no reconciliation point here.

You're just going to have like a straight up.

No, there's nothing to hold on to for you to be like,

well, maybe my model is a little similar, but not quite the same.

So I could, you know, adapt a little bit.

You're there's this, this, this ridge is too far.

It's too, it's too far gone.

So it is adaptive for you to be with people like you, but if it's too like you, then you're stagnating and eventually you're likely to potentially disappear.


SPEAKER_00:
Amazing.

Yeah, loads of things come to mind from that answer.

As you said, again, for those listening, as you said, just important to point out, as you said, to reduce free energy over time, so called expected free energy, in the actual mathematical formula, there is this balance of epistemic and pragmatic actions.

So we can't, we do have to explore the world, and we do have to sample in conditions of uncertainty.

So

if people are curious, this, this sort of evades the so called dark room problem, which was a kind of an initial contention to the free energy principle, which people can read about this over 10 years old now these arguments.

I guess listening to this, something that comes to my mind is

and then well, something that comes to my mind and maybe those listening is why does one end up in these states anyway?

So maybe I'm listening to this and I might, I'm not, but I might have been in some hypothetical world of flat earther and I might start panicking saying, well, what does that actually say about me fundamentally that I was amenable to those claims whatsoever?

Is there in some computational formulation or just some conceptual formulation, an idea about why some people

tend more than others to be wrapped up in conspiracy theory or groupthink, this monolithic way of cognising rather than cognitive flexibility.

And again, I don't mean to put these two on sort of binary polarities as if one is amazing and one's terrible.

As you say, we do spend time with people we agree with, all of us, as we should, or else we'd have no friends.

But is there a reason perhaps why

Some people end up really going down that rabbit hole and it's bizarre for those on the outside as to how they've got to that point.


SPEAKER_01:
Right.

So let's always remember that anything we're going to say is a conjecture because social systems are extremely complex and it's a little bit like chaos theory, like ascribing one or two causes to a very complex phenomenon is always wrong.

We can have a model and it's an approximation.

But we can't make too many blanket statements.

So that's how I'm going to caveat what I'm about to say next.

The second thing I'm about to say is another caveat is everyone does groupthink.

Everyone does it.

It's just that some groupthink is considered, and that's what I meant by the truth.

In the end, can you reproduce any of the experiments that come out from the scientific field?

Can you reproduce a proof?

you run it yourself do you understand everything that comes out the truth is no you have to rely on others for an epistemic chain like is this a justifiable belief yes maybe i think you know that person looks really confident and other people seem confident in them so i'm you know follow that and sometimes

What their model allows you to do is predictions about the world and those predictions happen to come true.

So you, through abductive inference, believe it must be true.

But it doesn't mean that.

In fact, it could be completely unrelated.

Maybe this is just like, you know, something that happens anyway.

If everybody told you, you've got to jump and the sun will come up.

So we've jumped and the sun kept coming up.

And you're like, all right, I'll jump.

And you're like, you're right, the sun did come up.


UNKNOWN:
Wow.


SPEAKER_01:
That works.

So like it could be completely wrong, but you still, you know, that's your best explanation so far.

It allowed you a prediction.

Your prediction worked.

OK, well, there you go.

There's some evidence for that.

In our case, it's more along the lines of our beliefs are the right beliefs and our ideology is the right ideology because it leads to X, Y, Z kind of KPI.

We've arbitrarily defined as the KPIs that correspond to what allows us to minimize free energy.

But it's highly contentious.

Oftentimes, the KPIs we define are the KPIs that allow us to stay in the state that we are, and so it's heavily slanted towards people in power, right?

Propaganda, media, et cetera, all of these tools are built to get you to believe that

You are correct and you are doing the right thing.

And that right thing is to give more money to rich people.

So, you know, let's keep that in perspective for the time being.

Why do some people fall down rabbit holes?

It's a very known thing that there are key funnels for certain kinds of ideologies because certain kinds of ideologies have a dynamic.

they very quickly allow you to feel better about yourself.

Feel better about yourself or feel more in control of the world.

Like you now have an answer.

You can now understand what's going on.

Your model feels like it reduced free energy pretty quickly.

That's one of the reasons conservatism is often very popular, or at least populist conservatism.

It tends to rely on very easy,

very simple answers to very complex problems.

Why is your life crap?

Immigrants, obviously.

It's immigrants.

It's not the vast, complex, industrial series of institutions that keep you in a position where everything around you has been eroded to the point where you have nothing to grab onto because that would be really complicated.

It's immigrants.

So when we understand that certain kinds of patterns of stark reduction of free energy exist in specific rhetorical patterns, we then understand, okay, well, who needs these kinds of reductions?

Who are the people that are currently feeling high uncertainty, very vulnerable, and need something to hold on to?

It's probably people whose model had evidence and sort of no longer does.

There is nothing in the world around them that continues to validate their perspective.

So now they're searching.

Give me anything, anything that will allow me to continue this model of myself, which has crafted who I am.

My phenotype depends on this model.

If it no longer exists, now I have to accept that

I am at the bottom of the food chain and I don't see a way out because it's complicated.

So who are the people likely to be like that?

Well, currently it's men who feel like they're in a crisis of masculinity.

It's very poor people who feel like they're disenfranchised and nobody's helping them.

And the only reason that would be the case is because the elites are sucking the blood of babies.

and to fulfill their evil ends.

They're bringing more immigrants in because the immigrants are the reason that you don't have jobs anymore, et cetera, et cetera, et cetera.

These conspiracy theories generally tend to give you erroneous but easy answers.

The problem is they're usually very wrong.

So they have to prop themselves up with these webs of tangled beliefs, which

locally are a very easy answer, right?

They're like, why?

Why do I see this?

Well, because you know, that thing and that thing is conceptually related, therefore, that must be the answer.

And so we get into this really interesting space where there's people with incredibly complex webs of we really doesn't make any sense if it's more complex, why would they hold on to this?

Well, because

Remember, these things are layered.

They also get a sense of control from this.

They're minimizing free energy somewhere.

At the level of the easy little answer, the easy little thing, they got a small chunk of explanation.

It validates their position in the world.

And they now feel like they have control over things.

They understand the world.

It's actionable.

So all of these reasons give you a very clear profile for the kind of person who is likely to fall

or that pattern of beliefs.


SPEAKER_00:
Fascinating.

Okay, good.

Yeah, that's really useful.

And yeah, perhaps good to remind people that in terms of the actual formula for the free energy principle, as you know, it can be cast as accuracy minus complexity of your model.

So always seeking

the so-called most accurate model in the sense that it fulfills our preferences without added layers of complexity, without greater degrees of freedom.

And maybe to flesh out what you were saying,

You're right, people have these incredibly complex models of how the world works, and it's a p default ring, and that's somehow linked to 911.

But they kind of internally cohere in some strange narrative way.

They're not complex in the sense that they involve contradictions, because if they did, that would be the buildup prediction error.

So it's like you have this stacked generative model, but because everything kind of contextualizes and constrains everything at the lower level, there's actually minimal prediction error to reduce.

It's just that

if you actually spit that out to an external world it just it turns out to be and i think that kind of explains why some of these people have massive existential crises at some point if they're just like oh that whole thing was really really wrong because the whole edifice comes crumbling down and you get this enormous spike of prediction error every layer um and there's something interesting too like even when they encounter a contradiction


SPEAKER_01:
They have the power to resolve it.

They can just make up another thing.

They'll just look at the patterns and be like, this is sort of like that.

That's the explanation.

And that's powerful, right?

You have direct power to minimize free energy right there and then.

It's super powerful and attractive.


SPEAKER_00:
So I can imagine what's happening here in the way you've just described it is you have this sensory signal, which signals that, oh, your belief was wrong.

But because my priors are so sticky, because they're granted so much precision, I can kind of just ignore that evidence against them.

And it's not like I'm actively doing that.

That's just part and parcel of the way that I'm constituted.

Good.

Well, I think, well, that's, it's kind of a bleak picture.

But actually, I think...

it impels us to have some epistemic humility, which is that all of our explanations for the world are probably in the service of minimizing complexity while trying to maintain a modicum of accuracy.

And we're missing out on some important details.

And that's true for all of us, not only the so-called crazy ones.

This leads us quite nicely.

Yeah, go, go, go, go.

Please.


SPEAKER_01:
think it's not just a bleak picture i think it's also hopeful because it gives you a pathway like how do we allow or help people to potentially get out of that yeah and the way to do that is through kindness through empathy and through a soft approach out you can't you can't bash someone on the head to change their beliefs but you can slowly without making fun of them without

telling them that everything they believe is wrong, just slowly untangle one thing, you know, like be part of who they are.

And if you approach this in clinical practice, what this means is instead of telling someone, your belief is psychotic, this is absolutely not real.

You should more like be like, okay, well, tell me what you see.

Bring me into your belief.

And together, from your perspective, let's move out slowly.

It's actually...

something that they do now in certain approaches to clinical practice with people who go through psychosis.

And it does work.

It does make people feel better.

It pulls them away from the edge.

And it's an epistemic stance that is generally less pathologizing.


SPEAKER_00:
Right.

It comes back to what we were saying, this kind of more humane approach to understanding one another.

And yeah, and just flows nicely from like, we're not making this stuff up.

This is how the maths and the physics unfolds in active inference.

You would slowly change the precision that you deploy on certain parts of your model through conversation.

But as you said, it's very rare that

You can do that in a single blow.

So probably not advisable.

Which actually leads us, you know, this conversation regarding social norms, the way we group together, the way our cognition is distributed.

We're not just siloed off inside our skulls.

We rely on one another.

We offload to one another.

That brings us very nicely onto a second part of your work, which I feel like because of your background in sexology, you've been working on social scripts for a long time and you have a very deep rooted interest in it.

So it's gonna be really cool to unpick this with you.

And yes, it is, well, I'd like to discuss social scripts and the kind of active inference account of them.

Obviously when people hear the word script, they think of drama, the theater,

films whatever they might not think that their life is scripted unless they've got some sort of truman show syndrome um what again very simple question to start what is a script and what does it say about our behavior right so there are many again many definitions of scripts this is part of the reason we did these studies is oftentimes in social sciences the work is beautiful it's deep it's um


SPEAKER_01:
Again, empathetic, but it tends to not perfectly define its terms or at least not agree on the definition of the term.

So many paradigms exist that use the same term and can't agree on exactly what it means.

If you try to bring it together, they will adamantly disagree that these terms can be combined because of fundamental issues in the definitions.

That's something I kind of wanted to curtail.

through a formalism.

And so what we showed is that there were basically two main ways to understand scripts.

The first way was through weak scripts.

Weak scripts are the cluster of concepts that generally exist together, and you can sort of draw a Markov blanket around it.

So that's the reason that prototype theory essentially shows, well, can you really define a chair?

There's many things that can be a chair.

Most things that are a chair do not fulfill what we generally consider to be the prototype of a chair.

Lots of chairs don't have backs, or lots of chairs possibly can't be sat on.

Is it still a chair?

Some would define it as a chair nonetheless.

There's many ways that you can define a chair, but in general, there's a cluster of elements

some of which are higher probability and other lower probabilities.

So think of a restaurant.

It's the example everybody gives.

In a restaurant, you generally know there's probably food.

You're probably going to sit.

Not every time, but probably.

You're probably going to sit.

Probably food.

It's probably paid.

It's not free.

You can just go and get it.

And in general, but not always, there's people giving you the food and making the food for you.

So these are concepts that are loosely understood.

More likely or not associated with the concept of restaurant.

But now there's the strong scripts.

So inside that cluster of scripts, there's a sequence.

You don't eat right away.

You're going to enter the restaurant.

Probably going to get seated by someone.

You don't always pick your seat.

Sometimes you do, sometimes you don't.

But, you know, you're going to sit somewhere.

And then you're going to order something.

You're going to look at something.

You're going to order something.

And then you're going to eat.

Maybe.

Maybe you pay first.

So you see how the scripts themselves have some wiggleness to them.

Like some parts of the scripts can come before or after, but there is generally a sense that something comes before another thing.

And there's this, some of these are higher probabilities.

So like I have to enter the restaurant before I eat there.

And I probably have to eat before I pay.

These are like generally, generally pretty high probability.

In between can wiggle a lot.

Like,

Maybe I'm going to take 30 minutes to order.

Maybe I'm not even going to look at a menu.

Maybe I'm going to be brought food in the dark.

See what I mean?

There's wiggleness.

And that allows for some scripts to sort of change a little bit.

This is also why scripts in general tend to be a little bit porous.

They don't stay the same thing forever.

So you have scripts that are of a very long temporal scale in the way they unfold.

And there are scripts that have a very long temporal scale in the way they repeat.

So, for instance, a wedding, a marriage, that's something that's existed for a while.

Like, we've had marriages for a long time.

The exact way that marriages unfolded, that, you know, that's changed over time and it's still changing.

And its purpose also has changed to some extent.

The people it applied to changed also.

But in general, you know,

We consider marriages to be between two people who are going to pool their resources together and we assume want to stick around with one another for a while.

And so in general, we need for a wedding to happen, for two people to know each other, at least a little bit, you know, at least once, just a little bit.

They need to go through the ritual of marriage because otherwise that doesn't apply.

And then they probably need to share some of their resources.

that repeats over time that's repeated for maybe like I don't exactly know when the first weddings happen but I know there are marriages in the Bible so you know there's some degree to that um but each marriage lasts like 20 years maybe maybe 40 maybe 60 if you're lucky like you know what I mean like it used to be very short bursts of time now it can be shorter or longer but the repetition of it happens so that time scale is very long even though the actual

script itself is very short and then there's very long scripts like um like a war will happen over a long period of time or like um the rise of uh the rise of an empire generally follows the same kind of elements of the scripts or the rule of a ruler etc so these can be very very long um so that's that's a script


SPEAKER_00:
That's the script.

Good.

I think, yes, the first thing that I can think of when we hear the word script, again, it's obviously acting to some degree.

Before I get into the question of kind of authenticity.

I think when people hear about mechanistic explanations of behavior, they slightly worry that they're being reduced to some automated robot, especially when we talk about something like habits in active inference.

So lots of papers sort of borrow from reinforcement learning approaches to habits and this notion of belief-free habits versus belief-based behavior or value-free, value-based, however you want to formalize it.

but it kind of comes down to this idea that through repeated practice, we arise in a certain context and act in a certain way.

And under very sort of stringent conditions, like if I touch a hot pan,

You know, my body is reacting in a habitual way.

I know it might seem extremely reductive to say so, but I'm jumping out of the way, but I'm automated in that way.

I do not choose to.

And it seems like in principle, one could codify a loss of behavior in that way, whether it's opening a door, whether it's what to do in the restaurant.

So how much flexibility or degrees of freedom does the active inference framework

Or scriptology, I don't know if that's the word, the ology for scripts, grant the agent in these social constructs.

You said that they're quite porous, they change over time, but within the actual synchronic moment where the agent is acting, where does their own agency and power to actually act upon the world come from?


SPEAKER_01:
Well, it depends on where you're positioned relative to the script.

So everything is perspectival, again.

And let's imagine that you coarse-grain the world into things.

You're not seeing every pixel, right?

You're seeing clusters.

And again, these clusters are defined by these weak scripts.

So how can you recognize something as a door?

Well, something is a door because it has a handle.

And you can sort of see the shape.

It's not quite the wall.

Have you ever been, say, to Japan, where it's wood panels?

Would you be able to tell where the door is if there isn't a handle?

Like, you'd have to see someone opening the door.

And then you'd be like, all right, that's a door.

It opens and I can go through, like, that's a door.

Similarly, have you ever been to one of those very fancy, fancy mansions where everything is glass and

There's no specific handle.

So you're like, Oh God, I want to go outside.

And it's like, it's right in front and you don't know what to do.

You wait for someone to do it.

So in that sense, that's where scripts are interesting.

You are going to learn by looking at other people do things.

And instead of you figuring out a new way to interact with that door, you're just going to repeat that it's competitionally efficient.

just you know you have a purpose you want to be on the other side let's just do the same thing but now let's imagine you know there's a door in front of you it's obviously your door and there's someone coming and you're at this awkward distance where it's like if you don't open the door for them it's kind of rude yeah they're not quite there yet so what are you gonna do you don't have the right answer there's no

Proper way to exist here because it's a little bit ambiguous what you're supposed to do You're in the middle of two scripts here.

You could open it over yourself and go and risk being a little rude so there's there's a there's a cost here or you could open the door for yourself and wait for them and There's a cost of maybe it's awkward because now they feel obliged to run so you're gonna weigh the costs of this and you can change by your by your decision by your script by what you're choosing to enact here and

You're going to tip the scale a little bit.

So in general, the way the script changes in these uncertain places, in these moments of like, this hasn't quite been defined.

There really isn't much of an answer here that is like the obvious one.

So I'm going to tip the scales.

And next time, depending on the outcome, the cost I paid, this will act as evidence for the future of you and that other person.

You have crafted the world through your choice here.

So I wrote a paper on this with Pierre Poirier on gender, essentially explaining, and gender seemed to be pretty binary.

This is not a blanket statement.

Obviously, it's not binary everywhere.

Not across cultures, not all the time.

But let's say in North America, let's say in the 50s, it was fine.

Pretty binary, right?

It was pretty clear.

If you were a woman, you were this.

If you were a man, you were this.

It seemed like it fit within specific roles.

Or let's say the 30s, because the 50s is exactly the time where it started shifting.

But let's say in the 30s.

And then the war happened.

And what happened?

Well, you didn't have enough men to do what was needed to do.

So the script that was generally functionally associated to a role was no longer being able to be fulfilled.

but we still needed the function.

And so then we were like, well, okay, well, what's the closest thing to a man?

Well, you know, another human, a woman, woman could do this.

It's just not in their current role, but they can do this.

And so it's like, all right, well, the script that generally was associated to women now expanded a little bit because it expanded the porosity also expanded because the, the semantics semantics are either, um,

very strictly constricted or they start smashing into each other.

Because if I connect one point, everything else can now be connected, right?

So if I can work, I can wear pants.

If I can wear pants, I can wear skirts.

If I can wear skirts, I control my body.

If I control my body, I can have sex.

You see what I mean?

Like it kind of all flows from these natural, normally rigid connections, but you made one connection and therefore, well, now you can make more connections.

And there was no real,

functional error when women did that right when women wearing pants it might shock you socially but she's not going to die you're not going to die so it's acceptable you just have to find the right people around who to do it um and so with urbanization with the fact that after the war uh people gathered in city centers with industrialization etc like with you you had to repopulate

you had to find a job, you had to, et cetera, et cetera, et cetera, et cetera.

You ended up with people being able to connect with one another.

Technology also increased and grew.

We had more access to media, more access to each other.

And through this mishmash, you were capable of fostering certain kinds of porosity such that, well, now gender is, you know, does it mean the same as it once did?

Do we even freeze?

those categories anymore.

Some people don't even think so anymore.

Whether they're right or not is irrelevant.

To them, it's now meaningless.

They don't need that category to exist in the world.

Some people still do, and they use it.

But we found that the fluidity of gender has been made so overt, so clear to people that they feel they can act upon that.

They feel they have the choice to change the script themselves.

So I think it starts from

There's either a functional need for a switch to happen or there's so much uncertainty that effectively it can go either way.

So it'll really be about, you know, your decision.

It's kind of it's a little bit random.

Maybe your specific position in the world will make it so that you are going to make that choice.

But somebody else coming from a different perception would have made a different choice.

So the script in itself was slightly uncertain at that level.


SPEAKER_00:
Cool.

Yeah.

And I guess the conception that comes to my mind is that one can't think about script evolution in terms of these enormous steps.

I mean, clearly you do have these occasional historical moments that call for radical shifts in social convention.

But if you think about the evolution of a restaurant, for example, there's been no single event, at least that comes to my mind, which means that people have to wait to be

sat in a European restaurant, and maybe they go and sit at this table in America, whatever it is.

But maybe through these kind of incremental changes require, you know, necessities pop up in certain niches and are less relevant than others.

So you see this kind of staggered evolution.

Which brings me kind of a question I probably should have started with, which is, in terms of active inference,

You spoke a lot about function, like when the function changes, so the script will change.

Well, that brings me to just ask a very simple question, which is what is the function of a script for free energy minimizing creatures like ourselves?

What does it allow us to do?

And maybe perhaps on the flip side of that, how does it constrain us?


SPEAKER_01:
So scripts allow us to avoid computing everything all the time.

Because you have a script, you could make plans much farther into the future.

You can know how to act in a situation, even though it might be a little novel.

So you stay relatively stable no matter where you go, right?

There's never, I mean, it's not true.

If you have a lot of scripts and you know about a lot of scripts, no matter where you go, you're probably going to be fine.

You're probably going to know how to get what you need.

who to talk to when you need something, et cetera, et cetera.

So because you have these sort of pre-baked policies that have high certainty that you can recognize because you can understand what triggers that script, you're very adapted to the environment.

And that also allows us to avoid friction.

When I know what I'm supposed to do at a restaurant, I'm not going to cause a big commotion by doing something ridiculous or by doing something highly unexpected for everybody else.

We're naturally going to self-organize into the purposes we meant to fulfill when we went there.

That's why it's an interesting experience to go to a different country and make errors and realize, oh, wow, okay, my assumptions were wrong.

This is not how things are done everywhere.

There are other ways to do things.

loosen the prior over, this is the script, and you add an extra layer of context.

This is the script here, and this is the script here, and your model becomes a little bit more complex, but also more adaptable.

So scripts are superbly useful for self-organization and for planning over time.


SPEAKER_00:
Splendid.

Yeah, that's just a super useful addendum insofar as

Yeah, we are limited creatures.

If we could do perfect Bayesian inference and compute every single contextual element of our world, then we probably wouldn't need them.

We would just act omnipotently and omnisciently in all contexts, unfortunately.

Well, maybe not unfortunately.

It's probably quite a good thing.

I was talking about one inherent worry that perhaps all of us have is that

we're just becoming the vehicles of our culture that we're acting without really thinking that we just embody our norms, without really ever having a moment to think, well, did I choose those?

Did they choose me?

This may be irrelevant, but I'm curious to see what you think about it.

There's an Italian playwright called Pierrandello, Luigi Pierrandello, who was writing in the early of 20th century, modernist playwright.

And he wrote a play called Henry IV, Enrico Quarto in Italian.

And in that,

a kind of man acts um they're doing a sort of play in which he is playing the role of a nobleman on a night and he falls off and he ends up believing that he is henry the 12th century henry the fourth and acts accordingly and shapes the whole world so that because he's very rich originally so that all of his butlers end up wearing medieval wear and whatever

Anyway, it unravels during the play that actually for the past five years, he's known, like they all think that he's mad.

He's actually returned back to his normal condition, but continues to act as Henry IV because philosophically,

Those people are more mad than him because he's actually chosen his madness.

He's chosen his script, whereas other people just embody or vehicles for the ideas that are relevant to their context.

Which brings me to something that Jung said, which is that people don't have ideas.

Ideas have people.

So just throwing it out there, all of this kind of smorgasbord of authenticity, to what degree do you think that active inference agents really are anything beyond the contingencies of their time?

Do we have anything new that we can bring or is it only a matter of circumstance when we do something kind of random and change things?

Is there any notion here of genuine agency?


SPEAKER_01:
Ooh, genuine agency.


SPEAKER_00:
I know, I know, I know.

I know it's forward.

It's loaded, that question.


SPEAKER_01:
I think agency is a continuum.


SPEAKER_00:
Yeah.


SPEAKER_01:
It's first of all, agency is really hard to define.

In psychology, you sort of define it.

But if you try to define it anywhere outside of its functional, like observable metrics, like they did this and they assigned it to themselves kind of thing.

it's really hard to define, but if we were to define it as say a degree of entropy, let's imagine that I expect you to do this when that, right?

So there's like this absolute baseline that you can be perfectly predicted.

And then based on that, anything you do outside of that prediction is a form of agency.

If I don't factor your actions into this, you're going to be this.

But given your actions, there's a degree of entropy.

And the more unlikely your actions are, the more agentic you might be, because you factored into the shift in the decision.

Let's assume that's a definition, but it would be a very contentious one.

Not everybody would agree with that.

I think we are very agentic.

constantly exploring.

We're not just exploiting.

We're not just pragmatic.

In fact, these two terms exist, epistemic and pragmatic, within the active inference framework where you try to balance between the two.

You are trying to balance between a pragmatic outcome that keeps you alive, which would be what these people are doing and following the person.

They're trying to stay alive and doing what he's saying because they kind of don't have a choice if they want to keep going.

To do so, they might also be very creative.

They might improve the model of the world.

They might be gathering different things.

They might not act exactly the way he had thought because if he could compute it all, then why would other people do it?

You need somebody to gather the knowledge that you currently don't have.

So there's always a degree to which agents within certain sets

are agentic and so that's where we define those scripted supersets there's highly certain things like who has the money well it's not me it's you so that thing i know if i want the money i know that you're going to tell me to do something but in the way in the something in the distance between the moment that something is fulfilled and me fulfilling it there's like there's

There's a million miles.

It's practically infinite.

Depending on the granularity of the order as well, there's some space for agency.

And so what happens is we sort of agree on what fulfillment means here, maybe.

But you didn't necessarily have a very clear picture of what that fulfillment meant.

have some criteria and everything else kind of falls outside and if we were to again fully try to define what those criteria mean we would go in sort of infinite regress of it's impossible to define everything you need a definition to define yeah so um because of that like infinite metonymy because of that space in between the the

the time you are given the action and the fulfillment, there is space for agency.


SPEAKER_00:
Interesting.

As you said, it probably would raise some contentions from the philosophers.

guess my my pushback on that just to sort of play devil's advocate would be um this notion i mean just back down to the definition of agency and we don't have to go deep into this um because it's somewhat peripheral but i guess it's interesting to think of it as entropy um away from my expectations of you given my model of you is obviously contingent on my knowledge of you right so if i'm a baby

well everyone is super free because they're doing wacky stuff the whole time so i guess the on the complete other end of the spectrum you have a god or laplace's demon who can model everything you're going to do in and that you're just kind of some porn in some chess game so completely as you as you said it's you're a dependent yeah exactly it's all perspective um which which i guess is actually one of the fundamental insights of active inference which is that um

what is free energy to you might not be free energy to me because we're all self-evidencing or self-organizing for our own ends, for the maintenance of our own models.

All right, excellent.

Cool, well then we'll jump to the third paper, the third theme, but it actually all kind of ties together, I think, which makes sense given that obviously it's your work and these are coherent interests.

You wrote a paper recently

I think it was this year with with Ali Ali Ramju, who's obviously is a great friend of the Institute, and helps Daniel out with with everything.

So shout out to Ali.

And this one really piqued my interest.

So this is intra active inference.

I think right now it's am I right says only out on psi archive at the moment.

Is that right now?


SPEAKER_01:
Yes, it's a preprint.


SPEAKER_00:
It's still a preprint.

Well, definitely people should check out because it's a great paper.

They might have to get their head around some terminology because it's quite rich with specific terminology.

But I'd love to be able to unpick some of the insights that...

you and Ali came up with in that paper.

I guess the first one here is that you say that, and I can quote here, a genuine FEP, so FEP is free energy principle, a genuine FEP-based ontology must reject any discontinuity between matter and subjectivity as FEP does not confer any uniqueness to cognitive or active agents.

The same mathematical technology applies equally to a whole range of entities from inert rocks to the brain.

So

There's a natural tie there to what we've just been talking about in terms of agency and free will.

But I want to actually stay a little bit away from that and talk more about this discontinuity, this separation that we sort of intuitively make between matter and mind or matter and subjectivity.

Just for the kind of layman people out there who go, well, of course they're different because this is all mental.

and the chair in front of me is not, and okay.

So what does this, so this sounds bizarre.

For them, what does it mean to reject a discontinuity between matter and subjectivity?


SPEAKER_01:
Right.

So let's start from the beginning.


SPEAKER_00:
Please.


SPEAKER_01:
Materialization is the key here.

Matter isn't static.

It's constantly

becoming, it's shaping itself, it's being shaped.

And basically, that's how what we're discussing, which is neo-materialism, is distinct from materialism, where materialism presupposes elements outside of, say, the perception of the individual.

It presupposes it.

There is a true outside in the world, and you have to do your best to figure out what is that truth.

In neo-materialism, we're basically saying

It's not that simple.

We have basically a relationship between what is being perceived, the perspective, the observer, and what is being perceived at all.

The element outside is really just a relationship between two things becoming together.

So, basically, we try to recognize that there's an indeterminacy of matter, and this movement of matter is constrained by possibility spaces.

So, like, we discussed, like, the scripts, right?

Scripts are possibility spaces, but they are also constrained by, say, physical constraints.

Like, you can't fly.

That's just not a thing.

But, you know, there are perceptions that shape what you consider to be things around you, and you would shape the things based on what you consider to be these things.

So basically, given these possibility spaces, you're going to create metastable formations.

And everything is going to align around these metastable formations.

It's the niche formation idea.


SPEAKER_00:
Right.


SPEAKER_01:
And so materialization in this sense is, um, the iterative process where matter continually redefines limits and boundaries.

These are constantly changing and being, um, negotiated.

And this negotiation is due to the way that things, um, self evidence, right?

So matter being relational and

in a sense, self-caused, because things self-organize, is moving this division between the ontology that aligns the view of the self and the matter.

Here, everything is self-evidencing.

Everything is self-organizing.

And effectively, you are also constituting others, and they are constituting you.

others or other things.

So the boundary here is virtual.

It's dependent on a third sort of perspective that defines that you are not the thing in front of you.

This is where we basically determine that there's a probability of sensory state that's interpreted as evident for the agent's existence.

And so everything around you, both

is evidence for your existence and you evidence as well.


SPEAKER_00:
Right.


SPEAKER_01:
So far, is that clear?


SPEAKER_00:
Yeah.

Just for those who maybe are more familiar with the terminology of active inference, am I right to say that this possibility space could be defined as an attractor set?

Is that a bit of a blanket statement?


SPEAKER_01:
No, I think that's interesting.

So basically, in the possibility spaces, or at least according to De La Ronda and Deleuze, you have virtual and actual properties and capacities.

And so if we relate them to the dynamics of FEP and active inference, you basically just have a temporal depth of these possibilities that are directly afforded to you

or potential given say a new agency of the, uh, the elements in play.

And so, yes, you have farther away attractor set that are more or less likely given the complexity that would need to be given that we need to give rise to them.

So if you think of energy, energy is just, um, the, the, the improbability of something happening, high energy means it's highly unlikely for it to happen.

Um, and so, um,

when we deal with these possibility spaces, we're basically talking about the co-constitution of what might happen and what is happening given the collapse of the possibilities by what you are doing and what the world is doing to you.

So you're constantly in this sort of

in this twister where you act on the world and it acts on you and it's constantly being shaped.


SPEAKER_00:
Right and I guess an important elision there not only between subjectivity and matter but also ontology and epistemology.

So ontology is the study of what is and the epistemology the study of how we know what there is or what it is in the sense that

a free energy minimising agent is in the game of basically getting to know themselves in some strange way.

So they're garnering evidence for this own implicit model that they have of themselves and the world that they want, given this attractor state.

So that's a really beautiful way that to know oneself is to be oneself or vice versa.

And I know Carl has some strange inversion of the Cartesian that I think therefore I am.

This is a really interesting question, because I think, or it's a really interesting area, because it just brings up so many fundamental questions.

One being that I think in your paper, you speak about how one can never perfectly actualise a

or be static in a possibility set or put in terms of attracting states, you could never be at stasis, you could never be fully attracted to your attractor set.

You're always in, well, at least for us, we're in this kind of itinerant state where we are open to the world and therefore we have to deal with its kind of stochastic nature.

I guess maybe I'm being an idiot.

What would the world look like if we managed to reach that stasis?

So I understand that this dynamic unfolding, this mutuality of self and other, this getting to know oneself, what we're really coming to is the notion that matter is a process rather than this kind of rarefied static thing.

But if that, I mean, if that process essentially came to a halt, are we invoking that matter itself recedes, the matter hides, if matter is processed, once this thing stops, if it reaches its attracting state, if I reach perfect homeostasis, whatever, do I stop?

Is my ontology rooted in my dynamism?

Or is there something over, what does the materialist say about that?

Or is there something over and beyond this process of self-evidencing?


SPEAKER_01:
No, I think you're touching upon it.

I think if there's no more energy, if there's no more information being exchanged, there's nothing happening.

And therefore, if nothing's happening, you're dead.


SPEAKER_00:
Is there a strange paradox there that

from the outside, if the claim is that this existential imperative is to minimize free energy, to try and get to this precise level of homeostasis, it sounds like what we're trying to get to is equilibrium.


SPEAKER_01:
I just want to stop.


SPEAKER_00:
Why am I cursed with this fact that I can't, that I'm cold and hot.

But that seems to be

the goal, but paradoxically, it would all stop if that was fulfilled, right?

That's kind of the way that I'm interpreting what you're saying.

So again, I'm not giving a teleology to something that doesn't have a teleology.

But how do you kind of unpick that paradox?


SPEAKER_01:
Yeah, well, it's because we're not meant to fulfill

We are the way we are because the universe is the way it is.

The universe leads towards entropy.

Eventually, there is no more energy.

Eventually, everything is spent.

Eventually, everything is equivalent.

There's no information to be gathered anymore.

That's just how things are.

And it exists on such a temporal scale, on such a massive temporal scale, that

At the level of us, this is and always will be.

So we are not meant to ever reach a state where there is no information to be exchanged, where things don't change anymore.

So obviously, we are constantly pulling in the other direction.

And it kind of looks like we're trying to reach a point where we no longer move, but that will never happen.

So it's natural that if the slope is like this, you're trying to go like this.

And basically what you're saying, yeah, but what if you reach the top of the mountain?

What happens then?

Well, yeah, if we reach the top of the mountain, eventually we would cause our own issue, right?

We would reach the point we're trying to avoid reaching.

But the thing is we'll never really get there.

And at the same time, consider we are a product of this system because the system is ongoing.

so we also contribute to reaching this point we are consuming energy and we are going to help eventually reach the heat death of the universe this is not like obviously we're we're trivial at this point right we're a blip but imagine we got better at it then we would probably just make it much much much faster than it would have been otherwise


SPEAKER_00:
That's what a lot of people think.

In many ways, that kind of looks like what we're doing.

Yes, maybe I need to just banish this romantic notion that there might be some top of the mountain.

My Sisyphean struggle will actually have some benefit, some positive outcome.

I think something that you mentioned there in passing, which is really important and really interesting and might

not always be made explicit to those who are learning about active inference is that we're talking about information here.

So I'd love to hear a bit more about information.

We think of matter, again, as this kind of static entity, the models that we learn at school.

But at least my very layman is we have this with the free energy principle and just modern physics as it's unfolding.

There's really beautiful convergence of matter and information and you have statistical mechanics and you have stuff like Shannon information and all of these really fascinating concepts.

When you say that we will get to a point where there is no more information transfer,

where basically, yeah, just everything is as is.

Again, just digging into the ontology there,

is that to say that, and I know there's no hard and fast answer is that just like, are you postulating some kind of it from bits as David Chalmers would say that like kind of the fundamental unit that we're talking here is information and matter is in some way emergent from that?

Is there some ontological hierarchy ontological structure that you can point to there?

Or is it fuzzy?


SPEAKER_01:
So that's an active

area of research, obviously.

There is no perfect answer in the relationship between information and matter.

We can always point to the Landauer principle.

We can talk about how mass and energy are related, and if you have energy, then you have a sense of information, because effectively, information is related to the concept of survival, where the negative log probability of an observation becoming true

uh is effectively you know surprisal and therefore information so there's a relationship there right and if what we're talking about is the relationship between um subjectivity and therefore perception and uh or at least observer dynamics and matter which are mutually shaping there is a sense in which the way that information is is is exchanged effectively

shapes the boundary which gives rise to what we consider the ontological scale at which matter exists.

So in a sense, yes.

But really, when we talk about information, we're really talking about the degree to which a certain bound on a surprisal is related to a sensory data in a model of the

So information is negatory.

When you have unlikely something, that's information.

If everything is equally likely, then there's no information there.

So that's where we get to this relationship between the heat death of the universe and the absolutely equivalent probabilities for anything, because there is nothing there.

And the fact that if you reach the point where you weren't exchanging any information, effectively, there's nothing going on.

There is no information to exchange.


SPEAKER_00:
Yeah, I think that sheds light on a potential point of confusion that people might have when they read the active inference literature, which is that when we're talking about self-information,

we are not talking about a kind of self-evidencing result.

Maximal self-information is not self-evidencing.

In fact, it's quite the opposite.

So that's useful to remember.

If everything was unfolding predictably, you would have less information.

Your model would be working optimally.

If you're getting more information out of your model, well, then there's some divergence going on there.

So yeah, it can all be kind of cast as this...

quite simple statistical picture.

Excellent.

Good.

Well, now, the last thing I want to speak to about I mean, this was fascinating.

And honestly, on this topic, I would love to just go on forever.

Because you've also got another excellent paper that people should check out, which is about consciousness.

Um,

and holographic screens.

And so there's plenty, there's plenty for people to get their teeth into, we really barely touched the surface.

But I guess the last thing that I want to speak about in some depth is your work at versus.

and artificial intelligence, and your PhD as well, of course, which is in cognitive computing.

So obviously within the limits of disclosure and how much you're allowed to talk about, maybe you could kind of give us a little bit of information, pardon the pun, about what Versys is, what Versys does, and how it's kind of revolutionizing

intelligence in some ways and how that's being embodied or encoded into artificial agents.


SPEAKER_01:
Right.

So I guess I can give you the key difference.

Most people consider, just like we discussed, that intelligence is a property of an inside and that everything else is outside.

What we consider is that intelligence is a phenomenon.

It happens as a function of the interactions between different kind of individuals, agents, objects.

There is information being passed and it's the integration of this information that can be self-reflexive through different kinds of boundaries that enable acting in a specific kind of way relative to an outcome you wish to see happen.

So you accrue agency

given how much information you can sort of pool together.

So our approach to intelligence is shared intelligence.

And we consider that shared intelligence is not just about shared bits of data.

It's also creating perspectives, forcing and embedding into the world

And because you have an embedding into the world, you have the possibility to dissolve.

And when you have the possibility to dissolve an information, you have valence.

When you have valence, you have emotion, and then you have the possibility for empathy, you have the possibility for theory of mind, coordination, cooperation, and eventually shared intelligence.

These are key elements that have not been used in the literature so far, or at least are not being

delved into as much from, say, the machine learning crowd.

They're doing amazing work.

They've allowed for very deep processing of data.

But this data is always decontextualized.

The key issue currently is LLMs hallucinate.

And they hallucinate because they have no clue what the context really is or what you're trying to do really.

and where they are relative to this contest.

They have no loop.

And more than no loop, they have no true sense of how to coordinate with you.

They don't know if something happened.

They don't know if you're happy with the answer, really.

Like you can give a thumb up or a thumb down to chat GPT, but while it's giving the answer, it doesn't have that.

So all of these elements together make it so that potentially

this approach is going to hit a wall our approach is more natural and gives rise to this fundamental principle of mutual information exchange where in order to improve your model of the world you have to get to know the thing in front of you better you have to integrate part of it as a part of you and now you're shaping each other

So this whole alignment problem sort of falls out because it's no longer one about one true value alignment.

It's about how all of these pieces coordinate together and create a harmonious self-organizing system that reaches towards something that resembles a metastable state.


SPEAKER_00:
Awesome.

So just to kind of backtrack there and maybe give a broader picture,

really good that we now have a kind of understanding of what Versys is trying to implement.

I thought the mention of kind of the machine learning law and the LLMs is really useful here, because people have, I think, have a notion now, because there's been so much news about ChatGPT and BARD and neural networks, that that's what artificial intelligence is.

perhaps you could give us a very simple carving up of the space in which, if you could, a very brief sort of synopsis about how, what an LLM is, what a large language model is, how it works, what, you know, like vector spaces and probabilities and stuff like this, and then what an active inference system might look like.


SPEAKER_01:
whether that is embodied or not so i might be leading you down on the path yeah so essentially uh transformers or attention mechanisms that have self-attention and attention to the text in question they embed different uh relationships between different words that give you essentially a probability of a next word being produced

It's really that simple.

It's a hierarchy that retains certain words and then gives you probabilities at different scales.

So the more complex your transformer hierarchy, the more complex your attention can be.

But effectively, it's just that.

What is the probability of this word given that that word and that word and that word were said?

So it's content to content, really.

It doesn't understand the structure of the world.

It doesn't have a model.

What it has

is just a sort of given this word, that word, given that word, that word, given that word, that word.

And it's very performant because knowledge, the accrued knowledge that we have crafted in language has a sort of underlying model.

Like we don't talk nonsense.

Most of the time when we talk, we try to make sense.

We're not just spouting out a bunch of random words out of nowhere that speak to nothing of our reality.

But that's limited to that.

Our approach basically gives our models the capacity to, from what it observes, create another layer.

And that layer tries to understand what the causes are.

Why did this happen?

Not what word preempted that word, but

why was that word used and in that case what you would get if you created an llm with an active inference agent what you would do is it's not just something that predicts the next word it's something that predicts the cause of the next word and therefore has a sense of why you're saying what you're saying what you're trying to do

and what it might say if it chooses to go along with that perspective.

Another difference is that the MLMs are not really placing themselves in that loop.

It has self-attention relative to words, but it doesn't know what it is.

It doesn't know what its actions are.

It can't make a choice relative to an objective.

That's something that you get if you have an agent that places itself

within the loop and understands the outcomes of its own actions relative to these causes.


SPEAKER_00:
Great.

Yeah, that's a really useful separation there, a distinction between the two.

An interesting aside that we could get into, but maybe we don't have the time and it's not worth it, which is about

The degree to which language does indeed refer to some external world, because you have all the, you know, the semantic externalists, semantic internalists.

I'm sure Mr. Chomsky would have something interesting to say, but actually I'll refrain.

Something I have heard, which I think is a bit more interesting.

And I was really happy that you mentioned the action perception loop that you are encoding, but maybe a better word there would be that you're embodying or you're allowing these agents to embody.

And I really want to have just a moment to speak about embodiment because these large language models don't have sensory states.

They don't have active states.

And for anyone who's seen a Markov blanket, that seems pretty fundamental to adaptive intelligence, to being able to enact self-organization in the way that we do it.

Something that I have heard is that in some ways this embodiment grants us, and perhaps the robots in the end, with a sort of care.

And I mean care there in the Heideggerian sense.

There's a kind of...

There's a tendency to lean towards the world in a way in which we're interested in the way it's coupled to us and the way it interacts with us and the way we interact with it.

Whereas if we are not embodied, exactly because you don't have that causal impact on the world, you don't embody that fundamental phenomenology of care.

It doesn't really mean much to you, to put it in sort of layman's terms.

I guess bringing it back to a topic that always I want to speak about and need to stop speaking about so much is consciousness.

It's a very loaded question, but is there a degree to which embodying an artificial intelligence doesn't just endow it with goals that are just in principle more aligned with ours, but an actual sense of phenomenology, a consciousness,

And if not, why not?

And if not, what else would we need to add?

And again, I'm not expecting you to solve the whole problem in 10 minutes, but just what is the kind of active inference perspective on a question like that?


SPEAKER_01:
Right.

So I think we need to reformulate what embodiment means.

I think embodiment is just a relation to self within a context, right?

And so, because, because why would a body just mean this?

Are you less embodied when you're in the virtual world?

I would argue you actually are because in effect, your perception of your own body is just an illusion created by your brain.

So what is embodiment really?

I think embodiment is that it's a relationship to self relative to context, which means there's a boundary somewhere.

If you are the entire context, there's no boundary.

So there's no real embodiment.

You have to somehow separate something.

Now, maybe you separate your own parts.

Maybe you are every part, but you understand how the different parts are different from one another.

That's maybe one thing, right?

If you imagine the scenario of, say, an overlord AI that is connected to everything and everyone, its embodiment might be how it can pull

different strands of perspectives given as different pieces, for instance.

So we have to rethink our understanding of environment to be a little less centered on our own experience.

And this leads us to isometry.

Why would phenomenology only be given to something that resembles a physical, because I imagine that's what you kind of had in mind, right?

Yeah, sort of in mind like a humanoid,

sort of android thing which we would give to an AI and therefore they would be anchored in the world in the same way that we are and effectively that would give them a phenomenology.

But I think if we're thinking of embodiment, we're really just thinking of how the thing, the agent, relates to itself relative to its boundaries and how those boundaries define how information is processed relative to its objectives.

I think the problem with that is that it doesn't ensure that we will understand each other.

What we want is to endow the AI with the possibility to truly grok us and for us to truly grok it.

We need to be able to exchange this information in a way that's useful.

So we tend to want to give it an embodiment that resembles ours.

I do think

embodiment in the sense that I defined it is required for what we understand to be consciousness because you need the boundaries to reflect each other.

Without the boundaries reflecting each other, you have a form of information being integrated, but you don't have self and therefore you don't have self reflection.

And I don't think given that you necessarily have access in agency over that access.

You just have movement along some gradients.

So that's what I think will be required to give a form of consciousness to AIs that have a version of embodiment.


SPEAKER_00:
But just to clarify, are you saying that a recursive or embedded system of Markov blankets in which information is being exchanged up and down and in which errors are being passed up and

temporally slower levels contextualizing these higher order levels.

Are you saying that is sufficient for consciousness?

I understand that it might be necessary for consciousness.

But something that I've heard you and Maxwell speak about actually on with the Active Inference Institute is this notion that

the that consciousness might might entail some layer of that cake, which is so called right only.

So it isn't being contextualized.

It is just, I don't know what is issuing information?

How would you?

What does that model look like?

Well, yeah, I don't want to completely butcher it.

So I'll pass it over to you.


SPEAKER_01:
I think the right only parts of the models are just, they're dependent.

Like, depending on the scale, everything can be right only.

Like, it's really a question of what is the degree of access.

And I think when we define right only, we define the conditions for the thing to survive.

You don't have a choice but to breathe air, right?

You don't have a choice but to drink water.

These are not choices you're making.

So to some extent, there is one layer at which something is like it has to happen, right?

And therefore, it's not something that is written on because it's likely to change.

So at the level of an AI,

That's what I mean by the perspectival aspect of their embodiment with a boundary.

If they have a boundary, some boundary, whatever that boundary is, they have a condition for that boundary to be dissolved.

And that condition, those conditions, define the right only layers of the boundary.


SPEAKER_00:
Does that mean in some sense that being able to, for the system to recognize it's distinct from something else, from everything else, is a prerequisite condition for consciousness within this model?


SPEAKER_01:
That's right.

That's right.

In order for a thing to have a self and an embodiment, it has to be somehow bound from other things.

Like it has to be different.

to some extent.

It has to be able to take perspectives that enable this difference.

Like I said, if you had this oral word that you pull into different consciousnesses, it would still, for a time being, be able to pull this specific boundary and embody it in a sense.


SPEAKER_00:
Yes, yes.

And that feeds back very nicely into this Heideggerian notion of care, to take a stance on something, I think is a way that he might express that.

Wonderful.

I mean, I guess my final comment on this would be, given this, given the kind of the perspectivalness, the self versus other distinction is so, at least in this model, crucial to a sense of what it is like to be anything.

how much credence?

Or how do you rejoin the those who speak of boundless consciousness experiences?

Or?

Yeah, ego, death, or oneness?

Are these delusional?

Do they?

Or do they invoke some minimal sense of self?

Where do they fit into this story?


UNKNOWN:
So


SPEAKER_01:
I think that's a really interesting topic, and I'm not someone who's going to invalidate anyone's experience.

So obviously, again, take what I'm saying with a grain of salt.

Your experience trumps my perspective on your experience.

What I think happens is you're not actually dissolving, right?

You are dissolving certain layers of what generally constitutes yourself.

And that experience is powerful and transcendental.

And because you feel like what usually constitutes yourself is dissolved.

You feel like you're connected to everything else, but you're not, you still, you, you still embedded in your body.

These practices are conceptual and tap into

Would we believe to be what that would be like?

But the truth is you don't know.

No one who's ever reported that experience wasn't in a body.

So I think we all tap into that same experience when we go through something that resembles ego death in the sense that there are layers that become nil.

Like we dissolve these layers, but not all layers, which means on a fundamental level,

infinite consciousness is still being tapped into from a perspective.


SPEAKER_00:
Awesome.

Yeah, I mean, I guess I ought to point people to where this philosophical foundation is, and people should definitely check out Thomas Metzinger.

And something that's really important to point out here is that when we're talking about self,

One ought to keep in mind that there's a distinction here between the so-called ontological self, this kind of entity that I think people instinctively associate with, a kind of Cartesian soul, this unchanging entity, and perhaps what we're more concerned with, which is the so-called phenomenal self model, how the system works.

models itself, how it reflects on its own being, how it takes a stance on its own being and what it conceives of itself.

And perhaps the attenuation of some of this self-modelling is exactly what's happening in psychedelics.

And here we speak to people like George Dean, who I'm trying to get on, and I think that would be fascinating.

And some of the work that I'm working on is completely to do with self-modelling.

So this is all very neatly tied into current work going on in Active Inference.

Mal, this has been awesome.

I know our time is basically up.

What do people have to look forward to on your side?

What are the kind of interesting things that you're working on?

Where can people find you?

Where can people reach out to you if they're exhilarated by the stuff we've spoken about today and want to find out more?


SPEAKER_01:
So I'm working on several papers, one on

emergence and inactive inference.

I'm also working on a paper on self-esteem and how to model it in social groups.

And I'm working on a paper on sustainability and how that arises from the notion of resilience and mutually effective resilience.

You can find me on LinkedIn, on Google Scholar and sometimes on Twitter.

Very, very rarely.

I would much more encourage people to reach out to me

by email if they find it on one of my papers or on LinkedIn directly.


SPEAKER_00:
Awesome.

Thank you, Mal.

This was great.


SPEAKER_01:
Thank you.
SPEAKER_01:
Hello and welcome everyone.

It is September 14th, 2023, and we are in the course, Physics as Information Processing.

It's lecture five.

Looking forward to this lecture and then a little conversation.

Chris, off to you.

Thank you.


SPEAKER_00:
Okay.

Thank you, Daniel.

And welcome to session five of Physics of Information Processing.

And today we're going to talk about space-time.

And particularly about the idea that space-time is emergent from information processing.

And this has now become a very mainstream idea in the quantum information and quantum gravity communities.

And there's still, of course...

some people holding out for the idea that space-time is fundamental.

But I would say that at least in these communities, I expect it's become now the dominant idea that space-time is not fundamental, that space-time is not part of the basic ontology of reality or something like that.

and that instead, spacetime is a construct.

So that's what today's session is going to be about.

So for those of you who are just joining or who would like a little review, we started this course talking about quantum information theory.

And the idea that quantum information theory is actually a new science about systems that communicate across a boundary.

And the systems are always called Alice and Bob for kind of historical reasons.

And emphasize that this new science is topological.

It's about connectivity between systems.

It's not geometric.

So it doesn't assume a spacetime embedding.

So it leaves open the possibility that spacetime is actually a construct.

It doesn't assume spacetime up front.

And in the second session of the course, I think, we talked about how this boundary could always be represented as an array of qubits, and that what the systems on each side of the boundary do to communicate is prepare and then measure the states of these qubits.

So they communicate by exchanging

classical information via a quantum channel.

And this introduces a particular kind of quantum noise whenever they don't share the reference frames that they use to prepare the bits and measure the bits.

So it's not necessarily a perfect communication channel from a classical fidelity point of view,

but all of the noise is quantum noise, it's not classical, and it always derives from differences in the reference frames that are used by the two agents.

We reformulated the free energy principle in this setting, and the free energy principle turns out to be very simple.

It turns out to be a classical limit of the principle of unitarity, which is just the principle that information is conserved.

And we saw that the approach to perfect alignment between the two agents or perfect prediction on the part of the two agents becomes more challenging, not surprisingly, as system complexity increases.

So the more complex

the communication, the harder it is to predict, as one would expect.

In the last session, we talked about how constraints on thermodynamic free energy induce compartmentalization in agents.

So whenever agents interact with their environments in fairly complex ways,

then they end up being compartmentalized into compartments that have to communicate classically.

And this is solely a result of quantum information theory.

You don't have to assume any biology to get this compartmentalization.

But clearly this is something that we see in biological systems.

Not only do we see compartments, we also see hierarchical control, which is likewise a sort of automatic outcome of the theory for complex systems.

And finally, last time, we talked a little bit about how these compartments communicate and introduced the idea of communication protocols that involve local operations on a shared quantum resource of some kind.

and classical communication.

We talked a fair amount about how classical communication is a little bit problematic to define in quantum theory because, of course, you're introducing classicality, and so you have to make some very specific assumptions about what's classical.

And we'll see later on today what the key assumption that gives you classicality is as we talk about space time.

But before I go into the topic of space-time, I want to spend a little time... Okay, so this... Sorry, this is where we're going.

We're talking about space-time, and we'll talk about applications to biology next time.

But before we get going on this, I want to spend a little bit of time talking about explanation.

And this is motivated...

by a question that came up, a very long question that touched on many different topics that came up in the interactive Q&A last time.

And a fair amount of that question had to do with, directly or indirectly, the question of what scale-free explanation looks like.

and how it compares to scientific explanation in general, most of which is reductive.

So I want to make a short detour into the question of how reductionist theories, which all of you are probably very familiar with, relate to scale-free theories.

And particularly, the free energy principle is a scale-free theory.

So it's a theory that's unlike most of the theories that most of you probably learned about in college.

So we're all familiar with reductionist theories and the reductionist idea of scientific explanation, which is really an outcome of philosophy of science in the 20s and 30s.

People like Rudolf Carnap.

were among the first to actually make precise what this idea of reductive explanation is.

But the basic idea is that the laws or the dynamics or the formalism that describes behavior at some micro scale, the behavior of little things,

actually explains the behavior of big things.

So the basic reductionist idea is that what the little things are doing is what's important, and what big things are doing are either emergent phenomena or they're just epiphenomenal.

They're essentially just appearances.

And this defines a fundamental scale.

So the real assumption in any reductionist theory is that there's some fundamental scale at which the real dynamics takes place and everything else is just an appearance or it's some sort of emergent phenomenon.

And

I try to be very careful when I use the word emergence because it's one of the most ambiguous words in the philosophy of science and people use it to mean all sorts of different things.

But whenever you say emergent, it always sort of implies the idea that it's emergent from something.

And so the idea of emergence is essentially a reductionist idea.

because it at least implicitly assumes that there's some microscale from which the macroscale is emergent.

And so the discussions of emergent phenomena are often very consistent with reductionism, even if they claim not to be.

And

We're all taught that science is reductionist.

And there are certainly people who publicly claim that science is reductionist.

Richard Dawkins is certainly a shining example of this.

But I suspect that a lot of scientists just kind of play lip service to reductionism without really believing in it.

because they don't really believe that the macroscopic phenomena that they are studying, life, for example, is completely determined by what's going on at the microscale, or what's going on at the microscale plus some stochastic noise or something like that.

So I want to explicitly contrast this with a scale-free theory.

In a scale-free theory,

is not simply a holistic theory.

It's much more precise.

In a scale-free theory, little things and big things obey the same laws, or they have the same dynamics, or they're described by the same formalism.

So it's not that the macroscale is emergent from the microscale.

The macroscale and the microscale actually obey the same laws.

And the relationship between macro and micro is probably best stated as one of implementation.

Macroscale things have microscale components, and those microscale components are doing something or other, and whatever they're doing implements the macroscale system and its behaviors.

But this implementation is not an explanation, or at least it's not an explanation all by itself.

And the bumper sticker version of scale-free theories is just as above, so below.

Whatever laws you have at the macro scale are the same ones you have at the micro scale and vice versa.

So scale-free theories, in a sense, are simple.

It's the same theory at every level of description.

So what is this idea of scale, and how does it relate to what we've talked about in this course?

Well, what we've talked about in this course all has to do with what an agent can observe on its boundary.

So scale has to do with how information is encoded on the boundary.

And we can describe that scale in many different ways.

Here's a commonplace way to describe it in physics, by a relationship between energy and distance or size.

You've probably all heard of the Planck scale.

The Planck scale is the smallest scale at which current physics makes any sense.

And in energetic terms, it's defined by energies on the order of 10 to the 19th GeV, which is giga electron volts.

The LHC, the Large Hadron Collider in Switzerland, gets to about 10 to the 4th GeV.

So many, many orders of magnitude less than the Planck scale.

So the Planck scale is extremely energetic compared to what we can probe experimentally.

The Planck scale energy of 10 to the ninth GeV corresponds to a distance of about 10 to the minus 35 meters, which is incredibly tiny.

An intermediate scale on this diagram is about the scale of a nucleon.

So for example, a proton or a neutron.

They have a rest mass of about one GeV and a size of about one femtometer.

So this intermediate scale is roughly the scale of nuclear physics.

And then if you go to our scale, the scale of things that are about a meter in size, human beings, tables and chairs, lots of animals,

the things we're most familiar with, that corresponds to an incredibly tiny energy.

10 to the minus 5 electron volts is about the energy of a photon of radiation.

And a meter is about the wavelength of long wave radio.

So if you think about visible light,

has a wavelength of about 10 to the minus five meters and energy of about an electron volt.

So it's not surprising that visible light feels energetic to us because it's orders of magnitude above our natural scale of energy, of this sort of thermal environment in which we exist.

So it's the encoding scale and the boundary

that is going to relate theories of things that we see to each other.

And this ends up giving a kind of complicated picture.

If you think of Alice interacting with Bob and measuring things on the boundary,

Sorry, I'm a little bit ahead of myself.

I just wanted to show you a picture of a reductive theory.

This is the Big Bang.

You're all familiar with this.

Something happens at the Planck scale, and the entire universe results from it by just a scale change.

So this is a reductive theory.

A scale-free theory is much more complicated.

So here's what a scale-free theory looks like.

Alice interacts with Bob, and she's looking at her boundary, and her boundary states, the boundary states she sees could be encoded at different scales.

So there may be a small scale, scale one, and a bigger scale, scale two.

And what Alice is looking for is some sort of theory that

that relates the first scale, the microscale, to the macroscale.

And so it's not that the microscale came first like it did in the Big Bang and the macroscale came later.

These are encodings that can be happening simultaneously.

And the theoretical task is to find theories that relate between these scales.

And these are implementation theories, or you might call them embedding theories from one scale into another.

And what we want as a consistency criterion is for these theories to be constant in time.

So as Bob evolves in time,

And so what Alice sees are these encodings on her boundary.

She wants the relationship between encodings at different scales to stay constant, because if it doesn't, she'll never be able to figure anything out.

She'll just have this chaotic relationship between things happening at different scales, and nothing will make sense.

So you have this complicated picture in scale-free theories.

And if you look at this and it seems vaguely familiar or even very familiar, then it should, because this is exactly the kind of explanatory structure or semantic structure that you have in computer science.

If you're studying the behavior of a computer, which is just some piece of hardware, some physical system, then you can describe what that piece of hardware is doing at different scales.

And in fact, what we call the hardware level of description, when we talk about our laptops, for example—

is typically the level of description of circuits of transistors inside microprocessors.

We think of that as the hardware level.

But of course, that's just a description, too, that's layered on top of lots of much smaller-scale dynamics, all the way down to the scale of atoms and eventually nuclei and elementary particles and all that other stuff.

So there's some quantum stuff that's going on that we describe as having a classical hardware level of bit exchange.

And then we describe the behavior of programs on top of that.

So what we want, what makes a computer useful, is that we can assign semantics to programs

And we can talk about semantic relationships between programs.

For example, the relationship between the internet protocol stuff that Zoom is doing and what you actually see on your user interface and hear on your user interface.

That needs to stay fixed over time or Zoom becomes incomprehensible and not programmable, and in fact, not even well-defined as an algorithmic system.

And the same is true for the relationship between the operating system and the hardware description, or between the operating system and Zoom as a program.

Computer science works in exactly this scale-free way.

And so, in a sense, scale-free explanation is very familiar.

It's exactly the kind of explanation that we have in computer science.

And that's why I referred to the relationship between scales as one of implementation, because that's a term that we're familiar with from thinking about

computers implementing programs.

So what are these theories?

If we think in biological terms, we can think of pathways being embedded in a cell or cells being embedded in tissues all the way up to

niches of various kinds, which may be social niches being embedded in ecosystems and ecosystems being embedded in the biosphere.

And so we have all of these essentially semantic theories that talk about how one scale embeds in another scale.

And if we have these sorts of scale transitions, we can think of representing the relationship between transitions at different scales in terms of the activity of some kind of renormalization group, which is transforming entities and processes at one scale into entities and processes at another scale.

And I think in biology,

we can at least put forward the hypothesis that in all the way from pathways embedding in cells to ecosystems embedding in the biosphere, we could put forward the hypothesis at least that the renormalization group flow across all of these scales is trivial.

That is that the embedding theories all have the same formal structure, that the structure that describes how pathways embed into cells is the same structure that describes how ecosystems embed into the biosphere.

Now, I certainly can't prove that.

I believe that it's likely.

So I think it's an interesting hypothesis to investigate.

And this is an hypothesis that I want to leave you with as a result of this class is something to think about.

To what extent in thinking about biological systems or even social systems

are the embedding theories the same?

So, in thinking this way, that the physics of the situation has the same formal structure as computer science has in terms of explanation, we're of course faced with a problem

which is that we're not just reconstructing physics.

We're talking about the physics of interacting observers.

We're talking about the physics of communication.

So we're modeling how we do physics.

And that makes our theory self-referential.

And so we have to base the music from Gödel's theorem

which tells us essentially that powerful self-referential theories can't be both consistent and complete.

And by powerful, I just mean powerful enough to express arithmetic, so not terribly powerful.

So this becomes an issue that we have to live with.

We will always be faced with this issue

of having to construct meta-theories, which tell us how our lower-level theories work.

But we never get to the end of that process.

We never can get to theories that are both consistent and complete, because our tower of theories becomes progressively actually more powerful as it goes up.

in a sense, poses a problem, but in another sense, just tells us what life is like as a scientist, that you can't have everything all the time doing science.

But we probably all already knew that.

So let's now talk about space, end of digression.

and see how these ideas apply to a system or a tower of systems that can construct a spatial embedding in its world, in its observed world.

So let's start out thinking about a system that can see its environment, but what it sees is a completely uniform environment where nothing is happening.

So if you have a uniform environment where nothing is happening, you can't distinguish one part of the environment from another.

So you have no sense of space.

So what I want to do is build up from scratch what's needed to have a sense of space.

And then we'll talk a little bit about what organisms are doing.

And I think a very interesting question is, if you look at phylogeny, at what points in phylogeny, what lineages in phylogeny actually need to have a sense of space?

And what do they have to have in terms of computational power to have that sense of space?

So if you just have a uniform environment, you don't have a sense of space.

But the first thing we can do with an environment is to divide it up into some number of parts that are somehow distinguishable.

So suppose we can segment the environment into two different parts.

And if you think about E. coli, for example, E. coli has a bunch of sensors on the front and a bunch of effectors on the back.

So it can sense much more about the front part of the environment than the back part of the environment.

So already with E. coli, you have this kind of segmentation of the environment into parts.

But so far, nothing has happened in this environment.

We just have segments that can be distinguished.

So let's consider an environment like this that has distinguishable segments.

What one next wants to know about these segments is whether they're next to each other.

And if we just have a bunch of segments,

We just have a set, and a set doesn't have any order or any relationship information.

So I've drawn this environment as a rectangle of rectangles, but that's just a representation

And it looks like there are relationships between these rectangles, but if they're just a set, then there are no relationships.

If we want to add relationships, then we have to add them explicitly.

So let's add some connections between these different segments of the environment to turn that set into a graph.

that says explicitly, for example, that the light green segment is attached to the light blue segment, and it's also attached to the dark green segment.

And the dark blue segment is attached to the light blue segment and attached to the dark green segment.

So now we have something that's much more mathematically complicated than a set.

We have a graph that has some connection information.

So we have a connection topology.

Now, adding this topology does something very important.

It breaks the exchange symmetry that is there if you just have a set.

So if we just have a set, it doesn't make any difference if I exchange, for example, the positions of the upper right light blue segment with the upper left light green segment.

But if we have this connection topology, then it does make a difference.

I've twisted the graph around.

And if I have to break connections to exchange things, then I've actually changed the topology.

So adding a topology, adding connections, can disrupt a symmetry that's there if there is no topology.

So now let's think about, now we have the structure that's required to think about something happening.

So suppose that at some time, as measured in the bit counter time reference frame of the observing system, something happens.

So a dot appears in the light green segment.

And we can express this actually in the notation of quantum field theory.

We can think of the light green segment as a field, and the dot appearing is the occurrence of an excitation in that field.

And so we can write this notation that says that the green field was excited to produce a dot.

which is up arrow green dot.

And sometime after that, it may turn out that the dot disappears in the green field, and some sort of dot appears down in the dark blue field.

And again, we can write this in this kind of creation and destruction operator notation that one has in quantum field theory.

And so this says that the green field is no longer excited, but now the dark blue field is excited.

So this doesn't imply anything about these excitations being the same.

In fact, they're completely different.

One is an excitation of the green field and one is an excitation of the blue field.

But at least now something is happening in the environment.

So we can think about what does it take for an observer to notice that something is happening in its environment.

It has to be able to detect

a change occurring in some segment of its environment.

And that change can involve something happening or something stopping happening in that segment of its environment.

But it doesn't yet have to have any idea of what an object is.

But we can add that.

If the system has a memory

we can have the system connect these events happening in different parts of its environment.

So if the system has a memory that has some number of steps, say, n steps, but here it could even just be two steps, it can see transitions between these two different states.

where there's a dot in the green field, and then a dot in the dark blue field, and then a dot in the green field, and then a dot in the dark blue field, and then a dot in the green field, et cetera.

So I've expressed this in this sort of field theory notation over on the right side.

And what this is is a flip-flop.

It's an observed periodic behavior.

So this is the basic idea behind a clock.

So any observer that can see this, a number of time steps internally, can interpret what's going on in its external environment as a clock ticking.

It can see some periodicity.

So as long as you can have memory and the ability to detect change, you can see periodic behavior in the environment.

So you can see the environment acting like a clock.

So let's think a bit about what this does and does not require.

So seeing a clock in the environment

requires having distinguishable, non-exchangeable sectors of the environment.

So you have to have this connection topology that breaks the exchange symmetry.

It requires having these sectors implement some sort of flip-flop, so something periodic happening.

And you have to have enough memory to see that periodicity.

But it doesn't require any sense of object identity.

So the excitation in the green part and the excitation in the blue part can still just be field excitations of the green part and the blue part.

There's nothing that says that those have to be the same thing.

They just have to appear and disappear in some periodic way.

And having a clock doesn't require a metric space.

You don't need to have any sense of distance defined on the environment.

You just have to have this segmentation that is not rearrangeable.

So this tells us something extremely important, which is that time is more fundamental than space.

Time in the exterior environment is much simpler and easier to observe than space in the exterior environment.

So if we're thinking biologically about phylogeny, we might expect to see organisms

that can detect periodic behavior in the external environment long before they can detect spatial organization in the exterior environment.

And we know that organisms, even down to microbial mats, can respond to the diurnal cycle.

So we know that organisms, in a sense, can detect environmental clocks.

Even organisms that we have no reason to believe divide the environment up into spaces or the sort of space that we're used to, like a Euclidean space.

So now we're talking about adding space to this idea of time and the environment.

So the next step is to be able to see motion.

And being able to see one thing move from one segment of the environment to another segment, as opposed to something happening in one segment and then something happens in another segment.

One has to have this idea of object persistence, that there's a thing that has an identity.

and its identity doesn't change over time.

And it doesn't change when you move it someplace else, when it appears someplace else in the environment.

It's the same object.

So this is an idea that psychologists call object persistence, and it's an idea that physicists call translational invariance.

Translational invariance

means two things.

It means that if I have an object and I move it to a different location, I don't change the object.

It's the same object that I move from here to there.

So the location doesn't change the object.

But it also means the reverse of that.

It means that the object doesn't change the location.

my container remains invariant even though I move the object around in it.

So these are kind of the two faces of translational invariance, and they're the two faces of object persistence.

As the object moves through time, its identity remains fixed, and it doesn't change the time that it's moving through.

So if I'm measuring its motion with a clock, moving the object doesn't change the clock.

These are clearly very classical ideas.

And we talked about earlier classical communication.

And when we talked about classical communication in, for example, the context of a Bell EPR experiment,

We talked about the observers being able to write down some data on a piece of paper or on a disk drive or something and hand those data to someone else.

So this is translational invariance.

The data can't change when Alice gives it to Bob.

Otherwise, there isn't classical communication taking place.

And of course, there's an up to noise built into here, right?

The piece of paper might get wet and the ink might run a little bit, but it's still the same message.

There's still some identity being assumed.

So object persistence is not an idea we get from quantum theory.

It's an idea that we impose on top of quantum theory.

And it's this idea that's intrinsically classical.

So we're looking right here at the quantum to classical transition.

This is it.

This is what gets added to quantum theory to get classical physics.

It's this idea of object persistence.

Now, once we have object persistence, we can kind of go to town constructing space.

And here's how we do it.

We all learned about object persistence this way when we were six months old, doing motor babbling and observing our hands and feeling how our bodies worked and seeing objects that we could manipulate and that didn't change when we manipulated them.

So this is the birth of object persistence.

This is the birth of classicality in human psychology.

So now let's start thinking about building a coordinate system.

What's a coordinate system?

A coordinate system is just a set of labels that are attached to these segments of the environment.

And to build a coordinate system, you have to have an origin.

And an origin means an object, something that's fixed, something that doesn't move around.

And so a coordinate system requires the ability to identify a stigmatic memory, requires the ability to find something in the environment that has a meaning that stays constant over time.

And that's what a stigmergic memory is.

It's something in the environment that provides us with a memory resource that we can count on being the same.

And it's having this fixed point that allows us to label these connected segments of the environment that contain objects that move.

and be able to say, okay, the object moved from this part of the environment to another part.

And I can give those parts labels with respect to this other part that doesn't move, this origin point.

And once I have this, I can construct other invariances.

So for example, I can consider an object that rotates around this origin without changing its identity.

And that defines what's called rotational invariance.

And I can also talk about an object that gets bigger and smaller compared to this fixed point.

And that defines radial distance.

And if you think about the baby in the previous slide that's doing motor babbling, it sees things getting smaller as it moves them farther away.

So the visual system does this computation for us and constructs this radial coordinate for us with respect to our bodies, or specifically our heads, our eyes.

So in doing this very early motor activity, we're actually building the spatial coordinate system

for our brains that's implemented by the hippocampus, the play cells in the hippocampus.

And if I have an origin point in the environment, I can also infer that I am moving, that I have changed my location with respect to that object.

And that's a considerably more complicated inference because you have to have a representation of the self, of you, to say, I moved.

So this is something that comes after you've developed this spatial coordinate system.

You have to be able to locate, in a sense, metacognitively, a particular object that's you in that coordinate system.

So we're going to set that aside and now talk about adding some numbers, so adding a metric.

And adding a metric requires another particular kind of object, which is a ruler, a particular fixed object that can be moved around.

and that has translational invariance and rotational invariance up to some transformation, up to some tensor.

And in Euclidean space, that tensor is the identity.

So in Euclidean space, you have perfect translational and rotational invariance.

As you get into general relativity,

then the ruler can actually change its length as you rotate, or the ruler can change its length as it moves through space.

So effectively, your spacetime can deform.

The coordinates of your spacetime can deform, and that gives you the representation of mass, of course, in general relativity.

So we talked several sessions ago about quantum reference frames.

And I use the example of rulers and clocks as quantum reference frames.

So now we see how to construct those from observations and symmetry breaking and object identity, all of which we need to define a quantum reference frame out in the environment, like a ruler.

And notice that we don't need any of that environmental stuff

to define those QRFs within our own processing.

And in fact, the ones that exist within us, we have no direct access to.

We can look at someone else's brain and talk about their spatial representation system, but we can't look inside our own brains and talk about our spatial representation system.

So this is how we get a metric space from the ground up.

And we've talked about this from a very psychological or biological point of view.

Starting from sort of motor capabilities, motor babbling, for example,

a measure of effort which gives you some sense of mass so if it's easy to move something it's light and if it's hard to move something it's heavy and we understand that when we're six months old because we can measure how much force we're having to apply with our muscles we have a reference internally for this force that we're applying and we

use that internal measurement to attribute this idea of mass to things in the environment.

We have to have memory for manipulations.

We have to be able to identify variances that define objects, object persistence, and we have to be able to build a ruler that we can move around and use to define a metric space.

Now, this is very different from the way physicists approach this.

If you look at the physics literature, then people start with information transfer.

They talk about entanglement as a basic resource.

There's a lot of discussion of bulk boundary duality, and the most well-known one is...

the duality between anti-de Sitter space, which is the sort of space you see inside a black hole, and conformal field theory, quantum field theory on the boundary of the black hole.

So things you can see, for example, going on on the horizon of a black hole are dual to the geometry and the interior by this mathematical relationship.

You can use that mathematical relationship to construct a quantum error correcting code by using the symmetries of the field theory as a resource for redundancy that allows you to represent the same thing in different places.

And as soon as you can do that, you can build up a network of elements that look basically like elements of a lattice.

And a lattice is a graph to which you can assign symmetric relationships and you get a metric space.

And in physics, typically the goal is to get Einstein's equations out of this, to start with some information theoretic sorts of constructs and get the equations of general relativity.

But what are the equations of general relativity?

These are things that Einstein derived by doing fairly simple thought experiments using the invariances of ordinary Euclidean space combined with ideas from special relativity about the speed of light being constant.

And he added the principle of equivalence, which is the idea that

essentially mass and acceleration are the same thing, or that gravity and acceleration are the same, that weight and acceleration are the same thing.

So it's at least reasonable to expect that this inferential process that's going on in theoretical physics is generating the same thing

although in a very formalized way, that we can generate by thinking about how organisms deal with their environments.

And so I want to leave this with you as the second hypothesis here.

The first one was the hypothesis about a trivial renormalization group flow across scales in biology.

The second hypothesis is that these two approaches to emergent spacetime are actually giving us the same information and maybe starting out with the same information, in fact.

We may be able to think about these fundamental building blocks

of information flow and entanglement in terms of the sorts of perceptual relationships that we've been talking about in this class.

So there's our second hypothesis to think about going forward.

And I just want to leave you with the idea that organisms, at least,

manage this very interesting process that we can think of as starting with the construct of memory.

Organisms have memory of multiple scales, and it's memory that allows them to formulate the idea that they're objects.

And as soon as you have objects that persist, you have a source of redundancy.

So you can do error correction.

And if you can do error correction, if you can move the same thing from one place to another and have it encode the same information, then you can build spacetime.

And as soon as you have spacetime, you can use the spacetime as more memory.

You can use the space-time itself as an error-correcting code because space-time has the nice ability to hold lots of copies of some informative structure.

So you can build libraries, you can build computers, or you can build the internet if you have space-time, which gives you more memory, which gives you more object persistence, which gives you more error correction, and the circle just spins around.

So organisms do this.

And the question is, how does it get off the ground?

And next time we can at least speculate about that kind of origin of life type of question in the context of the FEP and its realization in simple systems of communicating observers.

So as always, I encourage you to use the interactive Q&A.

The fifth discussion session with Andrew, who's unfortunately not here today, will be the last day of September, Saturday at 5, as usual, 5 o'clock European time, 8 o'clock California time.

And then the last session of the class will be the 12th of October.

So thank you very much, and I look forward to your questions in the interactive Q&A.


SPEAKER_01:
Thank you, Chris.

Can I ask some short questions?

Sure.

All right.

So first from Lana wrote, is the periodicity the time lengths between and during the excitations built in?

So this was when we had the four cells and there was the inklings of a clock coming into play.


SPEAKER_00:
I think the simplest answer is that if nothing else is happening in the environment except this periodic behavior, then it's not actually meaningful to talk yet about the time between the events.

So if I have a periodic, a system that's quasi-periodic, for example, and nothing else is going on in my environment, then I can't distinguish that from a perfectly periodic system because I would have to have a perfectly periodic reference to know that the other system was aperiodic.

And if you think about the internal clock being driven by information flow into the system, then something has to be happening in the environment for information to be flowing into the system and being counted.

So again, as we talked about a few sessions ago, the internal memory only gets updated when something happens.

So internal and external get tied together.

And this is why we can use external clocks.

But it's also why we depend on external clocks, because our intuitive sense of time is very malleable.

And so we actually, as humans, use external clocks

to correct for the malleability of our own intuitive sense of time.


SPEAKER_01:
Wow.

And there's probably a lot to be said for that Procrustean nature of having the chronometer, the clock on the wall, and the tick, tick, tick as an evolutionarily novel ruler of time playing a logistical role in time-bound society.


SPEAKER_00:
Yeah, and think of earlier clocks that earlier cultures have used and that animals use.

I mean, we have the solar clock, the lunar clock, etc.

And the orbit of Venus clock that was so important to the Mayans and so on.

So there are lots of available clocks in the environment if you have the ability to notice them and enough memory to realize that they're clocks.


SPEAKER_01:
Awesome.

All right.

Here's a question from Upcycle Club.

They wrote, how does representing the network structure and dynamics with quantum embeddings affect the semantic interpretation of the network?


SPEAKER_00:
The embedding theory is the simplest semantic interpretation of the network.

And again, a good way to think of this is through the lens of computer science.

As soon as you have a programming language that allows variable binding, then you've built a very simple kind of semantics into the language.

if you have X as a variable and you have variable binding, then at different times in the execution of the program, X can have different causal consequences for downstream computation because it's been bound to a different value.

So it has different semantics.

So the semantics of X

Again, go back to Gregory Bateson.

The basis of semantics is differences that make a difference, make a difference to behavior, make a difference to actionability.

So variable binding gives you differences that actually make a difference to what the program does.

So what is that?

that is the semantics that's assigned by your embedding theory between what the programming language is doing and what the operating system is doing and so forth.

And I think it's no surprise that the semantics of programming languages end up being

represented with these very general mathematical constructs like category theory, which are essentially tools for representing arbitrary relations between systems.

And that's what embedding theories are.

They're theories that state relationships between systems.


SPEAKER_01:
Awesome.

And very provocative to explore on the biological and life and on the space-time and matter sides, which are, of course, seamlessly integrated through, if only, our own experiences.


SPEAKER_00:
Yeah, this is another place where it's useful to go back to this difference between reductive theories and scale-free theories.

And scale-free theories are all about semantics because they represent the existence of stuff happening at different scales.

And so they naturally raise the question of what are these relationships?

What are these embedding theories?

And in reductive theories, there's a very deep sense in which semantics doesn't matter.

If you think that all that's going on that's important is the stuff that's going on at the Planck scale, then you don't need any semantics.

And so...

You see this in late 60s, early 70s old-fashioned AI, where people talk about...

Computation is purely syntactic.

And this is a very reductionist view of computation that leaves out the semantics.

And without the semantics, the computation is useless.

It doesn't tell us anything at all unless we can map it to something.

So, we have this philosophy of science tradition that tells us that semantics is not important.

And we end up

with tools like the FEP that essentially tell us that semantics is all important.

And from a biological point of view, the most natural response is, of course, we knew that all the time.

That's why we're doing biology.

So all of this ties together.


SPEAKER_01:
Would you say that FEP says semantics is important, and then what does it say about what semantics are important?

Or is that entirely left up to the modeler?


SPEAKER_00:
Well, it tells you a lot about what's important for the system itself, right?

The system itself is dealing with its environment.

And it's trying to increase its predictive power.

So it's building a theory of the environment.

And that theory is intrinsically semantic, right?

It's about the environment.

And it's actionable.

The theory is meant to drive action on the environment that will either yield more information,

or get the environment to behave, to do what I'm predicting it's going to do.

So I think the FEP is intrinsically semantic in that sense, that it's about an agent modeling its environment.


SPEAKER_01:
yes and about the semantics of action as opposed to a partial or limited semantics of passive inferrers or semantics of abstraction

Which kind of leaves the important link between the agent and the environment through sense-making and decision selection basically unaddressed because it's kind of Descartes in the thought experiment in the room, and here when we actually engage with the boundary of the room, that's where we start.


SPEAKER_00:
Yeah.

Yeah, the FEP completely destroys this classical notion of the passive observer.

as does quantum theory.

There are no passive observers in quantum theory.

So this is a construct that we inherited from a classical worldview that is seriously misleading.


SPEAKER_01:
A lot more to say, but that's a great place to end.

Thank you, Chris.


SPEAKER_00:
Okay.

Thank you, Daniel.

And thanks to you all.

And ask questions.


SPEAKER_01:
See you next time.


SPEAKER_00:
Cheers.

We'll see you next time.
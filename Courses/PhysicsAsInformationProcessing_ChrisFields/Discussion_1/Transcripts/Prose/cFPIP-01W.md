---
title:  'Physics as Information Processing - Discussion 1, "Historical Perspective"'
author:
- 'Ander Aguirre (Ohio State University) [![Orcid](images/orcid.png)](https://orcid.org/0000-0002-6337-8292)'
- 'Daniel Friedman (Active Inference Institute; University of California, Davis) [![Orcid](images/orcid.png)](https://orcid.org/0000-0001-6232-9096)'
- 'Ross Berger'
- 'Avel Guénin-Carlut (Active Inference Institute, Verses Lab, University of Sussex, Kairos Research) [![Orcid](images/orcid.png)](https://orcid.org/0000-0001-8239-7264)'
- 'V. Bleu Knight (Active Inference Institute) [![Orcid](images/orcid.png)](https://orcid.org/0000-0002-9894-1989)'
- 'Alexey Tolchinsky, Psy.D. (The George Washington University) [![Orcid](images/orcid.png)](https://orcid.org/0009-0009-4721-515X)' 
- 'Francesco Balzan'
- 'Corby Prior'
date: "2023-06-03 Version 1.0"
...

## Discussion 1, "Historical Perspective"

_Daniel:_
 Hello and welcome, everyone.
 It's June 3, 2023.
 We're in the first discussion section for Physics as Information Processing course with Chris Fields.
 So welcome to all who are here on and off camera.

 Really exciting to see so many people joining for this participatory discussion section.
 So in this first interval, we're going to go to anyone who raises their hand, and it would be awesome to hear from everyone who would like to share what excites them about what they heard in the first lecture.
 How has their background and journey led to them joining?
 What are they hoping to learn and do in this course?
 So firsT hand we'll get to go, maybe.

 Ander, you can actually start first, just speaking as a normal participant.
 And then meanwhile, anyone is welcome to raise their hands.

_Ander:_
 That sounds good.
 So I am a mathematician by training, but I've always been interested in the more physics side of things.
 And in particular, I followed Chris's work pretty closely since probably 2019.

 I'm just very interested in the whole ontology of quantum measurement, what it means.
 I feel like very often when it's taught in physics classes, it's hand waved away.
 And even though I myself subscribe to the philosophy of shut up and compute, sometimes not everything you need to do a little bit of philosophy, maybe, and this may be one of the things where you need to do it and you need to address it from first principles.
 Right?
 And I think Chris is doing that.

 So I'm just here to learn more.
 And yeah, that's basically it.

_Daniel:_
 Awesome.
 I'll just go from the top left because I'm not necessarily watching, like, what order, but Ross first.

_Ross:_
 Hi.
 I'm interested in the kind of overlap between cognitive science and physics and seeing if there are any panpsychist implications of the observation that physical processes seem to follow the same kind of principle that we see applied to brains and cognitive systems.

_Daniel:_
 Awesome.
 Thank you. Avel?

_Avel:_
 Hi.
 I'm Avel Guenin-Carlut.
 I have mixed backgrounds, mostly physics and cognitive science.
 Right now, I could be qualified as a philosopher of physics and cognitive science.
 And I'm interested in basically formalizing participatorialism, the idea that observer participates in creating reality.

 I did some work in formalizing that in the case of social norms and things like language.
 And I met a block, based on my lack of knowledge of quantum physics and the mathematical tools that I used for that.
 So I'm here in a course to get those formal skills.

_Daniel:_
 Awesome.
 Thank you,
 Avel. Bleu?

_Bleu:_
 I think Alexey was first.

_Daniel:_
 Okay, Alexey, go for it.

_Bleu:_
 Sorry.

_Alexey:_
 Hi, I'm Alexey.
 I'm a clinical psychologist in Maryland.

 In the States.
 I work with patients, do psychotherapy mostly.
 My interest came through Mark Solms, who is the creator and leader of neuropsychoanalysis, and he has been collaborating with Friston on “the hard problem of consciousness,” and he's now working on AI with emotions.

 I blame him.
 I say it with respect and love that for introducing mathematics into our field, which was mostly devoid of mathematics.
 And people started looking at these partial differential equations and Markov blankets.
 So I wrote a paper about a possible extension of their model to include chaos theory and I just want to understand deeper concepts involved.
 I have some understanding of them, but perhaps not enough.

 And I'm enjoying this course a great deal.
 I think Chris, among other things, is a fantastic teacher and he is able to express things very clearly and I have awe – I have goosebumps.

 So I'm very grateful to everybody and thank you so much.

_Daniel:_
 Cool.
 Bleu. Then, Francesco.

_Bleu:_
 Hi, everyone.
 I'm Bleu Knight.
 I'm with the Active Inference Institute.
 So I've been tracking this work of Chris's connecting quantum information and Active Inference for, I don't know, a couple of years now.
 Through that thread, I'm super interested actually in the aspect of spacetime arising from communication.

 I think that that's like fascinating to dive into and how it connects to the Buddhist and maybe more Eastern religion concept of dependent arising, like how the interdependence of all things leads to their existence from that aspect.
 I think that's what I'm most interested in exploring. My background is neuroscience, but I've also been studying practicing Buddhism for like 25 years.
 So yeah, that's it.
 Thanks.

_Daniel:_
 Awesome.
 Francesco. Then, Corby.

_Francesco:_
 Thanks,
 Daniel. Ciao, everyone!
 Super happy to be here.

 I also have a mixed background because I studied cognitive anthropology and philosophy of science.
 And currently a few months ago I started a PhD in artificial intelligence in an 8th application in the educational field.
 And yes, I'm actually super excited about all the science-based implications that Chris Fields mentioned in the first lecture.
 The connection...I have no background in physics, but the connections between the first principles, physics and free energy principle and scientific cognition in general is what is really getting me excited these days.

_Daniel:_
 Thank you.

 Cool.
 Corby. Then,anyone else who raises their hand.

_Corby:_
 Yeah, great to be here.
 My background is in quantum engineering.
 I've always been motivated by figuring out what the next big thing in computing was.
 In high school, Moore’s Law was sort of coming to an end and it seemed like a really big opportunity to figure out what the next big computing architecture might be.
 So I did academic research in neuromorphic computing, quantum computing and quantum sensors.

 And I felt like I was a little bit too early for all of those things.
 So I went into cloud and cloud gaming and streaming AI services.
 But I'm still extremely motivated to figure out what the next big thing is. And the Holographic principle seems like a good foundation to explore that.

_Daniel:_
 Awesome.
 I'll raise my own hand then.
 Anyone else is welcome to go.
 So, I'm Daniel.
 I'm a researcher at the institute and have known Ander for some years and learned about all kinds of stochastic matrices and how the math and the quantum were linked.

 And we had talked and joked about learning from Chris Fields for a long time.
 So when the opportunity arose to enact it and extend it and open it, we were very excited and also don't have much of a formal background in physics outside of working with Yaaka Bin Ali on Live Stream 49.

 So it's just going to be a great time to learn and see where it all goes.

 Anyone else want to say hello in this kind of opening section?

 Okay, well, continuing forward, of course, please just feel free to raise your hand anytime you'd like to jump into the stack. Ander, for several minutes or however you see, could you reflect on Lecture One?
 Where did Chris enter?
 How did he proceed?
 And where did the lecture take us?

 Thank you.

_Ander:_
 Yeah.

_Daniel:_
 {Can you zoom in a little bit more on the PDF?}

_Ander:_
 {Can you hear me?}

_Daniel:_
 {Thank you.}
 {Go for it.}

_Ander:_
 Let's take a look at the slides from last time.
 So I think if we were to summarize this whole lecture, basically what Chris was trying to say is that there are many hands coming from different directions, pointing at the same thing, right?

 Pointing at the fact that information will have a central aspect in physics in the future, but also possibly biology, right?
 So the earliest hints came from statistical mechanics, if you wish, but also quantum mechanics, right?
 And here we need to make an important but subtle distinction between what it is, the uncertainty principle, right?
 Because we think of that as the informational part, right?
 Like a limit on the resolution on information and what is the observer effect.

 Just to make that distinction clear.
 I think Chris is talking about the observer effect.
 The uncertainty principle is an artifact of the wave nature of quantum mechanics, right?
 You can get it from properties of the Fourier transform, cautious words, inequality.
 But at the end of the day, it's what I was saying earlier, right?

 What is the basic picture of measurement, right?
 And that's sort of what Chris with Glazebrook and Marciano is trying to address.
 And the first two hints came from quantum mechanics around 1900 and statistical mechanics a few decades earlier.
 Then there's more stuff, right?
 Like, obviously physicists resisted to inertia of hundreds of years of a totally objective, observer-independent universe, but the hints continued to pile on, some stuff coming from math.

 But perhaps the most interesting is, again, how all those paradoxes from statistical mechanics were made sense of with [Rolf] Landauer's proposal.
 And then as physics got more fancy past QFT and stuff, people started thinking about quantum information more seriously.
 So we had Bell's theorem and all these proposals by Deutsche and Wheeler and so on.
 And I should say that this is not a fringe idea in physics.
 The paradigm of "it from qubit" is very much mainstream.

 I mean, Google it.
 A lot of physicists think about it.
 I don't know how many of them talk about it, openly, but it's been a widespread folk intuition, so to speak, that information will have this sort of central role.
 And the last sort of hints pointing in that direction of information having a central role came on the one side from high energy stuff.

 And that's what the Holographic principle is.
 It originally arose in the context of black hole thermodynamics, but then it was extended to what's called the Ads CFT [anti–de Sitter/conformal field theory] correspondence.
 And the very most recent stuff that was basically Chris' work, on the one side from high energy stuff.
 Sorry, I can speak a little more loudly.

_Daniel:_
 It's all good.

 Continue.

_Ander:_
 Yeah, I thought there was a question.

_Daniel:_
 Someone was just unmuted.
 It's all good.

_Ander:_
 Okay.
 Yeah.
 So that's basically it, right?
 There's hints from many directions, and most recently the work of Friston and Levin coming from biology, all pointing in the same direction, right.
 That we might have to make sense of some sort of eventually scale-free information processing [per ?] for what it means to be an observer.

 All of these things are pointing in that direction.
 And I think Chris's work is addressing that head on.
 Right.
 As I said earlier, sometimes it's good to do computations, but sometimes you need to take a step back and think more philosophically about what you're doing and what the bigger picture is.

 And then maybe after people talk a little bit, we can warm up to the second lecture.
 The other windows that I have open here, some of them are the papers that are to come.

 Yeah.
 Anyone?

_Daniel:_
 Please feel free to raise your hand on this.
 Alexey and then anyone else?

_Alexey:_
 I had one thought about this thing when you mentioned that there's various perspectives on the same concept.
 And I'm trying to be cautious.
 On the one hand, I think, again, this amazing clarity and precision with which Chris expressed thoughts is fantastic and enjoyable.
 On the other hand, it requires simplification and generalization.
 And what I observe in my field is people take a term like entropy and run with it.

 And then they get very quickly from thermodynamic entropy to Shannon to whatever entropy.
 And they talk about sort of and that's not the case.
 I mean, if we go back to Clausius and all the others, there was a system of postulates and axioms that must be maintained.
 And if you step outside of that so, like us organisms, we're open systems.
 We're not closed systems.

 So we should not apply directly the second law thermodynamics and I deal a little more with Kolmogorov-Sinai entropy, which is not the same thing.
 And I guess when we very quickly move from sort of thermodynamics to statistical mechanics with Boltzman to all the way around and we assume it is the same thing, I think we may need to be careful because we are just ignoring the differences in assumptions.

 Shannon talked about telegraph.
 He talked about a receiver getting a message from a sender, right?

 And Clausius talked about gas.
 But I do think it's a balancing act and there's no way to not have simplification and generalization.
 I just wanted to say somewhere, somehow we need to kind of go back there and just say entropy is not entropy is not entropy.
 And all these things are kind of long papers, right?

_Ander:_
 Yes.

_Daniel:_
 Yakub, then Ali, then anyone else?

_Ander:_
 I just wanted to ask for, I guess perhaps slightly longer comment from Alexey on what you see as the fundamental differences in different descriptions of entropy and whether there is some general concept that could be extracted from all of them, such as level of information or uncertainty about a particular system even though these specific formulations of entropy, they talk about, they were derived from different specific systems and they have different equations describing them and they mean slightly different things, do you think it would be wrong to say that there is some commonality between them?

_Alexey:_
 Should I answer Daniel or yeah, please.
 I don't know, I just think we need to at some point formalize it and be careful with these generalizations.
 For example, when we talk about Active Inference and Friston right.
 When we say living organisms must minimize I'm quoting Friston, by the way, these are not my thoughts.
 The entropy of sensory states, right?

 And then people take that idea sometimes and generalize and they say that that's what life is, minimization of entropy.
 Right?
 But if you actually look into human brain and measure entropy, then it goes all over the place, up, down and in between.
 And so I don't think that's an accurate description.
 And the reason for that is where we shifted away from the assumptions.

 And again, so Kolmogorov-Sinai entropy talks about the phase space and the divergence of trajectories in phase space and sort of the volume and phase space, while when you talk about this Friston's entropy of sensory states, these are probability distributions.

 And Shannon talked about the uncertainty of the receiver, about the message sent by the sender.
 So if you want to extract the same thing, I suppose you can.
 But it's important not to lose at least the difference between what kind of object are we applying to?
 Are we looking at probability distributions?

 Are we looking at phase space?
 Are we looking at ideal gas?
 Where are we at?
 And we're just not to say entropy is entropy is entropy.
 That's not the case.

 It's just a formula, but depending on the context, assumptions and things, it can be very different.

_Daniel:_
 Thanks, Ali.
 And Avel.

_Speaker I:_
 Yes.
 In regard to Alex's comments, I just wanted to mention the research of people like Rudolph Hanel and Stefan Surger [spelling?], because for many years they've been exploring this idea of different conceptions of entropy and they've come up with at least a dozen different meanings for entropy.
 And they've formulated it in a kind of generalized framework, but at the same time, they've also devised a way to somehow classify them into equivalence classes and to see whether one or more of those conceptions of entropy can be theoretically derived from some others or they're totally distinct and different conceptions of entropy.
 So, yes, I agree that in some situations, we need to be more specific about what exactly we mean by entropy and information.

 I mean, based on the question or the situation of interest.

_Daniel:_
 Thanks, Avel.

_Avel:_
 So, just to vote in the entropy discussion, I want to emphasize that entropy is PLNP [? write the concept's name].
 So it's a function of probability distribution, which is I think I'm not sure what the term is in English, but it's a measure of how big something is in a space.
 And to get that, you have to define a space.
 And in many cases, it's pretty arbitrary to decide what the space you're looking at is.
 For example, you talk I don't know if it was Alexey, someone talked about entropy of fMRI.

 Well, those are aggregate measurements of general activity.
 This is not the space of all possible configuration.
 And even if I add something like the space of all, let's say, functional configuration, the state in neuron that reflect activity, this would be another thing than the space of atomic configuration, where the specific atoms are.

 You don't have a warranty that when you define entropy, you define it on a meaningful space that's not given.

 I kind of think there is one entropy and that's it, and it's an information construct.
 But also we have to if we agree to that, then it entails that we have to find it.
 It's not just given out there in the data.

_Daniel:_
 Well, certainly being precise in the formalism and in natural language for what we mean by information and entropy and all these terms is pretty important.
 I mean, the course we're in is physics as information processing.
 But without an understanding of how information is being meant here or what measurements and transformations we're performing, what informational operations, then it is a non sequitur.
 So how do we go about clarifying or understanding what kinds of theoretical results in a given domain or, like, what is shown on the timeline here might apply to some other area?

 Just one thought.

 Anyone else can raise their hand with a thought or a question.
 Otherwise Ander, you can maybe flip over to select a few of the submitted questions.

 Cool.
 Could you zoom in with control plus a fair amount?

 Thanks.
 So, everyone, we really appreciate the submitted questions.
 We've been able to get 13 answered by Chris so far.
 And as with the transcripts of the discussions and lectures, the questions and answers are going to be part of what is published by the Active Inference Journal.
 So we really encourage basic questions, advanced questions, just there's no question that doesn't make sense to submit.

 So, Ander, maybe just pick one question that you think is interesting to begin with and summarize Chris's response, and then everyone else is welcome to kind of add some other thoughts on this.

_Ander:_
 So, I'm going to scroll up and down just to sort of sample.
 Let me start with the first two.
 Can you hear me well?

 Yes, since perhaps I think you submitted this as a trial, right?
 Is that right?
 Yeah.
 So what is information?
 What is time?

 Here is Chris's response.
 I guess everyone has a chance to look at it.
 Now, if I am to summarize this, the one thing I've heard that personally, the one I like the most, answer, at least as an adash, to summarize it, is that information are differences that make a difference.
 This I might have heard from Chris, I might have heard from someone else.
 I don't remember.

 But that's one thing.
 Now, onto the question of what is time, and I guess this also addresses Alexey's question of what is entropy.

_Ander:_
 I'll make a bigger point now.
 What is time?
 This is partially addressed, and I guess the main thing I'll be doing here is rather than look back, look forward into what's to come in the course, in particular, I'm thinking about these two papers [The Physical Meaning of the Holographic Principle-Fields, Glazebrook and Marciano, and A Free Energy Principle for Generic Quantum Systems--Fields, Friston, Glazebrook and Levin] and in particular, and I finally chat with Chris about this this picture over here [The Physical Meaning of the Holographic Principle, Figure 5. Cartoon illustration of QRFs required to observe and write a readable memory of an environmental state |E)].
 Okay.

_Daniel:_
 More?

_Ander:_
 Yeah.
 I'll get back to it in a bit.
 But basically, this sort of, like, kills two birds with one stone, right?
 This picture, and especially the text around it, and the title of this paper is The Physical Meaning of the Holographic Principle.

 Okay.
 You guys can see here archive 22, ten, blah, blah, blah.
 Anyway, this addresses the notion of time and entropy, because the claim that's made in this paper is that these are local observer relative definitions, and we can look into more detail around it.
 But basically, the coordinate time, t subscript A, is an entropic measure, so it is non-decreasing, and it goes up with the observer measuring entropy in the system.
 Because of the symmetry of the definition the second law applies to both.
 So both observer and environment see entropy increasing in their environment and vice versa.

 Anyways, from this picture, one gets that you have this internal circle time QRF.
 So this just is a necessary artifact of recording information into a memory, that's the claim made in the paper, and that essentially induces the passage of time, at least from the perspective of the observer.
 It also, I think, Alexey was talking about, which I totally understand and agree that there's different notions of entropy.

 Now, I suppose I would be the one to like definitions and so on, but I do think it's good to have a certain amount of flexibility.
 So I see it the same as energy, right?
 Like, there are different kinds of energy.
 You know, there's energy-mass equivalences.
 And, you know, if you push it into supersymmetry, you can think of it as equivalent to force also.

 But I do think there is some value in being sloppy or intuitive about the notion.
 So the fact that entropy yes, there are different definitions.
 I don't see it quite as a problem one can explore here.
 Around this page, there is a definition of local entropic time.

 One can think of it as phase space, or one can think of it as coding theory, a la Shannon.
 But it is at the end of the day, I do think it is true that both of them are hinting at the same direction.
 Just like you may have different kinds of energy, but they're not unrelated concepts.
 Yeah.
 I hope one day we get very precise definitions out of everything, but for now, I think it's good to speculate.

 Okay.

_Daniel:_
 Corby and then anyone else on this?

_Corby:_
 Yeah, quick question.
 So I haven't read that specific paper, but is there any work on the frequency or the rate at which one could read and write information on the boundary?

_Ander:_
 Yes, well, I can't think of the paper exactly, but this is lower bounded.

 I think Chris talked about it, if not in the last lecture.
 I think I've seen a talk of him on YouTube.
 It's lower bounded by the uncertainty principle.
 Right.

 For instance, Planck's constant is in units of action, which is energy times time.
 Right.
 So [Rolf] Landauer's principle says you need a certain amount of energy to erase information and you have time.
 Right.
 And then you have an energy-time uncertainty principle. Putting those things together, you have a minimum amount of action, which is literally the minimum.

 The action is measuring what I said earlier.
 Right.
 Like differences that make a difference, the minimum difference that you can make.
 So the minimum difference to erase a bit or rewrite it on the boundary, so to speak.

 Right.

 So, yeah, that would be given by the uncertain relation, energy times time.
 I don't know exactly what the figure is, but it's obviously very small compared to everyday life.

_Corby:_
 Got it.
 And is there work on the upper bound?
 Like, for example, would it be the number of degrees of freedom on the boundary?

_Ander:_
 No, I don't think it's bounded from above.
 But obviously then we do want to bond it from above for technological reasons, if you wish.
 But I think that is an engineering problem, not one of physics.

_Daniel:_
 Ander, a few questions on this figure.
 What are Y and E?
 B, the blue oval, as Chris has pointed out, is going to consistently be the screen, the blanket.
 And so we often associate that with at least some blankets, they include these information gathering and utilizing systems.

 So what is happening internal to the agent here?
 What kind of computation is happening?
 And is this something that is actually proposed to be occurring?
 Or is this a schematic or a map of how we could think about something occurring?

_Ander:_
 I think this question is going to be better addressed by Chris himself, probably in the next lecture.
 For now, what I can say is I can reiterate what I was trying to say earlier, which is that what this addresses and this basic measurement picture, so to speak, is the observer effect, right?
 Like this notion that you cannot observe something without disturbing the environment.
 So if you then try to make sense of this by constraining from first principles, [Rolf] Landauer's principle and so on, basic requirements on information processing from the point of view of energy, then you get that this boundary is constrained in some ways, right?
 This Y is what's called the memory sector, meaning that you don't just act on the immediate information that you perceive.

 You may also act on stuff that's stored that you have perceived before. Then sure, E, the red part is what you're perceiving right now, the environment.

 But actually I don't know if I have it in here, but this is further broken up into other parts.
 So not schematically here, let me try to find it over here.

 Now, I don't know, actually, bear with me for a second.
 I think you might be here.

 That's the same picture anyways.
 I don't want to waste time by trying to find it, but back to the original picture sure.
 What you're measuring, and again, Chris will talk about this in more detail, but from first principle assumptions, they deduce that you're going to have to deploy your finite resources for gathering information in clever ways on the boundary, right?

 So you're going to have to deploy some resources for memory, some resources as a heat sink, right?
 So you're going to be processing information, you're going to be dumping so called low quality stuff into the environment, you're going to be dumping heat and then other than the memory, you're going to be focusing on what's going on right now in this environment.

 And what I wanted to say is that it can be further broken up into what's called pointer and reference sectors.
 So you can imagine this E sector here has to be broken into further sectors because then there is the other first principle notion.
 Okay?
 So what does it mean to observe something?
 The differences that make a difference, right?

 So there is only information with context, right?
 There is no information per se.
 It's all semantics, right?
 So you only observe things in context.
 So some things are going to have to stay the same, namely reference states here, and some things are going to bear, right?

 So you can think of this as the gauge in your car, right?
 Obviously the circle stays the same and that's your context, so to speak, with the numbers in it.
 They never go away, they stay the same.
 But as you speed up or down, the gauge is going to change.
 And that's how you know only in that context you can tell your own speed when you're driving a car, right?

 So anyways, this is just a teaser for things to come.
 We can go back to more questions.
 Daniel, what do you think?
 I'll pick maybe two or three more over here.
 Yeah.

_Daniel:_
 Pick another question, or anyone raising their hand, or if anyone in the live chat wants to write something, I'll read it.
 How about the Wick rotation?
 Could you summarize what the Wick rotation is and then state this question about it?

_Ander:_
 That's a great question.
 I wish I knew the answer better.

 But formally speaking, the Wick rotation is just what Chris said, you take a quantity, multiply by i [an imaginary time variable], the square root of -1, that amounts to a 90 degree rotation in a complex plane.
 Fine.
 Now, that appears to be a trick.
 And what it does effectively is take you from quantum mechanics to statistical mechanics, roughly speaking.

 Now, why it works, I have no clue.
 I don't know the answer myself.
 But now when things appear to be a trick and they get you something, maybe there's more to it than what's apparent at first, but I myself don't know the answer.
 It's a very good question, but I do think it may have to do with the fact that on the one hand, quantum and statistical mechanics both are talking about constraints on information processing, maybe from different perspectives.

 But again, that's just speculation on my end, so it's maybe a way of relating the two.

_Daniel:_
 Could we just review, just read Chris's full answer to this.
 Read the question, and then Chris's full answer.

_Ander:_
 Does the Wick rotation tell us that matter-energy somehow exists 90 degrees perpendicular to a three-dimensional space?

_Daniel:_
 And then I'll just read Chris's answer.
 Then anyone?
 Please raise your hand if you want to add something more on this.
 He wrote, in a sense, yes, time is perpendicular to our three dimensions of space, matter, and energy.
 We ourselves exist in time.

 Wick is pointing to the intimate relationship between how we measure time and how we determine or judge that something maintains its identity through time, i.e., stays the same thing through time.
 To tell time by a clock, I have to be sure I'm looking at the same clock, set the same way, etcetera.
 We'll get to this in July and August.

 Could you connect this, Ander, to some of the semantic information flows discussed in Live stream 17 or any of the other Cone-Cocone diagrams from the papers that you showed earlier?

_Ander:_
 Sure.
 Again, I have to preface it by saying that I am working on understanding it myself, but just from small email exchange that I had with Chris and what's said in this paper, one can sort of see that it's related to the picture I was showing earlier, namely this one.
 Right.
 But let's scroll down a little bit to page to page 23, 24.

 Here they talk about the Wick rotation again.

 So I think it has it has to do with, at least from what I heard from Chris' email, it has to do with this notion of local time, the definition of local time in this observation picture, and the fact that you rotate 90 egrees twice, you get the opposite sign, basically, right?
 So the local time, effectively, that's going to ensure that the picture is symmetric and that the flow is going to be opposite in the direction towards the observer and towards the environment.

_Daniel:_
 Okay, Alexey, then.
 Anyone else raising their hands?

_Alexey:_
 I am struggling to understand and part of my difficulty bear with me for being so kind, but I remember Sean Carroll's lecture on time in different versions of physics, and it took a whole hour.
 So are we talking Newtonian time, which is universally the same?
 Are we talking relative time, philosophically?

 Are we talking about people who believe that only present here and now is real, while future and past do not exist?
 So when we say time is perpendicular too, my sense is that just like Chris mentioned in the lecture, we're reinventing the time.
 We're creating a new definition by calling it the same word we're all familiar with.
 And that creates some dissonance where I just think that maybe we need to kind of say that this is something new.
 And if it's not something new, then which version of the time are we talking about?

 Einsteinian, Newtonian or whatever?
 It's a simple question before we say it's perpendicular, do people agree that future is real or not?

_Daniel:_
 I'll give it all on this, Alexey.
 Again, speculation neither here nor then.
 If we take Chris seriously, that communication is an ontological primary, and that rather than putting communications protocols in some sort of time Newtonian time, Einsteinian time, these are these different time concepts, these different scaffolds that then communication gets inscribed within or stretched within.
 But if we start with a topological understanding of communication rather than a geometric understanding of spacetime--so the topology of the blanket and the observer in their sensorium, then different kinds of cognitive agents, ranging from what we would call nonliving to what we could call living, they, depending on their cognitive sophistication are able to create or project different time concepts.

 Time concepts that are not useful will lead to their own material dissolution, the failure to maintain that kind of cognitive sophistication, which requires not just information gathering and utilization, but also energy gathering and utilization.
 And so when you have multiple agents, who are, whether you conceptualize them as within a message passing relationship and having no surprise on the clock on the wall, then they're like in a local time synchrony.
 But the primary case is one entity engaging with its sensorium and itself modeling time in terms of progression.
 And so these previous time concepts and these different attempts to kind of survey the structure of spacetime reflect different cognitive models that cognitive agents have proposed for time and that communication actually takes ontological primacy over all of these ramified, potentially overlapping, potentially non overlapping time concepts.

 But they're downstream of agents cognitive models, not vice versa.
 I'm not sure. It's how I interpreted what Chris said about communication being an ontological primary, though.

_Alexey:_
 So if we take one instance of a COVID-19 virus, a single kind of instance, right, it's got no memory and no ability to plan.
 Is there a local time for that virus versus humanity observing how the virus spreads across the globe?
 And we have agents with memory and planning.
 Is there time relative to a single photon?

 My hunch is that there's a possibility that we're simply inventing something brand new and defining it brand new and they draw in parallel back to all these other times.
 I understand the effort, but is that a brand new time?
 Is it some other kind of time?
 I don't know.

_Ander:_
 Daniel.

 Can I say words?
 So I understand Alexey's question, and I think it's sort of related to, at least morally speaking, to your concern about different kinds of entropy, right?
 And what I can say is that also they have Glazebrook, Marciano and so on they have this recent paper on quantum error correction [Communication Protocols and Quantum Error-Correcting Codes from the Perspective of Topological Quantum Field Theory], so I have yet to read it and take a closer look.
 But here they talk about spacetime, right?

 And my hunch, to address Alexey's question is that if you could sort of recover the more familiar notions of time in a correspondence principle sort of way by taking some Newtonian limit or H bar goes to zero or speed of light goes to infinity and so on, that would be very satisfactory.
 Sure, they may be doing a new thing, but recovering old stuff in the new thing is a good sanity check.

 And so long as we can see that being worked out, I think I'd be very happy.

_Daniel:_
 And just to the virus point, I would argue that viruses, through their physical engagement, they do have memory, but they don't necessarily have the awareness or metacognition on that memory, except as granted by intergenerational evolutionary type pressure.
 But they do have past events modifying them within some set of possibilities.
 So they're just being them in their internal time.
 There isn't a cognitive observer necessarily.

_Ander:_
 In terms of like a narrative "I", "I am watching the clock", but they do change.

 But it's certainly very provocative if this is not.
 Or we could also look to Kronos and Cairos and about the relationship of time and timeliness and agentic time.

 And then at this point, are we stretching just this one four letter word too far, just like we brought up with information entropy?
 Can we think about these as kind of like continents and then just if someone said, I visited this continent, you know that you need to specify a lot more, or what granularity are these topics? Corby and then anyone else raising their hands?

_Corby:_
 Yeah, on this topic, for a Markov blanket, you have a transition matrix.
 My question is, how are you allowed to increment that transition matrix to calculate a future state?

 What does that mean in terms of time?
 Are you only allowed to increment it by the lower bound time, which is h over Ln and kBT.
 Does that question make sense?

_Daniel:_
 I'll give a first pass on this.
 We have discrete time and continuous time models in Active Inference.

 For a discrete time model where you have the B transition matrix, then the choice of delta T is a modeler's decision.
 And we talked about this a lot in live stream 52 on accelerated optimization, there's some delta T that's like, going to get you the most accelerated optimization.
 If you have too small of delta T, then you're either oversampling or you're maybe even physically unrealistic like you pointed to.
 Or if you have some long delta T, you're also not making an informative simulation in the discrete time case where you're explicitly making time steps.
 Contrast with the continuous time generative model, where we're doing more of a Taylor series expansion on the generalized coordinates, at which point delta T is not required.

 You're merely just taking an extended approximation from a given snapshot, and so delta T doesn't even come into play.

_Corby:_
 Got it.
 Okay, that makes sense.
 I don't know how it ties into time, but it helps clarify a little bit.

_Daniel:_
 Okay.
 Anyone else can raise their hand or Ander kind of maybe one more question or one more topic from a paper?
 Just what's one more puzzle piece that you think gives us a first coat of paint from questions submitted or from one of these papers that you have up?
 What's something that helps us connect the dots a little bit?

_Ander:_
 I don't think I have a good answer.
 Let me look at the questions again. Regarding time,I'll just reiterate what I said as I look over these questions, which is that I believe Chris will address it.

 I don't know if you can see what I'm sharing on the screen, but the paper, whose title is Communication Protocols and Quantum Error-Correcting Codes from the Perspective of Topological Quantum Field Theory from March of this year does talk about spacetime.
 It's a quantum error correction code, and to me personally, it'd be nice to see if that can be more familiar notions of time, such as special relativity, can be recovered as limits in correspondence principles.

 Do you want me to say anything else?

_Daniel:_
 If anyone could even just summarize what are these codes referring to?
 Or how are these codes one and the same as communication protocols?
 Or are they related to the blanket and the heat that was being output as waste?

 What will these codes allow us to do better if we understood them more?

_Ander:_
 I don't know that I can answer that right now.
 I don't think I understand the question.
 Yeah, I just don't know.
 Sorry.

_Daniel:_
 Terry.
 And then anyone else raising their hands.

_Ross:_
 I'm not a physicist.
 I don't understand maths at all.

 But what attracted me to this idea of physics as information processing is that it feels like at a base level, whatever the unknowable external state is, it could be perceived as made up of information.
 You know, we kind of think of ourselves living in a particulate universe, but at a base level there are these, and stop me if I'm just talking 14 year old nonsense here, but this notion of quantum fields, they feel like informational fields.
 And that the concept of physics is purely information that we then interpret as a physical universe which we inhabit because we can't see beyond our Markov blanket.

 I find this a really exciting idea. Whenever you start to think of the surprisal as the variational free energy between sort of your prior and your posterior, if you start to think of that variational free energy, not as energy but as an information gap.
 So that if you see the energy as actual... because energy, when you think of energy in physics terms, it's kind of this magical thing that allows work to happen.
 But what is it?

 We've got a term for it.

 Is it jewels or whatever?
 But what does it mean?
 It has this magical property.
 But if you think of it as information, then it starts to make sense.
 So variational free energy starts to be seen as a difference in information between your prior and your posterior.

 So whenever you start to think about time, time is this conceptual metric that we use almost because we have memory.
 And I wonder if we are getting trapped in this semantics.
 And all I have is English because I don't understand the maths really.
 But you guys come in here and you do understand the maths and then you convert it into English, which I think I understand.
 And then I can think of it like a kid in a sweetie shop or a physics class at 14 years of age.

 I don't really know what I'm saying.
 I don't know what I'm saying.
 But at a fundamental level, everything that we're talking about, I can understand it as information, and I'm excited to see how things develop.
 What are your thoughts on this craziness?

_Daniel:_
 So many great pieces in there.
 Physics as information processing.
 What is it that actually allows the steam engine to run and the ball to move?
 That kind of physics.
 And then as the timeline laid out, also occurring during a period of immense technology and apparatus development and digitization and all of this and unconventional computing as well.

 It's almost like this materialist dualism that instead of with mind and body that there was some kind of dualism between the types of forces and energies and ways of thinking about what made the steam engine work and then what makes a computer work.
 And so you'd understand how the computer could fall off a cliff, but then you wouldn't necessarily be able to use that exact kind of mechanics and physics and dynamics to study like how the computer operated.

 But you could say it weighed eleven pounds.
 And so I think the provocation, which is as you hinted towards first off, would be awesome to have a natural language math and graphical synthesis, the triple play.
 But also then to use a common approach to understanding this and what wasn't working or is not proposed to work, is trying to fit information into that non-cognitive universe.

 But rather if we take the cognitive primacy of communication, then all of these forces as inferred by the sensorium, which is all we're ever going to have, totally the most valid first principles constraint to make, maybe then we'll be able to have a lot more synthesis about even how steam engines work when we understand that we're bound in that inference by our sensorium and all that comes along with that and the variational free energy bounding surprise on our generative model.

 Dean.

_Dean:_
 I'm going to come at this from the position of a 14-year old lens because I don't think there's anything unique about that.
 I think on the most basic level, what I took away from Chris's lecture was that there's information processing of at least two types.
 One is figuring it out, as in Figure 5 or any of these other figures that are up on the screen right now, or anything that stabilizes the world around us.
 I think examples have come up in this conversation already of topology or geometry.
 There's a second type of information processing which is outing the figure type of information which is searching.

 And I think Alexey used the expression which time are we talking about?
 So these two types of processes are quite different.
 If you do a comparative analysis of the two, you have information practicing and you have to have both of the processes to really get a sense conceptually of what it is that communication is really talking about.

 If you are information practicing, you are way-finding until you collapse to something we might describe in a verb tense, as "way found". It's in the quantizing of that invariant state, that vast ocean, if you want to think of it.
 Metaphorically out to all horizons on the manifold into a variant or a nuanced or a discretized or an orientation type of state where all of a sudden things like currents and temperature shifts and even things that we've applied labels to, like entropy and a transition matrix suddenly appear out of the seeming nothingness or the invariance.

 So that's not the math, but that's kind of stepping back and saying what is the math trying to say?
 I think it boils down to the comparative analysis.
 And then I'm hoping that in the coming lectures, Chris starts opening that up and doing kind of the back and forth between the two types of figuring it out and outing the figures.

 Because I think when both are part of the conversation, that's when we start to get a sense of what's really going on.

1:_Daniel:_
 Thank you.
 In this sort of closing sessions, especially if anyone hasn't added anything, please feel free to give any last reflections or questions arising.
 So first, as many people as would like can reflect that way.
 And then after every hand has been addressed, then Ander if you could just give us a kind of look ahead.
 So Avel first, then anyone else with just where does this discussion take us and where do you want to continue to learn more about Avel?

1:_Avel:_
 So, two things.
 First, if I walk out of here without understanding what is Cone-Cocone, I will be disappointed.
 And I don't actually expect it to happen in like 12 hours.
 That seems like it's something.
 But more seriously, I can agree with that difficulty that there is a unity between physical structure and communication protocols and that communication protocols are quantum reference frame, kind of individuate physical states, individuate meaningful measurement outcomes that are ray-file as physical possibilities.

1: But the question is then where do communication protocols/quantum merchants frame come from?
 Like, what are the physical determinants that bring about those things?
 And that seems like a question that could be addressed at the conceptual level of this course.
 And I would like, I saw the live stream 17, so that did not help.

1: Anyway, I would like to have more grip into that if we can address it in the [?] courses.

1:_Daniel:_
 Thank you.

1: Anyone else?
 Especially if they haven't added anything yet, want to give kind of a closing thought?

1: Yeah, there's a lot to learn and Chris brings up a lot in his lecture, even though we've only had one.
 It's one of the denser hours of epistemic resourcing that I've seen.
 And there's also all of these accessory and cutting edge works that Ander is showing.
 So I'm really looking forward to how we can continue to curate and improve the questions that we have, so that the uncertainties in our generative model that we have, how would this be useful?

1: What does this mean?

1: Why is this variable this way?
 What is the implication for that?
 Those are all the great questions and let's just get them inscribed on our classical documents so that we can have other quantum cognitive agents reduce their uncertainty by looking at the responses.
 So with that, Ander, if you could just look ahead to lecture two and just point towards where we're going to be going.

1:_Ander:_
 Yes, lecture 2 will, I think, from what I hear from Chris, is going to start to address these basic ontological aspects of the quantum measurement.
 So the two sources to look at would be two papers, mainly this one, Physical Meaning of Holographic Principle.
 This one's, by far the most pedagogical one.
 So if I had to suggest that folks look at one, it would be this one.
 It's very easy to read, I think, but maybe for a bit more detail there's the slightly older one and they talk at length about Cone-Cocone background here in "Free Energy Principle for Generic Content Systems".

1: So I think that's what's going to come in these two lectures.
 Basically, the main ideas behind these two papers, and yeah, that's all I have to say.

1:_Daniel:_
 All right, well, thank you for all who joined this first discussion.
 Hope that it was fun for you and for the audience.
 And check out the course site for the readings to get ahead of the second lecture.
 We'll all watch the second lecture and come back in about a month for the second discussion.
 Everyone's welcome to join, and the website has the registration information.

1: And so now that we've seen a little bit of one way, it can be, we may do something similar or we may do something different for the second discussion.
 So thanks again, everybody.
 See you later.

1:_Ander:_
 Bye.

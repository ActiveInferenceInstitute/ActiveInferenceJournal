<h1 id="lecture-1-historical-perspective">Lecture 1, “Historical
Perspective”</h1>
<p><img src="../../Video/cFPIP-01L_00001.png"
alt="Chris Fields presents “Physics as Information Processing” at Active Inference Institute, 2023" /></p>
<h2 id="introduction">Introduction</h2>
<p><em>Daniel:</em> Hello and welcome, everyone, to the Active Inference
Institute. This is Session 1 of the course “Physics as Information
Processing” with Chris Fields. First we’ll have Ander Aguirre and Chris
Fields introduce themselves.</p>
<p>And then we’ll carry on with the first lecture, here. Check out the
video description for a link to the <a
href="https://coda.io/@active-inference-institute/fields-physics-2023">Course
Overview website</a>, where you can ask questions that will be answered
asynchronously; register to participate in the discussions, which happen
about two weeks after each of the six lecture sessions; and just learn
more about this area. So thank you both so much for joining into this
adventure we are starting now.</p>
<p>And first, please, Ander Aguirre, introduce yourself. And then Chris’
introduction and lecture. Thank you.</p>
<p><em>Ander:</em> Hello! So I’ll be the course assistant.</p>
<p>And I’m a postdoc in math, specializing in probability. And I have a
deep interest in the physics of information. And I’ve been familiar with
Chris’s papers for a while. So, yeah, just here to learn myself!</p>
<p><em>Chris:</em> Thank you, Ander. And thank you, Daniel. I’m Chris
Fields. And I’ll be presenting this course in six sessions, and Ander
will be organizing discussion sessions after each of those. And all of
this is explained on the course website. So let’s start!</p>
<h2 id="historical-perspective-physics">Historical Perspective:
Physics</h2>
<p>This is a course on “Physics as Information Processing,” and this
first session will be a historical perspective on the idea that physics
is, or is about, information processing.</p>
<p><img src="../../Video/Slide2.PNG"
alt="Wittgenstein, Landaur, Wheeler" /></p>
<p>And I’ll just start with a few quotations that span the middle of the
20th century - from <strong>[Ludwig] Wittgenstein</strong> in the 1920s
saying “The world is all that is the case,” so, defining the world in
terms of facts, not objects;</p>
<p><strong>[Rolf] Landauer</strong> in the early ’60s, proclaiming that
“Information is Physical;” and then <strong>John Archibald
Wheeler</strong>, who in many ways is the grandfather of this era
recently, stating it is “It from Bit,”</p>
<p>so things come from information, i.e. bitstrings. And if nothing
else, this shows that formulations of this idea get pithier as the 20th
century rolls on!</p>
<p><img src="../../Video/Slide3.PNG"
alt="Physics, Clausius to Friston" /></p>
<p>But the history goes back farther clearly than the 20th century. But
I’m only going to really talk about a piece of it and the timeline that
I’ll actually discuss today, the most relevant history of this idea,
goes back to the mid-19th Century.</p>
<h3 id="clausius">Clausius</h3>
<p>And the first specific thing I’ll talk about is <strong>[Rudolf]
Clausius</strong>’s definition of entropy.</p>
<p>But with the beginning of the understanding of thermodynamics and the
role of information in thermodynamics, you get this very interesting
multidisciplinary progression of ideas that incorporates the beginning
of quantum theory.</p>
<p>And the beginning of <strong>quantum theory</strong> can kind of be
dated to the [fifth]{first} <strong>Solvay conference</strong> in
[1927]{1928}, and the famous debate between <strong>[Nils] Bohr</strong>
and <strong>[Albert] Einstein</strong> over whether quantum theory is
about knowledge, information - or [about] objects, things. But it
incorporates a lot of work in computer science and logic and
mathematics. So, interestingly, computer science was born effectively in
the mid-30s with the work of <strong>[Alonzo] Church</strong> and
<strong>[Alan] Turing</strong>, which very rapidly converged with the
work in physics.</p>
<h3 id="physics-computation-information">Physics, Computation,
Information</h3>
<p>So today we’ll be talking about both computer science and physics.
And then in the second half of the 20th Century, this just exploded into
a huge area.</p>
<p>And in consequence of that mid-20th Century development, we’re
beginning to see a new idea about physics which is roughly encapsulated
in quantum information theory.</p>
<p><img src="../../Video/Slide4.PNG"
alt="Physics is about information transfer across boundaries" /></p>
<p>And the new idea is this. It’s that what physics is actually about is
information transfer across <strong>boundaries</strong> and the
information… We can represent the information transfer like this. And
this is a convention I’ll use.</p>
<p>A <strong>boundary</strong> is always a blue ellipse. And the agents
that are exchanging information across this boundary are conventionally
called Alice and Bob, which is just a more polite way of saying A and
B.</p>
<p>And when you think about this picture, it becomes clear that what
physics is really about is communication. And this is a wild
redescription of the idea of what physics is, compared to the ideas of
Newton or LaPlace or even the 19th century ideas. And it’s very
different from the idea that’s been preserved in 20th Century physics in
the lineage of Einstein and others who viewed <strong>classical
physics</strong> as, in a sense, either as completely fundamental or as
a fundamental adjunct to quantum theory.</p>
<p>So this way of thinking about physics is a very deeply
quantum-theoretic way of thinking about physics.</p>
<h2 id="course-outline">Course Outline</h2>
<p><img src="../../Video/Slide5.PNG"
alt="Where we are going in this course" /></p>
<p>And where we’re going in this course today is really “how did this
all happen?” It’s the origin story. And then in the next session, I want
to discuss quantum information theory explicitly. And in particular, how
quantum theory makes this conclusion that physics is about communication
very simple and obvious, much more obvious than it is in classical
physics where it takes <em>work</em> to formulate this idea. Then in the
next session, we’re going to talk about <strong>semantics</strong>, and
how observations become meaningful to the agents who make them and hence
how actions become meaningful to the agents who make them.</p>
<p>Then in August, we’ll talk about <strong>communication
theory</strong> a little bit more explicitly, and talk about how agents
employ multiple communication channels when they’re communicating.</p>
<p>And this is obvious when you think of people communicating: they not
only talk to each other, they look at the same things, they point to
things, et cetera. So this is what I mean by multiple communication
channels. Then in September, we’ll leverage that discussion to talk
about how <strong>spacetime</strong> actually emerges from
communication. And this is one of the most important aspects, I think,
of quantum information theory.</p>
<p>It provides us with a way of viewing spacetime as an emergent
phenomenon, that communication is what is <em>fundamental</em> in some
ontological sense; and the <em>box</em> in which it happens, spacetime,
is not. So in the final session in October we’ll talk about applications
to biology via the <strong>Free Energy Principle</strong>, and future
directions both in physics and biology and elsewhere. So it’s going to
be an interesting ride!</p>
<p>I’m keeping formalism to a minimum, because we’re directing this
toward the <em>broad</em> array of people who are interested in
<strong>Active Inference</strong> and who are involved with the Active
Inference Institute. And I would ask you to hold questions (because we
have a lot to get through in an hour) for the interactive discussion and
for the discussion forum.</p>
<p>So I hope I explain things well enough that all of the concepts will
be understandable. If not, <strong>Wikipedia</strong> is actually a
wonderful resource in this area for just definitions of terms. So if
there’s anything that… just a term that is a trip-up, try Wikipedia.
It’s probably a very good source for what these terms mean.</p>
<h2 id="back-to-the-physics">Back to the Physics</h2>
<p><img src="../../Video/Slide6.PNG" alt="Our story begins…" /></p>
<p>So let’s start! Our story, as I said, begins in the 19th Century.</p>
<h3 id="steam-engines">Steam Engines</h3>
<p>And in the mid-19th Century, lots of physicists were devoting their
efforts to figuring out how to make better steam engines. And one
question that arises when you’re trying to design a steam engine is
“what happens <em>physically</em> when you add heat to a system at
constant temperature?” So if you’re building a steam engine, you’ve got
a boiler, because you need to make steam. And as you turn up the heat to
your boiler, you get more steam, but the temperature doesn’t change. So
this is a mystery.</p>
<p>What is the heat actually <em>adding</em> to the boiler that is not
increasing the temperature? And Clausius responded to this question in a
way that’s sort of typical for a physicist or a mathematician. Since he
didn’t know what the answer was, he just <em>invented a new name</em>
for whatever it was, and gave it a formal definition.</p>
<p>So he called it <strong>entropy</strong>, which is a made-up word
that, if it was translated from the Greek, would roughly mean
“transformation content,” en-tropy. And he represented it by a simple
equation that the change in this new concept, entropy, which is always
called S, is just equal to the change in heat, Q, at constant
temperature, T.</p>
<p>So obviously, this equation just reformulates the question in
declarative form, saying “whatever this stuff is, its changes in this
stuff are just changes in heat at constant temperature.” Well, heat is
energy. And this wasn’t completely recognized in the mid-19th Century.
But the way you’ll see this equation in a current textbook is “dS is the
change in <em>energy</em> at constant temperature.”</p>
<p>So even more commonly, you would see it written as “the change in
energy is equal to the temperature times the change in entropy.” It’s
the most common sort of textbook way of seeing this.</p>
<p>But the question, of course, is, “What <em>is</em> this quantity?
What is this entropy? What does this concept <em>mean?</em>”</p>
<p><img src="../../Video/Slide7.PNG" alt="But what is entropy?" /></p>
<h3 id="boltzmann">Boltzmann</h3>
<p>And about 15 years after Clausius proposed it, <strong>[Ludwig]
Boltzmann</strong> had the key insight, which is that “entropy is a
measurement of <em>our uncertainty</em> about the <strong>state</strong>
the system is in.” And in particular, he again, of course, went to
formalism, and said “the entropy, S, is equal to some constant times the
number of states that the system can be in that look the same to
us.”</p>
<p>And since that number of states is enormous, the way to make that
manageable is to take the <em>log</em> of the number of states.</p>
<p>The <strong>natural log</strong> is <em>ln.</em> And this constant k
is called <strong>Boltzmann’s constant</strong>. And Boltzmann was able
to do this because he subscribed to a radical, very unpopular theory
that material things, including gases like air, were made of atoms; and
heat made the atoms move around. And as you increase the amount of heat,
the atoms can move in many different ways.</p>
<p>So the number of <em>states</em> that they can be in that look the
same to us <em>increases.</em> And that’s what entropy is. It’s this
increase in the number of states that the system can be in that all look
the same to us with the measurements that we can make. And since they
look the same to us, we’re <em>uncertain</em> about exactly what state
they’re in. So entropy is a measure of uncertainty.</p>
<p>This was really the beginning of modern physics, because what it says
now is that “decreasing uncertainty requires energy.” It links a
measurement of uncertainty to a measurement of energy. And if you think
about the <strong>uncertainty principle</strong> in quantum theory, the
core idea of the uncertainty principle is “you can’t <em>measure</em> a
system without <em>disturbing</em> it.” So to actually <em>act</em> on a
system requires energy.</p>
<p>And that’s what you have to do to get information. So here’s
Boltzmann, basically inventing quantum theory.</p>
<p><img src="../../Video/Slide8.PNG" alt="fast forward to 1900" /></p>
<h3 id="planck">Planck</h3>
<p>So we’re going to fast forward by another 15 years, to 1900. And in
1900, <strong>[Max] Planck</strong> solved this problem called the
“black-body radiation problem,” which was basically “how much heat does
your hot boiler give off into the air?”</p>
<p>And all of the measurements of the heat that hot boilers gave off to
the air ran into problems in classical physics, and caused
contradictions and quantities that went to infinity.</p>
<p>And all of that was bad. So many people were trying to solve this
problem. And Planck solved it by making a simple postulate. He said “the
energy of the radiation is proportional to its frequency.” So its
<em>color</em> in the case of light. And if you go higher frequency, you
end up in ultraviolet and X rays and gamma rays.</p>
<p>If you go to lower frequency, you go into microwaves and radio and
all of that. So this is a nice way of talking about radiation. And it
turned out that this solved the problem! I mean, just assuming this
simple proportionality relationship produced spectra for black-body
radiation that worked, that matched what you saw experimentally. Well,
this means something very important.</p>
<p>It means because this number <em>h</em>, the proportionality constant
called <strong>Planck’s constant</strong>, is a <em>number,</em> it’s
finite, it means that energy comes in discrete units of <em>h</em>. You
can have one <em>h</em> or two <em>h</em> or 10 million <em>h</em>, but
you can’t have half an <em>h</em> of energy. So it’s quantized.</p>
<p>And this is widely recognized as the birth of quantum theory. But of
course, we should have known this already, if we just thought a little
bit, right? We know that changes of energy are proportional to changes
in entropy by temperature.</p>
<p>And we know that entropy is a measure of the number of states, and
numbers of states are just numbers. You can have one state, two states,
three states, 10 million states, 100 billion states, but they’re all
just a number - one, up.</p>
<p>And it’s not infinite. There’s not an infinite number of states
unless you have an infinite amount of energy, which you don’t have. So
we knew already that entropy could only take discrete values.</p>
<p>And since energy and entropy are basically the same thing, we knew
already that energy could only take discrete values. So we
<em>could</em> have realized in 1900 that energy is quantized because
the number of states is quantized. So it shouldn’t have really been a
<em>mystery</em> why energy was quantized. But it was a mystery, and it
stayed a mystery, and it’s still a mystery! People still debate the
meaning of quantum theory.</p>
<p><img src="../../Video/Slide9.PNG" alt="But already in 1900…" /></p>
<p>But another thing we <em>could</em> have known in 1900 was something
<em>very</em> important, and it’s that this quantum of action, Planck’s
constant, which has units of action, which is energy times time, is
intimately related to Boltzmann’s constant.</p>
<p>And Boltzmann’s constant has units of energy over temperature.</p>
<p>But this wasn’t actually understood until the 1950s. No one really
figured {that} this out. There was this relation until the 1950s.</p>
<h3 id="wick">Wick</h3>
<p>And when it was figured out, it was figured out by a guy named
<strong>Gian Carlo Wick</strong>. And he introduced this notion of the
<strong>Wick rotation</strong> by realizing that if you have an equation
in classical physics, and in it there’s the term “1 over kT,”</p>
<p>you can always replace that “1 over kT” with this other expression,
“<em>i</em> <em>t</em> over h-bar.” H-bar is just <em>h</em> divided by
2 pi; and you’ll get an equation that’s valid in quantum theory. And
this is typically described in textbooks as “a trick.” And whenever
something in physics is described as a trick, what that really means is
it’s something we don’t understand.</p>
<p>And lots of papers have been written about the meaning of the Wick
rotation. But to start to understand the Wick rotation, I want to look
at this equation a little bit. “1 over kT” is 1 over an energy. And
“<em>i</em> <em>t</em> over h-bar” is 1 over an energy, since h-bar is
units of energy times time. We have time in this equation, and then we
have this factor, <em>i</em>, which is typically thought of as just an
imaginary number. So it’s the square root of minus one.</p>
<p>And - what’s this “<em>i</em>” doing in this equation? And in fact,
you see factors of <em>i</em> <em>t</em> over h-bar (if you’re familiar
with quantum theory) <em>everywhere</em> in quantum theory! So what’s
the meaning of this “imaginary number?”</p>
<p>And if you just think of “<em>i</em> <em>t</em>,” or of <em>i</em>,
as this “arbitrary, imaginary number” that somehow renders the equation
mysterious, then the whole of quantum theory is mysterious. But the Wick
rotation is very mysterious.</p>
<p>But what <em>i</em> is actually is an <strong>operator</strong>. And
if you think of the real numbers as an axis (which is always drawn
horizontally)… So here it is. Here’s zero, and the axis is pointing that
way. {(I’m sorry - I can’t get the camera really far enough away to see
my arm here.)}</p>
<p>So what is <em>i</em>? <em>i</em> is actually an operator that
rotates the whole real axis by 90 degrees. So if you see a plot of
complex numbers, then the real numbers go this way, and the, quote,
“imaginary numbers” go that way. So what multiplying by <em>i</em> has
done is rotate by 90 degrees.</p>
<p>And of course, if you do <em>i</em> squared, you rotate
<em>twice</em> by 90 degrees (my arm won’t do this), but you end up
pointing <em>that</em> way, and those are the negative numbers.</p>
<p>So <em>i</em> is an operator that rotates something by 90 degrees,
and if you rotate four times by 90 degrees, you’re back to the identity.
So <em>i</em> to the fourth is one. So this tells us something very
interesting, which is that what the Wick rotation is really talking
about is a rotation. It’s a geometrical equation. And in July, we’ll
come back to this and really probe what this Wick rotation <em>means,
physically.</em></p>
<p>But as I said, this wasn’t understood ’till the 1950s. And by the
1950s, a lot had happened. So - quantum theory had been developed. Bohr
and Einstein had had their debate. Particle accelerators had been
built.</p>
<p>The atomic bomb had been built.</p>
<p>Nuclear physics was well on its way. So an enormous amount of
practical physics had been done. Quantum theory was highly developed.
People were starting to think about quantum field theory before they
made this simple realization that’s formulated in the Wick rotation. So
this is a harbinger of things to come. But before we continue in
physics, we need to backtrack in time a little bit, and look at what the
mathematicians were doing.</p>
<h2 id="mathematics">Mathematics</h2>
<p><img src="../../Video/Slide10.PNG"
alt="Meanwhile, across the hall…" /></p>
<h3 id="gödel">Gödel</h3>
<p>So across the hall in the math department, one year after the Solvay
conference, in 1929, Kurt Gödel proved his famous first incompleteness
theorem. And the theorem states that “no formal system that contains
arithmetic can be both consistent and complete.” And that means that
either there are true statements that aren’t provable in the formal
system; <em>or</em> there are false statements that are provable;
<em>or,</em> of course, both.</p>
<p>And Gödel’s proof is actually extremely simple. Almost all of the
work in the proof is setting up all of the notation and procedures and
so forth to formulate within arithmetic the sentence, “This sentence is
not provable.”</p>
<p>And once you have that sentence formulated within arithmetic, then
the conclusion of the proof is obvious. If you can prove the sentence,
“this sentence is not provable,” then you’ve proved something that’s
false. And if you can’t prove it, then there are sentences that you
can’t prove in arithmetic. So this was incredibly bad news for
mathematicians who thought that finite discrete operations, which is
what proofs are and also what computations are, can exhaustively
enumerate the facts. And this was the assumption behind Wittgenstein’s
claim that “the world is all that is the case,” the world is a
collection of <em>facts.</em></p>
<p>And optimistically, he thought that first order logic would allow us
to enumerate all those facts and we’d be <em>done.</em> Dreams of a
Final Theory, again!</p>
<p>So Gödel’s theorem means that no system with finite capabilities, no
system that can just do finite discrete operations, can fully describe
its <em>environment.</em> It will always be in an environment where
there are true things that aren’t provable or false things that are
provable. But I think more relative to a discussion of agents, is that
it means that no <em>agent</em> can describe itself. Any agent’s theory
of itself {will either contain true statements that it can’t derive or
false statements, or it} will either <em>miss</em> true statements that
it can’t derive, or will end up deriving things that are false about
itself. Of course, we see this in psychology all the time.</p>
<p>So an immediate consequence of Gödel’s theorem was an intense
investigation of what computation actually <em>is,</em> what it
<em>meant</em> to talk about finite discrete operations.</p>
<p><img src="../../Video/Slide11.PNG" alt="What is computation?" /></p>
<h3 id="computation">Computation</h3>
<p>And two leaders of this were, of course, Church and Turing. And
here’s a picture of a Turing machine, which is just a little device with
a couple of tapes, and a tape reader, and a simple logic unit that
either writes a one or a zero if it sees a one or a zero. And they
defined a <strong>computation</strong> as a process that can be
implemented in finite time by such a machine, or by Church’s
<strong>lambda calculus</strong>, or by any of the now hundreds of other
methods that are provably equivalent to a Turing machine. So what does
<em>this</em> mean?</p>
<p>It means that computation is a <em>physical</em> process that can be
mechanized; and it turns out, mechanized in any one of a huge array of
ways. It means that many different implementations of any computation
are possible. So I can do it on a Turing machine, I can do it on my
laptop, I can do it on my head, et cetera. The most important things it
means is that there are questions with no computable answer. This is the
Revenge of Gödel’s theorem.</p>
<p>And two of the most famous questions of this kind are, “Given some
arbitrary program, will it halt? Will it get to an answer in finite
time?” And the answer to that question is, “This is undecidable. No
procedure can figure this out.”</p>
<p>And the other undecidable question is, “Given some arbitrary program,
what <em>function</em> does it compute?” And you’d think that would be
simple, that you can read a program and figure out what function it
computes. But it turns out that is undecidable. That cannot be done by
any finite process. So this was another body blow to the goal of
understanding everything with finite discrete processes.</p>
<p><img src="../../Video/Slide12.PNG" alt="The stage is now set…" /></p>
<p>But it also set the stage for something <em>new.</em> It set the
stage for thinking about an agent who interacts with a computational
process by giving it an input, and then looking at its output some time
later.</p>
<p>And this, of course, will look familiar, because I’ve included the
blue ellipse, which is the boundary, which these days we call a
“<strong>user interface</strong>.” And the user interface just allows
some finite action on the system, and then the ability to observe some
finite response by the system. So we can now ask, “What can Alice
determine by acting in some finite way, and then making some finite
number of observations?” - i.e. receiving some finite number of outputs
from the system that she’s acting on.</p>
<p><img src="../../Video/Slide13.PNG"
alt="A flurry of negative answers" /></p>
<p>And the first 20 years of this produced a <em>large</em> number of
answers, all of them negative. So to go back to Turing, he proved that
Alice can’t tell what’s <em>implementing</em> the function that she sees
being implemented. She can’t tell whether a given input will lead to an
output - that’s the <strong>halting problem</strong>.</p>
<p><strong>[Claude Elwood] Shannon</strong> showed that Alice can’t tell
what the inputs <em>mean</em> to the system - his whole theory of
communication is completely independent of semantics, and his theory of
communication actually accurately describes what Alice can observe.
<strong>[Henry Gordon] Rice</strong> is the one who proved that you
can’t determine what program is generating the outputs. <a
href="https://www.ams.org/journals/tran/1953-074-02/S0002-9947-1953-0053041-6/S0002-9947-1953-0053041-6.pdf">Classes
of Recursively Enumerable Sets and Their Decision Problems</a></p>
<p>And then <strong>[Darrell R.] Moore</strong> proved a very similar
result in a completely different formal setting of general cybernetics,
that you can’t tell what <em>process</em> is generating the outputs by
finite observation. But what Alice <em>can</em> do, is build a
predictive <em>model</em> of what generates the output she sees in
response to her inputs, and test it by designing new inputs. And this
is, of course, as <strong>Karl Popper</strong> told us, the process that
we call “science.”</p>
<p>So Alice can do science even though she can’t answer any of these
fundamental questions.</p>
<p><img src="../../Video/Slide14.PNG"
alt="A major technological consequence" /></p>
<p>Now, this, of course, has a huge technological consequence. Since
this theory of computation tells us that processes are effectively
virtual, we don’t know what they are and we can’t <em>determine</em>
what they are except in theory, by <em>making</em> a theory.
Technologically, it means we’re free to <em>use</em> virtualization
everywhere because we have to deal with it anyway. And this allows us to
build multilevel architectures. It means that we can architect computers
where no layer of the computation has any idea what’s going on below or
above it, and doesn’t need to.</p>
<p>And that’s what makes practical programming possible. So from these
no-go theorems that tell you what you <em>can’t</em> do, you actually
get an enormous boost into [inaudible] and use, to probe the world and
develop theories and on and on and on.</p>
<p>So in a sense, Gödel birthed not only computer science, but practical
computing by showing us that virtualization is just the way the world
works.</p>
<h2 id="physics-again">Physics Again</h2>
<p><img src="../../Video/Slide15.PNG"
alt="Feynman expressed these same ideas" /></p>
<h3 id="feynman">Feynman</h3>
<p>So now let’s go back to physics, where these ideas were replicated,
basically reintroduced, reinvented by Feynman in developing his path
integral formulation of quantum theory. And basically what Feyman
realized was that in any physical process, the observer, Alice, prepares
some state that she’s interested in.</p>
<p>She prepares some input to an experiment. Then she lets something
happen, and then she sees what the result of the experiment is. And the
canonical experiment in physics is scattering. You {you know} fire two
protons at each other, and they intersect someplace, and stuff comes
out, and you measure the stuff that comes out.</p>
<p>And what you can measure is momentum and spin energy, things like
that, position.</p>
<p>So these processes conserve the total values of the things you can
measure. So in particular, they preserve momentum and angular momentum,
and they preserve {you know} other things that are harder to measure and
are only approximately conserved anyway, like <strong>lepton
number</strong>. But Feynman’s contribution to this way of thinking
about experiments was to say, “Look, if you want to understand the
output, you have to <em>sum</em> over all of the possible processes that
could have produced the output from the input, no matter how improbable
they are.” So the famous idea from Feynman <strong>diagrams</strong> is
if you have an electron that’s scattering off an atom, you measure the
initial state of the electron that you’ve generated with an accelerator
or something, and you measure the final state, which involves momentum
and spin and so forth.</p>
<p>And for all you knew, partway through the process, the electron
disappeared, and an entire universe appeared and then annihilated with a
copy of an entire anti-universe, and the electron came back out!</p>
<p>And you have to include processes like that if you really want to
understand and correctly predict the outcomes of your measurements. And
<strong>Murray Gell-Mann</strong> lifted the bumper sticker slogan from
T. H. White in the book “The Once and Future King,” “Everything not
explicitly forbidden is mandatory.” That sums up Feynman’s idea. And
this is called the <strong>totalitarian principle</strong>, since it’s
what’s written in T. H. White’s section in the book “The Once and Future
King” on the kingdom of the ants, for which “everything not explicitly
forbidden is mandatory.” So let’s think about a real example that’s {a
bit actually it’s…} the same as scattering.</p>
<p><img src="../../Video/Slide16.PNG"
alt="The ultimate scattering experiment is a black hole" /></p>
<p>The ultimate scattering experiment in physics is a black hole. Stuff
goes into the horizon, stars, whole galaxies, whatever. Something
happens, and stuff comes out. And what comes out is <strong>Hawking
radiation</strong>, and information is conserved if the information in
the Hawking radiation is actually the same as the information in the
“stuff” that went in. And conservation is not qualitative. It’s
quantitative. It’s only the <strong>quantity of information</strong>
that’s supposed to be preserved. And you can ask, “How much is the
information that’s being emitted by a black hole that’s emitting Hawking
radiation?” And <strong>[Jacob David] Bekenstein</strong> was the one
who figured this out, using an incredibly simple argument. But I’ll just
give the answer here.</p>
<p>The answer is that the total entropy of the black hole is its area
divided by four. And the area in this equation has to be computed in
<strong>Planck units</strong>, which are units where Planck’s constant
and the speed of light and Boltzmann’s constant and other interesting
things are all set equal to <em>one.</em> So one Planck area is the
Planck length squared, which turns out to be about ten to the minus 70th
meters squared. So this says that black holes are the most entropic
entities we know of. So a black hole about this big [gesturing], with a
radius of about a meter, has an entropy of about ten to the 70th, which
is, of course, an astonishingly big number. And a black hole with the
area of the sun - so a moderate sized cosmological black hole, a real
thing that we can observe with a gravity wave telescope or something -
has an entropy of about ten to the 79th.</p>
<p>And really big black holes, which are bigger than the entire solar
system, have entropies into the ten to the 80th, 10 to the 85th, or
something. So these are enormously entropic entities.</p>
<p>But Beckenstein didn’t just tell us that. He told us something about
the <em>structure</em> of the interface, the horizon of a black
hole.</p>
<p><img src="../../Video/Slide17.PNG" alt="Recall that S = lnΩ" /></p>
<p>And this is what he told us.</p>
<p>You can compute entropy in bits just by using logs base two instead
of logs base <em>e</em>, natural logs. And that’s just multiplying the
natural log by about 1.4. So we can write the entropy {of black hole,}
of black hole in units of bits, and it’s about a over six.</p>
<p>So what does this mean to say that we can think of the entropy in
terms of <em>bits?</em> What it says is that we can think of the
interface in terms of a bit array. And we can think of all of these bits
as encoded on this interface at a density of one bit every roughly six
Planck lengths squared, six plank areas.</p>
<p>So this is an incredibly dense encoding in a black hole.</p>
<p>And it’s an interesting idea about black holes. But what gets really
interesting is what happens when you <em>generalize</em> it. And of
course, physicists are prone to generalization, and that’s what happened
<em>next.</em></p>
<p><img src="../../Video/Slide18.PNG"
alt="We just have to generalize" /></p>
<h3 id="the-holographic-principle">The Holographic Principle</h3>
<p><strong>Gerard ’t Hooft</strong>, almost immediately thereafter, on
the basis of Beckenstein’s work, formulated the <strong>Holographic
Principle</strong>.</p>
<p>And what the Holographic Principle says is “We can think of
<em>any</em> system as <em>approximately</em> a black hole. And the only
approximation is the encoding density.” The <em>boundary</em> of any
system encodes the information that we can <em>get</em> about the system
at some density. And the density is less than the density for a black
hole because we’re not black holes.</p>
<p>And this rules out a lot by limiting the amount of information that
you can <em>extract</em> from a system to the amount of information that
you can actually <em>write on its boundary.</em> And one of the things
that it rules out is knowing the <strong>geometry</strong> on the inside
of the system.</p>
<p>And ’t Hooft put it this way, which I think is a brilliant thought
experiment. He says, “Look, the metric inside this system can be <em>so
curved,</em> effectively the space time inside the system can be so
curved, that you could stick an entire universe inside. And we never
know, because it wouldn’t change the amount of entropy, number of bits
that are encoded on the boundary. So the inside geometry can be
anything. And you can’t find out by looking at the outside of the
system.”</p>
<p>Now, the Holographic Principle almost follows from classical physics!
There’s <strong>[Leonhard]</strong> <strong>Euler’s theorem</strong>,
which tells you about one over R squared forces, and you can think of
the force as penetrating the boundary at some density. And the
difference between Euler’s theorem and the Holographic Principle is just
that the density in classical physics can go to infinity.</p>
<p>And in the Holographic Principle, it can only go to this maximal
density that’s achieved by a black hole. Why? Because if you try to get
any higher, more information, the information gets sucked into the black
hole, and you can’t get it out.</p>
<p>So the Holographic Principle becomes a <em>guiding principle</em> for
thinking about any physical system and what it means to extract
information from a physical system. And in fact (we’ll talk about this
next time), you can say exactly what the information encoded on the
boundary of any system <em>is.</em> And what I’ll go through next time
is seeing that the information that’s encoded on the boundary is a
specification of the energy that’s being exchanged by the interaction,
which, of course, is linear in the number of bits. It’s just counting
the number of bits.</p>
<p><img src="../../Video/Slide19.PNG"
alt="Quantum information theory" /></p>
<p>Okay! So with the Holographic Principle, we now have a complete new
science about systems that are exchanging finite discrete information
across a boundary by encoding that information <em>on</em> the boundary
and then reading the information <em>off</em> the boundary.</p>
<p>So here’s a new way of thinking about physical interaction. Alice
writes a message <em>on</em> her boundary with Bob, and Bob reads the
message <em>off</em> the boundary, and then writes his own new message,
which Alice then reads.</p>
<p>And of course, that’s what’s happening <em>right now.</em> I’m
writing information by speaking <em>on</em> an interface, which is
effectively the Internet. And you’re reading that information
<em>off</em> the Internet by listening to it. And when we get time for
questions, you’ll be writing information on the Internet, which I’ll be
reading. Now, the key thing about this new science is that it’s
<strong>topological</strong>. It’s about connectivity across some
communication channel, which we represent as a boundary.</p>
<p>It’s not geometric, so it doesn’t assume anything about
spacetime.</p>
<p>So it allows us to <em>build a model</em> of <em>space</em> in
particular as an emergent phenomenon. And it allows us to see
<em>time</em> as not some absolute external abstraction, but something
that the communicating agents measure for themselves. And so they each
act in their <em>own time.</em> And we get a very natural measure of
time in terms of how many bits I see coming across my boundary.</p>
<p>And again, we’ll talk about that in the next couple of sessions, and
then talk about it more thoroughly in September when we talk about
emergent spacetime. So quantum information theory looks very different
from the physics that came before, not because it’s adopted a new
formalism, the tools of quantum theory; it’s because it’s entirely
changed the <em>thinking</em> about what physics <em>is</em> and what
it’s <em>about,</em> and replaced this idea of forces and balls banging
into each other and all of that with the idea of communication between
agents. And of course that’s familiar from an Active Inference
perspective.</p>
<p><img src="../../Video/Slide20.PNG" alt="Four NOs…" /></p>
<p>So we can now back up a little bit to see what they were doing in
<em>classical</em> physics during this period. {Well, let me go on a
little bit. Sorry.}</p>
<h3 id="wheeler-on-the-new-physics">Wheeler on The New Physics</h3>
<p>This is just a slide quoting Wheeler, who of course is the most
radical {in terms of} and <em>pithiest,</em> in terms of formulating
these ideas. But here’s his characterization of this new physics.</p>
<p>It’s not reductive: all of the reasoning is circular; you don’t have
smaller things and then yet smaller things, and then yet smaller things
forever.</p>
<p>There are no laws: so what you see on the interface is a message;
it’s not something that’s governed by laws since the beginning of the
universe.</p>
<p>There’s no continuum: so there’s nothing described by real numbers;
it’s all described by integers; everything’s done in finite dimensional
spaces.</p>
<p>And finally, and most importantly, there’s no “spacetime
<em>box</em>” in which things happen. So think of how radical this is!
It means there’s no big bang, there’s no big rip, there’s no bouncing
universe. All of those ideas are out the window because they’re
classical and they’re about spacetime. And what the new physics wants to
do is derive spacetime out of basically users’ experiences.</p>
<p>So the agents here are all “observer-participants” in Wheeler’s
language. But what that just means is <em>agents</em> that want to
communicate. And it’s their communication that gives rise to
physics.</p>
<p><img src="../../Video/Slide21.PNG"
alt="Meanwhile, back in classical physics…" /></p>
<h3 id="markov-blankets">Markov Blankets</h3>
<p>So <em>now</em> we’ll go back to classical physics. And what was
happening, or one thing that was happening in classical physics at that
time, was a lot of thinking about <strong>stochastic causal
networks</strong>. And <strong>[Judea] Pearl</strong> realized that if
you have any stochastic causal network that’s
<strong>unidirectional</strong>, then around any <strong>node</strong>,
you can draw what he called a <strong>Markov blanket</strong>.</p>
<p>And a Markov blanket is just the set of nodes in the network that
absorb all outside causation and then transmit that causation into
whatever node you’re interested in; and then absorb all causation coming
from the node you’re interested in and transmit it to the rest of the
world.</p>
<p>And so we can redraw that in “part b” here, and it should look very
familiar. A Markov blanket is just a classical-physics way of talking
about a <strong>holographic screen</strong>. And the number of nodes in
the Markov blanket, or in particular the number of <strong>degrees of
freedom</strong> times the number of nodes, is just the number of bits
that flow across that Markov blanket. So it’s the entropy of the
effective holographic screen.</p>
<p>All these ideas were reinvented more or less independently within
classical physics.</p>
<h2 id="active-inference">Active Inference</h2>
<p><img src="../../Video/Slide22.PNG"
alt="An MB defines a persistent thing" /></p>
<p>And it was from this classical physics background that <strong>Karl
Friston</strong> came up with the idea that a Markov blanket defines a
persistent [**** inaudible ****] [<strong>thing</strong>, a system that
persists through time], at least from an Active Inference Institute
point of view everyone is familiar with, because it’s the foundation of
the idea of Active Inference.</p>
<p>Any system that persists through time does so by making sure that it
doesn’t dissolve into its environment. Well, what does that
<em>mean?</em> It means it persists through time by maintaining the
integrity of its Markov blanket, or the integrity of its boundary. So
this of course is just a tautology. But it’s a very interesting and very
productive tautology, because it says that any system is using the
information it gets on its boundary from its environment to build a
model of how its environment behaves. And then it uses that model to act
back on its environment to test and refine its model.</p>
<p>And again, as Popper told us, this is just what <em>science</em> is.
So what the free energy principle really tells us is that all systems
are agents that are doing science all the time.</p>
<p>So physics is effectively not just the study of communication, but
it’s the study of agents doing science with each other, pairs of agents
who are trying to figure each other out by their communicative
exchanges. So that’s the history of how we got from {1930s sorry,} 1850s
thermodynamics to the free energy principle, and how the free energy
principle connects to these very deep and extremely radical (especially
within context) ideas in quantum theory and quantum cosmology and
computer science. All of which tell us that the world we see is a
projection that’s being written on our boundaries by a process that we
have no access to - except the procedure of Active Inference, or the
procedure of science - which is to formulate predictive models and test
them by doing things in the world and seeing how the world responds.</p>
<h2 id="conclusion">Conclusion</h2>
<p><img src="../../Video/Slide23.PNG"
alt="1st discussion session: Sat. 3 June 2023" /></p>
<p>So that’s it for this session.</p>
<p>The first discussion session, which Ander is going to lead, will be
the 3rd of June at this same time, i.e. 5 o’clock European time. And
then my session number two will be in mid June.</p>
<p>And if you look at the course website <a
href="https://coda.io/@active-inference-institute/fields-physics-2023/questions-answers-3">1st
Discussion; 2nd Lecture</a>, there’s this subsidiary website for
interactive Q and A. And I invite everyone to post questions, and
discuss them; and hope that we’ll have an interesting exchange and that
everyone will come to sort of an understanding of what was talked about
today through discussion.</p>
<p>And we’ll be interested in being back in June to see how to formulate
this in quantum theory.</p>
<p>So thank you very much and thank you again, Daniel, for organizing
this and hosting it and putting together all the technical things
necessary to pull this off. I could never do that on my own.</p>
<p><em>Daniel:</em> Thank you. We’re really excited here.</p>
<p>Any closing thoughts? Or Ander, I’d love to hear your reflection just
briefly. How would you have told that history; or how does that history
reflect on the areas that you’re familiar with?</p>
<p><em>Ander:</em> Well, I don’t think I have much to add. I’ve been
mainly interested in the Wick rotation part. I’m excited to hear more
about that, Chris. But no, I found this to be a very nice summary. I’m
less familiar with the staff on computation, formal computation, despite
being a mathematician by training, it was a little more familiar with
all the physics stuff, but yeah, I thought it was great.</p>
<p><em>Chris:</em> Thank you.</p>
<p><em>Daniel:</em> All right. Thank you. I will close it. So in about
two weeks, we’ll have the first discussion. Everyone’s welcome to join.
I will share with Ander and Chris the questions, and we can develop
that.</p>
<p>And we’ll have that on the course front end before the coming
discussion. So thanks again, fellows. See you next time.</p>

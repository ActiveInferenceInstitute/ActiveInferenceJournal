title,author,id,date,text
Historical Perspective: Physics,Chris Fields,CFPIP01L,2023-05-18,"This is a course on 'Physics as Information Processing,' and this first session will be a historical perspective on the idea that physics is, or is about, information processing.\n And I'll just start with a few quotations that span the middle of the 20th century - from Ludwig Wittgenstein in the 1920s saying 'The world is all that is the case,' so, defining the world in terms of facts, not objects; Rolf Landauer in the early '60s, proclaiming that 'Information is Physical;' and then John Archibald Wheeler, who in many ways is the grandfather of this era recently, stating it is 'It from Bit,' so things come from information, i.e. bitstrings.\n And if nothing else, this shows that formulations of this idea get pithier as the 20th century rolls on!\n But the history goes back farther clearly than the 20th century.\n But I'm only going to really talk about a piece of it and the timeline that I'll actually discuss today, the most relevant history of this idea, goes back to the mid-19th Century."
Clausius,Chris Fields,CFPIP01L,2023-05-18,"And the first specific thing I'll talk about is Rudolf Clausius's definition of entropy.\n\n But with the beginning of the understanding of thermodynamics and the role of information in thermodynamics, you get this very interesting multidisciplinary progression of ideas that incorporates the beginning of quantum theory.\n\n And the beginning of quantum theory can kind of be dated to the fifth Solvay conference in 1927, and the famous debate between Nils Bohr and Albert Einstein over whether quantum theory is about knowledge, information - or about objects, things.\n But it incorporates a lot of work in computer science and logic and mathematics.\n So, interestingly, computer science was born effectively in the mid-30s with the work of Alonzo Church and Alan Turing, which very rapidly converged with the work in physics."
"Physics, Computation, Information",Chris Fields,CFPIP01L,2023-05-18,"So today we'll be talking about both computer science and physics.\n And then in the second half of the 20th Century, this just exploded into a huge area.\n\n And in consequence of that mid-20th Century development, we're beginning to see a new idea about physics which is roughly encapsulated in quantum information theory.\n\nPhysics is about information transfer across boundaries\n\n And the new idea is this.\n It's that what physics is actually about is information transfer across boundaries and the information... We can represent the information transfer like this.\n And this is a convention I'll use.\n\n A boundary is always a blue ellipse.\n And the agents that are exchanging information across this boundary are conventionally called Alice and Bob, which is just a more polite way of saying A and B.\n\n And when you think about this picture, it becomes clear that what physics is really about is communication.\n And this is a wild redescription of the idea of what physics is, compared to the ideas of Newton or LaPlace or even the 19th century ideas.\n And it's very different from the idea that's been preserved in 20th Century physics in the lineage of Einstein and others who viewed classical physics as, in a sense, either as completely fundamental or as a fundamental adjunct to quantum theory.\n\n So this way of thinking about physics is a very deeply quantum-theoretic way of thinking about physics."
Course Outline,Chris Fields,CFPIP01L,2023-05-18,"Where we are going in this course?\n\n And where we're going in this course today is really 'how did this all happen?'\n It's the origin story.\n And then in the next session, I want to discuss quantum information theory explicitly. And in particular, how quantum theory makes this conclusion that physics is about communication very simple and obvious, much more obvious than it is in classical physics where it takes work to formulate this idea.\n Then in the next session, we're going to talk about semantics, and how observations become meaningful to the agents who make them and hence how actions become meaningful to the agents who make them.\n\n Then in August, we'll talk about communication theory a little bit more explicitly, and talk about how agents employ multiple communication channels when they're communicating.\n\n And this is obvious when you think of people communicating: they not only talk to each other, they look at the same things, they point to things, et cetera.\n So this is what I mean by multiple communication channels.\n Then in September, we'll leverage that discussion to talk about how spacetime actually emerges from communication.\n And this is one of the most important aspects, I think, of quantum information theory.\n\n It provides us with a way of viewing spacetime as an emergent phenomenon, that communication is what is fundamental in some ontological sense; and the box in which it happens, spacetime, is not.\n So in the final session in October we'll talk about applications to biology via the Free Energy Principle, and future directions both in physics and biology and elsewhere."
Course Summary,Chris Fields,CFPIP01L,2023-05-18,"So it's going to be an interesting ride!\n\n I'm keeping formalism to a minimum, because we're directing this toward the broad array of people who are interested in Active Inference and who are involved with the Active Inference Institute.\n And I would ask you to hold questions (because we have a lot to get through in an hour) for the interactive discussion and for the discussion forum.\n\n So I hope I explain things well enough that all of the concepts will be understandable.\n If not, Wikipedia is actually a wonderful resource in this area for just definitions of terms.\n So if there's anything that... just a term that is a trip-up, try Wikipedia.\n It's probably a very good source for what these terms mean."
Steam Engines,Chris Fields,CFPIP01L,2023-05-18,"And in the mid-19th Century, lots of physicists were devoting their efforts to figuring out how to make better steam engines.\n  And one question that arises when you're trying to design a steam engine is 'what happens physically when you add heat to a system at constant temperature?'\n So if you're building a steam engine, you've got a boiler, because you need to make steam.\n And as you turn up the heat to your boiler, you get more steam, but the temperature doesn't change.\n So this is a mystery.\n\n What is the heat actually adding to the boiler that is not increasing the temperature?\n And Clausius responded to this question in a way that's sort of typical for a physicist or a mathematician.\n Since he didn't know what the answer was, he just invented a new name for whatever it was, and gave it a formal definition.\n\n So he called it entropy, which is a made-up word that, if it was translated from the Greek, would roughly mean 'transformation content,' en-tropy.\n And he represented it by a simple equation that the change in this new concept, entropy, which is always called S, is just equal to the change in heat, Q, at constant temperature, T."
What is Heat,Chris Fields,CFPIP01L,2023-05-18,"So obviously, this equation just reformulates the question in declarative form, saying 'whatever this stuff is, its changes in this stuff are just changes in heat at constant temperature.'\n Well, heat is energy.\n And this wasn't completely recognized in the mid-19th Century. But the way you'll see this equation in a current textbook is 'dS is the change in energy at constant temperature.'\n\n So even more commonly, you would see it written as 'the change in energy is equal to the temperature times the change in entropy.'\n It's the most common sort of textbook way of seeing this.\n\n But the question, of course, is, 'What is this quantity?\n What is this entropy?\n What does this concept mean?'\n\nBut what is entropy?"
Boltzmann,Chris Fields,CFPIP01L,2023-05-18,"And about 15 years after Clausius proposed it, Ludwig Boltzmann had the key insight, which is that 'entropy is a measurement of our uncertainty about the state the system is in.'\n And in particular, he again, of course, went to formalism, and said 'the entropy, S, is equal to some constant times the number of states that the system can be in that look the same to us.'\n\n And since that number of states is enormous, the way to make that manageable is to take the log of the number of states.\n\n The natural log is ln.\n And this constant k is called Boltzmann's constant.\n And Boltzmann was able to do this because he subscribed to a radical, very unpopular theory that material things, including gases like air, were made of atoms; and heat made the atoms move around.\n And as you increase the amount of heat, the atoms can move in many different ways.\n\n So the number of states that they can be in that look the same to us increases.\n And that's what entropy is.\n It's this increase in the number of states that the system can be in that all look the same to us with the measurements that we can make.\n And since they look the same to us, we're uncertain about exactly what state they're in.\n So entropy is a measure of uncertainty."
Boltzmann Founds Modern Physics,Chris Fields,CFPIP01L,2023-05-18,"This was really the beginning of modern physics,\n because what it says now is that 'decreasing uncertainty requires energy.'\n It links a measurement of uncertainty to a measurement of energy.\n And if you think about the uncertainty principle in quantum theory, the core idea of the uncertainty principle is 'you can't measure a system without disturbing it.'\n So to actually act on a system requires energy.\n\n And that's what you have to do to get information.\n So here's Boltzmann, basically inventing quantum theory."
Planck,Chris Fields,CFPIP01L,2023-05-18,"Fast forward to 1900.\n So we're going to fast forward by another 15 years, to 1900.\n And in 1900, [Max] Planck solved this problem called the 'black-body radiation problem,' which was basically 'how much heat does your hot boiler give off into the air?'\n\n And all of the measurements of the heat that hot boilers gave off to the air ran into problems in classical physics, and caused contradictions and quantities that went to infinity.\n\n And all of that was bad.\n So many people were trying to solve this problem. And Planck solved it by making a simple postulate.\n He said 'the energy of the radiation is proportional to its frequency.'\n So its color in the case of light.\n And if you go higher frequency, you end up in ultraviolet and X rays and gamma rays.\n\n If you go to lower frequency, you go into microwaves and radio and all of that.\n So this is a nice way of talking about radiation.\n And it turned out that this solved the problem!\n I mean, just assuming this simple proportionality relationship produced spectra for black-body radiation that worked, that matched what you saw experimentally.\n Well, this means something very important."
Planck and Countable States,Chris Fields,CFPIP01L,2023-05-18,"It means because this number h, the proportionality constant called Planck's constant, is a number,  it's finite, it means that energy comes in discrete units of h.\n You can have one h or two h or 10 million h, but you can't have half an h of energy.\n So it's quantized.\n\n And this is widely recognized as the birth of quantum theory.\n But of course, we should have known this already,  if we just thought a little bit,  right?\n We know that changes of energy are proportional to changes in entropy by temperature.\n\n And we know that entropy is a measure of the number of states, and numbers of states are just numbers.\n You can have one state, two states, three states, 10 million states, 100 billion states, but they're all just a number - one, up.\n\n And it's not infinite.\n There's not an infinite number of states unless you have an infinite amount of energy, which you don't have.\n So we knew already that entropy could only take discrete values.\n\n And since energy and entropy are basically the same thing, we knew already that energy could only take discrete values.\n So we could have realized in 1900 that energy is quantized because the number of states is quantized.\n So it shouldn't have really been a mystery why energy was quantized. But it was a mystery, and it stayed a mystery, and it's still a mystery!\n People still debate the meaning of quantum theory."

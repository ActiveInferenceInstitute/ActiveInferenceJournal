
00:00 _Speaker A:_
Alright.

00:01 _Speaker B:_
Hello and welcome everyone.
It is July 11, 2023.
We are in active inference for the social sciences and today is going to be a lecture by Ben White on basics of active inference, the active inference agent.
So Ben, thank you for the lecture.
Off to you and we're looking forward to it.

00:23 _Speaker A:_
Thank you very much.
Yeah, me too.
Very happy to be a part of this, to be kicking things off with the first module on this course.
I'm going to be talking about the active inference agent and trying to cover some of the basics.
I won't take too long introducing myself.
Avil gave a very thorough introduction in his talk recently, but I'm a second year PhD student in philosophy at the University of Sussex, and I work with Andy Clark and with Avel and some other people using active inference to try and find things out about our relationship with technology and within the context of well being and mental health.
The aims for today I'm going to try and provide quite a wide ranging overview of how active inference has connected with areas of philosophical interest and particularly how they relate to the individual agent and the experience of the agent and how the agent finds themselves in the world.
So I've split this up to look at several different defined topics.
So I'm going to move quite quickly through mind, agency, emotion and phenomenology, and I'm going to say a little bit about the self as well.
And then there's going to be a case study at the end that I'm going to move through if we have time.
So one of the other aims that I have is to lay out very clearly some of the core concepts and core mechanisms like precision, weighting and prediction error generative models.
I'm not going to go into any technical details whatsoever.
This is all going to be fairly abstract.
My background and my interest in these frameworks is philosophical, so it's quite abstract in terms of how everything fits together.
And I'm going to try and bring some of these threads together to build up a layered picture of what the active inference agent is like.
Because this course is kind of aimed towards seeing how active inference applies to larger scales, collective behavioral, social norms, sociocultural landscapes.
And I think in order to do justice to those things, we need to first have a good idea of the individual in active inference.
So we've got our work cut out for us.
So I'll get started.
So before I dive in, I wanted to just say a little bit about how these frameworks hang together.
So active inference is based on Carl Fristen's free energy principle.
I think most of us are familiar with that.
But the free energy principle just states that in order to persist through time, biological organisms must occupy only those states that it would expect to occupy, given the type of thing that it is.
So free energy is essentially a measure of the disatunement between a system and its environment.
And I'm going to say a little bit more about attunement and how that gets cashed out in various ways as we go on.
Active inference is a process theory that essentially explains how embodied organisms actually go about remaining in those expected states.
So how we actually go about minimizing free energy.
And the idea is that all adaptive behavior is explained by agents garnering evidence to confirm their own expectations.
Predictive processing.
For the purposes of the lecture today, I'm going to treat these as synonymous because most of the work that I kind of came up through as I was learning about this was predictive processing.
And all of the papers that we're using today that refer to predictive processing refer to a predictive processing that I take to be more or less synonymous with active inference.
So that's a very embodied, inactive flavor of predictive processing.
I think generally the difference is that predictive processing can be a much broader term and predictive processing can also apply to passive models of perception.
But we're not going to concern ourselves with that.
Okay, so some key concepts of active inference, predictive processing then.
So the framework phase that agents embody or agents have, we won't concern ourselves with the nuance there, but they have a generative model, which is essentially a kind of understanding or a model of the regularities that underpin the dynamics between the agent and the environment.
So another way to think about this in very rough terms is a kind of mental model of the regularities in the world and how the agent exists in the world.
And using that model, the agent can generate predictions about its own sensory states.
So given the model, what kinds of sensory states would it expect to find itself in in a particular set of circumstances?
And so where those predictions don't actually match the sensory inputs.
So where there's a discrepancy between the actual incoming sense data and the prediction, prediction errors are generated and prediction errors are going to flow upwards through the hierarchy.
So the generative model is said to be a hierarchical model and predictions flow downwards and prediction errors flow upwards.
And the sole imperative of the brain body system according to this framework is to minimize those prediction errors.
So a good way to kind of get a little handle on this is to think about just the example of perception.
So perception traditionally so I'm talking about visual perception here.
It's traditionally been understood as a bottom up process.
And what that means is that the brain waits for incoming sensory signals and then it processes them as they come in and it combines them in kind of increasingly abstract ways.
And what we see is some combination of those sensory signals that have come in and been processed and combined.
So if you think about me leaving my apartment in the morning and stepping out onto the street and seeing some cars.
What's happening there is some sensory information is hitting my retina and it's being processed first with very basic features like light shade, edges and so on and so forth.
And then it's being combined with my understanding of what a car is or what a bus stop is.
And that gives me my kind of visual field.
Predictive processing, famously, at this point, flips this picture upside down on its head.
So visual perception under predictive processing is a top down affair.
So the generative model is going to encode multilevel expectations about the likely scene.
So just to be very clear, when we talk about multilevel and hierarchical, the higher up levels are going to be tracking longer time scales and higher levels of abstraction.
The lower levels are going to be much faster, much more concrete.
So a changeable scene.
So there's going to be some variation in what I see when I leave my apartment every day is going to mean that there's always going to be some error in that picture.
So it's never going to be the same two days in a row or it's extremely unlikely to be.
So no matter how well I can predict what I'm going to see when I leave my house, it's always going to be cleaned up by some prediction errors.
So essentially, when I step outside and I step into the street, my brain is already anticipating what it's going to see out there.
So I know where particular cars are going to be parked, I know where the bus stop is, so on and so forth.
And the perceptual experience that we have is a construction.
We actually experience the expectation that has been kind of cleaned up post prediction error.
There's a little bit of debate there about exactly what it is that we experience.
But for our purposes today, we can say that visual perception is essentially a construction.
So on the face of it, prediction errors can be minimized using one of two strategies.
And this is going to be really important.
So we have what I just described there, which is perceptual inference.
That's where we revise our predictions to better fit the evidence.
So essentially we update our model to better reflect the regularities in the environment.
Or we can engage in active inference, which is where the framework gets its name from, which is where we essentially make the evidence fit the model.
So that's where we in some sense update the world so it conforms to the original prediction.
And active inference agents bring about those expected states by sampling the world in a biased way, so they have certain expectations and then we sample the world to confirm the predictions.
And I put perceptual inference there in scare quotes because perceptual inference on kind of readings of most readings of active inference just becomes a kind of action readiness.
Or as Ramstard and colleagues put it, a state estimation so it's just perception and action are just wrapped up in this continuous loop.
But it's really all about the action.
It's really all about bringing about those expected states, and perception is just kind of in service of that process.
And so Berenberg and colleagues refer to the active inference agent as a crooked scientist, which is something I really like.
I think that's a very charming image.
So we are scientists that are engaging in some very dodgy behavior because we're not really interested in having an accurate model.
We're more interested in confirming our original predictions.
Okay, so that was a very rapid overview of some of the core concepts and mechanisms, but they're going to crop up continuously.
So I think we're going to get a much better grasp on them as we go on.
In this section, I'm going to start to expand on some of those things that we've just put on the table, and I'm going to begin to flesh them out in terms of thinking about how active inference agents actually exist in the world.
So I should say I put this here to remind me.
So those of you who are taking part in the course, I'm going to make available a document with all of the readings that firstly, all of the readings that I've used to make these slides and then a bunch of other readings as well that I think are going to be very relevant.
And obviously, they're going to be organized in terms of the structure of this presentation as well.
So this is just I've put a little sample up on these slides, but there's going to be much more in the document.
So I think it's always good to start with some guiding questions.
So I've thrown up three here.
The first is how does active inference characterize cognition?
So we've just looked at predictions and prediction errors, but we're going to expand on that a little bit.
And we're going to ask what kinds of processes does cognition involve?
How is cognition related to the material environment?
This is going to be something how cognition relates to the environment is going to be very important in subsequent weeks.
And we're going to ask quite briefly what kinds of vehicles might realize cognitive processing.
Okay, so here's a couple of whales.
Whales are going to be a recurring theme in this lecture, but I wanted to take a step back and ask about Attunement because this is one of those concepts that gets thrown around a lot.
And I think once it's grasped, it becomes very clear, very easy to use.
But I think prior to it being grasped, it can be a little bit opaque and a bit slippery.
So on the left we have a humback whale that was found dead in the Amazon rainforest, allegedly.
I don't know if this was true.
And on the right we have a whale in its natural environment.
And I think clearly the intuition here is that in one case the whale is very well attuned to its environment and in the other case it's not.
And I think that we can start by asking what is it about the ocean that allows the whale to be well attuned?
And what I want to say here is that the particular dynamics between the ocean and the whale allow the whale to do certain things that it can't do when it's in the Amazon.
So in other words, we might say that the ocean affords certain things to the whale, specifically filter feeding, being able to move its vast weight around with ease, things that, unfortunately, a whale in the Amazon can't do.
So affordances are opportunities for action.
They are things, roughly speaking, in the environment that the agent can act upon.
And there's some debate about the proper way to really characterize affordances, but I like to think of them as relational properties emerging from an environmental feature and some skilled embodied ability of the agent.
So the same thing in the environment will not afford the same thing to different organisms.
So, for example, soil to a human agent might afford walking on or laying down on, but to an Earthworm it affords very different things.
And also, even amongst the same organisms with humans, especially, different surfaces and features of the environment will afford different things depending on your particular skill set.
And agents are going to go about minimizing their uncertainty, so minimizing prediction errors, staying attuned to their environment through an engagement with affordances.
And this is going to be something that is going to drift away slightly as we move through some sections and then it's going to come back very heavily at the end.
And I should say that the idea of affordances, to my knowledge, it first emerges in the work of James Gibson the Ecological Approach to Visual Perception.
I think it's chapter eight in that book that really lays out a theory of affordances.
So this idea that systems or organisms maintain their organization through ongoing action means that active inference has been labeled as quintessentially inactive by Cole Friston.
And I think that this is a nice inactivism is an older framework from theoretical cognitive science.
And I think it's a nice way to understand some of the key features of active inference.
But it does come with a little bit of historical philosophical baggage.
And I'm going to try and unpack that really quickly because I want to unpack exactly what it is that we're committing ourselves to when we think about cognition in inactivist terms.
So inactivism comes in various strands very, very quickly and roughly, you have autopoietic inactivism, which commits itself to what's known as the mind life continuity thesis.
This is essentially that the structures that give a ground to life itself are the same structures that instantiate cognitive processes.
Sensing on this view is teleological because it's oriented around supporting vital systems.
So things in the environment have intrinsic meaning to the organism because they support the vital functioning of the organism in some way.
Sensory motor inactivism emphasizes how mobile and embodied agents essentially enact the grounds and conditions for their own sensory engagements with the world.
So the world is not a kind of neutral static, given that we're parachuted into actually the conditions through which we sense the world are the conditions that we can move and kind of interact with the world in various ways.
And then there's radical inactivism.
And this is the view that cognition is purely grounded in these agent environment dynamics.
It's a purely dynamicist account of cognition and there's no role whatsoever for internal representations.
So certainly in certain brands of sensory motor inactivism, there remains a role for internal manipulation of representations.
And mental representations are just mental states that are about something in the world.
So some mental state is reconstructing some feature of the world.
And this all harks back to kind of old debates in philosophy of cognitive science between internalist notions of cognition and more embodied, embedded, extended.
And we'll take more of a look at that in a second.
But the question here is what are we committing ourselves to with active inference?
Active inference?
And I need to be a little bit careful because I think that people are going to disagree with this to certain extents, but I think we're certainly committing ourselves to sensory motor and activism.
I think that's fairly uncontroversial.
And I would say as well that the underpinning of active inference by the free energy principle, by my lights, also commits us, or it certainly chimes very deeply with the core tenets of autopoietic inactivism.
But I think what's really interesting in trying to understand cognition in the active inference agent is to understand what role, if any, there are for representations.
So if you have any familiarity with the literature on active inference, you'll know that active inference people are always banging on about models, prediction errors, various other computationally sounding things.
And so the worry here is at least for well, it's not necessarily a worry, but the thought would be are we committing ourselves to a form of cognitivism?
And again, cognitivism is generally considered to be an old fashioned notion of cognition as like computation over symbolic representations in the brain according to some syntactic set of rules or something.
And typically over the last couple of decades in cognitive science, people have been moving away from that view.
So it would be interesting if we were committing ourselves to something that we'd already moved away from.
And some people think that we are making that commitment.
So Jacob Howie, for example, has argued in various places that essentially predictive processing or active inference commits us to these very rich reconstructive notions of representation.
And even Andy Clark Jacob Howie's foil in many places has said himself that he thinks that representations play a role here as well.
So in a talk in 2016.
Clark said it's internal representation.
I think it is.
These are representation using stories, but it's not internal representation in quite the way that we originally thought about it.
So there's some differences, perhaps major differences between Howie and Clark there, but others have pushed back.
So there's a paper by Maxwell Ramstead and colleagues from 2020, I think it's the tale of two densities paper where they say that a proper understanding of the generative models in active inference suggests that there's no role for representations.
So this is the kind of anti Howie view.
So I really like this paper from Axel Constant and Andy Clark and Carl Friston which essentially tries to broker some kind of peace in a debate that in some form or another has been raging for decades.
So it'd be quite nice to finally put this to bed.
So they argue that we can find, in the formalisms of active inference, we can find grounds for an armistice here.
So they argue that the formalisms of active inference show that, quote representational and non representational cognitive processes can be implemented by the brain under active inference.
And they do this by showing that active inference accommodates a dual architecture.
So they show that on the one hand, in some sense the brain has to represent because under active inference action selection policies are happening over extended timescales.
And so the agent has to engage in some kind of counterfactual thinking.
So if I do X, what happens?
If I do, y what happens?
And so on.
And the argument, very roughly speaking, is that that is going to entail some kind of representational, some use of representations.
But on the other hand, they argue that what they call deontic actions are processed through different mappings in the generative model and they're directly triggered by sensory input.
So a deontic action is an action that is ingrained by our sense of the expectations of others.
So the example that the authors use is stopping at some red traffic lights in the middle of the night when there's nobody else around.
So in a sense the claim is that we through a kind of social conditioning.
We know what other people would expect us to do in those circumstances and that allows for this kind of dynamic reflexive action policy setting.
And in those cases there's no room for representative or there's no need for representations there.
I would definitely suggest people interested in this go and read this paper because you might already have thought of some potential objections here to the claim that representations aren't needed for deontic action.
And they do go into some detail in talking about potential objections there.
So that's the kind of status of the role of representations.
It's a live debate, not everybody agrees.
But it certainly looks as though active inference has the formal tools to accommodate both sides of the debate.
And it might turn out that that's a good thing because we can make everybody happy or it might turn out to be a bad thing because you might think, well, we've not really settled anything one way or another.
I wanted to take some time though, to mention extended active inference.
So I've been talking about inactivism and I've mentioned that I've mentioned this debate between cognitivism and older ways of thinking about sorry, cognitivism and newer ways of thinking about cognition as embodied or embedded.
And these are part of what's known as the four e paradigm.
And clearly I think we are talking about inactive cognition and you get embodied and embedded kind of for free with this.
So the only one left on the table is the extended mind.
And this is just the claim that under some conditions the system that's realizing cognitive processing can literally extend outwards from the mind.
So I'll put some nice readings for this up if anybody's unfamiliar and interested.
But it's usually the most controversial of the four ease.
And Andy Clark, who is one of the original authors of the extended mind paper, really thinks that active inference accommodates the extended mind theory.
And it's not just that it accommodates it, but it actually solves some old problems in the extended mind about knowing how and why the brain knows when and where to recruit certain extraneural resources.
And part of the argument for this, what really sits at the core is Clark develops this argument based on the fact that the generative models in active inference are always in this, always making a trade off between accuracy and complexity.
So you want your model of the world to be accurate enough that you can act effectively on the world, but you also want to minimize the metabolic costs of having models that are overly complex.
And so Clark utilizes this insight to make the case that this explains that in some cases as agents we make use of these extra neural kind of props and tools and things.
And he says that the imperative to reduce uncertainty provides a location neutral cost function that's always been missing from the older arguments around extended mind.
So the upshot to this, I think, is that you end up with a view of agents as deeply coupled with their environments.
So under active inference it's just nonsensical to try and think about agents as decoupled from their environment.
The generative models themselves are of the dynamics between the agent and the environment.
So these two things can't be decoupled perception, action loops.
So remember I said that perceptual and active inference are essentially the same thing.
They're the part of this circular causality and they see the agent constantly producing their own worlds.
They occupy the states that they're expecting to through action.
And we might think, based on the literature that I've gone through, that there's some role here for representations.
So we're not committing ourselves.
The active inference agent is not one that fits with very radical interpretations of inactivism and active inference agents are porous.
They're open to recruiting external material features of the environment to reduce complexity and therefore minimize the metabolic costs of realizing those expected states.
Okay, so that was a bit of active inference set against the backdrop of some older questions in philosophy of mind theoretical cognitive science.
But I'm going to jump straight into agency and motivation now, because there's a lot to get through here.
I'm going to start with a dark room problem.
This is a canonical philosophical worry in active inference.
So a lot of people, I think, will be familiar with this.
And in really brief terms, it's really easy to state.
The challenge here is to say, if it's all about minimizing prediction error, why don't we just find the most predictable environments possible and stay there and live a nice error free life?
So Andy Clark paints a picture of a dark room, and you can just be strapped up to an intravenous drip, feeding you the nutrients you need, and you can just be really happy.
But clearly, we don't do that.
Active inference sorry.
Agents in the world, us, we generally typically sometimes we do enjoy dark rooms, but most of the time we don't for any extended period of time.
And so there's a challenge here about how active inference can account for what appear to be motivated agential actions that necessarily mean we're confronted with some degree of uncertainty.
And I'm going to set active inference up in a kind of dialectic with folk psychology here, because folk psychology faces no such concerns.
So, to be very clear, folk psychology is just the kind of propositional linguistic framework that the folk use to talk about and describe motivated action.
So it's a language we typically use every day to describe the reasons people are doing things.
So folk psychology has this belief, desire, intention, schema, where, if I want to explain, my friend going to grab a beer.
I would say, my friend believes there's beer in the fridge.
My friend desires beer.
And so those are combined to form the intention to go and get beer.
So folk psychology doesn't have these kinds of problems.
So just a little bit more on the dark room problem and another whale.
So the dark room problem invites more than one answer.
We're going to see more than one answer to it as we go along.
One response is to talk about what drives curiosity and play and exploration, and we are going to see more of that.
But the first response offered by Fristen, Thornton, and Clark is to highlight the fact that organisms have basic needs to eat, drink, and reproduce what you might call evolutionarily ingrained needs, and that those needs are going to necessitate some form of exploratory behavior.
So staying in the dark room will inevitably, under most circumstances, going to lead to physical dissipation.
So the question is, are these kinds of appeals enough?
And I wanted to pull up a paper by Colin Klein 2018 that I think is like a really nice articulation of these challenges.
Ultimately, I think he's wrong and it falls flat, but I think it's a strong articulation.
So Klein argues that predictions alone simply can't account for the motivational character of these needs.
So Klein says, look, it's fine that you've got these needs, nobody's going to deny that, but you just have this one primitive to describe how those needs motivate you, and prediction is just not going to do it.
So prediction alone, as your sole primitive is not going to be able it's like predicting is not the same thing as being motivated by something.
And Klein is really not happy with appeals to these phenotypic needs.
And he construes these in the relevance of the evolutionary history of the organism and goes into a little bit of detail.
So he says, look, these expectations are either going to be about states, possible states, so the state of being fed or clothed or warm, or they're going to be about state transitions, about what the appropriate action is in one state to move to another state.
And he says expectations about states themselves aren't going to work because you have this problem of accounting for novelty.
So it might be the case that in my evolutionary past, all of my ancestors have had the same expectations about which states they're going to occupy.
But there are lots of states in my life that I could be motivated to occupy that there's simply no evolutionary history to account for.
And he says there's a very similar or similarly difficult problem with state transitions, where essentially what we're talking about with state transitions are the most appropriate actions.
So I have expectations about the actions that are going to fill foot that I need to take under certain circumstances that are going to get me to a particular expected state.
And Klein's worry is that there's just going to be too many possible actions in any given situation.
So there's just a kind of explosion that's just going to be unaccountable for.
There's just going to be too many different ways of getting to an expected state that we won't be able to account for in our past.
So it's fairly robust, thorough, challenge used, built on the dark room problem.
And I think actually it's a worry with two faces.
So I think the first is about how active inference can describe agential behavior with its austere, purely docsastic landscape.
So if you imagine somebody looking at their house burning down, it seems like you need more than beliefs to explain any course of action they take.
So there has to be desire in there somewhere.
So this is an example Andy Clark uses in one of his papers.
And he says the desire to run in and rescue some item that you want from the house or the desire to claim on the insurance is the thing that's going to.
Dictate how you explain the person's behavior.
So that's the first problem.
We need to look at how folk psychology tessellates with prediction.
And the other problem is maybe a little bit, maybe a little bit deeper, and that's to ask, how does active inference actually account for agential behavior?
So can we even tell some story using just predictions and prediction error about what really moves agents to act?
So firstly, thinking about folk psychology, there's a couple of ways to go here and I think ultimately we can answer this worry.
So Ryan Smith and colleagues in 2022 in their paper active inference models do not contradict folk psychology.
They argue that the formalisms of active inference can accommodate a belief desire distinction.
Now, I'll admit that it's a very math heavy paper and it's a little bit beyond my capabilities, but the claim here is simply that there's no contradiction.
That actually the worry that the important thing we have in folk psychology is desire.
And we're not getting that in active inference.
This is just an unfounded worry.
But there's another strategy as well.
So Joe Deuhurst, in his 2017 paper, he employs a different strategy and he says, look, what are we even doing with folk psychology?
Let's not kind of be too hasty and we need to think about what are we doing when we talk about these propositional kind of states.
So he says the first option is to say, well, we're just being realists here.
So when I say that X desires Y, I'm actually positing the real existence as a fine grained mental state in X's head.
And he thinks that this is actually the wrong way to think about things.
He thinks that folk psychology actually it's an instrumentalist tool.
It's a kind of framework that we use just for picking out coarse grained behavioral patterns in some useful way.
So if we agree with Deuhurst here and we just become instrumentalists about folk psychology again, it looks as though there's different we're not forced to kind of worry that there's some kind of empirical problem here where the mechanisms of active inference are in some sense incommensurable with real properties that we're positing the existence of.
Because we're just going to say that those things don't really exist in the way that we've typically assumed that they might.
So either way, whichever route you go down here, it looks like active inference agents have beliefs and that we're going to be fine in describing motivated action.
So what about the second problem?
So Andy Clark in 2020 authored a direct response decline.
So Clark wants to argue that actually using predictions and prediction errors and precision weighting, which I'll explain in a second, we can get a picture of agential action.
So the first thing Clark does is to outline the fact that under active inference, we have a very nice, well worn account of embodied action.
So, in other words, what it takes to actually move the body to perform different actions is perfectly folded into active inference under the same kind of computational flow of prediction error minimization.
Just in a nutshell, for me to move my arm, what I'm doing is making a prediction that my arm is in someplace and then minimizing the prediction error.
So once that's on the table, Clark says, okay, that's fine.
We have this account of action, but that's not really what Klein means.
So the move Clark makes is to appeal to the hierarchical nested structure of the generative model and to say that higher level beliefs, which are about temporally extended future states, these are going to perform a really important role as a kind of controller for lower level actions.
So the idea is that if I have a high level temporally extended expectation to attain a PhD, what happens is that high level belief becomes unpacked at lower levels in the hierarchy.
And what you get are more fine grained, faster action policies that the system expects to help support that higher level expectation.
And there's a mechanism in play here called precision weighting, which applies to prediction error signals because obviously not all of the predictions we make about the environment are going to be equally important.
We're not going to have equal confidence in all of our actions to bring about an expected state.
And so what precision waiting does is essentially communicate how much confidence we have in a particular action policy to move us into that expected state under perceptual inference.
Precision waiting just signals how newsworthy prediction errors are, so how important they are for the system.
So precision weighting drives attention, for example, when we're talking about predictive processing.
But in this case, Clark's claim is that when you have this kind of nested hierarchy of expectations, so you have temporarily extended expectations at the high level, unpacked into a kind of web of lower level action policies.
And precision waiting is kind of modulating the system's expectations about which of those policies are most likely to bring about the expected state.
You get something that looks a lot like motivated directed action.
And agency is kind of inherent in this model because the system rather has to model itself as a cause of those expected states.
So if I have these high level expectations, I, in a very inherent way, have to model my own cause or efficacy to bring about those states.
So on a very low level, the system has to model some form of agent because I can be the cause of change to my own sensory states, so I can cover my own eyes and so on and so forth.
So kind of on this view, agency is a latent variable in these systems when it's making these predictions.
So it just kind of turns up.
And Clark argues that you essentially end up with something that looks very much like agents on the folk psychology.
But he thinks this is a better view because we end up with a, quote, different and arguably much more unified internal architecture trading in a single currency.
So Clark thinks, look, okay, Klein's worry is we have this one primitive.
We have just prediction, and the concern is that we're not going to be able to explain agential action.
But actually there are ways that we can appeal to the hierarchical structure of the model and precision weighting.
And it turns out that this is a much more elegant picture as well.
But Klein might press the worry, and anybody listening might kind of think, well, we've kind of done some work there, but why do we expect the things that we expect in the first place?
So why is my long term expectation to attain a PhD and not to join the French Foreign Legion, for example?
What means that I expect one thing rather than another?
And here Clark just bites a bullet and says, nobody can explain that.
We've kind of shifted to talking about questions about spookier, questions about free will.
And Clark says, look, if this is what you want, if this is what you're demanding, then you have to recognize that simply appealing to folk psychology doesn't get you that either.
People with these kinds of worries need to be careful in how much they demand from the active inference agent.
And I think the real answer to that question actually, this is more speculative, but the real answer to that, a deeper answer to these kinds of questions will lie in the kinds of things that we're going to look at in subsequent weeks.
So, for example, the reason I have the expectation to attain a PhD rather than join the French Foreign Legion is because I exist in a particular kind of sociocultural landscape with specific norms, and a lot of my actions and beliefs are tempered and shaped by the expectations of people around me.
So this is just an overview.
We went over the dark room problem.
Why do we do anything at all if it's all about certainty and prediction error minimization this worry?
I said you can kind of highlight two aspects of this worry.
One is an actual account of agency under active inference, which we have.
We have this Andy Clark model of agency to do with hierarchical nested, unfolding action policies over time and different precision mappings.
And then we have this worry about the relationship with folk psychology.
So how do we describe agential behavior, motivated behavior?
How do we describe the behavior of agents if we only have recourse to prediction?
And as we saw, there's one argument here is that actually active inference, the formalisms can accommodate this belief desire distinction.
And the other move that I really like is to just say, look, folk psychology is just this instrumental tool.
It is, in effect, it's part of the sociocultural landscape that we use to shape each other's expectations by putting these labels to explain our actions.
It's a kind of form of active inference itself.
So there's this kind of philosophically interesting debate about folk psychology, but it turns out that everything's okay and active inference agents are in fact agents with beliefs, desires and hopes and dreams and so on.
Okay?
That's agency.
We're making good time.
Okay?
So in this section I'm going to introduce quite a lot of stuff, but it's going to be all tied together hopefully rather neatly in the case study.
At the end, I'm going to talk about alistasis and emotion.
I'm going to say very briefly a little bit about the self as well.
Here's some examples of some of the core readings I've used.
Again, these are all going to be up in the document as well, which will be made available.
And I thoroughly advise you to just have a browse through these.
There's some really interesting stuff there.
I'm not going to talk about psychedelics or meditation or depersonalization, but there is some really cool active inference literature on those topics as well, particularly papers on psychedelics and ego dissolution by George Dean.
A new paper just came out by Sipolito jonas Mago and Robert Carhartt harris and I think Fernando Rosas as well.
I think I might be mistaken on the authors there, but it's a very new paper.
Okay, so the, the thing that is going to underpin a lot of what I'm now going to go on to say is going to be this notion of alistasis.
And this plays a major role in a lot of work on emotion and the self in active inference.
Lisa Feldman Barrett, whose theory of emotion we're going to look at in a second, has argued that the brain's fundamental purpose, what it evolved to do through that imperative to minimize prediction error, is to manage the body's metabolic budget.
So the first prior is kind of our most expected state.
The first prior in the sense of our strongest prior, prior probability, our highest expectation is to have good allostatic control.
But I'm getting ahead of myself there.
All that means is that we're balancing our body's metabolic budget or like energy budget in an effective way.
So error signals, like I said before, that are highly precise are those that are weighted very highly.
So prediction error signals that have high precision weighting are the synaptic gain on those error signals is going to be increased.
And that's going to drive learning, which means in future our expectations around those kinds of error signals is going to be we're going to pay more attention to those.
And the most highly weighted error signals that we're going to get are to do with our own homeostatic set points within our own body.
So if my temperature started to rise rapidly, my body temperature started to rise or drop rapidly, that prediction error signal, the violation of those expectations is really going to grab my attention and I'm going to do something about it.
And one way that we maintain these homeostatic states is through the body's own autonomic reactions.
So were I to start getting very cold, I would probably start to shiver.
And so the body is trying to kind of rectify those deviations from homeostatic set points.
But another thing that organisms engage in, and humans do this to really remarkable degrees.
And again, this is going to be something that I suspect is going to come out in the subsequent weeks, is that we engage in quite complex and sophisticated anticipatory actions to maintain homeostasis.
And this is something called allostasis.
So allostatic control, which I mentioned earlier as this first prior, is essentially the regulation of homeostasis through action.
So it's actions that I can take to ensure that I stay within those important bounds.
So shivering is good, but living in a house with central heating is better.
Having a warm bed and living in a community where we've had these people of kind of engineers have worked to create a kind of material environment that acts as a kind of background support to a lot of these needs.
Something that's another important concept here is allostatic load.
So this is essentially the result of not having good allostatic control.
So again, allostatic control is just engaging effectively in actions that are maintaining those set points, whereas allostatic load is the physiological costs to the agent of losing that control.
So there's so many studies here that have just shown the immense physiological costs of operating beyond those set points and not having good control.
So allostatic load has been linked with reduced cardiac health, sleep problems, gut issues, mental health issues, depression, basically everything.
It's really, really bad and so good allostatic control is very important and as we're going to see, it, underpins quite a few very important things.
So really we could dedicate a whole hour to looking at emotion, but I'm going to blast through it very quickly right now.
And this slide, I should say one of the core readings on the document and the reading that I'm basing this on is Lisa Feldman Barrett's 2017 paper, the Theory of Constructed Emotion.
The main idea here is this is one of those theories that on the face of it, it seems quite simple, but actually there's quite a lot of subtlety and nuance to it.
Allostatic control means inferring the causes of changes to the body's internal state.
So in other words, if I want to keep good allostatic control, I need to know not just what's going on out there, but I need to know what's going on inside me as well.
Okay?
So if there's some crucial change happening, I need to be on top of it.
And this happens through exactly the same process as the same predictive error, minimization process that I'm engaging with with the outside world.
So the brain is making predictions about its own internal states and then it's going to engage in some kind of action to minimize any prediction errors.
And these prediction errors are going to be very highly weighted.
They're very important.
But here's an interesting thing.
So there was a very unethical experiment done by Shaktor and Singer, I think in the 60s, who they were emotion researchers.
And they conducted an experiment where they injected subjects without the subject's knowledge.
I think they injected them with adrenaline and then they exposed the subjects to different stimuli.
So some subjects were exposed to one type of stimulus and some subjects were exposed to another.
And even though the subjects had essentially been pushed to have the same, to experience the same physiological changes in their body, they reported different emotional, different kinds of feelings, depended on depending on the stimulus that they'd been exposed to.
So this highlights something really important.
And again, I feel compelled to say that I am moving through this incredibly quickly.
There's so much to say here.
But the nub of it is this in order to keep good allostatic control, in order to know what to do, the brain needs to contextualize those changes in the body because there's not a one to one mapping between changes in the body and an external stimulus.
So sometimes changes, physiological changes in our body are about something in the world, they're a response to something.
And it's very, very important that the brain is kind of taking a confluence of those inputs to try and contextualize, okay, what's actually happening and what do I need to do?
So an example of this is if you imagine running into a bear while you're hiking in Wyoming, which is something that's actually happened to me, I've experienced it, it's terrifying.
And you imagine going on a scary or a date that you're really nervous about.
The physiological changes that are happening in the body are, give or take, more or less the same.
So perspiration, elevated heart rate, breathing changes.
So the underlying physiological footprint is very, very similar.
But what you need to do to manage that situation, the apt action, is very, very different.
And so the claim here, the kind of revolutionary claim in a way, is that emotions, the things we experience subjectively, are constructions, just like percepts are.
So just like my experience of the car when I step outside my flat, is a construction that explains away the prediction error and gives me a cleaned up picture of the world.
Emotions are essentially doing the same thing.
They're explaining away these prediction errors and telling me this is the best explanation of these changes within your body contextualized against this external stimulus.
So in the case of the date, the apt action is to chill out and be normal to a certain extent.
And in the case of the bear, you get the hell out of that.
Although, interestingly, you don't.
Running away from a bear is a terrible idea.
You should not turn your back and run away, but you get the idea that it's going to be, what you should do in each situation is completely different.
So that's emotion for the active inference agent and something I don't want to lose here is what you get with this is really kind of cool because it turns out emotions and subject we're kind of moving into subjectivity here.
It turns out emotions are really they're just part of the same cognitive architecture.
So there's no difference here.
There's no dualism between his thinking and his emotion.
And emotion is this kind of fuzzy stuff that we're not really interested in.
Actually emotion, subjective embodied feeling is core.
It's absolutely vital to understanding how we maintain this allostatic control and how we engage in adaptive behaviors and how we remain gripped to our affordances in the environment.
And the notion of grip is going to be something that comes up again very soon.
So very briefly, because this is something that I've really already talked about, but I want to put a slightly different spin on it right now.
I've already said that allostatic control for humans can involve very, very temporarily extended timescales.
And we saw this with Andy's model of agency.
So if you think about something like saving into a retirement fund, for example, we're doing something that the idea is that maybe decades from now is going to kind of minor spikes in prediction error called the disappointment of not having as much money right now is going to pay off later.
These are the kind of things that humans engage in and our generative models have to be temporarily thick.
So George Dean writes that the levels of temporal extension are hierarchically interlocked.
So shorter timescales inform expectations of what the agent can do over longer timescales.
So these different levels of the hierarchy are in a kind of dialogue with each other telling you what kinds of things are possible and what kinds of things you should expect.
And when we were talking about agency, we talked about the way that agency is kind of baked into active inference because in order for any of this to work, the system has to have as a hypothesis, I can do things okay?
And this is more or less taken to be the same thing that underpins the sense of self under active inference.
So the system implicitly infers its own ability to bring about intended sensory consequences.
And so Dean writes here, quote in Pristine it is in this sense that implicit in a model of sampling is a representation or sense of agency.
So a minimal sense of self, it's kind of like the flip side of agency comes baked in here.
So one has to model oneself as a potential cause of sensory inputs.
So George Dean and colleagues, I think George Dean, Mark Miller and Sam Wilkinson propose a model of the self under active inference called the allostatic control model.
And the move they make here is that we have to recognize that it's not just our self efficacy that has to be modeled.
We also have to have some kind of idea in there of how we're actually doing.
So we have to model our own capabilities in order to set precision waiting on future action.
So we don't simply infer that we can be the cause of sensory states, but also infer our own capabilities to successfully bring about expected states at longer time scales.
So the phrasing that they use is tracking the performance or fitness of the model over time.
So part of that tracking is going to be affective and emotional.
And so there are strong links.
We're kind of really digging our hooks into subjectivity at this point.
But the claim with the Allostatic Control model is you get this rather than just a minimal self, you actually get a longer term phenomenological self emerge from these interlocking timescales, which are modeling not only the agent's own causal efficacy, but also modeling how good your own model is at doing what you need it to do.
And like I said, I said when I was talking about the reading and I want to flag this again because I'm really not doing justice to this in the way that I should.
But this has formed the basis for a lot of really interesting work on phenomena like depersonalization on the role of psychedelics and the potential for psychedelics in therapy and in meditation, ego dissolution and kind of that kind of stuff in meditative settings.
So anybody with any interest in the self or in those kinds of practices should definitely go and give a closer look to these readings that I'll make sure in the document.
Okay, so quick overview here.
So active inference agents maintain homeostasis through anticipatory action, and this is something we call Allostatic Control.
To do this, the system must engage in interceptive prediction over its own internal states.
Actually, I think I've got time to do this, so I'll just deviate slightly.
The topic of interceptive inference is really interesting in the context of active inference.
So I want to flag work by Sarah Garfinkle, who I believe is at UCL, who used to be at Sussex.
And Professor Garfinkle's work looks at the central importance of interceptive predictions in various psychopathologies, especially, I think she's looked very closely at PTSD and anxiety disorders.
And it turns out that an agent's accuracy in their interceptive predictions is a good indicator as to the kinds of symptoms that they're going to like, the strength and severity of their symptoms in these kinds of disorders.
And what she's shown is that if you can increase the accuracy of interceptive predictions, this forms the basis of treatment for these symptoms.
So in short, the better you are at interceptive predicting.
I think one of the measures that they use there in her lab is how well people can track their own heartbeats without feeling their pulse.
So if you just sit there and try and say, like, my heart's beating like this, then the more accurate you are and the more any training you can be given to be more accurate, it forms the basis of effective treatments for these kinds of symptoms.
So I just wanted to flag that because the world of research in interceptive inferences large, diverse, vibrant, super interesting and very much worth our time.
Okay, where was I?
Here.
Okay, so to maintain all the static control, the system has to engage in interceptive prediction over its internal states.
And then to best explain changes in those internal states, the system has to contextualize those changes against some extraceptive input and that emotion categories serve that role.
So just to be very clear, on this view, emotions are constructions.
There are some important deviations from other theories of emotion here, particularly what Barrett calls the basic theory of emotion.
So on her view, emotions themselves have no underlying kind of essence or neural footprint.
And there's an ongoing live debate there as well between these theories.
And also just at the end, the good allostatic control in human agents is this very temporarily thick phenomena.
So we engage in very long term planning that involves a lot of the kinds of counterfactual representations that we talked about with the axial constant paper and this is hypothesized to form the basis of our phenomenological sense of self.
Okay, we're up to an hour good.
We're running on time.
This is the final section and it's probably going to be probably going to be the longest section.
What I want to do here is I want to try and pull together some of the things I've been talking about because it's been very fast and very abstract and I want to try and pull some of these things together in a slightly more concrete example.
So the case study I'm going to use is addiction.
So there's a particular account of addiction in the predictive process in literature that I think in a very succinct, sharp way highlights a lot of the things I've been talking about and puts them within a context that is quite stark and easy to understand.
But in order to do that, I'm going to have to introduce a new, slightly more difficult to understand mechanism within the framework that crops up in a lot of the predictive process in literature.
And what this is going to do hopefully as well, is give us a direct bridge to phenomenology.
So when we talk about things like neurocomputational phenomenology, the things I'm going to be talking about here have underpinned a lot of the work in that area.
So these are some of the readings we're going to look at some work by mostly a lot of the papers we're going to look at.
First author mark Miller or Mark Miller's been second or third author.
And I bring that up because in the discussion section on the 25th when we have an hour to kind of unpack and answer people's questions we're actually going to be joined by Mark Miller.
So if anybody has any specific questions about the things I want to talk about now in this section, he's going to be the man to answer those.
But also I'm going to put a lot of secondary readings up again in the document so you'd be able to find them all there.
Okay, so let's switch gears slightly.
I want to keep the focus on feelings, although what I'm going to be talking about here are not what might be called like full blooded emotion states, but I want to talk about something a little bit more subtle.
So one thing that might seem really obvious, kind of so obvious that we often overlook it when things are going okay, is that it generally feels like something to be an agent.
So when we're going about whatever business we have in the world, we always feel like something.
And it's not entirely clear how best to characterize that.
So I want to draw on some work in phenomenology that some active inference theorists have also drawn on and made use of.
Because I think this is a really good grounding for some of the things I'm going to go on to say.
Certainly proved to be a good grounding for some of the work that I'm going to highlight.
So these are two books by Matthew Ratcliffe.
I think he's at the University of York and has done really, really cool work in kind of really getting to grips with the what it's likeness to live in certain conditions.
And Ratcliffe, he argues that he's identified this certain area, this space of feelings that have been largely neglected in the wider literature, and he describes these as existential feelings.
So he says existential feelings are, quote, they are not intentional states directed at however many objects, and they are not feelings of the body or some part of it.
Instead, they amount to a felt sense of belonging in the world.
So the main difference between a full blooded emotion state and an existential feeling is that the emotion state is about something.
So if I'm disgusted, I'm typically all the time I'm disgusted about something in the world.
So emotions are intentional in that way.
Ratcliffe elaborates on existential feelings by I'm going to quote this at length because I quite like it.
He says people sometimes talk of feeling alive, dead, distant, detached, dislodged, estranged, isolated, otherworldly, indifferent to everything, overwhelmed, suffocated, cut off, lost, disconnected, out of sorts, not oneself, out of touch with things, out of it, not quite with it, separate, in harmony with things, at peace with things.
There are reference to feelings of unreality, heightened existence, surreality, familiarity, unfamiliarity, strangeness, isolation, emptiness, belonging, being at home in the world, being at one with things, significance, insignificance, and the list goes on.
So this is really, really cool work and I recommend everybody to go and read Matthew Ratcliffe.
But some of the work.
I really like in predictive process, in an active inference, has taken this idea of an existential feeling and what they've recognized is that these kinds of feelings are really about how we are orientated in regards to the world and the affordances in it.
So they're background bodily states of action readiness.
So how are we relating to how do we find ourselves in relation to the world?
So we're talking about the general structuring that's present in our everyday phenomenology.
So I really want to make that clear that what this work is doing is building a bridge between computational models and the way even the underpinnings of those models in neurobiology and phenomenological experience.
So the thing that I need to introduce, and this can be, it's fairly straightforward, but it can be a little bit tricky at times, is this notion of error dynamics.
This is really important.
So like I just said, aerodynamics is going to provide this conceptual bridge between different levels of analysis and it elaborates on how systems are sensitive to the success or failure of their own action policies unfolding across multiple timescales.
So the recognition here is that we talk a lot about action policies and expected states, but the reality is that we have a multitude of goals and action policies and expected states that are interlocking, overlapping, constantly shifting.
The picture is messy.
And so I like this quote from the 2019 paper that says the roller coaster of continual increases and decreases of errors that accompany life become expected and are folded into our expectations.
For such a system it becomes important not only to track the constantly fluctuating instantaneous errors, but also to pay attention to the dynamics of error reduction over longer time scales.
So the systems, the upshot of this is that these authors believe, building on some other work in computational modeling, that systems must track the rate of change in overall error reduction relative to the system's expectations.
So this is the kind of global upwards or downwards trend in prediction error.
That's what the system is sensitive to, how that relates to the expectations.
So the idea here is that aerodynamics provide a system with a much broader ongoing sense of how they're doing to their multitude of goals over time.
So a big step here is that those changes in the rate of reduction feed back into the system.
So they're folded back within the system and they serve the role of modulating precision on action policies.
So let's say I'm engaged in some task, I have a particular expectation about how I'm going to do at that task and when I actually engage with the task, I do slightly worse than expected.
The idea there is that that rate of change, so the rate of error reduction is going to decrease until I'm reducing error worse than my expectations.
And in a typical healthily functioning system, that rate of change, the sensitivity to that rate of change is going to feed back and reset the precision weighting on those actions.
So it's going to reduce my confidence in that particular action policy such that I might try a different strategy, I might try a different tactic, or I might just throw whatever it is that I'm doing away and go and do something else.
So we're starting to see this picture of through expectations, sensitivity to change and feedback onto precision waiting.
It's starting to dictate how I manage my action policies.
So there's this ongoing reciprocal relationship between aerodynamics and my future expectations.
Now, crucially, the claim here is that on the subject level, all of this manifests as these existential feelings of some kind.
So the way that I feel on a kind of day to day basis is a reflection of how I'm doing in terms of these sensitivity to aerodynamics.
So when I fail at the task and precision waiting is down regulated on that particular action policy, the thing that actually kind of that moves me to go and do something else are my embodied feelings of frustration that I experience.
So as conscious agents, those embodied feelings are what are tuning us towards acting more effectively.
So optimal behavioral on this account is characterized as a feeling of grip towards a shifting field of affordances.
So I'm going to unpack this quickly.
But the feeling of grip in a nutshell is just this feeling that we're engaging with our affordances in the best way that we can and that we're either meeting or exceeding our expectations for prediction error, minimization what they mean by a field of affordances.
And this is again really important.
A landscape of affordances is just the total opportunities in my niche.
So my landscape of affordances right now is Brighton.
It's not really going to change very quickly unless I get on a plane and fly somewhere else.
But affordances aren't neutral in that way.
They don't just exist there.
The affordances that stand out to me as relevant or attractive is going to change quite rapidly depending on what it is I'm doing, on my internal state, on the context and so on.
So right now my water bottle stands out as attractive, a couple of keys on my keyboard stand out as attractive.
But that's going to shift quite rapidly once this lecture is over and I turn to do something else.
So the idea of grip is that we exhibit a stability in our attunement such that we can remain attuned as that field changes.
And this is part of the skilled intentionality framework, which I won't get into because I don't want theoretical overload, but I've put some papers up on this slide as references and of course the reading will be in the document.
But skilled intentionality refers to the selective openness and responsiveness to multiple relevant affordances in a concrete situation.
So it's just this recognition that things change quickly for multiple reasons and we really need to exhibit a kind of flexibility and stability in those engagements.
And aerodynamics has been applied in lots of very, very cool ways.
I can't recommend this work highly enough so aerodynamics, I won't run through how everything has been explained just for time reasons, but you can kind of take a look at this yourself.
But aerodynamics has been proposed as a more complete answer to the dark room problem that we were talking about earlier, because it can explain the drive for exploration and curiosity, because it literally feels good to minimize prediction errors better than expected.
So again, that rate of change manifests as embodied feeling.
So we're always looking for opportunities to do better than expected.
And kind of this has been suggested to account for feelings of boredom, prompting us to go out and find new things to do.
And similarly, aerodynamics has provided a computational account of play, which has for a while has remained a kind of mystery because it's on the face of it like, why would we engage in play when it's metabolically costly and we need that energy for other things?
Like Lisa Feldman, barrett says managing energy is super important.
So why do baby animals burn?
It all engaging in playful behavior.
Aerodynamics has a story for us there.
And there are also aerodynamics based accounts of happiness and well being and depression as well.
So I won't go into the details there.
But it turns out that aerodynamics and this connection to phenomenology is this super malleable, flexible kind of feature of the framework that really underpins a lot of what we experience and do as active inference agents.
Okay, so I'm going to talk a little bit about addiction.
Got some time here.
Things are going well.
So I had some options here because I wanted to use a case study that really brought out some of the things I was talking about.
And as I've just said, aerodynamics has been applied in a bunch of different areas and it's all very much worth talking about.
But I chose addiction because, well, as you'll see, as I kind of make clear, I think it also brings out not just the way that all of this fits together, but also why it's good and why seeing people as active inference agents and broader structures as active inference structures is actually a worthwhile thing to do.
So addiction was once viewed as a moral failing.
So the kind of Victorian addict is too rotten to make the right choice, too self indulgent.
I say Victorian unless you're Peter Hitchens and then you still subscribe to this view.
But that's the old fashioned view.
There was something wrong with the addict, some moral or spiritual failing, and gradually that changed to what the dominant view is today, which is the disease model.
And the idea here is that addiction is a disease and that disease is characterized by changes in the brain, which changes to brain structures, which mean the addict can't help but act in a certain way when confronted with a certain stimulus.
So it's this loss of control that's engendered through these brain structures.
And this is the idea of it being a disease.
The work that I'm going to describe now, I won't go into too much detail here, but it's kind of founded on this thought that this disease model, it can't be quite right because these changes in brain structure, essentially that's what the brain does.
It's just learning.
This is what we do all the time.
Essentially, me completing my PhD is a process of me undergoing changes to my brain such that I'm kind of disposed to act in certain ways.
Mark Lewis I think it's Mark Lewis in his book 2018 The Biology of Desire, argues, look, addiction is more of just a dysfunction in learning.
It's not really a disease.
So we need to think about it a little bit differently.
And this work by Miller and colleagues, 2020, that's where it picks up.
Okay?
So the idea is that those brain changes engendered through exposure to a stimulus.
In this case we're going to talk about a drug.
We're going to leave behavioral addiction aside for a second that clearly plays a critical role.
So nobody's going to deny the kind of chemical, like the importance of the biology here that's going on in the brain.
And of course, different drugs have different chemical profiles in that regard.
But on this view we're going to kind of shift focus from that slightly and we're going to have a much more active picture where we view addiction as a self organizing process of whole agent environment systems.
Okay, so I have this extract here it is in the dynamic interaction between the agent and its environment that addiction is born and endures.
So the harm of addiction emerges from a breakdown in the broader stable dynamics that typically underpin a flourishing life work, family, friends, health, and so on.
So a typically flourishing life has a broad kind of bush of concerns over different timescales in different areas.
And what we're going to see is that the harm of addiction is in a narrowing of those concerns.
So what happens is when somebody takes a drug to some people who are exposed to drugs, that drug is going to, through direct action on some neural mechanisms, is going to signal to the agent that they've done better than expected at reducing prediction error.
Of course this is a losery.
They haven't really done better than expected in anything that matters.
But the signal they get is, wow, like, we have just reduced prediction error at this insane rate.
And repeated use through this aerodynamics mechanism, repeated use is going to tune our expectations about the kinds of prediction error slopes, the kind of slopes of prediction error that are available to us in the environment.
And we're going to start to expect those vertiginous slopes of error reduction if we continue to engage with the drug.
And once those expectations become deeply ingrained, anything else we do is going to start to feel like failure.
So if I get used to taking cocaine all the time and then I have to sit through a family meal where I don't have any drugs and I have to kind of sit still and there's no dopamine available, I don't have my phone.
I'm going to experience that as feelings of frustration and as if I'm being prevented from doing something I want to do.
And I'm going to experience that as doing worse than expected.
So I've learnt these expectations for prediction error minimization and I'm not getting them unless I engage with the drug.
And of course, it's worth noting that in many cases, these drugs are going to have chemical withdrawal effects that compound that as well, especially things like nicotine.
And alcohol in particular is very nasty.
So alcohol can kill you if you try and withdraw from it too quickly.
So they really do pull the agent into a grip.
So what happens here is the agent develops a kind of suboptimal grip.
So you start to enact a new world, a new identity.
So instead of doing all the things you once cared about, like different hobbies, you start to hang out with different people, you start to frequent different places, like a dealer's house or wherever, and very slowly you start to carve out essentially a niche.
The addict enacts a particular niche through their actions and they have quite a good grip on that niche.
It's just that it doesn't speak to a broader array of concerns.
So what's really important is that this pushes back against the disease model, because on this view, the active inference agent, well, the system is doing essentially what it was evolved to do.
So there's another quote from the paper here that I like that says their habits allow them to remain well attuned and keep prediction errors under tight control so long as they remain within the narrow confines of such a niche.
And this is important because it shows that allostatic control and prediction error minimization are not the same thing.
They come apart.
And this really speaks to a question that was asked after Avil session recently about what does prediction error actually look like?
Well, in this case, you have a system that has learned that engagement with certain affordances gives it an illusion, but it has this feedback that says, this is the state you expect to occupy, or this is the state that the agent has learned to expect to occupy.
And it's carved out a niche of affordances that means it's very likely to occupy that state.
So it actually has a really good grip on the situation.
It's just that that niche is very narrow and it's constricted, meaning it's simply those habits and that web of habitual action policies are just not well suited to maintain good allostatic control.
So everything else in the addict's life starts to suffer.
So we can kind of think of this as a form of niche construction gone wrong.
And I think we're going to look at niche construction in the coming weeks as well.
So why is this whole agent well, so I should say, just to back up a second.
So what I like about this account of addiction is that it's a thoroughly inactive account of addiction.
It really brings out the kind of inactive core of active inference.
So the addict is addiction becomes an identity.
It becomes a world that the person actually produces through their actions.
So rather than the disease model where you have a person and a stimulus and exposure, whenever they're exposed to that stimulus, they're going to have problems because of these brain changes.
What this does justice to is actually the activity of the agent.
So they're not accidentally bumping into this stimulus in the environment and they're not just seeking it out, but they're deliberately restructuring their environment to make this state more and more likely over time.
So it really does justice to the efficacy and the agency of the agent rather than giving that agency a more backseat role that it takes in the disease model.
But the other thing that it does, because it's this inactive picture, is that it also does justice to the environmental features.
So it kind of speaks to the fact that addiction is not just all in the head.
Actually, some of the problems of addiction come with the fact that it is this procedural, constructive process.
And one of my favorite experiments of all time is I don't know if people are familiar with the Rat City experiments.
This is why I've got the image up here.
And this was a very cool experiment where originally it was found that if you exposed rats to a drug laced stimulus in the environment, I think it was heroin laced water, or it might have been cocaine.
But the original experiments found that nearly all the rats would kill themselves with the drug, so they would just keep at the drug until they died.
And it was a very high percentage of the rats would engage in that destructive behavior until the experiments were rerun some time later.
And instead of having the rat just in a boring cage on its own, it had the rats in Rat Park, which was a very social, exciting, brightly colored environment with lots of different affordances.
And what they found was that given the same choice, a vastly reduced proportion of the rats would choose to engage with the drug.
Okay, so it's capturing the importance of niche construction that I think is really nice here.
And it also speaks to the role played by embodied affect in phenomenology.
There's something that it's like to be an addict and that really matters.
It really matters that the addict has these embodied experiences when they're engaging with particular sets of affordances so that's really summed up in the overview that I've put here.
So these were the things that I wanted to emphasize.
Active inference agents are embodied.
They're embedded in a landscape.
That landscape is constituted by particular affordances.
And we can distinguish between a landscape of affordances and a field of affordances.
And that distinction really speaks to the inherent affectivity and normativity of affordances, which affordances stand out to us at a particular time, which exert a pull over us.
And active inference agents enact their worlds through action.
And this is nowhere starker than in the case of addiction.
Active inference agents are effective.
So emotion plays this super crucial role, keeping us tuned, keeping us in that grip with our affordances, and helping us contextualize those important physiological changes.
Active inference agents are agents, okay?
So we do have a way to describe behavior as agential and motivated, and we do have these cognitive architectures that can make sense of agential action.
And active inference agents have this inbuilt sense of self that comes with agency as well.
So that's pretty much it.
This final slide just goes over some of the things that we can expect.
So what I've done today, I hope, has laid a good groundwork and has installed some higher level expectations to learn more about active inference.
And what we're going to do in the subsequent weeks is now scale this up to see how when you have more than one active inference agent together, how the expectations of some can impact the expectations of others and individuals.
And I look forward to learning more about that.
But as for me, that's it.

1:29:55 _Speaker B:_
Amazing.
Ben, awesome work.
What can people look forward to next week or in two weeks if they join the discussions section?
Like, what's it going to be like or who would you encourage to join?

1:30:08 _Speaker A:_
Definitely, I mean, the more people that come along, I think the better it's going to be.
I'm really looking forward to it.
I'm kind of just in the first place, intrigued to see we covered so much ground.
I'm just interested to see where people want to go with this and see where people's interests lie.
I assume that people are going to have questions, and I think maybe the best way to do it would be to explore some of people's questions during the discussion.
We can use them to kick things off and keep things going, but I think it's likely that we have a whole hour, so we've certainly got time to really dig into things in some detail.
And like I said before, we're going to have Mark Miller with us.
And Mark is just awesome in discussion, very passionate and enthusiastic about his work, and he can do a much better job than I can in explaining it.
So just to have the opportunity to speak with Mark about his work, I would say definitely come along.
All right.
Anything else?

1:31:13 _Speaker B:_
Otherwise, again, awesome job.

1:31:15 _Speaker A:_
No, I think that's it.
Thanks for listening, I guess.
Well, thanks.
It was a pleasure.

1:31:20 _Speaker B:_
It was ambitious to do what you did elegantly with no equations or getting snagged on any of the potentially infinite philosophical and technical and normative and all these different layers of challenge and diversions.
So to really cut a path there is great.

1:31:46 _Speaker A:_
Yeah.
I think it just speaks to there's so much good philosophical work in these areas in active inference and predictive processing.
And I should say, again, we didn't really do justice to any of the work I talked about.
So the final thing I would say is definitely the document is there with the reading, and please go and actually look at the work of these people, because some of it's really good.
All right.

1:32:13 _Speaker B:_
Till next time.

1:32:14 _Speaker A:_
Cool.
Thank you very much.
See you.

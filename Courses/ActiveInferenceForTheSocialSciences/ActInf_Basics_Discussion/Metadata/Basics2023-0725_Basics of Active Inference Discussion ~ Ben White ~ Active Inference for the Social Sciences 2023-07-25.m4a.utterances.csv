start	end	speaker	confidence	text
420	970	A	0.60527	You.
5180	5896	B	0.98482	All right.
5998	55240	C	0.9425457575757573	July 25, 2023. And we are in the discussion section, basics of Active Inference. This is the first discussion section that we've had for this course, so thank you all for joining. And it's going to be regarding the topics that Ben White introduced in his recent lecture. So everyone will be welcome to pop in and introduce themselves. And then I know that Ben has some ideas to discuss and many other spontaneous and written questions will come into play. So with that exit for now and pass to perhaps some of our first time live streaming guests.
67850	113430	D	0.9554101562500001	I guess I've already spoken, so I may as well continue. So I'm Darius. I am Master's student. I'm just about to finish at UCL in sort of social distributed cognition. I work in a social cognition lab looking at salience regulation and attentional mechanisms within social contexts, but all within an active inference framework, within the Bayesian brain hypothesis framework. And, yeah, it's a sort of recent discovery and obsession. And so I'm sort of getting to grips with both the high road and the low road. And so I've had the opportunity to chat to Mark and some other researchers who have been super informative, but always looking to learn more and get my head around even more of the sort of philosophical and mathematical theory.
115770	118600	A	0.9414433333333333	Cool. Sounds good.
124170	125880	E	0.9959483333333333	Hi, everyone. Can you hear me?
126410	127298	A	0.7531	Yep.
127474	180202	E	0.919169649122807	I'm Francisco Balsan. I'm a PhD student at the University of Bologna, Italy, actually working on intersection between artificial intelligence and education. I have a background in cognitive anthropology and philosophy of science, and I met Axel and Maxlin a few years ago. So I dive into the rabbit hole of active inference and currently pretty much interested in multiscale active inference models of scientific cognition. So pretty interested about the agent level modeling of scientific reasoning assumption that might emerge from the interaction with a scientific environment. So we're referring to scientific nature, construction and all these type of top down and bottom up interactions. So super interested to hear something from you.
180336	196310	A	0.9260094999999999	Thank you. Thank you very much. Would anybody else like to introduce themselves before we start? So you go first.
196840	254410	F	0.9331687142857145	Okay. Hi, I'm Regina Sagin. You can call me Gina. I'm from Guatemala. And well, I'm not from the social sciences. I come from biology and neuroscience. And I'm here because I'm working on a project as a technical operator in this big project in the Xscape and material minds. And I do the technical part and experiments, but I want to learn the philosophical part. That's the background that makes the theory of everything. We're going to experiment. So I'm here to learn. And also, I am like when you see the Olympics, that you see all the swimmers, and I wish there was a normal guy to see how good these people are so you could compare. So that's me today, right? I understand half of what's going on, but I'm happy to be here, so yeah. Hi.
261560	261924	A	0.98381	Sorry.
261962	327080	G	0.9447148275862064	I'm Leigh. So my name is Lee. I'm a PhD student at the University of York studying systems transformation. And I'm joining because I'm using perceptual control theory at the minute to model examples of effective practice in organizations. And I've read quite a lot of active inference papers, and I understand that there's a lot of resonance overlap between perceptual control theory and active inference, although I understand it's within a predictive framework. And also it's kind of a level of abstraction higher, so you're able to kind of quantify the difference between sort of the predictive state or the outcome state and the current state. So what I'm really trying to understand is how might that level of abstraction be useful in what I'm doing, actually, because I understand it's a much more kind of concurrent theory and framework than perceptual control theory.
330950	461980	A	0.970380063091483	Yeah, very cool. Anybody else? Is that everybody? Okay, well, has everybody seen the lecture that I gave a couple of weeks ago? Okay, so I think it might be helpful then, if I kind of very briefly go over what we covered in that lecture just to kind of jog people's memories, and then we can maybe pick up some questions from there. So the lecture last week was on the basics of active inference, and it was intended to be an introduction to the framework on a very abstract, philosophical level. So it left out all of the kind of mathematics and technical aspects of the framework. And the objective really was to lay the groundwork for future weeks in this course, because this is obviously a course on active inference in the social sciences. And so in the subsequent weeks, we're going to be looking at topics like collective behavior, social cognition, shared norms, and niche construction. And so what I wanted to do was to put on the table a kind of fully fleshed out picture of what the individual agent looks like in active inference. And so to do this, we looked at emotion, agency, mind, so we looked at kind of the role of action and perception, the role of internal representations in active inference. And then as a case study, we looked at an active inference based account of addiction to kind of bring all of these threads together, to kind of articulate a lot of the things that I'd said in the previous sections. The questions here don't have to be specifically tethered to that lecture. I suppose we can just start with any general questions about active inference and philosophy and how active inference applies to individual agents, or if anybody did want to pick up on any threads from the lecture, then we can go from there.
466210	469858	F	0.8681622222222223	Can I ask something just for the basic questions?
470024	471140	A	0.9695366666666666	Yeah, of course.
471830	490360	F	0.9207858333333336	I don't understand because I think you explain it later, but how does creativity and science in general enters active imperents? How does trying to be creative and not doing what you predict enters this room?
490810	497210	A	0.9998108333333332	Yeah, this is a really good question. Did you see the lecture?
498670	501100	F	0.9837442857142857	Yeah, but I remember half of it.
501630	506620	A	0.9598609999999999	That's fine, but do you remember the dark room problem?
507090	510000	F	0.9993462500000001	Yeah, I know that's where the answer is.
513090	565760	A	0.954410597014925	I think there are several interesting dark room problem, some of them more interesting than others. And I tried to cover I obviously didn't have time to cover all of those in the lecture. But for those that perhaps aren't familiar, the dark room problem is this kind of canonical philosophical worry about predictive processing and active inference that says essentially, if all we're trying to do is minimize prediction errors, why don't we just find maximally predictable environments? And why do we not just lay in a dark room hooked up to an intravenous drip and enjoy kind of error free lifestyle? So I think that's the question that you're articulating there. I just quickly see a hand from Darius. Darius, did you want to jump in, or was it a separate question?
567490	571806	D	0.8567742105263157	It's a separate question. So I thought I would just do it in advance. But yeah, no, ignore me.
571828	869306	A	0.943694068027213	I'll circle a call background in a second then. But feel free, I should say. Actually, this is the first time I've kind of chaired a discussion like this, so I'll be explicit and say people should just feel free to jump in whenever they want. If you want to take the disco, if you want to kind of add something or kind of build on a question that we're answering, feel free to jump in. But, yeah, with the dark room problem, as I'm aware, the first answer to that question and this is something that I did cover in the lecture, is essentially a big part of active inference is to recognize that agents are it's a kind of fundamentally embodied framework, okay? So that can be kind of cashed out in various ways. But one of the most important ways is that the expected states of the organism or that the agent are necessarily rooted in the phenotype of the organism and the kind of evolutionary biological needs that come with having a particular body. So there was a paper that I referenced in a lecture by there's three authors on this paper. It's andy Clark, Karl Friston, and then I'm forgetting the name of the third author. If anybody can recall, feel free to drop it in the chat. But this is an answer to the dark room, where it builds on that notion of embodiment and says, look, creatures like us, human agents, could sit in the dark room and do nothing and just kind of enjoy the very predictable march of hunger, thirst, et cetera. But obviously, those particular biologists out of the room. And so you have this very fundamental level. We have this very fundamental kind of basis upon which we have these needs that need to be met. And so we need to engage in exploratory behaviors on the topic of novelty, because that's a kind of base. I take that to be a kind of baseline answer. But there are clearly kinds of activities and behaviors that we do engage in going beyond the dark room kind of curiosity play, creativity, engaging in kind of why do we create works of art? And there's not an obvious link between those kinds of behaviors and the kinds of biological needs that are emphasized in the original answer to the dark room worry. So in answer to Regina's question about creativity, I think that a good starting point for this is the section on aerodynamics that I introduced in the lecture. So this was the idea that there's this strong connection between affectivity as a kind of emotional, embodied feeling state and the changes to the rate in prediction error minimization. So I don't know. I am now very much articulating Mark's work here. So I don't know if Mark wants to jump in at any point and do a much better job than I can. I think there's certainly work on the horizon. I know that there are people who are currently building up theories of artisticity that are based on this idea of aerodynamics. So Mark's paper on play with Mark Anderson that I mentioned has a really interesting idea where if you explore playful behaviors which are kind of fundamentally creative and it's an inherently creative exercise, I think the really interesting idea that comes out of that is one thing that we engage in because it feels good is we create niches of very manageable prediction error that we can then minimize. So, yeah, that's playful behavior is the answer to the puzzle of play. So the puzzle of play is why do we engage in metabolically quite costly and they don't have any obvious benefit. And the idea there is that, well, there's kind of several ideas in there, but one of them is that it just literally feels good because minimizing prediction error feels good to us. So whenever we do better than expect at minimizing prediction error, it feels good. Obviously, I don't know for sure, but I think that any answer that we give regarding artistic creativity is going to fall within that kind of bulb that essentially we engage in. The part of the creative process is just constructing those that give us tasks of manageable prediction error that ideally fit at the boundaries of our skilled capabilities.
869338	1096790	B	0.9564902544910197	Can I add two quick things there, Ben? So that was a great explanation, but just to hit two points a little bit harder, one, you might think all that matters error. That's why it looked a little bit paradoxical that we also create antithetical to our modus operandi, which is to reduce error. But when you remember that for our kind prediction error minimizing system, we have a really deep temporal model. We're managing uncertainty at a big horizon, maybe even multigenerational, where we're thinking about our kids or our kids. I mean, who knows how deep the Generative model actually runs? It turns out that error over that stops developing error minimizing skills and abilities and just hangs out in one microniche, because that microniche is going to be upset sooner, right? I mean, it's such a great example of where we thought we were in a really set vector state and then suddenly it gets jostled and all of us go, oh my goodness, what do we do? We've been bumped. And it turns out that the best error minimize will be the two kinds of errors that are hates. Those errors are digestible. So that means not too complex that you can't do anything with it, you can't learn anything from it, but also not so boring that there's nothing to learn. If we hang out, if we're sensitive to and we hang out at the edge of our capability, then we keep developing new error minimizing abilities, which actually sets us up to be good error minimizing systems over the long run. So even though we're investing metabolism now, we're setting ourselves up to be able to manage, especially in the deep end of the pool, black swans manage uncertainty that we're not going to be able to predict. They're really unpredictable. Unpredictable that comes from hanging out at this edge. So part of our curiosity and playfulness and creativity are going to be about us making and digesting novel slopes of Volatility. I think that's one really great answer for why you get playfulness and curiosity out of this and creativity. I'll just drop one more and we don't have to get into it too deeply here. But here's one other one that we're thinking of. There's something really special in the creation of art, especially in a public sphere, that I think is so interesting and that somebody needs to be sort of looking at this. There's actually a new collection coming out in Philosophical Trans B on Art and Predictive Processing, which I think you should check out if you're interested in these topics. But the idea that we've been thinking about is there are ways that we can bring our Generative model out and put it into a public sphere. You can take something which is typically on the inside and you can put it outside, and then you can have other error minimizing systems look at it and fiddle around with it and then we can reimbibe it. We're doing this all the time. That's what language allows us to do and what writing allows us to do. Think about maths. You're bringing up part of your predictive understanding, you're laying it out and then other predictive agents can fiddle around with it and then you can take it back in as part of your updating your own model. And I don't know how much more I can say here. I just think this is a kind of horizon. But I think there's a really juicy thing to say here. About where real art is. Like Tolstoy. Maybe Leo Tolstoy was already on this, where you put something of the Creator in the Creation and then other people receive that and then they can do something else with it, and then you're able to sort of take it back in all under the guise of sort of Minimizing Volatility or understanding Volatility, maybe. That was a bit deep.
1097130	1110554	A	0.9213721875000002	No, that was really good. Yeah. So in the chat I just dropped a link to yeah, so somebody just asked about papers related to hanging out at Edge of Our Capabilities.
1110682	1111920	B	0.9328619999999999	I'll do that right now.
1112370	1255660	A	0.9455735264483635	There sure are some papers on that. Mark's got a few. I already dropped a link in there to an Aon article by some of the old Expect project crew, including Mark Kate Nave, George Dean and Andy Clark on the value of uncertainty, which is I think it's a really good start point for some of these questions. Just because Aeon is a kind of non academic place where, you know, it's a non technical introduction. And there's a wonderful example in that paper of a guy named I think it's Max Hawkings. Is that right? Mark? Max who? He's a kind of tech guy and he realized he was getting kind of bored with his life. He'd inadvertently trapped himself in a dark room because his life had become so rote and scheduled and systematic. He realized that he would be kind of incredibly easy to kidnap because he was in the same place at the same time every single day and he took drastic action to break out of that dark room. So he kind of introduced a randomization algorithm that it would kind of choose for him where he was going to eat, where he was going to go, which shows he was going to attend. And this example is a really nice centerpiece in this article. I think this is a nice place to start because it's also grounded in discussions about epistemic actions as well, because there's also as cool as play and art and creativity are there's also really good reasons that uncertainty is valuable as well? Right? Because while we might want to exploit all of the opportunities for kind of nourishment and valuable prediction error minimization in our environment, there are of course going to be times where those opportunities are exhausted and we need to go and explore different ones. And I think there's some stuff in there as well on epistemic actions within the context of navigation as well. So sometimes we're going to have to temporarily move further away from our target in order to minimize our uncertainty about where we're going. So there's lots and lots of stuff on the value of uncertainty and it kind of is very much at the forefront of some of the really interesting philosophical applications of the framework. For sure. Okay, by all means. Yeah, we've got time.
1256350	1287320	F	0.9301249411764706	I know you have a question that is but with the value of uncertainty. It's just that I'm thinking because I've been working a lot with the magicians and there's a way of using uncertainty as entertainment that it's very difficult to understand as a concept for entertainment. Why would you like to be in a place where you want to predict but you cannot, and you still want to participate in this activity? So for me, magic shows, it's kind of breaking a little.
1290730	1321300	A	0.9530823232323228	Spoke to I forget the person's name, but there was an Xcape meeting here at Sussex and I spoke to one of the guys on Escape who had been working on magic. I don't know if you know who I'm talking about, but I think they have a book on magic. And I think that's really cool to think about from a predictive processing perspective, the idea that magic is just these wonderful kind of violations of expectations. In a certain sense, I think predictive processing provides a nice, really intuitive framework for thinking about those kinds of things.
1321670	1479794	B	0.9598006153846163	One little point there, Regina. Just about again, if it feels paradoxical just to sort of complexify your view, remember that this is always happening in a hierarchical system. So just because you have errors at one level of the system doesn't mean you're going to have sort of critical, cascading, unbearable errors at higher levels. So that's why it's fun to go to horror movies. So I dropped the paper here. Also, we have a new paper coming out on horror movies. I'm pretty interested in this, but it's the same as going to the magic show in some way. Now, the reason why those errors are fun is because we're safe in a theater. We're with our friends. We have lots of sugar in our system from eating popcorn and candy. We can control the amount of scary by covering our eyes. There's lots of control here. And yet we're getting media that's pinging all of our evolutionarily ancient error tracking systems so that we're getting jumps in the physiology as if there's a bunch of volatility that we need to manage, and yet we're in a completely safe space, which is really fun for our kind of system because we're hierarchically deep predictive systems. The high level here isn't jeopardized. It knows I'm in a theater, but the low level stuff is still registering all sorts of literal volatilities. But of course, they get squashed. They don't cascade all the way up. Or if you're the kind of person where they do tend to cascade all the way up, then you're also the kind of person who doesn't like going to horror movies because it kind of scares you. Go home and think, oh, goodness, maybe ghosts are real and maybe I live with them. You wouldn't be going there then. So with magic, it's the same sort of thing with error. I don't know if you've ever who does the really extreme magic. David Blaine, where he was in a block. Yeah, david Blaine, have you ever seen him? He goes to Haiti where you have a community that really believes in magic, and he does some magic and he gets into trouble. Like suddenly all the young men are, no, no, that's no bueno. That you're doing this. And then the camera pans back and he, no, no, hold you here, I'll show you how it's done. It was just a trick. I'll just show you outside. It's not real magic because there they're not having fun with it anymore. They're like, no, that's not a good thing to do. So it just goes to show that our engagement with magic is mostly a sort of scary play, I think.
1479832	1540310	A	0.9502962275449098	One thing I just kind of sorry about the seagulls. I don't know if you can hear those. There are seagulls going crazy just outside my window. One thing I would add is, I think, another aspect to this part of the framework wherein you have this value of prediction error, kind of epistemic and effective emotional value of prediction error. It's one of the kind of key elements in this notion of agency get in active inference that I think is really going to be really important to in mind as you go through the further weeks of this course. As well, like I said in the lecture, it's not necessarily speaking to the metaphysical question of free will, but it's certainly moving us away from this picture of the agent as a kind of automaton that's just following rules. There's real space here for kind of individuality and creativity as well. Okay, Darius has been waiting a little while, so let's go over to him.
1540460	1567006	D	0.9134354237288139	Happily patient, happily mean. This is this question, I guess, is deriving from my thoughts about the meeting of the high road and the low road, some of the recent work that's been coming out of Maxwell Ramstead and actually a conversation I had with him, which is that the generative model. So my question, given that given and.
1567108	1567760	E	0.98096	Talk.
1569810	1624500	D	0.9247938513513511	Is regarding the notion of affordances, I think it's natural that as cognitive scientists and philosophers, we take the perspective intuitively of the agent in the agent arena relationship and how the agent is in the business of reducing prediction error. I was just wondering whether, as philosophers, you and Mark have thought about what is it like for the whole system to be in the business of reducing its prediction error? So I'm interested in flow states. And so, for example, my thinking is that in a canonical example of a flow state, let's say a rock climber, the climbing wall, is also in the business of reducing its prediction error as to afford the opportunity for it to be exploited stealth or themselves are in the business of reducing prediction error. So I'm wondering whether this expansion of affordances is bi directional trial, infinitely directional, better?
1629430	1642150	A	0.7933272727272727	That's an amazing question. You mentioned Axel constant, but somebody else.
1642220	1645240	D	0.8299939999999999	Someone else. But I spoke to Maxwell Ramsey's last week.
1648350	1715082	A	0.9255029496402872	There's a paper on I'll find it in a second. On niche construction and affordances. I think it's Axel comps. They talk about the kind of the symmetry between a niche and an agent. Very much in the same vein as you were just talking about. There this idea that there's not just this unidirectional kind of fit between the agent and the environment or the niche is actually modeling generative model of the agent as well, implicitly. So I think the best I can do there direction of some really interesting work on that, but certainly it's not something that plays a central role in my thinking recently. Yeah. So we have the participants chat. Yeah. A variation approach to niche construction. Yeah. I don't know if Mark has anything to add or anybody else worth thinking about.
1715136	1725954	B	0.9590115384615385	Yeah. Darius, can I just check one thing? You asked. I don't know how much I need to add here, but did you think that the wall was reducing free energy relative to the climber? Did you say that?
1726072	1726980	D	0.7566999999999999	That's right.
1729270	1739202	B	0.9186269230769231	I don't know how that can that be the case? Can that be the case? The wall as a thing is maintaining itself relative self evidencing.
1739266	1739494	H	0.9988	Right?
1739532	1776642	B	0.9506791262135919	Yeah. But it's not really a duet of one here. It's not like it has a generative model that's deep, where it's modeling the climber, modeling it the same way as tango dancers do. A tango duo are having cooperative flow states because each movement opens a vista of possible error, minimizing opportunities for the partner, who in turn opens a vista of error minimizing opportunities for the partner and backwards and forwards into this ever opening expanse of affordance capabilities. But an inanimate object and an agent don't really have that same dynamic. So I was wondering what you were thinking there.
1776776	1826740	D	0.9437364137931032	So my thinking was that this kind of bleeds into the notion of fluidity of affordances in the kind of Gibsonian sense and the fluidity of concepts. So I agree in the sense of physical self evidencing, it needs to model the rain, it needs to model the physical environment so as to maintain itself. But what about as it's a for it's it's so a sheer rock face is not a climbing wall. And that's because it doesn't self evidence as a climbing wall, it doesn't offer the affordances to be climbed. That's why I was kind of referring to which is that affordances change based on the agent arena relationship, right. Your car's affordances change whether it's working or broken down. Similarly, the climbing wall changes whether it's got footholds handholds or whether it's just a sheer rock face. That was my thinking.
1827590	1891000	B	0.9359720338983042	It seems slightly spooky to me that we'd think of the wall as self evidencing in that way. I think the language that we tend to use because Julian Kirsten, Eric Reitfeld, Yelly Brunberg and folks often use the idea of affordances and fields of affordances in the active inference way. The radical end of the pool there is typically that the affordances don't only happen on the agent side, but rather happen from a dynamic of the changing volatile environment and the generative model of the agent and that they're collaborating in dynamic ongoing ways such that the affordances are emergent between them. That certainly seems right to me. The idea to push it a little bit further and think about the wall self evidencing, I think it's a step too spooky for me. I think I would feel safer a little bit closer to thinking about the affordances are changing relative to the dynamics of the wall, but I don't know what it would mean for the wall to be evidencing, the climber or itself.
1891450	2051226	A	0.9547788625592422	Ben that's stirred a couple of thoughts a little longer on the work by affordances because there's this really nice distinction in their work between what they call well, I'm not sure if they I think I think that the term landscape of affordances is a little bit older, but they make this distinction between a landscape of affordances and a field of affordances. And I think the reason this is really important to get on the table is because this distinction is kind of directly targeting the dynamic, shifting, very affective nature of affordances. So on the one hand, you have this landscape of affordances that is relatively static. So right now my landscape of affordances is Brighton. And the landscape of affordances in that sense is not really going to change very rapidly unless I kind of jump on a plane and go somewhere else. But the field of affordances has this thoroughly kind of normative, affective character to it. So depending on my internal state at any given time, depending on my expectations or my desires, say, my field of affordances is going to shift very rapidly. So not just predicated on my internal state, but also kind of contextual cues as well. So if something were to happen in the environment that was afforded very high precision weight in, then that's likely to shift my field of affordances significantly. I think that the reason that I'm kind of emphasizing this to Darius's point is and kind of building on what Mark said, it doesn't seem like this distinction at least would apply from the perspective of the wall, say, because this kind of affectivity and normativity just isn't a feature of the wall's experience of the world. And also to kind of run with affordances. Perhaps you could say a bit more about how you would kind of think about this. Darius because affordances are kind of opportunities for to. There's some debate about this, but I like to think of them as like a relational property between you have the skilled capabilities of an embodied agent and then you have some feature of the environment. So I think when you're thinking from the perspective of the wall, you have the features of the environment because you. Have the embodied characteristics of the agent are kind of playing that role. But it's not clear to me what the kind of skilled capabilities of the wall is. So yeah, if you could just say a bit more about that.
2051408	2112640	D	0.9395306666666664	Yeah, I mean, it may be the case, as you pointed out, that actually the technical term affordance as being opportunities for action is misplaced. This is a lot of this I've been formulating, having read this very new paper by Ramsey and colleagues on Bayesian mechanics of physics of and by beliefs, where he really rams home this point, that the system, the generative model is the system of the kind of stochastic differential equations across the state space. So across the Marko blanca, across the particle with its Marko blanket and the external states. And that is the system which is in that system itself is in the business of reducing free energy. So it's not just the active inference agent, it's the whole system which is embedded in a whole system of other things. And so that was my general thinking, is that just on the prerequisite that these systems exist just on that a prior axiom, there has to be some kind of self evidencing of that system itself. So maybe affordance is the wrong word.
2114850	2128994	B	0.9609802439024391	That might be the trick right there. As soon as you provoke affordance, you're provoking phenomenology. And so now we're not talking just about statistical variations. Now we're talking about lived experience. That might be the linchpin right there, I think.
2129112	2160080	A	0.9253185714285713	I think that is right. Yeah. And I would say I think, Darius, you're absolutely on the right track and there is this ambiguity about affordances. But aside from that, I think you're absolutely mean it's just nonsensical to think about an agent absent an environment under active to. It is this agent environment system in its totality that's minimizing free energy. They have to be taken together as one. And I know Avel, you've had your hand up for a little while there. Is there something you wanted to add?
2162050	2163886	H	0.86303	Yes. Do you hear me?
2163908	2166126	A	0.9933633333333334	Well, we do. I do anyway.
2166228	2380260	H	0.9108800234192046	Okay, so about the afferences world having afferences and basically the symmetry of agents and environment is a property of active inference. Well, of the frenzy principle. So if you buy the frenzy principle and you also buy that it entails that agent have affordances or any approximal notion, then you also buy that walls have affordances. So there are basically three position you can have on that. One is that active inference is correct. Wall are fucking affordances and they do self evidencing, which I would not go with. Another option is that you have basically devolved in the detail. So, for example, maybe the adjunct environment is not the wall. It's something that is broader, like Gaia system, whatever, and it just happens. It includes the wall, but it is not the wall. And it does do self balancing. And the third is that active inference is wrong. We don't have affordances based on whatever is the formal presentation of active inference. And I would go in that direction of this phenomenology stuff. So we have, I would say, philosophical evidence in the sense that denying this would lead to nonsense. But information is based on observation, and some system do observing much more actively than others. And maybe it's a good thing to have in your formalism. I'd say so. You have maybe a stronger case for the way phenomenology affordances are constructed in the quantum formulation of the FEP, if there is such a thing, which, like Chris Fields, someone who we should baseline believe claims there is. Because then you have formalization which is not in terms of dynamics per se, so causal constraints, but observation and indirectly phenomenology. Maybe you have something that is stronger there. But right now, the link sorry, there is actually a bunch of people who have a quite good hold on science, which are the ecological activity and cognition on one hand and between cognition and the metabolism, the constitution of the living thing that cognites on the other and one of their definition of agency cognition. Pick one entails interactional, asymmetry I think is the word. That is something we would need to translate into active inference, well into the FEP for active inference to have a strong grip over this affordancing. Until then, we have a kind of handwriting way. It relates it to more conceptual approaches that are related to an active and ecological approach to psychology. And, yeah, the formalism lags basically behind the concept for now. Sorry for the long talk.
2381690	2412080	A	0.9400273333333334	No, that was great. Much appreciated. Anybody else want to come in on a question of affordances? Because I know affordance has played a fairly substantial role in the lecture. Anybody has any questions related to affordances or action perception affordances, then now's the time.
2413090	2417678	E	0.8727681818181818	Yeah, I do have one, but it's not well structured yet.
2417844	2418960	A	0.6370833333333333	That's really fine.
2420130	2467418	E	0.9413074999999999	Something that I've been thinking about for the last month because I started to depend on artificial intelligence field coming from a humanities background and philosophical background. So I'm mixing many things in this period, mainly following the active inference as a crossroad between many disciplines, and this is very beautiful about the framework. And I was super curious about the question proposed by Darius, and I was then thinking about now we're actually facing a new environment and a new landscape in which we're interacting with actually generating over revolutionary time and synchronizing over evolution.
2467594	2566720	A	0.9571599999999995	Technological alien agencies operating within our landscape and field of affordances. So I think there's a lot of really interesting work to potentially be done there. I will take the opportunity to shamelessly plug a preprint that Mark and I have released recently, where one thing that I'm really interested in, that I can kind of speak with some confidence on is the fact that I do think there's a point at which the theory of affordances or the language of affordances, starts to break down to. So the preprint I'll share around is looking at ambient technology specifically. So this is technology that the whole kind of impetus behind its design and conceptualization is that the user doesn't have to actually do anything. So it precedes the user Pragmatically and epistemically. It knows what kinds of things you want it to do, and it just does them in the background. And it works by shifting kind of it subtly shifts the material environment such that it impacts your field of affordances in real time. And the argument that we make in that paper is essentially that the affordances approach doesn't really work for this and that we need to kind of think again, I think that that might be how to conceptualize generative AI, for example, under the affordances framework. I don't know of anybody that's done any work on that, but it would certainly make a really cool project.
2570090	2572440	H	0.9442083333333334	You are making it right now.
2573770	2574840	A	0.991445	What's that?
2576410	2580840	H	0.97067875	You are making it right now. I know because you sent me the paper to review.
2581450	2582520	A	0.994715	Oh, yeah.
2584910	2585820	E	0.9245	Thank you.
2590030	2645580	A	0.9525902941176466	Early stages of a paper on the role of generative AI in classrooms. So, from the perspective of thinking about what kinds of affordance does generative AI represent in a learning environment, particularly, we're kind of applying active inference to classroom design and looking at it from the angles of different educational and pedagogical theories. But it's very early days. It probably depends on your view of generative II in general. I mean, it depends whether you would consider it a real cognitive agent or not. I think some people are more inclined to think of it as this is a thing that has agency, it has real understanding, and other people are maybe less inclined to think of it in those terms. And I think that might bear significantly on how you think of it.
2646930	2665410	E	0.9111592592592593	Yeah, that's super cool. That basically the last project you mentioned is very in line with my PhD project right now because my main PhD. Project is our European foundings, and it's about the AI for personalized education, and I'm trying to follow it through the active inference framework from a multi level perspective.
2667350	2675350	A	0.8477399999999999	Very cool. Yeah, we should we should probably talk more about that. Okay. Darius.
2677930	2678294	B	0.99296	Yeah?
2678332	2713602	D	0.9582003999999995	I wanted to ask I mean, this may be directed more mark so I know this is kind of his work in terms of slopes of uncertainty and about doing better than expected at reducing prediction error over time. I was wondering kind of how the architecture of that is built into the particle, into the generative model because we have the kind of for me, I don't know if it's an hour and out sort of conflict, but we have these kind of implicit priors that we're going to fulfill certain expectations or minimize prediction error regarding certain things.
2713656	2713826	H	0.99246	Right.
2713848	2754110	D	0.9655472727272725	So homeostatic priors or happiness, well being, whatever it is. But then your claim is that we have the higher order beliefs, the higher order predictions that over and above, that I also need to do well, better than expected at reducing prediction error over time. So it's still quite fuzzy in my mind. But is there a kind of potential conflict there between the general priors that the system has and then actually, in a sense, violating those expectations by going over and above them, which itself constitutes an expectation? How do you kind of resolve that tension?
2755810	3073340	B	0.9571147582697223	Yeah, that's interesting, but this is very much in Ben's wheelhouse too. Ben and I have been working on sloppy stuff for almost as long as I've been doing on his original paper called We'll Get It Up. They do a really good job of showing technically where this sits within a deep parametric model. Lara Senved Smith's paper on metacognition also does this by showing you have these depths of modeling where models above are modeling models below, and optimizing over those models below. So we have some good computational backbone for thinking, for not only thinking that this is the case, but beginning to express how it does the work it does. So let's leave that for digging into it, though, technically, sort of on our own, let me say sort of at a higher, more abstract level still, hopefully it's useful. It's not weird to think that this kind of anticipatory system is not only making predictions about the world, but it's also part of what it's predicting is how fast or slow, how efficient it is in particular contexts at resolving certain kinds of errors. That's a perfectly fine thing to think that we're also predicting slopes of engagement. And then all we're saying then is system also pays attention to when those expectations are breached and it's learning from those breaches that should just be the bread and butter for what the system does anyway. So precision is a second order process in much the same way precision is about how reliable are lower level predictions and then using that amount to toggle how impactful either errors or predictions are. So we've already got baked into the system, right from early days, this idea that the system is not only making predictions, but monitoring its own predictive processing regimes and then toggling based on how reliable those substreams are. All we're adding here is when we first thought about those mechanisms, one thing that can happen, this is just good for everybody who's interested in active inference, because I bump into this with my students all the time when we say something like precision weighting, a tendency to think of thing. So we go to find this precision like where's the biological or precision weighting? But when we actually get into a biosystem and we look for these things, the truth is precision is going to be weighted in lots of different ways. I mean, that's the real frontier of this research is to actually find how these things are instantiated and the answer is going to be multifarious. I mean, you're going to have precision adjustments happening throughout the system in lots of ways. It could be synchrony and asynchronous and desynchronies between systems. It can be neuromodulatory chemicals, it can be structural shapes within the brain. I mean, precision is going to be set in lots of different ways. So all that we're pointing out here is that one of the ways that precision is being set, one of the ways that the system is tracking how efficient it is and then upping or lowering the amount of impact error signals or predictions have, is that it's happening in an embodied way. So we've looked over to the effective search and lo and behold, there are all these signatures that we were looking for for this part of the machinery. So not weird that the system is tracking its own regularities and adjusting. That was baked in right from the beginning. How it does that, that's one of the frontiers. It's going to happen in lots of different ways. Lo and behold. Affect the dynamics, the shoe fits, they look like they do that stuff and then you bring it to the lab and you go back to like reward prediction error research and sure enough, neuromodulatory chemicals, reward systems are tuning affective dynamics relative to better than and worse than slopes of uncertainty management. It's exactly what we would expect given the computational model. So then it was an easy next step to start saying, well look, there's one of the ways that precision, there's one of the ways that affect system. Now just notice there it's the only precision, it's not the only thing the affected system is doing. We never want to say we're careful not to be reductionist to say, oh, affect is always aerodynamics. I don't think that's right. I think it's that aerodynamics are expressed in part effectively and they have this impact on the system that we've known about for a long time, even from just the reward prediction error literature. Does that help or was that a bit is that okay?
3074190	3074940	D	0.99978	Yeah.
3078370	3174256	A	0.9422976033057846	Just to add two things really quickly there because I know time is kind of catching up with us. I would emphasize as well, just as a kind of general .1 of the things I really like about the work on aerodynamics and affect is it has this nice kind of broad capture of the kinds of affectivity that agents can experience. So when we talk about affectivity, we're not just talking about full blooded emotions or even just moods, but we're talking about what? One of the things I kind of drew attention to in the lecture was Matthew Ratcliffe's work on existential feelings, which I think is just such the connection between active inference and phenomenology that you find in some of this work is just insanely powerful. And it's one of the things that really drew me into the framework the second thing, just building one thing that I don't think I mentioned in the lecture is active inference. It has been described as a quintessentially metacognitive framework. So you have kind of built into the architecture, you have expectations over expect, so you have kind of predictions about predictions and this has proved to be just, again, just phenomenally useful for thinking about certain aspects of phenomenology as well. So yeah, I think that these are like real strengths of the framework. Okay, I don't know how we're doing for time. Are we strictly limited on or can.
3174278	3178980	H	0.8820859999999999	We we are not as far as I know. Daniel.
3181480	3202620	A	0.9581970149253728	Well, I can probably do I definitely do another 15 if anybody has any more questions. And people, I should say as well, that doesn't mean that we're all held captive to my time frame. So if people do need to leave within the next five or ten minutes, that's absolutely fine, but I'm certainly happy to carry on if anybody has any more questions or comments.
3207060	3297100	G	0.9561164192139731	If no one else has got any, I would quite like to go back to earlier in the discussion you were talking about the value of uncertainty because in organizations in which I'm working with, we often have conversations about uncertainty and what we find is over time there tends to be a drift towards risk aversion. So we're working with a large international construction company at the Minute, and they used to have a culture in which innovation was quite well embedded. And over time, it's kind of drifted towards very, very risk averse culture where actually, rather than learning through the process and being able to tolerate uncertainty around opportunities and what can be accomplished and that kind of surveillance that comes with the intrinsic reward, I suppose, of being able to reduce uncertainty about capabilities to a situation where they're trying to anticipate all of the risks upfront before they even get involved in the project. So there's some kind of valence switch. Mark and you mentioned David Blaine earlier and something about the context there was a switch in valence from yeah, this is a kind of a play, this is a safe thing to do, to actually know this has real consequences. So I'd like to explore that a little bit more actually, if anyone's got kind of any insights or papers or comments.
3300000	3347470	A	0.9597670909090907	Yeah, I think that's a really interesting question, actually. It seems like one of the things we've been talking about is this connection between uncertainty, prediction, error, minimization and affectivity in individual agents. And it sounds like what you're asking is how does that translate to collectives? So one of the things about active inference that we're going to see in subsequent weeks is how it scales up to kind of larger systems like companies or groups that are trying to achieve some kind of shared goal. How does the value of uncertainty translate onto how is it scalable in that sense? Right? Would that be a fair kind of.
3348640	3366388	G	0.9392053333333334	Yeah, I guess so. Uncertainty often when it's talked about uncertainty is talked about as risk, which is, I suppose, is predictions or anticipation of outcomes that we don't want.
3366474	3366916	D	0.99459	Right.
3367018	3405570	G	0.9546932098765429	Whereas there's also positive uncertainty, which is, I suppose, simply stated opportunities through curiosity. What might we be able to achieve this kind of novelty seeking, I guess that new opportunities that we haven't exploited, and they're just in the kind of organizational systems. I think there are various pressures that cause it, but over time, you see these cultural shifts towards a very kind of risk averse where the valence is obviously quite negative, quite an unpleasant feeling for people.
3407780	3722100	B	0.9575146363636372	Yeah, this is good. This is right at the heart of my current work. I'm really interested. So we just had a big stint where active inference models were starting to be used in computational psychiatry, especially for pathological disorders. So addiction, depression, disassociative disorders, OCD PTSD. It's quite a sexy framework for thinking about some of the ways that the cognitive system breaks down and the sort of new move right now. I mean, we have a collection coming out right now in Neuroscience of Consciousness. Is to think about these things in terms of, okay, if we are predictive systems and we have a sufficiently rich model of how that system works in a particular niche, what sorts of ways can we intervene on that system in order to have positive outcomes, rather than just modeling what the negative outcomes are? And I hear that a lot in what you're saying now. So I'm just going to drop one link for you there. This is Casper Hess again wrote a very small paper with a nice little model called Sophisticated Affective Inference. Here's a little bot that tends towards catastrophe catastrophizing the future. If it's given like, fun medium fun medium fun dangerous. Over time, it tends to expect the dangerous one. Like after 10,000 iterations, it basically lives in the worst possible scenario, which is a nice little this is my wife, this is most people today. So the question is, I want to know, given the model, why does that happen and what can we be doing to intervene? And part of the answer is going to be we need to become more tolerant of uncertainty. That's one of the things that the system can be better or worse at. So this just takes the computational modeling and then looks to things that we already know about emotional regulation. That's part of the story. So we've done a little bit of work on this with our predictive dynamics of happiness and wellbeing, and Ryan Smith is definitely doing work on this with active inference and well being. So definitely check that out if you're interested here. I'll just flag one interesting thing here that relates just to what we were just talking about, about layers of modeling. One way that let's say two ways. There's two ways that a system, and it's probably going to be lots. But the two that come to mind that a system can become more tolerant to uncertainty is one exposure. So this is why exposure therapy might be useful for our kind of a system. You want to expose the system to volatility at the lower and middle levels of the hierarchy and have it turn out okay. So that one of the things that the system can come to predict is not only particular outcomes, but it can also predict how much error is involved in particular outcomes. So, for instance, when I used to give a pro talk, I was always really nervous, and I don't know if that ever really went away, but I've had so much exposure to the anxiety of giving a professional talk that basically I don't notice it anymore. But if you were to ask me at the beginning of my talk, mark right now, what's your phenomenology? And I sort of meditatively looked, I'd probably say, yeah, I'm nervous. But if you hadn't have said that and you were just like, hey, what's up? I've been like, oh yeah, I'm great. It's all good. I don't even notice it anymore. That's because the system knows that I have errors go up, but as soon as I start talking, they drop away. And because I know the arc that even error in the system, it becomes non newsworthy. It's no longer interesting volatility to be tracking that's just from exposure. So what's happening is you're having errors at a lower or middle level that a higher level is now modeling, saying when we come here, we should expect this arc of error. Same thing when you work out like real gym rats, they can feel good. You're having your system right, but at the higher level, it's now learned not only that that has a natural arc, but that that's a good sign at a higher level. So now you're getting positive prediction error slope high and negative prediction error slope low. Okay, so one exposure. Two, you can model your own responses. This is something Ben and I work on with horror movies. You can take an active role in mindfully observing your own reactions to volatility. And this comes up in our paper on horror. We've already dropped the link today if you want to check it out. Something really special happens when the system models its own reaction to volatility. It starts to learn that even reactions to volatility, they don't need to be compounded up into dangers, that it's okay to be uncertain at certain levels of the system. So how that translates into business, I'm not sure, other than I know tolerance to uncertainty is a marker of business success. And I suspect the framework in a fit with lots of mindfulness work about learning to tolerate uncertainty.
3722440	3728932	G	0.9297478571428571	Is there anything about the role of language in kind of modulating those metacognitive?
3729076	3730024	B	0.96176	Oh, it's good.
3730142	3736170	H	0.9161061111111112	If I may, I was preparing an answer on that specifically, and I was nervously waiting for opportunity.
3736540	3737530	A	0.74648	Sorry, Mark.
3742400	3924676	H	0.9260196623376628	Meta in terms of evolutionary cognitive archaeology, we don't know when language emerge, but we do know that, let's say an encouraging semantic system. So you can build things in language, and you can expect things in the world to correspond to things in language. So, for example, if you that's abstract, as I say, it's abstract, but you can, for example, have a self identity, and I'm a French, and I communicate with the expectation that are embedded in that identity with other people. And that means we communicate over norms. So we have specific expectation of what we will do. And those expectations, they become embedded in specific symbolic markers that are embedded in language. And something that the thing that it was strongly evoked in my mind when you talked of tolerance to insecurity is that as far as documented history goes, Europeans well, Indo Europeans, they are pretty strong on the idea that things have a nature and they act lawfully. And their nature and their laws, they somehow correspond to linguistic categories. So I can say stuff like chickens quack, and that's a proper explanation of what are chicken and why they quack. And if you look in Chinese philosophy, as far as written history goes, you have a much accent on Wei Wu Wei. So a poor translation would be effortless action. Closer translation would be action without action. So it's something that is quite close to the phenomenology of flow in that you observe yourself doing things and you do not apply a conscious effort to the flow of what you're doing. And that looks like something that is closer to the phenomenology you'd expect. If you apply recursive preactive cognition with less or less heavy or more effectively integrated symbolic concurring like use of language as something that you actually expect to be meaningful and to constrain strongly your actions and marginally, I'd expect, very strongly expect that linguistic categories will map and what you build with it. So, I don't know, self identities, plans, whatever will map cleanly onto what you observe. You will be very anxious about things, because that does not happen usually. And yeah, besides grand discourse on interpret traits, which are usually not that constructive sorry, I got lost because I did.
3924698	4042750	B	0.9606511878453033	Not let me just say one thing before we move off this point. A simple way that language might help is by invoking cognitive flexibility. So oftentimes we think about the management of error being either updating predictions or acting on the world. That's the dyad. Usually what it overlooks is the third one, which is we can also manage volatility by redeploying precision in a better way. So rather than just updating your model to fit the world or updating the world to fit your model, you can just change the set of what matters so that you're not really updating the model and you're not really changing the world. You're just changing the problem landscape. And that's something language allows us to do. It's maybe one of the really amazing things that language allows us to do is we can use language to bootstrap that kind of precision adjustment. So if the train doesn't come on time, we both notice it. Okay. And we feel bad that's perceptual updating. We might go and get a taxi that's active updating. But I might just say to you, wow, look, isn't it lovely that now we get a little bit more time to read our book or to continue our conversation or let's finish our coffee with a little bit of ease. I mean, we get an extra 20 minutes. Now even just saying that redeploys precision over the problem. Space now the train being late isn't volatility in the system. The train being late is signaling to the system that a better than expected slope has been achieved. Isn't that so interesting? The exact same occurrence is either undigestible volatility, right, like you're going to be late. Oh my goodness. Or it's an opportunity for an improvement in the system, which is now you get more time with the person that you're taking the train with. And that was just a matter of linguistic perturbance of the way that precision is being looking at. If you wanted to dig into the research here, I would look up cognitive flexibility and language or coaching cognitive flexibility is such a good point here.
4045520	4127960	H	0.9205398222222217	So you have a paper by Nick Clark on this specific encouraging role of language. And something it entails is that basically you get like if the active infrastructure is correct, you flexibly predict a flow in the interoceptive proprioceptive exteriorceptive. Is that even a thing space? And that is what builds an integrated experience and integrated cognition and language. It adds another layer of complexity. If you can just talk to yourself in your head, that is another dimension that you can predict and that can be coherent or incoherent with your world. And so that is an extremely powerful encouraging system because I just have to tell a plan to myself and boom, that pushes me, nudges me nicely throughout the plan. But then you can have a different level of meta expectation over how language corresponds to reality. And I'd say a very strong factor of aversion to uncertainty is whether you expect your plans or your linguistic structure to nicely map onto reality because it will not happen. But if you expect strongly that it will be the case, you will have to make it happen somehow. And here you will adapt rigid strategies and you will lose flexibility, but also language necessary for flexibility to be the case in the first place. So it's about coupling between dimension of cognition.
4130540	4229112	A	0.9476338617886176	Yeah. And I would just kind of risk and danger are two things that I've been really interested in across a kind of variety of different contexts. And I would say some of the. Things that have been said like how you can influence the system by externalizing things through language and the different strategies that systems, whatever scale they're on, have minimizing prediction error, minimizing free energy. But also I would just emphasize the importance of external context here as well. So even in the case of a collective like a business, you still have to take into account the environment in which the business is operating. And I've been really interested in a particularly extreme example of context. I used to still do some work in philosophy of sport and I've been really interested in dangerous sports and why there seems to be this insane contextual effect where within a very kind of narrow contextual band of sporting practice, people seem willing to take on risks that outside of that context would just be completely insane. And I think this is something that's probably going to come up again in subsequent weeks as well here when we talk about the way that expectations on an individual or a collective basis can be set through other minds. So what somebody else expect myself to do? And so on and so on. Darius, you've had your hand up a while. Do you want to jump in?
4229246	4243596	D	0.9443729411764704	Yes, please. I'm not sure this is the sort of grand synthesis of any of this, but flow was mentioned. As I said, it's a kind of pet topic of mine and something that.
4243618	4245192	A	0.9343925000000001	We know about flow.
4245256	4348700	D	0.9487826530612247	And this is why it's integrated into this idea of language and agency, is that at least in its original conception, success, mahali, and the sort of qualitative experience associated with flow, you would find stuff like not only the dilation of time, but also the reduction in self consciousness. And now this is sort of picking up a lot of work that happens at UCL and people like Jeremy Skipper how integrated language is into the sense of self. And it seems to me, and this is kind of shooting from the hip, that all of these very deep rooted, I guess they're kind of psychotechnologies the self language seem to dissipate at this goldilocks zone, at this point of at this edge of criticality, at this flow state. Which makes me think if the flow state is a phenomenological offshoot of being optimally reducing prediction error, what is the kind of role of the concept of the self or linguistic functions? Because it seems to me that that becomes a regulatory function when that optimality is reduced, when stuff starts going wrong. So I think you can also think about this in the kind of Heideggerian or draythe's sense of just opening a door. You only start to represent the door in yourself. You only give it the linguistic object of a door when you can't open it. When you open a door standard, there is no representation going there. I mean, you could even argue there's no qualia there. So I'm wondering how deep that runs and what we can say about the role of agency, selfhood, maybe even consciousness because they don't seem to be that prevalent, at least from my understanding. When we are at the edge of criticality.
4349360	4381480	A	0.9481551851851847	The suggestion you're making there is it sounds like what you're saying that these things you call them psychotenologies, which I really like. Like language, like certain aspects of self modeling. They are a kind of scaffolding or like a ladder that you then kind of gets kicked away once you reach a certain kind of edge of criticality, like where the performance becomes I don't know what you're performing at such a level that you just don't need those scaffolding.
4382060	4445708	D	0.9540882258064514	Yeah. Or the self regulatory mechanisms that we harness when we're not in flow are linguistic or agentic in essence. And you don't need that when you're at this edge of criticality. And I only postulate that because I'm only thinking, how deep does that go? Because I know, obviously the active inference framework is trying to tackle in some ways the hard problem as well. And there are some flotations of the idea, at least within the flow community, about the kind of I don't want to say elimination, but the moderation, the modulation of qualia under flow states. So could we also consider that to be a construct which happens when we're not necessarily optimally producing prediction error, a couple of things.
4445794	4587690	A	0.9473429461756372	So I'm certainly not the person to stop talking about consciousness, so I won't. But some of the readings that I mentioned in the lecture might be interesting for you on this topic. So if we're talking about certain structures that seem very pervasive in our phenomenology and how those structures can sometimes kind of dissolve away and how that might be accounted for under an active inference framework, there's some really interesting work on psychedelics by George Dean and some of George Dean's co conspirators. I know Mark and Sam Wilkinson worked on a paper with George so George Dean's done some work on kind of ego dissolution and certain kinds of experiences related to selfhood, kind of and I kind of don't want to overstep the mark, but I think they fall generally under this kind of rebus model of psychedelic efficacy. So I don't know if people are familiar with this, but this kind of finds natural articulation through predictive processing and hierarchical self modeling and that kind of stuff. I also wanted to really quickly flag on this topic. I'll find the paper and I'll put it up on the coder because I don't think I'll be able to find it quickly enough now. But there was a paper that came out very recently that showed the efficacy of internal self talk on sports performance. They did an experiment with cyclists and they found that when cyclists were allowed to engage in kind of constant self talk with them, like an inner monologue in their head, their performance at cycling was measurably boosted, which I thought was really interesting and kind of relevant here. But it's interesting because it sounds like it might prima facia be in tension with what we were saying about flow states as well. Right. So if the real edge of performance is a place where these structures of phenomenology bleed away, it kind of calls into question, like, to what extent are these psychotechnologies effective and in what way are they effective? There's just a whole kind of universe of really interesting questions there.
4588380	4670870	H	0.9294132795698922	So, if I may, self food, indirectly will be the central topic of the last two sessions. My position about it is that self food, in a sense, we use all of the time, is a quite high level construct that emerges from the ability we have to compare our activity to a socially embedded model of our activity. So that is something that is quite specific to humans because of their linguistic ability and or because of their tendency to what Thomaslo calls shared intentionality. So the ability to collectively define an. So basically, this is something that enables a very deep, very profound and robust transmission of cultural knowledge and transmission of norms. This is what enables the construction of norms. But it's quite heavy cognitively and it is predictable that if you're busy comparing what you do to what an idealized version of yourself is doing, you're going to invest less creative energy in actually doing the thing that you would if you just did a thing. So I'd say you have a pretty direct entailment of this flow thing from this model.
4675720	4676228	A	0.99611	Yeah.
4676314	4677744	H	0.946834	Yes. This is my conclusion.
4677792	4695710	A	0.9626710909090906	Sorry. So we've run almost 30 minutes over time and I know Mark's had to go and I'm going to have to go very I think I pretty much have to go. So I just wondered if anybody has any final kind of quick questions or comments, maybe we can move towards wrapping it up.
4698560	4706130	C	0.9874414285714286	I'll make a few comments, but Ben, you're welcome to leave and other people welcome to stay if they would like.
4707700	4708770	A	0.9907766666666666	Thank you for.
4712180	4960000	C	0.9545446632124355	Well, first, just on a logistical note, there was a little bit of lag. So I'll encourage everyone who's listening, as well as who's participating, that we eventually plan to, as with all of our live streams, develop a transcript which will be curated and published, and eventually not so far away in the future, that'll be enabled with speech modeling to generate podcast quality audio. So the future will be smooth, we promise. But a few really interesting pieces that I picked out while I was embodying and listening. So, Regina, you opened the discussion with surprise and creativity, and indeed, it sounds like it's going to be attention. How can a framework whose imperative is bounded surprise as opposed to, say, maximized reward? How can a framework whose imperative is surprise, minimization and bounding be used to generate novelty? And I felt like people provided a lot of really cool answers. And one other thing it reminds me of is Doug Hofstadter's notion of spectishnish. I don't know, how do you pronounce it, but sphexishness, and it's the specs wasp. And so he says, well, forget this whole creativity thing, because when we talk about creativity, it's like some sort of pantheon, like divine status. The muses have to be called in. And so then, oh, that's not real creativity, and this isn't real creativity. So instead, he says, well, let's just focus on what would be the opposite. And that would be the rote procedural following, even when there's an opportunity for what we would call creativity. And then there's a continuum. So why does somebody paint that painting? Well, why didn't they invent a new genre? Well, why didn't they invent a new media? And so all activity is existing within this continuum of creativeness, which has many features such as generativity and productivity, but also novelty, but abounded novelty. It's not creative to knock the chessboard pieces off in a way that there might be an elegant chess move who only somebody in a certain cognitive setting could detect the aesthetics of. But I think that was awesome. And then the second piece that really was powerful was what mark's train. I guess the trains don't run on time where Mark is, but that's okay, because he was able to, rather than treat that as an undigestible surprise, that was able to be basically cognitively metabolized into an opportunity for friendship through language as a social medium. And then he connected that to cognitive flexibility and coaching. And then I thought about self talk and our inner monologue inner voice. And then Darius, that was very provocative, that we aren't having self talk, potentially not even having a self in the flow, manifold experience of the self or the self talk, or again, potentially even the total actual self as a technology. And so then healthy self talk would guide us to the flow and be kind of like a self limiting technology because it would be used in order to not be needed to be used, which is kind of how we would hope adaptive technologies would be versus a maladaptive technology would be something that entrenches its own utilization potentially at the expense of the functionality. Kind of like doom scrolling status. But that's internal rumination. And those are the kinds of things that people have simulated in active inference. So those were some really cool pieces. That was a great discussion.
4961060	5142750	A	0.952809384920636	Could I just add something on the topic of creativity that I should have said earlier as well? Because I think it's really relevant to Regina's question and just the topic of creative and novel behaviors in general under active inference. I think that whenever we talk about especially artistic creativity, there's a tendency, even for people who are kind of very well burrowed into active inference and activism and these kinds of frameworks from cognitive science, I think there's a tendency to think of creativity as still like it's almost like the last bastion of internal cognitiveist thinking. We think creativity is something that happens in here and bursts outwards. And so how does that happen with this kind of whatever cognitive architecture we're positing? But there's really very cool work by I have to give a shout out to Mike Wheeler, who has written some papers on the extended mind and creativity. And one thing that's not really come up very much that I didn't get time to talk about in the lecture is this really nice natural marriage between the extended mind and active inference, something that Andy Clark has been working on. And I know he's going to be doing a lot more work on it. But the claim Wheeler makes and some others I think Joanna Zelenska has written a little bit about this as well, is the fact that creativity itself is just as subject to kind of material, the sculpting effects of material environments and sociocultural environments as everything else is. So creativity is not this kind of romantic with a capital R kind of internal process that kind of bursts forth, but creative thinking is just as much extended and embedded as everything else. And there's some really great examples in Mike Wheeler's essay. I don't know, is anybody familiar with the band Alt J? They're a British band. They were kind of really big a few years ago. They won the Mercury Music Award because they have a super original sound. So AltJ have this really quiet tinkling. Yeah, they are a great band. They're one of my favorites. And they have this really quiet tinky sound that everybody kind of assumed was just the result of some kind of internal genius on the part of the band. And as it turns out, AltJ, originally they tried to rehearse as an indie rock band, but they were confined to rehearsing in an apartment block, and they kept getting noise complaints. And so they ended up developing this quiet tinkly sound because it was the only sound they could rehearse that wouldn't get them kicked out of their apartment. And I think this is a really beautiful example of how even the most creative processes that seem, on the face of it, really creative, are in fact just a subject to these external kind of this kind of agent environment system that underpins the whole framework. So that's something that's going to apply to absolutely everything.
5144000	5194590	C	0.9507949397590361	Awesome. And just the last question, people are welcome to drop off the next section that we're heading into. I'll be doing the lecture that's going to be in August, and it's going to be on collective behavior. So what would people like to learn or focus on about collective behavior? Anyone who hasn't spoken or it can just be a thought question, but I haven't prepared the slides at all, so I'm happy to take suggestions. Yeah, Darius and then anyone else?
5194960	5234250	D	0.9367530681818178	I think what might be interesting is if we're saying that these systems are all in the business of reducing prediction error, of self evidencing. Why do we see a diversity why do we see a diversity of social cultures, of norms, of standards at a global scale? Why do we not just see some homogeneous way to reduce picture there, which would manifest as the kind of singular culture which is in the kind of business of self evidencing? That just kind of popped in my mind.
5236060	5236810	A	0.99999	Cool.
5241920	5247710	C	0.9605989473684213	Anyone else want to give a thought on collective behavior or on any other aspect? Otherwise it's been great.
5248320	5261330	H	0.8850267741935485	I'd like to make a comment of creativity, but does anyone like at the moment, if you want to make a comment on the next session, I want to put that.
5262900	5264290	I	0.7259024999999999	Okay, go for it.
5265460	5413330	H	0.9188652662721891	I'd say that to complement what Ben said, that creativity is intrinsically very hard to model because by definition, creativity is something that brings about something new, let us say. And the mathematics we have to describe physics and life, they are not very good at new things. Because the most basic tool you have, the basic way to represent a system that literally everyone uses, is a stead space, which is the least of all possibilities of a system. If you say something is creative, you're likely to think that it means it can bring about new possibilities. And this intuition, it conflicts directly with the very basic use of math. Describe it. So it is basically the same issues I was referring to earlier when I talked about the comparison between active inference and an activist. So let's say activity biology inspired model of cognition, and a core concept historically in those circles is autopoiesis. So the ability of the living to self create, literally, this is Greek for self creation. And you had a lot of drift and conceptualization and ambiguities that were resolved or not resolved and led to the crisis of framework, blah, blah, blah. But today we talk of autonomy rather than autopoiesis. And we define autonomy as the property for a system of constraints over the activity of a system to reproduce themselves. And because we're talking constraints, just the ability to influence causality outcomes, there is no really a prior that dynamics in this space would be conservative. So we have a possibility for constraints to bring about new things and reconfigure themselves. And the notion of Hydrancy ADUs is based on that. It's the ability to, let's say, reconfigure constraints in your environment, and so is the notion of creativity, which would be likely a very close proxy to that. But it's something that we do not know how to present. And most of the mathematics that exist are structurally enabled, represent. And so, yeah, that's a big one.
5417400	5418390	C	0.94406	So true.
5421000	5421860	A	0.99515	Michael.
5426600	5506770	I	0.9322450322580642	Good to be here. Sorry I was late. What comes to mind for me at the know, I mentioned units of collective, like, what is it, a we? Is it a we a pair? What are the units of we. And any thoughts or insights about that? The stages of the emergence of formation into a collective from a me to a we, for example, or god, I had one was on the tip of my tongue, and it just escaped me. Oh. This idea of precision that I think it was Mark Miller was talking about, or Matt saying at the if there's options in the looking at collective what might be some of the most well understood limiting beliefs about when we use language of collective that keeps us repeating the same blind spots. And how might we think differently about collective so that we escape those predispositions, if you will? Makes sense.
5509700	5511824	C	0.9495771428571428	They'll do what I can do in.
5511862	5515392	I	0.9907528571428571	The solo lecture, and I'll look forward.
5515526	5517884	C	0.9334571428571428	To the conversation where we can unpack.
5517932	5518530	D	0.51559	It.
5520260	5523376	I	0.932529090909091	Exciting. Thank you for doing it. I'm looking forward to it.
5523478	5545320	C	0.962	Yeah, it's awesome. Nathan or David, you want to add anything? All right, then. Thank you all. Hope everyone is enjoying the course so far. Thanks, Avel, again for coordinating it. And to Ben for this great section. Now you can put on your student hat again.
5546010	5547750	H	0.9791019999999999	And to you for organizing.
5548650	5550214	C	0.9172757142857143	Thank you. See you all next time.
5550252	5550840	E	0.96196	Thanks.
5551610	5552210	B	0.89185	Bye.
5552290	5553080	I	0.917965	Thank you.
5554010	5554660	D	0.982595	Thanks, everyone.

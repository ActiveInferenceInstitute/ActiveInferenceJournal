SPEAKER_00:
All right, it's cohort four, October 31st, 23, and we're our first discussion of chapter nine.

So today we can totally look at questions and look over the chapter, but is there anything that anybody wants to bring up about chapter nine?

Welcome Andrew.

Any general thoughts on nine?

Model based data analysis before we kind of look through some look over the structure of the chapter and look over the questions.


SPEAKER_02:
OK, just hop in to the structure.

A good overview seems nice on this one.


SPEAKER_00:
Cool.

Yeah, and Esmail had mentioned epistemology.

So talk about leading with a epistemological assertion.

The models described in this book are only useful if they can answer scientific questions.

Is that the only way a model can be useful?

Or is this such a broad sentiment that everything can be cast as a scientific question?

So even something of purely epistemological...

or epistemic status could be under this scope.

Anyways though, in this chapter, we focus on the ways in which active inference can be applied to understanding empirical data.

So that's kind of the big plot twist in chapter nine is that up till here, everything has been very much like proposed,

as sort of intrinsically coherent like the rat in the tea maze or other um calculations they're all kind of proposed as um just thought experiments more than anything else although there's a lot of citations to the empirical work like in chapter five here though they're going to go into detail about how you

interface the generative model with empirical data and so in that way it kind of complements chapter six because here we're actually gonna take that generative model and connect it with data

Our general goal is to recover the parameters of the generative model that a subject's brain uses to produce behavior, the subjective model.

So we'll see this graphically soon, but just to be clear, to recover the parameters, a synonym there would be like to identify plausible parameters.

So if we knew that the height of children in the classroom was distributed according to a normal distribution,

Then we have the data, which is kind of distribution free.

It's just a list of numbers.

And then the task of chapter nine is basically juxtapose the structure of that model with the data and recover the parameters.

Okay, section 9.2 goes into this meta Bayesian perspective or meta Bayesian methodology.

they bring up two related reasons for fitting a computational model to observe behavior the first is to recover parameters to give an account of a single individual from a broader population or comparing two groups or two populations or more against each other so that is taking a fixed model structure

and then multiple samples of data could be one person on multiple days or multiple people on multiple days or whatever the design of the experiment is and then for a fixed model structure identify something useful about that data like

about one group versus another.

The second reason is to compare alternative hypotheses expressed as models that represent different explanations for behavioral phenomena.

So here, instead of preconditioning on one model, we're actually going to use the data to interrogate a portfolio of models.

So we have performance of multiple individuals on multiple days.

So then we might say, okay, well, the simplest model is all individuals are the same drawing from the same distribution, all days draw from the same distribution.

Then we have a model where individuals don't differ, but days differ.

But then we have individuals differing, but not days.

And then we have, there's an individual by day interaction.

So this is familiar to anyone who's basically done any kind of statistical modeling.

which is that adding more parameters to a model essentially, or almost by definition, always increases the accuracy of the model on the training data.

That doesn't mean it increases the accuracy in and out of sample.

So that's like the training and test relationship of overfitting.

But adding parameters basically always gets you to fit more data.

or fit the data more closely.

However, the return on investment per parameter can diminish.

And so that is why in Bayesian statistics, people use methods like the IKIKI information criterion, AIC, or the Bayesian information criterion, BIC, to identify, well, where are we adding parameters?

Where's this kind of like knee or this special inflection point

where yeah of course we can still add more parameters and get more fit but we're like basically at a nice optimal trade-off between adding parameters and fitting the data which is exactly what variational free energy is so this is two reasons one is to take a model identify something interesting like a pattern in the data

the other which is complementary is to use data to identify or differentiate hypotheses in the world so yeah actually having the day factor and the individual factor those add value but having an individual by day we dilute too much just just like raise your hand if you have any thoughts if you want to add anything

So this is a pretty standard statistical framing.

Here is where we're going to revisit some of the key concepts from earlier in the book, like the generative model of Chapter 4, discrete time model of Chapter 7, all of this.

And now we're going to kind of put it in a laboratory context.

So we have the subject of a behavioral experiment.

so let's not worry too much about like objectivity of truth and subjectivity of experience but let's just think about a behavioral study so the subject of the behavioral study is the one whom we are developing the cognitive model of their cognitive model their generative model might be about the laboratory like the maze

However, the subject model is what we've been discussing in basically all of our kind of agent-centric cognitive modeling discussions.

And then in Chapter 9, here we're just going to pop out and add another dashed ring and a solid box.

around that to represent the fact that it's kind of like now we have the lab and the subject here's the experimental stimuli going into the black box here's the observed behavior objectively coming out or it could be said empirically coming out

so this kind of gives a whole enclosed or a closed Loop representation of like the person in the fmri machine and then here's the experimental stimuli being provided to them could be a fixed sequence it could be drawn from a distribution of stimuli and then there's the behavioral outcome

so even the setting of the the behavioral observational moment the ethological setting this is also amenable to Bayesian statistics so we could talk about optimal design of experimental parameters we could talk about optimal

David Price- Delivery of different stimuli we could talk about the information content for different sequences of behavior we could do a statistical power analysis or simulation should we observe a few individuals for a long time, a lot of individuals for a short time.

These are like very empirical laboratory statistical considerations that will be familiar to anyone basically doing a behavioral study or really just any other study, even if it's not like squirrel behavior.

But if you have a limited amount of resources or time or cameras or whatever it is in the lab, you're going to find yourself basically in a setting like this.

and so that's why this is a really nice figure and chapter because it steps outside of just the kind of abstracted generative model and now it's like okay now now we're going to build the setting around that and so it's kind of here's outside the building but now it's like the laboratories like the Markov blanket around the rat in the maze

Which is to say, if you control the laboratory, if it's a controlled experiment, then what's happening outside of the laboratory shouldn't matter, should be conditionally independent with respect to the subjective model.

So this is kind of compatible with a lot of Bayesian epistemology in empirical sciences.

Okay.

Any thoughts or questions on that?

Or we'll continue with end of 9.2, then 9.3.

It's called Metabasian...

or I don't know, maybe you could say epi-Bayesian or something, double Bayesian, because of this wrapping with a Bayesian statistician around the Bayesian generative model.

And then they even say, we're using Bayesian procedures twice.

First to evaluate the subject's posterior beliefs about action.

So posterior means posterior to the observation.

So we're going to flash the red light.

Then we observe an action.

and then we're going to do parameter recovery about the subject's posterior beliefs on action posterior to the observation of the red light that's the subjective model and secondly to evaluate our posterior beliefs about the unknown priors that we have so just a purely Bayesian approach to studying the total setting of the behavioral observation

Otherwise, you'd have this kind of free-floating Bayesian model of the subject, but it would be like a view from nowhere because you wouldn't have completed the meta-Bayesian setup.

Okay, that's 9.2.

9.3 is going to connect us with a statistical method that's used in a lot of statistical approximation algorithms, and it's discussed in SPM a lot as well.

So variational Laplace may be used for more generic likelihood functions than encountered earlier, which were defined as Gaussian.

So variational Laplace, we won't go through all the details here, but usually what the Laplace approximation is going to do is it's going to find the single highest point on the distribution, the single maximum likelihood,

solution, which is why the likelihood calculation from earlier matters.

And then going to fit an upside down parabola

So if the distribution has a central tendency, the variational Laplace can do very well.

If the distribution has fat tails or has a multimodal tendency, variational Laplace will not do well if it's Gaussian.

Kind of a technical subsection.

but it's possible to tractably optimize the variational Laplace so it's going to be misleading in some settings but it's going to be super useful in some settings but either way it's very calculable so it's used for a lot of statistical optimizations

Okay, so a common critique and valid point that people often bring up about Bayesian statistics is, well, don't you just get out what you put in?

Like if your prior is such and such, then don't you just recover your prior in the posterior?

And there's multiple ways to address that.

One of them is

If you start with multiple different priors, so from the same family, parameterized differently, or from different families, and you recover your prior, then you actually can empirically say that there isn't information in your data.

But to the extent that those diverse priors converge on a similar posterior, you can empirically say that there is signal in the data.

It's not binary, but those are different heuristics that are used.

So one approach to kind of get around or to address this reviewer type comment that like, oh, well, Bayesian statistics just returns your priors is to try different priors.

And there's also another approach, which is called parametric empirical Bayes.

So it's called empirical because we're taking empirical data and using that to initiate the cycle of optimization.

So for example, if we were gonna be doing a Bayesian model of the height of the children in the classroom, one approach would be like, we know that people range from one feet to 10 feet tall.

So we're gonna do six feet plus or minus five.

as a Gaussian another approach would be wow we really don't know it could be between zero and a thousand feet or something like that like just try to do this sort of like purposeful open-endedness but that might wash out true signal in the data like even if a bunch of people were the same exact height you've only shrunk your distribution now to between zero and 500 feet tall

So what's the way to get around that?

Well, one is trying multiple prior sets, but another one is to take the first batch of empirical data, parameterize a Gaussian based upon that, and then use that Gaussian or an overdispersed form of it as your prior for subsequent analysis.

This discussion

on design matrix so representing the experimental design as a matrix like if we had five people on um you know multiple different days you have like data point one and then in person a they'll have a one and then all zeros for the other columns

So representing the design matrix of the experiment as a matrix, and then putting that within a generalized regression framework is discussed more in the SPM textbook than in this textbook.

So this is kind of another technical subsection talking about how you can use empirical

guidance on initial prior sets in a generalized linear modeling setting with experiments summarized by their design matrix to do a regression on model coefficients so you can do statistics on those coefficients like testing whether the slope is different than zero

So to go back to the earlier example where we had like multiple individuals on multiple days, you have the simplest model, no effective individual, no effective day.

And then you have model two, effective individual, model three, effective day, model four, effective both, or model three, effective both.

And then you can test the relative likelihoods of these different models.

And then for whichever one is the best selected model,

then you can say okay the coefficient was 0.1 plus or minus 0.5 0.05 so the slope was two standard deviations away from flat so at this alpha level it was a statistically significant slope for the best selected model

it's kind of speeding rapidly through a few different parts of statistics but that's basically what is happening when you just smash together the design matrix and the experimental coefficients you're basically doing a regression and then you're calculating essentially a p-value based upon the slope of the regressions conditioned on the design matrix

Okay.

So actually just, I'll just raise your hand or unmute, or if you want to add anything, but just kind of going over this chapter.

9.5 is a lot like chapter six.

Chapter six was the recipe for making a generative model.

9.5 is the instructions for model-based analysis.

So 9.5 is almost like a CODA to six.

So the steps are summarized in Figure 9-2.

First, collect data, or you might collect data later, and you might do all these other steps earlier, like if you were going to pre-register the study or do a simulation or a statistical power calculation.

So you don't have to collect the data first.

Again, just like Chapter 6, this is kind of like more like stuff that has to be done, not really a list that must be done in order.

However, at the very least, you'd want to know what sensory input is available to the subject and what data you're getting out.

Sensory data for the subject, what data are coming out?

As a kind of side note, many studies have been waylaid because there was sensory data that wasn't intended that was getting to the subject.

So I've heard examples ranging from it was a study of circadian behavior and there was construction in the building.

So at certain times a day or during certain cohorts or replicates.

there was sound or just in a animal facility different animal handlers or different light schedules or different temperature different air conditioning forest fires that we had in California like the sensory inputs available to the subject doesn't mean just the ones you want and so that can disrupt a lot of the statistics

But this is basically like saying, what data are you going to get from what kind of subject?

And then what type of tests are you interested in doing on that data?

There's always exploratory data analysis, EDA, also criticized as being kind of like a fishing expedition.

So sometimes people go into a study with really specific hypotheses, other times more open-ended.

To formulate a POMDP model.

If you're using a discrete time setting, which is kind of how they're going to do it.

They're not going to template this with a continuous time model.

So we get a fully specified, but not solved POMDP.

So structurally described, but not parameterized POMDP.

specify a likelihood function we can look a little bit more at spm mdp vbx to try to see exactly uh what degrees of freedom are here but i i my understanding is this does not this is not something that needs to be designed on like a per experiment level

specify prior beliefs, these are the priors that are being proposed for that meta Bayesian setting.

So if it's kind of a, if there's some priors that are centered on zero, other times you have a prior distribution over a distribution.

So it's like, it's bounded by zero and one.

at which point you could use uniform or some other proper distribution.

Solve for posterior probability, standard inference scheme.

Again, we could look at this routine, but this just takes in the above information and makes a calculation.

So the output of steps one through five is going to be these estimates of different parameters of the model.

Then you can do the group level analysis.

We can look at SPM, DCM.

This is the dynamic causal modeling.

um so that's one approach that's discussed more in spm or you can basically use any statistical method canonical variates uh analysis cva similar to the independent contrasts ica or principal components analysis pca or just anova any number of statistical analyses

So here's the summary.

We have the data, we have the POMDP, and the way that it's going to get calculated.

All of this gets brought together.

So U tilde, tilde always means through time.

And here's U tilde, the action as selected, as observed behavior.

So that's what we're getting from the data is the behavior, survey data, movement data, eye gaze data.

And then here, we're gonna get a likelihood function for saying what's the likelihood of given behavior conditioned upon the parameters, big theta, all the parameters,

the sequence of observations received, and the generative model, M. So that is kind of what the subject is doing, is it's doing this likelihood calculation.

What's the likeliest behavioral sequence given basically all the parameters accessible to me, the cognitive model, as well as the observations.

And then we invert that.

And that's now happening on the scientist's side.

So we're going to invert so that we can recover the parameters, big theta, conditioned upon the model, the observations of the subject and the action of the subject.

That is proportional to...

So it might be off by a multiplier, but it's monotonically proportional to a factorized disassembly, which is the parameters given the model, which is something that can be taken off, multiplied by here, exactly what we have here.

p of u tilde conditioned upon theta observations in m now we can specify our prior beliefs on this left side because this is basically our our beliefs about theta given the model and again we would test a range of these

and or use parametric empirical Bayes to generate empirically grounded beliefs but with this inverted model we can then do statistics on calculation of data we have X the design Matrix beta which is a variable with the regression coefficients and a noise so it's kind of like y equals MX plus B plus noise term

except it's like a mega regression on a much more sophisticated kind of data but in the end here's those five groups and then here's this one's parameter range was you know 0.2 plus or minus one this one was five plus or minus one this was four plus or minus one and this is like the kind of figure that you'd get in the paper

where there'd be like a bar A saying, oh, these are not significantly different.

Bar B, these two are not significantly different.

And then an asterisk saying these two, individual one, two, and three were significantly different than four and five at this p-value.

So chapter six builds us up to constructing the generative model.

if you have empirical data already at hand or if you intend to collect empirical data and you already know what structure it's going to be you kind of design this complementary relationship between the observations that the generative model is going to get and then the actions that the generative model takes which are going to be observations for the behavioral scientists

And then if the behavioral scientist can specify this likelihood model and a prior belief distribution, we can invert what it is that the subjective cognitive model is doing to recover differences or patterns amongst parameters for different individuals or times or contexts.

so this takes us from a chapter six which is just how to build the generative model and lays out the big steps for like how we would then get to the figure for the paper or to test okay here's five different doses of a drug and then on the y-axis is the attention parameter variance on this which is we're going to interpret that as attention and here's how it differs across doses of the drug

so it's a lot of statistics here being very very quickly recited in a Bayesian cognitive behavioral modeling setting could you repeat the example with the drug dosage again

Yeah.

So let's just say we had five different doses of drug or control and low, medium, high, and very high.

And then each of those, we're just going to do, of course, you can have N individuals on N days with N doses.

That'd be like a fully replicated experiment, but you have some subset.

So we have five groups and then we have 30 individuals per group.

um rats so that we don't need to talk to the irb or we probably still do um so then you have the rats do these different behaviors now if you just wanted to do statistics on their behavior you wouldn't even need a cognitive model so if all you were going to do was going to do like i'm going to do the average um percentage of time moving between the five groups

Well, then you could just read that off from the data and just say, okay, it was like, you know, 10%, 15%, this and that.

So you could have a little scatter plot.

And then you could say, do these groups statistically differ in the percentage of time moving?

So that would just be like literally from the data to step six.

However, you might also have a cognitive model.

And so there you would take that data and then see the actions of that, the data, which are the actions of the subject, and then utilize this constructed cognitive model to do inference on some cognitive parameter of these five groups.

So not simply descriptive statistics on the action output.

And then, and then these, these scores are referring to some proposed cognitive parameter.


SPEAKER_01:
It's like an example of a cognitive parameter.

I'm very interested in like, I like, uh, yeah, like that.

I just want like an example.


SPEAKER_00:
Yeah.

That's a good question.

Um,

Let's look to some of the work of Ryan Smith.

Okay, let's see if this will have a nice generative model.

Some of these will.

Perfect.

So here we see our classic downstairs bit, figure 4.3, downstairs.

The empirical data are going to be the heart beating empirically.

and then the hidden state is going to be, I believe in this situation, the interoception of the heartbeat.

And then A, which of course maps between observations and hidden states, if A were the identity matrix, this would be a fully observable setting.

But of course, A isn't exactly the identity matrix.

And in fact, they're interesting in the precision of

So then they can parameterize this model and then you can test whether the precision, the interoceptive awareness differs amongst individuals or between individuals who have this or that diagnosis.

Let's see if there are other models.

Here's another one.

This one has to do with gastrointestinal perception.

Again, we see the downstairs.

So then what would be the next step?

What would be the way to integrate action?

Well, we know action intervenes in the B matrix.

So now if you had a descending action condition, and then you could say, okay, now do or don't try to pay attention to your heart or your gut.

and then you could do statistics on the efficacy of the b matrix but now these are making larger and larger experiments and that's where you get back to like the resource limitations and laboratory and stuff like that but yeah then you could say well now i'm interested in this summary statistic describing the difference between the average

precision when we do and don't tell them to do this and there's some individuals where there's no difference like telling them to pay attention or not didn't differ their precision and there were some individuals where telling them did alter their precision but of course those precision estimates are features of a statistical model which is why we had to construct this those precision estimates and the statistics about how those precision estimates differ those are not features of

simply the heart rate but this is a big this is a big apparatus to bring out so if you just are actually interested in the descriptive statistics if your question can be addressed with description of description of behavior you literally don't need a cognitive model

this is being brought out when you are trying to make inference about something that's not directly objective yeah then they reference these two papers that do have slightly different ways of looking at this eye

saccade situation um again to this kind of bayesian epistemology and also like sociology of neurodiversity really we hear so much rational irrational irrational non-rational all this um and

active inference or just Bayesian epistemology has kind of like a simple and a deflationary, but also kind of a maddening answer to that, which is all the generative models are on their path of least action.

So those paths may differ, but the paths are all alike in that they're all paths of least action.

So it's all Bayes optimal.

One person might be engaging in a repetitive behavior,

Another person might be ruminating.

Another might be having anxiety.

Could be human, could be non-human, but those are all paths of least action.

So obviously there's a lot to say about this, but that gives a little bit of a space between like disorder implies that there's like an ordered and a disordered.

But here it's saying inference is working fine.

but on a flawed generative model.

But again, you could even go further than that and say, well, there's nothing flawed with it.

But that's the kind of way that it takes that conversation.

Table 9.1 reviews a bunch of settings where these kinds of parameters have been done.

addiction impulsivity compulsivity delusion hallucination interpersonal personality disorder oculomotor syndromes pharmacotherapy prefrontal syndromes that seems pretty general not sure that one is visual neglect disorders of interoceptive inference and that's where the Ryan Smith work especially is relevant

that's the summary so it's kind of an interesting chapter with a few of these mosaic sections beginning with a large scoping frame on cognitive behavioral modeling

then having these two kind of technical subsections then going into a chapter six like interlude about how this work is empirically deployed providing two examples of generative models in the eye tracking setting going into this kind of like social definitions of pathology

and empirical work on pathology space summary.

people who are interested in this like design Matrix or just like experimental statistics SPM textbook is really the place to look another kind of interesting note is um

the methods for active inference are very well developed in the spm toolkit in matlab and so for neuroimaging and those kinds of studies there's a ton of empirical work however most people who are in data science today are not using matlab using python and pi mdp leading python

active inference package is only now beginning to include these empirical going from data to the model type model inversion tools those do exist in the julia rx infer world um which is one reason i believe why

such a common kind of paper today is like we built a generative model we synthesized some data we simulated data because going from the data back to the model is actually not so easy today for a full active inference model but as that becomes more plausible

There's going to be a whole new forest of low hanging fruit for those who choose to forage, because this data could be any data.

So this could be click data.

It could be ant foraging data.

I mean, these are just regular data sets that people have already analyzed, and then it will be a purely empirical question.

What cognitive modeling can add

in comparison with purely descriptive statistical methods so as that capacity becomes like unlocked in the active inference ecosystem it won't always be like situation generative model simulation we'll be able to like start with empirical data about real systems economic data psychological data and so on

and then already have that while we're having the generative model discussion that's going to reduce a lot of degrees of freedom on the generative model because we're knowing that it's going to interface with the data a certain way so honestly it's like a whole mode of discourse and work in the active space that we just don't see but this chapter lays it out

Because where we do see that mode of work is literally Fristin et al's work in MATLAB with the SPM package.


SPEAKER_01:
Is the SPM textbook, is that on the website for the institute?

Or is that something I should just be on the mind?


SPEAKER_00:
I'll put it in the chat.

I believe that this is the most recent.

Yeah, SPM 12 manual.

is probably the most recent.

This is probably one to look at.

The last print version is this one from 2007.

So it has some typos.

It is less about like action in the loop and the control theory

However, the relationship between sensor fusion and hidden state inference, neuronal dynamics inference, this is absolutely a slam dunk.

So it's very, very, and even just like, um,

even just seeing the kinds of statistical methods that are in play between neuroimaging observations and then just the cognitive modeling, and then having action kind of come out of the neuronal dynamics, one, I think that that is not the only learning path, but I think that that can give a more solid basis

for the kind of downstairs part of the model um and then also when we're talking about like attention as mental action or covert action common theme that's basically like saying well we have this descending um internal action that's intervening in hidden state Dynamics

So then it's kind of like, but SPM is like the train.

And then now it's like, okay, well, action intervening in attention is like, I don't know, throwing something in between the spokes of the train wheel or having a hand on the train wheel.

But this train downstairs is basically statistically summarized in SPM.

so it doesn't go as heavy into the action embodiment all of these topics that we like know and love as part of the discourse today but I think it's a really useful prerequisite or background work so that's different from the manual that's uh yeah

I'm not exactly sure what the overlap is between the manual and the 2007 book.

We should definitely use a language model to find out.

I may or may not have a PDF of this book.

Yeah.

And it just helps give us a sense of what Friston et al were working on only 15 years ago, primarily.

And that helps contextualize like, oh, this whole like physics thing.

of cognitive systems versus just the statistics of neuroimaging but how how nicely they come together and as someone who's never done neuroimaging

it was just kind of cool to like see the kinds of challenges that they have.

And there's a lot of transferable information.

Like the fMRI bold signal has like a latency of several seconds.

So, you know, it takes two seconds to come on and it takes 10 seconds to fade off or something.

So then if you're timing experimental stimuli, the stimuli can't come in every half a second, but also if you wait five minutes,

you're gonna reduce your statistical power because you only have a fixed amount of time in a scanner.

So like, these are like very empirical, very pragmatic aspects

so it just makes me think about how in kind of future textbook groups or future internships or whatever it is like there's the journey of chapter six there's the journey on the low road and high road then there's the journey on chapter six to build a generative model for a situation of interest and then there's the the journey of doing an experiment

And of how you reckon with these resource constraints and with data that don't uniquely identify models, all these different things.

So there's a lot of cool things that chapter nine helps open up.

any just thoughts or ideas that people are having at this point in the group?


SPEAKER_02:
Yeah, I find this chapter to be just incredibly important as far as application of active inference goes.

And I think it was great that you kind of directed us to Ryan Smith's work and others.

I think that there are enough

general principles here for this chapter to be useful.

And that said, actually being able to see, like direct examples of the application in already published works seems very, you know, useful, and that we actually have something kind of more concrete to to get our hands into.


SPEAKER_00:
Yeah.

So here's like interoceptive precision.

So this is a, this is a, they did parameter recovery on the A matrix.

Then you can say, well, what's the correlation coefficient between this coefficient we identified, IP, and these empirical phenomena?

Reaction time, reaction time variability.

These are just descriptive statistics.

but on the x-axis the rows are precisions a difference in the precision prior learning rate and learning rate so the rows are actually cognitive parameters and then the columns are empirical descriptive statistics blue is a positive correlation and red is a negative correlation

so this is like immensely transferable if you think about well the observations and the summary statistics are on the blanket so if you're just trying to draw conclusions about just the blanket um you know there's a factory it's making blankets if you just wanted the average color or the average size of the blanket you could just measure

but then if you wanted to do some sort of inference about what was happening in the factory and then you could do this kind of correlation between here's the outcomes and here's the cognitive parameters that might lead to those outcomes this starts to really find some interesting possible because you and you could also look at the correlations of different cognitive parameters

And then here it is with the statistical differences.

And you say, well, you know, we don't find a statistically significant relationship between interoceptive precision and BMI.

Negative slope, but it's just insignificantly different from zero.

But we do find this statistical relationship here.

So, yeah.

And that's what differentiates cognitive modeling from purely behavioral work.

Okay.

Well, let's see.

Next week, we return.

second discussion on nine there are some questions already on nine and there's probably also um a bunch more we can add but we'll come back to nine next week any last thoughts on this


SPEAKER_02:
Yeah, you know, maybe

As an extra thought, maybe next week, unless there are any new incomers who do want a refresher as a summary or something, maybe going over another example or two that you could think of.

I mean, even this just at the very end was super helpful, like the correlation graph between the cognitive variables and the collected data.

So if you had any thoughts on maybe something, like a simple walkthrough or two, or maybe opening that up.

could be really great.

Yeah, just a direct application and the concrete examples super, super helpful.


SPEAKER_00:
Cool.

Sounds good.

Yeah, maybe if you find a paper that you like, then send to me and we can see if it's relevant.

Or maybe we can look at one of the papers in table nine one, or one, one or two of these ice arcade papers.

sure we can look at another one but yeah cool all right thank you everybody farewell thank you so much
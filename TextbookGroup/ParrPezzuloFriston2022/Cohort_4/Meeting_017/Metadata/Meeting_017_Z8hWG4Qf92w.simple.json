[
  {
    "start": 2.832,
    "end": 3.413,
    "text": " All right, welcome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5.556,
    "end": 7.338,
    "text": "It's October 17th, 2023.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 7.438,
    "end": 13.586,
    "text": "And we're in our first discussion of chapter eight in cohort four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 13.706,
    "end": 25.141,
    "text": "So to begin with, what are just any thoughts or memories that you fellows have about chapter eight?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 27.965,
    "end": 32.13,
    "text": "Just any, any piece that you remembered or",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 34.186,
    "end": 54.523,
    "text": " anything about eight um I was curious about the part where they talk about how movement and attenuation like sensory attenuation that was interesting to me where they I hope I remember calling this correctly but it was like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 55.448,
    "end": 83.532,
    "text": " you didn't want to perfectly predict it at a certain point for like to perceive movement which was interesting um I don't know if I perfectly understand that but uh I kind of see an intuitive aspect to that I guess cool yeah that's a very interesting section Andrew anything that that you",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 84.862,
    "end": 87.386,
    "text": " Think about it, yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 87.806,
    "end": 100.384,
    "text": "Yeah, I guess I'd second an interest in sensory attenuation as well, because I guess we could get to that, but like box 8.1 is kind of the key bit of interest there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 100.424,
    "end": 104.871,
    "text": "The relationship between precision, attention and sensory attenuation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 104.911,
    "end": 113.383,
    "text": "They seem to more or less equate precision and attention.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 114.207,
    "end": 131.612,
    "text": " I guess the idea is you have some kind of belief that is, you know, I am not moving and you need to downplay or decrease the precision of that belief such that you can actually move in the first place.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 131.892,
    "end": 136.198,
    "text": "Otherwise, if you have very high precision, I am not moving.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 136.297,
    "end": 158.676,
    "text": " um that kind of stays in place and so you never actually get started on moving um yeah no I thought that was really interesting and it comes up a little bit more later in the chapter on how that kind of plays out the movement of limbs and such so I thought that was pretty pretty fascinating yeah",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 159.483,
    "end": 182.453,
    "text": " totally and like where else does that apply like can we understand the example they give with a motor behavior connected to the actin fontology and then what other analogous um sensory attenuation cases might be happening let's just see if it's been explored in a question",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 190.465,
    "end": 199.576,
    "text": " We'll definitely look at this question about precision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 206.225,
    "end": 207.726,
    "text": "And then here's the box 8.1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 207.847,
    "end": 215.376,
    "text": "Okay, no answer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 235.623,
    "end": 240.207,
    "text": " What's interesting about it, Ajith?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 240.868,
    "end": 242.87,
    "text": "Or what would you want to know about it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 242.99,
    "end": 261.548,
    "text": "I guess it was just something where when I read it, it made sense, but I'm still trying to... Let me see if I can find it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 261.568,
    "end": 263.87,
    "text": "It made sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 263.99,
    "end": 265.191,
    "text": "It was just...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 266.555,
    "end": 287.685,
    "text": " interesting that we aren't like was it were they saying that the perception of movement itself like in order to perceive movement it was like we were decreasing how um while we were predicting uh those states",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 290.062,
    "end": 314.888,
    "text": " order to generate movement we must be able to ignore the sensory consequences which is to say disattend to it or attenuate to it yeah of that movement to form the initially false belief that i am moving okay okay yeah that's like just i don't know something about that's very interesting it's uh so",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 315.205,
    "end": 319.731,
    "text": " And then afterwards, are you we are trying to make up for that by.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 319.771,
    "end": 322.234,
    "text": "Trying to.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 324.157,
    "end": 324.837,
    "text": "predict those.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 325.759,
    "end": 328.943,
    "text": "or get those appropriate like inferences right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 332.107,
    "end": 343.221,
    "text": "it's not you're not you're going to start attending to it again, or is it you just decrease how much attention you're putting on to those states at first and then after your.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 344.787,
    "end": 349.514,
    "text": " Certainly it's an oscillatory pattern.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 349.975,
    "end": 356.003,
    "text": "So one analogy or another case where sensory attenuation is really important is the eye saccade.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 356.784,
    "end": 360.029,
    "text": "So when the eyes are fixed, the precision is high.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 360.971,
    "end": 367.42,
    "text": "So that changes in the visual scene when the eyes are fixed can be informative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 368.281,
    "end": 372.347,
    "text": "And then during head movement and or eye movement,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 372.935,
    "end": 398.487,
    "text": " kind of have like a hyper prior that pixel changes or or um are uninformative because they're they're not going to be very useful there's you're expecting high volatility on when there's movement so during that there's like a sensory attenuation during the cicade and then as soon as the cicade ends it's just like re-engages attention",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 398.467,
    "end": 411.781,
    "text": " So it's kind of like when the signal to contract the bicep occurs, there's also a descending relaxation of the tricep in the kind of healthy setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 412.342,
    "end": 414.765,
    "text": "So it's not just like pull on one side and pull on the other side.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 414.825,
    "end": 418.228,
    "text": "It's like a joint contraction and relaxation signal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 419.41,
    "end": 423.394,
    "text": "That's happening with two opponent muscles.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 424.454,
    "end": 449.01,
    "text": " whereas the isochate attenuation example is more like opponent processing with a motor and an attention variable but they they alternate okay so we have the isochate case with visual perception getting out of a chair",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 451.032,
    "end": 454.016,
    "text": " And there probably are other differences here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 454.497,
    "end": 464.39,
    "text": "Like, you could argue whether the eye saccade can... It's kind of like, it's considering the counterfactual.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 465.492,
    "end": 470.939,
    "text": "Just like the getting out of the chair is, I am not in a chair, is the counterfactual.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 471.941,
    "end": 474.825,
    "text": "The eye saccade counterfactual is like, I am looking over there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 477.568,
    "end": 478.069,
    "text": "But you're not.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 480.158,
    "end": 489.928,
    "text": " And so the belief distribution, ultimately you wanna shift your location in that space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 492.851,
    "end": 508.427,
    "text": "But if the current precision around the current state estimate doesn't have significant mass covering that counterfactual, then it can't be switched over to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 509.47,
    "end": 533.695,
    "text": " So sensory attenuation relaxes the attention, expands the variance, reduces the precision, makes it so it's like, instead of like 99% sure that it's this way, well, it's 80% sure that it's this way, 20% that it's that way, that opens the door to the counterfactual being realized through action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 534.215,
    "end": 537.739,
    "text": "Just to kind of restate it a bunch of ways, because I think",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 539.643,
    "end": 567.608,
    "text": " if it is the case that that's how um attention and attenuation work then probably it's going to be prevalent in like every sensory motor system because the alternative would be constant precision yeah and then would so constant precision would make it would actually make you less likely to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 568.178,
    "end": 594.102,
    "text": " um be able to process movement because there's a lot of uh it's almost like you're be able like less likely to get new information is that kind of a way of thinking about it is that why we're kind of relaxing the attention so we can be more flexible with sort of uh states that are changing very rapidly",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 597.187,
    "end": 597.908,
    "text": " I think that's true.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 598.188,
    "end": 601.171,
    "text": "So let's think about what would happen if you had constant precision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 601.831,
    "end": 608.958,
    "text": "Constant precision to your visual field in an isochate or constant precision to your proprioception for the getting out of the chair case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 609.719,
    "end": 616.826,
    "text": "Well, when your eyes were moving, just where would you fix your constant value?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 617.446,
    "end": 626.755,
    "text": "If you fixed it at the level that your surprise should be at for being fixed, then it'll be shocking when there's movement.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 627.207,
    "end": 639.155,
    "text": " If you calibrate to when the pixels are moving a lot, then you won't be able to glean as much information when they're not moving because you'll be disattending.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 639.376,
    "end": 643.225,
    "text": "Or you could pick a lukewarm value and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 643.205,
    "end": 670.174,
    "text": " kind of blend those two be a little bit more surprised than you should be when it's surprising and then a little bit less informative when your gaze is fixed so obviously an approach to resolve this tension is to have oscillation or some kind of conditional attention so that you can tune it and do the best you can do in each moment instead of trying to like fix a value",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 670.795,
    "end": 684.904,
    "text": " when we already know that there's a different optimal precision for different settings or like through time within the same setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 684.924,
    "end": 685.024,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 702.925,
    "end": 703.726,
    "text": " That makes sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 703.766,
    "end": 729.683,
    "text": "It's kind of like, uh, uh, trying to think of a good sort of analogy, but, uh, just like you're actually doing worse by it's kind of like, you know, you're grasping, like, have you ever heard of that analogy where you're grasping like a seed can't hold too hard or, uh, like slip out, but if you hold it too loose, it'll also slip out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 729.703,
    "end": 731.886,
    "text": "So you need to like adjust.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 733.857,
    "end": 758.177,
    "text": " yeah that's that's good connection that's also related to kind of optimal grasp like optimal grasp on a pen is not the maximum compression force for a variety of reasons you know you'd be exhausted you couldn't have um dexterous movement or something like that so like then that's related to like not over or under fitting um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 759.794,
    "end": 787.894,
    "text": " that's kind of the vfe imperative which is you want to fit a distribution given the family of variational distributions that you're choosing from gaussian or whatever it is you want to parameterize it so that if you were to make it any tighter you'd be overfitting and then if you made it any looser you would be having left information that you could have used so it's on some trade-off frontier",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 789.173,
    "end": 813.814,
    "text": " um which we can be along with the kl divergence so it's like a it's you know it's in a good spot and it's on a good optimizable manifold that if the variational family is at all appropriate you're going to get to the optimal grasp now maybe it's like um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 814.52,
    "end": 840.679,
    "text": " trying to do optimal grasp on something that's too small or too large to be grasped that would be like if the distribution family was inappropriate then you'd come to the best grasp you could of you know the mountain but there you it wouldn't be possible to actually do it so that kind of connects like the the model that doesn't under overfit still may have an adequacy of one percent",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 842.448,
    "end": 865.418,
    "text": " given the data and all these other things and then and then one other connection there is like um the kalman filter or just generalized filtering so we're getting measurements from the thermometer through time one or more thermometers through time and then we're tracking the temperature distribution um or stock prices or something",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 866.444,
    "end": 894.51,
    "text": " then you want to be tracking both the expected mean temperature and the variance like the volatility or just the uncertainty the the sort of fundamental uncertainty or volatility and then also the measurements associated so the measurement associated volatility is more like a matrix the actual Dynamics are more like B but and so but",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 895.807,
    "end": 918.422,
    "text": " whatever extent the noise or the variability comes from either the sensor measurements or the underlying dynamics of the system, you want to be fitting this curve through time such that new noisy data measurements are not falling outside the curve, because then that's more surprising than it should be, then you should widen the curve.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 918.638,
    "end": 932.155,
    "text": " Or if all the data are falling like within the plus or minus one standard deviation on the curve, then you could have done better by tightening the distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 935.92,
    "end": 945.051,
    "text": "Sanjeev's work on the generalized filtering is really nice on this, on the generalized filtering.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 946.634,
    "end": 957.893,
    "text": " kind of connecting into the base Bayesian optimal filtering in perception and then bringing in action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 960.117,
    "end": 980.083,
    "text": " This also kind of reminds me of exploration, exploitation, trade-off and reinforcement learning and soft value functions, where they actually, instead of just trying to optimize a reward, they try to also increase the entropy, like an entropy value.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 980.148,
    "end": 985.323,
    "text": " So which seems kind of counterintuitive, but it's actually performs really well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 985.343,
    "end": 990.417,
    "text": "It makes it very flexible and able to deal with perturbances specifically.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 990.618,
    "end": 995.592,
    "text": "So like disturbances, like if you have a robot that's",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 995.572,
    "end": 1022.102,
    "text": " trying to work in an environment where things can start disturbing it and things like that having sort of that like trying to optimize that entropy value as well as the reward value makes it so it can actually adjust and uh deal with disturbances a lot better which kind of reminds me of like movement also it's kind of like disturbances to sort of your oh yeah",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1022.842,
    "end": 1026.848,
    "text": " Yeah, and there's two kinds of motor disturbances.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1027.629,
    "end": 1030.353,
    "text": "Ones you have agency over, ones you don't have agency over.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1030.973,
    "end": 1032.395,
    "text": "Which ones are more surprising?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1033.637,
    "end": 1034.839,
    "text": "The ones you don't have agency over.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1035.079,
    "end": 1045.494,
    "text": "Not a recommendation, but you can slightly push the eye so that it's kind of like a non-saccade-induced visual shift.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1046.115,
    "end": 1047.136,
    "text": "Of course, that's surprising.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1048.979,
    "end": 1051.903,
    "text": "Or if you can't tickle yourself,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1052.17,
    "end": 1072.693,
    "text": " because the expected proprioceptive consequences of tickling yourself are not attended to because they're expected so they're confirmed um or yeah just how we expect how far do we expect the stare to be",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1074.158,
    "end": 1076.761,
    "text": " If it's a little bit more or less, then it's surprising.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1077.803,
    "end": 1085.472,
    "text": "So then just the expected consequences of action are normalized too.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1085.492,
    "end": 1095.785,
    "text": "I mean, sensations are perceived, calibrated to what their expected states are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1096.907,
    "end": 1099.39,
    "text": "This is kind of the whole predictive processing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1099.41,
    "end": 1102.714,
    "text": "Everything is like...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1102.981,
    "end": 1131.958,
    "text": " sensory attenuation is kind of a corollary of a first principles corollary of this kind of action and perception as inference and then it just like um uh predictive coding in the visual system this is empirically found in different systems and then it'd be interesting like what other phenomena",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1133.356,
    "end": 1160.781,
    "text": " are like where do we see allostasis or attenuation in um in other systems like if you have um the bacteria and the gradient of sugar or like antibiotics or something it recalibrates to the value with like a kind of log sensitivity so that it can always have optimal grasp on making a decision",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1161.79,
    "end": 1189.536,
    "text": " about the gradient if the gradient were a thousand times longer than the bacteria it couldn't detect it if the gradient was a millionth of the wavelength of a bacteria it couldn't detect it but then there's some range that it can actually do the calibration that gives the most statistical resolution just like the aperture of a camera or the noise level of a sensor",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1191.035,
    "end": 1207.501,
    "text": " And so then when you're calibrating the microphone or whatever it is, the two minimal things, or the one minimal thing is just the mean, or that's like the delta, the Dirac delta function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1207.521,
    "end": 1219.219,
    "text": "If you can only estimate one parameter about to calibrate the microphone in the room, you would want to know potentially the average or the mean or the median volume.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1220.735,
    "end": 1223.818,
    "text": " And then the second thing would be a variance parameter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1226.54,
    "end": 1241.654,
    "text": "Those might be jointly estimated for certain one parameter families of distributions, like Poisson distribution, where you only use one parameter to describe the family.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1242.855,
    "end": 1249.881,
    "text": "And then that is like where like the mean and the variance are equal is a one parameter estimation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1251.481,
    "end": 1278.349,
    "text": " but in other case the kind of two that are helpful are the central tendency and then the variance it doesn't have to be Gaussian but you kind of need those two because if you don't have a measure of this of a central tendency and then how wide a field of the central tendency then it's like you're kind of saying that it's a diverging situation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1280.287,
    "end": 1282.29,
    "text": " so that it's not really identifiable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1287.397,
    "end": 1297.732,
    "text": "Good stats, pieces, all of regular, these are common topics in other statistics settings.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1298.974,
    "end": 1303.56,
    "text": "And it's kind of cool because they're often discussed in the signal processing",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1304.789,
    "end": 1333.648,
    "text": " inbound passive inference side common filter all the variational autoencoder and then also in in a kind of slightly siloed way in the control theory optimal control KL optimal control way so then those two statistical problems can be understood as kind of like two complementary",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1335.282,
    "end": 1359.838,
    "text": " special cases there's optimal signal processing when you have no agency or no action control and then there's optimal control in a completely known environment where there's no epistemic value which is like the figure from earlier with showing expected free energy and then when certain things are included or not what special cases results",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1367.986,
    "end": 1390.888,
    "text": " So sensory attenuation, it's a useful tractable motif that does that, that, uh, addresses the, the, the kind of realistic statistical problems by modulating and learning attention.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1393.737,
    "end": 1407.874,
    "text": " Another thing I was curious about was the generalized synchrony where they're talking about the birds doing the bird song and then the other one would try to finish the other segments of the song.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1408.335,
    "end": 1421.01,
    "text": "It just kind of started going into something I'm like been interested in, which is like communication and even like linguistics and just",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1421.564,
    "end": 1443.167,
    "text": " like how that seems similar to communicating language and like when you talk to like a language model you're like people a lot of people say it's just predicting sort of the best response or what's the most likely next response um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1443.147,
    "end": 1463.383,
    "text": " and they kind of just remind me reminded me of like this generalized synchrony where the birds are singing the song and then the next segment of the song would be sort of it's kind of like a response almost to the other bird if there's like two birds singing um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1464.848,
    "end": 1482.087,
    "text": " yeah I guess I was just like curious and like how that and communication works like is this like the very most basic form of communicating in active inference like between two sorts of agents.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1484.25,
    "end": 1484.63,
    "text": "Like.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1486.872,
    "end": 1487.994,
    "text": "yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1488.014,
    "end": 1488.915,
    "text": "yeah good questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1490.256,
    "end": 1490.537,
    "text": "Andrew.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1492.339,
    "end": 1493.64,
    "text": "Oh yeah I just wanted to.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1494.852,
    "end": 1504.526,
    "text": " I previously skimmed a paper by Friston and Parr, and they actually reference it here at the bottom of 165.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1505.928,
    "end": 1510.715,
    "text": "They built another model that basically has two agents communicating with one another.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1510.735,
    "end": 1519.408,
    "text": "And with Synchrony, I'd rather go with the birdsong example, I guess, since it's more top of mind.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1519.54,
    "end": 1536.368,
    "text": " um it's just it's rather fascinating there's this um described like the birds are basically learning one another's model parameters right such that they start making virtually the same",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1536.837,
    "end": 1539.599,
    "text": " inferences, you know, one is with the other.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1539.639,
    "end": 1553.071,
    "text": "I think this is a wonderful kind of example, too, because I can imagine two musicians, you know, wanting to play the same song, staying in time with one another, kind of following the same script or sheet music.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1554.172,
    "end": 1566.422,
    "text": "But yeah, I'll try to find that that paper just because I think it's if you're if you find interest in it, it's you know, it's more based on potentially what humans would be doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1567.094,
    "end": 1572.842,
    "text": " It includes an entire model of how do you construct in communication.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1573.123,
    "end": 1574.084,
    "text": "What is a question?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1575.106,
    "end": 1584.46,
    "text": "What are the different states involved with questions, asking one another questions, answering questions, and the like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1584.48,
    "end": 1586.082,
    "text": "I'd love to check that out, yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1586.197,
    "end": 1587.079,
    "text": " Yeah, that's cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1587.721,
    "end": 1595.662,
    "text": "Yeah, you even said like scripts and Albert Austin at all with the strong and weak scripts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1595.682,
    "end": 1599.953,
    "text": "So you have two musicians that could be they could literally have the same sheet music.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1600.692,
    "end": 1630.395,
    "text": " that could be truly isomorphic sheet music like they're both playing the first trumpet part on this one song at the same time it could also be a general synchrony which is to say not necessarily a lockstep maybe it's a call and response so there's still a perfect mutual information between the two sheet music readings that are ongoing even though they might be playing different instruments or they might be playing different parts so that's like what you",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1630.527,
    "end": 1659.235,
    "text": " in that birdsong example and subsequently they've referred to as like the singing from the same hymn sheet and then kind of stepping out into the improv is another step that I know some music and other researchers are interested in because like with questions or just other um turn-taking mechanisms because obviously it's different when there's epistemic transfer in communication of of whatever kind",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1659.687,
    "end": 1665.736,
    "text": " versus just the co-performance of something that's fixed ahead of time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1666.717,
    "end": 1667.919,
    "text": "So that's one key piece.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1668.019,
    "end": 1672.946,
    "text": "And then the point about the large language models is a really good one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1673.967,
    "end": 1685.464,
    "text": "So the likeliest continuation of this trained model, it's almost like the current token is Y, that's the observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1686.285,
    "end": 1689.289,
    "text": "And then we have like Q, big,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1689.269,
    "end": 1713.268,
    "text": " foundation model Q now it's not necessarily a variational Bayesian model but then there's going to be some variational free energy landscape and then when the temperature is low it picks simply the next likeliest outcome and then when as the temperature increases you flatten that landscape so it starts sampling like more broadly",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1714.919,
    "end": 1738.513,
    "text": " to the point of breaking out of kind of semantic ruts but decreasing potentially like the fluency or the coherence so that that's like we have the path of least action and then we have that temperature or shaky hand parameter because in language selection of topic and sentence and word and phoneme and all of that is like this nested policy selection task",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1739.253,
    "end": 1746.923,
    "text": " And then, so we have both the path of least action, that's kind of the ridge on the bottom of the valley.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1746.963,
    "end": 1764.045,
    "text": "And then we have this like sampling technique where you can sample along the strictest, likeliest path of continuation, or you can sample more broadly, like from that valley.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1765.967,
    "end": 1767.589,
    "text": "So it's an example of how like,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1767.94,
    "end": 1794.792,
    "text": " statistics or active inference approaches could be taken even if GPT for whatever doesn't need to be applying active inference from the inside but we could still talk about its expectations or preferences and how that conditions its actions and all these different aspects but it's mapped on territory so we'll be doing like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1794.974,
    "end": 1818.973,
    "text": " cognitive mapping of gpt4 not reverse engineering or engineering from the inside so the birdsong example is a singing from the same hymn sheet example where there's an expected and so it's an interesting and a good good continuation from the sensory attenuation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1820.27,
    "end": 1827.878,
    "text": " they both have an expectation in their generative, each bird in its generative model of how the song is supposed to go.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1829.42,
    "end": 1835.005,
    "text": "And then the question is, are they going to be singing that song or not?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1835.806,
    "end": 1840.011,
    "text": "And so in this example, it's like, well, like someone should be singing it this way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1840.031,
    "end": 1842.613,
    "text": "And then it finds itself singing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1842.734,
    "end": 1848.96,
    "text": "And then as it continues to sing, takes its turn, its precision drops.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1850.29,
    "end": 1860.267,
    "text": " we could talk about why precision might drop through time and then eventually that leads to a uh consideration hey maybe I'm not the one continuing this sock",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1861.006,
    "end": 1866.993,
    "text": " And then that's like the kind of turn-taking mechanic, but it's not the only turn-taking mechanic that you could design.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1867.433,
    "end": 1873.76,
    "text": "You could use like a stop, you know, word like over or something like that, or start words or, you know, you can make any mechanism.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1874.461,
    "end": 1882.109,
    "text": "And then I think they show in that first paper with Frist and Frith or in some other papers, like maybe what might have you mentioned about like the learning of the model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1882.61,
    "end": 1888.056,
    "text": "And in that case, there was like a younger bird, like a more plastic bird with lower precision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1888.757,
    "end": 1890.979,
    "text": "And then over multiple rounds,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1890.959,
    "end": 1908.428,
    "text": " it learnt how the song was supposed to be and then the first bird was taken away and then the new one was added in and they showed that basically you could do like this successive passaging of entrainments in the generalized synchrony set",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1917.132,
    "end": 1933.015,
    "text": " so they mentioned like this leads to the birds trying to like match their internal models um by trying to match how they continue singing and I was",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1932.995,
    "end": 1951.442,
    "text": " I mean, I definitely want to check out like how it was like all of this was like how first and all of them connected this to communication because like I guess I'm like wondering like how you communicate bits of information directly because",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1952.485,
    "end": 1979.05,
    "text": " like it makes sense you match the internal models and then you uh I guess like you take turns and you communicate the next section and then um I guess what informs the other bird just like from like a sort of base like information Theory type uh perspective it's like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1979.62,
    "end": 2007.703,
    "text": " now the other bird knows that they're singing the same song um I guess that is like some information that's being communicated and then um yeah it's like matching the same song and then I guess uh I guess that is kind of like an analog for language if you consider every idea or concept a song and you're trying to match with the same concept",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2008.915,
    "end": 2011.602,
    "text": " or like some communication that you're trying to communicate.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2011.622,
    "end": 2021.467,
    "text": "I think there's some, I was just going to say, I think there's some theory somewhere.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2021.487,
    "end": 2023.011,
    "text": "I don't want to misquote someone.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2023.412,
    "end": 2025.979,
    "text": "I think it was from a neuroscientist.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2026.938,
    "end": 2043.613,
    "text": " But the idea that whenever two people are communicating, it seems to be the case that including using like neurophysiological measures like EEG or otherwise, that there tends to be an increasing kind of synchrony between them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2043.593,
    "end": 2048.24,
    "text": " Um, that includes in the, in the kind of spectral dimension, like frequency.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2048.821,
    "end": 2057.774,
    "text": "Um, like, if you did like a spectral decomposition of the EG and look at the frequencies going on, like, their, their frequencies tend to slowly begin to match one another.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2057.814,
    "end": 2066.867,
    "text": "Um, and furthermore, if you kind of analyze the contents of the conversation, which I'm not quite sure how you do this with high precision mathematically, but",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2066.847,
    "end": 2078.618,
    "text": " The idea is like two people communicating with each other are slowly reaching some kind of convergence or consensus in what the conversation is about, about agreements upon the topic.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2079.178,
    "end": 2085.404,
    "text": "Even if you disagree at some point, you do converge in the idea of, well, now I know your stance and I also know my stance.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2085.444,
    "end": 2088.647,
    "text": "So at least that kind of truth has been converged upon.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2088.787,
    "end": 2094.913,
    "text": "So I guess to sum it up, like communication is some kind of general",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2095.467,
    "end": 2103.797,
    "text": " the process of convergence or synchrony of information, of brain frequencies.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2104.337,
    "end": 2120.115,
    "text": "And I guess here with chapter eight, synchronization of chaos, because I guess we haven't really gotten into that, but using the Lorentz system as one potential continuous model as well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2121.657,
    "end": 2123.179,
    "text": "Just sharing all that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2124.323,
    "end": 2136.107,
    "text": " That reminds me of like how communication in like wolframs really add is sort of like between observers is kind of explained, it was like.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2137.59,
    "end": 2144.223,
    "text": "Between the observers, they have like shared like segments of the really add that they can both like kind of agree upon.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2145.987,
    "end": 2146.247,
    "text": "hmm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2147.071,
    "end": 2169.888,
    "text": " which kind of reminds me of sort of like trying to match your internal model or, you know, like what you were saying, like, at least you converge that you both are at, or some understanding, even if it's on an agreement, it's like you're converging on how your stance is related to the other person's stance or like that relationship itself is kind of like a concept on its own.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2171.01,
    "end": 2171.07,
    "text": "Um,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2172.62,
    "end": 2200.551,
    "text": " is very interesting it's like we each we we're putting both of our cards on the table now we can see each other's cards so to speak like that information has not been shared um right even if the information that you brought to the table uh was different uh right now you've kind of come together here you have shared information and furthermore you are in the an instance of a particular conversation right so it's yeah yeah",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2201.83,
    "end": 2217.578,
    "text": " and like in information theory like communication channels um it's kind of like um you're trying to it I mean a lot of this chapter reminded me a lot of communication channels from information theory just like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2217.98,
    "end": 2224.235,
    "text": " trying to know what the original input was that goes across the channel, and then you get an output.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2224.415,
    "end": 2233.457,
    "text": "But the output might not be the same as the input, and you have to see if it matches, and if it's if there's noise, then there might be some variation with the output.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2233.497,
    "end": 2234.74,
    "text": "So it's just",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2234.72,
    "end": 2258.922,
    "text": " kind of like trying to decrease that noise and then that's why you calculate the capacity of what sort of the Max rate of bits you could communicate across this channel without sort of um where you know for a fact that um you'll be able to map it directly to sort of the original input or like the cause no",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2263.087,
    "end": 2289.685,
    "text": " so yeah a ton of great connections I think it's going to be really exciting to see the quantum FEP Chris Fields semantic information flow meet up with the wolf from rulliad I think that should be impending and but um yeah and then there's kind of like the the agent's environment communication that's Alice and Bob",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2290.694,
    "end": 2312.652,
    "text": " And then sometimes we can kind of take the Alice as agent, environment as agent, Bob as agent, and then just sort of like summarize it with just the holographic screen with the classical information, which is kind of like saying that there's an inert intermediating entity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2312.918,
    "end": 2322.092,
    "text": " So quantum on either side of the boundary and classical or n-dimensional on the boundary and then n plus dimensions in the quantum.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2323.314,
    "end": 2331.968,
    "text": "And then there's simpler examples too, like having a bunch of metronomes on a table.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2332.929,
    "end": 2340.04,
    "text": "So if the table is fixed and then there can't be like force transduction amongst the metronomes,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2340.104,
    "end": 2366.905,
    "text": " they will continue unabated with whatever population distribution is initiated and then if there is coupling non-zero coupling then like that's the kind of famous videos where they enter into lockstep they're simple or conservative active things so the general synchrony that they come to is like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2368.133,
    "end": 2392.943,
    "text": " either all aligned or a two phase but then I've also seen it where it's two phase and then it kind of falls a little bit off and then it actually slides all the way to back to lockstep because that's a slightly more favorable synchrony but they're all exact versus one where say it's like 50 50 or 99 one with one going in the counter face so that's kind of like the simplest generalized synchrony",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2394.256,
    "end": 2404.045,
    "text": " And then it's a huge open topic with semantics and linguistics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2404.065,
    "end": 2408.097,
    "text": "Seems like there's a lot of information in the sequences of tokens.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2409.308,
    "end": 2410.089,
    "text": " How many bits?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2410.129,
    "end": 2410.77,
    "text": "How many NATs?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2410.81,
    "end": 2411.571,
    "text": "Relative to what?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2412.332,
    "end": 2414.275,
    "text": "But also, it's not just the sequence of tokens.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2415.056,
    "end": 2424.67,
    "text": "We know that in our generation of language and seemingly our perception that, of course, the timing, tone, context, all these other features come into play.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2424.69,
    "end": 2427.554,
    "text": "So then what is their mutual information?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2429.336,
    "end": 2434.303,
    "text": "And there's settings where maybe just the tokens are necessary and sufficient.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2434.722,
    "end": 2439.267,
    "text": " There's another situation where there might not even be a token, like a silent sign.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2439.808,
    "end": 2440.789,
    "text": "What does that say?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2442.431,
    "end": 2446.616,
    "text": "So then, yeah, these are great questions of communication.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2446.656,
    "end": 2455.667,
    "text": "I think we'll continue to see like in a lot of different fronts, like ratcheting of the, okay, now sophisticated and the affective.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2456.227,
    "end": 2459.231,
    "text": "And now then they have these models of the nested.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2459.771,
    "end": 2460.132,
    "text": "Look at this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2460.172,
    "end": 2464.577,
    "text": "This is like figure 4.3, identical on top.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2465.182,
    "end": 2472.52,
    "text": " And then the bottom, whereas in figure 4.3 in the textbook, the bottom is the continuous time, gm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2473.603,
    "end": 2481.402,
    "text": "Here, we see the discrete time, gm, converted into the Forney factor graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2484.233,
    "end": 2487.379,
    "text": " And that supports the message passing more approximately.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2488.06,
    "end": 2491.066,
    "text": "But we're dealing with this exact same discrete time model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2491.687,
    "end": 2497.638,
    "text": "And then as we see with like the folk psychology models, often the more cognitive or symbolic tasks are discrete.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2498.299,
    "end": 2502.447,
    "text": "And then the hybrid model is like where it hits the ground, it's continuous.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2502.527,
    "end": 2505.533,
    "text": "Like for example, the larynx,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2506.103,
    "end": 2529.833,
    "text": " um contraction or voice box or lip or tongue might be a continuous time continuous actuation model so you have some continuous um um voice apparatus with appropriate reception in the actuation making phonemes but then once you're in the space of identifiable phonemes",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2530.353,
    "end": 2542.605,
    "text": " or lexemes, then you can be at this level where they're now making discrete time models at the symbolic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2542.985,
    "end": 2545.087,
    "text": "So they've abstracted away from the phoneme.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2545.748,
    "end": 2546.849,
    "text": "They're not dealing with I hear.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2547.59,
    "end": 2549.592,
    "text": "That's kind of implicitly underneath it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2550.653,
    "end": 2557.98,
    "text": "They're just saying that they discreetly produce and recognize these words.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2559.175,
    "end": 2561.739,
    "text": " But then that's what's so nice about the composability of Act-Inf.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2561.759,
    "end": 2563.282,
    "text": "Okay, now here they're taking it deep.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2564.204,
    "end": 2568.932,
    "text": "We see that for three, but now there's each time step has three nested.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2569.733,
    "end": 2571.657,
    "text": "So that's discrete within discrete nesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2572.398,
    "end": 2582.415,
    "text": "And then you could also just say, instead of the observation being a discrete token, you could plug in a continuous model there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2583.712,
    "end": 2609.331,
    "text": " then you could evaluate you know empirically or what are the computational requirements or where is it where are you getting more um value add and i think well the chaos thing that's explored a lot in the uh stochastic chaos and markov blankets paper",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2610.323,
    "end": 2636.702,
    "text": " I think 34 livestream 34. and then it's interesting how like even though a lot and I think Thomas Parr may have remarked on this in the book stream 2.1 like the earliest active models were the continuous time and then to accommodate classically symbolic cognitive processes and also explicit planning as inference",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2637.307,
    "end": 2642.556,
    "text": " And also to integrate with the POMDP, well known in planning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2643.577,
    "end": 2647.303,
    "text": "There was a movement towards the discrete.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2647.323,
    "end": 2649.467,
    "text": "And then there's DaCosta et al.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2649.487,
    "end": 2652.672,
    "text": "2020, like a synthesis of active inference and discrete state spaces.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2652.732,
    "end": 2663.55,
    "text": "So not like it's like finished and people continue to develop new motifs and discrete compositions, but kind of like the fundamentals of the discrete were then focused on more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2663.952,
    "end": 2693.014,
    "text": " And then potentially now with the path-based formulation of Bayesian mechanics, G-theory, everything that Dalton and Maxwell at all are working on, there's kind of like a renewed emphasis and generalization on continuous structures like paths, sheaths, bundles, topos.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2695.441,
    "end": 2721.381,
    "text": " things that the book doesn't necessarily go into but sets the stage for and then figure 8.6 kind of like um shows in the simplest possible example which is the same one that live stream 46 the act inf does not contradict folk psychology the simplest hybrid model where you have a discrete higher level symbolic",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2721.698,
    "end": 2732.535,
    "text": " um dimension and then this kind of like sensory motor continuous time so this is like figure 4.3 gets smushed together",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2755.786,
    "end": 2772.416,
    "text": " And then, yeah, I mean, just in terms of the location in the book, second half is the more pragmatic model, you know, construction and implementation based focus, starting with chapter six with the recipe.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2773.758,
    "end": 2777.946,
    "text": "And then just like the low and the high road were sort of this dialectic",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2779.85,
    "end": 2808.574,
    "text": " that that are unified or contained you know of or by active inference in seven and eight we have that distinction and then again with with this figure 8.6 by the end of chapter 8 we're already landed with the synthesis of seven and eight whereas two and three they really are like different branches and they meet in the middle and seven and eight it's more like I mean they're just",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2809.55,
    "end": 2833.768,
    "text": " tucked together at the end of eight and then chapter nine introduces okay well what if instead of specifying a gm hashtag chapter six and then generating synthetic data what if we already had collected data of a certain structure we knew that we were going to collect data of a certain structure how could we then prepare a gm that would recognize that data allow us to do statistics on it",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2834.356,
    "end": 2839.944,
    "text": " whether we do discrete, continuous, or hybrid models, chapter nine, and then the end of the book.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2841.847,
    "end": 2857.81,
    "text": "So it's just like so funny how second half starts fast, describes a few key, oh, here's the vertebrates, here's the invertebrates, or whatever, here's the kingdom of the fungi, here's the other kingdom.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2859.853,
    "end": 2862.557,
    "text": "Then chapter nine gets more empirically oriented,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2864.393,
    "end": 2869.778,
    "text": " And chapter 10 simply reviews and contextualizes active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2871.64,
    "end": 2872.641,
    "text": "So it's a quick ride.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2874.523,
    "end": 2875.504,
    "text": "It is very fast.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2882.591,
    "end": 2883.973,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2883.993,
    "end": 2885.454,
    "text": "Let's just see what the other questions were.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2885.734,
    "end": 2893.202,
    "text": "How is five figure five, two related to figure eight, four, five, two hierarchical predictive processing eight, four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2895.292,
    "end": 2922.868,
    "text": " lorenz synchronization we should we should procedurally generate every single possible question how is every figure i mean um i mean for a brief second i thought maybe it would relate it to eight point figure 8.1 which is a little more i don't know um not quite message passing but at least it's",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2923.473,
    "end": 2944.873,
    "text": " you know, we're still in the realm of looking at error terms, basically these basic equations with Lauren's, to go with Lauren's systems though and relating that to message passing, I'm not so sure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2945.461,
    "end": 2974.962,
    "text": " there's no clear there's no yeah it's nowhere my brain quickly goes well that's the attenuated space where any account could be possible or more accounts could be possible like again to this kind of question prompt question answer or even hegelian problem reaction solution structure it's interesting like which questions just have",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2976.427,
    "end": 3004.51,
    "text": " an answer which ones are like the grand canyon where the continuation there should only be like one specific continuation like what's the relationship between policy and affordance there may be a formal answer not even to say that that would be the only answer still but there might be a sumo expressible necessary sufficient answer which is like pretty good",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3005.333,
    "end": 3016.185,
    "text": " But then for, for, for any question, but especially some of them, these questions are basically just evocative, which is totally fine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3016.205,
    "end": 3016.886,
    "text": "I think it's fun.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3016.946,
    "end": 3024.074,
    "text": "We're only gonna experience a, a, even just with a 30 or 50 or how many figures there are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3024.094,
    "end": 3028.399,
    "text": "I mean, that would be like 2,500 questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3029.46,
    "end": 3029.62,
    "text": "Mm-hmm .",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3031.152,
    "end": 3058.847,
    "text": " so so just like then so that's the epistemic foraging which question should we ask prompt engineering conversation Theory all these different things like folding back into our the way that we're learning active like okay now that we've looked at what are the causes and consequences of sensory attenuation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3060.076,
    "end": 3063.6,
    "text": " Now, what are we going to pay more or less attention to?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3063.62,
    "end": 3071.688,
    "text": "Or now that we've talked about communication along these lines, then how do we do what?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3071.708,
    "end": 3079.455,
    "text": "A lot of fun options there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3081.457,
    "end": 3082.919,
    "text": "Here's the improvisation question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3084.821,
    "end": 3085.061,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3087.023,
    "end": 3088.024,
    "text": "I'll just move it to this one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3097.066,
    "end": 3103.676,
    "text": " Any, any, uh, last thoughts?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3107.962,
    "end": 3109.304,
    "text": "Um, yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3109.325,
    "end": 3122.905,
    "text": "So in chapter nine, is it more about like using active inference on sorts of like, if you want it to use it on like a dataset and to detect something or, you know, things like that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3125.146,
    "end": 3129.01,
    "text": " Um, yeah, basically it points that way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3129.411,
    "end": 3133.716,
    "text": "It is not the guidebook or the Google CoLab notebook or whatever.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3133.776,
    "end": 3135.598,
    "text": "That's just, oh yeah, load in your CSV.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3135.778,
    "end": 3137.5,
    "text": "And then this is just going to analyze the data for you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3137.52,
    "end": 3141.464,
    "text": "So it's not to that point, but what I think nine does, it's important.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3141.985,
    "end": 3148.572,
    "text": "First, 9.1 is a schema of the behavioral scientific setting, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3148.94,
    "end": 3161.792,
    "text": " where you have a behavioral science laboratory making a cognitive model of the rat or the ants or whatever, which might be a map back of the environment, but it frames it in a way that we can handle with Act-Inf.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3162.473,
    "end": 3176.787,
    "text": "And then analogous to how chapter six gave a recipe for making the GM, figure nine two gives us the roadmap from going from the data plus the GM",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3178.235,
    "end": 3201.66,
    "text": " to basically the kind of figure that would be in a published paper like we proposed um a personality variable or a psychometric variable of this kind continuous or discreet or whatever it was we had the generative model we had these data and likelihood function we inverted the model in light of our prior beliefs",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3201.995,
    "end": 3205.763,
    "text": " which might be uniform across groups or whatever it was in the setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3206.805,
    "end": 3212.236,
    "text": "And we think, and this is kind of like Ryan Smith's precision psychiatry work, you'll see a ton of things like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3212.817,
    "end": 3219.43,
    "text": "It's like, and people who had this or this categorization or this or that continuous variable",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3220.793,
    "end": 3229.323,
    "text": " here was the correlation or the regression between our inference about this parameter and whether or not they were diagnosed this way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3230.224,
    "end": 3235.711,
    "text": "And here's the t-test and it looks like something that you'd find in a paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3236.692,
    "end": 3250.429,
    "text": "In other words, supporting an empirical claim, being part of a fabric of evidence about the empirical system, rather than just having a hypothetical GM",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3251.455,
    "end": 3254.56,
    "text": " And then speculating like it could be this way or it could be that way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3255.662,
    "end": 3259.749,
    "text": "Because I mean, and until you parameterize it and especially for this data, because it usually could.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3259.769,
    "end": 3267.161,
    "text": "So kind of like a six and nine, a little bit of a Jimi Hendrix thing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3270.647,
    "end": 3270.907,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3272.069,
    "end": 3272.33,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3274.033,
    "end": 3274.754,
    "text": "Thank you, fellas.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3274.994,
    "end": 3276.096,
    "text": "See you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3277.882,
    "end": 3282.576,
    "text": " next time see you next week bye bye thank you",
    "speaker": "SPEAKER_01"
  }
]
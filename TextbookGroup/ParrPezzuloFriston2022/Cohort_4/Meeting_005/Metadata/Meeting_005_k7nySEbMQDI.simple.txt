SPEAKER_06:
Hello everyone, it's July 4th, 2023, and we're in our first discussion on Chapter 2.

So, let us head over there, and before we jump into anything specific, does anyone have any general reflections or thoughts on Chapter 2?

just any any pieces that stuck with them from their reading or just overall reflections from chapter two on the low road yes if i could go i'm not sure whether you guys yeah please um


SPEAKER_04:
I was curious in terms of the equations that relate to free energy.


SPEAKER_05:
So the manifestations of free energy in those three or four equations.

Mathematically, how are those derived?

Because in the textbook,

they kind of just seem to be axiomatically placed there.

But I was wondering, what is the underlying mathematical root of those equations from which we work?


SPEAKER_03:
Thanks, Ali.


SPEAKER_02:
Sylvain Perron, Thank you yeah the derivations is before equations at least 2.5 and 2.6 is.

I mean, we've done basically complete steps for the derivations of each of those equations.

And I'll upload a file explaining each step after the meeting if you want.

But I'm not sure where to put the files, Daniel.

Could you please?

I mean, where do you think would be most helpful to put those files?

What file format is it?


SPEAKER_06:
uh it's a pdf so what one option is to just at the top of the notes you could always just drag in a pdf all right just drop it here another option is to um in the github repo for the textbook to add it there in some format um but yeah certainly dropping a pdf or taking screenshots in here would be awesome but yeah this is a great question


SPEAKER_02:
And one more thing about derivations of these central equations is they're not necessarily

Roozbeh Gharakhloo, evidence from the textbook or the way they're expressing the textbook how we can get from each line of the equations to the next slide so I agree, we need some intermediary steps and even we need to employ some mathematical.

some mathematical theorems and rules such as James' inequality rule to get to some of those equations.

But I hope, I mean, the derivations we've worked out would be helpful if people want to follow the exact details of each derivation.


SPEAKER_06:
Yeah, that's awesome work with, um, Ali and Jonathan and Jakob also did some of these preliminary, um, connections for a few of the equations.

Cool.

Oh, and we're gonna come to, to these equations and we'll talk about them a lot more soon, but any other just overall chapter two thoughts or, or reflections.


SPEAKER_00:
um i thought something that was interesting to me is uh there's kind of a however loose identification between an agent in active inference and their priors this is pulling from page 39 that we can say in active inference the identity of an agent is isomorphic with its priors or the relationship between

I suppose, the agent and their prior preferences.


SPEAKER_06:
Yeah, the priors are what the thing brings through the needle of the present to confront the incoming observation to then be updated.

into the posterior but then that just becomes the prior for the next moment so like if it's not coming with you through the priors it's not making it through the bottleneck of the present it has been forgotten so everything has to come through the presence in order for the past to influence the future that's like the Markovian property of like a Markov chain so not Markov blanket necessarily but actually that is a Markov blanket through time

past only influences the future through the present.

So everything has to flow through the present in order to have influence.

But that is a very interesting claim.

Any other thoughts or ideas on two, just overall?

Cool, we'll look at some questions, but just the big picture for the next month, I guess for all of July, two weeks on chapter two, that's the low road, and then two weeks on chapter three, that's gonna be the high road.

So first we're gonna talk about how, and this is gonna get us from the basic tautologically true Bayesian theorem

all the way on through heuristics that are used to fit Bayesian models of perception and action, that's gonna get us to active inference.

And then we're gonna kind of like take a breath, reset, start not from the how on the low road, but the why on the high road.

And that's gonna get us to the same place.

And then that will drop us off at chapter four, which is going to be like another level of depth into the generative model that is used at active inference.

First, I'll just check.

Okay, cool.

So I'm going to chapter two specific questions for cohort four.

You can just follow the little token if you want.

Okay.

All right.

Why does the minimization of divergence not also lead to the maximization of model evidence?

Are you not generating new accurate priors that are evidenced by the incoming sensorium?

So if the person who asked this is here, they can unpack it a little bit or anyone else can provide a thought or an idea on this.


SPEAKER_05:
yeah so i wrote this just uh with an eye on the equation um maybe it'd be good to get it up if possible but no worries if not um which has the kind of action perception uh dual aspect so the first bit is for perception with the kl divergence and the second bit is with action

Gareth J. My question, therefore, is if we are getting closer to optimal Bayesian inference would you lowering the bound on free energy.

Gareth J. Surely optimizing or getting close to optimal Bayesian inferences therefore driving us to update our priors in a way that will be.

that in a way that basically perfectly reduces prediction error and therefore minimizes free energy so I wasn't sure whether is it just that in reality action and perception are always coupled or is there um is there something you know is there a limit to optimizing perceptual inference

yeah great um topics which equation were you referencing just so we can look at it so it's so if you go back on the textbook um it's worth two so it's on 2.5 and but it's the last one I think so keep going oh sorry 2.6 um yeah so it's the divergence evidence one


SPEAKER_06:
okay okay okay great all right equation 2.5 variational free energy okay so first just to just to give a thought again anyone can raise their hand up if they if they want to add more um the minimization of divergence does lead to the maximization of model evidence so the the um

relationship to keep in mind is that the model with the most evidence for it is going to be the one that minimizes surprise

Minimization of surprise is itself intractable.

And so we use this bounding approximation, which is the expected free energy.

So let's think about this with the heights of the children in the classroom.

So we wanna have the maximum evidence estimation for the heights of the children in the classroom.

That's like the ultimate statistical problem that we wanna solve.

Now, if we had the model parameterization that minimized our surprise, so as new heights were drawn, we were minimally surprised, that would be equivalent to having the maximum evidence model.

And then surprise itself, which is just like the log of P, that is intractable, and that's why we have a bounding function.

So the minimization of divergence is used as an attractable way because this is going to be a constant with respect to Q. So in terms of the distribution that we actually control the knobs on, Q, this is not going to have any gradient to it.

And so that's why variational free energy can be incrementally optimized by reducing the KL divergence here.

And you are generating priors that are, you're updating priors based upon incoming sensory data.

I'll just note though that action is not included in this equation.

This is purely a functional, it's a function of a function.

It's a functional variational free energy of Q, the distribution that we control, variational approximator, and the data Y. But there's no action selection here.

Anyone else want to add a thought on this?


SPEAKER_03:
Yeah, Ali, thanks.


SPEAKER_02:
Well, actually, this duality between minimization

of the free energy on one hand and maximizing the evidence afforded by the generative model on the other is what usually referred to as self-evidencing mechanism after Jacob Howie's well-known paper from 2014 or 16, I guess.

So the idea is that, well, I mean, a particle and by particle, I mean

the joint set of internal and blanket states of a system will look as if it is gathering evidence for its own generative model and where its generative model also entails the prior belief

that it somehow acts upon later by choosing proper sets of, I mean, policy.

And by policy, I mean a set of actions selected by the agent or the particle as somehow,

minimizing the expected free energy at this time.

So variation of free energy provides a kind of upper limit to

Uh, to, to the way that, uh, the agent tries to minimize, uh, its perceptual, uh, uncertainties, but then later on in order to, uh, effectuate that uncertainties and to, uh, kind of have causal intervention, uh, onto the environment.

then it needs to select a policy in order to minimize its expected free energy.

So in this sense, both variational free energy and expected free energy can be seen as parallel or dual to each other.

And we can obviously see from the form of the equations of 2.5 and 2.6 that they're actually mathematically and formally are parallel.


SPEAKER_06:
Thanks.

Another way to see this is like, we're going to come back to it multiple times, but variational free energy is like the real-time perceptual, how's it going?

Because it is purely inferential.

It's real-time as a functional of beliefs and incoming data.

So if we're talking about body temperature and homeostasis,

This is just like how the body temperature is.

And so we kind of latch our model's performance and the desire to not under or overfit.

That is latched to the ultimate observable layer with incoming data.

But this is a passive equation.

But this is the same variational free energy that you would see in a variational autoencoder.

if you had a some bayesian statistics problem and you were just trying to um interpolate um pixels in a variational auto encoder you wouldn't want to over under fit this is exactly what you would have and then in order to have causal efficacy and the imperative there just like mentioned this is going to be a constant term with respect to q

So we can minimize the divergence, this KL divergence, which is always bigger than zero, and that pushes down F towards surprise.

If we had the minimally surprising model, we'd have the model with the most evidence.

And then in 2.6, we get into causal efficacy in a sort of prospective way.

Younghoon?


SPEAKER_01:
Yeah, it's just, I think it's a minor issue, but...

Because our objective is to lower the variation of free energy, it seems to me like it's better to explain surprise as the lower bound of variation of free energy rather than saying that the variation of free energy is the upper bound.

Because, well, the our objective is to reduce the KOI versions of Qx with the posterior so that the free energy itself can be minimized to the surprise itself, so I think it's more intuitive to explain as surprise being the lower bound of the variation of free energy.


SPEAKER_06:
Yeah, that's a good point.

And also, once you start, oh, well, there's negative F. And so the upper and the lower, whichever way you want to approach this divergence from, when the KL divergence is minimized, then the distance between these two distributions is lower.

Sorry, not the distance, the divergence.

The only difference between a divergence and a distance in this setting is like a distance is the same from A to B as B to A. And it turns out the KL doesn't have that property, which is why it's called a divergence, not a distance.

However, when variational free energy is minimized, then the approximating belief function Q

with respect to the incoming data are doing as well as they can at bounding surprise.

That's the best fitting model with the most evidence.

I mean, if you knew optimally how surprised to be by what was gonna happen next in the movie, you would have the best model of what was about to happen in the movie.

And so maximum model evidence

can be approached by minimizing surprise minimizing surprise is itself intractable compute so we minimize a bound on surprise which is the variational free energy Ali


SPEAKER_02:
Actually, that's exactly the elbow or evidence lower bound as encountered in the RL literature or in, I don't know, some other resources.

But yeah, I agree that sometimes it can, at times it can be confusing because elbow is basically the negative variation of free energy, right?

So yeah, that's exactly, almost exactly the same concept.


SPEAKER_03:
Yeah.


SPEAKER_06:
And then

Martin, you asked in the chat, can surprise be lowered through action?

Well, yes, these are the two ways to minimize free energy.

Change your mind, perception, learning, change the world through action.

Now, action cannot directly reduce your surprise about sensory outcomes.

So let's just think about body temperature.

We expect and prefer 37.

We're getting surprised because we're at 40.

Now, no action is going to pick up the temperature and move it back to 37, but an action may influence the world, turning on the air conditioner or taking off the jacket, so that future sensory observations are minimally surprising.

And that's exactly what equation 2.6 describes, which is we're going to evaluate

Future courses of action, turn on the air conditioner or don't.

These are our two policy options.

Evaluate these two alternative policies in terms of two different criteria.

What we stand to learn here, Y tilde is the only difference between these two.

Double line is the KL divergence.

That's what you're taking the divergence between.

So if Y tilde gave you no information, then this KL divergence would be zero.

So if the policy doesn't bring you any, if the new observations don't bring you information, the informational gain is zero.

Whereas if you stand to learn a lot from the observations that you expect to get under that policy, the informational gain or the epistemic value is high.

pragmatic value is about the alignment of observations with your preferences and so actions are selected prospectively in order to um balance surprise but that also includes this epistemic component so it's not just like the kind of dark room well then why don't we just close our eyes and you know

curl up because things become more surprising while you're not looking so informational gain is still part of the picture but yes absolutely and that's why self-evidencing is this kind of like unifying imperative that helps us understand the real-time flow of belief updating and sensory perception on one hand

and extends that prospectively into the action select action selection case with expected free energy so can surprise be lowered through action not directly however actions are selected prospectively based on their causal efficacy in the world with the imperative of bounding surprise about those observations Darius


SPEAKER_05:
yes i recognize um i may be jumping the gun on this and maybe we will come to it but um in order to select uh prospective action uh policies uh so as to minimize future free energy presumably just to run the sort of causal regression back there has to be some priors um regarding the probability of that policy reducing free energy so my question is according to the active inference framework

what are the initial priors that govern action?

And I'm talking even here at a very sort of developmental level.

I imagine that genetics has a role to play, but is there also just a picking out of good outcomes initially on basically randomness?


SPEAKER_06:
Yes, this is a very good question, and it speaks to some of the challenging aspects of modeling, which is what do you explicitly versus implicitly bake into the model?

So this is like looking a little bit ahead to figure 7.3.

It's the same as figure 4.3, basically.

the um causal efficacies in the world are captured by the b matrix if i do policy one versus policy two how is that going to change hidden states and then hidden states emit observations so how do agents come to know that turning on the air conditioner reduces the temperature or that putting on a jacket increases the temperature

So that's the question of the B matrix being fit.

Obviously, if your B matrix were washed out, action would be meaningless.

Like if you didn't know or if your actions didn't have any consequence, then action selection is kind of for naught.

And then depending on the situation, you could think about B being inherited, like inherited prior or like the first prior potentially developmentally.

Also B can be learnt.

So the consequences of action can be learnt from an initially plastic B. Ali?


SPEAKER_02:
Actually, this is a question that's been asked over and over again as a kind of criticism for active inference framework regarding how exactly those prior beliefs emerged from.

So, I mean, talking a bit more theoretically, philosophically, or even more fundamentally, well, FEP

starts from the presupposition that there is an agent or a particle with certain internal states and then attempts to describe the geometry of the information that's

been related to the interaction between those internal states and the external states through Markov blanket.

So FEP by itself doesn't say anything about how those internal states emerge from at the first place.

Of course, it provides an account for the self-organization through, I mean, describing

Amit Singer- Describing the process that such internal states can increasingly get more complicated becoming higher dimensional and so on, but I mean by itself it doesn't provide any account.

for any explanation for the emergence of those internal states at the first place.

So if we take that as the basic premise of the active inference, I mean, as a precondition for the application of FEP, then I think this question or this criticism can be resolved because

obviously FEP never attempts to provide anything more than it claims, right?

So, yeah, that's, I think, one of the crucial points that we need to keep in mind regarding what exactly are the claims, are the theoretical and empirical claims of FEP inactive improvement.


SPEAKER_06:
awesome.

And this this reminds me of the delightful actual constants.

argument.

The free energy principle, it's not about what it takes.

It's about what took you there.

If you're ready for some philosophical tongue twisters.

The free energy principle is not concerned with the conditions of existence.

So not what has to be the case for something to exist.

That's an ontological analysis.

But rather with what must have been the case given that a thing exists.

So the presupposition of there being a thing is what Ali highlighted.

It's not about figuring out what it takes to be alive.

Again, what conditions lead to life

flip it it's about a system of interest and understanding what must have happened for that system of interest to have been that way which is self-evidencing behavior so it's not that self-evidencing behavior begets life necessarily although that also may be true it's that things that are persistent

whether they're inert and simple or conservative active systems, or even strange, sophisticated cognitive systems, whatever level of complexity the thing is, we're gonna be able to do this kind of FEP or active inference analysis.

But that doesn't mean that this is a forward-looking analysis about what it takes to be alive.

But this is an excellent work by Axel.

Okay, cool.

Let's continue through these questions, but these are fun.

Page 22.

The results of inference are not necessarily accurate in any objective sense.

for a number of reasons biological creatures operate on the basis of limited computational and energetic resources ultimately as all computers do too would these limitations influence the strength of priors in addition to confidence and thus minimize Bayesian surprise for example observations might give rise to surprise but the generative model is not updated

due to the inferred energetic or cognitive costs of updating.

Practical examples being not wanting to change an opinion because doing so requires investment of time to think, discern, etc.

Yeah, I think it's a great insight.

People can feel free to add.

However,

Strategy selection, cognitive strategy selection may or may not have ultimate accuracy as its imperative.

as the question highlights whether it's a energy ultimately energetic cost or it's some type of like psychological or energy or cognitive cost of updating or even a time cost or an uncertainty cost or a social penalty type cost um there are a whole host of reasons that an update could be considered Bayesian optimal

and not fully or only align with the most accurate account.


SPEAKER_05:
Darius?

Just a question.

What popped to mind was,

might that not be the minimization of prediction error or free energy at a higher level of the hierarchy so i may not want to change my mind because of social pressures in turn fulfilling my prediction that i will not be socially ostracized so will a bayesian with an optimal bayesian inference engine might this still be optimal it's just that we're getting skewed by looking maybe not holistically at the entire hierarchy


SPEAKER_06:
yes this is exactly exactly it like how how can um um even pathological or or harmful behavior be understood as base optimal

And the answer there is given the setup of the generative model, the priors the way they are, and the state transition inferences the way they are, the attention the way it is, then name your DSM condition.

It is a base optimal given the totality of the cognitive model.

And so it's not like some behavior is like, let's just say rational or irrational.

They're all doing optimal Bayesian inference.

The ball is rolling downhill.

You might not like where the ball rolls or society may say that the ball should have gone this way, not that way.

But it's all like water flowing to the bottom of the hill or the ball rolling to the bottom of the hill on the landscape.

And that's optimal Bayesian inference.

so that's a that's a totally great um point and they'll talk more about um like optimalities measured in relation to expected free energy well if you have a distorted perception of how things are happening in the world then that agent's optimal selection again doesn't mean that the payoff is going to be the highest

It just means that it's an optimality concept that's agentic.

So optimal Bayesian inference doesn't mean best of all possible worlds, all the agent does is win, fitness is optimized, the game theory is transcended.

Optimal Bayesian inference just means given the generative model, the ball rolls down the hill.

And so if you account for, for example, higher order considerations, then you can end up with what might otherwise be seen as like aberrant or pathological behavior.

So for example, being unable to confidently predict the reactions of others in a social setting

might lead one to pursue a sort of self-oriented behavior because it has understandable, predictable consequences like self-stimulation.

Again, that's not to say that it's optimal in terms of the social reward received.

It's just under the optimality criterion that generative models are actually playing out under

all neurodiversity beyond humans even just cognitive diversity is being modeled with this optimal Bayesian framework but connecting back to the cybernetics good regulator theorem and requisite diversity while it's true the results of inference are not necessarily accurate over

the developmental or evolutionary timescales, at least the results of inference have to be adequate enough to keep you in the game.

Otherwise, you're very quickly going to be swept off the table.

So that's kind of this qualified pragmatism, scientific realism.

It's not trying to paint...

how it really is out there but causal um architectures of the external world though they're not directly observable have to be at least understood well enough in order to make adaptive decision making in a dissipative or adversarial Niche

Alexi, what you described accounts for pathology without insight.

When the patient suffers and knows something is off, I am not sure it's described this way.

Pathology with insight present depression.

Yeah, metacognition.

Okay.

Prediction, error, surprise, variational free energy.

These terms are related to one another, but sometimes the relations are not immediately clear, causing some confusion.

The common objective of perception action can be formalized as a minimization of the discrepancy.

Sometimes this is operationalized in terms of prediction error.

When is the discrepancy not operationalized as a prediction error?

If we calculate surprise as per page 19,

Is this a different than the calculation for related terms?

What is the possible confusion interchangeably using these terms?

Okay, I'll give a thought, but Ali or anyone else, please feel free.

Prediction error.

is like the simple differential or the simple discrepancy i thought it was going to be 10 it came in at 11 so the the simple discrepancy was this amount of units of temperature

Thought it was going to be 10 degrees, came in at 11.

The prediction error was plus one unit of temperature.

In contrast, surprise is an information theoretic quantity.

It's measured in nats or in bits.

But surprise is like looking at the distribution.

How surprising is that given observation?

Kind of like a Z-score in statistics.

Like, oh, this was three standard deviations.

Now, it may be the case that one degree temperature is one standard deviation.

And so it was one standard deviation of surprise for that plus one prediction error.

And then variational free energy is shown in equation 2.5.

They're related to each other because where the prediction error is zero, surprise is minimized, but not necessarily zero.

still non-zero surprise associated with picking the exact highest point on the gaussian minimized surprise but not zero surprise and similarly picking that exact midpoint on the gaussian would minimize variational free energy okay i thought that the classroom was four feet plus or minus one i pull a height it's four feet so it's a minimal surprise minimizing free energy

minimizing prediction error so they're kind of all um colleagues and helping us point to to different aspects of model fit but they're not the exact same quantity Ali and then anyone else


SPEAKER_02:
Also, it relates to the difference between the exact Bayesian inference and the approximate Bayesian inference, because basically when we talk about free energy, it refers to the exact amount of the surprisal that needs to be minimized.

But when we talk about variation of free energy,

we utilize the approximate Bayesian inference techniques in order to come up with a bound or more precisely an upper bound to minimize the surprise.

It's not the exact Bayesian inference because it would be intractable in most real-world situations, but rather it provides an alternative

way of

Roozbeh Gharakhloo, Dealing with the complex situations by providing an upper bound for the surprise, though, mainly the variational free energy also I think it might.

Roozbeh Gharakhloo, I mean it might be helpful to.

Roozbeh Gharakhloo, point out the paper by and you're as i'm going to show me Chi which provides 14 different definitions for the surprise, though.

Roozbeh Gharakhloo, In different contexts so.

Uh, we also had a guest stream, uh, with, uh, which I recommend watching and, um, looking up that paper as well.


SPEAKER_06:
Yeah, this was very, um, very rich paper and stream and discussion where they derived, I think.

Yeah.

How many 18 or 14 surprise concepts.

include it and show you know again just intuitively if your model is fitting as well as possible if the evidence is the highest you'd be right to expect that surprise would be lowest in general but it turns out that there's actually a plethora of ways that surprise is computed and they're they're not exactly equivalent

And, and we expect that many other ontology terms, attention, anticipation, all these kinds of terms where we have a, a broad everyday sense of the term.

And then we also make technical definitions of them in the active inference ontology.

Many other terms may have also, um, just like words in the dictionary have multiple bylines.

Great question though.

Okay.

All right.

Equation 2.6.

So the trade-off between information gain, let's copy it in.

trade-off between information gain and pragmatic value is quite understandable so that's referring to this kind of top line classical decomposition of expected free energy this is our unified imperative for action selection which is we should select policies based upon their pragmatic and epistemic value so

Not to talk about the other guys, but in reward centric learning, you need to coerce epistemic value into pragmatic value.

because it's like a single currency of pragmatism or reward.

So you have reward itself.

And then you say, well, it's worthwhile to go on this little exploration because the expected reward or the expected utility is such and such.

So would you rather have $1 on the ground now or 10% chance of 100 by going on this exploration?

And so reward learning would go on that route, depending on some risk discounting, which we'll look at.

Whereas an expected free energy,

We're just jointly considering both the epistemic and the pragmatic.

Now we're going to look at some different decompositions of G. How expected ambiguity and risk interact or are counterbalanced is quite hard to be understood.

Can anyone provide a good explanation?

Okay.

So does anyone want to give a thought on how would they read G?

um the second line and I think we'll all look forward to um the derivations and also look to improve the the natural language descriptions with just like the verbatim readings um but what does anyone see in this second line

I'll just give a thought.

So H is entropy.

So this is how blurry or sharp a distribution is, is captured by its entropy.

So the first term, expected ambiguity.

Here we have a distribution of observations through time, Y tilde, given hidden states through time.

So this is like how entropic, how...

sharply defined will observations be so you can imagine a super if there's two outcomes it's going to be really ambiguous if both the outcomes have this have um messy observations you know whether we win or lose if we're getting this static on the radio then the ambiguity is high

Whereas this term is gonna be favoring policies that lead to unambiguous resolution of the relationship between hidden states and sensory data.

So here, the kind of sub imperative is pursue policies, big fancy E, expectation,

pursue policies that have less ambiguous expected entropies.

So this is just sensory.

This is only about the mapping between the hidden states through time and the sensory observations that are happening through time.

The second term, we have a KL divergence, double line,

on the left side of the parentheses are the sensory observations through time.

And so it's the difference between the sensory observations conditioned on what we do and the sensory observations conditioned on what we prefer, C. So this is risk not in the danger sense specifically,

but rather a risky outcome or a risky policy is one that leads to a range of outcomes.

A non-risky policy is one that maps tightly to a certain outcome.

So this expected ambiguity term helps us pursue policies that give unambiguous sensory data about hidden states of the world.

And this risk term helps us pursue policies that our causal intervention has efficacy in.

the being able to shape sensory outcomes.

This next.

Yeah, Andrew.


SPEAKER_00:
Oh, I just wanted to say the

The most help prior to your explanation for understanding these terms were just the simple examples given in textbooks.

So with ambiguity, I give the example of like the probability of heads or tails in the coin flip conditioned on whether it is sunny or raining will be maximally ambiguous as there's no relationship between the weather and the 50-50 chance of heads or tails.

So that seems to relate to something like independence of events in that case.

entirely independent, the result of your coin flip is very distinct and doesn't provide you any useful information about the other scenario.

And then with risk, similarly to what you were saying, the kind of one-to-many mapping between policies and outcomes.

For example, a highly risky policy or action is related to playing a slot machine.

Or say you have a slot machine that has three potential outcomes, each with equivalent probabilities.

Well, I do this one action.

There are a variety of outcomes, you know, that could come out of this.

And so there's very high risk to your action as opposed to, you know, pulling a slot machine and having a guaranteed win every time.


SPEAKER_06:
Nice.

Awesome.

Junghoon?


SPEAKER_01:
uh i actually wrote down this question because i got a bit confused about um well so policies that seem to be uh reducing the ambiguity also seems to reduce the risks so like it made me kind of confused because well both

both values would be heading towards the same policy.

So it doesn't seem like it's a trade-off at all.

So compared to the information gain and pragmatic value, which seems to be very intuitively a trade-off, I couldn't catch the

a good example or a good explanation about how these two values, expected ambiguity and risk, are interacting with each other.


SPEAKER_06:
That's a very, very good point.

although explore-exploit is kind of like a classical trade-off and it's one that's going to be approached in the book, you could imagine situations where there's two policies and one of them gives more epistemic value and pragmatic value.

And similarly, you could imagine situations where they are in trade-off or tension.

And so similarly, it would be helpful to think of examples like real or contrived

um where ambiguity and risk are are aligned or not ali what do you think you're you're muted ali or


SPEAKER_03:
I'm sorry.


SPEAKER_02:
Sorry.

Can you hear me now?

Okay.

All right.

So the expected free energy somehow unifies various accounts of resolving the ambiguity from different disciplines and different, I mean, from several accounts in psychology, machine learning and so on.

So for instance,

accounting for the risk both in terms of outcomes and in terms of states is inspired from

Kahneman and Tversky's work on risk-sensitive policies as applied to economics and also the callback Leibler control engineering.

So here, actually minimizing the risk somehow

It corresponds to aligning our predictions to our prior preferences.

But well, in the absence of the risk, all we have is just the ambiguity that is basically accounted for

in the accounts from James and also Lindley, in which they only account for the resolving of the uncertainties through diminishing the ambiguities and not accounting for the risk implied in each situation.

I think we're short on time, but it can be explained in a bit more detail how exactly those three or four lines of equations can be related to each other and how each of them corresponds with some of the other accounts of how we can resolve those ambiguities and uncertainties through different techniques.

But

nonetheless expected free energy to the best of my knowledge is I mean provides the most all-encompassing and unified way of accounting for all of these various techniques of reducing the uncertainty but we can talk about that a bit more in the next session I guess yeah thank you Ali figure 2.6


SPEAKER_06:
summarizes a few of those threads it shows how like in the absence of pragmatic value what you're left with is an information seeking imperative conversely in the absence of epistemic gain you're left with naked epistemic utility Theory

And under another restricted subset, kind of special case of G, you end up with risk-sensitive policy, which is part of that Kahneman and Tversky line of work.

Well, glad that we were able to get to the cohort four, chapter two questions.

Next week, we will be at a different time.

but we'll continue on chapter two.

So whether you'll be able to be here or not, please feel free to add more questions in and upvote other questions.

I mean, there's almost 50 questions, but please add upvotes and we'll improve the discourse and maybe record some short videos and continue these great topics together.

as we proceed along the low road.

So thank you everybody.

See you next time.

Thank you.


SPEAKER_03:
Bye.

Thank you.


SPEAKER_06:
Thank you.
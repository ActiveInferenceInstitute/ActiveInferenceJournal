SPEAKER_05:
Hi everyone, it's August 1st and we're in Cohort 4, having our first discussion on Chapter 4.

So before we go into more detail on Chapter 4 and everything, does anyone have any just overall thoughts or comments on Chapter 4?

What did they expect it would be?

delivering as a chapter?

How was it?


SPEAKER_02:
Or what was any aspect that people remembered or anything?


SPEAKER_04:
I found this chapter to be rather mathematically dense, I suppose, and the authors do warn of that at the beginning.

of the chapter.

I mean, I think it's great so far as it starts to introduce how they're using basis theorem, how they kind of exploit Jensen's inequality to move in the direction of like calculating free energy, using logarithmic functions in order to do that.

You know,

The initial models of the static perceptual tasks also seems like a really nice introduction to moving on to BOMDPs, as well as the continuous models used in active inference.

So in a way, it felt like it kind of started with nice first principles and built up from there.

But that said, I can

start to see maybe the necessity of having a math learning group for some folks here, given this chapter.


SPEAKER_05:
Thanks.

Yeah, anyone else?

Any just overall thoughts or things?


SPEAKER_00:
I would just totally agree 100% with what Andrew shared, basically.

It's just similar thoughts.

i mean basically the the mats are very well explained and it goes gradually but i think it would be beneficial yeah to yeah to go even deeper and slower through them maybe separately definitely any other thoughts people have


SPEAKER_05:
Yeah, Darius.

So then anyone else?


SPEAKER_03:
Yeah, Daniel, it's less a point about the chapter, although I agree with the previous sentiments that it is mathematically dense.

It's more of a point about the maths learning group.

I think myself and several others just having trouble locating it.

in terms of where it actually is on the discord so the past couple of weeks i've been trying to get into some of the voice chaps and it doesn't there doesn't seem to be anything kind of happening on them so there might be a bit of confusion


SPEAKER_05:
yeah thanks well it is uh people can use the general voice chat just the top voice channel and uh you know if there's no one there just still jump in and stick in there because otherwise i see sometimes people jump in and then immediately jump out and then that then other people think that there aren't anyone around so just jump in and hang in and you'll be able to see people's like icon and see what channel they're in um

I mean, yeah, it's a, it's a, it's a decentralized learning group.

What I, people can edit pages or, or just ping me if there's any support needed, I'll try to join when I can, but it can re I mean, there's, there's so, so much work on the more introductory side, like kind of just going

from the fundamentals on through all the work that Jonathan and Ali and others are doing too.

So there's no lack of things to do in math learning group.

I hope people do like connect on it.

Younghoon?


SPEAKER_02:
Wait, I don't hear you.

I can see you're on mute.

Maybe frozen, though.


SPEAKER_05:
Can anyone else hear or no?

Okay.

Okay, when you're back, feel free to continue.

Any other...

General thoughts on for anything writing in the chat.


SPEAKER_04:
Just another small specific observation that's a as a kind of a practicing entry level data scientist.

The way something that helped me understand the static perceptual models earlier on is that it.

It almost feels like it's just describing something like the process of binary classification.

One of the models in Figure 4.2, the very first one, you have the example of your prior being prevalence of a disease.

Is there a disease, yes or no?

You have your true positive rate, your true negative rate, and you end up with the result of your test.

I found that to be rather interesting and useful.

Just are we trying to predict does someone have a disease or not?

There's more that goes into that step by step.


SPEAKER_05:
Yeah, thanks.

That's one of the classic Bayesian examples.

Like there's a test that's 99% accurate and there's disease that one in 10,000 people have.

What is the probability that you have the disease if you got a positive test result?

and the kind of snap answer type one answer is um 99 because if it was positive and it's 99 accurate then how could it not be 99 but when you actually unpack the base equation it's actually not 99 it's actually still unlikely um in that with those parameters um young oh yeah sorry about it go for it


SPEAKER_01:
So I find these mathematical equations in chapter four to be a little bit ambiguous.

especially like notations, like dimensions, or whether it's a vector, or whether it's a scalar, or et cetera, depending on the context.

I find especially difficulty to understand the exact equation when there are some functions called categorical functions.

So I just accepted the concept, but

just wanted to make it more clarified so i actually uploaded some pdf and questions to the chapter four because i i got stuck with some um derivations of it so that might be in the court for chapter four questions and you can look it up later on awesome


SPEAKER_05:
awesome.

We'll definitely look at it.

And also here I see one this Overleaf link.

This was prepared.

Well, I guess he just added this in.

This is from Jonathan shock.


SPEAKER_01:
Yes, I found it to be very helpful.


SPEAKER_05:
That's, that's awesome.

Thank you, Jonathan.

Yeah, I mean, there's even already there's all there's so many pieces of materials.

It's, you know, it's like a second level question how we how we organize things so that they're discoverable and kind of annotated so that somebody who's looking for, like this level of detail can can connect with this type of material.

Let's just read his basic remarks.

Free energy and expected free energy are introduced almost in parallel, but it's not clear which one should be used when or exactly why one needs both of them.

This is relatively simple that the free energy is used to improve the model itself.

Bruce Bunnick, Jr.

: Whereas the expected free energy is used to make decisions about which is the best action to take which will lead to states which are preferable and states which lead to more epistemic value which will themselves themselves allow one to update the free energy.

Bruce Bunnick, Jr.

: awesome.

Bruce Bunnick, Jr.

: yeah this looks really good.

Bruce Bunnick, Jr.

: um.

Any other like just thought or question or area in four that people want to jump to?


SPEAKER_01:
Young again?

Oh yeah, so I'm not sure whether it's a good explanation or I'm not sure whether it's a correct intuition.

But for me, it feels like comparing with reinforcement learning, reducing the free energy is something a little bit related to offline learning, while minimizing the expected free energy seems like to be related to online learning.

you choose decisions and then you try to reduce the free energy online or the expected free energy while you reduce the variation of free energy offline.

So I'm not saying that those are equivalent, but it seems to me that they are somehow related.


SPEAKER_05:
Maybe just summarize what is online-offline learning.


SPEAKER_01:
Oh, not really a master of reinforcement learning, but based on what I understood about it is that online learning is learning while you're doing the action while offline learning is something more like you learn the value of something based on the state trajectories that you've already visited.

So it's more like learning by replaying.

I would say it can if anyone thinks that I'm explaining it in a very ambiguous term or if I'm wrong, just let me know.


SPEAKER_05:
Well, yeah, definitely an area to look into, Darius.


SPEAKER_03:
I think what might be useful for me and perhaps others is just getting a bit more clarity on exactly how we define the generative model in active inference.

So on one side, and you see it expressed in this chapter, that it comprises the prior and the likelihood term in Bayes's equation.

But then on the other, I've seen other theorists talk about how it's actually basically the entire conditionally dependent dynamics across the Markov blanket.

So it includes the external dynamics.

It's not just what's internally encoded in the internal states of the particle.

So unless that's just a continuing conflict or continuing debates within the active inference community, um, it would be good just to get a little bit of clarity.

Cause I think that also helps us explain exactly what is minimizing free energy and whether we can talk about agent having a generative model or being a generative model.


SPEAKER_05:
Yes.

It's a great point.

And it has come up before.

So in fact, there are about one and a half definitions of what the generative model is.

And they're not identical.

They're kind of nested concepts, and it certainly does matter, and so I can understand why it's a point of confusion.

In the 2022 textbook that we're reading right now, generative model is very, very clearly differentiated from the generative process.

So like when we're looking at the figures, it's like, here's the generative model.

That's like the agent.

Sometimes we would call that the particular states.

because it constitutes the whole particle.

Just like if we were talking about a dust particle Brownian motion, now we're talking about the cognitive particle.

So that includes the blanket states and the internal states.

And so that is sometimes called the generative model.

And the generative model, as it's framed in the textbook, is to understand or to make this model of the generative process, which is the actual process that gives rise to observations.

and so this distinction between generative process and generative model is um is heavily used in the textbook um however more recently some authors have have uh taken a slightly different approach or definition so as you mentioned and for example um in

in this representation from Maxwell on, I guess, a month ago or something around then, here generative model is used to describe the entire particular partition, all the states, including the external states.

And here's Ali's question.

earlier, there was an emphasis on distinguishing between generative process and generative model, but in this figure, figure one, which you show on the slides, we don't see any generative process.

So where does generative process fit in the diagram?

What is the distinction between the concepts?

And yeah, I'll put these into the CODA as well.

at least Maxwell is not using the generative model, generative process distinction as much.

Here's how I kind of reconcile this.

Whatever you want to call different parts of the particular partition, you need all four of them to be defined in the model.

So whether you just call them, you know, whether you buy them all a cart or whether you buy them in a bundle, all the particular states, all the states need to be described.

Otherwise, you're going to have a null variable.

And now there's several we could, you know, we could talk about.

You have your friends who like this music and you have your friends who like this food.

And so there's different nested levels of describing the friend group with these four friends.

One of them is the way that the textbook describes with the generative process, external only, and the generative model with these three on the right, blanket and internal.

Another group that we talk about sometimes is just the blanket states.

And of course, states are not intrinsically blanket states because every state is blanket to something else and so on, but in this setting, blanket.

And then another set of states that we talk about sometimes are the autonomous states, which is just the internal states and the active states.

autonomous states are the two things that can be changed to reduce free energy change your mind learning perception attention Etc and action change the world so that's the no thumb on the scale so whether or not you focus on these three on the right and call it a generative model and this is a generative process or whether you call all four of the generative model

the end of the day the variables are defined the same way and the variational and the expected free energy are still regarding the autonomous states with respect to the sensory states and so generative model generative process is a little bit of like a linguistic descriptor or handle but it it um

it doesn't change anything to give a different English name to describe a subset of notes.

So I don't think it's an issue.

However, it is confusing.


SPEAKER_03:
Can I just get one more point of clarification on that?

Yeah, yeah.

If you go back to that diagram, that would be quite useful, actually.

here so yeah that's perfect um in terms of self-evidencing then if we take Maxwell's idea that it's the generative model is the system the across the Markov blanket do we talk could one consider that system to be self-evidencing or is it only the particle which excludes the external states which is self-evidencing


SPEAKER_05:
That's a great question.

I think also calls back to your rock climbing example.

Okay, the rock climber is doing something cognitive about the wall, but is the wall doing something cognitive about the climber?

Or is there some, are we considering the entire system as doing some kind of gradient descent?

Yeah, that's, it's a good question.

either side of the blanket, I believe, may be seen as doing some kind of self-evidencing.

Now, in the case of one active adoptive sentient entity and one inert one, the inert entity's self-evidencing is going to be trivial.

So just because there's self-evidencing on both sides of the blanket doesn't mean it's the same kind of self-evidencing, but you can also make situations where it is the same kind of self-evidencing across both sides, like in the birdsong example.

does that uh do two uh uh when when two sentient agents are engaged in self-evidencing does that constitute higher uh level of self-evidencing at the entire system level description possibly i i mean if if um if they're both reducing free energy now what if one is

reducing free energy and that is creating an increase in free energy for the other could that increase the net free energy on the system I I don't know or is it always the case that that it um that the ball has to roll downhill and the more massive cognitively massive self-evidencer takes a stronger hand and and that makes it move down on average also I don't know


SPEAKER_04:
Andrew.

I don't want to.

Hold too far away from the kind of current formulation is being discussed here, but something that has been useful for me and I think I please correct me if I'm misstating this, but with differentiating the generative model and the generative process in part is that the generative model is presumed to have.

Matt Pinyan, Something like preferences or the the its own see matrix or its own homeostatic set points which I believe the generative process does not have.

Matt Pinyan, So we have like from Chapter three and so on, and the generative process or the environment of the the agent, the generative model is presumed to have like.

Matt Pinyan, You know dissipative forces and should be and so on, and so the generative model is kind of acting against those.

in order to realize its own expectations or preferences.

That is, I want to survive as a generative model.

The environment around me has no set of direct preferences saying, like, Andrew needs to survive.

I'm the one who's saying that.

And so despite the fact that I'm attempting to

infer or approximate the external states that I can get around in the world, like I'm also kind of acting against it, or at least, you know, in a way that doesn't, it's never going to exactly equate to the generative process around me.

And this is where the map does not equal the territory kind of formulation starts to come into play and becomes all the more important.

Any of that track?


SPEAKER_05:
Yeah, that's it's a great point.

Certainly map doesn't have to be the territory, even structurally like the generative model.

Too cold.

Just right.

Too hot.

Three state categorical.

That's the categorical distribution.

or it could be a continuous variable, 0 to 1 or 0 to 100.

And then outside, it could be a totally different structure.

And so the generative process, I believe it's, or if you just call it the process generating observations,

that could just be a sine wave or it could be a square wave or it could be a deterministic um system and so that that makes it very clear like like you said that when we're talking about the generative model of the agents we're talking about a b c d e those are the variables that have to be defined that's figure 4.3 basically whether we're working in continuous time or in um

discrete time these are the variables that have to be defined or accounted for in the generative model this doesn't describe how those observations are generated so this is kind of like just the agent lifted from the environment so it still is help but and it's the one that's exerting preference and um engaging in sentient behavior and all of that and then it's like helpful again whether you call it generative process or

underlying process generating the observations or the components of the system that generate the agent's observations, just to make it clear that we're not always in a situation where it's two active entities talking to each other, like the bird song example, but rather we can be in a situation where, like the rock climber that Darius brought up, where you have like this figure 4.3 agent

And then there can be an articulation with a generative process, again, or whatever you want to call it, that is not doing like policy selection, except in a trivial way.

yeah these are all great points and and it's funny you know it reflects how things update and and evolve and helps us get a get a parallax on on what's really happening what are like just the purely verbal descriptors we're actually like the term like the English term isn't doing any work

like generative model, generative process.

The term doesn't necessarily do any work.

What does do work though is statements like this.

Calculation of free energy requires data, Y, family of variational distribution, Q, and the generative model.

Those three things together can get a free energy.

that also helps reveal like why discussion of well when this happens then the free energy would go up those are speculative statements they can be super useful and interesting but until the calculation has occurred it would be like um talking about a linear regression that hasn't been performed or talking about a time series analysis that hasn't been performed so it makes it clear where's the conversational and the speculative

And then how do we get to a point where we can calculate the free energy about the data with respect to the distributions?

Equation 2.5, variational free energy about the variational distribution and the data.

So that's how we get there.

Any other kind of thoughts on four?

Also, a few people probably have watched it, but just a few days ago, Ali and I did a video overview on four.

Well, we can turn to a question.

That someone has.

OK, both these questions look very good and they're later slightly later in the chapter and slightly with the math details.

I wonder if Jonathan's work may address some of this, but we'll try to get there.

Let's start a little earlier on.

And also, yeah, people can upvote the questions so that we can pick a place to start.

But does anyone have a specific question they want to bring up?

Or do we want to walk through and just get to just see what the sections are about?

Okay.

Bayesian brain describes using Bayesian statistics.

Well, there's sort of an instrumentalist flavor and a realist flavor, as with many things.

The realist flavor would be the brain does Bayesian statistics, which might be very deep, might be very overreaching.

The instrumentalist flavor is we can model what the brain is doing with Bayesian statistics, which is to say like updating.

What kinds of Bayesian statistics?

Well, the inbound sense-making signal processing side, and then the outbound action selection control theory side.

Two operations of kind of the general cybernetic entity, and we can use Bayesian models, Bayesian methods there.

um that's the problem of inferring states of the world perception inbound sense making and inferring the course of action action selection outbound planning exact inference is generally computationally intractable or infeasible separate from that limitation in psychology

often what's being considered is bounded rationality or like constraint limited um cognition so how can we respect that we're modeling bounded rationality approximate rationality uh limited finite systems um and kind of um at the same time um

move beyond this limitation of exact Bayes being intractable.

Like in the frog jumping out of the hand example, it was just some small matrices to multiply.

And so the exact Bayesian calculation was not out of our grasp.

However, if we were going to be doing some kind of large scale data analysis, then

of course, we would need to do some approximation methods.

There's two large, just speaking broadly, there's two ways to approach this kind of approximate Bayesian calculation.

And they're related and there's many, many sub flavors and Livestream 52 is a good place to learn more about this.

But there's sampling methods and variational methods.

Sampling-based methods or Monte Carlo methods, that came from the card games that were played in Monte Carlo.

And it was like, well, what's the probability of getting two twos, four red cards, and one clubs?

I mean, you could write an expression and try to analytically solve that.

Or the sampling-based approach, which has become called the Monte Carlo approach, is where you just draw...

um hands of cards and then you draw a million hands and you say okay then there was three so then in the end you say my best estimate is this happens three out of a million times now

you can probably see a lot of like limitations, but advantages of sampling.

On one hand, it allows you to approximate arbitrary, potentially analytically unruly distributions.

So even where you don't have a nice smooth function or you don't know a lot about the system, sampling may work quite well.

On the other hand, you never really know if you're sampling appropriately or if you've sampled to completion.

The other methods that are utilized, and also this cannot be done incrementally very well at all.

You need to curate many, many, many samples in order to make an estimate.

And where state spaces are large, like the state spaces of all genomic diversity for an animal with a billion base pairs in its genome, there's going to be certain things where it's just not really tractable.

In contrast, variational methods are an information theory driven approach that is going to use all the tools that we'll come to know and love, like the KL divergence and the variational free energy.

So that's kind of like why we're taking this path at all, because exact Bayes would be epic, but it's intractable.

Exact Bayes would allow us to compute the minimum surprise model, which would be the maximum evidence model.

we're not going to be able to compute exact base in general.

How can we approximate it?

Well, with sampling and or variational.

Sampling has some situational advantages, but variational is a much, much, much more flexible approach.

It's a lot more tractable.

It's incrementally optimizable.

We can reduce the KL divergence and improve our model fit.

incrementally and kind of indubitably like we will always make the fit better if we reduce the kale divergence we're making the fit better now that doesn't mean that the final fit or anywhere along the way is adequate for your business or your company or you know whatever you could be moving a gaussian distribution to its best possible fit location in the middle of a bimodal distribution

And still be totally missing the point, so this isn't to say like it works it's just to say the ball rolls downhill.

that's like the big picture on why variational methods are being used.

So chapter four is gonna describe generative models, which are using Bayesian networks, using Bayesian methods to describe cognitive systems, sense making and action.

And then the variational methods are going to support us doing optimization and inference with these models, approaching exact Bayes asymptotically.

But instead of focusing on surprise minimization itself, which we're not going to be able to directly calculate, we're going to be able to calculate something that is bounding surprise.

And that's going to be the variational free energy.

And in figure 2.4, we kind of saw what that looked like as well.

variational free energy, equation 2.5, the distributions and the data.

And if we can shrink that divergence, we're getting ratcheting closer to surprise.

So that's why it's possible to use variational free energy to incrementally optimize and update and learn and use that attractively.

jensen's inequality it'd be awesome to have more of a um uh like an unpacking on this because this is kind of um it's very provocative in general but suffice to say that because of the curvature of the natural log function or any other function that's concave down like that um

we're then able to use that property of this curvature type function and properties of logs in order to generate an inequality such that the log of the expectation is always greater than the expectation of the log.

So it's kind of like an application of Jensen's inequality for concave down functions in this setting to heuristic bounding of what we would really want to know.

There's some further rearranging.

Again, I'll review later, perhaps Jonathan's writing makes this clear or other people can fill in some dots.

But this is all kind of like more detail on equation 2.5, just helping us understand variational free energy as kind of like the Bayesian optimal way

To fit where's jonathan's writings on the Chapter four page, it is here and i'll put it in the chat Thank you Kate.

So that was 4.2 section 4.2 was just saying.

Bayesian inference, frog jumping out of the hand.

If you can calculate it exactly, go for it.

No one's going to stop you.

But in situations where we want to do an approximatable heuristic,

and one that's incrementally optimizable, we can use free energy.

This is also known as energy-based methods or energy-based learning or variational methods going back to decades ago.

So this is not invented by active inference at all.

It's also called the evidence lower bound in Bayesian statistics.

And so this is just applying that methodology, approximate Bayesian computation,

with a variational flavor not the sampling flavor to in this case the question of perception now you know having made the the um software update from just pure exact base to approximate base

Now we can calculate the free energy for a generative model.

So whereas an exact base, we would be talking about getting the maximum evidence model, minimum surprise model, plug and chug.

Now we're gonna get the minimum free energy model, which is going to be converging towards that minimum surprise, maximum evidence.

The book is going to return and continue to juxtapose and contrast two major architectural decisions that are made for dynamical models.

Now, not all models are dynamical.

Not all models involve something changing through time.

But in general, when you are modeling something that's changing through time, you can either have a continuous time model or a discrete time model.

Discretization sometimes still has to come into play for continuous models and so on.

But this is like the two broad approaches to dealing with dynamical systems, continuous time, discrete time.

And that's why figure 4.3, we're always coming back to again and again with the discrete time and with the continuous time.

Because it's like a Rosetta Stone that's like saying, whether you take this approach to modeling time or that approach,

There are some consequences and there's different settings where they may be more or less appropriate, but we're still talking about the same infrastructure and logic for generative models.

But before building up to 4.3, which has this like downstairs perception, passive perceptual inference component, and then an upstairs decision-making component and analogously for continuous time,

Figure 4.2 is just getting us familiar with what these causal graph structures are and do.

And so it's a fun exercise or question like to first off draw different situations that you're interested in or thinking of as like Bayesian causal graphs.

And then also to look at the ones provided here and then think about what real world situations map onto these, like maybe here's, was there a lightning strike or not unobserved, but then we observe the light and the sound, or there could be other examples, but, but this is just kind of like bringing the level of rigor to causal flow chart mapping.

and these are really extensible and composable like you can connect them in many different ways and given their connectivity you've defined like the necessary and sufficient information for that model it's not like oh and there's a secret connection from you know this one back to that one it's like what's written is there and included and calculated with standard methods

It's not included, isn't included.

It'd be like a calculator is only going to add what you tell it to add.

So inference is going to proceed along this infrastructure without some other secret third thing influencing it.

So they talk a bit more about just how to read the relationships between different variables.

Then they get to figure 4.3.

which is the Rosetta Stone figure between the discrete time and the continuous time.

Anyone want to give a thought or a question on figure 4.3?

Well, we're going to return to the contrasts of discrete time and continuous time multiple times.

It's going to come back most fully in Chapter 7 and Chapter 8, which are wholly dedicated to discrete and continuous time.

But in this section, it's going to foreshadow and discuss those two different types.

Darius?


SPEAKER_03:
presumably this model um is a kind of simplification of what might be going on in the cognitive creature I because I presume there are some backward Loops in which the observations that you make are informing your action policy hence the action perception Loop um

Similarly, I also make the presumption that the state dependencies have to be informed by prior action policies and beliefs.

So priors about these.

So I presume it's just this architecture needs to be built out a little bit more or else it's not really governed by any particular telos, by any particular incentive.


SPEAKER_05:
Yeah, great points.

There is a prior.

that helps get the ball rolling.

Um, and, and so that prior kind of starts the chain, but then in subsequent, um, time steps, the, the action and the unfolding of the situation at the prior time point, um, sets how things are.

Um, and certainly these are like, these are like, uh,

like a frame it does not have more advanced cognitive features like attention anticipation

so on those things get get layered in but this is kind of like the kernel or like the motif that we then can can elaborate and bring richness to and so that's like why sometimes there's an adjective in front of active inference like sophisticated active inference or deep active inference affective active inference and those adjectives call attention to certain structural

motifs that are being brought into play and this is kind of like this is like the lowest common denominator because we have a prior on hidden state what's a prior on the temperature of the room and then we have the temperature of the room changing through time and the thermometer readings emitted at each time

That's the downstairs passive inference bit.

D is a prior hidden state, real temperature.

B describes how temperatures change through time.

A describes the relationship between temperature emitting observations or observations inferring back to temperature.

Downstairs, looking like an E facing down, passive inference.

Where do we embed action in the partially observable Markov decision process?

Well, the partially observable part is on the bottom because we have parts that are observable and parts that aren't.

So it's partially observable.

The Markovian property through time is that the past only influences the future through the present.

Partially observable Markov.

And then the last piece is the decision process.

pi is here policy it's intervening in how the world changes through time and what guides policy selection well you could make a pomdp that has a stochastic policy selection or has reward driven policy selection or as we're going to see we utilize expected free energy to guide policy selection so this is kind of

bringing the well-known POMDP format to bear on our setting.

Younghoon?


SPEAKER_01:
Yeah, so I'm a bit confused about this generative model because, as you said, it's a generative model.

So does this model actually mean that we're doing some kind of cognitive process about this?

Because what I understood is that number one, number two, and number three, which is like maybe the number three, which should be located as the transition metrics

for a policy pie is actually something that is like a real dynamics of the external world, not something that we are estimating so.

that's that's something that i'm being confused so because if you're saying generic model, it means it sounds to me like something that is happening in our brain, but in what I understood was that this is a diagram about the external world itself.


SPEAKER_05:
Great, great question.

So in fact, we see 2 representations.

One of them is the particular partition looking representation with the 4 internal and external blanket states.

Here we don't see external states at all, and it's not even shown like where they connect in.

And this is actually all happening within the cognitive model of the agent.

I think the figure that clarifies this the most is 9.1.

So now we're pulling back to the behavioral experimenters lab.

So here's going to be the rat in the maze.

we are using the pomdp um architecture to make a model of the rats beliefs about temperature changing through time and its beliefs about how policy influences that its beliefs about how temperature and thermometer are linked

So the POMDP is describing the agent's parameterization.

It is not the actual true unfolding of the system.

Here, the observations are being actually, the generative process is in this outer layer.

if we want to think about it that way.

So yes, the B matrix is something that the agent learns about how states change through time and how its actions intervene.

But that's not actually how things change through time.

That's tracked by another variable.


SPEAKER_01:
Yeah.

Oh yeah, so does it mean that those.

The key.

Function allocation key should be actually to because it's an approximation isn't it.

So, like I don't really remember which chapter, it was but maybe chapter to the diagram actually kind of build that he is actually.

annotation for the notation for the uh probability happening in the real external world while two is something that we approximate so basically i think it's it's better it's much more accurate to say uh q pi q o given s and etc yes i i i


SPEAKER_05:
would like to see someone else also check on that.

However, yeah, I believe it makes sense to think about these as the belief distributions of the agent and hence utilize Q, whereas here P is just being used as like a generic statistical distribution.

But I am not 100% confident on that, but I agree.

And that is how we think about it from the agent's perspective.

And I agree that the Q's and the P's, mind your Q's and P's, it does get confusing because it's like, well, wait, but if we know anything about P, why are we not just using that information?

Why are we even messing around with Q if we can contrast it with P?


SPEAKER_01:
So these like minor, um, notation, uh, ambiguity is actually making me kind of confused about the equations though.


SPEAKER_05:
Just wanted to clarify.

Well in the, um, I, I would love to help connect the ontology terms, which then we can support with translations and everything.

to the equations and to the different representations because i i agree and um the equations work out and then second layer renderings or schematics

often include or exclude things that that reduce the clarity of what they're representing um well in the rest of four discrete time is explored more that's basically more math about the top part of figure four three section four five continuous time more math on the bottom of figure four three

Some extremely spicy and interesting topics with Markov blankets and message passing and some representations of message passing.

Generalized coordinates in motion and some visualizations about that, connecting it to Taylor series expansion.

Laplace approximation, another tool in the toolbox to help approximate

uh a given inference problem by using a quadratic expansion around the mode more message passing and that's the end of four so definitely it's a dense chapter most of the density is in this second half of four where it's just giving a lot more detail on discrete time and on continuous time

And the big picture is we're introducing the kernel or the minimal motif and some of the mechanics associated with using discrete time and continuous time models.

So next time, we will turn to the cohort four questions.

So it'd be awesome if people add questions and vote them.

And we'll come back and take it from there.

Okay.

Any last thoughts?

Okay.

Thank you.

Farewell, everybody.


SPEAKER_04:
Bye.
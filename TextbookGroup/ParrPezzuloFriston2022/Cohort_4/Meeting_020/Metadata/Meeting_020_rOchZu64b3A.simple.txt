SPEAKER_01:
All right.

Hello, everyone.

Welcome back.

It's our second discussion on chapter nine.

So we can look at the chapter and look at previous questions.

But before we do so, does anyone just want to give any general comment or thought or question on nine?

Like any section of nine that was interesting or any topic.


SPEAKER_00:
Sorry, I'm running a bit behind in the reading, so I haven't looked at chapter nine.


SPEAKER_01:
Or even another topic or another question if you want to go to it.


SPEAKER_04:
So sorry.


SPEAKER_01:
Go for it.


SPEAKER_04:
Oh, no, I just, I'm, this is my first time attending, and I did not know that we were going to be working on chapter nine.

So now that I I'll be out compared, but I was focusing, I focused my work mainly on making sure that I understand some key concepts in the book.

And so I'm not really quite ready for the task today.


SPEAKER_01:
Oh, it's all good.

Thank you for joining.

I mean, it's always listed here, but also it's kind of like we even talk about things outside the textbook.

So we can talk about whatever people want.

even just the theme of chapter nine with model-based data analysis.

I mean, this is the chapter where the models are connected to empirical data.

That's always a useful topic.

Or just anything people want to talk about.

Or we can look at some prior questions.


SPEAKER_04:
The only thing, just to

really minor comment about the book i just noticed that parts one and two are not don't have labels on the index page that's that's uh that's one thing that i picked up on um but like i said i'm i'm

New to this topic, I will bet I will do a lot of reading this week, but I really wanted to make sure that I understood where the free energy principle concept came from.

So I'm really way behind.


SPEAKER_01:
It's all good.

It's all good.

When you say part one and part two, you mean chapters one through five and chapter six through 10?


SPEAKER_04:
Well, when I have the PDFs,

And on the PDF on the first on the index page I noticed that that parts one and two there's no there's no label it just has the list of the sections.


SPEAKER_01:
Like this.

Part one and part two.


SPEAKER_04:
Part one and part two.

On the on right on the in the contents page.


SPEAKER_01:
Yeah.


SPEAKER_04:
so the label the label of the labels of the parts are not in the right um included and i didn't know if that was deliberate or yeah i think the part one and part two is just kind of like an informal okay like the chapters are the main components yeah


SPEAKER_01:
And if you want to, like, for the origins of the FEP, Chapter 10 might give you some information.


SPEAKER_04:
Oh, good.


SPEAKER_01:
Okay.

Yeah.

Okay.

Any other Chapter 9 related topics or Andrew had some things to discuss or anyone else could bring something up?

Okay.

Yeah, Orange, go for it.


SPEAKER_02:
Yes.


SPEAKER_01:
Please, go ahead.


SPEAKER_02:
Oh, okay.

Thanks, Daniel.

Thanks, Andrew.

Yes, I have a couple of questions regarding Chapter 9.

So I wonder how should I share?


SPEAKER_01:
Please add them to the questions.

Everyone is just overwhelmingly encouraged to add the written questions to the table.


SPEAKER_02:
That's always the best way.

So write the questions in the table.


SPEAKER_01:
Yeah, make a blank row and just put it into the question part.


SPEAKER_02:
Okay, okay.

Yeah, let me...

Okay, so I have to go to this this website right to share the questions.


SPEAKER_01:
yeah you can follow, like the you can click like the token for active inference to follow to where I am in Chapter nine.


SPEAKER_02:
Yes, I found chapter nine.


SPEAKER_01:
So add the questions there.


SPEAKER_02:
Okay.

Okay, let me check if I'm on the same page as you do.

Yeah.

Chapter nine.

Yeah, yeah, yeah.

I'm there.

So, okay.

Okay, so I have just juggle between.

So first, I have a question regarding equation 9.2.

So what is...

um this theta Alpha in equation 9.2 any similarity did you enter the question or where did you write okay okay what is this I'm gonna I just copied it into there

but yeah okay I read the wrong place okay it's okay it's okay just this is how we can get all the questions together what is Theta Alpha so because it hasn't show up in our previous you know chapters before right this Theta Theta sub Alpha yes Theta sub Alpha yeah


SPEAKER_01:
Yeah.

Okay.

So just to take a step back, we're using variational Laplace approximation.

So what this technique is doing is it's taking some distribution.

It's going to find the central tendency of it, and then it's going to fit an inverted parabola over it.

so that could be a narrow parabola or it could be a wide parabola so that's the variational laplace so it is a lot like kind of fitting a gaussian approximation but of course a gaussian has support everywhere and it continues whereas a parabola like just goes down um so that's what the laplace approximation does

Jason Serck- theta is being used as commonly to just refer to the total set of parameters okay.


SPEAKER_02:
Jason Serck- Now i'm.


SPEAKER_01:
Jason Serck- Down here.

Jason Serck- This is going to be in the context of policy selection.

there's going to be beliefs about action sequences we can take these beliefs and calculate the average probability of pursuing an action sequence so this is in the spirit of pursuing action selection as a pure statistical problem and finding the most likely path path of least action not appealing to any kind of like intermediate reward function or anything like that

This requires us to distribute the probability for each policy over the actions implied by that policy.

So the array U is going to connect the actions or the affordances to the policy.

Actions or affordances are things that can happen in a single time step.

Policies are sequences of multiple time steps in length.

So it's basically like saying each policy was evaluated, so time step n, and then each policy is described in terms of what actions it contains.

Finally, a soft max temperature parameter, theta sub alpha, is applied to account for randomness or shaky handedness in behavior not accounted for by the model.

So this is kind of just brought up here, but it's explored more in other papers.

When the softmax is one, your behavior is sampled directly from your policy posterior.

That's matching behavior.

When the soft max is very high, then you select the best behavior, even if it's only slightly better.

And then if your soft max is very low, it basically erases differences in the posterior likelihood of action.


SPEAKER_02:
Okay, so Daniel, this new parameter is not needed in the active inference model, right?

It's only needed when we do this parameter estimation, when we compute the likelihood function.


SPEAKER_01:
Right.

I mean, the core active inference model is very sparse.

It doesn't contain learning or attention or shaky hand.

So the textbook is showing us all these different motifs, none of them which you have to do.

But if your phenomena of interest is captured by this type of behavior, this is a useful motif.

but it's not like any of them are being required for us to do in all situations.

It's just saying this is one kind of structural model that can capture decision-making behavior.


SPEAKER_02:
Okay, okay.


SPEAKER_01:
But you're right, like previous examples didn't have this.


SPEAKER_02:
Right, right.

I also would like to clarify the policy you just mentioned there.

So policy, you said the policy specifies actions at each time point, if we think about the discrete time model, right?

So it simply says that, I don't know, what action I would take at time one, what action I would take at time two.


SPEAKER_01:
Yep.

Here, I just went to the ontology, just looked up policy.

It's a sequence of actions.


SPEAKER_02:
Okay.

I wonder, because in my area, when I think about policy as a...

as mapping from state to action, not simply from time to action.

So you're thinking about, you know, policies in time.

Why would I do in time to what I do?

In general, a policy defined as if in a certain state, what should I do?

Time just stays one dimension of the states.

Is this a special feature of active inference or is something consistent with other dynamic process?


SPEAKER_01:
It's more similar than not to how policy selection is discussed in reinforcement learning, deep Q learning, because fundamentally we're talking about action selection.

So it's overwhelmingly more similar than not.

But yes, there are some slight differences because here we're only talking about policies as sequences of action through time and then using other features of the model like other state inferences of the model to update our posterior on policy.

But we're not making policy explicitly a function of

other states other than time so it's the same yeah it's generally the same control theory setting with just a slightly different setup but but you you should document what is meant by policy in that setting pull out the quote or pull out how it's meant and then let's just juxtapose it side by side and then it'll be clear


SPEAKER_02:
right in control theory in general the policies is the actions function not only the time but also the system states right right that's right the sequences of actions are a function of parameters the model and a sequence of stimuli that's here the likelihood of the parameters


SPEAKER_01:
is describing the sequence of actions through time conditioned upon parameters model and observations we can go more into it but it's it's it's the same setting of inferring actions from other features of the model

okay so what do you what do you observes are these actions okay yeah um here here's us in the behavioral Laboratory observing utilda the objective empirical behavior of the system of interest

objective meaning empirical like what we objectively measured and recorded empirically in our laboratory so what we recorded as data is the output of the subject of our behavioral experiments

So this isn't taking on the philosophy of objective and subjective and all of this.

This is just a behavioral setup.

Here's the rat that's our behavioral subject.

It has a cognitive model, a generative model, and we're observing its behavior.

And that is being used as our empirical scientific data for cognitive modeling.


SPEAKER_02:
mm-hmm so oteona is also input by the uh experimental designer by the um so it's it's given kind of a parameter right the stimuli yeah the experimental stimuli is a action by the experimenter and a sensory state for the subject


SPEAKER_00:
Yeah, I think that the key here is that u tilde is the output of the behavioral experiment and the input of the fitting process that gives you the theta.

Once you have done the measurement, u tilde, you can guess which parameter is the best one to describe this behavior.


SPEAKER_02:
Right, right.

Yeah, here is the g state.

It's action to find the best parameter theta, right?

That's what it is.

or it's the purpose, theta is the parameters.

So the other question I have actually in this figure 9.1 is what is m?

So it seems m and theta are both parameters.


SPEAKER_01:
All the parameters of the model.


SPEAKER_02:
Then why do we use different symbols?

One uses m, another uses theta.


SPEAKER_01:
M for the generative model of the subject and theta for the experimental parameters.


SPEAKER_02:
Okay, for example, in the TMA's example chapter 7, what is M and what is theta?

What are the differences?


SPEAKER_01:
M would be the parameters that describe the cognitive model of the rat.

theta for the parameters describing the setup of the teammates like the idea that 50 of the time the food is over here or that when the food is here it has a 99 probability of reward okay so M basically represents other parameters not uh this is just a schema you know this is just saying yes this is it's you don't have to use exactly this one and only Bayesian graph

So it's just saying whatever the subjective model you construct is, you're going to be conditioning upon the experimental parameters and the cognitive model parameters.


SPEAKER_02:
Okay.

So theta is experimental parameters.

So those are the parameters that we're interested in, right?


SPEAKER_01:
And they might be some of the ones you're interested in, but different laboratories are interested in different questions.

Some might have a very trivial subjective model and be interested in doing inference up here.

Other times this might be fixed up here and there might be something interesting here.


SPEAKER_04:
May I interject a question?


SPEAKER_01:
Yeah.


SPEAKER_04:
Okay, so Daniel, something I've been thinking about, this is more of a philosophical question, but I really like this presentation.

I just wondered if it matters in the free energy principle approach to be worried about nested cognitive systems.

because in the work that i'm doing nestedness is uh for the system that i'm describing their subsystem so we have a system within another system within another system so

From a very abstract point of view, the nestedness is that not relevant because at every boundary you're just dealing with a different set of sensory input from something else.

So it doesn't matter if you're nested in anything, you're just a system and you can model the behaviors of that system separately from the fact that it's receiving input.

The inputs are coming from multiple levels of

higher order that affect it.

Does that make sense?


SPEAKER_01:
Yeah.

I mean, nested models are naturally amenable to this type of modeling.

So here's an example of a nested model.

So here we see this kind of figure 4.3 generative model shape.

Um, now that, that like E downstairs, like a downward facing E that's like the sense making component and then the upstairs with a policy selection.

So now that is kind of expanded so that each leaf of the E has another nested E.

and so there's nothing in active inference that says that systems must be nested however it is very amenable to modeling collective lateral interactions within a level and then having nested models across levels and then just briefly two kinds of nested models that we see sometimes the nested model refers to an actual spatial enclosure so

yeah a continent and then a region within a continent and then a village within the region um with like slower and bigger things being at higher levels but also we see nested models for metacognition where you have higher levels of metacognition as nested observers and and top-down actors of lower level of cognition so it's not spatially enclosing but it can be thought of as like a cognitive enclosure


SPEAKER_04:
I understand that, yes.

That was very helpful.

Thank you.

But so let me see if I can ask this a little.

This is very abstract, but is it the case that even, so even when we're looking at those nested models, the nest, the submodels, the ones that are nested within a bigger model,

Are there any modeling issues that you need to add to your subsystem in order to take into account that it's receiving inputs from multiple higher order systems?

Or does it, or do you just, from a modeling perspective, it doesn't matter?

You just look at, yes, they look identical.


SPEAKER_01:
in principle it's well behaved in practice the challenge is that nested models get very very large very quickly so then it's like well you know I mean you know the pancreas is nested in the liver and the liver and the chest and the person and the family and it's like all of a sudden you're just with a model with with millions of degrees of freedom

with limited data sets.

And yet, making a appropriately articulated nested model actually can help the model embody the structure of the world, which uses the data more effectively.

So a smartly constructed nested model will help you make the best of the data you have, but a sort of nested model construction as a fishing expedition ends up losing interpretability, even if it ends up recapitulating real features.


SPEAKER_04:
um yes, so no that's that Thank you, that was super helpful because actually so one of the boundaries to just to give an example, one of the boundaries said i'm working with this the boundary between or thinking about.

the human body under conditions of our early evolutionary adaptiveness environment.

And what's really interesting about humans is that we have this brain-gut-axis, psychoneuroimmunological, sophisticated system that has a very well-documented effect on the brain.

when the immune system is reacting to a pathogen.

So it sends signals to the brain and causes what's called the sickness behavior, right?

So that's an example of communication that we understand that's going on between subsystems and the brain.

um but we actually can talk about the communication the way that the communication is happening so you don't have to be doing a fishing expedition um you know eventually i would like to be able to model those relationships in some way yeah that's a great transition actually to what um andrew um brought up last week so like since we're chapter nine


SPEAKER_01:
um you know we were talking about the empirical data but but the book only has limited amount of empirical data but it cites um heavily these other studies but then Andrew suggested that we go into this one so I mean Andrew what what did you want to discuss about this paper I pulled out some of the the um figures that we can talk about but what do you want to talk about it sure um


SPEAKER_03:
Yeah, I had a limited amount of time available between last week and this week, but the initial thought was just kind of because this chapter lays out kind of a series of steps that one could follow as a sort of recipe.

The idea was maybe we could identify in layman's terms, well, as far as we can in layman's terms, like

something like what are what are those steps as played out like for example in this paper here um yeah so we kind of have like we can match like to each step of the recipe provided like maybe what's going on here right awesome yeah so chapter six we constructed the generative model


SPEAKER_01:
um here and these are all like steps that have to happen but this isn't exactly needing to happen this order but chapter six we basically construct the um generative model that's step two and it's associated likelihood function which is just the function of behavior being emitted conditioned upon parameters observations in the generative model like just that we talked about with 9.2 and then we collect data we crunch it all together

invert the Bayesian model, and then do statistics on the outcome.

So let's see what it looks like in the case of Ryan Smith et al's work on gut inference.

So they develop a Bayesian computational model for gastrointestinal interoception.

So this is not exactly like the microbiome neurotransmitter, slower timescale interaction, but this is the faster like vagal nerve mediated

interoception with the gastrointestinal um so it was completed by healthy individuals they were undergoing EEG so there's getting neuroimaging data and peripheral physiological recording like certain like some measures of the body and then this I also found this kind of interesting

these self-report questions how hungry do you feel how pl you know how difficult was this task this is just showing like you can assess a variety of physiological measures self-report measures these are all data

Another important thing to keep in mind is it's not about constructing the one generative model.

It's about having a portfolio of related generative models that do and don't have different components, and then you kind of test across them.

Like when they talked about what are the two reasons to do cognitive modeling, one of them was if you have a fixed generative model, then you can look for group differences.

Another one is from the dataset, you can identify which models are appropriate.

So here, these 12 models differ kind of combinatorically in their different features.

So standard statistical modeling technique, just factor A, just factor B, A and B together, A and B and C, just C and D together.

So that's what they're going to do.

And in table one, they define the same ontology terms that we see in the textbook.

Here's the test.

They swallow this pill that vibrates in their stomach.

And then they signal whether they did or didn't perceive it.

So the true state

hidden state is whether it vibrated or not that's known because that's controlled by the experimenter um and then the observations are the perception that they experience obviously if it was a fully observable situation you'd have like the a matrix would be an identity matrix you would just simply be able to perceive it

But there's some intensity of signal where what they're going to test is that, well, it turns out that it's not 100% perceived, but also it's not 0% perceived.

And maybe there are interesting patterns of people's perception.

Here is literally where we see the Bayesian graph.

now this is a model that doesn't include action because the person is not in control of vibrating this little thing so we only are seeing the downstairs part of the model so in that way it's um much like the early example in chapter seven

So this was the music listening example in Chapter seven just the downstairs the prior hidden state through time observations at each time step, but no upstairs action selection component.

Prior.

hidden state on the actual vibration, observations, whether it's perceived.

A is the mapping between vibration state and observation.

B is how it's transformed.

So that model could be specified ahead of time.

You could pre-register the study.

So you kind of know that you want that.

And then they describe this a lot more in the caption.

They also describe...

Yeah.

So then they do some descriptive statistics on this model.

The interoceptive precision.

So that's the precision of the A matrix.

If the A matrix is just the identity matrix, it's the most precise.

That means that you have perfect observation.

If the A matrix was low precision, it was just a wash.

You wouldn't be able to perceive anything.

And then it could be anything in between.

prior beliefs favoring the presence of vibration, PV, that's the D variable.

And then the learning rate is describing the rate of change on B. So those are not simply descriptive features of the empirical data.

If you just wanted to say who's has a higher accuracy or who's more right or something you could just summarize the data you wouldn't even need to do cognitive modeling.

Then they look at all of these different empirical patterns so here on the columns, these are um.

prima facie components of the behavioral study these are just descriptive outcomes from what empirically did happen like the true positive and negative rate and the reaction time patterns on the rows are these cognitive parameters that again they identified with their model so features of their model map not the territory and yet there are still these interesting patterns

um then they they can do different conditions and show that they're the cognitive parameters that they're interested in that those cognitive parameters are variable in different settings

Here they look at the cognitively inferred parameters on the rows and then a self-report and some kind of like personal data agent BMI.

Just like other scientific studies, like the kind of result is like a point estimate and a variance.

So you can say some of these results are significant at different thresholds.

Now they're going to connect the cognitive model parameter to EEG dynamics.

here they look at clustering of erps event related potentials so these are kind of like dynamical patterns in the eeg over different time scales and then they can cluster different regions by the similarity of their response and look at how those kind of traveling waves are related to um

those traveling waves, the ERPs, are related to those different cognitive modeling parameters.

And they do that for other brain regions too.


SPEAKER_02:
Excuse me, Daniel, can I ask a question?

Yeah.

The coordination matrix you showed, sorry, before with the learning rate, is learning rate estimated from the...

experimental data yes everything is estimated from the data in this paper okay so so how do they construct the mechanical function to do the estimation we use the um EEG data to to estimate


SPEAKER_01:
I mean, you look at the paper for more details, but the EEG data are being taken as kind of like, this is, you know, what you see is what you get here, right?

There's no EEG here.

There's just a prior on vibration and then an observation and then the mapping between the observation and the vibration or not.

that's kind of like the core phenomena that they're studying and then they have the age and the bmi and the eeg and the self-report so then those other data are kind of overlaid on the timeline of this kind of core model and then you can look for correlations between um this graph and the eeg

So I don't think you need to take in the likelihood of the EEG data specifically.


SPEAKER_02:
But in order to estimate ETA, the learning rate, they do need a likelihood function rate.

So ETA is just a parameter of this active inference model.


SPEAKER_01:
Yeah.

I mean, we can look for more details on how the learning rate is calculated, but they describe it in that caption.

I mean, it's it's described.

But yeah, look into it and see if that's how you would describe learning rate.

And these are features of the map, not the territory.

So again, it's common for people to look at this and then reference some feature of the world.

But it's not about referencing some feature of the world to validate or invalidate this model.

It's about generating a portfolio of models that have different features and then propagating that portfolio of models forwards and then saying, actually, learning rate does matter because the one that had it versus the one that didn't had this better fit.

And someone says, oh, but you didn't think about this.

And then you go, okay, cool.

Now let's add model 13 with that feature.

And that's the real, that's the delight of the empirical conversation.

Just people bring things up and it's like, okay, either we have explored it and we can talk about the impact of that variable or we haven't considered it yet.

So then thank you for the suggestion.

Let's add it in and then come back to it.

And that's the iterative modeling process.

how do we how do we know which model is a better description um so you have 12 models too yeah yeah great question they they describe this the most common method used is the Bayesian information criterion BIC or the related um AIC

um the BIC criteria basically um rewards fitting a model uh fitting the data well and it penalizes having extra parameters and so because you're all every model with another parameter you will always get a better fit like a principal component analysis the next principal component always is going to soak up more variants

So make the best fitting model is actually a kind of ironically useless singular goal.

But if you say, I want the model to be at the optimal trade-off where like the parameters are like punching above their weight.

So that's usually how people select it.

But that's not the only model selection imperative.

You might say, I'm okay with having a huge model.

I just want the most accurate one.

then you might take one that has more parameters or you might say i don't need the most accurate one i need it to just be better than 50 and i need it to you know fit on an index card yes there isn't an answer in the general case for which model you should select and you might want to actually just hold the whole portfolio and just keep that whole distribution that's like ensemble modeling

yeah so i'm just wondering why it doesn't show the selection criteria in the table when the selection criteria yeah so the bic they didn't show us right um well not in that table like i i mean i the the models are compared in some formal way but in that image they're just they're just listing which ones they are but yeah i could have included like the likelihood or the bic for the models or the number of parameters

um it's just not shown in that representation maybe in the supplement or maybe somewhere else but but yeah that that's usually what is shown you see like it will be like model one and it will be like Delta BIC zero and then it will show how how much behind in the BIC the other ones are

And then if we were studying an active setting, like let's just say that now we're in a little modified version where the person can be in the red or the orange room.

And they can move between those two rooms.

And then those two rooms have like a different probability of the stomach thing vibrating.

So now there's some policy variable.

Well, that's the power of the base graph.

It's just another parameter to estimate.

So the same, and then someone says, well, what about having a policy on the air conditioning as well?

Then you add the air conditioning policy.

Oh, and now we want the observations not just to be the interoception.

Now we want to have like, you know, exteroception too.

So you just keep composing and building the models.

But in chapter nine, that's where we make the contact with the empirical model.

And that kind of like reminder that there are real things to study and that you can design the generative model ahead of time and then do the experiment.

And then just as soon as you collect the data, you just dump it into the generative model and you basically have your paper ready.

That's one great option.

Another option is the data have already been collected and you know what structure they are.

and then you can construct a generative model that will load in that structure so whether the data are collected before or after obviously it's it's different scientifically but in both cases this chapter gets us to the point where we can take in data bring it into this kind of format but then in the end we make the kind of bar bar plots and

time series and correlation matrices that we would see like from literally any other scientific paper yeah what what did anyone think about this


SPEAKER_04:
Well, I just kind of stepping back and running to ask you.

So is a lot of the literature that on recent literature on the energy principle about, you know, when we're going to the empirical world is figuring out

or trying to infer what generative models different systems have in order for us to be able to begin to test processes from the free energy principle?


SPEAKER_01:
That's a great question.

Yeah, definitely like one line of research or multiple lines of research are related to the fundamental physics of cognition.

Some lines of research are more computational capacity building.

And so they're interested in motifs and computational implementations.

Another set of research that's more empirically oriented is looking at empirical data and developing cognitive models.

that's one direction of empiricism and then the other direction of empiricism is with the cognitive model in hand looking at differences amongst individuals or groups and like Ryan Smith et al's work has both of those directions of empiricism sometimes um like in this paper they're taking a simple setting and then they're exploring a range of cognitive models

In other settings, they're basically taking a cognitive model identified from previous work or however, and then asking, okay, do individuals with these different diagnoses differ, for example?

And those two directions of empiricism can coexist in one paper.

But then it's kind of like this joint identification of what should the model be, but then how do the individuals differ in the model?

And here's those two reasons that they say.

Computational phenotyping with the model in hand describing individuals or with the data in hand, ask which model you want to utilize.


SPEAKER_02:
Daniel, can I ask you a question here?

Yeah.

About TMA's example.

In chapter 7, the textbook provides a complete example about the active inference model.

And in this chapter, they also talk about that TMA's example.

But I wonder if there is any empirical data


SPEAKER_01:
about the the mouse you know movement and that can we can like you know use the same example to illustrate the the basic steps and idea in chapter nine yeah i mean it's a great question something i've wondered about too don't we have like hundreds of thousands of data points on the teammates i don't know what data set that is but but these are experiments that have been done

those are simulated data not the experimental data right yeah here they just were using the TMAs in the synthetic direction but yeah it's a great project to find an empirical data set um and then you would need to make the generative model for it because it wouldn't be exactly this but this structure might work well

yeah i mean this is the work that i hope that the textbook gets us all to is going to behavioral settings and studies that have been done and building generative model but this is literally the work that we have to do well you're not aware of anyone have done it right because


SPEAKER_02:
I asked a few people who extend this T-Maze example, but none of them have done empirical study and estimate the model parameters.


SPEAKER_01:
I've never seen it with like the mammal teammates again even though they're there are actually probably thousands of data sets on the teammates so I think that's a very viable option perhaps people use the teammates because it's easy to make a point about epistemic value but usually the teammates isn't the case that people are interested in studying

But if it were a lab that were studying decision-making or addiction or learning in rodents, then they would be actually looking to do a cognitive model with the teammates.

So that's where the collaboration between people with active inference expertise and domain experts, and then you have an iterated modeling process with them, are all the features included in this model and the back and forth there

And then find out what the generative model, what kind of empirical insights you can have.

And in that case, if it's already a data set, it's collected in a published paper.

That's the perfect setting because then you can say, here's what active inference goes beyond descriptive statistics and what it actually, what value it does add to the setting.


SPEAKER_03:
yeah yeah there's a lot of behavioral data sets to do this on yeah if you were actually looking for data sets i mean there's a great um

the busaki labs there's a yori busaki neuroscientist at nyu he has an entire lab dedicated to um things like for example studying like mice and rats are kind of his expertise and you know running many experiments where they're trying to use electrophysiological data to track

what is going on in mice, what's going on in their neurons whenever they're navigating a maze.

And it's where they do research on things like place cells and all these fascinating things where if you draw out a topological map out of the EEG data, you can actually see like the mouse is tracking where it is locationally.

So I see Daniel's point to bring it back that there are many experiments done

that highly relate to the TMA's example, but I guess it's around there that you can have collaboration between someone who's an expert in active inference versus those who carry out these kinds of experiments to test the data that's already been collected.

but within more of an active inference kind of framing because the lab technicians here at Busaki or otherwise are attempting to understand something more like the neurobiology, but they're not necessarily getting too in-depth with the cognitive modeling aspect or the kinds of questions that someone doing research in active inference might be asking.

That's my thought on that.


SPEAKER_02:
Yeah.


SPEAKER_04:
Thanks, Angel.

So this is a question that is bugging me, and I just want to bring it up now before I get really deep into all the material, which is

When working with human subjects, you know, one of the things that I see is that it's always been a huge problem about my research is that as soon as you enter into the interviewee setting, that in itself, you know, it has a huge observer effect.

So from the perspective of active inference, you're dealing with the fact that this

individual from whom they're getting some information about their generative model in their heads they have a generative model in their heads about that interaction and making and already trying to make assumptions or thinking about what is this interviewer going to ask me and what are their expectations because that's what we humans do and so if you could tell me early on where

In this world of active influences that address because it's a huge issue in in my systems in the systems that I work with.


SPEAKER_01:
yeah.

it's a big question there's a lot a lot of pieces to say on it okay um suffice to say that studying like a pendulum is an easier active system than study an active system that has Theory of Minds okay thank you thank you that was like really well good so yeah it's not to be found in this image but this image is kind of like yeah

pointing towards it okay yeah thank you to the textbook group um we return for chapter 10 next time okay thank you thank you very much take care bye
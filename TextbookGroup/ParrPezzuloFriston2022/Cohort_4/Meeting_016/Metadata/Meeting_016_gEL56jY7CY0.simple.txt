SPEAKER_00:
All right, greetings everyone.

It is October 10th, 10-10, 23, and we're in our discussion of Chapter 7.

So last week, Ali and I recorded like the short video overviews of Chapters 9 and 10, just because there was only a few of us during our Cohort 4 slot.

and we just wanted to complete those um overviews so now we're in the chapter seven um discussion so we'll jump over to the chapter seven page um and before we look at like any of the specific questions like does anybody just want to share um

What did they think about chapter seven?

What, what was their impression or, or how would they summarize it to, to somebody or what was just exciting to them about chapter seven?


SPEAKER_03:
Well, can I start?

Um, so hello.

daniel hi yeah go for it go for it yeah so so my um impression of chapter seven is about uh it's mainly about the uh discrete uh state and discrete outcome model of the active inference and i found it is particularly uh useful to

have a detailed understanding of one of the example, which is this T maze example.

And it is kind of the first active inference model that I can get my hand on and try it out and change those parameters or the matrices used in the active inference model.

and see how the active inference actually works.


SPEAKER_00:
Awesome.

I think that's kind of the point.

Anyone else want to give a thought on 7?


SPEAKER_01:
Yeah, I would just second everything that Wenxuan said just now.

I think it's a great way to actually see maybe how some of the ideas that we've been reading about prior in the textbook actually start to hang together and play out in an example.

I think that the

The use of discrete variables is highly like kind of interpretable I think this is a it comes a little bit more naturally to me to read this chapter versus.

You know Chapter eight with continuous variables, because we get to see like you're you know more precise specific things that have discrete choices and.

and things that occur and then also just some kind of interesting observations that come out of that was a different.

um kind of components of the model like you know as you get further in the chapter there's a discussion of like here are the interesting things that happen whenever you don't include preferences or have highly uniform preferences like there's just pure novelty seeking on the part of the agent who

Like in the team A's example, they just can't sit still.

They never come back to the same spot.

It's pure just seeking epistemic value, it seems, because there's nothing pragmatic to do, because there's nothing in particular that they're going for, because it's not been hard-coded into the initialized preferences.

So yeah, just giving us that really big,

Matt Pinyan, model for active inference here in the discrete form, in addition, seeing the interesting dynamics that can kind of play out emerging properties and so on.


SPEAKER_00:
Matt Pinyan, awesome.

Matt Pinyan, anyone else want to make a general.

Matt Pinyan, comment on seven.


SPEAKER_02:
I would just echo that, given that I've come from a psychology background, and so seeing the exploration, exploitation paradigm just being laid out in that way, and they make their comment.

They believe they've come to understand how these two go hand in hand instead of being competitive.

I thought that was also very, very intuitive.


SPEAKER_00:
Cool.

Yes.

yeah the continuous time setting is kind of like having your hand on the rudder in a boat and uh so you're making continuous actions and there's a continuous flow in these paths and the discrete time is like chess or checkers like time clicks discreetly and the actions are discrete the states are discrete and so that can make it kind of

clearer a little bit more cut and dry um cool does anybody want to jump to a specific section of seven or we can look at the questions and just like look at the upvoted ones or if anybody submitted any and just like look at what's interesting in seven or or somebody can propose a section that that they want to look at


SPEAKER_03:
I'm mostly interested in chapter 7.3, decision-making and planning as a reference.

I wonder if someone else has preference to talk about the 7.3.


SPEAKER_00:
Yeah, let's get to 7.3.

Basically, 7.1 is just one paragraph we can see here.

7.2 is just a passive inference model.

This is just the downstairs section of the discrete time model.

We have hidden states changing through time, being related to observations at each time point and a prior.

So it's just the downstairs of figure 4.3, and it's kind of worked out in multiple stages with this music listening example.

so then we get to 7.3 now we take that downstairs the part that looks like an e um and we add in the action so this is where we actually get active inference and then what makes it active inference under the free energy principle is that the unified imperative for action selection is going to be expected free energy

so right because there are other models that have um sense making or signal processing on the inbound and then decision making or action selection or control theory on the outbound like that is in a loose sense active inference but when we're talking about active inference we're talking about active inference and the free energy principle

which is what we get when we bring in the variational and the expected free energy.

Okay, so what aspect of 7.3 do you want to explore?


SPEAKER_03:
Well, first about this figure that you're showing here.

I have a question about the fourth line from the bottom.

Do you see third while C and E appear in the panel on the right?

Do you see that sentence?

Mm-hmm.

Where is the E?

I couldn't find the E. Yes.


SPEAKER_00:
So E is the habit.

E represents the prior distribution on policies.

So let's just say that we have four affordances, up, down, left, right.

The E vector is like 0.25, 0.25, 0.25, 0.25.

So in that situation, E conveys through its inclusion which actions are possible.

And by the value in E, that is the prior on policy.

So if they were all 0.25s and there was four affordances, then that would be like saying those options are

in the absence of being sharpened by expected free energy are equally likely to be taken.

And, um, we can look at, um, the, the nervous system diagram in figure 5.4.

So here's E so here on the indirect pathway observations are coming in habits are being triggered.

on the direct pathway we have expected free energy sharpening our habit distribution that doesn't just mean yeah it doesn't just mean exaggerating it it could upweight something like let's just say there's a rare policy um like but in a given situation it has high epistemic and or pragmatic value

then you'd hope that even though your prior on um doing it is very low like what's the prior on taking off your seat belt well on a 10-hour car ride it's low but then when the car is stopped and you want to leave that's an amazingly salient thing to do so he basically hosts what can be done and in the absence of any decision-making process

the distribution of actions to be taken, and then G sharpens that to update the policy posterior.

And then the policy posterior is where action is sampled from.


SPEAKER_03:
I see.

I see.

From the figure of 7.3, it seems that C and E plays the same role as affecting the distribution of policy, right?


SPEAKER_00:
they kind of absolutely um absolutely like um free energy expected free energy here G can be understood as a statistical distribution about policy conditioned upon C and E we we just discussed why E matters for the calculation of policy posterior

C matters because of equation 2.6.

So C matters for G calculation because C is going to decide what is of pragmatic value.

So in that representation that they have in 7.3, they're basically just saying policy is being computed

conditioned upon preference and on habit.


SPEAKER_03:
Okay.

So C is preference, E is habit.

So they encode different kind of preferences of the policies.


SPEAKER_00:
Yeah.


SPEAKER_03:
Okay.

Thank you, Daniel.

That's very, very good.


UNKNOWN:
Cool.


SPEAKER_00:
Yeah, I mean, anyone else have like a thought on 7.3 or a question on 7.3 or just how they see it or how they interpret it?


SPEAKER_03:
So I have further question.

So again, on this figure of 7.3, so we see the policy pie affects the, the, the,

state-pervenient transition matrix B. I wonder if it's possible to model the policy also affect the matrix A, meaning that it not only affects the pervenient transition matrix, but also affects the transformation from the state to the outcome.


SPEAKER_00:
David Price- Great great question all kind of mentioned two ways that you could think about that so.

David Price- As you mentioned policy intervenes in be how things change their time, like when you turn on the air conditioner.

doesn't like reach into the room and just like modify the thermometer it modifies how the temperature changes through time so it modifies the dynamics of the hidden state the temperature and then that results in a thermometer changing so first answer to the question is we don't need policy to change a if we want if we prefer it to be cooler

we take the policy to turn on the air conditioner then in the next time step the temperature of the room is going to drop let's say one policy is you know turn on the air conditioner temperature drops by one turn off it it goes up by one so you turn it on the temperature drops in the room and then the thermometer is cooler so one answer is

you don't really want to touch a, because the mapping between hidden states and observations, isn't really something you can act on directly.

And so we're intervening in the real underlying hidden causal process.

And then that admits up first.

And the second answer is you could develop an active inference model where a meta cognitive agent selects, which a matrix to use.

So, um, maybe you decide whether to put on sunglasses or not.

And then when you have sunglasses on colors are mapped to your vision this way.

And when you do something different, they're mapped a different way.

So that would be taking a policy choice on which aid to use.


SPEAKER_03:
Okay.


SPEAKER_00:
But in most settings, the relationship between hidden states and observations, it's orthogonal to, or you could say it's factorized away from how hidden states change their time.

Keeping this model sparse in the sense that keeping as few edges as possible,

keeps the computational cost low and keeps the kind of parsimony high.


SPEAKER_03:
Thanks.

That's very good.

So yeah, so it really depends on how the states and the outcomes are defined.

Maybe in my case, we can define it in such a way that the policy affects only the state's transition probability, not affecting the translation from the state to outcomes.


SPEAKER_00:
Yeah, in almost all cases.

you will have policy only intervening in B. So making a generative model, I mean, this chapter six, making a generative model is, if you can describe what all of these are in your system of interest, you literally have the generative model.

And how do you categorize different things that you have or want to include into this?

Well, observations are literally the data.

If it's in a spreadsheet, it's literally an observation.

then policies are actions.

So if something is something that an agent can do and then everything else is a hidden state.

Now, if you're in a fully observable situation like a chessboard, then it can seem like there's a redundancy, like the observation is the location of the piece and the hidden state is the true location of the piece.

So in that case, it's not really a partially observable Markov decision process, POMDP, it's just an MDP because it's a fully observable decision process.

But A allows us to bring in the partial observability.

But if you're in a fully observable setting, then you can almost think about

direct intervention however even in that case then the a matrix is just an identity matrix like it maps perfectly between hidden states you know you have an accurate thermometer 37 just maps to 37. then a is an identity matrix and it has basically no computational cost and then your model is more flexible in case you ever did have a noisy thermometer or a missing measurement if you had a pomdp


SPEAKER_03:
then it's fine but if you were to only have the mdp then you can't deal with the noisy or a partially observable setting yes thank you thank you daniel that's very detailed yeah i'll hold out uh hold my questions for okay other participants to ask yeah


SPEAKER_00:
Anyone want to just give a thought or a question on this section?


SPEAKER_01:
um yeah I guess just since it's it's been a bit since I actually read like really did a deep dive on this particular chapter um but um I just I'm trying to make sure I grasp like whenever they're describing the the teammates shortly after um

Yeah, I guess we're still in 7.3.

They just start to describe like the A matrices.

Like in this case, they become tensors.

With the TMAs, you have their outcomes and then there are locations and then there are contexts.

So I guess the, you know, the mouse,

uh searching for a reward is looking for i guess the outcome is like whether the the space they land on is a reward or not um and uh the location is just you know them navigating the maze itself the locations of the maze and then also you have the context i might i might be mixing those up a bit but it's just

just trying to make sure i can grasp like we're looking at potentially 3d 4d and so on um like kind of tensor um you know quantities here that are based on all of the kind of dimensions to what the mouse is doing rather than just uh yeah looking at it's like a 2d thing is as nice and simple as that would be yes um


SPEAKER_00:
Basically, yes.

So A, I mean, you could think of it as, okay, well, pull back.

The context is whether you're in world state A or world state B. So the two contexts are whether the food is on the left or the right in this study.

You did mention that.

Okay.

So now A maps between observations and hidden states.

there's going to be two types of hidden states.

And so each of them projects out to a given observation.

A1 is about location.

A2 is about context.

I don't know if you would properly call this a tensor, but certainly this is two matrices.

But it's not, but they're different shapes.

This is a five by five matrix.

And this is a three by four matrix.

When we get to B, it will be a true tensor in the sense that every slice of B is going to be a matrix with the same dimensionality.

But here it's kind of like two parallel channels.

Like this would be analogous to having two sensory modalities, right?

So if we had vision and hearing, those A matrices could map to the same hidden state, like maybe, you know, is there lightning?

Or is there a thunderstorm?

And you might have a mapping between hearing and is there a thunderstorm?

And vision and is there a thunderstorm?

Or you can have those two observation modalities

map to two different hidden state modalities like in this case i just want to make one more point before we continue um forward in the teammates because um it was brought up about the explore exploit the most probable policies are those that lead to the lowest expected free energy

Again, this is one of the key distinguishing features, if not the key distinguishing feature between active inference, guided policy selection, and reinforcement learning or reward maximization.

There is no proposal of a reward or utility function that gets optimized.

There's just the definition of the generative model, and then the generative model does the likeliest thing, given its setup.

Again, that doesn't mean the animal lives.

It doesn't mean that X, Y, Z. It just means that given how the generative model is set up, it does the likeliest thing, and then given empirical results,

David Price- Data chapter nine we set up or parameterize the generative model to be the likeliest one.


SPEAKER_03:
David Price- So no okay.

David Price- yeah sorry yeah I do have a question regarding 7.4 where you're right there.

David Price- yeah here.

David Price- So i'm i'm a bit confused what she and Q means um.

Do we always use P represents the prior or the true distribution and Q as approximate distribution?

Is this Q the one we actually want, the decision that we intend to change our belief by updating our approximation to the real probability?

So P is the real map.


SPEAKER_00:
Exactly.

Q is the real mapping in this case.

So it's a natural log.

So this is kind of like taking a surprisal.

Okay.

It's of observations conditioned upon preferences.

So this is really what you observe in relationship to what you prefer.

You, and the reason why it has the P is because we're talking about like the, the actual observations coming in and Q is the variational distribution that we control.


SPEAKER_03:
Okay.

Okay.

Yeah.

Now it's clear.

So we actually, we can change Q to minimize this G Q G pi, this object function here.

Right.


SPEAKER_00:
yeah um yeah like q are about our beliefs whereas we don't really get to control in the same way like we q of observations conditioned upon policy so what do i believe the sequence of future observations will be o tilde conditioned upon what i do that's all in the mind

that is different than what is the sequence of observations given my preferences.

You can change your preferences, but that doesn't change observations given your preferences.

And similarly, you could change your belief about hidden states, but you can't in the same way control the mapping between observations and hidden states in actuality.

yeah and this is like a restatement in equation 7.4 of equation 2.6 expected free energy so we have same epistemic and pragmatic value okay so let's go through the teammates and just see how this is gonna play out okay so the two factors so factor is being used to describe a different kind of hidden state

So again, there's two factors here.

There's the context factor and the location factor.

One of those is a controllable factor, location, whereas context is not a controllable factor.

If it were, there would be another policy type, change the food to the left to the right.

But we're not in that setting.

So this example brings up the distinction between controllable and uncontrollable factors.

Observations.

They are the fundamental observables that's in the spreadsheet.

Then everything that's not an observation is a hidden state factor, state factor.

And then some state factors are controllable.

Those are the ones that have policies intervening in their B matrices.

Other state factors are uncontrollable.

It's just a latent cause of the world.

that's proposed within a cognitive model but with respect to that uncontrollable state factor that's like listening to music the first example that's like a passive inference on that state factor okay so the controllable state are the are the uh are the locations of the mouse right right because it can move

uncontrollable stays are the locations of the stimulus yeah because the mouse doesn't get to decide where the food is all right yeah okay yeah um so and that's where agency and self-efficacy can come into play is to what extent do actions bear upon um

can can the agent take actions and make a difference that makes a difference take an action that changes some state in the world controllable factor and how are and then this is all about how they evaluate that it could be chosen because it has high pragmatic value because doing that kind of exerting that kind of control

brings observations closer to alignment with preferences, pragmatic value, or it might lead to new information about the world, epistemic value.

And that's the entropy, the expected difference of the entropy of observations conditioned upon hidden states

in comparison with observations conditioned upon policy so this is basically like the entropy of the a matrix so that's that's the mapping between observations and hidden states

Here, we're asking how much is it gonna matter differently if we condition upon policy?

So let's just say that the A matrix has a given entropy.

The A matrix has a given uncertainty associated with it.

The least uncertain A matrix is like the identity matrix.

Just one on the diagonal and all zeros elsewhere.

You have perfect knowledge.

It becomes a fully observable setting.

The most entropy A matrix is just all uniform.

Basically, the measurements don't even matter because all the measurements are equally likely to map to any hidden state.

So that's on the left side.

And now the question is, conditioned upon action,

is there gonna be a difference in the sharpness of that mapping?

Well, if it's a policy that doesn't lead to sharpening of the A, it doesn't have epistemic value.

In comparison, if there's a policy such that you believe that taking that policy is gonna sharpen your understanding of outcomes, then this is a non-zero value.

So that's the epistemology.


SPEAKER_03:
Yes, Daniel.

However, you said that, you know, from the model, the A matrix is fixed, right?

It's not affected by, you know, all the entries in A matrix are not affected by the policy pi.

Because policy pi affects only the state transition probability matrix B. And

What do you mean by sharpening A using policy pi?


SPEAKER_00:
Good question.

So there's a few pieces here.

First off, there's A matrix learning.

That is not happening in this example.

But it is a cognitive phenomena that the A matrix could be updated, but it's not being.

There's no learning here.

Now, let's just say that we have observations and hidden states.

One of the observations doesn't lead to gaining information about hidden states.

Like it maps 50% of the time to A or B.

another observation gives you a lot of information about the world.

Then that policy can, the policy that leads you to get that informative observation, that has a high epistemic value.

Now, that doesn't mean that policy reaches into A,

in the factorization of the base graph right so it's it's not like every single thing it's like saying well um how could what the finger does influence the knee they're not connected directly it's like well yeah but the whole generative model is executed as a whole so um

You can, I mean, to put it a different way, you can use expressions in the active inference ontology that associate different variables that aren't necessarily connected through an edge here.

So these edges are not the only possible sentences that you can construct when talking about the generative model.

So you can talk about how G is related to O, right?

Obviously, because observations are what are, O tilde are what are being taken into account when we're talking about G, but there's no line between G and observations.

So this is just asking, will those observations be informative?

and and the observations being informative doesn't need to entail updating of the a matrix I see I see yeah good point good yeah or you know there's two people you can ask one of them he flips a coin the other one right tells you the truth so the epistemic value with going to the person who gives you the good information is high

even if you have a fixed understanding of that situation.

Let's look at it in the TMAs setting.


SPEAKER_03:
Yeah.


SPEAKER_00:
Okay.

Okay.

So technically, okay.

Technically the A matrices are tensors, but it's a little bit of an irregular one.

Okay.

So here we have these two kind of parallel A's.

on the first and the second factors mapping the actual location.

Here on the rows, note that there's this L and the R. This is because the hidden states

are in the cases of being on the right, being on the left, and being in the starting position, those are all identity mappings.

But now because we're in the black on the right setting, the epistemic Q at the bottom, when you are physically in the bottom, you go to the hidden state, which tells you you're in the R context.

And there's all zeros here.

That hidden state cannot be reached.

But of course, when the context is switched, it's the exact opposite.

There's a one here.

Going to the bottom maps you to the, it's the L state, and there is no way to reach the R state.

And we also see a flipping of the 98s.

98 is here on the bottom left and on the right.

And here it's on the top left on the right.

This is, again, just to show these matrices are necessary and sufficient for specifying the situation.

They describe literally exactly what is happening.

All that your generative model knows is what you specify in these matrices.

There's no other information.

So that's the A matrix.

This is showing the two contexts.

and it's showing the mappings between location and this kind of like location plus distinction between the messages that the epistemic cue on the bottom could provide.

And then also notice that when you're in the starting or the bottom position,

you're you're in the sort of like dashed line like you're not finding out anything but then when you're in but then there's a 98 chance you have a 98 sensory accuracy there still is a two percent mapping between um you know um if you know if you're on the left in this context and it's black there's still a two percent chance it maps to white

Okay, now we get to B. So similarly, we're gonna have two B matrices or two B tensors for each of these state factors.

B2, they're gonna introduce first because B2, the context stays constant over time and can be represented as an identity matrix.

So basically this is just keeping the same thing through time within a trial.

And this is where we get the true B tensor.

Here, this describes location changes as a function of movement selection.

So we have from and to.

And then there's four actions.

B does all the work.

Let's look at the generative model.

Observations are just data points.

Hidden states are hidden states.

They're just descriptors, proposed or actual latent descriptors.

And then this is just a sensory mapping.

but all the work of the meaning of embedded action is going to play out through B. The dimensionality of B is also higher because B is whatever the dimensionality is of the state factor, then as many slices as there are policies.

Because each policy

is going to change how the world changes now if all of these slices were the same that would be like saying whichever policy you select the world is going to change in the same way so in that situation there's no real causal efficacy or agency of that agent yes so b is really where it's at

And in this example, the B is just given and fixed, but there are other examples that we could explore that you can look up where there's B matrix learning.

So that's learning the consequences of one's actions through taking different policies.

So that's a situation kind of like a multi-armed bandit where you actually have to learn the mapping

between different things.

Preferences.

Here, C2 is the context associated preference vector.

So we have, if there's, the dashed line is neutral.

So this is, these three elements correspond

to these three elements, zero, six, negative and six.

And then these five in the top C correspond to these five.

And then as they say, there's a slight, they give a negative one to the starting position just to kind of bump it off and get it going.

Because if it really had no preferences, I mean,

all of this comes into the parameterization why is it six and not nine and negative nine why not nine and negative two that is the parameterization question so when we're in a pedagogical setting and we're just like looking to play around that's when it's super helpful to have the the code and like do parameter sweeps or just by hand play around with how different variables change the outcome of the model um

And then in chapter nine, it discusses more, well, how would you go from a bunch of videos of rats in the maze to inferring what the likeliest parameters are?

But here they're just given.

Also, just note that these values are given in...

away that it's actually like e to the blank.

So the six here has actually a very interpretable meaning in terms of the ratio of the preference.

So this is not just like six arbitrary points.

or like, I don't know.

It is just literally describing kind of like the odds ratio.

Right.

D, in the context domain, it's just one half, one one.

Or it could have just said 0.5, 0.5.

But they're conveying

that it has to sum to one something must happen this is a distribution and the values are the same so uniform distribution this is a perfect knowledge over the starting position right daniel cass or christine here


SPEAKER_03:
So this D2 is half and half probability, right?

But this is not known to the agent who makes the active inference, right?


SPEAKER_00:
It is known.

It starts out completely unsure.

It starts out with a 50-50 belief.

But it's kind of funny because it's like, it's 100% sure that it's 50-50.

Like same as a coin flip.

You're 100% sure that it's 50-50.

If you had a more sophisticated model, and it is shown in a later figure, you could have a hyper prior on D. So you could ask, how confident am I about the prior?

You can do that for any of them.


SPEAKER_03:
okay okay so you have a hyper prior that can uh characterize d2 uh maybe character by some parameter right so you can later on learn those parameters exactly exactly any parameter that you want to learn in a bayesian statistic setting you need a hyper prior if you don't if how if it's just a fixed parameter


SPEAKER_00:
then it's like the equivalent of the Dirac Delta function it's just like a spike like you have like it's just like a single observation right yeah um and and it's always a very interesting thing in this case or in in your own generative models like what is being made explicit

And then what is actually being kept implicit?

For example, how does the rat know that it's going to get the information at that location?


SPEAKER_03:
Right.

More question.

How does the rat know it's half and half?

It's just given.


SPEAKER_00:
I mean, that's like asking how does the rat know that moving to the left is going to get it to the left?

But also keep the figure 9.2 in mind.

Weird, it's like it can't.

9.1.

we're making a map of the rat here is literally figure four figure 4.3 or figure 7.3 right there d is not shown so we're just describing its map of its space we're not claiming to make a total account of everything that the rat knows um but yeah i mean there's more to say on this example

Then there's an I example.

Here in 7.10, here's the prior on D. Here's the lowercase letters or hyperpriors on everything.

Then another example with a maze finding agent.

And then a hierarchical nested model.

where you put a GM as a leaf in another GM and that is shown in a nested example so it's kind of a long chapter actually but because there's a lot of examples that are kind of like densely packed but just in our last like minutes here what what does anyone find interesting or what questions they still have


SPEAKER_03:
Well, I still have one more question in the end of section 7.3.

Okay.

So it's two sentences above 7.4.


SPEAKER_00:
Okay.


SPEAKER_03:
Yeah, you're still at.

OK.

OK.

I think further up.

So it talks about the neural neural activity.

OK, yeah.

So the second to the last sentence.

They talk about field potentials and neural activities, right?

uh in figure 7.2 yeah um so so my question is how realistic is this um chima's model uh compared to the experimental data so we have this is a simulation right so basically we constructed the actual uh inverse model using the data to simulate

the activities of a mouse in the teammates.

So my question is, how does the simulation results compare to the actual biological experiment, the activities of the mouse and how the mouse brain activities


SPEAKER_00:
imply or or correspond to this numerical simulation i'm not aware of them doing this in a rat doing the teammates but it may exist we should look it up in other settings it maps very very well now that's just like from from my perspective it's just a bonus because active inference is not developed as an account of brains

The particular partition, the Bayesian mechanics, all of the cognitive physics doesn't need to be about any specific system.

However, as the empirical lines of work have shown, various features of the model that are very non-trivial are recapitulated in biometrics.

That suggests that the analytical solution

and the biological embodiments have a convergence strategy.

An example of that is predictive coding.

Predictive coding was invented by humans as a way to do video encoding because it turns out that frame differencing is the most efficient way to do video encoding.

Like instead of just trying to compress every frame,

you basically compress the B matrix because if something doesn't change, you don't even need to mention it in the next slide.

So predictive coding was a purely information theoretic consideration, but it turned out to be utilized in neural coding situations.

So I see it as positive where it aligns, but where it doesn't align, then

it doesn't.

It just means that there's something else happening that isn't just narrowly that.


SPEAKER_03:
Yeah, because you see in figure 7.2, they show the neural activities and the fire arrays and local field potentials.

So there must be some mechanism to translate the outcome of the numerical simulation into

those neural activities, right, the biological embodiment of the numerical size.

So through what mechanism do they transform this numerical into the field potentials, for example, neural activities?


SPEAKER_00:
Yeah, it's all in the MATLAB code in the MATLAB functions that are used.

It describes it in more detail, but basically the LFPs are associated with, with belief updating.


SPEAKER_03:
Hmm.

Have a firearm race.

Are they associated with the decision?


SPEAKER_00:
You could imagine different firing rates or different, I mean, and this has been studied extensively in the FMRI and EEG setting.

Does the ERP in the ERG or does the local field potential associate with a more intense stimuli, with a more expected stimuli, with a less expected stimuli, with learning?

Those are all hypotheses that people have explored.

And for different stimuli in different settings, you see different patterns.

Okay, any last comments here?


SPEAKER_01:
I mean, I don't wanna keep us too long, but I just, yeah, I still,

as a last bit, I find the novelty seeking rather interesting.

Like just the example, it's roughly around page 145, 146.

Like if you were to model

give it like if there were an absence of preferences for an agent then like i'm just quoting here the drive to novelty resolution give it by expected free energy minimization leads our simulated creature to avoid any previously visited locations um just like

You know, it becomes like a purely like the model is attempting to resolve expected free energy, if I understand this correctly, by just it's just seeking more and more information.

It's going to places it hasn't been before.

um there's nothing pragmatic about returning to previous places it's not attempting to realize any preferences that say i need to go back right so i just thought that's a really nice kind of kind of emergent property that we see whenever you leave out preferences here and it for me it kind of

lays a lot of emphasis on like the importance of of preferences their impact upon behavior in these models and just maybe makes me think of like how to you know maybe start with preferences how to model those appropriately because it can highly impact um right like an agent's behavior yep great points and the strength of the preferences overall


SPEAKER_00:
And where that ends up on a continuum between novelty seeking and basically ruthless pragmatism.

Indeed.

All right.

Thank you all.

We go into chapter eight next time in the later time slot.

So peace till next time.

Bye.


SPEAKER_03:
Thank you, Daniel.

Good day.
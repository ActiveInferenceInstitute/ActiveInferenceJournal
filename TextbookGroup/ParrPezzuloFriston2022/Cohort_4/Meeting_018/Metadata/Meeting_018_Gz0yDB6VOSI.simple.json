[
  {
    "start": 3.457,
    "end": 12.188,
    "text": " Alright, hey everyone, it's October 24th, 23, and we're in our second discussion of Chapter 8 in Cohort 4.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 13.689,
    "end": 26.545,
    "text": "So, we can do any number of things, but does anyone want to begin with any thoughts on Chapter 8?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 38.759,
    "end": 41.742,
    "text": " We can look over past notes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 42.443,
    "end": 45.967,
    "text": "We can look at the extant questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 47.929,
    "end": 50.591,
    "text": "We can hear any new questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 50.712,
    "end": 51.893,
    "text": "We can look at the text.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 63.185,
    "end": 66.268,
    "text": "Yeah, we have a very simple question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 66.417,
    "end": 72.027,
    "text": " In the continuous case, is there some kind of standard code to use to try modeling?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 72.047,
    "end": 80.062,
    "text": "I mean, to test some very simple models as there is for the discrete case with the SPM framework?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 82.927,
    "end": 84.57,
    "text": "That's a very good question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 87.375,
    "end": 90.18,
    "text": "I'll give my thought, just it may not be fully accurate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 91.477,
    "end": 102.815,
    "text": " In the MATLAB, SPM-based and Python, PyMDP-based code, there's been an emphasis on the discrete modeling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 104.097,
    "end": 115.154,
    "text": "In the Rx and Fur Julia package, I believe that there's a lot more continuous time models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 117.738,
    "end": 117.838,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 118.173,
    "end": 128.143,
    "text": " So I'm not offhand familiar with any continuous time implementations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 128.263,
    "end": 131.186,
    "text": "We can look at the implementations table that we have.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 134.029,
    "end": 144.4,
    "text": "Eventually, I'd love to see these repositories, of course, expand and grow, but also annotated.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 145.561,
    "end": 148.044,
    "text": " with different features of the model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 149.226,
    "end": 166.187,
    "text": "So then we could filter and say, I'm looking for this language, continuous time, hierarchical model, but almost all of the Python that I've seen was discreet.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 167.469,
    "end": 167.569,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 177.117,
    "end": 180.141,
    "text": " Any other random thoughts or questions?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 180.722,
    "end": 183.466,
    "text": "Otherwise, we can look at some popular questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 187.171,
    "end": 189.194,
    "text": "And look at the notes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 205.016,
    "end": 205.977,
    "text": "Does anyone have any",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 208.387,
    "end": 234.748,
    "text": " just even from outside of active inference thoughts on continuous and discrete time anything with digital and analog do we see one of them as being the general case of the other or do they both derive from a point",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 254.152,
    "end": 256.314,
    "text": " Wangshan?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 256.494,
    "end": 257.755,
    "text": "Yes, Daniel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 257.996,
    "end": 267.285,
    "text": "I'm just wondering, what is the fundamental difference between continuous time model and discrete time model?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 268.065,
    "end": 274.692,
    "text": "Is that the fundamental difference is in terms of the state transition?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 274.752,
    "end": 283.18,
    "text": "In the continuous model, it's represented as a differential equation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 283.598,
    "end": 291.308,
    "text": " And in the dynamic model, in the discrete model, it's expressed as a kind of a difference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 296.435,
    "end": 297.557,
    "text": "Great question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 302.724,
    "end": 305.147,
    "text": "Does anyone have a thought on this?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 305.167,
    "end": 305.648,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 305.668,
    "end": 307.33,
    "text": "Here, what does continuous mean?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 307.51,
    "end": 312.357,
    "text": "Continuous in time or continuous in the state space?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 312.877,
    "end": 330.565,
    "text": " uh or refer to both i suppose it you know mostly means it means the continuous scene in time right now um does anyone want to give a thought on this",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 342.307,
    "end": 345.258,
    "text": " Yes, it is referring to the treatment of time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 346.482,
    "end": 353.869,
    "text": "So here in figure 4-3, we have basically chapter 7 and chapter 8 laid out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 355.351,
    "end": 363.186,
    "text": " In the discrete time setting, then let's talk about what's the same first.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 363.828,
    "end": 366.473,
    "text": "Broadly, their architecture is the same.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 366.713,
    "end": 368.276,
    "text": "That's why they're being laid out this way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 368.878,
    "end": 369.659,
    "text": "What else is the same?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 370.481,
    "end": 372.685,
    "text": "Priors are set the same way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 373.863,
    "end": 402.486,
    "text": " similarly enough d and also the upstairs the policy selection apparatus is broadly the same we're still dealing with free energy based policy selection mechanics um also what's the same is the hidden state to observable mapping the partially observable setting with two so",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 402.736,
    "end": 429.385,
    "text": " really what's different is kind of like the core of the horizon here in the discrete time setting we have explicit hidden state um or external state um calculations at given time points t minus one t t plus one right those themselves as you pointed out could be continuous",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 430.073,
    "end": 433.217,
    "text": " So that could be like a number between zero and one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 434.819,
    "end": 440.546,
    "text": "Um, but we're estimating that at 1 PM, 2 PM, 3 PM, 4 PM.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 441.748,
    "end": 442.088,
    "text": "Right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 442.108,
    "end": 457.768,
    "text": "So, and then, um, in the continuous time setting, we only are keeping an explicit prediction of the hidden state at the current moments X.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 459.183,
    "end": 471.035,
    "text": " And then we're dealing with the past and the future with a Taylor series expansion, also known as the generalized coordinates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 472.016,
    "end": 487.992,
    "text": "So here, whereas B plays the role of a Markov transition matrix in the discrete time model and has a kind of straightforward interpretation where you have the hidden state at a given time, you multiply it by the B, and then you basically transition into the next state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 489.035,
    "end": 516.443,
    "text": " right the analogous role in the continuous time setting is a temporal derivative right and so we're extending out a Taylor series which technically goes forever usually the accuracy of a Taylor series expansion begins to drop off quite rapidly unless it's a repetitive signal um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 517.587,
    "end": 526.258,
    "text": " So then in the continuous time setting, I guess you could have a discrete state space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 527.62,
    "end": 530.723,
    "text": "Like, you know, the light switch is either on or off.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 532.065,
    "end": 538.313,
    "text": "And we're going to have a model that's continuous in time of being on or off.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 540.301,
    "end": 565.65,
    "text": " that doesn't mean it's the best way but it's definitely a good point that you're raising which is that the treatment of time has a um very strong bearing on the semantics of the generative model whereas whether a given variable like an observation or a hidden state",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 567.638,
    "end": 595.705,
    "text": " is itself continuous or discrete is a little bit of a smaller difference this this figure 4.3 okay yeah um um yeah a question to continue sorry one chance are you are you done",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 596.41,
    "end": 597.492,
    "text": " No, no, no, go ahead.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 597.572,
    "end": 597.972,
    "text": "Go ahead.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 598.593,
    "end": 599.074,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 599.094,
    "end": 600.616,
    "text": "Oh yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 600.636,
    "end": 614.336,
    "text": "So, uh, to go on on this, uh, in the discrete time, um, in the implementation, uh, within each step, there is some kind of continuous updating of the neural activity associated with all the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 616.399,
    "end": 619.684,
    "text": "Um, so.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 622.145,
    "end": 651.983,
    "text": " uh you know how does that uh go in this picture yeah this is also a very good question like the the this kind of interpolative yeah continuous dynamical component that seems to be extraneous or",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 653.87,
    "end": 683.117,
    "text": " it's unclear if it's purely interpolative, like if a calculation is being done here, here, here, and here, and then it just being like kind of like spline fit, or rather if there is a continuous time differential equation or something, but then if there was a exact concordance,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 683.722,
    "end": 713.104,
    "text": " of the discrete time and some differential equation solving underneath why wouldn't we just focus on that concordance i don't know if this is accessory code inside of spm uh i mean that's linked to the to the message passing within each step",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 716.189,
    "end": 723.826,
    "text": " So within each step, you know, if you have, for example, I mean, usually they use 16 steps to implement the message passing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 724.868,
    "end": 736.273,
    "text": "And you will have this differential equation implemented as, you know, 16 different updates in a discretization of...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 737.941,
    "end": 765.703,
    "text": " okay so then would we say that this is truly a continuous time interpolation or is it just that we're doing like 16 with we're treating each time step in the discrete model as like an Epoch and then having 16 kind of adjustment steps within the cheaper yeah the later is my understanding yeah so it could be",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 766.915,
    "end": 791.775,
    "text": " so that they're well neurobiologically they analogize this to the local field potentials which is like a EEG recording but just statistically the rate of change of log expectations so using message passing they can continue to run message passing",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 793.139,
    "end": 799.849,
    "text": " Okay, well, let's assume the trivial case, which is the variable is already exactly update.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 799.909,
    "end": 802.352,
    "text": "It's already at its stationary state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 803.514,
    "end": 805.737,
    "text": "So new irrelevant information comes in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 808.702,
    "end": 813.488,
    "text": "Then whether you message passed one time or a million times, it wouldn't really matter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 814.47,
    "end": 814.59,
    "text": "Right."
  },
  {
    "start": 815.667,
    "end": 844.003,
    "text": " contrast there might be a situation which is going to be more prevalent where there is some amount of bayesian updating that needs to occur um the whole point of using free energy is that we're able to approach problems that we can't do one step exact bays on we have something that's incrementally optimizable",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 844.321,
    "end": 846.944,
    "text": " through message passing with free energy minimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 847.926,
    "end": 853.953,
    "text": "So then how many rounds of message passing do you have to do?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 855.435,
    "end": 869.873,
    "text": "Well, one approach is when the rate of change of free energy or the rate of change of log expectations is low,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 872.215,
    "end": 892.014,
    "text": " Um, or sometimes I've seen when like the overall Delta is small, like just when round after round, we're getting small updates, but, but both of those ha have a degree of freedom because maybe you could continue in that slow regime for a long time, just like any, uh, model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 897.799,
    "end": 900.822,
    "text": "So they're doing multiple message passing rounds.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 907.653,
    "end": 924.919,
    "text": " have you run the code to to reproduce 7.13 uh not this one no okay yeah I think that would be helpful to see because other even still like does it message pass up to here and then it goes down to there and is that 0.2 or is two",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 927.65,
    "end": 950.232,
    "text": " well I would I would say that for each I mean each step is composed of three steps here uh and for each small step you have this uh you know 16 by default uh message passing did they say the 16 in the text or is that from the paper",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 951.173,
    "end": 971.157,
    "text": " uh well that's uh the default implementation with spm uh okay about the text here yeah sorry okay yes interesting I I think also this is a uh a place to look at the rx infer um toolkit",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 972.419,
    "end": 978.565,
    "text": " as well as the reactive message passing variant.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 979.706,
    "end": 990.657,
    "text": "So in the SPM-based message passing, every message, basically one round is advanced.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 990.837,
    "end": 994.521,
    "text": "Every single node, messages are propagated across the entire network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 996.222,
    "end": 999.085,
    "text": "In the reactive message passing setting,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 999.065,
    "end": 1012.641,
    "text": " it opens the door to like a just-in-time type operation where different nodes can be receiving messages at different rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1014.043,
    "end": 1020.25,
    "text": "In fact, later today, we have live stream 55.0 on message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1020.41,
    "end": 1025.015,
    "text": "And then in the coming two weeks with Magnus Kudel on message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1026.838,
    "end": 1028.059,
    "text": "But it makes me wonder like,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1033.152,
    "end": 1041.048,
    "text": " Is there still message passing in the continuous time model?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1043.553,
    "end": 1052.41,
    "text": "I'm tempted to say yes, any Bayes graph, since any Bayes graph has a message passing dual.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1054.617,
    "end": 1080.642,
    "text": " then whatever the semantics of the base graph are whether it's describing markov transition probabilities or whether it's describing a temporal derivative for a taylor series expansion you still could construct the message passing to do that um i i i'm actually even gonna",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1082.191,
    "end": 1111.573,
    "text": " bring in this one slide from message passing just because it uh okay three kinds of graphs i and interestingly all three of these kinds of graphs come into play in the textbook but i think in this 2017 paper they they lay it out most clearly bayesian graphs",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1112.026,
    "end": 1135.165,
    "text": " nodes are variables edges are dependencies figure 4.3 base graph 40 factor graph nodes are functions of a distribution over variables edges represent variables per se",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1139.144,
    "end": 1140.005,
    "text": " So here's 4.3.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1141.126,
    "end": 1143.148,
    "text": "Well, here's figure 7.3, same thing, 4.3.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1144.95,
    "end": 1146.212,
    "text": "So these two are equivalent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1147.914,
    "end": 1153.2,
    "text": "Now this, which is to say 4.3, is equivalent to this Forney factor graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1159.527,
    "end": 1164.873,
    "text": "And then the third kind of graph is the neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1166.995,
    "end": 1168.797,
    "text": "Nodes are sufficient statistics",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1171.882,
    "end": 1174.385,
    "text": " edges are exchanges of sufficient statistics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1178.489,
    "end": 1179.47,
    "text": "So it's kind of interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1179.991,
    "end": 1187.238,
    "text": "On one hand, we shouldn't be super surprised because the graph is obviously an extremely general type of data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1188.66,
    "end": 1196.147,
    "text": "So the idea that there would be like multiple kinds of graphs, it's not too surprising.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1198.19,
    "end": 1201.433,
    "text": "Can you give an example of the neural networks graph?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1202.83,
    "end": 1210.301,
    "text": " Which one is the one we just showed on page 26?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1210.742,
    "end": 1216.951,
    "text": "On page 26, you have Bayesian graph and the factor graph, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1218.934,
    "end": 1219.255,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1219.275,
    "end": 1221.478,
    "text": "So here on the top are the Bayes graphs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1224.242,
    "end": 1227.047,
    "text": "So these two are literally identical.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1227.147,
    "end": 1228.669,
    "text": "This is from the textbook.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1228.818,
    "end": 1229.9,
    "text": " Hashtag group.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1230.38,
    "end": 1235.589,
    "text": "And this is from the graphical brain paper, which I'll just put into the chat just in case you want to see.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1237.131,
    "end": 1244.903,
    "text": "Then every Bayes graph, any Bayesian network can be expressed as a factor graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1245.777,
    "end": 1252.624,
    "text": " So basically, they contain the same necessary and sufficient information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1252.984,
    "end": 1262.974,
    "text": "However, there are operations and procedures that can be done on a factor graph that cannot be done on the Bayesian graph representation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1263.935,
    "end": 1274.265,
    "text": "So under the hood, which is, I think, something to investigate in SPM and Python and Julia, when we specify our generative model as a Bayes graph,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1275.224,
    "end": 1278.527,
    "text": " And then we use standardized routines for message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1279.508,
    "end": 1286.035,
    "text": "It's very interesting to ask, well, what's happening under the hood that converts that base graph into a factor graph?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1287.717,
    "end": 1301.25,
    "text": "The advantage of the factor graph, key advantage, is that as these numbers suggest, you can create node local procedural logistics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1302.591,
    "end": 1305.214,
    "text": "So you can develop per node,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1306.595,
    "end": 1322.022,
    "text": " an order of operations that calculates that node, which allows you to develop a procedure to accurately update the entire graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1323.183,
    "end": 1332.194,
    "text": " Whereas if this base graph is presented, it's kind of like, well, should we calculate this part first or should we calculate that part first?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1332.315,
    "end": 1333.216,
    "text": "Is it even going to matter?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1333.256,
    "end": 1342.688,
    "text": "Okay, now the third kind of graph, the neural network or circuit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1346.353,
    "end": 1353.021,
    "text": "Again, this is, I think, worthwhile to go into because I think when they're talking about a neural network,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1354.418,
    "end": 1375.397,
    "text": " this could this could be wrong but um these are the kind of yeah let's let's look in graphical brain just to see if they give an example the neural network yeah yeah because i'm not sure if they're okay yeah okay we'll get okay here we go",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1377.301,
    "end": 1378.743,
    "text": " Here's a neural network graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1378.763,
    "end": 1389.257,
    "text": "So I don't know if this includes standard slash mainstream neural network architecture, like ConvNet type stuff.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1389.277,
    "end": 1391.36,
    "text": "I'm not sure if it would include that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1393.002,
    "end": 1406.16,
    "text": "And then also we've seen neural slash cognitive architectural representations described as a base graph, for example,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1408.756,
    "end": 1419.728,
    "text": " in let's say here chapter five well so that's figure five two predictive coding",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1420.872,
    "end": 1445.631,
    "text": " so here they these kinds of things now on one hand I see that I see that in terms of um sufficient statistics of unknown variables and other auxiliary variables like prediction errors that's literally what's happening here the mean is the summary statistic of the central tendency and then the um error residual",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1446.522,
    "end": 1468.952,
    "text": " but I'm not sure if this can be interpreted both as a base graph and a neural graph or if this is one or the other because in either case it's slightly different in its representation one key piece that I think is happening here is there's like multiple at each layer",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1470.232,
    "end": 1491.143,
    "text": " but I'm not sure if that's being if these threes or the two at each are being used to describe three time points kind of as traditional or whether it's alluding to the idea that there's a population of neurons and then the variable is a summary statistic on like a rate code",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1502.159,
    "end": 1503.62,
    "text": " should not be taken too seriously.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1507.684,
    "end": 1520.096,
    "text": "Yeah, and certainly if they are kind of have one to one correspondence among the three graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1520.116,
    "end": 1530.546,
    "text": "Yeah, that would be a great like that would be like the triple play with the three kinds of graphs, whereas they only really show two in connection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1531.724,
    "end": 1533.306,
    "text": " Figure 11.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1539.111,
    "end": 1539.672,
    "text": "Figure 9.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1542.095,
    "end": 1542.415,
    "text": "What?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1548.361,
    "end": 1549.702,
    "text": "But here's what figure 10 looks like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1552.465,
    "end": 1560.313,
    "text": "This looks a lot like figure 5.5.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1561.407,
    "end": 1579.734,
    "text": " We have the prefrontal, the sort of abstraction goal selection, then the descending prediction down into the dopaminergic policy selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1581.597,
    "end": 1588.647,
    "text": "We talked about the trade-off between basically habit-driven policy selection on the left and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1588.778,
    "end": 1594.244,
    "text": " And then expected free energy sharpened policy updating on the right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1595.465,
    "end": 1597.027,
    "text": "That's like the dopaminergic tone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1598.309,
    "end": 1611.383,
    "text": "And then the descending prediction down to the spinal arc with a kind of simple differential based set point predictor, sensory motor actuator.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1611.403,
    "end": 1617.79,
    "text": "This is like literally the same.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1619.525,
    "end": 1623.771,
    "text": " We have that prefrontal stack.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1623.791,
    "end": 1637.392,
    "text": "Here, we have a policy and a G. And then, ultimately, a descent into the spinal arc.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1641.358,
    "end": 1647.287,
    "text": "But what makes this... Question one, what makes this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1650.811,
    "end": 1674.861,
    "text": " different than just saying well it's a base network with a neural semantics I mean don't the nodes as unknown variables already have an interpretation as sufficient statistics question one question two oh yeah go ahead",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1676.309,
    "end": 1703.818,
    "text": " yeah one thing is that's you know when you have category categorical distributions to get the sufficient statistic you need the full distribution uh well you know continuous time I think they always work with um Gaussian distribution you know in which you know having the mean and standard deviation is enough yeah they're they're definitely working commonly with the Gaussian family yeah",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1705.992,
    "end": 1727.528,
    "text": " although in the hierarchical Gaussian setting the hierarchical Gaussian filter has high expressivity so it's not like they can only model simple things no no yeah yeah so how what makes this a neural network",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1728.487,
    "end": 1750.815,
    "text": " are they using neural network like many machine learning researchers would use neural network today sigmoid activation function all that i don't exactly expect so um so how is this different than a bayesian network part one and then are are these models in content could these models be simply",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1756.195,
    "end": 1758.638,
    "text": " It does not include continuous message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1762.002,
    "end": 1766.147,
    "text": "So here they're dealing with categorical and continuous state spaces.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1769.13,
    "end": 1771.933,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1771.953,
    "end": 1782.085,
    "text": "Oh, see, but here these generalized observations describe a trajectory in continuous time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1791.818,
    "end": 1794.641,
    "text": " So here we see the x, x prime, x double prime.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1796.664,
    "end": 1799.827,
    "text": "So this is a lot like- The derivatives over time?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1800.488,
    "end": 1802.17,
    "text": "First order, second order over time?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1802.751,
    "end": 1803.612,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1803.632,
    "end": 1803.732,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1804.673,
    "end": 1807.897,
    "text": "And those are the generalized coordinates of motion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1808.918,
    "end": 1814.865,
    "text": "The position of the car, the velocity of the car, acceleration of the car, and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1814.905,
    "end": 1817.708,
    "text": "And then the more derivatives that you have,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1818.16,
    "end": 1823.185,
    "text": " that's using the Taylor series or the generalized coordinates to expand out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1824.347,
    "end": 1837.181,
    "text": "Livestream 26 on the Bayesian mechanics for stationary processes or something like that, that is very helpful.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1837.201,
    "end": 1840.244,
    "text": "Like, let's just say you had a pendulum just moving back and forth.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1841.365,
    "end": 1846.171,
    "text": "Well, its position is obviously never the same moment to moment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1846.531,
    "end": 1847.492,
    "text": "It's changing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1847.995,
    "end": 1877.142,
    "text": " its velocity is changing moment to moment but you're going to get to a derivative not that high up for a pendulum where that derivative is not changing and that is stationarity on the generalized coordinates or if you had something that was moving around in a circle again the position is changing the velocity would always be changing like the actual vector heading would be different at each moment",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1877.695,
    "end": 1899.535,
    "text": " but the Excel oh it's accelerating a little bit forward and to the left it's going counterclockwise so that would be stationary so that's a huge advantage of the continuous time is like if you can find the articulation where something has a regular",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1901.624,
    "end": 1916.367,
    "text": " mechanics, then the the generalized coordinates, the Taylor series expansion, like, capture it really, really, really concisely.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1918.651,
    "end": 1923.919,
    "text": "Whereas if you were doing a discrete model of that something going around in a circle,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1928.253,
    "end": 1954.092,
    "text": " you you would I don't know maybe struggle to to pull out that simplicity especially yeah I'm thinking when you implement this uh continuous time model in computer you have to discretize the time anyway right yeah this is a great question",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1956.569,
    "end": 1964.624,
    "text": " well, it didn't becomes a discrete times, if it discrete the time, will it become a discrete model?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1965.365,
    "end": 1968.812,
    "text": "Or it's actually fundamentally different from the discrete model?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1975.264,
    "end": 1976.406,
    "text": "You're right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1977.368,
    "end": 1978.189,
    "text": "I would say,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1979.249,
    "end": 1988.017,
    "text": " that still would not be called a discrete time model because discrete time model, the B matrix would be, you know, the Markov transition probabilities.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1988.097,
    "end": 1991.881,
    "text": "Whereas here you're still trying to learn the derivative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1993.322,
    "end": 2006.134,
    "text": "So then you take some approach, like you have some Delta T and then you shrink the Delta T. And if you find that as you're shrinking the Delta T you're converging on your estimate,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2007.599,
    "end": 2016.611,
    "text": " of the derivative, we're studying the line y equals x. And we start with a delta x of 5.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2017.572,
    "end": 2018.573,
    "text": "And we find the slope is 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2018.874,
    "end": 2020.096,
    "text": "And then we do delta x equals 4.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2020.116,
    "end": 2021.838,
    "text": "It's still a slope of 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2022.078,
    "end": 2027.545,
    "text": "So we're converged in our estimate of slope as we shrink the delta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2029.288,
    "end": 2033.353,
    "text": "So then we feel confident in our discretization",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2034.532,
    "end": 2052.111,
    "text": " but we're still in the continuous time setting, but we're just needing to take this digital approximation on the continuous, on estimating the continuous time dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2055.414,
    "end": 2062.041,
    "text": "Lance DaCosta is one of the researchers who has, I think,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2063.185,
    "end": 2071.069,
    "text": " expanded on this some of the most because in in 26",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2073.749,
    "end": 2087.172,
    "text": " Christopher Lustrii, BASI mechanics for stationary processes here's where there was the discussion of the non non stationary steady States stationary non stationary steady States in the generalized coordinate settings also even with three.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2088.233,
    "end": 2098.17,
    "text": "Christopher Lustrii, layers like X X prime X double prime that corresponds to PID control in engineering the something integral derivative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2098.268,
    "end": 2114.867,
    "text": " Um, so this is a very common engineering, um, technique then, um, in 52, this question of the Delta T came into play.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2115.328,
    "end": 2125.98,
    "text": "So like, obviously we want the underlying free energy landscape to be smooth because we want it to be like analytically",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2127.074,
    "end": 2154.921,
    "text": " well behaved we want to be able to take derivatives so we want the free energy landscape to be smooth but we're doing it on digital computers so there has to be some approximation step in in space and time even if we're dealing conceptually with analytical space that is continuous in space continuous in time um so how does that work well",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2156.521,
    "end": 2182.495,
    "text": " in this paper they talk about how do you do inference on that um on the Delta T how do you decide how much that step should be and there's a very evocative um image so we're the ball is rolling to the bottom of the hill we always talk about this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2183.234,
    "end": 2186.899,
    "text": " But we're on a computer simulating the ball rolling to the bottom of the hill.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2187.76,
    "end": 2191.365,
    "text": "So we are going to make these discretizations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2192.246,
    "end": 2197.554,
    "text": "Now, how much time should we simulate?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2197.834,
    "end": 2202.22,
    "text": "Again, we have the equations that say that the ball is going to roll smoothly in time, smoothly in space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2203.502,
    "end": 2204.483,
    "text": "But we need to simulate that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2206.522,
    "end": 2231.01,
    "text": " they talk about this um Delta T question in terms of accelerated optimization like you could choose a Delta T that was so ridiculously small that it basically you know it wasn't moving like it was just super super you were just modifying a parameter by like a billionth um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2231.243,
    "end": 2257.352,
    "text": " conversely you could make Delta T that would lead to these kind of like chaotic sampling because you'd be like you'd be like okay the ball's here and it's headed this way and then you just go okay now let's just put it over there and then now it's like a totally different space and then it's like whoa what is happening now it's going that way so if your jump is too big then you're you're like you're getting kind of like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2258.615,
    "end": 2269.385,
    "text": " I don't know if you'd say you're getting aliased on that space, but if you're taking too large of jumps, you're not actually going to detect the regularities landscape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2270.366,
    "end": 2273.509,
    "text": "You'd be on the side of the bull and you'd overshoot and you'd find yourself way off the curve.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2273.529,
    "end": 2277.373,
    "text": "You'd have to recorrect back to the curve and then you'd find yourself going way down here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2278.234,
    "end": 2286.462,
    "text": "And so then there's this kind of second order inference question about what the delta T should be",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2287.252,
    "end": 2312.871,
    "text": " then they talk about like um that in terms of the ball rolling to the bottom of the hill with different fluid viscosities so if you're like in honey you stay kind of move you never really reach a terminal acceleration velocity versus if you're like in water versus air you like gain more speed",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2315.753,
    "end": 2323.846,
    "text": " And so that leverages the position of the ball on the bull and the vector direction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2325.368,
    "end": 2333.381,
    "text": "But then knowing how far you should go is a higher derivative question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2334.042,
    "end": 2342.115,
    "text": "It's like if the car's going one mile an hour, then there's going to be a delta T that's going to lead to optimal sampling of that car.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2342.416,
    "end": 2343.397,
    "text": " like one second later.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2344.198,
    "end": 2349.062,
    "text": "But then if the car's going a thousand miles an hour, you might want the delta T to be shorter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2358.551,
    "end": 2365.237,
    "text": "Do all of these, like, and I think this, again, returns to the code question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2368.781,
    "end": 2371.343,
    "text": "Do we just like swap out?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2372.47,
    "end": 2394.478,
    "text": " continuous time if we have a model with a given state space how hard is it to go from a continuous to a discrete time implementation like is it could we just flag it up at the top and just say we're going to do both then we're just going to see what they look like or you know",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2395.218,
    "end": 2398.141,
    "text": " Is it a flag or a setting that can just be called?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2399.302,
    "end": 2407.91,
    "text": "Or does it require more of a structural custom creation for one or the other such that it's a little bit harder to repurpose?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2412.255,
    "end": 2412.715,
    "text": "I don't know.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2413.075,
    "end": 2415.858,
    "text": "In fact, I've done very little with the continuous time models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2419.942,
    "end": 2423.065,
    "text": "But the Rx and Fur... The Rx and Fur...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2425.103,
    "end": 2437.323,
    "text": " it certainly is fine with continuous.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2440.769,
    "end": 2445.897,
    "text": "So this is the package for continuous time models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2446.585,
    "end": 2453.615,
    "text": " Um, it's actually a, a package for, um, like message passing and basic graphs generally.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2453.956,
    "end": 2457.801,
    "text": "So you can make, you don't need, this isn't even an active inference specific package.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2458.923,
    "end": 2467.515,
    "text": "And so I think some of the reasons why pyMDP gets more use, at least today, first off more people know Python than Julia.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2468.697,
    "end": 2475.006,
    "text": "Second off pyMDP like SPM has multiple methods.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2475.458,
    "end": 2501.483,
    "text": " that are specifically relevant for doing active inference generative models like there's a class to define the the pomdp agent so that makes it a lot easier to define whereas here here's you can do active inference but um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2504.703,
    "end": 2534.305,
    "text": " you have to write it from scratch basically but so it's all there because if you if you can frame it as a bayesian graph and we know that we can then rx infer is a general base graph implementation approach which allows you to specify the base graph",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2535.601,
    "end": 2548.428,
    "text": " And then under the hood, I believe flips it into the message passing format and actually does the inference with message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2551.935,
    "end": 2554.42,
    "text": "But these are, these are very.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2556.172,
    "end": 2568.925,
    "text": " So yeah, or just saying I'm, I'm not a Julia expert either, but like some features that are presented already in RX and fur are very promising.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2576.433,
    "end": 2578.615,
    "text": "Let's see if there's continuous time specifically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2579.116,
    "end": 2579.316,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2579.456,
    "end": 2579.696,
    "text": "Good.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2580.577,
    "end": 2583.36,
    "text": "Try not to SPM the discrete case act.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2584.1,
    "end": 2604.741,
    "text": " try it out it's it's quite nice using the spm package oh yeah totally i haven't tried any other packages yeah i think this could be a good",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2606.257,
    "end": 2607.62,
    "text": " I mean, birth of rise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2607.7,
    "end": 2613.533,
    "text": "And again, we're having a live stream in the coming weeks with the authors of this package.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2613.593,
    "end": 2616.058,
    "text": "And so we can ask these kinds of questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2617.381,
    "end": 2623.033,
    "text": "But just, I mean, let's like,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2625.46,
    "end": 2639.477,
    "text": " This textbook, the 2022 textbook that we're ostensibly discussing is kind of the MATLAB SPM era and approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2641.019,
    "end": 2641.54,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2643.382,
    "end": 2652.413,
    "text": "PyMDP represents a sort of partially first principles approach, partially ported over from SPM approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2652.764,
    "end": 2656.188,
    "text": " and they've focused their development on discrete time modeling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2658.17,
    "end": 2660.152,
    "text": "What is the name again?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2660.212,
    "end": 2660.773,
    "text": "PyMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2662.134,
    "end": 2662.695,
    "text": "PyMDP.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2663.235,
    "end": 2663.476,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2664.677,
    "end": 2667.32,
    "text": "Inferactively, the PyMDP package.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2668.802,
    "end": 2679.193,
    "text": "Whereas RxInfer set out to solve a significantly more general problem of just doing Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2680.017,
    "end": 2686.127,
    "text": " But using PyMDP to construct arbitrary Bayesian graphs is probably not the best package for that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2686.948,
    "end": 2691.856,
    "text": "But again, it has all these helpful methods for making POMDPs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2694.48,
    "end": 2702.312,
    "text": "Movement is a classic case of continuous time modeling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2703.473,
    "end": 2731.388,
    "text": " the analog real-time continuous nature of proprioception and actuation so sensory motor control this is an absolutely classic setting for continuous time modeling by thinking about what that continuous time model is in the game of doing not as",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2731.958,
    "end": 2753.747,
    "text": " maximizing utility or reward function per se but reducing divergence to a set point and finding paths or trajectories in a space towards a set point already this foreshadows this kind of like sensory motor continuous layer",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2754.098,
    "end": 2775.482,
    "text": " then a higher order discrete now it could be discrete time but at the very least discrete state space with like selection of different goals so you know to oversimplify the brain is send is is um setting discrete preferred states for the body to be in",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2775.833,
    "end": 2796.357,
    "text": " then the body uses its um basal intelligence to realize paths navigate paths to that set point there is a discussion i think we may have talked last week about this some about the sensory attenuation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2797.4,
    "end": 2820.302,
    "text": " about the precision Dynamics we talked about like getting up out of a chair we talked about moving with the eyes the eye Cicade and about how as during the execution of a movement you're still getting sensory feedback and so if you expect the consequences of a movement like when I when I um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2820.468,
    "end": 2825.494,
    "text": " move my eyes or move my head, I'm expecting all of the visual input to change a certain way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2825.534,
    "end": 2830.52,
    "text": "When I get out of a chair, I'm expecting my touch receptors to feel a certain way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2831.442,
    "end": 2837.809,
    "text": "Those expectations are calibrated out so that they do not rise to our attention.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2840.373,
    "end": 2847.201,
    "text": "And so that can be understood in terms of a transient suppression of attention, which is attenuation,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2847.535,
    "end": 2875.315,
    "text": " then once the body is fixed again or the gaze is fixed again there's a re-engagement of precision so you know pay attention to when things outside of your control are happening but calibrate out the expected sensory consequences of action and the easiest way if you know that there's going to be like um you know if you know that you're going to drop something loud",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2876.408,
    "end": 2903.192,
    "text": " just turn off your hearing for a split second while you drop something loud instead of engaging in in some more sophisticated like noise cancellation method they then move to an ecological analogy with a generalized lock of Volterra dynamics this is also called winterless competition because like plants are before carnivores like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2904.421,
    "end": 2928.859,
    "text": " none of them can like win they're all in this oscillatory relationship in this three-dimensional space and that's also been used to model winnerless neural dynamics like if you have two brain regions that have a mutual inhibitory relationship like the one that's on top inhibits the other one um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2930.375,
    "end": 2934.902,
    "text": " until the repression switch happens.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2935.503,
    "end": 2938.207,
    "text": "So that's explicitly brought into this winnerless competition.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2938.327,
    "end": 2947.181,
    "text": "Locke and Volterra was some of the earlier models of sequential actions in active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2947.201,
    "end": 2952.91,
    "text": "And this Thomas Parr in the book stream 2.1, I think, he talked about this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2953.07,
    "end": 2958.458,
    "text": "So like there was sort of the continuous time, this was like 2008, 2012, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2958.995,
    "end": 2969.235,
    "text": " the continuous time models were great with like real time flow, but doing like a, so it was all good for, you know, the arm, close the arm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2969.315,
    "end": 2970.858,
    "text": "Okay, now open the arm, now close the arm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2971.66,
    "end": 2979.315,
    "text": "But to do a multi-stage action, how do you get that into continuous time?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2979.38,
    "end": 3001.793,
    "text": " so the first approach that was shown here was um like saying how do you get a multi-stage action where first I want the plant population to drop then I want this carnivore population to go up or something so there are ways to tune the continuous relationships so that you can get um a kind of canonical unrolling",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3003.342,
    "end": 3030.632,
    "text": " in response to a certain stimuli or endogenously generated which is what they did in the handwriting case however as thomas pointed to people wanted to account for explicit planning because even though the models tuned up to do this kind of repetitive scribble there was no explicit planning and so that motivated the development of the discrete time formalism",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3030.848,
    "end": 3035.393,
    "text": " and the POMDP formalism, which had been widely used for planning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3037.155,
    "end": 3055.154,
    "text": "And now we're kind of merging back those streams with continuous and discrete times, especially in light of the recent advances in Bayesian mechanics with path integrals, g-theory, these kinds of topics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3059.038,
    "end": 3060.72,
    "text": "Learning in continuous models,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3064.43,
    "end": 3084.278,
    "text": " generalized synchrony now generalized synchrony you could also think about this in a discrete time setting but it works really nicely in a continuous time setting too this was um this specifically coupled Lorenz system",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3085.979,
    "end": 3113.218,
    "text": " explored more in livestream 34 on the stochastic chaos and Markov blankets turns out that you can get generalized synchrony between um two chaotic systems between two non-chaotic systems and so on so you can get entrainment and generalized synchrony even when there's an analytical chaotic relationship",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3116.337,
    "end": 3142.918,
    "text": " is it hard to fit chaotic processes in real life yes probably but in the example of the two birds that are like singing where their turn taking is described by like it's like you're like spiraling on one side of the lens attractor and then there's like a flip into the other side of the phase attractor that describes the turn taking and the generalized synchrony well",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3145.649,
    "end": 3171.91,
    "text": " and combining the generalized synchrony plus learning with the two birds we can look at how like when we have a learning if the models didn't have learning obviously they would just continue to act along wherever however they were in the beginning but with learning they can come to have higher mutual information on each other closer to this like y equals x line",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3172.767,
    "end": 3180.415,
    "text": " So that represents the tightening of the generalized synchrony, which doesn't mean lockstep.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3180.795,
    "end": 3182.297,
    "text": "It doesn't mean that they're singing at the same time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3182.477,
    "end": 3186.902,
    "text": "It actually means that their turn-taking is sharper in its alternation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3190.285,
    "end": 3198.994,
    "text": "Then in the close, missing line here, in the close of the chapter, they introduce the kind of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3200.189,
    "end": 3228.306,
    "text": " folk psychological model that gets explored in live stream 46 that we discussed earlier where we have a continuous time sensory motor actuation as a leaf in a higher order discrete time model then there are these just um examples of isocating with mixed or hybrid models",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3229.417,
    "end": 3237.306,
    "text": " where we have a discrete time goal selection, and then the continuous time cicade.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3240.17,
    "end": 3250.322,
    "text": "Gaussian mixture model, big topic here about iterative clustering, kind of like k-means clustering.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3252.684,
    "end": 3259.272,
    "text": "And then they review 10 or 12 continuous time cycles.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3260.602,
    "end": 3261.183,
    "text": " settings.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3267.31,
    "end": 3285.63,
    "text": "So a very interesting one, certainly one that we could benefit a lot from having, like the same setting described with continuous and discrete time to get a better handle on",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3288.277,
    "end": 3295.203,
    "text": " whether we, do we make the state space and then do we add the time layer and we can do it both?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3295.883,
    "end": 3298.165,
    "text": "Is there any downside to making models that way?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3298.205,
    "end": 3316.861,
    "text": "Or if we think back to chapter six and the recipe, is the decision that we make about continuous or discrete time something that happens very early and in a more committal way?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3317.442,
    "end": 3317.862,
    "text": "I don't know.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3319.512,
    "end": 3325.944,
    "text": " And the way that the packages are today may not be how packages are forever.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3340.291,
    "end": 3343.056,
    "text": "So then I have one question regarding the",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3343.205,
    "end": 3352.758,
    "text": " the example given in chapter eight, this example, like all the figures, they are simulated by computers, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3352.778,
    "end": 3363.492,
    "text": "They are not the data connect from experiment or from the nature, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3363.512,
    "end": 3363.953,
    "text": "Correct.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3364.173,
    "end": 3371.423,
    "text": "In the examples here, the generative model was specified and then synthetic data were generated.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3371.588,
    "end": 3396.027,
    "text": " so that's kind of like the forward direction and then chapter nine is where we get into the discussion of okay now what if we have empirical data and we want to parameterize the generative model but here they just set the generative model and then synthesized data however they obviously set up the generative model to anticipate the kind of data that they were collecting in the lab",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3397.441,
    "end": 3399.403,
    "text": " So this is the forward direction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3400.264,
    "end": 3409.715,
    "text": "And then chapter nine is when we go into model-based data analysis with going from empirical data to parameterized generative models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3414.401,
    "end": 3416.163,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3416.363,
    "end": 3416.463,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3416.503,
    "end": 3417.304,
    "text": "Thank you, Daniel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3417.645,
    "end": 3417.905,
    "text": "Thanks.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3417.925,
    "end": 3418.325,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3418.486,
    "end": 3419.046,
    "text": "Thank you, Esmail.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3419.066,
    "end": 3419.687,
    "text": "Thank you, Anshan.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3419.827,
    "end": 3420.148,
    "text": "Farewell.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3420.248,
    "end": 3420.448,
    "text": "Bye.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3421.87,
    "end": 3422.15,
    "text": "Thanks, Lloyd.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3422.17,
    "end": 3422.25,
    "text": "Bye."
  }
]
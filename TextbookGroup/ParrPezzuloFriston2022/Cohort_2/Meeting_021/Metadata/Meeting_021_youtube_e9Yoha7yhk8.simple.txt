SPEAKER_01:
Hello, it's March 22nd, 2023.

We're in cohort two in our second discussion of chapter nine.

We're going to jump right in to anything that people are thinking about related to chapter nine or just in general.

So whomever would like to bring something up, go for it.


SPEAKER_03:
Yeah.

So I have a question that you said last time, I don't know whether it was in what context it exactly was, that you would say we talked about or you talked about affordances and you said that policies are not affordances.

And then I was wondering what then affordances are, because I understood policies are not affordances because they are in the future.

But then I was thinking what that should mean.

Because the present is a point in time infinitely small, which would imply that affordances doesn't exist.

Or how would you see this?

So if one thinks that an affordance is if we are in one state, and the policy is a sequence of states in the future,

But then even if we have this embedded state, I can be in the state of learning, for instance, active inference, which also implies some actions in the future.

So I was a little bit confused about this notion and why you would think that policies are not affordances.

Maybe I misunderstood.


SPEAKER_01:
Thanks.

Olli, want to give a first thought?

And then I'll be happy to add some.


SPEAKER_02:
Well, yeah, actually, well, in fact, in active inference literature, there are some divergence between the way terms used in this context as compared to their other connotations or other meanings in

some other contexts such as reinforcement learning and so on.

So basically, in active inference by policy, they generally mean a sequence of actions

But, for instance, in our learning policy is defined as just a single action and affordance again is kind of problematic term because we have.

different notions of affordance for example in ecological psychology or in some other areas but to the best of my understanding in active inference affordance refers simply to a single action in some cases

But in some other cases, it can take on some other additional layers of meaning.

So for example, we can affordances or pragmatic affordances and they somehow conflated with each other and they can be confusing to delineate which

notion of affordance is referred to here in this context.

So yeah, those are pretty much the basic or at least some of the points that can help delineate those two concepts, affordance and policy.

But I'm sure Daniel would have some other thoughts on that as well.


SPEAKER_01:
Awesome.

Yes.

lot of good points so big picture there are divergences between how the terms are used in Act-Inf and in reinforcement ecological psychology we hope and basically believe that there's coherence in how the terms are used but that doesn't guarantee coherence within and across areas which themselves are not necessarily internally or externally consistent

so policies are sequences of actions over a given time horizon

affordances are the instantaneous single action possibilities described by the e variable so map not territory if you're in a maze and you can be going up down left right then those are your affordances and over a time horizon of four you could have policies of a time horizon of four with all the combinatorial possibilities of up down left right to the fourth power um

Then there are those sometimes informally applied statements like a pragmatic affordance or an epistemic affordance, which is like when action is being taken that is oriented around epistemic value, but this is kind of like a modifier.

Like it's a risky affordance or it's a fun affordance.

not necessarily like a special subtype in a in a formal way um and it's described by the e variable you could have a nested generative model so like live stream 42 with a slam simultaneous localization and mapping at the lower level the robot has up down left right affordances at the higher level

the policy selection, the affordances have to do with locations moving amongst locations.

And so you can have nested affordances because you can have nested models.

That's not any controversial or hot take on the system itself,

it's just describing the model and then what we can look into more and what Paolo is researching and some others is explicitly in contrast with the ecological psychology concept of the gibsonian affordance

use what was also called like affordance 1.0 2.0 and then maxwell ramstead taking the perspective that active inference affordances are affordances 3.0 the gibsonian advocates emphasize that affordances are directly perceived action capacities

So for them, indeed, it is not making sense that there could be affordances in the future, but people have also considered planning from an ecological psychology perspective.

But their emphasis is on the direct perception of action possibilities, which coming from the E-vector perspective, it's kind of like take it or leave it.

The E-vector might be

known in a metacognitive way might be directly perceived or it might not sorry what is an e-vector the e-vector is with the capacities for action

here in step-by-step the E vector is the habit these are the actions that can be taken are enumerated and the values in the cells reflect the prior on action we can see that in the textbook in

the figures for chapter five so here we have a dopamine mediated so neuromodulator mediated balance

between two modes of policy selection here in some dopaminergic regions in the mammalian brain.

So in both cases, observations are coming in.

In the indirect pathway, the E vector is basically passed along, and the policy posterior, which could be sampled from neutrally,

or you could have a shaky hand and erase differences in the policy posterior or you could have a precise hand and always select the maximum element maximum posterior action probability in the indirect pathway the habit vector is basically just passed along so action selection resembles action prior whereas in the

expected free energy deliberative type 2 archetype of action selection there's this sharpening or updating of the action prior into a policy posterior that's been reweighted by expected free energy

This is highly schematic, and not all the variables are shown in many cases.

It's just schema.


SPEAKER_03:
So habits is something bottom up.

Oh, sorry.

And the g vector is something top down.

So that's planned.

And the other one is routine or something.


SPEAKER_01:
Yes.

Yes.

As always, these are aspects of generative models, not of organisms.

So it's hard because we want to be referring to the habits and the deliberation patterns that cognitive systems do.

also be aware that like e and g are phenomena of our model they're not phenomenas of the world but that being said the role of e is to through its membership or composition enumerate what instantaneous actions are possible if there's up down left right then the e vector is going to have four elements if there's nine directions it can move then e has a length of nine

the values within the e vector are the prior on those actions being selected in an instance so if there is a uniform e vector across up down left right one one one one then there's no prior asymmetry towards taking any of those um

actions whereas if the e vector was 10 1 1 1 then that is equivalent to saying like that um in the absence of further expected free energy updates that action prior will be recapitulated in future action and so you can imagine a model where e is updatable or not

Like a simple updating model for E would be every time you take an action, drop a pebble into that urn.

So that's the learning as counting strategy, which works for many things so that the action prior comes to embody previous action posteriors.

That's habit.

And so what initially may have required a deliberative expected free energy expenditure becomes canalized or entrenched in E, allowing, for example, skill or skillful performance to occur without the deliberative high dopamine

setting in figure 5.4 here the dopamine variable is gamma and yes two of the places to look for further discussion on that some of it is the ontology

project where with paulo and others were talking about like

Ramstad and Camaro and all of these different perspectives on affordance and representation.

And also there's been an interesting discussion about like similarities and differences with the continuous and the discrete time model.

So the affordance is 3.0 discussion.

And then the other angle for critical perspectives on the active strategies and counter rebuttals is the Markov blanket trick


SPEAKER_02:
I just wanted to point to a kind of possible point of a point of possible confusion that can arise when dealing with these ABC

sorry, DE matrices and vectors, because sometimes it's difficult to remember which matrix or which vector encodes what.

So one simple mnemonic that I found helpful in remembering those notational conventions is

As we go from A to E, we move from instantaneous observations into our prior and learned and habit beliefs about the world.

For instance, A encodes our beliefs about the relationship between

the hidden state and the observable outcomes.

B is about the likelihood of the transition.

C encodes our preferences.

D encodes our beliefs about initial hidden states.

And finally, E encodes our habits.

So yeah, I think that might help to remember those terms.

I don't know.


SPEAKER_03:
And G is some kind of metaphysical expectations or something like this?

Or what is G then?


SPEAKER_01:
Well, it's all metaphysical.

They're all informational because they're all variables.

Although one can take the Chris Fields perspective, information as physics... I expected free energy, right?

Yeah, yeah, yeah.

But that being said, we continue down the line with F as variational free energy and G as expected free energy.

However, these are all symbols.

And there's sort of a common...

set of symbols in some sense but but they're by no means necessarily these letters it kind of goes without saying but it definitely can be a point of confusion but um let's look at our favorite step by step pull in the image for the 100th time

Then we have S and O. O, observation.

S, hidden state.

D, prior on hidden states.

A, the emission recognition matrix between S and O. This is the tale of two densities.

Because A can go from S to O,

emitting in the generative direction plausible observations from hidden states or a can be used in its recognition capacity to update s based upon incoming observations so in the upper left static perception case all you need to appeal to is a b or i'm sorry a and d s and o

then dynamic perception it's still passive but it's changing through time and what changes the hidden state through time is b the transition matrix on hidden states to enable policy selection at least two other things have to come into play well at least one but really two the one that must come into play is simply the enumeration of action capacities which is

listed in the membership of e and the cells contain quantitative information on the prior probability of those actions being selected aka habit you could have e and just be taking actions however what is it that steers the ship on which actions are selected

Well, the way we do that in Active Inference is not by proposing a utility function and maximizing utility function and a reward function, but to encode through C our preferences, what we expect slash prefer.

We talk about them as preferences because they play a functional role

teleologically as preferences in the sense that pragmatic behavior reduces the divergence between observations and preferences.

So in the sense that they are preferences, they are that.

But what C does is it's like a thumb on the scale of observations, not by changing anything downstairs with a tail of two densities,

but rather through expected free energy, selecting policies that are expected to reduce the divergence between observations and expectations.

And by way of reducing divergences between observations and expectations about observations,

we acquire pragmatic value oriented towards C, in the extreme case, fully realizing C, exactly having a homeostatic temperature, exactly having how much money we want in the bank, whatever it is.

And so functionally, C is called preferences.

And then there's this little extra piece on the right

that's also reflected in that dopaminergic image with gamma as a temperature gamma and beta as one over each other and those um gamma being a parameterization of a big gamma distribution

And then be one over each other.

So whether you choose to think about it as like temperature or inverse temperature, that is a precision on policy selection.

And then last thought on this.

This is not the only active inference model.

You could have a precision on anything.

You could have a precision on D. You could have a precision on A. You could have a precision on B.

So these are like some birds I've seen.

This is not the overall exhaustive zoo of all possible birds or the one possible bird.

These are like certain configurations of base graphs, which we can use a controlled ontology to describe so that we can apply them in cybernetic contexts.

For example, designing ecosystems of shared intelligence.


SPEAKER_02:
I also believe beta is technically called stochasticity.

So in some literature, they explicitly use this term stochasticity to refer to that beta parameter, which sometimes for the sake of simplicity is equated to one.

in order to not complicate the distributions, especially the Gaussian distribution.

But sometimes it can be an arbitrary value of stochasticity.


SPEAKER_01:
Yes, thank you.

And when we see the learning, figure 7.10,

here learning in a bayesian setting can be seen as as having a prior on that variable so if we have a fixed parameter um d then it's just like a um like a dirac delta

hashtag category theory, but it's like one point that's just fixed and it's a fixed value.

There's two states and it's one, zero.

So just, it is the first state, or you could imagine the instantiation of that prior parameterization to be drawn from another distribution.

And then you could imagine learning or updating within that setting.

But if you have just a fixed parameter, then it can't be updated.

But if you have a fixed parameter or associated with a prior, then you've opened the door to learning and invoked all of these other

of bayesian considerations like making sure that your prior is not simply being recapitulated in the output so like testing for different priors and doing all these different things for standards rigorous bayesian statistics michael


SPEAKER_03:
Yeah, so in this graph that you just showed, an affordance would then be the relation between S and O?


SPEAKER_01:
No.

S and O, hidden states and observations.

So A is the tail of two densities.

Here's the temperature of the room, S, and the thermometer, O. So the recognition density is what's the temperature...

given the reading and then the generative density direction is from the temperature what's the thermometer so the downstairs is all passive inference downstairs passive inference here's our prior on the temperature in the room temperature in the room changing through time we're not doing anything about it and at every time step it's emitting an observation

Where action comes in is in the upstairs with E, the actions that we can take and associated habituality around them.

And then as to do more than simply recapitulate habit, we have preferences, which have a semantics of being expectations about observations.

i expect myself to be homeostatic and i'm going to take actions that make that realized so reward learning paradigm i'm rewarded by having homeostatic temperature and i follow policies that are rewarding active inference self-evidencing paradigm i expect to be homeostatic and so i take action selection

to reduce divergences between the observations that I do get and what I expect slash prefer.


SPEAKER_03:
So can the notion of affordances then be explained with these graphs that you showed?

Is it a vertical slide through this architecture?

I mean, it has to do with the O and with preferences.

That's what you just said, right?


SPEAKER_01:
The actions are taken.

I mean, in a way, the actions are taken.

in the adaptive case in service pragmatically of reducing the divergence between c and o that is pragmatic value is reducing that and that's with the kl divergence we're reducing the divergence between c and o that's pragmatic value epistemic value is

reducing our uncertainty about different things.

Where are the affordances?

Can it be seen as like a slice in time in this graph?

So the affordances are enumerated in the E variable.

And in every single time step, they are going to be crunched by G. And then B, let's just say that there are two affordances.

We can turn the heater on or we can have the heater off.

B is going to have two slices.

B is like a tensor.

It can be any, the dimensionality is really important and looking at the notebooks and the code around these things is some of the best way to get intuition for this.

But basically B has a slice for every affordance.

So there's like a B, like a transition matrix for the heater being off.

And then there's a transition matrix for the heater being on.

So like, let's just say that in our hidden state, there's three temperatures, cold, medium, and hot.

So again, map, not territory.

We're not saying that there isn't a quantitative temperature in the room.

It just, we're choosing to model the hidden state with three discrete states.

And so if the heater is off, the B matrix, let's just say, looks like the identity matrix.

So if the heater is off,

the room's temperature does not change.

If the heater is on, we bump from cold to medium, medium to hot, and we stay in hot.

And so then the ones would be like in different locations on that slice of B. What do you think about that?


SPEAKER_03:
Yeah, so trying to digest.

So you're saying the affordances are encoded in the transitions between states.


SPEAKER_01:
The affordances are most plainly listed in the E variable.

However, every affordance is also represented with a different transitions, different transition slice in the world.

If there was just one transition matrix, then our action wouldn't do anything because the world is just going to change the way it's going to change no matter what we select.

So then all policies would be equally meaningless in service of reducing the divergence between our preferences and observations.

So again, not a feature of the real world that there's actually a B matrix out there that has a number of slices equivalent to the number of actions that can be taken.

But given the dimensionality of E, there is a slice corresponding to each element of E in B. And then if affordance one is taken, slice one is used in the transition.

If a forwardance 2 is taken, slice 2 is used in passing s forward.


SPEAKER_03:
OK.

And what if there is somehow an implication if I'm in right now and there's an implication for the future?

And well, perhaps not really a deterministic one,

So I decide with my decision right now what happens in the future.

But the future would not be an affordance in that sense.

Even though it's part of maybe a policy, it's not an affordance because it's not related to that transition that is immediately activated.

That's right now the case.

Something like this?


SPEAKER_01:
that that's a a a commendable compatibilist position between the ecological psychologists and active inference is like saying well affordances are real in the moment and so policies which also include actions that are taken potentially in the future like imagined actions those are also those will be real in their moment and they're cognitively real in this moment

And that's why this also invokes the discussion around representationalism, because if one takes a realist perspective and says, well, this is what they're doing, they're evaluating all counterfactuals in the organism's brain or in the computer program or something, versus is this just an instrumental analytical framework

That doesn't actually mean that there's an E vector or the enumeration of counterfactual policies at all.

So the realist angle would be, this is the causal architecture and patterns in this Bayes graph will be associated with, for example, neural activity patterns, which can be understood as neural representations of these quantities and processes.

The instrumentalist is under absolutely no compulsion for this architecture to reflect anything in specific about the slime mold, the ant colony, the civilization.

It's not like there actually has to be the enumeration of counterfactuals about hypothetical action sequences.

But we list those as a convenience.

then we can apply tree surge branching time active inference etc etc as heuristics in the space that is combinatorially constructed but that from an instrumentalist point of view doesn't mean that's what the system is doing and that is like at one of one of the heart of the tension

because the gibsonian affordance concept centers a quintessentially realist phenomena which is what the organism is actually instantaneously perceiving in terms of action capacities chairs are for sitting which means potentially under like some um people's interpretation of radical and activism like if you don't see the chair

you're not having that instantaneous perception of action capacity.

Whereas an active inference, especially from an instrumental reading, we're basically on the plus side, completely free from that.

But on the downside, people need to be very careful that they don't sneak the realism back in.

and say, well, here's going to be the central nervous system, just conceptually, decision-making, and here's going to be the nested model of the hand.

And then it's all good, they go off on their instrumental way, and then all of a sudden they're interpreting this as the elbow, as if this architecture was reflecting, for example, something anatomical.

so instrumentalism allows one to fly freely with this analytical framework and leave a lot of baggage at the door however in the case where realist conclusions are being sought to be constructed one has to be careful that they weren't actually taking a covert realism along for the ride

good news is i think people can do that because they don't have any problems with linear regression but then again some do in some cases but the arc architecture of that base graph

can be seen of similar to structural equation modeling.

It's not exactly the same, but having edges and nodes is not the system's mechanics.

But given this statistical artifact

it's very easy to say that this causes that.

Instead of being proper and saying it's associated with that.

Especially because you get into directed edges.

It's like, well, this is unidirectionally associated with that through time.

It's like, come on, then just say it.

What do you think?


SPEAKER_03:
Yeah, if you ask me, I'm digesting it.

I need some more time to get all this a little bit clearer.

But thanks so much to elaborate on this question.

Thank you so much.


SPEAKER_01:
Yeah, this is like the heart of the textbook and the topic.

and this is a model architecture it's not the only model architecture but being able to interpret what these model architectures are helps us understand a lot about active inference and then I guess as we are seeing it also shines a lot of interesting light on broader questions about scientific modeling

in bigger discussions which i mean to kind of return to chapter nine that's the metabasian perspective that's why we're centering this ethological moment with the modeler's construction of a cognitive model whose aboutness is the setup the experimenter has constructed

and again that to make that explicit and encoded in our model structurally is the innovation because upon inquiry I don't think this is a contentious layout like if inside of the dashed line there was a linear regression or an SPM model

and we said, okay, we did the T maze and we had the rat hooked up to the whatever, optogenetics or to the confocal microscope or to the EEG or to the video camera or to the ultrasound.

It's like, you got all these scientific tools.

Okay, we set up an experimental context and then we observe behavior.

Now, what is it that was in between

the stimuli and the observed behavior?

Well, obviously it was the mouse or the rat itself.

Okay, so we want to have at the very least some kind of model of our rat that is able to perceive experimental stimuli and then output behavior.

So we could put a linear regression here.

Here's temperature on the x-axis and likelihood of eating cheese on the y-axis.

and we make a regression or we do model comparison.

We say, okay, is it better compatible with a linear regression or a breakpoint regression or a quadratic regression?

And so you have the modeling discussion and they go, okay, we use the BIC and the optimal model to describe the relationship between temperature and cheese eating is a breakpoint regression or it's just a simple linear regression.

Would someone turn around and say, why would you say mice are linear regressions?

The obvious response would be, no, we just use the linear regression instrumentally to describe our data.

Check out the method section, look again.

We followed pretty standard process of modeling

so that we were able to explain data but not overfit, accuracy minus complexity, and we use standard software tools and packages so that everybody could replicate it in its open source.

We're definitely not saying that mice are linear regressions.

But when figure 4.3 goes into the box,

different questions arise.

And I'm not even saying that what I have just proposed is infallible.

I think people can have a different perspective on that.

For example, they could say, well, yeah, if you do a linear regression, you're not saying the mouse is a linear regression, but there's something different about active inference modeling

when you put figure 4.3 inside the box you've taken on a different set of commitments that's a totally valid point to make if somebody can point to those commitments and that's a discussion to have but simply considering active inference as like a software package or an analytics framework for behavior

just replace figure 4.3 in your mind's eye with linear regression and ask what claims are valid or not.

And there are differences between the models.

So there may be certain things that go one way for linear aggressions and another way for POMDPs.

However, there's also a broad set of criticisms that people level at figure 4.3

that are throwing out the linear regression as well.

But it's a format that people are more familiar with.


SPEAKER_03:
I assume this explains much more, potentially at least.


SPEAKER_01:
You could generate a POMDP with zero explanatory predictive value.

And you could have a linear regression with a good R squared or not.

It's not a function of the type of model selected.

In fact, the space of POMDPs is just from a parametric perspective, it's much larger.

Like...

Here, this is still a relatively simple, this is even without showing the dimensionality of these variables.

Like if there's 10 affordances, B is going to have 10 slices.

And then if S has 10 hidden, even if it's 10 discrete options, so you're talking about some large tensors and parameterizing them from data is non-trivial.

That's what chapter nine is about.

And that's why we have often called for the development of statistical power analysis techniques, because even simple models have many free parameters that might be related in complex ways.

Like if C is like within this range, then changes in A do or don't do that.

but if A becomes more ambiguous, then this consequence happens over here.

And those kinds of like rippling effects in the model are not constrained to which edges are drawn.

So we need much better statistical power analysis techniques in order to anticipate

the kinds of statistical considerations that people use for linear regressions and RNA-seq analyses every day.

But there's nothing about this model that makes it fit any given data set better or worse.

These are all modeler's choice.

Any closing thoughts or questions on nine before we scan 10?

i also think it'll be interesting to see like as we have more notebooks and and accessible software packages that will help um support this deflationary perspective on active inference because it'll be like well what what what part of this um tiger is the is the b matrix

we'll have like a notebook it's like well now here's the b matrix in the notebook but it's not in the world and and then just coming at that nexus many many times will be useful but also i think including people in empirical research is going to help with that because for one who hasn't read a paper and or for one who hasn't participated in empirical behavioral research

neither of which are bad or good states just saying some people have these experiences and some don't that um separation of statistical models from reality like mice are not linear aggressions mice are not pomdps that is definitely like a posture and i don't mean it's a facade

It's a stance that is built through statistical investigation, at the very least reading it, if not actually doing it oneself.

Like once one sees that there's a hundred or an open-ended number of ways to model just the thermometer and the true temperature of the room, that person will think differently about map and territory.

This is the final chapter 10.

In general, we are least aware of what our minds do best by Dr. Minsky.

Chapter 10, we're going to wrap up active inference main theoretical points from the first half of the book, chapters one through five, and the practical implementations from the second part, six through 10.

Then we connect the dots, abstracting away from specific models discussed to focus on integrative aspects.

benefit of active is it provides a complete solution to the adaptive problems that sentient organisms have to solve this is definitely a sentence that we've um discussed in prior cohorts does it provide a complete solution or does it set the table for a useful

solution or or some other way of of saying that you know have the previous nine chapters literally resolved all the problems or have they helped us learn the linguistics and techniques that help us frame the problems so that we can come to heuristic solutions but again these are the kinds of things we can discuss um 10.2 wrapping up

this par at all textbook offers a systematic account of the theoretical underpinnings part one practical implementations of active inference part two they review chapter one chapter two long low road chapter three high road well do you need your temperature tomorrow chapter five

highlights some of the applications and research in active inference in the mammalian nervous system.

Chapter six, the recipe to design active inference models.

Chapter seven and eight, discrete and continuous time generative models in active inference.

Chapter nine, active inference in the context of model-based data analysis.

Connecting the dots.

Integrative perspectives on active inference.

Daniel Dennett and the whole iguana.

Hey, instead of going deep down the rabbit hole with the prospective working memory module subsystem and then developing this whole just-so story backwards from that, how about modeling a complete cognitive creature and environmental niche for it to cope with?

They talk about that whole iguana perspective and the way that diverse cognitive phenomena, attention, memory, anticipation, et cetera, et cetera, can be understood of using this active inference first principles framework.

Active inference doesn't start by assembling separate predefined cognitive functions, perception, decision-making, and planning.

Rather, it starts by providing a complete solution or approach to derive implications about cognitive functions.

That helps us identify patterns of cognitive phenomena.

For example, optimal foraging in different settings.

Finally, active inference offers a principled means of understanding corresponding neural computations.

it speaks to multiple levels of mars hierarchy mars levels and very briefly we'll discuss more in the coming weeks implications for psychological function as if we were sketching or prompting a psychology textbook predictive brains minds and processing what does active inference have to do and how does it build within the lineage of research on the predictive brain predictive mind

perception and different psychological and philosophical lineages for example going back to helmholtz and before on perception bayesian brain action idea motor theory cybernetics optimal control theory

utility in decision-making, Bayesian decision theory, reinforcement learning, planning as inference, behavior and bounded rationality, free energy theory of bounded rationality, valence, emotion, and motivation,

homeostasis, allostasis, and pteroceptive processing, attention, salience, and epistemic dynamics, role learning, causal inference, and fast generalization, active inference, and other fields, open directions, social-cultural dynamics, machine learning and robotics, and summary with the Lord of the Rings quote.

Ultimately, we are confident that you will continue to pursue active inference in some form.

Okay, thanks for the Chapter 9 discussion.

We will come back for the coming two weeks in Chapter 10 and then one final session.

So, thank you all.
SPEAKER_00:
all right greetings thanks for joining all it's march 29th 2023 we're in cohort 2 in our first discussion on chapter 10. so let's head over to chapter 10 and um begin with any comments really about anything within the active

area um several were voiced before we begun so perhaps terry feel free to begin with um what you wanted to point towards and let's just see where that goes thank you


SPEAKER_02:
In essence, my question is about the concept of a system of volitional consciousness that sits behind the nodal active inference model, creating

I suppose, making decisions about the strategic direction of the organism.

And I find, just repeatedly within this chapter, there seems to be this reference to a conscious entity behind the algorithm.

And my sense from...

trying to understand this material is that it's almost like in meditation where, you know, the experience is the experience, that almost there is no I behind the experience and that it just jars with me when we're presented with a...

almost complete active inference model where our actions in the world are actually experiments to gain information about the external state to ultimately update the model.

What consciousness is, as you said at the start, is a difficult concept, but it seems strange to me within the context of this model that we refer back to some sort of underpinning, strategic, conscious, experiential driver of action when that seems outwith this model or unnecessary for this model.


SPEAKER_00:
um uh michael or ali want to give any first thoughts on this yeah ali go for it


SPEAKER_03:
Yeah, well, actually, I don't think they necessarily somehow, I mean, the active inference framework doesn't require any

conscious entity in the sense of our anthropic sense of this word consciousness because what FEP actually tries to model is a kind of quote-unquote as-if epistemology

So, by as-if epistemology, it doesn't mean that everything that can be modeled with FPP formulation necessarily has this sense of anthropic consciousness, but

It's a kind of self-referential way of talking about the perspective of being modeled.

So I'm sure you've encountered this word self-evidencing throughout this book, which refers to the same concept of as-if epistemology.

So it's one of,

I'm sure Daniel's favorite figures in the book, figure 9.1, also captures this sense of perspectival view of kind of the nature of this modeling is not that we have this sense of understanding

omniscience that can be applied to the environment or that can be reasoned through

And that can also be applied to our sensorium or perceptorium, but it's like we have this model inside a kind of,

apparatus that can see its behavior as if following through a kind of trajectory in the probability density space.

So by volition,

That you're referring to.

It's my understanding.

That it's also.

Another term.

That they've also used.

That's related to this.

Self-evidencing.

Is the object.

Or the agent.

Tries to infer its environment.

Through.

A kind of.

What was this term?

It was.

Something like.

indirect person?

No, it wasn't.

A demo, can you help me find the term for the indirect perception of the specific term that's used in the literature to refer to indirect perception?

Sorry, just a second.

I don't exactly remember.

Okay, I'll find it and I'll get back to you in just one second.


SPEAKER_02:
Okay.

Stop me if I'm going down some unnecessary rabbit hole.

But it's almost as if this...

system active inference represents a model by which one could create a psychological zombie in that by active inference without any sense of internal self one could behave in the world by minimizing variational free energy at

in the way that we behave in the world.

And, um, Daniel Dennett seemed to, in one of his books sort of talk about that, um, uh, the, our, our, our, you know, that we have active won't rather than active, um, uh, than rather than free will, you know?

And, um, I, I suppose the, the, the implication of that is that we, um,

that our conscious reality is like a story we tell ourselves almost shortly after the event.

And yet in this book, there is this reference to a conscious will within the framework, certainly within that last chapter, chapter five that we're, isn't it chapter five we're on today?

So I mean, I don't have this worked out of my head, but I'm just interested.

It's entirely possible that one could say active inference isn't about what it doesn't speak to consciousness.

Consciousness is another thing.

Active inference just gives another is a potential model that drives

decision-making within the organism.

And that's okay.

But I'm just interested in other people's thoughts on this.


SPEAKER_00:
It's great topics.

So just a few notes.

Broadly,

there are perspectives on consciousness.

Well, first I'll just note, consciousness appears to be only used once in the book.

So while it is true that it's an area of interest and was the live stream with Vanja Visa this morning, I mean, and the authors all explore elsewhere, but any interpretation of consciousness, I don't even think it's within the...

not a pillar of the framework it's something that is like inferred towards so i think that will always leave a fundamental um uncertainty and pluralism and then we can think about at least a gradient of um accounts of consciousness in some accounts consciousness is um after the fact in some way that it could still be anticipatory

from a predictive processing sense, but it does not have a causal power.

It's a narrative told to, let's say, minimize variational free energy to make sense of memories.

And so that kind of narrative expositional self as epiphenomena and experience without causal impact.

And then that is of course contrasted with causal accounts of consciousness.

that's where whether you come from some philosophical tradition or certain folk psychological traditions um which both can arise during scientific writing as well things like emotions or awareness or even attention are actually described as causally upstream of other phenomena and that

puts one of course squarely in the kind of cartesian dualism area of philosophy like how does something that is not physical influence something that is physical um so the the the the dominant

mode to me on that question is consciousness is something we can model about and around with active inference but no perspective on consciousness seems fundamental to using statistics in this way

and so i expect and already find confirmatory evidence that some people take a more epiphenomenal approach to consciousness and some people taking a more causal approach for example treating it like a hidden latent factor that does have a causal impact in a causal bayesian sense and then also just recognizing yeah we don't know exactly what it is but that's how we do active modeling we propose latent causes

and those become our generative model so in some ways there may be more similarities between these two than their hyperbolic forms suggest and in either case I feel like it's a continuum that's accommodated methodologically by what Ali referred to as the as if epistemology

And that, I think, connects to Daniel Dennett's intentional stance.

Like, I'm going to play this chess computer like it's trying to beat me.

If it's not, then I want to win.

So if it wants me to win too, then that's going to be great.

But I'm going to hedge my bets and play like it's trying to win.

And I think that kind of intentional stance is not just warranted, it's actually supported

by these methodological tools terry or michael well in in some ways and we're getting way off topic and then i will be quiet oh no this is we're technically in chapter 10 so we're we're totally on just recap and meme status


SPEAKER_02:
OK, but it's like, but you would say that, wouldn't you?

You know, if consciousness is an after the fact story we tell ourselves, you know, it's at the model.

if the world is probably more likely that one finds oneself in front of a computer play playing chess because of the, you know, all of the previous events that have occurred during your, your life and that you are now playing chess.

So the story you tell yourself is that I'm going to play to win, but actually that it wasn't the, it wasn't the conscious decision that got you there.

If consciousness is an after the fact.

Yeah.

you know, a hallucination or story that we create to justify the things that are happening in the external, in the unknown external state, you know, because the computer is in the unknown external state.

It's something that we have internally created as our model based on the information we're getting across our Markov blanket.

Am I getting too mad?


SPEAKER_00:
No, I'll give a short note and then Ali.

So one answer from a biological perspective is, especially over evolutionary time, but also within a generation, you will only repeatedly observe things that are

fighting to live and persisting amidst challenging and adversarial settings.

So if we just say, oh, well, just the person in front of the computer, what would be subconsciously leading them to want to win?

But if we say we're in a chess tournament series and only people who have won previous chess tournaments defined by winning in this arbitrary way, only they are persisting in this tournament.

Well, then why,

Over time, you will come to find that those agents are acting as if they want to win at chess.

Now, then there's the debate.

Well, what if they're just acting like they want to win at chess, but they don't really want to win?

They're chess zombies.

They're just acting like it.

And then that's the pluralism.

Some people look at that chess tournament sequence and they go, yeah, I actually have like reasons to think that people don't have experience or intentions or that consciousness is just a narrative told after the fact.

And so they're just acting like they want to win.

And the other person tells us extremely intimate causal story about how the motivation of the person drives their action selection.

But in the end, they're unified by this as if epistemology, which is to say the intentional stance.

Ali?


SPEAKER_03:
Yeah, also this kind of as if epistemology comes from, at least partially comes from Kantian transcendental idealism.

So for Kant, concepts do not apply to reality in the form in which we can only think them, but

it is also urgently required for any theoretical practice to think and investigate objects with the help of the concepts as if they were given or determined in a specific way.

So this kind of as-if epistemology

You see, concepts have to acknowledge that a reality is presupposed independently of the choice of its semantics or its meanings.

On the other hand, in their self-reference and self-similarity, they work with this heuristic of as if,

so that it can be constructed in this way as if reality and concept would actually coincide with each other.

But some recent philosophers, such as Karen Broad,

attempts to overcome this kind of as if epistemology uh for example in her agent realism agential realism um by means of construction of interactive uh discursive in which uh both epistemology and ontology can be seen as the very same thing and uh we wouldn't be required to

draw a boundary between the perception on the one hand and the thing that we're trying to percept or model on the other hand so through this uh intra action not interaction but intra action or constructive interaction

they constitute each other.

So they give rise to their very existence.

And also, I think in some view,

FEP is perfectly compatible with this more loose sense of onto epistemology because one of the requirements of the FEP is if things exist,

how they should behave, right?

So it somehow presupposes this kind of interactive discourse by basically eliminating the boundary between the thing that we're trying to model and

I mean, the model or the map.

So that's basically the argument behind that map territory fallacy.

So, yeah, this is a bit more complicated in this sense that if we want to get to the bottom of this kind of epistemology, we will hit a dead end if we just...

If we just go through this more traditional Western epistemology of distinguishing between the thing that we're trying to perceive and the perception itself.

But as I said, FEP somehow tries to overcome this problem in the first place.


SPEAKER_02:
That's beautifully described.

and nicely closed down.

The thing that I was thinking initially was the assumption that in the external state there's actually a chess competition happening is assuming almost this, you know, that our...

internal state representation is an accurate or a true likeness of what the external state is which i think ali was kind of saying but um i my my sense is that it clearly can't be because all of our

um all of our all of our qualia are um uh endeavors that the the the the central nervous system makes to interpret um signals that are generated without the qualia that we apply to them yeah no these are these are great one short thought then michael um yeah the internal state


SPEAKER_00:
does not in the model does not have to have the same structure as the external state so like often we'll talk about well the external state in the model could be a continuous temperature and the internal state could just be maybe too hot just right or too cold so they can have different structure

And that reflects that they don't even have to be of the same type, even within the model, let alone the territory.

However, by the cybernetic principles, like good regulator theorem, again, eventually, you can always just create an edge case where the agent has no mapping between their internal cognitive dynamics and the external regularities.

Like it's just playing the game randomly with respect to the environmental variability.

And in a research or an education setting, that may be pedagogical.

But when we're talking about real measurements that we made, even of like the pen on my desk, it is resisting dissipation over some spatial temporal timescale.

And that is actually the surprising implication of if things exist.

how they should behave is like, well, I am repeatedly measuring this pen.

So now what can I say about that?

And it turns out you can actually say a lot if you bake into your understanding that you are observing something repeatedly existing.

And that's exactly what we see in figure 9.1.

And so we have a unity of the scenario

in our model via articulating the partitions on a base graph so that means they're still totally up in the air mapping territory and all of that but within our model we're we're approaching that in a way that sets the stage for people to then take further perspectives about deeper questions michael


SPEAKER_01:
Yeah, I think these questions maybe somehow relate to this tale of two density papers you mentioned last time.

And my understanding of this paper is that they try to argue that there is no representation in the mind, but the mind is a controlled system.

And then they talk about two different distributions.

So one distribution is the recognition distribution, and the other one is the generative system, if I understood correctly.

But I didn't really understand how this distribution comes into being through the generative system.

So they equal one with an enacted.

So the recognition system is the embodied system, and the generative system is the enacted system.

And my understanding is that this is all that there is.

So for instance, they say here on page 235, the generative model manifests as a control system that uses exploitable structural similarities

encoded in the internal states of the organism.

So instead of having representations, they talk about control states.

And do I understand this correct?

So how can that be a distribution?

I didn't really understand where these two distributions are.


SPEAKER_00:
Great.

All great questions.

So I'll try to address some.

So this question of exploitable similarity,

is very much like the good regulator theorem in cybernetics.

So it doesn't mean that the temperature in the room, let's just say that it's physiologically relevant.

So there have to be some sensors and some processing apparatus and also action apparatus, but at the very least just for cognition, a sensory and processing apparatus that is able to like measure temperature.

can be a proxy of it doesn't need to be strict isomorphism they say it's weaker than a strict isomorphism but it actually has to have some sort of causal relevance now if you're like well i'm in a simulation where the temperature is fluctuating but it has no survival value then it's like okay that's like saying you know cosmic rays are going through us they don't have survival value and so

They're not part of the cognitive model in this setting.

But this points to this idea that there is information in the sensory apparatus about action.

It's actionable intelligence with respect to persistence.

Why persistence?

Well, we could say in pursuit of reward, but then we would need to ask how systems persist.

Why persistence?

Because we're at least repeatedly measuring the system.

So again, with the as-if epistemology, it's as if in order to persist amidst dissipative and adversarial settings, it's as if there's an exploitable information content in the perceptual umwelt that enables that kind of thing to persist.

That's like the tautology that also goes beyond a tautology.

Now to the specifics of the tail of two densities.

This also might've been the first dot zero in six, I'm not sure.

In Bayesian statistical analysis, models can be run in two different directions.

This could be like a hyperprior and here's the hyperpriors on the right and the data are on the left.

So it's like you can specify the hyperprior or the hidden state distribution and you can emit pseudo data.

That's called generative.

Or you can have data and then parameterize hidden states.

That's the recognition model.

So this could be the actual measurements of children's height in the classroom.

Here's our generative statistical model, Gaussian family, mean and variance.

So given a mean and a variance estimator, two numbers, metaphysical, statistical, we can generate infinite class size of synthetic height data.

Or from one height observation or more, we can update our generative model of the world.

Okay.

Then,

more broadly we're interested in this relationship between the world and the agent agent in the niche so perception and how that updates the agent's learning and uh and perception and then how the agent acts on the world and what they argue in this paper

is that the recognition model in a Bayesian statistical setting, so going from measurements, which might be noisy,

And the height in centimeters isn't like the real height of the person.

The number on the scale isn't the weight.

The number on the thermometer is not the temperature.

It's data.

This is why the empirical perspective on behavioral modeling is so essential.

Because when we go on this Bayesian side, it's really easy to talk about how data are recognized and update hyperparameters.

And then hyperparameters can be in the business of continually generating unfolding predictions.

We have a noisy camera with a blind spot.

We're updating our generative model of vision, and then we're going to be, in this case, just creating more synthetic vision data.

In inactivism and all of the associated ease, embodiment, et cetera, et cetera, et cetera, et cetera, they are focused on

the materiality of the engagement of the agent with the world.

In this paper, Tale of Two Densities, they draw these two together to argue in the context of what we'll just point to in figure 4.3 with the A matrix here, two.

There's a generative production of synthetic observations.

You could use it that way.

That's the generative direction of A. Or you could use A in its recognition capacity to update hidden states from observations.

So even without getting into the generativity of action, which is what in the inactivist's setting, what they're most interested in,

is the actual causal consequences of action, just within a figure, point three, hidden state inference, we can think about a tail of two densities and about how Bayesian statistics helps reconcile this tail of two densities because the same generative model, which is just what we call whatever Bayes graph you construct, the generative model, can be used

equally flexibly to do inference about any part of it.

Like maybe you know the hidden states, but you're trying to determine the prior.

So these graphs are agnostic as to what you know and what you want to infer.

Now it turns out that for real settings that we find in the behavioral lab and out in the fields, we get observations

We don't have access to hidden states, but we want to infer them.

We don't even think A or B matrices or D matrices exist in the world.

We purely want to infer them as statistical parameters.

And then we want to know about action.

And in some settings, we have the organism's trace of actions, and we're trying to infer their cognitive parameters.

That's the figure 9.1 setting.

We saw their actions.

It was data for us.

And we're looking to parameterize a cognitive model for them.

Another setting is we don't know what action will be taken because we're the ones taking it.

And that is like a prospective application of these types of models

to take ongoing streams of observations and then perform action selection in the moment.

But these frameworks are enormously general with whether they're used for online or offline inference and like what you actually have access to measuring and how.

So little long, but the tale of two densities,

is about how the bayesian epistemology and structuring of the relationship between observations and inferences about observations maps in the inactivist setting to some aspects of how we think about perception cognition and action in the niche and their paper positions that

within the debates around representation because representation are although there's different perspectives etc postulated to be like the on agent aspects that have semantics about the world


SPEAKER_01:
Yeah, and they say the agent just controls the world through their own actions.

That's my understanding.

Yeah.


SPEAKER_00:
Yeah.


SPEAKER_01:
So then the two densities are on the incoming side and in the outcoming side.

OK.

That's the idea.


SPEAKER_00:
We may be stretching.


SPEAKER_01:
The density is not the constitution of the states, of the internal states, but the observable actions.


SPEAKER_00:
We may be stretching the tale of two densities, making more of a set of novels around densities, because it's just ultimately a cultural reference.

The technical point

is that these variables are statistical distributions and hence they are densities they're statistical densities over some support so the two densities that we might be interested in or amongst two of the ones that we're interested in making a unified account of are the perceptual density which is about how data map on to hidden states like given retinal data

is that a car that's a recognition model and then there's kind of like two kinds of generative densities you could run that car recognition model backwards say given the label of a car generate me cars stable diffusion generative ai but then there's also this action as a generative step

that brings in the actionable and cybernetic constraints which is why they focus on the exploitable similarity but in general Bayesian models you can do this kind of observable hidden state inference about anything

The organism, Bayesian brain, or if we choose to use Bayesian statistics, it's going to be doing something like that.

The world has a real temperature, which is like molecules moving at different speeds.

Perception occurs through sensors, which transform whatever it is the molecules are doing into something totally, totally different.


SPEAKER_02:
Can I just make a little point about that?

I remember watching Richard Feynman, just a short video, where he was describing temperature as actually infrared radiation, which is part of the electromagnetic spectrum.

On one part of that spectrum, we see light, and on the other, we experience temperature.

So temperature is actually a proxy for something else.

The mechanism that we have to perceive temperature is then a proxy for the amount of energy within that, for example, room.

So it's not...

Our response to the infrared information

is a response that's not based on necessarily that infrared is going to damage us.

It's that there's too much energy in this room.

And the tool that I have to assess that energy in the room is actually monitoring the infrared radiation, which I can't see, but I can feel.

Any comment on that?


SPEAKER_00:
Yeah, like by analogy, how about tasting different foods?

And there might be a toxin, like we can't directly tell that something is gonna be contaminated, but over time, successful entities in an unchanging niche, now that's not every case, but just with that simple case at first, successful entities in an unchanging niche will come to exploit similarities

which is to say mutual informations between the recognition model and the actionability.

And so they don't need to be directly measuring or even internally representing this is going to make me sick or this is the wavelength of light.

Any and all informative cues of sensors are going to be

aligned towards adaptive action.

That's the synthesis of signal processing and control theory in active inference.

And it turns out that by taking a first principled approach to the synthesis of those two fields, again, just coarsely, we have new insights on the question of representation in cognitive science.


SPEAKER_02:
Yes, but taste is a very interesting phenomenon in that we have sweet, salt, sour, bitter, and yet I think there are trillions of different tastes that we can actually create or...

or i think smell is probably associated with taste as well so you know from a limited amount of sensory information we can create a vast array of um experience


SPEAKER_00:
Yeah.

I mean, with the Jolly Ranchers, I don't know, just a candy I had in childhood, like they literally taste the same if you don't see the color.

But if you do see the color, even if you try to forget, they will taste different.

So, yeah.

So different, that's like referred to a sensor fusion.

Different sensors come together in the generative model.


SPEAKER_02:
And it really speaks to the predictions.

It really speaks to what are our priors, you know?


SPEAKER_00:
another kind of mind bender on this is like the visual example there's so much abundant um on ramps to predictive processing like we don't notice our blind spot we don't have any blur when we do the isocate several times per second because of sensory attenuation you know color vision and precision in the periphery okay i'm on board with a generative model of vision

I've kind of dispelled this sort of incoming information camera processing model of vision.

I really believe in the visual generative model getting updated.

And it's like, but taste?

It's like, yeah, by extension.

What does that really mean that we're generating our taste experience?

Well, then why wouldn't I just generate tastes that taste amazing?

Yeah.

well, why wouldn't your generative model of vision just generate what you like to see?

And that's where the real organismal constraints and where appeals to our development, ecology, and evolution do come into play.

So again, contrived Gedanken can be proposed absent these super obvious constraints for real systems.

However, in the context of empirical behavioral modeling,

it's usually sufficient to say the prior distribution on this or the trade-off between these two different drives is inherited from history and development or even from intergenerational processes.


SPEAKER_01:
But isn't it that we generate a good taste going to a baker where we like to go and going towards those areas where we get those kinds of rewards that we prefer?

I mean, doesn't that make sense also?

Always coming back to the baker.


SPEAKER_00:
Yes, I don't know why.

Yes, okay, okay, here we were.

Okay, chapter seven.

We were engaged in a policy vacillation.

We were not sure if we were wanting to get bread or if we wanted to go to the barber.

So now the question of the taste.

Yes, if we...

Expect and prefer a given taste, like sweet.

And I think this holds true for sugar, but it can hold true for other things too.

Then in the free energy evaluation of different policies with expected free energy, expecting observations that align with your preferences will be associated with pragmatic value.

So that is how preference for sweeter tastes puts its thumb on the scale of policy selection, biases policy selection towards tasting sugar as a pragmatic reward, but that happens amidst a relationship with epistemic value as well.

So it's like epistemic value could be pursued

Like we need to find out how to get to the baker.

I mean, there's many ways to go, but yes, you would parameterize a generative model with a preference.

Let's think about this case with sugar and the baker in those two settings that we described.

So in one case, we're in figure 9.1 and we're observing people's behavior.

So we have their credit card history.

Whoops.

And we're observing who is going to the baker or what they're ordering or what they're clicking on online.

And now we're going to do inference on a generative model for them.

It's the as if generative model.

And it's like, wow, this person has a very strong C vector, very strong preference vector for sugar.

Like given alternatives, they're always going for the sweeter one.

And this person has a neutral preference for sugar.

And this person actually like moves away from sugar as represented by our parameterization of the C vector.

So that's one setting.

We have data on a bunch of different entities on what they chose in different sugar concentrations.

And then the other setting would be, now step into the trajectory of one of those cognitive models

and now ask what your next action should be.

And that's the as if a gentle or intentional stance.

So this is the behavioral scientist's view from the outside.

And then that is going to be flexible with allowing us also to take views as if from the inside.

And just because we can take an as if view from the inside

doesn't mean that that's the territory.

That's literally the map territory fallacy.

But it means that within the limits of our cognitive model, we do have that opportunity.


SPEAKER_02:
One of the things that strikes me as you talk about this is that

One of the interviews Carl Friston has done online, he talks about our actions during the day that we get up and we have our breakfast and then we go to the bus and we go to work and we do our work.

And actually, most of our life is little routines.

We think that we have these independent lives where we are constantly making...

decisions about what do I want to eat today?

Where will I go?

But actually, the person who ends up going to the baker will find that on their walk to the bus stop, they pass the bakery and they get a trigger from the smell of the bakery to do the thing that, you know, to predict that.

It's just interesting that we don't exist anymore.

As these completely independent, no matter where you live, no matter who you are, there are so many aspects of your life that are just so, so repeatable that we actually live within a very constricted box, apart from odd times when we step outside that box.

I don't know if that's relevant to that picture, but...


SPEAKER_00:
Oh, it's awesome.

I mean, very interesting to point to the state of exception.

In Livestream 10 with Mao and Ramstad et al., who are the other authors?

Mao, Axel, Karl, and Maxwell.

they talk about like strong and weak scripts.

A strong script in a social setting would be like if we were literally reading from a screenplay or if we were doing some sort of highly ritualized behavior.

Whereas the weaker script concept refers to like norms, normativity, expectations, X kind of person should act like this.

I'm X kind of person.

I would be unsurprised by doing X. Oh, what do you know?

I'm doing X. And that kind of progressive identity capture and development of understanding what kind of thing one is, and then what are the two ways you can reduce your discrepancy?

You can learn, change what you think that kind of thing is, and you can act.

So undergrad should act this way.

Here's the distribution of undergrads.

If one's understanding of what an undergrad was, was totally flat with respect to everything, there's nothing that characterizes an undergrad versus anything else, just from one person's perspective.

Well, then that aspect of their identity wouldn't shape their policy selection.

Whereas if someone says, well, as a X, I should be doing Y.

can be approached from a purely statistical deflationary perspective like i just expect myself to be doing this like or it can go even beyond that too so these are these are all very relevant and learning about the the fundamentals of this kind of statistical modeling help us

I hope, I prefer also that it helps us actually respect the complexity and the richness of these phenomena, which just simply cannot be expressed in words or math, which is why we talk about the map.


SPEAKER_02:
One of my wonderings about all this, because I do find the maths and the pictures and the diagrams all very difficult to grasp.

In some way, is there a potential that if we really understood the model, we could start to use that as a system to inform our understanding of

individual behaviours.

I mean, I'm a pain doctor, so my interest in all this is this truth that we can alter our... If you're in pain, if you have back pain, you can alter your prediction about your back pain and actually alter your back pain.

And I'm just wondering if this kind of modelling could in some way give us a tool to help us to...

support other humans in understanding their ability, that they can update their priors, you know?


SPEAKER_00:
Yeah, it's a great vision.

I think the closest that we've explored that is with the help of Ian Tennant.

who has a interoception focus and this therapeutic alliance concept where a lot of these kind of avenues and like, it's not as simple as just wish it away any more than the generative model of vision would be like, I want to see, you know, a unicorn in front of me.

So how do we reconcile that agential realism that Ali mentioned earlier?

Like,

this active co-construction and negotiation and sense-making from our sensorium and the ways that our narratives and the scripts and the expectations shape our inference and action and choose to have a preference and to act to push things in a certain way.


SPEAKER_02:
Yeah.

Absolutely.

Yeah.

But, but, but,

I suppose what I'm kind of thinking is, could you have a Sam Altman created model where you run the model and that can inform the understanding of

and individuals how they got there and potentially offer some insights into what to do about it.

You know, could this speak to a model that could be run where we could kind of do the experiment and then that supports an individual on actually going on the course of action

to change their experience.

That's kind of what really blows my mind about this.

And that in the current world that has only come into existence in the past month or so, this actually seems almost plausible.

Because of the similar layered nodal systems that the likes of ChatGPT are using to protect language?


SPEAKER_00:
Yes, it's a great... It's not active inference.


SPEAKER_02:
You know, it's not active inference.

You could model it around a notion of active inference in the training of the system.


SPEAKER_00:
there's there's so many avenues to explore there let's in our discussion next week let's look at um that again but absolutely with respect to figure 9.1s about us the data exists the means motive opportunity anyone who wants to can do things

that's a big question so for next week thank you though i'll stop the recording so
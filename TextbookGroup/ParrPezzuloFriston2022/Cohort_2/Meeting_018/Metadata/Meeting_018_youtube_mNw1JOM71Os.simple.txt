SPEAKER_00:
Hello, it's March 1st, 2023.

We're in cohort two of the Pardal textbook group.

We're in meeting 18, our first discussion on chapter eight.

First, Michael, if you wanted to just raise the context or question that you wanted to begin with, and we'll just pick up there.

You're on mute.

I'm not sure if you're asking.


SPEAKER_02:
Sorry, my microphone was off.

Yes.

Yeah.

So the map territory fallacy, I was wondering what this is and whether you could perhaps elaborate a little bit on that fallacy.


SPEAKER_00:
All right.

I'm moving it to chapter 9, even though we're in chapter 8, because we're going to bring in a figure from chapter 9.

Ali, do you want to give a first thought on this?


SPEAKER_01:
Yeah, sure.

Actually, map territory fallacy is one of the fallacies that is widely used to somehow...

somehow criticize free energy principle because as the concept may the name of the concept may suggest some believe that it's a kind of fallacy to somehow conflate the map with the territory so by this I mean that free energy principle claims that well we have

So very informally, we have a map of the principle or the theory or whatever, and the territory that it applies to.

So the system or the agent that we're trying to model.

But on the other hand, by claiming that the system or the agent actually does the kind of inference that recovers the map itself,

Some people believe that this kind of conflation that map converts into territory or the vice versa is a fallacious claim.

So we need to separate map from the territory.

But in this recent paper on the map territory fallacy,

Ramstead et al argued that actually this is not a fallacy at all.

It is the structure of the FEP theory.

So in other words, we're well aware that there is a kind of inference as if epistemology happening here.

So yeah, but...

It's a bit more extensive.

They have lined up some number of arguments to defuse this map territory fallacy fallacy and why it's not fallacious at all.

So yeah, that's basically the gist of it, in my opinion.


SPEAKER_02:
Does it have to do with a distinction between model and process?

And the model is the, maybe the model is the map and the, no, the other way.

The process is the map and the model in the external world, I understand.

No, the external world is the, what is it?


SPEAKER_01:
Process is the external world and the model is the,

I mean the generative model of the agent itself.

It definitely relates to that but on the other hand it also relates to this concept of self-evidencing which is a way to describe the mechanism by which the model in the environment or let's say the agent in the environment models itself according to the

generative model.

So it's a kind of, as the name suggests, a self referential way of modeling the system in the environment.


SPEAKER_00:
Yes, it's definitely a roller coaster, and not a resolved one.

But still, we can look to some of these

key recent works so the map territory fallacy would be just like it's been since the classical Borges writing this realization like the map is not the territory kind of related like all models are useful or some models are useful but none are true like correlation is not causation this sort of broad space of aphorisms that's basically like

instrumentalism isn't realism as we might say it so that's the map territory fallacy would be making the fallacy that the map is the territory i made a linear regression between cancer and heart disease ergo they are actually causal that way and when we're using active inference framework one might be slightly more tempted than the linear regression

to say, well, because I've generated a model that has a structure that seems to articulate certain natural things, this is like an explanation.

It might even have predictive power.

And so that might lead somebody to make this kind of a fallacy that the map is the territory.

Then Ramstead et al have kind of

agreed and amplified which is saying actually when when you use the map territory fallacy to criticize the kind of cognitive modeling in the fep and bayesian mechanics you're acting fallaciously and there's several angles on that unpack more in the paper one angle is you might be um

chopping off more than you want to like you may realize it or not your critique may extend to simply all quantitative modeling and many people want to like excise fep

from the space of scientific modeling in good graces, but then they don't wanna, for example, dismantle the architecture of using statistics.

So that's one angle, which is like, if you actually seek out the root of your descent with this kind of cognitive modeling, you're gonna be cutting off a lot of what is just understood to be statistical modeling

So it's like, you've gone so far into denying the map territory relationship that you've actually biopsied more than just the FEP.

And then they go further, which is that several features of the FEP make it the ideal model of generic systems in statistical physics, which is a bit of an abstract statement.

So we can bring it back to the textbook.

with figure 9.1, which just as Michael brought up, this came up in the PAR recent book stream with PAR, where we asked what was happening in this figure and had some discussion on it.

And here's the experimenter in the outer shell.

Making a cognitive model, that cognitive model of let's just say the rat in the tea maze has an aboutness that reflects the experimental setting and context.

so it's making the experimenter human in this outer shell is making a cognitive model which is a map so all the caveats of linear regressions apply maps not the territory don't want to fall into the map territory fallacy however it's also the case that that cognitive model that map is map-like

in its relationality to the experimental context.

So there's this view from the outside, black box, cognitive model being proposed as a hypothesis, discrete time, chapter seven, continuous time, chapter eight, whatever it may be.

But also the aboutness of that map is about this generative process.

which is the generative process handing stimuli to the generative model of the agent and so that duality view from the inside and view from the outside makes the FEP uniquely the ideal model of generic systems so they're saying yeah for sure don't do the naive fallacy

but don't engage in the fallacy fallacy.

And then one final piece is if anyone has a dissent with this, let's just see their alternative.

Let's not simply tolerate a standalone criticism, which is actually like a space for improvement, but is anyone seriously proposing an alternative to this structure?

And the answer is not really.

Ali?


SPEAKER_01:
Just to add slightly more detail to what you just explained in terms of the paper itself.

Well, they actually lay out two lines of argument against map territory fallacy.

One is, as I mentioned, the misconception that the

uh, process and the model or the map and the territory are conflated, uh, which they say they're not actually conflated, but the territory behaves as if it were a map.

So it's a kind of nested representation.

Uh, and, uh, the other line of argument is as, uh, Daniel mentioned, uh, rises from a kind of misunderstanding of, uh, the, uh,

the kind of modeling used in statistical physics, and in particular, the principle of maximum entropy to model the environment.

So the way that, I mean, in FVP, the way that agent models or self-evidences the environment is through CMAP or a constrained maximum entropy principle.

So yeah, I just wanted to

add those two bits of detail there.

Nice.


SPEAKER_02:
Michael.

So isn't this that a map and a territory implies that there is an isometry between the map and the territory?

But in free energy, in several points, they claim these things are independent.

So the process and the model

is actually independent through these Markov blankets and so on.

And we cannot actually know what happens outside the world, which is quite different from that idea of map and territory.


SPEAKER_00:
Well, great questions.

I think there's two threads there that are very tightly intertwined, but they're really important to consider separately.

So first, you said, is there an isomorphism or an isometry

between the map and the territory.

Isomorphism in the strict sense, meaning identical structure, is not required.

The generative process

Even just talking about the map, the generative process could be a continuous variable and the generative model could be like discrete or a binary and categorical variable.

So isometry is not even required between the internal states and the external states within the map.

And then it's an even broader space where you don't even know if isometry even applies.

to, for example, the structure of the real territory.

So for sure, isometry or isomorphism is not required.

Some kind of adaptive resemblance will come to be seen.

That's like the good regulator theorem from cybernetics.

Any regulatory adaptive system is as if, that epistemology that Ollie mentioned, it's as if it has an effective model of its niche.

Again, that doesn't need to be isomorphic, but evolution or far from equilibrium thermodynamics does make it the case that you're gonna get swept off the table if you can't adapt.

And so over generations and through development, it'll come to be the case that there's functional relationships and a semiotic process, but it doesn't have to be as stringent as an isomorphism.

And the second aspect was the statistical independence that is the technical definition of a Markov blanket.

So Markov blanket, it's the nodes

upon which two other sets of nodes in a Bayesian graph are conditionally independent.

So the fact that we're invoking the Bayes graph right off the bat, we know that we're talking about the map.

So territories don't have Markov blankets.

Maps do.

So already we're within the map discussion space.

And what the Markov blanket does is it precludes direct

consequences, direct causal impact from internal and external states.

So no telepathy, no telekinesis.

It's just a model constraint.

However, still through the blankets, there can be the propagation of causal influences on a base graph.

For example, a light could turn on in the generative process that could shine photons onto the sense state, and then that could update the

implications can still propagate on a base graph through the conditional independence of the Markov blanket, just like person A could tell person B and then person B could tell person C. And that doesn't mean that person A told C. Yes, Michael, go for it.


SPEAKER_02:
So you're talking about the free energy principle.

But does this apply also to the notion of maps and territory as, for instance, Google Maps, for instance?

They should have a more strict notion of isometry, I guess.

It's just a question of what you were talking about,

free energy principle or the notion of maps and territory in general so to what extent does this metaphor you think that this metaphor really applies to arbitrary depths um if i make my question clear um


SPEAKER_00:
I'm just, for those who are watching the video, I just have to like, my screen is like frozen.

So I'm just needing to kind of reset the screen share.

But I mean, arguably Google Maps, it depends on what you mean by an isometry.

That's a two-dimensional GPS landscape.

So some kind of distorted representation, like making it flat or,


SPEAKER_02:
is done within the bounds of what's useful i'm not sure what what else needs to be claimed for the purposes of google maps okay and so you're saying the same also applies to this uh these markov blankets and the free energy principle what do you think ollie


SPEAKER_01:
Again, we go back again and again to Peirce's taxonomy of symbols.

Actually, maps, so either, I don't know, Google Maps or circuit diagrams or these kinds of maps can act as a kind of icon for the object instead of the index.

Sorry, I thought I got disconnected.

Icon instead of index.

So an icon is a kind of sign that stands for the object through the relationship.

So this kind of relation between the icon and the object is a kind of structure-preserving mapping, especially in the case of...

geographical maps or any other cartographical different mappings.

But in my opinion, when we talk about maps and territories,

in relation with physical systems or biological systems or whatever, we use this term in a slightly more broader and a much more looser term.

And it doesn't require to be an exactly structure preserving mapping.

So it's more like,

a mathematical mapping between a mathematical relation between a sets of object or sets of states and the other sets of states namely generator processes and the generative model so that's it I mean there isn't any need to impose any additional requirement for


SPEAKER_00:
uh the exact structure of those mappings it's a general mathematical mapping per se awesome yeah one other piece like from the empirical practice side that i think informs our philosophy of science is very rarely is the empirical researcher simply interested in producing one final model

modeling is almost always an iterative process that involves the maintenance and diversification and selection from portfolios of models so the findings of a paper might be like this

bayes space that we explored of models these structural families suggest that the two factors are better than the one factor or that short-term memory does interact with valence in this way so right off the bat empirically we're talking about portfolios of models

Hence, there's obviously a multiple-to-one mapping of maps to system of interest.

Ergo, it's a bit limited to try to over-interpret any map and even get into this discussion of whether it's actually the territory.

It's not.

That's why we're doing it.

That's statistics.

That's modeling.

We have a whole host of models on our computer.

None of them are the territory, but it's so tantalizing because first off, unlike the linear regression, we're talking about the action perception loop, which brings this sort of tangibility within our grasp.

And also there's this question of language with the instrumentalism and the realism

being mixed together conceptually and linguistically well the agent is minimizing free energy is it though or is it the modeler's generative model of the actual embodied organism so

Somebody who knows that may be able to speak or interpret flexibly and understand that when people are talking about an organism minimizing surprise or the imperative for an organism to minimize surprise, be able to, on one hand, lean into the realism and on the other hand, lean into the instrumentalism.

But neither of those, I guess, is like a standalone...

interpretation or or straightforward implication from any set of formalisms let's go to chapter eight and let's see how it plays out with continuous time all right as it's our um first discussion in eight let's uh

scan through and then we'll come to the questions next week okay so everything flows nothing stands still one of the cherished expressions of process philosophy and dynamicism chapter 8 compliments 7 by continuing the discussion of generative models

Chapter 7 was about discrete time.

Chapter 8 is on continuous state-space models.

And it'll be brought up in multiple ways that continuous time models, continuous perceptual models, and continuous action models are often well-suited for proximate generative models.

Not using the term formally, but just like

dealing most proximally with incoming sensory data and on the action side dealing most proximally with motor actuation turns out that those settings often are continuous in nature like the amount of pressure on a proprio receptor or the amount of volume by an auditory sensor

the sensory sides and on the action side like the angle of an elbow or the amount of force being exerted by a muscle those perceptual and motor settings are really amenable to continuous modeling whereas discrete time models are often amenable to categorical symbolic and discrete settings

section 8.2 and just raise your hand if you have like a want to add something or a question 8.2 from chapter 4 the generative model in continuous time can be written as a pair of stochastic equations determining how states hidden states x generate data observables y so that's the top equation is the data are being generated

by a generator function on x, hidden states, plus a noise term on y, and change in x, x dot, derivative on x, is a function of the current state of x and v, which are slowly varying causes or contexts, plus a noise term on x.

This is a classical formalization.

And in the context of SPM and brain imaging, this is like, why is the fMRI or the EEG measurements?

X are the unobserved neural states, neural activity dynamics.

And there are some sensor noise.

And then the dynamics of neural activity, which again are unobserved, inferred from data,

are a function of current state of those neural activity patterns and slowly varying causes, which might be like an attentional factor or might be an experimental factor.

And more broadly, just this partition into like signal and noise comes up again and again.

Ali?


SPEAKER_01:
I just wanted to point out the second equation is also called

Ito's stochastic differential equation.

So if someone come across that term, that's exactly this second equation of 8.1.

Awesome.


SPEAKER_00:
These stochastic calculi help us do integrals and derivatives on stochastic processes.

Action is absent from equation 8.1.

This is because, as outlined in the recipe, action is part of the generative process, not the generative model.

So this is potentially like saying the consequences of action are part of how things are actually generated, which is to say the generative process.

Even though action selection is a function of a generative model,

the consequences of action in terms of modifying how the world changes are occurring in the generative process um if we write down the dynamics of the real world generative process but of course this is like the dynamics of the map of the real world we would have to include action so we have a very similar equation but u is now replaced

for v and there's bold g and f this is a general formalization with this sort of observable layer caused by hidden states and the change in the hidden states being related to their current state and causal factors so they're going to specify in equation 8.3

a simple version that's compatible with the above and noiseless.

So it's a point attractor.

When x is less than v, the expected rate of x is positive and vice versa.

So this is a set point attractor.

They're going to define a generative process where action will change to fulfill the predictions of 8.3.

so this is an active movement towards a point attractor so that point attractor might reflect a preferred target for the eyes or a preferred location in space for a limb and this is bringing us from this totally general setting

into an active attainment or an active agency exertion that's compatible with those formalizations they're going to take it one level more complex

A more sophisticated model recognizes that forces generated by muscles change velocity, not the position.

So you don't just pick up and move your limb.

You're actually modifying a derivative of position, which is velocity and acceleration.

And so here we start to see a little bit of what could be extended into higher and higher derivatives with the generalized coordinates of motion.

But now F of X and V, which is to say the function that is a function of the current state of the hidden state and the slowly causal varying factors are going to be explicitly written as this.

X1 is the position and X2 is the velocity.

Now this generative model,

is describing Hooke's law so this is a spring movement it's a conservative spring we can write down a generative model that predicts the dynamics that would unfold if there was a spring drawing a limb to a desired location so that's like one level more advanced than the point attractor which just linearly picks up the arm and moves it to the point

Now we have like this Hooke's law spring force drawing and oscillating.

And then we can imagine you can keep on going.

You can have a damping spring and so on.


UNKNOWN:
8.3.


SPEAKER_00:
Continuous time formalizations are well suited to characterization of movements and really just nonlinear dynamical systems wherein discretization of time and space is efficient.

hashtag live stream 52, discretization algorithms and optimization.

Figure 8.1, we're returning to the spinal reflex case from chapter five, descending predictions, getting mashed up with incoming proprioceptive data, resulting in an error residual,

that is modifying the trajectory of the actions in the face space shown here.

X is the position of the limb and V is where it's being drawn to.

X is current position.

V is the target location.

8.1 precision attention and sensory attenuation we could talk more about it but this related to a discussion that we had um in office hours last week about the uh sensory attenuation

here are some of our discussions we talked about why it's relevant to model sensory attenuation which is like the reduction of precision on proprioception in order to facilitate action selection and that's also kind of described here

precision sensory input you just get shut down no movement is executed it's catatonic they're going to move into two classical dynamical systems one with uh predator prey like dynamics lock of volterra and the second with chaotic dynamics lock of volterra

predator prey model, also called a winnerless competition.

Equation 8.6 gives the Lotka-Volterra dynamics.

And here on the top is the winnerless competition amongst plant herbivore and carnivore.

also one could imagine this is like activity patterns in the fmri or eeg with different brain regions and that's exactly how it's been used and here is the phase space this is like three three lines plotted on time axis but we can also think about that as a trajectory in a three-dimensional space so some sort of cube and these there's just one point abc or phc

that are moving in this three-dimensional space on some trajectory.

And then here, if you look at one side or the other of a cube, you can look at how there's something like a slowly drifting limit cycle when you project down to a given axis.

Now they're going to move that log of Volterra, winnerless competition, predator-prey dynamic into sequential action.

Here's how sequential action has been used in eye blink conditioning experiments and in writing generation of sequential information patterns.

And also in the PAR Bookstream 2.1,

was a great discussion by thomas so he said early act inf it was continuous it was generalized filtering and then there was the engagement with action people started thinking about how to get sequential dynamics emergence of sequences from continuous dynamics was a key step towards purposeful planning that's figure 8.3

Then it's like, okay, that's all good for making this kind of semi-regularized scribble, but what if you wanted actual policy selection on which word to write?

That's going to be really hard to bake into a Locke-Volterra model.

So things moved into the explicit discrete state space model with the POMDP formalization.

And that led to planning as inference and all these other features.

And now we're moving back into these hybrid and hierarchical models where continuous state spaces are reintroduced as a low level of hierarchical modeling.

8.2, learning continuous models.

Continuous state spaces are a lot like discrete state spaces.

In discrete state spaces, you're basically taking a sum across a finite number of entities.

In continuous state spaces, you're taking an integral from here to there.

And there are different approximations that facilitate those integrals.

The PMDP formalization of Chapter 7 has largely superseded the use of generalized Locke-Volterra systems in ACT-INF.

but it's still extremely didactic and pedagogical and used in the lower levels of hierarchical models.

Now they're going to move from the Lac of Volterra to the Lorenz attractor system, which has some parameters that are reflective of weather systems.

That's been studied in the context of birdsong.

And they're going to use the Lorenz attractor in the Birdsong case to talk about generalized synchrony.

So generalized synchrony doesn't mean lockstep synchronization.

It means that there are entities whose dynamics are on a manifold.

other words their dynamics are not untethered from each other there's mutual information such that they're co-ambulating on a lower dimensional manifold so the birdsong example has been in multiple papers it's a chaotic attractor that's underlying the emission of birdsong by the two birds

live stream 32 is on coupled chaotic systems so that's where the math are unpacked the most but here we can just see on the right side of figure 8.5 that after learning there's a tightening of the first and the second birds expectations towards a manifold

It's called singing from the same hymn sheet.

Briston's quite fond of that phrasing.

And each bird contributes sections of the song, and there's some interesting connections back to sensory attenuation again.

Section 8.5 goes into hybrid, discrete, and continuous models, which we've basically been talking about multiple times.

So figure 8.6.

First off, note that the G should be connected to the pi.

then note the fractal rake structure we have the rake here figure 4.3 on the bottom continuous time v x x prime double prime and so on but these continuous time models lower half of figure 4.3 are embedded within a discrete time

higher level a b d s t minus one t t plus one so we see both parts of figure 4.3 discrete time on top d b a past present and future and now we're seeing it as a top level nesting continuous time lower level model

and here x derivative of x second derivative on x key difference between discrete time and continuous time models michael


SPEAKER_02:
Yeah, I have a question.

What happens if the embedded time frames are not fitting into the upper frame?

So if, for instance, here the lower parts overlap with the states of the upper part, so you have a time frame and maybe

and chopping up of time into segments, but the segments overlap.

Is there a possibility to model such kinds of things?


SPEAKER_00:
Yeah, a few thoughts.

Good question.

The first is that the continuous time model

doesn't make explicit predictions about the future.

The continuous time model is a lot like a Taylor series approximation.

So although it embodies through the generalized coordinates, which are one, two, and three shown here, but could be up to six or more, it embodies some patterns that extend beyond the present, but the continuous time formalization is actually always real time.

That's one answer.

A more empirical answer is if you have a model where there's like sort of clashing timescales, you're going to end up with multicolinearity in fitting the data to empirical results.

So you'll get a variety of model fitting aberrations because the variance is going to be partitioned in a sort of

a discordant way so some random seed might partition a lot of the variants up here and then another random seed partitions it down here so if there's discordant overlap you're just going to end up with a headache trying to get that model to be useful so no one's going to stop you but that doesn't mean that it's going to be an effective architecture for actual parameterization


SPEAKER_02:
So does it mean we need to squeeze somehow the stuff into these boxes, into these rectangles, embedded rectangles?

There's not really a way to have these overlapping boxes there.


SPEAKER_00:
What's the case that we're thinking about?

Because pretty much here is the movement of time at some click.

And then this could be every second is when the new limb location is chosen.

And then within each second, there's the continuous movement of the limb.


SPEAKER_02:
Yeah, so for instance, they talk about a sentence.

So we have words and sentences, paragraphs, texts, and so on.

But now, if you look at prosodic features, they may overlap with sentence boundaries.

So you wouldn't have a kind of pause structure that fits into the sentence structure.

There could be two different ways of chopping up the sentence in an acoustic way and in terms of words and sentences and so on.


SPEAKER_00:
i think it's a super interesting question which is like this strict nesting like well isocades continuously let's just say and they're recognizing letters okay letters are strictly nested within words words are strictly nested within sentences paragraphs and so on and one might want to go oh and paragraphs are strictly nested within narratives it's like well no not necessarily

there might be a narrative boundaries that are up down left right of the paragraphs so let's see it start by just graphing it how you see it and then let's go from there but in principle it's not going to be addressable but when you actually write it out you may discover again a portfolio of model structures which can be used empirically to differentiate hypotheses

And they're maps.

So maybe it turns out that a simpler model where prosody is sentence delimited is more effective because there's so fewer parameters.

So the Bayesian information criterion favors that model.

But that doesn't mean that prosody is sentence delimited.

It means that a statistical structure with sentence delimited prosody was selected for information theoretic purposes.

map territory but we want to say things about the territory so that's always the challenge we do a regression between ant foraging activity and the humidity and we want to say when it's more humid the ants forage more that seems like the natural thing to say and it might be our motivating question

But it's always hard to remember that it's like, ah, higher values in the humidity measure are correlated with higher foraging activity.

It's like, just say that foraging increases with more humidity.

But that is the departure point of the map territory fallacy.

So,

It's often more resolvable than people want to know in the specifics.

That's the deflationary perspective and less resolvable than we want to know in principle.

Ali?


SPEAKER_01:
And also one reason behind formulating

sorry, the continuous dynamics in terms of Taylor's series is because it also maps onto the Lagrangian formulation of mechanics because one of the reasons Lagrangian mechanics is used instead of the traditional

Newtonian mechanics is to just get rid of time parameters so instead of time they use momentum and also the Lagrangian so we don't need time explicitly defined or described in order to account for the behavior of the trajectories or to describe the behaviors of the trajectories

All we need is this series of successive differentiations in the terms of Taylor series to be able to describe the whole behavior of the system because accounting explicitly for time parameter is much difficult to model than

than describing these dynamical systems in terms of their generalized coordinates, in other words their Lagrangian formulations.

In the path integral formulation of FEP, that's basically the solution proposed by Dalton, Friston, and others to use Lagrangian formulation to describe these systems.

Nice.


SPEAKER_00:
Good point.

Equation 8.8.

is describing the transformation from the discrete target selection of the upper level of this model and the way that that is received by the continuous time model.

So here the discrete time target selection is then received by the continuous time I-saccade model.

that is an experimental setting that has been studied relatively extensively here in the middle of figure 8.7 we see the target selection being modeled and the circadian and here's the calling back to that spring-like movement here's like the acceleration and then the dampening as the target is fixed upon

So isocating has been studied relatively extensively in the context of information-driven continuous modeling.

Box 8.3 is a bit of a side note.

on discretization so a gaussian mixture model or very related is a k means model where one takes a cluster of data points in space and asks how many clusters k fit this scattering of points

You could just have 1k with just a point in the center, and then there's some variance distribution around it, or you could say 2k.

And then you can look at the error profiles to determine an adequate number of clusters.

Somewhere between one cluster and one cluster per data point, there's going to be a k-means value that is statistically optimal.

8.6 in this chapter, which is relatively short and in some ways kind of introducing the continuous time formalism so that we can get to these important topics like hybrid models, they overview applications of continuous time GMs.

This is a huge topic and much has been left out.

See table 8.1.

Synthetic birdsong.

Ocular Motor Delay, Conditioned Reflex, Smooth Pursuit Eye Movement, 2012.

Psychosis, Illusions, 2012.

Cicades, many papers.

Action Observation, Attention, Hybrid Discrete Continuous Models, Self-Organization.

It's a foundation for further exploration.

they focused on movement prediction it simplifies the treatment of motor control we don't need any additional machinery or inverse models and this was also a topic picked up in the skilled performance discussion of hippolyto at all talking about interactionism rather than instructionalism

They talked about the Lotka-Volterra winnerless competition, Predator Prey, and the Lorenza Tractor chaotic system.

In all those cases, it was possible to find this concept of generalized synchrony, which isn't lockstep, but it's like a mutual information or a manifold co-traversal.

and then they introduce the hybrid models which tend to have continuous lower level dynamics and discrete higher level dynamics that's it for today next time we're going to come back to chapter 9 which is going to be talking about model-based data analysis fitting models to data

and next week uh we'll do chapter eight questions and come through it again and preview nine so thank you fellows see you next time
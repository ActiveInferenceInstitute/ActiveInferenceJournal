[
  {
    "start": 0.73,
    "end": 1.43,
    "text": " Greetings, everyone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2.571,
    "end": 5.754,
    "text": "It is October 21st, 2022.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5.794,
    "end": 8.517,
    "text": "We're in textbook group number 50.1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 9.078,
    "end": 13.942,
    "text": "No, I just have a slight typo in my OBS.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 15.324,
    "end": 18.627,
    "text": "It's cohort two and meeting eight.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 19.668,
    "end": 24.052,
    "text": "We're discussing chapter four in our first discussion of chapter four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 26.394,
    "end": 40.983,
    "text": " We're going to, as we did not have any chapter or four specific questions from cohort two, today largely we're going to be reviewing and revising questions that were initially addressed in cohort one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 42.677,
    "end": 52.919,
    "text": " taking a second pass and seeing where we can like add some context and simple or complex or however questions that we can add.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 53.119,
    "end": 68.143,
    "text": "So you can be adding in the chat or in the questions tables directly, or like just unmuting and addressing them so that we can have high quality and quantity",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 69.304,
    "end": 93.992,
    "text": " of a large diversity of questions on the material that people are curious about when they're reviewing the text before we go to the questions table does anyone have any general thoughts or comments on chapter four yes please ollie",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 98.45,
    "end": 106.716,
    "text": " As we discussed in the previous cohort, this chapter is probably the most technical one in the whole book.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 107.377,
    "end": 113.422,
    "text": "So as people go through the formalism here in this chapter,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 115.042,
    "end": 139.891,
    "text": " I think it's quite normal to not fully understand every single equation there, so in case there are some difficulties in getting the content of this chapter, I think some of these difficulties, not all of them, would be unpacked in the later chapters.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 143.414,
    "end": 171.853,
    "text": " you very much totally agreed anyone else want to make a general chapter four comment yeah there's such an interesting rhythm chapter one being largely contextualizing and framing the chapter two three dialectic",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 172.799,
    "end": 198.246,
    "text": " the low road and the high road chapter four with a focus on the generative model and it is a very technical chapter chapter five whiplashing over to neurobiology and less on the fundamental formalisms more on the neurobiological relationships that constitutes the epistemic first half of the book",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 199.793,
    "end": 227.555,
    "text": " second half of the book has kind of a rhyming pattern but less focused on epistemic and more on pragmatic although the pragma of epistem whereas chapter six is focusing more on the recipe for making the model and less of the contextualizing then there's another chapter pair seven and eight discrete and continuous time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 228.618,
    "end": 237.403,
    "text": " And then chapter nine, which is where we're heading into in just 50 minutes in the first cohort with model-based data analysis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 237.863,
    "end": 243.606,
    "text": "And then chapter 10 as sort of like the symmetry of chapter one and the three appendices.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 250.69,
    "end": 250.91,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 251.63,
    "end": 257.874,
    "text": "If anyone has comments, of course, just like raise your hand or speak up or put it in the chat or however.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 260.842,
    "end": 266.405,
    "text": " I'll un-upvote these so we can upvote them when we've gone through them today.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 268.726,
    "end": 268.966,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 276.209,
    "end": 282.112,
    "text": "Well, let's scroll through the chapter before we go to the questions just to get a view of it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 286.018,
    "end": 290.562,
    "text": " This chapter complements the preceding chapter's conceptual treatment of Act-Inf with a more formal treatment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 291.383,
    "end": 298.749,
    "text": "It sets out the relationship between free energy Bayesian inference, the form of generative models, and the dynamics obtained from minimizing free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 299.309,
    "end": 301.411,
    "text": "The key focus is on how time is represented.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 304.053,
    "end": 311.68,
    "text": "4.2 Bayesian Inference to Free Energy Equation 401 recalls Bayes' Theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 314.431,
    "end": 326.514,
    "text": " Figure 4.1 describes Jensen inequality, which is related to the tonicity of the natural log or just the logarithm function in general.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 328.274,
    "end": 331.615,
    "text": "And it's a pretty fundamental point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 332.795,
    "end": 343.938,
    "text": "And it speaks to the advantage of using, for example, log probabilities and the different kinds of operations that are possible on logged values",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 352.967,
    "end": 367.71,
    "text": " Here is an application of Jensen's inequality to derive a free energy value as a bounded, as a bounding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 382.253,
    "end": 386.096,
    "text": " rearrangements amongst Bayes' theorem and free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 392.66,
    "end": 393.381,
    "text": "Generative models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 394.421,
    "end": 396.363,
    "text": "To calculate the free energy, we need three things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 396.663,
    "end": 402.187,
    "text": "Data, a family of variational distributions, and a generative model comprising a prior and a likelihood.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 404.548,
    "end": 407.531,
    "text": "This section has two types of generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 408.41,
    "end": 410.631,
    "text": " The first is dealing with categorical variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 411.291,
    "end": 413.212,
    "text": "The second deals with continuous variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 418.575,
    "end": 428.519,
    "text": "Figure 4.2 is helping gain some familiarity with like nodes and edges in a Bayesian graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 429.379,
    "end": 432.241,
    "text": "So here in a Bayes graph,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 433.337,
    "end": 459.829,
    "text": " nodes are variables circular nodes are variables and edges which are like labeled with a with a box reflect their relationships so one can imagine and we'll return to this in the questions these map on to situations where like there's a hidden cause and an observation you know temperature in the room thermometer reading",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 461.465,
    "end": 474.356,
    "text": " two things that influence one outcome, one hidden thing that influences two outcomes, a hierarchical model where something changes something that results in an outcome, and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 475.897,
    "end": 489.569,
    "text": "And then the boxes are labeled according to the way that we might see that in a more formulaic conditional likelihood expression.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 490.643,
    "end": 516.506,
    "text": " vertical line means conditional upon so like box two is the probability distribution of y conditioned upon x because of the directedness of this arrow figure four three is a very central figure in the textbook and it",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 519.375,
    "end": 545.384,
    "text": " sets up the similarities and differences largely between the discrete time and the continuous time formulations of active inference generative models in terms of partially observable markov decision processes through discrete time and for the um more uh taylor series expansion like approach used for continuous time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 546.363,
    "end": 550.486,
    "text": " Does anyone want to give a thought or an overview on what is happening in Figure 4-3?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 565.098,
    "end": 565.739,
    "text": "Yes, Ali.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 566.479,
    "end": 567.08,
    "text": "And then anyone else?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 569.926,
    "end": 582.576,
    "text": " Well, the top figure is what happens when we want to model discrete time event.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 583.276,
    "end": 592.023,
    "text": "And the bottom one is more conducive to modeling continuous time phenomena.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 593.023,
    "end": 615.499,
    "text": " So as we see, they're quite parallel and the only difference or at least the most important difference here is the role that policies play in the discrete time case and the v play in the continuous time case, the policies denoted with the pi and the top figure",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 616.259,
    "end": 628.564,
    "text": " And the top figure relates to the POMDP approach to modeling the discrete time phenomena, as we'll see at length in Chapter 7.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 629.265,
    "end": 634.907,
    "text": "And the bottom one is unpacked in Chapter 8, I guess.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 634.927,
    "end": 635.407,
    "text": "Excellent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 645.72,
    "end": 658.85,
    "text": " Again, anyone can raise their hand and go for it, but just to kind of describe what the nodes and edges are here, we'll be thinking about like the temperature in the room is the hidden state and the thermometer is the observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 659.911,
    "end": 665.796,
    "text": "So in the discrete time formulation, this is the temperature in the room through time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 666.196,
    "end": 667.037,
    "text": "It's the hidden state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 667.617,
    "end": 668.838,
    "text": "It's in the generative process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 670.159,
    "end": 671.92,
    "text": "Then there's an A matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 673.103,
    "end": 702.165,
    "text": " here labeled as two which is the distribution of observations on the thermometer conditioned upon the temperature at that time so the a matrix can be used as a recognition density to do inference on the hidden state given the observation or it can be used in a generative capacity given a temperature to emit distributions of thermometer observations the three corresponds to the b matrix",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 703.271,
    "end": 710.116,
    "text": " which is the distribution of the state at the next time conditioned upon the state at the given time and the policy pi.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 711.177,
    "end": 715.74,
    "text": "So three or B describes how the temperature is changing through time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 716.44,
    "end": 724.066,
    "text": "So this kind of through line of hidden state and then a B matrix or three, and then a hidden state, this is something changing through time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 724.726,
    "end": 732.192,
    "text": "And then pi is policy, which is a function of the decision-making of the entity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 733.252,
    "end": 756.749,
    "text": " it influences how states change through time that's the discrete time formulation time point negative one zero and one or however um and often you see a d or a prior which is uh how the first hidden state value gets initialized that's the discrete time formulation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 757.987,
    "end": 766.134,
    "text": " In the continuous time formulation, they're being laid out side by side because the isomorphy of the models is apparent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 766.974,
    "end": 769.957,
    "text": "They have the same topological structure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 771.858,
    "end": 774.32,
    "text": "However, there are some really important differences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 775.561,
    "end": 786.01,
    "text": "Rather than describing how a hidden state changes through time at explicitly demarcated time points, like minute 12, minute 13, minute 14,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 788.991,
    "end": 817.134,
    "text": " um what's happening here is that the the value of the hidden state at the focal time is still in an a matrix relationship with observables so the probability of y given x is exactly analogous to this one of the observations conditioned on the hidden state however taking the place of the b matrix",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 818.186,
    "end": 839.224,
    "text": " three above here, which describes how hidden states change their time in the discrete time formulation, is actually a edge that describes the derivative of x conditioned upon x and v, which relates to slowly changing causes in the world, analogous to policy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 840.325,
    "end": 845.349,
    "text": "And so this is actually not doing explicit prediction of future explicit time points.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 846.687,
    "end": 867.397,
    "text": " but rather it's doing something like a Taylor series expansion, where as you take more and more derivatives further and further away from a focal point, you gain better and better approximation through power series expansion from a focal point using higher and higher derivatives.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 875.734,
    "end": 899.246,
    "text": " here's like an example here's a sine wave and the first derivative just gives you this locally linearizable range but there's a well-defined computational complexity for a taylor series you can say the zeroth order is just zero and the first order just a linear regression",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 900.373,
    "end": 902.855,
    "text": " And then there's just a quadratic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 903.035,
    "end": 907.017,
    "text": "And then there's just a third power relationship and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 907.697,
    "end": 918.864,
    "text": "And so as you get more and more getting into the blue and the purple and so on, you get better and better fit and you extend the range of fit further and further out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 919.985,
    "end": 921.666,
    "text": "And it always gets strictly better.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 926.243,
    "end": 936.831,
    "text": " This is going to come back in chapter 7 and 8, which are going to focus more directly on the discrete and continuous time formulations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 937.832,
    "end": 945.478,
    "text": "But for here in 4, they're just being introduced as different ways that we can have a generative model in active.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 947.98,
    "end": 952.403,
    "text": "So now, section 4.4 can come to the top of 4.3.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 956.963,
    "end": 983.502,
    "text": " are some formalisms related to categorical distributions that are compatible with partially observable markov decision processes here's an expected free energy g that describes some different partitionings on as a function of policies",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 984.864,
    "end": 1009.177,
    "text": " evaluating policies based upon their impact on the pomdp four key ingredients for the discrete time generative model a b c d then moving to continuous time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1010.339,
    "end": 1017.124,
    "text": " But first, there is kind of a technical interlude on Markov blankets.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1017.384,
    "end": 1022.448,
    "text": "Previously, there was a box defining the Markov blanket, and here it's discussed in the context of message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1022.648,
    "end": 1025.19,
    "text": "Yes, Giuseppe, anywhere you want to call attention to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1025.21,
    "end": 1030.513,
    "text": "Thank you, Dan.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1031.114,
    "end": 1032.995,
    "text": "Going back to the figure 4.3,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1035.768,
    "end": 1051.84,
    "text": " So in looking at the parallel between the top and the bottom figure, so in the top figure, pi, that is the policy, is obviously something that is selected by the agent, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1055.022,
    "end": 1058.665,
    "text": "What about nu in the Greek letter that is",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1063.976,
    "end": 1088.42,
    "text": " that is parallel to pi in the bottom figure new is it because it said slowly slowly varying causes in in the environment or something like that but is that something emitted or selected by the agent or is it something because",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1094.875,
    "end": 1096.055,
    "text": " Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1096.956,
    "end": 1097.216,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1097.256,
    "end": 1098.476,
    "text": "Does anyone have a thought on this?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1099.217,
    "end": 1112.321,
    "text": "The distinction is just between discrete and continuous is not between like, something that that it's autonomous, autonomously selected by the agent or something that is happening in the environment, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1117.123,
    "end": 1117.323,
    "text": "Ali,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1122.011,
    "end": 1139.318,
    "text": " Actually, in continuous time situation, the new here can also be related to the non-cognitive aspects of the system, such as the attractors of the system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1139.938,
    "end": 1146.18,
    "text": " So that's why it's described in terms of the slowly varying causes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1146.98,
    "end": 1159.243,
    "text": "For instance, in chapter 8, we will see that, well, we have some static variables according to which the system behaves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1159.984,
    "end": 1167.506,
    "text": "So that's not exactly something that the system will choose per se.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1168.827,
    "end": 1177.454,
    "text": " It's some variables or some states towards which the system will approach.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1177.714,
    "end": 1181.717,
    "text": "So that's why they termed it in that way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1182.258,
    "end": 1195.789,
    "text": "So, for instance, when applying to movements, that can be some sort of anatomical constraint of the movement, something like that?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1196.969,
    "end": 1222.154,
    "text": " exactly yeah i see okay thank you yeah thank you for um sharing that ali it reminds me of the question that came later action is part of in this is chapter eight action is part of the generative process not the generative model and so the actual",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1226.189,
    "end": 1248.281,
    "text": " interpretation of slowly varying causes and the mapping of slowly varying causes to real world phenomena is one interesting area another general point",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1250.137,
    "end": 1260.6,
    "text": " is that if we look to the figures, we'll see that the first figure is the action perception loop.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1262.52,
    "end": 1276.344,
    "text": "And initially, there's some usage of the Bayes graph in terms of elaboration on the action perception loop, and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1278.712,
    "end": 1289.298,
    "text": " But when we get to chapter four and the generative model, we don't see the action perception loop represented the same way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1291.439,
    "end": 1303.866,
    "text": "And so it's an interesting question to ask where these variables map onto, for example, the entity model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1304.607,
    "end": 1307.008,
    "text": "This isn't simply an entity model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1313.885,
    "end": 1318.626,
    "text": " Okay, so just completing our quick scan before we go to the questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1320.447,
    "end": 1321.967,
    "text": "Message passing and Markov blankets.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1323.128,
    "end": 1330.29,
    "text": "This is definitely an area that we could learn a lot more about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1331.87,
    "end": 1341.973,
    "text": "And this is also an area where software packages exist with the work of the BIAS lab and their Forney package.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1351.01,
    "end": 1379.16,
    "text": " okay figure four four bayesian message passing does anyone want to describe this figure just one thought well the overall thought is is this supposed to be something that we're supposed to in detail",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1380.382,
    "end": 1403.657,
    "text": " understand or is it more like an illustrative representation i was thinking exactly the same it's like here's the kind of food you could make or is this like here's how you make this one dish um there are uh",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1407.785,
    "end": 1413.169,
    "text": " Also, it's not read from left to right per se at the level of the figure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1413.829,
    "end": 1415.27,
    "text": "The caption begins with a right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1416.591,
    "end": 1423.836,
    "text": "So starting with the left, the scheme could be expanded hierarchically, collapsing over time steps and policies for simplicity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1424.577,
    "end": 1430.121,
    "text": "So just looking at the left one, here the subscripts have I on the bottom.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1430.901,
    "end": 1432.743,
    "text": "Then the subscripts have I plus one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1433.817,
    "end": 1437.779,
    "text": " but you see the same squiggle, pi, s, epsilon.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1438.639,
    "end": 1462.831,
    "text": "So this is showing that within the context of a hierarchical model, you can have recurring patterns of topological relationships of a Bayes graph to enable, for example, hierarchical predictive processing, ACT-INF, Bayesian action selection, et cetera models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1464.059,
    "end": 1468.12,
    "text": " And then that is collapsed over time steps and policies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1469.78,
    "end": 1470.64,
    "text": "So what's happening here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1471.541,
    "end": 1475.701,
    "text": "So this is going to be unpacking within one layer on the right side.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1477.082,
    "end": 1489.904,
    "text": "And we see like, so here there's no superscript, but we can just take that to be like pi i. So this is going to be this quartet being expanded out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1490.344,
    "end": 1493.465,
    "text": "So here's O tilde, like O through time coming in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1494.801,
    "end": 1499.685,
    "text": " And here's like the actual discrete time observations coming in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1502.227,
    "end": 1512.136,
    "text": "Here's epsilon and S in a predictive processing capacity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1517.06,
    "end": 1522.084,
    "text": "And these are illustrating with labeled edges, some of them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1524.062,
    "end": 1550.769,
    "text": " relationships between these different variables squiggle in this case that's funny when you search for the squiggle it finds the it finds the other sigma as well squiggle sub pi t",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1553.047,
    "end": 1557.188,
    "text": " is two natural logs being subtracted.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1557.848,
    "end": 1559.508,
    "text": "A natural log subtracting another natural log.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1559.809,
    "end": 1564.51,
    "text": "So what happens when you subtract, what's log A minus log B?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1565.89,
    "end": 1580.033,
    "text": "It's the same as log A divided by B. Log, open parenthesis, A divided by B. So this is computing a ratio between the observation and the preference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1582.867,
    "end": 1610.176,
    "text": " if that is one then the incoming observations are exactly the preferences the preference distribution if there's a surplus then the observation is higher if there's a deficit below one then it means that it's lower i hope that is correct way to say it um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1611.556,
    "end": 1615.378,
    "text": " So here's message passing and relationships amongst variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1616.259,
    "end": 1626.244,
    "text": "And we see that kind of like with an epsilon intermediating, we see the S relating to the O at a given time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1626.764,
    "end": 1628.545,
    "text": "So this is a lot like the A matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1634.048,
    "end": 1635.329,
    "text": "And here's the role of policy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1642.225,
    "end": 1644.686,
    "text": " Now we're in the continuous time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1644.706,
    "end": 1644.686,
    "text": "4.5."
  },
  {
    "start": 1645.506,
    "end": 1647.367,
    "text": "Ali, please.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1650.668,
    "end": 1662.152,
    "text": "I think this figure can be usefully compared with figure 7.12 because I believe the figure we'll see in chapter 7, 7.12, I guess.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1662.172,
    "end": 1663.593,
    "text": "Yes, that's it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1663.613,
    "end": 1663.833,
    "text": "Is the...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1670.335,
    "end": 1685.601,
    "text": " I mean, the other interpretation of this figure from chapter four in terms of its hierarchical relationships, but from a graphical probabilistic modeling point of view.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1686.142,
    "end": 1694.505,
    "text": "And the other thing is that, I mean, final position sigma, I don't think,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1695.305,
    "end": 1710.228,
    "text": " It's defined explicitly in the text, but it's been defined as the expected prediction error in the step-by-step tutorial paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1710.328,
    "end": 1718.97,
    "text": "So yes, that's also, I wanted to mention that definition as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1719.91,
    "end": 1720.23,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1724.775,
    "end": 1747.556,
    "text": " this step-by-step tutorial which is also a four-part model stream is very very useful in some ways it is almost textbook length whereas in chapter four there was first the general discussion of base graphs",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1748.396,
    "end": 1761.938,
    "text": " like again, one cause, one observation, one cause, two observations, two to one, one to one to one, and then instantly into the juxtaposition between discrete and continuous time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1763.099,
    "end": 1777.381,
    "text": "As we've looked through today, in contrast, the approach taken in step-by-step first off is to only focus on the discrete time formulation, but then they walk through a sequence",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1778.829,
    "end": 1794.516,
    "text": " of graphs and connect them to like increasing levels of complexity building towards the one that we see in 4.3 it's a little later than one might expect in the paper",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1807.608,
    "end": 1808.509,
    "text": " Sorry about that, here it is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1811.27,
    "end": 1823.579,
    "text": "Starting with just a simple graph that is exactly analogous to the one on the top left.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1824.359,
    "end": 1826.26,
    "text": "So they start with a static perception.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1829.022,
    "end": 1834.506,
    "text": "Then in the step-by-step, they introduce this kind of E motif,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1836.84,
    "end": 1850.193,
    "text": " by showing a prior and three time points, two changes, observations mediated by the A. This is corresponding to the music listening example that's going to come up later in the textbook.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1852.496,
    "end": 1859.863,
    "text": "Then policy as a way to influence state transition matrixes is introduced on the top right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1861.323,
    "end": 1887.449,
    "text": " and free energy as an objective function guided by preferences to do policy selection as inference and then finally on the bottom right there is some precision on policies which is just to bring in another variable beta and gamma which have like a inverse relationship to each other so that you can talk about like um high precision as",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1889.835,
    "end": 1911.916,
    "text": " low ambiguity and vice versa so it's a different ordering of talking about generative models okay we're in the continuous time branch i'm just copying a link that ollie sent",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1917.908,
    "end": 1939.128,
    "text": " Sorry, I just wanted to mention that the continuous time counterpart for that step-by-step paper might be this one from 2017 by Raphael Bogach, because in this paper it almost exclusively deals with the continuous time active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1952.796,
    "end": 1952.996,
    "text": " Great.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1959.717,
    "end": 1961.058,
    "text": "Generalized coordinates of motion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1962.918,
    "end": 1965.739,
    "text": "This is again bringing back this idea of the Taylor series.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1966.139,
    "end": 1981.762,
    "text": "If this black line is the function that has to be approximated, if you only have one parameter to approximate it and you want to do it at this line, your first is just the actual value of the function at t equals zero, tau equals zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1982.751,
    "end": 1984.212,
    "text": " then the first derivative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1985.933,
    "end": 1997.362,
    "text": "Value at the time X of O and the first derivative, then adding in the second derivative and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1997.802,
    "end": 2006.989,
    "text": "So as you add more and more and more derivatives, you'll get better and better approximation to arbitrary functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2008.41,
    "end": 2010.772,
    "text": "And you'll get that with a totally known form",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2012.281,
    "end": 2018.644,
    "text": " that can be represented in a vector as the generalized coordinates of motion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2019.524,
    "end": 2028.028,
    "text": "And in physics, it's used in engineering, it's used in PID control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2028.768,
    "end": 2040.373,
    "text": "This has come up in many live streams and discussions because you can say whatever function I'm approximating, I want the first five derivatives.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2041.428,
    "end": 2046.85,
    "text": " And then you know that your data structure is only gonna have five coefficients, for example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2050.031,
    "end": 2068.377,
    "text": "And they've joked many times, like six generalized coordinates are sufficient, six layers of the brain cortex regions, though in some ways that's kind of, that makes it seem like the six layers of the cortex are like this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2070.328,
    "end": 2098.415,
    "text": " actually the cortical column is not like little six floors where each one is its own predictive loop actually the layers are connected differently more formalisms in the continuous time setting with generalized coordinates of motion tilde over a variable is representing trajectories",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2103.032,
    "end": 2111.177,
    "text": " One footnote is that this is still in the state-based base of Bayesian mechanics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2112.418,
    "end": 2116.36,
    "text": "Ali or anyone else, please correct if this is not the case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2117.121,
    "end": 2131.51,
    "text": "So the tilde here is a summary notation helping us represent trajectories of states, which is moving us towards the path formulation of Bayesian mechanics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2132.486,
    "end": 2135.288,
    "text": " but is not the same as a true path formulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2138.009,
    "end": 2139.31,
    "text": "Hope that is fair to say.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2144.073,
    "end": 2145.974,
    "text": "The Laplace approximation?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2147.475,
    "end": 2148.316,
    "text": "Yes, Giuseppe, please.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2149.416,
    "end": 2153.839,
    "text": "Yeah, so in the tilde formalism now,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2154.343,
    "end": 2156.684,
    "text": " when we are talking about generalized coordinates.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2157.345,
    "end": 2171.252,
    "text": "If we were talking about standard coordinates, X tilde would be X at time T comma X at time T plus one, X at time T plus two, et cetera.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2171.792,
    "end": 2178.896,
    "text": "Now in the generalized coordinates, the X tilde would be X comma T plus one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2180.097,
    "end": 2184.599,
    "text": " first derivative of x, comma, second derivative of x. Is that correct?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2185.6,
    "end": 2185.82,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2187.821,
    "end": 2203.267,
    "text": "So O tilde with just a location coordinate would be either a list or a vector, depending on whether you want to think about it in terms of a list with comma or in terms of a vector going down through time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2204.068,
    "end": 2204.448,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2205.068,
    "end": 2207.209,
    "text": "Then with generalized coordinates,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2212.122,
    "end": 2212.923,
    "text": " it is a matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2214.444,
    "end": 2217.666,
    "text": "So just it could be represented differently.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2217.987,
    "end": 2222.871,
    "text": "But here, let's just say that we're going to be working with it as a list, like you said.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2224.452,
    "end": 2230.837,
    "text": "So in this case, columns correspond to the list above.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2232.138,
    "end": 2233.359,
    "text": "Those are the time points.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2234.98,
    "end": 2239.584,
    "text": "rows going down are the generalized coordinates of motions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2240.908,
    "end": 2260.048,
    "text": " so for each time point one two three four five we have a vector which is describing the location the first derivative the second derivative and so on i see i see so there would be six rows cool if we had six generalized coordinates of motion",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2267.842,
    "end": 2281.331,
    "text": " And this is used and discussed in Livestream 26, particularly, and several other places.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2282.452,
    "end": 2286.014,
    "text": "Generalized coordinates are like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2289.741,
    "end": 2298.703,
    "text": " They're kind of not like a trick or anything to get dynamics into a static representation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2300.384,
    "end": 2304.505,
    "text": "Like if you just take a picture of a car, you can't tell if it's moving or not.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2305.525,
    "end": 2316.288,
    "text": "But if you said this is a generalized picture of a car, it's at 12 on the number line and its velocity is 10 this way,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2317.37,
    "end": 2337.289,
    "text": " and its acceleration it's slowing down and it's going to the right and the way that it's slowing down is speeding up then you can start to say a lot more about the next steps of the car exactly and that is representable in a static um snapshot ollie",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2341.975,
    "end": 2357.89,
    "text": " Another way to looking at the generalized coordinate is to see it as a dimension reduction technique, because when we want to describe as a very simple case,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2359.291,
    "end": 2381.199,
    "text": " a trajectory in a cartesian coordinates well we'll definitely need at least for example in a two-dimensional trajectory we'll need two dimensions but if we look at it from a generalized coordinates perspective we can just put it into the Riemann manifold",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2381.759,
    "end": 2390.846,
    "text": " and jettison one of the dimensions which encapsulates that trajectory through a specific time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2391.346,
    "end": 2405.536,
    "text": "So one of the dimensions would be reduced, and then we can just look at the trajectory directly, look at the behavior of the trajectory directly from the consecutive derivatives we take.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2405.936,
    "end": 2410.119,
    "text": "So that's a dimension reduction technique as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2413.689,
    "end": 2430.528,
    "text": " next point yeah when you said uh you said on the romanian romanian romanian manifold you said can you",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2432.421,
    "end": 2433.461,
    "text": " Can you explain that?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2434.102,
    "end": 2452.467,
    "text": "Yeah, because in the Poincar\u00e9 or Riemannian manifold, we're dealing with not exactly the Cartesian coordinates, but it's just the state space defined as these kind of generalized coordinates.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2453.568,
    "end": 2475.946,
    "text": " So it is already a one-dimension reduced coordinate, so to speak So that's the whole point behind encompassing the state space in that Riemannian manifold, not in the Cartesian coordinates",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2476.807,
    "end": 2491.228,
    "text": " because Cartesian coordinates need some global coordinates to define the states of the space, but in generalized coordinates it's more like a kind of localized coordinates, not global ones.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2494.324,
    "end": 2523.187,
    "text": " yes this is very well i don't know i don't know much about the romanian or the punker manifold so we also have some readings to to do to understand that but yeah this is very deep and powerful um analytical insight one other way to think about generalized coordinates as a dimensionality reduction technique from a more computational applied perspective is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2524.235,
    "end": 2546.423,
    "text": " if you want five coordinates of motion you know exactly the size of that data object if you say i'm going to consider 20 time points and five generalized coordinates of motion you know exactly the shape of that parameter or that variable now whether that captures whether that manifold",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2547.731,
    "end": 2557.813,
    "text": " or subspace captures 10 or 90 or 100% of the variance in the original space is situational.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2558.794,
    "end": 2570.597,
    "text": "But you will capture some variance, just like every principal component analysis will capture some variance in the space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2572.837,
    "end": 2575.658,
    "text": "And whether it captures enough is situational, but",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2576.584,
    "end": 2606.083,
    "text": " you know you get some and you know that you've reduced the dimensionality to a well-defined amount which in many cases is um essential versus having like an unbounded number of of unbounded size um the motor reflex case has been especially studied from the continuous time perspective",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2607.045,
    "end": 2609.085,
    "text": " That's going to come up in the second half of the book.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2609.486,
    "end": 2614.587,
    "text": "It comes up in a lot of different other cases and in chapter five, motor reflexes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2618.527,
    "end": 2622.008,
    "text": "Box four three describes the Laplace approximation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2623.108,
    "end": 2634.351,
    "text": "I'll just note again as frequently that last names don't really reduce uncertainty about what something is talking about much of the time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2635.29,
    "end": 2647.773,
    "text": " And so there'd be ways to describe some of these things that wouldn't rely as much on specific last names because there's like a many to many mapping between the technique and different last names and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2648.193,
    "end": 2661.097,
    "text": "But just working with what we have, Laplace approximation is a quadratic approximator that is an expansion around the mode of a distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2662.318,
    "end": 2671.05,
    "text": " So the procedure for generating a Laplacian approximation is you find the mode of a distribution, which is the tallest point on the distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2672.391,
    "end": 2678.279,
    "text": "Then an upside down parabola is like a fit to that mode.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2679.127,
    "end": 2685.472,
    "text": " The tip of the parabola is going to be the mode of the distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2686.553,
    "end": 2700.065,
    "text": "The width of the parabola is going to reflect a second parameter, a variance parameter that captures as much as possible of the original distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2703.808,
    "end": 2705.809,
    "text": "So this is kind of like...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2707.154,
    "end": 2721.279,
    "text": " A good thing about Laplace approximation is that no matter what shape the original distribution has, you know your Laplace approximation is only going to need two parameters, the mode and the variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2721.939,
    "end": 2728.722,
    "text": "And so in that way, it's a lot like a Gaussian approximator, although the Gaussian function is different than the quadratic approximator.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2730.922,
    "end": 2734.844,
    "text": "They're similar because they both have two parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2735.831,
    "end": 2740.633,
    "text": " the location of the midpoint and how wide the function should be stretched.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2741.573,
    "end": 2761.22,
    "text": "So for empirical distributions, which might not fit cleanly to any specific analytical family, this can be very helpful because again, you know that some density of the target distribution will be captured.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2762.182,
    "end": 2777.525,
    "text": " Now, if you're dealing with like a super tricky distribution where there's four humps, all of very similar height, then the Laplace approximation is gonna get tricked because it's only gonna capture like at best 25% of the variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2778.226,
    "end": 2782.907,
    "text": "And it's gonna, you're gonna be very much like unaware of these other three big peaks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2785.267,
    "end": 2789.328,
    "text": "However, four distributions that have a central tendency",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2790.967,
    "end": 2791.788,
    "text": " broadly speaking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2792.588,
    "end": 2795.07,
    "text": "The Laplace approximation does extremely well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2796.572,
    "end": 2809.723,
    "text": "Some of these subtleties between like Laplace approximation, variational Bayes and non-parametric bootstrap Bayes sampling techniques are explored in the SPM textbook, especially.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2816.348,
    "end": 2816.829,
    "text": "All of this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2818.348,
    "end": 2839.062,
    "text": " was still inferential but not action including and that is very analogous to the approach taken in the paper discussed in uh live stream 43 which is on predictive processing itself predictive coding a theoretical experimental review",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2840.427,
    "end": 2845.67,
    "text": " In this one, 80% or something of the paper is about hierarchical predictive processing architectures.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2846.39,
    "end": 2849.872,
    "text": "And then they go, all right, now we're just going to slip in action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2852.653,
    "end": 2854.454,
    "text": "And it turns out to play very well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2858.536,
    "end": 2863.539,
    "text": "Here we see a figure that's kind of resonant with this one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2865.16,
    "end": 2866.841,
    "text": "We have a discrete time formulation here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2867.827,
    "end": 2873.591,
    "text": " We know that because we see an explicit time variable and a time minus one and time plus one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2874.532,
    "end": 2881.637,
    "text": "So that's how one knows that they're dealing with a discrete time formulation, even if the variables were called different things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2884.379,
    "end": 2892.144,
    "text": "In contrast, we see a hierarchical architecture again, but",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2893.829,
    "end": 2920.867,
    "text": " everything has been transposed into the notation of the continuous time where we saw like Y coming in whereas we saw O coming in here and that was what we saw in four three O coming in to discrete time Y coming in to continuous time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2925.071,
    "end": 2951.942,
    "text": " so this figure four six is like related to the previous message passing figure and then there's a summary so chapter four as Ali mentioned at the beginning is relatively technical there's a lot of techniques and notation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2955.109,
    "end": 2982.465,
    "text": " and forays into more general topics ranging from Jensen's inequality at the beginning of the chapter to generalized coordinates of motion message passing in the context of Markov blankets evidence lower bound and free energy calculations and all of that is being done in this side by side way between discrete and continuous time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2985.41,
    "end": 2992.695,
    "text": " And that is broadly what chapter four describes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2993.576,
    "end": 3000.301,
    "text": "All of that is being done because we're exploring generative models in active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3004.524,
    "end": 3013.21,
    "text": "Okay, in our final minutes, what are some thoughts or ideas that people have that we can come to next week?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3024.014,
    "end": 3024.294,
    "text": " Yvonne?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3034.877,
    "end": 3035.777,
    "text": "I'm fast now.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3048.5,
    "end": 3050.06,
    "text": "What does it mean to understand Chapter 4?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3050.941,
    "end": 3052.581,
    "text": "What are we looking to get out of Chapter 4?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3071.633,
    "end": 3084.146,
    "text": " I think a good understanding of the discrete and continuous time distinction is, in my opinion, the essence of what is in chapter four.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3085.828,
    "end": 3091.614,
    "text": "But I would like to understand more about the message passing, because I don't really get it all the way.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3093.914,
    "end": 3103.2,
    "text": " as I would also love to pick apart and understand the larger paper on message passing by Friston and some colleagues that I don't remember.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3103.22,
    "end": 3104.861,
    "text": "Yeah.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3105.762,
    "end": 3112.567,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3112.627,
    "end": 3121.873,
    "text": "For me, the greatest challenge is that even though you're beginning to grasp the theoretical",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3122.728,
    "end": 3139.265,
    "text": " uh details the the moving from there to actually implementing something is is still uh still quite difficult um and uh you know freestone says uh quotes",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3140.006,
    "end": 3154.681,
    "text": " a lot of time Feynman saying you can only understand when you start when you can do it but still it's a that passage in active inference is still quite challenging for me",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3156.546,
    "end": 3162.09,
    "text": " even after having read the whole book and a few of the other papers.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3163.671,
    "end": 3179.043,
    "text": "So this is the, even though you understand this chapter four, which has a lot of the theory, but still going from there to implementing a model, it's not easy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3179.063,
    "end": 3182.345,
    "text": "I don't know if there's something that affects everybody.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3182.546,
    "end": 3182.926,
    "text": "It does.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3183.976,
    "end": 3209.175,
    "text": " affect me for sure well certainly it does it's the fractal state of play at this moment that's why people aren't just like blasting out active models because there's like a lot of uh trails to blaze and also just as always compare it with a linear regression you could read a 100 500 10 000 page textbook on linear aggression",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3213.297,
    "end": 3219.579,
    "text": " And then you'd sit down in front of your programming language and you may not specifically know how to quote, apply linear regression.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3220.899,
    "end": 3231.522,
    "text": "Linear regression has had a hundred years of empirically focused development on data input, output, visualization, training, model adequacy, model selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3232.502,
    "end": 3241.544,
    "text": "There are just whole areas of applied statistics that haven't turned their attention to this area, which is why we're doing it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3243.449,
    "end": 3266.572,
    "text": " so the most ancient comparator is linear regression then to get one step closer to seeing what the theory practice synthesis looks like the spm textbook documentation and literature help us see what it could look like and then it'll have aspects of both of those and some novelties",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3272.717,
    "end": 3279.549,
    "text": " very well thank you all for this fun meeting thank you see you next week thank you bye-bye",
    "speaker": "SPEAKER_00"
  }
]
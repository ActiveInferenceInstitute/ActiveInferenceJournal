SPEAKER_03:
hi it's february 8th 2023 we're in meeting 15 of the act inf textbook group we're in our second discussion in cohort two on chapter six so

Last week, we looked at a few questions.

And in this modeling chapter, there's many questions and topics we can come to.

So does anybody want to just raise a question or a thought on six?

Otherwise, we will look through questions and see what we can do.


SPEAKER_02:
Yeah, well, I've put a few questions there that came to my mind.

Great.


SPEAKER_03:
Which one would you like to start with?


SPEAKER_02:
So we already talked about this last time, this figure 6.1 on page 110, I believe it is.

And where is it?

page 108.

And my question was this vertical connection there between active states and sensory states.

And I was not able to really understand why we would need to model this.

And I've put this into the question there under chapter 6 when I read out that question.

So how is the mutual interaction between active states and sensory states meant?

Can they mutually change their states without impacting neither internal nor external states?

So that means they cycle around.

If yes, what is it supposed to model, for instance,

put their cycle around 1,000 times, mutually modifying their states, and then eventually arrive at a state where they change the external or internal states somehow.

So means my question is, if they mutually

are able to change inside these Markov blankets the states without any trace in the external or internal states, then they can catapult or manipulate themselves to come into a situation where suddenly they produce a trace.

And if this is allowed, then I'm wondering, what are we supposed to model?

with such processes.

So because, I mean, you can then explain any arbitrary behavior.

And is it Turing complete, this kind of machine that we are then trying to implement there?

And basically, doesn't it undermine explainability of behavior?

So I guess you got my question, but maybe I'm understanding something wrong there.


SPEAKER_03:
Anyone else want to give?

Yes, please, Ali, and then go on.


SPEAKER_00:
I believe those bidirectional paths between active states and sensory states are simply there for the model to be more comprehensive and

They basically refer to something like reflex arcs, which don't necessarily need to pass through cortical paths.

So that's something much more directly manipulated.

But of course, those direct paths between active states and sensory states could be

removed from the diagram and we can talk about bypassing internal states or external states when the reflex arcs happen.

But I believe those direct paths are there to account for those kinds of intrinsic or somehow those

the behaviors akin to reflex arcs.

Thanks, Salih.


SPEAKER_02:
But isn't this then the question of what you define to be internal states?

If you say the internal states only start up from a certain point above the spine and below, it's no more internal.

then it's an arbitrary decision, right?

So you could just also say that everything that happens in the hypothalamus is not part of internal states and only what is in the prefrontal cortex is.

And I mean, how?

I mean, this is arbitrary, no?

And what is the reason to have such kind of arbitrary decisions there?

you could then put everything into this direct interaction, depending on where you... Yeah, yes.


SPEAKER_00:
Actually, it is, to some degree, an arbitrary boundary, because as we also read in Chapter 6, defining the Markov blanket boundary depends on the situation we're trying to model.

So it's not necessarily something

predefined.

And it depends what behavior we're trying to model.

So yeah, in some sense, it depends on the context.

And yeah.


SPEAKER_02:
But see, my point is, if we are trying to model reflexes, we would put this into the internal states.

But if you allow this direct interaction between active states and sensory states, you're basically

Isn't this just like saying there is behavior that we cannot explain, but we want to have it in our model?


SPEAKER_03:
I will give a thought, but first, Jakob.


SPEAKER_01:
yeah i was um i just wanted to add that you could i i think i agree with ali's point that it's probably there for um more i guess explainability or just to kind of um

point to the fact that the active states and sensory states form like a single blanket but I think you could also think of it as enabling another loop with the internal states perhaps where if the external states are very high dimensional and you're in a partially observable

state where you have like a window of observation then the active states can directly change the sensory states without necessarily changing the external states it's like um psychotic eye movement it's not changing the external states but it is changing your window of observation through action

And I think that Carl mentioned in one of his talks that these arrows might also represent different levels of sophistication, but I'm not totally clear on how that can be explained.


SPEAKER_03:
Thanks.

Okay.

A few angles on this.

So I think the general modeler perspective is what some call an arbitrary decision.

Others might call a modeler degree of freedom.

So the idea that just because something is subjective or arbitrary devalues it.

Rather, that is the space in which decisions are made.

And so it's rarely a bad thing

to have a broader family to choose from.

And then let's imagine that there is, okay, second point is the particular partition, which is this one.

So here's the particle, the blanket and the internal states.

then the autonomous states which are the ones that we're most interested in developing imperatives for are internal states and active states so those are called the autonomous states they're the ones like if you had total control of your mind and action and you had the optimal policy like you'd be thinking and acting optimally

then that leaves the sensory states as being what we're trying to reduce the divergence with in terms of what we actually observe and what we expect slash prefer okay so the particular partitioning comes into being or a defined set of variables in a base graph it's not like one variable is simply internal states so depending on what model you construct

there might be a variable might be internal with respect to one but then blanket with respect to another so if we were doing um an eye circadian model like you you might be able to make a base graph where different things corresponded to differences um and then also this allows us to have a model where the entire cognitive behavior is

accomplished by the blanket so you could have a model where these edges with internal states do nothing and that is corresponding to the the strange types that we talked about previously like it corresponds to an inert particle that's not doing anything that we might call truly cognitive which is to say that its perception and action are mediated entirely by its blanket

And so there are systems where the entire performance can be understood on the holograph, on the blanket.

Other systems, we might want to extend that.

And there might be a system where this edge is parameterized from empirical data to be zero, or where you set this to be zero.

But another way to view this graph

is as a four by four matrix where the edges reflect relationships.

So if it was just four independent variables, it would look like the identity matrix.

And then the off-diagonal elements of that four by four matrix reflect the causal influence of different variables on each other.

And so you can have the around the clock, you could have the bi-directional around the clock,

could have the telepathic model where there's a direct edge between internal and external so you could fit the full four by four but that would be equivalent to fitting a linear regression with four variables testing for all by all interactions

which you might want to do, also that has the least statistical power.

And so in terms of which edges it makes sense to fix to zero, removing the telepathy slash telekinesis edge is a really important one.

And then there's a few other edges that are removed, like internal backward causing sense and external backwards causing action.

But that's not to say this is the only topology of the action-perception loop.

Michael?


SPEAKER_02:
Yeah, so I'm just wondering how... Well, as I said, I'm trying to apply this to our translation process data.

So we have gaze data and keystrokes and so on.

And there is one observation in translation that

It's a mixture of priming processes, so of partially automatized processes, and monitoring processes where higher cognitive decisions or reflective thought maybe comes into play.

So it means that translation is a mixture of automatized production routines and monitoring intervention.

And I'm wondering whether this model then would

could be used to say that these priming processes are these direct links between sensation and action.

And these higher order reflective processes, and so somebody maybe thinks of in which context does the translation make sense and all this, whether this would then be better modeled with the other route.

So in this case, of course, these priming processes happen in the head somewhere.

It's not in the spine, right?

So translator produces and nevertheless uses the head, but there's priming processes and other kinds of processes.

Do you think that makes sense to view it that way?


SPEAKER_03:
I think we can actually very quickly run the recipe and understand Ali first, and then we'll come to the recipe.


SPEAKER_00:
I think one point that might help to understand this diagram slightly better is that in most of the literature related to active inference and FEP,

The word particle is used to refer to the joint set of internal states and blanket states.

But in some other papers, instead of particles, they use system or agent.

So actually, in some sense, those markup blankets

belong to the agent we're trying to model.

But the reason behind distinguishing between the markup blanket and internal state is purely for modeling purposes.

I'm not sure if it makes sense or not.

But yeah, so in any case, those markup blankets

would actually constitute parts of this ancient agent we're trying to model here.


SPEAKER_01:
OK, thank you.


SPEAKER_03:
Let's just imagine just rapid fire.

You have the most translation experience, but others can imagine potentially what

translation setting would be so we'll just give one thought we don't need to um expound on all of them but we're gonna focus on the particular partition and then we'll use that to understand like what family of action perception loops are we actually fitting so what question are we looking to answer

Or what phenomena are we looking to model?

Are you asking me?

Sure.

What's an interesting question for you or for anyone about translation, human translation?


SPEAKER_02:
Yeah, so we would like, I guess, try to understand translator-environment interaction and how translators produce translations.

What are the difficulties?

What are the processes that go on?

Difficulties.


SPEAKER_03:
OK.

Which data do we have?


SPEAKER_02:
We have the observations on the Markov blanket.

So we have gaze data, reading patterns on the source and the target text, and we have typing behavior.

So the insertions and deletions that happen in the texts.

OK.


SPEAKER_03:
All right.

now we're going to go to the form of the generative model as we mentioned last week like it doesn't have to necessarily be in this order each time but now we're going to so here no markov blankets need to be mentioned this is just empirically which data exists or what are we trying to model now let's think about um

using the ontology what are the particular partitions going to be so okay the action states these ones do correspond to actions that you you have collected on so we have isocades

which there's actually there are active models of sentence comprehension and typing so here the eye saccades could be understood as being selected from directions that the eye can move and the typing could be understood as like at the letter or the word scale okay what sense data or sense states are going to be incoming


SPEAKER_02:
Well, I would put these eye saccades into the sense thing or the reading pattern.

So with an eye tracker, we can see where the eyes are on the text.

So we can see the items, the tokens that are read.


SPEAKER_03:
Yes.

So the sense could be the incoming visual perception, which token is being viewed.

But the eye saccade is an action.

yeah but okay this is a policy up down left right that the eye can move to obtain a different sense state okay so they're describing random variables in a base graph they're not necessarily describing um spatially separated parts of an organism okay now this is where

there's a lot of degrees of freedom in what is being modeled as the internal the external state so does anyone have a thought on what the internal or the external states might be


SPEAKER_02:
So the way we are trying to see this is that the internal states can be fluent.

So the person can be in a steady state.

So they can be just looking a little bit ahead, fluently translating.

So it's a kind of fluent production state.

exactly non-equilibrium steady state.

Or they can be in a state where they are confused or hesitating.

We can see this by the eye gaze data going back and forth in the text, revision, trying out some options, and so on.

So there's kind of a hesitation.

That would be an internal state, in my understanding, searching and, well, a kind of surprise.

Or they could be in a state where they try to, so what I think is called epistemic state, where they go ahead intentionally and try to activate the correct or the appropriate mental resources to be able to then use this in a translation in a steady state.

Does that make sense?


SPEAKER_03:
Okay.

I might put forth that fluency, confusion, hesitation, or just thinking, these are like categorizations.

that you're going to be able to delineate from the phase space of a variable.

And so the question is, what is the variable?

So one option, but anyone feel free to give a different option would be the external state are the true hidden semantics of the text.

And the internal state would be the received semantics of the text.

like just thinking about numbers like we're doing a cooking recipe but we're less fluent or we're translating a cooking recipe so it's three cups of sugar and two cups of flour we're going to be engaging in isocades to reduce our uncertainty about the semantics of a text

But of course, we can't directly see the semantics of a text.

What do we get?

Incoming visual tokens.

What do visual tokens do?

Visual tokens reduce our uncertainty about the semantics.

What do we do now that we've resolved semantics?

Well, we can take actions.

which constraining or if we didn't have to type this is a complex because there's like two kind of very different kinds of actions but let's just say we were just reading to understand so we're doing silent translation we're just reading and understanding something in a in a language so then if you had sufficiently resolved your uncertainty about us about the the word you'd imagine that you had taken isocade

to where you expect the next piece of semantics to be resolved, next word.

Whereas if the incoming sensory states are not adequately resolving your uncertainty, you might pause or you might even go back.

And then from patterns of flow across the agent states,

then you could categorize those into categories like fluent reading hesitant reading distracted that's one option or one can imbue the model with like a little bit more like a more opinionated model where you could say an internal state is going to be

whether they're in this or that mode.

So just to kind of summarize that, you can have internal states reflect semantics and then have a secondary categorization of different zones in that phase space, or you can have a more opinionated model

that is explicitly modeling transitions between, for example, fluent production and confusion.


SPEAKER_02:
I need to think about this.

Good ideas.

Thank you.

Yes.


SPEAKER_03:
Yeah.

And then just to kind of let's just continue just briefly to this last piece.

How would we set up the generative model?

Well, let's look at how they talk about setting up the generative model.

What are the variables and priors?

Okay, so here was just natural language.

So the semantics of the text.

So what should that variable be?

Are we reading a text that's just ones and zeros?

Or is there going to be, are we doing genre detection and there's five genre?

Or what is the actual state space of different variables?

For typing, are we going to have the state space be letters?

and then basically have like a transition matrix of letters which is like some of the earliest information theory or will we have a transition matrix of words or a transition matrix of motifs so those are all what some might call subjective but they're model or degrees of freedom there isn't a right answer there's just what model is made we


SPEAKER_02:
We have actually something we call activity units.

So we can chop up the text or the translation process into smaller pieces where we can see whether a person types or reads a text or has concurrent typing and reading.

And so we can chop up the timeline into smaller units, which we call activity units.

And this, I think, would be the units on which we would like to.

work.

So the idea here is not to simulate a translation system.

We're not so much interested what is actually the text, the translation being produced, but say the textual elements

but rather the process elements.

So it means how long does it take?

What is the interval between successive keystrokes?

And what is the offset between gaze and typing and so on?

What are the gazing patterns?

So how long ahead do people read in the text?

and so on.

So it's more the temporal structure that we want to model, not the semantic structure, actually.


SPEAKER_03:
Yes, great.


SPEAKER_02:
So we are not trying to simulate a machine translation system.


SPEAKER_03:
Yep.

So this could be simply like the most coarse-grained typing model would be there's two affordances, typing or not typing.

Yes.

Yeah, and again...

one person could say these are subjective differences between models but these are the kinds of models if one were to really do a research project on that you could compare these three models or you could say we chose to pursue a model with this state space for this reason or like which parts are fixed and which must be learned well

We know that there's things that can be learned by real humans, but the question is what is going to be fixed or learned in this model?

And fixed models are simpler computationally and they have a simpler interpretation, but sometimes the function of learning then gets displaced and results in

distorted behavior from other parameters so this is why modeling is an iterative process and why it involves constructing sometimes vast families of cousin models to understand the stability of different parameters because maybe once you you enable learning on something

then that might change another parameter because all of a sudden it doesn't need to be taking on some kind of role and then which aspects are perception-like?

which aspects are learning-like?

and then the last question is how to set up the generative process

Interesting little hidden character.

Not sure if that's just my reader, but that's very interesting.

What are the elements of a generative process and how do they differ from a generative model?

So the generative process, the niche, it could be another Act-Inf agent, or it could just be a static text.

that passes tokens in order or it could be a conversant but the exact structure of the state space and then which families of statistical distributions one chooses for priors those are modular degrees of freedom

won't just be one for any natural system in in our last third what would be another good chapter six direction to go there's a lot

Okay, the decision, the choice between discrete and continuous models is more simply articulated using discrete time models.

Okay, who has a thought on why that is?

That's a very good question.

Here's one possible answer.

What was the question again?

Why is it the decision to model alternative futures contingent on policy selection is largely tied up with a choice between discrete and continuous models?

Why are alternative futures more simply articulated with discrete time models?

in the discrete state space model figure 4.3 as usual hidden states past now future and that could have temporal depth even further but we're making an explicit prediction about like the hidden state at time seven

if we're interested in time depth of seven.

And then if we have two affordances, we're evaluating all of those combinatorics, every policy, which is a sequence of actions of length seven.

For each of those policies, we're going to evaluate what the hidden states will be.

So you could ask under policy 22,

up, up, up, down, up, up, up, what would the hidden state be?

And then what observations might I expect?

So the discrete state space formalization is amenable to talking about specific moments in specific policy that are under consideration.

And then of course, like those policies are going to be evaluated according to their expected free energy.

The continuous time case is structurally similar, which is the big visual message of figure 4.3.

But note that while B in the discrete time case is a transition matrix, how does the time point move into the next time point?

So you multiply forward with B, and then that's how the hidden states move forward.

And that's where policy intervenes, how hidden states change through time.

In the continuous time case, it's a lot more like a Taylor series expansion because node three here is the derivative of x conditioned upon x and v causes.

So in this case, it's not that we're explicitly predicting at five time steps from the future if I do this or that,

but rather under a given policy that's being considered there's a function that's being approximated with higher and higher derivatives so it doesn't have as straightforward of an interpretation you still could ask under this Taylor series approximation

at five time steps out what's my prediction so you can ask that and you could even compare under um derivative uh under v1 or v2 two different ways of taking derivatives we can evaluate so it's not that you can't evaluate the um

alternate futures contingent on policy selection.

And that is exactly what happens with expected free energy in continuous state space models.

So it is possible to compare alternate futures and still use expected free energy, but only in the discrete case do we actually have sequences of actions defined along the way.


SPEAKER_02:
So does this mean if, in the nondiscrete case, we need to look into the seventh derivation to see what happens, say, seven points in time ahead?


SPEAKER_03:
No.

That's a great question.

You could do a Taylor series a depth of 1.

So a Taylor series depth with approximation of one, let's see if there's a graph.

Okay.

Okay.

So let's just say we do a Taylor series with one term.

So it just is literally a flat line.

We can look ahead any depth.

We could look ahead a thousand time points with a Taylor series of depth of one.

and we don't need to explicitly predict those thousand time steps.

Whereas if we wanted a thousand time steps deep with discrete, we would need to explicitly model every single step.

What is the B matrix from 997 to 998?

So that's the strength and the weakness of the Taylor series.

The Taylor series, you can have a really simple approximation.

Oh, well, how about two terms?

Let's do the green one.

And you could use that two-term approximation any depth.

And as you get more and more terms, they, just like principal component dimensions, they always make a better and better approximation.

Sometimes a lot, sometimes a little, but you always do get better.

So kind of like the contact zone between the true function, blue, cosine in this situation,

The contact zone, as you do higher approximations, always extends out more.

Like brown peels off here.

Purple peels off here.

And that's part of the issue is also with Taylor series and just modeling in general, you don't really know how well you're doing

because the green line is not going to complain.

Oh, what's the cosine approximation of 11?

That's negative 200.

No, but the model's not going to complain.

Ali?

Possible unmute, Ali.

Thank you.


SPEAKER_00:
First of all, sorry, I have some connection problems.

I'm not sure if my voice is intelligible, but one interesting point is actually some neuroscientists such as Rizzolatti and Sinigaglia, two prominent Italian neuroscientists, explain this choice of actions in terms of mosaic of zones.

zones.

So each zone in this mosaic would contain information about the parameters that must be used to select the most appropriate variant of action to act on an object or


SPEAKER_03:
okay okay we we did just lose um only but we got that part mosaic of zones um okay let's see if we can get but um so suffice to say discrete state space downside you have to explicitly model you know the upside you get to explicitly model downside you have to explicitly model

So if you want to do temporal depth of 30, you need to run that out across the combinatorics of policy for a depth of 30.

So the computational complexity can be extremely high, which we explored in the branching time active inference discussions.

So here is like, it makes a massive tree search.

So it's kind of like a chess algorithm, at least some chess algorithms.

continuous state space has a very very defined uh computational complexity however it doesn't have the exact um ability to say like other than just generating this smooth function you can't really say too much more like how would it have been different if I went up up up up up down versus up again

Okay.

Ali, we heard you with a mosaic of zones, so you could maybe add the links into the page.

Any one or two more questions that people would like to come to?

How about this one?

What is meant by a variable?

States or observations.

How can states be continuous?

So states can be continuous.

A continuous variable is like one where there's a smooth knob that is controlling the value.

So this is like a smooth knob.

It could be any number between 0 and 1.

It could be 0.77772.

Or discrete could be like 0 or 1.

Or it could be 1, 2, 3, 4, 5.

So that's how states can be continuous.

What is meant by variable states or observations?

So variables and states basically mean the same thing.

Like a variable is just a, it's a parameter in an equation or in a program and variables take on states.

So the variable could be continuous between 0 and 1, and then the state that it's in is 0.6, and then observations are one kind of variable.

Michael?


SPEAKER_02:
Yeah, so how would you then model the difference between the observations and the states?

Is there a problem?

Is that straightforward?

So if I have a knob and I can turn it on and off, so I have two observations for that knob, or I can turn it like a dimmer or something,

with infinite number of observations, but the knob itself is just one thing.

So I'm not sure.


SPEAKER_03:
Yeah, great.


SPEAKER_02:
I should understand that.


SPEAKER_03:
OK, so we're doing a volume knob.

The knob is the territory.

It has physical, it's made of metal.

OK, now there's multiple maps that you could make of that knob.

You could ask whether it's playing or not.

That would be a binary discrete space.

You could say we're going to do 10 volume levels.

This would be discrete with 10 possible states.

Or you could do 100.

Or you could do a continuous variable.

any number from zero to one so it's like a it's a total classic map territory question state spaces are about variables variables are in the map if i make a variable corresponding to weight like of a person it could be over under 100 pounds

It could be integers.

It could be tenths of a pound.

That's still a discrete state space.

It could be a continuous number.

Jonathan wrote, you could then imagine that observations could be discrete even when states may be continuous.

It may be that we can't tell the difference between the very finely graded states.

Yes.

great point like just noticeable difference um and when you do discretized models it becomes like a trade-off where the more coarse graining you do the simpler the model is but you're making bigger and bigger buckets that are clumping together more and more disparate things

Whereas if you do this really fine scale discretized model, like hundredth of a pound, and we're going to be weighing people, then you can get all these like bizarre phenomena where it's like, well, if they're 137.1 pound, then this is the case.

But if they're 137.2, then this is the case.

So finding that optimal discretization is a big topic in optimization in general and in modeling.

And then another piece to remember, the state space of the external state.

So we're still not even talking about the territory itself, but the state space of the external variables.

So our model of temperature, it doesn't have to be the same as the cognitive model of the agent.

So we could say in the simulation, temperature is an integer.

and then they're going to have three states cold just right and hot or here in the external world it could be continuous variable and then in the brain it could be three state or it could be a continuous variable or it could be anything else what matters is the observation coming in and then how that gets fit into the generative model ali


SPEAKER_00:
Yeah, apologies, I got disconnected.

As I was saying, some neuroscientists explain the choice of appropriate action in terms of the mosaic of zones.

And by that, they mean that each zone is the mosaic that would contain the information

to act upon the affordances or the goal-directed sequences to act upon the affordances.

So, as an example, take for instance the task of picking up a cup of coffee.

This seemingly simple task can be done in various ways depending on whether we are drinking from it,

In that case, we would probably grasp it by the handle.

Or if we're washing it, so in that case, we would probably grasp it by the rim or moving it out of the way.

which in that case the grasping it by the body would probably be the more appropriate sequence of action.

So which of these actions is deemed appropriate for a task is worked out within that mosaic of zones which they also correlate with some cortical areas which

maps those mosaic zones quite literally in order to afford the agent this ability to pick the appropriate action in each situation.


SPEAKER_03:
Thank you.

Cool.

All right.

That is our second discussion on Chapter 6.

So 6, in summary,

outlines some of the most important design choices that have been made in setting up an active inference model.

We have explored a lot of auxiliary routes and questions.

and hopefully this conveys some of the richness and excitement and openness around making a model because even for simple phenomena there's many ways to do it and even for simple phenomena it's not just about making one model and parameterizing it once it's like an iterated modeling process where you're generating these families of models

They provided a recipe, some guidelines and thoughts.

They point out that it doesn't have to be followed in that particular order.

This sets up the remainder of the book, which puts the ideas into practice through a series of illustrative examples designed to showcase the theoretical principles presented in the first half of the book.

In everything that follows, the only differences amongst the example rest on the design choices we have highlighted.

Part two illustrates systems with different boundaries, discrete or continuous dynamics at different time scales, and that will all be implementing ACT-INF.

So we're going to head into chapter seven next time.

Chapter seven is active inference in discrete time.

We're going to look at generative models that have a discrete time characteristic.

Chapter eight is continuous time.

So that distinction was kind of raised earlier in figure 4.3.

Chapter 7 and 8 are like a pair that are going to highlight those two different possibilities.

Chapter 9 is about data-driven analysis and chapter 10 is a conclusion chapter.

So that concludes this session.

Thanks everybody for the great times and we'll come back next week for chapter 7.

Thank you.


SPEAKER_02:
Thank you.
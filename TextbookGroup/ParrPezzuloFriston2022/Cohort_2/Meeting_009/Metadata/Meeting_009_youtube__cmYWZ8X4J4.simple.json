[
  {
    "start": 1.445,
    "end": 27.448,
    "text": " hello everyone thanks for joining it's october 28th 2022 active textbook group cohort two we're in the ninth oh we're in the every the weeks are shifted back by one is that true",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 30.728,
    "end": 31.688,
    "text": " There's two 21s.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 34.069,
    "end": 37.409,
    "text": "Or there's been some kind of other time glitch.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 40.63,
    "end": 41.17,
    "text": "But today is 1028.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 41.41,
    "end": 44.451,
    "text": "Anyways.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 49.852,
    "end": 51.313,
    "text": "We're discussing chapter four.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 52.193,
    "end": 54.513,
    "text": "Continuing our discussion of chapter four.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 55.633,
    "end": 57.814,
    "text": "So where would anyone like to",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 59.938,
    "end": 60.598,
    "text": " start with?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 62.079,
    "end": 70.2,
    "text": "Does anyone have a general chapter four remark or a particular question that they want to look at?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 73.161,
    "end": 73.781,
    "text": "Or anything else?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 83.404,
    "end": 84.404,
    "text": "Daniel, it's Bronwyn.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 84.424,
    "end": 85.304,
    "text": "Mm-hmm.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 86.841,
    "end": 109.388,
    "text": " um there's just a lot in chapter four obviously and it's sort of i think it takes in a lot of different aspects of active inference and then sort of piles them all in there and then throws out a whole lot of maths so you know it's unbelievably dense but and and",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 110.029,
    "end": 111.449,
    "text": " You know, I don't know any math.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 112.21,
    "end": 117.151,
    "text": "But interestingly, as I read it, there's some sort of themes that come out that are sort of simplistic.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 117.972,
    "end": 125.934,
    "text": "And I'm wondering if somehow that can be sort of, if you can sort of summarize that today, if that makes sense.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 129.615,
    "end": 137.798,
    "text": "Just sort of like a encapsulated paragraph of the process it goes through.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 141.052,
    "end": 141.713,
    "text": " That was one thing.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 141.773,
    "end": 147.297,
    "text": "The other thing was the box 4.1, the message passing and inference.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 147.397,
    "end": 151.78,
    "text": "I didn't quite get the difference between variational message passing and the belief propagation.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 155.543,
    "end": 157.004,
    "text": "Whether I can talk a bit about that.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 157.024,
    "end": 167.232,
    "text": "Yeah, that's about it really.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 185.009,
    "end": 198.975,
    "text": " okay does anyone have any thought on this in chat or or just anything else to add or we can try to address these",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 217.056,
    "end": 217.356,
    "text": " Okay.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 227.699,
    "end": 245.225,
    "text": "Well, one way to look at the theme of chapter four or the kind of looking beyond all of the technical details and the technical details, it's kind of like an iceberg because they could show a little or they could show a ton.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 246.997,
    "end": 252.442,
    "text": " Every time they show one, they could also show a whole set of ways to get to it or not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 256.226,
    "end": 270.28,
    "text": "So it's kind of like... Maybe... It's like a variable of the text, how much of the math they just mention versus explicitly write out.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 271.51,
    "end": 281.035,
    "text": " Because there's a lot of papers with no equations at all, like a philosophy paper that will still say because of the variational free energy or because the generative model is this way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 282.576,
    "end": 288.6,
    "text": "So kind of referencing the same topic, but different texts differ in how much formalism they have.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 290.861,
    "end": 294.043,
    "text": "What is the role of the generative model in active inference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 298.505,
    "end": 299.846,
    "text": "What does anyone think about that?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 311.278,
    "end": 320.465,
    "text": " Or in the textbook, where's any quote where they use, where they say a generative model is blank, or this is the role of generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 320.485,
    "end": 322.927,
    "text": "We can look for some too, or any other thoughts someone has.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 329.953,
    "end": 337.338,
    "text": "I mean, I think it says that they, that they can vary depending on, on the inference problem.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 337.619,
    "end": 339.2,
    "text": "That's one thing about generative models.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 355.083,
    "end": 360.912,
    "text": " Yeah, another way to that that variation points really important.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 361.974,
    "end": 368.344,
    "text": "The variation is sometimes described with like fine tuning within a given inference problem.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 369.502,
    "end": 378.388,
    "text": " which is sometimes framed as perception in the most vast timescale and learning and memory in a medium timescale.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 379.609,
    "end": 389.016,
    "text": "And then also different implementations or different model structures can vary across inference problems.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 397.81,
    "end": 400.851,
    "text": " There's also different ways to write and see the generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 402.492,
    "end": 408.814,
    "text": "Can anyone just list or suggest how are generative models shown or represented?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 408.834,
    "end": 426.9,
    "text": "So I was at the...",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 428.066,
    "end": 429.206,
    "text": " The factor graphs?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 430.207,
    "end": 430.487,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 432.708,
    "end": 434.149,
    "text": "This is going to... The 4.3.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 435.249,
    "end": 435.829,
    "text": "In which section?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 436.95,
    "end": 437.19,
    "text": "Figure 4.3.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 437.29,
    "end": 437.67,
    "text": "Is it... 4.2.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 437.71,
    "end": 439.971,
    "text": "So how... Yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 458.447,
    "end": 482.179,
    "text": " One view of the generative model is like this figure 4.3 kind of classic slash common representation, the Bayes graph view with the nodes being a random variable and edges being relationships among variables.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 483.439,
    "end": 512.812,
    "text": " so like if it if every variable were connected to every variable like every time point we're causal on every time point every hidden factor influence every hidden factor you just have like a fully connected all by all causal association matrix so that's like the weakest world model and then you can say well the the present is like the only intermediating",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 514.159,
    "end": 543.196,
    "text": " markov blanket between the future and the past or this is the you know perception is the only intermediating markov blanket between observations and hidden states if you are willing to take those um structural assumptions and embody them in the model the sparsity of the model under many different ways of calculating the model it will it just like always gets easier",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 544.48,
    "end": 553.282,
    "text": " you might be restricting yourself to a very local neighborhood of models, but pretty much the more you can constrain the edges and the combinatorics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 555.103,
    "end": 563.665,
    "text": "People can add other remarks on this, but simpler models are just simply easier to fit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 566.346,
    "end": 571.967,
    "text": "Better optimization is always achieved by making",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 573.476,
    "end": 576.218,
    "text": " quadratic or curved type assumptions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 577.92,
    "end": 589.01,
    "text": "It's like a general optimization principle to make things on a smoother, more incrementally updatable related problem.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 604.188,
    "end": 605.349,
    "text": " They deal with time differently.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 607.611,
    "end": 608.912,
    "text": "They deal with everything differently.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 609.693,
    "end": 613.036,
    "text": "Time, slash, space, etc.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 613.216,
    "end": 613.596,
    "text": "differently.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 616.499,
    "end": 617.46,
    "text": "Sorry, can you hear me now?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 617.88,
    "end": 618.38,
    "text": "Yes, Ali.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 619.061,
    "end": 622.124,
    "text": "Go ahead.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 622.464,
    "end": 625.607,
    "text": "Yeah, I just wanted to mention this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 626.442,
    "end": 644.257,
    "text": " paper, a tale of densities, which clears up some of the misunderstandings or misconceptions around generative model, because more often than not, people think of generative model as just",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 645.53,
    "end": 652.836,
    "text": " structural representation of the external world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 653.476,
    "end": 664.325,
    "text": "But in active inference, the generative model is usually represented in terms of its recognition density.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 664.905,
    "end": 673.051,
    "text": "So this recognition density is something that enables us to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 674.693,
    "end": 684.183,
    "text": " or let's put it this way, it's an approximate posterior probability density that inverts generative mapping from consequences to causes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 684.843,
    "end": 693.593,
    "text": "So it enables us to recognize the hidden states based on the observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 694.313,
    "end": 704.482,
    "text": " and inferring the causes of sensory outcomes and not just simply as a static representation of the external world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 715.471,
    "end": 719.174,
    "text": "Great point, Ali, to kind of unpack this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 720.188,
    "end": 724.412,
    "text": " And this is a great, great entry point.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 726.834,
    "end": 729.557,
    "text": "The Tale of Two Cities is a book about two cities.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 732.079,
    "end": 736.142,
    "text": "The Tale of Two Densities is some work with Ramstead and others.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 736.443,
    "end": 737.103,
    "text": "And there's a...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 738.875,
    "end": 762.857,
    "text": " uh at least some stream on it and the two densities are like the two directions which a bayesian type model can be used which is in its recognition capacity and in its generative capacity so one point is the difference between a sort of like descriptive statistical approach",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 768.362,
    "end": 780.19,
    "text": " whether with p-values or any other type of statistics where you're just taking observables only and describing them statistically or summarizing them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 781.691,
    "end": 795.881,
    "text": "The two densities are those two bases of the Bayesian model, which is about its capacity to generate data from hidden states and, as Ali said, to be inverted",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 797.168,
    "end": 804.351,
    "text": " or to be able to perform inference on hidden states from observables.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 805.651,
    "end": 825.118,
    "text": "And that is exactly the distinction of S and O. S are unobserved hidden states of the world, which in a causal graphical framework",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 827.802,
    "end": 845.794,
    "text": " there's different interpretations of what it means that causal force but it's it's the gm family that one is embodying and then the observations are up so blue",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 847.153,
    "end": 847.493,
    "text": " I'm sorry.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 847.573,
    "end": 850.575,
    "text": "I just was going to add to that if you're, if you're done, but I don't know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 850.595,
    "end": 850.835,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 851.335,
    "end": 858.298,
    "text": "Um, I just wanted to, uh, I always love listening to that live stream and Maxwell really stated it really nicely.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 858.358,
    "end": 862.981,
    "text": "Like the differences between, um, the recognition density and the generative density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 863.841,
    "end": 868.904,
    "text": "Uh, it was how he described it was like, if you're stacking up dominoes,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 872.077,
    "end": 873.498,
    "text": " And then you're going to knock them over, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 873.558,
    "end": 877.321,
    "text": "Like, so the recognition density is like the stack of dominoes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 877.821,
    "end": 881.864,
    "text": "And then the generative density is like the action occurring between the dominoes falling down.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 882.184,
    "end": 883.225,
    "text": "Like, I always like that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 883.265,
    "end": 887.848,
    "text": "And it's locked into my mind and really kind of helps me get a grip on maybe the differences in there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 902.99,
    "end": 911.636,
    "text": " Okay, Brock or someone, what have you added this quote for?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 917.801,
    "end": 928.048,
    "text": "Just on page 64, this is like a referencing equation 4.1, which is right here, it just says that...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 930.163,
    "end": 941.587,
    "text": " that the generative model is the point that prior and likelihood are sufficient to compute model evidence and probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 944.008,
    "end": 955.953,
    "text": "So I guess it's a slightly more formal, specific definition and narrow, because right after that, it talks about computational tractability of doing that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 956.073,
    "end": 959.614,
    "text": "And that seems like the best",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 961.429,
    "end": 988.248,
    "text": " starting point of the generative model is blank in the book that i was able to find nice good summary like in the frog jumping example prior and likelihood left side prior",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 989.315,
    "end": 1017.618,
    "text": " prior beliefs on likelihood of the hidden state on the probability of the hidden state before the observation likelihood model the larger matrix it has a different dimensionality than this one the prior and it's the mapping of different things",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1020.305,
    "end": 1021.726,
    "text": " Then there's an observation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1022.926,
    "end": 1028.168,
    "text": "But in a sense, none of these beliefs or likelihood models are observed.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1029.108,
    "end": 1037.131,
    "text": "The observation, O, which is like the experimental data actually being collected, is here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1038.492,
    "end": 1047.855,
    "text": "And then everything else are like the cognitive states or internal states or states as modeled by the modeler.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1049.73,
    "end": 1076.016,
    "text": " and then there's some updating and in chapter two that was being described like in terms of the surprise self-information bayesian surprise but in the context of what brock just read prior and likelihood comprise a generative model prior likelihood that already compose a generative model",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1076.954,
    "end": 1080.055,
    "text": " but it's like a car that hasn't been run.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1081.456,
    "end": 1085.338,
    "text": "And then there's some, that's when data starts being able to update the model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1089.119,
    "end": 1097.203,
    "text": "So that's why many times we see equations like a Bayesian",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1099.324,
    "end": 1122.38,
    "text": " equation whether represented graphically or with like um in like a just a single line form uh because to specify a bayesian statistical problem is to have specified the generative model and vice versa ali",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1126.125,
    "end": 1142.118,
    "text": " And I think it's a general issue regarding the whole active inference literature, because whenever we read a paper in active inference, we need to be quite clear about what do the authors mean by each concept.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1142.178,
    "end": 1151.946,
    "text": "Because, for instance, in this paper from 2022, learning generative models for active inference using tensor networks,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1153.498,
    "end": 1174.89,
    "text": " They have defined generative model in its traditional statistical form and it doesn't exactly maps onto this somehow more comprehensive or modified definition of generative model in terms of recognition density and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1175.43,
    "end": 1186.155,
    "text": " So yeah, and it also extends to every other concept we encounter in any other active inference paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1192.738,
    "end": 1193.959,
    "text": "Thanks, great point.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1205.342,
    "end": 1229.55,
    "text": " um yeah that's good point the the purpose of just bringing back to chapter two and this example is just to um situate everything that's happening with the theoretical like data-free math of act inf",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1231.582,
    "end": 1252.818,
    "text": " and also the data-oriented aspect of Act-Inf, which is, to give a deflationary perspective, a specified Bayesian statistical model, and then data that that model has legibility around.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1254.619,
    "end": 1259.543,
    "text": "Of course, it could be the wrong model for the situation",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1261.551,
    "end": 1286.245,
    "text": " so on but this is like this is like the statistical scenario which is just prior likelihood data and then this posterior can become that one's prior and that's just a uh yeah can be modeled different ways in continuous time in discrete time",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1288.979,
    "end": 1315.115,
    "text": " And again, the whole point is just reduce the theoretical novelty of active inference because it's more like Bayesian statistical modeling in this mode than many other things.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1316.806,
    "end": 1324.53,
    "text": " And so a lot of the questions, and there are great questions that come up and have come up in statistics before, but kind of in a new way here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1326.632,
    "end": 1328.473,
    "text": "These are general statistical questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1329.213,
    "end": 1331.995,
    "text": "They're statistical questions involving action.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1334.576,
    "end": 1344.322,
    "text": "Building on some of the causal statistical developments from Pearl and others,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1346.294,
    "end": 1359.561,
    "text": " who brought in just the concept of the Markov boundary in that 90s, 80s, 90s and beyond period.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1361.942,
    "end": 1375.129,
    "text": "It's using those kinds of statistical techniques developed for Bayesian statistical settings in the last five to 50 years",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1377.543,
    "end": 1404.831,
    "text": " in the setting of action and niche dynamics also connecting in some way that's not fully characterized yet with some more physics-like formalizations like the Bayesian mechanics",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1408.143,
    "end": 1417.405,
    "text": " But that's not the first kind of connection between statistical optimization and physics theories.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1420.366,
    "end": 1428.668,
    "text": "Which is kind of the whole idea of like why you can kind of roll the ball to the bottom of the hill physically and then the statistical optimization works like that too.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1433.672,
    "end": 1455.789,
    "text": " so the generative model chapter to kind of return to four and the first part of the of braun's earlier question i think is this chapter is describing many important aspects and two fundamental types of generative models",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1456.926,
    "end": 1467.229,
    "text": " Those dealing with active inference in a discrete time formalization and a continuous time formalization, because there's also other ways to do it, probably.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1470.471,
    "end": 1480.414,
    "text": "It's just giving technical details on what those generative models are, because that is the entity description.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1482.659,
    "end": 1510.622,
    "text": " that's the unit of perception cognition action that's the empirical model as used in the computer to recognize data to generate data so this is like kind of the core a core idea and sometimes it's dealt and like um yeah sometimes it's just mentioned but when it's mentioned that's why this is the textbook",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1512.199,
    "end": 1531.034,
    "text": " So that in some other philosophy paper, they're not going to have this, but this is kind of what is being discussed when those terms are used in qualitative settings or in conversation with all these caveat, of course, that everyone is using it differently.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1532.175,
    "end": 1541.543,
    "text": "And then in terms of like scientific review and in evaluating like a technology or some claim or something,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1542.816,
    "end": 1556.342,
    "text": " if someone says the generative model does this or it has this characteristic, these are like the specific aspects that would characterize that claim being true.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1556.362,
    "end": 1567.506,
    "text": "So that was good.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1567.586,
    "end": 1571.828,
    "text": "Very interesting discussion on the generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1573.956,
    "end": 1574.817,
    "text": " How about the box?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1596.296,
    "end": 1596.576,
    "text": "Oli?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1599.775,
    "end": 1623.963,
    "text": " Well, as far as I understand it, the difference between variational message passing and belief propagation basically comes to the difference in a holistic view of the approximate inference computation and the recursive or step-by-step computation of it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1624.763,
    "end": 1627.024,
    "text": "So in variational message passing,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1629.045,
    "end": 1644.115,
    "text": " we actually have the already actualized trajectory for messages and we know all the conditional properties of children with regard to their parents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1644.935,
    "end": 1655.022,
    "text": "And with those informations, we can compute the whole approximate free energy of the whole process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1655.082,
    "end": 1657.584,
    "text": "But for belief propagation,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1659.206,
    "end": 1669.846,
    "text": " we need to compute that recursively in each step without necessarily needing to have all the information for all the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1671.168,
    "end": 1682.498,
    "text": " conditional probabilities of children with regard to parents beforehand.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1683.619,
    "end": 1694.528,
    "text": "But another important difference between them is that the belief propagation takes account for all the messages that has happened",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1695.308,
    "end": 1703.09,
    "text": " up to the point of B, or the point that we're momentarily dealing with that point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1703.23,
    "end": 1719.435,
    "text": "So because of that, we divide the whole process with the probability of B, because we don't want to include B in the process yet.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1721.695,
    "end": 1730.583,
    "text": " That's basically something similar to a kind of deductive way of doing inference and the inductive way of doing that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1745.096,
    "end": 1745.436,
    "text": "Awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1745.636,
    "end": 1746.277,
    "text": "Thank you, Ali.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1746.557,
    "end": 1747.678,
    "text": "That's very insightful.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1752.15,
    "end": 1755.871,
    "text": " Let's look at 4.1 and see where we're seeing some aspects of this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1756.272,
    "end": 1759.053,
    "text": "Well, let's pull out one more level.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1759.133,
    "end": 1772.318,
    "text": "Ali, what would you say, what category of things do variational message passing and belief propagation, what is the category that even makes these things comparable?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1776.333,
    "end": 1791.824,
    "text": " Well, the similarities between, at least the structural similarity between these two ways of computation is their dependency on the expectation of the conditional probability.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1791.944,
    "end": 1799.148,
    "text": "So in each case, we're just computing the inference in terms of their expectation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1800.429,
    "end": 1805.393,
    "text": "And as we see, even the equations look quite similar to each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1806.734,
    "end": 1817.109,
    "text": " But as they pointed out in the bottom of the box, this is a slightly non-standard use of the expectation operator.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1817.75,
    "end": 1821.716,
    "text": "So we need to be aware of this point as well because",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1823.737,
    "end": 1842.391,
    "text": " Well, if both of those expectation operators were exactly the same, there wouldn't be any difference, any similarities between those two kinds of inference schemes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1846.82,
    "end": 1848.461,
    "text": " All right, thanks a lot for that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1849.702,
    "end": 1850.743,
    "text": "Okay, a few notes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1850.843,
    "end": 1856.528,
    "text": "One is these will be fun to see the descriptions of.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1859.69,
    "end": 1865.495,
    "text": "And just, yeah, like what these expectations are over.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1866.816,
    "end": 1876.083,
    "text": "Keeping in mind the expectation here is about the central tendency of a statistical distribution, not a time projection.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1878.624,
    "end": 1892.591,
    "text": " So talking about, as Ali highlighted, the expectations on variables, that doesn't necessarily have to be a time series prediction.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1894.091,
    "end": 1907.838,
    "text": "In fact, one of the most important uses is just to calculate a rolling snapshot in the context of prior and likelihood distributions as incoming data come in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1910.942,
    "end": 1918.065,
    "text": " So they're like parameter fitting modeling approaches.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1920.867,
    "end": 1939.075,
    "text": "And is it fair to say that for a given analytically expressed Bayesian equation, so formula, it's possible to take the approach of working with it more along one branch or more along another branch?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1943.948,
    "end": 1946.589,
    "text": " Is that the case, Ali, or anyone else?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1948.729,
    "end": 1953.79,
    "text": "Papers tend to take one approach or another, but there's also some papers that probably compare them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1955.15,
    "end": 1967.493,
    "text": "But they are distinct implementations, potentially with some differences in outcome that would be interesting to explore.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1968.203,
    "end": 1974.431,
    "text": " But is it the case that they're both possible to be used for a given Bayesian equation?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1974.932,
    "end": 1982.542,
    "text": "If we wanted to do the frog jumping out of the hand, could there be a variational message passing and a belief propagation implementation?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1986.318,
    "end": 1987.219,
    "text": " Well, I guess so.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1987.419,
    "end": 2005.218,
    "text": "If the kind of situation we're dealing with doesn't necessarily restrict us to use the global view versus the local view of the inference, then I guess both of them can be applied.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2015.044,
    "end": 2016.165,
    "text": " Let's say it this way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2017.726,
    "end": 2019.467,
    "text": "Sorry, just to add another point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2021.889,
    "end": 2036.978,
    "text": "Yeah, the variation message passing is kind of, let's say, omniscient-like view of the situation because we know all the conditional probabilities of each node.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2037.579,
    "end": 2044.243,
    "text": "But for the belief propagation, we're dealing with the explore-exploit dilemma.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2052.56,
    "end": 2055.883,
    "text": " Also, their computational implementations could be different.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2056.844,
    "end": 2060.127,
    "text": "So a given language or package might only enable one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2060.147,
    "end": 2067.154,
    "text": "And one other...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2071.26,
    "end": 2075.622,
    "text": " Let's return in the last 20 minutes back to the text.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2075.663,
    "end": 2077.083,
    "text": "And if we go there, we go there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2078.544,
    "end": 2088.309,
    "text": "But this also relates to the factor graphs and some mathematical equivalencies.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2090.037,
    "end": 2116.601,
    "text": " with factor graphs and bayesian graphs that is some of the work since like 2017 and a little bit before of devrise and par and friston where these algorithms can uh statistically have good computational implementations or just",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2118.868,
    "end": 2142.025,
    "text": " how good exactly probably is an open question but these but uh there are procedural rules for calculating large graphs of this structure",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2147.0,
    "end": 2154.523,
    "text": " which is why they represent recent developments in Bayesian computational statistics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2158.324,
    "end": 2173.91,
    "text": "It would be awesome to hear from someone who's working with those kinds of large scale implementations, but that is my kind of outside interpretation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2192.577,
    "end": 2193.237,
    "text": " Ali, go ahead.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2195.88,
    "end": 2215.337,
    "text": "And also there are some recent developments in the, for, I mean, making these kinds of computations more tractable and making, reducing the complexity classes of the computations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2217.038,
    "end": 2227.966,
    "text": " One example is this recent algorithm called Branching Time Active Inference, developed by Th\u00e9ophile Champion and others.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2228.407,
    "end": 2239.415,
    "text": "And also the work done by Baron Millich, such as the inference learning or the hybrid predictive coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2240.296,
    "end": 2264.598,
    "text": " is also an attempt to reduce the complexity classes of these kind of implementations which can be, I believe, can be a promising path towards achieving to a more tractable and computationally effective way of computing these inferences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2274.528,
    "end": 2301.367,
    "text": " nice thank you for that yeah uh there's sort of like a meme like machine learning is statistics and all that so these this chapter and hopefully seeing how some of these developments from pure bayesian statistics",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2303.783,
    "end": 2318.056,
    "text": " But in the more recent era, there have also been a computational kind of interplay with the pure theory development.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2318.997,
    "end": 2328.585,
    "text": "For example, bootstrap-based algorithms, non-parametric statistics, where one resamples from distributions,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2329.565,
    "end": 2348.47,
    "text": " does that many many times that kind of sampling based approach was worked out on paper long before it became computationally plausible to actually sample and store large amounts of information",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2350.475,
    "end": 2367.67,
    "text": " And then once that computational availability started to happen and better sampling algorithms and better hardware, then that area of statistics can become applied.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2370.287,
    "end": 2375.429,
    "text": " which sometimes requires more theoretical developments, but often not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2376.169,
    "end": 2387.753,
    "text": "It's often just simply a question of the implementation being written from like a code or computational implementation perspective.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2390.474,
    "end": 2398.777,
    "text": "So there's a lot of equivalences in Act-Inf, but I think the juxtaposition of figure 4-3",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2402.526,
    "end": 2427.342,
    "text": " generative models with Markov blankets first, and then saying these are different heuristics, computational implementations, model fitting approaches, and others, others.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2430.211,
    "end": 2447.914,
    "text": " that if you can state it like it looks in figure 4.3, then this kind of computational statistical branch will become accessible.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2448.794,
    "end": 2457.696,
    "text": "It's like if they showed here's the linear regression and here's the R method or here's the Python script that you can use.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2458.948,
    "end": 2464.172,
    "text": " Or here's the optimization, here's three ways that you can optimize a linear regression.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2465.073,
    "end": 2466.994,
    "text": "And there are multiple ways to do linear regression.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2469.176,
    "end": 2471.438,
    "text": "And there's different software packages associated with them too.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2472.999,
    "end": 2476.222,
    "text": "And so it's like, if your GMs have this structure",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2479.076,
    "end": 2490.621,
    "text": " then these are optimization implementation approaches that very importantly are not sampling based.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2492.142,
    "end": 2506.568,
    "text": "Now, complexly enough, sampling may have to come into play in the real computational implementation, but that's kind of a deeper or more specific question to a given,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2507.924,
    "end": 2526.219,
    "text": " way that it's uh applied but just more broadly these are not just trying to sample data until the landscape is like fully coded these are um models that with their sparsity",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2528.223,
    "end": 2554.942,
    "text": " like what's not connected embody sometimes incredibly strong structural assumptions about the world which is why there's the whole question of structured learning because once you're structured in the GM like we'll see it in chapter six as well the kind of structure is set and then there's like a fine tuning where the actual variables get tuned to the right values",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2556.377,
    "end": 2560.62,
    "text": " That's learning and perception, but also learning can be structural.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2565.464,
    "end": 2585.961,
    "text": "So I hope that kind of provides some concordances with the generative model and speaks to its like total centrality in ACT-INF in chapter four and connects it to box four one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2587.711,
    "end": 2610.056,
    "text": " that's describing some exciting techniques for computing large base graph type generative models one last comment on this and then we'll still have more time just time for kind of overviewing again is like",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2611.59,
    "end": 2639.298,
    "text": " i think people have certainly said before done anyone specific right now but just that there is a lot of focus on the markov blanket concept and the sort of like everything about the markov blanket the mystery but less focus on the generative model concept so when people then thinking about",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2640.779,
    "end": 2646.584,
    "text": " How do we model organizations or this cognitive feature?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2648.265,
    "end": 2651.487,
    "text": "There's a lot of focus on like, well, what are the nested Markov blankets?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2653.048,
    "end": 2669.3,
    "text": "And now there's a way to think about that as nested Markov blankets, recognizing the generative model that isn't just the entity scale blanket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2670.737,
    "end": 2689.002,
    "text": " But that kind of nested Markov blanket representation, multi-scale systems, kind of like just general point, what gets often not brought up in those situations is like, okay, society's nested Markov blankets, but what are the generative models?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2691.383,
    "end": 2696.284,
    "text": "If you specify the generative model, you'll have also specified the Markov blankets.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2698.188,
    "end": 2705.31,
    "text": " because you'll have defined the particular things embodied in the actual generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2708.491,
    "end": 2717.734,
    "text": "So it's like, is it an active inference claim that a situation has been analyzed in terms of interacting generative models and generative processes?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2719.875,
    "end": 2723.776,
    "text": "Or is it a general claim about multi-scale systems being nested?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2727.17,
    "end": 2740.439,
    "text": " So when we think about just the encapsulating Matryoshka nature of the nested blankets, the generative model is not specified in more cases than not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2742.96,
    "end": 2744.401,
    "text": "Hope that's an interesting and fair point.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2752.587,
    "end": 2752.787,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2754.83,
    "end": 2777.528,
    "text": " we have 10 to 12 minutes left is there any other like thought or question Brock wrote spooky inference in close proximity to surprise generates noisy definitions I was just kidding about Ali uh saying lots of",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2779.127,
    "end": 2781.728,
    "text": " different names for things.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2782.569,
    "end": 2785.43,
    "text": "We call that spooky action at a distance entanglement now.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2786.791,
    "end": 2794.754,
    "text": "But at the time, it was a bunch of different things when it was first observed.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2796.355,
    "end": 2797.256,
    "text": "And it was surprising.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2798.236,
    "end": 2799.237,
    "text": "So, anyways.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2799.477,
    "end": 2807.841,
    "text": "I don't know if, yeah, it's worth, like, thinking or just linking, like,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2810.467,
    "end": 2818.7,
    "text": " Lingering a little bit on the intractability of specifying like the model entirely in most in a lot of cases.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2828.939,
    "end": 2844.736,
    "text": " thanks yeah i think that kind of um anticipates some discussions that are in like chapter six with the recipe for how to make the generative model and the difference between like",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2845.757,
    "end": 2870.263,
    "text": " fully specifying a quote toy model which is to say like a model where you can see it graphically at like this like just kind of like this like this is the whole model um in that case you may be able to create a little information bubble where that is the full model specification",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2872.165,
    "end": 2897.138,
    "text": " but then in any real situation even like doing research let alone doing anything with causal feedback in the loop the the fully specified model just is somewhere between doesn't exist can't be specified won't be specified shouldn't be specified",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2903.214,
    "end": 2903.794,
    "text": " So then what?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2904.155,
    "end": 2904.535,
    "text": "What then?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2904.555,
    "end": 2930.049,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2930.55,
    "end": 2930.71,
    "text": "Any...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2935.236,
    "end": 2936.117,
    "text": " Thoughts on four?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2937.378,
    "end": 2940.001,
    "text": "Or let's just look quickly to five.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2951.892,
    "end": 2952.693,
    "text": "Ali, go for it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2955.11,
    "end": 2957.693,
    "text": " Yeah, just one small point here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2957.713,
    "end": 2969.003,
    "text": "I'm not sure if I've mentioned it before or not, but there's kind of another type of active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2969.024,
    "end": 2978.773,
    "text": "Of course, we have very different classes or categories of active inference, but one specific type is called sophisticated inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2979.734,
    "end": 3001.114,
    "text": " which kind of deals with the situations in which, well, I mean, instead of predicting what would become later at each stage, it tries to compute everything that has come before or",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3003.236,
    "end": 3019.885,
    "text": " in order to have a way to compute counterfactual consequences or the things that we need to somehow the kind of decision-making processes and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3020.585,
    "end": 3036.212,
    "text": " And interestingly, in the paper branching time active inference, they've stated that this type of active inference is a subset of branching time active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3036.312,
    "end": 3046.237,
    "text": "I mean, if we just take that branching time active inference and separate it into",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3047.177,
    "end": 3057.961,
    "text": " forward and backward propagation trajectories will come with traditional active inference and sophisticated active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3058.521,
    "end": 3066.543,
    "text": "So I think the main paper for sophisticated active inference is this one that I'm posting here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3067.003,
    "end": 3071.925,
    "text": "So that's just one small point I wanted to mention.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3076.701,
    "end": 3077.581,
    "text": " Great point.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3078.882,
    "end": 3081.783,
    "text": "Thanks for sharing this generative model knowledge.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3083.744,
    "end": 3085.785,
    "text": "It's very helpful.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3090.067,
    "end": 3092.888,
    "text": "Livestream 11 was sophisticated affective inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3094.109,
    "end": 3103.673,
    "text": "So that was with a secondary or, you know, here, verbally secondary adjective, affective.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3105.14,
    "end": 3125.076,
    "text": " reflecting that they were modeling an affective variable or taking an affective interpretation of their model's cognitive function they were doing affective inference in the context of sophisticated inference which is as Ali described",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3127.441,
    "end": 3153.677,
    "text": " allows for a kind of mapping and modeling of past present and future which is like why in 4.3 there's like a time minus one and time plus one it could have been written as just time time plus one time plus two and been like a two-step planning problem but also there's um you can choose different pieces of the time horizons to model ali",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3155.974,
    "end": 3175.64,
    "text": " Or a better way, probably a better way to put it is to say that in situations where we have beliefs about beliefs, this kind of sophisticated inference can come into play instead of just beliefs about states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3179.261,
    "end": 3184.302,
    "text": "And the composability of Bayesian graphs",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3186.48,
    "end": 3213.918,
    "text": " is high which is what allows all of these different structural models to be explored like nested models that represent spatial enclosure nested models that represent counterfactuals models that consider the past and are able to reconsider the past observations models that don't all of the like a memoryless process",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3218.769,
    "end": 3243.324,
    "text": " wow all right chapter five is quite different and we'll just look through it and see how different it is it's gonna pick up on this message passing topic which will be cool to revisit again and consider it primarily from the context of neurobiology in this chapter we focus on the process theories that arise from chapter four",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3245.393,
    "end": 3250.115,
    "text": " Central to this implementation of Bayesian belief updating is the notion of Bayesian message passing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3252.497,
    "end": 3253.637,
    "text": "Microcircuits and messages.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3256.499,
    "end": 3257.739,
    "text": "Cortical layers.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3258.74,
    "end": 3260.261,
    "text": "Part of the mammalian brain.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3260.981,
    "end": 3262.322,
    "text": "Mammal neuroanatomy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3263.422,
    "end": 3266.524,
    "text": "Mammal computational cerebral neuroanatomy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3270.526,
    "end": 3274.488,
    "text": "Abstract hierarchical predictive coding models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3280.347,
    "end": 3309.578,
    "text": " motor reflex arc in the spinal column of a mammal policy selection computational neuroanatomy in the mammal brain putative roles of neurotransmitters in active inference",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3313.53,
    "end": 3337.69,
    "text": " different neurotransmitters and they're sort of like putative or coarse-grained or hypothesized computational role and different studies that support that computational role for that neurotransmitter",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3339.386,
    "end": 3346.674,
    "text": " Not saying it's like the only thing it does or the only way to look at it, just they found empirical support for it playing that statistical role in those experiments.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3350.237,
    "end": 3358.526,
    "text": "Then some early moves towards integrating continuous and discrete models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3360.578,
    "end": 3362.639,
    "text": " which returns later in the second half of the book.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3363.24,
    "end": 3369.905,
    "text": "And the closing figure is showing all of the three examples from neuroanatomy together.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3370.885,
    "end": 3388.898,
    "text": "Planning with those dopaminergic brain regions, then the prefrontal cortical regions on top, and the spinal reflex motor behavioral arc with action.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3390.441,
    "end": 3405.805,
    "text": " So it's like when we've been talking about planning as inference and prediction and generalization, categorization as inference, learning as inference, action as inference, this is the graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3410.527,
    "end": 3411.567,
    "text": "So it's a different chapter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3412.007,
    "end": 3413.568,
    "text": "It's not as equation driven.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3415.055,
    "end": 3435.565,
    "text": " and uh it's primarily based on those examples of mammal and generalized computational neuroanatomy so it's a cool last chapter for the first half all right any other last comments",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3447.694,
    "end": 3448.863,
    "text": " Okay, I'll stop recording.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3448.923,
    "end": 3449.205,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  }
]
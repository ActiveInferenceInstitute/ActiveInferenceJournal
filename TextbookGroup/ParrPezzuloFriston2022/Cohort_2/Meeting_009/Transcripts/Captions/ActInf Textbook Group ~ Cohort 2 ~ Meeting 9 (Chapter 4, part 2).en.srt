1
00:00:01,199 --> 00:00:04,259
hello everyone thanks for joining it's

2
00:00:04,259 --> 00:00:07,980
October 28 2022

3
00:00:07,980 --> 00:00:12,059
active textbook group cohort 2

4
00:00:12,059 --> 00:00:15,200
run the ninth

5
00:00:16,079 --> 00:00:19,220
oh we're in the

6
00:00:23,820 --> 00:00:26,580
every the weeks are shifted back by one

7
00:00:26,580 --> 00:00:29,479
is that true

8
00:00:30,539 --> 00:00:33,780
there's two 21s

9
00:00:33,780 --> 00:00:36,899
or there's been some kind of other time

10
00:00:36,899 --> 00:00:39,620
glitch

11
00:00:40,500 --> 00:00:43,860
but today is 10 28.

12
00:00:43,860 --> 00:00:46,579
anyways

13
00:00:46,620 --> 00:00:48,680
um

14
00:00:49,620 --> 00:00:51,960
we're discussing chapter four

15
00:00:51,960 --> 00:00:53,700
continuing our discussion of chapter

16
00:00:53,700 --> 00:00:55,379
four

17
00:00:55,379 --> 00:00:59,640
so where would anyone like to

18
00:00:59,640 --> 00:01:01,860
start with

19
00:01:01,860 --> 00:01:04,920
does anyone have a general chapter four

20
00:01:04,920 --> 00:01:08,040
remark or a particular question

21
00:01:08,040 --> 00:01:12,299
that they want to look at

22
00:01:12,299 --> 00:01:15,860
and that or anything else

23
00:01:22,860 --> 00:01:26,479
uh Daniel it's brumlin

24
00:01:27,180 --> 00:01:30,299
um there's just a lot in chapter four

25
00:01:30,299 --> 00:01:33,780
obviously and it's sort of I think it

26
00:01:33,780 --> 00:01:36,600
takes in a lot of

27
00:01:36,600 --> 00:01:38,820
different aspects of active inference

28
00:01:38,820 --> 00:01:40,680
and then sort of parse them all in there

29
00:01:40,680 --> 00:01:42,720
and then throws out a whole lot of maths

30
00:01:42,720 --> 00:01:45,720
so you know

31
00:01:45,720 --> 00:01:48,659
it's unbelievably dense but

32
00:01:48,659 --> 00:01:51,060
and and you know I don't know any math

33
00:01:51,060 --> 00:01:52,020
so

34
00:01:52,020 --> 00:01:54,060
but interestingly as I read it there's

35
00:01:54,060 --> 00:01:56,100
some sort of themes that come out that

36
00:01:56,100 --> 00:01:57,780
are sort of simplistic

37
00:01:57,780 --> 00:02:00,540
and I'm wondering if somehow that can be

38
00:02:00,540 --> 00:02:02,040
sort of

39
00:02:02,040 --> 00:02:05,040
if you can sort of summarize that today

40
00:02:05,040 --> 00:02:07,259
if that makes sense

41
00:02:07,259 --> 00:02:09,239
um

42
00:02:09,239 --> 00:02:11,720
they're just sort of like a

43
00:02:11,720 --> 00:02:14,340
capsulated um

44
00:02:14,340 --> 00:02:19,940
paragraph of the process it goes through

45
00:02:20,819 --> 00:02:22,920
that was one thing the other thing was

46
00:02:22,920 --> 00:02:24,239
um

47
00:02:24,239 --> 00:02:26,879
the Box 4.1 the message passing and

48
00:02:26,879 --> 00:02:28,500
inference so I didn't quite get the

49
00:02:28,500 --> 00:02:30,000
difference between variational message

50
00:02:30,000 --> 00:02:33,739
passing and the belief propagation

51
00:02:34,879 --> 00:02:38,700
whether it can talk a bit about that

52
00:02:38,700 --> 00:02:41,700
okay

53
00:02:45,239 --> 00:02:49,400
yeah that's about it really

54
00:03:04,800 --> 00:03:08,360
okay does anyone have any

55
00:03:08,700 --> 00:03:11,580
thought on this in chat or or just

56
00:03:11,580 --> 00:03:14,480
anything else to add

57
00:03:14,640 --> 00:03:17,720
or we can

58
00:03:18,060 --> 00:03:21,260
try to address these

59
00:03:36,780 --> 00:03:39,440
okay

60
00:03:47,519 --> 00:03:50,519
well one way to look at the theme

61
00:03:50,519 --> 00:03:54,180
of chapter four or the kind of

62
00:03:54,180 --> 00:03:57,239
looking beyond all of the technical

63
00:03:57,239 --> 00:03:59,400
details

64
00:03:59,400 --> 00:04:01,799
and the technical details it's kind of

65
00:04:01,799 --> 00:04:04,019
like an iceberg because they could show

66
00:04:04,019 --> 00:04:06,780
a little or they could show a ton

67
00:04:06,780 --> 00:04:08,760
every time they show one they could also

68
00:04:08,760 --> 00:04:10,140
show a whole

69
00:04:10,140 --> 00:04:14,420
set of ways to get to it or not

70
00:04:15,900 --> 00:04:18,000
so it's kind of like

71
00:04:18,000 --> 00:04:20,060
um

72
00:04:20,160 --> 00:04:23,600
uh maybe

73
00:04:23,699 --> 00:04:25,620
it's like it's like a variable of the

74
00:04:25,620 --> 00:04:27,660
text how much of the math they just

75
00:04:27,660 --> 00:04:31,320
mentioned versus explicitly write out

76
00:04:31,320 --> 00:04:33,060
because there's a lot of papers with no

77
00:04:33,060 --> 00:04:35,639
equations at all like a philosophy paper

78
00:04:35,639 --> 00:04:37,979
that will still say because of the

79
00:04:37,979 --> 00:04:39,900
variation of re-energy or because the

80
00:04:39,900 --> 00:04:42,419
generative model is this way

81
00:04:42,419 --> 00:04:45,360
so kind of referencing the same topic

82
00:04:45,360 --> 00:04:47,460
and so but different texts differ in how

83
00:04:47,460 --> 00:04:50,580
much formalism they have

84
00:04:50,580 --> 00:04:52,860
what is the role of the generative model

85
00:04:52,860 --> 00:04:56,180
in active inference

86
00:04:58,259 --> 00:05:02,060
what does anyone think about that

87
00:05:09,960 --> 00:05:11,040
um

88
00:05:11,040 --> 00:05:13,259
or in the textbook where is any quote

89
00:05:13,259 --> 00:05:15,360
where they use where they say a

90
00:05:15,360 --> 00:05:18,419
generative model is blank

91
00:05:18,419 --> 00:05:20,460
or this is the role of generative model

92
00:05:20,460 --> 00:05:22,199
we can look for some too or any other

93
00:05:22,199 --> 00:05:24,979
thought someone has

94
00:05:29,280 --> 00:05:31,500
at the end I'm mean I think it says that

95
00:05:31,500 --> 00:05:34,860
they that they can vary

96
00:05:34,860 --> 00:05:37,259
depending on on the inference problem

97
00:05:37,259 --> 00:05:41,419
that's one thing about generative models

98
00:05:54,900 --> 00:05:56,759
yeah another

99
00:05:56,759 --> 00:06:00,300
way to that that variation point is

100
00:06:00,300 --> 00:06:01,740
really important

101
00:06:01,740 --> 00:06:04,320
the variation is sometimes described

102
00:06:04,320 --> 00:06:07,500
with like fine tuning within a given

103
00:06:07,500 --> 00:06:09,180
inference problem

104
00:06:09,180 --> 00:06:11,520
which is sometimes framed as perception

105
00:06:11,520 --> 00:06:15,479
in the most fast time scale and learning

106
00:06:15,479 --> 00:06:19,259
and memory in a medium time scale

107
00:06:19,259 --> 00:06:21,800
and then also

108
00:06:21,800 --> 00:06:24,960
different implementations or different

109
00:06:24,960 --> 00:06:27,960
like model structures can vary across

110
00:06:27,960 --> 00:06:30,240
inference problems

111
00:06:30,240 --> 00:06:32,720
hmm

112
00:06:37,620 --> 00:06:39,300
there's also different ways to like

113
00:06:39,300 --> 00:06:42,300
write and see the generative model

114
00:06:42,300 --> 00:06:46,199
can anyone just like list or suggest

115
00:06:46,199 --> 00:06:48,300
how are generative models shown or

116
00:06:48,300 --> 00:06:50,900
represented

117
00:07:06,180 --> 00:07:11,300
so is that the um the effect to graphs

118
00:07:15,000 --> 00:07:16,800
in which section

119
00:07:16,800 --> 00:07:19,800
figure 4.3

120
00:07:19,800 --> 00:07:22,520
is it

121
00:07:31,020 --> 00:07:34,020
4.2

122
00:07:34,139 --> 00:07:35,639
so how

123
00:07:35,639 --> 00:07:37,819
um yeah

124
00:07:37,819 --> 00:07:40,440
one View

125
00:07:40,440 --> 00:07:43,020
of the generative model

126
00:07:43,020 --> 00:07:46,680
is like this figure 4.3

127
00:07:46,680 --> 00:07:48,900
kind of

128
00:07:48,900 --> 00:07:51,780
classic slash common representation

129
00:07:51,780 --> 00:07:54,780
the base graph View

130
00:07:54,780 --> 00:07:57,319
with the nodes being a random variable

131
00:07:57,319 --> 00:08:01,740
and edges being relationships among

132
00:08:01,740 --> 00:08:03,120
variables

133
00:08:03,120 --> 00:08:05,520
and so like if if every variable were

134
00:08:05,520 --> 00:08:07,380
connected to every variable

135
00:08:07,380 --> 00:08:10,139
like every time Point were causal at

136
00:08:10,139 --> 00:08:12,060
every time point

137
00:08:12,060 --> 00:08:14,460
every hidden Factor influence every

138
00:08:14,460 --> 00:08:16,860
hidden Factor you just have like a fully

139
00:08:16,860 --> 00:08:17,940
connected

140
00:08:17,940 --> 00:08:20,280
all by all

141
00:08:20,280 --> 00:08:23,360
causal Association Matrix

142
00:08:23,360 --> 00:08:26,879
so that's like the weakest World model

143
00:08:26,879 --> 00:08:30,240
and then you can say well the the

144
00:08:30,240 --> 00:08:33,839
present is like the only intermediating

145
00:08:33,839 --> 00:08:36,120
Markov blanket between the future and

146
00:08:36,120 --> 00:08:37,200
the past

147
00:08:37,200 --> 00:08:40,140
or this is the you know perception is

148
00:08:40,140 --> 00:08:42,479
the only intermediating Markov blanket

149
00:08:42,479 --> 00:08:46,500
between observations and hidden States

150
00:08:46,500 --> 00:08:50,600
if you are willing to take those

151
00:08:51,240 --> 00:08:53,399
um structural assumptions and embody

152
00:08:53,399 --> 00:08:55,680
them in the model the sparsity of the

153
00:08:55,680 --> 00:08:57,120
model

154
00:08:57,120 --> 00:08:59,700
under many different ways of calculating

155
00:08:59,700 --> 00:09:00,779
the model

156
00:09:00,779 --> 00:09:04,260
it will it just like always gets easier

157
00:09:04,260 --> 00:09:05,880
you might be restricting yourself to a

158
00:09:05,880 --> 00:09:08,279
very local neighborhood of models

159
00:09:08,279 --> 00:09:10,140
but pretty much the more you can

160
00:09:10,140 --> 00:09:12,540
constrain the edges and the

161
00:09:12,540 --> 00:09:14,940
combinatorics

162
00:09:14,940 --> 00:09:17,760
people cannot add other remarks on this

163
00:09:17,760 --> 00:09:20,700
but you're always simpler models are

164
00:09:20,700 --> 00:09:21,959
just

165
00:09:21,959 --> 00:09:25,699
simply easier to fit

166
00:09:25,820 --> 00:09:29,459
better I mean better optimization

167
00:09:29,459 --> 00:09:33,360
is always achieved by making

168
00:09:33,360 --> 00:09:37,680
quadratic or curved type assumptions

169
00:09:37,680 --> 00:09:39,720
it's like a general optimization

170
00:09:39,720 --> 00:09:43,560
principle to make things on a smoother

171
00:09:43,560 --> 00:09:47,880
more incrementally updatable

172
00:09:47,940 --> 00:09:50,959
related problem

173
00:10:01,700 --> 00:10:03,660
foreign

174
00:10:03,660 --> 00:10:07,200
they deal with time differently

175
00:10:07,200 --> 00:10:09,540
they deal with everything differently

176
00:10:09,540 --> 00:10:11,880
time slash

177
00:10:11,880 --> 00:10:15,680
space Etc differently

178
00:10:16,019 --> 00:10:19,140
I'm sorry can you hear me now yes Ali go

179
00:10:19,140 --> 00:10:21,319
ahead

180
00:10:21,480 --> 00:10:25,860
uh yeah uh I just wanted to mention this

181
00:10:25,860 --> 00:10:29,220
paper a clear look through densities

182
00:10:29,220 --> 00:10:31,160
which

183
00:10:31,160 --> 00:10:34,820
uh clears up some of the uh

184
00:10:34,820 --> 00:10:36,839
misunderstandings or misconceptions

185
00:10:36,839 --> 00:10:40,980
around a generative model because more

186
00:10:40,980 --> 00:10:42,779
often than not people think of

187
00:10:42,779 --> 00:10:45,060
generative model as just

188
00:10:45,060 --> 00:10:50,940
a structural representation of the uh of

189
00:10:50,940 --> 00:10:54,420
the entire of the external world but in

190
00:10:54,420 --> 00:10:56,839
active inference

191
00:10:56,839 --> 00:11:00,980
the generative model is uh usually

192
00:11:00,980 --> 00:11:03,660
represented in terms of its recognition

193
00:11:03,660 --> 00:11:09,620
density so this recognition density is

194
00:11:09,620 --> 00:11:14,040
something that enables us to

195
00:11:14,040 --> 00:11:16,680
uh or let's put it this way it's an

196
00:11:16,680 --> 00:11:18,839
approximate posterior probability

197
00:11:18,839 --> 00:11:22,200
density that inverts generative mapping

198
00:11:22,200 --> 00:11:25,620
from consequences to causes so it

199
00:11:25,620 --> 00:11:30,660
enables us to uh recognize the hidden

200
00:11:30,660 --> 00:11:34,820
States based on the observations and

201
00:11:34,820 --> 00:11:37,800
inferring the causes of sensory outcomes

202
00:11:37,800 --> 00:11:42,540
and not not just simply as a static

203
00:11:42,540 --> 00:11:46,519
representation of the external world

204
00:11:51,300 --> 00:11:53,300
um

205
00:11:55,260 --> 00:11:57,180
great Point Ali

206
00:11:57,180 --> 00:11:59,940
to like kind of unpack this

207
00:11:59,940 --> 00:12:02,339
and and this is a great

208
00:12:02,339 --> 00:12:06,060
um great entry point

209
00:12:06,060 --> 00:12:08,579
um the The Tale of Two Cities is a book

210
00:12:08,579 --> 00:12:11,040
about two cities

211
00:12:11,040 --> 00:12:13,920
uh the tale of two densities is some

212
00:12:13,920 --> 00:12:16,740
work with ramstead and others and

213
00:12:16,740 --> 00:12:18,440
there's a

214
00:12:18,440 --> 00:12:20,820
uh at least some

215
00:12:20,820 --> 00:12:22,500
stream on it

216
00:12:22,500 --> 00:12:24,600
and the two densities are like the two

217
00:12:24,600 --> 00:12:28,140
directions which a Bayesian type model

218
00:12:28,140 --> 00:12:31,260
can be used which is in its recognition

219
00:12:31,260 --> 00:12:32,700
capacity

220
00:12:32,700 --> 00:12:36,420
and in its generative capacity

221
00:12:36,420 --> 00:12:40,740
so one point is the difference between a

222
00:12:40,740 --> 00:12:42,420
sort of like descriptive statistical

223
00:12:42,420 --> 00:12:44,959
approach

224
00:12:48,120 --> 00:12:50,820
whether with p-values

225
00:12:50,820 --> 00:12:54,240
or any other type of um statistics where

226
00:12:54,240 --> 00:12:57,060
you're just taking observables only and

227
00:12:57,060 --> 00:12:59,160
describing them statistically

228
00:12:59,160 --> 00:13:01,440
or summarizing them

229
00:13:01,440 --> 00:13:03,959
that the two densities

230
00:13:03,959 --> 00:13:06,959
are those two bases of the Bayesian

231
00:13:06,959 --> 00:13:08,100
model

232
00:13:08,100 --> 00:13:11,220
which is about its capacity to generate

233
00:13:11,220 --> 00:13:13,440
data from hidden States

234
00:13:13,440 --> 00:13:16,800
and as Ali said to be inverted

235
00:13:16,800 --> 00:13:20,880
or to be able to perform inference on

236
00:13:20,880 --> 00:13:23,600
hidden States from

237
00:13:23,600 --> 00:13:25,320
observables

238
00:13:25,320 --> 00:13:28,560
and that is exactly

239
00:13:28,560 --> 00:13:30,300
the

240
00:13:30,300 --> 00:13:31,980
um distinction

241
00:13:31,980 --> 00:13:35,279
of s and O

242
00:13:35,279 --> 00:13:37,519
s r

243
00:13:37,519 --> 00:13:40,620
unobserved unobserved hidden States

244
00:13:40,620 --> 00:13:42,360
of the world

245
00:13:42,360 --> 00:13:47,120
which in a causal graphical framework

246
00:13:47,579 --> 00:13:49,260
there's different interpretations of

247
00:13:49,260 --> 00:13:51,240
what it means

248
00:13:51,240 --> 00:13:53,399
that causal

249
00:13:53,399 --> 00:13:57,480
Force but it's it's the GM family that

250
00:13:57,480 --> 00:14:00,380
one is embodying

251
00:14:01,380 --> 00:14:03,660
and then the observations are up

252
00:14:03,660 --> 00:14:05,399
so

253
00:14:05,399 --> 00:14:06,440
blue

254
00:14:06,440 --> 00:14:08,579
I'm sorry I just was going to add to

255
00:14:08,579 --> 00:14:11,459
that if you're if you're dead Okay

256
00:14:11,459 --> 00:14:13,500
um I just wanted to I always love

257
00:14:13,500 --> 00:14:15,660
listening to that live stream and

258
00:14:15,660 --> 00:14:18,300
Maxwell really stated it really nicely

259
00:14:18,300 --> 00:14:20,339
like the differences between

260
00:14:20,339 --> 00:14:22,200
um the recognition density and the

261
00:14:22,200 --> 00:14:26,100
generative density uh it was how he

262
00:14:26,100 --> 00:14:28,139
described it was like if you're stacking

263
00:14:28,139 --> 00:14:30,600
up dominoes

264
00:14:30,600 --> 00:14:33,000
um the this and then you're gonna knock

265
00:14:33,000 --> 00:14:35,459
them over right like so the recognition

266
00:14:35,459 --> 00:14:37,440
density is like the stack of dominoes

267
00:14:37,440 --> 00:14:39,480
and then the generative density is like

268
00:14:39,480 --> 00:14:41,100
the action occurring between The

269
00:14:41,100 --> 00:14:42,899
Dominoes falling down like I I always

270
00:14:42,899 --> 00:14:44,699
like that and it's locked into my mind

271
00:14:44,699 --> 00:14:46,620
and really kind of helps me get a grip

272
00:14:46,620 --> 00:14:49,760
on maybe the differences

273
00:15:02,699 --> 00:15:05,300
okay

274
00:15:05,399 --> 00:15:07,260
um Brock or

275
00:15:07,260 --> 00:15:11,459
someone what have you added this quote

276
00:15:11,459 --> 00:15:13,699
for

277
00:15:17,160 --> 00:15:22,920
um just on page 64 this is like a

278
00:15:22,920 --> 00:15:26,060
referencing equation 4.1 which is uh

279
00:15:26,060 --> 00:15:29,820
right here it just says that um

280
00:15:29,820 --> 00:15:34,170
that the generative model is

281
00:15:34,170 --> 00:15:34,560
[Music]

282
00:15:34,560 --> 00:15:36,240
um

283
00:15:36,240 --> 00:15:39,480
the point that prior and likelihood are

284
00:15:39,480 --> 00:15:41,160
sufficient to compute model evidence and

285
00:15:41,160 --> 00:15:43,579
probability

286
00:15:43,860 --> 00:15:45,180
so

287
00:15:45,180 --> 00:15:47,100
I guess it's a slightly more formal

288
00:15:47,100 --> 00:15:49,440
specific definition

289
00:15:49,440 --> 00:15:52,079
and narrow because right after that it

290
00:15:52,079 --> 00:15:53,220
talks about

291
00:15:53,220 --> 00:15:55,560
um computational tractability of doing

292
00:15:55,560 --> 00:15:57,180
that and

293
00:15:57,180 --> 00:16:01,560
that seems like the best

294
00:16:01,560 --> 00:16:04,939
starting point of

295
00:16:05,339 --> 00:16:08,100
the generative model is blank in the

296
00:16:08,100 --> 00:16:11,420
book that I was able to find

297
00:16:11,459 --> 00:16:13,500
nice good

298
00:16:13,500 --> 00:16:16,380
summary like

299
00:16:16,380 --> 00:16:18,480
in the Frog

300
00:16:18,480 --> 00:16:21,860
jumping example

301
00:16:23,040 --> 00:16:26,100
prior and likelihood

302
00:16:26,100 --> 00:16:27,779
left side

303
00:16:27,779 --> 00:16:29,180
prior

304
00:16:29,180 --> 00:16:33,000
beliefs on likelihood of the Hidden

305
00:16:33,000 --> 00:16:34,560
state

306
00:16:34,560 --> 00:16:37,199
on the probability of the Hidden state

307
00:16:37,199 --> 00:16:40,380
before the observation

308
00:16:40,380 --> 00:16:43,019
likelihood model

309
00:16:43,019 --> 00:16:45,839
the larger

310
00:16:45,839 --> 00:16:48,000
Matrix

311
00:16:48,000 --> 00:16:51,300
it has a different dimensionality

312
00:16:51,300 --> 00:16:55,019
than this one the prior

313
00:16:55,019 --> 00:16:59,600
and it's the mapping of different things

314
00:17:00,120 --> 00:17:02,759
then there's an observation

315
00:17:02,759 --> 00:17:05,939
but in a sense none of these beliefs or

316
00:17:05,939 --> 00:17:08,939
likelihoods models are observed

317
00:17:08,939 --> 00:17:11,579
the observation o

318
00:17:11,579 --> 00:17:13,740
which is like the experimental data

319
00:17:13,740 --> 00:17:16,500
actually being collected

320
00:17:16,500 --> 00:17:18,299
is here

321
00:17:18,299 --> 00:17:20,880
and then everything else are like the

322
00:17:20,880 --> 00:17:23,880
cognitive States or internal States

323
00:17:23,880 --> 00:17:26,939
or States as modeled by the

324
00:17:26,939 --> 00:17:29,520
modeler

325
00:17:29,520 --> 00:17:32,940
and then there's some updating

326
00:17:32,940 --> 00:17:34,620
and in chapter two that was being

327
00:17:34,620 --> 00:17:36,900
described like in terms of

328
00:17:36,900 --> 00:17:40,919
the surprise self-information

329
00:17:40,919 --> 00:17:43,559
Bayesian surprise

330
00:17:43,559 --> 00:17:46,140
but in the context of what Brock just

331
00:17:46,140 --> 00:17:48,539
read prior in the likelihood comprise a

332
00:17:48,539 --> 00:17:50,280
generative model

333
00:17:50,280 --> 00:17:53,600
prior likelihood

334
00:17:53,940 --> 00:17:56,820
the already composed a generative model

335
00:17:56,820 --> 00:17:58,620
but it's like it's like a car that

336
00:17:58,620 --> 00:18:01,260
hasn't been run

337
00:18:01,260 --> 00:18:03,539
and then there's some that's when data

338
00:18:03,539 --> 00:18:07,280
starts being able to update the model

339
00:18:08,880 --> 00:18:13,679
so that's why many times uh we see

340
00:18:13,679 --> 00:18:16,020
equations

341
00:18:16,020 --> 00:18:19,080
like a Bayesian

342
00:18:19,080 --> 00:18:20,760
equation

343
00:18:20,760 --> 00:18:24,000
whether represented graphically

344
00:18:24,000 --> 00:18:25,860
or with like

345
00:18:25,860 --> 00:18:29,880
um in like a just a single line form

346
00:18:29,880 --> 00:18:33,539
uh because to specify a Bayesian

347
00:18:33,539 --> 00:18:36,480
statistical problem

348
00:18:36,480 --> 00:18:38,760
is to have specified the generative

349
00:18:38,760 --> 00:18:40,440
model

350
00:18:40,440 --> 00:18:44,539
and vice versa Ali

351
00:18:45,660 --> 00:18:49,080
uh and I think it's a general issue

352
00:18:49,080 --> 00:18:51,720
regarding uh the whole active inference

353
00:18:51,720 --> 00:18:54,600
literature because whenever we read a

354
00:18:54,600 --> 00:18:56,960
paper an active inference we need to be

355
00:18:56,960 --> 00:19:00,720
quite clear about uh what do the authors

356
00:19:00,720 --> 00:19:03,480
mean by each concept because for

357
00:19:03,480 --> 00:19:06,440
instance in this that paper from

358
00:19:06,440 --> 00:19:09,780
2022 learning generative models for

359
00:19:09,780 --> 00:19:12,799
active inference using tensor Networks

360
00:19:12,799 --> 00:19:17,100
uh they have defined generative model uh

361
00:19:17,100 --> 00:19:20,520
in its traditional statistical form and

362
00:19:20,520 --> 00:19:24,240
not exactly uh it doesn't exactly maps

363
00:19:24,240 --> 00:19:28,559
onto uh this somehow more uh

364
00:19:28,559 --> 00:19:31,200
comprehensive or modified definition of

365
00:19:31,200 --> 00:19:33,440
generative model in terms of recognition

366
00:19:33,440 --> 00:19:37,460
density and so on so uh yeah

367
00:19:37,460 --> 00:19:40,559
and it also extends to every other

368
00:19:40,559 --> 00:19:45,360
concept we encounter in any other active

369
00:19:45,360 --> 00:19:48,139
inference paper

370
00:19:50,160 --> 00:19:52,760
foreign

371
00:20:09,919 --> 00:20:12,780
purpose of just bringing back to chapter

372
00:20:12,780 --> 00:20:13,740
two

373
00:20:13,740 --> 00:20:15,660
and this example

374
00:20:15,660 --> 00:20:18,539
is just to

375
00:20:18,539 --> 00:20:19,160
um

376
00:20:19,160 --> 00:20:22,860
situate everything that's happening with

377
00:20:22,860 --> 00:20:24,720
the

378
00:20:24,720 --> 00:20:29,100
theoretical like data free math of

379
00:20:29,100 --> 00:20:31,260
active

380
00:20:31,260 --> 00:20:34,860
and also the data oriented aspect of

381
00:20:34,860 --> 00:20:36,419
active

382
00:20:36,419 --> 00:20:38,700
which is

383
00:20:38,700 --> 00:20:40,080
uh

384
00:20:40,080 --> 00:20:42,900
to give a deflationary perspective a

385
00:20:42,900 --> 00:20:46,559
specified Bayesian statistical model

386
00:20:46,559 --> 00:20:48,660
and then data

387
00:20:48,660 --> 00:20:51,299
that that model has

388
00:20:51,299 --> 00:20:54,539
legibility around

389
00:20:54,539 --> 00:20:56,039
of course you can

390
00:20:56,039 --> 00:20:58,320
you know have it you know it could be

391
00:20:58,320 --> 00:21:01,260
the wrong model for the situation

392
00:21:01,260 --> 00:21:03,480
and so on but

393
00:21:03,480 --> 00:21:05,460
this is like this is like the

394
00:21:05,460 --> 00:21:07,980
statistical scenario

395
00:21:07,980 --> 00:21:10,559
which is just prior

396
00:21:10,559 --> 00:21:12,360
likelihood

397
00:21:12,360 --> 00:21:14,220
data

398
00:21:14,220 --> 00:21:16,200
and then this posterior can become that

399
00:21:16,200 --> 00:21:17,580
one's prior

400
00:21:17,580 --> 00:21:20,100
and that's just a

401
00:21:20,100 --> 00:21:23,220
all right yeah can be modeled different

402
00:21:23,220 --> 00:21:24,360
ways

403
00:21:24,360 --> 00:21:28,280
in continuous time industry time

404
00:21:28,740 --> 00:21:30,780
and again the whole point is just reduce

405
00:21:30,780 --> 00:21:32,580
the uh

406
00:21:32,580 --> 00:21:34,559
theoretical

407
00:21:34,559 --> 00:21:38,640
novelty of active inference

408
00:21:38,640 --> 00:21:41,580
because uh it's

409
00:21:41,580 --> 00:21:44,580
more like

410
00:21:44,580 --> 00:21:48,559
Bayesian statistical modeling

411
00:21:48,780 --> 00:21:50,760
in this

412
00:21:50,760 --> 00:21:52,919
mode

413
00:21:52,919 --> 00:21:56,580
than many other things

414
00:21:56,580 --> 00:21:58,679
and so a lot of the questions and it's

415
00:21:58,679 --> 00:22:00,059
it's there there are great questions

416
00:22:00,059 --> 00:22:01,740
that come up and have come up in

417
00:22:01,740 --> 00:22:03,900
statistics before but kind of in a new

418
00:22:03,900 --> 00:22:05,700
way here

419
00:22:05,700 --> 00:22:07,860
like these are General statistical

420
00:22:07,860 --> 00:22:10,260
questions they're statistical questions

421
00:22:10,260 --> 00:22:14,000
involving action

422
00:22:14,400 --> 00:22:20,240
building on some of the causal

423
00:22:20,520 --> 00:22:23,580
statistical developments from Pearl

424
00:22:23,580 --> 00:22:26,100
and others

425
00:22:26,100 --> 00:22:27,419
who

426
00:22:27,419 --> 00:22:31,679
brought in like just the concept of the

427
00:22:31,679 --> 00:22:35,000
Markov boundary

428
00:22:35,159 --> 00:22:38,640
in in that like 90s 80s and 90s and

429
00:22:38,640 --> 00:22:41,280
Beyond period

430
00:22:41,280 --> 00:22:45,500
um it's using those kinds of

431
00:22:45,500 --> 00:22:48,059
statistical techniques

432
00:22:48,059 --> 00:22:50,820
developed for Bayesian statistical

433
00:22:50,820 --> 00:22:53,520
settings in the last

434
00:22:53,520 --> 00:22:57,179
five to fifty years

435
00:22:57,179 --> 00:23:00,720
in the setting of action

436
00:23:00,720 --> 00:23:04,760
and Niche Dynamics

437
00:23:06,059 --> 00:23:07,740
also

438
00:23:07,740 --> 00:23:09,179
connecting

439
00:23:09,179 --> 00:23:11,220
in some way that's not fully

440
00:23:11,220 --> 00:23:13,080
characterized yet

441
00:23:13,080 --> 00:23:17,039
with some

442
00:23:17,700 --> 00:23:21,059
uh more physics like

443
00:23:21,059 --> 00:23:23,280
formalizations

444
00:23:23,280 --> 00:23:26,899
like the Bayesian mechanics

445
00:23:27,960 --> 00:23:29,700
but that's not the first kind of

446
00:23:29,700 --> 00:23:30,660
connection between

447
00:23:30,660 --> 00:23:31,860
[Music]

448
00:23:31,860 --> 00:23:34,860
statistical optimization

449
00:23:34,860 --> 00:23:36,299
and

450
00:23:36,299 --> 00:23:39,500
physics theories

451
00:23:40,140 --> 00:23:41,940
which is kind of the whole idea of like

452
00:23:41,940 --> 00:23:44,460
why you can kind of roll the ball to the

453
00:23:44,460 --> 00:23:46,080
bottom of the hill physically and then

454
00:23:46,080 --> 00:23:48,240
the statistical optimization works like

455
00:23:48,240 --> 00:23:50,720
that too

456
00:23:53,460 --> 00:23:54,720
so

457
00:23:54,720 --> 00:23:56,880
degenerative model chapter to kind of

458
00:23:56,880 --> 00:23:59,159
return to four and

459
00:23:59,159 --> 00:24:00,659
the

460
00:24:00,659 --> 00:24:02,460
first part of the

461
00:24:02,460 --> 00:24:05,640
of bronwyn's earlier question I think is

462
00:24:05,640 --> 00:24:08,000
this chapter is

463
00:24:08,000 --> 00:24:11,880
describing many important aspects

464
00:24:11,880 --> 00:24:14,159
and two fundamental

465
00:24:14,159 --> 00:24:16,740
types of generative models

466
00:24:16,740 --> 00:24:20,520
those dealing with active inference in a

467
00:24:20,520 --> 00:24:23,100
discrete time formalization and a

468
00:24:23,100 --> 00:24:24,780
continuous time formalization because

469
00:24:24,780 --> 00:24:26,400
there's also other ways to do it

470
00:24:26,400 --> 00:24:29,179
probably

471
00:24:30,179 --> 00:24:32,580
it's just giving

472
00:24:32,580 --> 00:24:35,940
technical details

473
00:24:35,940 --> 00:24:38,280
on what those generative models are

474
00:24:38,280 --> 00:24:42,440
because that is the entity description

475
00:24:42,480 --> 00:24:45,000
that's the unit of perception cognition

476
00:24:45,000 --> 00:24:48,299
action that's the empirical model as

477
00:24:48,299 --> 00:24:51,179
used in the computer

478
00:24:51,179 --> 00:24:55,200
to recognize data to generate data

479
00:24:55,200 --> 00:24:58,320
so this is like kind of the core a core

480
00:24:58,320 --> 00:25:00,860
idea

481
00:25:01,559 --> 00:25:05,299
and sometimes it's dealt and like um

482
00:25:05,299 --> 00:25:08,400
yeah sometimes it's just mentioned but

483
00:25:08,400 --> 00:25:10,200
when it's mentioned that's why this is

484
00:25:10,200 --> 00:25:12,000
the textbook

485
00:25:12,000 --> 00:25:13,860
so that in some other velocity paper

486
00:25:13,860 --> 00:25:15,299
they're they're not going to have this

487
00:25:15,299 --> 00:25:18,840
but this is kind of

488
00:25:18,840 --> 00:25:21,720
what is being discussed when those terms

489
00:25:21,720 --> 00:25:23,340
are used in

490
00:25:23,340 --> 00:25:27,179
qualitative settings or in conversation

491
00:25:27,179 --> 00:25:28,919
with all these caveat of course that

492
00:25:28,919 --> 00:25:31,919
everyone is using it differently

493
00:25:31,919 --> 00:25:34,380
and then in terms of like

494
00:25:34,380 --> 00:25:36,179
scientific

495
00:25:36,179 --> 00:25:39,779
review and in uh evaluating like a

496
00:25:39,779 --> 00:25:42,539
technology or some claim or something

497
00:25:42,539 --> 00:25:44,039
if someone says the generative model

498
00:25:44,039 --> 00:25:46,980
does this or it has this characteristic

499
00:25:46,980 --> 00:25:50,520
these are like the specific aspects

500
00:25:50,520 --> 00:25:53,120
that

501
00:25:53,940 --> 00:25:58,460
would characterize that claim being true

502
00:26:06,200 --> 00:26:09,480
sounds good very interesting

503
00:26:09,480 --> 00:26:10,980
um discussion on

504
00:26:10,980 --> 00:26:13,279
degenerative model

505
00:26:13,279 --> 00:26:16,940
how about the Box

506
00:26:36,000 --> 00:26:38,779
Ollie

507
00:26:39,200 --> 00:26:42,480
well as far as I understand it the

508
00:26:42,480 --> 00:26:44,100
difference between variational message

509
00:26:44,100 --> 00:26:47,340
passing and belief propagation uh

510
00:26:47,340 --> 00:26:50,340
basically comes to

511
00:26:50,340 --> 00:26:52,380
um the difference

512
00:26:52,380 --> 00:26:56,700
um in a holistic uh view of the

513
00:26:56,700 --> 00:26:59,580
approximate inference computation and

514
00:26:59,580 --> 00:27:01,820
the recursive or

515
00:27:01,820 --> 00:27:05,700
step-by-step computation of it so in

516
00:27:05,700 --> 00:27:07,919
variational message passing

517
00:27:07,919 --> 00:27:13,279
case we actually have the already

518
00:27:13,279 --> 00:27:16,740
actualized trajectory for messages and

519
00:27:16,740 --> 00:27:19,320
we know all the uh conditional

520
00:27:19,320 --> 00:27:23,159
properties of children uh with regard to

521
00:27:23,159 --> 00:27:26,400
their parents uh and uh with those

522
00:27:26,400 --> 00:27:30,679
informations we can compute the whole

523
00:27:30,679 --> 00:27:34,620
approximate free energy of the whole

524
00:27:34,620 --> 00:27:39,600
process but for belief propagation uh we

525
00:27:39,600 --> 00:27:42,480
need to compute that recursively in each

526
00:27:42,480 --> 00:27:44,539
step without

527
00:27:44,539 --> 00:27:48,659
necessarily needing to have all the

528
00:27:48,659 --> 00:27:51,120
information for all the probably

529
00:27:51,120 --> 00:27:54,559
condition proper probabilities of

530
00:27:54,559 --> 00:27:56,600
parents per

531
00:27:56,600 --> 00:27:59,340
I'm sorry children with regard to

532
00:27:59,340 --> 00:28:01,100
parents

533
00:28:01,100 --> 00:28:05,039
beforehand so but another important

534
00:28:05,039 --> 00:28:07,860
difference between them is that uh the

535
00:28:07,860 --> 00:28:10,559
belief propagation uh

536
00:28:10,559 --> 00:28:13,200
uh takes account for all the messages

537
00:28:13,200 --> 00:28:17,700
that has happened up to the point of b

538
00:28:17,700 --> 00:28:21,020
or the point that we're

539
00:28:21,020 --> 00:28:23,760
momentarily dealing with that point so

540
00:28:23,760 --> 00:28:26,900
because of that we

541
00:28:26,900 --> 00:28:30,559
divide the whole process

542
00:28:30,559 --> 00:28:35,820
with the probability of the B because we

543
00:28:35,820 --> 00:28:38,100
don't want to include the b in the

544
00:28:38,100 --> 00:28:42,539
process uh yet so that's basically

545
00:28:42,539 --> 00:28:45,840
something similar to a kind of deductive

546
00:28:45,840 --> 00:28:48,960
way of doing inference and the inductive

547
00:28:48,960 --> 00:28:52,640
uh way of doing that

548
00:29:04,799 --> 00:29:07,200
awesome thank you Ali that's very

549
00:29:07,200 --> 00:29:09,860
insightful

550
00:29:11,940 --> 00:29:14,400
let's look at 4.1 and see where we're

551
00:29:14,400 --> 00:29:17,580
seeing some aspects of this well let's

552
00:29:17,580 --> 00:29:20,039
pull out one more level Ali what would

553
00:29:20,039 --> 00:29:22,380
you say what category

554
00:29:22,380 --> 00:29:26,039
of things do variational message passing

555
00:29:26,039 --> 00:29:29,120
and belief propagation

556
00:29:29,159 --> 00:29:31,020
what is the category that even makes

557
00:29:31,020 --> 00:29:34,340
these things comparable

558
00:29:35,480 --> 00:29:39,240
uh well the similarities between at

559
00:29:39,240 --> 00:29:41,120
least the structural similarity between

560
00:29:41,120 --> 00:29:44,940
these two ways of computation is their

561
00:29:44,940 --> 00:29:49,039
dependency on uh the ex the expectation

562
00:29:49,039 --> 00:29:51,779
of the conditional problem probability

563
00:29:51,779 --> 00:29:55,580
so uh in each case uh we're just

564
00:29:55,580 --> 00:29:58,080
Computing the inference in terms of

565
00:29:58,080 --> 00:29:59,720
their expectation

566
00:29:59,720 --> 00:30:03,899
and as we see uh even the equations uh

567
00:30:03,899 --> 00:30:07,799
look quite similar to each other so but

568
00:30:07,799 --> 00:30:11,039
as they've pointed out uh in the bottom

569
00:30:11,039 --> 00:30:14,520
of the Box uh this is a slightly

570
00:30:14,520 --> 00:30:16,500
non-standard use of the expectation

571
00:30:16,500 --> 00:30:20,640
operator so we need to be aware of this

572
00:30:20,640 --> 00:30:26,279
point as well because uh well if if both

573
00:30:26,279 --> 00:30:29,340
of those expectation operators were

574
00:30:29,340 --> 00:30:32,520
exactly the same there wouldn't be any

575
00:30:32,520 --> 00:30:37,200
uh difference any uh similarities

576
00:30:37,200 --> 00:30:41,299
between those two kinds of

577
00:30:41,520 --> 00:30:44,658
um inferences schemes

578
00:30:46,679 --> 00:30:49,380
all right thanks a lot for that

579
00:30:49,380 --> 00:30:52,559
okay if you notice one is these will be

580
00:30:52,559 --> 00:30:54,419
fun to see the

581
00:30:54,419 --> 00:30:56,100
descriptions

582
00:30:56,100 --> 00:30:58,520
of

583
00:30:59,399 --> 00:31:02,700
and uh just yeah like what what

584
00:31:02,700 --> 00:31:06,600
these expectations are over

585
00:31:06,600 --> 00:31:08,399
keep in mind the expectation here is

586
00:31:08,399 --> 00:31:12,120
about the central tendency of a

587
00:31:12,120 --> 00:31:14,059
statistical distribution

588
00:31:14,059 --> 00:31:18,139
not a Time projection

589
00:31:18,480 --> 00:31:20,640
so talking about

590
00:31:20,640 --> 00:31:24,419
as Ali highlighted the expectations on

591
00:31:24,419 --> 00:31:26,340
variables

592
00:31:26,340 --> 00:31:30,779
that doesn't necessarily have to be

593
00:31:30,779 --> 00:31:33,840
a Time series prediction

594
00:31:33,840 --> 00:31:35,399
in fact

595
00:31:35,399 --> 00:31:38,880
one of the most important uses is just

596
00:31:38,880 --> 00:31:42,240
to calculate a rolling snapshot

597
00:31:42,240 --> 00:31:44,460
in the context of Prior and likelihood

598
00:31:44,460 --> 00:31:49,940
distributions as incoming data come in

599
00:31:50,700 --> 00:31:53,700
so they're like parameter

600
00:31:53,700 --> 00:31:56,539
bidding

601
00:31:56,640 --> 00:32:00,140
modeling approaches

602
00:32:00,600 --> 00:32:03,799
and is it fair to say that for a given

603
00:32:03,799 --> 00:32:07,320
analytically expressed Bayesian equation

604
00:32:07,320 --> 00:32:10,279
so formula

605
00:32:11,039 --> 00:32:12,960
it's possible to

606
00:32:12,960 --> 00:32:16,320
take the approach of of working with it

607
00:32:16,320 --> 00:32:18,360
more along one branch or more along

608
00:32:18,360 --> 00:32:21,139
another branch

609
00:32:23,700 --> 00:32:25,799
is that the case Ali

610
00:32:25,799 --> 00:32:28,500
or anyone else

611
00:32:28,500 --> 00:32:30,899
papers tend to like take one approach or

612
00:32:30,899 --> 00:32:33,059
another but there's also some papers to

613
00:32:33,059 --> 00:32:34,980
probably compare them

614
00:32:34,980 --> 00:32:36,659
but they are they're distinct

615
00:32:36,659 --> 00:32:39,360
implementations

616
00:32:39,360 --> 00:32:41,820
potentially with some

617
00:32:41,820 --> 00:32:45,120
differences in

618
00:32:45,120 --> 00:32:47,220
outcome that would be interesting to

619
00:32:47,220 --> 00:32:49,500
explore but is it the case that they're

620
00:32:49,500 --> 00:32:52,679
both possible to be used

621
00:32:52,679 --> 00:32:55,260
for a given Bayesian equation if we

622
00:32:55,260 --> 00:32:56,940
wanted to do the frog jumping out of the

623
00:32:56,940 --> 00:32:58,080
hands

624
00:32:58,080 --> 00:32:59,460
could there be a variational message

625
00:32:59,460 --> 00:33:01,200
passing and a belief propagation

626
00:33:01,200 --> 00:33:04,200
implementation

627
00:33:05,820 --> 00:33:09,600
uh well I guess so if uh the the kind of

628
00:33:09,600 --> 00:33:12,120
situation we're dealing with doesn't

629
00:33:12,120 --> 00:33:16,159
necessarily uh restrict us to use

630
00:33:16,159 --> 00:33:19,380
the the global over the global view

631
00:33:19,380 --> 00:33:22,440
versus the local view of the inference

632
00:33:22,440 --> 00:33:27,260
then I guess both of them can be applied

633
00:33:34,620 --> 00:33:37,100
okay let's save this way

634
00:33:37,100 --> 00:33:40,679
sorry just yeah

635
00:33:40,679 --> 00:33:44,840
uh yeah the variation message passing is

636
00:33:44,840 --> 00:33:50,340
uh kind of uh let's say omniscient like

637
00:33:50,340 --> 00:33:53,279
view of the situation because we know

638
00:33:53,279 --> 00:33:56,519
all the conditional probabilities of

639
00:33:56,519 --> 00:33:59,940
each note but uh for the belief

640
00:33:59,940 --> 00:34:02,820
propagation we're dealing with the

641
00:34:02,820 --> 00:34:05,580
explore exploit dilemma

642
00:34:05,580 --> 00:34:08,120
hmm

643
00:34:12,300 --> 00:34:14,699
also their computational implementations

644
00:34:14,699 --> 00:34:16,619
could be different

645
00:34:16,619 --> 00:34:19,020
so a given language or package might

646
00:34:19,020 --> 00:34:20,879
only enable one

647
00:34:20,879 --> 00:34:24,020
and um

648
00:34:26,460 --> 00:34:29,540
one other well

649
00:34:30,560 --> 00:34:32,940
let's return in our in the last 20

650
00:34:32,940 --> 00:34:36,418
minutes back to the detection if we go

651
00:34:36,418 --> 00:34:38,159
there we go there

652
00:34:38,159 --> 00:34:41,580
um but this also relates to the factor

653
00:34:41,580 --> 00:34:43,099
graphs

654
00:34:43,099 --> 00:34:46,040
and some

655
00:34:46,040 --> 00:34:49,739
mathematical equivalencies

656
00:34:49,739 --> 00:34:51,179
with

657
00:34:51,179 --> 00:34:54,418
Factor graphs and Bayesian graphs

658
00:34:54,418 --> 00:34:58,500
that is some of the work since like 2017

659
00:34:58,500 --> 00:35:02,040
and a little bit before of DeVries and

660
00:35:02,040 --> 00:35:04,200
power and friston

661
00:35:04,200 --> 00:35:05,640
where

662
00:35:05,640 --> 00:35:11,480
these algorithms can uh statistically

663
00:35:11,640 --> 00:35:13,260
have good

664
00:35:13,260 --> 00:35:15,839
computational implementations

665
00:35:15,839 --> 00:35:18,660
or just

666
00:35:18,660 --> 00:35:22,160
how good exactly

667
00:35:22,260 --> 00:35:26,300
probably is an open question

668
00:35:29,220 --> 00:35:33,300
but these but uh

669
00:35:33,300 --> 00:35:35,220
there are

670
00:35:35,220 --> 00:35:39,000
procedural rules for calculating

671
00:35:39,000 --> 00:35:44,060
large graphs of this structure

672
00:35:46,859 --> 00:35:49,140
which is why they represent recent

673
00:35:49,140 --> 00:35:52,260
developments in

674
00:35:52,260 --> 00:35:56,599
Bayesian computational statistics

675
00:35:58,079 --> 00:36:00,420
it would be awesome to hear from someone

676
00:36:00,420 --> 00:36:02,400
who's working with those kinds of

677
00:36:02,400 --> 00:36:06,180
uh large scale

678
00:36:06,180 --> 00:36:11,480
implementations but that is my kind of

679
00:36:11,760 --> 00:36:15,240
outside interpretation

680
00:36:25,260 --> 00:36:27,440
foreign

681
00:36:32,060 --> 00:36:35,240
Ollie good

682
00:36:35,400 --> 00:36:37,560
uh and also there are some recent

683
00:36:37,560 --> 00:36:39,660
developments

684
00:36:39,660 --> 00:36:40,260
um

685
00:36:40,260 --> 00:36:44,820
in the uh for um I mean making these

686
00:36:44,820 --> 00:36:47,099
kind of computations uh more tractable

687
00:36:47,099 --> 00:36:51,540
and uh making uh the reducing the

688
00:36:51,540 --> 00:36:54,720
complexity uh classes of the

689
00:36:54,720 --> 00:36:56,640
computations

690
00:36:56,640 --> 00:37:01,220
um one example is this recent uh

691
00:37:01,220 --> 00:37:04,619
algorithm called branching time active

692
00:37:04,619 --> 00:37:07,320
inference developed by theophy champion

693
00:37:07,320 --> 00:37:09,980
and others and also

694
00:37:09,980 --> 00:37:13,740
the work done by Baron millage such as

695
00:37:13,740 --> 00:37:17,359
the inference learning or the

696
00:37:17,359 --> 00:37:22,700
hybrid predictive coding is also

697
00:37:22,700 --> 00:37:26,160
an attempt to reduce the complexity

698
00:37:26,160 --> 00:37:28,800
classes of these kind of implementations

699
00:37:28,800 --> 00:37:31,320
which can be

700
00:37:31,320 --> 00:37:34,760
I I believe uh can be a promising path

701
00:37:34,760 --> 00:37:39,960
towards uh achieving to a more tractable

702
00:37:39,960 --> 00:37:43,380
and computationally effective way of

703
00:37:43,380 --> 00:37:46,700
um Computing these inferences

704
00:37:54,300 --> 00:37:57,180
nice thank you for that

705
00:37:57,180 --> 00:38:00,839
yeah I

706
00:38:02,839 --> 00:38:05,460
there's sort of like a meme like machine

707
00:38:05,460 --> 00:38:09,320
learning is statistics and all that

708
00:38:09,320 --> 00:38:11,700
so these

709
00:38:11,700 --> 00:38:15,180
this chapter and

710
00:38:15,180 --> 00:38:17,579
hopefully seeing how some of these

711
00:38:17,579 --> 00:38:19,619
developments from

712
00:38:19,619 --> 00:38:23,420
pure Bayesian statistics

713
00:38:23,640 --> 00:38:27,180
but in the more recent era

714
00:38:27,180 --> 00:38:29,820
there have also been

715
00:38:29,820 --> 00:38:31,200
uh

716
00:38:31,200 --> 00:38:33,119
computational

717
00:38:33,119 --> 00:38:36,359
kind of interplay with

718
00:38:36,359 --> 00:38:38,820
the pure theory development

719
00:38:38,820 --> 00:38:40,440
for example

720
00:38:40,440 --> 00:38:41,000
um

721
00:38:41,000 --> 00:38:44,420
bootstrap based algorithms

722
00:38:44,420 --> 00:38:46,980
non-parametric statistics where one like

723
00:38:46,980 --> 00:38:49,320
resamples from distributions

724
00:38:49,320 --> 00:38:51,540
and does that many many times

725
00:38:51,540 --> 00:38:54,119
that kind of sampling based approach

726
00:38:54,119 --> 00:38:56,400
was worked out

727
00:38:56,400 --> 00:38:59,240
on paper

728
00:38:59,400 --> 00:39:02,460
long before it became computationally

729
00:39:02,460 --> 00:39:04,020
plausible

730
00:39:04,020 --> 00:39:06,359
to actually sample

731
00:39:06,359 --> 00:39:10,260
and store large amounts of information

732
00:39:10,260 --> 00:39:13,680
and then once that uh computational

733
00:39:13,680 --> 00:39:16,140
availability started to happen and

734
00:39:16,140 --> 00:39:18,420
better sampling algorithms

735
00:39:18,420 --> 00:39:20,400
and better Hardware

736
00:39:20,400 --> 00:39:22,020
than

737
00:39:22,020 --> 00:39:26,960
that area of Statistics can become

738
00:39:27,000 --> 00:39:29,660
applied

739
00:39:30,060 --> 00:39:31,740
which sometimes requires more

740
00:39:31,740 --> 00:39:33,540
theoretical developments

741
00:39:33,540 --> 00:39:37,740
but often not it's often just simply a

742
00:39:37,740 --> 00:39:40,619
question of the implementation being

743
00:39:40,619 --> 00:39:42,180
written

744
00:39:42,180 --> 00:39:45,000
from like a code

745
00:39:45,000 --> 00:39:47,220
or computational implementation

746
00:39:47,220 --> 00:39:49,759
perspective

747
00:39:50,220 --> 00:39:53,820
so there's a lot of equivalences in

748
00:39:53,820 --> 00:39:55,380
active

749
00:39:55,380 --> 00:39:57,960
but I think so the juxtaposition of

750
00:39:57,960 --> 00:40:01,339
figure four three

751
00:40:02,400 --> 00:40:05,300
generative models

752
00:40:06,180 --> 00:40:08,280
with

753
00:40:08,280 --> 00:40:11,900
Markov blankets first

754
00:40:12,420 --> 00:40:17,599
and then saying these are different

755
00:40:18,200 --> 00:40:22,980
heuristics computational implementations

756
00:40:22,980 --> 00:40:26,339
model fitting approaches

757
00:40:26,339 --> 00:40:29,480
and others others

758
00:40:29,940 --> 00:40:32,700
that if you can state it like it looks

759
00:40:32,700 --> 00:40:35,960
in figure 4.3

760
00:40:36,119 --> 00:40:40,619
then this kind of

761
00:40:40,619 --> 00:40:42,839
computational

762
00:40:42,839 --> 00:40:47,099
statistical Branch will become

763
00:40:47,099 --> 00:40:48,599
accessible

764
00:40:48,599 --> 00:40:50,099
it's like if they showed here's the

765
00:40:50,099 --> 00:40:51,720
linear regression

766
00:40:51,720 --> 00:40:55,200
and here's the r method or here's the

767
00:40:55,200 --> 00:40:56,820
python script

768
00:40:56,820 --> 00:40:58,680
that you can use

769
00:40:58,680 --> 00:41:02,160
or here's the um optimization here's

770
00:41:02,160 --> 00:41:03,540
three ways that you can optimize a

771
00:41:03,540 --> 00:41:04,859
linear regression

772
00:41:04,859 --> 00:41:06,599
and there are multiple ways to do linear

773
00:41:06,599 --> 00:41:08,940
regression

774
00:41:08,940 --> 00:41:10,440
and there's different software packages

775
00:41:10,440 --> 00:41:12,839
associated with them too

776
00:41:12,839 --> 00:41:15,960
so it's like if your GMS have this

777
00:41:15,960 --> 00:41:18,380
structure

778
00:41:18,839 --> 00:41:20,460
then

779
00:41:20,460 --> 00:41:21,960
these are

780
00:41:21,960 --> 00:41:24,380
optimization

781
00:41:24,380 --> 00:41:27,060
implementation approaches

782
00:41:27,060 --> 00:41:30,119
that very importantly are not sampling

783
00:41:30,119 --> 00:41:31,920
based

784
00:41:31,920 --> 00:41:34,260
now

785
00:41:34,260 --> 00:41:36,780
complexly enough sampling may have to

786
00:41:36,780 --> 00:41:39,839
come into play in the real computational

787
00:41:39,839 --> 00:41:41,640
implementation

788
00:41:41,640 --> 00:41:43,380
but that's kind of a

789
00:41:43,380 --> 00:41:46,380
deeper or more specific question to a

790
00:41:46,380 --> 00:41:47,700
given

791
00:41:47,700 --> 00:41:50,880
way that it's uh applied but just more

792
00:41:50,880 --> 00:41:53,880
broadly these are not

793
00:41:53,880 --> 00:41:57,540
just trying to sample data until the

794
00:41:57,540 --> 00:42:01,740
landscape is like fully coded

795
00:42:01,740 --> 00:42:03,420
these are

796
00:42:03,420 --> 00:42:08,040
um models that with their sparcity

797
00:42:08,040 --> 00:42:10,079
like what's not connected

798
00:42:10,079 --> 00:42:11,700
embody

799
00:42:11,700 --> 00:42:14,280
sometimes incredibly strong

800
00:42:14,280 --> 00:42:17,760
structural assumptions about the world

801
00:42:17,760 --> 00:42:19,800
which is why there's the whole question

802
00:42:19,800 --> 00:42:22,140
of structured learning

803
00:42:22,140 --> 00:42:25,380
because once you're structured in the GM

804
00:42:25,380 --> 00:42:28,200
like we'll see in chapter six as well

805
00:42:28,200 --> 00:42:30,480
the kind of structure is set and then

806
00:42:30,480 --> 00:42:32,640
there's like a fine tuning where the

807
00:42:32,640 --> 00:42:34,440
actual variables get tuned to the right

808
00:42:34,440 --> 00:42:36,180
values

809
00:42:36,180 --> 00:42:38,940
that's learning and perception

810
00:42:38,940 --> 00:42:42,599
but also learning can be structural

811
00:42:42,599 --> 00:42:44,720
um

812
00:42:45,300 --> 00:42:48,060
so I hope that kind of provides some

813
00:42:48,060 --> 00:42:51,960
concordances with the generative model

814
00:42:51,960 --> 00:42:56,220
and speaks to its like

815
00:42:56,220 --> 00:42:58,319
total centrality

816
00:42:58,319 --> 00:43:01,319
inactive

817
00:43:01,500 --> 00:43:04,859
in chapter four and connects it to

818
00:43:04,859 --> 00:43:07,500
box four one

819
00:43:07,500 --> 00:43:09,480
that's describing

820
00:43:09,480 --> 00:43:10,800
some

821
00:43:10,800 --> 00:43:12,780
exciting

822
00:43:12,780 --> 00:43:17,940
techniques for computing large

823
00:43:17,940 --> 00:43:23,060
base graph type generative models

824
00:43:23,160 --> 00:43:25,140
one last comment on this and then we'll

825
00:43:25,140 --> 00:43:27,660
still have more time just time for

826
00:43:27,660 --> 00:43:31,380
kind of overviewing again is like

827
00:43:31,380 --> 00:43:34,440
I think people have certainly said

828
00:43:34,440 --> 00:43:36,780
before done anyone specific right now

829
00:43:36,780 --> 00:43:38,400
but just that

830
00:43:38,400 --> 00:43:41,819
there is a lot of focus on the Markov

831
00:43:41,819 --> 00:43:44,160
blanket concept

832
00:43:44,160 --> 00:43:47,339
and the sort of like

833
00:43:47,339 --> 00:43:49,980
everything about the Marco blanket the

834
00:43:49,980 --> 00:43:52,020
mystery

835
00:43:52,020 --> 00:43:54,480
but less focus on the generative model

836
00:43:54,480 --> 00:43:56,760
concept

837
00:43:56,760 --> 00:44:00,119
so when people then thinking about

838
00:44:00,119 --> 00:44:04,079
of how do we model organizations or

839
00:44:04,079 --> 00:44:04,680
um

840
00:44:04,680 --> 00:44:07,980
this cognitive feature

841
00:44:07,980 --> 00:44:10,140
there's a lot of focus on like well what

842
00:44:10,140 --> 00:44:12,780
are the nested Markov blankets

843
00:44:12,780 --> 00:44:15,660
and now there's a way to

844
00:44:15,660 --> 00:44:17,400
think about that as nested Markell

845
00:44:17,400 --> 00:44:19,040
blankets

846
00:44:19,040 --> 00:44:23,180
recognizing the generative model

847
00:44:23,880 --> 00:44:28,740
that isn't just the entity scale

848
00:44:28,740 --> 00:44:30,480
blanket

849
00:44:30,480 --> 00:44:33,420
but that kind of nested Markov blanket

850
00:44:33,420 --> 00:44:35,760
representation multi-skill systems kind

851
00:44:35,760 --> 00:44:38,400
of like just general point

852
00:44:38,400 --> 00:44:42,660
what gets often not brought up in those

853
00:44:42,660 --> 00:44:44,220
situations is like

854
00:44:44,220 --> 00:44:46,339
okay society's nested Mark all blankets

855
00:44:46,339 --> 00:44:51,180
but what are the generative models

856
00:44:51,180 --> 00:44:53,400
if you specify the generative model

857
00:44:53,400 --> 00:44:55,859
you'll have also specified the markup

858
00:44:55,859 --> 00:44:58,020
blankets

859
00:44:58,020 --> 00:44:59,819
because you'll have defined the

860
00:44:59,819 --> 00:45:02,760
particular things

861
00:45:02,760 --> 00:45:07,280
embodied in the actual generative model

862
00:45:08,339 --> 00:45:10,500
so it's like is it an active inference

863
00:45:10,500 --> 00:45:11,520
claim

864
00:45:11,520 --> 00:45:14,640
that a situation has been analyzed

865
00:45:14,640 --> 00:45:16,500
in terms of interacting generative

866
00:45:16,500 --> 00:45:19,619
models and generative processes

867
00:45:19,619 --> 00:45:21,599
or is it a general Claim about

868
00:45:21,599 --> 00:45:25,940
multi-scale systems being nested

869
00:45:26,940 --> 00:45:29,160
so when we think about just the

870
00:45:29,160 --> 00:45:30,560
encapsulating

871
00:45:30,560 --> 00:45:33,119
matriuska nature

872
00:45:33,119 --> 00:45:35,579
of the nested blankets

873
00:45:35,579 --> 00:45:38,339
degenerative model is not

874
00:45:38,339 --> 00:45:42,500
specified in more cases than not

875
00:45:42,780 --> 00:45:44,160
hope that's an interesting and Fair

876
00:45:44,160 --> 00:45:46,460
Point

877
00:45:52,319 --> 00:45:54,660
okay

878
00:45:54,660 --> 00:45:57,000
we have 10 to 12 minutes left is there

879
00:45:57,000 --> 00:45:59,280
any other like

880
00:45:59,280 --> 00:46:02,599
thought or question

881
00:46:02,640 --> 00:46:04,859
Brock wrote Spooky inference in close

882
00:46:04,859 --> 00:46:07,020
proximity to surprise generates noisy

883
00:46:07,020 --> 00:46:09,619
definitions

884
00:46:12,000 --> 00:46:13,800
I was just

885
00:46:13,800 --> 00:46:18,900
kidding about ali uh saying lots of

886
00:46:18,900 --> 00:46:20,180
different

887
00:46:20,180 --> 00:46:23,760
names for things we call that spooky

888
00:46:23,760 --> 00:46:26,579
action at a distance entanglement now

889
00:46:26,579 --> 00:46:29,640
but at the time it was a bunch of

890
00:46:29,640 --> 00:46:31,079
different things

891
00:46:31,079 --> 00:46:33,980
when it was first

892
00:46:34,079 --> 00:46:36,119
observed

893
00:46:36,119 --> 00:46:38,099
and it was surprising

894
00:46:38,099 --> 00:46:41,780
so anyways um

895
00:46:42,000 --> 00:46:44,540
I don't know if uh yeah it's worth like

896
00:46:44,540 --> 00:46:46,800
uh thinking

897
00:46:46,800 --> 00:46:50,160
or just leaking like

898
00:46:50,160 --> 00:46:51,660
lingering a little bit on the

899
00:46:51,660 --> 00:46:53,819
intractability of

900
00:46:53,819 --> 00:46:57,839
specifying like the model entirely in

901
00:46:57,839 --> 00:47:00,799
most in a lot of cases

902
00:47:08,700 --> 00:47:12,119
thanks yeah I think that kind of

903
00:47:12,119 --> 00:47:13,079
um

904
00:47:13,079 --> 00:47:16,200
anticipates some discussions that are in

905
00:47:16,200 --> 00:47:19,619
like chapter six with a recipe for how

906
00:47:19,619 --> 00:47:22,520
to make the generative model

907
00:47:22,520 --> 00:47:25,619
and the difference between like

908
00:47:25,619 --> 00:47:27,660
fully specifying

909
00:47:27,660 --> 00:47:30,300
a quote toy model

910
00:47:30,300 --> 00:47:32,520
which is to say like a model where you

911
00:47:32,520 --> 00:47:35,220
can see it graphically

912
00:47:35,220 --> 00:47:37,800
at like this like just kind of like this

913
00:47:37,800 --> 00:47:40,859
like this is the whole model

914
00:47:40,859 --> 00:47:43,500
um in that case you may be able to

915
00:47:43,500 --> 00:47:46,200
create

916
00:47:46,200 --> 00:47:48,599
a little information bubble where that

917
00:47:48,599 --> 00:47:51,960
is the full model specification

918
00:47:51,960 --> 00:47:55,140
but then in any real

919
00:47:55,140 --> 00:47:57,000
situation

920
00:47:57,000 --> 00:48:01,140
even like doing research

921
00:48:01,140 --> 00:48:04,079
let alone doing anything with causal

922
00:48:04,079 --> 00:48:05,220
feedback

923
00:48:05,220 --> 00:48:07,980
in the loop

924
00:48:07,980 --> 00:48:10,319
the the fully specified model just is

925
00:48:10,319 --> 00:48:13,140
somewhere between doesn't exist

926
00:48:13,140 --> 00:48:15,440
can't be specified won't be specified

927
00:48:15,440 --> 00:48:18,980
shouldn't be specified

928
00:48:22,980 --> 00:48:26,599
so then what what then

929
00:48:31,800 --> 00:48:34,800
foreign

930
00:48:55,020 --> 00:48:57,119
thoughts on four

931
00:48:57,119 --> 00:49:02,119
or let's just look quickly to five

932
00:49:11,579 --> 00:49:14,579
Ollie go for it

933
00:49:14,579 --> 00:49:19,200
uh yeah just one small Point here uh I'm

934
00:49:19,200 --> 00:49:20,819
not sure if

935
00:49:20,819 --> 00:49:24,540
uh I mentioned it before or not but uh

936
00:49:24,540 --> 00:49:27,960
there's uh kind of another type of

937
00:49:27,960 --> 00:49:30,420
active inference of course we have a

938
00:49:30,420 --> 00:49:32,819
very different classes or categories of

939
00:49:32,819 --> 00:49:35,099
active inference but uh one specific

940
00:49:35,099 --> 00:49:38,099
type is called uh sophisticated

941
00:49:38,099 --> 00:49:43,140
inference which kind of deals with uh

942
00:49:43,140 --> 00:49:45,900
the the situations in which

943
00:49:45,900 --> 00:49:51,300
um well I mean uh instead of predicting

944
00:49:51,300 --> 00:49:55,859
uh what would become later in at each

945
00:49:55,859 --> 00:49:59,579
stage it tries to compute everything

946
00:49:59,579 --> 00:50:02,480
that has come before or

947
00:50:02,480 --> 00:50:07,200
in order to have a way to compute

948
00:50:07,200 --> 00:50:10,500
um counter factual consequences or the

949
00:50:10,500 --> 00:50:13,859
things that we need to

950
00:50:13,859 --> 00:50:17,660
um or somehow the kind of uh

951
00:50:17,660 --> 00:50:21,720
decision-making processes and so on and

952
00:50:21,720 --> 00:50:25,560
um interestingly in the paper branching

953
00:50:25,560 --> 00:50:28,380
time active inference uh they've stated

954
00:50:28,380 --> 00:50:33,720
that uh this type of active inference is

955
00:50:33,720 --> 00:50:35,880
a subset of branching time active

956
00:50:35,880 --> 00:50:40,400
inference I mean if we just

957
00:50:40,400 --> 00:50:43,440
put take that branching time active

958
00:50:43,440 --> 00:50:47,640
inference and separate it into forward

959
00:50:47,640 --> 00:50:51,660
and backward propagation uh trajectories

960
00:50:51,660 --> 00:50:54,839
will come with a traditional active

961
00:50:54,839 --> 00:50:57,599
inference and sophisticated active

962
00:50:57,599 --> 00:51:01,440
inference so I think the main paper for

963
00:51:01,440 --> 00:51:05,220
sophisticated active inference is this

964
00:51:05,220 --> 00:51:08,460
one that I'm putting here so

965
00:51:08,460 --> 00:51:11,220
yeah because that's just one small point

966
00:51:11,220 --> 00:51:14,058
I wanted to mention

967
00:51:16,500 --> 00:51:18,720
great point

968
00:51:18,720 --> 00:51:21,059
thanks for sharing this generative model

969
00:51:21,059 --> 00:51:23,700
knowledge

970
00:51:23,700 --> 00:51:25,140
it's very

971
00:51:25,140 --> 00:51:27,799
helpful

972
00:51:29,819 --> 00:51:31,859
live stream 11 was sophisticated

973
00:51:31,859 --> 00:51:33,839
affective inference

974
00:51:33,839 --> 00:51:35,819
so that was

975
00:51:35,819 --> 00:51:39,800
with a secondary or you know here

976
00:51:39,800 --> 00:51:42,240
verbally secondary

977
00:51:42,240 --> 00:51:44,880
adjective effective

978
00:51:44,880 --> 00:51:47,339
reflecting that they were modeling an

979
00:51:47,339 --> 00:51:49,440
affective variable or taking an

980
00:51:49,440 --> 00:51:51,480
affective interpretation of their

981
00:51:51,480 --> 00:51:54,540
model's cognitive function

982
00:51:54,540 --> 00:51:57,240
they were doing affective inference

983
00:51:57,240 --> 00:51:59,280
in the context of sophisticated

984
00:51:59,280 --> 00:52:01,859
inference

985
00:52:01,859 --> 00:52:03,540
which is as

986
00:52:03,540 --> 00:52:07,160
um Ali described

987
00:52:07,200 --> 00:52:11,040
allows for a kind of mapping and

988
00:52:11,040 --> 00:52:13,859
modeling of past present and future

989
00:52:13,859 --> 00:52:16,200
which is like why in four three there's

990
00:52:16,200 --> 00:52:18,900
like a time minus one and time plus one

991
00:52:18,900 --> 00:52:21,300
it could have been written as just time

992
00:52:21,300 --> 00:52:23,760
time plus one time plus two

993
00:52:23,760 --> 00:52:25,440
and been like a two-step planning

994
00:52:25,440 --> 00:52:28,740
problem but also there's um

995
00:52:28,740 --> 00:52:30,900
you can choose different pieces of the

996
00:52:30,900 --> 00:52:35,119
time Horizons to model Ali

997
00:52:35,119 --> 00:52:38,700
or a better way to a probably a better

998
00:52:38,700 --> 00:52:42,900
way to put it is to say that in

999
00:52:42,900 --> 00:52:45,540
situations where we have beliefs about

1000
00:52:45,540 --> 00:52:49,020
beliefs uh this kind of sophisticated

1001
00:52:49,020 --> 00:52:53,099
inference can come to play

1002
00:52:53,099 --> 00:52:57,740
instead of just bleeds about States

1003
00:52:58,920 --> 00:53:00,720
and

1004
00:53:00,720 --> 00:53:06,180
the composability of Bayesian graphs

1005
00:53:06,180 --> 00:53:09,020
is high

1006
00:53:09,540 --> 00:53:11,640
which is what allows all of these

1007
00:53:11,640 --> 00:53:13,920
different structural models to be

1008
00:53:13,920 --> 00:53:16,859
explored like nested models that

1009
00:53:16,859 --> 00:53:19,380
represent spatial enclosure nested

1010
00:53:19,380 --> 00:53:21,300
models that represent

1011
00:53:21,300 --> 00:53:23,839
counterfactuals

1012
00:53:23,839 --> 00:53:26,579
models that consider the past and are

1013
00:53:26,579 --> 00:53:29,520
able to reconsider the past observations

1014
00:53:29,520 --> 00:53:33,000
models that don't all of the like if of

1015
00:53:33,000 --> 00:53:35,900
memoryless process

1016
00:53:38,579 --> 00:53:41,099
wow

1017
00:53:41,099 --> 00:53:43,140
all right chapter five is quite

1018
00:53:43,140 --> 00:53:45,300
different and we'll just look through it

1019
00:53:45,300 --> 00:53:47,880
and see how different it is

1020
00:53:47,880 --> 00:53:49,859
it's gonna pick up on this message

1021
00:53:49,859 --> 00:53:51,059
passing

1022
00:53:51,059 --> 00:53:53,520
topic which will be cool to revisit

1023
00:53:53,520 --> 00:53:55,920
again and consider it primarily from the

1024
00:53:55,920 --> 00:53:59,460
context of neurobiology

1025
00:53:59,460 --> 00:54:01,319
in this chapter we focus on the process

1026
00:54:01,319 --> 00:54:05,220
theories that arise from chapter four

1027
00:54:05,220 --> 00:54:07,079
Central to this implementation of

1028
00:54:07,079 --> 00:54:08,819
Bayesian belief updating is the notion

1029
00:54:08,819 --> 00:54:12,240
of Bayesian message passing

1030
00:54:12,240 --> 00:54:15,500
microcircus and messages

1031
00:54:16,319 --> 00:54:18,540
cortical players

1032
00:54:18,540 --> 00:54:21,480
part of the mammalian brain mammal

1033
00:54:21,480 --> 00:54:23,220
neural Anatomy

1034
00:54:23,220 --> 00:54:25,859
mammal computational cerebral

1035
00:54:25,859 --> 00:54:28,700
neuroanatomy

1036
00:54:30,300 --> 00:54:31,819
abstract

1037
00:54:31,819 --> 00:54:36,500
hierarchical predictive coding model

1038
00:54:40,140 --> 00:54:42,900
motor reflex arc

1039
00:54:42,900 --> 00:54:47,180
in the spinal column of metal

1040
00:54:55,260 --> 00:54:58,800
policy selection

1041
00:54:58,800 --> 00:55:01,559
computational neuroanatomy in the mammal

1042
00:55:01,559 --> 00:55:03,859
brain

1043
00:55:06,720 --> 00:55:09,059
putative roles of neurotransmitters in

1044
00:55:09,059 --> 00:55:11,780
active inference

1045
00:55:13,319 --> 00:55:15,780
different neurotransmitters

1046
00:55:15,780 --> 00:55:17,940
and they're sort of like

1047
00:55:17,940 --> 00:55:19,440
putative

1048
00:55:19,440 --> 00:55:21,720
or

1049
00:55:21,720 --> 00:55:25,440
coarse grained or hypothesized

1050
00:55:25,440 --> 00:55:28,700
computational role

1051
00:55:30,720 --> 00:55:34,140
and different studies that support that

1052
00:55:34,140 --> 00:55:36,540
computational role

1053
00:55:36,540 --> 00:55:39,180
for that neurotransmitter

1054
00:55:39,180 --> 00:55:40,559
not saying it's like the only thing it

1055
00:55:40,559 --> 00:55:42,119
does or the only way to look at it just

1056
00:55:42,119 --> 00:55:43,680
they found empirical support for it

1057
00:55:43,680 --> 00:55:46,200
playing that statistical role in those

1058
00:55:46,200 --> 00:55:48,799
experiments

1059
00:55:50,040 --> 00:55:51,240
then

1060
00:55:51,240 --> 00:55:54,300
some early moves

1061
00:55:54,300 --> 00:55:57,780
towards integrating continuous and

1062
00:55:57,780 --> 00:56:00,359
discrete models

1063
00:56:00,359 --> 00:56:02,280
which returns later in the second half

1064
00:56:02,280 --> 00:56:05,460
of the book and the closing figure is

1065
00:56:05,460 --> 00:56:08,220
showing all of the three examples from

1066
00:56:08,220 --> 00:56:10,680
neuroanatomy together

1067
00:56:10,680 --> 00:56:12,359
planning

1068
00:56:12,359 --> 00:56:16,260
with those dopaminergic brain regions

1069
00:56:16,260 --> 00:56:18,599
then the prefrontal

1070
00:56:18,599 --> 00:56:20,280
cortical regions

1071
00:56:20,280 --> 00:56:22,319
on top

1072
00:56:22,319 --> 00:56:24,660
and the spinal

1073
00:56:24,660 --> 00:56:30,300
reflex motor behavioral art with action

1074
00:56:30,300 --> 00:56:32,160
so it's like when we've been talking

1075
00:56:32,160 --> 00:56:35,099
about planning as inference

1076
00:56:35,099 --> 00:56:37,079
and prediction and generalization

1077
00:56:37,079 --> 00:56:40,740
categorization as inference learning as

1078
00:56:40,740 --> 00:56:41,940
inference

1079
00:56:41,940 --> 00:56:44,760
action as inference

1080
00:56:44,760 --> 00:56:47,960
this is the graph

1081
00:56:50,339 --> 00:56:52,859
so it's a different chapter it's not as

1082
00:56:52,859 --> 00:56:54,660
equation driven

1083
00:56:54,660 --> 00:56:56,339
and uh

1084
00:56:56,339 --> 00:57:00,000
it's primarily based on those examples

1085
00:57:00,000 --> 00:57:02,099
of mammal

1086
00:57:02,099 --> 00:57:04,020
and generalized computational

1087
00:57:04,020 --> 00:57:06,960
neuroanatomy

1088
00:57:06,960 --> 00:57:08,880
so it's a cool

1089
00:57:08,880 --> 00:57:12,180
last chapter for the first half

1090
00:57:12,180 --> 00:57:13,800
all right

1091
00:57:13,800 --> 00:57:17,780
any other last comments

1092
00:57:27,480 --> 00:57:31,460
okay I'll stop recording thank you


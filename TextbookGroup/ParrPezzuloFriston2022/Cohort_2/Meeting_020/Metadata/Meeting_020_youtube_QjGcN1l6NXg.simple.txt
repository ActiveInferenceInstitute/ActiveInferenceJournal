SPEAKER_00:
Okay.

Welcome.

Thanks.

It's March 15th, 2023.

We're in meeting 20 of cohort two in our first discussion on chapter nine.

So as we begin this discussion of nine, is there any general comment or question about nine that anyone wants to bring up?

one part on nine is it at least points the direction towards an avenue that many people who are interested in learning and applying active inference want to go down which is to make generative models yes that help us understand different cognitive systems and also have models that can accept empirical data

so that we can enact the tail of two densities.

So we can use the model in the generative direction to go from specified parameters to output data sets of synthetic data, kind of like generative AI, but then also to recognize

outcomes in the world or data and then update or parameterize our generative models from those data and it's that tale of two densities the generative density and the recognition density the ability to run these models both ways to generate valid synthetic data from a generative model

and if you design your experiment so that those data which are generated are also actually being generated in the real world then you can just instantly plug it back in um and we'll i'm sure unpack and explore more so we'll go through the chapter but is there any just general comment or general thoughts on chapter nine first

okay so let's look at one previous question so how did chapter four on the generative model relate to chapter nine as we're getting close to the end of the book chapter six was a recipe it was about applying active inference chapter four we were learning the necessary prerequisites and chapter one and or two and three were the roads to chapter four

then chapter five we got the neurobiology and some examples chapter six we got the recipe chapter seven and eight was the discrete time and continuous time generative models and now we're in chapter nine where we're learning how to solve problems and it's about getting one's hands dirty with case studies and problem solving

we'll see this in the figure upcoming a key distinction is between the subjective model and the objective model there's different readings of the first sentence and in some readings it could be seen as playing fast and loose with instrumentalism and realism

So the subjective model though, is that which is subject specific from the subject's perspective.

Now, it turns out that it is a map that we're making from the subject's perspective.

So it isn't the actual territory of the subjectivity, but it is from the subject's perspective

and then to meet that and make the full model we are going to have an objective model which doesn't mean that it's the one and only it means that it's our view from the outside seeing that thing as an object this entire setup with the tail of two densities and the the meta bayesian

enables us to make Bayesian cognitive models and think about experimentation in a Bayesian context, which lets us talk about Bayes optimal cognition in different systems.

And one area that that has been especially relevant is in computational phenotyping.

This chapter deals with the utility

of active inference formulations in analyzing data from behavioral experiments.

It's going beyond the proof of principle simulations.

So in the proof of principle domain, we have the jumping frog in the hand, we have the rat in the tea maze, and so on.

And here, active inference is going to be used to answer scientific questions.

Since the subjective GM

is the generator of behavior in active inference then the scientific hypotheses that we're going to entertain about the causes and consequences of phenotype must be in terms of hypotheses about alternative generative models so the challenge is to

just like an abductive logic challenge, generate a portfolio or spaces of alternative generative models, cognitive models, and then ask for a given string of observations what the evidence is for different alternative hypotheses provided.

And then at that point, one could take a more classical or frequentist approach and

could also take a more pure bayesian approach and ask about like the base factor and the relative support of different models but the upstream part is to generate alternative generative models explicitly or implicitly by specifying state spaces of possibilities

And then chapter nine is when we're plugging in empirical data and then going to ask about which of those generative models or spaces within the possibilities of the GMs have what evidence with respect to the observables.

If you have any thought or question, just totally go for it.

Otherwise, we'll just continue to walk through it on the first discussion.

Michael, go for it.


SPEAKER_02:
Yeah, I mean, I guess you'll still come to it, but I'm struggling with this mathematics very much.

And last time, we had this interesting discussion about temporal models, the difference between chapter 7 and chapter 8.

And I was thinking that for my data that I have with this gaze data and the keystrokes and all this, these temporal models would be perhaps much better suited

But then we discussed about these different kinds of derivations.

And I'm really unsure what that implies, whether I should

find a way to express my data in kinds of waves so that one can compute somehow derivations from this.

So we discussed about the disappearance of time in these models.

And I'm really struggling to understand what that means in my data.

that I have.

But I don't know whether this really relates to what you were saying before.

Yeah.


SPEAKER_00:
Well, it's definitely an experience and a challenge when you have an actual data set.

And for people who are already experimentalists,

it's not like we can just dream up the most creative generative model and then build a lab to record those data in many cases we're actually constrained in what we can do or we already have the data set and then we're going to work backwards from the data set to generative models yep that's the case in my case yes

yeah yeah so i mean one um um uh beware all who pass this point is everything just like everything that led to the collection of your data was an experimentalist's choice you could have collected more or different um participants you could have done this or that in the data collection

And now, let's just say that we're in the analysis phase, everything downstream, everything about the structure of the generative model and so on is all going to be a modeler's degrees of freedom.

So we are looking for different spaces to specify and then different portfolios of generative models to test against.

And one of the great things about Bayesian statistics is that wildly disparate

models architecturally can be compared through the bayes factor and the bayesian information criterion so just to explain why that's relevant relative to the um frequentist statistics when comparing frequentist statistical models

one of the only principled ways to actually test for which model is better is the hierarchical likelihood ratio test so you can say i have two strictly nested models i have factor a and factor b and now i'm going to test for an interaction effect of a x b

then you can ask whether there's a statistically significant improvement by taking on that interaction effect but if you said i have a model with a x b and then i have a model with b and c and c and d and d and e and x and y those models cannot be directly compared because their um likelihoods are not comparable in contrast

Bayesian models can be compared in various ways.

The Bayes factor, the BIC, accuracy minus complexity, free energy, even when there's a very, very different model anatomies.

So it's not even like you have to decide whether you want to do only a discrete or only a continuous time generative model.

and hopefully as the software pipelines become more developed it'll be like you'll specify all the different channels that your data can flow through and then see how wildly different models match up with the data and look at their relative evidence or the areas that they fit well or don't in

And you still might have to prune that down in some talk or paper, but also maybe it'll be possible to share those computational resources also with that kind of high bandwidth.

But yeah, we'll return to it.

So two reasons why we want to fit data, like why chapter nine.

The first is to estimate parameters of interest from a group.

those kinds of computational phenotypes might be useful for some other setting the second reason is to compare alternative hypotheses so here we have in the simplest case two groups and we want to ask if there is a difference in some parameter in the cognitive model a related

reason to use computational phenotyping is potentially even for one person or one group or it could be for two groups is to compare two different models against one another and ask which one has different um evidence and so these are the two agendas estimating parameters given a cognitive model and

Evaluation among models given data.


SPEAKER_02:
That's the same hypothesis.

You have the same hypothesis.

You just replace the model.


SPEAKER_00:
and then uh same data so like uh like uh let's just say in the first case what this would look like would be people who have condition X or not I'm going to measure their attention with with this given attention model I'm going to test if there's a significant difference in this attention parameter

This one would say, I have two attention models.

I have one where attention is a single layer model and I have one where there's like a nested model on attention.

And now on the whole data set or on just this one person, I'm going to ask which of these two cognitive models is more consistent with the data that I see.

And these two agendas are perfectly compatible with different terms within Bayes' theorem, which is a tight synthesis of the kind of fully Bayesian epistemology.

It's like we're doing Bayesian science with Bayesian statistics.

We're approaching our data collection and analysis in an optimal information setting.

okay now we get to figure nine one with the objective and the subjective so does anyone want to give a thought on figure 9.1 what they see here or what they see this figure doing in the context of the book okay just to give a first thought then again just raise your hand or write in the chat or anything

everything that we saw in chapters four five seven eight all those generative models all those base graphs those are in the center in the dashed line and so it's a map but it's a map that we're constructing as if it were a view from the inside

so here's the mouse in the T-Maze and here's the POMDP in this case it's a discrete time you can tell because it has the past present and future transition matrix and so on and so we've made a discrete time model of the mouse in the T-Maze so it's the as if mouse's view from the inside

and that's what makes sense when we're doing computational phenotyping we don't want to do model selection on um omniscient mice because we don't think that that exists in the world so the kinds of constraints that we think actually are uh justifiable in the world like the only input is sensory and the only outputs are actions those constraints are embodied within the gm that we make for the subject

but the subject is also the object of our study.

The object of our study or the system of interest of our study is that mouse in the tea maze.

And so with respect to how we designed that tea maze experiment and whether we do it with one trial on one day, or we do it with this other block design, or we have it replicated across laboratories, those experimental parameters

that our action selections from us can be understood as being drawn from distributions of possible experiments.

And what our action selections by the experimenter are being passed into the subjects model as an observation.

What is passed out of the subject through action

is the input for the observer so we're kind of wrapping the generative models that we saw in chapter four etc calling those subjective models not to mean that they're willy-nilly but just to say that they are as if from the subject's perspective

And then wrapping that as a behavioral researcher in an objective model, not to mean that it's the one and only true model, but it's the one from the outside with the subject as object.

And this is called meta-Bayesian.

So in some ways, this is a formalization of ethology.

with the cognitive modeling twist, because we're explicitly saying that we're interested in this cognitive or internal map.


SPEAKER_02:
Michael?

Has this to do with a self-evidencing experimenter that they put in the data and they get the results and then they model the subject in between in such a way

that the expectations become true, that the input-output relations.

And so has it to do with the self-evidencing ideas?


SPEAKER_00:
Yeah.


SPEAKER_02:
The experimenter.


SPEAKER_00:
Yeah, it's very interesting.

Like our subjective model, we can see why that is set up in a way to be self-evidencing.

We can say that the mouse is moving around in the tea maze so that it reduces divergence, so that it can be the kind of mouse that it expects and prefers to be.

We can understand its diverse behavioral selections in light of self-evidencing.

If we're going to consider ourselves in a Bayesian light as well, then also that entails a certain kind of epistemological self-evidencing.

now in the extreme or in one simple reading self-evidencing would be like you write down the number four oh i wonder what number four great write down the number four look at the number it's four well that isn't what self-evidencing means any more than for the organism it means to stay in the dark room

which has been a point of philosophical contention for like literally 10 years people say well if organisms are acting to reduce surprise or to self-evidence why don't they just stay in the dark room dark room challenge you know throw down the gauntlet and it's like right if the generative model didn't have survival requirements and its only imperative was to have the reduced divergence with a photon distribution

You would, and you do see that behavior.

But that's not what organisms do.

And maps aren't territories.

So if you actually built a realistic generative model, you would find that self-evidencing doesn't just mean staying in the dark room.

It also means self-evidencing for temperature and thirst and all these other drives.

So self-evidencing by the researcher plays out

in a way that would need its own generative model to be specific about but for example if the researcher says my commitment is to maximum information experimentation i'm not trying to simply confirm what my friend told me my commitment and expectation slash preference is for maximally informative experiment design

then the experiment that they design can be understood as self-evidencing even if the data points are maximally surprising that would be the point ali


SPEAKER_01:
And also about self-evidencing, you see in recent literature, especially since 2019, there's two distinct way of formulating self-evidencing.

One is in terms of Bayesian model evidence, which is the approach taken in this chapter as well.

And one is in the terms of

Lagrangian of the autonomous paths of active particles.

So these autonomous paths are actually, or in other words, these Lagrangians are actually the expected free energy, which can by themselves decompose into two separate terms corresponding with expected costs on one hand and expected information gain.

So in this kind of formulation, the expected cost is the surprisal of sensory paths, which is also what's referred to as the sensory surprisal or the self-evidencing of the

self-organization systems.

So there are slight, there are some subtle differences between these two formulations, but ultimately they converge onto same results providing, I mean, a description of the behavior of these systems in, or in particular active


SPEAKER_00:
particles in the environment in terms of minimizing their expected free energy thank you um when we set up the ethological moment in this way it's an active inference problem

So it can be approached using all of the amazing tools that are being developed.

Our focus is on ACT-INF and specifically discrete time.

However, the generic methods may be used with other likelihood functions.

So other normative models of behavior

in active inference our normativity is around self-evidencing things should persistent things should minimize or bound their surprise but there's no issue with having a subjective model with a different normativity that it should maximize reward as per a utility function defined

Now there will be a generic inference scheme that can be used for metabasian inference.

Then there's going to be a recipe and then a key example.

Readers uninterested in certain details are invited to skip 9.3 and 9.4.

Okay, we'll keep that in mind.

Parametric empirical Bayes

is a technique that's used to jump start a bayesian analysis with empirical data

so a common question by adherents and critics in bayesian statistics is like well what priors should i choose because there's no such thing as a unbiased or a non-biased prior there's the uniform distribution like the flat probability distribution but even that

is over a certain interval and it still embodies a prior albeit one that is uniform across the range but there's other situations where the uniform distribution is not even a possible selection for example you can't have a uniform distribution across all possible heights of trees at some point you just got to be like you know objects have this range of sizes

And so there's always this question about selection of priors and hyper priors and so on.

Now, in many cases, in practice,

it can be shown that it doesn't matter that much.

For example, somebody will show that with two wildly divergent prior distributions, there's so much signal in the data that it converges to the same posterior.

So that's often used as evidence like, hey, we tried different priors and we always got the same convergence.

even with different families of priors so we feel like this signal is being driven by patterns in the data and it's not an aberration because of our choice of like a very opinionated prior so that's one strategy people take presentation of alternative priors another approach that can be taken

it's not exclusive to what was previously mentioned is parametric empirical base and so in this situation empirical data are used to kick start setting the priors so we're going to be measuring the height of children in the whole school first we're going to measure one classroom and then we're going to use the mean and the variance of that classroom

as our prior moving forward for the other classrooms so it's just a way to take empirical data and use the empirical data summary statistics like mean and variance for gaussian

to jumpstart subsequent Bayesian analyses, including of the data that generated that prior.

Now that analysis by itself won't be super informative because you kind of already described the maximum likelihood solution for that one data set.

But with subsampling and so on, there's some ways to make PEB really useful.

right so 9.3 9.4 it was like reminded that they're not essential so we'll move a little faster through them but we can return to it um next time okay here we have the variables that are being used in the meta bayesian image in nine point figure 9.1 and there's a focus on the likelihood function and um

Bayesian model evidence is closely linked to surprise or self-evidencing because the model with the most evidence is the one that is least surprised by data.

So sometimes we're talking more about maximizing likelihood, other times minimizing the surprise, but the tuning of the parameters

is not going to clash between those approaches.

Pi are those policy selections.

And so pi is going to be based upon a minimization of free energy over all the pis.

So this is like act-inf.

free energy minimization of the generative model not of the territory of the GM is going to be used as the imperative for action selection which is what we observed from the mouse and the teammates so if we have the model that does best at predicting their behavior

then we've done a good compromised job on the subject of perception, cognition, action.

And there's just a mention of this temperature parameter, shaky handedness, which is just kind of fun to like think about even just from an intuition perspective where in the shaky hand limit, the high temperature, everything is shaking a lot.

it eradicates differences in the likelihood of taking different actions.

Like if something is just vibrating, it just ends up making all action selection choices equally likely.

Whereas like in the sort of sure-handed limit, then you follow either your habits or your free energy minimization habits

without further hindrance so sure-handed optimization of policy shaky-handed if and if it's soft max is one then you're sampling from the posterior so there's two actions and your posterior um evaluation is 60 40.

this parameter is one you you select those policies 60 and 40 percent of the time respectively if you turn up the temperature it converges back to 50 50 and if you turn down the temperature you're always going to select the higher um likelihood one there's some details on the laplace

um assumption there's probably a lot of detail that could be gone into but Laplace puts a quadratic function that is censored at the maximum likelihood solution to a distribution so if it's um bimodal there's two humps to the distribution Laplace will get stuck at one of them

or you'll get other kinds of issues like it'll be stuck at one but very over dispersed and so on so this isn't approximating the distribution completely but it turns out it's a super computable way that in many reasonable situations does really well now we return to the peb so we want to ask regular anova type statistical modeling questions

here we have a linear model this is kind of like spm style where x is a design matrix it's also used in linear regressor models won't go into this in too much more detail but this is kind of like saying just like you could do a linear regression with a design matrix on um height

you can do a linear regression with a design matrix on generative model parameters and still test for an effect of age by height on attention or something like that 9.5 instructions for model-based analysis Ali go for it


SPEAKER_01:
Yeah, I just wanted to point out the relation between the Laplace assumption and the form of the recognition density.

Because very briefly, Laplace assumption mathematically leads to this assumption that the shape of the recognition density would be necessarily a Gaussian density.

So in that case,

the variation of free energy would be equivalent to Gibbs energy.

But without that Laplace assumption, then we would have

variational free energy equals Gibbs energy minus the entropy of the recognition density.

And by recognition density, I mean the approximate probability density for hidden input variables under a certain generative model.


SPEAKER_00:
Thank you.

Many times...

getting to a Gaussian or a quadratic is simplifying it's symmetric it has the minimum number of parameters 2 to describe like the mean and the scaling the mean and the variance but that doesn't mean distributions in the world are like that

however for ones that are even roughly shaped like that like they have a central tendency these kinds of symmetric models can be like super super effective because they have very very well established methods for solving okay

now we're going to get to instructions for the model-based analysis so definitely contrast slash juxtapose this with the recipe from chapter six so in six we were kind of coming to the system what are we modeling why are we doing it what structurally are we looking to include in our gm

are we going to specifically set up that gm and then how are we going to set up the generative process okay doesn't have to be in order but these are what we'll need to figure out now in chapter nine we have some instructions for how to go from not just building a gm like we saw in six but to using the gm for gm based analysis so

collecting behavioral data including metadata or demographic data two formulate a pomdp I I think here they could have said construct a GM as per chapter six but they're focusing like they mentioned above on the discrete time case so that's why they're focusing on the pomdp

specify a likelihood function that is going to simulate behavior and quantify the likelihood of observed actions so we started with data then we formed a pomdp basically like it was laid out in chapter six

turns out we're doing a pomdp like chapter seven but it's basically like formulating the gm and now we're going to actually be using that gm for doing the kinds of papers that people write about where this age group had with this statistical confidence level a higher likelihood of this or that um how should the model be used to calculate a likelihood

specification of priors that's where um you can take a zero centered um approach or again one could use parametric empirical base to just say all previous studies found this distribution so that's going to be my prior distribution um

Here's our prior probability and our likelihood.

What we need now is the posterior probability and the model evidence.

Likelihood function, prior, solving for, now with data, posterior distributions, and evidence.

so we put information in on the front half of Bayes equation now we're solving for the second half of Bayes equation and there are routines in SPM in MATLAB that help with this lastly perform the group level analysis this is also an opportunity for using peb

or you can do any other kind of truly wide variety of statistical methods you could fall back to frequentism and just do a t-test just say all right i had 17 in this group and 35 in this group and we're going to t-test to see if this attention parameter was significantly different you could use a canonical variant analysis or an independent contrast analysis or principal component analysis

any statistical method that you would see in any paper this is just doing statistics at this point a little bit more detail and figure 9.2 is showing the six step inversion procedure

Collect the data.

Make the model.

Specify the likelihood function.

P of U tilde.

Remember back to 91, U tilde is the observed behavior.

So we're doing behavioral modeling.

What we're interested in is the distribution.

of probabilities of um behaviors given theta o and m theta o and m so now we've kind of like specified the pumdp for the mouse in the in the maze but we've pulled back our likelihood function

to in a way um abstract or insulate from all of those subjective model parameters such that our statistical analysis is only about objective model parameters so it'd be like if we were making a cognitive model of a decision to take x or y behavior

We'd want to do that, then collect the data on the behavior and then do a demographic analysis on how X or Y is related with this or that.

And those statistics are not an appeal to the cognitive model of the decision maker.

They're actually a layer of descriptive statistics that will be absolutely familiar to anyone who's done behavioral modeling in the lab.

It's just a slightly different way of looking at them.

but it can be seen as doing model evaluation and estimation using statistics in a way that doesn't mention any of the inner cognitive model parameters.

Because things have been framed in this nice pan meta Bayesian way, we can do the tail of two densities

And here is kind of a visualization of what the t-test would look like.

Those error bars could be bootstrapped, et cetera, et cetera.

But just imagine that they're from a t-test.

So here's five groups.

here we have some parameter that we modeled they're uh novelty seeking something or some other specific model inside and then we can ask like is there more variance within or among these bars

And maybe you see a bar with an asterisk here.

These two are indistinguishable, but they're both significantly different than these three.

And this one is significantly different from that one, but not that.

So you see those kinds of regular ANOVA-type statistics here in a Bayesian setting.

So specification of the GM, Chapter 6 and all that,

allows us to do statistics on it just like if we had some advanced model of materials properties and then we were just testing which one was like harder or softer um before analyzing actual data we may want to check face validity by generating fictive data

so taking just a look at it visually and also doing parameter recovery so that means can you identify or recover parameters or do you have issues with your model like co-linearity

where there's two parameters that are so covariate with each other that sometimes you'll run the model and it will explain the data equally well with wildly different combinations of those two parameters, just for example.

And again, classical SPM-like angle with a design matrix.

it's kind of amazing how it works but the design matrix encodes all of the experimental metadata and then you have like a matrix of regressors and you just smash the design matrix into this beta and somehow the dimensions work out and it's like what you'd want to know

like in SPM it'll be one Matrix with this is the um the age and so on of the of the patients and here's a big Matrix with all of the fmri data and you just smash them together and it and then it sets up this if it's all set up properly

okay and then just we'll look at these upcoming sections but then talk more with open questions and and uh look at any other details there's going to be an analysis of the mirza at all saccade eye movement and smooth pursuit of eye movement atoms and mirza smooth tracking of eye movement on the left atoms

discrete tracking of eye movements Mirza not much more said than that obviously there's papers to go back to and those are even generative models that we can use replicate them and modify them

models of false inference so this is going to be approaching that kind of challenging Nexus of well if it's base optimal then how is their maladaptive behavior how can people have delusions how can there be the accommodation of chemical changes that that just fundamentally alter the substrate of cognition

table 9.1 has a sample of examples that um are papers exploring computational pathology in primarily humans but almost surely vertebrates

addiction impulsivity compulsivity delusions hallucination interpersonal and personality disorder oculomotor syndrome pharmacotherapy prefrontal syndromes visual neglect disorders of interoceptive inference so just giving us some examples and the notes

tend to be defining what that pathology was operationalized as by those studies and then briefly what kind of generative models were used

to support what new explanations predictions or control opportunities are presented when we have for example an active model of oculomotor syndrome as opposed to some other framework for describing that behavior um a useful way of thinking about causes of pathological behaviors to think about the prior belief used for policies

how each part of this may be disrupted to give rise to abnormal policy selection so one could look at a figure and just imagine that we're dealing with a very simple uh mouse-like patient on the top here um or um just looking at it here

we're interested in that behavioral policy selection component that is uh conditioned on preferences and affordances or habits so that's why they mentioned c and e because maybe there's a group of people who are repeatedly or compulsively taking a given behavior but again just totally simplifying making this up

There might be some who prefer that behavior and understand the consequences of their action, so they engage in the selection of the behavior because they expect slash prefer it.

In another case, it actually might not be aligned with the preferences, but it has become habitual, in which case there's a different cognitive model parameterization,

and potentially different consequences or treatment opportunities however the experiment needs to be carefully designed to resolve those possibilities which is why a great advance is to be able to basically not just pre-register what you're going to do but

to do the statistical power analysis and ask which regions of experimental state space are going to be Bayesian optimal for, for example, distinguishing two groups of people, some of whom have a habitual bias and others have a strong prior preference for something.

But it's not just like any given set of experimental data are going to resolve this because these are not measurable things in the world.

They're inferred parameters in our cognitive map.

So it's not just going to happen because we tried.

That being said, we can make the GM ahead of time and do the simulations, do that parameter identification and recovery to determine what kinds of experimental paradigms could resolve that if that was what we were trying to resolve.

Then there's a few more interesting points here on everything from salience and neuromodulators and different pathologies.

And they summarize.

They outlined an approach to use the models

previously described to pose questions to empirical data so chapter six was like the recipe for making the GM and then here we saw like the kind of second stage of the recipe the instructions for model-based analysis that helped us see like not just the building of the model which got us to here but now we see the whole six stages that gets us like to the visualization and statistical analysis that we want in the paper

They also highlighted that you can use these methods to distinguish groups, for example, with a given cognitive model, or you can use it to distinguish alternative model hypotheses given data from one or more group.

They gave a bunch of examples in Table 9-1.

and they focused on the examples of Adams and Mirza et al in eye tracking pursuit giving just a total clear example of how even for the very same data you can model it with continuous or discrete time it's a generic method or experimental design

that offers an opportunity to answer questions about the function of the nervous system and health and disease.

Okay.

That's good for our first discussion on nine.

We'll come back next week and check any questions that people have added.

And yeah, thanks.

We'll come back next week.
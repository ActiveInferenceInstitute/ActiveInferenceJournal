[
  {
    "start": 0.814,
    "end": 1.394,
    "text": " Hello, everyone.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2.015,
    "end": 10.661,
    "text": "It is September 30th, 2022, and we're in cohort two, meeting number five.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 10.881,
    "end": 16.746,
    "text": "We're having our second discussion on the chapter two of the textbook.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 17.566,
    "end": 22.209,
    "text": "So we'll jump in shortly to the questions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 23.305,
    "end": 30.128,
    "text": " But first, just wanted to highlight a stream that'll be happening later today, which is number 49.0.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 30.348,
    "end": 34.57,
    "text": "And I'll share it in the chat here.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 38.112,
    "end": 40.653,
    "text": "Or you can find it through the live stream page.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 42.374,
    "end": 46.256,
    "text": "This is with Ali and Jakob, who are here.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 47.196,
    "end": 50.498,
    "text": "And the upcoming two weeks, we'll have Dalton Soktyevidevel",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 51.667,
    "end": 77.063,
    "text": " coming to discuss the paper and this paper is um it's quite an opus and we we had a lot of fun preparing for it all right so we'll go to the questions and i think these are all just great beginning points is there anyone who wants to",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 79.07,
    "end": 88.736,
    "text": " point to a question that they're interested to begin with, or a question that they asked, or one that they feel has a natural starting point?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 91.458,
    "end": 93.059,
    "text": "Otherwise, I can pick one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 98.002,
    "end": 101.384,
    "text": "Daniel, the question on thinking between cognition",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 103.009,
    "end": 105.51,
    "text": " and thinking was one that I put there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 105.53,
    "end": 114.712,
    "text": "It was just from the beginning of the chapter, that little quote, my thinking is first and last and always for the sake of my doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 115.552,
    "end": 125.074,
    "text": "It's just when you read, I always get confused between cognition and thinking and knowing and all those sort of words and their relationships to each other.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 125.234,
    "end": 127.915,
    "text": "So I was just wondering if we could just sort of clarify that a little bit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 130.711,
    "end": 131.912,
    "text": " Alright, what does anyone think?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 132.812,
    "end": 134.593,
    "text": "Are these terms synonymous?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 136.555,
    "end": 144.379,
    "text": "Or are different things being pointed at with different terms like this?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 148.882,
    "end": 153.705,
    "text": "Well, I would maybe say that thinking could be interpreted as action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 158.706,
    "end": 164.45,
    "text": " And knowing maybe could be a consequence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 164.79,
    "end": 171.695,
    "text": "Well, knowing could be the specific representation of beliefs in the brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 174.237,
    "end": 181.902,
    "text": "And then cognition is, I see that as the kind of overarching blanket.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 189.246,
    "end": 190.546,
    "text": " Well, thinking is, Ali?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 195.607,
    "end": 204.789,
    "text": "Yeah, well, actually, there are many different angles that we can tackle the conceptual differences between these three terms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 205.789,
    "end": 208.99,
    "text": "I'm going to start from the last term, knowing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 210.07,
    "end": 215.031,
    "text": "Well, in epistemology, the standard definition for knowing is justified through belief.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 215.892,
    "end": 216.172,
    "text": "So if",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 218.944,
    "end": 247.318,
    "text": " we take that as a definition of knowing then we can go to thinking as the act of gaining that justified true belief as Jakob has mentioned some people describe thinking in terms of action and then cognition is a kind of a conscious state in which the thinking happens",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 248.417,
    "end": 260.364,
    "text": " I mean, if it's not the action of thinking per se, it's the situation or the state in which these kind of thinkings and knowings happen.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 261.485,
    "end": 267.688,
    "text": "But obviously, this is a very simplistic distinction between them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 267.708,
    "end": 276.754,
    "text": "There are very nuanced discussions going on in the philosophy of mind about the differences between whether",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 278.736,
    "end": 290.142,
    "text": " Are these three concepts mutually exclusive, or they can be related in a kind of hierarchical way to each other?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 290.162,
    "end": 297.886,
    "text": "Awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 298.106,
    "end": 299.947,
    "text": "Very deep answers.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 302.449,
    "end": 326.14,
    "text": " one other take would be thinking is a gerund verb it's a process whereas cognition is a noun and just from a sentiment perspective thinking seems to imply like a process and perhaps the experiencer",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 327.267,
    "end": 348.49,
    "text": " like the subconscious regulation of blood pressure might not be considered thoughtful or thinking, whereas especially as cognitive theories are kind of pervading all areas in a type of like pan-cognitivism,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 349.949,
    "end": 356.473,
    "text": " then cognition is a bit more neutral with respect to phenomenology and awareness.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 357.373,
    "end": 360.095,
    "text": "So one could say a computer is doing a type of cognition.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 360.295,
    "end": 363.397,
    "text": "The sand pile is doing a type of cognitive process.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 365.218,
    "end": 367.199,
    "text": "This membrane is doing cognition.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 367.259,
    "end": 368.74,
    "text": "It's equalizing the temperature.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 369.36,
    "end": 371.441,
    "text": "Those can all be described cognitively.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 372.862,
    "end": 378.526,
    "text": "And especially as we take active inference, which is a framework derived in the neurosciences,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 379.54,
    "end": 383.182,
    "text": " where these terms might have the least space between them.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 384.603,
    "end": 399.091,
    "text": "But then the question of whether a computer is doing thinking or knowing is perhaps a further field, whereas to say that a computer is doing cognitive process or information processing might be less controversial.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 403.393,
    "end": 404.634,
    "text": "Like extended cognition.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 406.002,
    "end": 434.774,
    "text": " then i guess we just don't hear as much about extended thinking or extended knowing but when they're all um in our body you know then these seem to have like the most synonymy my cognition is first and last always for the sake of my doing",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 438.151,
    "end": 440.872,
    "text": " Okay, any other like thoughts or comments on this question?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 448.696,
    "end": 450.597,
    "text": "All right, let's kind of, oh yeah, Bronwyn.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 451.878,
    "end": 459.521,
    "text": "I just want to just say, so it sounds as if it's something that's sort of still in evolution or under discussion.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 460.301,
    "end": 467.565,
    "text": "Is it really still under investigation or attempting to find clarity in this language?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 470.613,
    "end": 481.575,
    "text": " I mean, I like that thinking's the verb and that's sort of where I was, the direction that I was having with thinking that it's an act.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 483.316,
    "end": 485.356,
    "text": "That's my sort of take on it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 485.816,
    "end": 488.317,
    "text": "And cognition is more of the overriding blanket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 489.877,
    "end": 491.117,
    "text": "So, yeah, so thank you for that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 491.177,
    "end": 491.657,
    "text": "That's good.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 493.678,
    "end": 493.838,
    "text": "Yeah.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 495.194,
    "end": 510.13,
    "text": " And we might be able to find coherence like in our own understandings or within the active inference ontology, but the larger scale historical arc around these topics, as well as like",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 511.641,
    "end": 531.361,
    "text": " non-english versions of related topics that's probably more like a philosophical or maybe even like theological domain kind of just trails off but within these terms as used in active inference it may be possible to have a lot more clarity",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 533.021,
    "end": 534.982,
    "text": " especially around cognition.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 535.962,
    "end": 541.725,
    "text": "For the purposes of this model, we're defining cognition as the information processing between input and output.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 542.165,
    "end": 543.166,
    "text": "Not saying that's the only definition.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 543.746,
    "end": 549.549,
    "text": "If one is to say, that's what I mean by cognition, there doesn't need to be a qualia.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 550.789,
    "end": 558.693,
    "text": "But then somebody else, if their definition of thinking or knowing is going to be related to qualia and not information processing...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 560.834,
    "end": 563.776,
    "text": " then that's going to be verging into a different area.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 568.059,
    "end": 568.28,
    "text": "Okay.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 569.12,
    "end": 573.984,
    "text": "So let's kind of continue on this broad and important theme.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 575.565,
    "end": 578.627,
    "text": "How does active inference fit in with neuroscience?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 580.168,
    "end": 586.573,
    "text": "Does anyone who asked the question or provided some of this answers and discourse want to give a thought?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 591.982,
    "end": 593.642,
    "text": " Yeah, I asked the question.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 595.623,
    "end": 596.983,
    "text": "Great.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 598.804,
    "end": 600.104,
    "text": "What are we actually doing here?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 600.984,
    "end": 604.605,
    "text": "Are we doing neuroscience or are we doing machine learning?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 607.506,
    "end": 610.226,
    "text": "I think it's some sort of combination of both.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 610.546,
    "end": 618.588,
    "text": "But when we're talking about things, do we have some guidance of what we're talking about?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 620.307,
    "end": 638.311,
    "text": " It seems as though the original variation of free energy fits in with the biology, there's some correspondence with the physical of the brain, which is related to superficial pyramidal cells or whatever.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 640.391,
    "end": 648.133,
    "text": "The expected free energy seems so much like reinforcement learning and quite away from the neuroscience.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 652.421,
    "end": 652.661,
    "text": " Great.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 652.821,
    "end": 655.263,
    "text": "Thanks for the comments and ideas.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 655.363,
    "end": 661.587,
    "text": "Does anyone want to add or speak to these faces?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 661.987,
    "end": 662.728,
    "text": "Ali, thank you.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 667.311,
    "end": 680.019,
    "text": "I would say about FEP literature before 2019 was mainly concerned with the neuroscientific descriptions of the mind.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 681.926,
    "end": 684.029,
    "text": " using this active inference framework.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 684.67,
    "end": 694.205,
    "text": "But from around 2019, things have changed and a lot of research are going on which try to relate",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 695.928,
    "end": 712.473,
    "text": " active inference framework to other areas in physics, in economics, in even climatology or other kinds of phenomena which can be modeled using FVP slash active inference framework.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 713.133,
    "end": 718.895,
    "text": "So I'd say that currently the active inference",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 719.855,
    "end": 748.098,
    "text": " framework is just is or at least it converges to be a kind of modeling framework or a kind of mathematical structure which would encompass all can potentially encompass many different modeling frameworks in it so yeah that's the current status of the research",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 749.815,
    "end": 750.576,
    "text": " an active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 750.716,
    "end": 755.639,
    "text": "It's not solely focused on neuroscientific descriptions of the mind.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 757.58,
    "end": 757.88,
    "text": "Awesome.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 758.48,
    "end": 758.68,
    "text": "Thanks.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 758.741,
    "end": 759.061,
    "text": "Jakub?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 761.902,
    "end": 772.649,
    "text": "Yeah, I would maybe add to the point that Neil mentioned that sometimes it feels like it's neuroscience and sometimes it feels like it's reinforcement learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 773.189,
    "end": 773.45,
    "text": "I think",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 776.963,
    "end": 793.838,
    "text": " I would think more about active inference on the same level as thermodynamics and physics, where the specific model is not necessarily that important, although we are assuming we are in this kind of Bayesian scheme.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 795.439,
    "end": 802.623,
    "text": " Reinforcement learning has also been applied to modeling the role of dopamine in the brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 802.763,
    "end": 815.71,
    "text": "So in a sense, we could say if we if we put it in the in that context, we could say that reinforcement learning is also related to neuroscience, even though it's also used in other contexts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 816.311,
    "end": 819.933,
    "text": "So I guess I'd say that acting if it's",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 821.59,
    "end": 830.559,
    "text": " Again, this kind of overarching blanket that covers multiple areas, but it's not necessarily one or the other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 830.579,
    "end": 834.302,
    "text": "Awesome.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 835.003,
    "end": 837.145,
    "text": "All right, I'll add a few more thoughts and then anyone who",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 838.373,
    "end": 839.053,
    "text": " That's a comment.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 839.714,
    "end": 845.316,
    "text": "So first, this is going to be a slide from later today in livestream 49.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 847.217,
    "end": 851.879,
    "text": "And it's a Maxwell Ramstead tweet thread, you know, hashtag scholarship in 2022.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 854.244,
    "end": 861.566,
    "text": " And Maxwell describes first wave free energy models, which didn't have policy selection per se.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 862.386,
    "end": 868.068,
    "text": "The dynamics of internal and active states, also known as particular states, because that's like the moving particle.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 868.268,
    "end": 871.809,
    "text": "It's like the internal states and the blanket make up the particle.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 872.489,
    "end": 876.07,
    "text": "And then the autonomous states are the internal states and the action states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 876.13,
    "end": 878.251,
    "text": "Those are the ones like under direct control.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 879.329,
    "end": 883.098,
    "text": " whereas you can't directly control your sensory states, but they're still part of your particle.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 883.699,
    "end": 887.828,
    "text": "So there was a gradient descent happening on the autonomous states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 888.692,
    "end": 896.974,
    "text": " So it was kind of a one-step model and hence couldn't really be said to entertain policy selection, counterfactuals, planning as inference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 897.194,
    "end": 900.916,
    "text": "And so that was like very much a variational free energy framing.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 902.416,
    "end": 915.06,
    "text": "And that's totally consistent with a wide range of previous Bayesian work on gradient descent, variational autoencoders, the variational Bayesian methods, all of that area.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 916.34,
    "end": 923.685,
    "text": " Second wave models, starting around 2012, equipped agents with beliefs about state transitions, especially sensory consequences of movement.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 924.726,
    "end": 937.035,
    "text": "The emphasis on the sensory outcomes of action has been linked by others to perceptual control theory and also bridges towards embodiment.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 938.82,
    "end": 946.247,
    "text": " This enables the design of models with proper policy selection that evaluated average free energy expected under each policy.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 947.729,
    "end": 959.361,
    "text": "Then third wave models use recursive expected free energy functional, which enables some more advanced modeling tools like counterfactuals.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 960.936,
    "end": 962.436,
    "text": " So that is one angle.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 963.917,
    "end": 968.557,
    "text": "Another angle, and so the question was spot on with kind of beginning with neuroscience.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 969.297,
    "end": 987.321,
    "text": "If we go to Carl Fristen's Google Scholar page, his most cited work and the work that makes him the most cited neuroscientist alive slash ever, you'll find, although the Free Energy Principle 2010 paper",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 989.335,
    "end": 992.919,
    "text": " has been cited as like a keystone FE paper.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 994.24,
    "end": 1003.889,
    "text": "Almost all these several tens of thousands are related to methods related to statistical parametric mapping, SPM.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1004.793,
    "end": 1022.86,
    "text": " which is arising from the Wellcome Imaging Center at the University College in London and the decades of Carl and others work in neuroimaging basic research and the tool development around it.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1024.741,
    "end": 1032.945,
    "text": "So coming out of this framework where they were doing time series modeling, like dynamical modeling on",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1034.031,
    "end": 1036.973,
    "text": " hidden state inferences on spatial data.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1037.193,
    "end": 1049.379,
    "text": "Like you're getting the MRI data, but then you're inferring the neural activation that gives rise to this as mediated by like sensor and various sources of stochasticity.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1052.161,
    "end": 1061.826,
    "text": "The generality of that framework, especially in light of some philosophical contributions by, well,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1062.931,
    "end": 1075.592,
    "text": " In some ways, since 2010, since 2005 to 2010, Friston was already going for the sort of across domain applications of free energy.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1076.233,
    "end": 1076.574,
    "text": "And so...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1080.262,
    "end": 1094.313,
    "text": " Another kind of related, like potentially extremely formally related, but just in its scope is like the Constructal Law by Adrian Bajon and other works like that, like looking for patterns across systems.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1097.356,
    "end": 1106.644,
    "text": "And then, as Ali mentioned, that scope was then followed up on with people working towards specific applications in different areas.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1107.95,
    "end": 1121.201,
    "text": " And then now that there's been such developments with the software packages and mathematical advances and formalisms, that's where we see the encompassing of a lot of different modeling frameworks.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1123.363,
    "end": 1125.405,
    "text": "This is a great question too.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1125.665,
    "end": 1130.469,
    "text": "How should we be thinking of active inferences relationship with the way we think the brain actually works?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1132.011,
    "end": 1133.652,
    "text": "Chapter five is going to be...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1135.754,
    "end": 1162.646,
    "text": " about that it's going to be focusing on specific neural systems like the spinal reflex arc and also like um uh cortical processing and there's some there's going to be a lot of fun discussions on that because there's kind of like a map territory issue like if you use a linear regression on the brain no one thinks the brain is a linear regression",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1163.68,
    "end": 1175.227,
    "text": " But if you use active inference on the brain, what is it that makes people think that something is being done with the map that isn't just a neural model?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1176.928,
    "end": 1182.392,
    "text": "Like in SPM, no one said, well, the brain is a statistic parametric map because we use the SPM package.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1183.772,
    "end": 1192.338,
    "text": "Yet it can feel like people are using an active inference model and the claims that they derive are, this is the brain doing active inference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1193.288,
    "end": 1197.512,
    "text": " but isn't that kind of like using the SPM package and then saying that the brain is doing SPM?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1199.073,
    "end": 1201.295,
    "text": "So these are really important questions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1207.04,
    "end": 1221.432,
    "text": "One other note would be the structure of neuroscience in terms of interacting and nested information processing units",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1222.89,
    "end": 1233.079,
    "text": " is a natural fit for how active inference enables modeling of interactions and nesting of information processing units.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1239.285,
    "end": 1242.348,
    "text": "Any other thoughts or questions on this area?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1246.425,
    "end": 1275.462,
    "text": " right yeah yeah thanks thanks for for clearing that up i'm looking forward to chapter five then oh awesome yeah all right let's look at we have so these look like there's some surprise related questions and then we have some questions on free energy okay so are the conditions inconditional probabilities",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1278.072,
    "end": 1283.653,
    "text": " So regarding the conditions in conditional probabilities, are they mostly subjective, objective, or something else?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1285.213,
    "end": 1286.513,
    "text": "We may have explored it last week.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1289.494,
    "end": 1293.075,
    "text": "Models in their construction are prior dependent, even in figure 2.1.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1295.115,
    "end": 1297.415,
    "text": "Friars are necessarily multi-perspectival.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1298.996,
    "end": 1300.776,
    "text": "Is there anything anyone wants to add?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1302.316,
    "end": 1306.517,
    "text": "The conditions are conditioned upon by the modeler.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1307.81,
    "end": 1310.03,
    "text": " This is kind of like a map territory distinction.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1311.271,
    "end": 1319.473,
    "text": "Like in the writing of the expression, the vertical line means that the modeler's conditioning the first part upon the second part.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1320.713,
    "end": 1329.695,
    "text": "And then the question of in the real world, whether something is conditioned upon is kind of the difference between the map and the territory.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1331.955,
    "end": 1335.656,
    "text": "But is there any other like area to explore here?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1344.13,
    "end": 1344.37,
    "text": " Okay.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1348.474,
    "end": 1356.06,
    "text": "What are some examples of the difference between natural psychological surprise and Bayesian surprise?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1357.741,
    "end": 1357.982,
    "text": "Okay.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1359.283,
    "end": 1363.887,
    "text": "So let's think about those two surprise definitions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1367.93,
    "end": 1372.374,
    "text": "The first notion of surprise, and they're both going to be measured in information theoretic units.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1375.0,
    "end": 1385.963,
    "text": " The first type of surprise is just the word surprise by itself, and it is how surprising a given observation is.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1386.943,
    "end": 1398.106,
    "text": "So if the classroom height is four feet plus or minus one, you're gonna be minimally surprised with four feet observations, and you're gonna get more and more surprised as the observations get further from four.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1400.406,
    "end": 1403.347,
    "text": "That is what is described in this column.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1404.48,
    "end": 1418.024,
    "text": " This takes in an observation and then it contrasts that observation's difference with respect to the parameterization of whatever family of distribution is being modeled.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1419.405,
    "end": 1426.267,
    "text": "And then that tells you how surprising any given observation is, given how a distribution is set.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1427.472,
    "end": 1441.72,
    "text": " The second notion of surprise is Bayesian surprise, and that's referring not to how surprising a data point is, but how far that data point updates the distribution.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1442.82,
    "end": 1455.047,
    "text": "So depending on how fixed you are in your priors, you could imagine a super surprising data point with zero Bayesian surprise because it doesn't move the distribution at all, but it was super surprising.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1456.138,
    "end": 1465.363,
    "text": " And then conversely, you could imagine a data point that's not too surprising, but it updates the priors to fit it exactly.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1467.824,
    "end": 1472.926,
    "text": "Now the question asks about natural or psychological surprise.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1474.527,
    "end": 1480.11,
    "text": "So that is connecting to kind of a experiential or psychological question.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1483.717,
    "end": 1495.321,
    "text": " How would anyone relate these formal concepts of surprise and Bayesian surprise to natural or psychological surprise?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1507.806,
    "end": 1507.966,
    "text": "Ali?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1512.436,
    "end": 1534.782,
    "text": " Well, one interesting example that I think can illuminate the distinction between these two kinds of surprises is the Monty Hall problem Because, you see, the result of... I mean, when we solve the Monty Hall problem, the result is very surprising psychologically",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1535.973,
    "end": 1541.654,
    "text": " but not in probabilistically or Bayesian terms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1542.074,
    "end": 1549.676,
    "text": "So you see, the Monty Hall problems actually is a very famous puzzle.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1550.056,
    "end": 1554.657,
    "text": "I'm not sure if everybody is familiar with that, but I'm going to briefly sketch it out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1555.417,
    "end": 1558.338,
    "text": "Suppose that we have three doors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1560.619,
    "end": 1563.561,
    "text": " behind one of which is a prize.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1564.501,
    "end": 1569.565,
    "text": "And the moderator of the competition opens up",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1571.874,
    "end": 1582.802,
    "text": " a non-prized door and asks us if we want to change our initial choice of which door we want to open.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1583.243,
    "end": 1599.796,
    "text": "So our intuition says that regardless of before opening that door and after opening the door, our choice would be just one half of the probability of being",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1601.218,
    "end": 1603.321,
    "text": " the price behind one of those doors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1603.821,
    "end": 1616.616,
    "text": "But if we look at it in terms of the Bayesian inference or Bayesian probability, we can see that it's not actually one half, but it's one third.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1616.736,
    "end": 1618.538,
    "text": "So we would have...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1622.292,
    "end": 1630.456,
    "text": " we can double our chances of winning if we change our choice after the moderator opens that non-prized door.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1631.016,
    "end": 1639.74,
    "text": "But this is actually a very surprising conclusion, I mean, in the sense of psychologically surprising.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1640.28,
    "end": 1646.123,
    "text": "And it has even confused some of the greatest mathematicians, for example,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1650.711,
    "end": 1665.223,
    "text": " Erd\u0151s didn't accept this, but finally after seeing some computer simulations of this problem, he finally accepted this result.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1665.303,
    "end": 1677.112,
    "text": "But the point here is to define surprise in terms of its Bayesian inference is just a kind of mathematical",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1680.835,
    "end": 1692.431,
    "text": " or it's a kind of probabilistic concept and it doesn't necessarily maps isomorphically or one-to-one to our psychological surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1694.093,
    "end": 1694.933,
    "text": " Yeah, awesome.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1695.514,
    "end": 1706.159,
    "text": "And this is going to come up again and again in the textbook and in our work in active inference, like there'll be a variance parameter and they'll say, this is the anxiety parameter.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1706.919,
    "end": 1709.74,
    "text": "Well, is that one in the same?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1711.001,
    "end": 1712.902,
    "text": "You know, does that parameter value translate?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1713.582,
    "end": 1720.045,
    "text": "And it's a very general issue in parametric cognitive modeling.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1721.252,
    "end": 1726.854,
    "text": " and also about interpretability of parameterized models, but it's an important one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1727.795,
    "end": 1734.337,
    "text": "So one could imagine some ways in a situation specific way to address this.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1734.977,
    "end": 1743.741,
    "text": "Like if you have participants in an experiment, you're making a parametric cognitive model of they're looking at the screen and stuff.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1744.755,
    "end": 1752.482,
    "text": " And so you're able to calculate, given what you believe their cognitive parameters to be, the surprise and the Bayesian surprise of different stimuli.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1753.463,
    "end": 1761.01,
    "text": "And then you could ask them, just purely emotionally, how surprising would you say that was?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1762.111,
    "end": 1768.717,
    "text": "And then you could do some sort of correlation between the self-reported psychological surprise and the",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1770.0,
    "end": 1793.594,
    "text": " the values that you inferred but one can already see that there's like some complexity there for example around the enculturation of the communication or understanding of what psychological surprise is so on the pro side this is a very tangible and accessible experience that people basically get what does it mean to be surprised you know you open the box and it's not what you thought",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1795.319,
    "end": 1797.62,
    "text": " How does that relate to technical and formal definitions?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1798.28,
    "end": 1800.261,
    "text": "Sometimes it's clear, sometimes it's a little unclear.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1800.281,
    "end": 1808.465,
    "text": "All right, and one more surprise related question.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1809.306,
    "end": 1810.306,
    "text": "All right, table two one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1811.467,
    "end": 1815.309,
    "text": "Why does Gaussian surprise not contain a variance parameter?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1816.509,
    "end": 1818.01,
    "text": "Why a pi product?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1818.87,
    "end": 1824.093,
    "text": "Why not surprise perhaps equals natural log of the normal distribution of the mean",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1825.707,
    "end": 1833.509,
    "text": " of the data point observed, mu commonly used for the mean and sigma commonly used for the variance.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1842.551,
    "end": 1845.631,
    "text": "I think there's a few ways to explore this.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1848.792,
    "end": 1852.553,
    "text": "Pi is also sometimes used as a variance parameter.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1854.15,
    "end": 1858.031,
    "text": " So this can be seen as like, and it's multiplied.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1858.171,
    "end": 1861.692,
    "text": "I don't think this is pi as like a, it's not a big pi.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1862.012,
    "end": 1864.973,
    "text": "It's not a multiplicative product.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1865.353,
    "end": 1866.934,
    "text": "Like this sigma is a big sigma.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1867.534,
    "end": 1869.895,
    "text": "And so it's a sum over dot, dot, dot, dot, dot.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1871.67,
    "end": 1875.914,
    "text": " Whereas a lowercase sigma smaller font is used as like a parameter.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1876.495,
    "end": 1885.123,
    "text": "So I believe that the parameterization that the notation that they're using for the Gaussian, someone please correct if this is not true.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1886.144,
    "end": 1888.406,
    "text": "Mu is the mean of the Gaussian.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1889.587,
    "end": 1891.91,
    "text": "Pi is the variance.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1893.678,
    "end": 1895.399,
    "text": " It's precision.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1895.96,
    "end": 1897.541,
    "text": "Thank you.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1898.102,
    "end": 1910.972,
    "text": "Yeah, and so by using the pi, it alludes to the p. And so this is a precision-weighted difference between the observation and the parametric mean.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1911.012,
    "end": 1915.035,
    "text": "So we do have both of the parameters of the Gaussian.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1916.276,
    "end": 1918.878,
    "text": "If the observation is the mean...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1920.071,
    "end": 1941.074,
    "text": " then it's going to be minimally surprising as the the the residual the difference between the observation and the mean increases then it gets um more surprising is there more to add on this",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1944.02,
    "end": 1958.172,
    "text": " Well, I was expecting to see an equation where there's a normal function, e to the minus x minus mu over sigma squared, whatever.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1960.134,
    "end": 1969.783,
    "text": "I would have thought the surprise was the negative log probability of that, which actually might come out as x minus mu.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1971.977,
    "end": 1973.537,
    "text": " I'm talking through to the answer, yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1974.098,
    "end": 1980.76,
    "text": "X minus mu over variance, which is X minus mu times precision, isn't it?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1981.24,
    "end": 1982.46,
    "text": "So yeah, that makes sense.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1983.42,
    "end": 1999.385,
    "text": "Okay, just to ask or clarify, does it have to do with the fact that the surprise, fancy I, fancy J, whatever, if you want to look at it, is in a natural logged scale?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2001.556,
    "end": 2009.509,
    "text": " So we don't see a natural log or anything in the representation because we're already in that space?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2011.612,
    "end": 2013.015,
    "text": "Yeah, yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2013.375,
    "end": 2016.18,
    "text": "I think me trying to explain",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2017.045,
    "end": 2019.026,
    "text": " Myself, I answered the question there.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2019.626,
    "end": 2029.872,
    "text": "I think it's because it's, yeah, you're taking the log of an exponent, so it just, yeah, and with pi now being precision, that all makes sense.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2031.453,
    "end": 2031.773,
    "text": "Awesome.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2033.054,
    "end": 2033.474,
    "text": "That's great.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2033.874,
    "end": 2034.775,
    "text": "Yeah, cool.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2036.215,
    "end": 2043.76,
    "text": "Okay, now let's move to talking about the generative model, and then at the end, we'll explore...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2045.049,
    "end": 2049.21,
    "text": " what expected and variational free energy are with respect to generative models.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2051.251,
    "end": 2069.556,
    "text": "So, are these ideas about the fit between the internal generative model and observations, which is variational free energy, that's equation 2.5, and expected free energy, that's 2.6, applicable to states other than ordinary waking states?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2070.498,
    "end": 2077.22,
    "text": " So altered states of cognition or consciousness, such as sleep, dreams, sensory deprivation, seemingly disembodied states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2078.66,
    "end": 2082.161,
    "text": "Sometimes sensation doesn't seem to play a large role in experience.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2088.242,
    "end": 2094.104,
    "text": "There's probably a whole host of ways to go here.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2098.157,
    "end": 2114.127,
    "text": " So one of them is that the modeling of a generative model as being continually engaged in perception, cognition, action is applicable whether the input is boring or not.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2114.147,
    "end": 2116.709,
    "text": "It's just one note.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2117.53,
    "end": 2122.313,
    "text": "Another one is while sensory deprivation, it doesn't mean your senses are off.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2123.553,
    "end": 2134.797,
    "text": " It just means that the novelty or the diversity of stimuli is reduced or deprived, but you're still having observations.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2135.457,
    "end": 2140.098,
    "text": "Like the camera, if it's on, is still getting observations even when the lens cover is on.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2141.179,
    "end": 2148.421,
    "text": "They might be boring or not informative, but in the sense that we're talking about observations,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2149.503,
    "end": 2154.765,
    "text": " one cannot be deprived of observations without having that sense just removed.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2160.526,
    "end": 2166.428,
    "text": "Sleep and dreams, if the modeler chose to do it that way, then I don't see any reason why not.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2166.748,
    "end": 2175.671,
    "text": "And sometimes primary sensation may not seem to play a large role in experience.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2176.701,
    "end": 2185.17,
    "text": " However, it's actually altered states of consciousness that people have used to explore the role of the generative model in perception.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2186.471,
    "end": 2191.996,
    "text": "So an example of that might be the Albus paper by Adam Saffron.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2193.798,
    "end": 2195.14,
    "text": "There's this one image.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2195.26,
    "end": 2196.481,
    "text": "It's like very funny.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2201.107,
    "end": 2207.13,
    "text": " It's going to start with a person doing a cup of tea, and then they're going to start talking to aliens.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2209.151,
    "end": 2211.152,
    "text": "Maybe it's in a later paper by Saffron.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2216.683,
    "end": 2221.948,
    "text": " So, okay, so here's different brain regions and the person's like, okay, I'm seeing the apple, here's an affordance.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2222.208,
    "end": 2224.55,
    "text": "These brain regions are being activated in a certain way.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2224.57,
    "end": 2227.172,
    "text": "And then I look down at my foot, there's my foot.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2228.213,
    "end": 2234.658,
    "text": "And then that's the medium or that's the threshold dose of a psychotropic something.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2235.699,
    "end": 2242.805,
    "text": "Then in the medium dose, activation patterns are altering the vividness",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2244.136,
    "end": 2247.037,
    "text": " and the association amongst different ideas.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2249.318,
    "end": 2266.164,
    "text": "And then in the extreme doses, there's like different patterns of linking that are leading to experiences that on one hand, somebody could say, well, they're lifted from the sensory observations.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2267.225,
    "end": 2272.327,
    "text": "But another way to put it is the ongoing flow of sensory observations",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2273.317,
    "end": 2277.202,
    "text": " is being integrated and processed differently.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2277.242,
    "end": 2289.8,
    "text": "It's leading to a different self-organized harmonic mode in the language of Saffron, such that the perception, the experience, which is what we have is different.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2291.65,
    "end": 2304.894,
    "text": " And one like little piece of lore around the Albus is Carhart Harris, who's a psychedelics researcher in Friston, wrote this paper, Rebus, Relaxed Beliefs Under Psychedelics.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2305.494,
    "end": 2306.234,
    "text": "Just relax.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2307.775,
    "end": 2313.437,
    "text": "And then Adam followed with this paper, Albus.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2314.277,
    "end": 2317.298,
    "text": "It's like, well, it's not simply relaxed beliefs.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2318.516,
    "end": 2325.961,
    "text": " In fact, in the hierarchical predictive processing architecture, some beliefs might be strengthened.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2327.162,
    "end": 2337.149,
    "text": "Like top-down priors might be strengthened in their impact, and that's why the priors might be seen in a static field.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2338.456,
    "end": 2353.088,
    "text": " You still, like, there might be ways in which actually top-down beliefs are sharpened and that's why sounds could be heard from water bubbling or images could be seen in the static or in the tea leaves.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2354.869,
    "end": 2357.271,
    "text": "So it's not just like relaxed beliefs.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2357.611,
    "end": 2359.553,
    "text": "Simply, it's just more generally altered.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2366.022,
    "end": 2371.746,
    "text": " And that sets up perfectly to go through equation 2.5 and 2.6 and this related question.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2371.766,
    "end": 2374.728,
    "text": "All right.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2377.329,
    "end": 2385.134,
    "text": "So we're going to look at equation 2.5 and 2.6 and explore how tractable they are.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2390.537,
    "end": 2390.737,
    "text": "Let's...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2391.826,
    "end": 2393.186,
    "text": " Recall equation 2.5 and 2.6.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2393.346,
    "end": 2404.548,
    "text": "So first, as always, thanks for the epic work to the people who added derivations and the natural language descriptions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2407.069,
    "end": 2409.95,
    "text": "Okay, so what is happening in equation 2.5?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2412.63,
    "end": 2417.051,
    "text": "What variables are coming into the picture and how are they being processed?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2418.174,
    "end": 2421.699,
    "text": " What is this F describing?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2423.081,
    "end": 2425.264,
    "text": "What's a situation where F is big?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2425.845,
    "end": 2427.567,
    "text": "What's a situation where F is low?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2430.552,
    "end": 2432.555,
    "text": "Does anyone want to give a thought first?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2446.775,
    "end": 2452.459,
    "text": " Also in these descriptions, the letters are connected to the ontology terms.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2453.299,
    "end": 2457.982,
    "text": "So just a few that are important to know is X are the hidden states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2459.403,
    "end": 2460.724,
    "text": "Y are the observations.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2462.245,
    "end": 2463.786,
    "text": "That's one important one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2466.288,
    "end": 2472.572,
    "text": "Another important one is that P and Q are two distributions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2473.472,
    "end": 2476.214,
    "text": "Q is like the posterior updated distribution.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2482.552,
    "end": 2484.273,
    "text": " Does anyone want to add anything on 2.5 or 2.6?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2484.914,
    "end": 2491.019,
    "text": "Or we can start to think about what it's actually doing or calculating.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2494.542,
    "end": 2497.064,
    "text": "Okay, so some pieces to note.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2499.446,
    "end": 2503.669,
    "text": "Variational free energy is not prospective.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2504.61,
    "end": 2511.736,
    "text": "It's dealing with the real-time flow of data, a single data point in the present,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2512.881,
    "end": 2528.535,
    "text": " and then how that data point can be evaluated with respect to the current parameterization of that data point.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2529.896,
    "end": 2536.762,
    "text": "So this is like a function, F, variational free energy, and it's gonna be bringing two arguments in.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2538.032,
    "end": 2545.679,
    "text": " Those two arguments, like the two things that we need to combine, which they happen to be functions, q is a function.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2546.68,
    "end": 2550.443,
    "text": "And so f is called a functional because it's like a function of a function.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2551.785,
    "end": 2559.992,
    "text": "So what we need to know to calculate f is this variational distribution and the incoming data point.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2564.159,
    "end": 2576.023,
    "text": " One other thing to keep in mind before we jump into it is we talk about like minimizing free energy, but with a negative sign, minimizing and maximizing are the same.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2576.983,
    "end": 2583.666,
    "text": "So we can just think about as minimizing or maximizing as just like a relativization procedure.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2584.935,
    "end": 2607.75,
    "text": " And it turns out that in this minimization framework, free energy minimization, expected free energy minimization are like the less surprising ones, which are going to be understood to be the better ones or more preferable because they're more consistent with being that kind of a thing.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2609.432,
    "end": 2611.273,
    "text": "Those are going to have lower values.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2617.904,
    "end": 2622.026,
    "text": " Now, just some of the sub functions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2623.326,
    "end": 2626.768,
    "text": "Fancy E is an expectation of.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2627.528,
    "end": 2633.331,
    "text": "So we talked about it previously, but this is not like a future expectation, like expected returns on an investment.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2633.891,
    "end": 2639.574,
    "text": "This is like the expectation of a Gaussian with a mean of four and a variance of one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2639.894,
    "end": 2641.234,
    "text": "The expectation is four.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2642.115,
    "end": 2646.737,
    "text": "So this is like an average of some other distribution.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2649.201,
    "end": 2653.123,
    "text": " In contrast, h is an entropy function.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2654.724,
    "end": 2668.731,
    "text": "The entropy is taking in a distribution and then it is calculating how dispersed that distribution is, how disordered that distribution is.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2671.172,
    "end": 2674.794,
    "text": "ln is the natural log, so that's a logarithm.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2675.614,
    "end": 2678.196,
    "text": "And then the only other kind of like wrapper here",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2679.386,
    "end": 2681.407,
    "text": " is this KL divergence.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2682.668,
    "end": 2691.953,
    "text": "And so KL divergence, we talked about it last time as well, the two lines are like the two distributions that the KL divergence is contrasting.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2693.073,
    "end": 2701.858,
    "text": "Okay, so if we're taking expectations, then the higher the value, the higher the expectation is.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2702.038,
    "end": 2706.16,
    "text": "This is just like the average height in the classroom, expectation of the class.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2707.641,
    "end": 2708.662,
    "text": "Higher expectation,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2710.376,
    "end": 2713.079,
    "text": " Higher internally goes to higher here.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2714.24,
    "end": 2718.325,
    "text": "For entropy, more dispersed is higher entropy.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2720.907,
    "end": 2724.191,
    "text": "For natural log, it's a transformation of a number",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2725.896,
    "end": 2754.013,
    "text": " but they're monotonically related so as the number gets bigger the natural log gets bigger too it slows down in how fast it gets bigger but a bigger number on the number line is always going to have a higher natural log just keep in mind that there's like a minus sign so it's actually like the other the other way and then lastly the kl divergence if these two distributions are the same their divergence is zero",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2755.226,
    "end": 2773.139,
    "text": " and when the distributions are increasingly different this kl term becomes bigger okay anyone want to add a comment in here yes ali and then gsm",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2775.804,
    "end": 2789.34,
    "text": " Yeah, I just wanted to mention that for, I mean, for the expected or variational free energy and the expected free energy, computations are not that costly, but when we come to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2790.721,
    "end": 2802.81,
    "text": " to computing the priors over all the policies up to a specific time horizon, that would increase the complexity.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2802.89,
    "end": 2805.952,
    "text": "I mean, it has an exponential complexity class.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2806.433,
    "end": 2815.899,
    "text": "So that's why some people have experimented with proposing some alternative implementations for active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2817.741,
    "end": 2819.542,
    "text": "For example, one of which is branching",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2820.851,
    "end": 2847.966,
    "text": " time active inference which reduces this complexity significantly or the work by baron millage and so on uh but yes i mean the expected free energy or the variation of free energy uh i mean they're not costly per uh by themselves but um when the policy comes along uh that's a different story",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2849.491,
    "end": 2851.913,
    "text": " Yeah, just one quick note and then Giuseppe.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2853.314,
    "end": 2861.02,
    "text": "The variational framework is a tractability increasing move from the outset.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2861.901,
    "end": 2866.605,
    "text": "We have a factorized model, so it's not like every variable influences every variable.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2867.305,
    "end": 2876.292,
    "text": "And what variational inference does is it actually uses specific, optimizable families of distributions",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2877.253,
    "end": 2895.59,
    "text": " are set by the modeler and takes into account the sparsity of the variables how they're related and it turns out that that's actually um for what it is a very tractable approach then within this variational inference framework",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2896.658,
    "end": 2908.063,
    "text": " the F, variational free energy, is going to be like far more computable because all it's doing is taking in one data point and then comparing it with the priors, basically.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2909.103,
    "end": 2915.426,
    "text": "Whereas expected free energy is conditioned upon counterfactual policies.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2916.407,
    "end": 2923.37,
    "text": "And so that requires enumerating policies of a given time horizon and then evaluating them relative to each other.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2924.338,
    "end": 2934.225,
    "text": " And so while each given consideration is not that computationally challenging, as Ali mentioned, the space of possible policies can be vast.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2934.766,
    "end": 2945.734,
    "text": "So if one has like four affordances, like four, you can go up, down, left, right, and you want to consider a time horizon of two, you have 16 policies of time step length two.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2946.575,
    "end": 2949.497,
    "text": "But if you wanted to consider policies of time step length 100, you would have",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2951.775,
    "end": 2955.717,
    "text": " of course, an exponentially increasing number of possible ways to look.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2956.138,
    "end": 2972.947,
    "text": "And that's what motivates not just the variational approach, which does kind of as well as we can within a given policy, but also tree search and selection strategies, not unlike chess playing computers in the 80s.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2974.383,
    "end": 2983.807,
    "text": " where it was like, well, we can't just consider every branch, but if we prune a branch just because there's one move that doesn't look awesome, we're gonna be doing just local optimization.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2984.387,
    "end": 2999.854,
    "text": "And so it's always this balance with using generative models to explore branches more comprehensively and deeply, but not prune them preemptively in case there's like a sacrifice of a pawn and then it gets you a castle.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3000.916,
    "end": 3023.519,
    "text": " don't want to discard that move but as those get more and more sparse well you sacrifice a pawn and 200 moves later you get the castle it's hard to imagine just from information theory perspective how that needle in the haystack is going to be found giuseppe um yes now just just a couple of um",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3025.14,
    "end": 3033.094,
    "text": " of details for the people that are not too versed mathematically, that in the equation 2.5,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3035.896,
    "end": 3053.428,
    "text": " one thing to highlight is that the expectation of what is in square brackets is always expectation with respect to a specific probability, which is, in this case, the approximate posterior qx.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3054.329,
    "end": 3058.712,
    "text": "So it's a kind of a, if you want a weighted average,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3059.707,
    "end": 3065.051,
    "text": " but with the weights provided by that specific probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3066.853,
    "end": 3074.659,
    "text": "So it's an integral, but, and also that entropy is basically the average surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3078.902,
    "end": 3079.242,
    "text": "Awesome.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3079.702,
    "end": 3080.703,
    "text": "Very nice point.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3080.783,
    "end": 3084.126,
    "text": "Entropy is average surprise.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3087.057,
    "end": 3090.438,
    "text": " Or we can say it's the expectation of surprise.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3091.638,
    "end": 3092.699,
    "text": "Yes, yes.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3093.279,
    "end": 3093.559,
    "text": "Yeah.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3094.259,
    "end": 3103.662,
    "text": "So, and this is kind of where we start to see some of these, like make it fit well, but don't fit too well.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3104.462,
    "end": 3116.426,
    "text": "Like we can bound, you can say, I have what I believe to be just all things considered the best model because I'm expecting 10 plus or minus one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3117.62,
    "end": 3142.374,
    "text": " so even though not every single pull is 10 so i'm getting a non-zero surprise as i'm pulling you know 9.5 10.5 10.3 9.8 but my model of 10 with an expected surprise i wouldn't change a thing about it that's where variational free energy can get you",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3147.952,
    "end": 3155.494,
    "text": " Jakob provided some derivations that connect the first line to the second line.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3158.574,
    "end": 3161.455,
    "text": "So that's one interesting thing to explore.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3162.795,
    "end": 3172.217,
    "text": "And a really nice move here that's highlighted is using some of the definitions of the operations of natural logs.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3174.485,
    "end": 3179.889,
    "text": " to separate them as basically like a sum of logs.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3182.091,
    "end": 3187.454,
    "text": "And for logs, adding them as multiplication and subtracting them as division.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3191.057,
    "end": 3203.166,
    "text": "So that's why like ln of q of x minus ln of p of x can be written as ln of q over p. And then this, the expectation of the natural log",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3204.514,
    "end": 3209.879,
    "text": " of a distribution divided by another distribution is the definition of the KL divergence.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3211.4,
    "end": 3213.281,
    "text": "So that's where the KL comes from actually.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3215.884,
    "end": 3220.487,
    "text": "So that's one really nice piece to think about.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3222.749,
    "end": 3229.895,
    "text": "And then with how line one gets to line three, one can also trace",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3231.158,
    "end": 3260.093,
    "text": " how the variables are kind of being like split and recombined and each of these formalizations are going to have like a slightly different advantage in certain situations like entropy you have a term that's only dependent upon the hidden state expected surprise here you have a term with evidence that's only dependent upon the information coming in",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3261.567,
    "end": 3270.691,
    "text": " So this can be seen as actually like a constant with respect to your optimization of beliefs about hidden states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3272.812,
    "end": 3275.453,
    "text": "And so that can even simplify some optimizations further.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3279.996,
    "end": 3283.157,
    "text": "Equation six kind of rhymes with equation 2.5.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3283.617,
    "end": 3289.14,
    "text": "The partitionings are different.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3290.421,
    "end": 3295.104,
    "text": " but you'll find that the operations which are used are analogous.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3295.344,
    "end": 3298.245,
    "text": "We see expectations over belief distributions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3300.367,
    "end": 3305.71,
    "text": "We see KL divergences and we see entropy terms.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3310.252,
    "end": 3316.256,
    "text": "We also notice that the function or the functional G is over policies",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3317.523,
    "end": 3329.806,
    "text": " because the arguments that are being taken into account for expected free energy have to be the policies and beliefs about those policies, like their consequence on sensory outcomes.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3331.586,
    "end": 3336.367,
    "text": "And that's why almost everywhere you see it's conditioning on policy.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3338.808,
    "end": 3340.888,
    "text": "So it's like, there's four options.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3341.969,
    "end": 3343.049,
    "text": "We're going to calculate",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3344.359,
    "end": 3346.621,
    "text": " a few things and this will return to it.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3349.002,
    "end": 3367.334,
    "text": "But one of the key pieces is that pragmatic value, like kind of the imperative of pragmatism is not the maximization of a reward, which is common in reward maximization and reinforcement learning.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3368.288,
    "end": 3377.195,
    "text": " but rather it's going to be about the reduction of difference between preferences and observations.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3382.92,
    "end": 3390.206,
    "text": "All right, well, thanks for this meeting.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3395.15,
    "end": 3397.032,
    "text": "That was the end of chapter two.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3398.43,
    "end": 3402.491,
    "text": " In the next two weeks, we're going to be heading into chapter three.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3403.851,
    "end": 3407.432,
    "text": "That's going to be the high road to active inference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3408.712,
    "end": 3413.473,
    "text": "So chapter two started with like the Bayes theorem and the frog jumping out of the hand.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3414.773,
    "end": 3424.814,
    "text": "Here, we're going to introduce Markov blanket formalism and talk about some imperatives of surprise minimization and the principle of least action and a few other things.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3425.535,
    "end": 3426.815,
    "text": "So thank you everybody.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3427.375,
    "end": 3427.735,
    "text": "See you soon.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3429.022,
    "end": 3429.615,
    "text": " Thank you, Daniel.",
    "speaker": "SPEAKER_02"
  }
]
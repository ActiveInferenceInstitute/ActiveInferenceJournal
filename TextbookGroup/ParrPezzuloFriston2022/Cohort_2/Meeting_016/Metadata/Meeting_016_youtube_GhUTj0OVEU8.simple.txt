SPEAKER_00:
hello it's february 15th 2023 we're in cohort 2 of the active textbook we are in our first discussion on chapter 7. so let's just first look at how chapter 7 is anticipated in the text

then any of you are welcome to give some thoughts or just raise any thoughts or questions on chapter seven and then that will go wherever it goes we can also look at previously submitted questions as well as walk through the text and get a big picture of what's happening so the first mention of chapter seven is in chapter one when they're summarizing chapter seven

and they describe that in chapter seven they're going to discuss active inference models addressing problems formulated in discrete times as hidden Markov models and partially observable Markov decision processes and they mention the examples they're going to use which is a left or right at a T maze and they also bring in topics information seeking learning novelty seeking they talk about how preference

represented by the c parameter is going to be unpacked in chapter seven in chapter four they talk about how message passing will be unpacked further in chapter seven in chapter five

They talk about E and policy.

So as they're mentioning a bunch of the parameters, E, pi, C, message passing, they're describing that they're gonna come back to it in chapter seven.

If we want to change the way the signal is interpreted, we need to rely on learning.

We will return to this in detail in chapter seven.

In chapter seven, we will discuss a biologically plausible example of factorization.

specifically the what and the where streams and in chapter six in the recipe they talk about the updating in the form of the updating so several more mentions of chapter seven in chapter six and then we actually get to chapter seven itself all right so

anyone just to begin with what were your thoughts or feelings on reading chapter 7 or what came to mind as you read or remembered reading chapter 7 active inference in discrete time just unmute and go for it


SPEAKER_02:
Well, I didn't read it completely, but there's a number of questions that popped up in, for instance, in figure 7.3.

So the A matrix is conditioned not only on one state, but a couple of states.

Is that correct?

So the sequence of preceding states plays a role.

That's different to hidden Markov model, right?

And then so this would be one question, how this could be.

And then another question is whether this pi actually makes this whole automaton more powerful, or is this just a convenient way of representing it?

So could this pi actually be merged into the state as an additional factor or something?

So multiply the number of states and, or does it really increase the computational power of that whole automaton?

Great questions.


SPEAKER_01:
right so yeah yeah jacob go for it first um i think that the in terms of putting the pie into the states themselves um i

I don't think I've seen that ever being done in an actual model.

And I think this is more of...

a representation rather than really how the model would be implemented, because the pie is just is just the policy.

And it's I think it's it like branches out to the to the two bees from like a single node only to represent that each bee is also conditioned on the policy, but not necessarily that


SPEAKER_00:
it's like a one that that it happens within like one time snapshot yes yeah in in some other work jacob and i have talked a lot about like why is pi shown to intervene twice and even more generally why is it shown with three time steps yes it helps us understand that it's like past present and future

but the action perception loop also could be shown with just the transition between two steps and pi intervening once whereas this is a little bit ambiguous whether it's a policy of length two intervening at this time point or whether it's just showing the continual unfolding of policy

interleaving between the hidden states in a sort of open-ended way um but what what do you mean michael by increasing the computational power um i mean it it would be possible to have the

dynamics of the hidden states through B be complex but by removing pi and the action selection apparatus then if we only had the rake down below it would just be a passive inference algorithm with a potentially complex endogenous dynamic


SPEAKER_02:
Well, I had to, can I give an example?

Yeah, yeah.

So I was thinking of, so is that some kind of recursion possible?

For instance, I was thinking of the following example.

Assume that I want to go and buy a bread, and then I go out, and then on my way to the bakery, I see a haircut, and I think, oh, before buying this bread, I could go to the haircutter.

And then crossing the street, I see a shop with journals.

And then I think, oh, when I sit there with a haircut, I should maybe buy a journal to read something.

Then I go to that journal.

I see a friend.

And the friend tells me, oh, I have a new car.

You want to come to see my car?

And so I pass the half of the day going forward.

And then eventually, I backtrack, coming back from the car, going

by going to the hair cutter, buying the bread, and in the evening I go home.

So this is a kind of a recursive thing, right?

So there is an action, and then there is an intervention with another action.

So it's a kind of a recursive thing.

And each of those actions maybe follows a policy, but that can be...

There can be another policy that intervenes, but then I backtrack from there.

So that would actually be, yes, a recursive kind of a policy.

Is something like this possible?

So this would really, I guess, increase the computational power

with respect to just completing one policy or one sequence of actions and then going to the next where one iterates the stuff instead of the possibility to do that recursively.

So something like this I was thinking.

So is it possible to have some recursive policies there?


SPEAKER_00:
One quick note is to Livestream 42, which was on robotics and the SLAM, simultaneous localization and mapping, there's a higher level policy selection around what location the robot wants to go, and then a lower level tactical level that is about the actual motor actuation.

So in that example that you gave,

There might be like a higher level model of which location you want to go towards.

And then there's a lower level footstep model.

And then this shifting of decision could be modeled as policies on which location one is seeking.

So as with many features, it's not necessarily intrinsic to this figure 7.3.

as the kernel but you can compose it in order to capture some of those dynamics ali and then yakup


SPEAKER_03:
Yeah, well, actually, I think described in those terms, I mean, recursively updating the generative model is just for the can just be for the purpose of building a comprehensive generative model.

I mean, as comprehensively as we can.

But when we include the policies

as the additional parameter in our generative model.

Actually, this is a kind of a technique used for bringing the elements of action selection to or within the Bayesian belief updating, which is called the planning as inference.

So in order to do that,

policy selection or, I mean, constructing a space of all possible actions is a necessity, a requirement to do these kinds of planning as inference.

But yeah, it's just a kind of mathematical trick, if you will, to somehow bring together

the action selection into the mechanism of Bayesian belief updating.


SPEAKER_02:
But this possibility... Oh, sorry.

Continue.

Yeah, so this possibility is not in picture 7-3, right?

So because these arrows go from pi to the B matrix, but there's not a kind of an arrow going back.

So are you saying the trick would be that one learns a pi inside a pi somehow?


SPEAKER_03:
Yeah, you see, this is just one policy, a picture, a snapshot of a one policy.

But in more comprehensive models, we have the state of... I mean, we have policies and not just one single policy to select from.

So, yeah, that picture just shows...

how choosing a single policy can affect the generative model and how it can do the inference via planning or action selection.

But in reality, when dealing with more comprehensive models,

we don't necessarily have just one single policy to select from.

I mean, it would be tautological to have just one single policy, so we need a space of possibility.


SPEAKER_00:
If there was just one policy and there was no agency around selecting it,

then it reverts to a passive inference challenge whereas the structure of the b matrix which you can explore from a hands-on perspective like by looking at the dimensionality of it in pi mdp the inference problem is set up such that agents are deciding between pi 1 and pi 2 which are evaluated according to their expected free energy and pi 1 and pi 2 do different things

to be so we can think of like a b1 and b2 not time point one and two but variant or slice one and two that change how hidden states change their time so jacob um


SPEAKER_01:
I guess on the question of recursion and with your example of going to a barber and then seeing, oh, I want to buy a magazine to read and the policy changing throughout.

I guess I'm not really sure I see the recursion there in an active inference model because I think it's more, I'd say it's more like a very high dimensional model where you are perhaps performing

uh where you're always like narrowing down your policy search space um and with new observations you're like adjusting that search space based on your preferences in that particular uh observational modality like you get reminded that there are magazines and you can buy magazines to read

while you're at another place, so it kind of extends that computation.

But I don't think it's necessarily recursive.

And also, I think the figure... I think you mentioned something about prediction errors or something like that.

I don't think that's necessarily encapsulated in the figure because that only shows...

Well, I guess it's not explicit, but we can assume that the B there is static and you can imagine a lot of other things happening if you would be learning the B matrix or doing model selection as well and adjusting your policy based on what observational modality has the highest precision.


SPEAKER_02:
And can I answer to this?

So I think, well, maybe you can represent this in a way of an iteration or the way you say.

But it would be recursive in my understanding of these terms if going, for instance, to the baker is a policy, means it's a series of actions.

Right?

And so if this is a policy, and then in the middle of this, there is somehow another policy that pops up.

Oh, I go first to the hair cutter or to the barber or whatever.

And then this one policy still exists, but inside there is another kind of sequence of actions, another plan, and so on.

popping up, and if this could happen recursive.

I mean, in my understanding of these terms, it would be kind of recursive.


SPEAKER_00:
Yeah, it can be through nested modeling, which is one canonical way that recursion is implemented.

So it's unfolding sequentially through time.

but also like this higher level discrete time.

So that's again, we're chapter seven, we're in discrete time.

And so this might be a slower click.

So maybe the footsteps are taking place on a second to second policy tick timeline, whereas a policy in which location one wants to go is being evaluated at a slower level.

And if this nested level is playing out at a similar frequency to the lower level, then as a modeler, you might be better off just having it as a single level model.

And as an organism, you'd get into these kind of like paralysis cases where if you were actually switching policies about where you want to go,

then this would be incoherent down in the lower level.

So just to return to the initial question, is it possible to merge policy into the hidden states as a causal latent factor?

Well, in a way, policy intervenes in how hidden states change through time.

So policy does influence hidden states.

Without an action selection apparatus, it's not active inference.

It's not even a control theory problem.

It's just a question of identifying latent causes from the unfolding of a hidden states and observables that you can't control.

And that's a classical SPM type

challenge like S might be the latent unobserved neural activity and O might be multi-sensor fusion with fMRI EEG and MEG so this can still be very complex in its unfolding through time with the observations in the hidden states what would make it an active inference problem well if you weren't just observing sensor modalities but if the experimenter could take a policy

that was intervening how neural dynamics change through time, which is what experimental design is, which is why it's so compatible with thinking about the meta-Bayesian approach with a behavioral researcher.

So Pi influences hidden states, but if we merely folded into B,

we would lose the ability to do actual action selection.

But if it were a passive inference problem, we still could use variational inference.

We still could use free energy.

We still could even align outcomes with our preferences.

We could still calculate and bound surprise.

The active embedding of active inference

is to include action as a parameter, like Ali mentioned.

Within the Bayesian brain context, to have a parameter that you do Bayesian inference on, and then extend that not just from perception and causal inference, which is what's happening on the bottom, like perception is the A. Causal inference can be understood as B, but action selection does require an additional apparatus.

Passive inference, more like signal processing.

Action, more like control theory.

Unified imperative.


SPEAKER_02:
So the G would then be predicting the future, expectations, developing of expectations, and so on.

What would it be?

What is G?

The G, the pi.


SPEAKER_00:
Oh, sorry.

Well, pi is the combinatorics of affordances over a given time horizon.

So merely generating pi as a list is not intrinsically difficult.

It's just the search space of possible action sequences over a given time horizon.

If there's two options and you're doing a time horizon of three, then it's two cubed.

G is the expected free energy.

which updates the policy prior.

So take that list and it's your prior probability of those policies, which could be either just a uniform prior or could reflect habit, like a priori, how likely are policies.

And then G sharpens or updates that policy vector into a policy posterior,

which then the agent samples their actions from.


SPEAKER_02:
And how is this meant?

So assume that I'm... So you're saying the first is the past, and then... So is this...

Is this re-estimated at every state?

So this pi would somehow have expectations over future affordances, as you say, right?

Would say which transitions are more likely to the right of this graph?

And would that be re-estimated at each time step?


SPEAKER_00:
Or how would that... Yeah, there's multiple possible levels.

Ali, go first, then I'll give a thought.


SPEAKER_03:
Well, basically, pi actually is just an additional parameter multiplied to...

the basic Bayes theorem formula we had before.

So instead of just multiplying likelihood and the priors simply, we also multiply the probability of selecting a policy and that's just it.

So pi is just an additional parameter in that equation from back in chapter two.


SPEAKER_00:
yeah equation 2.6 is where g is defined so g is a a function of policies so it takes in as arguments the vector of policies and then conditioning as we see in every single of these rephrasing every single one is conditioned upon a given policy

And then different policies are compared by their expected free energy.

And then depending on the precision on the action posterior, which isn't shown in 7.3, you can have a spectrum of behavior.

If you have extremely high precision, like it could be thought of as low temperature, in that case,

one policy that's um let's just say one is um it's one percent better than the other then under the low temperature asymptote the high precision asymptote the policy that's even tiny tiny bit better will always be selected

under the neutral temperature zone policies are selected based upon their posterior likelihood so if one of them is 51 and the other is 49 when you soft max it then it's selected 51 and 49 of the time respectively and then in the high temperature or low precision asymptote also known as shaky hand

the differences amongst policies in their expected free energy basically get erased.

And at that point, policies are selected essentially without respect to their posterior likelihood.

this is one of the key pieces though because again we're not calculating different policies and asking well how rewarding are they how much expected reward do they have and then ranking or selecting based on that rather there's a unified imperative expected free energy which under different decompositions can be shown to have these different interpretations

that is then used to generate a policy posterior and then as just described that policy posterior in the context of a of a temperature or precision variable can exhibit behaviors ranging from always selecting the slightly preferred policy in the low temperature limit neutral temperature selecting according to the posterior high temperature erasing differences in the posterior


SPEAKER_03:
Yeah, sorry, I just wanted to point out that this G matrix here is still prior to learning.

So if we want to add learning to the policies, things would get a little bit more, not to say complicated, but more extended equations would be needed.

So we haven't included learning

in this specific formulation yet, it's prior to learning.


SPEAKER_00:
Yes, thanks.

I also want to highlight that in the category of phenomena that we can model but are not just simply in this image,

you mentioned like learning new policies.

So it's like in the fixed, fixed, fixed case, you're still able to do action selection, but you're not learning or changing which policies you can enact

you're not changing what affordances you believe you have, nor are you updating B, which is to say you're not updating your beliefs about how policies influence hidden state changes.

The next level of introduction of novelty is learning on B, where you can learn about the different consequences of policy.

And then another level, which is starting to approach into like the structural learning, because you're actually changing not just parameters within the model, but dimensionality of parameters within the model is like, well, what if I take a policy that opens up another set of possible policies?

So this is like, see the cat run of sentences.

And to introduce recursive or referential clauses or learning, it's a more complex sentence, so to speak.

But this is the archetype that sets the stage for what cognitive model structures could exhibit that kind of a phenomena.

Michael?


SPEAKER_02:
Yeah, so a policy would be a number of actions, would be a sequence of states.

Am I understanding correct?

And if this is the case, then a policy would select a number of states.

And my question, I guess, is so then I come at the end, or at what point are these policies then calculated?

So I come to the end of an action, and then only is the new policy computer selected, or is it constantly the case?

So if I decide to buy a bread, for instance, and then I go out and I cross the street and do this and that, so there is a couple of actions involved.

And so my decision to buy a bread was done before.

But on the way, is that policy recalculated?

And maybe on the way, I think, oh,

something else is more reasonable to do.


SPEAKER_00:
Okay, so again, that's a slightly nested scenario with multiple levels of policy that you're looking at.

And then just to be clear, so policy, it's a sequence of actions.

So let's just say that the affordances, there's two affordances, up or down.

And then time horizon is three.

Well, let's just say it's two.

There are four policies, UU, UD, DU, DD.

So generating the state space of policies is trivial.

It's just all of the strings of affordances.

Now you ask, where does policy inference happen?

Where and how does it happen?

And there's a few different answers.

So first off, here's from our active inference and modeling conflict paper.

Here, we chose to show only a two-step model.

So instead of the three, because we were anticipating that

um ambiguity where pi is intervening twice when actually control is only exerted at any given moment but it's it's anticipated um and then we map this onto the uda observe orient decide act this isn't a formal mapping per se but it it helps understand sequentially for example in a reactive mass message passing paradigm

what different informally categorized states of the perception cognition action cycle how that maps onto this bayesian graph so since states come in cognition on occurs or one could also take a broader perspective then but this updates one's beliefs about the world then there's an inference on action

So here are the antecedents of policy selection, which are E, what can be done, the affordances.

C, what is preferred, the preferences, preferred slash expected.

And then with C and E, as it shows here, G is a function of policies conditioned upon C and E.

with those antecedents taken care of we can update our beliefs on pi to move from a policy prior to a policy posterior those policy posteriors can then be sampled from as discussed earlier at high low or neutral temperature and then the selected policy

intervenes and ends up choosing a slice in B. And then the slice of B that's chosen influences how the hidden states change.

For example, if there's two policies on a one time set basis, so same as the affordances in this case, turn on the heater or do not turn on the heater.

So maybe if we don't turn on the heater, then the B matrix just passes the hidden state forward.

the temperature doesn't change.

If we do turn on the heater, the temperature increases immediately.

Again, simple example.

So we looked at the thermometer, O, updated our belief given our priors about the temperature of the room.

Then with respect to what we can do, E, and what we expect slash prefer, C, we updated our policy prior into a policy posterior.

We then selected the action that's most likely, not the most rewarding action, the most likely action, and that influenced how the hidden states change through time.

And then the OODA cycle begins again.

So action intervenes in the discrete time formalization, which is where we're in chapter seven,

It intervenes in every time click.


SPEAKER_02:
Okay, thank you so much.


SPEAKER_00:
And also, The Pragmatic Turn is a book co-edited by Friston, but it discusses what has happened in neuroscience with people embedding action and thinking about action in neuroscience.

And that's relatively recent, which is really exciting.

Okay.

But these are, these are great questions.

And then let's just also go to the second question that you raised there, and then we'll scan through all of chapter seven.

So is the A matrix in figure 7.3 conditioned on one state or a series of states?

So yes.

Jakob feel free to add any more details on this one but as it's shown here from a base graph perspective it is O is only conditioned on S but when we think about across the vector of observations they can be understood as being conditioned upon the time series of hidden states

so each observation is only conditioned upon its associated hidden state that's the partially observable and hidden states pass through the markov needle of the presence that's the markovian property the past only influences the future through the present which isn't to say that's how reality is that just to say it's a simplifying structural assumption that one could vary if they chose

but it is a little ambiguous.

I can see why somebody might think that observation at a given T, let's say T plus one, is a function that's conditioned upon all the previous times, but not directly.

Observations are like cul-de-sacs.

And again, A is the tail of two densities.

the recognition density which is from observations updating beliefs about hidden states bayesian recognition model and the generative density which is from a hidden state generating pseudo observations and the way that those two densities can be alternated

is used and has been used for decades in expectation maximization algorithms okay cool great questions here um does anyone want to go to any of the chapter seven questions or should we scan through in the last 15 minutes here just like what happens in chapter seven

all right let's scan raise your hands when you want and i'll immediately go to you if you want to um just add a note on like as we scan through so we will scan through the whole thing in the next 10 minutes okay starting with the quote what i cannot create i do not understand famous quote take it or leave it but now we're into the building the recipe has been disclosed in chapter six

prerequisite knowledge has been distributed in chapters one through five ideally they start with perceptual processing this is very much like the step-by-step paper step by step starts even simpler with starting with just a static inference

but here they go immediately to dynamic perceptual inference this is just the bottom of the rake this is a passive inference but it is changing through time they're going to be using a music example speaking to their target audience which is ali um a musical example

so we're listening to some music not necessarily playing the music but the outcome is the note that's actually intended i'm sorry the outcome is what is actually played and the hidden state is the intended note and just from looking at this a matrix we can see that like on the diagonals if it was the identity matrix

that would mean that there was a perfect mapping between hidden state and observations but this is not an identity matrix it has off diagonal values which is to say it's like a blurry a matrix and in the limit of a blurry a an ambiguous a you can imagine if it was all 2.5s across then there would be no information on hidden state given the outcome

The other extreme would be perfect information with the identity matrix.

And you can see that it's also rescaled.

So the numbers can be shown kind of simply as integers and then it's rescaled so that it's a proper probability.

So 70% of the time, the musician hits their intended note.

Row one, column one, seven.

B is the probability of transitions.

So we go from note one to note two with 97.

So notice the diagonal.

If that was the identity matrix on B, if that was all B was, you would basically dwell wherever you began.

If B is all off diagonal, you never stay in the same spot twice.

And so here is a sequence.

of notes because one almost always moves to two two almost always moves to three three almost always moves to four and four almost always moves to one then d is the prior and so it's just being taken as a noiseless estimate that the first note is the beginning note michael


SPEAKER_02:
So is this a very boring tune that plays four notes in the round eternally?


SPEAKER_00:
Exactly.

It's like G-E-B in Eternal Golden Braid, except it's like G-E-B-X.

G-E-B-X.

Okay, very boring.

Yes, it is a simple one.

so 7.1 to 7.3 equations completely specify the hmm note that it's a hidden markov model because it has this partially observable character but it's not a decision process because there's no action selection but all we needed to do was to specify a b and d so to totally specify the hmm was a b d we didn't have to say which notes we preferred we didn't have to say anything about what we could do with c or e okay

then there are some simulations shown um there's a few things about this simulation first as the earlier question has raised why are there continuous interpolations when we're in the discrete time world a general comment is these graphs with multiple black lines are a little hard to interpret because of course we can't tell which line is which

another issue with 7.2 figures specifically is the bottom left quadrant is empty so just a few points but what's basically happening is in this simulation run which would only happen this way um several percent of the time but they're showing it um basically um in the upper right um

the retrospective beliefs are shown so the retrospective beliefs are shown with a correct song even though the actual observations were note one two two four so this observation was recognized and it did increase the posterior on note three being played

However, because the belief was so strong that note three would be played in this time, it did not move the needle far enough to actually hear that note.

So that's like when somebody hears something that isn't said because it's likely that it was said.

Okay, so that's a fun model, but it's not action inference yet.

So that's where we get to 7.3, where we've been spending a lot of today's discussion.

7.3 discusses the addition of an extra variable on which transition probabilities are conditioned.

So it's like really, when we think B, we should really think B vertical line pi, because the inference on action means that we have actual options for action, which is to say there are multiple Bs that can influence how S unfolds,

So we're really talking about B's condition on policy, unless we're talking about like the whole B caterpillar, the whole tensor of B. Okay.

Here's where expected free energy and one of the partitionings is shown.

which is discussed in chapter two equation 2.6 and in chapter four so next week when we're in chapter seven for the second time let's can let's return talk a little bit more about the technical parts of like what exactly equation 7.4 means um it's a decomposition that enables epistemic drive towards information gathering that's this first part of the right hand side and the pragmatic drive

which is about realizing prior beliefs, reducing surprise about prior beliefs, not maximizing a reward function, reducing diversions between preferences and outcomes.

They're going to unpack the full 7.3 figure in the context of a teammates, which is a rat that's able to go left or right to find food or not.

or it's able to get a q that's the q location r there's four locations that it can be in and there's modeler degrees of freedom and how different things are represented so this isn't even the only way to represent this toy example but this matrix does a really good job of showing like

what are the states what are the dimensionality of the difference variables which we can come to next week so here's context two here's context one here we have black on the right side white on the left here we have black on the left white on the right and there's some subtle differences which you can note like specifically with lines two and three here's a one here

Here's a one on the third line because the cue is suggesting that it's on the right.

Here, the cue is showing it's on the left.

And in model stream 7.2, that example is like very hands-on and in some ways even better than this one.

And it includes things like learning, but it's a very similar task.

Here is B2.

so they're showing the dimensionality of what those B slices look like for different policies this is representing the B tensor here are preferences

This is what a preference vector looks like.

And in step-by-step, the paper and Model Stream 1 series and in Model Stream 7 series, there's some discussion on what does C mean?

Is it being reflected as log probabilities?

Why would one do 6 and negative 6 versus 6,000 and negative 6,000 or 0.6 and negative 0.6?

What do those values actually mean?

and the priors provided.

So now that we've provided a controllable B, which is the manifestation of policy options, and provided a C, we can do active inference because we have now

we need those two pieces really that's what brought us from the hmm to the pomdp was what we can do and what we prefer um section 7.4 goes into information seeking it unpacks the epistemic value into a few more sub decompositions and it talks about how that plays out there's some nuance here again there's also some semi-confusing black and white graphs

but other ones are clear.

We can talk about how epistemic value is being realized in this example.

Box one is a bit of a side note on uncertainty and precision.

and here's showing really the temperature or the precision highest temperature lowest entropy most uniform distribution lowest temperature highest entropy most specific distribution here you're sharpening here you're blurring okay 7.9 as mentioned um two pages earlier is an isocate so um

There are four locations and the I's are circading.

Again, one could ask about where are these continuous interpolations coming from since we're in chapter seven in a fully discrete situation, but it's all good.

So I circading different sequences or traces of I circades across four locations driven by epistemic value.

in section 7.5 learning and novelty is introduced so as we were mentioning earlier learning with respect to the a or the b matrix so far we've been talking about them as fixed

here is where learning happens on a and b and just like you would do with any bayesian inference whether on action or on the consequences of action or on perception learning is modeled as a hyper prior or a variable that governs a prior on another parameter so here is like a parameter um that is adding a layer

to the rake we still see the core figure 4.3 figure 7.3 motif but there are these new little edges that are the learning edges so it's the same POMDP as figure 7.3

But the priors for each hidden states now depend on the variables which themselves come with prior beliefs.

So the prior on the variables A and B is in the Bayesian paradigm how we learn or update our values on A and B. Brief discussion on conjugate priors.

Some more equations that we can come to next week about updating and learning on A.

another look in equation 711 on g which is very similar to what we saw in equation 2.6 different partitionings on expected free energy policy then there's a final example that's now in a maze and so this is some maze seeking entity we can come to this next week

in hierarchical inference, hierarchical or deep inference, something we kind of mentioned today, but we'll look more at next week, is first there's a box on structure learning.

which as mentioned earlier is when we think about regular non-structure learning as like fine-tuning quantitative parameters within a given dimensionally defined generative model structure learning and what you can do with structure learning like specifically Bayesian model reduction is when we're actually modifying the dimensionality of the generative model that's in box 7.3

Here is a nested generative model, which is almost exactly the form that we looked at from live stream 42, where we see like the rake, classical, figure 4.3, figure 7.3 paradigm, and then there's like a meta rake.

where here's another E, downward facing, and we have policy at a higher level.

And different generative models have taken different interpretations on what the lower and the higher level are.

Like in Livestream 42, the lower level was tactical movement of a robot, and the higher level was strategic location preference.

Whereas in Livestream 28, Sanved Smith et al., the higher levels were associated with attention, and the lower levels with perception.

Another example with decision-making we'll come to, and then the chapter summarizes.

So thank you for this session.

We'll stop recording and take a break before the next one.
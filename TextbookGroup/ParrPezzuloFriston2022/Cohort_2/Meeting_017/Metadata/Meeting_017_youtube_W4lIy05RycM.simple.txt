SPEAKER_00:
hello it's february 22nd 2023 2 2 2 2 3 and we're in the second discussion of chapter 7 in cohort 2. before we go to the questions of chapter 7 and then at the end a look ahead to chapter 8

we are going to have thomas parr joined for a live stream in five days and so anybody if they would like to raise an area of questions could be conceptual

Or it could just be a general textbook level or personal question.

So of course, feel free to add anything, but just while we're here, any thought or like kind of general piece that people are thinking that Thomas could reduce our uncertainty about?

All right, just, of course, add it when you see fit.

Okay, there were several questions that we've already looked at from 7 and several more.

We can also look a little bit more.

Just again, we looked at the first question,

sections of the chapter itself so does anyone have a chapter seven question that they want to go to or add or do they prefer to walk through the sections in the textbook

let's look at equation 7.4 see if we have anything annotated for it okay we don't have the natural language yet so let's just look a little let's pick back up

with the rat in the maze example go through it connect it to the equations and then return to some to several of the questions down here in seven so in the earliest part of chapter seven was a passive inference with the example of listening to music

now the upstairs action selection part is going to come into play the imperative for action selection is going to be G G like we saw in chapter 5 takes the action prior the prior on different policies which can be understood as like habit and then

sharpens it according to just qualitatively provide epistemic and pragmatic value and quantitatively as described in equation 2.6 and 7.4 does anyone want to give a reading or a thought on 7.4 just what is it saying in and of itself

and or how does or might this relate to in their words the resolution of the exploration exploitation dilemma yeah ali go for it


SPEAKER_01:
Well, yeah, as we saw before in chapter two, I guess, but I think it was in a more comprehensive terms.

But here we only have a part of that equation, which basically deals with the tradeoff between negative, I mean, epistemic value and the pragmatic value.

and how they relate to each other because as we saw in chapter two the trade-off or the dilemma between exploration and exploitation is basically one of the fundamental requirements for any sentient behavior for any sentient agent because as long as

the agent wants to explore the environment in order to informally discover new

let's say opportunities or new resources, be it, I don't know, food resource or any other resource necessary for its survival in the environment, it needs to somehow have a trade-off between this kind of having the pragmatic value on one hand, namely to survive in the environment, and on the other hand, to

to discover new resources for its survival, which basically relates to the pragmatic value.

Either of those two requirements, if taken to its extreme far, would probably result in the demise of the agent.

Take, for instance, an agent residing in an environment.

for a certain period of time, it can survive in that environment using the pre-known resources there.

But when the resources is about to diminish, then

that agent needs to explore a little bit further in order to discover some new resources and continue its persistence through time or its survival.


SPEAKER_00:
Yeah, great.

Yes, Michael?


SPEAKER_02:
Yeah, so I'm a little bit wondering, in these epistemic considerations, only things can be considered that the agent actually already knows about, right?

And then I'm...

wondering why at several places, I think I said this in chapter 2 and here in this chapter 2, also 7.2, also that one would like to find the most unlikely path, or one would like to increase the information value.

That's what I understood.

So the path that should be searched for would be the path that maximizes information, which means it's the most uncertain path.

And I'm incorrect?


SPEAKER_00:
there's aspects that I think are correct, but I just want to draw that out.

So actually what is being selected for policy,

are the most likely paths what is an agent like me most likely to do we're sampling our actions from the action posterior so rather than scaling policies by their reward and ranking them by their reward in reward learning and choosing sampling from the reward posterior for example we're going to be sampling from an action posterior so

In the sense that our actions are selected based upon their posterior likelihood, we're actually pursuing the most likely path of action, indeed the path of least action.

However, we are seeking in the epistemic component, especially maximally informative sequences of future observations.

So in the special case where pragmatic value doesn't matter,

Like we have no preference over outcomes.

We don't care if our bank account goes up or down.

Then we implement a pure InfoMax novelty search.

In a situation that's totally known, there's no epistemic value to be obtained, we have a special case of pure utility pragmatic value.

However, in cases where both utility pragmatic and epistemic are on the table, G is proposed to resolve this dilemma by adjusting the action prior into an action posterior that respects the importance of both of those imperatives.

That's the unified imperative of action and inference.


SPEAKER_02:
Yeah, so I'm not sure how I should understand this.

So for instance, I want to go to the baker and buy some bread.

And then the most informative, I mean, the more unlikely thing is actually that I go to the barber and cut my hairs.

So I would, because that has more information and going to the baker and

considering what kind of bread I'm going to buy, I go to the barber because that is more informative.

I'm not sure I understand how I should understand this.


SPEAKER_00:
These values are calculated about generative models, not about territories.

So again, it's natural and intuitive to want to be like,

directly mapping from an equation or calculation to a real world territory situation but keep in mind these are um ultimately numerical values calculated for generative model it doesn't make sense to talk about free energy being minimized like for example of a person doing translation simply or of an ant simply foraging it's of the generative model that these values are being calculated on

And so if one proposes a generative model of location behavior, so kind of in a previous discussion, we had like a higher order, like what location am I going to?

Barber, home, baker, you know?

And then we have a lower level model with our footsteps.

So that's kind of analogous to the live stream 42 slam robotics example.

Then in that situation,

one could ask how different policy selections were being influenced by their respective epistemic or pragmatic value but it is not the case that we can directly draw from a variable to the territory these are like summary variables or re-weightings on our map and it only makes sense to talk about these summaries on maps not territories ali


SPEAKER_01:
Actually, Michael Levin has an excellent paper from 2019, The Computational Boundary of a Self, in which he has an interesting diagram showing the cognitive sphere or cognitive light cones, if you will, of each cognitive or intelligent agent in which

uh it can have an effect and uh it's um of uh more or less uh of its concern so uh it also integrates the yeah that's it so uh so for example um for humans uh because of uh well or highly intelligent

species or agent because of the complexity of its goal-directed activity and agency it results in a kind of larger cognitive boundaries

And for somewhat, let's say, quote, unquote, simpler agents, that would result in a much smaller light cone area of cognitive boundaries.

So yeah, depending on the complexity of the tasks that the agent is capable of doing, that territory of

its cognitive effectivity would vary according to the complexity of the thesis.


SPEAKER_00:
Yeah.

So, these are, G is a functional that is going to sharpen our action prior according to such and such.

If we just wanted to pass through habit,

we would just pass through habit.

And in chapter five, in the nervous system example,

this is what's reflected in the bottom here with this dopaminergic circuitry where on the left side of this basal ganglia or whatever mammalian tissue it happens to be on the left side there's kind of like a habit pass through like e the prior on affordance is basically passed through

Whereas in G, I'm sorry, on the right side, G is sharpening the policy distribution.

So if we just wanna pass through habit, we don't even need to apply G. However, if we want to sharpen,

our action selection the moment to respect expected free energy in a prospective planning like fashion then we want to calculate um g so what is being calculated with g again this is an equation 2.6 but also here we're taking another look at it so there's two terms looking first at the pragmatic value

And the special cases were already mentioned and elsewhere about when one of these or the other is zero.

Okay, so fancy E, it's an expectation regarding our Q variational distribution that we control of O tilde observation sequence conditioned upon policy.

So we're going to see that both terms are conditioned upon policy.

So if there's only two affordances, then we're only going to compute two different Gs.

So this is how we kind of prevent like spiraling and computing like all possible world futures.

We're conditioning on action.

And we're interested in, we're reducing our uncertainty about which actions to take.

So that's what this first part is saying.

So expectation of the variational distribution,

of upcoming observation sequences conditioned upon actions policies that we can take and it is going to be um a natural log to help facilitate just all kinds of things that logs facilitate but it's it's a monotonic relationship with

what is being described here.

And this is about observations conditioned on preferences, C. So where observations are close to preferences, pragmatic value is high.

Where our observations are different than our preferences, pragmatic value is low.

Rather than associating pragmatic value with a reward function

we're associating it with bounding our surprise, natural log, self-information, surprisal, of our preference slash expectation.

The epistemic value component, which can be kind of seen as like a sub-function with like a negative i, but it's interesting that also this is kind of a surprisal-like

um like even pragmatic value enters into an information theoretic surprise like formulation so even our pragmatic value is still informational it's not you know tokens

here we see again an expectation over our variational distribution but here rather than on the sequence of observations through time o tilde it's the sequence of hidden states through time so this is um now looking at underlying latent states conditioned upon policy and there's two there's a difference of two entropies

then here's p which is like the the kind of actual distribution and then q the distribution we control now within the parentheses o tilde in both case o tilde conditioned upon states minus o tilde conditioned upon policies so there's definitely a few ways to unpack it and i think we've we've um explored some but um

broadly if the policy is not further reducing your uncertainty about this relationship the epistemic value is low if the policy is reducing your uncertainty about s via o then there's epistemic value

I think there's like a lot of open questions that are pretty contextual.

Like how do these implicit or tacit understandings, like how does the rat know that going to that cue location is going to reduce its uncertainty?

How does it know the semantics of the left versus the right cue?

How does it know what, you know, this is not the whole story.

but within the context of doing variational inference on a structurally determined model this is the basis of the separation between pragmatic value defined as alignment between observations and preferences

and epistemic value defined as reduction of uncertainty, which is to say entropy, of how policies sharpen our understanding of the relationship between hidden states and observations.

Michael?


SPEAKER_02:
Yeah, so both of these are expectations.

So that is with respect to future things that might happen.


SPEAKER_00:
These expectations are actually used in the sense of a mean.

The time horizon is implicit within pi.

Whether that is being calculated, whether g is being calculated over pi of length 1, 2, 3, etc.

in the discrete time formalization.

But these expectations are actually like center of gravity, Laplace approximation, central limit theorem.

Think expectation of a distribution.

Even though it's about the future,

We're actually computing like the center of gravity or policies of different times, but we're not funny enough.

Like we are, but we aren't talking about future time.


SPEAKER_02:
OK, so what I maybe don't really understand here is the difference between P and Q. I was of the understanding that P is something that is present in the presence that we experience, and Q is something that happens in the future.

Now I'm a little bit confused because P and Q both relate here in this equation to the future, right?

So how can this be understood?


SPEAKER_00:
The difference between P and Q is not between the present and the future.

P is like the underlying distribution and Q is our variational approximation.

this is like the actual entropy of the relationship between observations and hidden states whereas this is the relationship of the entropy um the entropy of the variational generative models relationship between those two ali uh yeah i think uh


SPEAKER_01:
equation 2.3 might help to somehow clarify that ambiguity between P and Q, because in equation 2.3, yeah, that's it.

The KL divergence between

Q and P is defined as the expectation between the surprisal of Q and the surprisal of P. And one interesting thing about equation 7.4 is that the

epistemic value is the expectation with respect to the hidden states, but the pragmatic value is the expectation with respect to the observations.

So we need to keep both of them in mind because here, as Daniel just mentioned, P is just the

Q is the variational, I mean, free energy, but P is the standard probability function we apply to a random variable.

So that's why they have distinguished between those two functions, because Q has some

a specific, um, uh, definition.

So for example, in equation 2.2, uh, the probability of P without any, uh, yeah, that's, that's just, uh, the simple derivation of the probability of, uh,

a random variable being of a specific value is calculated without taking into account cues or other parameters.


SPEAKER_00:
Nice.

Yes, the fact that pragmatic value is associated with outcomes is what ties closely to perceptual control theory.

and the fact that epistemic value is associated with hidden states is related to the um avoidal of overfitting we're not reducing our epistemic value on observations observations are being evaluated for their pragmatic value hidden state inferences are being evaluated and and

through them policies are being evaluated for their epistemic value michael


SPEAKER_02:
Yeah, so I am a little bit confused about this.

You said that Q relates to the variational free energy.

But the variational free energy, I mean, we made this distinction between the G, which is the expected free energy, and the F, which is the variational free energy.

And now inside that,

Expected free energy is a variational free energy.


SPEAKER_00:
So, yeah, just to be clear, Q is the variational distribution.

This is the guy we control.

Q is the guy we control.

P, we don't.

Mind your P's and Q's.

Q is the one we control.

Variational free energy is a real-time functional that's a function of

our variational density queue and incoming data.

It doesn't engage with planning.

Expected free energy and a host of cousins like free energy of the expected future that Baron Milledge and others have proposed.

This is not like the only way to even do this energy-based method in the future.

this is rather than a function of taking in only q and y as arguments here the primary argument is the policy prior vector and then the computation of this functional involves q the variational distribution but like q could be a gaussian distribution

So we'd be looking to parameterize the variational parameters, you know, the median and the variance, the two parameters of a Gaussian.

And the true generative process may not be Gaussian.

But we would be looking, just like in a linear regression, we'd be looking to minimize the sum of squares with the L2 norm,

In this variational inference setting, we'd be looking to fit the best Gaussian mean and variance given the family of the variational distributions that we had chosen.


SPEAKER_01:
Ali?

Yeah, so one other angle can be E is the actual posterior probability that is calculated according to the actual states we have at our disposal.

But Q is the approximate posterior.

So it's not the exact posterior that has been calculated in P. But yeah, Q is

related to variation of free energy in the sense that it's deployed in calculating the variation of free energy because for the, in order for the free energy to be a tractable parameter to be, I mean, implementable in a computational task, we need to use this approximate variation of free energy because otherwise

we don't have either all the information we need to calculate the exact posteriors, or it's too intractable to compute.

So that's the reason behind using approximate variational free energy.


SPEAKER_00:
Great.

And I think one kind of, as a valiant, this idea of like probably approximately correct,

is kind of like when they say almost surely in math but more on the statistics side if it's probably approximately correct it's like actually within the context of approximate bays that is what is gonna be not just good enough but like as good as you can kind of aim for um okay matrices are defined

a has two slices a1 and a2 where we can think of it as two modalities of a and the dimensionality is clear in the MATLAB implementations it's clear in the pyMDP it's not exactly exactly the same syntax but like the structure of these matrices in principle are the same

a1 is describing the probabilistic mapping from location to exteroceptive cues so it's not exactly an identity matrix but basically this is the accurate gps sensor when it's at this location start location it knows exactly where it is so again you can be like well how does it get this implicit knowledge of where it is

Right, that's part of the complexity of cognitive modeling.

But suffice to say that we're in a setting where the location is known without error.

But one can imagine that there would be different outcomes and the need to do different parameterizations if there were off-diagonal elements.

A2 is the probability.

So they sum across to one.

or the i'm sorry they they call them sum to one about the the food location the the between the spatial location and then the outcome of food so semantically this represents beliefs about food location in the a here 7.5 is the same as 7.4 it's just that here it's like two in the upper left 98 on the off

here we see 98 on the upper left two on the off here we have one in the third row empty on the second here those two rows have been flipped to represent the black on the left side versus the black on the right side so the food can be either on the left or the right side the rat can either make the move to go to the left or the right side or can go to the queue if it decides to go for the queue

not the Q letter, but the C-U-E, if it decides to receive an informational Q, then it will find out where the food is.

Again, check out model stream 7.2 with the epistemic foraging and all of that sort of stuff.

These slices of B are describing movement affordances.

Because again, with respect to the hidden state being location, B describes how different policy selection bears upon hidden states changing through time.

C is describing the relative preference for different outcomes.

So the C1 vector

is about the first modality five of them that's the locations and there's a negative for the starting location so that's like a little bit of like a like a kind of get going because if it preferred to start location it might just stay there then in the second modality that's referring to this shorter vector of these one two three and that's these one two three um

in various modeling settings we've already started to see that like the c scaling matters a lot and so that's one reason why i think it's tentative or hypothetical to say that simply like equation 7.4 resolves explore exploit because it sets up the space for explore exploit to be framed absolutely it does however

depending on how the variables are parameterized you can end up with an agent who still goes for the pragmatic value every time or who still goes for the information value every time so in terms of outcomes it's not simply resolved by knowing what equation 2.5 is but rather this opens up a space of modeling in which adaptive solutions can be reached so if this was six thousand

then 0, 6,000, 0.

It's like reward is so good, it's going to be pursued.

If this was flat, 0, 0, 0, then it would be pure epistemic information.

So that's why it's so important that we develop statistical power methods and parameter sweeps across simulations because just making one T maze and saying like, oh, look, the rat does this or that.

It's kind of like, okay,

d also is described for the two modalities and um it um describes the priors on the two there's a belief about the first location i'm actually just just i'm a little bit curious why there's five elements here but only four elements here

And then in the second modality there, which again is referring, maybe this is like a minus, there's one fewer element here because there's only two here.

Here's like reflecting an equal prior over where the food is.

So a priori is 50-50 left or right.

So do you do a 50-50 bet left or right on the food?

Or do you go to get the epistemic Q

which of course reduces your uncertainty about where the food is now we're going to dive a little bit deeper into epistemic value itself so in 7.4 they're saying we're going to go ahead and call just the epistemic value decomposition term i equation 7.8

so here is just a separation of our as it was written in the prior equation that is going to be rewritten in terms of a kl divergence q s and pi q and s and pi they're the same on both sides here's p o s here's q o pi

and it's it's definitely worthwhile i think for us to write out what these verbally mean because they have very i think interesting um implications for again like what is the epistemic value and then here is um with this semicolon being noted this is reflecting a pure information gain salience bayesian surprise imperative

Here's some simulations showing at time two, it goes to get the information and then it proceeds to the left.

Ali?


SPEAKER_01:
Yeah, just about your previous remark about why C has five elements and D has four elements, because C actually,

corresponds with the elements of A matrices, because A matrices actually describe the states that the environment is dealing with, but because D corresponds with B matrix, and because B matrix is a transition matrix,

That's why we don't need to have a redundant additional element for that.

So D being priors, yeah, maps onto B matrix.


SPEAKER_00:
Thanks.

And right now, you have to specify the tensor dimensionality and add some labels, but they're developing a lot of functions in PyMDP, and people who work with it can definitely have a lot of feedback on slightly more intuitive...

and scalable ways to specify dimensionality, which is currently one of the rate limiting steps in PyMDP application.

Okay.

Just getting through seven and then just so we can look ahead to eight continuous time.

So precision, negative entropy, negentropy.

This is the high temperature and low temperature.

High temperature, differences are erased.

Low precision.

low temperature absolute zero high precision differences are accentuated that has technical usage especially in certain statistical distributions isochate paradigm par and frist in 2017.

Icicating is, while our eyes may dwell on what we may associate with pragmatically valuable, broadly speaking, icicating is driven by information gain.

So it's a great place to study how pragmatic semantics at a higher level

are in a predictive processing type relationship with isocating at a lower mechanical level with this level almost surely being driven by information gain learning and novelty here um one kind of question that was arising was like why is our dashed line between only here but apart from that um

priors are being provided on a and b so that's sort of like they can again and it's like a little bit like wise b over here so it's about the topology not the geometry in this situation what's happening is that a prior is being provided on a

so that these A and B in principle can be learnable.

How that plays out and how that influences the pseudocode of the action perception loop, PyMDP is very clear about that.

Conjugate priors, this is facilitating, amongst other abilities, the Dirichlet conjugate prior for the categorical distribution.

So if we have probability distribution over categorical outcomes, is it hot or cold?

Then the Dirichlet can be used in an earn counting way

Like every time you observe a warm day or a cold day, you just put in a ball of that color into the urn.

And then it could be two and two.

And then the next ball is very influential.

It could be a thousand and a thousand.

Then the next ball is not so influential.

And that turns out to be what's called a conjugate prior.

that enables the categorical distribution to stay as a classical probability distribution Kolomogorov's axioms and all of that while also allowing the conjugate prior to have this like counting or learning by counting strategy inferential approach to learning this is a difference with many other areas

they're proposing learning schemes that may or may not be biologically relevant.

Whereas a lot of work has gone into understanding how the perceptual or fast inference and learning or slower parameter learning processes are unified but of distinct time scales and active inference.

here in 711 equation they are going to delve more into like learning as active yes it's about policy everything's being conditioned upon policy there's a lot that one can say about the integration of action so this maze learning task is fundamentally intertwined with the action selection

The agents updating of their likelihood is related to their action selection amidst uncertainty.

They're not seen as like two separated phases.

Here's where they just drop some kind of threads and further literature, maybe further versions of the textbook are going to come into more detail.

Hierarchical deep inference, nested modeling, something that people talk and think about a lot.

But here we see the figure 4.3 or figure 7.3, the rake and the policy selection fractal.

It's a rake of rakes.

And structure learning and Bayesian model reduction.

Just some model comparison and analysis techniques that support Bayesian statistics.

Another example involving a hierarchical inference model is described and that has been used in the context of reading and other visual inference tasks.

That's chapter seven.

Let's quickly look to chapter eight.

Chapter seven was discrete time generative models.

Chapter eight is gonna be continuous time generative models.

main setting in which discrete time models have been studied is central or cognitive decision making categorical decision making in contrast the major setting in which continuous time active inference models to this time have been used in is in continuous movement or motor control tasks

So like in Livestream 46, active models do not contradict folk psychology with Alex Kiefer, Ryan Smith, and Maxwell Ramstad.

They talk about like motor AI, MAI, and decision active inference, DAI.

Here, there's going to be a proto-Bayesian physics-like model.

representation that's going to enable us to do continuous things specifically the separation of a data observation sequence in terms of an underlying function and a fluctuation stochastic term and then a latent state derivative with a little dot

change in latent states as a function of the current status of the hidden state and slowly varying causes which are going to be playing the role of policies in continuous time formalization and again partitioning out of our stochastic fluctuations so actually this part is allowed is this part is guaranteed to have zero mean and that's what facilitates like bayesian mechanics a lot of other statistics um

again just like quickly looking through it this y g x and f relationships continue to be built up to again talk and this is very spm like here's our actual sensor readers readings from the eeg fmri meg their function of neural activity neural activity is a hidden latent state it is a function of current neural activity and attention for example

That physics analogy in the continuous state space can be extended into Newtonian dynamics.

And as Dalton and Bayesian mechanics have

laid out in the last year or two, it goes way, way further than this, but this is a spring.

So this is Hooke's law as a mere active inference entity.

This is like a boring active entity that just dampens, but it's just to show that we can understand the generalized coordinates of motion

which is to say position, velocity, acceleration, snap, crackle, pop, et cetera, et cetera, et cetera, generalized coordinates of motion within this continuous time framework.

Dynamical systems often which are conditioned to be smooth and differentiable are amenable to continuous time formalizations.

In the discrete time, we'd say, well, we have one, two, and three, and then we're going to be moving to one of the locations in the grid world.

And we have a transition matrix over grid world.

In the continuous time setting, here, this is the spinal cord with the butterfly.

Here's the sensory data coming in, Y, and then a descending hidden state inference.

and uh error deviation that's being resolved through policy selection in the continuous space so this is hearkening back to chapter five and the spinal cord reflex

how continuous time generative models can be applied to continuous spinal motor reflexes as opposed to the usually more centrally associated discrete categorical decision-making, DAI.

There's a sidebar on precision attention and attenuation.

There's the discussion of Latke-Volterra, which is also called predator-prey or winnerless dynamics.

You can't win in this ecology.

There are ecologies that collapse, of course, but activity dynamics amongst brain regions have been modeled for decades as these winnerless competitions, also called neural Darwinism, and that's relating to Friston's

lineage with edelman more examples of how these kinds of winnerless competitions can be used in the context of continuous time generative models to generate smooth action sequences like this sort of gibberish cursive writing learning is um approached in a continuous model

Everything in chapter eight is kind of being contrasted or juxtaposed with what we saw in chapter seven in the discrete time.

They introduced the Lorenz attractor, which is unpacked in significantly more detail in live stream 32 on Markov blankets and stochastic chaos.

It is basically this example with the Lorenz attractor.

which is showing that generalized synchrony which isn't lockstep but rather mutual information amongst coupled dynamical systems can happen even when those coupled dynamical systems are Lorenz attractors they don't have to be like both like simple linear systems um so that's like good to know and they've used that to simulate birdsong

Where before learning, they're kind of like on and off a synchronization manifold.

And then after learning, they've sharpened their joint distribution.

Again, this begs further questions.

How do they know the semantics of the song and how did they learn the song?

But this is just an example at this level of complexity.

8.5, hybrid models.

here we have the rake within a rake but notice that the outer rake there should be a line connecting this g to pi the outer rake is d a i here we see hidden states through time t minus one now future tail of two densities observations through time classic dai chapter 7 up top but the nested model has the same structure figure 4.3 but here we see the position

velocity acceleration generalized coordinates pattern occurring so this is called a hybrid model because the discrete time model is nesting a continuous time model so this has been used to model things like where do i want to look from a categorical perspective and then here's the oculomotor saccades at a continuous model that's nested within that more equations

isochade model, more equations, summary, and key advances in continuous time models.

End of the chapter.

All right.

Thank you.

We'll take a break and then begin with the next cohort in just two minutes.
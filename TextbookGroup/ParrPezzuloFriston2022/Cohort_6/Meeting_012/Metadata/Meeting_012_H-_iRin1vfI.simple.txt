SPEAKER_01:
Andrew Tucci- All right, welcome back to cohort six.

Andrew Tucci- Andrew.


SPEAKER_03:
Andrew Tucci- Take away.

Andrew Tucci- Really quickly, can you all hear me okay.

Andrew Tucci- Perfect um yeah so everyone welcome back over at six just resuming here midway through July we're going to be starting the second half of the textbook.

First half, we were introduced to a lot of the underlying ideas and mechanics behind active inference, Bayes' role, Bayesian brain hypothesis.

hidden states being inferred from observations, action as a means of trying to change the hidden state through transitions.

A lot of really kind of foundational ideas towards understanding active inference as well as following along the low road to active inference starting from

James J Mullooly, Ph.D.

: Bayesian Canada statistics, so as the higher adaptive inference following the principle of free energy minimization, so the second half of the textbook I think is.

It's great for better understanding how to more directly apply active inference.

So chapter six, a recipe for designing active inference models right off the bat kind of gives you a sense of what the rest of the textbook is actually going to potentially be about.

We'll be learning how to design models.

Chapter seven and eight will be about putting together discrete and continuous time models respectively.

So that's great for people who want to figure out which kind of model they need for their particular use case.

It's nice that they're rather expansive chapters on both types of those models.

Chapter nine gets more into analysis.

So once you've set up an active inference simulation, it starts to discuss how you can actually interpret your results.

depending on your use case, like how you might set things up, depending on what your particular question or inquiry is whenever you're setting up the simulation.

And finally, chapter 10 is a nice wrap up that is maybe slightly out of place compared to the rest of what I said about the second half of the book, but it's good.

It's a rather holistic

kind of conclusion type chap variables wrapping up the book comparing active inference to a variety of other disciplines and fields like control theory other social sciences psychology reinforcement learning as another approach to these kinds of problems and so on so just a quick overview on chapter six again a recipe for designing active inference models the book the

then Tro to this chapter, Friston et al write, the challenge is not to emulate the brain piece by piece, but to find the generative model that describes the problem the brain's trying to solve.

the model provides it so that refers to is the model provides a complete description of a system of interest and the resulting behavior inference and neural dynamics can all be derived from a model by minimizing free energy all these ideas should be familiar from the first half of the book but it's just more clearly stating yes active inference is heavily

inherited from or derived from uh different ways of approaching computational neuroscience that said we are not trying to recreate you know um billions of individual neurons and all of the kind of crazy uh sparsely connected architecture underlying say the human brain or otherwise the emphasis is upon developing generative models um you know this kind of gets into

difference between complexity and accuracy found in any machine learning related problem, reinforcement learning problem, so on.

So again, not trying to recreate the entire brain, just to where we can get things accurate enough to where they start to track and potentially could even be fitted to empirical data.

So whenever

We'll be introduced over time to more exemplar kind of models, but we can adapt the form of the models presented in this textbook to our respective problem of interest.

We can change their form.

Should they be shallow or hierarchical?

We can modify the variables involved, i.e., the beliefs.

So what are the states to infer?

What are the observations that the agent receives?

What are the actions or policies available to the agent to commit?

So from there, it's really just a four-step recipe.

There are entire discussions that could be had about each step, but in summary, first part, first step is decide which system are we modeling.

So what are the internal versus external states?

What are the sensory versus action states?

What is the Markov blanket of the system?

What separates it out?

Are there nested Markov blankets involved here?

Those are all basically questions.

And then step two of the recipe is deciding what's the most appropriate form for the generative model.

Like I mentioned earlier, chapter seven and chapter eight each discuss discrete versus continuous time models.

So that's a big decision to make.

potentially involving somewhat different computations for minimizing free energy underneath, and one of them we're working with categorical probability distributions, another one we're working more with Taylor series and approximations, all of which are covered in the textbook as well.

We know that hierarchical models versus shallow models.

So hierarchical models may operate at a slower time scale.

They might basically involve learning states or beliefs about states, which are then supplied back down to a lower layer, supplying priors for the lower layer at the next time step or otherwise.

Other questions involve like the depth of inference and planning.

William Newburry, Ph.D.

: How far into the future is the agent inferring states where questions about the expected free energy come up.

William Newburry, Ph.D.

: So step three of the recipe, how do we set up the generative model they've already touched on it, but we decide on the models variables states observations actions.

In more detail, that means what are the observation modalities?

What are the in-state factors overall?

Other things like how do we define the agent's habits or the priors over its available policies?

That's kind of its starting point for deciding how to act whenever we exclude all infractors from the equation.

And then finally, which parts of the model are fixed and which parts are learned?

So certain parts might be updated over time while others remain static variables in the model.

Finally, step four, how do we set up the generative process?

The quick and easy way of saying that is how do we define everything that is not the agent?

Right.

So so if it's the agent who commits actions, well, those actions are then going to technically be received by the environment is one kind of figurative way of looking at it.

So you need to define an environment that can take in actions and then at each step, whether in discrete or continuous time, output the appropriate observation.

which will then be received by the agent.

So that starts to define an acronym for transaction perception loop whenever setting things up in code.

So that's in general the sum of the chapter as far as its four steps go.

There's plenty of other things that are discussed along the way and those four steps broken down into smaller parts.

But

I think that's a good enough overview for now.

So I'll open up the floor for any kinds of questions or just general points of interest that people might want to bring up to start a conversation about.

And then I believe Daniel had some other thoughts regarding other projects that are going on at the Institute, depending on what we have time for.


SPEAKER_00:
I really enjoyed the first point, the which system are we modeling?

Because as we design a system that's going to have some specific environment, we're going to have some problem that we may have imagined solving inside of it, which, of course, we're going to set up our Markov blanket and we're going to set up all of our situations.

But it was interesting to me because it seems like that's exactly what nature did whenever it built the brain and us and stuff like that.

And it kind of put it into perspective that the resolution in which you're trying to solve is kind of an arbitrary decision of variables and where you arbitrarily decide to set up your Markov blanket.

And it kind of just set it into motion to me that, oh, it's just Markov blankets all the way down.


SPEAKER_03:
Yeah.

definitely i always any time i i start to reflect more on markov blankets and what's been written about them i just feel like we're in a world of uh of relativity um you know if i attempted to model like myself say like what separates me from what's around me um well that said i'm composed of a series of smaller markov blankets right i'm proposing a series of neurons and

and other sort of processes that potentially could be seen as engaging in active inference themselves or other inferential processes.

So yeah, it's so important to be able to delimit if you're going to actually apply active inference and set up a simulation, like what is your system of interest and what separates it from the so-called environment.

Otherwise, we don't know what to do.

We don't know what to code.

And yeah, we don't quite know what our field of study is.


SPEAKER_02:
Yeah, going off of that, I was just reading some of RxEnvironment's documentation earlier today.

And in that case, they actually treat the environment, or you can treat the environment like its own agent in a sense, right?

It just doesn't have a generative model behind it, but that's abstracted away.

Just going off the Markov blankets all the way down.


SPEAKER_01:
Yeah, a lot of great points.

Just on that last Rx environments piece,

it seems like part of what is happening here is the generalization of entity and then subtypes for like agent and niche and and that highlights what is also um like the deep symmetry between agent and niche with the same blanket one person's send and receive is the other person's receive and send in like a um Alice and Bob communication situation um

then also i mean there's there's a lot of other um pieces to say like if you do account for both directions across the interface like you have modeled what is at the overlap of the agent in the niche so at that point choosing like where which part belongs to which

it's like it's it's almost defined as the interface belonging to both or interfacing with both so that leads to a lot of like the more philosophical speculations and and discussions around Markov blanket while also just pragmatically giving a very nuanced way to talk about like nested systems


SPEAKER_02:
Absolutely.


SPEAKER_00:
I found it really, I'm sorry, please go ahead.


SPEAKER_02:
I was just going to say practically just it eases implementation a lot in the case of environments.


SPEAKER_00:
I found that really interesting.

And I also wanted to ask you guys, I can't seem to find it in the text embarrassingly right now, but whenever it was talking about creating an agent that could walk, they were saying that you could have an agent that embodies some generative model that learns how to walk.

Or you can have legs that are built with the dynamics that in inherently don't have a generative model, but know how to walk already know how to move in that environment.

I found that to be really interesting, because it asked, for me the question of, do we need the generative model?

Or do we just need a generative model that has the generative model?

And is that just a larger generative model in and of itself?

Do you guys think about that at all?

Do you have any thoughts?


SPEAKER_01:
one relevant direction brought up earlier was that like the choices about the model can always be cast as a higher order modeling topic and then another piece which I'll find the link for will involve like this kind of embodied modeling like um let's just say that the elbow has like a limited amount of lateral Mobility or something like that then if you're going to make a map of that control Theory setting

there might be two kinds of maps you can make one that just simplifies no there's only one um control variable and it has like this limit it has this kind of like frictions and things like that so then over evolution that would come to embody features that would make the first order inference easier and easier

but that's not a guarantee of just throwing some code on the page, but perhaps in, in iterated evolution with embodiments features of the model that in silica would be purely needing simulation would come to like off source or just be heuristics for what the embodiment was already doing.


SPEAKER_03:
Yeah.

Yeah, I think it comes back to, I personally view it as like that kind of complexity, accuracy trade off question again.

One of my favorite simulations from active inference, like pieces of literature I've read, I'm personally interested in

I guess what would be called computational psychiatry.

And there's this nice paper that was put together to try and simulate a classic teammates paradigm, but where the agent has like an effective layer.

All that is to say is that you take the teammates paradigm, you model a mouse that has

a hierarchical layer that relates to emotion and see how the mouse's behavior changes in the teammates and how it learns differently.

And I mean, super cool.

And that said, there are dozens upon dozens of like model architecture decisions that could have been made there.

You've added an entire aspect to the model and you could have

you know, there could be so many different things that you included in there.

So why, why just that one thing suddenly is what the question that comes next, rather than thinking about this is a massive addition.

So yeah, it's just a lot of it is relativity.

And I think classic way of going about things is to design a model, do your best.

And if you're designing it for a particular use case,

uh see if it's possible to try and fit that model to empirical data if you're trying to you know approximate behavior of a real living organism or if you're trying to produce something that you know has very high optimal behavior for a particular task robotics and so on so you just have to test things out


SPEAKER_01:
this is that kind of one of the examples it's like interesting how it's it is being pursued in neuromorphic in other areas like this is this is a great empirical example through differential stiffness and and control on differential stiffness you have multi-joints action

with the falling weight with differential stiffness and like different tune tuning stiffnesses but that's how posture actually is held rather than being like this wide open control space like well my shoulder could be at any angle and my elbow could be any angle well then then there's going to be many control like dimensions and interactions

So another topic to connect here is an example.

So in the summary Andrew gave of step three, how to set up the generative model, like what we associate to what?

This is kind of a link to the active inference ontology.

This is why we do learn a generic system independent scale free ontology, even though our pragmatic interest might be like in this specific, you know, X.

um here's taking basically what are coming up in chapter seven the discrete time generative model as the a b c d e o s by listing those and then here it's at three nested levels of modeling like the pheromone level most um feet on the ground

detecting pheromone perception that was kind of what we got into in the infra ants paper but then also we could think okay well but the location switching and that's kind of like the live stream 42 slam where the bottom level of the nested model was the posture and the top was location so you could have like a warehouse robot that was like i want to go like to location on the other side

but not have to do all the postural modeling just having a slower transition and then here it's like okay well and task could have a transition maybe it's just a binary transition or that would be the exploration how you model it but this is the whole point about the system and scale independent ontology is that it's not the details of the system

which is why the ontology doesn't have opinionation on like how many nest mates should be in a colony or how rapidly should nurses shift the forgers or back and forth like those are once you kind of see it very far in the last mile of modeling that system of interest and so it's totally described by chapter six why are we making this model

what are we actually choosing to model what what kind of features are we taking in as data what latent states are we going to consider but the tabular form helps like structure what otherwise can be a very open-ended assembly process I guess this is all it just clarifies it visually and this is also good input for language models with templates of other code


SPEAKER_00:
That's really fascinating.

And it also kind of reminds me of a problem that I heard of in self-driving cars from a researcher named Yann LeCun.

And it was in his paper about JEPA, which they're asking how you create a self-driving car that pays attention to important things like other cars on the road.

instead of just focusing on in machine learning just classically minimizing the loss because if you give really really high definition cameras to the environment the car is going to start looking where the most information content is which is going to be at the trees because those leaves moving there is so much information going into there but you're completely focusing on the wrong things if you're looking at trees whenever you're driving you might just crash

So what they have to do in choosing the right sort of variables is they found out they have to use these hierarchical models, which to try to cast it into active inference terms, they had to create a hierarchy of a few different Markov blankets.

However, the actual variables for the interfaces of the inward blankets had to be kind of coalescing inwards.

They had to be abstractions.

You aren't allowed to use the same variables again and again, because then you would find yourself

in a recursive problem where you keep looking at the trees instead of focusing on things like time scales or important variables that occur down the road.

And that's just something that I thought was tangentially related to setting up the generative model.

Really interesting stuff.


SPEAKER_01:
Yeah.

great comments a few ways that's played out so here's like one one um fan favorite paper from 2021 that makes a three-level phenomenological model so this is commonly replicated

and you're right like a nested model does instantiate a markov blanket such that let's just say that experience were residing in the green but it doesn't even matter about whether it's phenomenological like awareness without going too deep into that from the green model's perspective

the the node that interfaces with the orange is the markup blanket and then also it's like that actually goes further and so it is the reason why although sometimes the discussion is like what is the markup blanket quote around in agents

But at the same time, S13 is blanketed from S23 by B. B is the Markov blanket intervening between two timesteps.

It's the transition operator.

Or A, the observation to hidden state mapping, like the partially observable part, is the Markov blanket between the observable and the hidden state.

so like the markovian time concept past only influencing the future through the present that's just the mark i'll blanket through time so it's it's not a statement or expression about how any agent or any base network should be it's actually so pervasive that the content itself does like relatively limited work

because it's just a feature of any base graph that's not fully connected is going to have these markup blankets so it it doesn't like distinguish certain kinds of sparse connected base graphs from others it's just a feature that is like basically is the sparsity of the graph that is absolutely fascinating I can't wait to check that out thank you for bringing it up

yeah that that's a cool one and then another another angle to to like bring a more recent paper um than than the textbook is uh active data selection and information seeking which was live stream 57 with with friston and par in the dot one and then par joining in for dot two um and and great preparation work for the dot zero

Like that is one of the things that's super exciting about active inference is there can be a lot of useful tinkering along the low road and like generating different architectures for nested models and unified approaches to perception, cognition, action, et cetera.

Here are some explicit ways to value the information with epistemic value.

that gets into talking about pragmatic plus epistemic value but this is the difference between reinforcement learning and active inference in reinforcement learning epistemic features must get projected down to or onto or get lumped in with utility slash reward because there's like a uni

ranking that different policies are getting ranked against and then policies are selected or sampled from that utility basis and so there's all kinds of tricks like we'll give a bonus for learning information but that would also come to exactly the same question which is how should information be valued

And then like a sort of first pass approach, like just have frame rate differencing, which is the origins of predictive coding, where the frame rate differencing is giving high information content, that's where you would want to focus.

But that is like a rapidly moving tree, like you brought up, like it could have high frame rate differencing, but it wouldn't reduce uncertainty about, is there a tree there?

And so active data selection and considering epistemic value explicitly helps give exact and composable epistemic values for generative models and gives like a playground slash space where epistemic and pragmatic value are not the same as each other, and yet they can be balanced.

so there is the parametric balancing like if if it's an Explorer exploit teammates or multi-armed bandit it's not just like making this situation is going to guarantee that you have resolved the trade-off

even though language is kind of used to that extent or effect in the textbook that this like dissolves the explore exploit trade-off whereas a slightly more toned down approach that doesn't like promise a given result is it sets up the setting where explore exploit can be compared without collapsing like the explore component to some higher order exploitation problem


SPEAKER_03:
So a quick comment on that, that I've personally been putting together a three-hour tutorial on applying active inference.

I'll actually be presenting it in a couple days at an international conference on computational social sciences in Philadelphia a couple days from now.

And I've, you know, one of the strongest points of interest for social scientists is this explore-exploit dilemma.

And I think one of the most interesting points about it is it can almost be viewed as like both things are being included in roughly the same optimization problem.

And I think that the textbook is coming out of a context where this active inference is being heavily compared to reinforcement learning as a potential alternative way of going about producing these models.

And exploration and exploitation do tend to be treated separately, where you need an additional set of rules for defining what is it that defines exploratory behavior for an agent versus exploitative.

And it's seen, I think, as just introducing additional assumptions

uh into a reinforcement learning model in order to differentiate those whereas with active inference there's this sort of quote-unquote natural balancing between the two but also agreed that like yes now they're in the same optimization problem they're both included in computing expected free energy for an active inference agent but it doesn't necessarily mean the agent is always going to find

some theoretical correct amount of exploratory value or absolute value in particular action versus pragmatic.

There could be none.

There could be zero.

It could always think like, oh, if I do this or this, I'm never going to learn anything.

So I might as well just go for the goal, so to speak.

But yeah, sorry, I've just been thinking about that a lot while putting this together.

But maybe that's useful for someone thinking about it.


SPEAKER_01:
Yeah.

Where that's shown most clearly and analytically is figure 2.6.

here's expected free energy it's a functional on Pi which is like a vector of policy priors that sums to one and then actions are either drawn from the top or the most likely or sampled from Pi with either a straight sampling on Pi or you can have a temperature on action so sample from a more blurred out pie

a more sharpened pie what g does in these models expected free energy is it takes a policy prior prior to the update and then updates it into a policy posterior so it's sharpening policies according to pragmatic plus epistemic value both explicitly designed so it has these elements one two three four five but really it's two major terms the first one here is pragmatic value so if you only have pragmatic value

only utility maximization that is expected utility and bayesian decision theory so this does not encompass there to be learning or at a higher level it reflects a setting where a modeler has given a utilitarian value to a learning event so in practice that might pursue learning

like learning like behavior but it did so by saying well uh getting the problem right is worth one but learning how to do it is worth two so then pursue as many points as you can and then it starts learning capacities or something or you can have two three four five included and drop the pragmatic value this is like the novelty search Infomax

because it kind of has implicitly a flat utility distribution so then decisions are weighted only according to the amount of information that they provide which is operationalized as the kl divergence double line between hidden states x tilde

with pi either way and y tilde is the sequence of observations so if the sequence of observations was expected to contribute nothing to beliefs about extra time this kl divergence would be zero whereas if y tilde is anticipated to change beliefs on x through time that has a high kl divergence

So it's the pure epistemic element.

The pure pragmatic element is about alignment of observations, future observations through time with preferences, the prior distribution on observations.

And then those two pragmatic and epistemic imperatives come together in expected free energy.

And then there's the degree of freedom of how those are balanced and then choices about how would you dynamically allocate the balance?

So it's not solved in practice.

However, analytically, it's pretty clear.


UNKNOWN:
Well,


SPEAKER_01:
Another fun direction to at least begin here is like we're two years after publication of the book and one of the biggest changes in the Active Inference environment has been RxInfer.

Several of us are working on this and we have

uh figure 1.2 with with help from some of the bias lab I'll put this in the chat and and we'll see but the the goal of this kind of um shared project between the arts and for a project in textbook group the goal is to have code for every setting and figure in the textbook and that would be um

We can do it in a few different repos.

We can have copies in notebooks, so it can just be run.

There's a table of all RxInfer examples.

So here's the reactive base.

These are 23 examples from the documentation.

Here's Cobus Esterhausen.

Here's eight examples from his website, Learnable Loop.

then here's figure 1.2 so here's a notebook that will install you click run runtime run all first time it hits the first block it's going to install julia then it'll say reload the page that brings you into the julia kernel then runtime

run all again.

Second time through, it runs the tests that confirm that Julia is installed, and then it installs the packages that are relevant for RxInfer.

Then here is a great example of

without even going into Rx in the first performance slash engineering or like computational benefits, just focusing on the textbook group possibilities and educational possibilities.

we have figure 2.1 so it's like the frog of the apple is either jumping out of the hand or not and then we have an agent who has priors about the prevalence of frogs and apples in the world and then they update what they believe they're actually holding on to or did did hold on to based upon the observation of either a jump or not so the app model block

in rx and fur is like the nut to crack this is the closest not not the closest it could ever be but this is the closest we've ever seen the concordance between the equations that analytically define a generative model and the code to do so and so that also captures the biggest difference with rx and fur from a design

perspective overall which is in pi mdp and in matlab there's a procedural definition of for example the agents interfacing with the environment like

for a new observation then this happens that kicks off this calculate this update that move this to here and so it gives a script like flow that's amenable to explicit looping over like elements of planning or elements of of across multiple agents then there's a lot of degrees of freedom with like okay but then you know should agent one go first every time step or how do we deal with that

Oricson-Ferr replaces that with a declarative programming style.

So here, this is the Bayes graph as declared.

And it's basically what you see is what you get with the types of distributions.

And they could be parameterized here directly, or it could be parameterized elsewhere.

and then the inference is also as direct as you can get it's basically just calling that base graph as declared doing some stuff under the hood that we can explore more and then

Letting that graph in its definition be uncoupled from the actual message passing like logistics of information flow between the nodes.

That's the part that goes under the hood now with RxInfer.

So it opens up this possibility where

the app model specifications of the generative model look a lot like the equations and like the minimal way to describe a situation without this extra control flow of which nodes are updated in what order.

However, that has to be supplemented in.

So that's a lot of the ongoing work is like, what are the syntax and possibilities for these app model functions?

What else needs to come into play to have interpretability and kind of like getting it run without any errors also, but getting it working in a way that seems to...

David Price- function and maybe even leverage the reactive message passing part and then also what that would mean is not just for like different languages and programming backgrounds and everything, but then in this par textbook group, there would be a really clear path.

from like here's figure or 2.1 and now here's how you could continue to learn more about rx and fur and that would take one like all the way to the horizon of modeling it in advanced settings so that's a clear pathway whereas the 2022 textbook otherwise the appendix doesn't fully reproduce all the figures it's also in matlab so then it's like well

you're at a fork with whether you recommend or go to study these examples code in more detail or kind of start on a fresh slate with like pi MDP which is probably what in earlier cohorts we discussed a lot but now there's kind of a tiebreaker with RX and fur and the missing artifact that we could greatly benefit from would be the one time

finite basically task of making rx infer models and notebooks for the textbook group and in doing so making the accessory um kind of explorations and having the conversations that are like the questions in chapter six and so just kind of then then we like we have

like a tinkering format in common and one that we can run in notebooks and one that looks largely like the equations kind of written out more in a programming form.

So not, not, not to go too, too much on this, but, and, and this notebook also contains some extraneous information.

but if we can bridge it with rx infer and the textbook group it'll like really give a great connection between the educational on-ramps and the application that that is also like an open source framework


SPEAKER_03:
Yeah, I would second that.

I think that'd be super cool.

I've personally been using PyMDP for that tutorial I mentioned earlier.

And I mean, the developers of it are great.

You know, I've been talking with them personally a little bit.

But that said, yeah, I've seen a lot of the benefits of RxInfer and hope to get back into the RxInfer group.

Seems like really exciting work.

And finally, to bring it back,

I think especially for part two of the book, for anyone in the textbook groups coming in, cohorts looking to actually design active inference models themselves, like this chapter, like how do we define a system of interest?

How do we actually code that is the additional question that isn't necessarily answered in the chapters that we're reading here, right?

It's kind of implied in that

You know it's the authors of the textbook do use MATLAB, as you mentioned, Daniel and so it's kind of as well as the the appendix does include like some some figures of direct MATLAB code constructing a matrices and so on, and you know these but.

yeah being able to supplement the textbook group with.

RxInfert code, which is also, from my perspective, the most up-to-date way of maybe even doing active inference.

I think that would be incredibly exciting.

And I think it would also supply people with, one, a great learning resource, like, oh, this is how you do this.

And then two, it supplies a bunch of potential templates that people could download and modify for their own

personal experiences, in the sense that, you know, part of active inference is that we don't just learn by changing our perception, we learn by changing the world.

So making these kind of templates available for people for them to try and test out their ideas themselves, I think kind of fulfills the other half of the equation of maybe how to learn how to apply active inference in the first place.

Really exciting stuff.


SPEAKER_02:
Yeah, I just wanted to second the value of templates for RxInfer in that different models can be hierarchically composed along with each other.

So yeah, once one is specified, it has a lot of value going forward.


SPEAKER_01:
Yeah, getting some really clear documented examples.

and and getting the visualization functionalities and then the examples of what those motifs look like building libraries that can then be referenced by llms for code generation to um building some more methods for like the meta programming and and the interpretability moving in and out of other formats like there's a lot of exciting ways to go um

and chapter six is going to be kind of their meeting point because that's where we will that's that's the beginning of the modeling process again with the selection of the system of interest which is kind of this like open ended stage but then following that it starts to progressively specify and eventually in a way constrain the output of just a final generative model

So that, that demystifies, I think a lot of the process and, and we'll, we'll like create lower friction paths to get from like a given dataset to something that's, that's doing something useful.

I remember one discussion from many cohorts ago of like,

then there's the pragmatic value of making the generative model which would be like okay I'm making air conditioning and so I prefer it to be like low energy so then I need to make a working model so that it works and then there's also the epistemic value like people learning about active inference learning about themselves learning about each other um like learning about what is possible that it's like it never even

needs to reach the final artifact, although that final artifact might also provide epistemic value separate from its pragmatic value too.

So it's like not a worthless process, even if it and how hard different stages are.

It's super system dependent, and that frontier is very rapidly moving too.

So it's quite different than it was like one or two or three years ago.

So I would expect it to be also multiple fold to orders of magnitude easier in one, two, three years.

so it's just sort of like jumping in while you can and and then your own experiences and records will help calibrate and show where things actually are shifting and how easy or hard they are yep i do want to quickly add because i know we're about out of time but uh


SPEAKER_03:
much, much earlier, during introductions, someone, I think Zach had mentioned an interest in interpretability of models.

And I would love to see an RX and for like, I'm saying this partially because of just ignorance of what people have tried so far.

But of course, people are minimizing free energy, but maybe a little bit of emphasis on like,

different sorts of helper functions that can maybe pull out you know for a given time step for example what was the expected information gain um of you know choosing a particular action or policy for the agent like basically having functions that help you pull out like well here's the parts of this that are comprehensible right um for me i'm

putting together this tutorial for social scientists, it's like, well, if we can't extract any interesting or useful insights from these models, then why are we memorizing all of this relatively complex math?

Why are we spending our time doing that?

And so like, for me, it's like, oh, well, you can, you know, make, say 30 agents, and now you can have them do things that lead to emergent behaviors.

But now you can actually peer into

you know, if they're learning their likelihood models, now you can look at their particular beliefs and how they change over time and consider those to be something like cognitive metrics that we can actually interpret, like we can interpret them mathematically, we understand how they work, but we also can see how they impact behavior, model performance and longevity, and so on.

So yeah, it'd be super cool to

to just make sure with all these nice technical advancements that are being made with Julia and RxInfer to also supplement that with interpretability and having that as a ready resource as well.


SPEAKER_02:
Yeah.


SPEAKER_00:
No, absolutely.

Oh, sorry, after you.


SPEAKER_02:
Oh, yeah.

I was just going to say, I think today it's probably quite limited.

But RxInfer Debug Mode does show messages being passed.

It's mostly useful, to my understanding, recommended.

for when you're unable to optimize the graph.

You're not converging on a solution.

You're just running forever.

Usually, you're running into some sort of intractable problem.

I don't know how much that helps with interpretability.

I don't understand message passing super well.

And I just watched a talk from Dimitri Bagev earlier today.

It's linked to the engineering Discord.

General.

He talks about how it took him years to understand message passing.

So there's that.

Um, the other thing that I would, I don't think this functionality exists, but I would possibly look forward to maybe understanding these models, especially as they're growing complexity, um, is sort of like loss of function research on them.

So like disabling or removing parts of the subgraph, um, because these are all of the message passing happens locally.

It's just in between two nodes.

Um, so you can.

The example that's commonly used is like you could lose a sensor that's feeding into the model and it would still continue to work with.

It's just optimizing without that.

So yeah, maybe that would be kind of an interesting approach as well.


SPEAKER_01:
i'll try to give the shortest possible message passing that this this is a super interesting paper i hope we can like understand these different approximations but but leaving that for a second the 2017 paper with with burt

here's the key figure this is connecting figure 4.3 discrete time generative model this is a base graph nodes are the random variables edges are the causal relationships in the 40 factor graph the nodes and edges are flipped so that the nodes are like combiners or functions of where different variables which which are on the edges come from

There's, as they show, there's like a one-to-one mapping between a base graph and a 40-factor graph.

So they can contain the exact same information.

What you gain, though, with a factor graph is this like logistics or scheduling order.

So it allows you to like turn the crank ahead one step and to say that's going to take these 22 messages in this exact order.

what those messages are there's a lot of different like payloads they can contain but what it lets the whole graph do is do one descent on a variational free energy landscape for example it could probably also do other things so in 2017 they were like we're already making these base graphs here's a logistically deterministic scheduler that'll let us crank even on a large base graph

let us crank the whole thing ahead one time step at time.

That's message passing, variational message passing.

The difference with reactive and what Dimitri, I think, brought in with the rocket is it uncouples the update frequencies from different parts of the graph.

So instead of having to turn the whole crank and run this deterministic schedule of every single thing, you can have the reactive like just in time sending and receiving, I believe.

that opens up some other things that need to be specified like how the logistics are timed but it seems like those are just conventions and actually the reactivity means that like the graph doesn't have an order or tempo per se but it's like figuring out all those things are some of the most important things


SPEAKER_02:
Right.

And this kind of reactivity, I've wondered like the connection between message passing here and message passing in the case of like multi-thread or distributed computing.

It seems like with the reactivity, you're getting possibly very close to that.

I don't know if it's something that's been discussed much.


SPEAKER_01:
that's super interesting I think it's probably almost totally unexplored with a multi-threading there's multi-threading and concurrent uh programming as like a multi-agent situation right I don't know if how like if those kinds of moves have been brought into like possible multi-CPU or GPU RX and fur


SPEAKER_02:
yeah yeah i don't i don't know either it's it's very interesting the parallel there it seems yeah i'd love to learn more about it but i i also know not too much about distributed computing so maybe something to explore cool well this is a fun uh sort of re-entry into six like also we'll have the three-week rhythm so let's


SPEAKER_01:
in the third weeks let's either work on a textbook example and or annotating code from from kobus or the examples but let let's go into rx infer and um and curate the questions along the way and make the notebooks etc possibly we could record these sessions but we'll see if it just is more like an application workshop

Um, I think this will be really fun though, and, and help give like kind of a textbook part two plus, because after having done, and then also with, then we leave these kind of like perhaps multiple workshop templates for cohort seven, going through the second half.

And then that would be like a jumping off point for, okay, then you finish that, but you're already pretty familiar with art and fur by the end of the textbook group.

because the application oriented part of the book gives templates and examples.

So still many people slash all people will be learning code and Julia and Rx and Fur, but then that's where we can also connect to using LLMs for code explanations and all this other stuff.

So thank you, Andrew.

Thank you, John and Zach.


SPEAKER_00:
Thank you guys so much.

Daniel, I'm excited to read those papers and check out Rx and Fur.

I'll talk to you guys soon.

See you.

Bye.
SPEAKER_03:
okay it's September 14th 24 or 16th like my clock was looking weird um we're entering chapter nine for cohort six um do either of you want to type or ask any topics you want to


SPEAKER_02:
cover explore for nine otherwise we can look through the chapter i was actually about to i'm sorry go okay good i was actually going to ask if we could just like run through the chapter because this was the first chapter that i was like oh man we are jumping into creating this model uh could we just run through uh

one time into going through like what was the data and what was the model that we were actually applying as well as just going through the loop of the gear i'm sorry i've lost the figure it doesn't really matter for that last part yeah got it okay


SPEAKER_03:
Pulling back, I think going over the chapter is the first thing that we'll do.

Then there's the questions and some notes.

And then there's also looking at software, just speaking generally.

Maybe we can do that in one or two weeks from now.

Looking at the JAX branch on PyMDP, looking at RxInfer, and looking at other empirical work.

Let's start with the chapter though.


UNKNOWN:
Okay.


SPEAKER_03:
second half of the book applying active inference coming fresh off of chapter six recipe chapter seven and eight giving the continuous and discrete time now getting to using the models that are created for data analysis that's kind of revealing that in all the previous chapters two three four um not five

but then seven and eight when talking about building a generative model it's to produce synthetic data like to run a simulation forward here now we're also or alternatively looking at a setting of having a certain structure of data and then seeking to recover latent parameters instead of specifying the latent parameters and then generating the synthetic observation like data

Our general goal is to recover the parameters of the generative model that system of interest uses to produce behavior, subjective model.

9.2, Metabasian methods.

This is really well summarized in figure 9.1.

It's like zooming out a layer from figure 4.3, figure 7.3 type, POMDP, hashtag model stream 14.1,

um umdp that we've been specifying talking about minimizing the variational free energy and so on zooming out saying okay not just the mouse and the teammates now considering the observations for the mouse are actions for the researcher and then conversely the output behaviors of the mouse are objective observations for the ethologist

Interesting how it's using the notation of like the subject centered view.

because it's it's calling them oh where whereas you might also call them actions the researcher and then it's like it gives the the hint of the suggestion okay yeah you could make a policy if you wanted to do policy control variable um like another Pi sub scientist and then have that connect to O but otherwise we're just taking this as a given so we're just taking that as the input data for here and so that subjective model which does the forward

inference which is like um the kind of perception action loop that it's focused on that's happening with a given set of observations in an experimental context and outputting some behavior so this kind of

showing okay the data like the input data here's the the information we knew on the which where we put the food in the tea maze and here's the actual sequence of moves that the mouse made

um brief little statistical interlude that starts to to Bridge back from this question of

reducing uncertainty of the unknown parameters of interest, figuring out the subject's prior beliefs.

It's like, we know where their eye saccades were.

That's just empirical.

Or we know which things they bought.

That's just empirical.

But then to do belief modeling, cognitive modeling beyond just the pure behaviorist layer, then that is reducing uncertainty about these causal parameters of interest.

So how are those going to be analyzed from real finite data?

They go into variational Laplace.

This is something brought up in other textbooks, in the SPM textbook.

Okay, 9.3, 9.4 giving like two different ways, more of a classical statistically parametric, and then more of a empirical Bayesian approach to fitting a first kind of structural layer on these hidden belief distributions.

Variational Laplace centers a Gaussian in the log likelihood

Thomas Parr discusses this in 57.1 or 0.2, fits a Gaussian on the log likelihood.

So in the actual likelihood space, it doesn't have like a, or it's a parabola in the log space and then like it looks different in the non-log space.

But this just fits a central tendency on a belief distribution parametrically.

that that doesn't require any um that doesn't that fits a simple distribution to uh data parametric empirical base takes a slightly different approach which is like to kick start the estimation of the prior from the empirical overall distribution of the data or like a previous or related experiment

Here's the instructions for doing the model-based analysis.

So collect data first, or maybe the data are already collected, like it's a meta-analysis.

So just like the previous figure, the data that are important here are like, you know, the input, which arm was the food on?

Which color was the strobe light?

then the context of the experiment if if only to normalize out and say yeah actually the uh the digit in the in the minutes column has no correlation with this but it might be something just included formulate a pomdp model so that's as we saw here with taking the subjects

kind of first-person perspective on taking in observations and how it does its belief construction here's a map describing its belief construction representation and then um how that information becomes actionable and then we have just empirically the information on the temperature of the room and the the teammates

This information in the list is summarized in Figure 9-2.

So we had data collection, or we're reviewing some data that were already collected about the T-MACE.

But we have the lab-type data.

Then we have Chapter 7.3 POMDP for the T-MACE.

Three, specify likelihood function, Equation 9.2.

action selected depend on the observations made we could look more about this but it kind of gets to the punch line of the and and it's kind of subtle with the bold and what and the bold being vector but the the policy that's selected is the the argument that's minimizing the variational free energy over all the policies so that's a like how um

how decision making is going to be evaluated from the outside as if it's minimizing free energy as opposed to other approaches like as opposed to it maximizing reward this tells us how the model should be used to calculate a likelihood this typically calls a pmdp solver spm mdp vbx it's interesting these are like super oft referenced

scripts they're in matlab there are matlab engines that can call matlab if you have a real license from other languages so it can be really called within python um other options there's octave which is like an open source cousin of matlab oh well there's spm standalone which is actually matlab but it's just like a reduced kind of single s spm only um download

Then there's Octave and Patches.

I don't know how well the coverage is for patching certain things of SPM MATLAB into Octave.

There's Julia SPM, which is an attempt to port over to Julia.

There's PyCall and JuliaCall.

So basically calling languages within each other.

um and the getting these scripts which are kind of key central pillars of a lot of the inference across prior papers getting these scripts into um non-proprietary more easily callable um methods and implementations it's it's huge

Four, specify prior beliefs about the parameters in terms of expectations and precisions.

Here, setting a prior just to give an initial condition.

Again, that could be parametrically gleaned from data, or it could be empirically set like PEB.

Inverting a model is...

using this rearrangement that's possible with bayes equation and using kind of algebra and separation from there to calculate what we're interested in which is like the belief the the the um distributions of subjective beliefs beliefs of the mouse in the teammates conditioned upon all the stuff that we're putting into the model

Whereas the synthetic generative direction would be given the ad hoc specification of belief distributions, like what if it thought it was three times more likely?

Then you could generate realistic trains of behavior.

you take those final behavioral data estimates and it's like, here's five days of the week we did the experiment and belief whether we calibrated or didn't calibrate, like whether it's just a fear score or it's calibrated or it's somehow normalized, like it's a percentile or something like that.

And then this is like a figure, just normal figure from a paper.

We had five different condition groups and their belief on this distribution, here it was seven plus or minus one, five plus or minus one, one plus or minus one, negative one plus or minus one.

And then you can just do a t-test.

And then you put little stars, statistically different groups.

That's the main outline for nine.

I think it'll be awesome to see fully worked cases, like if we can get a repo that has all of these steps labeled this way.

The last piece is 9.6 gives a few examples.

9.3 looks at iCicade in a continuous and in a discrete control setting.

We could look to the paper if we wanted to look at that one.

Table 9.1 reviews many citations where these kinds of methods have been applied.

That's the chat.

So it's interesting because nine is like, it's kind of like a Jimi Hendrix thing, if six were nine.

And then six is like the recipe for building the generative model.

And then nine, starting, well, we still have to build it as per six.

But it's another recipe.

But they don't call it a recipe here.

Any thought or what should we continue with?


SPEAKER_02:
Whenever they introduced, I think it was the variational applause.

I may be misremembering.

They casually brought up, oh, yeah, you don't really need to understand this, but this is predictive coding.

This is just the example we're using.

If you don't mind, could you say a little bit about, if I'm remembering correctly, predictive coding, as well as in your experience, like what kind of models are being used right now in active inference in general?

Is it all going to be like just taking about the same model and trying it through different things?

Are many different models used whenever you're actually building these experiments or is it just really, really diverse?


SPEAKER_03:
Yeah, good questions.

Okay.

And then let's look at this page.

Hmm.

Okay.

2007 here.

2007.

All right.

First, let's see where it came up in this

okay this is totally pending pending pending just um seeing one um just kind of one take on it look octopus or others might be able to bring a lot more detail in

This is coming from the parametric statistic approach.

So kind of classical, pre-computational, at least pre-1900s, let's say, approaches to fitting distributions.

Whereas once you can do parametric empirical Bayes or like sampling based methods, then there's this whole alternative branch of doing sampling to fit analytically intractable distributions.

But if it's going to be like, well, how do you make it more analytically tractable?

That might be advantageous or simple, might be ineffective, but also it really does have certain advantages.

if it if it if it totally mangles the underlying structure then again it wasn't adequate so this isn't to say that it's adequate for every distribution but if you do a sampling based approach then you have to also put a nice behaving continuous distribution over that or or choose to not do that because the sampling gives you a good shadow or kind of scatter if you do it well but it still doesn't have the kind of smoothness

so it's kind of like if you want to get to the parametric why not go directly from um the data instead whereas now it's sometimes sampling is thought of as as an alternative approach but it doesn't really get to the same end point this is a incrementally like Matrix gradient based method

to fit based upon the the gradients of um of a Gaussian or a parabola or something downward facing over log likelihood and Thomas part mentioned it more on the stream

Possibly because it's working, again, whether it's, okay, maybe it is a Gaussian, fitting a Gaussian, but then the Gaussian is locally approximated with a quadratic.

So then that keeps the approximation of the gradient possible.

This paper describes it.

Okay.

That's just the preview.

but it seems like a lot for the preview okay so maybe for us to learn more about but it's a tractable way to use gradient information geometric gradients to incrementally optimize statistical models

being proposed existing kind of alternatively to a sampling based approach how is it actually being applied this is a great question like whenever I come across code or GitHub I try to bring in this table everyone everyone else can too as we build many more we'll think of better ways to organize it as as the number of examples gets way larger than this um

I mean, there's the whole RxInfer, RxEnvironments branch that's rapidly growing.

And there's a lot of MATLAB and SPM ones that are not added in here, but there's that large branch.

Those ones mostly...

call back to those spm vbx bays spm um mdp check except like all these kind of names so there it's like friston at all curated the functionality of those core utility files and then papers would use relatively simple generative models and then just know that if it was built a certain way that the um

generative and the data fitting directions would work so that's the kind of matlab um there's there's a long tail of languages like in the active inference package like rust go shell just like a bunch of random languages they're not super performant but it's enough to get started

C++, haven't seen any presentations per se using it, but I think some on the more engineering side might be.

Of course, one of the major questions is like, well, this is what is known in open source research, but anything other than that, of course, I don't know.

On the Python side, there's PyMDP.

which is the main Python package.

It has a characteristic and lovable slash challenging file structure that can make it a little challenging to get the exact thing you want.

However, there's a folder jacks, which is bringing in the kind of jacks, and there's more active work on this in the branches too.

So there's a lot of work there kind of to the chapter nine theme of fitting from data.

There's a lot of other Python work in model stream 14, which was shown briefly earlier.

ran way said that deep dreamer he said that friston said deep dreamer it is basically active inference so i mean at least i think rand said this which makes sense because one of the collaborators was danger hafner so

I think if we kind of just return to the original, what are the applications of Act-Inf?

What methods are people using to apply it?

There's the threads and the streams that can be followed from here.

And then there's the parallel slash is it Act-Inf question.

If you end up with a pragmatic and epistemic element to a RL or meta RL or POMDP type model, or something that can be described as such, I mean, has that not reached active inference from the low road by another way?

What do you think?


SPEAKER_02:
i kind of wanted to ask you about that especially at the end of the live stream he was talking about how there's different spaces that you could use as well as a work that i've never heard of before by uh simone i think he said uh from the oxford lab and he was saying that you can think of the model in terms of of course the model itself where we're doing a gradient descent so we're actually going to be able to talk about gradients flowing in one direction we can also talk about

of the actual concept space of the model, like Shimon was talking with representation spaces, I think it was called.

And then there's also going to be the thing that we're learning about right now, which is designing the environment through the POMDP and then having the model learn from that.

So there's going to be some environment space.

There's going to be some belief space that the model uses to interact with the environment.

And then there's also going to be the model itself, which is going to be in the space that back propagation is really natural.

I bring this up because in Figure 9.2, like we were looking at earlier, whenever we're actually looking at the model, we're thinking of it in the space of, oh, what are we looking to get out of it, which may not be the same space.

It may be like the POMDP, like the environment space that the model is working with that we're designing is not the native space that the model is working in, which is going to be the belief space, which reminds me a lot of Markov blankets, which maybe that has nothing to do with anything.

but i thought that was going to be an interesting link and i wanted to get your thoughts on it the movement of spaces of the changing of spaces as you move through these squares yeah let's return to the question let me just copy out what this quote was because i mean this is


SPEAKER_03:
this is this is very interesting to it that ran well for those who have not seen it it's it's really an informative stream some of the most information density on um epistemic value pragmatic value connections with active inference and reinforcement learning um

A lot of people's view of Dreamer versus deep active inference, he mentioned that deep active inference has been used in essentially three ways.

One of them is when neural networks and other approaches are used.

That's also called amortized inference, but that's kind of like deep learning, but deep active inference.

The other two are like temporal depth.

like long Horizon of planning and then the third is hierarchical depth like um and then we talked about how also those can be flattened uh deep active inference is that many people have implemented is that dreamers so similar to active inference especially I can't remember if which version of dreamer and if it does use information to gain

term in planning in a planning objective and if it does then you basically cannot tell whether there's any difference between dreamer and deep active inference um so in a sense even Carl Tristan has mentioned it sometimes that dreamer is basically some variations of an active inference agent what a truth drop

here was the major approach of the paper which is the pomdp has an aboutness of uncertainty subjectively like the a matrix describes uncertainties associated with the mapping between observations and hidden states and then the b describes the possibly conditional mapping of states unfolding um

that whole picture though can be seen as an mdp because like if you're programming in pi mdp you're making a pomdp but for you the pomdp is a fully observable mdp so then there are certain things that you can say that are results that only exist subjectively for mdps like chess but not for pomdps

But then as a modeler, you can say MDP-like things about POMDPs.

And it turns out that the key thing that the agent's behavior has to follow along is the information gain.

But it really just shows how different... And then I said something about FEP and the high road and low road.

And then he's like, I'm kind of like a low road guy.

I don't really have anything to say on that.

Which was just amazing and very humble and funny.

For how informed and how capable his presentation research is, that it's like...

it wasn't about fep he didn't really want to go there yet he was talking about the free energy functionals with like the highest degree of understanding so then that's just like wow free energy is on the low road

That's the whole thing with energy-based learning, variational autoencoder.

I mean, millions of people are using evidence lower bound in model fitting.

As Ram pointed out, RL and meta RL and Deep Dreamer go on the low road to indistinguishable from active inference.

So that really puts the low and the high road into context.


SPEAKER_01:
Pretty interesting.


SPEAKER_03:
Procedural method.

for, I mean, this is like live stream 42, but here with three light time layers, days, hours, minutes, and then using a procedural method to fill this in, as opposed to like doing this and then that one, and then just this, and then emitting just from here or something.

So, I mean, there's so many low road methods.

every method is on the low road but but none of it is in competition with the high road like before we started zach and i were talking a little bit about the reward learning also and and um kind of like okay want to build the skyscraper taller

it's just reward function it's taller and then in contrast it's sort of like um I want to reduce my uncertainty about looking from the 100th floor and then maybe you build um a 75 story and jump or you build a 200 and go to the 100th floor or something like that but then if there's like a um an equilibrium that can be reached when that preference is achieved it's just so interesting to see like

how those can yield this they could yield similar or different computational policies inferred to get there like there are situations where um the the the RL and the active agent would have indistinguishable behavior but and there's situations where they might have different behavior but

that would all be situational on literally just specifying, okay, what is the setting?

What are the data?

Maybe certain generic things could be said like, well, for these kinds of environments, this kind of outperformance would be expected or this kind of asymmetry would be expected.

some more generic statements about kinds of environments where there'd be different kinds of competitiveness.

But I mean, it still is just a basically empirical question.


SPEAKER_02:
do you think biology is going to help us solve that empirical question because i remember whenever it was inside of the book that introduced the uh system one and system two uh thinking fast and slow that's what it was and oh i'm sorry inside of that they were talking about the example of i believe it was falcons uh that were in california

And so whenever they were discussing it, they asked, what was the value that you would like to apply towards saving this many falcons?

And then what they did was either they introduced increasing the space or increasing the number of falcons that you would save.

And they asked, how much would you be willing to spend in order to donate to this cause?

And they started to vary the amount of difference that it would make.

And they really showed that even our own belief systems are unintuitive to us.

of how we actually value things and how much we value that especially at different scales is not something that we would expect from ourselves do you think that we would be able to find an empirically good way to solve this strictly from how we function biologically or how any mammal might function inside of us


SPEAKER_01:
Frank or Axel?


SPEAKER_00:
Sorry, I just got in here.

I've been setting up, so I didn't catch it.


SPEAKER_03:
Oh, it's all good.

Zach, it's a great question.

And is slash how is active inference related to being a theory of inquiry along the lines of the pragmatists?

What can be...

empirically and theoretically said about empirical situations like laboratory behaviorism and also theory producing situations considered empirically i don't know though if even a very strong account there would be equivalent to the resolution of uncertainty or the account for the like the last mile

So as to whether this RL or this ACT-INF model will do better on explaining this bacteria data set, that's like, to know that would be like to know, it's like the halting problem.

I mean, if you could just, it's like, it would be equivalent to knowing how a data would fit a model before doing it.

You can use heuristics.

And those heuristics may be 100% right in certain settings, but that still wouldn't be like actually knowing it.

But then you could have certain, it'd be like, okay, for linear models or for transformer models, like adding another layer sometimes takes more train time, but it never does worse.

then you could say certain things about two hypothetical architectures being applied to a data set or two models being applied but it's kind of like the bayesian model reduction question or like structure learning you could have a procedure for adding and pruning down latent accounts and and all these kinds of things

But if you already knew which model to apply before, then you might be coincidentally right with your prior, but you wouldn't have been right except by prior.


SPEAKER_02:
Okay, I see what you're saying now.

I think I was anthropomorphizing too much because I was thinking of 9.2 and I was thinking that like, oh, the ideal situation would be to where you design a system that's like, oh, it works like I work.


SPEAKER_01:
So that's where I was coming from.


SPEAKER_03:
Interesting.

I mean, because it could be like, it works like I think it works would be like, okay, this is a metronome.

and just changing temperature didn't matter, turning the dial totally explained the frequency of this.

So it's like, there are situations where this box is, well, or this could be just an AND gate, or this could be just some other discrete or designed system.

And then on the other side of that could do would be like, this is a person talking to another person

And then but that is so that is that which one is like, it works like I work.

Like seeing the world work as expected, or seeing the world work in a way that is open and surprising.

And I think these become such interesting questions as the technical, um,

as what the our our active computational niche is doing around us gets more active than like what is it that we're really doing but like opening I of course ironically doesn't have I don't think they release the technical information um it says you can read more on the technical research post

log linear but kind of subtle log compute better evals on domain expertise better coding it's available in cursor 01 mini is doesn't cost any extra they hide the chains of thought

But it's kind of like, but you don't even know whether what has happened, but it's doing, I mean, so then if it's doing, um,

kind of something between working memory transient um chain of thought with pruning planning and thought dreaming to plan planning to dream maybe using a uh energy-based method or an elbow and then like

also with with kind of a niche modification maybe chains of thoughts can read and write to each other or something like that there's just so many architectures people have proposed yeah axel


SPEAKER_00:
Yeah, I just have a quick question here in the end.

I'm sorry I missed pretty much most of this.

I was in transport.

But I have a bit of a more technical question.

I'm interested in the differences between RL and active inference.

And right now, I'm going through some of these PI MDP Colab notebooks.

And one of the first ones is a tutorial where you're setting up these different matrices for, for example,

like a is your matrix that maps the states to observations and b is your belief matrix about the beliefs of what will happen if you carry out a certain action and so on and so forth and yeah i think if you open it maybe the solutions will already be there and you can see what i'm talking about

But in this example, of course, it's a simple one.

It's unambiguous.

So you start out with all of the matrices that you need to solve the problem, or at least that's how it seems to me.

And I'm kind of interested in how little you need to specify.

When I first got interested in active inference, I was kind of attracted by the idea that you might not have to specify anything at all.

You would have an agent that learns how to carry out useful actions, not by having a reward function, but just by...

doing things that makes it able to control its environment better, which in the end will be things that perhaps are useful.

And I think my question is just, how little do you need to specify?

Is it possible to specify not even the agent's understanding of when it sees that it is in square one, it also will think that it should expect itself to be in square one?

What is the least thing that I can specify in this setup and still get an agent that learns all of these things itself?


SPEAKER_03:
Yeah.

Great question.

Um, first part on RL.

and active i think check out this model stream 14 that we discussed a little bit because it was some of the most distilled and um experienced comparisons but then this question of like what is needed to be specified so um you're totally right that that um this is kind of like it's like a canon inference canon or reverse canon set up on rails

with with a certain like it's kind of aimed on a certain axis already because it's like well how does it know it's in a teammates how does it know the the semiotics of how the queue at the bottom relates to um whether it's on the left or the right you know how does it know how to walk so it's like but that's where it becomes really important to clarify what are questions about the world

What are questions about generative models?

And then where are we developing generative models to address questions about the world?

And then those can go back and forth.

But it's like, if you wanted to study, well, how does the semiosis become associated between the Q and the T maze?

You could study that.

with a degree of freedom around that relation.

Or you could just say, we're assuming it's a perfectly trained rat that already 100% understands like the left and the right cues.

those are generative models that are being constructed for different cases so depending on what is is wanting to be um gotten intuition for like in a toy model or what kinds of research questions are being addressed from it's like it'd be like downloading a genome and just like model it it's like but i mean to what end a 3d shape or like you know the regulatory network it's like well all of them it's like yeah but then that's just the whole project

of the field or something that's that's not a specific research question to ask about so yes definitely and then when it comes down to in into this level it's like yeah so then but then what what about adding a fourth grid location or what how would it come down you know then then it has to change it so those are some of the rate limiting steps in the in implementing things like structure learning

in the models today which is like the dimensionality has all these repercussions or how does it how would it look how would it come to know that it could add a new affordance so can it be something very simple with a procedural approach to like changing the dimensionality and the number of slices on abcde and then could that like a procedure over abcde

have the ability to learn in this or that way it's an open question to and then okay you could set it up but but then if you set it up in a computational environment that only has like multi-armed bandit slot machines i mean it's not gonna code in python so

then that gets into the niche specification either the computational niche for for training and learning and inference or more of the robotics route with the real world niche


SPEAKER_00:
So yeah, I get what you're saying in terms of it's interesting how you might want to procedurally change your structure when you've already established it.

But then let me ask, what if you have established the exact structure?

You live in this simple grid world.

You can establish all of that.

And perhaps there are certain things you can do in the grid world.

Maybe you have to get keys in order to open doors, in order to go somewhere.

But there's no final reward in the goal position you want to go to.

do you think an active inference agent because it's curious and because it wants to learn to understand its world will figure out how to do the actions which will end up getting it to be able to predict what's gonna like get the furthest in the world just because it uh has that incentive perhaps


SPEAKER_03:
Okay, great question.

So we never really brought a model together, but we explored it, but it was 2021.

This was before like PyMDP and RSFER and all this, but it was like NetHack.

It's like this text-based explorer that's famously convoluted.

But even just considering like, yeah, how do you do items as affordances, but then do every turn...

Are you going to do like this mega deep act info on like every possible spell that you could cast in every direction?

Or how are we even going to learn to know possibly things that even people haven't written down?

Like how two spells interact with the dragon only on the 13th level or something.

It's like, so this could be a very interesting setting.

Again, it's, it's, it's hardest in general, but it's,

the the reinforcement learning a reward approach let's just say that it's like um you know kind of this surrealist uh dream grid world with the keys and the doors and all this other stuff I mean there's the physiological sort of grounded layer

which like what makes more sense saying I'm rewarded by being at physiology or I expect to be at physiology and that's just where I satisfy so let's just say physiology can be equally well explained by active or by a reward term um then it's like well where do you go then with your reward

then it's like if you set your reward arbitrary function on curiosity learning epistemic value then you're doing function approximation for epistemic value so then you've just created a sort of epistemic approximator rl agent there's nothing wrong with that it might be totally fine but um it's it's the low road side of a question that could be approached from a high road

which would just be the epistemic value expected information gain, things that might be important for the NetHack challenge.

But then how would you come to tune and adjust the epistemic and pragmatic balance on the fly?

It isn't that, you know, like, again, thinking about the RL and the ACT-IMP, it's like, let's just say there's a basketball hoop that's 20 feet tall.

And someone's like, I prefer myself to be dunking.

Or the reinforcement learning is just like, I want to jump higher and then I'll be able to dunk.

But it's like, that doesn't mean it's possible to even do it.

but the reinforcement learner would train to jump up to 10 feet and then they would just stall and they just would be like you know I think I could do better or I'm at the best score I've ever gotten or something the active would just have a a would reach a Plateau of surprise like I'm why am I three bits surprised about my jumping when I expect to be doing a 20-foot Dunk

But if it's an irresolvable situation, then it's been set up that it can't be done.

Whereas if it was an achievable situation, then both of those would have a possibility to be able to dunk.

But at least leaving the door open for the epistemic and having a stabilized open door

that isn't just pragmatic because if you operationalize the epistemic value into pragmatic value okay we train the reinforcement learning on learning then we get back to these questions right that we're having at the very beginning like well then it would learn at cost to other variables oh no well then we'll learn to balance that okay

But then you can just have this infinite like regress of reinforcement learning, but it still is in pragmatic value from the high road side.


SPEAKER_00:
Right.

I think I'm just having a hard time understanding exactly.

There was this paper once about how you could teach agents to learn to ride a bike just by trying to maximize empowerment.

So trying to make it so that these agents would not have a reward for having the bike upright, but would have a reward for

being able to do more things in the world.

And since they had the bike upright, they could drive further and go more places.

And because of that, they could do more things.

And I'm wondering if the same kind of thing happens in active inference, where if you had an agent in a grid with

a place where a key comes out if it would just go up and see that a key comes every 10 seconds and think that that's nice and now it's learned that and there's no more surprise or if it would somehow be incentivized to say oh but what happens if i take this key and go down into the bottom of the maze where there's this door even though it didn't know anything about that that that would open anything or


SPEAKER_03:
yeah this um ali was there for this stream with tom ringstrom it's it's a it's a totally viable path um it's interesting also about the bellman equations which come up in the mdp solutions so treating an appeal mdp like an mdp

which is a using Bellman optimality PDP MDP style on the agents PMDP um there might be situations where where expected free energy and optimal and empowerment are are like correlated or there might be um designable situations where they're differentiable

I think one important note and it's kind of the separation between the equation 2.5 and 2.6 is like 2.5 variational free energy is not prospective this is just like a real-time existence check so this is more like a a through the flow of the present

once you get into planning over expected Futures like there's expected free energy then there's free energy of the expected future f-e-e-f there then there's other approaches that have been proposed so I think there's a lot of constructions that highlight different parts of the future to say nothing of the of the procedural ways that you could like prune through Futures

but there might be the heuristic of maximum occupancy or maximal empowerments or maximal time discounted reward for some agent or time discounted learning or like the amount of this or that within a certain timeframe.

So I think there's a lot more ways to model the future.


SPEAKER_00:
Thank you so much.

Yeah, I'll definitely check out this paper.


SPEAKER_03:
Yeah, this it's a great connection, though.

And I think it's a similar, there might be other intrinsic motivators like, like, you know, would learning would wanting to learn a space equate to occupying it through time equate to empowerment in movement in that space?

Again, you could imagine a space in an agent where those are all indistinguishable.

But then they could have some other space where just like, oh, but the cells change at different rates.

So if you are equally occupying the space, then you're not equally learning it.

But then that's just getting into these more generic comparators.

But I mean, these are great questions, and these are all weaving together in a Chapter 9-esque way.

As we think about generating data from Act-Inf models, fitting Act-Inf models to data, I mean, if we can...

share and borrow infrastructurally with Dreamer and all of these mainstream machine learning models and contextualize it from a high road and add new paths on the low road.

Just a few memes away.


SPEAKER_01:
Okay.


SPEAKER_03:
Thank you all.

See you next time.


SPEAKER_01:
Thank you so much.


SPEAKER_03:
Bye.
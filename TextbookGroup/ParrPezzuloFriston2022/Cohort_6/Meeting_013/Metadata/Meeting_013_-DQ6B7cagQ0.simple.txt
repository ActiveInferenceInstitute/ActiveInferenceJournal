SPEAKER_01:
all right welcome back welcome back to cohort six couple quick updates as we mentioned with octopus and others there are math discussion sessions they're tuesdays at 12 utc octopus session videos has the links to the videos

so feel free um these should be awesome freewheeling and super epistemic sessions and as with the material on the book itself all kinds of questions from the most background and basic and confirmatory for one on through the most speculative and and farsighted for for others

It's always a contribution to write it down.

And as the math comes into play and the meta math and the math art, it's going to be a great journey together.

Today, we're in our second pass on chapter six, which is the recipe for active inference modeling.

And also a new or different feature in this interval of activity compared to previous weeks is the third week rhythm.

So in the third week for cohort six, going through the second half of the book, we'll

really focus on applying active inference.

Andrew has written a tutorial utilizing PyMDP with a focus on agent-based modeling for the social sciences.

So that should be pretty cool.

And additionally, as Rx and Fur development continues, we'll be learning more and more about that.

In cohort seven, we haven't quite determined what the third week activity will be.

in that uh slot so we'll find out uh discuss later today with that cohort and and see what's useful in the third week for now though let's uh jump over to chapter six we can start with any section of six any idea or page or figure in chapter six that anybody wants to bring up

we could look at any of the prior questions and also as chapter six is about applying and designing active inference models it can be any question that someone has so just raise your hand or write it in the chat or unmute and go for it

I'll just give one brief catch up to get us to six, to get us there in just a minute.

And then again, just type a question or raise your hand.

So part one of the book, they outline as kind of the epistemic and the background and learning.

Part two is heading kind of home stretch, heading downhill, heading back home into the applying.

Chapter one and chapter 10 are like bookends.

They're very similar chapters.

Chapter 10 is a bit more broad ranging and longer, whereas chapter one introduces the ideas, whereas they're similar because they provide perspectives.

In chapter one, this concept of the low road and the high road is introduced, and that's revisited in more detail in chapters two and three.

Low road is like the how and the what.

It's the actual architectures that are built.

It's the way that Bayes' theorem applies to perception and cognition and action in a unified cognitive model.

And then the high road comes from the free energy principle, physics of cognitive systems, Bayesian mechanics, the imperative to persist, to remeasure, and so on.

chapter two and three get us to active inference and chapter four is where we actually see the active inference generative model itself which is what's going to be given the recipe for in chapter six chapter four goes through the generative model describes some of its analytical properties and it introduces us to the idea that there's a discrete time and a continuous time form

chapter five message passing in neurobiology gives a quick review of one of the systems of interest that has received the most study to date in terms of active inference modeling which is like mammalian and human neuroanatomy that's part one of the book

Part two of the book goes into applying active inference.

Chapter six is very provocative and fun, and it's a recipe for designing active inference models.

And as we'll, I hope, unpack a little bit today, it doesn't require some

advanced level of programming or anything like that to get going and actually with augmented coding with templates and some other tools there's a lot that can be done just through a conversation with a domain expertise or familiarity in the active inference ontology chapter seven and eight that are going to focus on some of the features and examples of discrete time continuous time models

chapter nine introduces data into the picture how do empirical data relate to generative models and then again chapter 10 is a summary so that's the textbook it's it's a long pull and then when we kind of pull back to this level of course screening it looks really short too so what's a section or a question on six that that anybody would like to go to


SPEAKER_00:
Okay.


SPEAKER_01:
We'll just go to the questions that have been sorted by the thumbs up.

You can click on the symbol to follow wherever a person is in CODA, and I'm putting it in the chat as well.

So if you want to, you can hit thumbs up for the questions that are interesting, or again, just like type it or raise your hand.

Okay.

First question, what are the four steps in the recipe to construct an active inference model?

Does anyone want to give a thought on this?

How are these steps similar or different to any other kind of systems modeling or analysis technique that people have seen?

Okay.

John.

Okay, let's look at figure 6.1.

John, go for it if you want to.


SPEAKER_02:
Yeah, yeah, sorry.

I had to set up properly there.

I'm dealing with something at work right now, so I'm going to be in and out on this.

But a question I've kind of had come up, and maybe this is just from not seeing too many active inference agents implemented yet, is sort of how do you fit expected free energy in with free energy minimization?

Like for the current state,

versus looking into the future.

It's not something I saw detailed a ton in chapter six.

But yeah, and I guess answering the question you posed, I think that like looking at our allies, we don't have, you know, reward based policies, but we just focus on minimization for energy, expected for energy.

But yeah, like,

It's a little unclear to me how those two interplay, what their interplay is in implementation.


SPEAKER_01:
Great question.

Yeah, it's going to touch on a lot.

So if anybody wants to add in or go in deeper into some of this, please just write it or raise your hand.

Otherwise, let's get there.

So the question is,

How do we think about free energy minimization?

So the real-time free energy minimization with equation 2.5 and the prospective expected free energy minimization in equation 2.6.

So how do those pieces come together in an active inference generative model?

Okay, let's start with figure 6.1.

This is called the particular partition.

It's a pun.

Particular means like that specific one.

Also, particular means related to a particle.

And the agent or the cognitive particle

is consisting of the internal states and the blanket states so everything to the exclusion of the external states is called the particle blanket plus internal equals particle so it's like the brain and the skin so to speak that's figure separated from ground that's the agent separated from the niche

that's called the particular partition and the free energy principle makes this kind of axiomatic starting point that we're going to partition systems according to a blank and the the two directions there so this is kind of the the partition on what we're looking at okay now how is free energy minimized in in real time and where does planning and prospection come into this

have some external state let's say it's the actual temperature of the room sensory states coming in that's like thermometer readings internal states like internal representations of temperature and then action states which might be like to modify the air conditioner that's kind of the setup of the model david

uh yeah I was just wondering how uh discrete time would play in the um exchange between the different states in terms of being able to model fit for example yeah great question um the way it comes into play in discrete time we think about each of these nodes having like a number and then there's a discrete tick of the model and like all the numbers update in a tick

in continuous time these four states are thought of as being like four simultaneous flows so it's like the difference between like four discreetly updating synchronized changes and like four analog control knobs that are kind of always giving a value and then they can change independently okay so now let's look at the recipe then we'll get to equation 2.5 and 2.6

the recipe first we identify the system that we're modeling this is the cognitive particle that's that's or it's really the cognitive particle plus niche it's the total setting and then question three is basically how are we going to set up the particle

in terms of the blanket and the internal states.

And question four is, how are we going to set up the generative process in terms of the niche?

So these four questions basically just get us to the point of specifying the agent, including blanket, and the niche.

Okay.

So how does framing things in that way relate to the real-time inference and to prospection?

part of the answer is that it depends a lot on the system that we're discussing so it's kind of sometimes frustratingly general in general but it's because it depends so much on the system that we're modeling so here we see a slightly different representation of the particular partition also of course slightly different notation Etc and this is like a continuum from simpler systems on through systems that have

introspection or it can have planning.

But the overall scheme is very similar.

Okay.

So John asked, where does expected free energy fit in?

Let's look at the equations for it.

And then we can look at the PI MDP documentation to see what it looks like.

equation 2.5 is the variational free energy variational free energy it's a functional so it's a function that takes in a function and what it takes in is Q a belief distribution and Y a new observation data coming in so this is like the agent's belief

about temperature distribution that could be a super tight distribution very loose distribution and then Y is a data point and here are three equivalent ways to frame this or to decompose the same quantity the top uh line is using a physics-based ontology of energy minus entropy

The middle is using a statistical ontology of complexity minus accuracy, which is to say we want to reward models for fitting well.

However, we want to penalize them for having more parameters.

That's also known as the IKIKI information criterion or the Bayes information criterion, basically.

in that adding more parameters will always fit the data better so the imperative can't simply be fit the data better otherwise you take that to the end of the road and you just have this massively overfit model so the question is how do you penalize a model for its growth while rewarding it for fitting well and that's the kind of accuracy minus complexity uh trade-off frontier and then there's a divergence in evidence

representation these are not look-aheads this just relates to beliefs at a time point and an incoming data point that's because variational free energy is attractable optimizable bound on surprisal surprisal itself is often difficult to compute because it it can be a large computation

However, variational free energy is this tractable bound.

This is not introduced or invented by octave inference.

Variational autoencoders, any kind of variational inference uses the exact same approach.

It's known as the evidence lower bound or the variational lower bound.

So if you can't necessarily fit the likelihood directly to maximize evidence, which is minimizing surprise, there's this tractable bound.

So this is not something that is being introduced by the active inference literature, but rather variational methods, variational approximations are being applied to the whole iguana, to a unified model of perception, cognition, and action.

but equation 2.5 doesn't have any planet it's just about beliefs at a given moment and then what is the sort of base optimal move given this new observation David go for it oh yeah I was just wondering if the uh part of the entropy formula that was related to uh Heisenberg does anyone have a thought on that


SPEAKER_00:
You mean the... I'm wondering, I don't understand the question.

Sorry.


SPEAKER_01:
Yeah, did you mean like Heisenberg uncertainty or what?


SPEAKER_00:
Yeah, the uncertainty principle.

So in terms of the functions itself related to Shannon Entropies.


SPEAKER_01:
Yeah, let's look into it more.

there there may be some connections yeah just it does deal with um fast warrior transform so I was just thinking in terms of um agent modeling cool yeah interesting possibility the 2022 textbook doesn't go into the Quantum uh interpretations however maybe maybe we can look more into it okay so this is great for modeling

a boundable approximation yeah go for it david oh sorry oh sorry i was falling down boundable approximation for the situation of beliefs and a new data point coming in so that can apply to trivially simple systems like that have no reactivity or sentience at all or kind of

classical or conservative systems like um you know a given data point comes in and just it just makes a linear adjustment in a belief it also describes that real-time flow on more complex systems however it doesn't invoke any future time points this is just kind of like the slice of the now so when we were interested in modeling agents with planning that is when we move into the expected free energy

expected free energy is a cousin of variational free energy it's g it's also functional um instead of f it's like the one after f g its um argument that it takes is pi pi is the policy prior so it's a vector of all of the possible policies

Affordances are actions that can be taken in a moment, up, down, left, right.

Policies are sequences of affordances for a given time horizon.

So for a time horizon of one, the policy vector is the same as the affordances.

For a time horizon of two, the policy vector is of the length affordances squared.

so on so it's like up down left right are the affordances those are the opportunities for action in the niche policies could be just one one time step if there was no planning just action selection or policies could be longer like up up down up up right um so that's about the defined time horizon for the model which we'll look at in pi mdp um after this

g takes in the policy prior and it's going to update it according to how well these policies score on expected free energy what gives a good score for expected free energy here we see several decompositions there's kind of like three and a half decompositions because the the second and third line are really similar

first decomposition is in terms of epistemic value expected information gain and pragmatic value or utility there's a lot more to say on this just just going quickly but pragmatic value in terms of utility is the expected alignment between future observations y tilde and preferences their preferred observations

Whereas epistemic value is about the KL divergence between double line, beliefs about hidden states through time, Q on X tilde, conditioned upon policy either way, and observations.

So if the observations coming in are not expected to update beliefs, these two are the same and the KL divergence is zero.

information gain is zero if expected observations coming in are expected to update beliefs like I want to know what's on page 12 of the book what about the policy to look on page 12 of the book I expect those observations to give me information even if I don't know what direction the information is going to be updated in and that is assigned a high epistemic value

This is a point of differentiation with reinforcement or utility based models, which is those kinds of models only have pragmatic value or utility.

So they need to discover these kinds of tricks and methods to give a utility to information gain.

there's all kinds of tricks you can have heuristics like mix it up every 500 times steps or you could say give this price on information put this price on basic research whereas here they're kept separate um and there's some other representations but what's important here is whereas 2.5 was about our beliefs and then the data point coming in in a moment

expected free energy is arbitrarily forward looking for a given time horizon so a good example of of where this comes into play is in this um active inference epistemic training demo I'll put it in the in the chat as well and probably uh linked elsewhere um okay

Well, since we're in 6 and actually looking at how these models are coming into play, let's look at this script.

But again, just raise your hand or type it if you have any thoughts.

And also note, this is the PyMDP Python kind of script.

So RxInfer is quite different in its style, but this is what PyMDP looks like.

First, like many other Python scripts, different packages are imported.

Those can be core packages or other broad packages like NumPy.

And it also includes specific functionality from elsewhere in the PyMDP package.

So we can kind of read this in light of the recipe.

So just to remind yourself.

System of interest, like situation of interest.

What does the agent look like in terms of its perception, cognition, internal states, and action?

And then what does the environment look like?

So it's not going to be in that order in the script, but we'll see that all those boxes will get ticked.

Okay.

First, they set up the map.

So this is like declaring the generative process.

and it's this grid world where there are certain cells that have rewards or it could be just called food and then there's some that have cues which are like signals here they use matplotlib to visualize the grid so in in this world

there's two possible places where there could be food or not so it's kind of like a tea maze however the reason why it's called epistemic chaining is that there's different places where information can be found and that information reduces uncertainty about where the food is so that's the generative process they should call it grid world now we get to generative model um okay

We're in discrete time here.

Pi MDP is only in discrete time.

Here's figure 7.1.

So a little looking ahead to discrete time model.

This is the core sense-making motif.

This is what makes something partially observable, is we have observations coming in.

At each time point, the observations are mapped through A to hidden state S. B describes how hidden states transition through time at each time step.

And D is the prior just to kick off the chain.

So in the neuroimaging setting, like SPM, observations are the noisy sensors.

And then S are these latent activity patterns in the brain.

That's partially observable inference.

That's like a Kalman filter.

Active inference is going to drop in on top and add this upstairs bit.

But the downstairs is exactly the same.

That's the sense-making motif of observations kind of getting mapped into latent states.

Where the action comes into play is when policies intervene in how things change.

So we have observations like thermometer readings coming in at each time step, and they're getting mapped to beliefs about temperature in the room.

And then B is a matrix that is going to apply to the hidden state and fold it into the next time step.

So then B has these slices.

it's kind of like index cards and policy selection by the agent is like picking which index card is going to be applied to unroll into the next state so one of the index cards might be keeping something like just in the location setting keep something where it is no move then that would keep S the same

Whereas there might be turn on the air conditioner, move left, and then those would be represented in the structure of slices of B. David.


SPEAKER_00:
Uh, yeah, I was just wondering how this works in terms of, uh, weights and values for, um, each portion, because you had mentioned, um, with activate, uh, being applied at G as a bit, would that be considered, um, adding any data weight to, uh, to the formulas?


SPEAKER_01:
okay yeah good good question so again downstairs is our like perception and sense making here upstairs is our policy selection

Applying variational inference, evidence lower bound, variational free energy to the downstairs sense-making part is like a variational autoencoder.

The move in active inference is to say agents are described by the joint distribution of their beliefs about perception, cognition, and action.

then we're going to take a variational free energy on all of that so that's the unified modeling angle that is um what unifies like the perception inbound signal processing problem with the outbound control theory problem so then the question is like well okay how do you this is a structural model how do you get at the weights let's see how they do it in the demo okay so

here they're setting up the size of frank here they're setting up the size and the dimensionality of observations which are called obs and hidden state factors which are called num states so an important thing to to note is that um this is like the schema

or the sort of template motif.

However, let's just say that each grid cell, each cell in the grid had like a temperature, a smell, and a light variable.

There's modular choice in how that is represented.

You have three observations coming in, and then you could imagine like those all project onto one hidden state.

type or you could set it up so that like there was a hidden state for each of those sensory modalities and then those were or weren't integrated so this is like map territory this is why Toby St Clair Smith calls this work compositional cognitive cartography compositional because we're playing with Legos and category Theory and we're composing models

cognitive because we're talking about cognitive interfaces and things perception cognition action and then cartography because it's a science or a study of map making so this isn't to say that like if you broke open an animal you'd find this base graph it's to say that this is a map like a subway map that we're making that has all of these really tractable properties

so there's a lot of places for modeler degrees of freedom and there isn't necessarily like a correct map of a cognitive system any more than there would be a correct map of of a city here they're setting up the variables just in terms of their length

without going too much into Rx infer at this moment, one of the major challenges slash frustrations with PyMDP and with modeling is the dimensionality and making sure all the dimensionalities work out.

Otherwise you get like just a bunch of matrices that get multiplied and the shapes don't work and it just kind of like gives bugs.

next they're going to state a a maps between observations and hidden states so this is the downstairs sense making part the a matrix they make sure that its shape in terms of rows and columns is basically related to on one dimension the number of hidden states and on the other dimension the number of the type of observations

So you could have five mapping onto five, or you could have one mapping onto five, or you could have any combination.

Then they fill out all zeros just to get it going.

Here, there's three modalities on A. So A is a tensor.

Each slice goes from observations to hidden states within like one frame.

So you can think of it like three index cards or like three sheets in an Excel file or a tensor that has three stacks.

A0 is location.

a1 is q1 a2 is q2 this is zero indexing so it's kind of like the first element is the zero just like a birthday and so the numbers are being filled in and then there's this um third third modality a3 which is the observation of reward or not so

in how these matrices are built is where a lot of the nitty gritty happens in A. Okay, now they move to specifying B. B is the transition model.

So this is how do those hidden states change through time?

Again, there's the work of making sure that the sizes and the shapes of all the variables line up.

And then

all of the work of like what does action do all the action representation is loaded into b and it's like when you see the model it's like there's actually not many places that that uh again in this simple form there's not many places where learning can go but it's important to see that a is action independence

A is the mapping from observations to hidden states.

That can be a recognition direction, recognizing, or it can be a generating direction like synthetic data.

All of the efficacy of pi, all of the efficacy and the differences that make a difference with policy selection influence how things change through B. So here's where we see the actions having an impact

in terms of different b slices that can get pulled out in the b0 which is um location just like a0 was location we have we have efficacy of actions actions should change our location however there's no action that the mouse can take to change the cues

And so here, the B variables in that first and second modality are called uncontrollable because there's no policy that makes a difference for those modalities.

Preferences are specified.

We can talk about preferences, but they play the role in control.

contributing to pragmatic value calculations if you don't have any uh preference or you have a flat preference like outcomes are equally desirable then epistemic value or learning or novelty comes to dominate the behavior whereas conversely if you have hyper precise priors there's no epistemic opportunities and strong preferences you get the special case of utility maximization

then the prior over D is specified.

So that was A, B, C, D and E. It was just framed in terms of the actions, basically.

Any thoughts on this?

Okay.

Just like in the recipe, they then turn to the rules of the game.

drawing inspiration from machine learning and reinforcement learning all this they use an env for environment class which steps in discrete time one by one so we defined the agent in terms of a b c d and then they write this grid world environment example which is kind of like um

We can look in more detail, but this actually is the consequences of action.

Whereas before, when we were defining the agent, this is its beliefs about action, what that will do.

And then here are the actual consequences of choosing actions.

So this begets a question like, well, how does the agent come to know

the structure the consequences of its action that's like not addressed in this notebook but like it's like playing a video game and then like you're um you were used to having up on the joystick move you forward but then now you're in a inverted mode where it moves you backwards so just because the agent is specified to have a given beliefs like oh if I hit this button the room will get colder

But that's not necessarily a veridical belief.

But that's what the modeler is.

If they're interested in this question of how do agents come to learn the consequences of their action, maybe you'd set up a different model than I'm studying agents who already know the consequences of their action.

It's just like, what are you trying to ask?

Who's the model for?

All those early questions of chapter six.


UNKNOWN:
Okay.


SPEAKER_01:
All right.

Now we get to the active inference part.

initialize an agent and an instance of grid world again this is like chapter six we defined the the situation of interest then made the agent ABCD alphabet and then defined the environment the generative process grid world M

my agent is specified and here the arguments it's taking looks a little bit like redundant, but that gives you like a degree of freedom in specifying the agent.

So you could say, I want A and then I wanna customize it.

And so this is just a wrapper method that bundles together

a b c and d and policy length so that's where the expected free energy is going to come into play because at each time step it's going to compa compute variational free energy but the policy length is used for the expected free energy so in this situation you have five affordances up down left right and stay and there's four time four four time steps is the time horizon of consideration

So that means the policy vector is going to be five times five times five times five, and it will sum to one.

Here's another point of difference to emphasize with reinforcement learning.

So that's called the policy space.

Pi is the space of all policies that can be selected from.

In reinforcement learning or reward learning,

There's another function proposed called the utility or the reward, and it is going to give a value to each of those points in policy space.

So each policy is going to be given a value, and then that is going to rank the policies.

And then you could choose the most rewarding, most valuable policy, or you could probabilistically select from policies according to their expected value.

In active inference, there's no proposal of this auxiliary reward function.

We have our policy prior, which is like our habit.

And then whether we apply G or not, which updates to the policy posterior, that policy variable is directly sampled from.

So policies are selected based upon their likelihood.

either they're the likeliest thing that would happen so we just pull from the top of the list that's kind of like the most rewarding policy but rather than moving through this intermediary auxiliary function we're picking directly either the best policy the likeliest policy or probabilistically choosing from the list that's the whole physics of um decision making angle is

there's no secondary reward assessment it's just updating the policy variable and then choosing from it okay spin up the environment and reset it just clean up the environment now here's the actual active inference Loop so it's cool because python has this semi pseudo Cody feel

history of locations is just creating a trace or a log variable and the number of time steps is specified it's 10 time steps so it's going to run for 10 steps but the agent's window is four so while we're within the range of steps this is like where we get to kind of the uda like observe orient aside act because we're actually going to see procedurally what happens within the active inference Loop

first q of s that was um q of s is q of x q of hidden states so here it's it's taking in observations and it's inferring hidden states that's q of s

then there's policy inference so this is sense making that's the downstairs and then there's control that's the upstairs and then after updating the policy prior into the policy posterior the actions are selected the movements the the chosen action is is kind of locked in

So even though there was a probability distribution over the five options for movements, eventually one of them selected, and this just clarifies, yeah, which integer was it?

Was it 0, 1, 2, 3, or 4?

And then the implications of that action occur.

This prints a line to the terminal just to log what happened.

Then the environment takes a step predicated upon the choice of the action of the agents.

And then there's just an updating of some of the observations and the history gets appended too.

So this is what it looks like.

Visualization method.

So here, the agent starts here.

It comes up, or it starts in the top left.

It steps here to determine which of the Ls have the queue that's informative.


SPEAKER_00:
it navigates to there and then that told it that the bottom spot was the one that would have the preferred observation David so in terms of modeling the map itself and updating the agent priors for the expectations is there like a library call for a different environment or is it going to be like a different kind of data store for example


SPEAKER_01:
Yeah, there's a lot of ways to do this, like in PyMDP, RxInfer, et cetera.

One really useful articulation is right here.

So let's just say that the final agent method is going to take on A, B, C, D, E. One convenient approach is have like a bunch of A, like a bunch of A alternatives that you generate, you know, A, you know,

a noisy and then and then you make an agent constructor and let's say you have 10 options for a matrices and 10 options for b matrices then you make this a wrapper method that says okay now spin me up all 100 agents all 10 by 10 combinations of agents who have a matrix 1 through 10 and b matrix 1 through 10. and then using these other kind of methods like visualization and parameter sweeps

then you use those sorts of methods to sweep across combinations more modifications here they move the two cues to here or the two rewards here um so it's it's it's an interesting experience but that's what chapter six is like

to see ultimately there's more lines of code in this related to just the visualization methods sometimes it's it's quite curt and concise what is actually being specified so it's not like we're making mega sized models or needing to tinker with a wrench on a billion parameters

That's one of the interpretability elements of active inference.

So that was the output of the recipe.

Here's the recipe to follow.

just at a qualitative structural level that may have a lot of similarities with other systems modeling techniques people have learned about like from complexity science or systems dynamics or other other strategies of modeling and then we take the situation of interest and then basically decompose it

according to the particular partition that's what grounds this all under the free energy principle and then more specifically using the active ontology so that we know that we've gotten like the a the b the c and the d the observation and the hidden state with those in place you just write them in using language models or code models to help and then

the other functions are shared across models.

In our last bit, what are any thoughts or questions on that?

what they call a recipe here this is like more like the OODA loop this is just getting into the procedure of what happens at each time step but also it's like just like if the procedure for the recipe was a cup of flour a cup of sugar Etc and you had all those ingredients ready

you'd be able to make the recipe so the kind of functional food we're making is an agent that's able to do perception cognition action so if we have all these ingredients we have all of the precursor materials to do this observation sampled from the environment move that observation into the hidden states

through the A matrix, through variational free energy minimization.

What should I believe about temperature given this thermometer observation and my prior?

Equation 2.5.

Calculate the expected free energy of actions.

Equation 2.6.

Policy prior.

Rewate the policy prior according to expected free energy, epistemic plus pragmatic value.

Get the policy posterior.

Sample from it.

or just pick the best one or sample probabilistically from it, reenter that sampled action into the generative process, that's it.


SPEAKER_00:
Dan, is there a similar recipe book in terms of Rxinf?


SPEAKER_01:
we uh want to be building them making an rx infer model um so copied in like the same recipe should apply okay like chapter six is that a generality of like

prepare the main course prepare the drinks set the table but then of course it's all situational what's the main course what's the drink what's what's the main table um but uh we have Google collab notebooks drawing from kobus's code examples at learnable Loop so if if and I'll put these these links in the chat too so the here you can um jump in to uh uh

coding notebook and then like get all the way on through to to seeing it in rx infer like here it is with running a drone um there's some pretty major structural differences between time dp and rx infer in terms of how you set the table specify the main course etc um

And if people want to learn more, they could explore that document.

And then also, we're working to have all of the textbook examples for the textbook in RxInfer.

So let's see where 1.2 is.

Okay, figure 2.1.

So this is not a case of active inference.

This is just a passive inference.

There's beliefs about frogs and apples, like how prevalent they are in the world.

That's the prior beliefs about hidden states.

Then there's a likelihood model.

which is like the A matrix.

And that is a mapping.

It's a belief about a mapping, about jumping and not jumping and how they're related to frogs and apples.

So it's kind of a two by two shown here with these two bars, like apples jump 1% of the time, frogs jump 81% of the time.

Before we saw the observation, we believe that 10% of the items in the world were frogs.

We observed a jump,

and then we basically multiplied this shape by this shape and it turns out that a jump is 81 times more likely to have come from a frog than an apple just 81 versus one percent however we believe that apples were nine times more likely so um something was it was it was 9x down

in in the um odds and then we observe something that by 81 fold made it more likely so it went from 9x down to 9x up so that's the bayesian update and then this is what it looks like in rx infer so just really briefly here's installing julia so it's not related to rx infer

Here's installing, just showing, okay, have we installed Julia?

Here's installing the RxInfer package and other needed tools.

Frog or Apple.

It's a categorical distribution with 0.9 and 0.1.

Right here, 0.9, 0.1.

Here's the T matrix.

That's what it's just being called here, but the variable names don't matter.

It's what they are.

And then there's this observation that takes in those two variables and updates based upon observation.

And then in RxInfer, it just calls, you just call it and you say the data were surely a jump.

And then you return the posteriors.

So it's a different style and it's one that many of us are still learning.

However, when we have well-documented examples of everything in the textbook in RxInfer, it'll bring a lot of continuity between the kind of engineering grade application framework and the learning path.

So for anybody who wants to work in that way, it's super helpful.

Okay, I hope these have been some useful pieces for people.

Any last thoughts or questions on this?


SPEAKER_02:
I don't want to go over time here too much or anything,

One thing I have been thinking about is how you actually distinguish between a continuous versus a discrete space or time in that there are these limitations on your inference procedure, all of that calculating your expected energy and making a decision, taking an action.

So you're kind of living in a discrete space that's dependent on your

when you're working in the continuous space, which to me also raises a question on like inferring a policy about your temporal breadth of like the now, which is something that we kind of do with adrenaline, right?

So maybe we, or at least it feels so,

Yeah, I don't know.

That's just another dimension of modeling.


SPEAKER_01:
You're right.

It's another dimension or layer.

And that's part of the fun and the open-endedness of having such a composable framework.

Temporal thickness or anticipation of future agency over consideration of time thickness.

That's not here.

But if you can dream it, you can do it.

so you could make those kinds of structural hypotheses explicit so whether it's making a map of a cognitive system that does act as if it has these properties or if you're just on a flight of fancy or a flight of utility and you're like could we make a policy variable controlling this and then you just add another node in an edge

crank it again so that's that that is the fun is saying okay then how would we do that if there was an agent that was observing its adrenaline levels and the adrenaline actual levels were modifying temporality this way maybe that's a map of nowhere maybe it's a useful map so that that is the whole question

and and that is why there's such a space between the fundamentals like the particular partition variational free energy expected free energy like you can't say too much more generically than those equations the whole point is those equations whether in real time or in the anticipation those equations apply to whatever graph is constructed

but that graph could be anything could be um a lattice of just ones and zeros could be a continuous automata so it's like it doesn't even have to resemble something agentic and like Maxwell said in Discord several weeks ago but but I think about it all the time he said the focus on agents has reduced the apparent scope of active inference

because agent as we know it is not anywhere necessarily here now one interface that's interesting is that like cybernetic perception action interface but just more generally we could talk about like fabrics of message passing or ecosystems of shared intelligence so there's kind of the fundamentals and then it's all about what you build and and how the models really play out in in their environment so

that's kind of fun and it allows the the scaffolding and the externalization of cognitive hypotheses and the consideration of alternate cognitive models but like the Lego set is not opinionated on the architecture

Right.

That makes sense to me.

Yeah.

Thank you for the answer.

Cool.

Yes.

Good question.

Okay.

So, um, if this time works for people, then next, uh,

week we'll return in the third week and we'll see if these are um in the cohort seven recorded or not but we'll see um who has a feeling on what these sessions could be about maybe we revisit octopus math group developments maybe we kind of look at recent updates look at philosophy look at other uh areas and then in um the third week for cohort six

especially with Andrew giving the tutorial at the workshop at some conference and RxInfer and everything.

We'll get down to it with really applying it, including everybody can make a copy of the Colab notebook.

So everybody can be running it.

And it really demystifies a lot when you see it run in your browser.

you see which variables are defined how do the variables interact what did i put into this what do i get out of it so it's cool okay thank you very much see you all next time thanks have a good one
[
  {
    "start": 2.242,
    "end": 28.333,
    "text": " all right welcome everyone from cohort six and two-thirds um we're in chapter seven discussion and then also in cohort seven we're in chapter two so we'll maybe explore a little bit of both of them and also i'll just note that the octopus math sessions are in full effect",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 28.786,
    "end": 31.75,
    "text": " They're Tuesdays, one hour earlier than this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 33.232,
    "end": 37.798,
    "text": "The Zoom link is here and they're doing really fun.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 38.279,
    "end": 46.109,
    "text": "The recordings are available and they're doing fun math education and Octopus is super knowledgeable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 46.249,
    "end": 56.903,
    "text": "So if anyone has questions from very basic background math to more speculative maths, I recommend they look at the recordings or go to those sessions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 58.03,
    "end": 59.272,
    "text": " All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 62.055,
    "end": 72.689,
    "text": "So on seven or two, if it's relevant, what's something that anyone would like to begin with?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 83.663,
    "end": 86.367,
    "text": "So for the discretization of time,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 86.65,
    "end": 92.416,
    "text": " I feel like there were a few things that I wanted to be a little bit more defined.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 92.936,
    "end": 108.331,
    "text": "Could you say a little bit about how you actually take this thing that the discretization of time or states seems like it could be broken up into smaller and smaller pieces, which kind of leads me to believe that there is some bundling that's going on behind the scenes.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 108.812,
    "end": 115.138,
    "text": "And it makes me feel like I want this discretization as well as the definition of time to be brought into in a little bit more depth.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 115.574,
    "end": 117.679,
    "text": " Could you say a little bit about your thoughts about that?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 119.383,
    "end": 120.486,
    "text": "Yes, great question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 121.408,
    "end": 127.643,
    "text": "So there's a few different settings where you might be applying a discrete time model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 127.743,
    "end": 132.334,
    "text": "First, let's just go to figure 4.3.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 132.888,
    "end": 136.495,
    "text": " kind of the Rosetta Stone with the discrete and the continuous time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 137.237,
    "end": 146.695,
    "text": "So this image highlights that with the same structural relationship amongst different variables,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 146.827,
    "end": 151.534,
    "text": " we can have a discrete time or a continuous time active inference setting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 151.694,
    "end": 153.537,
    "text": "So that's kind of handled chapter seven and eight.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 153.557,
    "end": 155.921,
    "text": "That's what a big part of the second part of the book is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 156.762,
    "end": 159.366,
    "text": "So how is time discretized?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 159.446,
    "end": 161.289,
    "text": "Is this a claim about real time?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 161.369,
    "end": 166.337,
    "text": "Is this like an ontological claim about time or how does it come into play instrumentally?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 166.317,
    "end": 182.52,
    "text": " So sometimes you may be interested in making an example where there actually is a discrete time counter, like it's a metronome or a clock or you're using minutes or hours.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 182.5,
    "end": 200.859,
    "text": " So again, whether or not you took a ontological position that time underlying the minutes was continuous, if you were measuring by the minute, whether you're doing trading or some other kind of measurement, you could still make a discrete time just by having some kind of click.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 202.22,
    "end": 208.086,
    "text": "In the mouse teammates example, it's even less accurate.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 208.285,
    "end": 232.622,
    "text": " problematic i think because there's no clock time wall time it's just action time so it's like action state step one and then an action is taken so that's kind of like chess time where it's discrete and there's not even a reference to a wall clock time so that's kind of the simplest setting is where there's no clock it's just like a chess game",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 232.822,
    "end": 244.915,
    "text": " Then the sort of intermediate setting is real time is happening, but you're modeling discrete units of time, like daily or hourly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 245.856,
    "end": 257.408,
    "text": "And then the sort of most open and challenging of the settings relatively is where you have a degree of freedom around how you discretize the time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 257.568,
    "end": 260.331,
    "text": "Like what should the time discretization be?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 260.868,
    "end": 267.518,
    "text": " And so there, that is basically getting into a parameter sweep over possible delta Ts.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 269.02,
    "end": 272.745,
    "text": "And then that gets into, okay, so now we have multiple possible models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 273.046,
    "end": 276.731,
    "text": "Some are ticking at a finer scale, some are ticking at a slower scale.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 276.771,
    "end": 288.428,
    "text": "And then do we get like more accuracy or interpretability or are the benefits and accuracy and interpretability worth the increased cost for a faster ticking model?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 310.396,
    "end": 312.639,
    "text": " So do you mind if I just try and reiterate this again?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 312.659,
    "end": 324.195,
    "text": "So the question was, how do we make the choice about the sort of time that we're going to be modeling in the discrete sense?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 324.515,
    "end": 332.085,
    "text": "So we're going to have either some absolute clock time or some purely ordinal sense of events following one another.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 332.526,
    "end": 334.108,
    "text": "And the question is, how do we choose between those?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 350.305,
    "end": 354.071,
    "text": " Because I think if that is the question, it would be very use case dependent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 354.492,
    "end": 363.047,
    "text": "So in something where there's a lot of interacting components, you're going to have to be aware of conflicts between events that can happen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 363.087,
    "end": 371.722,
    "text": "But if you're just modeling a mouse and a maze, maybe you can just have a global clock time and not have to worry about the clashes of events.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 371.742,
    "end": 373.765,
    "text": "That's the only reason I bring up that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 376.765,
    "end": 401.136,
    "text": " yeah good and yeah here there's no clock at all it's just it's time point one and then an action is taken and then it's time point two right sorry that's what i meant you know you don't need a clock because you don't have to worry about clashes so i suppose i suppose it would be use case dependent um the choice let's say",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 402.753,
    "end": 406.96,
    "text": " Like there's nothing a priori in active infants that would perhaps necessitate one choice or the other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 409.324,
    "end": 410.085,
    "text": "At least I don't see how.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 412.69,
    "end": 418.72,
    "text": "In the chapter six recipe, I mean, this is a... Yeah, go ahead.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 419.122,
    "end": 433.023,
    "text": " Well, I just mean, let's say you're trying to just decide which of those two kinds of times you want to have without the specific use case, that is to say, the specific example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 433.604,
    "end": 435.446,
    "text": "I'm trying to think.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 435.486,
    "end": 438.311,
    "text": "There's nothing that would mitigate in favor of one or the other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 438.431,
    "end": 440.654,
    "text": "It's very use case dependent, I think.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 441.475,
    "end": 443.298,
    "text": "I could be wrong, though.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 443.318,
    "end": 444.64,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 445.649,
    "end": 451.081,
    "text": " In the particulars, every single aspect of the generative model is system or use case specific.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 451.362,
    "end": 459.28,
    "text": "There's nothing that you could reduce your uncertainty about further without reference to a particular system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 462.007,
    "end": 462.628,
    "text": "Yeah, yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 464.785,
    "end": 470.131,
    "text": " Like even in the chapter six recipe, that's one of the questions is, you know, what form should the model have?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 470.171,
    "end": 477.139,
    "text": "Should we use discrete time, discrete state space, continuous time, continuous state space, or hybrids of them?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 479.082,
    "end": 487.131,
    "text": "So at the system independent scale, we're just learning the motifs and the possible moves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 487.448,
    "end": 509.136,
    "text": " then they're all then then they're selected from and composed in making generative model for a specific situation so like in chapter seven it starts off familiarizing with the discrete time partially observable downstairs sense making part of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 509.116,
    "end": 536.98,
    "text": " hidden state temperature in the room observation thermometer readings or in the in the chapter 7 case it's the true note that was meant to be played and then this is the observed note that's listened to d is your beginning prior on hidden state it just kicks off this markov chain and then after that it the the prior for the following hidden state is just the preceding hidden state",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 537.5,
    "end": 545.953,
    "text": " A maps from hidden states to observations, so it can be run forward in the so-called generative direction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 546.353,
    "end": 554.125,
    "text": "Again, it's a little confusing with the namespace because the whole thing is a generative model, but it's generating data, synthetic data.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 554.341,
    "end": 569.481,
    "text": " like generating observations conditioned upon the temperature of the room, or in the recognition direction, taking in observations from the thermometer and then making inference about what the likely hidden state of the room is,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 569.984,
    "end": 572.649,
    "text": " And then B is a transition operator.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 573.53,
    "end": 577.237,
    "text": "So S is hidden state at a given time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 577.798,
    "end": 582.245,
    "text": "B is the discrete time state update function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 582.786,
    "end": 585.15,
    "text": "Then you get hidden state at the next time point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 586.412,
    "end": 595.468,
    "text": "So this is kind of the downward facing E that ends up being the downstairs of the POMDP.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 598.182,
    "end": 605.599,
    "text": " So this highlights just the partially observable component, and then action is brought in later.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 606.862,
    "end": 609.087,
    "text": "And the example in chapter seven is listening to music.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 611.592,
    "end": 616.343,
    "text": "So we can look more at that example, or we can go anywhere or any question that someone prefers.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 620.474,
    "end": 624.808,
    "text": " I have a question, I suppose, maybe following on from that POMDP with action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 624.828,
    "end": 627.597,
    "text": "So I think that's figure 7.3, if I'm not mistaken.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 627.858,
    "end": 632.172,
    "text": "But if anyone else has something they want to jump in with, please don't let me stop you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 632.978,
    "end": 634.621,
    "text": " So 7.3 here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 635.642,
    "end": 637.325,
    "text": "There's a bit of a discussion about this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 637.645,
    "end": 638.807,
    "text": "We've introduced action now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 639.669,
    "end": 641.211,
    "text": "Well, we've introduced policies now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 641.892,
    "end": 642.653,
    "text": "Very nice.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 644.256,
    "end": 645.678,
    "text": "We need the EFE to score these policies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 646.379,
    "end": 654.953,
    "text": "The question that I have is about, and this is not really, I don't really see here in this chapter where it's kind of finessed, but",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 654.933,
    "end": 683.269,
    "text": " on the next page i don't know if that's relatively easy to do equation 7.4 they actually give the expect free energy um and of course the the you know within the various terms you've got pi your policy um just kind of um you know interpreting this pi pi is a sequence of actions so there is in my mind let's say you know i'm trying to think how do i model some particular um event or you know system",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 683.249,
    "end": 690.701,
    "text": " It's going to be kind of bad to have to try and form approximate posterior beliefs over sequences of actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 690.821,
    "end": 692.844,
    "text": "That's going to get quite massive very quickly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 693.385,
    "end": 704.683,
    "text": "You're going to have to form exponentially many approximate posterior beliefs for one more action that you add to your policy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 705.22,
    "end": 710.526,
    "text": " So there arises pretty quickly this necessity to deal with that problem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 711.667,
    "end": 728.005,
    "text": "And I'm just wondering if in the text or in general, if there is a relatively, let's say, kind of first pass way we can deal with this problem, the one that comes to mind",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 728.221,
    "end": 757.84,
    "text": " immediately is to just assume that the um the the approximate posterior over policies is you can just do a mean field thing so instead of having to do you know beliefs over things that are 16 times into the future you just say okay i'm going to assume that i can multiply the probability of action one versus action two versus action three and form posterior beliefs over that over you know individual actions and sort of just you know take the product um but nevertheless this this is something that is a little bit more of a general",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 758.005,
    "end": 784.57,
    "text": " uh query that i've had for a long time about the efe you know you have to to kind of you're sort of forced to deal with this problem of how to uh represent beliefs over policies because those policies can get so enormous very quickly so i'm sorry that's maybe not too uh well formed but alas that is my question if anyone wants to jump in yeah does anyone want to give any thoughts or or",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 785.427,
    "end": 788.876,
    "text": " Is it related to anything someone's thought or wondered?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 788.996,
    "end": 790.259,
    "text": "And then let's explore it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 792.685,
    "end": 799.302,
    "text": "Yeah, so I've actually seen something somewhat similar to this question recently.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 800.463,
    "end": 806.092,
    "text": " where, you know, like the naive way is to do Monte Carlo sampling, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 806.292,
    "end": 812.061,
    "text": "And you just, you're looking over, like you said, like every possibility in this really deep tree.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 812.822,
    "end": 821.515,
    "text": "And then there's Markov chain Monte Carlo sampling, where you're doing it very intelligently.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 821.535,
    "end": 825.702,
    "text": "And there's a lot, I'm just starting to read on these things, but there's a lot of depth to that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 825.862,
    "end": 828.546,
    "text": "And it serves as a basis to,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 829.673,
    "end": 834.52,
    "text": " sort of the alternative approach to probabilistic programming from RxInfer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 835.982,
    "end": 837.043,
    "text": "Yeah, it's quite different.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 837.604,
    "end": 854.768,
    "text": "Yeah, it's quite different, but it can... I guess just one use case I've seen it on is where you condition on each pass time step, and like with RxInfer, you can kind of go back, you can trace every single step through it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 855.689,
    "end": 857.031,
    "text": "So...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 857.146,
    "end": 858.969,
    "text": " just like a high level of this algorithm.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 858.989,
    "end": 861.673,
    "text": "You sample, you know, one step and you're like, okay, this is pretty good.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 862.354,
    "end": 865.199,
    "text": "And then based on you condition off that you sample another.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 865.339,
    "end": 869.686,
    "text": "And if you run into a problem, you go back to like the last good state and then you try a different direction.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 870.547,
    "end": 871.548,
    "text": "Um, that's the high level.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 871.568,
    "end": 874.373,
    "text": "There's a lot of other really smart optimizations on it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 874.874,
    "end": 880.943,
    "text": "Um, but it, it can allow for example, like very effective path finding, um,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 881.615,
    "end": 883.446,
    "text": " with minimal tree exploration.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 885.196,
    "end": 885.961,
    "text": "Yeah, yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 886.222,
    "end": 888.435,
    "text": "Oh yeah, go ahead.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 888.905,
    "end": 892.889,
    "text": " I was just going to say, yeah, that sounds really cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 892.949,
    "end": 897.974,
    "text": "I actually did my honors project on this problem last year, and I have a heuristic tree search thing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 898.615,
    "end": 907.043,
    "text": "But the pain associated with that is, oh, you still have to enumerate some amount of actions into the future for some amount of policies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 907.323,
    "end": 908.604,
    "text": "And that gets very painful very quickly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 910.146,
    "end": 915.351,
    "text": "So things like heuristic tree search, things like, I suppose, some kind of Monte Carlo something,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 915.331,
    "end": 939.499,
    "text": " um they're sort of the only things that i'm aware of uh for for doing this kind of sampling based thing if we're not going to do arcs and fur type stuff maybe we're getting a little bit far afield now but um this is look let's just let's connect it back to to the the basic problem and there's a variety of methods to to solve it so the fundamental issue is that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 939.479,
    "end": 965.976,
    "text": " if we have a given number of affordances per time step let's just say there's two like left or right like it's a branching tree then for a function of growing time horizon that you're considering there's an exponentially growing number of total policies because it's it's doubling so it's just like that's the problem of chess it's a problem of getting specific about planning",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 966.833,
    "end": 991.643,
    "text": " any other kind of control method so then there's some some uh structural methods that can ameliorate this so one thing is like you might do a parameter sweep or search like what time horizon do i really need to plan for like do you need to plan 50 moves in chess or what fraction of the the value can you get with five moves of planning or something",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 991.69,
    "end": 994.84,
    "text": " there's, especially in active inference, there's nested modeling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 995.543,
    "end": 998.051,
    "text": "So let's just say we were doing a move every minute.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1000.037,
    "end": 1003.067,
    "text": "We could, and we wanted to model 120",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1003.267,
    "end": 1021.95,
    "text": " moves in the future well that might be like a huge number of exponentially exploding minutes or maybe it's only two hours okay so then we have computational methods for approximating these complex um computations",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1021.93,
    "end": 1025.113,
    "text": " Because we're going to think of it as this kind of inference problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1025.774,
    "end": 1033.722,
    "text": "And just because you can write out the inference problem, it's like, okay, but now the observations, it's a 4K video, 60 frames per second.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1034.363,
    "end": 1046.115,
    "text": "And the hidden state is going to be like, you know, so just because you can write the equation simply doesn't mean like that there's enough computational resources on earth to calculate it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1046.332,
    "end": 1047.855,
    "text": " But the equation still might be very simple.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1048.437,
    "end": 1065.032,
    "text": "So then when that is happening, there are, as kind of mentioned, there's several approaches ranging from different sampling based approaches, which attempt to empirically approximate complex or large or high dimensional distributions by",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1065.012,
    "end": 1083.156,
    "text": " speckling all over them or taking smart paths through them and then looking to reconstruct the entire posterior so in this case we have like an action prior is what we're taking into this whole inference question we want to update that action prior",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1084.283,
    "end": 1109.498,
    "text": " we want to update the probabilities of taking different actions according to their expected free energy which is to say we want to like up rank actions policies um that contribute pragmatic value observing future observations with our preferences and and those that contribute epistemic value okay so that's this question a few few different ways to deal with it you can structurally deal with it",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1110.254,
    "end": 1112.316,
    "text": " with changing the structure-generative model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1112.837,
    "end": 1115.96,
    "text": "You can do sampling-based or variational inference-based approaches.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1117.541,
    "end": 1120.484,
    "text": "Let's also get specific.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1120.644,
    "end": 1125.749,
    "text": "Here's in PyMDP tutorial one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1127.251,
    "end": 1130.694,
    "text": "This is what it looks like to get the expected free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1131.675,
    "end": 1135.739,
    "text": "So this is like operationalizing an equation like this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1136.88,
    "end": 1138.422,
    "text": "And this is...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1140.629,
    "end": 1147.518,
    "text": " programmed in the PyMDP imperative style.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1148.479,
    "end": 1156.97,
    "text": "So it's going to give a sort of procedural code for how you calculate the expected free energy for each policy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1158.372,
    "end": 1167.784,
    "text": "What expected free energy does is it just takes in the policy vector and for each policy considered separately, it calculates this value.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1168.979,
    "end": 1170.381,
    "text": " So it's shown here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1171.522,
    "end": 1173.544,
    "text": "And there's other ways to compute it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1173.945,
    "end": 1196.691,
    "text": "There's other ways to write out the procedure to compute this, but this is that sort of imperative style of PyMDP, where calculations are described that take in certain inputs, combine them in certain ways, and result in certain outputs, in this case, like the expected free energy of moving left or right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1197.7,
    "end": 1223.033,
    "text": " um then there's the just more again general questions for machine learning about like uh policy rollouts branching time active inference this was described in several papers this is a method of heuristic tree search like rolling out certain areas of the tree um that's oh leave there",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1224.16,
    "end": 1245.848,
    "text": " But yes, expected free energy calculations can be having growing computational complexity because if you want to be dealing with something that's a large policy space over long times, then that's a pretty large challenge.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1247.667,
    "end": 1253.176,
    "text": " Yeah, it's frustrating to have such a nice, beautiful equation that quote-unquote solves it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1253.657,
    "end": 1260.808,
    "text": "And it's theoretically beautiful, but it's very, very difficult in practice for, I suppose, non-toy problems to actually do.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1262.331,
    "end": 1265.796,
    "text": "Anyway, that was my main concern.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1266.498,
    "end": 1274.13,
    "text": "Yeah, I mean, look at the computational resources and compare it with other methods.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1275.41,
    "end": 1284.369,
    "text": " Because it isn't that each of these expected free energy calculations are necessarily that resource intensive.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1284.97,
    "end": 1293.347,
    "text": "Also, they can be paralyzed in the dispatch because computing the G for policy one and policy two, those are separate.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1294.51,
    "end": 1296.574,
    "text": "So that's a highly paralyzable step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1297.718,
    "end": 1317.769,
    "text": " And again, it just depends on the overall, like if you had a simulation with a thousand nest mates making a single step of action, then the computational bottleneck in that simulation might not be, gee, if you were doing a single agent doing a thousand time steps of planning, maybe it would be.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1319.487,
    "end": 1326.42,
    "text": " Yeah, that's probably how we actually get around it is by having many agents kind of work together with relatively low timescales.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1326.62,
    "end": 1326.761,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1326.781,
    "end": 1327.783,
    "text": "Interesting to think about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1327.803,
    "end": 1328.284,
    "text": "Pretty, pretty cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1328.885,
    "end": 1329.546,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1329.646,
    "end": 1337.04,
    "text": "I mean, multi-agents and or nested models reduce the need for explicit planning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1338.134,
    "end": 1362.117,
    "text": " because they have fewer explicitly planned state or like in live stream 42 with the robot slam the simultaneous localization mapping the lower level model was the postural model so extensive planning wasn't needed there because it was like body posture then the higher level model was essentially the location of the warehouse",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1362.991,
    "end": 1385.505,
    "text": " so then again planning was rarely more than one or two nodes deep so yeah that's because you had that extra extra structure of this multi-layer uh model I suppose yeah yeah yeah whereas trying to plan posture to get across the warehouse would have seemed intractable",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1386.262,
    "end": 1386.883,
    "text": " Oh, awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1386.903,
    "end": 1387.544,
    "text": "Thank you, Harris.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1388.205,
    "end": 1389.047,
    "text": "Yes, exactly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1389.167,
    "end": 1389.347,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1389.387,
    "end": 1411.543,
    "text": "Then also, this is sort of, this would be the deeper, the deeper tech angle would be like, as discussed in some of the message passing and Magnus Kuhldahl's work in Livestream 55, generalized free energy encompasses variational and expected free energy, and it enables",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1411.878,
    "end": 1440.383,
    "text": " another kind of non-planning based way to talk about reducing uncertainty about policy in the future okay stockfish uses alpha beta pruning search algorithm alpha beta pruning improves minimax search by avoiding variations that will never be reached in optimal play because either player will be wrecked the game yeah that's that's a classic chess algorithm and you can implement in your in your script",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1440.835,
    "end": 1456.931,
    "text": " Um, any, any, you could say only evaluate G for the even policies or, um, any policy with three of the same of, um, action selected in a row, just remove it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1458.253,
    "end": 1463.678,
    "text": "So th there's th it's not like we're pro crusties and we're being forced to do a calculation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1464.819,
    "end": 1469.644,
    "text": "These are just ways that you can choose what you want to calculate.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1471.632,
    "end": 1474.436,
    "text": " But it also may be a lot clearer in a specific case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1478.402,
    "end": 1481.066,
    "text": "And each of these are just matrix operations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1481.606,
    "end": 1496.728,
    "text": "So again, if you're dealing with massive matrices that you can't coarse grain or discretize, so you have a large matrix and you want to have continuous approximations and et cetera, it's like, well, then you're setting up for a larger problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1497.181,
    "end": 1505.053,
    "text": " But if your matrices are discrete or sparse, then these are super fast calculations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1508.018,
    "end": 1514.609,
    "text": "And that's kind of the amazing thing is there isn't any need, this is again, the contrast with the reward learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1515.129,
    "end": 1525.045,
    "text": "There's no secondary like, well, let's calculate the reward associated with the policy, question mark, question mark, question mark, and then use that to update policy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1525.092,
    "end": 1538.065,
    "text": " It's like, no, just directly calculate these exact conditional probabilities that are written here, reweight your policy prior into the policy posterior, and then select from it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1540.608,
    "end": 1550.138,
    "text": "Or like shown in policy five, here's a more of a biological inspiration on that challenge with the dopamine balance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1552.615,
    "end": 1557.34,
    "text": " So here it's saying, well, this is sort of habit on the left.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1558.741,
    "end": 1561.184,
    "text": "Here, policy is being determined just by the habit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1561.965,
    "end": 1563.887,
    "text": "So E is the policy prior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1564.908,
    "end": 1567.791,
    "text": "And so this is the kind of like thinking fast system one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1568.492,
    "end": 1574.718,
    "text": "And then here's thinking slow system two, where there's a deliberative reweighting of policy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1576.14,
    "end": 1581.205,
    "text": "So then there's this second level question, which is when should I deliberate",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1581.995,
    "end": 1590.126,
    "text": " And if maybe only deliberation has to be engaged in a tiny fraction of cases and habits can be carried through or updated and then just reused.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1600.961,
    "end": 1602.163,
    "text": "Okay, chapter seven, so.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1603.561,
    "end": 1623.882,
    "text": " starts off again raise your hand or or write if you have a question but it starts off familiarizing with the partially observable markov model or just the hidden markov model just introducing okay we're putting some space with a matrix between the temperature in the room and the thermometer reading",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1623.997,
    "end": 1633.953,
    "text": " So if it were fully observable, like here's the location of the chess piece on the board, and this is the observation of the chess piece on the board, then it's a fully observable Markov process.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1635.315,
    "end": 1637.539,
    "text": "There's an example with music playing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1640.704,
    "end": 1643.288,
    "text": "Then action is brought in.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1644.179,
    "end": 1671.756,
    "text": " action is not intervening in the hidden state directly it's not it's not reaching into the hidden state of the room's temperature and modifying it it's more like selecting an index card from the b tensor and then selecting which index card of b is going to get applied to update the state so then it's like there's one matrix transition matrix that describes",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1671.736,
    "end": 1698.098,
    "text": " air conditioner on and there's one that describes heater on and then the air conditioner one maps like 25 24 maps 24 to 23 so it just drops all the temperatures down by one and the heater one maps all the temperatures up by one so what action does is select which slice of b is going to be used",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1698.145,
    "end": 1707.095,
    "text": " to multiply with the prior state of S to reach the current state of S. So then the question arises, well, how do you choose which policy to take?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1708.156,
    "end": 1725.115,
    "text": "And here you could just use habit, like use just a fixed decision rule or probabilistic draw from a fixed policy prior, or there could be another way to select, and that's G,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1726.377,
    "end": 1754.747,
    "text": " one important figure for g is 2.6 so here's g again pragmatic value is now here on the first term that it it it doesn't matter whether it's first or second term pragmatic value epistemic value so an important relationship to remember is if you only have pragmatic value",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1756.128,
    "end": 1759.756,
    "text": " then you have utility maximization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1761.079,
    "end": 1774.407,
    "text": "So in a situation where there either is no epistemic value to gain, or it's just not considered, the special case of expected free energy without epistemic value is Bayesian decision theory, expected utility theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1775.095,
    "end": 1793.172,
    "text": " Conversely, if you have no pragmatic value, again, it's a situation that doesn't have it, or you're choosing not to model it, or you have like a uniform preference over outcomes, measurements, then you just get two, three, four, five, and you get InfoMax.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1796.074,
    "end": 1802.2,
    "text": "So then having both, it then begets this question, how do you balance both?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1802.534,
    "end": 1814.994,
    "text": " And that's where we see some of the slightly potentially inflated language around dissolving the explore-exploit dilemma.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1817.718,
    "end": 1826.973,
    "text": "Does it dissolve it or does it articulate it so that we have a degree of freedom to balance it and adaptively explore solutions?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1827.574,
    "end": 1828.896,
    "text": "But is that dissolving it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1830.8,
    "end": 1859.727,
    "text": " So just writing it out this way, like doesn't solve action selection in general or for a specific case, but it's a setting that allows modeling a tremendous range of cases, which isn't an answer in itself, but that's kind of the reason why there's so much parameterization to do and why fine tuning",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1860.719,
    "end": 1889.629,
    "text": " the balance between these two is an interesting topic okay back to seven interesting it just like reloaded differently didn't it okay",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1891.448,
    "end": 1893.43,
    "text": " then action comes into play.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1894.532,
    "end": 1899.257,
    "text": "We took that music listening example, but this was a passive music listening example.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1901.099,
    "end": 1904.603,
    "text": "So now we're considering action inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1905.464,
    "end": 1910.209,
    "text": "Unified inference problem, sense making downstairs, action selection upstairs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1913.193,
    "end": 1917.658,
    "text": "Little bit more detail, revisiting equation 2.6.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1918.499,
    "end": 1941.047,
    "text": " saying here's the functional that's used to update the policy distribution it's an energy-based functional that has a pragmatic and an epistemic component sometimes it gets a little tricky with like are we talking about the negative free energy or the the the negative of the free energy",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1947.085,
    "end": 1966.107,
    "text": " Suffice to say, just check the script and see how the values are being stored, and then just confirm it and verify it with what you know is a good policy in a limited situation, what you know is going to be an inferior one relatively, and just calibrate it and reflect it, and then add it in the documentation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1969.451,
    "end": 1975.017,
    "text": "So then we get to the team A's, the classic decision example.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1975.722,
    "end": 1989.719,
    "text": " Here, we only have like one A matrix, but it turns out that you can actually have multiple As that can have the same or different shapes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1990.42,
    "end": 1992.622,
    "text": "So it's kind of like parallel As.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1995.286,
    "end": 2003.856,
    "text": "A1 is mapping the sense-making relationship between location and the observation of the location.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2004.713,
    "end": 2009.919,
    "text": " The reason why there's four columns is because there's four locations, one, two, three, four.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2010.459,
    "end": 2033.505,
    "text": "But the reason why there's five rows is because there's like five sense-making outcomes because the bottom location can either give this L or the R. So when we're in context one, which is to say that the bottom Q is gonna reveal an R, then deterministically being there will reveal the R.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2033.84,
    "end": 2061.231,
    "text": " again this just begets the question this is just all modeling well how does the mouse come to know that locations reflect their location or how does it come to know the semantics of R how does it come to associate R with the right side like that that is the question that's what then but then you focus the map it's like saying well um it's a subway map of the city but then where where are the power lines",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2062.004,
    "end": 2065.368,
    "text": " It's like, there's a lot of ways to make models of the situation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2065.769,
    "end": 2087.337,
    "text": "The second A component is mapping from location to either the absence of either food or aversive stimuli, or mapping the probability of measuring the either food or aversive stimuli.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2088.43,
    "end": 2092.436,
    "text": " Here's context one, where the queue tells you it's on the right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2092.476,
    "end": 2096.281,
    "text": "And if you go to the right, 98% of the time, there's the food.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2097.543,
    "end": 2098.705,
    "text": "Here's context two.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2099.646,
    "end": 2100.608,
    "text": "So you see a swap.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2100.628,
    "end": 2102.23,
    "text": "Now the one is here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2102.25,
    "end": 2109.381,
    "text": "If you go to the queue, you get the L, and now the 98 is there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2110.582,
    "end": 2113.847,
    "text": "But the 98 was here, and the one was there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2115.076,
    "end": 2121.803,
    "text": " So that's the kind of like matrix surgery that in the nitty gritty, a lot of this gets down to.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2122.243,
    "end": 2125.446,
    "text": "It's like, what are the rows and the columns of A?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2127.328,
    "end": 2144.205,
    "text": "Now we get into B. Here, these are, as PyMDP goes into in a lot of depth, you have transition for controllable transitions and uncontrollable transitions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2145.35,
    "end": 2159.933,
    "text": " uncontrollable transitions are one where the agent's policy selection has no efficacy so it's not really a control setting it's more just like a time series unfolding it's kind of like a passive inference element",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2160.638,
    "end": 2166.848,
    "text": " And then the B matrix describes the agent's beliefs about its efficacy of action.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2167.569,
    "end": 2168.751,
    "text": "Again, this is another question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2169.172,
    "end": 2181.111,
    "text": "What if the driver believes that pushing on the gas will have this B matrix effect on their acceleration, but actually the car has a different acceleration?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2182.795,
    "end": 2209.984,
    "text": " if that were the question you were modeling then you might look at a situation where there's a mismatch between the believed efficacy and the actual efficacy but the simpler setting is one where like what the agent believes an action will do is what it actually does but that's a major degree of freedom in modeling is how do agents have accurate representations of sense making in action",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2213.423,
    "end": 2241.22,
    "text": " then so it just it's the abc's it's like sesame street then it goes into describing preferences just like reflecting the one and the two of the a a1 is location a2 is basically food so c1 corresponds to location there's a negative one kicking it off the starting blocks and then it has a flat locational preference",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2241.453,
    "end": 2249.914,
    "text": " This gets into interpretability and AI alignment and safety in all these topics, because here we can say, it's not learning a proxy measure.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2250.435,
    "end": 2256.39,
    "text": "There is no pragmatic value being contributed by moving up in the teammates or moving to the right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2256.978,
    "end": 2265.266,
    "text": " we can interpret this model and say, it does have an E to the sixth pragmatic preference for getting food.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2266.107,
    "end": 2268.809,
    "text": "That is contributing to pragmatic policy these ways.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2269.45,
    "end": 2278.418,
    "text": "You could even do trace back and you could look at how the expected free energy values were contributed to by specific pragmatic and epistemic value components.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2280.72,
    "end": 2283.503,
    "text": "And in the simple case, it's all just matrix math.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2284.584,
    "end": 2285.905,
    "text": "So that's the preferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2286.948,
    "end": 2312.348,
    "text": " Here, again, the thing to kind of look out for is are the preferences being encoded in exponent form already, in which case a six is e to the sixth fold preference, or you might choose in your program or in the program that you're using, six might reflect a six-fold preference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2312.564,
    "end": 2317.43,
    "text": " And then under the hood, it gets exponentiated and just dealt with a different way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2320.073,
    "end": 2324.678,
    "text": "A, B, C, D. Here, D1, location.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2325.319,
    "end": 2329.784,
    "text": "It begins with perfect knowledge that it's 100% in the starting location.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2330.725,
    "end": 2334.129,
    "text": "D2, that's about the food.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2334.81,
    "end": 2338.034,
    "text": "So D2 is saying, so it's about the food context.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2339.195,
    "end": 2341.558,
    "text": "It has an equal probability here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2343.091,
    "end": 2346.457,
    "text": " This is also alluding a little bit to the learning by counting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2347.679,
    "end": 2351.986,
    "text": "So this one, one is like, it's one half of one, one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2352.266,
    "end": 2353.869,
    "text": "So it's 0.5, 0.5.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2354.53,
    "end": 2361.181,
    "text": "But learning by counting, what that would look like would be every time that you, so this is trial by trial learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2362.183,
    "end": 2363.746,
    "text": "So let's just say that it was on the right side.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2364.347,
    "end": 2367.512,
    "text": "Then you could change this to one third,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2368.184,
    "end": 2395.643,
    "text": " one comma two so then that would be equivalent to saying point three and point six seven and then now let's just say it went to right again it'd be one fourth one comma three then it'd be point two five point seven five so that's the learning by counting because you basically just increment what you've seen and then if it's one million one million then it's like a higher precision belief",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2395.758,
    "end": 2399.062,
    "text": " because seeing one more is only going to move your update a tiny bit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2406.971,
    "end": 2415.842,
    "text": "The team is again mobilized to focus a little bit on exploration exploitation setting and information forging.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2422.79,
    "end": 2425.613,
    "text": "Continuing on the epistemic value theme,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2425.846,
    "end": 2433.719,
    "text": " They move into an isochade paradigm, but it's separated from where it's brought up by several pages.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2434.8,
    "end": 2438.947,
    "text": "There's a box that talks about precision just in a general sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2440.61,
    "end": 2441.791,
    "text": "Here's a saccade model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2442.132,
    "end": 2445.818,
    "text": "There are isochade models with discrete and continuous elements.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2450.105,
    "end": 2452.208,
    "text": "Then a discussion on learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2453.622,
    "end": 2462.678,
    "text": " If there's no hyper prior, there's no hyper parameter for a given parameter of interest, the base case is that it's fixed and not learnable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2463.841,
    "end": 2476.744,
    "text": "So when we're looking at the T maze, we're looking at like this A matrix, this A matrix is being used to run forward inference, like to run sensemaking.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2477.197,
    "end": 2503.626,
    "text": " and um to the extent that it has values that contribute to it it also contributes epistemic values to different policy but the a matrix itself is not learnable it is not an object of inference it's not updated the values are not updated in the base case but this describes how you could make a bayesian model where there is updating of the a the b etc",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2511.335,
    "end": 2512.716,
    "text": " This is the learning by counting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2521.445,
    "end": 2524.528,
    "text": "More unpacking of equation 2.6.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2526.89,
    "end": 2531.795,
    "text": "More different decompositions of expected free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2535.378,
    "end": 2538.321,
    "text": "And demonstrating that in this maze paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2543.667,
    "end": 2545.629,
    "text": " It's bringing in a lot of topics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2546.91,
    "end": 2557.861,
    "text": "Structure learning, viewing alternative structural models, including with different numbers of parameters, as essentially a policy choice of the modeler.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2559.303,
    "end": 2563.447,
    "text": "So then that allows you to have the meta-Bayesian approach to Bayesian model selection.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2565.969,
    "end": 2569.793,
    "text": "And then here, hierarchical nested modeling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2573.755,
    "end": 2575.017,
    "text": " and a nested model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2584.47,
    "end": 2585.792,
    "text": "So seven is kind of a lot.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2589.537,
    "end": 2599.932,
    "text": "But it covers starting from a passive listening example all the way on through action and then some of these",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2600.232,
    "end": 2624.788,
    "text": " uh doing a little bit of showing not telling on unifying cognitive modeling and treating different cognitive phenomena in terms of free energy does anyone have a question or we can look at some of the written questions or please please add many questions to these tables",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2633.206,
    "end": 2634.127,
    "text": " Nothing for me right now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2634.147,
    "end": 2634.347,
    "text": "Sorry.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2653.988,
    "end": 2657.712,
    "text": "This was some I think this was Eric sounds from several years ago.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2660.223,
    "end": 2689.285,
    "text": " like even taking another level of meta there is no dilemma explore exploits already stated in the clearest way yeah i mean there's trade-offs of that kind everywhere efficiency and robustness and so on so it's not like you have to choose one or the other or that if you choose one you can't choose the other yeah i suppose it's you know",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2689.467,
    "end": 2715.458,
    "text": " if he gives you a principled way perhaps of deciding which one you should trade off at any given time or maybe that's what it's meant by it solves it but yeah it's it's a question for interpretation of what it means to solve it's like saying because we have a volume knob you don't have to worry about the volume it's like no it means the sound engineer is worried about the volume",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2718.172,
    "end": 2719.794,
    "text": " It's like he has a principled way of controlling it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2720.234,
    "end": 2720.514,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2721.135,
    "end": 2723.817,
    "text": "And there's an interpretable, simple way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2724.158,
    "end": 2729.363,
    "text": "So, for example, you could have an agent with a fixed epistemic value.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2730.043,
    "end": 2733.707,
    "text": "So kind of certain uncertainties, known unknowns about the world.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2734.408,
    "end": 2740.153,
    "text": "And then there's a policy on pragmatic value, policy on the preferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2740.854,
    "end": 2747.52,
    "text": "Like I scale up and down how much I am excited about food.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2747.5,
    "end": 2776.663,
    "text": " but studying is always the same amount so then when i want to focus on studying i just kind of turn down how much i care about food or turn it up if i want food or you could say i don't have control over c in this case and then you could explore well but could we tune the epistemic side to modify essentially the curiosity from lower to higher levels to affect the same concept",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2777.639,
    "end": 2793.378,
    "text": " But whether you're modifying the scale of the pragmatic value or the scale of the epistemic value, you still need to then have this question, well, what would make an adaptive approach to navigating that trade-off?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2796.021,
    "end": 2797.142,
    "text": "Let's just see what else we have.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2798.063,
    "end": 2800.086,
    "text": "What I cannot create, I do not understand.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2801.187,
    "end": 2805.452,
    "text": "Some people feel, again, more funny comments that people have written.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2807.002,
    "end": 2808.224,
    "text": " The mild answer?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2808.264,
    "end": 2810.067,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2810.968,
    "end": 2811.83,
    "text": "The mild answer.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2812.29,
    "end": 2814.734,
    "text": "You need to implement the algorithms and see how they perform.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2815.515,
    "end": 2832.081,
    "text": "Well, the cool thing is there's notebooks like in RxInfer and PyMDP where even if you don't have any program language installed locally, like just in the browser, you can hit play and get the active inference agent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2832.635,
    "end": 2835.038,
    "text": " That may help certain people.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2835.179,
    "end": 2841.047,
    "text": "If you'd like to contribute to those efforts and improve the notebooks, that's a great way because a lot of people will benefit from it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2841.708,
    "end": 2842.529,
    "text": "Stronger answer here?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2842.569,
    "end": 2844.952,
    "text": "Okay, let's see.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2845.393,
    "end": 2850.04,
    "text": "Math and theory and hand-waving about how the brain might do thus and so are all weak tea.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2852.023,
    "end": 2858.652,
    "text": "The only way to really prove the idea is to build artifacts and do interesting things, especially things that other methods cannot accomplish.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2859.093,
    "end": 2862.237,
    "text": "That's sort of the embodied robotics,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2862.385,
    "end": 2866.269,
    "text": " J. F. Claudia moral computation method.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2872.075,
    "end": 2877.12,
    "text": "Rows and columns, these are great understanding checks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2877.14,
    "end": 2891.295,
    "text": "Like if you look at the rows and the columns of all the matrices that are reflected in figure 7.3,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2892.288,
    "end": 2898.795,
    "text": " you basically understand the discrete time active inference model, because that's what it is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2899.155,
    "end": 2909.546,
    "text": "Each of these are matrices or tensors, and then the operations are essentially manipulations or multiplications on these matrices.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2911.468,
    "end": 2920.438,
    "text": "So that's a very deflationary approach to active inference, but it's super informative because it's like, there's not like some other magic layer that then is applied here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2933.395,
    "end": 2954.087,
    "text": " similarities and differences between prediction error and free energy gradients that's a good one does that does that show up in uh chapter seven like um prediction error specifically it looks like it's in the caption",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2959.096,
    "end": 2986.116,
    "text": " some people use them including sometimes even possibly authors who who know better slash differently but and there's so it's you know words words words Etc Etc Etc but a prediction error is denominated in the units of what the prediction is about like if I predict that it's going to be 30 degrees and then it's 32 degrees the prediction error was two degrees",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2986.973,
    "end": 2994.763,
    "text": " Surprise is denominated in units of information theory, like the nats or the bits, depending on base E or base two.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2995.824,
    "end": 3002.253,
    "text": "And then free energy is a unitless quantity that's just a calculation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3003.314,
    "end": 3014.128,
    "text": "So you could have a free energy, relative free energy, just like you could have a maximum likelihood or a likelihood value for 30 degrees and 32 degrees, you can have a free energy value for 30 and 32 degrees,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3014.429,
    "end": 3017.874,
    "text": " as part of the model fitting with respect to a generative model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3018.475,
    "end": 3025.124,
    "text": "But once you get into talking about free energy or surprise, you're talking about with reference to a given generative model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3026.086,
    "end": 3036.641,
    "text": "Whereas prediction error in the kind of narrowest sense, also it does entail the model making the prediction, but it is denominated in the units of what the prediction is about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3040.126,
    "end": 3040.386,
    "text": "Hmm.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3044.281,
    "end": 3051.031,
    "text": " I like that, talking about what the prediction is about, because I think the two aren't inherently so related.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3051.752,
    "end": 3066.855,
    "text": "For example, if I'm trying to improve my pragmatics or something like that, in the sense that maybe I'm playing a fighting game, and if I'm understanding this correctly, the actual loss, what the model is reflecting back is that I'm practicing my combos.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3066.915,
    "end": 3072.904,
    "text": "I'm practicing something inside of that that is going to be some instrumental thing that I want in order to improve the game.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3072.884,
    "end": 3102.334,
    "text": " but in me doing that i actually start to lose more often so the actual expected law the actual thing that i expect to see is me improving my model of how the game works while the actual reality of it is that i am losing at the game more i think that's a interesting thing oh okay so it's like a game where um you're or it's like i wonder what will happen if i play this differently",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3103.09,
    "end": 3111.663,
    "text": " And then you might be learning more, even though the so kind of leaning into the epistemic side.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3113.006,
    "end": 3119.376,
    "text": "Like, yeah, what if I what if I try to play with what with just this weird chess strategy?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3119.396,
    "end": 3124.103,
    "text": "With a longer term pragmatic value or something?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3130.208,
    "end": 3159.591,
    "text": " an interesting point about the um prediction errors being denominated in specific units because it's it's called you know it's about some qualitative phenomena out there whereas the vfe is or you know for it's just more about the fit of your beliefs that's that's something i hadn't considered yeah free energy value is a lot like a model likelihood value for several reasons first off model likelihood saying maximum likelihood ml",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3160.651,
    "end": 3162.037,
    "text": " is minimum surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3163.283,
    "end": 3167.742,
    "text": "So when we say that free energy is kind of bounding surprise,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3168.447,
    "end": 3179.443,
    "text": " Like in the special case or in the complete case, the free energy is one of the same as the surprise minimization, which would make it the evidence maximization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3179.463,
    "end": 3187.254,
    "text": "Or in the sort of more general approximation case, it's heading closer and closer towards minimizing surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3187.294,
    "end": 3190.599,
    "text": "So it's heading closer and closer to maximizing model evidence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3190.679,
    "end": 3195.366,
    "text": "But key similarity with likelihood estimates",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3195.346,
    "end": 3224.678,
    "text": " uh or free energy estimates or like sum of squares is they're not comparable across different settings like the sum of squares the l2 norm that's used in linear regression that depends on the number of data points you add another data point and even if it doesn't change the regression line it's going to like it's it changes the sum of the squares of the residuals but",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3224.928,
    "end": 3252.522,
    "text": " that that so that's why the sum of squares are not compared for like dips data sets with different numbers of value so free energy or also see EG sum of squares likelihoods they're like a statistical diagnostic I mean these are summary variables on Maps",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3254.628,
    "end": 3273.379,
    "text": " again this is part of the the deflationary educational elements which is like once free energy is seen as the calculation that it's described as in the textbook it puts some of the discussions around real systems and their free energy into a different light",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3277.595,
    "end": 3286.574,
    "text": " Because it'd be like, well, would this person also argue that the sum of squares of the regression between height and weight is a real thing in the world?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3287.636,
    "end": 3292.787,
    "text": "Okay, would they argue that the free energy value of the relationship between height and weight is a real thing in the world?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3294.029,
    "end": 3295.452,
    "text": "So then what is the claim?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3303.413,
    "end": 3314.689,
    "text": " Any last thoughts or what people plan to like add for questions or work on for the coming few weeks?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3329.266,
    "end": 3343.449,
    "text": " I guess the only thing is the thing that I find is natural to follow on from this discussion is the kind of stuff I brought up earlier about particular ways to deal with the explosion in the policy space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3344.992,
    "end": 3352.945,
    "text": "That's kind of where I feel like this whole chapter, like where you would want to go after this, let's say.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3353.414,
    "end": 3361.265,
    "text": " So that's one thing I'm very interested in is trying to further those heuristic means of navigating those very large spaces.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3363.388,
    "end": 3366.032,
    "text": "Yep, that's an important topic.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3367.133,
    "end": 3381.193,
    "text": "I mean, also when build things out and whether timing them, like just using logging and timing functions in the script or using other ways,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3381.629,
    "end": 3394.582,
    "text": " just explore and be like, yeah, what does the runtime look, when I plot the runtime for one, two, three, four, five step time horizon, like what does that look like?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3395.203,
    "end": 3403.872,
    "text": "And then this model stream 6.1, this is really interesting work with the branching time active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3403.892,
    "end": 3406.995,
    "text": "And this is some of the only, still actually,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3407.431,
    "end": 3436.071,
    "text": " some of the only computational complexity like big O notation I don't know why it's not loading but for active inference so so look at that if you're interested in in computational complexity estimates but that's another great advantage that with rx infer and everything we will um realize which is like we let's just say that we knew okay temperature it's a number between one and a hundred",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3437.35,
    "end": 3439.012,
    "text": " Observations, number between 100.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3439.753,
    "end": 3441.355,
    "text": "This has this defined dimensionality.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3441.515,
    "end": 3442.977,
    "text": "This has this defined dimensionality.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3443.738,
    "end": 3452.349,
    "text": "In the discrete time, we could say exactly the RAM and the CPU and all these other attributes about the model statically.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3453.691,
    "end": 3461.301,
    "text": "So then you could anticipate resourcing deterministically, even for a probabilistic model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3463.363,
    "end": 3465.366,
    "text": "There's a lot of things like that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3466.19,
    "end": 3468.833,
    "text": " And then you can say, well, now we have three ways to implement this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3469.093,
    "end": 3471.075,
    "text": "We could do the full policy rollout.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3471.736,
    "end": 3472.657,
    "text": "That's going to be $19.99.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3473.078,
    "end": 3478.223,
    "text": "Or here's the half off, but we only sample half the policies.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3485.431,
    "end": 3489.736,
    "text": "Daniel, would you be able to link that big old paper or talk you'd mentioned?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3489.756,
    "end": 3496.163,
    "text": "It's actually something I'm curious about.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3497.324,
    "end": 3523.669,
    "text": " branching model stream there's actually two papers i believe there's that there is the original paper and then there's a follow-up which is the kind of big o stuff i think yes link that or just add it you know any and then anywhere we search we'll find it but yeah that that that's cool and uh yeah that was like sort of our i think just last week in rx and for we were discussing like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3524.662,
    "end": 3526.945,
    "text": " how much other computation is there?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3526.985,
    "end": 3529.188,
    "text": "Like what is the overhead?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3529.228,
    "end": 3546.973,
    "text": "And then if the payload is this sort of like hypothetical minimum computational complexity of just storing the matrices and doing the operations, and then it's like, how much is the computational burden of the message passing?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3548.236,
    "end": 3555.166,
    "text": " And like what are the situations where the matrix math has a given computational complexity and then the message passing has like this or that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3556.288,
    "end": 3556.488,
    "text": "Right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3557.389,
    "end": 3559.372,
    "text": "Yeah, that's a big open question to me.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3559.692,
    "end": 3564.92,
    "text": "I'm hoping to read more about where it's covered just because there's so many different message passing scenarios.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3567.824,
    "end": 3568.044,
    "text": "Yes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3568.105,
    "end": 3568.545,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3568.565,
    "end": 3571.369,
    "text": "So, all right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3572.691,
    "end": 3573.332,
    "text": "Paper two.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3578.847,
    "end": 3580.469,
    "text": " that's the sort of theory.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3580.85,
    "end": 3586.197,
    "text": "And then paper one is the complexity time, complexity class analysis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3588.32,
    "end": 3588.62,
    "text": "I believe.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3590.723,
    "end": 3590.983,
    "text": "Cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3591.003,
    "end": 3591.444,
    "text": "Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3591.905,
    "end": 3592.125,
    "text": "Cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3592.706,
    "end": 3608.567,
    "text": "Well, um, see all either at this time, like next week, or later today, Andrew will be facilitating, I believe.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3610.589,
    "end": 3611.814,
    "text": " So enjoy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3611.854,
    "end": 3615.911,
    "text": "Talk to you soon and not anything till then.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3616.734,
    "end": 3616.975,
    "text": "All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3617.818,
    "end": 3618.019,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3618.883,
    "end": 3619.485,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  }
]
[
  {
    "start": 2.242,
    "end": 31.303,
    "text": " all right welcome back cohort six we're in our first discussion on chapter five so let's jump over there um where does anyone want to begin could be like any page or figure or quote of five that they remembered or that they're curious about so first we'll just see any thoughts or ideas people have about five",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 32.92,
    "end": 35.993,
    "text": " Otherwise, we can look to the chapter and to the prior questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 38.825,
    "end": 39.106,
    "text": "Sure.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 39.226,
    "end": 43.183,
    "text": "I thought that this was a cool chapter because",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 43.433,
    "end": 71.77,
    "text": " it for the first time i think kind of gets a lot of the message passing mapped on to um specific biological systems that we actually have quite a bit of prior knowledge about um so i thought that was pretty neat like to actually see the sort of detailed examples of the pathways and the basal ganglia and that kind of thing i thought that was great yeah it's good",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 72.29,
    "end": 74.013,
    "text": " Yeah, I was taking a look at that myself.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 74.273,
    "end": 78.7,
    "text": "I don't know if we could go through that a little bit because I was just having some trouble with the path tracing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 78.72,
    "end": 88.136,
    "text": "It might have just been the model itself and the way that I was misreading it, but figure 5-1 and 5-2.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 88.516,
    "end": 96.289,
    "text": "Yeah, 5-5 is like the big overview abstract, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 96.522,
    "end": 108.382,
    "text": " It covers the prefrontal cortex graph, the dopaminergic, the basal ganglia graph, and the spinal reflex arc graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 108.923,
    "end": 113.13,
    "text": "And then earlier in the chapter, so now we'll go back to 5.1 and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 113.549,
    "end": 116.474,
    "text": " Earlier, it's going to go through those examples.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 117.075,
    "end": 121.302,
    "text": "And then those examples have a compositionality internally.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 122.244,
    "end": 123.386,
    "text": "That's the graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 123.406,
    "end": 135.507,
    "text": "And then this is kind of the cool thing in the interoperability is that because of that compositionality internally, you can kind of do wiring across graphs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 136.111,
    "end": 139.315,
    "text": " So that also speaks to the compositionality of the models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 140.076,
    "end": 165.004,
    "text": "And I think maybe one theme that we'll try to draw out and explore and see limitations of is like, there's a massive underlying hypothesis of computational neuroscience, which is like computational models can map biological territories.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 165.659,
    "end": 183.047,
    "text": " So there will be like some regions or some actual tissues that either do, that's more of the realist angle, or can be modeled as doing, that's more of the instrumentalist angle, they can be modeled as doing certain computations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 184.088,
    "end": 192.021,
    "text": "And if that's a viable hypothesis, then the maps that we make with the math that we have are going to be very apt.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 192.49,
    "end": 217.772,
    "text": " If it's an inviolable hypothesis, it could be extremely misleading because there could be a tissue and potentially the toolkit of mathematical operators just isn't adequate to make a good map of what that tissue does, leading to like a false confidence about the understanding of what it's doing in the body or like its role in development, evolution, all of that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 217.752,
    "end": 226.178,
    "text": " So so it's kind of like it, but that's so sublimated into the field decades on that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 226.218,
    "end": 230.952,
    "text": "It's like, of course, we're putting mathematical symbols on top of pictures of tissues",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 232.788,
    "end": 240.3,
    "text": " but yet that's actually like, that's kind of the invisible elephant in the room is like, what is happening here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 241.321,
    "end": 249.214,
    "text": "Applying these mathematical models, the graphical models and like using them as overlays to make sense of the functions of different tissues.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 256.686,
    "end": 258.028,
    "text": "Yeah, that's really interesting.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 258.188,
    "end": 259.47,
    "text": "I'd wonder,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 260.918,
    "end": 279.782,
    "text": " So perhaps kind of like in addition to that, one of the things that I had heard a few neuroscientists raise is the question of whether the kind of known connectivity maps in the brain suit themselves to this kind of explanation as well.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 280.883,
    "end": 283.366,
    "text": "Oh, I'm sorry, Andrew, you have your hand raised.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 283.406,
    "end": 284.648,
    "text": "Would you like to go?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 288.172,
    "end": 289.354,
    "text": " Oh, yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 289.375,
    "end": 290.156,
    "text": "Thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 290.617,
    "end": 301.68,
    "text": "Yeah, no, it's just a quick comment following from kind of what Daniel was describing this sort of like mapping the physiological to the to the computational or mathematical, I suppose.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 302.021,
    "end": 304.546,
    "text": "I heard this nice explanation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 304.606,
    "end": 307.412,
    "text": "It's in a previous book.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 307.392,
    "end": 335.374,
    "text": " live stream with the institute with Ryan Smith the computational psychiatrist and it was just whenever I was watching it it was like my first time hearing like this kind of attempt to directly relate neural activity to to like I guess the mathematical equation so he was like breaking down computation of like variational free energy and expected free energy and like equating like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 336.012,
    "end": 348.251,
    "text": " basically attempting to equate like whenever a term is subtracted from another term that could be viewed as like inhibition as opposed to excitation with the like addition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 349.272,
    "end": 356.904,
    "text": "And whenever something is multiplied in the equation, we could view that as like modulation in synaptic gain.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 357.024,
    "end": 360.99,
    "text": "So this just sort of, I mean, it might be taken a little too seriously,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 360.97,
    "end": 384.733,
    "text": " literally to just state it as simply as that, but it was just, yeah, it's super fascinating to me to think like, oh, like this is like really attempting to map neural computation with like, you know, a written symbolic mathematical formula, like subtraction, addition, we have inhibition and excitation, and then we have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 384.713,
    "end": 387.201,
    "text": " you know, modulation with the multiplication.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 387.221,
    "end": 389.949,
    "text": "So anyway, that was, it was, so it was more just a comment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 390.009,
    "end": 394.302,
    "text": "I can share the link to that stream if anybody would like to check it out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 397.353,
    "end": 397.754,
    "text": " Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 398.274,
    "end": 405.446,
    "text": "Like whether we're looking at gene expression levels of two loci, we're measuring it with RNA-seq or something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 405.486,
    "end": 412.577,
    "text": "We're looking at the firing rate of two different neural populations or single neurons or like two metabolites in the cell.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 412.717,
    "end": 417.825,
    "text": "It's like, okay, we're making measurements of some biological properties.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 417.805,
    "end": 421.51,
    "text": " And then that's the kind of O to S mapping.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 421.95,
    "end": 426.176,
    "text": "Okay, we have the RNA-seq data, that's the observation, and then the hidden state is the true gene expression level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 426.376,
    "end": 429.82,
    "text": "So that kind of is the partially observable component.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 429.84,
    "end": 439.813,
    "text": "So let's just say that we're in a highly observable setting, or whatever the case may be, we've gone past our measurement into this underlying space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 439.793,
    "end": 464.147,
    "text": " of how are these two loci related well one option is like the mutual information between them is zero like their covariance profile is just a scatter cloud there's no correlation so at that point at least at a first pass there's not like an interesting or non-noise relationship between the two variables",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 464.482,
    "end": 467.046,
    "text": " Not everything is causally connected.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 467.066,
    "end": 468.829,
    "text": "That's the sparsity of the biological system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 468.849,
    "end": 469.591,
    "text": "So it's like, that's okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 470.051,
    "end": 480.469,
    "text": "But then if there is some kind of informational mapping, then it's like, okay, at a first pass, does one go up when the other goes up or not?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 481.21,
    "end": 482.893,
    "text": "Is it a positive or negative relationship?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 482.993,
    "end": 484.496,
    "text": "Is it a U-shaped relationship?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 485.377,
    "end": 489.865,
    "text": "There's not an infinite number of possible mappings.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 490.756,
    "end": 495.243,
    "text": " And then how complex does the math need to be to capture that mapping?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 495.684,
    "end": 500.331,
    "text": "Well, that's kind of where we get into this accuracy minus complexity question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 500.671,
    "end": 502.554,
    "text": "Is it enough to just say, yeah, linear regression?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 503.195,
    "end": 506.5,
    "text": "Or is it better to do it as a piecewise defined function?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 507.001,
    "end": 509.786,
    "text": "But like, well, it's a linear relationship up to this point and then it's flat.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 511.108,
    "end": 519.801,
    "text": "Or would it be simpler to just fit a single parameter of linear regression and then still just try to capture as much variance as possible",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 520.237,
    "end": 525.083,
    "text": " Or should we go with this two parameter model with a slightly sharper slope and then a flat bridge?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 525.644,
    "end": 538.341,
    "text": "I mean, those are the model comparison questions that make, but the space of all the possible relationships does map pretty clearly to these operators that we have mathematically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 539.403,
    "end": 539.663,
    "text": "David.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 541.826,
    "end": 541.986,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 542.006,
    "end": 547.273,
    "text": "I was just wondering if you can classify the low size, uh, different nodes in that sense.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 549.228,
    "end": 549.95,
    "text": " Yeah, exactly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 550.612,
    "end": 552.277,
    "text": "Like a gene regulatory network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 552.498,
    "end": 556.029,
    "text": "The nodes would be the expression values of the gene.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 556.049,
    "end": 559.179,
    "text": "And then the edges would be like a causal connection, different kinds of edges.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 560.302,
    "end": 560.503,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 561.85,
    "end": 569.961,
    "text": " And this also relates to the earlier point about the connectivity maps and about different notions and different maps that are made.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 570.542,
    "end": 584.301,
    "text": "Also, this is kind of like the genealogy of active inference with the SPM package, which is exactly about, well, we don't only just model the anatomical structural connections.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 584.401,
    "end": 585.502,
    "text": "That's basically known.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 585.903,
    "end": 586.744,
    "text": "I mean...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 586.724,
    "end": 616.022,
    "text": " largely we know where different tracts of neurons project to um now there's also like the electrical field propagation so on so for another time but um okay so we have the structural connectivity that's something you can just slice an image and reconstruct in 3d but then what happens is the dynamic causal modeling approach says what are the causal the statistical edges",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 616.154,
    "end": 618.624,
    "text": " And then there's a few different kinds of statistical edges.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 619.468,
    "end": 625.472,
    "text": "You might want to just take a first pass, which sections are correlated with each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 625.722,
    "end": 629.066,
    "text": " and then the edges basically reflects a correlation value.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 629.486,
    "end": 633.931,
    "text": "So it's like these two have a 0.8 edge between them because they're 80% of the time correlated.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 635.513,
    "end": 637.836,
    "text": "So that's one kind of connectivity map.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 638.396,
    "end": 644.443,
    "text": "Another kind of connectivity map would be looking for the causal consequences of one region on another.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 644.824,
    "end": 651.511,
    "text": "So let's just say two regions are 80% correlated, but it turns out that that's because they're both downstream of another region.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 652.052,
    "end": 654.615,
    "text": "Well, in that case, the two downstream regions",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 655.726,
    "end": 662.615,
    "text": " the 80% is kind of, it's a consequence of some other causal relationships.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 663.556,
    "end": 672.127,
    "text": "So these are like different kinds of maps that connect nodes in ways that aren't necessarily only the structural.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 672.948,
    "end": 681.379,
    "text": "And that's like a kind of tension that comes up again and again, where we have nodes that are structurally connected to each other,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 681.815,
    "end": 691.392,
    "text": " like they're actually sending neural signals, but for a given pattern of interest, that may not be causally relevant.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 692.414,
    "end": 696.161,
    "text": "And then on the other hand, you have nodes that are causally relevant to each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 696.201,
    "end": 704.937,
    "text": "So you want to model like a causal edge on the base graph, but they might not be directly touching through a neural contact.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 706.402,
    "end": 731.447,
    "text": " okay so then we it gets a little um both like the simplest system slash world would be like all the anatomical connections have causal efficacy and none and there's nothing else so just knowing the topology of the system as it's actually connected that is the computational architecture",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 732.406,
    "end": 739.274,
    "text": " However, it's generally not that way, especially when making ultra reduced simpler models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 740.375,
    "end": 749.065,
    "text": "So like here, it's kind of a mixture of like on one hand talking about the projections of different anatomical regions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 750.286,
    "end": 757.034,
    "text": "But then on the other hand, that's not exactly or only the edges that are described here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 757.174,
    "end": 762.16,
    "text": "Like there's other projections that just aren't being focused on here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 766.477,
    "end": 766.717,
    "text": " Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 766.737,
    "end": 787.546,
    "text": "I guess that relates to, um, there was, I'm trying to find the term in the, in the paper or in the book actually, um, with a crossover effect and I forget the name of what it was, but I just saw a connection with, uh, interleukin and crossover.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 787.626,
    "end": 789.549,
    "text": "And there, there seems to be some correlation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 789.569,
    "end": 791.792,
    "text": "I just don't know enough about the, uh,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 793.713,
    "end": 806.541,
    "text": " The interaction if that might be related to, you know, different nodes interacting at different different time periods is like things stretch out, for example, in the frequency parameters.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 807.517,
    "end": 808.578,
    "text": " This is a great point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 809.259,
    "end": 825.256,
    "text": "This is basically, as presented here, it's just a model of like kind of the real-time fast neural inference, but we're not seeing like the effect of glia or interleukin or slower changes, let alone even just synaptic plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 825.777,
    "end": 833.625,
    "text": "So it's definitely a big open area is how do you incorporate this kind of runtime inference structure",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 833.791,
    "end": 840.9,
    "text": " with developments and neural and immune development, which change the structure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 842.162,
    "end": 847.349,
    "text": "Another fun paper, I'll put it here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 849.491,
    "end": 851.814,
    "text": "Could neuroscientists understand a microprocessor?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 854.097,
    "end": 857.622,
    "text": "And this just shots fired in the first sentence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 858.31,
    "end": 859.732,
    "text": " Why are people collecting all this data?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 860.032,
    "end": 864.478,
    "text": "I mean, why are there massively funded projects to sample synapses and all this stuff?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 865.179,
    "end": 873.971,
    "text": "Well, it's kind of related to this belief that if we had more data, we had better images, better satellite images of the territory, then we would have better maps.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 875.814,
    "end": 877.416,
    "text": "So this doesn't...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 878.628,
    "end": 905.108,
    "text": " conclusively put that idea down but they do something very fascinating and empirical which is we know the topology of a microprocessor and there's simpler microprocessors that can be simulated in silico that's kind of meta but we have the full processor simulation being emulated at like a tremendous overhead right on a more modern processor so",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 905.24,
    "end": 909.607,
    "text": " And then they're able to take neural recordings from the microprocessor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 910.148,
    "end": 912.291,
    "text": "And you could look at like the activity of a transistor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 912.732,
    "end": 920.143,
    "text": "You could also look at like the local field potential around a region by averaging out the electrical activity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 921.265,
    "end": 926.573,
    "text": "And then you could do single and double lesion experiments.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 926.688,
    "end": 931.357,
    "text": " And that kind of like loss of function is classic, like in genetics, like that's what people do for a long time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 931.417,
    "end": 936.646,
    "text": "It's like gain of functions and losses of function and looking at single and pairwise losses of function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 937.628,
    "end": 946.705,
    "text": "And then they presented that data set to neuroscientists and use their kind of general information processing algorithms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 947.14,
    "end": 959.997,
    "text": " And we see this kind of coming up like in integrated information theory, like they're talking about microprocessors and they're talking about neurons because both of those are being abstracted to the graphical network information dynamic setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 960.959,
    "end": 962.263,
    "text": "And",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 962.362,
    "end": 965.547,
    "text": " It's not to say someone couldn't analyze this information usefully.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 966.087,
    "end": 975.361,
    "text": "It just turns out that there are like things that are causally connected because we know the software running, not to say that there's software running on neurons, but again, it's analogy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 976.182,
    "end": 981.69,
    "text": "There's things where we know what computation is happening and the lesions are uninformative about it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 982.231,
    "end": 987.238,
    "text": "And it's just, there's false positives and there's false negatives, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 987.218,
    "end": 1006.595,
    "text": " That's not to say it's uninformative experiments, but it really calls into question a simple or kind of uncritical mapping between how things are structurally connected in the real world anatomically, and then what's the causal architecture of the system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1007.636,
    "end": 1011.5,
    "text": "This comes up in the act inside, like the whole Markov blanket discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1012.58,
    "end": 1016.584,
    "text": "Are Markov blankets reflecting the spatial and the temporal boundaries of things?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1017.914,
    "end": 1018.595,
    "text": " If so, where?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1018.635,
    "end": 1031.674,
    "text": "And or do Markov blankets just more generically reflect boundaries in state space, which might coincide with a spatial and temporal boundary, but it doesn't have to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1031.734,
    "end": 1040.327,
    "text": "There'd be boundaries in state space that would be within what would be called a spatial or temporal boundary.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1040.459,
    "end": 1042.041,
    "text": " And vice versa.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1043.003,
    "end": 1050.853,
    "text": "It would all depend on your sensitivity thresholds and whether you were trying to capture 80% of the variance or 99 or 99.999.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1051.875,
    "end": 1060.306,
    "text": "So that's what's so interesting, again, in this chapter five, which is like, we had the intro chapter one, low road, high road, two and three.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1061.047,
    "end": 1062.99,
    "text": "Chapter four with all the math.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1063.156,
    "end": 1066.4,
    "text": " and getting to the actual kernel of the generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1066.92,
    "end": 1073.849,
    "text": "And then now that is kind of perfect in and of itself, but it hasn't really been applied to any system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1074.449,
    "end": 1083.32,
    "text": "And then chapter five immediately is kind of like, here's some of the amazing insights you get by applying active inference to real territories.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1084.321,
    "end": 1089.467,
    "text": "And here's some also of the fundamental challenges and limitations of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1089.802,
    "end": 1118.712,
    "text": " that come into play when we are talking about maps and territories, rather than just kind of the pure science of map making, because you could just make these kind of pure maps of just, well, I'm imagining a territory that's, you know, there's a street map on the head of a pin, but then you can't necessarily build that or measure it in the real world, but you could talk about it in chapter four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1119.755,
    "end": 1126.709,
    "text": " Yeah, I guess that kind of speaks towards like some of the granularity as well in terms of like in silico versus in situ.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1126.769,
    "end": 1131.959,
    "text": "If you're doing a comparison, you know, going back to the transistor model, you know, how fine grain can you get?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1131.979,
    "end": 1136.287,
    "text": "I was just wondering if there's any correlation in that sense as well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1136.388,
    "end": 1141.678,
    "text": "So like, you know, the smaller the transistors that get, you know, nanometer wise,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1142.148,
    "end": 1160.764,
    "text": " um is there any correlation there with you know in situ with um with the dynamics of neuron interaction yeah i think a great example of that empirically is like it's often this is like early 1900s the debate was like",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1161.605,
    "end": 1168.201,
    "text": " Is the brain composed of cells that have gaps between them or the reticular theory?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1168.241,
    "end": 1169.504,
    "text": "That was the neuron doctrine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1170.185,
    "end": 1171.689,
    "text": "There are cells that are separated.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1171.709,
    "end": 1176.34,
    "text": "And then there's the reticular theory, which is kind of like, maybe it's all like a mycelial",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1176.978,
    "end": 1181.267,
    "text": " connected with actual direct connections.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1183.191,
    "end": 1185.977,
    "text": "And then that's the Golgi stain and Ramon y Cajal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1186.037,
    "end": 1191.589,
    "text": "And they were like, no, you can stain a cell and it fills out the cell and ends there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1192.03,
    "end": 1195.678,
    "text": "So that was like a big W for neuron doctrine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1195.895,
    "end": 1197.742,
    "text": " Then, you know, there's a little bit of complications.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1197.762,
    "end": 1202.7,
    "text": "There's some big holes in cells and there's parts where they are connected and so on, but largely the cells are separate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1202.8,
    "end": 1203.403,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1203.423,
    "end": 1207.879,
    "text": "So then the neuron became understood as kind of like the unit of modeling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1208.737,
    "end": 1210.32,
    "text": " But then there's several issues there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1210.68,
    "end": 1214.767,
    "text": "One is that's a huge model, even for a simple brain region.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1215.167,
    "end": 1217.912,
    "text": "So often we're modeling populations of neurons.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1218.432,
    "end": 1221.497,
    "text": "Like these nodes represent not single neurons, but populations of neurons.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1222.299,
    "end": 1225.444,
    "text": "And that's the concept of like the population encoding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1226.004,
    "end": 1230.171,
    "text": "And those are easier to work with in terms of coarse graining, because",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1230.742,
    "end": 1232.564,
    "text": " you're coarse-graining to fewer regions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1233.065,
    "end": 1237.49,
    "text": "And also the statistics of individual neurons are very spiky and noisy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1238.09,
    "end": 1247.842,
    "text": "Whereas when you're looking at the population statistics of a pool, then you can model it much more easily with like continuous distributions and like rate encoding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1248.302,
    "end": 1256.091,
    "text": "It's more like looking at an analog signal that's smoothly turning up and down rather than like spiking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1256.071,
    "end": 1263.14,
    "text": " But then going down a level, people have found just incredible computational dynamics in dendritic arbors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1264.883,
    "end": 1274.315,
    "text": "So even just choosing the neuron as the unit of analysis, like you can look at the computations or whatever you want to think about it as at a sub neural level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1275.557,
    "end": 1285.69,
    "text": "Like if a neuron had two branching sets of dendrites, you could look at the summation dynamics and the decay dynamics at a really fine scale level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1287.037,
    "end": 1309.686,
    "text": " then at that point like the neuron would be kind of your higher level outcome and that's part of the multi-scale um processes that are happening again with a map territory none of this is an issue it's like well if we wanted to do that um millimeter scale analysis of the sidewalk we could or we could just have the one kilometer",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1311.337,
    "end": 1321.411,
    "text": " But then that's where realist approaches to computational neuroscience are in a difficult position.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1322.713,
    "end": 1330.965,
    "text": "Because it's so clear, even at the single neuron level, that you're coarse graining and abstracting over many levels of functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1333.223,
    "end": 1333.544,
    "text": " Right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1333.564,
    "end": 1352.616,
    "text": "I mean, I see that in like, I mean, I haven't done too, too much research, but just from what I've read, you know, in terms of like Zorg gates and how an individual neuron interacts, once you get a field of neurons as an array, you know, how can you tell what gates are opening and closing and what's interacting or cascading across the entire field?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1358.671,
    "end": 1373.727,
    "text": " yeah like right now the nodes are just in this image the nodes are arranged to resemble the histological layers of the cortex the mammalian prefrontal cortex um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1374.618,
    "end": 1377.421,
    "text": " but all the causal relationships are topological.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1378.162,
    "end": 1387.232,
    "text": "So like technically you could like kind of rearrange the nodes on the page as long as you kept the connectivity and that wouldn't change the computational function here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1387.793,
    "end": 1391.437,
    "text": "It would just make it look less like the layers of the cortex.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1394.62,
    "end": 1396.923,
    "text": "Would you have any gradient like layer shifting?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1396.963,
    "end": 1403.19,
    "text": "So would you have, you know, any of the modalities shift between the layers because of that interaction?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1403.356,
    "end": 1409.645,
    "text": " Or would it remain in situ and just the connections themselves would be the things that are changing?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1411.608,
    "end": 1412.509,
    "text": "That's a good question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1412.529,
    "end": 1417.957,
    "text": "I mean, if your map were purely topological, then geometric changes wouldn't matter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1418.779,
    "end": 1429.414,
    "text": "However, if you did want to include bulk propagation of electrical flows or spatial effects, then that would be a different kind of model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1431.402,
    "end": 1434.326,
    "text": " So then, which is not even represented here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1434.507,
    "end": 1436.91,
    "text": "And then there's like different ways you could incorporate that into the model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1437.451,
    "end": 1443.22,
    "text": "Like you could give each node could have a position in a coordinate system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1444.041,
    "end": 1449.63,
    "text": "And then you could have another component of the model that had like a propagation through that coordinate system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1451.838,
    "end": 1456.145,
    "text": " And then this would be why it's so important to have the portfolio of models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1456.165,
    "end": 1461.514,
    "text": "You'd say, okay, I have the 100 parameter model here that's purely topological.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1461.955,
    "end": 1467.744,
    "text": "And then I have the 109 parameter model that has this, and then the 120 that has this other thing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1467.784,
    "end": 1472.272,
    "text": "And then accuracy minus complexity, Bayes information theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1473.273,
    "end": 1475.837,
    "text": "We want to reward explaining the variance in the data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1476.258,
    "end": 1478.622,
    "text": "We want to penalize having more parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1479.513,
    "end": 1481.416,
    "text": " How much should we penalize having more parameters?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1481.816,
    "end": 1484.38,
    "text": "Well, that depends on the modeler's sensitivity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1484.42,
    "end": 1492.552,
    "text": "Are they seeking to explain a higher and higher fraction of the variance at the cost of a model increasing parameters?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1493.253,
    "end": 1503.327,
    "text": "Or are they looking to actually take just the low-hanging fruit and have a kind of sparser model accepting that it will explain less variance?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1503.408,
    "end": 1510.579,
    "text": " There's no a priori answer because there's a setting where you want 99% variance explained at all costs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1511.28,
    "end": 1515.166,
    "text": "And then there's a setting where you want four variables and no more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1515.226,
    "end": 1519.813,
    "text": "And whatever variance you explain is just what you're going to get with four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1519.833,
    "end": 1527.485,
    "text": "So those are just the pragmatic considerations of making a given model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1527.505,
    "end": 1530.349,
    "text": "Great, thanks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1530.369,
    "end": 1530.85,
    "text": "Okay, so.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1531.808,
    "end": 1534.663,
    "text": " Here, we're in the cortical layers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1539.723,
    "end": 1546.352,
    "text": " As always, the disclaimer, this is just the mammalian nervous system plan.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1547.213,
    "end": 1553.181,
    "text": "Analogous and other functions are carried out by other like diverse nervous systems.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1553.682,
    "end": 1561.693,
    "text": "So I think it'd be epic to have like a chapter five insect supplement, especially because it's a fully observable neural system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1562.314,
    "end": 1564.296,
    "text": "You just lay it flat and just take the picture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1564.857,
    "end": 1568.582,
    "text": "So I think there could be mega insect brain modeling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1568.562,
    "end": 1571.024,
    "text": " but here's the cross section of the neural tissue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1571.064,
    "end": 1573.487,
    "text": "And here are those six layers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1574.988,
    "end": 1577.931,
    "text": "Here's the surface of the prefrontal cortex.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1578.671,
    "end": 1595.387,
    "text": "And then here's the columnar architecture, which are these kind of hexagonally tiled columns that are the basis of, for example, thousand brains by Jeff Hawkins.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1596.628,
    "end": 1597.489,
    "text": "This, um,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1600.085,
    "end": 1601.947,
    "text": " This is an interesting work.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1602.367,
    "end": 1602.747,
    "text": "Definitely.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1602.767,
    "end": 1605.169,
    "text": "I wonder about what they know about active.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1605.95,
    "end": 1617.98,
    "text": "Um, but it has to do with, um, the, the parallel parallelized decentralized architectures and kind of the, um, columnar, um, architecture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1618.901,
    "end": 1619.141,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1619.161,
    "end": 1619.982,
    "text": "Peter, I'm going to look at this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1628.69,
    "end": 1629.13,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1630.949,
    "end": 1631.37,
    "text": " Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1633.533,
    "end": 1633.794,
    "text": "Wow.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1633.814,
    "end": 1637.82,
    "text": "I mean, you know, these are the silent elephant in the room.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1638.401,
    "end": 1638.902,
    "text": "Isn't that great?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1638.922,
    "end": 1639.563,
    "text": "Yeah.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1640.745,
    "end": 1647.515,
    "text": "It's like, can we really just take highlighters and like shade different parts of real tissues?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1648.517,
    "end": 1650.46,
    "text": "Well, this one's doing prediction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1651.469,
    "end": 1660.201,
    "text": " Okay, but can in a different cognitive setting or a different like attentional regime, is it always doing predictions?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1660.221,
    "end": 1661.122,
    "text": "It's like about what?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1662.264,
    "end": 1663.846,
    "text": "And where are the glia?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1663.866,
    "end": 1665.108,
    "text": "There's all the slower questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1665.148,
    "end": 1667.011,
    "text": "So that's always really fun.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1667.471,
    "end": 1671.377,
    "text": "But this is great.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1671.477,
    "end": 1677.505,
    "text": "And it's looking at the relationships within and across the columns.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1679.248,
    "end": 1679.989,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1680.009,
    "end": 1680.109,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1680.494,
    "end": 1700.721,
    "text": " One thing I really like about this paper, and both these papers actually, is that for some of the underlying architectural stuff, like looking at the connections between laminar gradients across the brain and that kind of thing, it depends on the track tracing work of Helen Barbus, whose work is some of the most detailed and exacting that I've seen in the neural literature.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1701.883,
    "end": 1708.932,
    "text": "So if we're going to try to do this kind of mapping onto biological systems, which seems difficult enough with all the things that you've said, Daniel,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1709.249,
    "end": 1720.924,
    "text": " it's at least nice to like have a sort of tracing of the actual circuits that it feels like we can kind of take to the bank, you know?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1720.944,
    "end": 1721.304,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1721.845,
    "end": 1735.182,
    "text": "Like we take the tracing to the bank as kind of a fact, and then that is like our, that's like a very strong, very informative prior for causal relationships.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1735.202,
    "end": 1739.087,
    "text": "Like all things considered, where there's an actual edge,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1739.827,
    "end": 1744.71,
    "text": " it's pretty viable to suggest that there's a causal efficacy to that edge somehow.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1745.413,
    "end": 1747.221,
    "text": "And then when there's no causal edge,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1748.905,
    "end": 1750.346,
    "text": " There could be telepathy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1750.406,
    "end": 1753.109,
    "text": "There could be like long range bulk electrical transport.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1753.749,
    "end": 1757.253,
    "text": "There could be relationships mediated through other local connections.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1757.293,
    "end": 1764.039,
    "text": "But I mean, on a first pass, nodes that aren't connected, it makes sense to exclude those as having causal relevance for each other at a first pass.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1764.059,
    "end": 1772.647,
    "text": "So the structural is a great, or if we were talking about a communications network, it's like we know which people talk to whom or which computers talk to whom.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1773.628,
    "end": 1778.372,
    "text": "That's a starting position for the causal architecture of the model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1780.192,
    "end": 1784.781,
    "text": " So that is, I think, an important role for the neuroanatomy in this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1786.565,
    "end": 1794.741,
    "text": "So here, it's going to be focusing on one columnar slice.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1795.396,
    "end": 1797.439,
    "text": " And it's going to show it three different ways.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1798.36,
    "end": 1803.927,
    "text": "Moving from on the left, we have the labeled cell types.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1804.447,
    "end": 1812.657,
    "text": "So this is the most neuroanatomical where the edges are reflecting the actual projections anatomically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1812.677,
    "end": 1816.302,
    "text": "Again, not all of them, et cetera, but this is just describing anatomy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1816.903,
    "end": 1818.765,
    "text": "It's not even a Bayes graph yet.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1819.986,
    "end": 1822.129,
    "text": "Then in the middle,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1822.632,
    "end": 1825.757,
    "text": " we see kind of a classic predictive coding",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1827.222,
    "end": 1833.651,
    "text": " where we have mu's for means and then like E's for errors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1834.492,
    "end": 1838.898,
    "text": "And then we see the I's for a given level and I plus one for a higher level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1839.659,
    "end": 1842.303,
    "text": "So, and the X and the V subscripts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1842.664,
    "end": 1850.034,
    "text": "So this notation, which of course is resembling, but it doesn't pretend to exactly recapitulate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1850.795,
    "end": 1853.719,
    "text": "This is the predictive coding type of,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1853.699,
    "end": 1869.636,
    "text": " computational map on this territory, which is you have top-down predictions, and then you have incoming bottom-up sensory evidence, for example, which could be coming from the thalamus or lower regions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1870.097,
    "end": 1872.062,
    "text": "And then you have an error term,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1872.768,
    "end": 1874.691,
    "text": " That's just checking the differencing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1875.092,
    "end": 1885.009,
    "text": "So this is going to be a lot like what we see in the spinal reflex arc, but this is happening in kind of like a more cognitive setting, a more general setting in predictive coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1885.57,
    "end": 1892.081,
    "text": "And then here on the right, we get to the POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1892.983,
    "end": 1896.649,
    "text": "This is the most kind of chapter seven ish",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1897.473,
    "end": 1898.476,
    "text": " generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1899.338,
    "end": 1907.719,
    "text": "And although there are some letters that are a little bit outside of the figure 4.3, broadly, we're talking about the hidden state inference",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1909.1,
    "end": 1918.317,
    "text": " So we kind of move from most anatomical to the very high level computational predictive processing paradigm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1919.519,
    "end": 1922.485,
    "text": "Active inference is basically predictive processing with action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1923.326,
    "end": 1925.931,
    "text": "And then, but this is in a continuous setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1926.693,
    "end": 1930.72,
    "text": "And then it's brought into the POMDP setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1935.357,
    "end": 1960.383,
    "text": " So within a column, whether we think about it like kind of purely anatomically, whether we think of the column as doing predictive coding, predictive processing, or whether we think of it as doing a POMDP with like it's carrying an S estimate through time and then observations are coming in and here there's kind of anticipation to the O tilde through time and expected free energy and all that POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1962.025,
    "end": 1963.727,
    "text": "If we think about that within a column,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1964.72,
    "end": 1976.615,
    "text": " There's an interesting way to talk about the relationships among columns, which is that they're arranged side by side in the cortex.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1978.157,
    "end": 1985.907,
    "text": "And each one of the columns is like one layer of a hierarchical Bayesian model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1986.968,
    "end": 1993.056,
    "text": "And then you have lateral connections, which that was a great paper, Peter, that showed",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1994.521,
    "end": 1996.701,
    "text": " these kinds of lateral connections,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1998.588,
    "end": 2000.31,
    "text": " go between the layers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2000.451,
    "end": 2007.721,
    "text": "And then also just like mentioned, like where we see summation, like excitation empirically, we can associate that with addition.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2008.342,
    "end": 2014.951,
    "text": "Where we see inhibition, we could talk about that as subtraction of like a likelihood or like a rate constants.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2015.892,
    "end": 2025.726,
    "text": "And then when we see multiplication, which is kind of the same as division, just whether you do like one divided by, but multiplication division is like a neuromodulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2026.263,
    "end": 2030.487,
    "text": " because it doesn't directly summate or subtract.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2030.707,
    "end": 2033.29,
    "text": "It's like a coefficient that multiplies that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2033.61,
    "end": 2034.831,
    "text": "So that's neuromodulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2035.532,
    "end": 2039.997,
    "text": "And then here is like, we have like a three level nested model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2040.837,
    "end": 2049.206,
    "text": "So the six layers perform the role computationally of one layer of a nested Bayesian model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2049.226,
    "end": 2052.709,
    "text": "So it's just like, we're predicting a three digit number",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2052.925,
    "end": 2059.735,
    "text": " And we're breaking down that problem, factorizing it into the ones, tens, and hundreds place.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2060.816,
    "end": 2066.484,
    "text": "And then here's the top-down prediction is what it is, and then the bottom-up is coming up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2066.905,
    "end": 2082.207,
    "text": "And that's how these three layers, even though each one is only holding one digit, that's how the three of them just super broadly would do a three-layer model here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2085.292,
    "end": 2091.358,
    "text": " Okay, so now we're going to trace this path.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2093.52,
    "end": 2100.747,
    "text": "This is the descending motor prediction coming out of, for example, motor cortical region.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2101.468,
    "end": 2110.157,
    "text": "It's going to be projecting down to regions involved in motor control and neurons that have motor write their name.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2110.177,
    "end": 2111.358,
    "text": "So they have to be doing it, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2114.261,
    "end": 2115.282,
    "text": "Here's the projection down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2121.067,
    "end": 2133.824,
    "text": " Now we're going to go from mu tilde here, basically, the Bates cells.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2134.225,
    "end": 2135.326,
    "text": "Might be interesting to look at them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2136.848,
    "end": 2140.093,
    "text": "Sending down the pyramidal tract.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2141.975,
    "end": 2149.285,
    "text": "And it is synapsing onto this essentially combining and differencer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2150.547,
    "end": 2157.476,
    "text": " Here we have observations, Y, coming in from proprioceptive tissues in like the elbow.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2157.957,
    "end": 2161.001,
    "text": "So there's like all these different kinds of touch and proprioceptive receptors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2161.041,
    "end": 2162.243,
    "text": "Like some of them look like onions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2162.343,
    "end": 2164.306,
    "text": "So when they're depressed, they activate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2164.426,
    "end": 2165.687,
    "text": "They have different like shapes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2165.727,
    "end": 2166.328,
    "text": "It's kind of cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2167.41,
    "end": 2168.211,
    "text": "That is coming in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2169.653,
    "end": 2178.765,
    "text": "And then it is being passed along from the dorsal horn through an internal synapse within the spinal cord to the ventral horn population.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2179.927,
    "end": 2180.808,
    "text": " And then there's a difference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2181.729,
    "end": 2192.481,
    "text": "And this is just a first pass approximation, but where the descending prediction is exactly matching the incoming proprioception, no action is required.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2193.682,
    "end": 2196.205,
    "text": "And that kind of makes sense.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2196.225,
    "end": 2203.653,
    "text": "That's consistent with this differential driven approach to basically making moves.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2204.333,
    "end": 2206.896,
    "text": "And then if you have,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2209.34,
    "end": 2220.476,
    "text": " I guess it's in a different chapter where it shows the set point changing and it shows how the motor dynamics are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2221.278,
    "end": 2226.666,
    "text": "But this is just, this is like a kind of sub-motif of predictive processing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2227.527,
    "end": 2229.129,
    "text": "But this is just a frame differencer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2230.291,
    "end": 2235.198,
    "text": "And this is also very related to PID control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2236.14,
    "end": 2236.881,
    "text": "There's a set point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2237.982,
    "end": 2238.503,
    "text": "And then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2240.12,
    "end": 2248.307,
    "text": " Yeah, again, to a first approximation, if the temperature of the room is, if you set it to 72 and it's at 72,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2248.996,
    "end": 2250.338,
    "text": " why would you take an action?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2250.899,
    "end": 2252.341,
    "text": "Maybe you have a more sophisticated model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2252.361,
    "end": 2256.808,
    "text": "You have a super strong belief it's about to get hot and you know it takes time to turn on the air conditioner.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2256.868,
    "end": 2266.142,
    "text": "So even though it looks perfect right now, you actually want to turn on the air conditioner so that it counteracts like allostatically a future prediction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2266.542,
    "end": 2276.878,
    "text": "But just at a first pass, if you don't have that kind of allostatic anticipation, if something is at the level where it's set to be, no further action is taken.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2277.198,
    "end": 2277.619,
    "text": "David?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2278.966,
    "end": 2279.306,
    "text": " Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2279.327,
    "end": 2285.996,
    "text": "I mean, you, you had mentioned also in terms of like message passing, like what would the dopaminergic response be?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2286.016,
    "end": 2305.463,
    "text": "Would it be an initial kind of like packet, you know, that would be sent, that would be a priority of, you know, the cognitive response itself before the decision is made and how might that actually, you know, be false signaling in some, some cases, if you're unaware of, you know, an a priority situation, for example.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2309.763,
    "end": 2311.007,
    "text": " Let's this is one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2311.027,
    "end": 2320.374,
    "text": "A wonderful title and also very interesting paper, and I think there's some other ones on this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2320.795,
    "end": 2323.182,
    "text": "topic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2324.63,
    "end": 2338.529,
    "text": " But Palumbo investigates from like a history and philosophy of science perspective on the dopamine discourse, the meta dopamine studies about, well, dopamine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2338.81,
    "end": 2341.133,
    "text": "I mean, if we look it up, let's predict.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2341.253,
    "end": 2343.296,
    "text": "Is it going to call dopamine a reward molecule?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2346.22,
    "end": 2347.902,
    "text": "Pleasure, satisfaction, motivation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2352.428,
    "end": 2354.451,
    "text": "What is dopamine does?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2359.004,
    "end": 2360.812,
    "text": " I mean, desire.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2360.852,
    "end": 2367.158,
    "text": "So it's framed in this kind of salience motivation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2368.589,
    "end": 2372.194,
    "text": " reward reinforcement, but there's other functions for it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2372.875,
    "end": 2393.044,
    "text": "And in the body, like we were talking about before recording, like with dopamine in the immune system and all this, but Colombo kind of looks at, and he says, look, the empirical evidence, even for the dopaminergic brain regions, like we're about to get to in the basal ganglia, like there's populations of neurons that do have a correlative firing pattern with reward.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2393.485,
    "end": 2397.651,
    "text": "There's also patterns that are firing more with a surprise model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2397.631,
    "end": 2401.441,
    "text": " So there's one other Columbo paper that's like an unrelenting pluralism.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2401.481,
    "end": 2407.116,
    "text": "It's just very interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2407.898,
    "end": 2410.746,
    "text": "I see it as a way that...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2411.418,
    "end": 2427.84,
    "text": " forward-looking philosophers of biology can actually stymie i don't know effectively or not but um like kind of absolutism or like closing the book on what dopamine does",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2427.972,
    "end": 2432.939,
    "text": " When it's like, actually, we already have enough evidence to say that there's a plurality of functions for dopamine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2433.981,
    "end": 2448.882,
    "text": "So then further empirical evidence can be brought into a plural framework where dopamine plays different computational or cognitive roles rather than the debate needing to be like, what computational role does dopamine play?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2449.583,
    "end": 2451.906,
    "text": "We found evidence that it plays a role in prediction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2452.367,
    "end": 2453.889,
    "text": "Dopamine plays a role in prediction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2455.171,
    "end": 2456.713,
    "text": "But if we have already kind of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2457.25,
    "end": 2486.582,
    "text": " laid the topsoil with pluralism then it's kind of like all right great more points for pluralism um more points for this function of dopamine but but it can't be resolved in in principle what dopamine does because we already know that we have some tallies across different functional categories um there's no dopamine in this um circuit yet no dopamine directly okay i don't think",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2486.9,
    "end": 2504.634,
    "text": " I don't know exactly when neurotransmitters are here, but dopamine plays more of a neuromodulatory role than, especially in the, it plays a role in the central nervous system that way, but in the peripheral system, and then like famously at the neuromuscular junction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2504.935,
    "end": 2509.043,
    "text": "So this synapse here is like where you have acetylcholine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2514.473,
    "end": 2515.014,
    "text": " Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2515.034,
    "end": 2527.626,
    "text": "So this is just kind of a simple, um, differencing algorithm, but differencing algorithms are like, I mean, they're, they're, they're the heart of many things happening.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2527.826,
    "end": 2541.981,
    "text": "And also of course, going to the history, I mean, predictive coding, predictive processing, predictive compression was invented by frame, differencing video compression in like the 1970s and eighties.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2542.661,
    "end": 2543.282,
    "text": "And then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2546.08,
    "end": 2559.384,
    "text": " There is the Rao and Ballard mega citation 1999 paper that was like, oh, classical means whatever was done before, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2559.785,
    "end": 2562.79,
    "text": "So then there was these extra classical effects.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2563.36,
    "end": 2565.662,
    "text": " in the visual cortex.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2565.682,
    "end": 2583.738,
    "text": "And then they were like, wait a minute, if we think of what the visual cortex of doing as prediction, then that explains why there are these extra classical phenomena that are not being evoked by the stimuli because they're being invoked by the predictions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2584.999,
    "end": 2592.726,
    "text": "So then that kind of like broke, I'm sure there was like a little bit before too, but this big citation kind of broke the wall",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2593.246,
    "end": 2603.847,
    "text": " with just the idea that like hey even primary sensory processing is predictive so it's only been 25 years",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2604.417,
    "end": 2616.939,
    "text": " And that's the interesting thing is moving from the concept of, well, signal processing and sensory transduction is related to processing and distilling and extracting the information from the real good sensory data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2617.54,
    "end": 2619.684,
    "text": "It's so much out there and we're just getting the good stuff from that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2620.466,
    "end": 2629.041,
    "text": "Versus actually the sensory data coming in are like lagged, noisy, biased, partial, etc.,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2629.358,
    "end": 2637.426,
    "text": " However, they just kind of help tune an ongoing multimodal generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2638.266,
    "end": 2650.718,
    "text": "And that's why our visual field has color everywhere, no blind spot attention, all this other stuff, because we're not just like processing and sharpening the TV signal and then displaying it on the inner screen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2651.399,
    "end": 2659.146,
    "text": "There's an inner generativity that's multiple hierarchical nested levels away from the primary sensory information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2660.662,
    "end": 2669.533,
    "text": " But this may have made sense to look at before this one because this is a simpler motif.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2669.553,
    "end": 2677.242,
    "text": "It's kind of a core motif is this just concept of like take the difference and then just use the sign of the difference, positive or negative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2678.503,
    "end": 2681.707,
    "text": "Like at a first pass, that's the direction to go.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2682.188,
    "end": 2684.39,
    "text": "I mean, if there's a speed limit and you're over it, go down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2685.031,
    "end": 2686.753,
    "text": "If you're under it, go up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2686.733,
    "end": 2715.137,
    "text": " unless you have like some other higher order reason to do other than what seems obvious which is very cybernetic right yeah it's it's just like it's like bread and butter first order cybernetics and then second order and higher order cybernetics would would kind of be like where and when and how",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2715.522,
    "end": 2720.432,
    "text": " does the first order cybernetics lead to like a death spiral?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2721.934,
    "end": 2729.569,
    "text": "And then also could relate maybe to system one and system two, and just on a first order, take the heuristic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2730.271,
    "end": 2737.665,
    "text": "But then to override heuristics, which is now, this is exactly where it takes us here, is a more challenging thing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2738.877,
    "end": 2759.822,
    "text": " um path so here it's an interesting like visualization i think this was kind of like the scales of justice in a way because it's not saying that the left and the right basal ganglia have different topology it's actually just like showing two modes of function",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2759.802,
    "end": 2789.117,
    "text": " that are like it's like a rorschach blot unfolded and then the two sides are like the two different modes but it's not it's not saying that it has these two different architectures on the other side but basically there's two paths they could the indirect and the direct pathway are both bilateral as far as i understand but they're just separating them out here so these are different brain regions um with their acronyms it's a simplification okay um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2790.633,
    "end": 2795.741,
    "text": " In the indirect pathway, observations are coming in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2795.901,
    "end": 2801.589,
    "text": "And then there's O tilde, O through time, of a given policy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2802.671,
    "end": 2804.033,
    "text": "And here's E, the habit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2804.193,
    "end": 2805.835,
    "text": "This is the prior on policy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2806.537,
    "end": 2817.012,
    "text": "And then basically, though it's indirect, the prior on policy habit is just being passed on to what we choose.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2817.565,
    "end": 2827.523,
    "text": " So this is just kind of carrying forward our prior on what we do, and then either choosing the best ranked option or sampling from that policy posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2827.903,
    "end": 2831.189,
    "text": "So it's kind of like saying, let's not do a policy update.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2831.971,
    "end": 2835.557,
    "text": "Let's just make our policy posterior our policy prior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2836.258,
    "end": 2841.227,
    "text": "So no calculation of expected free energy, no G coming into play.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2842.557,
    "end": 2849.665,
    "text": " equivalent to not paying attention to observations, to incoming observations or expected future observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2849.685,
    "end": 2851.087,
    "text": "So it's mega simple.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2851.748,
    "end": 2860.357,
    "text": "And there might be an adaptive habit that in a given environment for which that habit was developed adaptively for, that's super simple.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2861.779,
    "end": 2872.311,
    "text": "On the other hand, there's a direct pathway mode that they're analogizing as actually doing the expected free energy calculation, G,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2872.544,
    "end": 2883.222,
    "text": " and updating the policy prior into the policy posterior, according to equation 2.6, the whole part about pragmatic and epistemic value.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2883.803,
    "end": 2890.013,
    "text": "So it brings in a ton more variables and computations, but it sharpens",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2890.094,
    "end": 2895.426,
    "text": " the prior into the posterior based upon the incoming observations and expected future observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2896.048,
    "end": 2907.715,
    "text": "So that's kind of like the deliberative system two thinking, whereas this might be analogized to the heuristic faster system one thinking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2907.955,
    "end": 2908.356,
    "text": "David?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2909.652,
    "end": 2909.953,
    "text": " Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2910.033,
    "end": 2920.892,
    "text": "How would that affect, like, if you're skipping that first part, would that direct to the map of like the ontological directly without then prior updating?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2921.092,
    "end": 2927.323,
    "text": "So you're kind of going without a map in that sense on the logical side of things, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2929.345,
    "end": 2930.227,
    "text": " That's interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2930.287,
    "end": 2933.993,
    "text": "Like habit is kind of like map free.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2934.033,
    "end": 2938.02,
    "text": "It's kind of like, not just, this is my turning distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2938.06,
    "end": 2939.984,
    "text": "I go 50, 50 left and right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2940.705,
    "end": 2947.877,
    "text": "And I'm just sampling from that wherever I am versus like the more deliberative would be like, well, if I turned left, then this is where I'd be.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2948.438,
    "end": 2950.141,
    "text": "If I turned right, this is where I'd be.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2950.983,
    "end": 2954.509,
    "text": "So the habitual is a more implicit distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2954.489,
    "end": 2977.881,
    "text": " ontology it inherits from the the ontology that that supports that habit whereas the explicit basically enumerates the ontology and creates like a local projections that could be based could be also leveraging information like from external maps or other maps andrew",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2981.557,
    "end": 3004.341,
    "text": " Oh yeah, I was just wanting to relate it to like the authors in the textbook relate like the computation and use of the empirical priors E in policy selection as like being akin to like a model-free system versus the use of G and computation of that to inform like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3004.878,
    "end": 3023.376,
    "text": " um policy selection and policy comparison is like a model based system which kind of made a little bit more like helped me track that a little bit more as someone with more of a data science rather than neuroscience background but um yeah no it's it's super interesting i'm trying to remember another paper",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3023.356,
    "end": 3046.368,
    "text": " that I had read that, you know, with, with, with the act in terms of like confidence and so on, there, there was one paper, I'll try to remember it, but it likens like an agent in its environment who has confidence in its model and also receives some kind of confirmation that its model is actually helping it to realize its preferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3046.988,
    "end": 3048.891,
    "text": "It's as if it becomes more",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3048.871,
    "end": 3077.629,
    "text": " confident in its um ability to use a model to compute g parentheses in order to realize its preferences right versus if it repeatedly hits like obstacles there might be a sort of regression into like oh my my confidence in my model is is very poor it relates to that um relates back to that gamma term as a kind of like precision or or or temperature or",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3078.385,
    "end": 3106.519,
    "text": " term that gets involved with the g computation in any case yeah whenever the agent becomes less confident in its model and its ability to actually compute like useful policies it starts to kind of regress back into like habitual behaviors that it has previously relied upon and then from there I presume like you know over time you might very well end up modulating your",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3106.988,
    "end": 3111.555,
    "text": " your habits as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3111.615,
    "end": 3119.988,
    "text": "Like if we view those as like values over policies that are sort of ready to go, like those get changed too over time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3120.008,
    "end": 3123.353,
    "text": "But anyway, I'm kind of rambling at this point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3123.373,
    "end": 3124.234,
    "text": "Those are great points.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3125.616,
    "end": 3134.329,
    "text": "This has always been where I get hung up on with a model-free, which is model-free just means simpler model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3136.182,
    "end": 3151.715,
    "text": " So, you know, when some things like next generation this or this, you know, none of this, but then it's like, it just, it's hard to keep track because people are using those as kind of like relative terminology, but it's even used in chapter 10.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3153.397,
    "end": 3161.904,
    "text": "I think it would make sense to have mentioned model free in chapter five, because it literally is mapping to this arbitration.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3164.326,
    "end": 3165.767,
    "text": "And then just in the last,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3166.473,
    "end": 3176.751,
    "text": " I mean, 5.1 describes work in the act-inf space and by others with linking the computational functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3176.831,
    "end": 3184.043,
    "text": "Oh, and dopamine is playing the judge-jury executioner role with a dopaminergic tone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3185.018,
    "end": 3207.938,
    "text": " setting the difference between habitual and deliberative behavior but there's a lot of like there's a lot of effects of dopamine so that's why it's not so simple as just taking like you know you know yay dopamine or nay but these kinds of things some some with friston and others others just more generally um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3212.744,
    "end": 3219.033,
    "text": " The pupil, I think the eye saccade and the pupil modeling is really interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3219.614,
    "end": 3225.803,
    "text": "Those are things that most people can't control consciously and they're not aware of consciously, but they reveal a ton about the generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3227.646,
    "end": 3241.907,
    "text": "And then they kind of allude to the favorite synthesis topic of this textbook, which is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3242.36,
    "end": 3246.025,
    "text": " Continuous and discrete hierarchies coming together.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3246.145,
    "end": 3248.688,
    "text": "Like every chapter, it kind of comes up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3249.389,
    "end": 3251.412,
    "text": "And then here we have it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3253.114,
    "end": 3259.562,
    "text": "If we use that, the rightmost form here, the POMDP discrete time-like form.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3260.003,
    "end": 3263.968,
    "text": "So we have a discrete neurosymbolic model here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3265.17,
    "end": 3271.438,
    "text": "And then down at the frontline workers, we have the continuous sensory motor model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3272.465,
    "end": 3281.665,
    "text": " form and then we have the policy selection module is not directly linking down",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3281.78,
    "end": 3282.661,
    "text": " to the spinal cord.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3282.701,
    "end": 3288.528,
    "text": "So it's not like prefrontal basal ganglia and then sends the message down to the spinal cord.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3288.548,
    "end": 3294.995,
    "text": "It's like the prefrontal and the basal ganglia are in a dialectic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3295.676,
    "end": 3307.669,
    "text": "And then there's a descending motor prediction that reflects multiple scales of the resolution of that dynamic by just passing down the set point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3308.999,
    "end": 3314.064,
    "text": " And then that is enacted autonomously with what's called a reflex arc.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3323.494,
    "end": 3325.295,
    "text": "They bring up a ton of other questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3331.822,
    "end": 3333.443,
    "text": "Cool topics for future discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3338.689,
    "end": 3338.989,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3340.606,
    "end": 3341.31,
    "text": " Thank you, everyone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3341.411,
    "end": 3343.885,
    "text": "See you later or next time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3344.83,
    "end": 3345.956,
    "text": "Have a good one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3345.977,
    "end": 3347.123,
    "text": "Thanks, everyone.",
    "speaker": "SPEAKER_00"
  }
]
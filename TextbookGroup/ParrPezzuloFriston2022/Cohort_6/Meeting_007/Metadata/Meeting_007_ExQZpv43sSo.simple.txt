SPEAKER_03:
All right, welcome back.

We're in the second discussion on chapter three.

So does anyone just have any quote or question or anything to kind of begin with?

Any thoughts where we can just start to look at the written questions from before?

Or some page or figure from three?


SPEAKER_02:
A basic sort of disorienting thing, I guess, in the diagram at the top of page 44, where it shows external states, internal states, action states, and sensory states.

I'm confused about what are realistic examples of influence on that direct arrow between active states and sensory states.

Like, again, I understand active state or the point of active state generally is to impact upon the world.

And like, eventually we see the effects of that in sensory states, but I guess the direct double arrow between the active states and sensory state seems confusing to me.


SPEAKER_03:
Yeah.

It doesn't have to have any influence in the real system, but it's, it,

it could but i think in a if we if the sensors and actuators of the system are separate then that edge may have no effect but in principle it exists because the the the states are not like pre-tagged it's always just based upon the choice the internal state

So a causal edge here.

In sketching out the specific system, you would quickly determine whether it's something causal in the system or not.

We'll see if it comes up in any of the examples.

Susan?


SPEAKER_01:
Yes.

So can you

I'm really unclear in what are the boundaries between the external and internal states?

In other words, specifically, what is action in this context?

And I'm wrestling with speech acts.

Not doing something is still an action on your environment if you think about it.


SPEAKER_03:
Yes.

Well, one aspect of action that is not really looked at in the textbook is deciding when to act.

Like in the kind of simplest format of the action perception loop, there's just a perception and an action every single time step.

So like the choice to like wait versus to do something at all, sometimes that's brought in as like a null action.


SPEAKER_01:
Okay.


SPEAKER_03:
Yeah, just like an action that kind of like keeps you where you are.


SPEAKER_01:
Okay.


SPEAKER_03:
There can be a nested model where the top is a binary decision, you know, make a move or not.

Within make a move, there's just the four moves.


SPEAKER_01:
Okay.


SPEAKER_03:
is is that have anything to do with the um transition function the um the the transition function is describing how the state the hidden states change through time so it's the beatrix

We're getting an observations and then the a kind of match the observations to the hidden states so from the thermometer readings to the temperature in the room, and then the transition function is the temperature of the room changing per delta T just per time interval.

The temperature in the room.

underlying changing.


SPEAKER_01:
And the formulas on the right there, that's actually for the transition functions?


SPEAKER_03:
Yeah.

So then the three here, which is B, is saying it's the distribution of the state at the next time point conditioned upon the state at the current time point and the policy.

So it's like the temperature in the room at the next time point is conditioned upon the current temperature in the room and whether I'm turning on the air conditioner or not.


SPEAKER_01:
Okay, okay.


SPEAKER_03:
And then which policy is like selecting which slice of B gets applied as the transition function.


SPEAKER_01:
And one last question.

So is this model just saying that there was a move made and there was a, and that's a set, is that, you know, as I'm seeing two different, is that perception action or is that just a different action repeating?

in the picture, you know, just looking at the, yeah.


SPEAKER_03:
Like the fact that there's two threes here?


SPEAKER_01:
No, actually, because you've got the upper and the lower.

So is that the higher high road, low road?


SPEAKER_03:
No?

So 4.3, the top graph is the discrete time model, and the lower graph is the continuous time model.

Okay.

And then in...

Here's just the discrete time model.


SPEAKER_01:
Okay.


SPEAKER_03:
Just the top half of figure 4.3.

Okay.

So here again, we see the B and it's the hidden state at the next time point, tau plus one.

Yeah.

Conditioned upon the current state and the policy.


SPEAKER_01:
Okay.

Thank you.


SPEAKER_03:
And I think as we kind of may have alluded to earlier, there's like this representation of the generative model, which is the partially observable Markov decision process.

And then there's this particular partition representation.

So here we have the kind of hinting at a circularity and the flows.

because the dot on top is the derivative.

So the flows across these four states, which we can associate from the perspective of these internal states as the sensory action and external.

And then there's this form, which is more like a branching tree a little bit, because here the observations are a dead end.

I don't exactly remember what we got to last week, but I think we looked at figure 3.2 with the scatter plots.

Does anyone want to ask a question, or should we just try to go through these?


UNKNOWN:
Okay.


SPEAKER_03:
Oh, I'm sorry.


SPEAKER_02:
That was a hiccup by accident.

I apologize.


SPEAKER_03:
Okay.

All right.

Let's look through some of these.

What's the difference between the terms Bayesian brain hypothesis, predictive processing, predictive coding, actin inference?

So it's discussed in several places.

Here's one reference.

these terms can be just kind of broadly used to refer to very similar uh theories and kinds of models or if you really get down to the last mile they could differ um bayesian brain hypothesis hypothesis suggesting it's something that could be tested and falsified for a given system like you can hypothesize that the brain is doing

Bayesian computation or is doing something that can be modeled with Bayesian computation, you could hypothesize that and have criteria for why you would accept that hypothesis or not.

And then you might as well do that and then find out for not.

So this is testable hypothesis describing whether brains, which are just some systems in the world, but a tiny subset of systems do Bayesian computation, however they do it or whatever their distributions are.

um active inference probably a lot of ways to define it in a broader sense it could just refer to models that have action included within the scope of inference like models that do sense making and action selection or to be more specific we might be talking about active inference models like perception action models under the free energy principle which is kind of chapter three high road and then predictive processing and predictive coding

Again, these are sometimes used like interchangeably, but to get technical about it, predictive processing is the kinds of architectures that will get visited in this textbook where there's a multi-scale model and higher levels are making predictions about lower levels.

And the entire processing architecture has to do with predictions, hence predictive processing.

Predictive coding kind of nuances that a little bit and highlights that the messages that are being passed encode predictions and prediction errors.

So you have predictive processing systems that do predictive coding.

you could also design one that had like one or the other so they're all really related but they're not exactly exactly the same term if one defines preferred states as expected states

then one can say that living organisms must minimize the surprise of their sensory observation.

I'm not clear on the ontology here.

Is it that preference is the same as expectation or that formally we can use them in the same equation?

There's a lot of partial related questions here.

i'm not sure i don't remember if we looked at this one last week when we're talking about the variable called preference c it's a distribution over observations i think last week we talked about how expectation can be used in kind of two senses one is the expectation is like about an event that hasn't happened yet

And that's a conversational usage of what we expect to happen in the future.

Also, a given distribution can have the expectation calculated of it.

That's the fancy E. And that's the same thing as saying the central tendency or the mean of that distribution.

In the case of pragmatic value in expected free energy equation 2.6,

Yes, when we talk about preference, we're talking about an expectation about observations.

But just saying expectation alone is almost not a complete sentence because it's like saying, well, I'm talking about the mean, but the mean of what variable?

So there's settings where it's clear from context what it means and that we're talking about the same variable, but you could also imagine a situation where you're talking about the central tendencies or the future possibilities of other variables.

So it's not that the terms are simply identical by definition, it's that in certain kinds of generative models, they're described the same way to highlight that there isn't the proposal of like a reward function, but rather there's a direct surprise bounding on expectation.

Aaron?


SPEAKER_02:
Yeah, well, okay.

This isn't exactly a refinement of this question, but it's related.

I guess like I have been struggling with how to think about, I guess, the distinction between priors and preferences in contexts where you, in advance of receiving some observation, have reason to expect

and I think in the narrow sense, that conditions will be other than you would prefer, right?

So if the weather report this morning says it's going to be 115 in the afternoon and I, you know, my car breaks down and I gotta, before I open the car door to get out on the side of the road, I expect it to be uncomfortable and challenging to my body's need for homeostasis.

I guess like in a,

Maybe it's not an ontological distinction, but I am struggling with under what conditions is reasonable to substitute a distribution that describes preferred states into a context where you would be talking about a distribution that describes your belief about states prior to getting more information about them.


SPEAKER_03:
Yeah, great question.

So let's just say we're talking about temperature.

So our C preference distribution is going to be like homeostasis.

It's just a Gaussian distribution around 37 C.

So we're going to talk about that as our preferences.

Now, it's also a prior.

That's why these things are like not complete sentences.

It's not that there's one variable, just only and always called prior.

Because any distribution that could be updated, you might be talking about the prior distribution of that distribution.

So hence, it's like when talking about like beliefs, it's like about which one of these priors, prior distribution about which one.

Now, when people don't further...

define which prior they're talking about usually they're talking about d the prior on hidden states so let's just say that um we're in the desert and we we have an accurate thermometer so our a matrix is is just the identity matrix just the ones on the diagonal so we're just getting accurate thermometer readings um we prefer 37 in terms of our homeostasis

but our prior belief is that it's gonna be 50.

And then we get a thermometer reading of 50, and then that's consistent with the hidden state estimate of the temperature being 50.

So there's totally no issue with preferences over observations interfering with the sense-making process, because C doesn't come into play with A or D.

However, the C does shape action through a pragmatic value, but C doesn't play a role in sensemaking or in estimating future time steps at all.

Is there a minimal example of a thing?

Is it correct to say a thing is the temporary persistence of a Markov blanket?

Perhaps that it's the persistence of something that can be modeled with a Markov blanket.

Gareth J. Are all things, then just non equilibrium steady state processes well considered one way they're at an equilibrium point to allow them to be measured.

Gareth J. But by virtue of our scenario overall big dissipative the fact that they're existing.

Gareth J. Is non equilibrium.

Do we need to be more strict in saying these things are only self-evidencing things rather than things in the colloquial sense?

Totally depends on what's being discussed.

If it's an inner particle, then it may not be providing any extra information to say that it's self-evidencing.

Like to say that rock is fulfilling predictions that it expects to be there.

First issue, you may not be adding any richness to the account.

And second off, you're proposing a map that's indicating some causal capacities for the rock that are unrealistic.

Whereas when you start getting into that squirrel is acting like XYZ, then you are creating parsimonious accounts of behavior that can't be accounted with simpler.

and you are making maps that have features that that system might actually have.

So the squirrel self-evidencing, it's probably more of a value add in terms of the account, and it's probably more plausible mechanistically.

What is the counterpart in an active agent to conserve versus dissipative physical systems?

What is the analogy to friction in a cognitive system?

How strong is this analogy?

That's an interesting question to explore.

One possibility is like, what is heat and what is work in Bayesian mechanics?

like if all of the information of the observation were used for belief updating, that would be like extracting 100% of the efficiency of the fuel.

If you perfectly burnt the fuel, the maximum extractable heat as work would be like from

the hydrocarbon down to whatever the terminal product of burning was.

Like you could burn the candle and extract all the heat from the candle or like how they test how much calories are in food.

Like you just burn it and there's a maximum of energy to extract.

So then by analogy, there's like a maximum amount of information to extract from a given signal, like described by its entropy.

So there might be something like how much of the entropy of the signal is going into belief updating versus how much is being partitioned to like dissipative.

But I'm not sure about friction specifically because friction isn't just the same as heat.

Okay.

Chapter 3, High Road.

Starts from the premise that to survive any living organism has to maintain itself in a suitable set of preferred states.

These preferred states are first and foremost defined by niche-specific evolutionary adaptations.

However, as we will see later in advanced organisms, these can also extend to learned cognitive goals.

I'm curious about the applicability of active inference to social structures such as economic systems, firms, self-organizing groups, and government systems, amongst others.

One can argue that social systems have preferred states created from both evolutionary niche and learned cognitive mechanisms.

Has active inference yet been applied to these sorts of systems?

Yeah, cool that some people have added some links.

Yes, applied to a limited extent.

Ali explains in the intro video Markov blanket is a boundary in the state space, not necessarily a spatial temporal boundary.

If we were talking about a spatial temporal state space like GPS coordinates comma time, then a boundary in the GPS comma time space would be a state space boundary corresponding to a spatial temporal boundary.

But if our state space were talking about something that wasn't spatial temporal, then the Markov blanket is not necessarily spatial temporal, even in the map, even before getting into the map territory stuff.

What allows the agent to be statistically separated from its boundary as observed through its blanket?

So I take it the blanket is a lens through which the agent observes their environment.

Yes.

Yes.

Would this lens be defined by their affordances?

Yes.

Just like on the inbound, it's kind of like glasses.

Like they're a real, like a lens.

And then like light comes in and gets focused in.

But then also if it were a light source, it would diffract out the other way.

It's not exactly the same because inbound perception isn't exactly the same as outbound action.

But it is like a two-way lens that has...


SPEAKER_01:
communication across interface susan wait on mute but then yes susan oh sorry um so i was listening to one of the uh videos and i've heard um kristen say that markov blankets were you know were really just about computational models

What do you have to say about that?


SPEAKER_03:
Like he meant that they were being introduced as a computational tool rather than like an ontological claim about reality or what?


SPEAKER_01:
Okay, that makes sense.

Thank you.


SPEAKER_03:
Like, let's just say we had 10 variables.

We could test a model that every variable influenced each other.

And every three-way combination matters uniquely and every five-way and seven and nine-way.

It's like we could test kind of the fullest possible model, but that's more parameters to estimate.

So it reduces the statistical power per parameter.

so this is a fundamental trade-off in statistical modeling which is like um the fewer variables you include for a given sample size the more statistical power you have however you run the risk of partitioning variants inappropriately whereas the more variables you include

that may come closer into alignment with what is like believed to be kind of like a very multi-causal nexus.

So it can kind of feel nice and appropriate to enable the possibility of intersections mattering for causality.

However, that can make the causal state spaces very large, diluting the statistical power to detect anything.

And so models are chosen to be at a trade-off point between strongly explaining too few factors and weakly explaining too many factors.

And that's why how many parameters are included in that kind of best performing trade-off model, that kind of parameter estimation is about the map, not about the territory.

does the distinction between active learning and active inference have to do with these important things?

I don't remember exactly which paper the active learning was, but I remember that, again, sometimes the term is used in a narrow or a broad sense.

In a broad sense, active learning and active inference are the same thing.

They could both be used to describe a system that's active and learning.

But then if you really get into it,

A paper might say, well, we're using this to mean this specifically.

So without the link, I don't remember exactly what was meant by active learning.

Are internal state parameters referring to the edge of the Markov state space?

No, they're referring to the inside.

Markov blanket is the edge like the crust around the internal state parameters?


SPEAKER_01:
Is there any identification for the edge of a Markov blanket?


SPEAKER_03:
Like whichever

whatever variable you're going to point to and say, that's the one that I'm going to call internal.

So that's just, that's a choice by the modeler, which one is going to be called internal.

Then all of the crust around it, all of the envelope that encloses it is the blanket of that state.

But if you had chosen a different state to choose as internal, you would have ended up getting a different set of blanketing nodes.

kind of like let's just say um you know we're we're doing google maps and so we're gonna say um for this trip from a to b this is the first street this is the last mile and then this is like the space between but if you had chosen a different journey maybe the street that was in the in between for the first journey is going to be the first mile for this other journey

In advanced organisms, preferred states can also extend to learned cognitive goals.

And in advanced organisms like human, they can achieve preferred states by increasing the abstract social cultural strategies.

My question is, if they are cast in terms of active inference, must all of these preferred states and strategies be ultimately linked to survival in some way?

Or in the case of advanced organisms, can free energy be related to something other than survival?

If acting that way was incompatible with existence, you wouldn't see it.

If acting that way was deleterious to fitness, you'd see it less.

So that's the kind of evolution-like component.

which is like, it doesn't mean that every single phenomena was necessary and sufficient for existential success, but certainly measuring something means that it wasn't incompatible with existence.

So then by the time that we're talking about organisms that might be with all kinds of different cognitive phenomena, especially if they're going to be like a new or changing niche, you could have maladaptive, you have a ball rolling downhill, and you could just be observing the last five minutes of a maladaptive trajectory of the ball rolling downhill.

so like past returns do not guarantee future returns or whatever they say can markup blankets be considered as decoupling an agent in an environment yes you could think of it as how they are coupled which is to say how they are decoupled like how they are articulated

The joint is both how things are connected and how they're separated.

Can surprise only be diminished through action?

So here's a quote from chapter two, pointing to the two ways that discrepancy is addressed in active inference, change of mind or change the world.

So if we're in the temperature situation, we prefer to be homeostatic and the temperature is higher than we expect, then we can either take affordances that we believe are going to bring the temperature back into alignment with our observation preferences, that's pragmatic value, change the world,

Or we could learn and come to dissipate the discrepancy just by updating what we expect things to be.

However, for like a physiological prior, those may not be learnable.


SPEAKER_00:
Huh?

I guess...

uh what happens if the observations uh were surprising and then they fall back into um non not surprising um values like the temperatures spiked up unexpectedly but then it just came back down to normal yes that's a good question so like um


SPEAKER_03:
a related question is like, what updating strategies are adaptive under what patterns of environmental regularity?

So if the environment was like very slowly changing, then a very slow changing belief updating might be appropriate.

If the environment were very rapidly changing, then you might want to track it more rapidly.

If the environment had like one in 10 times, let's just say there was like just erroneous measure.

So if it was like a noisy environment where sometimes you got like wild measurements, but then they were just like part of noise, then you would have something like a Kalman filter, which is basically what's happening here in the temperature example.

This kind of looks like CO2 measurements in the environment too, but it's kind of like you have these noisy measurements coming in, and then the Coleman filter is basically saying, how can we smooth this?

In the extreme case, it smooths just taking the moving average in this extreme slow case.

In the extreme fast case, the smoothing is just tracing exactly what you're getting, but then there's somewhere in between

that like smooths out some of the noise.


SPEAKER_00:
Could you put this under the umbrella of the action of updating your own beliefs?


SPEAKER_03:
Yeah, like moving a belief distribution could be understood as a cognitive action.

Like if you had the choice to do it or not.

or if you had the mental action to attend, if you had the, you have mental action on the dial of how much to pay attention to it.

Zero attention to it means it's gonna have no impact on your beliefs because if you don't pay attention to something, it's as if it didn't happen.

100% attention to it means you're gonna update your beliefs 100% to exactly whatever the last thing that was said was.

So then that would open up an internal policy

possibility of how much should I attend to this environment?

So then in a noisier environment, you would want to reduce your attention to make your smoothing slower, to average out more.

Whereas if you were in a more precise environment with like more relevant or salient information, you'd want to increase your attention to maximize information gained.


SPEAKER_00:
But then when we're measuring supplies and the temperature spikes up and then it spikes back down, I guess you could say that spike itself is a surprise, but then now your price for the temperature is kind of normalized again.

So, yeah, I guess, you know, like,

it could be a little bit confusing on how you think about, you know, supplies is only minimized through action.

But in this case, it kind of feels like it was minimized just from the environment itself.


SPEAKER_03:
Yeah, interesting.

Susan?


SPEAKER_01:
So is there a specific parameter or computation to account for the capacity constraints?

Amy Dabbs, And like cognitive capacity or cognitive control.

Amy Dabbs, You know, people slip into a state of overwhelm they're going to change state so yeah this would be a back to the transition, but I didn't know if it was.


SPEAKER_03:
David Larson, That that's really interesting I mean let's get you down and explore it because I think that's what we're not going to find that in the minimal.

P. O. Mdp.


SPEAKER_01:
Maybe just a set change, I guess.


SPEAKER_03:
Yeah.

But if we if we let's just say that we we imagine two internal states that the agent can be in one or legit.

What about So how about instead of too cold just right and just hot?

there's an internal model that basically is this epistemic, is this too boring?

Just right?

Or is this too much?

That doesn't mean that that's the ultimate truth.

You could have a temperature estimating agent that actually thinks that there's rooms that are too hot, but actually it could do just fine to them.

Just because it's the agent's estimate doesn't mean that it is

that but so there's there's the um the agent is doing um estimate it's getting challenge on the y-axis and then it's estimating how difficult is this content and then it's doing some estimate and then like the the accurate estimator

would have um basically would would have would know when it was in boring just right or too hard whereas you could have an optimistic estimator that you know skews to one way or the other way and then each of those three states might be associated with um like maybe in the in the just right phase maybe that's when there's a feeling like there's the highest amount of potential control

But then if something's too easy or too hard, somebody might feel like there's no point in exerting cognitive control because it won't matter.


SPEAKER_01:
Well, it'll matter if you have ADHD.

So, yeah, managing that.

I mean, it's the energy.

It's the entropy.

It comes back to the entropy.


SPEAKER_03:
Yeah, too ordered or too complex?

Yep, it does not have this.


SPEAKER_01:
Yeah, that would be good to add to that graph.

The flow graph.


SPEAKER_03:
Too redundant or too novel?


SPEAKER_01:
Yeah.


SPEAKER_03:
How is Markov blanket a restatement of the classical action perception cycle?

There's probably a lot of ways to... It's a bit of a restatement.

Perhaps because... That's so funny.

Like, cybernetic agent is like... I was expecting more that looked like this.

But hey...

mean so for for many decades i mean this is more similar than not to what we've just been discussing with predictive processing and active inference incoming sensors with a reference prediction and then the differential goes on to influence the actuator

that acts upon the external system which then responds and then passes another sensor observation um so conceptually like no one really disagrees that observations are coming in actions are going out and there's like causation on both sides or you know so the reason why the markov blanket is a bit of a restatement is because this cybernetic loop

was phrased before the Bayesian causal graph literature of the 1990s like with Judea Pearl so active inference restates the classic action perception cycle in terms of the causal graphs that have existed since the 1990s I will say I want to say that uh


SPEAKER_01:
The cybernetic viable systems models come a long way too, so don't throw the baby out with the bathwater.


SPEAKER_03:
No, definitely not.

Dave Douglas is a working lot with American cybernetic society and and I think the combining our journal work with the.

International Society for system science and cybernetics like it will reveal many, many.

yeah this these are these are like it's just here it's like well the circle is kind of conceptual but now we have a technical definition for that boundary in terms of causal graphs Markov blanket or in terms of category theory but um like Robert Rosen's ecology I don't know how useful the um

wikipedia's but he developed a lot of category theory ecology models again ones that that presage a lot of what were like here but it's just it's just that it's it's a schematic so it's very evocative

What's really different now is there are open source software packages and first principles formalism.

That's like, this is literally, far more similar than not.

Even this looks like those Markov blanket scatter plots in the chapter.

There's more similarities than not.

Hence, trying to get them all together.

Living systems resolve fundamental biological problems by exerting active control over their states.

is the way to understand why automatic mechanisms are active in thinking that these are still involved in calculating EFE, but the time horizon is small.

Is there another concept of passive control or is all control active?

Take the example of touching a hot kettle.

probably a lot of ways to think about this one um but the automatic mechanisms being active like I I don't think that they always need to have planning like for example secreting insulin and glucagon

and hormone regulation of blood sugar, that could just be reflexive actions, like as something gets to here, then more of this hormone is secreted, and then as it goes down, more of that hormone is secreted.

So that's still active control, but it's just reflexive active control.

Is there a concept of passive control that's almost like action without action, or like the ability to control something without acting?

Kieran Stonewall- Which you could probably design a situation where that does happen like where by not by non action the agents has the other system.

Kieran Stonewall- aligned with its preferences, but suffice to say it wouldn't be doing so by virtue of actions that it took i'm Aaron.


SPEAKER_02:
David Price- yeah I guess riffing on this and i'm in the chapter and a few other places in discussion on.

David Price- The the concept of like temporal depth of a system and like its ability to plan has come up and I guess like this is.

David Price- This is partly in this question we're talking about like some actions that a system undertakes are informed by that temporal depth, but some actions that the same system undertakes are.

really shallow, like removing your hand from the hot kettle, you don't need to see very many steps in the future to understand that's like the highest value thing you can do.

But so I guess that does that that also implies something about there being like a heterogeneity in the thickness of consideration of actions that the agent is driving.


SPEAKER_03:
Yeah, definitely.

Like,

the organism is a multi-scale phenomena so even in the same moment like it might the best map of what I was doing one hour ago might have been um regulating uh oxygen homeostasis over a second time scale but then you know food over a day-long time scale but then like financial you know well-being over a year so you could make multiple time scales of action

Even a slow action is happening in a moment.

And then the kind of no planning model, first off, it's the computationally simplest, so it's a great starting point.

And then you can look for deviations and then ask, do we need additional phenomena to account for what we're seeing?

Like birds flocking,

like the famous models don't involve any planning.

It just involves the visual field.

And basically it's like, if they're too close, like you pull back.

And then if they're like too far, you move towards them or something like that.

So then if you do a model and you find that you can replicate the statistics of this empirical flock,

There's no need to invoke planning, but then maybe there's a residual with some real system.

It's like, oh, actually, you know, let's try, let's see if planning could explain that variance gap.

Susan?

Wait, unmute then, yeah.


SPEAKER_01:
um oh gosh i was looking at one of your slides and and uh so i it i i totally lost my train of thought but i will say that i saw a video back on the the swarm and the studies show that a large percentage of the of the swarm is just following you know whoever is closest to them so you can't ever think this thing


SPEAKER_03:
yeah that's that's this is a very funny in the ants like I I don't I think it's a little bit sensationalist to call them lazy but it was it's like most nestmates at most moments are just standing there that's excess capacity or could it you know when if nestmates die or something if there's like an alarm and so it's like


SPEAKER_01:
If flocking is the goal, then... Yeah, well, I think there's a link back to the cognitive capacity and level of extraction.

In other words, if I don't understand more than two dimensions, then I can only reconfigure two dimensions.

Maybe every thought, but anyway, just a thought.


SPEAKER_03:
Yeah, Hector, he's been sharing some resources about the development that happens in a child between like the ages of four and seven as they're separating out like beliefs from knowledge, where they'll do the experiment, like a little character sees somebody put a ball into a box, then the character leaves the room and then the ball is moved and the character comes back and then you ask like, where does the character think the ball is?

And then it's kind of like theory of mind.

Like you have to understand that, well, the ball is really in a different place, but they think it's here.

So then that, um, that it's the holding up of multiple possible beliefs about where that ball is.

But if you didn't have the capacity to entertain and like to juxtapose the memory of the ball moving with when they left the room and all of that.

then you would collapse down to either the ball being you know to to incorrect answer to that question but then what's interesting about it is like one day the child will not be able to do it and then like two weeks later the child will be able to do that and then that capacity will never leave them for their life just like on a random day when they're five years old yeah okay cool well fun

chapter three interesting high road um hope we got to some good topics uh chapter four is is kind of like where the high and low road are gonna they're gonna merge back together and chapter four is gonna be the um the formal treatment as they say of degenerative models

that's going to include some kind of like background material on the causal graphs and then come to like figure 4.3 which is like the rosetta stone image with a discrete time and the continuous time generative models so these are two possible architectures that treat time differently that have the kind of top-down characteristics that were discussed on the high road

and have that low road construction.

So, thank you all.

See you next time.


SPEAKER_00:
Thanks, Daniel.


SPEAKER_03:
Bye.


SPEAKER_00:
Bye-bye.
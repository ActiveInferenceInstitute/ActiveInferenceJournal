SPEAKER_02:
All right greetings thank you everyone for joining we're in our second discussion on chapter four let's just jump to it um anyone can write a question in the chat or raise their hand or just like unmute and go for it and it could be like anything you remember from the chapter or like a part you liked or were curious about or just any any question that you want to jump in with


SPEAKER_01:
like any any section that people prefer just on immune just any page any section oh Aaron go for it so all um I I was really scratching my head trying to compare figure four four with figure four six um there's like

a very large amount of overlap between the two sets of diagrams.

But there's like some very so I feel like there was like a clear parallel set up.

There are some edges that are different between the two.

And I was like trying to understand, like, what are those changes mean?

And in both cases, I think especially the left-hand side diagram is virtually identical between 4.4 and 4.6.

I think I have the numbering right.

But yeah, I don't know if that's too much to fit into a question, but I thought the difference there must be meaningful, but I couldn't make sense of it.


SPEAKER_02:
it's a great question like for sure on first pass it's like two cities like maybe there's some differences in which roads and what's connected to what but at a first pass like they are both very very similar um so

A few things are being shown here.

To kind of give context on this question, let's pull back to figure 4-3.

So here we see this kind of like Rosetta Stone figure with a Bayesian graph.

for the discrete time model and a bayesian graph for the continuous time model on the bottom they're kind of being like laid out like this to emphasize their structural similarity even though they're treating time differently which we can return to but the top is treating variables in terms of discrete sequences of events whereas the continuous time is treating it in terms of higher and higher derivatives of a given um variable x

The important thing, though, that makes these similar, other than their structure, is that in a Bayes graph, you have edges are variables, and then the nodes are variables, and the edges are causal connections.

Okay, so now we get to a Bayesian message passing scheme.

So one of the really cool things about a Bayes graph is because the edges represent the sparsity of causal connections in the model, you only need to consider edges that are local to a node.

So it's like the traffic at a node is strictly and entirely defined by the edges leading in.

So when constructing a given Bayesian model,

you specify the kind of causal architecture of the world, and then messages are passed between the variables.

So this is going to be a slightly different base graph than the figure 4.3, but it's going to emphasize how certain kinds of architectures pass certain messages to each other, and then how that enables different kinds of computations to happen.

Like there's a difference variable here.

So like right off the bat, even though we see some variables that were...

in figure 4.3 like pi and s there's also different variables like the um epsilon and the the item um okay so two things are being highlighted in this figure 4.4 on the left is going to be about hierarchy and nesting on the right is going to be about unfolding dynamics through time

So the motif on the left that describes the nesting is going to be like each unit of four is kind of like a level in a nested hierarchical model.

And basically what's happening is the E is calculating an error, like epsilon for error.

And then those errors are being passed up to a higher order in the graphs.

so that is like a kind of classical predictive processing architecture we have strictly stacked levels and then what's happening within each level is it's doing a differencing and then passing the errors up that's the hierarchical motif

the dynamical motif is going through time so this one actually has more similarity to the discrete time in 4.3 and here it's kind of like a predictive processing but

emphasizing the dynamics here's the sequence of observations coming in current time step the future time step past time step and then each of those observations are kind of meeting in the middle and calculating an epsilon based upon the collision of the incoming observation and then the hidden state prediction so like the epsilon would be zero if the hidden state prediction was exactly what the observation was and then those are getting passed on up

um okay then 4.6 here the only difference is that the again the very similar shapes with like the motif of four and the passing up but now we're in the continuous time setting so that's why there's um more variables like we see in the continuous time world um

and now rather than representing the past present and future time step just like the top here past present and future time step in 4.6 now they are representing different orders of the generalized coordinates of motion

So the first, the second, and the third derivatives.

So 4.4 and 4.6 are kind of like transposed from figure 4.3, the top and the bottom, but now it's representing each of those halves in terms of the message passing that it reflects computationally.

So what's cool about it is it shows, again, a deeper similarity between these two kinds of models.

What's kind of challenging about it is it's introducing new notation

and it's not exactly the same model as before.

Any thoughts on that or more on that question?

Or it could be a different question that anyone has?

We can go to any any part of four.

Or like any any can just anyone can just unmute and go for it or write in the chat.

Yes, TCC.

it's kind of like parallax in vision like chapter four is presenting the generative models of active influence so in terms of where we are in the textbook like chapter one introduced it chapter one two and three approached active inference from the low road and the high road and then chapter four is like actually the kernel of active inference and then the discrete and the continuous time are going to be unpacked more in chapter seven

the discrete and chapter eight for the continuous time however here they're being really like tightly juxtaposed and interwoven again that speaks to a really deep point which is that time can be handled either in discrete or continuous fashion in active inference that's cool deep point what's challenging is that even when they're discussing similar

kind of cognitive functionalities the notation is very different between the two forms it would be confusing in a different way if the exact same notation was used because it wouldn't be clear whether we were talking about the discrete or the continuous time however this is kind of like a second set of vocabulary

we couldn't use exactly the same because some of the variables are slightly different but like using y for observations in the continuous time and then using o for observations in discrete time that's just how they do it in this chapter so it's kind of like they're introducing both time treatments

and emphasizing again and again and again that both of these are like two flavors and then they are going to revisit it in the dedicated chapter seven and chapter eight but it's kind of like a double-barreled chapter because they're introducing generative model not in terms of like well here's a simpler generative model now we're going to go to more complex they're introducing like a dual stream like the two simplest forms

But anyone can just unmute and go for it, or we can look at the chapter four questions or whatever people prefer.


SPEAKER_01:
Okay, if no one else has, I'll go again.

I'm confused also about the structure of policy variables.

Like, okay, so like a hidden state, an observed state, whatever, like I can like sort of acknowledge those as sort of like things that exist in like some not fully specified space.

Policy, especially because I think it's like trying to call back to reinforcement learning literature.

Like, I feel like in some...

What's the type of that?

If you were to describe the space in which a policy, either the discrete version, the pi, or the continuous version, the v, what that lives in, or if you were to implement it in a program, what the type associated with that is?

What's its shape?


SPEAKER_02:
Great question.

Pi is a list of all policies.

So it's always a vector and it always sums up to one.

And that's really the critical move that allows us to talk about the likelihood of a policy.

That's why habit is the prior on policy and then expected free energy updates are prior on policy into a posterior because the shape of that variable is a list that sums to one.

So it's a probability distribution.

So affordances are the actions that can be taken in a moment.

So if we're like in a grid world, the affordances will be like up, down, left, right, stay.

So it'll be like five.

Then if we're only a single time step agents, affordances are the same as policy space because we're only selecting the next action.

So let's say we have five options.

If we had an even prior across those five, it'd be like 0.2, 0.2, 0.2, 0.2.

And then that could be sampled from or updated into a policy posterior action.

If we're talking about an agent that does planning, then the shape of pie is affordances raised to the power of the time steps.

So if we have five affordances and then we're doing two time steps, then pie is 25 long.

It's up, up, up, down, upright.

And so it's all of the two way combinations.

So that's the kind of exponential explosion with policy planning, which we can come back to.

But at a first pass, the important things about PI are that it's a list.

It sums to one, which is what makes it a probability distribution.

And it's a distribution over policies that the agent is selecting from.


SPEAKER_01:
just just to clarify or check my understanding so um then whenever we see like uh v with the tilde representing a trajectory over policies like that is maybe should be regarded as something that's like because it's exponential and like uh unbounded number of steps is like not something that we should consider like concretely representable in a program


SPEAKER_02:
interesting so the the continuous time setting doesn't do explicit planning it does so the the pew mdp at the top here this is where we have the kind of classical computer science exponential tree search like more and more discrete combinations you know if you had five time steps deep you'd have five to the fifth discrete options it's kind of like a chess algorithm

um in the continuous time setting planning is not explicit like let's just say that we had some wild function that we were trying to approximate when doing this kind of a taylor series approximation like evaluate the function at zero and then take the first derivative so let's just say we stopped there we're just going to evaluate the function and then look at the local slope that already expands to everywhere

That doesn't mean it's going to be accurate everywhere, but basically the Taylor series does expand no matter how many levels deep you go.

It already implicitly includes basically infinite future planning.

But for explicit planning in a tree like setting, that's where the discrete time comes into play.

In, um,

in this page um thomas parr's discussion so early implementations were in the continuous state space because this has the most similarity with like bayesian filtering approaches then people started wondering well how do we get sequential dynamics then they moved into this kind of coupled dynamical systems space

but then that was only getting so far because there wasn't an explicit representation of planning.

Then they moved into the discrete planning environments.

Those branches kind of came back together by nesting the continuous dynamics at the lower level with the discrete dynamics at a higher level.

So you could do a lot of work in the continuous space,

And then also have that like wrapped by a discrete planning step.

That's like a little bit about how the timeline worked.

But just to summarize, in the discrete setting, this is the most like classical planning.

In the continuous setting, you don't have planning as such.

It's kind of like you have a car going around a turn.

In the discrete time world, it's trying to estimate all the branching paths it could take.

In the continuous time world, the driver's like, I'm turning 20 degrees to the left.

That's like the first derivative of my turning angle.

And then I'm just, I'm holding to that.

And then that curve continues.

So it's not explicitly saying at seven time steps from now, here's where I expect to be.

So it's the strength and the weakness of the discrete time.

The weakness is that there's an exponential explosion of the tree, just like a chess algorithm.

But the strength is that you can actually talk about the difference between up, up, up, up, up, down, and up, up, up, up, up, right, which you couldn't really get except by going super deep in the continuous space.

So they both have these kind of like strengths and weaknesses, and then that is resolved or addressed by building hierarchical nested models that have components that have continuous treatment of time and components with discrete times.

Good call.

Let's see.

In page 73.

Yes, someone else has already added this, but yes, good call.

There's several typos in the book.


SPEAKER_04:
Thanks.


SPEAKER_02:
we can go to any of the written questions or like just any any remark about anything that we've gotten up to or just like anything anything active related we could totally go for it because chapter four is just it's kind of like the heart of active inference um albeit described in a very um notational way

David or anyone go for it.


SPEAKER_03:
Yeah, I was just wondering if there's any relationship to the partial gamma function in regards to how the branching seems to work and fit backwards.

Like if you're mapping back up through the stack of the decision tree that maps back to the initial.

I just didn't see any of the math there in the chapter, but I've seen some correlation in some of the math that I've been working on recently.

So if you go further down, there's a section on branching.


SPEAKER_02:
and the relation yeah so behavior near branch point and the relation between the branches so i was just wondering if um if that fits in or if my intuition on the the correlation is incorrect there it's an interesting connection i don't know about like this analytic family but a few thoughts are inactive inference we're treating every moment as a branch point so that is the control

decision slash question is the branch point is defined by the affordances that can be taken at that point and so there's some um you know there's some kayaking down the river where no matter which way you branch it's like you're going to get kind of brought back in so like the long-term consequences don't matter but then there's other moments where to to go a little bit one way or the other is going to diverge

So that is kind of like you have the agents on the kayak and they have the behavioral branch point.

But of course, just knowing that they can tilt left or right doesn't tell you about like the river, which is kind of like the actual process unfolding.

So I'm not sure exactly how that would connect, but the function is a branch point.

another another possible connection there is like in this chapter we're seeing like the basic kernel like

the minimal essential model that describes um either you know in either these time treatments but then there's all kinds of other cognitive functionalities like what's known as sophisticated active inference is where in the moment you're not just planning um or trying to estimate like what will i be doing and what will i see but you can also

project out what will I be thinking then about the past so again that explodes the state space even further but it allows for like sort of prospective retrospective but that that kind of phenomena is not here but it's like a modification on this base model


SPEAKER_03:
Yeah, I guess I was seeing it more in terms of the primes and the double primes on the bottom set.

And maybe that was just my miscorrelation of just misreading the terminology as it's represented for the lower states.

So you see Y prime and then Y double prime there.


SPEAKER_02:
That's the first derivative and the second derivative.


SPEAKER_03:
Right.

But in order to move back, like if you were, if you were collapsing back into the initial kernel stage, wouldn't it have to still step through those premises in that order?

Or would it all collapse at the same exact time?


SPEAKER_02:
Matt Bolian- Again there's a lot to this question, I mean what i'll say is the role that three plays here the B matrix kind of conventionally.

Matt Bolian- This is a transition operator that's describing how things change from time step to time step, whereas the three operator in the continuous setting is describing as we go to the right.

Matt Bolian- The derivative and as we go to the left the integral.

So like you said, you have to walk back through the premises step by step.

From 10 time steps forward, you can go 10, 9, 8, 7, let's just say.

But you can't skip from 10 to 8.

At the very least, you would need a different transition operator to apply backwards.

Similarly, you might be able to go from x double prime

take the integral and then take the integral again but you couldn't take a double integral in one jump that wouldn't be the same as these integrals okay but it's kind of like yeah there's other ways to take it like the computational irreducibility angle which is like there might be a shortcut to just apply the double integral in one step

But then there's also probably equations that you can't apply a double integral.

And in a way, by doing a double integral in one step, you've just made a single, or by going two time steps backwards here, you've just taken one of a different kind of step.


SPEAKER_03:
Okay yeah cuz I ran into something recently that resolved into a five integral and that's where I got confused as to how that was even possible and i'm still trying to to work through the math a little bit to understand.

But that's kind of out of the scope, so I don't want to take too much time out of that.


SPEAKER_02:
if you're looking at something that's that's a really long sequence you might be able to have statistical power to talk about like the fifth derivative but you could simulate trajectories and then just plot the first derivative plot the second one plot and so on and then you'll just find that maybe there's interesting Dynamics of higher orders or maybe not

And then the empirical way that you could ask, like, how important are they?

Okay, so we make a real simulation that has five actual derivatives of influence.

And then we make a subset models that have all five that should be able to recover 100% of the variance.

And then you test the one with four, three, two, one.

And there's situations where even if there's really five, the first derivative covers 99% of it.

Or there might be ones where

The higher derivatives do matter.


SPEAKER_03:
Okay, thanks.


SPEAKER_02:
here just just again just to connect it here's just evaluating the point so to say that you know this is the this is like the true sea levels for the next 24 hours or this is the real temperature in the room so it's like if you can only have one number the best thing you can do is just say what is the temperature right now and that's this kind of first approximation and then if you were to have a second number you could say okay what's the temperature in the room and then what's the slope

And then if you had another number, you could say, okay, what's the temperature right now and the slope and how fast it's changing.

And so that process is the Taylor series expansion.

So that's the kind of way that they talk about it in approximating functions is you can keep on adding more and more approximating terms, but it can get unwieldy.

and you still get and yeah there's other things to say but this but I guess I'll just say this dash line does go off to Infinity like you could evaluate that dash line at this point but it'd be like way way way down and then like one of the issues with using the um Taylor series is you don't know how good or how bad the dash line is

Rupert Clayton, Except by then doing the laborious work of checking against the actual function.

Rupert Clayton, So, like here it's like it actually is doing really well but it's not telling you it's doing well, you would just find out by checking but then over here it's going to be doing certainly bad but it's not like it's going to throw up a yellow flag and tell you.


SPEAKER_03:
Rupert Clayton, um.

I guess my question in terms of that dotted line trending to infinity is, is that assumptive or is that, you know, provable in the sense that we just can't see past a certain state because it's, you know, a hidden state or is it calculable past that hidden state that it is going to be infinite?


SPEAKER_02:
The dashed line is very well behaved because it's a polynomial.

It is this polynomial.

Here's the first order polynomial, just a constant.

Here's the second order.

It's a linear y equals mx plus b, basically.

mx plus b intercept.

Now we're in the quadratic polynomial setting.

So it has all the kind of simple behavior of polynomials.

which is what makes it a well-behaved approximator for arbitrarily unruly functions.

But it doesn't tell you how good it is.

Okay.

Also just note, like, on the pragmatic side, PyMDP, the Python Active Inference Toolkit, is only in discrete time at this point.


SPEAKER_03:
Brett KenCairn, So a lot of to translate some of it over to because i'm starting to work a little bit with kiss kit or i'm trying to.

Brett KenCairn, To understand to try and you know use some of the functions there with quantum gates but that's a little bit beyond.

Brett KenCairn, To try and utilize that as a tool so i'm trying to put it together a little bit and then I just got a little bit confused over the last few days, putting some things together so.

Brett KenCairn, i'll keep looking at it appreciate it.


SPEAKER_02:
Brett KenCairn, Oh all good.

Okay, anyone else just unmute and go for it or feel free to type of question, or we can look at the upvoted questions here or any question that was written in the chapter six or cohort six questions.

Okay, not specific.

I think it's worth to kind of pull back a step and look at figure 4.2.

Figure 4.2 is really helpful because it kind of builds intuition for what these Bayes graphs are in terms of like, here's a situation on top left with one hidden state and one observable.

Like there's a real temperature in the room and then there's a thermometer.

Here's the situation with one hidden state and two observables.

here's a situation with two contributing factors and one observable like two things that contribute to a disease prevalence and here's a chain of causation so these are like these are the kinds of things people say like there was traffic because it was a sports game and a Friday night it's like okay that's this situation

or it was a Friday night so there was a sports game so there was traffic but those are the kinds of things that people talk about when they use language to describe causal sequences in the world and then these variables can be like anything like the observable could be any modality of observation

then the hidden states can be any latent causal factor including things that don't like actually exist in the world like without arguing too much about whether temperature actually exists in the world you can have latent causes that are just totally proposed for better and for worse but this maps onto like a huge fraction

of the kind of statistical inferences that are done.

this also is kind of like if if we had like a mind map and we were just like drawing out causal factors for what we cared about like this is kind of intuitively how you would do it like if one thing was causing multiple outcomes you'd draw it like this if there was multiple things contributing to cause an observation you could draw it like that

so this is kind of like where we touch with more intuitive notions of cause and then that is going to get deployed in this setting where we're talking about the causal relationships between observations hidden states and actions those are the big players here and that's being unfolded through time

But those are the things that we want to connect in our unified model of perception, cognition, and action is to a first approximation, perception, cognition about hidden states and action.

That's why those are like the major nodes in the white circles.


SPEAKER_00:
Has someone tried to implement this for like motor control?


SPEAKER_02:
yes um motor control has been approached from the human motor control perspective like handwriting and movement of the arm and things like that and also the robotic motor control setting at the kind of gross level like where to move within the warehouse and at the finer scale like robotic arms and things like that


SPEAKER_03:
Dan, wasn't that related to what the Hitachi group covered?

I just thought that might be a good video to point to.


SPEAKER_02:
Yeah, good call.

Thanks.

That was guest stream 75.

So here was a robotics video.

in silico so it was not physical robots it was a simulation of robots but but this is um a group that's working with physical robots and then they um the visualizations that they show in the in the stream were pretty interesting and provocative I think in the sense like it it um

it shows how multi-agents and also how social norms come into play.

So here we have basically the red agent is the active inference agent.

And then it's gonna be like coming on the opposite direction of the sidewalk.

Here, it's an altruistic other.

So the green one has the goal of making the red one go as little out of the way as possible.

Ricks, Whereas the selfish green one is trying to minimize its path.

Ricks, So those are kind of ways in which people implement like even these kind of social heuristics into the preference distributions and the action tendencies of different agents.

And that's kind of the journey that chapter six takes us on.

So that's kind of why the book can be read in many orders, because as it's written linearly, the first five chapters are like a ton of information.

That's the epistemic component.

And then the second half of the book, starting with chapter six, is like, well, here's the recipe for building the model.

Like, what are we modeling?

Why are we doing it?

What form...

should we use figure 4.3 so just the way the textbook is written it kind of shows us a lot of the tools in the workshop and then we get the recipe in chapter six but another way to do it and of course if you go through many times then it doesn't really matter as much where you began but another way to to go about it is start with the system of interest like a system that you're familiar with or curious about

and then start to reverse engineer like what are the observations here like who's observing what i mean i could explore even in his starbucks like who's observing what what are the actions that different agents are taking what preference distributions do they have you know some people want this level of sweetness or this um kind of coffee

So that's like the kind of low road approach is starting more with an actual system and then building up to what formalisms come into play rather than like learning all the math equations and then just hoping that something like crystallizes out of nothing.

Thank you, Cheryl.

I'm going to bring this in and let's look at it.

awesome yeah these are great points um like well you know the the brain is a system of systems so like if we here this could be like this is like our let's just say this is our portfolio of models i mean

the brain entertains multiple causal hypotheses about events.

So it is a system of systems.

And these are, yeah.

Go ahead.


SPEAKER_04:
I guess I meant it's not a system of just internal systems.

And that's where I get mixed up.

I mean, it's the whole of it, the whole experience in the world.

We are not just brains.

We are in ecosystems.

We're in social systems.

We're in, you know what I'm saying?


SPEAKER_02:
Yeah.

It's like, it's such a great and essential point.

That's why we focus so much on the action perception loop, which initially seems kind of esoteric, but then we start to see that like all agents around us are involved in it.

then the only way to account for that kind of like dynamic co-creation with the environment and the agent is to think about the interface every time step like if you're on a run or if you're driving you can't just focus on well just ignore the perception for the next time steps or just ignore the actions for the next time steps like action and perception are always happening the agent can't opt out of those happening

And so it is a joint entanglement with the environment.

And that's like, that's exactly what has to be kind of modeled to have a holistic model of even just a rat in a team.


SPEAKER_04:
Correct.

So you're right.

I like it.

I like the systems in systems within systems model, but we can only work with what we can work with at a time.

That does infer a certain amount of reductionism, though, as if I understand it at this level, I will understand it at the more, at the larger level.

So that's an assumption that we bring to this.


SPEAKER_02:
Yeah, again, you know, there's no single order for going through active inference, but figure 9.1 kind of captures that.

Like here in the center is the top of figure 4.3.

So this is the rat in the teammates.

That's the subject of our study.

here's the scientists in the lab so what we choose to present as stimuli are observations for the rat but they were choices by us and then the choices of the rat become observations for us and so you could close your analysis here and say well it's just a rat in a laboratory or you could model the building around the laboratory and you could ask like how does that context matter

And then you could ask, well, how confident are we that the rat behavior in the lab is like it is in the fields?

There's laboratory setups that it probably transfers and there's ones where it doesn't.

But just making the model of the rat behavior in the lab does not guarantee transfer.

And that's why the system of systems is so important.


SPEAKER_04:
Right.

Okay.

I love that.

But I'm not in chapter nine.

Metabazine inference.

Oh, my gosh.

I don't even know I'm going to get out of chapter four.

Okay.

So I'll wait.

I'll wait there.

But I do.

I do have a question.

Have you ever heard of Marilyn Monk, who's also at UCL?

She's a molecular biologist.

She has a really interesting theory on

uh, cause consciousness.

Yeah.

And, and, um, she takes the systems all the way from Adam to cosmos.

That's what, that's what got me thinking about what we're doing here in this, in this book, but a very interesting model.


SPEAKER_02:
that's cool like definitely some people are going that kind of like multi-scale like down to as small and fast as you can imagine like quantum and then up on through the cosmological and then this seeing this part here that reminded me of a really great book on evolution in four dimensions which is like

there are genetic epigenetic and metabolic and cultural and symbolic inheritances and there's situations where one explains all of the variation but there's situations where it explains not so it's not like you can just have this in principle battle and say oh well you know epigenetics wins or symbolic variation wins it's like the the field has been sketched

and then the empirical question is what matters where and how and then what but those are questions to address for specific systems in practice they're not questions that can be addressed you know dusted off once and for all in principle yeah yeah yeah yeah yeah yeah I mean all right yeah there's hope for me there's hope for me then there always is

It's, I mean, it's, it's fun and it's a learning.

There's so many things that, that connects because like we're talking about the systems that we talk about elsewhere.

So when we talk about organisms, we're, we're, we're talking about the same, we're not talking about some other system.

And this is more like a lens that we bring to systems.


SPEAKER_04:
Right.

A lens we bring to systems.

Mm-hmm.


SPEAKER_02:
another way to think about that is like active inference says less so we can say more but it's not like active inference is gonna you know it's not a slot machine it's not gonna deliver us like an account of a psychiatric or a social situation i mean the textbook is what it is

Like, numbers are just there for people to count things, let's just say.

So these models exist for us to analyze cognitive systems.

But if active inference already had an answer about how cognitive systems are, that would curtail it from being used to explore diverse intelligences and open-endedness of creating new things.


SPEAKER_04:
Yeah.

Got it.


SPEAKER_02:
that's why like yeah thank you that's why chapter four gives us the like degenerative model it's like this is like the linux kernel or like the software system but it's like but what do i do with a computer it's like well chapter five here's people using it to study mammals but in chapter four we see it in kind of its pure form in the general or in the abstract form

which is like both sometimes very frustrating or it feels like it's like a mega infinite learning journey but all of the connection from active to any system we make that connection that connection is not for us to find hmm


SPEAKER_03:
Dan, could you go back really quick for me to the chapter was scroll down.

I think it was five where you had gone down to consciousness related to that were introduced.

The differential between animal and trees.

This quote here or plants, yes, sorry.

so in terms of they have no brain so they're called plants but in the in the context of the tree itself isn't that wouldn't that be considered um

In the same way that show was just talking about different layers of the brain, where the tree structure itself right has its own branches and nodes that are still connected in the same field.

That may grow around but again you're also dealing with different dynamics, where those systems will still interact given how close they are for fitness of reaching the sun, you know variably if another leaf is over another day, for example.


SPEAKER_02:
I totally agree.

It's the kind of quote that a mammal said.

I think about this paper.

This is my PhD advisor and a colleague, a framework for plant behavior.

And I mean, now this is more of a common perspective.

but um there is some question whether the words to describe animal behavior such as the word behavior or foraging can be applied to the activities of plant i mean is a root tip not foraging for nutrients in a niche oh it's slow okay but not to itself it's slow to a person running around but um some people say oh plants don't have behavior they're just there but they do have behavior

and then active influence helps us say yeah here's the affordances they have and the time scales over which they have them but you couldn't say like something you know it doesn't have behavior just because it doesn't have a kind of behavior right well that's it go ahead go ahead sorry go ahead no go ahead sure I'm sorry


SPEAKER_04:
I was just saying that behavior assumes consciousness and Oxford Dictionary defines it as the state of being aware and responsive to one's surroundings.

So if you, like Marilyn Monk, she talks about awareness is sensing.

So if we say sensing and responsive to one's surrounding, then we have to say plants have a consciousness according to Oxford Dictionary.

so what you're what you're saying about the leaf and the tree and the root tip um brings the word consciousness here which is problematic for me because it has no end i mean what doesn't sense and respond so again they even rocks do that


SPEAKER_02:
that's that's these are the mega philosophical questions like does pancognitivism the idea like we approach all things as cognitive systems or that they truly are cognitive systems does pancognitivism imply panpsychism is there something it's like to be anything because all things are engaged across the interface in sensing and adapting

even something that's inert it still is bounded by an interface and there's still something on the other side of the interface even if it's just a cup of water but then is there something it's like to be a cup of water because it'd be different if the cup of water weren't there so it's doing something in relationship to its niche

And so that's people looking for continuums where it looks like they're sharp distinctions and looking for sharp distinctions amidst the continuum.

Like it has to have a higher order awareness, like it has to be aware that it's aware.

Like our attention is not only about target of our attention.

We're also aware that we're paying attention to something.

And this is where so many of the people bring in like meditation

the mindfulness connections and then some people use that to differentiate or distinguish human cognition from other types other people deploy like literally the same formalisms to argue for the continuity of human cognition and then this has been oh yeah go ahead


SPEAKER_00:
Has there been discussion around how much our senses contribute to our cognitive function?

I mean, just as a mind experiment, if you took a human brain and you put it into a dog's body, would we start thinking like a dog more?


SPEAKER_02:
Like, people have thought wild stuff.


SPEAKER_03:
well I mean just to come at that from a different perspective I was thinking about this the other day in terms of cybernetics right like if we get to a point where we're able to do a full transplant of you know a mind for example into a cybernetic system wouldn't that be the same argument you know would would you have the same kind of organ rejection for example um but it would would it be

And I guess that would be very assumptive at this point, but like, would the, the false body reject the mind or would the mind reject the false body in that sense?


SPEAKER_00:
I was speaking out more from this perspective of, you know, how necessary is it for intelligence or like general intelligence?

Like, you know, do we want, if we want to get to higher levels of intelligence, do we need a, an embodiment system that, you know, senses the world

um in a complex way that we have yeah i think we have like a hundred thousand like stencils receptors or something for our body and it's continuously feeding into our body through many different layers and you know maybe that's something that's crucial to you know the feelings and emotions that we have um yeah because there's theories where you know like our our stomach itself contributes to our you know cognitive function so

You know I wonder if there's been discussion there from your conference the norm in active influence Community, because the you know.

This Community seems to think more about you, I think the holistic.

yeah just the bigger picture in terms of intelligence.


SPEAKER_04:
So I did my PhD at the University of Virginia in the United States.

And there was a little known secret there of a guy who studied near-death experiences, reincarnation from all over the world.

Ian Stevenson was his name.

They didn't even really talk about him until years and years, decades later.

He was a well-known secret.

But his point was that consciousness can exist without a body and even without a brain.

And at that time, it really got me.

Yeah, there it is.

It really got me to thinking about this whole idea of truth.

There is a scientific model of truth in which, of course, this is done by humans.

But before there were scientists, there were still people seeking truth.

And even in active inference, they're called the mystics.

And then I think in our day, you know, there are poets who kind of intuitively or think deeply about something, but it's still a search for truth.

So I think for me, it has been useful to acknowledge these different ways and perspectives and modalities and not think one is greater than the other.


SPEAKER_02:
That's a great point.

There's so much to say.

It reminds me of like, there's the parable of the blind people and the elephants, and they're like touching different parts of the elephants.

Okay, now we could kind of modify that into one person can see, one can hear, one can smell, one can talk.

So there's different modalities that have not just touching a different part of the body, but they're actually getting like a different wavelength.

They're getting totally different modalities of the elephants.

That's one interesting component.

And then one other point is like, so earlier you asked like, what is the role of the sensation of perception in intelligence?

There's a ton of answers, but one answer that like Adam Saffron and others have pursued is the kinds of relationships that our sensory apparatus require to solve help us build architectures that reflect the structure of our reality, like grid cells in the brain.

those evolved because we were moving in spaces with certain geometries but then those became later utilized for other kinds of arbitrary cognitive navigation or like having hierarchically nested perceptual apparatus it arose to do

something that it was repurposed to do later because that motif is so effective at the kind of statistical challenge that it evolved to solve or address.

And there's probably no end to those motifs.


UNKNOWN:
Wow.


SPEAKER_01:
yeah it's super it's like chapter four so hard this is where it has to take us can i i know over time can i ask like a very fine grained question to zoom all the way back in yeah yeah um so i i kept getting confused in the text by some of the conventions in the math um

are bold faces and our serifs versus sans serifs meant to mean anything?

So if you look at equation 4.11, for example, we have caps serif F equals bold pi times bold sans serif F. And I looked around, I couldn't figure out like what, if anything, are those like changes in the specifics of how things are rendered meant to indicate?


SPEAKER_02:
I'm totally with you.

That's one reason why having the written equation form, I feel like is really vital.

Because even experts will not be able to converge upon this kind of a description.

without looking at the same page.

There's just too many ways to read even the equations.

And that's even before you get into like bold and italics and like font and stuff like that.

It looks like bold is only mentioned one time in the supplement or Appendix B referring to it as being like a sufficient statistic, but I don't think that's how it's being used above.

i feel like bold is being used to describe whether it's a vector or not but i i i don't know and like faced with that dauntingness of trying to sift through where they or if they define it for like typographical phenomena that i don't even necessarily know the name of that's why

I'm very excited for ontology-driven natural language descriptions of the equations.

And then we can render them to look however we want.

So we could render them with a full English word, or we could render it with a full non-English word, or we could use a Python variable.

Because I don't... Even if you could dig to the bottom and find out if they defined how they use the notation...

other papers are going to do different notation.

So it'd be kind of like a local victory.


SPEAKER_01:
Yeah, at this point, I would be satisfied, or I would be helped by local victory.

There's like a couple of pages where they like seem to use this distinction a lot.

And I was like, not following their choices at all.

But But yeah, I appreciate that, like a shared resource, like helps create some clarity.

But yeah.

Okay, equation for 11.

uh yeah as an example and then like above and below this there's a few there's like a number of others that are uh i think intending to use the same distinction and i i have to assume that the authors had some purpose in mind so i don't think it's just like matrix versus vector versus scalar


SPEAKER_02:
And the question is, what does the bold typing versus the italics mean?


SPEAKER_01:
What does the bold mean, and what does the serif versus sans serif mean?


SPEAKER_02:
Okay.


SPEAKER_01:
And if no one knows, I guess no one knows, and that's the state of the world.

What do you mean by sans serif?


SPEAKER_02:
You mean this F versus this F?


SPEAKER_01:
Right, so they are distinguished by the bold feature, but other things are also distinguished by the bold feature, but not by the serif versus anserif change.


SPEAKER_02:
Okay, it seems like we always have a co-occurrence of bold and straight, and italics is never found as bold.

So I'm tempted to say that bold and straight means sufficient statistics of,

whereas the italics is referring to the underlying variable as it is.

But we can literally just write the question down in one of the rows, and then whatever we can't resolve, we can email Fristen at all.


SPEAKER_01:
Okay, fair enough.


SPEAKER_02:
Thanks.

Yeah.

Yeah.

Yeah.

I mean, I dream of the day when we can mouse over

and have multiple modalities of what these are and and like have someone says i want to mouse over and see examples someone else says i want to mouse over and see like the programming variable i don't know if there's another way to make like a fixed artifact that that could speak to so many people in so many different situations

Okay.

All right.

Well, interesting times.


SPEAKER_00:
I'd end coding exercises that I can go through to understand these equations better.


SPEAKER_02:
Yes, I'd go over the code page.

Here's several dozen code examples.

Like when you see, and then also I would highly recommend the PyMDP documentation because like when it's shown as the Bayes graph and it's like, it seems like, again, it seems like it's gonna be like this math and analytical thing.

But when you go to active inference from scratch, you define A, B, C, D,

And then you have all of the steps, all of the utility functions are defined, and then you have the active inference loop.

And then once you've gotten to here, here's active inference loop A, B, C, D, and the environment and the time horizon.

So if you've run up at the equations and or you want to try a little bit more of like a bottom-up computer science or computer engineering approach, this can be really clarifying.

But those are very useful with PyMDP.

But any of the code examples will be fun or useful.

However, PyMDP and RxInfer

have the best documentation.

So, and we're here to make the material more pedagogical.

So if there's like a example that you're excited about, like literally let's have the authors do a live stream or let's make the code better annotated or documented or whatever we need to do.


SPEAKER_00:
Um, did they have any examples where they're doing, um,

active influence on just some basic um basic predictions like i don't know maybe temperature or yes like um the uh rx inferred documentation


SPEAKER_02:
has has like a range of examples um ranging from like like don't worry about something being too simple truly like even the examples um of coin flipping are a great place to start but this is a ton of information about all kinds of examples and then

Usually the people who are making the packages are focused on making the package, if not like working three jobs.

So if something's missing, I mean, change the world or change your mind.

So if you think there could be a cool example, that's what we're doing here.

So I hope that we can make it if it's not already there.

Yeah.

Wow.

That was chapter four.

The next two weeks, we'll come back for chapter five and look at more examples in the mammalian nervous system.

And then we'll have just a final meeting, kind of catch up, hopefully get people's feedback and connect on projects and just see where things are at in the Institute and the ecosystem.

So thank you all.

See you next time.


SPEAKER_00:
Thank you.

Bye-bye.

Thanks, everyone.


SPEAKER_02:
Thank you.
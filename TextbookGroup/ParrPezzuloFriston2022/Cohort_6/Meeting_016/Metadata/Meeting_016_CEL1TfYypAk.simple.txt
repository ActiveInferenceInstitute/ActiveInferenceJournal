SPEAKER_02:
when um man that's distracting okay i just watched your screen it's what's causing it where i was getting at was okay so on the one hand we we have this discretization and this continuity and on the other hand uh we can we have this embedding of the discreteness within the continuity or the continuity uh within the discreteness

um or there's like nestedness right where you like you have like discreteness continuity continuity within the continuity and so on and so forth but also uh while we have the prioritization of what's happening there's also kind of like a tuning aspect right like the tune like there's tuning mechanics happening right like when you think of stuff like hyper planes or hyper parameterization um i think a beautiful paper to look at

um is biological cognition by uh i think it's hubner and schulkin and um they bring up this term called heterarchical processing right and heterarchical has to do with like selective tuning or you think of like a like you're an audio engineer and you're mixing you have a you have a mixing board and you're trying to get the right trying to mix the channels and all that together the right way like this tuning or this adaptive tuning

rather but it's like there's like a mix of there's layers of the hierarchical and then the tuning happening right so it's not even though i would argue it's mostly hierarchical you still have this tuning happening right you still have this sort of like this degrees of tuning happening but i'm not sure if that i like i'm i already am like about i'm on chapter five right now just chapter four

but I'm not sure if the book really addresses like this hyper parameterization issue, but I guess you can kind of go over that.

Yeah.


SPEAKER_03:
Yeah.

That's I think that's a really interesting point too.

So whenever,

I don't want to find the figure.

I don't have it top of mind, but it's in the second half of the book, and it might be in the section on learning.

But everything you're saying about tuning and hyperparameters, those are absolutely, those are directly components, not just figuratively in active inference models.

And so what you can have is, like I mentioned precision earlier, that came up at the end of the

the chapter this chapter um but you can actually like precision can be modulated such that it impacts the rest of the model once you have a hierarchical model it's like it's not just you know every once in a while something happens here but most of the time it's over here it's like the whole model is integrated right and so you do have this really strong

constant interplay between the different layers.

So that hopefully at least figuratively gives you some sense of like the way you described like the continuous and the discrete and the discrete and the continuous.

It's like, well, as far as like coding a simulation, you do have to more clearly define one versus the other, at least at each layer.

But then that said, they're so tightly integrated.

It's as if, you know, the discrete is impacting the continuous and then get involved in and vice versa.

And so that's there.

And then somewhere in the textbook,

there's um there's another figure showing the pomdp we were looking at earlier but it also adds on hyper parameters and those very hyper parameters are what can be adjusted or tuned and they relate to precision for example you could have your agent learn its a matrix meaning the the likelihood matrix which figures in this chapter and so they might learn to you know

let's say we have our mouse in the T maze, the agent, right?

Let's say they repeatedly go through the maze over and over and through trial and error end up learning that the stimulus is always on the left.

Let's say we're the experimenters, we could choose that.

Let's say the stimulus is always on the left.

For an agent who undertakes or engages in learning

It's not only going to try and minimize its free energy, its variational free energy, but it'll actually start to modify and change its beliefs within that A matrix to where it'll start to associate the cheese or the attractive stimulus with the location left.

in the maze right it's belief about that um and so that's that's how that plays in the kind of tweaking to the models and so on so we have we have learning that something like hyper parameters here in the case of precisions uh are directly implicated so all that all that is here but that's a really interesting paper that you you brought up um i almost feel like other

folks, other members of the Institute might find some interest in that.

I found something, heterarchical control mechanisms in biology.


SPEAKER_02:
Does that sound right?

If there's any resources you have to suggest for me to look into, I'd love to take a look at it because I haven't really found something other than that one paper I mentioned.

It's Biological Cognition.

I think it's published through Cambridge.

When you look it up, Biological Cognition, it's the Cambridge article.

Um, it's paywalled, but, uh, I was able to get a copy of it.

Nice.

But, uh, yeah.

I hate when really nice papers are paywalled.

It's like, ugh, it's such a crime.

Anyway, yeah, I mean, I'm always looking for more papers that talk about this hyperparameterization stuff, because I feel like there's just not enough out there explaining that.

There's so much stuff talking about hierarchy and embedding things in hierarchies.

And even though I would say the hyperparameterization requires hierarchical stuff in order

to be effective, it's still not hierarchical in its nature.

But yeah, I got, you know, definitely throw it at me.

I'm Katmandu on discord.

So if you want to, if you want to DM me that those resources if you find them or even share it in one of the channels, I'll definitely I'm constantly eyeballing what's shared on server.


SPEAKER_00:
So yeah, in a

for the relationship between discrete and continuous.

In the machine learning paradigm, what they actually use is a mechanism called CLIP, which is simply going to take the energy relationship between this categorical and this discrete thing, and then just throw it in whatever's easier, which happens to be language in our case for large language models.

So what you do is you find the most straightforward abstraction layer.

This is going to be naturally hierarchical if we're talking about deep learning in this specific example.

You create that abstraction layer, and then you just keep it consistent through the entire thing.

Two other things that might be related to what you're talking about is Anthropics' most recent paper, which I've forgotten the name of.

I'll have to find it.

But they essentially talk about not hyperparameters, but

the expectation of the actual state that you're putting into it, which they define as a feature.

A feature is going to be deeply related to what we consider to be a concept, like a dog or a cat or something like that.

And being able to take these concepts, which kind of create these little bubbles that you can create a context around, there is some situation that it's talking about,

which has some set of features which are going to be activated.

How much you activate these certain features are going to be deeper related to what you're saying about these hyperparameters.

So deciding which hyperparameters to use is going to be found right now in machine learning through the creation of, do we make this feature really big?

Do we shut these features down to be really small?

And that's going to be how we control how these hierarchical mechanisms are actually going to

consistently be able to produce the results that we're looking for.

One other slightly related thing is of the application of going through the network and having a kind of a meta network decide what inside of the network to actually create.

This is going to be a very recent paper called, it gets into a bit of the weeds, but it's a mixture of a million experts, which essentially takes the entire hierarchical network that we're going to be dealing with

And all of the variables are going to be encoded into these one layer MLPs, these one layer fully connected networks.

And from there, we're going to find a way to create a meta network, which decides which ones to turn on.

And these ones that I'm talking about are going to be analogous to hyperparameters.

And it's taking a

large network, which again talks about the context that you're speaking of and being able to decide which ones are important, which facts are important inside of what we're talking about and only turning those on, which I think is deeply analogous to the hyperparameter thing.

But that does lead me to a question of whenever we're talking about hierarchies in the active inference domain,

How much does it matter?

You guys were talking about system one earlier.

How much does it matter with respect to continuous or discrete variables?

Once you get past the system one level of the direct input from the external environment that you're trying to understand, do you guys think that once you enter into a hierarchy, it's all just abstracted away?

Or do you guys think that the discreteness or the continuousness of the actual input is going to have really deep ramifications as you go down the line?


SPEAKER_01:
sorry that was a lot great great comment we'll each just give a short closing thoughts in these last minutes i mean shortest answer it's simply a hundred percent dictates it it's like asking about a transistor how would it matter what the composition of the metal is or how would it matter what the size of the plate or how closely these elements are it's like

It simply determines it.

So then there might be some situations where you could discretize a continuous and get this or that performance retention or the other way around.

But that's kind of the fun is the textbook is just showing us those fundamental motifs.

And Toby Smyth in the Structured Active Inference livestream earlier today.

showed that in a in a far more comprehensive way than the Markov blanket formalism with the broader applied category theory for interfaces.

So it's like all about how it's specified like a physical transistor or Linux kernel deployment.


SPEAKER_03:
I don't know if this is a little too simple relative to what we were all just discussing, but I did drop a link to one paper into the chat there, and that is deep active inference and scene construction.

And just given we have such little time left, I just wanted to drop something there as a kind of combination of things we're talking about, putting hierarchical modeling

And here, the idea of looking at particular features that, when combined, maybe allows for, let's say, classification or categorization of, in this case, the paper I shared, a visual stimuli.

It's rather interesting.

They put together a two-layer model where it's like a saccade experiment.

So it's like someone is viewing a two-by-two grid

and then they make inferences based on particular things that they see in each of the coordinates of that two by two grid and then they from there have to infer certain aspects of what's going on in each of those two grids and then

that gets pushed up to a higher discrete level that allows them to uh to make an inference as to how they should categorize the entire uh scene the entire two by two grid like what is going on in there it's based on a previous experiment that's very similar but uh it would just be it it's it's a way to kind of simplify uh a lot of the principles we're sort of talking about here

but also being able to see something like trying to theorize, like how does seed construction happen?

If you look at a broader,

uh expanse and you're trying to figure out like infer a belief about what is going on there well your eyes are limited and so they can only look at certain points so how do you start to construct a belief about what the overall scene is based on particular features that you look at and you have to make inferences about those individual features in the first place so you know for me to look at a

you know, an overview of like a street, and I don't know exactly where, you know, that is in the city, I have to look at different parts of the overall image, I have to infer, oh, that's a particular sign, that's a particular building, so on, eventually to the higher order inference of where am I?

What is this location?

So I just thought that one would be interesting.

I'll try to add

other things that that maybe come to mind uh in the coda um and uh yeah really really interesting talk today so thanks everyone thank you fellows see you yeah thank you so much bye guys bye
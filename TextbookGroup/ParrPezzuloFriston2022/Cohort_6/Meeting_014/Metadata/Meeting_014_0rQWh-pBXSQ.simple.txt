SPEAKER_03:
okay it's July 29th 24 and we're in the first of the applying active inference sessions for the third week of the cohort six and Andrew is going to give some overviews and share from his recent tutorial so go for it perfect um thanks Daniel and um


SPEAKER_02:
As I mentioned to the other session attendees today, I did have a bit of a full workday, so not fully prepped to to run through all this, but I am happy to present what I have and just kind of bear with me a little bit.

I presented this tutorial.

about a week and a half ago, July 17th at University of Pennsylvania for the 10th International Conference on Computational Social Science.

Wonderful experience doing that, by the way.

Super cool to see some other folks doing active inference research at that conference.

And super great to meet some folks there, including Brennan Klein, who I believe has had some degree of affiliation with the Institute in the past.

And he is also a PI MDP package developer, one of many.

got into contact with them while I was developing this tutorial.

So it's just excellent having him in the room.

It seems I did their package some amount of justice in putting together this tutorial.

So

The intention of this tutorial was multifold.

One, I myself am an intern at the Institute, and so I'm still learning a lot.

That said, I've been involved long enough to where I've been itching to learn much more about coding

more directly as opposed to reading the textbook inside and out, as I have been for at least a year now.

I've been involved with the textbook group and facilitating meetings.

So a learning experience for me, a teaching experience in the sense of presenting it at a conference, as well as hopefully developing it as a resource for

other folks at the Institute who might be coming at active inference from maybe an angle that they don't have a lot of prior experience with, say, reinforcement learning, the logic of which is very relatable to active inference and building simulations that way.

And then finally, the theme of the conference was social science.

And so I wanted to develop a multi-agent

model and i spent a little bit of time looking at at kind of popular uh agent-based modeling paradigms uh in that that people still kind of use and study in social sciences today uh so agent-based modeling being again this is one of those moments from you know kind of adaptively figuring out how to go about this but um

Yeah, let's jump to the slides maybe.

So there we are.

Yeah, I want to develop a multi-agent simulation that basically recreates a popular social science paradigm.

In this case, it's creating in number of agents, however you prefer to set it.

It groups them together, connects them via like network logic.

The code I wrote has two different options.

You can either make, say, in groups of M agents.

So you could have like four groups of five agents or otherwise, however you want to set that.

As I was discussing briefly with one of the other attendees today,

I work in education, so I particularly like this paradigm because I think about students in our classrooms, the schools that I work for, and the idea of like, okay, what's a good way of facilitating them learning, learning to cooperate with one another, but at the same time, not

not get too dependent upon relying upon each other for putting together a group project or something.

They should be able to learn in such a way that they develop both a sense of personal efficacy, but also being able to work within a group.

And so the paradigm has to do with something like collective problem solving, where you have agents and networks, they're collectively solving a problem.

The problem is based in what's called an NK landscape.

And it's basically just

basically agents seek to find the best solution to an abstract or hypothetical problem.

You have a whole ranking system where there are a series of potential solutions represented in some number of bits or digits.

And then there's some degree, some kind of complexity parameter that you can use such that whenever you increase it, it's as if,

changing your problem a little bit can dramatically change how good it is.

One quick way of saying it is like, well,

If you want to design a car, if you remove the wheels, it's going to be a terrible car.

You just changed one piece of it, and yet the entire solution, the entire schematic that you've built for your car is junk now.

So that's the idea.

There are these solutions agents are trying to solve that have a lot of interdependencies between the different parts of the solution.

So let's see.

So the tutorial, it related to different themes with the conference, agent-based modeling, which I've kind of already touched on.

And it highly relates to Active Inference in the sense that we're building agents.

Agents are in an environment.

Agents interact with one another and the environment itself.

like that's your overall kind of simulation, right?

And the way that this has traditionally been done in computational social science is to use software like NetLogo is a very popular one.

But the idea is to just write a script for your simulation.

It has its own kind of syntax and variables and so on specifically for NetLogo.

So it works really well whenever you

You can write the code a certain way and then you can easily create buttons and interface options whenever you go to run your simulation.

All of it's kind of really integrated.

But that said, it doesn't allow too much on the back end.

It doesn't allow for too much complexity.

And so usually the agents are not really integrated.

They're not agents in any way that we've learned about agents in active inference.

They're more like a non-adaptive thermostat, right?

If it's too hot, they warm things up.

If it's too, you know, and vice versa.

So that's it.

Regardless of circumstance, they don't learn anything.

They're just kind of hard coded.

And so my whole plan here was to recreate that popular, the NK landscape parallel problem solving paradigm

but using active inference agents such that agents kind of have something like autonomous decision making.

And let's see if we can find something.

It represents that.

I supplied some definitions of how we tend to think about agent-based modeling.

Again, I wrote this for an audience who might not have any awareness of what active inference is, but it's still a computational social science audience, and so they will be presumably familiar with agent-based modeling.

I include some guidelines on just modeling really anything agent-based modeling, but otherwise four common principles.

The first one, verisimilitude, which is basically just to say, are we making realistic assumptions?

Whenever we set up a model, an agent, and we have to make a series of assumptions.

And so do we...

Does the audience, do the readers, do they agree with those assumptions?

Are they reasonable to make?

Is there any kind of empirical support for the assumptions you're making?

If not, then suddenly we have some very faulty scaffolding for the entire simulation that's to come.

That said, we also need robustness.

Tiny changes to the model presumably should not lead to dramatic changes in the final outcome.

Otherwise, what you've done is that you've created a simulation or a model that's a little overfit that really works well for a particular circumstance, but not so much for

just about anything else you'd wanna use it for.

Reproducibility is really important.

I wanted to be very transparent in making this tutorial.

I made all the slides available.

All the code is available.

We have added the GitHub for this tutorial to the active inference applications or implementations on the CODA.

I can share that again with you all.

And then finally, number four, non-triviality, which would just be ideally we would actually extract insights from doing agent-based modeling or in our case, active inference.

If we end up just creating a simulation that shows us something that we already know from a ton of other literature doesn't come off as particularly useful.

It can be in the sense that it's reconfirming something from another angle.

But I don't know where we're at as far as the cost versus the benefits.

Yeah, the idea of underfitting versus overfitting versus producing balanced models.

This is just fitting a polynomial function to a sine wave.

And so the overfit model is great until it isn't.

It skyrockets and predicts that y will reach up to 400,000 there.

So anyway, many of you will probably be familiar with this already.

the idea of over versus under fitting in computational social science.

There's been a term that I argue exists in the literature over the past ten plus years.

People have identified there is a kind of beautiful simplicity to

to using that logo to make these kind of hard coded agents, because the point overall presumably is to try and see, OK, whenever you take these hard coded agents and have them interact, there are emergent behaviors and properties that come come out of running that simulation.

But it's really foregoing a lot of, you know, we have such incredibly simple assumptions that we're using whenever we make

simulations like that, there's kind of been a call in the literature for making kind of more rich, something like cognitive models where agents can make their own autonomous decisions.

Here we are.

And so there's been a push for that.

So having mechanisms for, say, like memory, decision making,

And and being able to act and react in different ways actually inferring the agents being able to infer kind of for themselves, what is going on, as opposed to a flat agent who just does X, if y else do Z kind of kind of logic.

And there's been a lot of references to interdisciplinary work, which also I think is very much up the alley of active inference in the sense of, you know, we're looking at neuroscience, we're looking at information theory and cybernetics, we're looking at biology, we're looking at reinforcement learning, machine learning, we're looking at many different things.

And a lot of kind of rich conversation that comes out of that.

So and then in addition, the kind of benefit to social scientists is that what happens whenever you're engaging in more like on the ground work where you actually want to design interventions or policies that that impact

ideally people's beliefs, not in a kind of pure social engineering kind of way, but what happens whenever people are engaging in self-damaging behaviors, or in my case, what happens whenever I would like to just design a new way of doing group projects that maybe better foster learning for the students where they can learn to work on their own and with others.

The kind of hard-coded agents in NetLogo, they don't learn anything.

There's no change in their beliefs.

Technically, they have zero beliefs.

There are a couple lines of code that tell them to do X if Y, otherwise do Z.

So, you know, this general description, all of you familiar with active inference will be familiar with a lot of this language of cognitive modeling, perception and action, generative or generative models.

I try to describe this as like we're kind of looking at models in a model.

So it's like, you know, we have the typical environment and then the agents are within it, but now within the agents, you know, is an entire generative model itself rather than a couple of hard-coded roles.

And, you know, there are other aspects here that we've touched on in the textbook group before.

There's the whole explore-exploit trade-off.

I kind of detail how

how that is implicated in active inference.

There's been different discussions over what the authors meant in the textbook or what that means whenever they say that exploitation and exploration is like naturally balanced in active inference.

And I try to provide a little bit of clarity to that in addition to the fact that many computational social scientists who are

uh wanting to do cognitive modeling are usually working with reinforcement learning based models and so they are familiar with uh this this explore exploit trade-off kind of issue um the point is not that active inference magically balances those two things it's that they're both kind of included uh as as terms whenever minimizing particularly expected free energy in the forms of epistemic

and pragmatic value.

That said, an agent could still only ever encounter pragmatic value or only ever encounter epistemic value.

The point is that both of those terms are within the same function, whereas in reinforcement learning, a common way of doing it is to

kind of hard code rule uh for what defines those two things and it's kind of left up to your own decision making on that like Epsilon greedy behaviors where you just say okay with probability one minus Epsilon you do an exploitative action um and and with probability

uh vara epsilon you do an exploratory action so it's the the point is that the agent here by minimizing free energy is kind of

choosing for itself which kind of behavior to go for.

And there can be a blend.

It doesn't have to be one or the other.

It can choose an action that will have both epistemic and pragmatic value.

So I think it's just conceptually a new way of maybe even thinking about what defines these kinds of behaviors in the first place.

I find it personally very interesting.

I go a bit, sorry.

I go a bit into reinforcement learning, but I've already kind of touched on that.

One of the attendees here, Zach, brought up POMDPs.

And those are used in reinforcement learning.

They're what we'll be looking at today and in the tutorial generally.

But the main idea here is that we're looking at an agent has

beliefs about latent states of the environment that they can only infer.

They engage in actions in order to attempt to pursue their preferences or goals or realize their priors over preferences.

And then that action is sent into the environment, which then itself

produces a new observation for the agent based on that action.

The agent then takes in that observation to update their beliefs about the hidden state.

So we also have Markov decision-making process, which is the same thing, but a little bit more simplified.

The agent has no,

latent states to infer um it's just a very direct they act the environment tells them the true state of the world and it's just back and forth so you have those two things um

You know, I discussed epsilon greedy behavior transition active inference.

You know, I kind of describe active inference is like, well, it's kind of it's kind of filling in some of the gaps that reinforcement learning generally seems to miss in my view.

I don't mean to pit these fields against each other.

I think that there's a lot of valuable communication that can come from trying to interface between the two of them.

But that said, specifically for

agent-based modeling and something like cognitive agent-based modeling, active inference provides just a lot more for informing what should those initial assumptions that we're making when we set up a simulation, when we build an agent, and so on.

like i think it provides a lot more material in in support for those kinds of assumptions that we're making i think that it's it's it's pulling from uh decades now of of uh research into neuronal dynamics uh into psychology and other fields such that you know i i don't i don't at all need to say that it's kind of the end-all

be all of things.

I think what it does do is provide a very nice framework that reinforcement learning is missing.

Reinforcement learning is usually focused upon

task optimization in most research that I see.

The point would not to be to emulate humans, it would be to emulate super humans.

You don't want humans, you want some kind of AI tool or assistant who can do the absolute best job possible, which makes plenty of sense for trying to optimize different kinds of processes and so on.

For someone like me who's interested in social science and computational psychiatry, I'm looking for a little bit more realism as opposed to building super robots, even though you can kind of do both with these same tools.

So active inference, as we know, supplies a neural process theory.

That said, I didn't want to scare everyone by suddenly talking about neurosciences if you have to be an expert in that, because as Friston and others say in the textbook, the idea is not to emulate the entire brain.

It's to find the generative model that describes the problem the brain is trying to solve.

So that helps us to kind of stay, kind of balance things, so to speak, between...

the active inference approach and maybe what agent-based modeling is attempting to do, which is to make things as simple as possible, but no simpler than that.

which algorithms and which architectures for approaching cognitive agent-based modeling.

So we have the free energy principle.

This supplies explicitly a belief-based framework for engaging in agent-based modeling, which is perfect for the social scientists who claim that

You know, we're lacking the ability to make agent based models that maybe are directed at people's beliefs.

That's yet another reason why I think active inference kind of supplies like this is a belief based framework that is precisely what we're trying to to look at a lot of the time, in addition to the various dynamics that are playing out.

Again, exploration-exploitation balance, kind of touched on that.

Agents are self-evidencing, which is they're trying to realize what they already believe, so to speak, and so they can engage in perception or action.

They can change their mind or they can change the world.

And then variational Bayesian inference, as we know from the textbook group.

Surprise, Bayesian surprise is often intractable whenever it comes to working with...

you know, continuous cases in the integrals or otherwise just the amount of computation involved or being unable to find the analytical solution in certain cases.

Oh yeah, and then, as I mentioned, with the whole kind of super humans development approach from reinforcement learning with active inference there's this notion of false inference so.

The whole point of introducing that is simply to say we're not trying to necessarily make super humans we're trying to.

to kind of better replicate what a human might do or some other sentient organism or otherwise, including just the idea of optimality is relative to the model.

Someone with, say, post-traumatic stress disorder has their own particular generative model and certain kinds of outcome behaviors that you see from them, while they might not

appear to be optimal for the current social situation they're in.

If someone suffers from symptoms, I say all of this as someone who also does research on PTSD.

Quick disclosure, but it might not be optimal for the actual situation that that person is in, but it is optimal as far as how their generative model is currently functioning as far as parameters and values go.

um so that that's that kind of realism element here um then the less active inference agents are highly competitive if you're trying to develop them for task optimization um so i just decided a little bit of research on that um and then the model architectures they're often used in

reinforcement learning like MDPs and POMDPs are also heavily used in the discrete cases for active inference agents as well.

Supplied some further reading.

Three hours sounds like a long time, but there's a ton to cover here.

I appreciated talking to a couple folks who do active inference at that conference, and they said they did not know how they could have fit everything into...

three hours and they they said honestly they think i did about the the best that one can do or something along those lines which was a big um compliment and and i felt really good about that because i was a little stressed on how do you fit everything in any case uh further reading uh for folks who are interested and so there's further reading on active inference in relation to free energy principle reinforcement learning

LLMs, which there's been research on combining

active inference agents with LLMs.

You can use LLMs as if it were the agent.

It can kind of report in natural language what it kind of thinks about the observation it just saw and why it's choosing the action it's choosing.

So it's kind of like having the code for the agent coalesce with these

chat GPT or otherwise prompts that kind of keep that sequence going along the way.


SPEAKER_03:
just a quick comment on that Andrew yeah absolutely single even single successful examples of the active inference syntax and semantics go in in like this multi-agent setting go an immense distance towards using augmented coding methods and saying okay that was four clusters of five edit it to do six clusters of nine or now add this like that gets things going in in a huge way so it's like kind of see the point that he's


SPEAKER_02:
pitching but also think about the bigger distribution you'll see adjacent possibles and many of those are like directly promptable right now sure yeah no no uh very very good point like to have that degree of integration um i think is is awesome and it's really exciting right now too um yeah no thank you thanks for that note um

I'll just kind of keep pushing on a little bit.

So again, this is for social scientists.

So here's some, you know, example kind of theory building examples of how active inference has been applied to the social sciences, examples towards development of treatments or other kinds of interventions.

You can think of like kind of policy interventions, too, at a broader scale.

Here, just general examples of active inference and changing contexts.

And these are only three examples out of many of them.

And I would argue that it seems like many of them involve changing context just to show how agents adapt.

to a change in the environment.

But I think it's just worthwhile to have a look at them.

And many social scientists are especially interested in things like, say, multi-armed bandit problems or cooperative joint action.

So I want to include those.

Some other example work.

Daniel Friedman, of course, being one of the major co-authors of the active inference paper and the ongoing research for that simulation, as well as

Another simulation, a couple of its co-authors also developed the PyMDP package.

It's actually from reading the epistemic communities under active inference paper that I wanted to contact the folks who have been working on the PyMDP package because I was curious how they did that.

But both of these papers were a big inspiration for me in trying to figure out as a new active inference coder, how do I make

you know, networks of agents as opposed to just making single agents, which I'd done up to the point where I read this paper.

So worthwhile to have a look at those for anyone who wants to get into multi-agent modeling.

more learning resources for the social sciences and active inference.

I would recommend these to really anyone, including folks at the Institute for sure.

There's also the entire Constructing Cultural Landscapes course that was delivered for the most part last year with the Institute.

So that's also a more verbal

uh verbally kind of delivered um series of courses that that relate this study of say culture and society to active inference uh but many of these other resources have more to do with coding like looking at

Sanjeev's upcoming Fundamentals of Active Inference textbook and the like.

A lot of good stuff here.

I did use a series of images from the textbook, always citing them, of course, but it's kind of like trying to move from this traditional rules-based modeling to inference-based modeling.

I recognize that I'm just getting to the point where you know this would have been roughly an hour into

the presentation as i gave it at the conference and which would also mean that there would be two more hours uh following that and we have 21 minutes and for our session here and then also i think there are other things that folks wish to discuss so maybe what i'll do now is just kind of shift into what i would say are some of the core components of putting together

an agent, like just a single agent.

And then that way anyone who's here today or watching this being streamed or the recording of it maybe can refer to the code if they would like to dig further.

But for building an active inference agent, which this is one of the figures in the textbook again, it's POMDP, what we'll do

This is kind of the idea of what, from my perspective, how you might want to design an agent.

You want to define the states.

What are they uncertain about?

What are the hidden states that they're trying to infer?

What are the observations that they come across?

Those observations are being generated by the environment.

So think about the environment and what kind of observations that it kind of sends out that the agent can then come into contact with.

uh defining controls which are something like that the actions that are available to the agent um but there's a you know there's different language there's actions there's there's controls and there's policies and so the way i've kind of interpreted that as a policy is like a sequence of actions that an agent can take an action is an action is just like a single unit like action available to the agent and then um

a control is like an action that necessarily does something to the environment that actually controls something.

Because if you look at some simulations, there will be actions available to the agent that don't actually do anything.

They just need to kind of be there to make the state transitions model that a POMDP has work.

So

which really simplifies the computation in that case.

Anyway, once we define those three things, states, observations, and controls, we would want to define these different probability distributions in the generative model of the agent.

So it has an A matrix, B, C, D, and E matrices.

And so we'll get into that.

And then after that, we can define the environment, which we've already been thinking about because we're thinking about actions and observations, each of which have a direct relationship with the environment.

Then after that, you define an active inference loop, like you'll have already defined and built your agent.

then you compose an active inference loop where you actually have the loop of an agent committing actions, receiving observations and so on from there.

And then ideally when running some kind of active inference simulation or any kind of agent-based modeling simulation, you want to have some kind of utilities that you've written out to help you plot, to record and plot

or otherwise the results of your simulation.

You can run the simulation various times with different parameter settings and then compare the results for each time you ran it to see what makes more sense or what is more accurate or whatever makes sense to you whenever you're testing your models.

But yeah, why do this if you can't see the results and gain insights, right?

And so that's the significance of comparison and revision.

So the agents that I had everyone build were, this is kind of my attempt to illustrate what that looks like as far as an active action perception or active inference loop goes.

So we can start, let's say here, an agent explores or exploits.

Those are just kind of pseudo clever names for the actions available.

But to explore means to work on developing a better solution than what the agent already has available.

An exploit is to steal

kind of steal the solution of a neighbor this is referring back to trying to engage in parallel problem solving to solve a problem uh based on an in-kind landscape right so so they can either try and improve their own solution kind of like studying or they can steal their neighbor's solution uh one that is better than what the agent currently has but they get to choose right it's not hard-coded in the original simulation it's hard-coded

to such that agents will necessarily steal a better solution if one is available to them.

And it's also a fully observable.

They always know that there's another solution that's better.

So in this case, like we have a partially observable environment, I think it's a little less realistic or excuse me,

one that i built i think it makes certain assumptions that are a little bit more realistic how do you know that everyone's solutions are better or worse than yours um you have to kind of study and understand their solution to a complex problem in order to adopt it for yourself and recreate it for yourself just at you know so anyway there are different ways that could be

But it's so that this is the general just they have actions available to them was actions are sent into the environment which processes the action returns in this case, two different.

Observations based on two different what are called observation modality, so you can think of them as kind of observation categories, and so the agents.

will see, based on their action, if they were kind of attending to themselves or their neighbor.

If you attend to yourself, the idea is that you're studying.

And so there's kind of a linkage here between exploring and observing yourself or exploiting and observing your neighbor, whose answer you just stole if you exploited.

And then they get to see if the new answer, whether they explored or exploited, if it was actually better than their previous answer or solution that they had.

And finally, they use that information to both infer the hidden state

um if if they were attending to themselves or their neighbor is a little counterintuitive but uh the idea is it's always the way that i have the agent's program it's always going to be one for one they're always going to know for sure if they explored it will lead to them having observed themselves

and they'll believe the hidden state is, oh, I'm attending to myself and I'm seeing improvements, or not seeing improvements, whichever observation outcome there is for improved versus non-improvement.

And then it incorporates learning, which is another concept that we'll have learned from the textbook group, especially chapter.

I think it's first is largely introduced to chapter four, maybe.

And then there's a lot more detail on it in later chapters, especially the second half of the textbook.

So that's.

I don't want to take up too much more of the time, and I think I've spoken quite a bit here.

But just so you know, there are you know, there's a lot more in-depth information.

And I used latex and all of that to recreate a lot of the formulas involved here that we'll also see in the textbook.

um but it's just you define your states define your observations and define the controls for the agent and then they get linked together so to speak through all these different matrices so the a matrix will be conditional probabilities of your observations given all states b matrix will be

uh you know what will the next state be based on a combination of the current state as well as an action that i do so if i'm hungry right now if i believe i'm hungry right now and i go eat food uh the next

hidden state at the next time step presumably might be that I am now satiated or will be satiated.

That's a way of thinking about that.

C matrix are your preferences to be realized or your priors over observations to be realized.

Your D matrix, probably the simplest one, is just at the beginning of each

step, you have a belief of what's going on.

They're like your priors over hidden states at the initial point of the simulation.

You need a starting point, basically.

And then finally, the E-matrix, which you don't necessarily have to define in PyMVP.

I can't quite recall what the case is in RxInfer, but the idea is that the agent starts off with some beliefs about

kind of which action they have available is better versus others.

And oftentimes you can just, oftentimes this is initialized to be uniform.

Maybe you intend for the agent to learn a task, and so you yourself don't know what the better action to take is, and neither does the agent.

And the idea is to start them with uniform beliefs over actions such that you can then apply the learning rules so they will kind of update what they think is better on their own.

And then there's direct, you know, these direct code references, like just as, you know, we see this kind of S superscript ATT.

Like I basically, I try to keep it consistent S ATT, like I aim to keep these things consistent across the tutorial.

And that way, if it's unclear what something means, you can double back, but once you catch on,

to the nomenclature that suddenly becomes easier to kind of follow things along.

So I included, again, code.

I included some outputs to help understand because PyMDP uses a kind of intricate way of using NumPy arrays as object arrays, and it allows storing kind of these, you know, irregularly shaped arrays.

whenever composing these different variables for your model.

But the point is, you know, you can construct an A matrix, B matrix, C, D, E, introduce learning, just what kind of what that is about.

Usually it occurs not at every time step.

You know, it's kind of like, oh, you have to take in an entire experience before you necessarily learn something.

That's sort of the idea.

or one of the ideas behind actual implementation of learning roles.

Learning, we're about out of time, I'm so sorry.

yeah and then once you have those things a b c d e and then um there are priors over a which are used for updating a um you can plug them into this agent constructor that the pi mdp package uses and there's a little bit of detail on the other arguments that i included um

But that's about it.

And then from there, it'll go into these kind of core functions of how the agent infers states based on observations, how it learns, which is just where it updates its variables, a, b, or d. I will note that I will be extending this tutorial in future for things related to model fitting.

At the time that I was developing this tutorial, I was using kind of like the standard PyPy version of PyMDP.

I did not at that moment in time realize that there have actually been an incredible amount of updates to that package that are really exciting related to model fitting, to other ways of doing inference like sophisticated inference, just a lot of stuff.

So this is as of now is kind of a living document.

It's not quite

finished.

But it's good enough to kind of get you to a point where you can develop an agent using PyMVP and then with the later code that we don't have time to go into, putting them into networks and then having them run through the active inference loop so they can attempt to solve the problem.

You know, kind of what that looks like is...

this.

So my kind of personal, you know, my sort of finding is that, you know, the way I have the simulation coded right now, I would, I would really want to find different ways of grouping students together.

As opposed to say, a fully connected network of students, right?

Like everyone work with everyone, right?

And

That's what I did here, a random network, but it's fully connected.

So it's not really random at all.

It's everyone's, everyone can take each other's answers as they want or not.

And basically the agents during, I have them go through two stages.

So during the first stage, they do eventually find the best solution and many of them kind of coalesce around it.

But that said, they learn,

to not rely upon themselves.

So by the end of the simulation, with one denoting exploitation and zero denoting exploration, the agents really rely upon just exploiting each other over and over.

And that carries over

going forward into the next simulation.

And because they're all trying to just steal the answers of their neighbor rather than relying on themselves to try and come up with a new solution or experimenting or studying,

um they don't even by the end after many time steps they don't actually even reach the optimal solution they just kind of plateau it's not a bad solution per se but still they plateau there and it only

kind of further reinforces like to not rely upon themselves and it's i could guess that that's like well you have 29 neighbors that you could take their answer instead of doing your own work why not do that um but uh it's you know in any case it's just this adds a lot more to trying to understand the paradigm because the original paper that produced this paradigm

They do not say fully connected networks are the best, but they do say that it does often lead to really good solutions.

And here, this actually is a way of looking at, like, how does this impact agents themselves?

Are these kinds of practices, do they have longevity?

Or do agents just kind of learn to

exploit one another to the detriment of the overall group actually by the end, because they might not reach the optimal solution.

So more ideally, the simulation would be run many times with changing the seed each time to see if maybe it's just by happenstance that these were the results, but I did run it several times and I frequently find the same result, but for the sake of a tutorial, we only do it once.

All right.

I think I'll stop things there.

I do want to apologize to everyone for taking so much time, but I really appreciate your patience with this.

I hope it was useful or interesting in some way, and I'll be sure to post the link to the GitHub in the chat presently.


SPEAKER_00:
I did have a few questions, if you were open to them.

You said that you were looking for a good solution to the problem.

But in searching for the solution, how do we know that the problem is well defined?

Inside of a lot of interpretability work, especially for reinforcement learning,

We're not ever allowed to say, this is what you want.

You're never allowed to tell the agent, this is the problem you want to solve.

We can only reinforce certain behaviors that seem to solve this problem.

So we run into a lot of problems, especially using like classical reinforcement learning, like a PPO and throw a transformer at it in order to solve a problem that is mostly aligned with the problem we want to solve, but slightly misaligned.

The classical example is whenever you have an agent moving right to the end of a video game to get a star, like Mario Bros or something like that.

But the agent never learns to want the star, despite that giving it the reward inside of this occasion.

It only learns move right.

How do you go about the problem of explaining the problem with active inference around it using the POMPD or DP?


SPEAKER_02:
Sure.

Thanks for your question.

And I hope I'm understanding it well enough.

But I mean, in this case, so the agent maybe I should have left the slides up.

I apologize.

But so it's whenever we

If you recall, the agent can receive two different kinds of observations.

And so one of them is if it's attending to itself or to a neighbor, that's all well and good.

The other one is if it's observing an improvement or no improvement in its solution.

So it's kind of like you can imagine just like a spreadsheet.

It's like one column or this potential solutions.

And each one of them has a kind of

they're called fitness values it's like they're exploring a fitness landscape but you can almost almost just think of it as like a ranking uh you know there are solutions all necessary solution all solutions are necessarily like better or worse than than other solutions so it's like a kind of ranking scheme um and so the agents

in their C matrix are programmed here to prefer an improvement.

And so they get that they don't know what the optimal solution is from the start.

They don't know necessarily how to get there.

They don't even know what its fitness value is.

They don't know any of that.

But as long as they know that they're going in the right or wrong kind of direction, which is that itself is very

very relatable to reinforcement learning right the the sense of like here's a reward signal like you you got a plus one or a minus one uh in this case the agent is just getting a improvement or no improvement it prefers per that c matrix the yeah the improvement and it dis prefers

No improvement.

So that's kind of the simplest way of getting at that.

And then what I think differentiates active inference agents from reinforcement learning models as far as interpretability goes, that we do have kind of like these words like preferences and habits and goals and so on, like all those are kind of baked into

the the the formulae that underlie active inference such that we can more directly interpret you know um maybe something as if the agent wants something or it's choosing this because of that or or otherwise that I think is kind of we I don't know if we necessarily get that kind of uh clear interpretability

from reinforcement learning models.

It's probably debatable, but that's my perspective.

I hope that was useful.


SPEAKER_03:
Just to shortly restate it, there's one and a half points.

First is the big picture is Goodhart's law is a challenge that's open.

Setting a measure and then having a proxy that measures that can become the proximate target of optimization.

The relationship between the proxy and the underlying state can diverge and that's what partially observable means.

Active inference benefits because the preference distribution is semantically interpretable over the observations.

So let's just say we have the maze on the right.

So then we have a belief about location and a belief about whether we have gotten to the prize or not.

We could say we looking at this model, we have a flat preference on location.

So assuredly, pragmatic value is not coming from location as a proxy.


SPEAKER_00:
that relates being able to control what is epistemic and pragmatic value and to limit pragmatic value calculations to only things that are loaded up on c and localize the epistemic value only to the uncertainty distributions i think i understand you're saying that like in classical reinforcement learning you have your loss which is not of assigned in variables to the environment because the uh pragmatics are assigned to the environment because the variables there

that means that we can get around this problem.

Well, not around it, like you said, good hearts, but it's going to be better.


SPEAKER_03:
is that what you mean i'm sorry i was trying to understand from this from this perspective reinforcement learning is like okay you're going to take all the policy space and you're going to propose this auxiliary function called reward and then you're going to use that as a single ranker but that is going to need to implicitly tie together epistemic and pragmatic value and it's also potentially computed in a like uninterpretable way with respect to which policies have what qualities

Okay, thank you guys both so much.


SPEAKER_00:
Great.


SPEAKER_02:
I just want to thank everyone here for this impromptu presentation, but it's exciting to see that it's maybe

going to be at least a little bit useful for folks at the Institute as well.

So there's a lot of cleaning that maybe needs to be done on the tutorial.

Maybe there are some excessive slides at the beginning because, again, it was intended to completely introduce every active inference in the first place to social scientists.

So there's probably some fat that can be trimmed using it as a resource for folks here or obviously here for active inference.

But yeah, I just want to thank everyone again.

This was really fun to kind of rehash this and also do a speed run of it, I suppose.

But I didn't quite clear it.

But yeah.


SPEAKER_03:
We'll do a future model stream.

And in that one, you could give the jump into the reduced version of the slides.

And you could just flip through them so people can pause.

Then we'll look at the notebook, the advantage of the notebook.

No one needs to install anything.

You can just hit run and get to the output.

And then also we'll work on getting the script based version up and that include more logging and interpretability analysis in the script.

All that tracks.

Chill.

Good idea.


SPEAKER_02:
Thanks, Dan.


SPEAKER_03:
And then abstract it and move it into RxInfern.


SPEAKER_02:
That may or may not be the move.

Yeah.

I like the folks, you know, developing


SPEAKER_01:
imdp quite a bit and i'm very excited about you know the new features they're adding and uh so i'm gonna keep working with that but you know that said um we'll see we'll see what's going on i have a quick question uh regarding um going through the rest of the last week we went over chapter one right so would chapter two be next week or


SPEAKER_03:
so the times alternate between cohort six and seven so when in doubt just check the schedules page just look for the date and the time that you're looking at but yes cohort seven will continue with chapter two and then chapter two and then the third week will be that kind of open one and then in cohort six

we'll do chapter seven for two weeks.

And then like that will be perfect for another deep dive on the discrete time.

Wait, so I'll have about two weeks to get through chapters three.


SPEAKER_01:
That's great.

You have three weeks, but yes.

Three weeks, wow.

All right.

Well, I guess I should take my time a little bit.


UNKNOWN:
Cool.


SPEAKER_01:
All right.

Thank you all.

Bye.

All right.
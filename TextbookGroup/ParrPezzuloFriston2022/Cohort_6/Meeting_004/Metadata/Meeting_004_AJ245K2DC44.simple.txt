SPEAKER_02:
All right, welcome back cohort six.

It's 219.

And we're in our first discussion of chapter two.

So Ali, thank you for facilitating and kick it off.

And then let's see where we go.


SPEAKER_04:
Okay, thank you.

Alright, so in chapter two, we delve into, I mean, the main

one of the main components of active inference theory, namely how to formulate action and perception based on variational approaches.

For perception, the optimization parameter that needs to be minimized is the variational free energy.

And for the action counterpart, the similar parameter would be the expected free energy.

So basically, in this chapter, we encounter two of the central equations of active inference theory, which are equations 2.5 and 2.6.

One for the variation of free energy, which formulates how the agent or the cognitive agent optimizes

Armenak Petrosyan, The variation free energy as its minimization parameter and similarly in equation 2.6 we see the same same mechanism as applied to a minimization of the expected free energy in the case of action.

But before going into the details of what those equations show and how they work in the context of active inference, it might be helpful to just go through the whole chapter in case there's any gap in the understanding of what the whole picture here is.

So first, as the title of the section 2.2 suggests, perception as inference is one of the motives of active inference because as we know, sometimes in the computational or the traditional cognitive science, we treat perception as a simple information processing mechanism.

But here in active inference,

perception is treated as a kind of inference and not just symbol processing or information processing approach.

And the way to do that is to formulate it in terms of Bayesian theory, Bayes theory.

So in box 2.1, there is,

this fundamental rule of probability about, I mean, sum and product rules, which are used in Bayes' theory.

And then in equation 2.1, the familiar Bayes' theorem is introduced.

And then through an example,

in figure 2.1, it shows how exactly priors and beliefs are formulated and expressed in Bayes' theorem.

And later, by using those sum and product rules, we can write the posteriors and specifically the posteriors in the denominator as a kind of marginalization over

uh over the probabilities of observations so that's basically what's been done in equation 2.2 and then uh here on page 20 there's this

a useful distinction between the notion of surprise as applied in psychology or at least folk psychology and the notion of surprise in active inference or namely the Bayesian surprise.

Sometimes it's called surprisal function.

So basically the difference between those two concepts are

they are obviously related, but they're not actually identical.

So when we talk about psychological surprise, it connotes something about the affective

a notion of that surprise.

I mean, we need to be affected by an unexpected observation in order for us to be surprised.

But here, when we talk about Bayesian surprise, it's much more formal and it only describes the degree or the extent of the unexpected event

regardless of how effectively it influences us as human observers or cognitive observers.

So it's just basically something very formal and statistical.

So one other key movements of active inference formulation is the use of callback-Leibler divergence.

for the comparison between the prediction and the observed data.

The justification to use Kullback-Leibler divergence comes from Neyman-Pearson lemma, which I don't think is stated explicitly in the text because

In Neyman-Pearson lemma, it states that the most efficient statistic to compare two distributions is the logarithm of their likelihood ratios.

So basically that's what callback-Leibler divergence is.

It's basically the, it's just,

the comparison between the logarithms of the likelihood ratios, the ratio between two likelihoods.

So here on page 20, we see the definition of Kullback-Leibler divergence in equation 2.3, and then it's applied to the example from figure 2.1,

in order to calculate the difference between the posterior readers and the priors from that example.

And then, okay, so here,

we come to an interesting discussion about the difference between biological, I mean, about biological inference and optimality.

So in this case, optimality can be, I mean, in active inference literature, optimality is defined in two distinct ways.

So here in this book, optimality is used

as a as a as a cost function that needs to be optimized or minimized.

So but there's another that's basically called Bayesian optimality.

But there is another notion of optimality, which is called Jane's optimality, which is used in Bayesian mechanics in order to calculate the dual aspect of active inference, namely

cmap or constraint maximum entropy principle which is the direct mirror of or the symmetrical counterpoint of fep but without going into much details about how that works and how that relates to bayesian inference here when the word optimality is used in this book it's almost always

It almost always means Bayesian optimality and not James optimality.

The view of active inference, here we treat biological organisms as machines that undertake these Bayesian optimality in order to optimize

uh the surprisal or minimize the variation of free energy or expected free energy and the way they do that is through a tripartite model namely the generative process and generative model and markov blanket that kind of acts as an interface between the generative model and the generative process

so very briefly generative process is what happens in the environment i mean external to the agent and the generative model is what the organism can perceive about the environment or at least anticipate the environment

Armenak Petrosyan, The data or the latent hidden states of the or latent states of the environment and it constitutes its general tomorrow, based on those processes.

Armenak Petrosyan, And the way it interacts with the generative process is through markup blanket which will come to much more detail in Chapter three.

So here on page 23, we can see this tripartite model in figure 2.2.

And then.

as again as the symmetrical counterpart for perception as action we can formulate that mechanism similarly for the action namely action as inference and uh the only thing that needs to be changed in equation 2.5 is the substitution of um

is a substitution of um i mean variation of free energy with the expected free energy so basically uh those two equations are uh related to each other and they're somehow

actually the mirror image of each other or counterparts to each other.

So, yeah, that's basically, I mean, the rest of the chapters from 2.5 until the end of the chapter is devoted to the derivation of those central equations, which we can talk about a little bit more detail this week or the next week, if you will.


SPEAKER_02:
Thank you all the awesome summary.

Does anyone want to?

Like what's the part that they liked if two or a question or thought one that was written in the questions that we can go to or a new one?


SPEAKER_01:
I have a question in terms of uh self-assessment so does it in terms of like creating almost like an Atlas in terms of like say if you're utilizing like a

a flow mechanic in terms of active inference so if you're in a state where you know you're working on a problem for example um using a tool and you've arrived at a certain point how to then map back without consciously interrupting your flow state

um what's the best way to kind of map backwards so you can extrapolate any information forwards so people have questions you know on how you arrived at a certain conclusion how you might be able to uh clarify if that makes sense there's lots of that question like um know how and know that so


SPEAKER_02:
Is it about knowing where you've gotten to or knowing how to navigate forwards or backwards?

Those might be all the same answer, or it could be all different answers and going, you know, swimming up the river, isn't going to help you swim back down the river, or there's a path on the side of the river.

Um, you might be able to use just passive recording, like


SPEAKER_01:
recording a screen grab or something like that so that certain parts could be checkpointed and and just kind of locked in I considered like obx in that sense um but I was just thinking in terms of like storage um you know if there's a way around that too in terms of like can you do a direct correlational map kind of like a neural net to go backwards through that you could build out

And then apply forwards as well.


SPEAKER_02:
Well, on the kind of, um, incremental improvement, you could, uh, use a time-lapse video in OBS.

Like you don't need to take a HD 30 frames per second.

You could do like a time-lapse and then, but to kind of bring it to the generative model side, you could come to develop your own model of perception and action in the interface with that tool.

And then by building out.


SPEAKER_01:
your model of perception and action with that tool you might arrive at a very distilled very reduced parameter description yeah i mean that's how i'm kind of considering it like how can i build nodes backwards that might give me inference forwards with you know less energy input in the future or in the present

Like, can you build neural nodes that would, you know, reproduce the same kind of predictive result?

By examining, you know, those priors and how they changed or branched from the origin.


SPEAKER_02:
I'll try to give one more active

thought on on that like so we're looking to be building some generative models something like figure 4.3 so a huge amount of the actual action selection dependent causal unfolding of the world is basically all loaded into b

Because D is just a prior, A is just a mapping between observations and hidden states.

So basically everything about like how one's actions in the world change it going forward is the expected free energy equation 2.6 B matrix here.

So let's just say that you were just doing a mouse movement tool.

One way to kind of engineer that would be try to, you know, memorize or try to reconstruct long sequences of planned mouse movements.

Another one might be just hyper locally understanding like the gradients around the mouse location or around the attention very locally.

And then trying to determine that attention landscape or that affordance landscape, and then try to either find yourself within or create new canyons of like canalized attention, where to go from point A to point B, it may be just as simple as just sliding downhill, rather than needing to like invoke symbolic cues.


SPEAKER_01:
Okay.

It makes sense.

I'm just trying to think, I guess it's a little bit more granular, but I was considering more, um, I mean, depending.

So personally for myself, I would like to build something, I guess that's more eye tracking because I do that more, I think than mouse tracking.

Um, but that's a different conversation for a different day.

So, yeah.

I guess it would still be, you could do X, Y parameterization in that same sense, but sorry, go ahead.


SPEAKER_04:
And if I understood you correctly, uh, there is another, uh, related, uh,

Sanyam Bhutani.

Sub theory of active inference called branching time active inference which kind of an attempt to improve the efficiency of active inference computation by using Bayesian filtering.

Sanyam Bhutani.

And.

Sanyam Bhutani.

So instead of.

The iteration of the update equations they alternate between integration of the evidence and the prediction of future states, so it significantly improves improves the and.

also reduces the complexity class of the of the algorithms and it improves the benchmarks significantly so.

yeah branching time active inference is a direct attempt to.

uh somehow do those kinds of computations with much less energy and with much more efficiency as well okay great thanks i'll redo that here's kind of a core


SPEAKER_02:
image with the two ways that the discrepancy is addressed or resolved with like change your mind or change the world like it's too warm in the room you know there's a plus it's plus three degrees too warm either that can come to be explained away by updating belief that that's the temperature or that can be changed materially

or you know the tension could be held there but these are kind of the options when there is a divergence and then how agents and the environment interact um thomas oh no i have a question so it's uh oh yeah go for it oh yeah just in general do you want to interrupt i wasn't meant to enter i'm not raising my hand to intervene on what's being proposed talk about it right now um


SPEAKER_03:
Daniel Passamaneck- Just a question, my question was related to in the policy section later partners chapter to the C parameter related to.

Daniel Passamaneck- g pi, so the future expect tense expectation part I was just curious like because that seems like a very important, but I was curious I saw there was a copy that element is there and it seems like on a surprising surprise.

Daniel Passamaneck- That there's a.

preference that there's an important part to play in the, I guess, I assume the pragmatic value part of the equation.

Yeah, it is.

It's part of that section.

So I'd love to hear, I'm expecting that probably talking more about later chapters, but that was just something that caught my mind, just more about that.

If there's some just clarification about where that's coming from and why that's there.


SPEAKER_02:
Yeah, Ali or anyone, what is the preference variable?

How does it come into play?

And how does active inferences treatment of expected and preferred observations have similarities and differences with, for example, reinforcement learning?


SPEAKER_04:
Yeah, so the preference variable, I mean, it's again one of the key moves in active inference to encode preferences.

So for expected free energy,

I mean, if we use the exact counterpart of equation 2.5, then, I mean, we wouldn't need to incorporate any other additional parameters

to encode preferences or anything like that.

But the dependence on policies can be dropped from the pragmatic value term.

That's the idea behind the encoding of the preferences.

Because although in some of the earlier papers of active inference,

It's not incorporated into the equations.

But in recent papers,

they tend to clearly distinguish this form of the free energy and media expected free energy from the variation free energy by incorporating that pragmatic value explicitly as conditioned on a preference variable.

Basically, the way they do it is to try to, again, embed the whole equation.

under or let me put it this way they by encoding the preference the whole equation can be somehow written down as a kind of lower bound of another optimization parameter which is which has incorporated this c parameter or the preference parameter

So basically what it means is that by replacing the true posterior with an approximate posterior,

This pragmatic value can be explicitly conditioned on the preference variable C. By flipping the terms inside that expectation value,

it can be treated as the exact opposite of the variational free energy without compromising, I mean, the symmetrical relationship between those two.

So why the epistemic value term here is important?

Well, because obviously, so take for instance, if we're in a dark room,

Armenak Petrosyan, there's this famous problem darkroom problem, the mapping between the hidden states and the and the observations would be entirely ambiguous, so the best way to reduce this ambiguity is to.

Armenak Petrosyan, Somehow seek out the observations that reduce this uncertainty by.

maximizing the change from prior to posterior bleed after a new observation or similarly by reducing the uncertainty about those hidden states by

reducing the variation of free energy.

So by applying the epistemic value here, it provides a way to navigate that space of possibilities when the agent tries to maximize its certainty about the posterior's belief.

So yeah, that's one of the key differences between the earlier active inference literature from about


SPEAKER_02:
up until 2021 and the later ones from 2021 up to recent papers thanks a little more on c so this is the pomdp partially observable markup decision process and every letter here is like a statistical distribution or a variable in the program so defining

The active inference agent is defining the ABC and D and so on.

So this is like defining the whole agent in its unfolding.

And so we can look at each of the nodes as statistical distributions or parameterized.

And then also when there's an edge, which is like a causal relationship, it's an influence.

And so we can talk about that as a conditional value, like conditioned upon this being the case, what does this?

And so that's the vertical line here.

How does active inference deal with goal oriented behavior?

So it's a big question, but at this kind of minimal essential level, the key move is to define a preference distribution explicitly over observations.

So in reward or reinforcement learning, you'd have observations coming in.

Like, let's just say we're talking about body temperature and different body temperatures would be associated with how rewarding they are.

And then movement would be performed to move to more rewarding states and utility discounting and all these other kind of ad hoc issues arising.

in contrast what happens in the kind of preference orientation in active inference which doesn't need to exist at all you could remove have a flat preference distribution just have purely epistemic behavior or or you could have purely preference driven behavior but what happens here is that the preferences which is hence the the origin of the joke preference slash expectation like what do you expect and prefer is going to happen

How does it become a self-fulfilling prophecy?

Because there's a coincidence of the preferred and the expected in a self-fulfilling agent.

So then the preference is explicitly the distribution of body temperatures.

So when I'm at 37 degrees, it's not like I'm in the most rewarding state.

And then if I drop down to 36, I need to increase my reward back by going to 37.

It's actually, I'm more surprised by being at 36 than 37.

And the ball rolls back downhill to the bottom of the bowl, resting in the low surprise state, the most preferred, the most expected, the bottom of the bowl,

rather than trying to hill climb a secondarily proposed reward function or utility function that's scaffolded on an observation distribution.

So observations are not secondarily evaluated.

There's just the definition of a preference over observations and that kind of grounds the model.

It's kind of like having it touch grass because otherwise how would the actions

come to um influence in this way that is control like um the the fundamental piece of the model and then just last piece here um here's the g equation 2.6 expected free energy and then this is just showing how when you remove or zero out certain components of this equation how you get other well-known forms

Like if you remove epistemic value, all of two, three, four, five, so all you retain is this pragmatic value of future observations conditioned upon preferences, you get expected utility theory.

Whereas if you drop pragmatic value and only have epistemic value, you have novelty, maximization, InfoMax, et cetera.

So this is kind of like a generalization on a bigger space of decision-making possibilities that run the gamut from arbitrarily pragmatic to arbitrarily epistemic and all of their mixtures, David.


SPEAKER_01:
Yeah.

So I just had a question related to, um, you said rolling back down, um, the bowl.

So in terms of, I'm just trying to think of it analogously, um, wouldn't that be kind of returned to priors, but at that, in that sense, would you bring your new experience back down into the bowl, therefore expanding the priors as well, or would it be combinational in a certain way?

We're bringing that back out of that lower state to that reward state and then back down with that transfer.

Any difference between the two states?


SPEAKER_02:
It's a great question.

There's been some work on C learning, preference learning.

So the easy setting is if preferences are a priori fixed, then yes, the pragmatic component of the agent will just, just like a moth for a flame, just like it will just keep trying to get to C. And so maybe for like a thermostat or something like a fixed C makes sense.

Then you could have a higher order control policy on C.

That's one option or, and, or you could have C learning.

Now C learning is a bit of a challenge because if you just updated your preferences to whatever was happening, you might as well not have preferences.

Like if you just kept on re updating what temperature you preferred to just whatever temperature was in the room, you might as well just be going with whatever is happening in the room.


SPEAKER_01:
Okay.

So it's more, more of a recomposition in that sense.

or reformalization?


SPEAKER_04:
Can I add something in answer to your question?

Sorry.

The probability of observation conditioned on C is totally different from the priors over states because

priors over states only encode beliefs about the true states of the word, irrespective of the preference or what is preferred.

But in this formulation, we, by using the condition, I mean by using the preference value C,

The observations with higher probabilities are treated as more rewarding so it's a kind of reward function, if you will, right but again it's totally different from the priors over states and also another point this kind of formulation enables a kind of.

planning as inference, which is distinct from both perception as inference and action as inference.

Because here, the use of these probability distributions to encode preferences and policy values brings the elements of action selection within the domain of Bayesian belief updating.

The key paper on that is

Kaplan and Friston's 2018 paper, namely, let me find the name of the paper.

It's something about, yeah, it's planning and navigation as active inference.

So this seemingly simple trick of using the preference value here, again, enables another form of inference, which is completely different from the previous ones as well.


SPEAKER_01:
Okay, great.

Thank you.


SPEAKER_02:
Does anyone want to ask a question or we can look at upvoted questions or we can look at any other question?

A lot happens in two.

This is kind of the first technical chapter, and it's the low road with the how.

Peter.


SPEAKER_00:
Hi there.

So I did have one question about the

um the kind of like tripartite model that's shown on page 23. um so i understand the idea that um i'm trying to sorry just use the the correct terms here um

So there's two hidden states that are listed, one that corresponds to the world and one that corresponds to the agent's model.

I understand the concept of why the hidden state associated with the process would be hidden, because it's on the other side of a Markov blanket.

Why would the hidden state associated with the model be hidden?

What's the sort of implication there?


SPEAKER_02:
Go for it, Ellie.


SPEAKER_04:
Uh, okay.

So the, uh, the implication of, uh, using the hidden state in the model, um, I mean, by, by using, um, X. Wait, I stopped hearing only.

Uh, okay.

Can you hear me?

Yeah, go ahead.

okay so by including x in the generative model uh it so the x asterisk is the true value of hidden state uh which is in the generative process right and in generative model the agent tries to model to or tries to proceed as accurately as possible

the true value of hidden state in the generative process but obviously it can't be done through direct perception or any kind of with any level of

complete certainty and it can only be done probabilistically so the reason on one side it's only x and on the other side it's x asterisk is to distinguish between those two values of x one being the true one the other

uh being the inferred one right uh so here the x as hidden state in the generator model is actually the inferred hidden state of the true value of hidden state in the generative process so i'm not sure if it's helpful or not but yeah well i'll add a few more pieces um


SPEAKER_02:
close synonyms first off there's many um kind of natural language assertions and descriptions that don't interfere with this essential particular partition um which one of which i'll explain but the hidden state also sometimes called an external state or a latent state um but it

It's externals hidden from the internal via the blanket and also there's a sense in which this is hidden from the point of view of the environment and that symmetry.

Here is a kind of very fun.

kind of late breaking post textbook group which is here's what we were just looking at.

generative model generative process, but then later.

we see Ramstead and others using generative model as system level description.

And then in this video and transcript, Ali basically asks that specifically.

So, suffice to say that when you're working with multi-agent simulations or where there's agents communicating across an interface, it isn't

always the most useful to be talking about internal and external because internal is always just specifically from one reference point.

So if you're talking about ecosystems of shared intelligence, then the way that they're addressing that with, with no fundamental change in the actual topology of the model, which is really what defines the variables, um,

At least they are using more generative model to describe the total constructed model representation.

So not just not focusing on partitioning generative models and process, but these are different natural language assertions on top of the same exact formalities.

But it just can be a little bit unclear because sometimes people are talking about the environment as generous process, but then it's like, but if the environment's another agent, then doesn't it have a generative model?

And what are we really talking about?

Well, again, the variables in the math or the variables in the computer program, when you build it, it wouldn't matter if you described it one way or the other per se, but this more recent is just a simpler, more inclusive, um, way to describe it.

However, it's also not.

distinction between generative model and process that the textbook insists on which is it's accurate it's not like it's going to mislead it's just that you'll also see people talking about generative model in a little bit of a broader way david then thomas yeah so uh just in terms of concept so i'm a little behind on some of the uh terminology so i'm just trying to grasp it a little bit more so i was just thinking in terms of the markov blanket um


SPEAKER_01:
as an opaque state versus a transparent state so we're updating are we able to is it kind of like a transference through that field state to another higher state or is it you know more of a transparency of the field being able to glean context from you know what is behind that and in that sense you know going back to the model that you were just showing if we

are let's say capable of generating something that includes the model the inferred state and the process wouldn't we then create another blanket encompassing the whole and then how would you approach that in terms of you know are you not reproducing at the same time a new state these are great questions yeah


SPEAKER_04:
Go ahead, Ali.

Sorry.

Okay, so the whole idea behind Markov Blanket and the justification for its use lies behind the concept of sparse coupling.

So in complex multidimensional agents or systems, there is a kind of sparse coupling between the agent and the environment in the sense that

the agent doesn't have any direct access to the environment, and the causal relationship between the disturbances of the environment or any kind of data of the environment and its causal effect on the internal state of the agent is completely nonlinear, or as Friston himself would like to call it, it's caused vicariously.

It's not direct, but it's a kind of indirect causal influence on the internal states of the agent.

So in that sense,

Yes, we can say that Markov blanket is a kind of opaque.

It's not transparent in the sense that there is a direct, I mean, transparent lens or direct access to environmental, to external states and data.

But on the other hand, there's this notion of Bayesian lens, which has developed recently through category theory.


SPEAKER_02:
yeah it is opaque they've explored that in the phenomenology setting like metzinger opaque states of mind and they've really mapped that closely to the blankie concept it's also opaque but but shines through that's why it's the holographic

And then you're totally right.

You can design or imagine special situations like where, you know, once you're dealing with metacognitive agents or counterfactual, like you can end up with some very creative and, and twisted settings.

And then it just like, well, you can make a map where San Francisco has, you know, an underground railroad and all these other things.

It's like, okay.

And then those, the map, the map play can be infinite.


SPEAKER_01:
Okay.


SPEAKER_02:
It's interesting.

Thank you.

Thank you, Thomas.


SPEAKER_03:
Yeah, to build on what Peter was asking originally with the hidden states, if we give them more, is this a part of the distinction of action policy discussed in this chapter in that sense of what we just what you just said, where the hypothetical and counterfactual operations, they seem to operate in likelihood as opposed to concrete probabilities.

So it seems like, at least in my notes I had about that, is that the hidden states, like basically the, I guess, expected X in the formulation is a segment or a trajectory of a hidden state over time.

And that's against, or I guess, conditionalized to pi, the conditional trajectories on the policies.

So just like there's some clarification about that.


SPEAKER_02:
yes okay so let's think about x tilde is the real temperature in the room through time tilde is just sequence through time we're prospectively talking about action now not all things prospectively play in action so vfe 2.5 might be sufficient if we were talking about like a rock but we're talking about something that's actually engaging in that counterfactual so the real temperature in the room x tilde through time and then the observations through time on the thermometer

um so one of the decompositions that we talk about the most with expected free energy and the one that aligns it the most with those other models shown earlier of like decision making is this epistemic and pragmatic value um distinction

But that decomposition is exactly equivalent to, first off, the physics-based energy myocentropy form.

That's what makes it a free energy.

But then these middle two forms, and these are linked, here is how ambiguous both have the same first term.

which is how ambiguous, what's the entropy, the policy dependent entropy of the mapping between thermometer readings and the temperature in the room.

So both first terms are basically saying like policies are better when they're less ambiguous in their sensory mapping.

And then this second term is basically very similar looking.

The only difference is here there's a Y tilde versus an X tilde.

because whatever our action selection dependence, uncertainty about thermometer readings is, if we had a perfectly accurate thermometer, our risk over thermometer readings would be the same as the risk over temperature in the room.

And then to the extent that we have even a tiny, tiny bit of noise in the thermometer, we're always more unsure about the temperature in the room than we are about the thermometer readings.

because these are the the proximate target of estimation of like preferences and then there's always further uncertainty beyond observables so how exactly complex like world unfoldings get wrapped into this that is the the challenge of making the relevant model

And what's interesting is that even if you write the equation in your program and it calculates it this way, you're also computing a quantity or scoring measure that has these equivalent interpretations.

But this is usually how it's written.

But then that's equivalent to this physics-based one.

This is the control theory-based one.

And then here's kind of like a

investment portfolio type one.


SPEAKER_00:
I just wanted to say thanks for that explanation, I think my misunderstanding with regards to X was thinking that hidden there meant something like hidden from the agents perspective or like metacognitively inaccessible, so this is very clarifying, thank you.


SPEAKER_02:
yeah like which especially when dealing with a complex cognitive entity that like could have access or not to its own state it's kind of like the simple case is just we're writing the software for a thermometer and air conditioner and it just has total access to the variables we're not going to worry about the phenomenology or like whether they had something that shielded from itself or not um

and and then again in this minimal case like the aboutness of x is exactly the the this x star the real temperature in the room but you could have a situation where the temperature in the room causes the lights to change and then the agent has a has a model where the latent state it's inferring is bitcoin price and it thinks that that's a causal relationship with the lights changing in the room

And it will do its best with the knobs that it has access to.

This is kind of what variational methods are.

Given the suite or the scope of the knobs that it can change, it will do its best with a KL divergence minimization to make the best approximation.

But that doesn't even mean that the aboutness of its model is remotely the same.

David Price- Like one case that's often talked about well, this could be a continuous variable outside and then the agent could just be saying too hot just right or too cold so like the type of variable and the aboutness of the variable can be very different.


SPEAKER_01:
David.

yeah so i'm still trying to grasp this and think i'm close but i'm not sure so i'm kind of looking at this in terms of analogously like unknown unknowns and how that would update our own priors um where you know we may not be conscious i don't want to use the word consciously but aware um that our priors are being updated and then we do externalize them later i guess at what point do we then re-internalize them as active


SPEAKER_02:
good question so let's just say that we had a kind of a cognitively like structurally cognitively fixed robot um and uh then it doesn't even have the possibility to like uh like let's just say it's inferring the the location of one you know it's hunting one prey

But actually there's two out there or there's none or something it's like it will keep on doing what it can without one with trying to find the position of one.

prey item and then i'm higher and higher levels of flexibility could could start to address a greater diversity of real settings externally, but then that unknown unknown.

is a really interesting question, because surprise is kind of like an agnostic or pregnostic even alarm signal, because the increase in surprise due to the temperature reading

that might be like kind of uh a known unknown like i can deal with this kind of temperature increase we've done it before or it could be something structurally different happening in the world that basically no action is going to be able to address right and then that's where like the structure learning comes into play and it's like okay well now let's propose new latent causes

Like, wow, the one-cause model of temperature in the room is surprising.

I'm going to propose a two-cause model of temperature.

And then the abductive process of proposing new accounts, but even that can still be beset with unknown unknowns.


SPEAKER_01:
right i guess i was approaching it in terms of and this i might go beyond the scope a little bit but in terms of sensory perception so like you know we have our five main senses but that's what's known to us right in terms of conceptualizing it there might be you know trending towards an infinite number of sensory inputs that you know the brain

might process differently which we might find as you know our modeling gets better and we deal more with cybernetics and you know evolution in that sense aren't we kind of fixing the model in that sense and not accounting for those extrasensory unknowns that might be incorporated later and would it require then a complete reconstruction of the model as we are conceptualizing it um


SPEAKER_02:
with the metaphor of legos this is like the minimal lego tetrahedra four pieces this is the minimal um you know pick it up and juggle with it four legos that are put together that makes a total cognitive simulation now if you want to deal with where there's like higher order like volitional or adaptive structural changes to the model

Just by sketching it out, you can see that the state space is going to expand like mega exponentially very fast.

So to do adaptive search or even persistence in like structurally volatile spaces is a very large question.

So it wouldn't unseat.

the Bayesian mechanics or make something simpler than this, but certainly you could make things way larger than this graph that have a lot of those kinds of like reflexive structural modification and structure learning.

It's just that in practice, first off, not all of those capacities have been brought from the code and the research, or sorry, from the research and the math into the code.

So sometimes that would need to be like implemented

um ad hoc in in a software package but we hope to build that into the open source packages and so on and then even if the capacity has been translated from the math into the code like structure learning has been then there's often a really really large optimization or parameter sweep step because you're just increasing the the state space of the model so vastly


SPEAKER_01:
okay so this is kind of your gen state right for your your seed growth basically so everything's going to grow out of this despite you know where it may grow to in that sense like through the expansion we can still go back to that base model


SPEAKER_02:
yeah like the the this uh structure learning is like proposing another latent cause and then asking whether the the more um the higher model complexity has sufficiently improved accuracy and then there's Bayesian model reduction which is you take your model and you ask like which of these parameters is like the worst and which one can I reduce out

And so with this kind of like plus one and minus one parameter moves structure learning and Bayesian model reduction, that's kind of like, that's like the AGI active kind of dream slash meme would be, you'd start with a single state space.

And then there would be a process that autonomously or, or in a scaffolded way that the agent would basically propose alternatives to its own model.

including like arbitrary or higher order levels of whatever cognitive state, and then guiding that kind of neuromorphic evolution.

Again, it's an even larger state space, yet it would have like perhaps some of these qualities like parsimony or like ability to reproducibly develop appropriately and so on.


SPEAKER_01:
Okay, great.

Thank you.


SPEAKER_02:
Well, we'll return next time for more low road, upvote or add more questions.

And then if like you want to make sure that it gets addressed because we didn't look at too many of these, just tag or something.

And then at this time next week, that'll be discrete time chapter seven in the cohort five, which is kind of like the models we were discussing today with the POMDPs.

Hence why bouncing back and forth and everything with the chapters is all good.

So thank you all till next time.

Bye.

Thanks guys.

Have a good one.
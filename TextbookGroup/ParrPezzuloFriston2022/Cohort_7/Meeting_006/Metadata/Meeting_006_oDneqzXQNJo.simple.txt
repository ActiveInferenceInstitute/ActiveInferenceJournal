SPEAKER_03:
All right, we're in an open slash general session.

So Adam, perhaps begin with the question and then Andrew, go for it.


SPEAKER_00:
Okay, thank you.

So in chapter seven, in the summary, it says that inference using both simple and more complex generative models can always proceed through free energy minimization, which illustrates the generality of the approach.

And my question is about whether or not there is a partition function for the model of the minimum free energy state.

And I wanted to note how about in section 7.3, they talk about sorting the states of the world into categories of whether the creature has control or whether it does not.

And then it says that the

the transition probabilities for the ones that it doesn't are invariant.

And then each state that the creature has a control over the world has like a unique transition probability.

So then it's always going to try to select the transition probability, which is most effective at bringing it towards a free energy minimum state, right?

So that would be the equilibrium.

I suppose, which was making me think about whether or not there's a partition function.


SPEAKER_02:
yeah so.

it's kind of like to answer that there are a few different pieces going on so yeah with like you can see in that chapter where.

let's see age.

126 that just the second page of the chapter, you can see like, like the breakdown of the B matrix is they call it at the bottom of the page.

And so, like, yeah, you're gonna have a unique B matrix for every hidden state factor.

And it's the dimensionality is kind of like the, we're gonna

what is the state going to be at the next time step conditioned on uh what you infer the state to be in the current time step as well as um a policy so it's kind of like the agent will uh you know they'll try to think what can i do now to change what i think is going on now in terms of the in state to um you know a particular hidden state that i i kind of want to see in the next step and

I would say in a kind of all things controlled for kind of way, what you said is correct.

It will go with what the action that is most probable to get it to a state it wants to be in.

And that said, because free energy is being run across all of these different components of the model,

It's not necessarily the case that you can just look at the B matrix and immediately know what the agent is going to do just because one state transition is more probable than another.

when it's computing expected free energy, for example, that, you know, it's broken down into epistemic and pragmatic value.

An agent might choose to transition.

It might choose to do something to make a state transition that it isn't really clear on.

For example, it might compute like, oh, there's epistemic value there.

You know, and then if you have your agent like learning its B matrix, those values will get updated.

over time.

So I hope that that kind of framing is is useful in some way, as far as a partition function.

So I just want to I want to better understand and maybe it'll be helpful for others to how would you define a partition function and in the way that you mean it?


SPEAKER_00:
yeah you know what um i don't even really know like honestly i've never actually formally studied um statistical physics so when i like i've asked there's also like a partition function in is it number theory and i was i asked all over nil about it on on youtube and he gave me this

I'm going to be rambling on.

He sent me a paper that was actually discussing the relationship between the partition function in number theory with the partition function in statistical physics.

And my question to him was, was it the same one?

And he said he didn't really know.

And I had the paper.

I was reading.

It was interesting.

I forget.

I don't have it offhand.

And in this case, I'm kind of wondering, like,

the word partitioning in terms of sorting the two states of the world from the ones we can change to the ones that we cannot and then at equilibrium like the um those states become stationary i suppose in some sense and uh so but i'm also wondering

I'm also wondering, would the partition function manifest as the different aspects of active inference that are supposedly emerging, right?


SPEAKER_03:
Let me give a few thoughts on this just really briefly, because you're in the right track.

Partition is being used in a, I hate to reuse the pun, but it's a particular way.

And so a lot of the momentum in the current kind of wave of active inference is from a 2019 paper from Carl Friston.

so a free energy principle for a particular physics particular here's the joke it cleaves off particles we call the entity a cognitive particle whether it's rock metronome or like more sophisticated etc and then it's a physics for those kinds of partitioned systems so that is taking a bayesian graph and then doing certain partitions on the graph

there's other uses for the term partition like you brought up the term from stat Mac so here partition function describes statistical properties of a system in thermodynamic equilibrium so there the free energy is like Gibbs free energy like why do candles burn once they're activated forward like why is time going that way why don't candles like unburn so that's on the thermodynamics

Then over the last like 50, 100 years, all of these mega developments with information and thermodynamics, like analogies between them or isomorphisms, but also unity between them, like with the Landauer limit and how much energy does it take to read or write a bit?

it's a kind of info thermodynamic space that the free energy principle is related to um so are there well described summary variables that describe systems that are in different kinds of info dynamic equilibrium there are so the simpler case is like a fixed point so i'd be like the rock is just resting on the ground it's like the belief is not updating

Then the next step would be like the simple update, like this billiard ball hit the rock.

It moved this much.

That's like this data point hit the prior.

It moved that much.

And then there's all kinds of like steady states that could be like an oscillator.

So you're going in the right area to see how those things are formalized.


SPEAKER_00:
Yeah, that um, so I guess it isn't like it wouldn't be describing, because I did read on on YouTube, there was saying the thermodynamic properties of a system like the temperature and volume.

And so if in what would those be parallel to like in an active inference system?

Would it be the aspects like decision making and balancing exploration and learning?


SPEAKER_02:
great question andrew what would you say to that um i mean yeah it's trying to figure out how how to phrase that well i mean it's you'd want to look at the i would just refer to like the the breakdowns the variational and expected free energy i suppose it's it's it's expected free energy where you're going to see that as i mentioned earlier that epistemic and

pragmatic value, differentiation, and that that plays out in learning as well, like expect, like free energy is being minimized during the process of, of learning where rather than, you know, inference, in the context of, of active inference, like inference is inferring states of the world, learning is inferring

uh, sort of model like parameters values, right?

So you're, you're going to be like the, the agent learns to adjust its own parameter values.

It's also going to infer moment by moment states, um, uh, the pragmatic and epistemic value play into that overall process.

And, and I mean, all these things are kind of happening.

um simultaneously so so so for me it's i'm a it's a little tough for me to like kind of rip out one part of that process and say that one thing is like what's happening um but i'm not sure if i'm answering that yeah questions super well or not but that's that's i think that was a key piece which is the epistemic and pragmatic value are a way to like


SPEAKER_03:
reweight the policy prior into the policy posterior so for a kind of thing that's not doing the cognitive task of something like policy updating so if something's drawing from habit that could be like a go to the simplest and fixed habit it's like the rock is always in location one so it's like it's being sampled from location one with a hundred percent again and again it's kind of like a redundant or a tautological statistical distribution

and that is also what comes up is like defining things as they're measured to be so then rather than the the space of possibly dissipative things he's like well the measurements over this time scale are such that it is returning to an attractor and then what are the dynamics of that attractor is it like just a marble going to the bottom of a bowl and perturbation brings it back or does the bottom of the bowl adjust that's like learning

Is there another landscape than all those kinds of questions?


SPEAKER_02:
Do you think it'd be helpful to, I don't know if there's the, I like the, one of the simpler equations from active inference for just like computing

like the final posterior over policies i i just i've always found that interesting and like helpful for like describing to folks lesser familiar with active inference it's like like your your posterior over policies typically is like a softmax function softmax function over negative expected free energy times gamma which is referred to as it's like a

precision over actions, and then minus, I'd rather pull it up, actually, just so I don't misspoke here.

The main point is that it includes expected free energy and variational free energy and habits, all of which, you know, Daniel just mentioned.

a moment ago and it helps to give a an intuitive sense of like all three of these components are are going into the agent's behavior like what action they end up selecting right it's like what expect expected free energy as what it expects to happen and that's where it's uh priors over observations come in so it's that's where its preferences come in and then uh

variational free energy is just reducing kind of uncertainty in the moment.

And then habits are like these empirical priors that have been built up over time.


SPEAKER_03:
Yeah.

Another kind of big moment in helping think about this was this particular kinds

showing continua from like in a continuum of modeling continuity not as much as like an ontological claim about any actual system but in terms of a modeling continuity with systems that are being interfaced with differently ranging from kind of like inner unidirectional cause to kind of simpler inert to simpler non-planning like

on through like planning like with other cognitive phenomena like reflexivity meaning creation things like that so the simple ones really do look like this the graphs but this graph on the bottom is way more of like a schematic and then play those graphs might be very large um

Sriharsha or Courtney have any question or like different direction to explore?


SPEAKER_01:
I don't at the moment.

I was just kind of listening to your guys' conversation and seeing what you gleaned from that since I'm not that far along yet.


SPEAKER_03:
cool thank you yeah it's like always like skipping around on on a record here's one this is the bayesian mechanics paper this is kind of a summary slide thank you sri harsha so this paper this is like doctrine's activity and maxwell ramsted and others working on the bayesian mechanics so here's classical physics classical mechanics

stationary action that's like why the baseball goes in the parabola then statistical physics says well it's not just like planets and baseballs and gravity it's other certain behaved summary statistics like temperature and pressure that was what we were just looking at with the partition functions um quantum

moves that into the not just the scale that was being explored but also like the imaginary or like another axis that's orthogonal so that's why there's the imaginary numbers and the rotation and it's an equivalent physics of uncertain systems

especially interpreted in this kind of scale free way so it's not just like unique mechanisms related to electrons but those contribute to it and and uh constitute why the variability patterns are the way they are but people have applied Quantum formalisms across different systems even without relying on like some sort of um reliance on electrons or protons or anything

then kind of in that same frame and generalization and you can generalize quantum to be statistical physics with no um wave distribution and you could summarize statistical physics to be like that's why the baseballs moves like a parabola instead of like diffusing

Bayesian physics extends that to arbitrary state space.

So quantum mechanical state spaces, maps of quantum mechanical or stat or classical are within certain categories of maps and models.

So this Bayesian mechanics thread, especially a lot of the rigor that Dalton and Chris Fields and some others brought in, is only in the last, like,

to post again post 2019 friston kind of entry into that fundamental uh level of the interface but it's kind of also threaded going prior but took a lot of uh consolidation and development turn and this is not brought up in the 2022 textbook so it's kind of like the textbook is at this checkpoint

where the baseline, so in the textbook, it's like, well, what are the math areas?

And it says linear algebra, so matrix multiplication, and then dynamical and stochastic systems, like probability on dynamical systems.

That is like a checkpoint.

And then that's been some of the most interest over the last years was what is the basis of Bayesian mechanics

How does it matter or differ from the 2022 textbook style?

Thank you, Andrew.

Anyone can give a thought or a different question?

yeah sorry daniel what what were you referring to in terms of the linear algebra and the stochastic probability um but in in the textbook so one part is where they sort of uh

mention the kinds of maths that are needed like those are good keywords to see like here linear algebra calculus and and series expansion and then also there's the appendices but these appendices are kind of just like the the um the tip of the iceberg uh

A is kind of some of the code, but there could be a lot of work in MATLAB and other languages to make code examples.

Like that's what we're doing in Julia right now.

Appendix B goes through the equations, but still it's only showing some.

And then here's OC is the code.

So A must be the backgrounds.


SPEAKER_00:
Yeah.

Have you ever watched Oliver Nell's YouTube?

Put a link.


SPEAKER_03:
I don't.

put a link I don't recognize no so here's the four topics linear algebra series approximation like series expansions function approximation variational calculus and stochastic dynamics so systems that have dynamical nature and probabilistic nature but that's the math

that's what but this is also foundational for going further into Bayesian mechanics because in many ways it's just like more um remixes and and kind of like concatenations of these core mathematical motifs there's been um there's been many people's contributions to making the math more um

relatable it's just like it's a it's a it's a great question and to an area to kind of run towards and contribute to and explore like jonathan shock is a math professor he wrote this great text um

however it it may be like you know more or less than someone is looking for but it is really cool even like like like even a p-value or or um like a bit there's like a lot of things to return to so it's not like a check it off your list because the basis of this is just like surprise and entropy so then that's like why it's important to return to it many times

Because it's not like you can just cram it and finish the topic and then just like never returned to that node.


SPEAKER_02:
Okay.

Cool.


SPEAKER_03:
Cool.

yeah and then in the textbook group like the work that uh octopus is doing right now over the years people have brought up many questions and thoughts about learning math and about how math connects and then octopus is now like more um holding down these meetings and just trying to coordinate more math educational work

That's something that's always there, always improvable if somebody wants to start, like just on any part.


SPEAKER_01:
I think Axel made some models and also mentioned that if people were interested in that, he'd be open to collaboration too.


SPEAKER_03:
Octopus said that?


SPEAKER_01:
Axel.


SPEAKER_03:
Axel Constant?


SPEAKER_01:
Axel Sorensen, I think in the chat.


SPEAKER_03:
Oh, Axel Sorensen.

Okay, cool.


SPEAKER_01:
Yeah.

cool yeah like for the math and stuff so people could see the interrelationship between the terms and things like that yeah on on well on the terms uh


SPEAKER_03:
if people want to follow up that like more formal but natural language so not using equations per se but um going into like terms in their relationship that's like another dimension with the active inference ontology so that's the like that's the generic system description

then those are the terms that actually apply to the generic system so in certain ways it's simpler than trying to give examples from systems because it could be like oh the action state of the robot well that's like you know that motor it's like well but then the motor has a sense element and then from the world's perspective it's it is a you know so it gets really specific when you are dealing with specific systems but the core ontology

is you don't even need to use these 64 terms it's very small and the kind of kernel of it is the particular partition and why and how there's a least action principle on a certain kind of partitioned graph so all the equations just setting up the path

then that's why these kinds of um nodes and edge figures come up all the time because that is the setting that's being considered and that's coming more from like the basie mechanics free energy principle side whereas um in in the textbook they actually set a target for active inference

which is why in figure 1.2, active inference is at the middle between that free energy principle, Bayesian mechanics, and then the bottom-up construction of real math and computational models.

So this is more like applied math and applied computer science, because it's like you're making matrices that represent a certain system,

or certain measurements and then up here is more like the generic it's like like why does gas burn why does wood do this and then here's like this specific this particular engine so then that helps recognize like somebody will make a claim analogously to something like well because candles burn my car will get 200 miles per gallon it's like not necessarily

or because metal has some structural integrity, this bridge design is gonna be fine.

So the bottom and the low road is like a very empirical inquiry and building.

And then here is just some of the more generic elements that are related to physics.

So that's why this book brought things together and structured it in a different way than like was existing.

Because it brings in that low road, high road approach, which can be like dug into and you can go deeper into low and high road.

But just the layout of like kind of setting context, low in the high road, just one way to think about it, but a very useful one.

then chapter four when they come together and then chapter five which is the most well-studied domain at this time which is mammal neuroscience so that's why even though there's sections like especially in four that are just like whoa what is happening um and and eventually we'll have more of like generative dials like turn up the math detail turn down the math detail but um in a balanced form

going from here's the context then take these two paths here's what it is here's where it's being used it does have those trails

um yeah the math the math learning education Courtney thank you all copy it in so like octopus can explore it yeah people have made many continua um what's kind of fun is there's there's a hope for like

doing something in a very different way because if you think about a technical concept like a p-value it's hard enough to communicate technically and bayesian um prior and posterior and updating even before getting into it just if they're learned one by one and through the kind of tech tree learning trees that are how they're currently taught

it's going to be a very long road with a lot of attrition so i think that's what motivates some of these questions like about math learning that people have um brought up over the years that's a really interesting layer to pull back to because like computers are used without math um

needed by the user so as that continuum and that stack starts to materialize like what and how are people learning what would be of epistemic and pragmatic value for them all those kinds of questions then sanjeev's textbook the fundamentals uh which he has finished the first volume of

It's two volumes, or this is just what I understand now.

First volume is related to active inference.

So that's kind of coming at it from the low road, largely.

Second volume is free energy principle, Bayesian mechanics.

However, the first volume is extremely comprehensive, gives a mega...

synthesis that's that will be um comprehensive enough for like very serious extended study and also use in language models Etc um so that will be very nice on the low road and then I think that's wise because it gives a more time for codification and synthesis on the high road elements

which I mean even if there is as uh minimum I mean that's the whole thing with Maxwell's equation and Newton's equation is sometimes it it projects down to these expressions that are just so small


SPEAKER_00:
yeah i mean there's a lot of a lot of there's a lot to go into here i suppose i suppose that um actually like typically in an active inference you're you're dealing with non-equilibrium states but you're always working it's always working to minimize the free energy so like it's it's non-equilibrium


SPEAKER_03:
Yes, importantly, it can include that situation.

So like you could have a cup of coffee in the room and they're at thermal equilibrium.

then you have non-equilibrium thermodynamics which is like heating and cooling and more phenomena if it was like a huge temperature difference or something so similarly it's good to check like yes i'm taking in this one data point i'm updating the prior here it is like and have a kind of um like it's at equilibrium like the temperature the thermometer is saying 70 my estimate is 70 it should stay at 70.

But then you can explore all of these situations where that is relaxed.


SPEAKER_00:
Yes.


SPEAKER_03:
So you have like a Kalman filter, which is like kind of a sliding window memory.

Then if you had it as just one thermometer measurement, it's going to jerk around and just reflect the measurements.

Or if you had it really like too heavy, it would just start taking the long run average.

So then that's like a Bayesian question, like what should the window and the forgetting be on the common filter?

And then the whole thing with the particular partition is like, now imagine if you were fitting that on the flows of all four of these.

So for each variable, they're all kind of undergoing that flow-based unfolding, which could be static, or it could be noisy, or it could be bimodal.

It's like trapped here, and then it's trapped here.

and then like what is the structure of all those variables it's not just for if you want to do something more than a thermometer abstraction but this is like the kernel that describes the particular partition and why and how that's coming from um or why and how you can have a path of least action on that flow which is what is set up here

and and improved and clarified also in but but this will always come up it's like there's the four states sense action internal and external fun and different letters depending on different people's use and so on um and then what their their time development is plausibly a function of

and that's reflected by the connections between the nodes um like that is what this is showing which is that there isn't a direct edge between internal and external that's tautological because it's what defines the interface if there was a direct edge you're just talking about a different situation

But these don't necessarily map on.

Again, none of this is a claim about how things are structured in the world.

It's like, if you are using a base graph, anything other than a fully connected graph, so if you have everything connected to every node, then there's no partition.

It's just like one connected mass.

If there are pieces that aren't connected to each other, then this situation comes into play.

That's interesting.

yeah totally and it's like a it's like a separate sort of um little subgraph yeah exactly like the the um this is the niche and this is our Roomba you know the motors again just simplifying the motors and the internal states and the temperature reading it's like we we have partitioned the Roomba from the niche but if there was like some sort of

wire between some inter then it wouldn't be internal state so it's like that's the whole chapter six plus plus plus question how do you make these kinds of maps for cognitive systems and and that helps sometimes to clarify like the the epistemic status of these maps because but why is this reflected that way it's like because it was constructed that way as the map

But they don't like, just take it alone, support or reject some kind of way that the world is.

But making them within a certain type brings the generality of cognitive modeling.

And then that's kind of just like the, how specific will you make the model?

Like, is the engine just one chamber?

and just like I'm just looking at heat across an engine overall or do you go and model heat at like the components or would you model heat transfer with this but those are all empirical and the the person of the team building that just addresses those as they actually happen how many Chambers should we model this engine is having it's like

are we building the model who's it for what questions are being asked that's what chapter six goes into um jeff also this morning and just like we're that was fun it was about the inspiration and the dream but it was

it was fun and and uh but the the pendulum is going to give more of that's going to come across more as like a uh that's more like solving an exercise in a physics class if it's like we have an oscillator with two states it's oscillating due to this

write the generative model that does this that's kind of that i mean it has like a kind of homework style feel not that there'd be only one answer but there'd be a continuum of problems from like there's only one answer as stated to there's multiple ways that you could have done that on through i mean it was just like where's time time perception

but then those those are those can you can still play with it and and provide and take different thoughts on it even if it's not and then it just is like oh well it's not about generating one specific model but if it was um if we're a startup and it the the financial um incentive was to make the thermometer with this or that property then it'd be again just more of an engineering question

also how it kind of revisits like chapter four shows discreet and continuous in one chapter like what's shared about them and then seven and eight revisit it but like just showing even from the beginning like it's the situation choice to model even how time is

Okay, okay.

Yeah, I'll put this in math learning resources.

You can add things, Adam or anyone, like, and if you're not sure.

Yeah, that's collaborative fun.

It's adding questions.

I mean, the hundreds of questions that have been asked did not come from nowhere.

and somebody with many questions could contribute many questions and someone who goes through many resources could just say here's what this one I just found it this way no one's gonna think it's like a total final judgment just this was useful for me learning this from here at least it helps show what different kinds of activity and different like practices can mean

and it it just helps show example yeah these are ways to there's no um death gap between like not knowing linear algebra and slightly understanding it but then that that comes down to everyone's specific situation how much time and what kind of thing and like yeah graphs like the learnable Loop

we this most recent blog post where kobus is able to from the rx and fur for the drone model for this to be automatically generated it's it's an incredible game changer it's it's amazing work it like

leapfrogs what visualization capacity for example exists in um python or matlab not because it's like a new visualization package it's just like you would be kind of starting on square zero it's like okay i have a base graph represented in matlab or python but i'm gonna just do like spread them all out or how is this even gonna work and for it to output like publication ready

And with the verified integrity from how it's actually specified, not being like this extensive PowerPoint, just kind of off-chain moves, which is doing one's best in the absence of this.

But I mean, this is like, whoa.


SPEAKER_00:
So what exactly is that graph figure there?


SPEAKER_03:
Yeah, great question.

So there's...

you like,

to continue to dig and explore the papers and live stream where this is explored more.

But basically briefly, there's two primary kinds of graphs that come into play in active inference.

The first one is a Bayesian graph.

So in that situation, the nodes are random variables and the edges resemble like causal influence.

So that's like causal modeling, all those kinds of like Bayesian models from Jadet Pearl and others.

nodes are variables edges are like functions that relate variables they're maps it turns out over the last like 10 years being applied more to to like Bert de Vries and Carl Friston but the fundamentals kind of lying in the previous decades every base graph has also this dual representation as what's called a Forney graph

Yeah.

Forney graphs, basically the nodes and the edges have the opposite character.

So it's like, whereas in a Bayes graph, you could have one node with like 50 connections because it could just be influenced by 50 variables.

Whereas the edge can only have two nodes it's connected to.

The Forney graph kind of inverts that where the nodes are like...

edges in the base graph and vice versa.

And there's like a dual relationship.

You can kind of represent them both ways and go between those two.

But it turns out that in certain forny graphs, you can take the Bayesian causal graph, you know, probability of rain, probability of that.

Here's the observation.

So that's the kind of like systems thinking, how are things connected to each other?

And then there's this rendering that RxInfer does under the hood.

to another structure that's equivalent and does the same calculation, but it does it in this way that can be like scheduled.

Whereas on a base graph, there'd be a lot of degrees of freedom in like, how do you properly update this and percolate messages?

And the factor graph gives a much stronger way to do that.

This is a technical detail, but it's like, what is making the Rx infer method more oriented towards the scaling and the engineering type efficacy?

Because it's a super strong engine under the hood for running these 40 factor graphs.

but also enables this more Bayesian semantic layer, possibly for people to interact with it, but to see it like we've seen it before and flip it also into the version that actually relates to how like the different things connect.

So just like new level of ability to look into the kind of base computational layers of these models.

in a but not by just making like custom visualizations like here's one some other large one like for and also

time-varying autoregressive this highlights which is so um wise by kobus like this isn't just about cybernetics this is just doing Bayesian statistics like on time series which could be like temperature climate markets Etc so it doesn't always need to relate to more questions about like embodiment or communication amongst agents it can just be like this is the graph and then that's just a regression

so it's not a theory about agents anything specific or any specific type of agent per se I mean everything of course I'm just saying saying informal takes but the fact that Phyllis is showing no you can use rx infer to do these just linear regression auto regression no no perception action here

Okay.

Well, whatever.

Also, I updated the calendar events so that it should say a little more clearly which piece is being discussed.

But yeah, we'll just continue on.

Any last comment or otherwise?

Thanks.


SPEAKER_00:
Thanks a lot.


SPEAKER_03:
Cool.

Appreciate it.

Farewell.

See you later.
SPEAKER_05:
all right welcome back cohort seven we're in our second discussion on chapter two so let's head over to two we can begin with anyone's thoughts or reflections or any question or idea or section of the text they want to begin with or curiosity from two or just some quote they want to go to

Anyone can just raise their hand or type it in the chat.


SPEAKER_04:
Okay, Axel, go for it.


SPEAKER_01:
sure so one thing that i found a bit interesting it's on page 26 of the book um it says the person can resolve this discrepancy in two ways it's talking about this thing of looking at um like expecting to see an apple when you look and then actually seeing a frog and it says first she could change her mind about what she's seeing so actually that it's a frog instead of an apple um or she can foveate the nearest apple tree and see something that looks very

resolved the initial discrepancy but in a different way so she's kind of doing an action to make the world fit more her model and they do another example afterwards but I am kind of curious about exactly how this trade-off is done when would you choose to do one thing over the other when would you

choose to do an action versus when would you choose to do a, like an update of your internal model, especially in a situation where you don't know, you know, perhaps if you've had this situation happen many times, you kind of learn that it's important to know that it's a frog.

So you don't want to look away, you want to update your model, but what if you're meeting it the first time?

I was a little bit confused about that.


SPEAKER_05:
Great question.

Anyone have a thought on this?


SPEAKER_02:
you're likely doing both in parallel much of the time somebody who's too fixated on one or the other that tends to be a neurotic problem or worse that's that's great um but it's a few few ways to take this first there's the secret third option which is neither


SPEAKER_05:
have to be resolved, but it is kind of like the equivalent of a ball on a shelf.

It's like an unresolved potential energy or like an unburned candle.

It's like an unresolved Gibbs free energy differential.

However, it isn't the case that a discrepancy is resolved one of these two ways.

However, these are the two ways that there can be the reduction of discrepancy or discrepancy can continue to increase.

So then the question is like, okay, in what ways are or could be discrepancy resolved through inference and through action?

There's different ways to take this.

In the particular partition,

So that's splitting up into these four nodes.

These two nodes that constitute the two directions of the blanket, and then the internal and external states that are partitioned off from each other from that blanket.

Those are all happening simultaneously.

So one answer is they're both separately in play.

So you can have a situation where neither belief updating nor action updating were happening, like a rock.

You can have a situation where both were concurrently updating.

Sometimes it can be helpful to think about these kind of multiple timescales of what changing the mind can look like.

The fastest timescale, which is kind of equivalent to the forward pass on a neural network, is just the forward inference.

That's just all the kind of parameters are staying the same.

And then new observation comes in, the Bayesian update cranks.

Then at a slightly slower time scales, corresponding to attention and learning mechanisms, there's kind of like, those are corresponding to like weight updating in a neural network.

And so those are like slower kinds of changing the mind.

Changing the world through action also, like that's very dependent on what the kind of action is.

For example, here,

the eyes were moved.

So it was a muscular action to bring the observations into alignment with the expectations.

However, there wasn't a change in the world.

So visual activity is kind of a special kind of action because it doesn't modify the world, but it is still a way to reduce discrepancy through action.

So it's kind of on the borderline, like, is that changing the world?

I mean, your eye is part of the world, so that is.

any kind of latent state.

Then there are the actions that have like a more turning on the air conditioner or the heater that actually modify

the world so short answer they're both happening uncoupled and then when you look at the specifics of the generative model then you can you could separately talk about like the proclivities of a model to update its beliefs if it has a fixed belief distribution then it's not going to update its beliefs if it has a very broad belief distribution prior then observations may update its beliefs a lot

and then similarly depending on like the efficacy and the agency of the actions that are being discussed then maybe taking actions to modify you know changing the temperature in the room if you have a really fixed physiological prior on body temperature

changing the world if there's no air conditioner or the agent doesn't know about it and it's able to like learn different physiological um temperatures or has a very broad temperature range that it could be okay in maybe that would be the behavior that you'd see here's like the temperature

Simply changing one's mind does not seem very adaptive, but acting to lower the body temperature... is.


SPEAKER_01:
Right.

Thanks a lot.

And I think I was just interested in about, so that makes sense to us that we get more out of trying to actively change our body temperature than just believing, oh, I actually don't have this body temperature anymore.

But where are we learning that?

Is that through some kind of trial and error that we're learning this?

Is this like an evolutionary prior or where did we get this from?


SPEAKER_05:
yeah it's a it's a good question so um when you construct the model then you get to create whatever the distribution is then I see well but how how did that prior on body temperature come to be and that the evolutionary account is um you know in the 98 degree thermal bacterial Niche the bacteria that didn't have optimal growth that didn't expect

those have been swept off the table, so we end up seeing a phenotypic distribution that's the fitness distribution of the bacteria in the thermal vent that corresponds to a thermal gradient.

Another kind of consideration is actions are being selected also from a probability distribution.

Like, there's no...

proposal of some auxiliary reward distribution.

We're updating our policy prior habit into the policy posterior.

We'll look at that later in the chapter.

So we're doing inference on action.

We have inference on perception.

That's the active inference.

So in terms of the belief distribution on perceptions,

about the temperature in the room but also there's the beliefs about turning on the air conditioner so now which which um then the the um sort of liability of both is defined so if the distribution that one of these if the habit distribution is like um a hundred percent I turn it on zero percent heater you know 100 air conditioners zero percent heater then that is like it's like a deterministic action selection

versus one that's more spread out across the action possibilities.

So you can kind of get that interpretability when you see the model.

And then that's why there's like, when you do specify models, like, and this comes up in chapter six, you kind of think more structurally about the model.

And then there tends to be a lot of parameter sweeping and like investigation across different combinations.

Like you could try 10 different kind of widths of prior on the perception side from very tight to very loose.

And then similarly on the action side, sweep across 10 by 10

and then look at, you know, coloring each cell by the probability of times that they turned on the air conditioner.

So it's not always just about finding like one kind of specific parameterization, especially with these more intuition pump models, but often it's more about sweeping and then looking at like, oh, you know, there's kind of a, there's a manifold where on this side, this is the case, and on that side, this is the case.


SPEAKER_01:
Thanks a lot.


SPEAKER_05:
Yeah, thank you.

Any other question or chat?

Oh, also just to sort of... Yeah, yeah, go ahead.

Andrew.


SPEAKER_04:
Are you saying this, but the space could potentially be not smooth, right, if there are jagged jumps in terms of response to different probabilities?

Does...

I'm not trying to be too technical, but does that make sense?

It could be a jagged space that has like piecewise squares or something.


SPEAKER_05:
Yeah.

It could be a fully discreet space.

I mean, there, there could be a continuous space also with, with non-linearities, but also it could just be a fully non-linear or I mean a fully discreet space.

Like it could be a checkers game or a chess game.


SPEAKER_04:
Mm-hmm .


SPEAKER_05:
So then those sweeps would have sharp boundaries.


SPEAKER_04:
Okay.


SPEAKER_05:
Yeah, go ahead.


SPEAKER_04:
Well, that just matches that it doesn't have to be nice.

Because I was thinking, why would you do parameter sweeping instead of some type of optimization like...


SPEAKER_05:
there may not be a nice you know smooth space to do optimization on so yeah yeah i mean you've explored that i know in a lot of areas sometimes when you fully like the fully synthetic when you fully get to specify it like we're playing connect four so it's a discrete space there's this discrete action space there's a discrete game play space or tic-tac-toe

then it's like well-behaved and discreet but then when you get into like discretized forms of systems that have continuous character then like then there's all these other issues that could come up

Also just want to make one kind of general comment.

Just chapters one and 10 are like bookends.

It's like introduction and kind of conclusion.

And they're similar chapters.

They contextualize active inference.

Chapter four is where we're going to meet the actual generative model itself.

The generative model is kind of like the transfer station or the midpoint on the high road and the low road.

We're in chapter two, so we're talking about model architecture.

So these are things that you can build.

These are starting from Bayes equation and using Bayesian statistics to build up architectures that have a perception inbound sense-making element and an action outgoing control theoretic element.

the high road is where we'll get to the free energy principle that's kind of like imperative for survival and some of those topics but in the low road it's kind of like tinkering and cobbling with different model architectures and motifs I have a question uh-huh so I'm


SPEAKER_03:
From the transitioning from free energy to expected free energy.

I mean, I guess it makes a lot of sense to actually take the expected value of the free energy, but is there like that derivation wasn't really that clear to me.

It was just like, here's the expected free energy.

Do you have any good resources of where you can actually see that derivation?


SPEAKER_05:
Great question.

Well, one is in Octopus's math work.

I think tomorrow and continuing, it'll be drawn out a little bit.

Also over it, you can follow it, click on the little icon for the A. Here's equation 2.5.

So a few things about the equation.

We have the screenshot.

also there's there's the uh plain latex this can help if you are putting it into like a language model and and we have some of these experiments with like interpreting and asking questions about it not not saying they're all 100 like complete or anything but in the canvas for two point and then also for for many but not all the equations there's the

um natural language like pronunciation basically which could be helpful and so if it's missing people people can add it so that that can um we'll have to fix this because it looks like a lot oh I I know what happened here

There we go.

So these are the lines.

All right.

In 2.5, there's a few resources.

So let's just, how do we get to 2.5 and 2.6?

These are some of the key equations.

Okay, equation 2.5 is variational free energy.

Both VFE and EFE are functionals.

So there are functions that have the arguments that are other functions.

The arguments can include other functions.

So F is the functional.

q is the variational belief distribution q is the distribution that we get to control the form of so it is set by the modeler to have like an analytically tractable form like a gaussian approximation so even if the world is non-gal you know the world's bimodal and then the modeler's saying but i'm using a gaussian so that gives me

nice methods.

Then also all the pathologies, like if the world's bimodal and you're fitting a Gaussian, you could, you know, potentially you fit only one hump, but not the other, or you end up blurring over both and fitting them.

So it's not, not, not that if, you know, if there's a mismatch on form, um,

then this will lead you downhill to not necessarily like the best of all possible outcomes, but given the selection of the form of Q, it will be the best possible outcome.

It's a functional of the belief distribution and one incoming data point.

Why?

So variational free energy is kind of like this real time check on like, how surprised am I with this data point?

Daniel Katz- variational free energy bounds surprise minimizing surprise is equivalent to maximizing model evidence if you were minimally surprised with a new temperature readings coming in.

Daniel Katz- That would be equivalent to saying you had like the maximum likelihood model of the temperatures coming you're making the best possible predictions.

is equivalent to saying that you were minimally surprised.

So this is bounding minimal surprise.

Then it is only dealing with belief in a moment and one data point coming in.

So that's why it's this kind of like real-time sense-making indicator.

2.6 is when we start looking into talking about observations that haven't happened yet, the future, and action planning.

g expected free energy it's taking in pi which is this policy vector it sums to one it's a probability distribution over actions the policy prior is the habit distribution g is going to sharpen that policy prior into a policy posterior which then you could sample from like according to the probabilities or you could just select the likeliest action

Okay, how do we get between the different lines?

Again, in this canvas here for 2.5 and 2.6, there's some useful resources.

Here's equation 2.5 walkthrough.

Ali Ramjoo, Yaqbs McCall, Jonathan Schock have all done, have contributed variously to these sections.

So that's one good PDF.

here i think this was jacob's the interesting thing for 2.5 and this is like again why there's so many dots to connect with the math is 2.5 actually um it's not that one you go to two and two you go to three

Maybe there's a way to do that as well, but here's the derivation going from one to two and then one to three.

So these might be some useful resources.

Also here, let's see if this resource is still there.

Professor Jonathan Schock has written up a sort of math overview.

August 12.

I don't know if that renders to the current date, just or if it was like actually edited this current date.

That also has a lot of information.

so that's something that that takes a lot of uh coats of paint i think but it it really is there and grounded and if we can pull out and help highlight like all those threads and show okay here's where this is grounded in the definition of shannon entropy or here's where this is grounded in the definition of surprise and likelihood that'll be cool

One other thing I'll say is variational free energy is kind of more of a classic quantity.

It's the quantity that's used in variational inference, like variational physics, variational autoencoders, and so on.

Whereas expected free energy is just one choice of a way to model futures.

For example, there's also, this is EFE, expected free energy.

Also there's free energy of the expected future.

So there's several other future oriented approximators that have been proposed, but variational free energy is kind of the classic.

Adam?


SPEAKER_00:
Yeah, like something that I'm wondering about, and I haven't really quite like understood yet, is that, so from,

like my, my own personal past and like where I'm coming from up until active inference, I had been reading quite a bit about like maximum entropy inference in like John Hart from Berkeley and Edward Thomas Janes and, uh, even like Pierre Laplace and, um, trying to think.

So anyways, definitely like ET Janes.

He was really like a proponent of max and inference and, um,

Prior to active inference, I had been thinking about like the stability of systems in the maximum entropy state and the way that if the energy of the free energy equation is constant, then by the increase of entropy, it automatically minimizes the free energy, right?

So just that the min free energy is the equivalent

or not necessarily, but can be equivalent to the maximum entropy.

And so I don't know if anyone else has been thinking about this or maybe if you like also, I don't know, any thoughts on that?


SPEAKER_05:
Yeah, that's a great question.

It has not been fully fleshed out, but here's one kind of recent

discussion where it came up and where they explicitly highlighted this okay so here was it um okay this is the active data sampling paper so observations um it's kind of this is kind of like like building on that discussion of the equation 2.5 vfe so that's where the variational free energy for data point and a prior are going to be described

You could also look at a data point and your prior and describe what was the epistemic value of that data point coming in?

How much did we learn from it?

Here, they're in a clinical study setting.

So the real target uncertainty to resolve is about the efficacy of these different treatments.

They use max-ent sampling, and then they also compare that with the expected information gain-driven sampling.

So there's a very... Okay, here we go.

Several of the examples would yield identical results had we adopted data sampling based upon max-entropy sampling.

the key differences emerge when variance around predicted outcomes is inhomogeneous.

So when there's equal variability profiles, like let's just say that we're interested in the temperature distribution in this room, and all of our sensors have equal sensor variability, then we can sample from the sections that we're least certain about the temperature,

And that's like a max-ent sampling strategy.

However, let's just say that there was regions in the room that had different variability, and there were sensors with different variability.

So just to kind of simplify it, let's just say we had high and low variability sensors, and there's high and low variability areas in the room.

So if we were just going to sample based upon variability, period, measurement variability, like total variability,

then maybe we'd be ending up sampling from sensors that were highly variable, but regions where we actually were not that uncertain about the temperature at that location.

So in the case where the sensor variability we don't have to worry about, then these approaches are equivalent.

but there is actually an additional level of nuance possible so it's it's closely aligned with Max and I think again it would be it would be great to to draw this out better but um it's sampling from a total epistemic value which is able to kind of like

accommodate that case where there's variability in sensors and the variability in the temperatures parts of the room yeah i see that i'll have to look into that sounds interesting cool yeah the slides are there in the paper um and and we that's a interesting thing to keep on exploring

what exactly what exactly um this is just on the website i can get to this yeah yeah um just here's the just like 57.0 that was the the background and then 57.1.2 um carl friston and thomas parr joined i forget which one of the weather it was down one or two but in one of them carl gave a very vociferous defense of epistemic value

Because oftentimes people say, okay, epistemic value learning in service of pragmatic value, right?

Like learning is good for something when it's pragmatic.

And he gives an extended thought on like how they're kind of like separate, but they're unified under the expected free energy.

But like epistemic value is its own distinct kind of value from pragmatic value.

whether or not those learnings contribute to epistemic I mean contribute to pragmatic outcomes no sorry all over my hand okay yeah this the epistemic um like active data sampling like on the biological side it's really interesting because it describes for example isocades

epistemic driven action selection, but then also they explore having a cost function.

So that could be like, how much, how much sense does it cost to make an API call

And then that is the parameterization question is like how much epistemic value, how many bits of information are worth one cent of an API call?

Or in the case of the clinical trial, there's like you could have a cost associated with the number of times that sampling occurs or cost reflected by a preference for survival.

But if the preference is purely for survival and it was like some kind of extreme utility driven clinical trial, then a sampling algorithm would choose to sample nothing.

So it'd be uninformative with zero observations.

However, it would have achieved maximum preference satisfaction because it wouldn't have observed any sickness or death.

So they talk about a few of these kind of like interesting cases.

And they build it up very clearly.

Like how do you add in the cost and all these different things.

And this is like a different graphical model than comes up in the textbook.

Again, it's the clinical trial setting, but to connect it with the textbook, like,

these are bayesian graphical models and so a lot of the the kind of structural analysis of a situation comes down to

are the variables what are like the observable variables what are the environmental variables what are the there's there's not really like an agent like you kind of sense an action agent unless you chose to see like the clinical design teams observations and decisions in that light um what are the observations what are the environmental variables what are their influences upon each other

that lays out like the skeleton.

And then there's the question of either generating synthetic data from that skeleton in the forward direction, or explored in chapter nine of the textbook is like, how do you go from observations to inference on those latent factors?

Like what is the efficacy of the treatment?


SPEAKER_03:
anyone else give it like a thought on chapter two or question or a page or we can look at some of the questions come on let's go for it yeah thanks um yeah i've been thinking a bit about like the distinction between free energy and expected free energy and sort of thinking about like so

like in a way aren't all actions requires planning or like is there actually a clean distinction in practice when we minimize F and when we minimize G or is it like a bit of a fussy line in between and in practice we're only doing the one or if you know what I'm getting at.


SPEAKER_05:
Yeah.

Great question.

So let's go to PyMDP documentation.

This can clarify a few of those things.

One heuristic is if you're considering explicit planning, so more than one time step ahead, like policy sequences that have a time horizon that's not one.

If you want to do explicit planning, tree search, policy rollout, deep active inference, then you do need to use expected free energy.

So if explicit planning is going to come into play,

then you go with expected free energy.

If it's just single time step, then you can just sort of have that reactive updating.

Yeah, hands up.

That's in the equations.

Equation 2.5.

Put it in the chat.

Yeah, so if there's explicit planning, yes, EFE has to come into play.

If you have an agent that is not doing explicit planning,

it could do action selection using VFE so what that would look like and and is like you have um uh belief distribution on the temperature in the room and a belief distribution on your habits related to the air conditioner like I'm 50 likely to turn on the air conditioner or not new observation comes in it instantaneously updates the belief distribution that's the sense making element and it instantaneously

policy distribution, which is then sampled from.

So there's not rollout or anticipation explicitly of future unobserved outcomes, which is what you would get with EFE, but you only get one time step there.

Here in PyMDP,

it's um they really especially in the um documentation make it pretty clear and declarative like there's only two actions here but here even for a single time step moving left and moving right there they are considered with expected free energies because there's actually like a question

expect to see if i went right so that's kind of like you know there's a lot ways to shade that but that's kind of like the teleological aspect of action is when expected free energy comes into play which is like

I went left versus what would I see if I went right versus the sort of past pushing forward without an explicit teleology would be like, I have a 50% chance of left and right prior on policy.

I just saw a light or a glucose gradient from the left that just bumped me to 60, 40.

I sampled from the 60, 40 and I go left, but it wasn't based around what would I see?

So it wasn't like based

of these two alternatives that's the real policy like planning as inference versus more of like a reactive policy just habit unupdated or updated habit distribution and then just drawing from that without explicit consideration of the consequences


SPEAKER_03:
And maybe just a short follow up question.

So yeah, I guess you can make it like really clear when, when writing a program about it, but in the context of, of living organisms, when you, yeah.

So like, what is, what is one time step that could maybe like develop that discussion a bit on like specific for living organisms?


SPEAKER_05:
Yeah.

Well, map and territory, how do we develop continuous and or discrete time models, maps of complex multi-scale systems?

Like there's not really a natural time click for a body per se.

It's not simply like the heartbeat.

It's not the seconds.

It's not the breath.

So then there's a lot of modeler degrees of freedom.

with how they model time.

And then to what extent the modeler chooses to test, here's some models I made where there's planning happening, and here's some models where planning isn't happening.

does anything plan or when does planning happen or like one old active joke was like active inference only works for one time step but then you can compress and make that one time step very complex so that you like it could be one multi like one nested time step that that would

Like if, let's just say that we were talking about like financial decision-making or something like that, and we had a policy decision that was being made at the yearly timescale.

If someone only knew about day-to-day decision-making, that one timescale of the yearly model might look like it had been planned over many days, or there could be a map

just said we have a single policy we make one decision per year but then that model wouldn't be able to make decisions day by day but it might look you know it this just gets into like how i mean and we can make many models from the biological phenomena and then that's kind of the interesting question like is there evidence for planning in isocades or is it simply markovian

And we can totally account for eye saccades based upon location of the gaze at a moment, the incoming observation, and the belief distribution about what would be epistemically valuable.

Or is there a residual that that model is not accounting for that is suggestive of two-step saccade planning?

I don't know.

But those are like the kinds of research pathways where if you are looking for, where is their planning happening?


SPEAKER_04:
Thank you.


SPEAKER_01:
Axel?

Sure.

So I'm pretty new to active inference.

I'm kind of trying to right now get a intuitive feeling for some of the equations, especially the one we were looking at before.

I'm wondering if we could just talk a little bit about how the terms kind of relate to each other and how actually I might be able to send you.

I just kind of cooked up something quickly before here in the chat.

I kind of tried to create a visualization of the

I don't know if you have access to it.

I sent the link.

Yeah.

So basically, let's see here.

Yes, the idea is that you have like the Q of X and the P of X and Y. And as far as I understand, I might have made some mistakes, but as far as I understand, you minimize the free energy.

We're looking at like the top version of the equation, right?

The first line.

So we have the Q of X, which is our belief distribution.

Is that correct?

like our belief of the hidden states yep then we have the p of y and x which is just the probability of y and x happening as far as i understand when p of y and x and q of x are like exactly the same that minimizes free energy is that correct yeah well great work with this this is this is super cool and it'd be awesome to to continue to develop these


SPEAKER_05:
um yeah like what variational optimization does is the modeler has chosen their family of variational distributions that they're going to be sliding the well-behaved knobs on so like we're going to be using a gaussian again that's not saying the world is gaussian it's saying the the modeler is choosing to to do optimization you know descent with this gaussian um

and then the ball rolling downhill on that variational free energy landscape until it rests at the bottom, what is the bottom of that energy well?

It's where that observation coming in, given the belief distribution, is the least surprising possible, or it bounds surprise.

Now, that bound, if the generative process were Gaussian and the generative model is Gaussian, then you'll get, like, then it bounds surprise and kisses it.

If the generative process were something different and you were doing a Gaussian approximation, you still would get the best possible Gaussian approximator, but then there would be still, like, a constant approximation.

that that was like surprised they couldn't be reduced because you were fitting the best possible Gaussian but it still was being um because it was like not the same family as a generative process couldn't reduce all the uncertainty but yeah this is really cool


SPEAKER_01:
This is just very quick.

But I do think it helps to kind of play around with it and get a feeling for how things change.

Because one thing I read in the book was that if you set the mean of both to zero,

um you can see that uh i have to add some kind of quick reset button as well but you can see that as the they say that if you have a little evidence which would mean that your p of x and y would be really really kind of broad right so if you turn the the bottom uh all the way up like the yeah you turn it up then you start getting a free energy that's higher but then actually as you

They say that you should, as far as I understand it, that when you don't have a lot of evidence, you should also be very uncertain.

So if you, I'm seeing the P of X and Y distribution as being broad, as not having a lot of evidence.

And then if you take the Q of X and take the standard deviation up on that, while the, or sorry, the standard deviation

you'll see that if the standard deviation of the P, X, and Y is also very, very broad, it does minimize variational free energy as well.

Does that make sense with what the truth is?


SPEAKER_05:
Yeah.

I'm really interested in this change from a single well

to potentially, I mean, potentially some plateau and then here potentially like a symmetry break where if the first observation coming in were here, maybe it breaks to the right and then continues to follow the gradient and down to here.

And then a future observation couldn't, even if it came from here, it'd be like, okay, now back to here and back.

then it's like trapped in an energy well so it's like what are these situations where even when we have a gaussian prior and a gaussian generative process like what is happening where we're getting a more like a um quartic like kind of two energy well

Free energy distribution.


SPEAKER_01:
So I'm not sure if I'm illustrating it in the wrong way.

And maybe like the blue graph is basically Q of X times the natural logarithm of P of Y and X before it gets kind of summed up into B. So that's kind of what that's showing.

I don't know if that was the same thing you were referring to, but just so you know.


SPEAKER_05:
Yeah, cool.

Maybe.

post the repo that it comes from, but this looks cool.


SPEAKER_01:
So just one quick follow up.

So earlier in the beginning, you were talking about I mean, can I understand it in that way that when you're very certain because they say in the book that if you are if you have little evidence, then you should also not be so certain in your model.

And I'm wondering whether that uncertainty is characterized by having a very broad question, for example, and whether the lack of evidence is also kind of


SPEAKER_05:
do you say represented by having a very broad gaussian if you're using a gaussian yes this kind of relates like you have to the standard deviation um what comes up later is this like learning by counting so let's just say we've observed one heads and one tails we might have um a distribution

but it might be very broad.

Whereas if we had 1 million heads and 1 million tails observed, then we would have very sharp Gaussian around 0.5.

So there's different ways to kind of implement that, but broadly, when you're less confident, when there's more epistemic value on the table, then belief distributions come to look like uniformity.

And then as you gain precision, it gets more of a central tendency.

And then the extreme case is basically a Dirac delta function.

So it's just a spike at one location that might not even be learnable.

And then it's just like, it's here.

And then even if you have a spike at 0.5, then it doesn't matter what you observe because there's no probability mass outside of that one spike.

like discarded whereas the uniform prior um is which is kind of like the frequentest case interestingly is like the the i mean it's the least biased in terms of its sensitivity but then then you realize oh but then the but updating is the bias that is statistical bias


SPEAKER_01:
Well, thanks a lot.

And if anyone else is interested in building these kind of tools or something, please reach out to me because I think it'd be pretty nice to have some more visualizations of how these things play together.

But thanks a lot for the explanation.


SPEAKER_05:
Yeah, it'd be so fun.

For each equation, we could have natural language and translations, pseudocode, some different languages, some interactables.

Like, what are the kind of like different forms that we could interact with?

And also like where and when and how, like in theory and in the specific implementations, you know, the dozens of implementations, like where did the modelers choose to use the energy minus entropy kind of statistical mechanics decomposition of F?

where did they use a complexity minus accuracy kind of statistics basing information criterion type and then where is there like a divergence minus evidence like if they're equivalent then does it matter or like are one of these more computationally intensive to compute or is there

one of these decompositions on F is better.

And then that really gets opened up in the policy selection for expected free energy.

Like we talk the most about epistemic plus pragmatic value.

That's a very interesting decomposition for policy selection.

it's equivalent to these two ambiguity plus risk kind of classic control theory-esque um investing type decompositions and then both of these are equivalent to the again energy minus entropy but then it's like but then are we even talking about planning if we're talking about energy minus entropy or how and then like

What if we had like a Venn diagram?

It's like, here's the total amounts.

Here's the variance that's getting partitioned into epistemic.

Here's the variance that's getting partitioned into pragmatic.

Now, I don't think the same variance is going to ambiguity and risk, nor to energy and entropy, because the terms are not equivalent to epistemic plus pragmatic value.

So it's like, so what loads on pragmatic value, then does that get, does that get loaded onto risk or onto ambiguity?

Does pragmatic value get loaded onto entropy or onto energy?

Again, is there a reason or how did different modelers do it?

The top line for G is the most common.

and the top line for for F is the most common I don't know though but those are like they're they're given as a first line because they're kind of like the classic form but it's it's an interesting topic the rule of four or more octopus yeah what is the rule the rule of four is an idea from teaching math that developed in the 90s that was


SPEAKER_04:
Every concept you look at, you try to have at least four different representations.

Usually they're verbal, numeric, symbolic, and verbal, numeric, symbolic, and visual.

And what you were saying about like, we have all these different representations and then we could add more was just echoed that for me.


SPEAKER_05:
Cool.

yeah and here we have we have there's we have different words in like different natural languages different graphical um symbolic could include executable code and then there could be some some wild cards games and stuff like that but yeah it always pops up it seems maybe i mean for each of the equations we could have a column for like these

And then that creates like, you know, if there's 50 equations, then it's just like, we can just say, okay, here's the empty cells.

Let's try to get something into each of these cells.

Yeah, that's I'll just delete this.

Um,

Yeah, I mean, this is cool and these are great questions.

That's what chapter two, again, we haven't even gotten to active inference.

This is saying, okay, base optimality, probabilistic, probably approximate base, like PAC, probably approximately correct.

Here's the particular partition, splitting systems into these four parts.

nodes minimum of four that's another rule of four more kind of splitting into four two blanket states internal and external getting into what's basically a variational auto encoder here it is framed in the sense making setting and then extending that variational auto encoder into the decision making setting

showing that that expected free energy unifies objective or imperative or kind of like loss function, a lot in common with loss functions, that the special cases of expected free energy fall out as other well-known heuristics.

Like if you only consider pragmatic value, you get expected utility theory.

No epistemic value, nothing to learn, special case of G, expected utility.

In contrast, if all outcomes are equally preferable, so there's no pragmatic value on the table, then epistemic value yields InfoMax.

So it's kind of like saying, if you're gonna compose with these Bayesian Legos, either in the real-time inference, and then also extending out into the planning and the action selection,

like anything any any any brick structures you build on the low road are going to be you're going to be able to approximate them with variational bayesian inference chapter three is gonna come in with with more of this physics flow based model and and say

And for the organism observing itself, or for you observing something else, something will act as if it is bounding its surprise about something else.

So low-road is like saying anything you build, you're going to be able to approximate it and do variational inference with that surprise bounding.

And then here is more of the physics.

saying, and you can model things such that it looks like that's what they're doing.

And then in four, that's where the generative models actually get shown.

And it's like, this was built up on the low road, and it's kind of

compatible from the high road and that's why it's placed at the middle it's just that also four has like again some more mathematical background like jensen's inequality and properties logarithms um variables and their dependencies in graphical models but then figure 4.3 does bring us to the discrete and the continuous time generative models

like many coats of paint like just kind of reading through it because there's not really just one order especially as everyone's gonna have different like questions about different parts um so it's kind of fun like what are the more the more pure background materials like putting this as figure 4.1 might make it seem like this is like finally we got to act in it's figure 4.1 it's like

Well, no, maybe that that's just the generic property of logarithms.

Cool.

Um, well, so, uh, next week for seven, well, uh, it'll be later or

join at this time next week and uh either andrew or i will revisit applying active again there's no there's no specific order so that that can be helpful like chapter seven goes into essentially the top half of figure 4.3 so it may be useful to to see it and um we can go into imdp

Python-based, very imperative, pseudocode-like format for the programming.

Like, get the observation, update this, do that, calculate this, do that.

Or we could explore a little bit towards RxInfer, which has this declarative style, where it's more like you declare the base graph.

Full stop.

Then you declare a kind of like logistical scheduling of how do you update different parts of it?

So that has, there's a few more steps that are possibly initially unintuitive, but then in operation, it gives the RxInfer style of declarative programming a lot more performance and flexibility.

Okay.

Cool, I mean, with anything else in the times between, just make the notes on the chapter pages or just prepare them however.

Oh, I'll copy in Dave's fun note.

Rule of four in Tagalog linguistics.


SPEAKER_04:
They don't write abstracts like they used to.

Okay.

Thank you.

See you all later.
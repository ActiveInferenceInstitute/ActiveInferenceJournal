[
  {
    "start": 0.065,
    "end": 24.385,
    "text": " yeah okay it's recording um so for for joseph for daniel for the rest of the institute who's uh following along remotely uh or via youtube um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 24.668,
    "end": 27.153,
    "text": " Welcome to the Active Inference Institute.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 28.135,
    "end": 40.821,
    "text": "We do a weekly textbook group readings twice on Thursdays, once in the morning, once in the evening, if you're roughly in the Western hemisphere.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 40.801,
    "end": 62.067,
    "text": " and um yeah so so today we're going over chapter seven in cohort seven um and we'll be talking about essentially the exemplar discrete state space models and active inference that would be hidden markov models and uh partially observable markov decision-making processes",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 62.047,
    "end": 73.163,
    "text": " We have a nice tea maze example, which is pretty classic in the field, as well as in behavioral and neuroscience and neuroimaging studies in general.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 73.223,
    "end": 90.147,
    "text": "And again, we're in the second half or part two of the book, which is highly dedicated to constructing essentially active inference models as opposed to part one, which is much more focused on theory and definitions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 90.127,
    "end": 110.566,
    "text": " And so it's for anyone who's interested in computational modeling, whether it be doing it themselves or getting a clearer grasp of kind of what are these computational models or cognitive models that we're putting together in order to, say, collaborate with a team of people who do do cognitive modeling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 110.586,
    "end": 114.069,
    "text": "And it is very worthwhile to spend time with the second half of the book.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 114.109,
    "end": 117.012,
    "text": "And then furthermore, there are just there are many",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 116.992,
    "end": 126.947,
    "text": " discrete state spaces in a very general sense, in a physical sense, in a kind of decision-making and human belief sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 127.267,
    "end": 134.017,
    "text": "It's referring to scenarios where essentially if we think about it in data, it's sort of like categories.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 134.578,
    "end": 135.92,
    "text": "It's thinking about binaries.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 136.841,
    "end": 146.155,
    "text": "It's not so much thinking about fully continuous number series, such as a range over one through a million where we could have so many different categories",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 147.164,
    "end": 173.442,
    "text": " different values including with the decimal points potentially an infinite number of values instead we're looking at say RGB color scales on old televisions so red blue green we could have a distribution between the three of them in the sense of how much red how much blue and how much green but nonetheless it's it's a kind of discrete categorization process so um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 173.742,
    "end": 188.258,
    "text": " yeah normally I give a rather long uh kind of chapter summary but I'm happy to just kind of open up the floor if uh Joseph you have any particular questions or things you'd like to discuss here",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 189.335,
    "end": 214.431,
    "text": " um I I would like to hear your summary um I also do have some questions um I haven't fully been able to understand um for example figure seven two so it's like I I guess I just don't understand what's plotted and and where the lines are coming from but um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 215.66,
    "end": 236.25,
    "text": " yeah I I'm very interested in in trying to build these models and understand how to build them and um and so this seems like a really critical chapter for for me yeah yeah no that's great um yeah figure seven two which I I personally have not spent",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 237.479,
    "end": 258.897,
    "text": " lot of time with recently so coming back to it is rather interesting yeah so the the this is where the chapter it essentially opens with hidden markov models uh before we jump up to uh partially observable markov decision-making processes so it's like we have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 258.877,
    "end": 270.777,
    "text": " um I believe my is my screen still being shared appropriately or can you not see anything uh no I don't see your screen okay got it thanks for that I just realized I doing a little bit of uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 272.174,
    "end": 301.27,
    "text": " display gymnastics um that should have done it so this is and a quick reference this is the coda for the textbook group and so we have this available to all members and we have uh not just a overview of the textbook group and you know dedicated uh meetings and and time availability and all those sorts of things for the separate cohorts we also have a pretty good breakdown of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 301.25,
    "end": 305.698,
    "text": " the active inference textbook, especially since it became open source.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 306.8,
    "end": 317.138,
    "text": "So see figure seven one, I know it's not the one you referenced, but just to make sure we're familiar with it enough, this is essentially a hidden Markov model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 318.0,
    "end": 324.912,
    "text": "And so understanding kind of the core dynamics here, we ultimately only have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 324.892,
    "end": 341.297,
    "text": " three different components in this model so we have D our prior over initial states it's kind of like what are our beliefs about states whenever we come into a situation as it were um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 341.868,
    "end": 370.153,
    "text": " and then from there uh that gets linked to uh observations that we receive right and the relationship between what we believe and sort of what we see is linked through this this likelihood matrix a so it's represented as kind of this broad categorical probability distribution that is conditional uh observations given our beliefs s or in states s",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 370.133,
    "end": 393.728,
    "text": " and then there's also a temporal dynamic over time right so we have the b matrix which is kind of our beliefs about how transitions change from one to another so that's why we have this sort of state at t minus one to state of t to state t plus one so it's this this temporal dynamic playing out over discrete time steps",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 394.13,
    "end": 400.001,
    "text": " And so from there, you can move to 7.2, a little bit of context.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 400.061,
    "end": 409.438,
    "text": "So this is an example that relates to the idea, I believe it's of someone who's playing an instrument.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 409.418,
    "end": 416.728,
    "text": " And they're trying to actually play a song in accord with the script, the sheet music that they're reading.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 416.768,
    "end": 422.977,
    "text": "And so we can quickly imagine a scenario where there's someone who is still learning how to play.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 423.017,
    "end": 424.76,
    "text": "This is practice for them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 424.82,
    "end": 433.452,
    "text": "Even if it's someone who is very skilled, they're maybe learning a new piece for like, I don't know, an orchestra performance they'll be doing soon.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 433.432,
    "end": 456.115,
    "text": " and so there's all kinds of room for error right and to to accidentally hit the wrong notes at the wrong time maybe you're still on beat but uh but you know you hit an a rather than an a flat and so that's sort of what this this example is about is being able to use the dynamics of the hidden markov model including what notes play over time",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 456.095,
    "end": 485.275,
    "text": " and then also what does the musician believe the note should be versus what we we actually hear um right and so so by being able to like hear and it's obviously a very kind of abstracted example there's a lot more to it there we're not saying anything about what the sheet music looks like but here we're just simplifying it to saying the musician has a belief about what the note should be versus what they actually observe that they're they're playing so",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 486.723,
    "end": 500.104,
    "text": " With this figure 7.2, I think the quickest way of kind of getting at what the authors of the textbook are trying to demonstrate for us is sort of an illustrative example is this upper right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 500.164,
    "end": 503.749,
    "text": "This is like beliefs about hidden states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 503.95,
    "end": 507.415,
    "text": "So the agent believes that the first note",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 507.395,
    "end": 535.225,
    "text": " uh in terms of over time should in fact be the first note as in it's in row one so that this row is like its own set of notes row two is uh yet another note so it could be read as like the top row is a the second row is a sharp the next row is b the next row is c not to confuse anyone who's not familiar with scales but we there is no b sharp essentially unless you're concerned with",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 535.205,
    "end": 558.597,
    "text": " uh Naturals and all that so um so each row is its own note and then I hear it's kind of like the the musician believes based on the sheet music that they should start with the first note A and then move to A sharp move to B move to C and then go back to A for the fifth note so they're just kind of descending a chromatic scale before they get back to A again",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 558.577,
    "end": 560.121,
    "text": " That's what they believe should happen.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 560.743,
    "end": 574.138,
    "text": "Meanwhile, with our O's or observations, what really happens is that we get an A, we get an A sharp, and then rather than getting the expected B, which is here,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 574.118,
    "end": 604.058,
    "text": " we that the actual observation is that they realize oh no they hit the same note twice in a row which is not correct but by the fourth time step they get back to what the note should be and then here uh at time step five they also hit the correct note so kind of what's what's happening here is that there is at s3 we would say that this is prediction error right and so with whenever the agent infers their posteriors over hidden states",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 604.038,
    "end": 611.266,
    "text": " Here, they're so conditioned to believe that it should be the right note that they kind of shaded this in as if it's correct.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 611.966,
    "end": 613.428,
    "text": "But there's this grayness here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 613.868,
    "end": 615.39,
    "text": "It's so light.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 616.231,
    "end": 621.176,
    "text": "But essentially, the idea is this is the note that should have happened there prior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 621.236,
    "end": 627.883,
    "text": "Their observation, however, gives them evidence that it's actually this gray space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 627.923,
    "end": 632.188,
    "text": "This is maybe actually the note that they heard.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 632.489,
    "end": 633.951,
    "text": " or that they believe really happened.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 634.672,
    "end": 643.923,
    "text": "So there are aspects of this that can seem kind of counterintuitive, but really just being able to grasp that the upper right are their beliefs about states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 645.085,
    "end": 649.35,
    "text": "The lower right is the observations they actually receive.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 650.091,
    "end": 655.678,
    "text": "And then a good reminder is, you know, in a discrete state space setting, it's like, oh, we're not looking at",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 656.148,
    "end": 675.49,
    "text": " you know the interim between a and a sharp you know some kind of microtonal situation we're just looking at uh we're assuming there's a and then there's a sharp and then there's B and there's C and ultimately there that's the Western scale of 12 notes um in the way that I'm describing an example so",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 675.47,
    "end": 701.882,
    "text": " now these these graphs on the left are a little more complex um and it will be good to spend a little bit of time with them i'll first just like read what it says in the textbook so so we'll go with lower left we're looking at negative free energy gradients or prediction errors the rate of change of the beliefs in the upper left plot",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 703.55,
    "end": 707.857,
    "text": " is determined by the value of these errors at each time step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 708.297,
    "end": 711.662,
    "text": "And so what we're seeing, these are negative free energy gradients.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 712.123,
    "end": 715.608,
    "text": "In terms of active inference, we're wanting to minimize prediction error.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 715.629,
    "end": 718.613,
    "text": "We're wanting to minimize variational free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 718.673,
    "end": 728.408,
    "text": "So if these are actually negative gradients, then that means that at each time step, we should actually want these to be rather large if they're negative.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 728.648,
    "end": 732.274,
    "text": "And so the smaller they are,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 732.254,
    "end": 747.04,
    "text": " it's kind of like saying that it's sort of the the worser scenario and then that seems to kind of constrain the agent's ability to to be confident or or kind of understand the the situation",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 747.29,
    "end": 758.123,
    "text": " So upper left, beliefs, posterior probabilities about each note in the sequence at each time step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 758.143,
    "end": 761.046,
    "text": "Yeah, so these are the posterior probabilities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 761.086,
    "end": 770.017,
    "text": "It's kind of like their ability to understand a particular note here of what it should have been has gone down, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 770.037,
    "end": 772.821,
    "text": "So there's a kind of constraint there that's been applied.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 773.101,
    "end": 776.605,
    "text": "And then once they start to hear the correct notes again, their confidence kind of,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 776.585,
    "end": 777.787,
    "text": " It kind of increases.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 778.549,
    "end": 779.17,
    "text": "I will be honest.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 779.19,
    "end": 787.728,
    "text": "These kinds of these two graphs on the left, they're much more rarely used today in active inference like these specific types.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 788.149,
    "end": 792.838,
    "text": "It might be because of the sort of kind of counterintuitive nature.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 792.818,
    "end": 819.4,
    "text": " because more often than not we actually are looking at you know the inverse of the situation so we're looking at positive free energy and so actually this would look quite bad it would look like step three is actually you know a rather good thing in terms of minimizing uncertainty and it's the other way around but i think that the authors who who compose this are really trying to get at a particular kind of point by representing it this way um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 819.7,
    "end": 823.865,
    "text": " I think I'm still maybe a little lost about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 825.707,
    "end": 833.617,
    "text": "So you said that, just focusing on this top left, the y-axis you said is probability.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 834.258,
    "end": 836.701,
    "text": "OK, so I get that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 837.462,
    "end": 839.525,
    "text": "And what are all the different lines?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 840.185,
    "end": 849.657,
    "text": "Are these different trials or?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 853.332,
    "end": 874.024,
    "text": " One of the things I'm really wanting to understand better is how the agent actually is learning, and maybe that comes later on after this example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 877.058,
    "end": 887.912,
    "text": " Where do we adjust the weights in these matrices according to the result we get from our minimization step?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 888.415,
    "end": 911.723,
    "text": " oh yeah no that's an excellent question very relevant so so in this example they don't actually apply learning at all so this is essentially like a static model yeah um and I think they're just using it to try and kind of get at the more inference level Dynamics as opposed to learning and so I I know that this is you know this is pretty familiar terminology for folks who",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 912.732,
    "end": 917.057,
    "text": " do other forms of kind of computational modeling, data science, deep learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 917.077,
    "end": 922.544,
    "text": "I heard you say that referring to it as adjusting the weights, all of that is very relevant here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 923.285,
    "end": 926.569,
    "text": "So for this particular model, yeah, there is no learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 926.949,
    "end": 941.507,
    "text": "What could happen is that, refer to this again, typically what an agent learns are these kind of probability distributions",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 941.892,
    "end": 954.433,
    "text": " over time and so Dean you know your your prior beliefs in the Bayesian sense those could be updated so you could have the agent could effectively learn its D matrix um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 955.223,
    "end": 979.24,
    "text": " a could be learned b could be learned actually any of its parameters including c and e which c is just a prior over observations e is a prior over over policies or actions um so so that way you know that's what's missing from this and it's also missing from this because we have no policy inference right so the hmm is kind of like can be viewed as a truncated",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 980.587,
    "end": 1008.687,
    "text": " version of the pomdp which is exactly equivalent until we get to here this connection to to pi or representing policies or action and so so an hmm is kind of like perception and no action uh partially observable markup decision making process is perception and here we have the connection to actions that it can take",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1009.088,
    "end": 1032.109,
    "text": " so we get these these kind of relationships where suddenly state transitions the b matrix recall earlier 7.2 the b is simply what is the next state going to be given the current state whereas here we now have um yeah we now have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1032.865,
    "end": 1037.596,
    "text": " what is the next state going to be given the current state as well as an action I take.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1038.278,
    "end": 1047.098,
    "text": "So that is sort of the kind of the agency, so to speak, that this agent has and its ability to kind of infer which actions to take.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1047.539,
    "end": 1049.203,
    "text": "And furthermore,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1049.909,
    "end": 1052.052,
    "text": " E. The prior over policies.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1052.073,
    "end": 1053.455,
    "text": "I mean, so that can be learned.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1053.695,
    "end": 1054.757,
    "text": "That gets incorporated.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1055.538,
    "end": 1058.062,
    "text": "C. Preferences, priors over observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1058.523,
    "end": 1060.606,
    "text": "Those are more often left static.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1061.248,
    "end": 1067.698,
    "text": "So those are more often left static in the sense that they become a kind of attracting point for particular observations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1067.932,
    "end": 1087.155,
    "text": " uh in that case that kind of supplies the agent with a sort of goal um it kind of like it's seeking out particular observations that it it prefers or expects um in the sort of theoretical literature the idea is that it relates to the notion of homeostasis so I prefer to observe my",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1087.135,
    "end": 1116.568,
    "text": " blood oxygen level being at a certain point or I prefer to not observe my stomach growling because usually it's a sign that I'm hungry and I need to do something about that which in a that context it would that that C would then inform what I should do via the B matrix of oh my stomach growling must mean I'm hungry and I know that if I eat food I will say I will no longer be hungry at the next time step so to speak um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1116.548,
    "end": 1132.718,
    "text": " so so again any of these can be learned and i know that there's a nice figure that kind of hits at the point of this but i don't want to get us too lost in the weeds um hybrid models",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1134.115,
    "end": 1155.094,
    "text": " that essentially what happens is that we end up with this kind of hyper parameterization of all of those model components okay I'll stop I'll stop scrolling around because it might be that we missed that one still going to renew um so you would have like a prior on D",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1155.361,
    "end": 1158.93,
    "text": " which itself is in the discrete state space setting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1159.171,
    "end": 1165.086,
    "text": "So it would be, it would just be like a kind of, so it'd be a Dirichlet distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1165.507,
    "end": 1170.72,
    "text": "So it would be like a categorical distribution, but the values can in fact exceed",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1170.7,
    "end": 1192.472,
    "text": " one they don't have to sum to one uh you could almost think of it as a kind of like dearest lake count system where it's like oh i i you know i have my d matrix but then i've also experienced like more of this state in many more time steps so it implies that the agent actually has a history",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1192.452,
    "end": 1212.317,
    "text": " of states and a history of observations that it's kind of accumulated and then that acts as a kind of hyper parameter on these matrices which then proceeds to modulate their values and then learning them itself will involve free energy minimization i mean it's just applying another free energy minimization rule",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1212.297,
    "end": 1232.061,
    "text": " um and so and so like minimizing free energy is sort of the global uh functional or optimization problem here for just about everything like state like inferring states occurs by minimizing variational free energy inferring policies",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1232.277,
    "end": 1254.657,
    "text": " heavily relies on minimizing expected free energy which earlier on in the textbook I think it's somewhere in the realm of chapters two through four there's kind of a broad breakdown about those terms so I'm sure that we should be able to find those we have the equations as well yeah great",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1256.543,
    "end": 1268.005,
    "text": " So something will notice very quickly between F and G. F is going to be for again state inference, which ends up being treated as a separate process related.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1268.766,
    "end": 1276.982,
    "text": "But if you were to code this, like if you were to write this in code, you would actually write like separate logics and even executions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1276.962,
    "end": 1303.746,
    "text": " of state inference versus policy inference state inference will involve minimizing f which without getting too lost in the various ways we can break this down we could look at it as complexity minus accuracy which is the sort of classic you know machine learning imperative of how do you create a model that can both be generalizable and not excessively complex and overfit",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1303.962,
    "end": 1309.23,
    "text": " but we need it to be accurate, so we don't want it to heavily underfit either.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1309.29,
    "end": 1320.749,
    "text": "So the complexity term there is just what is the KL divergence between your posteriors, what you think now, versus your priors, what you thought before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1320.789,
    "end": 1324.114,
    "text": "And then you want to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1324.533,
    "end": 1350.793,
    "text": " also keep accuracy high so we want to be able to find the expected value of the log of essentially observations given x so so all this is to say each one of these terms depending on what the sign is in front of them um we'll we'll want to either maximize or minimize so we'll want to maximize accuracy right because it has a negative sign in front of it",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1350.773,
    "end": 1379.228,
    "text": " vice versa for complexity we actually want to minimize that because at the end of the day we come back to f we're just trying to minimize that um similarly for g but the interesting thing with g is that whenever we look at just the top line information gain and pragmatic value we notice that there are negative signs in front of both of them and we're trying to minimize g",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1379.478,
    "end": 1398.482,
    "text": " So we actually here want to maximize information gain, meaning the expected value of the KL divergence between your posteriors, what you think now, conditioned on what you've seen and what you've done.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1398.85,
    "end": 1428.857,
    "text": " and the the divergence between that and just what you think now versus uh are conditioned on what you have done so this is kind of like saying oh now that I have actually observed the outcomes of what I have done and what I believe now versus simply just relying on you know sort of what I've done it it kind of prioritizes this idea of oh I need to collect more observations and I actually want those observations to be quite different",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1428.837,
    "end": 1449.512,
    "text": " from what i i'm used to seeing before what i'm so used to that i can practically sort of marginalize them out right so that that's sort of the exploratory behavior we see in active inference and furthermore just the fact that we have this functional that actually allows for both exploratory behavior",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1449.492,
    "end": 1471.5,
    "text": " and pragmatic value which is simply your observations conditioned on c so like i mentioned earlier c it's often used for defining like goals that the agent has it's kind of like what you expect is what you want which is what will help you maintain homeostasis or it's what will help you achieve that goal that you have in the far future or however you want to look at it",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1471.48,
    "end": 1487.447,
    "text": " um or goal in the immediate future and so you want your observations to essentially like you you want what you see to actually be accurate versus like your priors over observations right so that that's that's kind of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1487.984,
    "end": 1508.666,
    "text": " I don't know if you're familiar with reinforcement learning, but anything like epsilon greedy behavior or having to define manual rules such as like, oh, you should always exploit with a probability of 0.9 and 10% of the time, just do something random and see what happens.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1508.706,
    "end": 1511.089,
    "text": "Instead, this is...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1511.069,
    "end": 1540.452,
    "text": " what it defines exploratory behavior is directly implicated in computing minimizing expected free energy so that that's the claim in the textbook that these sort of naturally balance out which is not a sentence that really should be said that way in the sense of you could still have a scenario where a policy is evaluated such that it might not have any information gain at all it's just the fact that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1541.326,
    "end": 1542.329,
    "text": " Both are computed.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1542.369,
    "end": 1545.299,
    "text": "And so there is no manual definition here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1545.319,
    "end": 1549.513,
    "text": "We're just relying upon minimizing G. And these are particular equations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1549.831,
    "end": 1555.237,
    "text": " I might have gone much more deeply into that, and I hope I didn't miss your original question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1555.317,
    "end": 1556.198,
    "text": "No, I really appreciated that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1556.238,
    "end": 1565.989,
    "text": "I liked your explanation about the meaning of those different distributions that you're measuring the KL divergence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1567.691,
    "end": 1573.678,
    "text": "That was very helpful, actually, especially when you were talking about the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1574.215,
    "end": 1588.29,
    "text": " information gain and the fact that, you know, one is conditioned on why hat or y tilde and one is not I had, I just sort of not connected those dots before.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1589.513,
    "end": 1591.819,
    "text": "So, Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1594.111,
    "end": 1595.294,
    "text": " Yeah, no problem at all.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1595.775,
    "end": 1600.888,
    "text": "Can you say again, I think you said it already, but can you say again the difference between P and Q?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1601.028,
    "end": 1611.715,
    "text": "Is this related to the world model versus the agent's model, like the truth, the true world versus the agent's, what's in the agent's head, or is it something else?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1612.353,
    "end": 1638.782,
    "text": " yeah sure no good question so um if we were to kind of use the phrase is it just in the agent's head or the world it's all of this would be in the agent's head okay so yeah yeah so and then uh in the sense that like this is the agent's beliefs about states this is the these are the this is kind of the breakdown of the overall rule",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1638.762,
    "end": 1658.728,
    "text": " of computing free energy over state so states as in my my beliefs about the world and then g will be used for inferring what policy should i use so that's as opposed to state inference it's policy inference right okay so the and and all of it is in the in the agent's head as it were",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1658.708,
    "end": 1663.753,
    "text": " And then for Q and P, so Q is always used.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1664.093,
    "end": 1667.296,
    "text": "They always use that to denote posteriors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1667.937,
    "end": 1677.826,
    "text": "So this is sort of like, what do I think after a given instance versus what did I think before?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1679.067,
    "end": 1685.593,
    "text": "So there's this kind of model inversion process that happens where we compute F.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1686.198,
    "end": 1713.522,
    "text": " by sort of figure sorry about this figuring out our posteriors figuring out our relative to our priors and that's what allows us to kind of score the final like what is our final belief now that that we have all of that and we have now minimized the free energy right so just basically employing Bayesian inference but in the context of a fuller functional equation",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1718.26,
    "end": 1736.134,
    "text": " yeah so so you know there are a lot of interesting ways that these terms get broken down and i know that like for for a lot of folks who get involved with the institute and their learners uh especially those who are not terribly uh you know comfortable with math or",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1736.57,
    "end": 1763.421,
    "text": " for probability theory it's sort of like these things can be very intimidating right to just like quickly see like you know let us tell you all at once all these different ways you can break down G whenever you're still trying to learn what this little C is over here and what how what that means relative to anything else right and what's the tilde about right so and I'll also I guess I'll mention that the tilde is uh so G",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1763.924,
    "end": 1789.662,
    "text": " what you're doing here with G and the whole G over Pi is you're you're inferring your beliefs about each policy you have available over some inference Horizon which is why we call it expected free energy it's that it also is variational in the sense of like they're relying on different kinds of you know approximate methods variational methods here",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1789.642,
    "end": 1806.639,
    "text": " But the expected part is emphasized because if I notice I'm hungry and I'm trying to decide what to do, it's like, well, if I decide one policy I have is that I need food, there's going to be a series of steps that I'm going to need to take.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1806.659,
    "end": 1808.02,
    "text": "Do I have food in my fridge?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1808.12,
    "end": 1808.901,
    "text": "Is it at the store?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1809.482,
    "end": 1816.729,
    "text": "If I decide going to the store is what I want to do, then what are the intermediary steps of what I need to do from between now and then?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1816.709,
    "end": 1837.474,
    "text": " Um, surely I don't think about every single thing, like the entire process of what it will be like for me to walk down my car apartment complexes hallway, but more discreetly, I know that I will need to say, stand up and then I'll need to leave my apartment and then I'll need to travel or take transit or otherwise to the store and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1837.514,
    "end": 1839.156,
    "text": "So, so it's kind of like.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1839.136,
    "end": 1863.439,
    "text": " the tilde is just acknowledging that this is a sequence so what are the kind of expectations I have over what will happen if I follow a particular policy and how I think it will occur so um which is why we also don't see the tilde up here because this is more about moment to moment inference",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1863.622,
    "end": 1887.132,
    "text": " which again relates back to earlier the point about learning right because it that's that's an important distinction between the two and active inferences uh state inference is just about you know in real time you know you think about again the the musician playing notes in real time and and it's very directly related to their ability to hear the note that they're playing versus what they think it is",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1887.112,
    "end": 1913.288,
    "text": " um so that's that's one way of kind of just more conceptually conceiving of uh of these things so okay and and so I guess um especially with G it seems to me that the reason you would choose one breakdown over the other maybe with not just G but all of them but um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1913.572,
    "end": 1942.954,
    "text": " is because you're you're trying to figure out what's actually computable like based on the computing resources you have maybe it's a lot easier to compute um that fourth line there with the expected energy and entropy than the other options and in terms of uh what's actually solvable um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1944.233,
    "end": 1946.181,
    "text": " But that's possible.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1946.201,
    "end": 1950.658,
    "text": "I mean, it's never made very explicit why we have all of these different breakdowns.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1951.241,
    "end": 1953.349,
    "text": "The crucial thing is that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1953.532,
    "end": 1982.02,
    "text": " like the way the math plays out it's necessarily the case that this first line like one version of g is information gain minus pragmatic negative information gain minus pragmatic values g it's necessarily going to be equivalent to the value here um given the equation right and so it's sort of like now another sort of tricky thing is that the the this is sort of uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1982.0,
    "end": 1984.705,
    "text": " they will be less than or equal to these other terms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1985.466,
    "end": 2002.357,
    "text": "So this allows for almost, I don't want to get too abstract and mix things together, but it's almost like this is an additional, like this is the lower, excuse me, the upper bound of the upper bound on like expected future surprisal, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2002.377,
    "end": 2003.299,
    "text": "It's because...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2003.38,
    "end": 2030.451,
    "text": " we already know that um F from definitions of surprise like F is just an upper bound on surprise that works out mathematically and so it's kind of like saying well these are the upper bounds on surprise expected in the future and these are kind of the upper bound to the upper bound because of this sneaky uh less than or equal to sign here um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2030.802,
    "end": 2044.522,
    "text": " So so another thing is that it's sort of like the another aspect of this beyond that is the authors try to supply their own definitions of the different ways of breaking these down.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2045.343,
    "end": 2048.608,
    "text": "And so expected free energy and just about every.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2049.297,
    "end": 2074.056,
    "text": " and it's a broad generalization but I would I genuinely think maybe 80 to 90 of all literature you read in active inference most people are just going to focus on describing it this way information gain and pragmatic value because it comes off as the most intuitive and it's also the most interesting in terms of viewing things uh and like explore exploit paradigms it's just this is the clearest way of kind of um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2074.964,
    "end": 2102.727,
    "text": " kind of expressing that and so it's it's just intuitive like oh yes I want more information that actually increases like what what I thought before versus what I know now it's almost like I'm seeking out I hesitate to use the word learning because of its its technical connotation here but sort of like I want to learn more or I want to find more you know and so sort of a",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2102.825,
    "end": 2109.835,
    "text": " someone who wants to have fun doing something new for the sake of it, you know, I sort of think about that as a more colloquial way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2109.855,
    "end": 2116.725,
    "text": "And then pragmatic value is just, you know, is what I'm seeing or what I expect to see, is it going to be aligned with my priors?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2117.326,
    "end": 2128.883,
    "text": "So is this particular action, this particular policy I'm evaluating, like going to the store to get food, is that going to help me",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2129.842,
    "end": 2146.888,
    "text": " achieve observations that match what I want which is attaining food and kind of watching and myself eat it and then watching my stomach no longer growl is a kind of odd way of saying it but it's closer to the the ideal of how it would work",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2146.868,
    "end": 2167.307,
    "text": " yeah um I I think part of the the break the breakdowns here is just sort of it's taking into account so like we've expected ambiguity and these other terms uh some of them additionally get introduced in in chapter seven as well um posterior predictive entropy",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2167.287,
    "end": 2197.294,
    "text": " those kinds of terms it's kind of like just saying like oh it's it's sort of like this this idea that you know there's a lot of other dynamics of how we could view these plane now so expand expected ambiguity is like um oh that that for example is sort of what is the entropy of what I expect to see versus what I expect will be going on so um you know if if if I expect that the it will be light outside",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2197.612,
    "end": 2206.676,
    "text": " Because I believe it's daytime and I expect to see that it's dark outside because I believe that it is nighttime.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2207.337,
    "end": 2210.706,
    "text": "Then it's sort of like we have this nice one-to-one discrete mapping.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2210.826,
    "end": 2212.15,
    "text": "It's light out, it must be day.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2212.591,
    "end": 2214.135,
    "text": "It's dark out, it must be night.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2214.605,
    "end": 2231.027,
    "text": " um however if uh if we if we expect more of a one to mini mapping such as uh oh it's light outside it could be day or there or excuse me it's dark outside it could be night or it could be a solar eclipse right now",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2231.007,
    "end": 2258.196,
    "text": " right and I'm just not sure which I didn't pay attention to the news I don't know like what's going on right so um that's suddenly where you get more of a one to mini mapping between like what could this particular observation name um and here it's a it's a positive value so we'd actually want to like minimize that kind of that entropy of that situation that's the expected ambiguity um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2258.817,
    "end": 2261.48,
    "text": " And then risk over outcomes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2262.081,
    "end": 2271.193,
    "text": "So these risk terms that get related to, because there are a lot of people who go into this field, not just because they're interested in, say, neuroscience, but they might be interested in other fields.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2271.353,
    "end": 2279.964,
    "text": "I myself have a background and I did my master's at UChicago in this very broad social sciences program.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2280.004,
    "end": 2282.968,
    "text": "That's part of what brought me here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2283.269,
    "end": 2312.854,
    "text": " uh including an interest in economics so looking at risk so this is like your posteriors versus your priors like what what do you believe right now about what you would expect to see versus versus pi versus like what what do you believe after right so what what do i expect to kind of see from what i do versus what i i'm used to sort of um and finding the kl divergence between those",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2312.834,
    "end": 2340.765,
    "text": " right and you want to minimize that which is which is a little different so it's just there's so many ways to break it down and so many different ways to attempt to put into words why these equations work the way they do that's sort of the the trick isn't it of not just active inference but sort of cognitive and computational modeling in general like well if we have real data and we have real",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2341.64,
    "end": 2368.103,
    "text": " approximate methods of mapping things like beliefs and actions and inference learning processes what how do we put into natural language what any of those things mean so it's uh I think it's a fair attempt and I I personally like many other people kind of prefer this top one but it is interesting breaking the rest down and then it's it's kind of like the further you go down the line here we get closer and closer to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2368.994,
    "end": 2396.267,
    "text": " theoretical physics which is yet another kind of partner field of active inference and a lot of uh Carl first and other folks's um research right so just having energy terms that yeah my my background is in physics and I think that's a large part of why this um appeal to me is a I'm I'm",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2396.551,
    "end": 2398.633,
    "text": " working on a robotics company.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2398.653,
    "end": 2404.239,
    "text": "So that's how I came into this and found it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2404.379,
    "end": 2408.984,
    "text": "And why I think it attracted me was the analogy to physics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2409.165,
    "end": 2426.423,
    "text": "And I'm hoping to figure out some ways to reduce the amount of data that it requires for an agent to learn a task or to decide to do a sequence of motions or something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2428.242,
    "end": 2458.215,
    "text": " yeah no absolutely i mean it's uh i don't know if you skipped ahead to um chapter 10 in this textbook at any point but i think they spent some time on on robotics and and you know the the more theoretical aspects of like cybernetic processes and sort of feedback loop operations but then like suddenly with active inference and this very broad interdisciplinary aspect it's sort of like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2458.28,
    "end": 2462.285,
    "text": " because it can go so many ways because it comes out of neuroscience.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2462.305,
    "end": 2479.184,
    "text": "I mean, one way one kind of funny takeaway and maybe it's not funny, but I think it's kind of fun is like a lot of this relates to the brain and inference processes as well as looking at behavior like the teammates examples that we get in this textbook.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2479.845,
    "end": 2484.61,
    "text": "So, you know, like similarly to actually having a real mouse in a lab.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2485.051,
    "end": 2486.953,
    "text": "But then if",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2486.933,
    "end": 2514.088,
    "text": " computational psychiatry plays into this a lot and it's actually like it's a growing field and many people are trying to cognitively model and i'm working with a research group here as well uh but we're working on a on creating an active inference model of ptsd actually and uh sort of what are the behavioral uh dynamics there and and sort of the belief dynamics and can you find",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2514.068,
    "end": 2535.84,
    "text": " a a a generalizable but still approximate uh sort of model of these these dynamics in relation to neurobiology and then in future like being able to kind of fit that model to real empirical data that could be collected from you know willing participants uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2535.82,
    "end": 2548.8,
    "text": " who have ptsd um and and all this is to say the reason why i'm kind of going on about it despite the fact that you we started with robotics is that it kind of in an interesting way it's almost like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2548.882,
    "end": 2574.923,
    "text": " like is a bad machine learning model or a reinforcement learning model like if it's bad should we view that as a kind of like like a pathology like can we view suboptimal behavior as sort of something like pathological behavior so I I just think that's a that's a fascinating kind of take and I've talked to a few people who are more focused on sort of sort of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2574.903,
    "end": 2602.783,
    "text": " optimizing models rather than you know they're not worried about attending to human beings and modeling that they're wanting to create very efficient you know kind of engineering based you know tools and and and so on uh right and we've always had interesting conversations about that because because an active inference it's like there's a term that's used sometimes um the the term is a hyper precise priors",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2603.101,
    "end": 2629.362,
    "text": " so that that would kind of be like a scenario where to bring it back a little bit it could be for example a situation where because you see figures",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2633.847,
    "end": 2639.342,
    "text": " So imagine our musician example again.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2639.362,
    "end": 2642.09,
    "text": "We don't even have to worry about policies just yet.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2642.953,
    "end": 2649.19,
    "text": "Imagine this agent had a very precise",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2649.17,
    "end": 2672.511,
    "text": " hyper parameter like on their a like a very strong prior for their a matrix and for some reason they're just learning it at either an incredibly slow rate or not at all um and say the a matrix was really biased so like with it with the this example like say the agent was heavily biased",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2672.491,
    "end": 2691.36,
    "text": " to assuming that like if they're both like they're these beliefs are necessarily like one to one with with these to where even this kind of light gray space didn't appear so to speak it's like they wouldn't track the prediction error because they believe in the bayesian sense",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2691.34,
    "end": 2715.458,
    "text": " so strongly that this is in fact how things work you know perceptually given that the HMM is just like a perceptual model then it's kind of like saying like oh that's you know it it in in robotics that I mean that would be awful that means it's not able to like read the data streams in a way that allows it to make any kind of like flexible adjustable inferences instead it's",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2715.775,
    "end": 2738.475,
    "text": " it's going to just assume whatever it sees regardless of what it is just reconfirms like its beliefs about you know what's going on and if this were a pomdp then also what it should do in response to the data stream it's receiving but uh simultaneously the pathology bit is sort of like oh that would for a musician that would mean that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2738.455,
    "end": 2754.54,
    "text": " they believe they're really good at music all the time right because they're they're sorely convinced that their beliefs about what should be heard or what should be played or what's going on uh necessarily match the incoming",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2754.79,
    "end": 2780.612,
    "text": " data stream and so the incoming data stream will just kind of like it won't the variations in it won't really matter it's like it will still be you won't even hear that they made a mistake essentially they'll believe that it's right exactly and so not to you know not to sound in you know insensitive or or phrase this wrongly but it's i mean if you start thinking about things like schizophrenia or psychosis where",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2780.592,
    "end": 2807.798,
    "text": " something in reality is not going on at all but the person believes it is it's like in a certain way it's their their perceiving reality in a way that's heavily biased by these kind of priors that they're coming to the situation with right so you're sort of stuck in that uh if so long as the your your inference process is kind of you know kind of a lot pathological in that way so um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2808.706,
    "end": 2815.394,
    "text": " So yeah, it's, I mean, with robotics, I'm not, that's not my, my key field as it were.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2815.434,
    "end": 2823.844,
    "text": "So I can't say too much other than anytime I've skimmed it, there's always something being something interested being said.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2824.044,
    "end": 2832.013,
    "text": "And that, you know, with a lot of these models and various areas, um, they're highly performative, including.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2832.584,
    "end": 2852.567,
    "text": " in when comparing them let's say to reinforcement learning because it's another way of looking at active inference especially especially pomdps or any any model that is actually augmented with the capacity for action we're effectively just looking at okay here's an agent",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2852.547,
    "end": 2874.657,
    "text": " who receives observations and infers an action it should take um if we just look at it as a markup decision making process like remove the partially observable part then it would just kind of be like they are it's as if they already know what the state is they observe a state it's sort of more one-to-one we don't have this extra",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2874.637,
    "end": 2894.89,
    "text": " observation versus States part right but uh but but nonetheless I mean this is still the canonical bit because I mean this is as far as kind of the neuroscience picks go I mean we're always in a partially observable environment right so we don't always know what an observation means and",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2894.87,
    "end": 2911.883,
    "text": " We haven't even said anything about the possibility that the sensory receptor itself is bad, whether it be a person who's nearsighted and so anything far away is just very hard to see, or it be...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2911.863,
    "end": 2940.03,
    "text": " um you know a robot that has a a receptor that is just like not functioning correctly or um an EEG headset that like you know someone left the ground uh you know they didn't set that up right and so it it sees really high amplitudes not because of the person's neural activity it's because it's getting like supercharged um so in an accurate way so that's sort of the partially observable bit um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2940.601,
    "end": 2954.104,
    "text": " they do i mean anytime i look at model comparisons it's just you know people do very regularly benchmark um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2955.029,
    "end": 2959.013,
    "text": " Yeah, they, they regularly benchmark these models against reinforcement learning models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2959.953,
    "end": 2968.621,
    "text": "Um, usually again and again, you know, furnishing an inherent balance of exploration and exploitation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2968.641,
    "end": 2982.454,
    "text": "Like, so, so a lot of people point that out, that they just find a lot more sort of credibility in the notion of like, oh, these agents because of G uh, they're sort of naturally.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2985.235,
    "end": 2991.13,
    "text": " internally architecturally incentivized to seek out more information.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2991.711,
    "end": 2998.047,
    "text": "And nonetheless, like once they're affirmed that they have enough information, they are",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2998.313,
    "end": 3021.79,
    "text": " over time more than likely going to become like oh no i've seen that before so that they kind of know not to go necessarily in that direction however it's over a policy horizon so maybe that policy horizon is five steps in the future or in a continuous state space it could be you know a minute into the future uh and what they expect to kind of see over along the way um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3021.77,
    "end": 3030.421,
    "text": " You know, so so even if they think they've explored a space well, they might not necessarily know the full sequence of transitions that will happen over time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3030.881,
    "end": 3042.496,
    "text": "So it's very likely that they'll be able to, depending on your setup, continuously find ways of like getting information gain out of a situation, computing that, like incorporating that into their decision making.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3043.037,
    "end": 3047.322,
    "text": "But once they become much more reaffirmed that they've explored enough,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3047.774,
    "end": 3069.9,
    "text": " then they start to you know i'm i'm speaking from from experience causing those things sorry oh in monterey bay i'm not sure he's talking to us no uh there you go um so uh yeah i",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3070.268,
    "end": 3074.616,
    "text": " that kind of interrupted my sorry train of thought, but it's just to get to the point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3074.656,
    "end": 3087.518,
    "text": "It's like very often than not, I think that teammates example and spending just like an extra not a long time, but just like a minute or two because I know with robotics, you're usually looking at continuous streams, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3087.558,
    "end": 3088.48,
    "text": "I mean,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3088.46,
    "end": 3094.268,
    "text": " Yeah, I was just going to say yes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3094.949,
    "end": 3109.708,
    "text": "What I would love to be able to do is take a six degree of freedom robot arm and output the six joint variables that are continuous between some limits.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3109.908,
    "end": 3128.878,
    "text": " would all be given some sensory input and then beliefs about what that sensory input indicates about current state yeah no it's incredibly relevant to a lot of the work that is done here and it's sort of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3129.23,
    "end": 3153.094,
    "text": " know one way that they break down if you if you've looked at the book and and become more familiar with the notion of like an observation modality which is you know a very common sensical thing if you get rid of the jargon it's just sort of like you know i can see things versus feel things versus taste things like there are different ways that i can receive information",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3153.074,
    "end": 3171.032,
    "text": " and and so they tend to break those up into three different categories and so there's um exteroceptive which would be like information I receive from the outside which includes all the things they said earlier that the sort of common five senses of like I can see and touch and feel things",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3171.012,
    "end": 3200.496,
    "text": " interoceptive would be like internal bodily signals um that are coming like sort of from within that i'm not necessarily feeling them in the in the typical sense of like my skin is in you know sort of dermal aspects um and then there's proprioceptive and that that's if you come across that term and sorry by the way i don't mean to sound kind of sending if you already are familiar with it no no this is great thank you cool definitely",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3200.476,
    "end": 3206.706,
    "text": " yeah so so proprioceptive i'm just trying to find it quick and we might have to wrap up then is uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3208.475,
    "end": 3209.616,
    "text": " I know it's in here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3212.22,
    "end": 3220.891,
    "text": "Somewhere they have a nice sort of visual description of like the idea of like proprioceptive afferent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3221.191,
    "end": 3230.803,
    "text": "So this is, you know, this is still related to more like the brain, but proprioception, you know, it has much more to do with like movement.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3230.783,
    "end": 3232.425,
    "text": " and kind of reactive systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3232.685,
    "end": 3249.526,
    "text": "So there could be like a reflex arc sort of logic, for example, that might be of interest to you if you want a model that's kind of a little bit more complex or sort of savvy with putting together the architecture in regards to active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3249.947,
    "end": 3254.192,
    "text": "There's no necessity of it, though, and I think you might",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3254.408,
    "end": 3260.477,
    "text": " especially find Chapter 8 interesting, which is going to explicitly go over continuous models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3260.618,
    "end": 3267.609,
    "text": "It, of course, is different because of the continuous space, like time and continuous state space sort of setup.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3269.311,
    "end": 3273.598,
    "text": "But then in that case, we're looking at more like kind of these attracting",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3273.832,
    "end": 3292.482,
    "text": " points or or here this is uh this is rather complex but this is a lawrence you know chaotic system and sort of like different kinds of dynamics and viewing them in different ways and viewing how you know like i mentioned with the discrete state space setting there's c which is sort of defining your",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3292.462,
    "end": 3314.284,
    "text": " goals or your expected observations you want um the the sort of equivalent of that in continuous state space models you could view that as uh sort of like an attracting point or or a distribution of attracting points or an attracting distribution uh over a continuous space and just modeling it like that um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3314.264,
    "end": 3319.068,
    "text": " You also it relies a lot more on sort of Taylor series approximations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3319.388,
    "end": 3325.274,
    "text": "There's this notion called refers to it as the generalized coordinates of motion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3326.074,
    "end": 3333.281,
    "text": "So it's sort of like there are these more sort of working more with more directly and explicitly with derivatives over time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3334.041,
    "end": 3340.367,
    "text": "So, yeah, it's yeah, I realize that I'm actually kind of shot by the time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3341.288,
    "end": 3344.17,
    "text": "But thanks for this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3344.268,
    "end": 3358.783,
    "text": " opportunity to get into uh to sort of the deeper theoretics and and and really honing in on some of these equations uh you know some yeah it's sometimes folks with the here i'll uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3360.046,
    "end": 3376.013,
    "text": " pause um also thanks for bearing with the uh this is my first time ever when i'm doing this purely solo so yeah um so go ahead and stop recording and thank you everyone and see you at the next see you at the next one",
    "speaker": "SPEAKER_01"
  }
]
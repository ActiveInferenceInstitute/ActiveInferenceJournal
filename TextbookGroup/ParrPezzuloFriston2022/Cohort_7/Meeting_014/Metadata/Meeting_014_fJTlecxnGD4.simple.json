[
  {
    "start": 4.486,
    "end": 9.095,
    "text": " Welcome everyone to the Active Inference Institute textbook reading group.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 9.776,
    "end": 17.25,
    "text": "This is cohort seven, continuing on with chapter eight of the textbook.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 17.831,
    "end": 27.81,
    "text": "We already know that we are in the second half of the textbook, meaning we're much more focused on practice rather than theory, which we had in the first part.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 27.79,
    "end": 47.183,
    "text": " and chapter two we were introduced in chapter six excuse me in part two we were introduced in chapter six to a recipe for building active inference models so we're familiar with all of uh you know the core fundamentals including what is your system of interest or your agent or your generative model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 47.163,
    "end": 73.491,
    "text": " that you were looking at and then what is on the other hand uh to use mark solmes's words just as we identified a system of interest we have to figure out our not system that is what is not the system of interest what is the environment or other variables uh that you know exist in the world but are not um part and parcel with our agent right so that's",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 73.758,
    "end": 80.75,
    "text": " That means identifying what is our generative process or the environment or the knot system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 81.311,
    "end": 83.975,
    "text": "And so we are familiar with a lot of the core components.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 84.135,
    "end": 91.868,
    "text": "We look at things like hidden states, observations, and actions or policies, which are sequences of actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 92.27,
    "end": 97.241,
    "text": " And then we identify the variables of the models, the different parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 98.122,
    "end": 105.719,
    "text": "And from there, we just build upward and we look at what kind of model is necessary for understanding our particular system of interest.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 106.54,
    "end": 112.894,
    "text": "And so we looked at chapter seven, which had to do with discrete state space models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 112.874,
    "end": 129.027,
    "text": " and that covered largely the hidden markov model in the musician example and then built up to the pomdp or partially observable markov decision making process which is sort of the exemplar discrete state space model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 129.158,
    "end": 135.868,
    "text": " And that was illustrated through use of the T maze example, mouse choosing left or right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 135.908,
    "end": 153.013,
    "text": "So we know already with the discrete state space that we, if we followed up to the end of chapter seven, we can build models where there are discrete actions being taken, like the mouse chooses the left or the right arm of the maze, or it chooses to visit the informative queue along the way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 153.433,
    "end": 155.436,
    "text": "But these ultimately amount to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 155.416,
    "end": 183.685,
    "text": " three different actions per time step they are not a continuous state space right so for those who are very familiar with anything from just general statistics linear regression up to machine learning and deep learning not models that work with continuous state spaces oftentimes we want to create models that can predict continuous values such as 37.99 as opposed to choose left in the teammates",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 183.665,
    "end": 204.171,
    "text": " right and that also means being able to read continuous data such as from data streams iot devices financial data and there's a plethora of different kinds of continuous uh data to look at so that's what brings us here to chapter eight looking at continuous time models",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 204.151,
    "end": 207.538,
    "text": " These are arguably where active inference started.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 207.558,
    "end": 224.033,
    "text": "Neuronal signals are usually measured continuously, including from electroencephalography or EEG devices, as well as fMRI and looking at both signals, as well as a variety of other physiological data, including things like heart rate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 224.418,
    "end": 239.428,
    "text": " So whenever we get into chapter eight, we turn to continuous state space models, which the authors state are well suited for modeling physical fluctuations impinging on sensory receptors and for the continuous motion of the effectors, e.g.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 239.448,
    "end": 240.25,
    "text": "the muscles.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 240.23,
    "end": 257.412,
    "text": " we use to change the world around us, with model examples for the dynamical systems involved in motor control, as well as the concept of generalized synchrony, and finally constructing hybrid models for combining discrete and continuous variables at different levels, which features at the end of the chapter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 257.392,
    "end": 280.733,
    "text": " so largely we're introduced to notation for stochastic equations uh which involve are involved in continuous time models uh we see a lot of similar notation to what we've seen so far earlier on in the textbook the authors largely use y to denote observations and x to denote hidden states here this is not terribly different but we notice",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 280.713,
    "end": 308.813,
    "text": " that we're also looking at and excuse me let me go to this is technically the one for the generative model um here we're also looking at a more continuous state space we're looking at something like a kind of error term or or flux uh or stochasticity within an equation as well as we're seeing a derivative so we're looking at uh states evolving over time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 309.063,
    "end": 336.336,
    "text": " as a function of hidden states uh as well as well it rather was the rate of change of x given x and some kind of static or otherwise variable with stochasticity so from there we're introduced to other notions these are all very kind of standard ideas that of course are absolutely not unique to active inference this is active difference building upon a lot of previous",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 337.058,
    "end": 361.176,
    "text": " work and so we're looking at an attractor system right so so this uh generative model and equation 8.3 were introduced to this idea that v is a kind of attractor so whenever x is too high it tends to become its rate of change becomes negative as it heads back towards the value of the attractor and vice versa if x is too low",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 361.156,
    "end": 385.915,
    "text": " tends to increase back towards the value of the attractor and so that means as an attractor is a kind of attracting or fixed point um I I emphasize this despite its simplicity because when you think about it we're oftentimes trying to model systems that you know whenever it comes to agents or looking at organisms or anything that needs to persist",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 385.895,
    "end": 398.655,
    "text": " in some kind of way within an environment, whether it means an organism trying to take care of its homeostasis or it be a thermostat reacting to changes in temperature, we tend to be dealing with something like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 399.209,
    "end": 399.79,
    "text": " attractors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 400.27,
    "end": 412.567,
    "text": "We tend to deal with, you know, an attractor for me is that my body temperature tends to be roughly around 98 degrees Fahrenheit as I use Fahrenheit in the United States.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 413.088,
    "end": 420.197,
    "text": "And so we're looking at attractors in the sense of just where do things tend around, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 420.858,
    "end": 426.586,
    "text": "Without the attractor, we can potentially completely disperse depending on how our model is set up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 426.987,
    "end": 428.068,
    "text": "There's nothing to kind of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 428.318,
    "end": 430.003,
    "text": " guide us in our directionality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 430.364,
    "end": 440.792,
    "text": "And whenever it comes to maintaining a bodily or physiological or other kind of system that requires maintenance at some homeostatic level or range, we can think of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 441.211,
    "end": 450.623,
    "text": " the homeostatic level or range as very much kind of an attractor or set of attractors in which the model tends towards to persist.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 452.105,
    "end": 456.211,
    "text": "So we see a lot of these different kinds of dynamics play out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 456.231,
    "end": 461.177,
    "text": "We're also introduced to precision, attention, and sensory attenuation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 461.157,
    "end": 472.5,
    "text": " many of these make sense to introduce here precision not and potentially attention uh they they could come in more deeply in earlier chapters and of course they do",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 473.408,
    "end": 491.35,
    "text": " get mentioned in previous chapters but essentially they make sense here because they tend to be viewed as continuous values not necessarily but um so so they just relate much more to the material in this particular chapter with precision as a kind of sharpening of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 491.33,
    "end": 493.834,
    "text": " a probability distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 494.515,
    "end": 500.565,
    "text": "So if precision is below zero or excuse me, between zero and one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 501.126,
    "end": 509.44,
    "text": "So what can happen is that your probability distribution can become more uniform and vice versa if your precision is far above",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 509.42,
    "end": 515.307,
    "text": " or even a little bit above one, then the precision of your probability distribution is going to sharpen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 515.888,
    "end": 516.108,
    "text": "Right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 516.128,
    "end": 527.481,
    "text": "So a general sense of this is that if I have a strong amount of trust in the observations I'm seeing, that is, I can clearly tell that the thing that I'm seeing in the distance, I feel fully confident it is what it is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 527.942,
    "end": 529.884,
    "text": "And we kind of liken that to the idea of precision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 530.225,
    "end": 534.75,
    "text": "That means there's a very high probability that the thing I see in the distance is what I think it is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 535.291,
    "end": 539.055,
    "text": "The low near zero probability that it's going to be something else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 539.035,
    "end": 544.481,
    "text": " So we get this very sharp distribution where a lot of the density is centered around one possibility.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 545.942,
    "end": 553.17,
    "text": "Similarly, the other way around, if precision is low, we see a much more flat, broader distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 553.27,
    "end": 556.313,
    "text": "I have no idea what that thing in the distance is, and I'm not very clear on it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 556.774,
    "end": 560.578,
    "text": "So precision in that sense is kind of like a sharpening or lessening dynamic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 560.938,
    "end": 565.583,
    "text": "It can be related to synaptic gain in neuronal dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 565.563,
    "end": 583.96,
    "text": " um it's it's seen as being very crucial for uh looking at things like confidence right whether it be conference in your your a matrix your uh in terms of being able to interpret a situation or b matrix in terms of like being able to look forward or back",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 583.94,
    "end": 606.505,
    "text": " And whenever we're looking at continuous time models where we're no longer in the world of A and B matrices, although some of the dynamics are certainly similar in the sense of temporality that we find in a B matrix and relating observations or data or Y to hidden states or higher order beliefs or X.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 607.413,
    "end": 614.545,
    "text": " So we're also looking a lot at examples that use dynamical systems.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 615.146,
    "end": 618.792,
    "text": "And so a good example is the generalized Lotka-Volterra dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 619.333,
    "end": 626.825,
    "text": "This is kind of a classic ecological model that looks at relationships between plants, herbivores, and carnivores.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 627.366,
    "end": 630.872,
    "text": "The sort of commonsensical notion of it is that the carnivores",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 630.852,
    "end": 657.028,
    "text": " eat the herbivores who eat the plants and then the plants need enough time to regrow otherwise their population will completely die out which of course because you can see the dependencies between these three different um organisms they their their populations fluctuate over time as one proceeds to consume the other and yet they come back with reproduction",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 657.008,
    "end": 681.835,
    "text": " and yet further if they were to to strongly diverge from these kinds of patterns what we we would have is eventual extinction of at least one of the species which necessarily in this situation would lead to extinction of the rest of the remaining species right so we see this kind of movement of them in kind of cyclical patterns nearly and whenever we compare",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 682.422,
    "end": 701.798,
    "text": " the herbivores and the plants, or we compare the carnivores or the herbivores, over the course of playing out this simulation over time, we see that there is a kind of attracting shape to the continuous state space that is their populations or their joint",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 701.778,
    "end": 714.066,
    "text": " um populations or the way of looking at them as they correlate with one another in different ways over time we see these shapes play out as well whenever we look at the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 716.408,
    "end": 742.872,
    "text": " lauren's system and this is from the bird song example which is where there are two birds who are singing with one another and the idea is that they're going to kind of collaboratively finish a song where they take turns filling in space for the notes i almost think of two guitarists playing solos or something and they're like trading off back and forth",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 742.852,
    "end": 767.863,
    "text": " and yet they're still maintaining the same song so there's a kind of synchronicity the kind of rhythm that they need to maintain uh they need to stay on beat as it were um and there's variation in what they play over time right they're not just both playing the same notes to where we get some very you know we don't settle in some clear equilibrium and stay there there's a lot of variety going on and yet they have to stay in step",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 767.843,
    "end": 796.568,
    "text": " and um ideally they are in fact playing the same song so whenever we look at the Loren system and we look at kind of what's going on in the state spaces here we find attraction around two general areas of kind of kind of like two spheres of orbit right um as these two birds sing their songs over time and then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 797.155,
    "end": 809.578,
    "text": " they continue to explore this state space as they go right so we see like for before excuse me let's go with this after learning",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 810.014,
    "end": 816.029,
    "text": " they learn one another's patterns such that the state space they're exploring, it tends to follow this line.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 816.289,
    "end": 818.715,
    "text": "That is, they're much more kind of synchronized.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 819.016,
    "end": 827.837,
    "text": "And we notice relative to before learning where things were much more chaotic as they're still becoming attuned to one another's tunes,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 827.817,
    "end": 857.402,
    "text": " uh after learning not only is it more synchronized but we notice the axes have kind of increased right so in before learning they explore actually a significantly smaller area of the state space as their kind of correlated line between their actions you know it spreads around after learning it's much more focused and they actually make it much further in different directions of the state space presumably they've learned to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 857.382,
    "end": 871.257,
    "text": " really well attuned to one another while simultaneously being able to hit new notes that maybe weren't heard before the notes presumably being part of the state space or identifiable therein",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 871.422,
    "end": 887.524,
    "text": " A lot of this starts to look like whenever we consider attractors, but then we expand it to where there are multiple agents, it's interesting to see the different kinds of emergent phenomena that arise out of the dynamics that play out between them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 887.673,
    "end": 915.081,
    "text": " and how they synchronize because in active inference we've often talked about um agents who sort of their internal states you know presumably if the the agent is is sort of optimal enough for the environment that they're in and find ways to adapt and what will happen is that their internal states will somehow start to correlate strongly in in different ways and in different patterns and perhaps with the temporality to where it's not as simple as drawing a singular uh um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 915.061,
    "end": 915.823,
    "text": " J. Sam Hurley, Ph.D.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 915.803,
    "end": 924.903,
    "text": ": line, but rather it'll be something much more complex, like the dynamics, for example, of a lorne's attractor whenever we draw it out, and how the state space looks from there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 926.466,
    "end": 933.542,
    "text": "So all this is to say much of this chapter, while at first glance provides us with notation,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 933.522,
    "end": 952.189,
    "text": " and the necessary equations for solving for minimizing free energy in various ways and continuous state spaces using those kinds of models were also along the way that the author authors cleverly snuck in all these other aspects regarding synchronization communication",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 952.169,
    "end": 982.176,
    "text": " some of the building blocks for thinking about multi-agent simulations what does it mean for two systems whether it be an agent and environment or it be two agents or otherwise how do they synchronize one another and then we're also reintroduced to the pomdp structure which we see is like everything in the upper half of this graph excuse me it is graph and figure 8.6 um bayesian graph and then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 982.156,
    "end": 984.658,
    "text": " The upper half is the POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 985.119,
    "end": 1005.017,
    "text": "Everything below around where this dashed line is, we can see is a lower level, faster timescale hybrid model in the sense that for every iteration of the POMDP up here, we see that there are actually three iterations of the continuous state space model at the lower level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1006.138,
    "end": 1010.442,
    "text": "It's like you can imagine receiving a lot of data",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1010.59,
    "end": 1029.021,
    "text": " over time and then at some threshold of time steps such as three in this case we could send that upward to the higher level to say okay we received this data what does it mean right um it's kind of like taking in the full experience",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1030.942,
    "end": 1040.596,
    "text": " And then from there, the POMDP would then use its own model parameters to update different kinds of inferences to send those back down as priors to level one as we continue the inference process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1043.801,
    "end": 1049.389,
    "text": "We have some nice discussion again, looking at saccades and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1050.567,
    "end": 1054.691,
    "text": " those kinds of dynamics, which are much more clearly continuous state spaces.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1055.252,
    "end": 1058.715,
    "text": "More often than not, we're looking at kind of like continuous coordinate systems.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1059.576,
    "end": 1075.292,
    "text": "We also have mixture models and clustering, which again, kind of imagine for those unfamiliar with that kind of logic, imagine taking that hybrid model we just saw and kind of smashing it down such that we can receive continuous",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1075.272,
    "end": 1082.865,
    "text": " Jake Hamilton, data, but then use it to infer clusters or discrete state space data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1083.086,
    "end": 1083.326,
    "text": "Right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1083.507,
    "end": 1091.661,
    "text": "So you see a certain kind of pattern in the physiology, and you determine that someone shows a particular discrete",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1091.641,
    "end": 1116.282,
    "text": " health state such as they are healthy versus another combination of continuous values their heart rate their glucose levels and so on and in this particular state space uh discrete state space they are they are um moderately unhealthy right and and with this set of values right so it's kind of going continuous to discrete um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1117.848,
    "end": 1121.594,
    "text": " So I think I'll leave it there because I've already talked for quite some time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1121.974,
    "end": 1123.617,
    "text": "Apologies for that if it took away from anyone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1124.198,
    "end": 1136.076,
    "text": "I'd like to open up the conversation then if anyone has any questions or anything in particular that stuck out to them in this chapter, and we can discuss it if you like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1147.801,
    "end": 1159.484,
    "text": " There's always the... I'll make a first comment and then John or Isabella.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1159.804,
    "end": 1167.379,
    "text": "In the first equation of the chapter, it",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1178.446,
    "end": 1182.592,
    "text": " you said neural signals are measured continuously.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1183.033,
    "end": 1197.994,
    "text": "So oftentimes people look at the discrete spiking of a single neuron, even though that's a simplification, but measurements are often continuous and then later discretized.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1199.336,
    "end": 1207.828,
    "text": "So it's sort of like, if you look at the instrumentation that's actually coming off of real world sensors,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1208.247,
    "end": 1210.57,
    "text": " in a way they're kind of like analog.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1211.752,
    "end": 1226.374,
    "text": "And then digital systems discretize that for different kinds of advantages, but analog measurements are prevalent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1227.455,
    "end": 1237.63,
    "text": "And then this is just the, even though confusingly with the GN and F that don't map to equation 2.6 and 2.5,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1237.711,
    "end": 1266.047,
    "text": " this is sort of a different look on the s and the o from the pomdp in terms of having uh observables y which is o latent states x which is s and then instead of having the b transition matrix interpolated between different latent states rather there's a continuous um first derivative landscape with x dot",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1267.158,
    "end": 1277.709,
    "text": " So it kind of embodies the same or analogous elements, but with different letters.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1280.852,
    "end": 1281.453,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1281.473,
    "end": 1282.133,
    "text": "Thanks, Daniel.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1282.173,
    "end": 1283.375,
    "text": "Yeah, it's good points.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1283.495,
    "end": 1293.705,
    "text": "And because it's worth just really getting familiar more with the notation for anyone who's still kind of picking up on active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1294.883,
    "end": 1320.672,
    "text": " with this in one way and i don't i don't want to these are my words not the author's words of course but in a sense if you think about the the a matrix as a in the in the discrete state space case in chapter seven um and we think of how that is relating hidden states to observations then we do have something like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1321.31,
    "end": 1345.596,
    "text": " uh relationship here for this generative model that very much relates to two and then if we think of a b matrix as the probability of the next state conditioned on the current state and uh the current action or policy being pursued um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1345.846,
    "end": 1361.982,
    "text": " this of course is different we're looking at derivatives or rates of change but nonetheless we are essentially in this first example we're relating hidden states and observations and the second we are relating hidden states to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1361.962,
    "end": 1366.746,
    "text": " something like the temporal aspect of hidden states where the hidden state will come next.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1366.786,
    "end": 1379.337,
    "text": "So, of course, it's not transitioning to a new state necessarily, but it's looking at its rate of change, which then, of course, could be used to predict what the value of the next hidden state will be in this continuous state space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1379.357,
    "end": 1386.363,
    "text": "And then we also have v, almost like its own third variable, and it itself doesn't have to be static.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1386.403,
    "end": 1391.968,
    "text": "This, of course, is just like a nice simple example, and it can be used as an attractor in this case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1391.948,
    "end": 1398.769,
    "text": " and then also with neuronal signals just with my own so my own previous work on",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1399.34,
    "end": 1425.027,
    "text": " uh EEG signals it's so interesting in the sense of like a lot of them still are being uh recorded these signals are being recorded like digitally increasingly so and so you're kind of limited to the kind of rate at which the technology records and so even though you might be looking at something down to the 10th of a millisecond",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1426.019,
    "end": 1437.025,
    "text": " if we think about how when we take a derivative, we're technically like having to do a lot of discretization of values in order to, you know,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1437.916,
    "end": 1460.818,
    "text": " uh carry out differentiation or or integration then then similarly with eeg it's like okay well i'm still discretized to the tenth of a of a nanosecond or or am i of a millisecond or whatever you know rate it's recording at so it is it is interesting like once you get down to these fine fine lines it's like what is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1461.288,
    "end": 1472.614,
    "text": " discreet or continuous any anymore that's much more dramatic than than probably what the sentence that should actually come to mind but still it's uh there's more nuance but but still it's um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1473.117,
    "end": 1497.15,
    "text": " it is interesting to think about and for all intents and purposes for the most part with with neurophysiological or any kind of physiological data you still are typically looking at continuous data unless unless there's some kind of particular criteria right that that that someone makes a judgment about a value they see and then they just mark it as like you know",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1497.13,
    "end": 1524.96,
    "text": " on a likert scale one to five it's still a discrete state space with an implied ordering to it uh for example so anyway but now we're talking a lot about a I'm talking a lot about data types now um so see maybe something we could pick up on in the chapter eight questions unless anyone no okay no problem",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1527.05,
    "end": 1531.595,
    "text": " Now let's look at chapter eight of the coda.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1534.858,
    "end": 1539.804,
    "text": "Nice summary that Daniel and Ali did a couple of years ago.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1542.687,
    "end": 1543.087,
    "text": "Let's see.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1547.432,
    "end": 1555.2,
    "text": "Yeah, another random thought is it's just interesting that the analytical relationships hold for the discrete case at all.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1556.547,
    "end": 1563.157,
    "text": " when their more underlying differentiable flow-based form is continuous.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1564.439,
    "end": 1573.633,
    "text": "And yet it works like almost exactly how one would think in the discretized setting, kind of however you discretize it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1575.056,
    "end": 1582.487,
    "text": "So it's like underlying circle and then whatever resolution your screen is, it's like, it kind of works out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1583.648,
    "end": 1604.627,
    "text": " that doesn't seem guaranteed by the definition of a circle but yet it is you know it's an interesting way to to put it it's i mean it",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1606.919,
    "end": 1607.481,
    "text": " It's funny.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1607.521,
    "end": 1612.054,
    "text": "Sometimes it comes down to making your own kind of decision whenever it comes to the modeling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1612.074,
    "end": 1614.922,
    "text": "Do you want to view something as discrete?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1615.042,
    "end": 1622.042,
    "text": "But then, of course, if you're going to build an active inference model, then it becomes insanely important to decide which",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1622.562,
    "end": 1649.734,
    "text": " state space you're working with right because we then we do start to see you know I mean underneath there's similar mechanics between the two kinds of models if we're able to just say oh there's just two kinds there's continuous and screen with all the infinite varieties in between I suppose but um but still we end up with with very different Dynamics and so also worth talking about then um I mean",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1651.182,
    "end": 1679.097,
    "text": " I don't know, no one's had any questions about it, so I haven't gone into it, but there are the generalized coordinates of motion, which might just be... But these are essentially sort of the actual mathematical mechanics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1680.359,
    "end": 1684.47,
    "text": " behind continuous state space models, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1684.49,
    "end": 1691.55,
    "text": "So it ends up, what ends up happening is that for those who are familiar with Taylor series approximation,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1691.902,
    "end": 1704.681,
    "text": " like you can view it as just, you know, you start to expand the number of terms in your Taylor series expansion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1705.682,
    "end": 1713.654,
    "text": "The authors note that around six generalized coordinates are sufficient, meaning we're gonna expand that far.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1714.836,
    "end": 1718.601,
    "text": "And it works quite well",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1720.387,
    "end": 1741.45,
    "text": " not just as supplying these core functions and equations and then furthermore they're also very commonly used in analyzing neuronal signals um but many obviously many many many other types of data that don't have to have anything to do with neuronal anything necessarily but these are classic equations that you would find",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1741.97,
    "end": 1769.749,
    "text": " there and then it also works quite well in the sense of having a variational inference model right because as opposed to trying to like excessively predict into the future what will happen we can kind of simplify to or approximate behavior oftentimes as authors say with six generalized coordinates let's look at",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1777.036,
    "end": 1806.79,
    "text": " sensory attenuation what are the causes and consequences of sensory attenuation and check the chat my understanding nothing more than stochasticity within the agent chaotic movement from one attractor or internal state to another movement and sensory attenuation a point where you don't want to perfectly or accurately predict sensory consequences or associates of movement",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1807.934,
    "end": 1833.705,
    "text": " box 8.1 belief I am not moving need to downweight the precision of that belief make it wider or lighter so that action shift can occur policy shift occurs increase the attention again wow we're somewhere between uh a sentence and a set of short causal sort of predicates here uh very symbolic",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1833.787,
    "end": 1860.655,
    "text": " but actually we were getting warmer over time as we continued reading down these statements this is not a bad way of compressing the information that could help us to describe sensory attenuation why is that sensory attenuation so one of the interesting things that can sometimes go overlooked in active inference and it's worthwhile to to be aware of it for anyone who's interested in uh you know",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1861.023,
    "end": 1868.367,
    "text": " Michael Mann- kind of understanding and then utilizing the active inference framework, we know that agents are self evidencing right so.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1870.0,
    "end": 1898.421,
    "text": " self-evidencing well that means that in some way if we just take it at face value the idea is that an agent will try to realize what it already expects right we saw this in chapter seven with preferences and the more expanded you know way of looking at them is priors over observations and if they remain fixed which oftentimes they do such as in the teammates that the mouse always wants the cheese at no point does it start to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1898.401,
    "end": 1924.678,
    "text": " want the shock and devalue the cheese right so that reminds us of the kind of um attractor Dynamics right you know and so sometimes people have used the phrase goal prior which probably should not be too terribly confused with the phrase preferences but it has been used in in place of it to say that this is a prior belief you have and because it doesn't change",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1924.658,
    "end": 1947.139,
    "text": " that is where you are attracted to let's say so whenever it comes to sensory attenuation the odd thing comes up where in the moment you believe you are sitting still and you decide you would like to get up right but you can't because you strongly believe currently that you're sitting still",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1948.587,
    "end": 1952.952,
    "text": " But maybe you have in your goal prior that you do or your preferences, you do want to stand up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1953.573,
    "end": 1962.243,
    "text": "But you can imagine this kind of strange conflict that occurs, even though presumably all of you once in a more colloquial sense to stand up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1963.124,
    "end": 1966.408,
    "text": "So what is this sensory attenuation about?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1966.769,
    "end": 1968.17,
    "text": "And what is belief?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1968.551,
    "end": 1969.452,
    "text": "I am not moving.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1969.472,
    "end": 1974.217,
    "text": "Need to downweight the precision of that belief so that action shift can occur.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1974.277,
    "end": 1977.261,
    "text": "Policy shift occurs, increase the attention again.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1978.76,
    "end": 1981.272,
    "text": " Let's go to box 8.1c.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1981.452,
    "end": 1986.998,
    "text": "Of course, it's a text.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1989.273,
    "end": 1993.778,
    "text": " precision control is its role in movement generation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1993.838,
    "end": 1997.723,
    "text": "To understand this, it is worth thinking about what happens in the absence of this control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1997.843,
    "end": 2000.746,
    "text": "Imagine first that sensory data are predicted with high precision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2001.347,
    "end": 2002.328,
    "text": "I am sitting down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2002.588,
    "end": 2003.789,
    "text": "I notice I'm sitting down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2004.53,
    "end": 2012.119,
    "text": "The messages from these data are therefore afforded high synaptic gain and lead to voracious inferences about the position of some body part.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2012.76,
    "end": 2014.842,
    "text": "I am definitely sitting down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2014.822,
    "end": 2018.989,
    "text": " The problem with this is the equivalence between motor commands and predictions under active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2019.069,
    "end": 2028.383,
    "text": "An accurate belief that I am not moving, I'm not moving, I'm sitting down, cannot be used to predict the sensory consequences of movement vital for the initiation of that movement.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2028.524,
    "end": 2031.328,
    "text": "How will I ever move if I firmly believe I'm sitting down?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2032.557,
    "end": 2047.298,
    "text": " The idea is to attenuate or sort of partially, if not fully, I don't want to say fully, but to some significant actionable degree to reduce one's precision in their beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2047.558,
    "end": 2054.127,
    "text": "Meaning that dynamic I described earlier of sort of a flattening, which is...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2054.107,
    "end": 2076.857,
    "text": " they're implying that in order to stand up i need to actually temporarily attenuate my belief that i'm sitting down just so that the the proprioceptive signals and everything else can play out such that i can stand up right so so there's this kind of interesting philosophical nearly aspect of this that is we have to convince ourselves",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2078.018,
    "end": 2105.865,
    "text": " implicitly no assumption that any of this is actually explicit in the sense of fully conscious you're thinking this out loud in sentences or otherwise the sense is that you actually have to kind of pause your beliefs in some way or more literally it would be you have to introduce uncertainty in your understanding of the current situation in order to actually change that situation which is rather fascinating and it it comes up so it's something to potentially wrap your head around it's",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2106.79,
    "end": 2113.181,
    "text": " but it starts to make sense and then it becomes useful in a variety of situations, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2113.602,
    "end": 2123.5,
    "text": "For example, it's been found to be rather significant in cases of Parkinson's disease, where someone is struggling to attenuate",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2123.48,
    "end": 2152.508,
    "text": " um their their sensory intake right they're they struggle to to have fuller control over how their body responds of course uh and so it's a kind of by being able to understand sensory attenuation and its connection to things like precision precision control as well as being able to see in other areas of the textbook how they relate precision and in particular precision values",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2152.488,
    "end": 2182.073,
    "text": " to potentially being relatable to various neurotransmitters suddenly being able to understand these kinds of dynamics at least as of this point in time in the textbook a couple years ago by these authors and the work has not stopped since so and i don't think it's going to but at least as far as computational psychiatry is concerned being able to make these kinds of connections that are interpretable and have neuro biological substrates involved um can",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2182.458,
    "end": 2206.756,
    "text": " at least hold the promise of being able to better serve different kinds of interventions and treatments for those who do struggle with these kinds of pathological disorders in ways that were not necessarily visible before because of a lack of the kind of underlying computational dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2207.357,
    "end": 2210.001,
    "text": "And that's sort of why fields",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2210.167,
    "end": 2239.515,
    "text": " like computational psychiatry have been sort of taking off the past several years as the breadth of computational tools ever widens and more and more people produce open source resources so that broader populations of people can get involved in these kinds of work and of course they're not purely limited to computational psychiatry whatsoever we could imagine",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2239.58,
    "end": 2254.509,
    "text": " Anyone who's working in robotics, who is working with a, you know, some kind of robot, whether it be an automation system that's building car parts or it be like an actual robot that has limbs and navigates the world on its own.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2255.391,
    "end": 2260.761,
    "text": "In either case, imagine if it's running on inference principles.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2260.741,
    "end": 2284.403,
    "text": " especially a bayesian one where we can have the opportunity to have excessively strong priors such that the robot never does anything because it's too terribly convinced of the current situation that it doesn't do anything to change it well then suddenly sensory attenuation becomes one of the first concepts that you should look at",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2284.383,
    "end": 2313.529,
    "text": " and it brings up interesting implications for neurotransmitters as well and relating them to other kinds of cognitive systems right um being able to have the capacity for the synaptic gain precision control dynamics has a lot of implications for for building any kind of predictive system whenever we're looking at this or similar frameworks what is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2315.939,
    "end": 2322.304,
    "text": " Getting out of a chair, I was not the first.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2323.82,
    "end": 2346.739,
    "text": " yeah it's also sensory attenuation it's like especially when we think about awareness or higher level introspection we're attenuating almost all perception and only with really dedicated focus on sensory input can we even attend to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2347.597,
    "end": 2355.43,
    "text": " Yeah, just all of our skin, all of the sounds, all these other things, all spatial awareness, ability to think of any concept.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2355.451,
    "end": 2357.314,
    "text": "It's like, it's all being attenuated away.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2357.354,
    "end": 2360.68,
    "text": "That's sort of the attention question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2360.7,
    "end": 2362.984,
    "text": "So it's like, there's the attention is all you need.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2363.605,
    "end": 2366.77,
    "text": "And then there's the other side of the coin is attenuation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2368.117,
    "end": 2369.118,
    "text": " But that's the same thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2369.479,
    "end": 2377.73,
    "text": "It's just like whether you focus on up weighting what is to be focused on or down weighting what isn't.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2381.855,
    "end": 2382.076,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2383.217,
    "end": 2383.938,
    "text": "Yeah, definitely.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2383.958,
    "end": 2397.516,
    "text": "I mean, it's like if I'm driving on the highway in Chicago, as I occasionally do, I mean, if I attempt to maintain attention",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2398.222,
    "end": 2408.316,
    "text": " whenever it comes to every single car that's currently in my frame of vision, I'm certainly going to have a tougher time",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2408.532,
    "end": 2438.353,
    "text": " focusing on the particular things I need to attend to such as the car immediately in front of me that is suddenly breaking and so I should respond and so it's just another way of kind of restating Daniel's point that is the kind of attentional Dynamics needed to optimally or at least sub optimally but enough to survive a situation let's say um like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2438.823,
    "end": 2465.978,
    "text": " those are there and we can't really do without them um so much of the sensory data that we have hypothetically available to us goes very unnoticed right so and even for in the case of robotics like if you have a robot who is you know putting together sequentially different parts on a manufacturing belt um it's not",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2466.835,
    "end": 2496.842,
    "text": " if it takes in every single signal that it receives it might not be able to hone its attention in on the very particular task at hand right so um relates physiologically we can think of things like metabolism we can think about energy uh expenditure and and saving or conservation so if you took in every single thing you ever saw in its full entirety as if we had perfect",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2496.822,
    "end": 2516.907,
    "text": " you know um um like picture book like memory um photographic memory there it is and I mean the thing about it computationally I mean that's an absurd amount of data it's going to be a lot and if it all gets can you know um stored in long term",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2517.19,
    "end": 2547.113,
    "text": " memory so it's just it's it's it's interesting to think about I mean I've always appreciated in this chapter that despite the fact that it's strongly centered on in name continuous time models we actually sneak in a lot of really interesting concepts that are much harder to demonstrate or describe until we reach as part of the textbook this is the um this is where they apply",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2548.865,
    "end": 2563.59,
    "text": " or look at luck of Volterra dynamics again, but instead we're looking at it in terms of this eye blink conditioning problem, of course, related to the neurobiological substrate underneath.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2565.474,
    "end": 2574.349,
    "text": "And we're looking at these kind of classic ways of just looking at conditioned stimuli, unconditional stimuli, for those who've",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2574.498,
    "end": 2603.712,
    "text": " have backgrounds in psychology this will be um almost annoyingly familiar um we can look at how expectations play out but then we also see these kind of potentially cyclic dynamics that play out that necessarily have to be that way right or close enough to being that way such that the system can persist and then over here this was you know",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2604.029,
    "end": 2632.45,
    "text": " just as they're simulating the system of handwriting kind of the way it plays out and then it itself has a kind of near cyclical dynamic like we were to segment this right we see nearly identical structures right next to each other in what from far away looks like a simple accidental scribble foreign",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2649.878,
    "end": 2679.16,
    "text": " This is a little bit different to just do this, but I notice",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2681.688,
    "end": 2711.121,
    "text": " so i haven't had any particular questions and this is a julia library called rx infer and there's been a nice partnership between the developers of arcs and fur and the institute for quite some time now um a rather active group has formed around it uh for anyone who is ever interested",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2711.185,
    "end": 2737.311,
    "text": " and we're looking essentially at applying and learning and understanding as well as giving feedback to the developers for this package um and i the reason why i bring it up here and now is that in my opinion this offers some of the most interesting tools for looking at",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2737.291,
    "end": 2764.497,
    "text": " continuous time and such a cool website right um continuous time models and so for anyone who wanted to get into this chapter but are still becoming more familiar with active inference itself and are really just wanting to look at how to apply active inference in ways that they're already familiar with maybe computationally this website alone",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2765.371,
    "end": 2789.927,
    "text": " will show you a variety of ways to potentially accomplish what you want to accomplish ranging from the various equations and how they draw out variational free energy as well as Beth approximation um talking more about the underlying Dynamics involved that we get a very good picture of them in the textbook",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2790.262,
    "end": 2816.303,
    "text": " but very significant on this website as well plus you get to kind of see it like in action being tailored for direct modeling so they have an entire set of examples that it looks like they're still doing some modifications to their website but it's all here so we have basic examples bayesian linear regression",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2816.789,
    "end": 2821.176,
    "text": " just being able to simply look at the relationship between fuel consumption and speed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2822.078,
    "end": 2824.923,
    "text": "The faster you go, the more fuel you burn.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2827.006,
    "end": 2837.164,
    "text": "Hidden Markov Model, they've been expanding their set of use cases that ARCS and FERC could be used for.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2837.244,
    "end": 2841.531,
    "text": "And so now we're suddenly looking at a Hidden Markov Model as we saw in Chapter 7.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2843.418,
    "end": 2847.263,
    "text": " This is particularly a Roomba navigating different parts of a house.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2848.225,
    "end": 2866.81,
    "text": "And the hidden Markov aspect is it relates its visual sensory receptor of what the floor looks like to a hidden state that is its belief about what room it is in over time in our classic perception loop.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2867.171,
    "end": 2870.455,
    "text": "This is only perception in Markov models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2871.413,
    "end": 2882.588,
    "text": " And then they're starting to increase, even further lately, support for POMDP models, which is great.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2883.169,
    "end": 2898.45,
    "text": "And it suddenly makes this library incredibly relevant because that would mean that it also covers the kinds of states-based models that are covered in Briston's classic SPM-12",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2899.173,
    "end": 2913.927,
    "text": " package for MATLAB as well as PyMDP for Python, which the SPM does definitely allow for different kinds of continuous state space dynamic analysis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2915.509,
    "end": 2924.277,
    "text": "But RxInfer out of the box is very much ready to go for creating all kinds of models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2924.317,
    "end": 2927.34,
    "text": "Here's the Gaussian mixture model I mentioned earlier.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2927.607,
    "end": 2953.75,
    "text": " where you can take in continuous values, y, and then use them to infer discrete clusterings, right, that are close enough together such that we decide, oh, this makes up a distinct cluster, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2955.738,
    "end": 2959.908,
    "text": " So it's just interesting to think about the many, many ways.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2959.948,
    "end": 2966.324,
    "text": "For anyone who's familiar with deep learning, some of this will just be second nature to already be thinking about these things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2966.545,
    "end": 2973.662,
    "text": "But whenever we think about active inference, we think about Bayesian perspectives where we're always coming into a situation with a prior perspective.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2974.098,
    "end": 2999.92,
    "text": " and thinking about different data types and for anyone who's actually writing code and producing models what data they have available and the potentially infinite ways that they could put things together not just what data are you using and and what is the breadth of the types but also what are the inference processes between them and what kind of state space are they in",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3000.524,
    "end": 3021.823,
    "text": " are you going to discrete discretize something yourself because it makes sense to you but maybe it won't to someone else who looks at the same model they wouldn't approach it that way auto regressive models lagging a variable on itself yeah I mean these are classic models whenever it comes to looking at",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3022.647,
    "end": 3052.384,
    "text": " forecasting and looking at like you know where data is sparse and then suddenly you decide oh if I use the same data stream but just lag it on itself maybe it is a function of itself I get suddenly the the single data source you have becomes useful for predicting itself in that sense so then we can see that play out with observations and then the hidden state inference kind of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3053.579,
    "end": 3082.489,
    "text": " drags or follows after that so that this kind of lag that occurs over time so of course with genera um with continuous state spaces you know as opposed to feeling where like we're constricted to the a b c d and e's of pomdp models here you could have",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3083.482,
    "end": 3094.319,
    "text": " I mean, of course, your matrices in the POMDPs could be massive in the sense of you can include a broad variety of sensory receptors or data streams coming in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3094.379,
    "end": 3102.251,
    "text": "You could have a plethora of different kinds of hidden states or hidden state factors, each with their own discrete hidden states within.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3102.672,
    "end": 3104.514,
    "text": "You could make an incredibly complex model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3105.296,
    "end": 3111.345,
    "text": "But that said, with continuous time models, it often feels a little bit more...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3111.528,
    "end": 3139.847,
    "text": " general in the sense of you could have you could have various parameters some for the same thing some for different things some which are highly related to one another but computed a bit differently and then at the end of the day you're when you're learning them it starts to somewhat follow the logic of a deep neural network or otherwise we're just looking at something like how do we compute weights but how do we do it",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3141.397,
    "end": 3170.307,
    "text": " with respect to minimizing variational free energy as a functional of those parameters and the data coming in and furthermore much more distinctly how do we look at the rate of change in those systems and then here was the nice generalized synchrony piece how do we",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3170.877,
    "end": 3196.969,
    "text": " synchronize with one another in our daily activities i say is someone with a social sciences background so um great point with neural network analogy and reinforcement and the reward function as needed to give a gradient descendable strategy to optimize a functional of weights",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3197.827,
    "end": 3204.132,
    "text": " which don't directly correspond to the semantics of the probability distributions themselves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3204.152,
    "end": 3226.872,
    "text": "It's just hope that there's enough of these tiny guys firing that you can recapitulate at a higher level the semantics of a target distribution versus doing variational free energy as a direct gradient tractable landscape on the probability distributions of interest.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3230.328,
    "end": 3258.242,
    "text": " and how learning can have a rate and then that learning rate is fixed or you could learn the learning rate and then how fast you learned that it's like the buck stops somewhere in a given implementation even though there are like infinite dimensional approximations that's what brings me back to the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3259.741,
    "end": 3285.664,
    "text": " just the general sense of an attractor is well all these things could move we know from chapter six that a lot of it as well as chapter nine I say as we wrap up here but uh chapter nine we also had that that classic you know we're we're I'll just say we're we're always making our own decisions of how things should play out and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3286.167,
    "end": 3293.96,
    "text": " For one person, learning a learning rate is incredibly exciting and a means of testing a very particular hypothesis they have.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3294.781,
    "end": 3301.712,
    "text": "And then for another person, developing another kind of system or not developing a system, rather just trying to create a generative model that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3302.097,
    "end": 3326.839,
    "text": " generally accurately approximately approximates uh an organism of interest that they don't want to make any arbitrary decisions they want everything to be as synced up to the organism they're looking at as possible the idea of learning the learning rate becomes like dangerous is it going to throw your model results so far off or make them so terribly",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3326.819,
    "end": 3343.393,
    "text": " uh non-generalizable that it's like why do any of this in the first place or can you fit the model to find the learning rate you know that that would be probably a more balanced perspective um but in any case thanks everyone for",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3344.352,
    "end": 3358.733,
    "text": " Justin Cappos- joining today really appreciate it, and so a week from today, the same time will be cohort eight we'll be talking about Chapter three I believe the high road back of inference and then.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3358.831,
    "end": 3377.413,
    "text": " two weeks from today at this time we'll move on to chapter nine as I was just mentioning which will get much more into things like model fitting and different kinds of analysis and group analysis and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3377.748,
    "end": 3406.168,
    "text": " wrap up with models of false inference and that's very much where the kind of computational psychiatry line goes but many others can a robot be pathological that kind of question phrased exactly like that is ever more strongly on the minds of many today um so interesting ways of thinking about things but um yeah that's that's all for today folks so thank you very much",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3407.11,
    "end": 3407.756,
    "text": " Thank you, Andrew.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3407.776,
    "end": 3408.827,
    "text": "Thanks, Isabella and John.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3409.433,
    "end": 3409.695,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  }
]
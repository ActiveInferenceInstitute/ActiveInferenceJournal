SPEAKER_02:
Okay, welcome back, Cohort 7.

We're discussing Chapter 4, so anyone can bring up any comment or anything they remembered or a question from 4 or some other comment from earlier on, or we'll look at the chapter or look at questions.

But first, does anyone have a comment or just anything else they want to kick it off with?


SPEAKER_01:
Thank you.


SPEAKER_02:
okay type in the chat or raise your hand or anything um chapter four has two roads leading into it with chapter two and three so this is really where we see active inference generative models after the more general context of one and then the low and the high road in in chapters two and three um

Anyone, like, is there a part of four that anyone wants to go to or just a quote that's interesting just to begin with?


SPEAKER_01:
Okay.


SPEAKER_02:
4.1.

single paragraph describes in very concise kind of LLM-like style what the next sections are going to be.

Section 4.2 goes from

Bayes equation, Bayes theorem, exact Bayes, which is exactly how you update the prior into the posterior given an observation.

That's a calculation that is done like in one fell swoop.

idea of using some bound on exact bayesian inference so the idea of doing approximate bayesian inference using some other proxy not surprise exactly but a bound on surprise like variational free energy the idea here is that that bound could be picked to have certain properties like be able to optimize it incrementally with gradient descent like in machine learning

Like the title of the section, they do start with Bayes' theorem.

Though this chapter also says that it will present the active inference generative models, and it does, there are a few kind of background detours or interludes, like Jensen's inequality,

towards equation 4.2 uh they use the definition of surprise as log likelihood and then through the usage of that log or just any function that has like a decreasing uh rate of decrease of the slope but never turning back negative um

the log surprisal allows like a um strict bound to be described which is called the free energy or negative free energy like just depending on the negative sign

um okay jonathan shock a previous uh participant in a math teacher i think professor has written this document let's just see it for a second some of these it's like

okay I think this date is automatically chosen but that would be interesting if he updated it so recently let's look through this while there are some superb explanations in the book there are important details left out in the derivations which may not be that important on the first pass but I think make things much clearer when you want to understand it in detail

Our general aim will be to observe things that we expect to observe.

We will come on later to the things that we want to observe when we think about preferences.

But for now, we can just think about the things that our model predicts that we are likely to see.

Here's the part with Jensen's inequality.

So exact base, 2.1, just like in the book.

Jensen's inequality.

This is going into more lines of detail from what is in the book.

This gives some lines to follow.

This bottom line here

Here's surprisal, log likelihood.

This is the likelihood of that observation, and then log likelihood for a variety of reasons, to get some zero one interval properties, to get that boundedness, to be able to compare multiple, like orders of magnitude of probability,

So this is whatever it is.

Here's the KL divergence between the distribution that you can control your surprisal from and the empirical distribution peak.

um chapter 2.5 or chapter 2 equation 2.5 had the variational free energy and there's multiple lines it seems like he's going into more detail here connecting how free energy is described in chapter two and and then expected free energy in equation 2.6 and then connecting it more to chapter four

so that's that's also kind of a background math like a standalone math thing any ideas or questions on 4.1 or 4.2


SPEAKER_01:
Okay, so 4.1, again, single paragraph.


SPEAKER_02:
Yeah, Jeff, go for it.


SPEAKER_03:
Yeah, no, I was recently reading a paper and actually talking to Hector Dean and Magdalena about how the decision making process, you know, binary tree branching, it doesn't scale

It needs to be bounded, right?

Like we can only make one decision at one point in time, but we're limited by our, you know, by the body that we inhabit and our signal processing.


SPEAKER_01:
Okay.


SPEAKER_03:
when you have when you have systems of systems who are you know a scale free right uh nested agents right who are interdependent their their loss and compression there in order to isolate the signal from the noise

and also have a pruning function for that decision tree.


SPEAKER_01:
That's just where my mind is right now.

Thanks, Jeff.


SPEAKER_02:
Yeah, these are good.

you point to these kind of constraints on the inbound with the signal processing, and then the constraints on the outbound, I mean, the ultimate constraint on the outbound, one embodied body being able to only do one posture at once.

So it's this tension between, okay, you could plan out all these multi-joint actions, a couple of joints, a couple of affordances, a couple of time steps, that could be like

all the atoms of the universe.


SPEAKER_03:
And then so it's, it's, it's really interesting because Seth Lloyd and program in the universe, which she wrote back in 2006, he shows very clearly that like, you know, a game board, not big, not much bigger than a, than a go set has more potential states than all the, all the matter and energy in the universe.

Right.


SPEAKER_02:
so like you know in one run of the universe you will not explore all the potential states that could possibly exist yeah and i i don't know about the branching uh like multiverse but sometimes it's like that's just like saying it's kind of acceptance of that

whether that's an epistemically strong stance or whether that's a cope, but it's just like saying, yeah, that much branching really does happen.

It's like, okay, but it doesn't really get around that like chessboard explosion.

And then, I mean, that just is essentially an empirical validation that cognitive heuristics are used for sense-making and decision-making.


SPEAKER_03:
Even when you model very small sequential domains, it becomes very unwieldy, very quick as you scale the board.


SPEAKER_01:
One active connection there is this...


SPEAKER_02:
branching time, active inference direction, and maybe a little bit of more recent work.

This is drawing methods.

from dealing with these tree and combinatoric explosions whether that's whether you think about that in the combinatoric explosions of interpretations on the inbound or like action select joint action planning uh or time or plan that kind of tree search has been a classic problem like chess algorithms and and deep blue and beyond


SPEAKER_03:
Yeah, and AlphaGo and AlphaStar and whatnot, they've solved these for a specific domain.

Exactly.


SPEAKER_02:
And broadly, some of those approaches are Monte Carlo based.

Monte Carlo is like a city where gambling happens.

So these are the gambling sampling based approaches.

And then an alternative complementary approach, like some problems where both, neither one or the other do well, et cetera.

But another approach is to cast that as a variational inference or as a Bayesian inference problem, and then use variational methods to approximate the posterior, whereas Monte Carlo samples.

And Lance da Costa has done a lot of work on the sampling and the variational.

uh approaches but that that's uh yeah a few other random thoughts like it's it's easy to make a cognitive behavioral model that's like

it's just like maps that don't map to territory as well for one reason or another.

I mean, you could make it wildly overpowered.

Like you could have it be a mouse foraging model with a gigabyte of short-term memory of the GPS location.

It's like the model won't complain that mice don't actually remember the GPS coordinates.

So taking into account those constraints,

is often like the hard or a hard part of empirical modeling whereas if one is just more interested in like a looser like bio-inspired approach and then is seeking performance or some other attribute just loosely based upon biology then of course there's there's nothing holding back

but to actually connect and make useful predictions for a given like biological system then how how um that gets represented becomes challenging because it isn't just about making like the highest performance model because it's more about fitting to empirical data

There's many ways in practice to deal with this, ranging from coarse-graining data to picking simpler variational distributions using different kinds of model architectures so that there aren't like wide computations like the all by all by all, like you just figure out other ways

to structure the problem so that the computational properties are not so disadvantageous.

If it really even is a problem at that scale, there can be major heuristics like,

If the input data were 4K video, I mean, one approach you could take that in as a huge matrix with like millions of numbers.

And then in that case, maybe it is hard to run on.

But then maybe another approach like passes the video frame to an LLM and then gets back an estimate on which items are in it and then takes in that list of items.

So there's a lot of ways to deal with it.

And also just to study, okay, how does the runtime change as a function of like the size of the input or the number of sensors?

Like, is it linear as adding more sensors or sublinear or is it super linear?

4.2 is another background, but a really important intuition figure.

This sets us up to look at the Bayes graphs that will be like the POMDP and represent the perception and action components.

But happily, this is not unlike a systems dynamic or systems modeling

or just kind of like mind map or flow chart representation.

I mean, just talking about day-to-day situations, like there was a lightning strike and then there was light and sound.

I mean, maybe somebody wouldn't draw the two and the three boxes, just draw X directly connected to Y and to W, but topologically, this is pretty much

what is is conversationally described when people are talking about um observed and unobserved phenomena any random thoughts on that or or like just things people seeing this or about these base graphs overall I saw the muzzle muzzle flash and then I saw the impact

that's similar yeah here that's like this one yeah that same idea that okay nodes are going to represent distributions

being between zero and one, like probability distributions, or just random variables, like it could be a number between one and 10, or it could be just different kinds of support for those variables, different values that they could take on.

So it's kind of just like a variable in a program.

um that same idea that variables are declared and then their their connections and influences are represented with edges and some of these are observables the o's meaning that they're kind of taken even if they're understood to be a noisy sensor still the observation is taken as it is

um and then unobserved variables like the latent states and then the the sort of connection between signal processing on the bottom and control theory on the top and then that work is done with the b matrix with three

which is the probability function that maps the hidden state like the real temperature in the room at the next time point conditioned upon the current time point and the current behavior so this is a uh action conditions Markov transition Matrix

S is like a vector.

It could be like one temperature data point in the room, or it could be like a vector of thermometers.

this b is like a stack of index cards each card is s by s dimensions it just describes from and two so if you don't have any affordances or if there's just one affordance like this the just this downstairs part of the system is unfolding with with no action selection then it's just like from and two and that's like a markov transition matrix

And then policy selection is just selecting which of those index cards is going to be used for picking the transition to the next time point.

And then kind of a simpler but analogous role is played by the A matrix, which can be used to go from a given hidden state

to a distribution over outcomes.

So it's like, I know that my thermometer has plus or minus one measurement noise.

So if it were really 81, then I would expect this distribution between 80 and 82.

That's using it in the forward direction, like generating synthetic data.

And then also there's taking in a point measurement and then updating your prior

to what your best belief is now not shown here but that's basically where attention comes into play if you are not attending to the observation then it has no impact if you have complete attention to the observation then it overrules your prior and you update your belief to the observation

So that's like the precision or the kind of sharpness on A and how it relates to the strength of the product.

Figure 4.3 shows on the top the discrete time setting and on the bottom, the continuous time setting.

The kind of...

structural similarity is highlighted like with using the same box numbering for the action selection state transition and the sense making component one one three and two boxes at the same time different notation letters are used like o for the observations industry time and y um for continuous time

highlighting structural similarity with generative models, whether they used discrete time or continuous time, while also highlighting their differences.

And the major difference is that in discrete time, discrete specific time steps are considered, like S of T, T plus one, T plus two, for a given horizon.

Whereas in the continuous time setting, which is less about like transitions between states, discrete states, and more about like a continuous differentiable line passing through the present, this is more analogous to the Taylor series expansion.

So there are not actually future time points specifically predicted,

The only specific evaluation of the function is at the current time point, and then the past and the future get fit with the Taylor series.

Being able to fit further and further out

with more and more approximations.

So plus here for one low cost, you do have a prediction for every past and future data point.

The downside is it's hard to even know how relevant

that prediction is so like here we take the first uh we take the first pass which is just evaluating it at the current moment that's just this point then you take the first derivative that point and you get the red line so it's like okay so it's going up in the in the right and it's going down to the left

But then let's say you expand your approximation and now you're in the third powering.

But even though in the area around zero, it still has the same slope.

But now there's a totally different story with it going off to infinity quickly to the left and then down to the right.

And then go into the fourth powering.

And maybe it's a different story again.

So even though, again, it's all being guaranteed to get more and more accurate in the neighborhood, but knowing, even getting a precision on, well, how close is my prediction to the real one?

That can be challenging to know.

then it is an empirical question like how good of approximation is this taylor series expansion this gets revisited in chapter seven and eight so i mean this is kind of the main generative model description of the book since two and three get us to the generative models and six is about how to

design them, and five is about some specific ones that are used, and then nine about getting connected with the data, this figure, and then chapter seven and eight, which it sort of anticipates, are the main generative model descriptions.

So this is like a micro chapter seven.

Yeah, go for it, Carlos.


SPEAKER_00:
Yeah, so that applies also to the action of doing nothing, right?

Among the different potential policies, one is always doing nothing, right?

So that if I do nothing, for instance, it doesn't matter if it's in the continuous or the discrete approximation, you can refine your inference either by making continuous observations by doing nothing, even if you don't interact with the system.

Right or or the system itself could could without doing anything from your side could evolve and they naturally evolve so that applies also to that policy to the policy of doing nothing right is this correct.


SPEAKER_02:
yeah great great comment like um how is doing nothing dealt with.

So sometimes, let's just say we're in a grid world movement.

Sometimes do nothing is incorporated as an affordance.

So there's like up, down, left, right, stay.

Another approach would be higher level, stay or move.

then lower level up down left right so those either those could be used um and i mean you you point to a really important question which is the control theory and game theory it's often just framed like it's obvious when action should be taken and so in the turn-taking game like chess

You could talk about like, well, you know, when should this pawn, you know, advance or something like that.

But that's within the game time.

It's an alternating turn taking.

But then when there's a system that's changing external, then like, I mean, like an economic market, it's like timing the market.

there might be way broader changes that your actions are less influential on such that for achieving like pragmatic or epistemic value, it almost matters less which action is done, but more about like when it's done.

So that's like kind of another flavor of control theoretic question.

but it's not really so easy to address in the turn-taking game setting.

But it is more kind of amenable to ecosystems where things are really happening.


SPEAKER_00:
Yeah.

More dynamic ones in which, you know, you cannot infer the state just by one observation.

You need to wait for a number of observations that come along in order to be able to make a good inference of the current state.


SPEAKER_02:
Yeah, I mean, that's, that's even another angle, which is like, let's just say that measurements cost us one, like there's just a cost per measurement.

Um, so we have a budget.

so we can't just you know get a million measurements in a split second but then also there's like a rate of change of the underlying system this is like buying stock Market trading data or like planning and experimental data so then there it's this assessment of like well for deciding on what to do that's like the actionable side what is the trade-off of like which measurements and when to make those measurements yeah

Some of these would be really interesting to kind of sketch out and just show how different heuristics would lead to different kind of biases in the sense-making and decision-making.

If you have a certain number of credits for how much measurements you can make,

well if you immediately pursue the highest precision then you're initially even if it is initially accurate then that assessment would become inaccurate if you wait until the very end you cross the finish line like with the highest accuracy but then for for most of the time you had the least of an idea well if you if you space it out then like you may have a lower level but but above nothing

So then like that kind of comes back to this constraints topic and how the interesting pieces about how do you model those constraints?

Because that's where the decision, like the kind of wide open space, that's why it's so hard to predict what an E. coli, I mean, or a mouse will do.

Yeah, but yeah, definitely about doing nothing is as that's that.

Or I'm just thinking of in like an agent based model, sometimes there even is no stay option.

so then there's like a lot of movement just moving between two squares back and forth like if they're already on kind of a local Maxima or something but it's like but that that kind of behavior isn't really seen but that just because there isn't a stay option but just like walking around like the bird is just like sitting on the tree not like spending not resting by hopping back and forth

that's like a that's a that's a huge difference but that's just about having that's just about the structure of the model and how it's being fit yeah understood thanks yeah also it highlights how like free energy is very model specific so just whether thinking of the the perceptual

variational free energy calculation, like just the sense-making from noisy and partial data, or the action selection as an inference problem, that free energy calculation is based upon the data, the family of variational distributions, the generative model.

then that it yes that's an instrumentalist perspective but it highlights why like well what's the free energy of that squirrel why is it sitting on the ground like that it's like you're quite a ways off but if you described okay the data or the squirrel's location the variational distributions the gaussian that i'm making and the prior and the likelihood on location for the squirrel

Here's why it is or isn't surprising to me why my model is returning a higher low free energy for the squirrel being there.

But that's more of just describing your statistical model, but just a sort of qualitative description of free energy as if it was just naturally an unambiguous part of a biological system's kind of a stretch.


SPEAKER_03:
I'm reminded of the halting problem.

Like how?

Like you can, you know, write endless loops.

Kind of, kind of reminds me of, uh, you know, Hofstetter too, you know, good Lesher and Bach and, uh, uh, I am a strange loop.


SPEAKER_02:
while at uh I'm gonna welcome back to the this was uh uh in this this workshop over the previous days Chris Fields gave this talk I'll put it in our in our uh page and I'll put it in the chat and it just sort of there's it was just some interesting discussions like well you know never step in the same River twice we're we're never the same

we're we're whether you take that from a material like breathing co2 and oxygen on food um or just information and like planning and stuff like that it's like we're never the same so then what this part was really interesting

the neural like the i don't know if this comes from the isochade or from the motor like analogy function or something like that but the embodied math and and just the the way that the category theory and i think other equations get um interpreted

And then it's like, so then what is the same?

It's like, well, the identity is what is the sameness.

But then it's like, but then your identity when you're 15, when you're 30, that is different.

So it's just like, I don't think his talk answers it per se, but it gives some interesting stick drawings to begin with.

exploring like that sameness and differentness which is one of the most kind of pervasive cross-modal aspects of experience yet at the same time it's it's really not so simple when the our identity materially and cognitively and metacognitively is changing but it does have a sameness too

And he mentions Act-Inf and connects a little bit to the, he doesn't mention it, but the physics is information processing.

But just showing like the sort of usage of a variational free energy or something like that.

for like a phenomenological human state this is just prompted by the the girdle Escherbach it's like the whole point is that's a strange system it can't just be taken for granted like even what direction what layer of what was it that's why there isn't like an open source kernel for for the human minds it just

It seems to be so baffling and strange.

Again, revisiting equation 2.5 and 2.6.

Variational free energy, F of Q and Y,

it's with a belief distribution and a new data point so it's just like a real time um real time through time minimization it can include it could be a sense making only or it can include like instantaneous action inference so without explicit planning

single time step updating of beliefs including beliefs about action whereas if you want to get into explicit planning then expected free energy is used this is a lot like equation 2.6 still in the discrete time section this is just going through more details of stuff that's going to come back in chapter seven

4.5 continuous time, box, a blanketed off box on Markov blanket with its definition, upstream causes of X, downstream effects of X, and the co-parents of the children of X.

After that short definition, immediately going into these two methods, kind of like network propagation or percolation methods for having messages passed among nodes that implement this variational gradient descent.

Here, it's shown with kind of another notation set.

The left is showing a sort of classical predictive processing hierarchical model.

So observations through time are coming in, O tilde.

Each of these four that look closer together,

are one layer of the hierarchical predictive processing hierarchy.

At each level, there's a descending prediction that's getting contrasted with the ascending observation, and then that is getting turned into an error residual.

It's getting passed back up to a higher level.

So here's...

like primary sensory processing level, another nested level.

And then I think just the error unit for like an overall.

So this is like measurement coming in.

This is like the ones place.

This is like the tens place.

And then this is just like green light, yellow light, or red light.

How accurate is the tens place prediction?

That's a snapshot or kind of through time emphasizing the hierarchical predictive processing architecture.

Whether or how the brain does it is secondary to here.

It's just, this is a modeling motif that's used in the Bayesian systems.

Here on the right,

is looking more through time so here just like in the top of figure 4.3 it's observations coming at the current time point and the past and the present uh the past and the future time point and then just showing a little bit of like kind of the information flow of how those updates get passed um in order to update beliefs about policy

including like and updating hidden states conditioned on policies it it says it omits some intermediate connection so it's like not exactly sure um how much to look into the edge connections but that it is fully described but here they they don't show all the edges

Here, now we're kind of back into this continuous time case.

This is a really important set of equations.

The SPM, statistical parametric mapping, or like anything involving neuroimaging, this is kind of a classic pair, as well as in physics.

So in neuroimaging, this is the observations coming in with like a signal and a noise.

Could be a sensor noise.

And then x dot is like a spatialized

flow landscape of an underlying state like neural activity.

So the measurement of fMRI might be like just the intensity of the signal coming from a given voxel at a time.

That's the observation.

which is due to underlying like blood flow changes and also noise.

And then there's this underlying neural activity landscape, which is x dot.

So here's a situation where they're both normal distributions.

4.2, generalized coordinates of motion is the Taylor series.

You can take the Taylor series or generalized coordinate motions on the observation and on the latent state x dot.

So these are derivatives, first, second, et cetera, derivatives.

And again, in the kind of classic hierarchical predictive processing scheme that the precision comes into play.

Laplace approximation, free energy may be approximated by a quadratic expansion around the posterior mode.

So the mode is the most common, like the highest point on the probability distribution.

The median is the 50th percentile data point.

The mean is like the average value.

The mode is just picking the highest point, which is like the single maximum likelihood estimate.

And then the Laplace approximation is taking to the second power around that point.

So you have a quadratic curvature and then fitting that quadratic distribution.

So if, if, if it's not a, if there's not a central tendency, then of course, this will still only be able to fit a quadratic.

So if it's like three humps, then you're either going to have a really over dispersed quadratic or have one that just fits over one, but it's an approximation.

now it's again the hierarchical and the through time but instead of the o's it's the y's so now it's kind of swapped into the notation set of the continuous time that's chapter four

five is gonna gonna do some reviews on neural systems which is where these models have been applied most but yeah four is where the degenerative models at least the simple motifs for discrete and continuous time aren't shown and there's there's other motifs and patterns like hierarchical and things like that but um

they highlight the difference and the similarities between the continuous and discrete time and that's like a big structuring element of the book with chapter seven and eight


SPEAKER_00:
Daniel, just a question.

I haven't seen yet any real world problem that has been using this type of hierarchical model.

Everything I've seen so far is flat.

Can you give me some examples or references, links to works that use hierarchical model, please?


SPEAKER_02:
Yeah, good, good question for forum.

This robot navigation paper describes.

Okay, it's not an open access paper.

here's their hierarchical model so this is one setting in a multi-scale movement like locations in a warehouse and then robot body postures that's that's like one where the higher and lower level are both um like mechanical and positional um

the hierarchical hybrid model with continuous on the lower level and discrete on the top is kind of the folk psychological template that's that's that um in this smith at all paper i mean they argue this is basically the um implicit or explicit

generative model when people talk about, in a folk psychological sense, let's see if they have any.

I don't know if they have the visualization.

And then I think, okay, those are a few.

In the implementations, I agree, they're usually single layer,

uh it could be a range of things ranging from advanced models um happening uh proprietarily another option is that a model like um for example alex aurorbia's work and uh these kind of the neural uh generative coding this this does make hierarchical models

So there's a flatness because it's like a neural topology, a neuromorphic system, but then the calculation it implements could still be hierarchical.

And I think another interesting direction to explore

Danajar Hafner, Dream to Control.

Information gain in the planning objective.

Carl Fursten mentioned sometimes dreamers like some variation of active inference.

I think the textbook clearly

shows the model doesn't have to be hierarchical like architecturally or functionally for it to be active inference even for for perhaps interesting or multi-scale behavior to arise definitely showing more clear nested models that will be interesting and understanding like okay if you have to do 100 time steps let's see the 100 time step planner the 10 time step planner

the decade and then the ones place planner, the no planner, the single times, like kind of creating all of those alternative ways to parse the problem, I think would be very informative.

Because there's probably niches and inputs and sense-making and action problems for which like, but there's no free lunch.

But on some data set, each of those you could imagine could be effective or not.

But for real world settings, it's definitely an empirical question.

how because if it's just about like well the world the systems are hierarchical or they're nested but then that's just getting into the the map being the territory um has anybody ever framed it in such a way of the so reinforcement learning and deep learning


SPEAKER_03:
right we're getting these massive data sets and we're trying to figure out how to make an agent move about the world and it feels to me like the framing of active active inference as a way for an agent to move the world around it almost like comes to mind

instead of moving moving around a bounded environment right the the the agent is like moving the whole like the agent is the central foci of existence i i don't know what's been done but sometimes that's called like egocentric navigation


SPEAKER_02:
where the reference point is, where it's like in the grid ruled examples that we look at in PyMDP, the agent's like two comma two, but it's like, but how'd you know you were in a three by three maze?

Whereas for an open space, then the egocentric navigation, and then it also connects like to this, to the action and non-action earlier, because it's like, I mean, one heuristic of movement and plausibly something that is done for movement or like being in the car, it's like, well, you could think about that the world and its causes are fixed and then you're moving over that, or...

it's like being on a treadmill it's like yeah the freeway is like passing forward so then from the egocentric frame that's why it feels stationary even like being on a train but then from an allocentric frame you might think about yourself moving positionally across a fixed space so and again that goes to like making these animal behavioral models that are just like wildly off base

like people will for example set up an L-shaped wall and then they'll test okay well do animals have an internal space representation like will they take the hypotenuse because it's shorter to get to the food or or will they just follow the edge but it's like but the animal may not even have the vision to see across the diagonal or they may just have a habit to stay up against a wall because it's safer so it's like you

but so then having made a made a um allocentric model where it's like okay it's going to calculate a hypotenuse and then it's going to minimize the path distance it's like it's just has nothing to do with the heuristics of the animal so then then then what whatever the p value you get there is it's just kind of like it's a model output but it doesn't help address like an interesting question about the animal in in such a clean way


SPEAKER_03:
You know, it'd be really interesting to explore this in terms of the interior changes that occur in the predictive models of animals that undergo metamorphosis, like, you know, a caterpillar to a butterfly, or

the uh spawn of um barnacles right like they start as a uh mobile little you know and they can swim around the ocean and then they fix onto something and then like they're fixed and the world then is anchored for them yeah also like the i mean the multi-generation like monarch butterfly


SPEAKER_02:
are they using a are they just you could imagine all of the you could put a gps tracker etc and think of them moving through space or it could be a pure heuristic based upon like light and magnetism and seasonality all these things that doesn't require planning but then back again back to this combinatorics of planning it's like well if it's combinatorics of every muscle twitch it's like well if that's how if if if

know that's maybe not what is happening but this is this is the other map territory what and the the butterfly is able to do that but but can't necessarily emulate chess so even just getting the trick of the turn complete computer to get constraints and capacities that even resemble the biological system is totally not a given

And the model won't throw up a flag.

Again, if you give the butterfly a GPS coordinate and Bluetooth communication with the rest, it's like you can make that model, but it doesn't have the biological plausibility.


SPEAKER_01:
Okay.

Thank you.

See you next time.


SPEAKER_00:
Thanks.

Bye.
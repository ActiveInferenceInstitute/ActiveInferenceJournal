SPEAKER_02:
okay it's september 16 24 and andrew is going to give an overview of chapter four then we'll see what questions we have slash other questions already written so go for it andrew thanks daniel uh yeah so again this is chapter four the active inference uh 2022 textbook uh we're working with cohort seven so it's the first week of going through chapter four so i figured i'd give


SPEAKER_00:
an overview of the chapter.

We're already told off the bat in section 4.1, the introduction, that we're going to be introduced to the relationship here between free energy and Bayesian inference, the form of typical active inference generative models that we often see in active inference implementations.

And we'll also look at the dynamics obtained from minimizing these models free energy.

So specifically, we look here at both discrete and continuous time models, as well as the neuro biologically motivated idea of inferential message passing underlying the computations and predictive coding involved in active inference.

So again, this is the first half of the textbook.

So this is much more theory rather than direct implementation.

However, chapter four is particularly mathematically involved in comparison to the prior three chapters.

Of course, Bayes' rule figures very strongly here.

So we see that in section 4.2 from Bayesian inference to free energy.

um so we recall that we're working with approximate inference rather than exact bayesian optimality given the problem of tractability or intractability as well as neurobiological grounding of limited resources so the idea being that for biologically plausible models we have to actually consider any kind of

computational limitation of resources for metabolic and other biological limitations on the processes that proceed during free energy minimization.

So that's the kind of matching between biological plausibility and why we're looking at approximate rather than exact free energy minimization.

or rather surprise minimization.

The mathematics involved here are drawn from linear algebra, differentiation and calculus, and the Taylor series expansion.

The textbook appendices give further details on the relationships between these fields, so I strongly recommend looking at those if you are still kind of learning the maths involved.

We're shown in equation 4.1, Bayes' rule.

uh so here this this helps us to better understand a generative model in active inference uh which is a generative model can be described as uh here we can see it as like the probability of x times the probability of y conditioned on x and then we can flip those around on the other side of the equal sign uh and so that that gives us a way of understanding like

we can use Bayes' rule to potentially update any aspect or any component of Bayes' rule whenever something new is updated and just kind of sort of plug and chug for the rest, so to speak, right?

So that means whenever we have like new observations,

uh come in so our model our our active inference agent observes a new observation we can update its posterior beliefs about states so um part of how we're able to do this also this is kind of a key aspect to understanding what free energy

is relative to surprise and a lot of the other theoretical terms that were introduced to jensen's inequality

renders optimization here possible since minimizing surprise itself would be next to impossible it's intractable whenever it comes to like very complex models uh integrals that that things become rather difficult to keep track of or there are limited computational resources whenever doing the kind of computations involved with numerous or highly complex matrices um we we can use jensen's inequality which is

This idea that the log of an average is always greater than or equal to the average of a log.

It's, you know, I don't want to call it a trick, but in a sense, it's what allows us to say, oh, this tractable

variational quantity we call free energy is always greater than or equal to surprise.

You can see how that math is played out in equation 4.2 on page 65.

So the idea is that free energy becomes this tractable quantity that we can calculate

uh it it becomes an upper bound on surprise so well if it's the case that we can't perfectly compute surprise in all cases we can compute free energy and so if we know that we can reduce that free energy as much as possible uh it's it's going to close in that that bound that that bound is going to move further and further down closer to zero such that you know if we can ideally we'd want free energy to be zero and hopefully that's a good approximation of

surprise bearing also being zero it necessarily would have to be so um in equation 4.3 we're also introduced to kind of a broader um way as well as equation 4.4 uh how to actually compute free energy

And so we can see that that's a combination of two terms, a KL divergence or Kovac-Leibler divergence, which is a way of kind of quantifying the difference between two distributions.

How different are they from one another?

And so that's why the divergence term has these two different terms within it, right?

It's the difference between our variational posterior overstates, Q of X, the difference between that

and the probability of X conditioned on Y. So it's a way of kind of finding the difference.

If we can minimize that as much as possible, then all we're left with is negative log model evidence, which itself is kind of a proxy for surprise.

So if we can minimize the divergence as much as possible, then we come very close to also minimizing surprise.

Section 4.3, I'm just going to make a couple jumps here because it's a rather broad chapter.

Section 4.3, we're introduced to factor graph notation for static perception models first, which are kind of like a precursor to eventually getting into active inference.

uh in static perception models where we're just dealing with perception all that active inference is a combination of action uh perception and action you change your mind or you change the world and so without action we're just left with this kind of static perception right so it's this kind of model that updates based on new data but it doesn't itself act it doesn't do anything to change the world and and therefore it also lacks many other components uh in its architecture that you would find in a whole

active inference model.

So it gives us examples as to how prior and likelihood the core components of a generative model are involved in the model computation.

We're kind of explained like, in terms of like binary classification, how we can look at a static perception model through positive and true negative rates, folks who are familiar with the kind of machine learning.

Not even necessarily machine learning quite, but that kind of logic is very much at play there.

And then in Figure 4.3, we're then introduced to active inference models.

Specifically in Figure 4.3, there are discrete and continuous models where a temporal dimension has also been added, where states are conditionally dependent on the state at the prior step.

That is, the model carries its own beliefs about how states transition over time.

So here, actions or policies, policies are considered to be sequences of individual actions, are introduced.

And as I mentioned, state transitions are introduced.

The agent's actions derived from their policies impact the environment and observations they receive over time.

as we recall, agents are self-evidencing.

So what they will do is that they have a belief about how to go from state to state via acting.

And then by acting, they're going to choose actions that hopefully will elicit the observations they want to see.

And we've already been introduced to the idea of preferences, just like a prior over observations.

So the idea is you expect

to see or not necessarily see, but observe particular observations.

And so you act in order to eventually make those happen.

So if someone is hungry and they want food or they expect food, then they act in order to obtain it and eat it and so on.

So it's one of the most interesting

kind of aspects to the framework of academic inference is the self-evidencing, but it in fact plays out in the model logic as we look at it here in this chapter.

So to get further into those components, section 4.4 goes more into

Chris Wanner, specifically discrete time models or we're looking at specifically partially observable markup decision making processes, where, as we know from previous chapters, the agent has no direct no direct access to true states of the world and can only infer them.

Chris Wanner, So the agent will attempt to infer them.

through free energy minimization over its various model components.

So in a POMDP, in this case, we can map the components we've covered previously to components of the POMDP.

A is our likelihood model, which we've already seen in the equations describing a generative model.

B is our transitions model.

I already touched on that.

So it's a distribution over what is the next state going to be given, what the current state is, as well as an action that the agent can take now.

Basically, what is going on now and what should I do in order to get to a more preferable state?

And then D is our vector of priors over states, so what is the agent belief at the onset that's sort of like the initial belief that's kind of right for being updated.

And then, as I already mentioned earlier, we have preferences for the C vector over expected observations.

We're also introduced to G as a quantity to be minimized.

It's not exactly equivalent to variational free energy, which is often denoted F because G is kind of like the free energy of the future or expected free energy.

And that's where it's within that competition that C actually gets directly included.

So it's kind of like it allows agents capacity for planning.

That is, they actually incorporate what they want to see into

uh kind of their policies and ways of inferring how they should act uh it's really in g that that happens so we can think of g is as like kind of planning towards the future meanwhile f variational free energy is minimized sort of moment by moment you know we can also further decompose g so as we saw earlier f to be

uh decomposed into like a divergence term and a negative log model evidence term so with g this can be broken down into actually many ways uh one of them is info gain or information gain or epistemic value all those are equivalent and pragmatic value and these are probably this is one of the most uh interesting to me personally one of the most interesting aspects that active inference which is that

GE takes account of not just how can an agent realize its goals in terms of pragmatic value, which can also kind of be likened to the word utility as it's used in economics or game theory, but also information gain, which is an agent actually will act to reduce

uncertainty so part of that involves engaging in exploratory behaviors where the agent isn't necessarily trying to immediately realize a goal it has instead it might be seeking more information about say the environment that it's in you know so it's kind of like if you if you plan to to fulfill a project

like uh you know you could sit down and try and do the project but if you're not sure what kind of research you need to do yet uh and you skip that that those preliminary stages of seeking more info in order to realize your goal of finishing the project then you might end up

creating a very subpar project, right?

And so all of that plays into a kind of classic in reinforcement learning and other domains, like how do you balance or even define exploratory behavior with exploitative behavior?

exploitative meaning directly realizing goals which is also relatable to that pragmatic value term that we saw expected free energy so the idea of expected free energy part as an answer to that kind of explore exploit dilemma is that you the idea is you can have both you can have both of those included and be a

uh expected free energy minimization the agent kind of naturally balances those two things it doesn't mean that they're always going to be literally equivalent it doesn't mean like 0.5 for one and 0.5 for the other it just means that uh the way that this is commonly done in say reinforcement learning is that the programmer has to choose some way of defining what exploratory behavior is

And more often than not, it'll be through some kind of arbitrarily chosen rule, such as epsilon greedy behavior, which is where you say, OK, the agent will try to fulfill its goal with a probability of 0.9.

It'll pick an action that it believes best to fulfill its goal and realize that pragmatic value of 0.9.

But with a probability of 0.1, it'll just do something random, right?

And then that becomes equated to exploratory behavior.

Active inference is very different because you don't have to do that at all.

The modeler doesn't have to make a definition of what exploratory behavior is.

It's the agent who autonomously decides what is exploratory behavior.

And it's not necessarily doing it randomly either.

That's another crucial aspect.

forgive me if that's a lot, but I'm personally interested in that side of active inference.

And I think it's very valuable in applying active inference to other fields that more typically do try to resolve that explore-exploit dilemma, but don't always necessarily have an answer for how to go about it.

To wrap up,

some other topics that were introduced to or at least uh that are discussed or brought up in this chapter are uh markov blankets separating the agent from its environment or a system from uh as mark stone says uh the not system uh right so it's a it's a way of finding statistical independencies such that you can actually define a thing and kind of separate it out

from the environment around it, which is what allows us to kind of delineate, like, here's where free energy is happening.

It's happening within this agent and not, you know, it's internal to this particular system, right?

We're also, some other topics include generalized coordinates of motion.

um that'll come up much more in chapter eight future so i won't go into it but uh that's used more expressly for continuous time models and that's sort of how the free energy uh computation plays out so it a lot of it just relies upon ideas like taylor series expansions right so as you as you add more terms you can potentially become increasingly more accurate at kind of predicting

what direction certain quantities will move going forward.

And that's what allows for the agent to start to making these kind of continuous time predictions and then update its beliefs accordingly.

Bayesian message passing message passing, which which will figure much more in Chapter five.

but we just kind of get hints of it right now, this idea of passing messages along cortical layers, which can be represented in a broad variety of ways.

You can represent it more distinctly as like these are cortical microcircuits, or you can represent them as more simply like nodes and edges, like on a factor graph and imagine it like a message passing kind of network.

the active inference institute we we're working with folks at bias labs for example who have a very interesting way of of representing uh message passing on factor graphs or what they call constrained foreign factor graphs um it's a really nice kind of uh nomenclature and and and

generally quick way of like, sort of normalizing how we represent these kinds of things, because it allows for further work.

But in any case, message passing, yeah, as messages being passed, different kinds of local computations being formed, passed around, so kind of reminds of communication in the brain, you know, our, our neurons are not all fully connected.

uh we'd presumably be very overloaded if that were the case so we're dealing with these kind of you know sparse networks that uh you know pass messages to each other and kind of certain kinds of distinct ways um that you know at a broader scale allows us to update our beliefs following the active inference framework so um i'll leave things there uh sorry i realized now that was that intro was much longer than it than i

planned but uh i'll open the floor if anyone wants to discuss anything or has any questions or we can go into any particular topics thank you yeah as you pointed out it it's a


SPEAKER_02:
interesting chapter with a mix of both seemingly general mathematical points to make like bayes theorem jensen's inequality as well as putting some details in the appendices and of course even more just implicit for learning but general mathematical details general visualization motifs

and then it's kind of like the sighting of the models that will resurface in seven and eight with an interlude of five being the empirical evidence and studies in the most well-studied setting just by like number of studies the human nervous system then six with a recipe before going too much into the details of these specifically so it's almost like four is a preview

plus a bunch of backgrounds like pmdp again a general long existing um way to model perception and action so it's interesting that four really contains a lot of general math

and learning before and after a sort of preview of where it's actually going to be described in more detail in seven and eight after the recipe is shown and the evidence too.


SPEAKER_00:
Yeah, definitely.

Yeah, this and just this four is a kind of preview for the second half of the textbook.

It's like chapters two and three are also sort of these four runners up to chapter four.

I feel in that chapter two, we have the low road.

And so we can get to active inference from Bayes' rule.

And so, but of course, before we can get there, for anyone coming to this textbook who's interested in

um kind of cognitive computation and neuroscience uh and maybe maybe you have to kind of take a step to say how do we how do we teach people about this from the ground up and how do we get their interest and so so to begin with the bayesian brain hypothesis for example and starting with the base's role i think you really do have to to kind of make that point

and describe it well, but then of course, we're going to have to get to the mathematics about it.

And if we present all of that simultaneously, I don't know how well the rest of the textbook is going to read.

So we do have this kind of back and forth between dropping this into the maths, but pulling us back out, like resurfacing, like, where are we now?

And then chapter three, oh, we go much further above

The surface now we're at the high road and we're starting there with free energy minimization.

So in a certain way, I think the chapter four kind of brings us like, it's like, oh, both roads have now met.

Now we can, now we know that we're starting with phases roll from the bottom and we know that from the top.

And so now we get to get into factor graphs and, and

uh message passive and the actual like derivation of free energy and and why we can justify the computation of this quantity free energy using instance and equality and all these other sort of details um

I think chapter four is great in terms of I think it covers actually so much.

It's sort of like it precursors so many more advanced ideas, but then it also gathers together all the kind of simpler components in the preceding chapters.

And it's tough to find a concise way of summarizing it, though, I will say.

Let's look at any questions or see if any are typed, but let's just see.


SPEAKER_02:
Let's go from the bottom, just if there's new ones.

Okay.

Okay.

Oh, wow.

How are games related to active and generative models?

Okay, that's someone thinking about how to have a playable generative model.

Also another question.

Well, one reading of how games are related would be we could model games like checkers, chess, go, tic-tac-toe has been done, game theory, etc.

Then there's the point that Jana made there about the visualization.

And then another avenue that some people are exploring, like Pablo FM, this was like...

experiential act inf games where we based around ontology terms and and um role playing and then also pablo has made the three 3d adventure of curiosity game any other thoughts on how active relates to games


SPEAKER_00:
Okay, I think, yeah, I think they immensely relate to games and say that really quickly, like, you know, in the same way that because I don't know how generally the person means that question, but I also think of it as in the way that we can try and understand any entity in its environment, whether it be, you know, a neuron within a cortical layer, or

It'd be Mario jumping on enemies and getting mushrooms in a video game.

Like what we're kind of doing is that this all relates to the idea of studying behavior and how things kind of act and behave and how they update themselves internally as well.

So, I mean, that would be my answer.

I think that you could potentially

given the right like model architecture you could have an agent who plays almost any game um i i could easily see that being doable and reinforcement learning more generally already i mean people are using that to to both create games as well as uh you know take existing games and

So that's been done.

And so you can very much do that with active inference as well.

And then the fun part is if you do both other methods in reinforcement learning and then benchmark those and compare those two active inference models, a fun way of doing that.


SPEAKER_02:
So also this made me think about discrete and continuous time nested models.

So it was like baseball the way it used to be played.

You have the inning structure and then the discrete time with the pitches and any amount of time could it collapse between the two events?

And then there's the timed games like continuous time.

And then when more recently they added the shot clock or pitch clock to baseball, then that's like there's a discrete structure of events in at-bats and innings.

But then also it's like at the lowest level, there's a continuous duration.

It's not exactly the same thing, but it's just kind of like you can have discrete nested within discrete, continuous nested within continuous, or the hybrid models that are in chapter eight

And then a common hybrid model for the folk psychology and for the updated baseball timing is the continuous on the proximate level, and then the discrete at a slower level.


SPEAKER_00:
Yeah, that's a nice point.

Like with, with baseball, on the one hand, there can be kind of a real time visual process.

Say you're a batter, for example.

um there can be a sort of continuous real-time processing visualization for you of what the pitcher is going to do what kind of like throw that they're going to do and uh so you might be processing all that in real time but you could almost consider like the classification of oh they're going to pitch it this particular style or this style like

And then you can make that as kind of a more discreet classification choice of like that's what you're going to do more exactly, and so you can have definitely that sort of interplay based on how you do the nesting and the model between these like continuous.

Observation inputs or variables and and making these kind of discreet choices and the other way around it could work as well.


SPEAKER_02:
What would you say?

Does active inference require slash describe?

Let's separate it into two questions.

What would you say for these two questions?

Does it require observable behaviors?

Does it describe observable behaviors?


SPEAKER_00:
That's if we take

the theoretical presuppositions in the first half of the textbook to be true or at minimum entertain them then i would say from one angle no they don't require observable behaviors but then that i mean that's really going to get into like kind of philosophical semantic discussions of like observable technology right um like is the inside

of a rock currently minimizing free energy.

I can't see it.

It's hard for me to even say it's behavior because it doesn't really match up with my typical notion of behavior, which is where I might see something moving or varying over time.

But if you take the free energy principle to, if that's still standing for you, then

Uh, yes, like free energy minimization is occurring, is it occurring in, in many, many different systems around us.

And so that said, if you would like to, if you as a, as an experimenter, a scientist would then like to go on to describe something in terms of active inference by observing its behavior, um,

Then it just depends upon your experimental methodology.

Are you studying a mouse in a teammates experiment?

That more easily is observable in the sense that I think this question is coming from.

Then again, we also observe tons of social human behaviors where it's immensely more difficult

to experiment and see how active inference works there.

But if you take free energy, if you agree with the idea of free energy minimization, then you can start to kind of infer or use that as an axiom, so to speak, and kind of make inferences about how you think the whole society of people behaves or try and explain its behavior based upon that in active inference terms.

And from there, you could attempt to


SPEAKER_02:
model um but anyway i think i'm getting further away from the original question yeah i mean it's it's interesting you're right though does it require observed behaviors i mean if you're observing something every phenotype can be seen as a behavioral you know that's the leg length behavior of that time and scale and system but it's like without the observable it's like okay

know it's it's i'm imagining this creature's femur but it's like but then is that a mental observable so that does get into like can you you know would describing something that wasn't there but then what what are you really trying to what are you really trying to do with that model let's delete this one okay

Both the dot and the prime are used to indicate a derivative.

Equation 417.

Yeah, great question.

Here, the ticks are used for the generalized coordinates of motion.

So that is first, second, third, et cetera, derivative.

That's happening, these two stacks,

for two paired equations, y, which is the primary observables coming in from the imaging, for example, and then x dot, which is the spatialized flow map of the underlying neural states.

So just fitting a one-layer fMRI model, you have the fMRI raw data coming in with the observables on y, that's like the input for the regression,

And then you're fitting a rate of change on neural activity.

So that dot is like a special, just reflecting that you're modeling a sequence of observables and an underlying flow landscape.

And then both of those are being modeled across this applied series of derivatives for the Taylor series.


SPEAKER_00:
Yeah, I think that's also whenever it comes to the more strictly mathematical, I'd rather not add too many.

Yeah, it's hard to know.

Yeah, it's all good.

Okay, as a prime zero, just, you know, more, more primes means you're just expanding the table series expansion further outward, right by adding more.

terms, but then the dot is like the, that rate that you're trying to approximate.

Yeah.


SPEAKER_02:
And it's super, it is very general.

And then it's like, but if you're then, then that's something that Carl has joked about with like, this is like, so we'll say how many layers the brain have or how many layers of temporal depth, then he'll say like five or six or something.

And then say like, just because that's how many cortical layers there are first off.

But, but that's always been interesting to me because the cortical layers are not the hierarchical Bayesian layers.

each cortical columns five or whatever layers are one hierarchical element so i don't understand why he describes it in terms of then the anatomical layers but that being said then just also realistically higher derivatives outside of edge cases may be less important or there's ways to just make them go away which is like why pid control

which only goes to two or three generalized coordinates, is enough for a lot of robotics settings.

Okay.

411.

Some previous times, I know we've discussed the italics and the bold.

Okay.

413.

Okay.

um i'm not sure who wrote this i don't know who wrote it or who could evaluate it right now but


SPEAKER_00:
So, I mean, equation 4.11, that's like a decomposition into a sum over time.

You're going from free energy to like, you have that like F subscript pi tau term, which I believe isn't that referring to...

like here's the evaluated free energy of a particular policy at a particular time step um like basically like it whenever it comes to policy inference because the agents are always like looking at whatever they're doing policy inference they're looking at like they have a planning Horizon for inference Horizon

thus the tau.

And tau is usually used as like, it's used similarly to t, but the reason why it's a tau and not a t is because it's a reminder that this is the time step internal to the model.

So you'd be running a simulation for 50 time steps, but if the agent's current belief is about now, then it's just going to be tau.

It's not going to be t equals 35.

it could be t equals 35 but it'll still be just a house agent then pi being policy so that was my understanding of that bit and then the equivalent for s would be like here's the belief of you know what's going on now is this time uh this is just just to show this is not accurate but then it's like but is it a force


SPEAKER_02:
then it's like is that a super deep understanding slash metaphor where like variational free energy is a force acting on a policy habit or is that just a rabbit hole just an hallucination because it's not grounded in the ontology it's just looking at the latex standalone and so it just picks a a plausible sounding interpretation

yeah i mean i could easily imagine skepticism yeah okay another good math question let's just see if there's any okay let's just why is the gradient of policy of f being zero equivalent having policy which follows the soft max not sure we can think about it more but let's it's all good

But if someone has a great concise way to think about it, then add it.


SPEAKER_00:
I mean, that term on the right was very like that's just policy inference.

That's just I mean, they've not included you could include gamma and E in there, but that's that's just the definition of policy inference and where you're including expected and free energy minimization.

And then on the left hand side of the upper

is just the gradient of F. So the idea is an agent will infer a policy that attempts to bring free energy to zero is how I see that if I just look at that one image alone.


SPEAKER_02:
Yeah, it's possible.

It's like the stationarity for policy.

Like if you're pulling from the best possible policy distribution,

can't do better probabilistically than pulling from the best policy distribution that doesn't mean that you win every like coin flip or something but but you you wouldn't be able to out plan better than doing that yeah yeah you could yeah okay not sure my favorite equations oh yeah no yeah policy inference just because it's so it's fun to think about for me um


SPEAKER_00:
it's like oh like here's a way of quickly relating G is like what I think will happen versus F which is like what I think is going on right now versus E which are my habits which are kind of like what I've accumulated from the past so it's just kind of like it's incorporating past present and future but like really explicitly addressing like behavior and what someone does so so you could have like

super precise, like heavy habits.

You can think of like a cigarette smoker who deals with stress by smoking, right?

It's like, well, your G might be great and your F might be great.

But if your habit is like really, really bad, like really strong, then that's going to overtake the computation of your final, final posterior, posteriors over policies such that you, you go and smoke, you know, I don't, it's a crude way of,

like getting into it like that.

But I just kind of I look at that equation sometimes.

I'm just like, I don't know why I enjoy looking at this so much and like thinking about it anyway.

But yeah, we can move on to other questions.


SPEAKER_02:
Yeah.

Like those are the two halves of inference.

There's perceptual sense making inbound inference and then kind of like full stop.

that's on the a and then there's on the b policy inference and then there's so many questions related to that like what are how do you learn what actions you can take everything about habit and learning and updating context switching policy selection different time horizons different trade-offs different sensitivities

lot of shared apparatus with perception that's why it makes sense to model them in like a unified way and with a single imperative which is ultimately what happens the agents minimizing free energy on its joints distribution so it is getting jointly optimized and then yet still you can kind of hold multiple parts of it

constants and then still like zoom in on just how how one part gets optimized or like how policy is updated without considering because it's factored off from the a and b are factored off through each other through the s markup blanket this one pending um the someone to write the verbal description

Or writing a prompt that can transfer these into verbal descriptions with the ontology.

OK, nice.

A lot of good math questions.

What is meeting meaning of the tilde?

Etta.


SPEAKER_00:
That's so hard.


SPEAKER_02:
Is this the etta?


SPEAKER_00:
Yeah,

I think so.

Is that the that kind of doesn't that come in as some kind of additional cause or force?


SPEAKER_02:
Sorry, I can't tell.

I'm not exactly sure here because I thought that's it was this shape.

But maybe that's the we'll come back to this.

Oh, yes.

Okay.

Here's the twin dynamical equations, the fMRI kind of classic equations.

Here's the signal and noise of the sensor data.

Here's kind of the dominant trend underlying function and the faster frequency stochastic variation of the underlying neural map.

which could be itself noiseless as sometimes done.

Like there's a kind of perfect noiseless neural flow landscape, and then we're getting noisy sensors.

Also, there can be a noise term in the neural layer, so it itself is probabilistic.

And then it's like F and G, it's just common modeling, like just because F of X is the first one, and then it's common sometimes in notation just to use the next letter.

so there's a funny joke I will not repeat here just about like why the vitamins are named the way that they're named but it's like why are there you know 12 B's but then like why does it skip it's not it's not a scientifically accurate joke but it's fun the syntax for describing expectation is never described

Box.

Okay.

Someone can add more.

What is meant by niche?

Okay.

Here's one answer.

Environment of agent may be possible to modify, construct, interact.

What would you say is niche?


SPEAKER_00:
It's like, it feels so easy and tough at the same time.

I mean, it's the answer given seems really fair.

The first place my mind goes is it would be

T. The environment of an agent, but then that said, like to give put a little bit more meat on the bone of why we would use the word beach rather than environment is perhaps to emphasize the idea that, as we could think of like you know any.

organism or animal or human or otherwise being within a niche we can think of like an environment that specifically like the the organism fits or maybe better uh is trying to fit right um so that that's kind of how i think about it it's like uh i would be very out of my niche if i were uh stranded on the north pole

uh without reasonable thermal equipment uh if i just teleported right now to the pole or something right um but to be where i am right now where you know i have an entire kind of ecosystem of like affordances around me that that work well with maintaining all these homeostats that make up the phenotype that is me at this moment uh

that's that's a good niche to be at i think um so i don't know would you think of the niche that way just like it's like an environment but with like uh with extra friendlier steps to the agent or something like that i don't know yeah those were all good that was interesting that it's like the organism or system of interest it's like it fits there


SPEAKER_02:
and also fitness is calculated there so there's kind of layer and a half of of fitting in and then also like ecological psychology it's like no one was saying that organisms weren't in niches or environments yet ecological psychology was was a call to incorporate more

holistic and and kind of agentic ways of thinking about interactions with Ecology is like well if it can't see that wavelength then it doesn't make that ecological Q is invisible um then there's I think some of Axel Constance papers have like separated the selective Niche from the cognitive and developmental Niche

and so I I think a lot of me niche Theory makes these sub niches but but yeah especially when you build a generative model then it's kind of like saying what is outside of the territory versus what is outside or you know what are the con what is labeled niche on this map and then that's usually quite clear and categorical

This is in response to the previous session asking about how priors originate.

The first prior.

In addition to self-evidencing optimism bias, noted chapter two, I want to notion the mention of genes as prior.

I.e.

genetic inheritance has been framed and acted as part of the prior belief that constitutes the self.

I was unable to find it, but believe it was suggested by Paul Badcock.

Yeah, the hierarchically mechanistic mind, Badcock, Ramsted, Constance, Friston, and then perhaps more recently, but not quite exactly on there with the empirical, biological, and the bioinformatics data sets, but that'll be TBD.

Here's an example of like the sensory motor interface of the worm.

And so that kind of primary behavioral layer of interfacing with the physical material niche, et cetera, like the light coming in and receiving the actions of the organism.

And then there's also a slower adaptive process happening

One that can still be analyzed with particular partition, FEP, et cetera, renormalization.

And then here it's like, well, these are beliefs about, this is like a phenotype of a population being allelic distribution, like you'd have height distribution.

So, but there's a lot to do here to connect empirically.

Yeah, nice.


SPEAKER_00:
yeah that'll be what are your last thoughts on uh four and andrew and then sickly if you want sure uh i think all in all i think it's great i think it serves up a lot of information that like i i would almost dare to say that if if you had to read a chapter of this textbook in order to get a sense of active inference and you could only choose one chapter four might be the way to go and that said it's like


SPEAKER_02:
you can't quite contain the whole textbook in that one chapter you have to read the rest too but uh it's good it's a good chapter okay any other comments okay next time later or earlier for chapter four chapter nine

A nice compliment to four square numbers.

Also skipping the generative model details to focus on the on different parts of the math and science.


SPEAKER_01:
okay yeah sickly go for it sorry um yeah so sorry i i um didn't really contribute a lot i actually i'm just on chapter two at the moment so uh it's all just pretty new um but i um do you mean that the next one will be in chapter nine if i heard it correctly


SPEAKER_02:
Well, at this time, like next week at this UTC time.

Yeah.

It'll be... But it's all good.

It's just...

going across different chapters i mean you could even argue for the advantages of it it's not like it's a story where you'll you'll get spoiled by reading the end yeah so just the earlier time then it then will be next week earlier however many hours ago chapter four part two just altering times


SPEAKER_00:
yeah it's like we have two core two cohorts running simultaneously and then that way everyone who cannot make the morning session on a Monday can make but can make the evening session on the following Monday that way so we basically spend two weeks on every chapter but we switch which cohort is featured between Monday between a morning and evening if that makes sense so it's like

You know, right?

This is kept cohort 7 right now.

Next week it'll be cohort 6 at this time.

The following week it'll be cohort 7 at this time, so on.

But yeah, it gives everyone an opportunity to to participate in whichever chapter reading they want to get into.


SPEAKER_01:
Okay, thank you.

Thank you so much.


SPEAKER_02:
Yeah, feel free to add any written questions or just or

know write them in discord but then we'll just go wherever we go especially this many cohorts and okay have a good one thank you farewell
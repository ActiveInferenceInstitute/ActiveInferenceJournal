SPEAKER_03:
All right, welcome back, Cohort 7 and all.

Andrew, to you, and just take it however.

Sure.


SPEAKER_01:
All right.

Yeah, so welcome, everyone.

We're Cohort 7 here discussing Chapter 2 of the Active Inference textbook by Frist and Parr et al.

And here we're looking at the low road to active inference.

So as we saw in the previous chapter, there was a kind of schematic showing there might be two general paths that one can take to reach an understanding of active inference.

And so one of them is kind of starting from something like Bayesian mechanics and statistics, which is what the low road is all about.

That's what we'll be talking about today.

And then from the other

angle one can approach active inference by starting with the free energy principle and the general imperative for organisms to act and perceive and so on in order to achieve homeostasis where possible and then again we can reach active inference from there so I was just going to give a

hopefully brief summary of chapter two, starting from the first section.

It begins with the Helmholtzian or perhaps Kantian perspective of perception as unconscious inference.

as well as the Bayesian brain hypothesis.

And so with this framework, we're treating the cognitive mechanisms of perception, action, planning, and learning as Bayesian inference problems, as well as deriving a variational approximation

to overcome the typical intractability problems which arise when attempting to compute exact Bayesian inference.

Those final concepts are something we'll be getting into later in the chapter.

So to start with, there's perception as inference.

The authors claim

perception is not just a bottom-up process of turning sensory states or observations into internal representations of the outside as if it were a one-way.

process from from outside in uh despite the fact that this has been argued historically in various cognitive science traditions they say instead it is an inferential process that combines top-down prior information about the most likely causes of sensations with bottom-up sensory stimuli so it's more of an inside out process

We could liken this to comparing a prior hypothesis with the outcomes of an experiment, which act as our confirmatory or disconfirmatory evidence for our hypothesis.

So from there, we can relate this idea of priors and observations or evidence using Bayes' rule.

So Bayes' rule, we can break that down into these kind of component parts.

So we have our priors and probability of X is the nomenclature that's used in this chapter, more frequently used for describing continuous time active inference models.

And then we have our marginal probability of observations or evidence, probability of Y. And then these get related through what's called a likelihood model or function, which is probability of Y given X or the probability of the evidence given the priors.

And then we can flip that through the equation.

and use incoming new evidence to update what would be called our posterior probability of X conditioned on Y, so our probability of hidden states, given the new evidence.

James Heitinger, kind of a brief refresher, of course, this is this is basing probabilities so we're dealing with probability probabilistic reasoning so.

James Heitinger, For anyone in the math group or otherwise, who was still getting a feel for some of the mathematics going on here it's good to familiarize yourself at least at first with something simpler.

ideas behind probabilistic reasoning, including the sum rule and product rules regarding like probability distributions sum to one.

There are no negative probability values, maybe, you know, in the process of computing your final result, you might have something like that, but in any case, in the final outcome, no negative values.

And as well as like marginalization,

Uh, those kinds of things.

So we're given, um, we're given a example of like for, uh, someone who's visually perceiving an object and they're trying to decide, is it a frog or an apple?

So it's kinda like our, our latent, uh, state space is we're trying to figure out, like, we're just given two simple examples.

a frog or an apple, and that can be used to illustrate a variety of ideas, including, you know, they run through how to compute surprise based on, you know, if one initially strongly believes they're seeing an apple, but then they see this object jump, that gets, you know, incorporated into their beliefs and, you know,

if our agent assumes that apples don't jump, then we can see that there's going to be a lot of belief updating going on that might tell them like, oh, this is more than likely a frog.

And we can see how that plays out with Bayes' rule.

We can see how

you know, the observation or Y is the jumping and then the X is the late state.

Is it a frog or an apple?

And we have our likelihood model with probabilities saying, you know, the agent's beliefs.

What is the probability of seeing jumping given that it could be a frog?

Probability 0.81.

What's the probability of not seeing jumping given it's a frog?

0.19.

you notice those values sum up to one, so it's that sum rule.

And so, yeah, it's worth kind of wrapping your head around that particular example for anyone still learning the maths.

We see that really what's going on with belief updating is calculating this quantity surprise, which is measured in the kind of information theoretic quantity called maths.

It's really just the negative log

of the marginal probability over observations so surprise effectively quantifies the difference between a prior and a posterior probability it tells us how unlikely an observation was and then from there organisms undertake something like optimization through bayesian inference where the model updates its priors to reduce this surprise of quantities

So the probability of seeing a frog increases with that example given before.

And then we can view this also as model comparison in the sense that you had a model from before, and now you have a new model that also incorporates the new evidence.

kind of compared.

We also get a description of the distinction between surprise and Bayesian surprise, which here they use Kolbach-Leibler divergence to kind of equate that with Bayesian surprise.

And so Kolbach-Leibler divergence is used in a variety of fields, but it's basically it's a way of trying to kind of quickly score the difference between two different distributions.

So you can see that, you know,

here's what your beliefs were before here's what they are now call back leave blur divergence so i try and figure out how much belief updating had to happen to reach the new model so we're also given more details on active inference and the significance of biological inference and optimality

biological plausibility of computations, the concept of predictive coding, which is not necessarily commonplace, but it's a very well discussed model of something like perception and other kinds of processes going on in the brain for neuroscience.

And algorithms like predictive coding implement this more general Bayesian inference scheme, and they're central in a lot of modern biological treatments of perception and this kind of top-down and bottom-up kind of bidirectional scheme that we're discussing here.

With optimality, Bayesian inference involves optimizing the cost function for those who

uh familiar with kind of machine learning nomenclature uh here the cost option is variational free energy considers the full distribution over in states rather than just point estimates like the mean or using uh maximum likelihood estimation uh so the idea is that with to make

not just is variational inference a way of overcoming intractability, but it's also relatable to the notion that biological beings

have limitations.

They have a limited amount of resources at their disposal, like cognitive, metabolic, and so on.

So they rely on approximations, which here we might equate with the idea of computing a variational posterior when using Bayes' rule.

And this can be based, for example, on a mean field approximation, which we'll see later in chapter 4, or there are other methods for computing that.

From there, they move into action as inference.

So it's not just perception, as they noted at the beginning of the chapter.

Perception and planning and learning and action, they're all kind of working together as distinct cognitive functions, but they're effectively doing the same thing.

They're all working towards minimization of free energy.

Agents not only change their minds through perception, they also act on their environment to change it to influence the observations that the environment generates so that the agent can encounter less surprising observations.

That's in line with something like realizing their preferences or priors over observations that they hold.

So we're still, again, we're still looking at Bayes' rule.

We're still talking about observations as like evidence that probability of Y value.

And then this gets taken up into later discussions over the differences between variational free energy and expected free energy.

Whereas we had variational free energy, which takes into account past observations and current observations, expected free energy will also take into account something like future, not yet observed observations, right?

And so there's this kind of, that plays into the notion of planning, or planning is inference, where similar computations are carried out, but it's projecting into the future.

So we're introduced to the idea of policies, which are like sequences of actions an agent can take.

It's kind of like an agent can come up with these kind of counterfactual scenarios.

If I do this policy versus this policy versus this policy, what will be the outcome?

What will be the outcome in relation to what I want or rather what I expect, which is that priors over observations.

And then also we can break down that expected free energy term into a variety of smaller terms that have this kind of value of being interpretable.

So expected free energy can be broken down into information gain and pragmatic value, which relates to this kind of classic explore-exploit dilemma.

where in active inference, exploration and exploitation are actually incorporated into the same cost function.

You don't have to define them entirely separately or apply any kind of arbitrary rules to determine what defines an action or an observation or otherwise as information gain related or pragmatic value related.

They're both there.

Finally, at the end of the chapter, there's just kind of some general ideas that are thrown out there for consideration.

Again, a recap to variational free energy, which is an upper bound approximation to surprise and can be minimized efficiently using

chemical or neuronal message passing and information that is available to the organism's generative model.

We'll see a lot more on neuronal message passing in chapter five.

There's also this kind of more philosophical idea that maybe an agent is isomorphic with its priors, including its priors over observations.

It's like it's trying to realize these beliefs that it has.

And finally, in the next chapter after this,

James Forrest, Norcal PTACC, Be kind of a flip on the the road tactic inference it'll be the high road it'll instead start from the free energy principle, as opposed to basis role.

James Forrest, Norcal PTACC, I think that summary was long enough so i'll wrap it up there, but i'm just going to open the floor to any questions or anything anyone found interesting about this chapter or otherwise.

Yeah, I'll jump in on a couple of things.


SPEAKER_02:
I took some notes just reading through.

Just wanted to clarify a couple of things as well.

So I looked into the consideration of like forward mode.

So is it considered like a feed forward kind of like model in terms of our projection of our expectation or an agent's expectation in terms of implicit bias versus an updating of that model through interaction?

And then I kind of linked that together with an agreement versus an argument, which could end up with like a zero sum outcome, as opposed to the agreement, which gives you a range of agreement within that argument space.

Um, so I kind of considered it as a correlation from external sensory and potential unknown expression of inference as a coupled field of connected conscious, which is getting a little bit into the esoteric, but in terms of like the external summed with internal states, or is it more of a coupled state that has the properties of both continuous and discreet in certain cases?

So where you're linking the, or the cross correlation with the X and the Y variables, looking back to figure two, two, for example.


SPEAKER_01:
Yeah.

Okay.

So, so, so big questions there.

I mean, the, uh, and I'm.

Sorry, I'm sorry if this is backtracking, but just some things that come to mind in relation to what you said, at least is, you know, as far as it being a feed forward model, like there will be a lot more detail whenever message passing is brought up later.

And so there's there are these kind of forward and backward connections, depending on which algorithm you kind of choose for minimizing free energy.

So, you know, it's almost like the agent, because they engage not just in, you know, pulling from memory and present observations, but also expected observations in the future, there's a kind of like, it's as if the future influences the present, and then also the past influences the present.

And that's kind of just thinking about it as message passing on a graph that has a temporal dimension to it.

It's almost like you can imagine these two nodes that are passing messages like the past to the present and the future to the present.

So there's also a kind of like retrospective aspect to the way that this plays out.

And then with figure 2.2,

Yeah, that what that is doing is just trying to describe how, you know, there's this common phrase, the map is not the territory.

So there's no assumption here at all that the model equates to the environment and how the environment functions around it.

Like, you know, that an example could be like.

So I had this in my notes.

It's a little silly, but it's like a

magician and the true explanation for the the illusions that are being produced are far different from just saying like oh it's magic you know but the audience very well might believe oh it is magic right so um that that's kind of their given hypothesis or explanation for what they're seeing their observations that are coming in despite the fact that it's it's probably not at all you know equivalent to what's really going on right um

And so that can play into more realistic scenarios where someone very well might be trying to do something, a task, and they might solve a problem.

And the truth is they might not really understand it.

And yet what they have learned to do and the model, the generative model that they do carry nonetheless could end up being successful at solving the problem even though they're not really getting it.

And that can play into, you know, the active inference has been used for, like, studying psychopathology, for example.

So people experience delusions and, you know, things like that, that don't equate to the true environment around them.

But nonetheless, it's, you know, playing out in their beliefs, dating and their beliefs.


SPEAKER_02:
You know, as they speak to that one point in terms of like the agreement, when you have another agent that agrees, right, if you are both biased in that negative manner without any external

you know, way to unbiased yourself, right.

And even if you haven't encountered or your model hasn't encountered another agent, won't it then re bias and reinforce.

So you'll have a regression.

So like how, how can we account for a model that regresses into that negative log and kind of stays in like a loop?


SPEAKER_01:
Yeah.

I mean, it's, you know, there, there might be some,

I think that there's already existing active inference literature that tries to look at agents who are communicating with each other.

There's a bird song example that comes up just in this textbook alone.

But I mean, there's also a really nice paper on that point.

It's a little bit more social sciences oriented.

couple folks on the scientific advisory board with the institute um were involved in publishing yeah it's called the epistemic communities under active inference and it's kind of you know uh they set up a simulation with that you know actual active inference agents who um

they they communicate with one another and share their beliefs and it's it's purely just their beliefs like there's no reference to that direct outside world and what often happens uh depending on how you set up the agents and what their potential belief states can be and the observations they can come into contact with it's like it's really interesting because you can see like their their opinions about what's going on they there's a kind of uh you know a bias towards

kind of reconfirming their own beliefs.

And so if they hear opinions from another agent that match their own, there tends to be that kind of loop of like, okay, we both agree with each other.

And we just continue to more and more strongly reconfirm each other's beliefs, right?

So it's like it kind of gets worse if they have a distorted belief of what's going on.

And it's a really good simulation because it's like a multi-agent one, right?

So you can have agents who have their own, like what can happen is you can have many agents, certain ones agree with each other, other ones don't.

And suddenly you have these like polarization dynamics, you know, where they start to,

This group over here strongly believes a certain idea or have a certain belief and reconfirm it with each other.

Whereas this group over here is doing the same thing, but about entirely different beliefs.

So there can be that kind of looping aspect to it.

It's very relatable to the kind of dynamics we see in social networks and how people use Twitter and tweet.

There tend to be these kind of small

you know, in groups that form and that's the idea of an epistemic community.

You could use it to describe like, you know, a whole scientific field of research where, you know, there are these certain scientists in this particular sub-niche and they're just kind of all talking to each other and they may or may not actually be interfacing with other fields.


SPEAKER_02:
OK, thanks.

Yeah, that's kind of that kind of covered what I was trying to look at there.

I guess also like I wanted to tie it back to would that still be considered a hidden state then?

Because, you know, depending on the size of the group, because I mean, if if the inference of the majority is incorrect in the smaller group,

is correct right based on factual evidence despite the larger group not agreeing to it I mean the the ramifications are that the larger group is still most likely going to be the outcome in terms of how that model is moved forward with right yeah I mean yeah just just point blank like yeah you know this is like at least like from a discrete model perspective um


SPEAKER_01:
you know, oftentimes the partially observable markup decision-making processes are used to describe agents and model individual agents.

So, you know, again, with this figure that Daniel has here, it's just, you know, despite the, as external observers like ourselves who are like setting up this simulation, so we know this kind of true state within the simulation, it's like still the agent

It's a partially observable setting.

So at the end of the day, everything is just latent states or beliefs about latent states.


SPEAKER_02:
So in terms of that as transparency versus opaqueness, would it be periodically transparent in that case, or would it remain just like semi-transparent overall?


SPEAKER_01:
It would remain.

Yeah, it would remain semi-transparent.

I mean, you could you could set it up differently.

You could just do a regular markup decision making process where it's like rather than the agent receiving definitively observations, you could just say that they received the true state of the environment, in which case would be fully transparent.

Right.

Yeah.

Those are the kind of two paradigms that are used for discrete.

Discrete time models.

And you could have similar things with setting up continuous time models.

And I did want to harken back, Sarah, you did ask something regarding

continuous versus discrete, or if there's ever like a combination of the two.

And later in the textbook, especially I would have a look at chapters, maybe skim chapter six, but especially looking at like chapter seven and eight, that's where you get more details.

Each of those are dedicated to discrete time models and continuous time models.

but then there's also these ideas of like hierarchical models uh where you know perhaps on the the lower level of the hierarchy there's a continuous model say it's uh you're modeling an organism that's taking in you know real-time observations and maybe they're uh you know there are different like physiological signals going on or be related to neuroimaging data or you know heart rate things like that but then

on a higher level it could be something like more of a sort of cognitive belief related level like maybe you're trying to model like an agent who's moving around in real time physiological you know uh dynamics but then also they have you know beliefs about what's going on that are actual

attempts at kind of categorical representations of what's going on at the at that higher level it could be a discrete model that's like oh based on you know x y and z on this real-time lower level what do i think is going on you know at this this kind of higher order uh discrete level awesome thanks


SPEAKER_00:
To interject, because I was curious about that statement.

So for that case, you're talking about the observations or the sensors that the agent is itself taking on, that it's observing different distributions

observation input that it's taking in.

And so its internal model is actually multimodal, I guess you could say.

There's four or five different models that you're just going across.

And it's like, I observe color, and my sensors are continuous.

All that is a continuous signal.

And then I also observe ones and zeros from, I don't know, an ADC.

Maybe I'm not even.


SPEAKER_01:
um and so like that has a very different distribution between is is that what you were talking about in terms of um the observations yeah I would say somewhat uh or I want to say yes but I want to make sure that I understand kind of the nuances um so first of all yes like you can definitely have

like an agent who's basically taking in like multimodal sensory information.

In fact, the nomenclature in the textbook is there are different observation modalities is how they kind of, that's how they term that.

And so the three kind of common observation modalities they account for are interoception, exteroception, and proprioception.

um these are really commonly used in like neuroscience and cognitive science literature like completely you know you don't necessarily have to do anything with active inference to to find these terms in the literature but uh so interoception is kind of like observations coming from within um you know it kind of goes beyond the bounds of what we might think of it's like oh we just have five senses right but to to categorize it it's like interoception would be signals

you know, someone notices that they're hungry or something, they can notice that.

Exteroception would be like sensory information coming from without, you know, so things that you're seeing from outside of yourself or things that you're feeling externally.

And then finally proprioception is that relates to like physical space and movement.

So you can think of like a reflex arc, like a movement of your arms, for example.

And I think that the main distinctions between those three modalities, I think it's just useful depending on your use case, like your object of study you're looking at.

And then there are kind of patterns in those kind of data and ways of modeling them to where suddenly it does become

like uh pragmatic like instrumentally relevant to to distinguish them like that um and then yeah it's you yes uh you can have an agent who like you know they come into contact with things that are you know binaries like ones and zeros as opposed to like a continuous value like the heart rate or like a particular shading you know it's it's like it's like you can

take the color red and put it on this really broad continuous spectrum, but then you can also say, oh, it's red, not blue, right?

So those kind of dynamics, yeah, at all points to putting together an active inference agent.


SPEAKER_03:
Yeah, one related point there is like these different distributions.

So here's 6.2.

Here's the underlying latent cause.

And then that can lead to all these different kinds of chains of causation.

So depending on what the situation is, like the need for modeling, and then those different distributions could have different priors on them.

So for something that is maybe more survival loaded, like a certain prior,

prior preference playing the role of the pragmatic value for like blood ph so policies are sought to normalize that at at the expense of like learning opportunities whereas if you're looking at like isocade the isocade might be driven on a given time scale by purely visual ambiguity

So different distributions and like just different ways.

And that's kind of the flexibility of the overall framework to be able to show up at these materially different kinds of processes and have a unified approach and compose across them.


SPEAKER_02:
So I guess to speak to that, too, like I had a question related to this in terms of like how much is our sensory perception actually not free energy in terms of I mean, if you have a disability, you know, with.

It might be rod cone deficiency, right, where you're not having the same kind of color perception, but.

same thing with our own dynamics in terms of what we sensory we are sensory beings but our consciousness may not necessarily be right so like what we can detect

within a given range of just taking the light spectrum right is to the high infrared or the low infrared and beyond to microwave or ultraviolet but beyond that spectrum is the majority of the latent space where we might have sensory disposition but we just don't understand it yet so i'm just wondering how much of our

sensory disposition as a whole actually reduces or increases rather our expenditure of energy in the modeling and how it might skew the data you know given new parameters for sensory cognition if we find you know new data in the future


SPEAKER_03:
Yeah, that's interesting.

Like, why can't you just believe that you're seeing outside of the visual spectra?

It's like, it might just be a jump too far because there's the absence.

It's not, it's not just the absence of confirmatory or disconfirmatory, but it just, it's, it's, it's like, there's no packets coming in that channel.

But so there'd be no way it would be as, as fabricated as like a total imagination.

overlay on the visual fields but it'd be a freewheeling but then but then what if it were like you were looking through an infrared filter and you really got to know an area and then like you took off the filter for a second and there was like a quote after image the length of that after image would be about kind of the the um durability of the generative model on those non-visual realms


SPEAKER_02:
also thinking in terms of like new tool creation right where something would be testable so i mean if we get quantum to a state where you know just data in itself is exchanged you know at a rate that's theoretically faster than the speed of light right that would have to change our perspectives on a lot of

way that we model things right in terms of time especially especially if we're dealing with discrete or the opposite that's one of the reasons why i guess i'm kind of just going a bit too far beyond i don't want to jump too far out but in terms of like time domains especially if you're jumping through different models that are classifying and reordering things differently

when you need a taxonomy that's similar or flexible in terms of being able to take on you know a context-free kind of formality that is still able to express the detail without utilizing more energy


SPEAKER_01:
Yeah, I was trying to say, sorry.


SPEAKER_02:
Yeah, I was trying to think of an example that was kind of paradoxical in my mind, and it might just be my own approach to it.

And it's something I want to try, which was like I'm calling it like a method of zebras, where if you're classifying a zebra as a horse, is the zebra black or is the zebra white?

You can do, you know, variational distribution of if it's white or if it's black, does it still necessarily make the zebra a horse?

and to what degree are all zebras white or all zebras black and then how would you reclassify it under that you know categorization scheme yeah so so there's a you know there's a concept of a structure learning


SPEAKER_01:
not just used in active inference, but it is used in active inference.

And the idea is not just for a model to update these kind of, not just update its beliefs in terms of the final values that are found within the probability distributions describing its beliefs, but also to actually modify the general structure of its model in the sense of like, oh, introduce a new category

you know, previously known system or something like that, you can kind of relate that to the idea of like, something like a sort of rewiring neuronal connections and forming new connections that allow for just new, not just new information, but like new kinds of information almost to be formed.

And in that case, you could do something like Bayesian model comparison, for example, and use like, the

criterion or Bayesian information criterion, it would just be a matter of comparing models from there.

And then within both variational and expected free energy,

Whenever we look at how those are broken down into smaller subterms that are being subtracted or added from each other, et cetera, that's kind of that interpretable aspect of those terms that's nice.

It's like variational free energy can also be recomposed as negative complexity minus accuracy.

So they're both, or excuse me, plus, sorry.

kind of pull that one.

The point is to minimize complexity of the model while maximizing accuracy, which I think kind of gets at one of the points that you made, right?

So the idea would be to part of free energy minimization is to try and keep your model as simple as possible while keeping it accurate.

So that kind of trade off does play out both in structure learning and in the more general belief updating that we were discussing earlier.


SPEAKER_03:
Maybe one point to make about this kind of getting to active inference along the low road.

It's going to come together in Chapter 4 when we see the tools of Chapter 2 deployed under the kind of umbrella of Chapter 3 free energy principle.

So one reading of this chapter is like for different cognitive phenomena, different cognitive apparatus, like different matches or mismatches or degrees of freedom with different agents and niche.

It's like whether it's more like an inference or more like an action from a given side of the blanket, this partition is going to enable a lot of flexible modeling.

So it's like just saying a large variety of processes are going to be amenable to being partitioned this way because some of the special cases include like full observability.

And then there's the whole continuum of all the way on through total non-observability.

Like I'm flipping a coin in a room, that's the only data I'm getting.

And then there's another random number generator outside.

So it might be like a hopeless optimization from actually explaining the variant, but that's the special case that then you can rise and describe out of.

And the extent that it's possible to infer like causation is the sparsity of that graph.

So by kind of starting with a general case, then a variety of total to total absence of different kinds of arbitrary distributions can be framed.

And any which of those situations, they're framed in a way that's like going to fit into this approximate Bayesian computation setting.


SPEAKER_02:
so in terms of that too like i had another question that's kind of related to that um and it dealt with i can't i didn't write down what section it was under but it dealt with give and take in terms of external factors so how much can we consider

be optimization versus mimicry of an external factor that is given versus taken if the mimicry is misunderstood or sub-optimal copy of a realistic measure so if the mimicry is based on external external or internal bias that is agreed to be true despite the unknown falsehood in the data would would that still be considered

a positive that would you know flip that negative logarithm back over or would it kind of stay in that kind of that loop that we kind of talked about earlier in that sense I don't know we might be almost ranging into the


SPEAKER_01:
the the philosophical here um which there are plenty of conversations to be had from a more philosophical angle with active inference but uh as far as i'm aware it's still we're looking at like a part like like basically the the true state is still hidden it's just like the beliefs that the agent has like in their head so to speak or you know represented in some kind of way um

yeah there's there's no so so from that angle active inference it's like we're always kind of dealing with you know potentially it's almost like the observations themselves are like metrics that we're using to try and measure the the probability of hate uh in the state right so it's it's that's um well I guess I'll give one other one other connection just to the kind of again this like


SPEAKER_03:
agreement or like negotiation argument continuum so this is like think about it from the blankets perspective so here's the niche here's davis in the summer here's you know what i prefer to see pragmatic value for for my thermoreceptors so the thermoreceptor could be cleaved off if this was all you saw you might think wow well the thermo is getting cleaved off on the right side

but it's getting cleaved off on the left side too.

So that's just sort of a convention because the interface is by definition the part that's shared slash overlap.

So then there'd be a situation where it's like, wow, we both are tracing the same path.

then that'd be a situation where like the pragmatic values aligned.

That could be like an improv setting where it's just like, okay, we're going to do this now.

So it's like, we're setting another set of rules on either side of the blanket that are making the movement of why more coordinated.

But then if it was like, I want it cooler and the environment wants it hotter, then that's like the change, the world changed the mind.

But those, those are going to have a fundamental, like,

one getting more accurate will make the other one less accurate.

But then you're going to have other situations where there's like, where Y could be more accurate together than an inert one on the other side.


SPEAKER_02:
Yeah, that makes sense, I guess, to bring it like because the idea is kind of a little bit philosophical.

So to bring it back to more of a scientific standpoint, like in terms of and I know this is kind of leading edge stuff as well, but in terms of like increasing neuroplasticity, you're are you going to retain that information?

So if you can, you know, bring your own plasticity up in terms of, you know, neuronal firing,

or activity in the brain, is that going to remain as an expanded state in terms of like a library state, for example, that you can draw upon as opposed to, you know, what we were just talking about in terms of like

the loop of, you might be thinking that, you know, that space is being increased, but really what it's doing is actually reducing that space overall.

Cause you're also not allowing the priors to update that space.

So in terms of plasticity and especially with people that have lost plasticity, so like Alzheimer's, for example, if you can bring that neuroplasticity back, can you reactivate those regions to bring old memory back in, or would it have to be retrained?

Like a model would be retrained.


SPEAKER_03:
Yeah, good questions.

Substrate of memory, open questions.


SPEAKER_01:
Yeah, I'm sure there's someone somewhere that I'm not familiar with who's expressly doing research on Alzheimer's, potentially from an active inference perspective.

But yeah, I mean, all I can say from the active inference point of view is that, I mean, neuroplasticity

presumably is going to be guided by free energy minimization, right?

So there'll be that kind of consideration, because no one's actually considering anything, but, you know, the kind of consideration of like the complexity versus accuracy and the formation of new neuronal connections, as well as like kind of modulating those by way of neurotransmitters, like the

the kind of assumption made in active inference is that, yeah, it's just gonna be free energy minimization.

And then of course, with that kind of,

that kind of deterioration that is actually leading to more difficulty for an individual to say maintain homeostasis or the kind of life they want to live and presumably there's some kind of external element that you know unfortunately has made its way in or otherwise that that's you know it's as if the neuronal connections and so on are trying to

they're trying to keep going despite that, you know, occurring.

So they'll play into the process of the deterioration, you know, same way that one tries to fight off a disease or something.

But yeah, I don't know if that's clarifying, but that's


SPEAKER_02:
yeah no it is like it just i was just thinking back to i don't know if it was a stream from this week or last week dan you might be more familiar but uh dealt with uh the brainwave principle i think carl was there with a couple other people um and i was thinking yeah i was thinking in terms of you know the brainwave um that would be message passing as in terms of like free energy where you're receiving where you might not be conscious of it right but if you have that activity

or that space where it's a latent kind of memory that might crop up with a different experience.

It's like how that interaction might be considered to skew the data or would it actually bind the data into a more calculable space in terms of prediction and message passing and data sharing, cross modulation, et cetera.


SPEAKER_03:
There's essentially never a directional answer in the general case.

It just is a rare situation.

I think it'd be fun to think of some, but it's almost always possible to think of niches for which any given like anything could be the other way.

Right.

So the whole point is it doesn't have a meta prior on this or that.

which gives you the expressibility, but nothing's going to come along for free into the structure generative model.

David Blumen today gave a really interesting twist on this with training neural networks to approximate act-inf.

So that's this angle.

And his, his argument was like, at some level, you're going to have to do something like machine learning based parameter sweeping optimization, like,

If you're just building it pedagogically, you just want to see the gears turn.

Yeah, you might do a kind of a custom curated soft choice of parameters, or you might engineer a second layer optimization system.

But at some point, you're going to have to have that kernel of the generative model and the more strongly type thing.

It's going to be surrounded in a soft layer.

of cognition whether you go custom craft or whether you do some other kind of statistical method and he's like it's an approximation anyway you're approximating analytically it's already an approximation so if you're if you're approximating an approximator and in practice the way you approximate it really matters why not lean into it and just go for it and treat it like an engineering problem where it's just purely empirical almost anything you can think about


SPEAKER_02:
I mean, doesn't that lead into a quagmire of like back until like it's turtles all the way down though?

Like the more refined you go, like it just descends into more necessity for confinement.


SPEAKER_03:
That is if you think it's structured all the way, it's like, well, it's a Markov chain of turtles.

But if it's like, what if it was like, well, I see two turtles trailing off into the fog.

Hmm.

And then it's like one person says, I think they trail on forever.

And someone else says, I just don't know.

That's just where the information supply chain, I just don't know.

And I can't tell any sensory evidence to differentiate these hypotheses beyond this horizon.

So at some point, the statistical model is going to blur into the real world.

It's just cartography.

And so at that point, you have to accept that it does blur into the real world.


SPEAKER_02:
Yeah, I just was wondering like how much in terms of like how much of that blur becomes, you know, actualized or does it become like almost forced and imposed upon reality right through the predictive nature of like a sterile environment?

Like at a certain point, there has to be like a reality check that might just kind of not break the model, but might hinder the model in a way where it doesn't become as effective, right?

Because any of us may not have perceived


SPEAKER_03:
you know that that random outcome for example you can make a niche where a model's performance was strongly tied or you can make a model a niche where the outputs were not so you'd have to give a lot more information about what you're talking about but that's why it's like you're you know opening up a big Vista then specking it out within that


SPEAKER_02:
right i mean the narrow path is the more difficult to begin with right but that's usually the more fruitful it's like rather than the low-hanging fruit you know how are you getting to the top to get to the best kind of fit but my question i guess there is like at what point does that become more risk than is required of you know the model and does that then interfere with free energy as a whole

It's, I guess it goes back to just the basis of the trade-offs, but.


SPEAKER_01:
Yeah.

It's again, that like plastic, you know, complexity versus accuracy problem.

Sorry.

I'm also looking at a question that was asked in the chat.

So like at a point, a model becomes so complex that it hinders its performance.

And it's like, yeah, that, that can.

that can happen.

It might be that the agent is dealing with a really complex, you know, problem, but then over time, it just it figures out, you know, here's one very precise solution to the problem in a particular instance, but then it's just really heavily over fits to that particular problem.

Like that's, that's the one solution that kind of could come up with and then it's not particularly robust.

So as soon as

that suddenly that solution, that hypothesis or set of beliefs that the agent has just like completely falls apart in terms of performance or accuracy or something along those lines.


SPEAKER_02:
Cool framework as well though.

Go ahead.

Wouldn't that be dictated on like the solidity of like the framework itself that you're building off of rather than the structure will collapse into itself versus what will remain.

That's still salvageable in terms of re-updating.


SPEAKER_01:
Yeah.

So it'd be kind of like having bad priors, right?

Like that, this is, this is from whence you are like, you know, planning

And so it's like an agent who has their beliefs in the present and their prior information that they're working with.

And then once they start planning into the future, if those priors are completely terrible, so to speak, or inaccurate, at least relative to what's going on around them, then it's like, yeah, you're going to come up with some pretty faulty plans as well.

So broader projected hypotheses you're making are also going to be bumped.


SPEAKER_02:
it also leads to more reactivity I don't know if anybody's seen the I think it was a study on like I think it was like the hungry judge study something like that where they did a study of judges that made rulings before lunch and after lunch and the accuracy I thought that was interesting I don't know if anybody has run across that I'll have to check that out classic Sapolsky Contra will yeah um


SPEAKER_03:
Okay, thank you, Andrew.

So next week we'll stay in chapter two or come at this time and we'll be in chapter seven.

Chapter seven, I mean, it's revisiting the low road.

It's going through a specific kind of architecture.

So two and seven read fine together.

Again, interestingly, three and eight

with the continuous time in certain ways, having more affinity with the free energy principle path formulation.

But either way, another week in these chapters, then third week, possibly Andrew will do the social science model stream.

And then maybe in the applying, we could do some tweaking and adding different things like to the script and notebook version of your social science simulation.

And then in the third week here, we can see where we're at then.

And yeah, thank you all.

Thank you, Courtney.

Thank you, Zane and Dave and everyone.


SPEAKER_02:
See you.

Nice to meet you.


SPEAKER_03:
Peace.

Bye.

Later.


SPEAKER_00:
Thanks.
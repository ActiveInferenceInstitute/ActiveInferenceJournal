SPEAKER_01:
All right, welcome back, everyone.

We're in cohort seven, chapter seven.

So for some of us, this will be our first time on seven, others not, not so first.

But where would anyone like to begin with a specific quote or question about seven?

Or how did anybody take seven?

Or where do they see seven in the book?


SPEAKER_02:
Maybe I'll say I listened to the video of the last session, the evening session where Andrew talked about chapter seven.

And so what interests me and I'm still struggling how to code things like free energy principle and how to, you know, how all the math works, et cetera.

But and he did he did help explain that.

But what I got from these sections was that they progressively are looking at modeling different parts of mental life and showing that in each case, you know, the this apparatus can be used to model that.

And so in my case, like so

basically like perceptual processing, like deducing an intended signal, the T maze with the rat, like decision-making planning, epistemic versus pragmatic, and then information seeking with the ISACAD, seeking position where you can get it, kind of yielding a streetlight effect.

and then learning a novelty so this synthetic worm exploring its environment changing the generative model and then finally like hierarchical or deep inference uh words and versus sentences putting that together on separate time scales so so what it seems like is that um

you end up you know you you see oh i could use active inference for modeling this um and then um you do things of you know whatever sophistication you can and so i'm interested maybe in the opposite to say okay i'd like to develop a cognitive model i think that i'm gonna have like a continuous part and a discrete part like a procedural continuous mind and a discrete declarative mind

Then I'm going to maybe wonder how they discover critical points, navigate them, have a language of critical points.

I guess I'm just still learning about the active inference, but I'm trying to pick up on that connection between having a mental process or experience in life you're trying to model and then thinking, okay, where does prediction error come in?

I guess that's probably what it's about.


SPEAKER_01:
that's where i'm at right now thank you awesome summary anyone else want to give some thoughts on seven or like a section or a quote or a figure or equation that they want to jump to um i mean figure 7.2 with the the plots about uh


SPEAKER_00:
The listener are a little confusing, I guess.

I'm not sure what I'm seeing in the first column of panels there.

Maybe someone with some more insight could kind of walk us through what these panels are showing.


SPEAKER_01:
Yeah, it's a great point because this.

Chapter is the discrete setting.

But then it's like, wait a minute, why are there continuous traces here with a discretized model?

So these are kind of like some modeling outputs from the SPM MATLAB scripts.

Um, but yeah, we, we can definitely get, and also it doesn't help that they're black and white.


SPEAKER_00:
Yeah.


SPEAKER_01:
So it's a little bit like, well, but are we expected to, I mean, how can you trace?

So I agree they're, they're not super informative.


SPEAKER_00:
Do you know at least what they're meant to be showing?

Oh, yeah.


SPEAKER_02:
I could say the main idea, if you show the diagram there, if you go up to the diagram, the main idea is that there's an amateur musician playing some of the notes wrong.

But if you go to the screen, someone's listening and they know what the melody should be.

It should just go done, done, done, done, and then done.

So when the musician misplays it, which would be the lower right, they misplay it, but the person who has the idea of what the melody should probably be, they interpret it, well, it really was meant to be that.

It's the intended signal as opposed to the actual signal.

and so active inference is able to you know if you have a strong enough matrix where the pattern is so insistent that you know it's gonna survive the truth uh then um then it can model that so yeah okay and what's happening with these figures specifically so top left posterior probabilities about each note


SPEAKER_01:
So what's happening here is basically how confident are we?

Each of these is a different note, but they can't be determined.

So it's like in time point one, and also it doesn't help that it's kind of offset here, but it's sort of getting at there being like the most likely one

is on the top so that's one with the highest posterior probability which is kind of selected if you only get one selection and then on the lower left is the the prediction errors over time so like you can see there is a higher prediction error when the incorrect note is played


SPEAKER_00:
Okay.

But each line, though, is what?

It's opposed to your probability about a note?

Yeah.

Yes.


SPEAKER_01:
So it's like the latency, there's one spot for what's the true note being played.

Like, we're going to receive a one-hot observation.

Like, we either hear note 1, 2, 3, or 4.

That's coming in with a vector that looks like 1, 0, 0, 0 for the first note, like in this situation.

Then we have a prior belief distribution over those four.

So we could have a uniform belief over the four, 0.25, 0.25, 0.25, 0.25, or you could have 93, 3, 3.

Okay.

So then the relationship between the skewedness of your prior

And the attention to the sensory input is what dictates belief updating, which is when the prior becomes the posterior.

So in the extreme case of no attention, it's like the observation didn't even come in.

You just carry your prior forward.

In the extreme case of total attention, you just basically update your posterior to whatever the observation was.

And then everything in between is like

attention-weighted belief updating.

And so there could be some value of attention where this is exactly what would be heard.

But in that point, you don't even need to do Bayesian statistics.

You just do descriptive statistics on incoming information.

Yeah, or the converse.

So when you're looking at like the trace of your simulation, at each time point, you have the belief distribution over possible notes.

again even though it's only like one latent state dimension it's a distribution it's a categorical distribution over four options and so this is kind of like plotting the again we could replicate and color these lines to trace them more clearly but like we can see that in this s3 moment here's where it's sort of most mixish

there's just a little bit of weird stuff happening but to investigate it more we'd want to um run it but this is like this and that's why it's built up this way because again chapter six is kind of the recipe okay so just just to be clear so on the on the plot so like let's take one of these lines for example say that the top line um


SPEAKER_00:
yeah that one right there that is not a curve so this is going to correspond to the probability of the posterior probability that it's I don't know note one and then there's a corresponding line at the bottom where that where those where they bifurcate right is that the so like uh like at the split around like 0.3 do you see that on the


SPEAKER_01:
I'm really not sure because there's, there's a flat line at zero and one, which might just be like graphical.


SPEAKER_00:
Yeah.

I think it's just, yeah.


SPEAKER_01:
I think then there's one, two, three, four, five, six, seven, eight notes.


SPEAKER_00:
Yeah.

Well, there's four notes, right?

Right.

So what I'm not sure what the eight lines are.

So I, is it like, is it like one of the lines corresponds to the posterior probability that it's not the note that we're talking about?


SPEAKER_01:
know i i i'm not sure what it's what it's meant to be showing i agree and i'm not sure if though if there's eight notes like in the scale and they're like tending to zero here which is why at the end but then why why would you have high posterior probability belief in multiple notes at the end right i mean it looks like they're meant to be something to one so i think it it would have to be like


SPEAKER_00:
Like what, like two of the, like the lines come in pairs basically, I think.

Because otherwise it's not summing to one, right?


SPEAKER_02:
It's like an upper and a lower.

And maybe they don't sum to one.

They don't sum to one if you look at the third column.


SPEAKER_01:
They definitely don't sum to one, but then it's not a probability distribution.


SPEAKER_02:
What they look like, they look like error bounds, like from below and from above.

Like, how confident are you that you hear what you're hearing, right?

and somehow like with the third one the upper confidence just drops like uh i don't think you know we're not sure we can't be sure because there's a goof here so yeah again we should investigate it more i agree that this is not super clear also it could be if it's a posterior probabilities about each note


SPEAKER_01:
then it could be, there could be eight notes possible.

But then it's like, at this moment, it's like there's an 80% chance it's note one, 70%.

So that doesn't sum to one.


SPEAKER_00:
Because I was thinking, yeah, that's why I was thinking like, if you take that, that 80% example, there might be a corresponding line down below.

That's around like the 20%.

That's supposed to be like the, like the, not that note line, you know?

So it would sum to one, but is that not what it is?


SPEAKER_01:
Yeah, I, I, I agree.

And also just again, the fact that it's continuous.


SPEAKER_00:
Yeah.


SPEAKER_01:
And it has like different dynamics.

It's just, it's part of the trace of mat lab.


SPEAKER_00:
Okay.


SPEAKER_01:
In the discreet setting.

this doesn't really come into play but it's yeah i i agree like it's not very clear okay so i'm not alone in that that's good to know yeah and and it especially just because this kind of gradients

method, the continuous gradient sub time step scale gradients, which can happen, but that's not the calculations that are needed for a discrete calculation, which is just a Bayesian belief updating, which is just done in one shot in this case.

But yeah, like

this chapter the first two examples are kind of like two of the core tools in thinking about perception and action models it's they're really really critical just to structurally understand that there's observations that are emitted from a latent state

There could be seven dimensions of S projecting onto one O, or it could be the other way around, any configuration, and there's some intermediator A that is able to take a latent state distribution and project it onto an observation distribution and vice versa.

So that's the sort of bifacing nature of A, that it can be used in its generative capacity or in its recognition capacity.

You need a latent state prior to kick this whole thing off.

And it's usually shown with three, but really it's just two.

It's really just, you kick it off and then you're only looking at one transition step

And then where you transition to becomes the prior distribution on the next moment.

So even though there is a trace of observations, we don't model observations as causing successive sequential observations.

We model changes happening in an unfolding latent space that emit observations.

And then action enters the picture in choosing the transition operations in the latent state.

Like those are sort of key pieces that are at a very high structural level important for this type of modeling.

Anders?


SPEAKER_02:
And those latent spaces are kind of states, I mean, are generally kind of imaginary.

Like it's right.

I mean, like that's partly it's usually not necessarily like exactly real, but it's more like that's what's either modeled by the modeler or modeled by the organism.

but is that basically a good way to look at it?

Like they're not intended necessarily to be real.


SPEAKER_01:
Let's say it's a temperature situation.

The agent's model is like the true underlying temperature, but it's the agent's model of the true temperature and the observations they're getting.

So it's not the generative process, how it's described in the book, of like the quote, real temperature in the simulation.

so yes this is like these are all variables within the agent's cognitive model and then there's another which is also our model because we're modeling the agent it's like a okay we're both modeling together as friends yes and that's that is brought up in chapter nine that's the metabasian perspective

which is we are outside of the agent's generative model, proposing that as a map of a cognitive territory, which is itself mapping a reference outside of itself.

But there's another degree of modeler freedom in saying, well, there could be aspects of the environment of the agent that the agent doesn't even know about.

So that and I think.


SPEAKER_02:
I think that that's a key thing for discrete language, like how why discrete language works at all.

It's because different people modeling, you know, basically a similar situation are going to come up with similar answers, even though they're, you know, even though those latent states are all not real, but

but they're real in the sense that you're going to get similar answers.

And they're going to get discrete answers.

So you found the similar critical points.

So I think that's the wonder of discrete communication.


SPEAKER_01:
Yes.

That tension with continuous discrete comes up in so many guises, both in the treatment of time and in the treatment of the state spaces themselves.

So here we have in chapter seven, basically a focus though with some spice added, discrete state space, discrete time.

So again, confused by these traces, there's not necessarily differentiability.

There's just snapshot-based discrete updating.

In the continuous state space, but the advantage of the discretization

is first off it's very easy to understand the computational resourcing and when it comes to action selection it's easy to roll out counterfactuals like chess trace search chapter eight brings in the continuous time and continuous state space mathematics

But yeah, we can go to another section or we can look at the specification in PyMDP.


SPEAKER_02:
Um, or, or I wanted to ask, uh, is there an open source, like a version of MATLAB, like Octave, will that open work these files or not?


SPEAKER_01:
There's a standalone download of SPM that runs without a MATLAB license.

There is Octave, which is like a patched open source MATLAB.

I've never run SPM in Octave.

So that's why in the sort of post 2018-ish period, there was movement just away from MATLAB, but it still is used by many people and many models that are in there.

But of course, any language can emulate any language, but using Python and Julia,

and a lot of other languages.

It's taking us closer more to the data science pipelines that are in the other packages that are relevant.

So I would say the best place to look for this is in the PyMDP documentation.

That's why it's called PyMDP.

PyMDP is not trying to be a generic model of all base graphs, kind of in the spirit of Rx and Fur.

The agent is literally defined almost exactly according to the textbook.

The same TMA's situation.

But this is exactly...

how it's laid out with agents and the ABCDE.

It just goes through them.

Categorical distribution on A. B matrix.

That's how you encode the movement.

Preference.

Prior and hidden states, D.

And here, ABCD are specified.

And then when you, all there is to run, and POMDPs have been around for decades.

They're not always fit using a variational or an expected free energy.

But this is basically what the loop is.

You get the observation from, in this case, from the location in the grid world.

Belief updating on the latent state.

What is my true location occurs.

So that's inference of the latent states given the observation index and the A matrix and your prior distribution on latent states.

So that's belief updating.

That's the sense-making, that's the perception, that's the inbound, that's the music listening.

Then there's going to be action selection.

First, expected free energy is calculated for each possible action.

So that's for the given time horizon.

It's going to ask if I were to do that sequence of events, whether it's just one action, like one time step planning, or whether it's multiple time steps of planning in the discrete case,

Each of those chains of actions will be unrolled in terms of their expected effects on latent states via the B matrix.

And then those get unfolded into the expected observation distributions.

Those expected observation distributions are evaluated in terms of their epistemic and pragmatic value.

Pragmatic value, how well would those observations align with my preferences?

Epistemic value, what would be the expected information gain from those observations?

Those scores constitute the G vector, which lines up with your policy options.

Those scores in G, which don't have like an intrinsic meaning,

are used to reweight the policy prior into the policy posterior.

So policy prior is what you're believing that you're apt to do before the observation gets reweighted according to expected free energy values relatively and renormalized with a softmax so that it stays as a probability distribution over actions.

One action is chosen from the policy posterior distribution.

That is going to be the selected action.

And then at the end of this cycle, it kind of clicks ahead to the next latent state belief.

And then that will be used here in the next cycle as beliefs about states.

So that's one procedural way

to have the sort of imperative style for describing a perception action loop.

But there's other orderings that could be possible.

Like you could do different orderings.

I'll just say that.

So that's why there's a lot of degrees of modeler freedom.

Because this is not, it's not like this is the only way to model

the handling and that's even within this simple single observation single action setting but that's how it works in pi mdp which is basically what is described in the textbook nice that's super helpful um what did you mean by other orderings like orderings of what um like there's um

This is kind of a classic, like observation comes in, update beliefs, choose an action based upon the updated beliefs.

But for example, you could have a time step situation where it's like at the beginning of each time step, the agent chooses an action based upon what it believes.

and then gets an observation and uses that in the next time step.


SPEAKER_00:
I see.


SPEAKER_01:
So that would be, you know, and then especially if you're dealing with in this example, there's basically only two operations happening.

The state inference and the policy inference.

So those are kind of the two main categories of orderings, right?

Which one do you update first?

And it totally makes sense to update the observation first.

as you get more kinds of variables in play and more cognitive mechanisms, there become more ways to imperatively specify the ordering of the loop.

Like if there was a read and write to memory, if there was an attention element,

if there was a policy search, all these different features.

And then what's really interesting and what kind of returns the focus to the high road and the low road is like the free energy principle is describing the physics of flows across these graphs, not in discrete time.

but it can be discretized, but as this sort of unfolding simultaneous flow in terms of belief updating on this graph.

So, and the way RxInfer kind of aims to handle that is with reactive message passing sort of on-demand, just-in-time node updating,

But even then, that's not happening continuously.

It could be happening rapidly or infrequently, but it is happening in, especially on digital computers, at some point, everything is gonna be discretized in a way.

So figuring out what the duty cycle is for the agent, and that's where you get like the TOTE,

Like, this is sort of an older, like, test, operate, test, exit.

This is one, like, you know, whatever it sort of seems like esoteric or strange to be talking about the ordering of perception action.

It's like, there's tote.

And then of course there's the OODA loop, which is another discretized way.

And that has been connected with active inference as well.

So just when you get down to building on the low road, which is what chapter seven and eight and everything, the whole second half of the book is about, you get down to like these details about which variables are exactly undergoing which operations and which orderings.


SPEAKER_00:
that's not something that the framework is opinionated on but it is something that's required to have operating code yeah if so um i remember that like one of the central that's well to me anyway at the time that i read it it seemed like a central tenant uh was that what active inference sort of does was is is that it um sort of like flips the way that an agent

uh perceives the world in that instead of waiting for the observation to come in and then making a prediction about what they're seeing they make a prediction first about what they think they're going to see right and then they take the observation and it's the prediction error that like kind of cascades through the system right so is is that like a different order that you're talking about or is that uh kind of set in the way that it works that question makes sense yeah yeah totally so like um


SPEAKER_01:
a few ways to go about this if you have a sequence of alternating you know belief updating and then action updating you know or latent state belief updating action state updating at some point which one happened first right just alternating and so it it doesn't really matter but then when you think about the injection of an observation that's kind of what we're just exploring like

observation belief action observation belief action was how the pi mdp had it as opposed to observation action you know so once you get to three or more you do get different sequential possible orderings as well as at the level of the simulation where okay now we have the like for example if we look at the open ai gym environment so this is a discrete environment where every time step

um whatever it's called now but every time step like there's getting gets an action sends back so it's a kind of call and response style but when it's one agent one in one environment you could do that or you could have it send two and then one back or something like that but um then you kind of point to like this idea that that the agent is is like action as fulfilled prediction

Planning proceeds by inferring an action.

What?


SPEAKER_02:
The agent.

It feels in charge of nothing, but.

There's still like, I can't understand you.


SPEAKER_00:
Your audience putting out a little bit.


SPEAKER_02:
Um, the, uh, the, uh, the organ.

is in charge by making predictions all the time like even if it knows nothing then it predicts that it knows nothing but typically it knows a lot about its environment and it's uh you know that that that knowledge is stable and just it's reporting minor changes to that you know within that environment yes like


SPEAKER_01:
Yes, there's the sort of like inner agency of like, it's not just taking punches from the environment, even just when it comes to sensemaking.

So just thinking about the first 7.1 example,

each observation is meeting what can be thought of as like an active prediction, but it's the prior.

The prior is being brought to the table and meeting the observation, and that is the Bayesian updating moment.

So there's that element, which is, it's not just processing inbound information, but there's this sort of active listening approach.

And then that goes further when we actually consider

overt action, especially action in terms of what is expected to be perceived.

And then the agent is unsurprised when the consequences of action are what it expects to be.

Surprise enters when the consequences of action are not what was expected.

So that kind of centers action in

setting what is the envelope of observations that would or wouldn't be surprising.

But because it's so general and we describe rocks and other kinds of abstract boundaries, it's hard to personify it or apply too much of a psychological lens to the generic case.

But in cases that are

able to have that kind of psychological projection that's exactly that's how we compose expressions in the active inference ontology to make structured models for those phenomena


SPEAKER_02:
And one difference that comes up in cognition, I think that's very much the case where you're always predicting, so you can always be surprised and you can always deal with that surprise.

But in the case of emotion, you have expectations

And so if those aren't met, you may feel surprised or if they're met excited or sad or happy.

But in the case of, I would say, like fright or disgust, you refuse to make expectations.

You know, there's certain things that happen that you refuse to expect.

And so then somehow you're dealing with a different situation.

It's a different game.


SPEAKER_00:
That's interesting.

What do you mean?

You refuse to make an expectation in cases where what we're like, well, this is this is a simple, this is a simple model of emotion.


SPEAKER_02:
But if

I have a theory, you know, and it's confirmed.

I'm excited, you know, if it's about the real world.

If it's not confirmed, I'm surprised.

Oh, what happened?

But if things matter to me, then it's not just excited, surprised, or I'll be happy or I'll be sad, you know, I may be devastated.

but if something happened on the outside and i didn't expect it because it was too fast or too weird or just too some kind of i just didn't want to have that expectation those are the things that are frightening you see so those are or vice versa if it's on the inside and i just refuse to make certain expectations or they're just too sudden or they're too weird uh those are the things that are disgusting

So that's an emotional model, but it says that in the emotional world, it works a bit different than in the cognitive world.


SPEAKER_01:
Yeah, some of the citations and work on emotions and exactly that, like when the rate or type of change of a primary model, that is where certain emotional phenomena could be modeled.

And, but again, how you actually, what phenomena you choose, like for example, if we were doing a body temperature example, it's like maybe if the first derivative of surprise through time, or like the time rolling average of surprise is low, even if our body temperature is going up and down, our anxiety level is low.

But then if there's rapid changes,

there's a second level model or a model that's taking it as observation, the first or the second derivative of the changes in surprise and body temperature, that that could be associated with an anxious response.


SPEAKER_02:
And maybe to say like active inference is active by nature.

So in these situations, you may refuse to have a generative model, you know, of certain kinds, because that's not the generative model you want to live by, you know, or so, then when things go outside that you just resolve to passive inference, you just throw up, you just throw up, or you just, you just lay down and die, or just, you know, you go into some kind of reflex modes where there is no generative model, you're just acting, you know, passively passive inference.


SPEAKER_01:
I don't know if there's no generative model in that situation.

A person looking at that in an observation setting, it's not like just because a squirrel has a flight or fight, it doesn't cease to have a generative model.


SPEAKER_02:
um the well that's like no i guess so the thesis would be that in certain situations you don't need a generative model you just need to act and you just act as your program you don't need to learn you're just going to you know you're just going to but i think that is still a different model right it's just that you have maybe a different like a like like a refined or um confined set of actions that you could take as a result of whatever you're perceiving right

Yeah, no, like, for example, like death, like, I don't have a model of my death, I don't think about it, it's not healthy to think about death, you know.

So typically, like, you would not want to model certain things like that at all.

You just don't touch them.


SPEAKER_00:
They just know that you do have a, I don't know, like a state that is a reaction to death or thinking about death or in fact,

in your case the state would be not thinking about death right exactly so if i'm that's still part of your generative model i think well what i'm trying to say is that there's something more to life than just a generative model right


SPEAKER_01:
There's the territory.

I mean, this is just there's the territory.

This is the thing, ethology, behavioral scientific approach to modeling cognitive systems.

But it's like, yes, there's an infinite number of points that are sort of corollary, like that maps are not territories restated in any number of important ways.

like the structure of the observations coming in from the thermometer does not have to be anything having to do with what the cognitive model of the agent as set up by a given individual is.

And that's sort of the refuge of the instrumentalists because it's like, here's our proposal for how to reduce our uncertainty about this.

Do you have another tool?

What other tools are in trade-offs with this one?

Whereas the realist is actually looking to wrestle on that more ontological reality mapped dimension about things as they are rather than more of the statistical

approach to modeling perception and behavior as perceived which is why but that's part of this fascinating view from the inside view from the outside dialectic because people will bring up first-person phenomenological experiences and there's so much work on neurophenomenology and cognitive modeling from first-person perspective whereas in

behavioral science and entomology and things like that, it's not waiting for some kind of first-person report.


SPEAKER_02:
And I think there's a dialogue between the continuous mind and the discrete mind.

There's a procedural continuous mind that just does its thing reflexively.

But it looks... They're helping each other.

And so there's this discrete mind that's managing this generative model.

And it basically is able to... And somehow they're able to do... But sometimes the discrete mind just lets go and it just goes into continuous mode wherever things tumble.


SPEAKER_01:
Well, that's sort of what is laid out here.

But this whole thing is a generative model.

So it's not like there's any utility as far as I'm seeing it just to...

district the whole thing is the generative model and now whether it's done in a more habitual fashion or it engages in a discretized tree-based policy rollout or some other proposal for like recurrent switching between different modes however it's accounted for that accounting is the generative model

but it can engage in completely, you could say, well, I have a generative model where at the top level there's a switch and it's counting apples and then it switches to counting oranges.


SPEAKER_02:
I guess the presumption that I'm making is I'm drawing the line not between an organism and its environment, but between the organism's internal mind and then it's whatever mind it has that represents that environment, like the unconscious.

So on a human being, like it'd be the conscious and the unconscious, there's a gap.

That's what's being modeled with all of this framework.

So the unconscious is really just part of the world, so to speak, and you're trying to talk to your unconscious.


SPEAKER_01:
manager you know relate to your unconscious perceive it and and act on it yeah like the b matrices in the teammates example in chapter seven it's like these are the mouse's beliefs about location contingent upon it doing what and then that in this case gets just sort of faithfully enacted by the environments

but that part is not emphasized here.

But then there's situations like the windy grid world, which is a classic grid world kind of variance where you have that kind of up, down, left, right movements.

And then there's wind that blows it like zero or one or two cells.

in that situation or icy grid world where it slides occasionally so but but the agent's model might not it might be like whoa now i'm over here so it doesn't even know about wind or ice it's just like oh but i want to expect to be here i expect to get information if i go here but yes


SPEAKER_02:
So that's something in the triangle.

I talked about the triangle language, geometry language I'm doing.

And then I realized a good way to do it that's in this spirit is that the participant, the agent or the triangle creator, the one who's looking for these triangle centers, they only understand triangles in terms of integer lengths.

So it's basically like rational numbers.

So they don't have a concept of the real number.

So they're trying to find the center, which may not exist as a rational number.

And so that's the kind of like it's already in a paradoxical situation where it's trying to do something basically impossible.

So it'll do it as good as it can, and then it'll move on to another task, let's say.

So that's a nice.

So it's nice that it can enter into the model that way, naturally.


SPEAKER_01:
oh yeah like let's just say there was a super sharp preference for to land on the number 10. um and but you had the policy you could only move up or down by 50 to 80. so you were just sort of like overshooting and then undershooting overshooting and undershooting um and then you could have another level of the generative model that was about task switching

And there's different ways to do task switching or like going back to the the the playing dead example, like you can either have stay as one of the actions in a one layer model up, down, left, right, stay.

Or you could have a two level model where the top one is a binary decision to move or stay.

And then within move, there's four actions like

Those are all different maps.

And again, which map you make of the city depends on what you want to do with it, who's funding you, all these different kinds of questions.

What cognitive model you make is contingent upon what you want to account for, what you do and don't want to be surprised by.

And especially when it comes to these cognitive functions and modeling them, like...

where these models sit from mainly taking in data even though the in in theory these are totally just two two directions in the same window especially when we get to chapter nine with the empirical data analysis

But sometimes the model is set up to be taking in data and the modeler is interested in the parametric estimates at the ends.


SPEAKER_02:
So to use this framework to model, let's say, human experience, whatever I would find interesting, I think it seems like the crucial thing is to discover, well, what is the prediction and what is the error?

Because is that how I have to think in terms of in order to apply this active inference framework or are there other concepts I have to worry about?


SPEAKER_01:
You don't have to worry about anything.


SPEAKER_02:
I want to worry about something.


SPEAKER_01:
I think it's relevant to look at if we're thinking in discrete time.

But figure 4.3 shows the isomorphic nature of the continuous and the discrete time generative model kind of meta family units.

Again, these can be nested, composed, concatenated, all those kinds of category, theoretic operations are possible, but this is the motif, which is like, what is the unfolding state space?

What observables are sloughed off of that unfolding state space?

And then what are the interventions in that unfolding state space?

So there's sort of simplification cases where it's like, there's only one action.

So just it unfolds according to just a Markov chain without policy.

It's just a Markov process.

And then you go, okay, well, what if there were multi-way options, like in the sort of Wolfram computing ontology and the multi-way compute where there's like different bifurcating possibilities from a given position, then it's a Markov decision process.

And then if the observable is the chain unfolding itself, you don't need a partial observability elements.

then if you want to model sensory ambiguity or some kind of degenerate or or non-linear or ambiguous mapping between the true unfolding dynamics and observables to a given measurement maker then you need something like an a matrix


SPEAKER_00:
So if I could just interject there for a moment, because there's not much time left, and I have to go at 9.

Speaking of the A matrix, can we go to figure 7.4?

Yeah, these matrices.

They're not too complicated, but maybe we can just kind of go through each one and just be very clear about what they are.

So the caption, anyway, says... I think it's a typo in the caption, right?

It says...

It says each element of these matrices is the probability of the outcome illustrated at the end of the row, conditioned on the context being one.

And being the location indicated by the row.


SPEAKER_01:
So there should be the context being one.

Here's where it would have been clear if it was written as a one, but there's two contexts.

This is context one with the food on the right.

This is context two with the food on the left.

So here, flipping back and forth, we see basically, we see these two are flipping and this 298 is flipping.

So basically there's two A matrices in this example.

There's what context?

What context is one in?

That's A2.

And then there's what location am I in?

That's A1.


SPEAKER_00:
Okay, so that's confusing.

I thought the context meant what the rat saw the queue to be.

Yes.

The queue to be.

Yes.

The context is where this thing actually is.


SPEAKER_01:
There's kind of two scenarios, two actual scenarios.

Right.

When the context is unknown, that's this one.


SPEAKER_00:
Okay, so let's read off one of these columns just so that we can understand it.

So like in cell one and A2, this is the probability that the rat is going to be in.

So I think it's a typo.

So it should be that the rat is going to be in the state indicated by the column, right?

Given that there's no context or that what?


SPEAKER_01:
If I'm in the start location.

Okay.

I am sure that I don't know the context.

Don't know the context.

Okay.

If you're in the bottom location.

I'm going to find out what the context is.


SPEAKER_00:
So...

Then you do know the context then.


SPEAKER_01:
Yeah, that gets injected in when it does go there, when it discovers R. So then when it goes to the bottom, this is where A1 comes into play.

Okay.

When I get to the bottom in the context one, this is the difference between these two.

Yeah.

Then I find out that the Q is R.


SPEAKER_00:
Okay, so it's the probability that you find out that the cues are when you're, when you, hold on, sorry.


SPEAKER_01:
This is the observation.

And each observation, we're getting two features from the environment.

This is the no food.

Sorry if I said that this was the no context earlier.

This is neither food nor not food.

Okay.

This is yes for food.

This is no for food.

So starting position, I'm sure I'm in the starting position and there's no food.

Bottom, I'm sure that is telling me the Q is R and there's no food.


SPEAKER_00:
Okay, so let's go to row two because that's where something interesting is happening.

This one?


SPEAKER_01:
Yeah.

Yeah.

My belief is that if I go left...

that there will be a 2% chance of finding food and a 98% chance of no food.

If I go right, vice versa.


SPEAKER_00:
Right, but it's all given that the Q is on the right, that the Q points to the right.

yes so so does the rat know that the q points to the right yes so in this case it would have had to have learnt that that r means this and l means this okay so we assume so i think that's what i was missing so it's already been trained it's already been trained on the kit

Okay.

See, I didn't, okay.

I thought I had to like go down first and get the cue and then make a decision.


SPEAKER_01:
There's no learning in this model.


SPEAKER_00:
Okay.

So it just, so we assume that the rat has seen or knows or learned the cue.

Exactly.


SPEAKER_01:
And this is the whole, you know, journey.

Are we studying the learning acquisition of the cue or are we just looking at forward inference on a behavioral trial given learning?

Given learning.

Okay.

Thank you.

cool okay um thanks all again like there was daylight savings in some places but the place of reference look at the utc time in the coda look at the calendar events the utc time is not changing the calendar events don't change so that's where you'll see everything so good thank you thank you bye
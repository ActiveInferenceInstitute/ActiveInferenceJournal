start	end	speaker	confidence	text
1290	145530	A	0.9349875862068961	Welcome. It's cohort one meeting, 17 already. And it's September 23, 2022. We're having our first discussion of chapter seven in the textbook. Well, we have many ways to go. We have questions on chapter seven that people have been adding. We also have some summaries and overviews and can walk through the text directly. But first, does anyone want to just raise their hand or unmute and give any thought that they had on chapter seven, their experience reading it, their understanding of where it's situated in the textbook overall, what was in it, or what was not in it, et cetera? Okay, well, open again. Please raise your hands or write in the chat if you want to address anything at any point. I want to open by acknowledging a lot of the contributions Ali made and in some conversations that he and I had earlier this week that I believe all of you cohort wanderers will find interesting. But first, let's just start with this quotation. We're going to start with the opening quotation of the chapter. Then I'm going to surface some discussions with Ali, and then we're going to go into some details of the chapter. So the quote is, What I cannot create, I do not understand by RF. So what does anyone think about that quotation? Or what does it mean in this context? Um.
150780	213550	B	0.9300448913043479	I think I mean, it's more applicable. Applicable? Applicable. The more complex. The thing is that you're trying to build. But what priors or what way could your Generative Model create an accurate prediction of the operation of something or the underlying dynamics of something if you do not kind of step by step generate the affordance, take the actions that generate those affordances, I don't see any causal path to doing that. You would have to step through it necessarily, if it's computationally complex. So that's most things in the world.
214000	274690	A	0.9320162385321094	But yeah, awesome. It's like this is the low road answer. Like, how can the Generative Model have anything like understanding without generating? You can't just have it on the shelf. And then someone has added a mild answer. We can't have understanding just through mental envisioning that the algorithms need to be implemented for a learner's journey. And then this is also an even deeper or stronger point, which Friston and others have been working on for a long time, which is like it's sentient artifacts in the world that will be the realization of active inference. It's not just like some nice derivations. Brock and anyone else.
275300	339700	B	0.9356830555555552	I was just going to add something about hidden states. There again, coming back to computational complexity, but there's things now that we're starting to build that we don't really 100% understand all of the dynamics of what is or just in general. We invent things that we don't completely understand first. And then based on the observational, based on the evidence that we observe, that it is consistently exhibiting some behavior that directs our attention that directs where our generative model pursues more observations. But we're definitely getting towards a point where that's kind of no longer going to be possible in the way that we usually try to use math to just shortcut stuff, where the operation of the dynamics of the things that we're going to build are necessarily going to be the proof. Their existence will be their own proof.
343000	863800	A	0.9311357407407421	Awesome. Okay, so just fun starting quotation upload. That one for sure, though there's some detailed things about the examples and some of these I've been working on. Everyone is welcome for every chapter to be contributing on these pages. Like kind of just trying to overview what these examples are because a chapter, it states it up front, yet I missed it the first several times reading. It like these illustrated models of every color in the rainbow are the section titles, and those are the functionalities that are getting layered in sometimes building on the same model. Like there's like Team A's version one and then there's Team A's version two. Other times it's switching between models or showing two different models that illustrate the same cognitive functionality. Other times the simulation outputs are shown, other times they're not. So there's like a lot of heterogeneity in the observations in this chapter, but it really wasn't until I saw that these first paragraph words were the section titles that it made more sense how the chapter was being laid out. So that's just kind of one note. Ali, do you want to raise any thoughts or describe a little bit about our discussion on how we're going to move forward on the equations? Okay. If you unmute, then go for it. Otherwise okay. Yes. No, Mike. Thank you. Ellie. All good. So we were discussing how there are several layers. As with other scriptural traditions, there's levels of readings. So there's a level of reading that is even more granular or below where we've been currently targeting. Like, here's equation 25, there's a reading for accessibility and understanding that would potentially be like F open square bracket, q comma y closed bracket equals negative sign expectation over Q, open parentheses, et cetera. So it's like the reading of the notation. Then the notation can be substituted for terms in the active ontology, which is like the level that's being described here, but this is not to be seen here. This is the level where notation could be aligned across papers because their notational reading would be different, but then they might be referring to the same exact composition of the active ontology. Then once the ontology definition has been squared, the terms could be condensed into meaningful units like accuracy, complexity, risk, expected risk that gives symbolic in the more conversational sense rather than in the notational sense. And then that's where memes and themes and rhetoric around what active inference formalisms say start to arise. And then there's the consequences of changes in relationships within and outside the formalism because it's kind of like there's this larger structure for like a POMDP and changing one variable. If you're only looking at one view or one kind of sub formalism, turn it up to eleven yet that might influence other variables that aren't part of that formalism itself. And so this is like a very open question that touches on everything from the implications of changes to relationships that aren't formally specified. For example, the Bayes graph, the edges reflect a certain type of relationship as we've explored, but changes from one can propagate throughout a whole system, potentially in nonlinear ways. So it's always going to be this kind of open area. How changes, including counterfactuals about the formalism, what if risk were situated this way instead of that way? Or how does that relate in this case? So this is kind of an open one. Again, people please update it and modify it. But some of these equations which we'll get to. So we'll come to why we had that conversation in a 7.8 later. But that was like one really interesting conversation. And then there was one other okay, then just to kind of jump into a chapter seven topic but also kind of reframe our learning journey as we're all working through this for the first time in the textbook group. So they wrote, um, this choice in the teammates and then Eric or anyone else. I know that you had some questions on the teammates, or we can go to the earlier examples, but this choice of what to do do you seek out the informative queue and then go to the arm that you've now reduced your uncertainty about where the reward is? So do you take an epistemic action and then have a better chance of making the pragmatically good choice or do you just go and make the pragmatic choice? Do you take one step up or do you take one step back for two steps up? It speaks to the exploration exploitation dilemma in psychology and machine learning. A dilemma that's resolved under active inference. So we were just cracking up because first a low familiarity learner might not know what these terms are or like why it's relevant to even talk about this situation. Familiarity might look like knowing metaphors, examples, intuition, some relevant citations or historical anecdotes about this. The understanding in terms of what the active inference formalism says about this, if it's resolved under active inference then understanding that resolution is about the basis and then from understanding there could be like further developments and applications and then we'll come to this. But what was making us laugh was surely there are parameterizations in which the rat and the teammates always does one thing or always does the other. So it's not that the explore exploit trade off is simply resolved under active inference even though people commonly point to things like the ability to rapidly switch between exploratory and exploitative behavioral policies because there's parameterization of that model and structure learning including that that has to be accomplished such that the model can be in a kind of critical point where it behaves adaptively and thus in a situation manages the explore exploit tension under certain constraints. But how could it be said that active inference resolves that dilemma? So maybe to whomever wrote this one? What is happening? Yes, please. Eric, classical Eric writing here. So what about this?
865870	986270	C	0.9324412318840579	Yeah, I'm kind of the heckler at the garden party here. You can see in most of my comments and questions but yeah, maybe it's just best to I mean it's maybe better read than spoken but yeah, I don't see this as a valid claim that they well, first of all, I don't know why they call it a dilemma for psychology. It's a trade off. It's been understood for years and there's ways of formalizing it and you solve it through optimization. It might be a dilemma for a creature because they don't know what they should do, but that's not a dilemma for the field. And then I guess the other paragraph is I don't see that they offer any resolution. All they do is they put it into the same they use a common language, free energy. They put exploration and exploitation into the same equation and call it with this information based term called free energy which is perfectly valid. That's great. But that doesn't resolve the problem because as you pointed out, you've got parameters and the designer or somebody has to figure out what those relative weights are, the parameters are. And so you really haven't advanced anything fundamentally, simply by using this free energy way of expressing what the trade off is as opposed to some arbitrary measure called energy or preference or something like that. So I just think that it doesn't help them when they overclaim, which is my opinion and you can convince me otherwise, I'm open. But it seems to me it doesn't help them by overstating their contribution to resolving exploration versus exploitation.
988210	1097650	A	0.9275964248704653	Yes, very insightful. I actually totally agree. I purposely tried to not mention it as a dilemma either. I think tensions, trade offs, these are all valid but that's kind of micro linguistic and a more serious issue is that claims that do crystallized out from the text and things that people can copy and paste and quote if this were a courtroom scenario did you say that active inference resolves exploration exploitation? The answer has to be yes, it was claimed in the book. Could somebody contextualize that like it's providing a first principles way where there are tunable or learnable or parameter sweepable features of models that sit at a really well positioned intersection of human interpretability and manifolds of relevant model variation? Yes, in an extremely charitable reading one could kind of bring this from coming off over the edge back onto the table. That would also make active inference less surprising and less realistically hyperbolic and be part of this process of seeing the evidence in the realization, not in the kind of ironic like we have a different partitioning of a statistical variable value. So nice points Brock.
1100470	1201490	B	0.9185357668711652	I guess I wanted to agree with that and then also maybe play a little devil's advocate, it's all information so solved just because it's been formulated or renamed to something pragmatic value, epistemic value, it doesn't seem like that resolves it or something. I'm wondering if the reading of that though could be not that there is still a trade off but that the dilemma. Is that what they're talking about is that the trade off, like when do you explore or when do you exploit? Not that there is a trade off but that is this last sentence and a half here. The resolution stems from the minimization of free energy. Is that whether you would seek pragmatic value or epistemic value is conditioned on which your model believes to be the free energy minimizing choice though you still have the dilemma, so to speak, or the trade off. But which side of the trade off you take is maybe resolved.
1203990	1204430	C	0.97495	By a.
1204440	1207560	B	0.8981	Generative model here or not, I don't know.
1209610	1381188	A	0.9288187016574588	Thanks Brock. I think the field is set for a situational resolution that may be as good or maybe better than other ways that it's been addressed but it comes down to the exact numbers that are chosen. So like in the example of the teammates. So we can look at for example this zero six and negative six. What if it was negative 3000 and positive 3000? And so this came up in the live stream 45 with Ryan Smith on the folk psychology where the intensity of the preference was represented. And so if the preference is so we prefer having more food, is that going to be 1000 versus zero? Because if so even the scantest probability of achieving food will be pursued. In other words, that model is going to be parameterized way, way on one side and it's going to exploit only or it's only going to take locally greedy behavior. Whereas if someone said well my behavior for food is zero zero one then maybe even the most obvious strategy would not be undertaken because it's so close to having a flat preference. Why is zero one versus why is 3000 too high and zero one too low? Well it has to do with the ratios and the interactions of a lot of the parameters. So it's not even going to be like well my preference is three comma one. That is going to be just like two parameters being drawn out from a potentially massive parameterization of a model. And so preferences just only speaking of preferences may be very hard to compare across situations because it's not like plus six for the preference for food and negative six. It doesn't have a semantic meaning in the world. It's literally a model parameterization. And so then the question becomes how can this framework or this way of having of combining probabilistic inference and energy based inference for strategic decision making, how can those parameters be tuned into a region of subspace or manifold where the behavior is flexible and adaptive? So I think the stage is prepared for a first principles resolution in charitable reading.
1381284	1381880	B	0.58254	Sorry.
1382030	1382648	A	0.937615	Yes, please.
1382734	1394556	B	0.8950429166666668	Yeah, I was just going to say Ali shared this paper meta control of the exploitation exploration exploitation. It's in the nearby chat link.
1394738	1395470	A	0.99995	Nice.
1396800	1432570	B	0.8934332098765433	But yeah, it's not that it solves it like in some grand it eliminates exploration exploitation or that it even completely currently it's just the affordance, I guess seems to be there to explain which one why. Like you're going through that there's a preference heavy here or heavy there and it's not that it's base optimal, right? It's not that it makes sense or it's adaptable or anything like that necessarily, just that there is some causal through line, maybe.
1433500	1434296	A	1.0	Yes.
1434478	1553410	C	0.9210364044943817	Eric yeah, so I want to first read Ali's comment here. The critical point to establish is to define action selection as a balance between maximalization of expected reward and expected information gain which are functionals of posterior beliefs about latent states of the world. So that touches on one of the questions which I also wrote comments on, which is about the team, a's example in particular, and I don't know if we'll have time, but we may get to that one today. But the questions specifically I want to raise about this comment, or maybe I guess this is a quote about there being a balance between maximization of expected reward versus expected information gain. That's one way of having a trade off. But you might look for an agent that only wants to maximize reward in the long run, but in order to do that, that entails information gain and that therefore gives a motivation for why you want to attend to the other term, the information based learning based term or knowledge refining a state based term. But that's in the service of the larger gain. And that's essentially what a reinforcement learning tries to do is say look, we're going to look ahead so that the moves we take now will bring us advantage in the future and there's a time discounting and all that. So that takes away the idea of information as being a value in and of itself, but it's only for a purpose. I'll raise that specifically with regard to the team. A's example in a different question.
1554660	1763010	A	0.9252135679611665	Yeah, thank you. The balance between Pragmatic and Epistemic is the optimization. So that is the question and that is the critical point and that's the situational balance that modeling is going to be evolving around it's like some situationally optimal or preferable strategy around. That is the point. But pointing to the point is not the resolution. Does active inference provide us a natural language grounded first principles approach? It may, but that is also not dilemma resolution. So onto some questions, but these are really great points. Okay, this is just an example of a type of question that I encourage people, especially if they're going back to the earlier chapters or just whenever they feel like making a contribution of this kind. These are like super helpful questions. Again, the questions we write, some of them may be included in really important educational materials, not the least of which many further textbook groups, but potentially even far beyond that. So what would you ask somebody to understand their comprehension of the materials? So I understand what are the rows, columns and numbers in these equations? And it even says it in the text. It's the probability of the next state. So the next state is in the rows, and then the current state is in the columns. And this gets distributed across everything so that the rows and columns sum to one as any other transition probability matrix would be expected to have. So we don't need to discuss it unless somebody wants to go into a little more detail. But this is going to be at the core of that third step of the recipe, which is you might have the structural form from the second step of the recipe in chapter six. And then the next step is to kind of specify or instantiate, like in code, what the dimensions are of these matrices. This is just saying like they are matrices. There's some prior tensor matrix, some hidden state, some transition matrix, some emission matrix. And this is like the structure step two of the recipe. Step three, how many rows and columns. And as Yaakov and others have been exploring, there's the rows and columns of the analytical representation with the equations. And then there's how Pi MDP does it, and there's how different computational realizations do it. Jacob, would you want to add anything about the dimensionality of these matrices? It could be interesting.
1767060	1906110	D	0.9247406810035834	Yeah, I can probably speak more to how Pyme DB deals with the dimensionality, which currently is probably adds unnecessary redundancy to them. Like if you're dealing with more complex state space or rather more high dimensional state space, and you have different modalities that you want to encode in these matrices because you're then performing matrix multiplications, you need to keep the dimensions of the matrices in those given modalities equal, at least in time DP not in other methods as far as I'm aware. But that means that then you have dimensions within these matrices that don't contain any information. So say you have a grid that you represent for the likelihood mapping of observation and state in physical world, but then you have another type of observation, like where is another agent located with respect to you? Are they in a certain direction? And that might be represented by five observations, say, like they're above, they're below, they're to the right, to the left, or not at all. Then you would need to encode the observation in the nine by nine matrix, but then you need to replicate it five times because that is the same for each of those relative observations. But then when you want to encode the likelihood mapping just for the second type of observation. I'm actually not entirely sure what that would be, but I'm presuming it's going to be like five by five, but then replicated. Nine times or something like that, just so the dimensionality matches. Because then you're doing inference over both modalities and at least in prime DP, you're then performing matrix multiplications over both of them.
1907200	2038248	A	0.9100103265306118	Thank you. Jakub. One way I'm kind of like seeing that is there's the sparsity of the graph. But then, in Pym DP or in any given computational implementation, the data structures, including potentially, like, auxiliary data constructs, may have a dimensionality that reflects potentially, at worst, a combination of the dimensionality of other aspects as part of even just a Temp file that you didn't even exactly specify. There might be some intermediates, stated or unstated intermediates whose dimensionality isn't merely the dimensionality of the analytical representation. It's a little bit of a subtle implementational point. But this is the mountain to climb, to implement these kinds of models. And also, just while we're on this topic, I think it's one area where CAD CAD is going to be very interesting to explore in active block for its package, because we also can imagine that the execution order of certain operations, even in a single agent setting, not just the multi agent setting, is really relevant. So there's that question. All right, so figure 7.2. All right. So the first example is about a musician playing music. We are not aware of this example being used anywhere else. Like the citations following those. This musical note type representation is not shown. And previously we kind of discussed how some of these trace black and white images are really ambiguous because of being like, intersecting black lines. And then, of course, the empty square. Yes.
2038414	2042830	C	0.9305388235294118	Can you say where those black lines come from? You want to dive into that right now?
2043360	2076276	A	0.9321060317460317	Yeah. Well, okay. The upper left, each black line, even though it can be hard to trace them, I think they're just being shown like representationally. They're beliefs about each note in the sequence at each time step. So the time steps are on the x axis and the belief in terms of a probability are on the y axis. So initially right.
2076298	2096440	C	0.9240999999999998	So there's some there's some sort of and this gets to another question that was raised, but maybe this is time to talk about it. There's some sort of engine, some sort of differential equation simulator or something running underneath here right. That is not discussed in the chapter, but that's generating these belief curves.
2096520	2180456	A	0.9266796178343947	Yes. It's related to some functions in MATLAB. And I was just going to mention that because my sense from the discrete model would be you'd have five, let's say, points on time zero, then you'd have five points on time one. So I thought, well, what is happening with this kind of like little fractal crash right here? How could you have a continuously differentiable oscillating type behavior when all you're calculating analytically is just 12345? Then I thought, is it meaning is, is the, are the points actually at the midpoint? And there's a differential equation that's sort of like ribboning an action perception loop through a discrete time matrix. But then if this is a function that's evaluatable at every point, aren't we in a continuous time setting? So wouldn't that have to imply like a machinery for unpacking a continuous time action perception cycle from discrete time specification? So I agree very much.
2180638	2181370	C	0.70737	Yeah.
2183180	2183976	A	0.74221	Good.
2184158	2275284	C	0.9302160846560843	My guess is what they do is they've got the equations, the free energy equations and the parameters are these distributions. Okay, what's your belief in these different properties? And those parameters evolve over time through a differential equation because you turn these things into a differential equation and when you change state something like, okay, now we've got another note comes in, then that's what's going to trigger the differential equation to go in a different direction, essentially. And that's why you've got the five discrete kind of regimes here or target points that it kind of evolves to. And then the next note comes in and now the boundary conditions change, essentially. So your differential equation takes you in a different direction. I recall that there's something like you guys probably discussed this more than I paid attention, but there's some sort of universal solver that they are providing. So you just plug in your matrices, your state equations, your transition matrices, and then it'll solve it all for you. And that may be what this differential equation solver is about. Do you think that sounds right?
2275482	2527280	A	0.9333381606765339	Yes, I think whenever, variously in the method sections and supplemental sections of paper, they'll say in the paper, we're only specifying the generative model and the matrices and then we use standard routines to address it. Now, I don't think that has the, it's not the pinnacle of accessibility and reproducibility. However, it does at least use standard routines. But again, those are not always specified in the paper, which is an issue. So it'll be important to regenerate examples within SPM and then also to use some of these things that we were discussing with the ability to move between different languages, but like VBX variational, bayes X, I don't know what the X is actually for. It's one of the core functions. It is actually doing the variational inference in SPM. And again, SPM is like this sort of like chimera package because it arose from the immediacies of needing to do neuroimaging registration and dynamical analysis that then broached into dynamic causal modeling, random field theory based statistical testing, permutation testing all these areas that are not themselves formally linked per se, but rather useful as part of the toolkit of a Neuroimaging researcher. Then, just as we were a little bit discussing like chapter two, and of course, many times earlier in that generalized Bayesian inferential framework, hierarchical modeling, including action, is actually not the hardest thing. Yes, implementing action is one thing, but treating action selection as inference, planning as inference, it has some challenges that arise relative to just doing time series anticipation, but then those functions became compiled into SPM in a limited capacity. So again, they go through some of this. But I think it's a key issue. We'll definitely look forward to what the free energy gradients are, but where the continuous nature of these curves and why? There are the numbers that there are 1234-5678, 910. Now there's five notes. Ten is two times five. There's a little bit more that needs to be fleshed out there. And then one question that was like, again, somewhere between testing our comprehension and being clear about what we're doing here, and in this ambiguity, we can ask it without any bias, the negative free energy gradients. I e prediction errors. What is a prediction error? A free energy calculation. Well, ignore gradient for a second. Free energy calculations are not prediction errors. Prediction errors are observations minus expectations. So this is like some finite value in the state space of what is being measured. Free energy is not that it references potentially those variables, but it does things that we know about, like KL divergences and such. Okay, so free energy is not prediction error. And that's a huge difference between, for example, a predictive processing framework and a free energy hierarchical predictive architecture.
2527440	2532360	C	0.9553061538461539	Eric but they're saying the gradient and free energy is a prediction error.
2532860	2926950	A	0.9369085967302466	Exactly. So even if it were that's, then that was the next question, which was, what's the difference between free energy and the free energy gradients? Are we talking about a landscape of derivatives, of free energy energies, or are we talking about what here? Is it being described as a gradient, as a landscape? And hence gradient ish because it can be calculated over some discrete, bi continuous state space. It's like five skyscrapers that you could have derivatives over in the Y direction, but you couldn't take the partial x. Or is it truly or is it a continuous landscape that just being tethered to the discrete values in x? But then again, that's that whole continuous, discrete question. Or are the values gradients, which has an even more complex interpretation. So we don't have the answers on these, but it's hard to understand how these figures could have epistemic or pragmatic value for learners or practitioners without some of these questions being resolved. All right, the first example, this is just kind of from walking through. So the first example was just hidden Markov model hidden states updating through time, Bayesian filter column and filter. This is like just standard signal processing in a Bayesian framework. And we're. Going to see a lot of echoes of the step by step model stream one, they give an even simpler example with static perception, one step perception and then go second to the dynamic case, then introduce action pi policy and then nuance policy through like uncertainty and so on. So that's the first inferential case they're then going to head into decision making and planning as inference. So we see policy being introduced as a variable that influences how states change through time. Does policy do that or does action do that? In the one step limit, policies are actions are affordances, but Pi is specifically reserved for sequences of actions. So is it that sequences of actions like do that or is this the actual base topology? But nonetheless, that architecture allows the evaluation of alternate policies in terms of their relative expect variational or expected free energy, depending on whether a one step policy is accomplishable with variational free energy that's, like the instantaneous move, most consistent instantaneous maneuver versus even one step in advance is going to broach into this whole expected free energy space we had on 130. So there's a really important discussion. Bozaki is also very interesting researcher neural rhythms and so on. Factorization is a really important topic and being able to distinguish like neurologically what do we mean when we talk about the what and the where stream, the dorsal and the ventral stream and critiques of it, et cetera. But just what and where in the brain and then in the factor graphs and computationally because the unfactorized models, if we're just doing the all by all by all by all, those become intractable very fast. And the specification of the sparsity of the model is one and the same in the variational base framework as the factorization of the model. So factorization is like kind of focusing your search efforts on manifolds where you've constrained certain things to be like linked or unlinked. That is what motivates the structure learning problem and the need to have to not be locked into factorization schemes at a given level of analysis. Okay, another just question to explore. Like here we have the top half of figure 43. There's just one A matrix. So why are there two A matrices here? Well, it's not an impossible question and the symbols go a long way. Here's the four locations that the animal can be in starting position, bottom position, left arm or right arm. Then here are the it's five on the X here because that bottom could, could reveal an R or reveal an L. In this case it reveals an R and the food is honestly there. And then this is a second a matrix that is describing how location is associated with the food. None aversive or positive. But this ties into the earlier discussion. Like it might be all one thing to say well it's four x five and then four by three. But is this actually a four x five by three. I know it's not. Eric, what do you think?
2927400	2957090	C	0.9012040277777777	Yeah, and they'd mentioned in there it's a tensor also. And the tensor means that you're stacking this one, this image against the version where the reward is on the left side. So the tensor is going to be four x eight by two. So that makes sense for being the tensor. So the state is another layer on top of this. The hidden state is another layer on top of that.
2958180	3129700	A	0.9248595597484274	Yes, section 73. So this would be a fun kind of like PH d. Qualifying level. Question just describe the whole team ace every row, every column and every value. Why is it that way? So we're not going to type it out here, but to understand that example and what the B matrices are, and just to have agility and just identifying the differences between these matrices numerically, like pattern recognition, like, oh, here there's two ones, here two zeros. Here two zeros, here two ones, here one and zero. These stay the same. And then to be able to transpose that in your understanding to what this means in terms of transition frequencies, every place where there's a difference, it's a difference that makes a difference. So being able to understand what those are is about understanding this example. And one can imagine, especially if they're wanting to make an application of active inference that's more complex than this, four x four, being fluent with how these matrices are constructed, their dimensionality, how differences in the generative process are incorporated, how differences in the generative model are incorporated, all these different features are important. Yes, Eric, this is definitely very important. Question agreed. Like it, it looks extremely neural, and these traces come up all the time. Similarly, here we have three discrete time points starting going to the Q, getting the L, and then going to the left arm. But then what is this? Step one and a half is the one, all of these points. And then by two, the uncertainty is resolved. The belief about certain things goes to zero, the belief about other things goes to one. And this is just a pure interpolation. But that is quite a specific interpolation, including what appear to be some like dopaminergic spikes or something that are not reflected at either of the time points.
3130310	3146540	C	0.9459303571428569	My guess is that you've got three different blocks there. So if you take the one as being the width, one third of the way across the two is another third and the three is another third, then that switch there that you see happens as soon as you transition from the one to the two.
3149860	3182120	A	0.9160372222222222	Yes, although still there's even the graphical interpolation interpolation. Question and it's so easy to be like reading is it just a quirk of the dashed line? But are there two dashed lines? And two dashed lines diverged in a neural trace? But it's kind of clear. These ones, the discrete formulations are clearer.
3183340	3184090	B	0.68584	Yeah.
3184720	3191420	A	0.930407142857143	Okay. Are rats prone to useless behavior? What do you think? What useless behavior?
3194640	3204050	C	0.9495424137931036	I wrote it out. Okay, this is good time for today, but that's my most provocative comment on this one. And maybe we can save that for next week.
3204420	3236270	A	0.9190505172413793	Okay, perfect. Let's pick up with this one and any other questions. Then we'll glance over the visual cicade. Look at the learning example. Consider the hierarchical example. There's the maze. Then there's the hierarchical example. Another nested. Discrete meets neural continuous trace. And then we end on Stefan's squared. So thank you all. Have an excellent day.

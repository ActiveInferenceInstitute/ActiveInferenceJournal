1
00:00:01,020 --> 00:00:03,600
hey everyone it is

2
00:00:03,600 --> 00:00:07,500
October 21st 2022 we're in cohort one

3
00:00:07,500 --> 00:00:11,160
the textbook group it's meeting 21

4
00:00:11,160 --> 00:00:14,759
and we're having our first discussion on

5
00:00:14,759 --> 00:00:17,599
chapter nine

6
00:00:19,619 --> 00:00:22,260
chapter nine is model based data

7
00:00:22,260 --> 00:00:26,119
analysis let's look at what the section

8
00:00:26,519 --> 00:00:29,419
headers are

9
00:00:30,359 --> 00:00:33,239
there's a short introduction

10
00:00:33,239 --> 00:00:35,640
a discussion of meta Bayesian methods

11
00:00:35,640 --> 00:00:37,980
which is going to be very interesting

12
00:00:37,980 --> 00:00:40,500
and in some ways is even like an entry

13
00:00:40,500 --> 00:00:41,340
point

14
00:00:41,340 --> 00:00:43,800
to thinking about

15
00:00:43,800 --> 00:00:46,980
entity modeling in active inference

16
00:00:46,980 --> 00:00:49,980
and so that'll be kind of fun

17
00:00:49,980 --> 00:00:54,480
we'll return to variational Applause

18
00:00:54,480 --> 00:00:56,460
resonating with our

19
00:00:56,460 --> 00:00:58,500
just completed discussion on chapter

20
00:00:58,500 --> 00:01:01,199
four and LaPlace

21
00:01:01,199 --> 00:01:02,940
then

22
00:01:02,940 --> 00:01:05,519
section 9.4 9.5

23
00:01:05,519 --> 00:01:09,360
are going to help us see where data

24
00:01:09,360 --> 00:01:12,840
in terms of like gigabytes of actual

25
00:01:12,840 --> 00:01:14,820
good day Jacob

26
00:01:14,820 --> 00:01:18,180
actual data from measurements and so on

27
00:01:18,180 --> 00:01:21,060
where these come into play with models

28
00:01:21,060 --> 00:01:25,159
so how does one go from a um furnished

29
00:01:25,159 --> 00:01:27,659
trainable model

30
00:01:27,659 --> 00:01:31,740
to a parameterized specified model which

31
00:01:31,740 --> 00:01:32,759
actually

32
00:01:32,759 --> 00:01:35,759
is explaining variance in real world

33
00:01:35,759 --> 00:01:37,259
data sets

34
00:01:37,259 --> 00:01:39,860
and then there's some examples of GMS

35
00:01:39,860 --> 00:01:44,340
and some models of false inference

36
00:01:44,340 --> 00:01:47,040
um well

37
00:01:47,040 --> 00:01:50,100
I added only one General

38
00:01:50,100 --> 00:01:51,479
question

39
00:01:51,479 --> 00:01:53,280
from

40
00:01:53,280 --> 00:01:56,759
um a lighter reading

41
00:01:56,759 --> 00:02:00,000
I think as we all hear

42
00:02:00,000 --> 00:02:02,759
move through this we can generate a lot

43
00:02:02,759 --> 00:02:06,079
of other key points and questions

44
00:02:06,659 --> 00:02:08,220
um

45
00:02:08,220 --> 00:02:09,899
ultimately the models described in this

46
00:02:09,899 --> 00:02:11,520
book are only useful if they can answer

47
00:02:11,520 --> 00:02:14,420
scientific questions

48
00:02:17,700 --> 00:02:20,700
but just we could

49
00:02:20,700 --> 00:02:22,200
rehearse the introduction again but

50
00:02:22,200 --> 00:02:23,400
let's just go

51
00:02:23,400 --> 00:02:27,000
Um into it so metabasian methods

52
00:02:27,000 --> 00:02:28,980
this chapter deals with utility of

53
00:02:28,980 --> 00:02:30,840
active inference formulations and

54
00:02:30,840 --> 00:02:33,180
analyzing data from behavioral

55
00:02:33,180 --> 00:02:34,379
experiments

56
00:02:34,379 --> 00:02:38,180
so one could imagine all kinds of

57
00:02:38,180 --> 00:02:41,040
bodily verbal

58
00:02:41,040 --> 00:02:42,300
digital

59
00:02:42,300 --> 00:02:44,400
Behavior

60
00:02:44,400 --> 00:02:46,019
this goes beyond proof of principle

61
00:02:46,019 --> 00:02:47,400
simulations we've seen in previous

62
00:02:47,400 --> 00:02:49,440
chapters and instead

63
00:02:49,440 --> 00:02:52,019
exploits active inference in answering

64
00:02:52,019 --> 00:02:54,980
scientific questions

65
00:02:56,280 --> 00:02:57,840
broadly speaking there's two related

66
00:02:57,840 --> 00:02:59,340
reasons for fitting a computational

67
00:02:59,340 --> 00:03:01,680
model to observe Behavior

68
00:03:01,680 --> 00:03:03,720
the first is to estimate parameters of

69
00:03:03,720 --> 00:03:05,959
Interest

70
00:03:07,140 --> 00:03:09,360
the second is to compare alternative

71
00:03:09,360 --> 00:03:12,319
hypotheses

72
00:03:13,680 --> 00:03:16,620
so to parameterize

73
00:03:16,620 --> 00:03:18,659
from data

74
00:03:18,659 --> 00:03:21,959
is one opportunity

75
00:03:21,959 --> 00:03:23,879
that is to

76
00:03:23,879 --> 00:03:24,420
um

77
00:03:24,420 --> 00:03:27,720
make some model of brain function and

78
00:03:27,720 --> 00:03:28,980
then

79
00:03:28,980 --> 00:03:30,239
um

80
00:03:30,239 --> 00:03:33,360
understand within one person or one

81
00:03:33,360 --> 00:03:36,239
group or two groups

82
00:03:36,239 --> 00:03:38,099
how

83
00:03:38,099 --> 00:03:40,099
um

84
00:03:40,440 --> 00:03:44,459
you can consider those parameterizations

85
00:03:44,459 --> 00:03:48,540
to be phenotypes

86
00:03:48,920 --> 00:03:52,620
phenotype is something of a biological

87
00:03:52,620 --> 00:03:55,319
organism or system that's measurable so

88
00:03:55,319 --> 00:03:58,200
like femur length is a phenotype

89
00:03:58,200 --> 00:04:00,000
but also a phenotype doesn't have to

90
00:04:00,000 --> 00:04:01,500
just be something that's measurable on

91
00:04:01,500 --> 00:04:04,620
the body with a ruler like phenotype

92
00:04:04,620 --> 00:04:08,040
might be distance ran in the first 50

93
00:04:08,040 --> 00:04:11,040
seconds after the hawk flies through the

94
00:04:11,040 --> 00:04:14,040
sky on a cloudy day

95
00:04:14,040 --> 00:04:16,139
and so then that is still a measurement

96
00:04:16,139 --> 00:04:18,478
that could be inferred or discussed or

97
00:04:18,478 --> 00:04:19,978
made

98
00:04:19,978 --> 00:04:22,320
and we're talking about computational

99
00:04:22,320 --> 00:04:25,740
phenotyping because the data and

100
00:04:25,740 --> 00:04:28,560
observations for sure are like a basal

101
00:04:28,560 --> 00:04:30,060
phenotype

102
00:04:30,060 --> 00:04:33,300
like the button being clicked was

103
00:04:33,300 --> 00:04:34,199
um

104
00:04:34,199 --> 00:04:37,620
measurable pheno means to show

105
00:04:37,620 --> 00:04:39,720
but also we could talk about like the

106
00:04:39,720 --> 00:04:41,759
Precision variables in our cognitive

107
00:04:41,759 --> 00:04:46,560
model as being phenotypic

108
00:04:47,100 --> 00:04:50,699
and also model comparison

109
00:04:50,699 --> 00:04:52,680
can be used

110
00:04:52,680 --> 00:04:55,860
so whereas this first modality of

111
00:04:55,860 --> 00:04:58,800
parameterization is like the data set is

112
00:04:58,800 --> 00:05:02,520
fixed we collected 240 fmri data sets

113
00:05:02,520 --> 00:05:04,860
now we're going to parameterize so we're

114
00:05:04,860 --> 00:05:06,660
going to give some

115
00:05:06,660 --> 00:05:09,540
um plasticity to our model and we're

116
00:05:09,540 --> 00:05:11,400
going to fit parameters so that so that

117
00:05:11,400 --> 00:05:13,380
our model is like the best resembling it

118
00:05:13,380 --> 00:05:17,520
can to these 240 fmri data sets we have

119
00:05:17,520 --> 00:05:19,740
in the second modality

120
00:05:19,740 --> 00:05:23,820
we're treating in some ways the models

121
00:05:23,820 --> 00:05:26,220
as fixed

122
00:05:26,220 --> 00:05:29,100
and then evaluating

123
00:05:29,100 --> 00:05:31,080
to what extent

124
00:05:31,080 --> 00:05:34,259
different models

125
00:05:34,259 --> 00:05:36,900
Stack Up

126
00:05:36,900 --> 00:05:38,699
and that can be used at a very fine

127
00:05:38,699 --> 00:05:39,900
scale

128
00:05:39,900 --> 00:05:42,180
to look for different models that fit

129
00:05:42,180 --> 00:05:45,539
better to a given data set

130
00:05:45,539 --> 00:05:48,780
but also this is where we

131
00:05:48,780 --> 00:05:50,900
can talk about like

132
00:05:50,900 --> 00:05:53,880
actual biological explanations and

133
00:05:53,880 --> 00:05:55,560
predictions so like to give an example

134
00:05:55,560 --> 00:05:59,300
from yes please Ali first

135
00:05:59,960 --> 00:06:04,259
uh sorry a related question to this

136
00:06:04,259 --> 00:06:08,520
whole discussion is uh I think it was in

137
00:06:08,520 --> 00:06:12,539
section 9.2 uh it says this goes beyond

138
00:06:12,539 --> 00:06:14,940
the proof of principle simulations we've

139
00:06:14,940 --> 00:06:17,479
seen in previous chapters and instead

140
00:06:17,479 --> 00:06:20,220
exploits active inference and answering

141
00:06:20,220 --> 00:06:23,880
scientific questions uh I didn't quite

142
00:06:23,880 --> 00:06:27,360
understand this statement here because

143
00:06:27,360 --> 00:06:28,919
uh

144
00:06:28,919 --> 00:06:32,639
in the previous chapters uh we were we

145
00:06:32,639 --> 00:06:36,900
were also engaged with modeling uh the

146
00:06:36,900 --> 00:06:39,840
real scientific phenomena right so what

147
00:06:39,840 --> 00:06:42,360
does proof of principle simulation here

148
00:06:42,360 --> 00:06:47,039
exactly mean and how does it uh differ

149
00:06:47,039 --> 00:06:50,340
from the approach uh I mean from

150
00:06:50,340 --> 00:06:55,080
answering a real scientific questions

151
00:06:55,080 --> 00:06:56,940
yeah good question

152
00:06:56,940 --> 00:06:58,680
does anyone want to give a thought or I

153
00:06:58,680 --> 00:07:01,400
can give a thought too

154
00:07:02,280 --> 00:07:02,819
um

155
00:07:02,819 --> 00:07:05,940
I mean literally just these following

156
00:07:05,940 --> 00:07:10,860
that the first and second reasons to use

157
00:07:10,860 --> 00:07:11,819
um

158
00:07:11,819 --> 00:07:14,780
active inference in this way it's like

159
00:07:14,780 --> 00:07:19,020
instead of just modeling something

160
00:07:19,020 --> 00:07:22,199
and saying oh that looks like

161
00:07:22,199 --> 00:07:24,660
what what we intended the model or oh

162
00:07:24,660 --> 00:07:26,340
gosh it looks really predictive or

163
00:07:26,340 --> 00:07:27,300
whatever

164
00:07:27,300 --> 00:07:29,880
getting more rigorous with it to

165
00:07:29,880 --> 00:07:31,680
specifically

166
00:07:31,680 --> 00:07:34,500
you know enumerate parameters for a

167
00:07:34,500 --> 00:07:36,539
particular model

168
00:07:36,539 --> 00:07:37,680
right

169
00:07:37,680 --> 00:07:40,860
and and then compare multiple models

170
00:07:40,860 --> 00:07:43,860
like trying to say

171
00:07:43,860 --> 00:07:46,800
which one is you know uh

172
00:07:46,800 --> 00:07:48,900
more precise that's that's a different

173
00:07:48,900 --> 00:07:51,660
thing than what has was being done in

174
00:07:51,660 --> 00:07:53,819
the book previously like we're just

175
00:07:53,819 --> 00:07:55,919
building up the model and seeing how it

176
00:07:55,919 --> 00:07:59,520
kind of worked before and now we're like

177
00:07:59,520 --> 00:08:01,680
talking about putting it into a like a

178
00:08:01,680 --> 00:08:04,259
Machinery that's going to you know uh

179
00:08:04,259 --> 00:08:08,160
select on models like which ones best

180
00:08:08,160 --> 00:08:10,759
fit or whatever

181
00:08:12,900 --> 00:08:16,139
well said I I totally agree

182
00:08:16,139 --> 00:08:19,620
um preview although arguably like the

183
00:08:19,620 --> 00:08:21,720
chapter five neurobiology the tables of

184
00:08:21,720 --> 00:08:23,400
examples were answering scientific

185
00:08:23,400 --> 00:08:25,020
questions but those generative models

186
00:08:25,020 --> 00:08:27,419
were not largely presented but in the

187
00:08:27,419 --> 00:08:28,979
papers

188
00:08:28,979 --> 00:08:31,379
one could have gone in but yes the

189
00:08:31,379 --> 00:08:32,880
previous generative models were like

190
00:08:32,880 --> 00:08:34,440
built from the ground up

191
00:08:34,440 --> 00:08:36,080
and then like oh look at this

192
00:08:36,080 --> 00:08:38,520
interesting Behavior

193
00:08:38,520 --> 00:08:41,940
now we're building the Machinery to take

194
00:08:41,940 --> 00:08:43,080
in

195
00:08:43,080 --> 00:08:45,240
real data

196
00:08:45,240 --> 00:08:47,700
and fit that

197
00:08:47,700 --> 00:08:50,880
and then there's the the two uses

198
00:08:50,880 --> 00:08:53,760
of parameterization or the two uses of

199
00:08:53,760 --> 00:08:55,740
data fitting

200
00:08:55,740 --> 00:08:59,160
to find the best parameters for a given

201
00:08:59,160 --> 00:09:02,100
data set and model

202
00:09:02,100 --> 00:09:04,920
and also to compare models

203
00:09:04,920 --> 00:09:07,140
model comparison

204
00:09:07,140 --> 00:09:08,339
all right

205
00:09:08,339 --> 00:09:10,800
now we get to the very excellent figure

206
00:09:10,800 --> 00:09:13,320
9.1

207
00:09:13,320 --> 00:09:15,660
so

208
00:09:15,660 --> 00:09:18,199
oh

209
00:09:18,959 --> 00:09:19,940
um

210
00:09:19,940 --> 00:09:21,480
9.1

211
00:09:21,480 --> 00:09:25,820
is the meta Bayesian inference

212
00:09:26,760 --> 00:09:32,300
what does anyone see in this figure

213
00:09:37,380 --> 00:09:39,920
Ali

214
00:09:41,480 --> 00:09:45,480
well as pointed out in the text uh the

215
00:09:45,480 --> 00:09:47,120
inner box

216
00:09:47,120 --> 00:09:51,180
somehow represents the generative model

217
00:09:51,180 --> 00:09:55,380
of the phenomena we're trying to we're

218
00:09:55,380 --> 00:10:00,180
observing but the outer dashed box is

219
00:10:00,180 --> 00:10:04,019
somehow our generative model as The

220
00:10:04,019 --> 00:10:08,959
Observers so the term metabasian I think

221
00:10:08,959 --> 00:10:12,839
used here to denote this kind of change

222
00:10:12,839 --> 00:10:17,760
of perspective to higher uh to a higher

223
00:10:17,760 --> 00:10:21,380
level of viewpoint uh I mean

224
00:10:21,380 --> 00:10:27,000
to add another layer of hierarchy to the

225
00:10:27,000 --> 00:10:29,820
already hierarchical generative models

226
00:10:29,820 --> 00:10:32,540
we had before

227
00:10:33,720 --> 00:10:37,920
yeah great well said and this out so we

228
00:10:37,920 --> 00:10:41,640
we could have had an arbitrarily nested

229
00:10:41,640 --> 00:10:43,680
um well just to make a simple point this

230
00:10:43,680 --> 00:10:46,560
is the discrete time formulation pomdp

231
00:10:46,560 --> 00:10:48,060
figure 4.3

232
00:10:48,060 --> 00:10:50,640
and that can be nested in an arbitrary

233
00:10:50,640 --> 00:10:52,440
way and composed

234
00:10:52,440 --> 00:10:56,160
and meta Bayesian is like putting

235
00:10:56,160 --> 00:10:59,100
the wrapping paper on the gift because

236
00:10:59,100 --> 00:11:03,839
this outermost layer is us

237
00:11:03,839 --> 00:11:06,560
Brock

238
00:11:09,959 --> 00:11:12,120
um

239
00:11:12,120 --> 00:11:17,040
so the last time this came up was uh in

240
00:11:17,040 --> 00:11:19,140
the context I think it was in the other

241
00:11:19,140 --> 00:11:22,140
cohort but we were talking about

242
00:11:22,140 --> 00:11:24,199
um

243
00:11:24,540 --> 00:11:27,300
the you know provenance of the model the

244
00:11:27,300 --> 00:11:29,160
the impetus for it being

245
00:11:29,160 --> 00:11:30,000
um

246
00:11:30,000 --> 00:11:32,579
you know as a tool for

247
00:11:32,579 --> 00:11:36,240
uh diagnosis of patients for you know

248
00:11:36,240 --> 00:11:40,860
non-typical sort of neuro um so

249
00:11:40,860 --> 00:11:45,480
uh I'm I guess I'm curious maybe uh if

250
00:11:45,480 --> 00:11:47,339
there's a simple way to understand it

251
00:11:47,339 --> 00:11:51,000
from that perspective but I also have um

252
00:11:51,000 --> 00:11:53,220
a

253
00:11:53,220 --> 00:11:56,100
another question I guess which is

254
00:11:56,100 --> 00:12:00,980
If This Is Us modeling

255
00:12:02,579 --> 00:12:04,920
it seems like it would be like infinite

256
00:12:04,920 --> 00:12:07,019
really recursive because every time you

257
00:12:07,019 --> 00:12:09,060
add another layer like then there's

258
00:12:09,060 --> 00:12:10,980
another layer that's not being modeled

259
00:12:10,980 --> 00:12:13,380
of how we're actually modeling it and

260
00:12:13,380 --> 00:12:15,360
therefore we would have to but then if

261
00:12:15,360 --> 00:12:16,140
we

262
00:12:16,140 --> 00:12:19,079
account for that then

263
00:12:19,079 --> 00:12:21,600
then there would be another outer uh you

264
00:12:21,600 --> 00:12:23,820
know I don't know if that's just uh

265
00:12:23,820 --> 00:12:25,380
infinite regression towards the hidden

266
00:12:25,380 --> 00:12:26,160
state

267
00:12:26,160 --> 00:12:30,060
one or the other or but um

268
00:12:30,060 --> 00:12:33,360
or I'm just that's nonsense but

269
00:12:33,360 --> 00:12:37,380
yeah no no it's it's not I think

270
00:12:37,380 --> 00:12:38,760
let's let's

271
00:12:38,760 --> 00:12:42,779
talk about the uh setting of the

272
00:12:42,779 --> 00:12:46,139
clinical neuroimaging

273
00:12:46,139 --> 00:12:47,579
so

274
00:12:47,579 --> 00:12:51,360
the inner model is going to be the

275
00:12:51,360 --> 00:12:54,060
cognitive behavioral model of the

276
00:12:54,060 --> 00:12:56,220
patient just speaking of this the

277
00:12:56,220 --> 00:12:58,440
participant the recipient of action as

278
00:12:58,440 --> 00:13:01,200
the patient they're selecting green or

279
00:13:01,200 --> 00:13:02,579
blue coins or whatever they're doing

280
00:13:02,579 --> 00:13:05,220
some behavioral Paradigm isocating or

281
00:13:05,220 --> 00:13:07,200
choice or decision making

282
00:13:07,200 --> 00:13:11,760
we're modeling their action perception

283
00:13:11,760 --> 00:13:15,439
attention and so on

284
00:13:16,019 --> 00:13:18,180
whether we

285
00:13:18,180 --> 00:13:21,560
formalize it or not

286
00:13:22,860 --> 00:13:25,980
the experimenter's behavior is

287
00:13:25,980 --> 00:13:28,139
influencing

288
00:13:28,139 --> 00:13:31,139
the parameterization of the patients

289
00:13:31,139 --> 00:13:32,339
like

290
00:13:32,339 --> 00:13:36,300
we are choosing populations

291
00:13:36,300 --> 00:13:39,620
and sampling patients

292
00:13:44,100 --> 00:13:46,740
here is like the parameterization of the

293
00:13:46,740 --> 00:13:49,220
experiment

294
00:13:49,740 --> 00:13:51,600
we'll have 20 people they'll come in for

295
00:13:51,600 --> 00:13:54,779
two sessions we're gonna do 11

296
00:13:54,779 --> 00:13:56,940
um you know of this

297
00:13:56,940 --> 00:14:01,459
and here's like the data coming out of

298
00:14:01,680 --> 00:14:04,139
so this o here

299
00:14:04,139 --> 00:14:06,300
is the observation that the patient sees

300
00:14:06,300 --> 00:14:08,579
and their inference on what they're

301
00:14:08,579 --> 00:14:11,000
seeing

302
00:14:15,139 --> 00:14:18,000
experimental stimuli are actually an

303
00:14:18,000 --> 00:14:21,000
action from the experimenter

304
00:14:21,000 --> 00:14:24,600
we're pushing the o to their screen so

305
00:14:24,600 --> 00:14:29,480
this o tilde is their o sequence

306
00:14:29,940 --> 00:14:31,920
we're getting

307
00:14:31,920 --> 00:14:36,139
data from the experiment

308
00:14:39,120 --> 00:14:41,880
we could choose to

309
00:14:41,880 --> 00:14:45,540
be ignorant or implicit

310
00:14:45,540 --> 00:14:49,800
about our parameterization

311
00:14:51,060 --> 00:14:55,560
or maybe even this is a nested model

312
00:14:55,560 --> 00:14:59,459
we made a governance decision in the lab

313
00:14:59,459 --> 00:15:03,360
to decide who gets to decide this

314
00:15:03,360 --> 00:15:05,579
and the constraints were the cost and

315
00:15:05,579 --> 00:15:08,060
the availability

316
00:15:13,019 --> 00:15:15,360
nested

317
00:15:15,360 --> 00:15:19,040
modeling of the Outer Loop

318
00:15:20,820 --> 00:15:23,399
and what about infinite recursion

319
00:15:23,399 --> 00:15:24,779
and I think

320
00:15:24,779 --> 00:15:27,899
one silver lining or Saving Grace or

321
00:15:27,899 --> 00:15:30,360
whatever or just strength of the

322
00:15:30,360 --> 00:15:32,100
Bayesian graph or active inference

323
00:15:32,100 --> 00:15:33,480
framework

324
00:15:33,480 --> 00:15:36,079
is

325
00:15:36,300 --> 00:15:39,000
we know we can always

326
00:15:39,000 --> 00:15:40,980
Dive In

327
00:15:40,980 --> 00:15:44,880
and like build from a node or we can

328
00:15:44,880 --> 00:15:48,240
just treat the edges of our base graph

329
00:15:48,240 --> 00:15:50,639
as like a Markov blanket with the

330
00:15:50,639 --> 00:15:53,060
unknown

331
00:15:54,060 --> 00:15:56,220
so we could say we made a simple model

332
00:15:56,220 --> 00:15:59,459
of decision making here

333
00:15:59,459 --> 00:16:01,680
and then we could this could be a

334
00:16:01,680 --> 00:16:03,600
multi-agent simulation and each of those

335
00:16:03,600 --> 00:16:06,120
could have another nested model

336
00:16:06,120 --> 00:16:08,399
and then we could get it fixed then

337
00:16:08,399 --> 00:16:09,959
maybe it's irrelevant

338
00:16:09,959 --> 00:16:15,060
or maybe we want to know anyways but

339
00:16:15,060 --> 00:16:17,100
what good will it I don't know yeah or

340
00:16:17,100 --> 00:16:21,000
it doesn't fit and we do want to know or

341
00:16:21,000 --> 00:16:22,519
yeah let's just say that the the

342
00:16:22,519 --> 00:16:26,279
pragmatic value for us is um

343
00:16:26,279 --> 00:16:28,760
High statistical power and sensitivity

344
00:16:28,760 --> 00:16:32,720
for a diagnosis of a um

345
00:16:32,720 --> 00:16:35,040
neuro condition

346
00:16:35,040 --> 00:16:36,480
so like

347
00:16:36,480 --> 00:16:39,480
we want the inner loop

348
00:16:39,480 --> 00:16:42,600
to be in the point we don't want to be

349
00:16:42,600 --> 00:16:44,579
like spending money Beyond diminishing

350
00:16:44,579 --> 00:16:46,920
returns on our experiment

351
00:16:46,920 --> 00:16:49,079
we want to actually like diagnose and

352
00:16:49,079 --> 00:16:50,880
help the people in this limited

353
00:16:50,880 --> 00:16:52,560
situation

354
00:16:52,560 --> 00:16:53,639
um

355
00:16:53,639 --> 00:16:56,699
then one could do model comparison

356
00:16:56,699 --> 00:16:59,639
with just I'm ignoring all of this I'm

357
00:16:59,639 --> 00:17:01,500
just going to parameterize just just I'm

358
00:17:01,500 --> 00:17:03,360
going to call it like I see it

359
00:17:03,360 --> 00:17:05,699
then one could have another but but if

360
00:17:05,699 --> 00:17:08,459
one were to engage in a research program

361
00:17:08,459 --> 00:17:12,359
where we we collected one data set

362
00:17:12,359 --> 00:17:15,419
then yeah let's just call like we see it

363
00:17:15,419 --> 00:17:16,919
but now we have the opportunity to

364
00:17:16,919 --> 00:17:18,900
design a second cohort of fmri

365
00:17:18,900 --> 00:17:21,119
experiments

366
00:17:21,119 --> 00:17:23,220
should we do two people for a hundred

367
00:17:23,220 --> 00:17:24,599
sessions or a hundred people for two

368
00:17:24,599 --> 00:17:26,640
sessions who should those hundred people

369
00:17:26,640 --> 00:17:27,660
be

370
00:17:27,660 --> 00:17:31,260
so at that point having a policy

371
00:17:31,260 --> 00:17:33,179
selection

372
00:17:33,179 --> 00:17:36,179
outer loop enabling

373
00:17:36,179 --> 00:17:38,700
seems pretty relevant

374
00:17:38,700 --> 00:17:41,760
now we could make that

375
00:17:41,760 --> 00:17:44,280
um simple maybe it's we always pick 30

376
00:17:44,280 --> 00:17:46,140
participants

377
00:17:46,140 --> 00:17:48,299
maybe it's we either do a small

378
00:17:48,299 --> 00:17:49,919
experiment with three or we do a big one

379
00:17:49,919 --> 00:17:51,960
with 30. it's just simpler those are the

380
00:17:51,960 --> 00:17:54,179
two Pathways we have

381
00:17:54,179 --> 00:17:56,100
or maybe it's a

382
00:17:56,100 --> 00:17:57,299
you know

383
00:17:57,299 --> 00:17:59,280
Etc but then you could do model

384
00:17:59,280 --> 00:18:02,460
comparison and evaluation

385
00:18:02,460 --> 00:18:05,580
on those increasingly complex outer loop

386
00:18:05,580 --> 00:18:07,679
models

387
00:18:07,679 --> 00:18:09,600
none of that

388
00:18:09,600 --> 00:18:13,459
would be influencing

389
00:18:14,160 --> 00:18:16,980
the structure of the cognitive model of

390
00:18:16,980 --> 00:18:19,140
the patient

391
00:18:19,140 --> 00:18:20,460
so

392
00:18:20,460 --> 00:18:23,340
for all you block friends heads in the

393
00:18:23,340 --> 00:18:24,299
chat

394
00:18:24,299 --> 00:18:27,120
one could imagine that there's

395
00:18:27,120 --> 00:18:27,900
um

396
00:18:27,900 --> 00:18:31,679
human fmri cognitive model

397
00:18:31,679 --> 00:18:35,940
this is a versionable cognitive model

398
00:18:35,940 --> 00:18:37,799
open source

399
00:18:37,799 --> 00:18:43,320
and then different lab groups could have

400
00:18:43,320 --> 00:18:45,360
open or closed models with open or

401
00:18:45,360 --> 00:18:47,220
closed data

402
00:18:47,220 --> 00:18:50,460
around their decision making about how

403
00:18:50,460 --> 00:18:52,500
they're using

404
00:18:52,500 --> 00:18:55,580
a core model

405
00:19:00,360 --> 00:19:02,700
this is also just a on a more um

406
00:19:02,700 --> 00:19:05,760
qualitative point and then Ali like this

407
00:19:05,760 --> 00:19:08,640
is where we can start to talk about

408
00:19:08,640 --> 00:19:11,880
moving beyond implicit biases

409
00:19:11,880 --> 00:19:14,760
not just like perch implicit biases or

410
00:19:14,760 --> 00:19:16,679
anything like that it's just like we're

411
00:19:16,679 --> 00:19:19,919
doing investigation and exploration

412
00:19:19,919 --> 00:19:22,260
we're externalizing and exteriorizing

413
00:19:22,260 --> 00:19:24,360
our priors

414
00:19:24,360 --> 00:19:26,760
and we're working such that the relevant

415
00:19:26,760 --> 00:19:28,200
features

416
00:19:28,200 --> 00:19:31,340
that are in the Box

417
00:19:31,620 --> 00:19:34,140
are all externalized in terms of the

418
00:19:34,140 --> 00:19:38,460
scientific apparatuses parameterization

419
00:19:38,460 --> 00:19:41,039
and that's at least something where we

420
00:19:41,039 --> 00:19:42,660
can move towards and someone could say

421
00:19:42,660 --> 00:19:44,400
well you haven't made your hyper hyper

422
00:19:44,400 --> 00:19:46,799
hyper priors explicit

423
00:19:46,799 --> 00:19:48,900
but isn't that a better conversation to

424
00:19:48,900 --> 00:19:49,919
have than

425
00:19:49,919 --> 00:19:51,900
you didn't say why you chose this many

426
00:19:51,900 --> 00:19:54,179
people for the experiment

427
00:19:54,179 --> 00:19:56,520
it could be like you didn't State The

428
00:19:56,520 --> 00:19:59,039
Meta governance

429
00:19:59,039 --> 00:20:01,500
of how you came to the decision to do 30

430
00:20:01,500 --> 00:20:03,840
participants

431
00:20:03,840 --> 00:20:06,240
I mean it makes it it sounds like it

432
00:20:06,240 --> 00:20:09,059
makes it really a lot easier to think

433
00:20:09,059 --> 00:20:11,460
about comparing and Counting showing

434
00:20:11,460 --> 00:20:14,220
that you accounted for your methods um

435
00:20:14,220 --> 00:20:16,440
like it's the same you're doing the

436
00:20:16,440 --> 00:20:19,020
exact same experiment right

437
00:20:19,020 --> 00:20:22,320
um might be able to move the needle a

438
00:20:22,320 --> 00:20:23,940
little bit on some of the reproducible

439
00:20:23,940 --> 00:20:25,260
Earth

440
00:20:25,260 --> 00:20:28,260
replication crisis things like

441
00:20:28,260 --> 00:20:33,200
in that vein yeah great point

442
00:20:33,360 --> 00:20:35,479
um

443
00:20:36,179 --> 00:20:38,780
Ollie

444
00:20:41,220 --> 00:20:46,200
I don't see what uh Rock just mentions

445
00:20:46,200 --> 00:20:49,580
um as a side note uh I think

446
00:20:49,580 --> 00:20:52,380
approaching this

447
00:20:52,380 --> 00:20:56,340
um kind of I mean this approach to uh

448
00:20:56,340 --> 00:21:00,059
model based or uh better the

449
00:21:00,059 --> 00:21:03,140
marble-based data analysis

450
00:21:03,140 --> 00:21:06,020
takes the

451
00:21:06,020 --> 00:21:10,280
infinitism stance as an implicit uh

452
00:21:10,280 --> 00:21:13,559
somehow the philosophical stance because

453
00:21:13,559 --> 00:21:17,039
uh if we don't think in terms of

454
00:21:17,039 --> 00:21:18,840
infinitism

455
00:21:18,840 --> 00:21:21,500
I don't think we were able to

456
00:21:21,500 --> 00:21:24,799
model adequately these kind of

457
00:21:24,799 --> 00:21:28,919
metabasian reasoning because we'll get

458
00:21:28,919 --> 00:21:32,940
at some point to as Brock said just

459
00:21:32,940 --> 00:21:36,419
infinite regressive reasons so at least

460
00:21:36,419 --> 00:21:40,520
in my opinion it's somehow implicitly

461
00:21:40,520 --> 00:21:44,220
takes the infinitism stance here

462
00:21:44,220 --> 00:21:48,440
as uh proposed by Peter Klein

463
00:21:48,440 --> 00:21:52,260
at least in large part because as

464
00:21:52,260 --> 00:21:58,280
infinitism uh stands claims will need

465
00:21:58,280 --> 00:22:03,240
somehow the subjectively and objectively

466
00:22:03,240 --> 00:22:04,520
available

467
00:22:04,520 --> 00:22:10,980
reasons to uh to to model or to

468
00:22:10,980 --> 00:22:14,640
construct or Justify true beliefs in

469
00:22:14,640 --> 00:22:18,440
other words the reason must be uh

470
00:22:18,440 --> 00:22:21,480
non-repeating and infinitely available

471
00:22:21,480 --> 00:22:24,740
to us in order for us to be able to

472
00:22:24,740 --> 00:22:29,460
construct that true belief so I'm not

473
00:22:29,460 --> 00:22:33,600
quite sure about it but it looks very

474
00:22:33,600 --> 00:22:37,459
similar to infinitism stance

475
00:22:37,500 --> 00:22:39,360
yeah interesting

476
00:22:39,360 --> 00:22:41,700
um introduction wasn't

477
00:22:41,700 --> 00:22:44,100
familiar with this area

478
00:22:44,100 --> 00:22:45,720
um like

479
00:22:45,720 --> 00:22:48,120
approximate Bayesian computation bounded

480
00:22:48,120 --> 00:22:49,500
rationality

481
00:22:49,500 --> 00:22:52,140
the composability of Base graphs

482
00:22:52,140 --> 00:22:55,039
these are

483
00:22:55,140 --> 00:22:57,960
we get to eat our slice of cake

484
00:22:57,960 --> 00:23:00,360
and recognize that there is the rest of

485
00:23:00,360 --> 00:23:02,100
the cake and the table and the world

486
00:23:02,100 --> 00:23:04,080
outside of the restaurant

487
00:23:04,080 --> 00:23:06,659
and so on like we can just say the the

488
00:23:06,659 --> 00:23:09,900
GM that we made was the thermometer and

489
00:23:09,900 --> 00:23:11,820
a temperature value

490
00:23:11,820 --> 00:23:13,080
and someone can say but what about

491
00:23:13,080 --> 00:23:15,299
humidity and it's like there's a

492
00:23:15,299 --> 00:23:17,039
composability

493
00:23:17,039 --> 00:23:18,240
but

494
00:23:18,240 --> 00:23:19,500
um

495
00:23:19,500 --> 00:23:23,460
we're not within the positivist or the

496
00:23:23,460 --> 00:23:25,679
falsificationist

497
00:23:25,679 --> 00:23:27,659
like well my model of temperature and

498
00:23:27,659 --> 00:23:29,520
thermometer

499
00:23:29,520 --> 00:23:31,440
is positive evidence that that's all

500
00:23:31,440 --> 00:23:33,240
that's happening or it's the best model

501
00:23:33,240 --> 00:23:34,740
we have we're just waiting for it to be

502
00:23:34,740 --> 00:23:36,120
disproven

503
00:23:36,120 --> 00:23:39,659
we can have like a Bayesian portfolio

504
00:23:39,659 --> 00:23:43,020
of models and all of them

505
00:23:43,020 --> 00:23:46,440
can be understood as um Maps

506
00:23:46,440 --> 00:23:50,240
as composable Maps

507
00:23:54,299 --> 00:23:55,860
um

508
00:23:55,860 --> 00:23:58,919
could this lead to

509
00:23:58,919 --> 00:24:02,940
Bayes optimal experimental design

510
00:24:02,940 --> 00:24:06,000
of research programs

511
00:24:06,000 --> 00:24:09,299
like statistical power the the famous

512
00:24:09,299 --> 00:24:11,580
statistics quotation is like something

513
00:24:11,580 --> 00:24:14,460
about an autopsy like the statistician

514
00:24:14,460 --> 00:24:16,380
is not going to diagnose it it's they

515
00:24:16,380 --> 00:24:17,520
come in and they tell you what went

516
00:24:17,520 --> 00:24:19,799
wrong with the autopsy but that's also

517
00:24:19,799 --> 00:24:21,600
that's not fundamental to statistics

518
00:24:21,600 --> 00:24:23,220
that's actually just a little bit of

519
00:24:23,220 --> 00:24:24,960
like a joke about the practice of

520
00:24:24,960 --> 00:24:27,840
research which is like the

521
00:24:27,840 --> 00:24:29,820
biostatistician just being from my own

522
00:24:29,820 --> 00:24:32,100
research experience they're called in

523
00:24:32,100 --> 00:24:34,679
for assistance on a complex already

524
00:24:34,679 --> 00:24:37,260
collected data set

525
00:24:37,260 --> 00:24:39,720
and so it's like well we had three mice

526
00:24:39,720 --> 00:24:42,059
in March and then we had six mice in

527
00:24:42,059 --> 00:24:43,740
April

528
00:24:43,740 --> 00:24:46,620
how do we balance this

529
00:24:46,620 --> 00:24:50,299
versus The Upfront

530
00:24:50,460 --> 00:24:53,100
conversation to help design a high

531
00:24:53,100 --> 00:24:56,159
statistical power experiment

532
00:24:56,159 --> 00:24:57,480
and so like

533
00:24:57,480 --> 00:25:00,299
statistics up front

534
00:25:00,299 --> 00:25:02,820
it's gonna increase the reproducibility

535
00:25:02,820 --> 00:25:05,760
and alignability of experiments

536
00:25:05,760 --> 00:25:08,580
it is a sanity check if not a formal

537
00:25:08,580 --> 00:25:10,380
verification that the experiment is

538
00:25:10,380 --> 00:25:13,460
going to tell you something

539
00:25:14,220 --> 00:25:17,039
and it allows for um replicating

540
00:25:17,039 --> 00:25:19,799
experiments selecting experiments

541
00:25:19,799 --> 00:25:24,799
based upon like their statistical power

542
00:25:27,840 --> 00:25:29,220
okay

543
00:25:29,220 --> 00:25:31,320
following sections unpack an example of

544
00:25:31,320 --> 00:25:32,820
generic inference that may be used for

545
00:25:32,820 --> 00:25:34,380
metabasing inference the variational

546
00:25:34,380 --> 00:25:37,620
LaPlace with hierarchical models

547
00:25:37,620 --> 00:25:40,080
then there's going to be a simple recipe

548
00:25:40,080 --> 00:25:41,940
that's related to the uh one of the

549
00:25:41,940 --> 00:25:43,320
other questions up here

550
00:25:43,320 --> 00:25:45,860
okay

551
00:25:49,500 --> 00:25:52,080
variational LaPlace

552
00:25:52,080 --> 00:25:54,240
may be used for more generic likelihood

553
00:25:54,240 --> 00:25:55,919
functions than those encountered earlier

554
00:25:55,919 --> 00:25:58,440
which were defined as gaussian so

555
00:25:58,440 --> 00:25:59,700
previously

556
00:25:59,700 --> 00:26:02,340
the family of functions that we are

557
00:26:02,340 --> 00:26:04,020
doing variational inference with was

558
00:26:04,020 --> 00:26:06,659
gaussians

559
00:26:06,659 --> 00:26:09,059
now

560
00:26:09,059 --> 00:26:11,340
we can generalize Beyond using only

561
00:26:11,340 --> 00:26:14,899
gaussian distributions

562
00:26:14,940 --> 00:26:18,120
by saying whatever it is

563
00:26:18,120 --> 00:26:20,279
we're going to do a laplacian

564
00:26:20,279 --> 00:26:21,900
approximation

565
00:26:21,900 --> 00:26:23,880
so even if it's like a something very

566
00:26:23,880 --> 00:26:27,480
strange and as mentioned in um cohort 2

567
00:26:27,480 --> 00:26:30,120
discussion just an hour ago

568
00:26:30,120 --> 00:26:32,340
the LaPlace approximation has two

569
00:26:32,340 --> 00:26:34,919
parameters the the mode the center of

570
00:26:34,919 --> 00:26:37,559
the bell and just the width of the

571
00:26:37,559 --> 00:26:39,120
parabola

572
00:26:39,120 --> 00:26:41,640
it's guaranteed to capture some of the

573
00:26:41,640 --> 00:26:43,799
variants of the distribution for

574
00:26:43,799 --> 00:26:45,360
distributions with a central tendency

575
00:26:45,360 --> 00:26:48,360
gaussian or otherwise it can do well

576
00:26:48,360 --> 00:26:50,220
for distributions that are like

577
00:26:50,220 --> 00:26:51,720
multimodal

578
00:26:51,720 --> 00:26:54,120
it does not do well because it gets

579
00:26:54,120 --> 00:26:56,279
tricked to finding the mode which may

580
00:26:56,279 --> 00:26:58,260
only capture like a minority of the

581
00:26:58,260 --> 00:27:00,960
overall bulk and again the SPM textbook

582
00:27:00,960 --> 00:27:03,600
lays out side by side

583
00:27:03,600 --> 00:27:07,200
LaPlace variational Bays non-parametric

584
00:27:07,200 --> 00:27:10,100
sampling approaches

585
00:27:14,460 --> 00:27:17,580
from those laplacian approximations

586
00:27:17,580 --> 00:27:20,400
we're going to do variational inference

587
00:27:20,400 --> 00:27:22,860
so

588
00:27:22,860 --> 00:27:25,580
the real distribution might be

589
00:27:25,580 --> 00:27:28,500
unfactorizable or not we don't know what

590
00:27:28,500 --> 00:27:31,559
family of functions it even is

591
00:27:31,559 --> 00:27:34,440
but we know that the LaPlace

592
00:27:34,440 --> 00:27:36,539
approximations that we make on those

593
00:27:36,539 --> 00:27:39,539
distributions for sure will be amenable

594
00:27:39,539 --> 00:27:43,340
to variational inference

595
00:27:45,539 --> 00:27:48,779
and because of the quadratic nature

596
00:27:48,779 --> 00:27:51,659
we can do gradient Ascent so just like

597
00:27:51,659 --> 00:27:53,760
the ball going to the bottom of the bowl

598
00:27:53,760 --> 00:27:56,700
this is just an inverted Parabola so

599
00:27:56,700 --> 00:27:59,580
it's just gradient asense on the

600
00:27:59,580 --> 00:28:00,419
mountain

601
00:28:00,419 --> 00:28:01,980
all right

602
00:28:01,980 --> 00:28:05,779
parametric empirical base

603
00:28:09,000 --> 00:28:11,000
um

604
00:28:11,460 --> 00:28:13,980
this

605
00:28:13,980 --> 00:28:15,960
usage

606
00:28:15,960 --> 00:28:19,320
of a matrix to represent the

607
00:28:19,320 --> 00:28:21,720
experimental layout

608
00:28:21,720 --> 00:28:25,500
is heavily used in SPM

609
00:28:25,500 --> 00:28:29,900
this is looking a lot like a regression

610
00:28:32,880 --> 00:28:34,919
this is actually a little bit of a short

611
00:28:34,919 --> 00:28:38,360
section 9.4

612
00:28:38,680 --> 00:28:39,059
[Music]

613
00:28:39,059 --> 00:28:41,059
um

614
00:28:43,740 --> 00:28:45,900
this part here

615
00:28:45,900 --> 00:28:48,659
comparing the evidence for a model

616
00:28:48,659 --> 00:28:50,400
where the second element is allowed to

617
00:28:50,400 --> 00:28:51,779
deviate from zero

618
00:28:51,779 --> 00:28:54,179
or the precise belief it's zero

619
00:28:54,179 --> 00:28:57,360
that is actually very much again like

620
00:28:57,360 --> 00:29:00,000
regression testing like if you're gonna

621
00:29:00,000 --> 00:29:01,559
if you have um

622
00:29:01,559 --> 00:29:03,960
if you're testing for for the effect of

623
00:29:03,960 --> 00:29:06,120
a given

624
00:29:06,120 --> 00:29:09,419
um Factor on height

625
00:29:09,419 --> 00:29:12,299
that is related to the p-value you get

626
00:29:12,299 --> 00:29:14,279
for height

627
00:29:14,279 --> 00:29:16,919
is related to the model that includes

628
00:29:16,919 --> 00:29:18,720
height and the model that doesn't

629
00:29:18,720 --> 00:29:20,700
include height

630
00:29:20,700 --> 00:29:22,080
and then

631
00:29:22,080 --> 00:29:25,919
their sum of squares is compared

632
00:29:25,919 --> 00:29:28,380
within the appropriate

633
00:29:28,380 --> 00:29:30,659
statistical test family

634
00:29:30,659 --> 00:29:34,679
t-test z-test f-score all giving you a

635
00:29:34,679 --> 00:29:35,880
p-value

636
00:29:35,880 --> 00:29:39,179
so this is very much like

637
00:29:39,179 --> 00:29:43,159
statistics on regressions

638
00:29:43,440 --> 00:29:45,360
one thing

639
00:29:45,360 --> 00:29:46,980
maybe there there's a deeper reading

640
00:29:46,980 --> 00:29:49,799
where this is clear but one way to talk

641
00:29:49,799 --> 00:29:52,200
about parametric empirical Bays

642
00:29:52,200 --> 00:29:54,419
is that like well first off it is it's

643
00:29:54,419 --> 00:29:56,340
parametric

644
00:29:56,340 --> 00:29:58,200
you're dealing with parameters so

645
00:29:58,200 --> 00:29:59,940
sometimes that's not too helpful but the

646
00:29:59,940 --> 00:30:01,380
empirical part

647
00:30:01,380 --> 00:30:03,600
means like the way we're going to get

648
00:30:03,600 --> 00:30:05,399
our D Matrix

649
00:30:05,399 --> 00:30:06,840
are prior

650
00:30:06,840 --> 00:30:10,980
is from the data

651
00:30:10,980 --> 00:30:13,559
so like let's just say that we have

652
00:30:13,559 --> 00:30:14,880
um we're measuring height in the

653
00:30:14,880 --> 00:30:16,559
classroom

654
00:30:16,559 --> 00:30:18,779
we might be able to use chapter 6 to

655
00:30:18,779 --> 00:30:22,380
specify and furnish the model

656
00:30:22,380 --> 00:30:24,059
but then

657
00:30:24,059 --> 00:30:26,760
as soon as we make that first and then

658
00:30:26,760 --> 00:30:28,260
we might set our prior would say well

659
00:30:28,260 --> 00:30:30,240
let's have a super loose prior let's say

660
00:30:30,240 --> 00:30:31,740
that it could be like

661
00:30:31,740 --> 00:30:34,380
uniform across all you know

662
00:30:34,380 --> 00:30:36,360
but of course who knows what the maximum

663
00:30:36,360 --> 00:30:37,440
height is maybe it's not a human

664
00:30:37,440 --> 00:30:39,899
classroom so we can't just specify a

665
00:30:39,899 --> 00:30:41,700
uniform distribution

666
00:30:41,700 --> 00:30:43,679
across

667
00:30:43,679 --> 00:30:46,260
you know every finite value

668
00:30:46,260 --> 00:30:50,419
and so a way to kind of break that

669
00:30:50,700 --> 00:30:52,559
um a priori

670
00:30:52,559 --> 00:30:53,940
challenge

671
00:30:53,940 --> 00:30:56,820
where like you want your um prior to be

672
00:30:56,820 --> 00:30:58,020
loose enough

673
00:30:58,020 --> 00:30:59,940
to accommodate the full range of the

674
00:30:59,940 --> 00:31:01,620
possible data

675
00:31:01,620 --> 00:31:03,539
but also sharp enough so that you're not

676
00:31:03,539 --> 00:31:05,700
just like starting like on like it's

677
00:31:05,700 --> 00:31:08,820
just an absurdly uninformative prior all

678
00:31:08,820 --> 00:31:10,620
priors are informative even a uniform

679
00:31:10,620 --> 00:31:12,360
prior is still informative

680
00:31:12,360 --> 00:31:14,039
the thing that matters is the strength

681
00:31:14,039 --> 00:31:16,020
the weakness of the prior and the family

682
00:31:16,020 --> 00:31:17,460
of the prior

683
00:31:17,460 --> 00:31:19,500
so what you could do instead would be

684
00:31:19,500 --> 00:31:21,419
you could measure

685
00:31:21,419 --> 00:31:24,539
10 Heights or even one

686
00:31:24,539 --> 00:31:27,179
and then use the mean and the variance

687
00:31:27,179 --> 00:31:29,340
of that sample

688
00:31:29,340 --> 00:31:31,020
empirically

689
00:31:31,020 --> 00:31:33,779
to set your prior

690
00:31:33,779 --> 00:31:36,179
so set just the mean of the prior on

691
00:31:36,179 --> 00:31:38,100
height as the mean that you got from 10

692
00:31:38,100 --> 00:31:40,500
and then you could just over disperse

693
00:31:40,500 --> 00:31:43,100
the variance

694
00:31:43,140 --> 00:31:45,480
so if there's somebody who's outside of

695
00:31:45,480 --> 00:31:47,659
that bound

696
00:31:47,659 --> 00:31:50,460
now it's like you know

697
00:31:50,460 --> 00:31:53,340
you can be more confident but this is so

698
00:31:53,340 --> 00:31:54,960
this is widely used

699
00:31:54,960 --> 00:31:57,179
to take an empirical data set

700
00:31:57,179 --> 00:31:59,940
and then use the data sets empirical

701
00:31:59,940 --> 00:32:02,279
values and summary statistics to

702
00:32:02,279 --> 00:32:04,620
parameterize the generative model

703
00:32:04,620 --> 00:32:08,520
and then the ball goes from there

704
00:32:08,520 --> 00:32:11,940
um and also the expectation maximization

705
00:32:11,940 --> 00:32:13,740
algorithm

706
00:32:13,740 --> 00:32:16,140
where you have a given data set and you

707
00:32:16,140 --> 00:32:17,460
update the parameters in the generative

708
00:32:17,460 --> 00:32:19,380
model and then you generate data

709
00:32:19,380 --> 00:32:21,320
compatible with that model and continue

710
00:32:21,320 --> 00:32:24,899
that is um very close to PEB

711
00:32:24,899 --> 00:32:27,439
okay

712
00:32:27,600 --> 00:32:30,179
9.5 instructions for model based

713
00:32:30,179 --> 00:32:32,159
analysis

714
00:32:32,159 --> 00:32:35,659
so let's copy these out

715
00:32:35,940 --> 00:32:37,860
because the question was like how is

716
00:32:37,860 --> 00:32:39,000
this

717
00:32:39,000 --> 00:32:43,159
a figure 9.2 I guess section 9.5

718
00:32:44,820 --> 00:32:48,799
and it's summarized in 9.2

719
00:32:50,640 --> 00:32:53,840
here's the six steps

720
00:33:01,500 --> 00:33:03,659
oh someone added a great

721
00:33:03,659 --> 00:33:06,120
Okay so

722
00:33:06,120 --> 00:33:08,760
this is a little bit of a review of the

723
00:33:08,760 --> 00:33:10,500
textbook

724
00:33:10,500 --> 00:33:12,899
but we're starting with data

725
00:33:12,899 --> 00:33:14,279
collection

726
00:33:14,279 --> 00:33:15,840
maybe the data already have been

727
00:33:15,840 --> 00:33:16,919
collected

728
00:33:16,919 --> 00:33:18,480
so we're putting aside the situation

729
00:33:18,480 --> 00:33:20,100
where we're doing the meta modeling on

730
00:33:20,100 --> 00:33:21,960
ourselves and then doing experimental

731
00:33:21,960 --> 00:33:23,460
design and then collecting the data

732
00:33:23,460 --> 00:33:25,500
we're just going to take it from the

733
00:33:25,500 --> 00:33:28,880
data are collected

734
00:33:29,940 --> 00:33:32,220
a pomdp

735
00:33:32,220 --> 00:33:34,919
again discrete time in this case

736
00:33:34,919 --> 00:33:38,279
is structurally prepared

737
00:33:38,279 --> 00:33:42,679
according to the recipe in chapter six

738
00:33:43,019 --> 00:33:46,019
pomdps

739
00:33:48,299 --> 00:33:51,000
would we say are equivalent to

740
00:33:51,000 --> 00:33:53,399
specifying a likelihood function

741
00:33:53,399 --> 00:33:55,919
or they they embody or they entail a

742
00:33:55,919 --> 00:33:57,779
likelihood function

743
00:33:57,779 --> 00:34:01,460
what is the arrow between two and three

744
00:34:03,000 --> 00:34:05,580
does merely constructing a pomdp in this

745
00:34:05,580 --> 00:34:06,779
format

746
00:34:06,779 --> 00:34:10,879
uniquely identify a likelihood function

747
00:34:12,119 --> 00:34:14,219
or what what work has to happen right

748
00:34:14,219 --> 00:34:16,399
here

749
00:34:20,639 --> 00:34:23,099
just a thought we can you know learn

750
00:34:23,099 --> 00:34:25,399
yeah

751
00:34:25,980 --> 00:34:29,760
uh I don't think it uh these arrows

752
00:34:29,760 --> 00:34:33,179
necessarily mean uh that

753
00:34:33,179 --> 00:34:36,899
this particular component of uh the

754
00:34:36,899 --> 00:34:40,440
modeling process necessarily uh

755
00:34:40,440 --> 00:34:42,960
translates perfectly to the other stage

756
00:34:42,960 --> 00:34:46,199
uh I think uh they're more likely to be

757
00:34:46,199 --> 00:34:49,500
to show the dependencies among each

758
00:34:49,500 --> 00:34:54,119
steps so uh in the case of the third

759
00:34:54,119 --> 00:34:56,399
stage or the likelihood fun the

760
00:34:56,399 --> 00:34:58,280
construction of the likelihood function

761
00:34:58,280 --> 00:35:04,320
obviously it depends on what our pomdp

762
00:35:04,320 --> 00:35:08,940
model uh the functions so I'm not sure

763
00:35:08,940 --> 00:35:14,400
if we can I mean if it will I mean if

764
00:35:14,400 --> 00:35:18,540
they were 100 isomorphic I guess they

765
00:35:18,540 --> 00:35:20,480
would be redundant right

766
00:35:20,480 --> 00:35:25,079
so yeah I think those arrows show

767
00:35:25,079 --> 00:35:28,140
dependencies rather than translating

768
00:35:28,140 --> 00:35:30,980
into each other

769
00:35:33,060 --> 00:35:36,420
thanks I I

770
00:35:36,420 --> 00:35:40,160
think I broadly agree

771
00:35:41,940 --> 00:35:43,740
and there might also be some other ways

772
00:35:43,740 --> 00:35:47,598
to draw it like the prior beliefs

773
00:35:52,380 --> 00:35:55,700
is this Theta

774
00:35:56,220 --> 00:35:58,800
I think this is using the uh figure nine

775
00:35:58,800 --> 00:36:01,640
one ontology

776
00:36:02,040 --> 00:36:06,680
this data is Big Theta

777
00:36:08,339 --> 00:36:12,000
these are the experimenters parameters

778
00:36:12,000 --> 00:36:14,599
the model

779
00:36:16,440 --> 00:36:20,540
is the cognitive model of the entity

780
00:36:23,900 --> 00:36:26,220
italic U tilde

781
00:36:26,220 --> 00:36:28,320
is the observed data

782
00:36:28,320 --> 00:36:30,000
so here's the observed data getting

783
00:36:30,000 --> 00:36:32,540
passed

784
00:36:33,240 --> 00:36:36,720
with pomdp and Associated likelihood

785
00:36:36,720 --> 00:36:39,078
function

786
00:36:42,720 --> 00:36:45,420
we can the likelihood okay the pmdp

787
00:36:45,420 --> 00:36:46,740
describes variables in their

788
00:36:46,740 --> 00:36:48,420
relationships

789
00:36:48,420 --> 00:36:50,400
the likelihood function

790
00:36:50,400 --> 00:36:52,920
allows us to create p

791
00:36:52,920 --> 00:36:55,740
which is the distribution of

792
00:36:55,740 --> 00:36:58,680
observed Behavior U tilde

793
00:36:58,680 --> 00:37:01,320
conditioned upon

794
00:37:01,320 --> 00:37:03,800
experimental parameters data

795
00:37:03,800 --> 00:37:06,480
observations which are in this case the

796
00:37:06,480 --> 00:37:08,160
ones that are provided to stimuli

797
00:37:08,160 --> 00:37:09,900
experimental stimuli

798
00:37:09,900 --> 00:37:12,740
and the model

799
00:37:12,839 --> 00:37:15,960
then the parametric empirical Bays

800
00:37:15,960 --> 00:37:20,040
comes in when we actually get the data

801
00:37:20,040 --> 00:37:23,099
it comes in in six but we're getting

802
00:37:23,099 --> 00:37:26,160
there we're on the path when we have the

803
00:37:26,160 --> 00:37:29,160
data flowing from the experiment

804
00:37:29,160 --> 00:37:33,300
we can then do the model inversion

805
00:37:33,300 --> 00:37:36,599
rather than describing

806
00:37:36,599 --> 00:37:40,859
the distribution of behavior that would

807
00:37:40,859 --> 00:37:45,859
be generated by Theta o m

808
00:37:46,079 --> 00:37:47,940
we're going to invert it

809
00:37:47,940 --> 00:37:52,380
so we can talk about the distribution of

810
00:37:52,380 --> 00:37:54,920
parameters experimental parameters

811
00:37:54,920 --> 00:37:58,820
conditioned upon mou

812
00:37:59,520 --> 00:38:02,520
Bayesian equation

813
00:38:02,520 --> 00:38:05,640
allows us to have a proportionality

814
00:38:05,640 --> 00:38:09,440
between what we really want to know

815
00:38:09,839 --> 00:38:11,880
this Top Line

816
00:38:11,880 --> 00:38:14,280
and

817
00:38:14,280 --> 00:38:17,000
a

818
00:38:18,780 --> 00:38:20,220
I don't know if I can call it a joint

819
00:38:20,220 --> 00:38:22,560
distribution but these two multiplied

820
00:38:22,560 --> 00:38:24,420
distributions

821
00:38:24,420 --> 00:38:26,760
which is the probability of the param of

822
00:38:26,760 --> 00:38:28,440
the experimental parameters given the

823
00:38:28,440 --> 00:38:30,180
model

824
00:38:30,180 --> 00:38:32,760
and separating out

825
00:38:32,760 --> 00:38:35,339
the probability of U

826
00:38:35,339 --> 00:38:38,099
H condition on all the rest

827
00:38:38,099 --> 00:38:40,740
so this is leveraging the sparsity

828
00:38:40,740 --> 00:38:44,180
of the inverted model

829
00:38:44,220 --> 00:38:47,400
to facilitate a form

830
00:38:47,400 --> 00:38:50,400
that is amenable to linear regression

831
00:38:50,400 --> 00:38:51,480
type

832
00:38:51,480 --> 00:38:55,079
equations this is like y equals MX plus

833
00:38:55,079 --> 00:38:55,980
b

834
00:38:55,980 --> 00:38:58,440
it's not

835
00:38:58,440 --> 00:39:01,320
but it is this is like the same

836
00:39:01,320 --> 00:39:04,160
structure

837
00:39:04,380 --> 00:39:08,599
y m x plus b

838
00:39:09,000 --> 00:39:12,420
but then these bars and their Associated

839
00:39:12,420 --> 00:39:16,400
variances which come from a LaPlace

840
00:39:16,500 --> 00:39:20,640
this is the p-value and the effect size

841
00:39:20,640 --> 00:39:22,380
so for this third modality the effect

842
00:39:22,380 --> 00:39:24,839
size is zero and the variance is you

843
00:39:24,839 --> 00:39:26,700
know such and such

844
00:39:26,700 --> 00:39:29,099
and then for this one we could say

845
00:39:29,099 --> 00:39:32,400
the p-value or the base Factor

846
00:39:32,400 --> 00:39:36,000
for this Factor mattering

847
00:39:36,000 --> 00:39:38,640
it it's it's high the evidence for this

848
00:39:38,640 --> 00:39:42,259
mattering is extremely high

849
00:39:43,920 --> 00:39:45,540
so this is like If This Were a bar chart

850
00:39:45,540 --> 00:39:46,680
and we're

851
00:39:46,680 --> 00:39:48,780
um if we were in frequency statistics

852
00:39:48,780 --> 00:39:50,460
land

853
00:39:50,460 --> 00:39:54,480
and it were like 10 plus or minus 1.

854
00:39:54,480 --> 00:39:57,180
then we we have a z-score of 10.

855
00:39:57,180 --> 00:39:59,880
we're 10 standard deviations away from

856
00:39:59,880 --> 00:40:03,839
zero so the p-value of the effect size

857
00:40:03,839 --> 00:40:07,339
being greater than zero is

858
00:40:07,560 --> 00:40:09,359
whatever it is for a p-value of a

859
00:40:09,359 --> 00:40:12,740
z-score of 10. very low

860
00:40:28,020 --> 00:40:31,700
but again this is like where the real

861
00:40:33,740 --> 00:40:36,960
data collection

862
00:40:36,960 --> 00:40:40,380
is going to be happening and interfacing

863
00:40:40,380 --> 00:40:43,200
so now here's their description

864
00:40:43,200 --> 00:40:46,140
collect behavioral data

865
00:40:46,140 --> 00:40:49,700
formulate the pomdp

866
00:40:50,280 --> 00:40:52,200
it takes parameters as input outputs are

867
00:40:52,200 --> 00:40:56,779
fully specified but not yet solved pomdp

868
00:41:00,599 --> 00:41:05,420
specify likelihood likelihood function

869
00:41:06,260 --> 00:41:10,099
specify prior beliefs

870
00:41:10,140 --> 00:41:11,700
often these will be centered on zero

871
00:41:11,700 --> 00:41:13,200
with precisions reflecting plausible

872
00:41:13,200 --> 00:41:15,240
ranges

873
00:41:15,240 --> 00:41:18,439
that's interesting note

874
00:41:30,720 --> 00:41:34,700
solve for posterior and model evidence

875
00:41:39,119 --> 00:41:41,220
Newton is probably a reference there to

876
00:41:41,220 --> 00:41:43,980
some gradient

877
00:41:43,980 --> 00:41:46,740
derivative base gradient method I'm not

878
00:41:46,740 --> 00:41:48,839
exactly sure

879
00:41:48,839 --> 00:41:50,700
group level analysis

880
00:41:50,700 --> 00:41:52,680
treating the estimated parameters for

881
00:41:52,680 --> 00:41:54,060
each individual as if they were

882
00:41:54,060 --> 00:41:56,040
generated by a second level model

883
00:41:56,040 --> 00:41:59,760
this is exactly structurally

884
00:41:59,760 --> 00:42:02,220
Anova

885
00:42:02,220 --> 00:42:06,359
the group level analysis of variance

886
00:42:06,359 --> 00:42:07,859
like

887
00:42:07,859 --> 00:42:10,560
we had people throw the ball

888
00:42:10,560 --> 00:42:13,980
there was type 1 and type 2 a person

889
00:42:13,980 --> 00:42:16,920
and then the the weather type to is

890
00:42:16,920 --> 00:42:19,320
there an effective type

891
00:42:19,320 --> 00:42:22,320
you make two models one of them is a

892
00:42:22,320 --> 00:42:25,140
model without type

893
00:42:25,140 --> 00:42:26,880
one of them is a model with two

894
00:42:26,880 --> 00:42:28,800
categories of type

895
00:42:28,800 --> 00:42:31,140
then those two models

896
00:42:31,140 --> 00:42:33,240
can be compared

897
00:42:33,240 --> 00:42:36,300
in terms in the frequentest world

898
00:42:36,300 --> 00:42:38,040
in terms of their hierarchical

899
00:42:38,040 --> 00:42:40,980
likelihood

900
00:42:40,980 --> 00:42:43,440
in the Bayesian World however a

901
00:42:43,440 --> 00:42:44,579
limitation of the hierarchical

902
00:42:44,579 --> 00:42:46,440
likelihood ratio test

903
00:42:46,440 --> 00:42:50,160
is that the models must be strictly

904
00:42:50,160 --> 00:42:52,260
structurally nested

905
00:42:52,260 --> 00:42:53,940
they have to reflect direct

906
00:42:53,940 --> 00:42:56,339
simplifications or elaborations of each

907
00:42:56,339 --> 00:42:57,660
other

908
00:42:57,660 --> 00:42:59,339
in contrast

909
00:42:59,339 --> 00:43:01,440
Bayesian modeling allows us to use the

910
00:43:01,440 --> 00:43:03,060
Bayes Factor

911
00:43:03,060 --> 00:43:07,079
which can compare models that don't

912
00:43:07,079 --> 00:43:09,380
need to be

913
00:43:09,380 --> 00:43:12,980
strictly nested

914
00:43:13,980 --> 00:43:16,700
so you could

915
00:43:17,099 --> 00:43:21,000
test very structurally different

916
00:43:21,000 --> 00:43:23,220
models against each other

917
00:43:23,220 --> 00:43:25,079
it's kind of like information as a

918
00:43:25,079 --> 00:43:27,619
common currency

919
00:43:28,200 --> 00:43:30,599
in the Bayesian approach

920
00:43:30,599 --> 00:43:34,260
it I I considered uh variable one two

921
00:43:34,260 --> 00:43:35,520
and three

922
00:43:35,520 --> 00:43:36,839
and then someone else could compare

923
00:43:36,839 --> 00:43:39,119
variable four five six

924
00:43:39,119 --> 00:43:41,040
and you can be like well how how good

925
00:43:41,040 --> 00:43:42,960
are those two different models against

926
00:43:42,960 --> 00:43:45,180
what we care about

927
00:43:45,180 --> 00:43:48,300
in terms of informational Criterion AIC

928
00:43:48,300 --> 00:43:51,599
Bic the base Factor

929
00:43:51,599 --> 00:43:53,460
whereas if somebody said well I did

930
00:43:53,460 --> 00:43:55,980
model uh with one two three and the

931
00:43:55,980 --> 00:43:59,040
p-value is 0.01 and I did this with four

932
00:43:59,040 --> 00:44:02,640
five six and the p-value was 0.01

933
00:44:02,640 --> 00:44:05,880
those can't be directly compared because

934
00:44:05,880 --> 00:44:08,220
the p-value is against like a local null

935
00:44:08,220 --> 00:44:10,319
hypothesis and there's just like

936
00:44:10,319 --> 00:44:12,839
probably other issues

937
00:44:12,839 --> 00:44:14,280
so it's one huge advantage of the

938
00:44:14,280 --> 00:44:15,780
Bayesian like

939
00:44:15,780 --> 00:44:18,000
we're kind of we're past we're 50 years

940
00:44:18,000 --> 00:44:20,160
past the Rubicon or 400 or whatever on

941
00:44:20,160 --> 00:44:22,140
base but like

942
00:44:22,140 --> 00:44:24,599
Bayesian formulations allow for the

943
00:44:24,599 --> 00:44:26,760
generative recognition model the tale of

944
00:44:26,760 --> 00:44:27,900
two densities

945
00:44:27,900 --> 00:44:29,640
so you can specify something that

946
00:44:29,640 --> 00:44:32,160
generates and recognizes data

947
00:44:32,160 --> 00:44:34,740
that's one advantage another Advantage

948
00:44:34,740 --> 00:44:36,660
is that you have access to all the

949
00:44:36,660 --> 00:44:38,579
statistical tools

950
00:44:38,579 --> 00:44:41,760
of frequentive statistics

951
00:44:41,760 --> 00:44:44,520
and you can also

952
00:44:44,520 --> 00:44:46,680
access this like informational

953
00:44:46,680 --> 00:44:49,619
statistics

954
00:44:49,619 --> 00:44:51,480
I'm sure there's like way way way more

955
00:44:51,480 --> 00:44:53,400
rabbit holes there but like broadly

956
00:44:53,400 --> 00:44:54,480
speaking

957
00:44:54,480 --> 00:44:56,099
instead of taking experimental

958
00:44:56,099 --> 00:44:58,500
observations in frequentism and then

959
00:44:58,500 --> 00:45:01,740
mapping them onto a t distribution only

960
00:45:01,740 --> 00:45:03,420
and then saying well now because of the

961
00:45:03,420 --> 00:45:05,040
p-value

962
00:45:05,040 --> 00:45:07,079
let me publish this paper

963
00:45:07,079 --> 00:45:09,300
in the Bayes worlds

964
00:45:09,300 --> 00:45:12,359
there are like richer comparisons and

965
00:45:12,359 --> 00:45:14,819
like more nuanced penalization functions

966
00:45:14,819 --> 00:45:16,619
of how many parameters versus how much

967
00:45:16,619 --> 00:45:19,579
variance explains

968
00:45:21,660 --> 00:45:23,579
you know just for example if anyone

969
00:45:23,579 --> 00:45:25,500
thought bayesians were too confident or

970
00:45:25,500 --> 00:45:28,079
if it wasn't the um

971
00:45:28,079 --> 00:45:29,819
leading

972
00:45:29,819 --> 00:45:31,980
modern strategy for dealing with

973
00:45:31,980 --> 00:45:34,640
uncertainty

974
00:45:35,540 --> 00:45:40,400
this design Matrix for a linear model

975
00:45:41,520 --> 00:45:45,000
the SPM design matrices are like very

976
00:45:45,000 --> 00:45:48,260
interesting looking

977
00:45:48,599 --> 00:45:51,599
so the textbook has many many many

978
00:45:51,599 --> 00:45:53,940
of these matrices they're encoded on a

979
00:45:53,940 --> 00:45:56,480
grayscale

980
00:45:56,700 --> 00:45:58,980
so like What's Happening Here

981
00:45:58,980 --> 00:46:01,319
here are images through time each row is

982
00:46:01,319 --> 00:46:03,119
an image

983
00:46:03,119 --> 00:46:05,400
could be an image in a Time series could

984
00:46:05,400 --> 00:46:08,599
be like different static images

985
00:46:09,119 --> 00:46:11,520
here third column is time

986
00:46:11,520 --> 00:46:13,980
so this is time point zero one two three

987
00:46:13,980 --> 00:46:16,500
four five six times and then here's

988
00:46:16,500 --> 00:46:17,940
condition one and two

989
00:46:17,940 --> 00:46:20,520
first condition whether one is white and

990
00:46:20,520 --> 00:46:22,260
black it doesn't matter

991
00:46:22,260 --> 00:46:23,520
here

992
00:46:23,520 --> 00:46:24,900
um condition let's just say Y is one

993
00:46:24,900 --> 00:46:28,440
condition one was in effect

994
00:46:28,440 --> 00:46:31,260
condition two is not and vice versa

995
00:46:31,260 --> 00:46:32,640
but you could have overlapping

996
00:46:32,640 --> 00:46:33,960
conditions

997
00:46:33,960 --> 00:46:38,119
so it's like a graphical representation

998
00:46:38,520 --> 00:46:40,859
and like this this is also common to see

999
00:46:40,859 --> 00:46:42,480
this kind of like waterfall

1000
00:46:42,480 --> 00:46:44,640
here's

1001
00:46:44,640 --> 00:46:47,700
um patient one two three four five

1002
00:46:47,700 --> 00:46:50,280
Here's the the the four condition the

1003
00:46:50,280 --> 00:46:51,839
three conditions

1004
00:46:51,839 --> 00:46:54,060
in a randomized order

1005
00:46:54,060 --> 00:46:58,079
for the one two three four five patients

1006
00:46:58,079 --> 00:47:00,660
here's their um Global blood flow here's

1007
00:47:00,660 --> 00:47:03,920
some other measurements Etc

1008
00:47:04,140 --> 00:47:07,260
but it turns out with so this summarizes

1009
00:47:07,260 --> 00:47:11,900
the total design of the experiment

1010
00:47:12,480 --> 00:47:15,780
and it turns out like

1011
00:47:15,780 --> 00:47:18,859
you just

1012
00:47:21,119 --> 00:47:25,819
smash it like a matrix multiplication

1013
00:47:27,180 --> 00:47:31,560
like the design Matrix multiplied by the

1014
00:47:31,560 --> 00:47:34,079
observations

1015
00:47:34,079 --> 00:47:35,760
and one can imagine like if the

1016
00:47:35,760 --> 00:47:37,500
observations have no relationship with

1017
00:47:37,500 --> 00:47:39,599
the design Matrix

1018
00:47:39,599 --> 00:47:41,460
there's a null hypothesis that you could

1019
00:47:41,460 --> 00:47:45,020
get from random Matrix Theory

1020
00:47:45,480 --> 00:47:47,819
if the experimental design strongly

1021
00:47:47,819 --> 00:47:51,680
influences the observations

1022
00:47:51,720 --> 00:47:55,939
there's some informational outcome

1023
00:47:58,260 --> 00:48:01,140
96 in our final few minutes examples of

1024
00:48:01,140 --> 00:48:03,720
generative models

1025
00:48:03,720 --> 00:48:05,839
two experiments outlined in nine five

1026
00:48:05,839 --> 00:48:10,819
the details are not important oh

1027
00:48:10,859 --> 00:48:15,839
but it's too um isocating models

1028
00:48:15,839 --> 00:48:17,819
this is a continuous

1029
00:48:17,819 --> 00:48:20,420
time

1030
00:48:21,180 --> 00:48:22,740
left

1031
00:48:22,740 --> 00:48:26,000
tracking a moving Target

1032
00:48:26,099 --> 00:48:28,700
right

1033
00:48:29,940 --> 00:48:34,400
was a categorical eye tracking

1034
00:48:35,339 --> 00:48:38,520
as we've mentioned in this list like

1035
00:48:38,520 --> 00:48:41,700
doing some kind of a webcam

1036
00:48:41,700 --> 00:48:45,060
eye tracking cognitive model

1037
00:48:45,060 --> 00:48:48,900
would be massive

1038
00:48:49,560 --> 00:48:53,819
it's also very you know

1039
00:48:53,819 --> 00:48:57,240
sensitive technology I think

1040
00:48:57,240 --> 00:48:59,280
because it would be able to determine

1041
00:48:59,280 --> 00:49:01,619
somebody's cognitive model and their

1042
00:49:01,619 --> 00:49:05,000
attention in different ways

1043
00:49:05,280 --> 00:49:07,020
but

1044
00:49:07,020 --> 00:49:09,359
really be interesting

1045
00:49:09,359 --> 00:49:10,200
um

1046
00:49:10,200 --> 00:49:11,700
well it's going to say they they

1047
00:49:11,700 --> 00:49:14,880
actually already do that in specifically

1048
00:49:14,880 --> 00:49:16,680
for uh

1049
00:49:16,680 --> 00:49:18,900
like Championship like first person

1050
00:49:18,900 --> 00:49:21,119
shooter games

1051
00:49:21,119 --> 00:49:24,000
to try to catch people

1052
00:49:24,000 --> 00:49:27,720
uh using aimbots

1053
00:49:27,720 --> 00:49:29,819
like so they try to predict

1054
00:49:29,819 --> 00:49:31,500
you know where your eyes are going to be

1055
00:49:31,500 --> 00:49:34,020
based on the data in the game

1056
00:49:34,020 --> 00:49:35,640
and if you

1057
00:49:35,640 --> 00:49:37,859
are shooting you know or you're you're

1058
00:49:37,859 --> 00:49:39,839
acquiring the target faster than your

1059
00:49:39,839 --> 00:49:42,240
eyes uh you know the quiet could have

1060
00:49:42,240 --> 00:49:46,260
acquired the target then wow

1061
00:49:46,260 --> 00:49:48,540
yeah

1062
00:49:48,540 --> 00:49:50,160
well yeah interesting maybe even some of

1063
00:49:50,160 --> 00:49:50,940
the

1064
00:49:50,940 --> 00:49:53,819
um behaviors already

1065
00:49:53,819 --> 00:49:55,680
are out there so it's not like this is

1066
00:49:55,680 --> 00:49:58,200
yeah like maybe the data

1067
00:49:58,200 --> 00:50:00,660
already it's like that moving moving

1068
00:50:00,660 --> 00:50:04,680
Target one yeah yeah

1069
00:50:05,339 --> 00:50:07,380
and like

1070
00:50:07,380 --> 00:50:10,140
interpreting this as like an occupancy

1071
00:50:10,140 --> 00:50:12,359
density

1072
00:50:12,359 --> 00:50:15,240
but then having a cognitive model

1073
00:50:15,240 --> 00:50:17,160
where that is like an uncertainty

1074
00:50:17,160 --> 00:50:19,380
reduction or a salience

1075
00:50:19,380 --> 00:50:22,740
or relevance observation

1076
00:50:22,740 --> 00:50:25,200
which is implicitly what these usages

1077
00:50:25,200 --> 00:50:27,740
are

1078
00:50:27,900 --> 00:50:31,500
Disney trailers

1079
00:50:31,800 --> 00:50:33,800
um

1080
00:50:35,040 --> 00:50:37,800
models of false inference Julius Caesar

1081
00:50:37,800 --> 00:50:41,940
we had the Obama now we have the Caesar

1082
00:50:41,940 --> 00:50:42,839
um

1083
00:50:42,839 --> 00:50:44,880
discussion of phase optimality and

1084
00:50:44,880 --> 00:50:47,420
disorders

1085
00:50:48,119 --> 00:50:51,359
different pathological and neurodiverse

1086
00:50:51,359 --> 00:50:53,520
conditions addiction impulsivity

1087
00:50:53,520 --> 00:50:56,780
compulsivity delusions hallucinations

1088
00:50:56,780 --> 00:50:58,800
interpersonal personality disorders

1089
00:50:58,800 --> 00:51:02,000
ocular motor syndromes

1090
00:51:02,000 --> 00:51:04,260
pharmacotherapy prefrontal syndromes

1091
00:51:04,260 --> 00:51:05,880
visual neglect

1092
00:51:05,880 --> 00:51:09,359
disorders of enterocepted inference

1093
00:51:09,359 --> 00:51:11,839
so

1094
00:51:13,619 --> 00:51:15,839
if there's a parameters set that works

1095
00:51:15,839 --> 00:51:16,619
well

1096
00:51:16,619 --> 00:51:18,660
there's going to be

1097
00:51:18,660 --> 00:51:20,700
various parameter sets that exhibit

1098
00:51:20,700 --> 00:51:22,260
difference

1099
00:51:22,260 --> 00:51:24,119
outcomes

1100
00:51:24,119 --> 00:51:28,020
and then ask per Foucault Society is the

1101
00:51:28,020 --> 00:51:29,880
definer of Madness and all of that and

1102
00:51:29,880 --> 00:51:31,319
the DSM and

1103
00:51:31,319 --> 00:51:34,559
so you know asterisk asterisk asterisk

1104
00:51:34,559 --> 00:51:37,680
but broadly speaking these are models of

1105
00:51:37,680 --> 00:51:40,098
pathology

1106
00:51:42,839 --> 00:51:44,640
we outlined an approach that uses

1107
00:51:44,640 --> 00:51:46,859
theoretical models described in previous

1108
00:51:46,859 --> 00:51:48,599
chapters to pose questions to empirical

1109
00:51:48,599 --> 00:51:50,400
data

1110
00:51:50,400 --> 00:51:52,920
this lets us use active inference as a

1111
00:51:52,920 --> 00:51:54,180
non-invasive tool to probe the

1112
00:51:54,180 --> 00:51:55,980
computational processes that individuals

1113
00:51:55,980 --> 00:51:58,640
use to make decisions

1114
00:51:58,640 --> 00:52:01,740
so funny

1115
00:52:01,740 --> 00:52:05,720
well the model is not invasive

1116
00:52:07,319 --> 00:52:09,000
okay

1117
00:52:09,000 --> 00:52:12,800
I don't think any model is invasive

1118
00:52:12,839 --> 00:52:17,240
ultimately the six steps in figure 9.1

1119
00:52:20,520 --> 00:52:24,380
but it's not that's an error

1120
00:52:25,380 --> 00:52:28,760
it's figure 9.2

1121
00:52:50,579 --> 00:52:52,680
six steps in figure 9.2 provide a

1122
00:52:52,680 --> 00:52:55,440
generic method for Designing experiments

1123
00:52:55,440 --> 00:52:57,780
to non-invasively interrogate implicit

1124
00:52:57,780 --> 00:53:00,420
generative models people

1125
00:53:00,420 --> 00:53:04,880
or other systems used to drive Behavior

1126
00:53:07,020 --> 00:53:08,520
that's an opportunity to answer

1127
00:53:08,520 --> 00:53:09,839
questions about the function of the

1128
00:53:09,839 --> 00:53:12,059
nervous system in health and indices

1129
00:53:12,059 --> 00:53:13,140
all right

1130
00:53:13,140 --> 00:53:14,880
thank you fellows looking forward to the

1131
00:53:14,880 --> 00:53:18,200
conversation next week as well

1132
00:53:18,900 --> 00:53:22,700
thank you peace bye


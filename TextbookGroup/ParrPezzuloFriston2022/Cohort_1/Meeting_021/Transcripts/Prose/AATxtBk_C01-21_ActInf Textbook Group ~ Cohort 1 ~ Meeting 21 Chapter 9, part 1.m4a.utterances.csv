start	end	speaker	confidence	text
1210	357750	A	0.9178314026402652	Hey everyone. It is October 21, 2022. We're in cohort one of the textbook group. It's meeting 21 and we're having our first discussion on chapter nine. Chapter nine is model based Data analysis. Let's look at what the section the headers are. There's a short introduction, a discussion of metabasian methods, which is going to be very interesting and in some ways is even like an entry point to thinking about entity modeling in active inference. And so that'll be kind of fun. We'll return to variational LaPlace resonating with our just completed discussion on chapter four. And LaPlace then section 9.49.5 are going to help us see where data in terms of like gigabytes of actual good day, Yakup, actual data from measurements and so on, where these come into play with models. So how does one go from a furnished trainable model to a parameterized specified model, which actually is explaining variance in real world data sets? And then there's some examples of GMs and some models of false inference. Well, I added only one general question from a lighter reading. I think as we all here move through this, we can generate a lot of other key points and questions. Ultimately, the models described in this book are only useful if they can answer scientific questions. We could rehearse the introduction again, but let's just go into it. So, metabasian methods. This chapter deals with the utility of active inference formulations in analyzing data from behavioral experiments so one could imagine all kinds of bodily, verbal, digital behavior. This goes beyond proof of principle simulations we've seen in previous chapters and instead exploits active inference in answering scientific questions. Broadly speaking, there's two related reasons for fitting a computational model to observe behavior. The first is to estimate parameters of interest. The second is to compare alternative hypotheses. So to parameterize from data is one opportunity, that is to make some model of brain function and then understand within one person or one group or two groups how you can consider those parameterizations to be phenotypes. Phenotype is something of a biological organism or system that's measurable. So like femur length is a phenotype, but also a phenotype doesn't have to just be something that's measurable on the body with a ruler. Like phenotype might be distance ran in the first 50 seconds after the hawk flies through the sky on a cloudy day. And so then that is still a measurement that could be inferred or discussed or made. And we're talking about computational phenotyping because the data and observations for sure are like a basal phenotype, like the button being clicked was measurable phenot to show. But also we could talk about the precision variables in our cognitive model as being phenotypic. And also model comparison can be used. So whereas this first modality of parameterization is like the data set is fixed, we collected 240 fMRI data sets. Now we're going to parameterize. So we're going to give some plasticity to our model and we're going to fit parameters so that our model is like the best resembling it can to these 240 fMRI data sets we have. In the second modality, we're treating in some ways the models as fixed and then evaluating to what extent different models stack up. And that can be used at a very fine scale to look for different models that fit better to a given data set. But also, this is where we can talk about actual biological explanations and predictions. So, like to give an example from yes, please, Ali. First.
360520	413350	B	0.9150581609195403	Sorry, a related question to this whole discussion is I think it was in Section 9.2, it says this goes beyond the proof of principle simulations we have seen in previous chapters and instead exploits active inference in answering scientific questions. I didn't quite understand this statement here because in the previous chapters, we were also engaged with modeling the real scientific phenomena. Right. So what does proof of principle simulation here exactly mean and how does it differ from the approach from answering real scientific questions?
415080	419690	A	0.9482923529411766	Yeah, good question. Does anyone want to give a thought? Or I can give a thought, too.
422780	489070	C	0.952105040650406	I mean, literally, just these following that the first and second reasons to use active inference in this way. Instead of just modeling something and saying, oh, that looks like what we intended to model, or oh, gosh, it looks really predictive, or whatever getting more rigorous with it to specifically enumerate parameters for a particular model. And then compare multiple models. Like trying to say which one is more precise. That's a different thing than what was being done in the book previously. Like, we're just building up the model and seeing how it kind of worked before, and now we're like talking about putting it into a machinery that's going to select on models like which one's best fit or whatever.
492950	578300	A	0.941492196969697	Well said. Yeah, I totally agree. Although arguably, like the chapter five Neurobiology, the tables of examples were answering scientific questions, but those generative models were not largely presented, but in the papers, one could have gone in. But yes, the previous generative models were like, built from the ground up and then like, oh, look at this intersting behavior. Now we're building the machinery to take in real data and fit that. And then there's the two uses of parameterization or the two uses of data fitting to find the best parameters for a given data set and model, and also to compare models, model comparison. All right, now we get to the very excellent figure 9.1. So 9.1 is the metabasian inference. What does anyone see in this figure? Ali.
582000	630830	B	0.8836438571428572	Well, as pointed out in the text, the inner box somehow represents the generative model of the phenomena we're observing, but the outer dashed box is somehow our generative model as the observers. So the term metabasian, I think used here to denote this kind of change of perspective to a higher level of viewpoint to add another layer of hierarchy to the already hierarchical generative models we had before.
633780	664680	A	0.9181344642857141	Yeah, great. Well said. We could have had an arbitrarily nested. Well, just to make a simple point, this is the discrete time formulation POMDP, Figure 4.3. And that can be nested in an arbitrary way and composed and metabasian is like putting the wrapping paper on the gift because this outermost layer is us. Brock.
669020	752260	C	0.923467465753424	Um, so the last time this came up was in the context. I think it was in the other cohort. But we were talking about the provenance of the model, the impetus for it being as a tool for diagnosis of patients for nontypical sort of neuron. I guess I'm curious maybe if there's a simple way to understand it from that perspective. But I also have another question, I guess, which is if this is us modeling, it seems like it would be like infinitely recursive because every time you add another layer, then there's another layer that's not being modeled of how we're actually modeling it. And therefore we would have to but then if we accounted for that, then there would be another outer. I don't know if that's just infinite regression towards the hidden state, one or the other or that's nonsense.
753480	967380	A	0.9314795268138797	Yeah, it's not. I think let's talk about the setting of the clinical neuroimaging. So the inner model is going to be the cognitive behavioral model of the patient just speaking of the participant, the recipient of action as the patient. They're selecting green or blue coins or whatever. They're doing some behavioral paradigm isolating or choice or decision making. We're modeling their action, perception, attention and so on. Whether we formalize it or not, the experimenter's behavior is influencing the parameterization of the patients. Like, we are choosing populations and sampling patients here is like the parameterization of the experiment. We'll have 20 people. They'll come in for two sessions. We're going to do eleven of this. And here's, like the data coming out of so this O here is the observation that the patient sees and their inference on what they're seeing. Experimental stimuli are actually an action from the experimenter. We're pushing the O to their screen. So this O tilde is their O sequence. We're getting data from the experiment. We could choose to be ignorant or implicit about our parameterization or maybe even this is a nested model. We made a governance decision in the lab to decide who gets to decide this. And the constraints were the cost and the availability nested modeling of the outer loop. And what about infinite recursion? And I think one silver lining or saving grace or whatever, or just strength of the Bayesian graph or active inference framework is we know we can always dive in and build from a node or we can just treat the edges of our base graph as like a Markov blanket with the unknown. So we could say we made a simple model of decision making here and then this could be a multi agent simulation and each of those could have another nested model and then we could.
967850	979640	C	0.9308682142857144	Then maybe it's irrelevant or maybe we want to know anyways. But what good will it I don't know or it doesn't fit. We do want to know.
980910	1202660	A	0.936622949308756	Yeah. Let's just say that the Pragmatic value for us is high statistical power and sensitivity for diagnosis of a neuro condition. So we want the inner loop to be in the point. We don't want to be spending money beyond diminishing returns on our experiment. We want to actually diagnose and help the people in this limited situation. Then one could do model comparison with just I'm ignoring all of this. I'm just going to parameterize I'm going to call it like I see it. Then one could have another. But if one were to engage in a research program where we collected one data set, then yeah, let's just call like we see it. But now we have the opportunity to design a second cohort of fMRI experiments. Should we do two people for 100 sessions or 100 people for two sessions? Who should those hundred people be? So at that point, having a policy selection outer loop enabling seems pretty relevant. Now, we could make that simple. Maybe we always pick 30 participants. Maybe it's we either do a small experiment with three or we do a big one with 30. It's just simpler. Those are the two pathways we have. Or maybe it's et cetera. But then you could do model comparison and evaluation on those increasingly complex outer loop models. None of that would be influencing the structure of the cognitive model of the patient. So for all you block, for instance, heads in the chat, one could imagine that there's human fMRI cognitive model. This is a versionable cognitive model, open source. And then different lab groups could have open or closed models with open or closed data around their decision making about how they're using a core model. This is also just on a more qualitative point. And then I'll leave. This is where we can start to talk about moving beyond implicit biases. Not just like purging implicit biases or anything like that. It's just like we're doing investigation and exploration. We're externalizing and exteriorizing our priors and we're working such that the relevant features that are in the box are all externalized in terms of the scientific apparatuses parameterization and that's at least something where we can move towards. And someone could say, well, you haven't made your hyper hyper hyper priors explicit, but isn't that a better conversation to have than you didn't say why you chose this many people for the experiment. It could be like you didn't state the meta governance of how you came to the decision to do 30 participants.
1205800	1229480	C	0.9140185185185185	It makes it really a lot easier to think about comparing and counting, showing that you accounted for your methods. And it's the same you're doing the exact same experiment, right? Might be able to move the needle a little bit on some of the reproduced earth replication crisis, things like in that vein.
1229640	1237100	A	0.9570725	Yeah, great point, Ollie.
1241680	1355700	B	0.9067525153374233	Adding to what well, just mentioned. As a side note, I think approaching this kind of I mean, this approach to model based or better, the model based data analysis takes the infinitism stance as an implicit, somehow philosophical stance. Because if we don't think in terms of infinitism, I don't think we were able to model adequately these kind of metabasian reasoning because we'll get at some point to, as Brock said, just infinite regressive reasons. So at least in my opinion, it somehow implicitly takes the infinitism stance here as proposed by Peter Klein, at least in large part because, as infinitism claims, will need somehow the subjectively and objectively available reasons to model or to construct justify true belief. In other words, the reason must be non repeatingly and infinitely available to us in order for us to be able to construct that true belief. So I'm not quite sure about it, but it looks very similar to infinitism stance.
1357340	2064530	A	0.9284902380952391	Yeah. Interesting introduction. Wasn't familiar with this area. Like approximate Bayesian computation, bounded rationality, the composability of Bayes graphs. These are we get to eat our slice of cake and recognize that there's the rest of the cake and the table and the world outside of the restaurant and so on. Like we can just say the GM that we made was the thermometer and a temperature value. And someone can say, but what about humidity? And it's like there's a composability, but we're not within the Positivist or the falsificationist. Like, well, my model of temperature and thermometer is positive evidence that that's all that's happening. Or it's the best model we have. We're just waiting for it to be disproven. We can have like a Bayesian portfolio of models and all of them can be understood as maps, as composable maps. Could this lead to Bayes optimal experimental design of research programs like statistical power? The famous statistics quotation is like something about an autopsy. Like the statistician is not going to diagnose it. They come in and they tell you what went wrong with the autopsy. But that's not fundamental to statistics. That's actually just a little bit of like a joke about the practice of research, which is like the biostatistician. Just speaking from my own research experience, they're called in for assistance on a complex, already collected data set. And so it's like, well, we had three mice in March and then we had six mice in April. How do we balance this versus the Upfront conversation to help design a high statistical power experiment? And so, like statistics Upfront, it's going to increase the reproducibility and alignability of experiments. It is a sanity check, if not a formal verification that the experiment is going to tell you something. And it allows for replicating experiments, selecting experiments based upon their statistical power. Okay, following sections, unpack an example of generic inference that may be used for metabasian inference, the variational LaPlace with hierarchical models. Then there's going to be a simple recipe that's related to one of the other questions up here. Okay. Variational LaPlace may be used for more generic likelihood functions than those encountered earlier, which were defined as Gaussian. So previously, the family of functions that we are doing variational inference with was Gaussians. Now, we can generalize beyond using only Gaussian distributions by saying, whatever it is, we're going to do a Laplacian approximation. So even if it's like something very strange, and as mentioned in cohort two discussion just an hour ago, LaPlace approximation has two parameters the mode, the center of the bell, and just the width of the parabola. It's guaranteed to capture some of the variance of the distribution. For distributions with a central tendency, gaussian or otherwise, it can do well for distributions that are like multimodal. It does not do well because it gets tricked to finding the mode, which may only capture, like, a minority of the overall bulk. And again, the SPM textbook lays out side by side LaPlace variational bays, non parametric sampling approaches. From those Laplacian approximations, we're going to do variational inference. So the real distribution might be unfactorizable or not. We don't know what family of functions it even is. But we know that the LaPlace approximation that we make on those distributions for sure will be amenable to variational inference. And because of the quadratic nature, we can do gradient ascent. So just like the ball going to the bottom of the bowl, this is just an inverted parabola. So it's just gradient ascent on the mountain. All right, parametric empirical baseage of a matrix to represent the experimental layout is heavily used in SPM. This is looking a lot like a regression. This is actually a little bit of a short section 9.4. This part here comparing the evidence for a model where the second element is allowed to deviate from zero or the precise belief at zero, that is actually very much again like regression testing. If you're testing for the effect of a given factor on height that is related to the p value you get for height is related to the model that includes height and the model that doesn't include height, and then their sum of squares is compared within the appropriate statistical test, family t test, z, test, F score, all giving you a p value. So this is very much like statistics on regressions. One thing, maybe there's a deeper reading where this is clearer, but one way to talk about parametric empirical Bayes, is that, well, first off, it's parametric. You're dealing with parameters, so sometimes that's not too helpful. But the empirical part means, like, the way we're going to get our D matrix, our prior, is from the data. So let's just say that we're measuring height in the classroom. We might be able to use chapter six to specify and furnish the model. But then as soon as we make that first and then we might set our prior, we say, well let's have a super loose prior. Let's say that it could be like uniform across all but of course who knows what the maximum height is. Maybe it's not a human classroom. So we can't just specify a uniform distribution across every finite value. And so a way to kind of break that. A priori challenge where you want your prior to be loose enough to accommodate the full range of the possible data but also sharp enough so that you're not just like starting on like it's just an absurdly uninformative prior. All priors are informative. Even a uniform prior is still informative. The thing that matters is the strength, the weakness of the prior and the family of the prior. So what you could do instead would be you could measure ten heights or even one and then use the mean and the variance of that sample empirically to set your prior. So set just the mean of the prior on height as the mean that you got from ten and then you could just over disperse the variance. So if there's somebody who's outside of that bound now it's like you can be more confident. So this is widely used to take an empirical data set and then use the data set's empirical values and summary statistics to parameterize the generative model. And then the ball goes from there and also the expectation maximization algorithm where you have a given data set and you update the parameters in the generative model and then you generate data compatible with that model and continue. That is very close to PEB. Okay, 9.5 instructions for model based analysis. So let's copy these out because the question was like how is this figure 9.2? I guess section 9.5 and it's summarized in 9.2. Here's the six steps. Oh, someone added a great okay, so this is a little bit of a review of the textbook but we're starting with data collection. Maybe the data already have been collected. So we're putting aside the situation where we're doing the meta modeling on ourself and then doing experimental design and then collecting the data. We're just going to take it from the data are collected. A POMDP again discrete time in this case is structurally prepared according to the recipe in chapter six. Pumdps would we say are equivalent to specifying a likelihood function or they embody or they entail a likelihood function. What is the arrow between two and three? Does merely constructing a POMDP in this format uniquely identify a likelihood function or what work has to happen right here? Just a thought we can learn. Go ahead.
2066600	2129290	B	0.9132429032258063	I don't think these arrows necessarily mean that this particular component of the modeling process necessarily translates perfectly to the other stage. I think they're more likely to show the dependencies among each steps. So in the case of the third stage or the likelihood, the construction of the likelihood function, obviously it depends on what our PMDP model functions. So I'm not sure if we can if they were 100% isomorphic, I guess they would be redundant. Right? So yeah, I think those arrows show dependencies rather than translating into each other.
2133130	2948190	A	0.9222278294573657	Thanks. I think I broadly agree. And there might also be some other ways to draw it, like the prior beliefs. Is this theta. I think this is using the figure nine one ontology this theta is big theta. These are the experimenters parameters. The model is the cognitive model of the entity. Italic u tilde is the observed data. So here's the observed data getting passed with the POMDP and associated likelihood function. We can the likelihood, okay, the PMDP describes variables in their relationships. The likelihood function allows us to create p, which is the distribution of observed behavior u tilde conditioned upon experimental parameters, theta observations, which are in this case the ones that are provided the stimuli, experimental stimuli and the model. Then the parametric empirical Bayes comes in when we actually get the data, it comes in in six. But we're getting there, we're on the path. When we have the data flowing from the experiment, we can then do the model inversion. Rather than describing the distribution of behavior that would be generated by theta o m, we're going to invert it so we can talk about the distribution of parameters. Experimental parameters conditioned upon MoU Bayesian equation allows us to have a proportionality between what we really want to know this top line and a I don't know if I can call it a joint distribution, but these two multiplied distributions, which is the probability of the experimental parameters given the model and separating out the probability of U conditioned on all the rest. So this is leveraging the sparsity of the inverted model to facilitate a form that is amenable to linear regression type equations. This is like y equals MX plus B. It's not, but it is. This is like the same structure y MX plus b, but then these bars and their associated variances which come from LaPlace, this is the p value and the effect size. So for this third modality, the effect size is zero and the variance is such and such. And then for this one we could say the p value or the base factor for this factor mattering, it's high. The evidence for this mattering is extremely high. So this is like if this were a bar chart and if we were in frequent statistics land and it were like ten plus or minus one, then we have a z score of ten. We're ten standard deviations away from zero. So the p value of the effect size being greater than zero is whatever it is for a p value of a z score of ten, very low. Again, this is like where the real data collection is going to be happening and interfacing. So now, here's their description. Collect behavioral data. Formulate the POMDP. It takes parameters as input outputs a fully specified but not yet solved POMDP specify likely likelihood function, specify prior beliefs. Often these will be centered on zero with precisions reflecting plausible ranges. That's interesting. Note solve for posterior and model evidence. Newton is probably a reference there to some gradient derivative based gradient method. I'm not exactly sure. Group level Analysis treating the estimated parameters for each individual as if they were generated by a second level model. This is exactly structurally ANOVA the group level analysis of variance. Like we had people throw the ball. There was type one and type two a person and then whether type two is there an effective type. You make two models. One of them is a model without type. One of them is a model with two categories of type. Then those two models can be compared in terms in the frequentist world, in terms of their hierarchical likelihood. In the Bayesian world, however, a limitation of the hierarchical likelihood ratio test is that the models must be strictly structurally nested. They have to reflect direct, simplifications or elaborations of each other. In contrast, Bayesian modeling allows us to use the Bayes factor, which can compare models that don't need to be strictly nested. So you could test very structurally different models against each other. It's kind of like information as a common currency. In the Bayesian approach, I considered variable one, two and three. And then someone else could compare variable four, five, six. And you can be like, well, how, how much good are those two different models against what we care about in terms of informational criterion? AIC BIC the Bayes factor. Whereas if somebody said, well, I did model with one, two, three and the p value was zero one and I did this with four, five, six and the p value is zero one, those can't be directly compared because the p value is against like a local null hypothesis and there's just like, probably other issues. So it's one huge advantage of the Bayesian. We're 50 years past the rubicon or 400 or whatever on Bayes. But Bayesian formulations allow for the generative recognition model, the tail of two densities. So you can specify something that generates and recognizes data. That's one advantage. Another advantage is that you have access to all the statistical tools of frequent statistics and you can also access this like informational statistics. I'm sure there's way more rabbit holes there. But like, broadly speaking, instead of taking experimental observations in frequentism and then mapping them onto a t distribution only and then saying well now, because of the p value, let me publish this paper. In the Bayes world, there are richer comparisons and more nuanced penalization functions of how many parameters versus how much variance explained. Just for example, if anyone thought Bayesians were too confident or if it wasn't the leading modern strategy for dealing with uncertainty. This design matrix for a linear model, the SPM design matrices are like very interesting looking. So the textbook has many, many of these matrices. They're encoded on a gray scale. So like, what's happening here? Here are images through time. Each row is an image, could be an image in a time series, could be like different static images. Here, third column is time. So this is time .0,123,456. And then here's condition one and two. First condition, whether one is white and black, it doesn't matter. Here condition, let's just say white is one. Condition one was in effect, condition two is not, and vice versa. But you could have overlapping conditions. So it's like a graphical representation. And this is also common to see this kind of like waterfall. Here's patient 12345. Here's the three conditions in a randomized order for the 12345 patients. Here's their global blood flow, here's some other measurements, et cetera. But it turns out with so this summarizes the total design of the experiment. And it turns out like you just smash it like a matrix multiplication, like the design matrix multiplied by the observations. And one can imagine, like if the observations have no relationship with the design matrix, there's a null hypothesis that you could get from random matrix theory. If the experimental design strongly influences the observations, there's some informational. Outcome 96 in our final few minutes, examples of generative models, two experiments outlined in nine five. The details are not important. Oh, but it's two isocating models. This is a continuous time left. Tracking a moving target right was a categorical eye tracking, as we've mentioned and discussed, doing some kind of a webcam eye tracking cognitive model would be massive. It's also very sensitive technology, I think, because it would be able to determine somebody's cognitive model and their attention in different ways. But it would be interesting.
2950160	2984550	C	0.9237915151515153	I was going to say they actually already do that specifically for championship, like first person shooter games to try to catch people using Aim bots. So they try to predict where your eyes are going to be based on the data in the game. And if you are shooting or you're acquiring the target faster than your eyes could have acquired the target, then, wow.
2986300	2998960	A	0.9035947826086957	Yeah, interesting. Maybe even some of the behaviors already are out there. So it's not like this is maybe the data already exists.
2998980	3002030	C	0.8983357142857142	It's like that moving target one. Yeah.
3005360	3196670	A	0.9315227906976743	And interpreting this as like an Occupancy density, but then having a cognitive model where that is like an uncertainty reduction or a salience or relevance observation, which is implicitly what these usages are. Disney trailers, models of false inference, julius Caesar, we had the Obama, now we have the Caesar. Discussion of phase optimality and disorders, different pathological and neurodiverse conditions, addiction, impulsivity, compulsivity, delusions, hallucinations, interpersonal personality disorders, ocular motor syndromes, pharmacotherapy prefrontal syndromes, visual neglect, disorders of interoceptive inference. So if there's a parameters set that works, well, there's going to be various parameter sets that exhibit different outcomes. And then, as per Foucault, society is the definer of madness and all of that. And the DSM asterisk but broadly speaking, these are models of pathology. We outlined an approach that uses theoretical models described in previous chapters to pose questions to empirical data. This lets us use active inference as a non invasive tool to probe the computational processes that individuals use to make decisions. So funny. Well, the model is not invasive, okay? I don't think any model is invasive. Ultimately, the six steps in Figure 9.1 but it's not. That's an error. It's Figure 9.26. Steps in Figure 9.2 provide a generic method for designing experiments to non invasively, interrogate, implicit generative models people or other systems use to drive behavior. That's an opportunity to answer questions about the function of the nervous system in health and indices. All right, thank you, fellows. Looking forward to the conversation next week as well.
3198880	3199676	B	0.999205	Thank you.
3199778	3201020	A	0.96024	Peace. Bye.

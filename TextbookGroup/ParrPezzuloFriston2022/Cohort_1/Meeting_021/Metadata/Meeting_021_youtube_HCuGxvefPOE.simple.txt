SPEAKER_01:
Hey everyone, it is October 21st, 2022.

We're in cohort one of the textbook group.

It's meeting 21 and we're having our first discussion on chapter nine.

Chapter nine is model-based data analysis.

Let's look at what the section headers are.

There's a short introduction, a discussion of meta Bayesian methods, which is going to be very interesting.

And in some ways is even like an entry point to thinking about entity modeling in active inference.

And so that'll be kind of fun.

We'll return to variational Laplace.

resonating with our just completed discussion on chapter four and Laplace then section 9.4 9.5 are going to help us see where data in terms of like gigabytes of actual good day Jakob

actual data from measurements and so on where these come into play with models so how does one go from a um furnished trainable model to a parameterized specified model which actually is explaining variance in real world data sets

And then there's some examples of GMs and some models of false inference.

Well, I added only one general question from a lighter reading.

I think as we all here move through this, we can generate a lot of other key points and questions.

Ultimately, the models described in this book are only useful if they can answer scientific questions.

Let's just... We could rehearse the introduction again, but let's just go into it.

So, Metabasian methods.

This chapter deals with the utility of active inference formulations in analyzing data from behavioral experiments.

So one could imagine all kinds of

bodily, verbal, digital behavior.

This goes beyond proof of principle simulations we've seen in previous chapters and instead exploits active inference in answering scientific questions.

Broadly speaking, there's two related reasons for fitting a computational model to observe behavior.

The first is to estimate parameters of interest,

The second is to compare alternative hypotheses.

So to parameterize from data is one opportunity.

That is to make some model of brain function and then understand within one person or one group or two groups

how you can consider those parameterizations to be phenotypes.

Phenotype is something of a biological organism or system that's measurable.

So like femur length is a phenotype.

But also a phenotype doesn't have to just be something that's measurable on the body with a ruler.

Like phenotype might be distance ran in the first 50 seconds after the hawk flies through the sky on a cloudy day.

And so then that is still a measurement that could be inferred or discussed or made.

And we're talking about computational phenotyping because the data and observations for sure are like a basal phenotype.

Like the button being clicked was measurable.

Pheno means to show.

But also we could talk about like the precision variables in our cognitive model as being phenotypic.

and also model comparison can be used.

So whereas this first modality of parameterization is like the dataset is fixed.

We collected 240 fMRI datasets.

Now we're going to parameterize.

So we're going to give some plasticity to our model and we're going to fit parameters so that our model is like the best resembling it can to these 240 fMRI datasets we have.

In the second modality, we're treating in some ways the models as fixed and then evaluating to what extent different models stack up.

And that can be used at a very fine scale to look for different models that fit better to a given data set.

but also this is where we can talk about like actual biological explanations and predictions.

So like to give an example from, oh yes, please, Allie first.


SPEAKER_00:
Sorry, a related question to this whole discussion is, I think it was in section 9.2, it says this goes beyond the proof of principle simulations we've seen in previous chapters and instead exploits active inference and answering scientific questions.

I didn't quite understand this statement here because in the previous chapters, we were also engaged with modeling the real scientific phenomena, right?

So what does proof of principle simulation here exactly mean?

And how does it differ from the approach, I mean, from answering


SPEAKER_02:
the real scientific questions yeah good question does anyone want to give a thought or i can give a thought too um i mean literally just these following that the first and second reasons to use um active inference in this way it's like spec instead of just modeling something

and saying, oh, that looks like what we intended to model or, oh, gosh, it looks really predictive or whatever, getting more rigorous with it to specifically enumerate parameters for a particular model and then compare multiple models, like trying to say which one is

more precise that's that's a different thing than what has was being done in the book previously like we're just building up the model and seeing how it kind of worked before and now we're like talking about putting it into a like a machinery that's going to you know uh select on models like which one's best fit or whatever well said yeah i i totally agree um


SPEAKER_01:
although arguably like the chapter five neurobiology, the tables of examples were answering scientific questions, but those generative models were not largely presented.

But in the papers, one could have gone in.

But yes, the previous generative models were like built from the ground up and then like, oh, look at this interesting behavior.

Now we're building the machinery to take in real data

and fit that and then there's the the the two uses of parameterization or the two uses of data fitting to find the best parameters for a given data set and model and also to compare models model comparison all right now we get to the very excellent figure 9.1 so

9.1 is the meta Bayesian inference.

What does anyone see in this figure?


UNKNOWN:
I'll wait.


SPEAKER_00:
Well, as pointed out in the text, the inner box somehow represents the generative model of the phenomena we're trying to, we're observing, but the outer dashed box is somehow our generative model as the observers.

So the term metabasian, I think,

used here to denote this kind of change of perspective to a higher level of viewpoint.

I mean, to add another layer of hierarchy to the already hierarchical generative models we had before.


SPEAKER_01:
yeah great well said and this out so we we could have had an arbitrarily nested um well just to make a simple point this is the discrete time formulation pomdp figure 4.3 and that can be nested in an arbitrary way and composed and meta bayesian is like putting the wrapping paper on the gift because this outermost layer is us

Brock?


SPEAKER_02:
So the last time this came up was in the context, I think it was in the other cohort, but we were talking about

the provenance of the model, the impetus for it being as a tool for diagnosis of patients for non-typical sort of neuron.

So I guess I'm curious maybe if there's a simple way to understand it from that perspective.

But I also have a

Another question, I guess, which is, if this is us modeling, it seems like it would be like infinite really recursive.

Because every time you add another layer, then there's another layer that's not being modeled of how we're actually modeling it.

And therefore, we would have to.

But then if we accounted for that, then

there would be another outer uh you know i don't know if that's just uh infinite regression towards the hidden state one or the other or but um or i'm just that's nonsense but yeah no it's it's not i think let's let's talk about the uh setting of the clinical neuroimaging so


SPEAKER_01:
The inner model is going to be the cognitive behavioral model of the patient.

Just speaking of the recipient of action as the patient.

They're selecting green or blue coins or whatever.

They're doing some behavioral paradigm, isocating or choice or decision-making.

We're modeling their action, perception, attention, and so on.

Whether we...

formalize it or not, the experimenter's behavior is influencing the parameterization of the patients.

Like we are choosing populations and sampling patients.

Here is like the parameterization of the experiment.

have 20 people they'll come in for two sessions we're gonna do 11 um you know of this and here's like the data coming out of so this o here is the observation that the patient sees and their inference on what they're seeing

Experimental stimuli are actually an action from the experimenter.

We're pushing the O to their screen.

So this O tilde is their O sequence.

We're getting data from the experiment.

We could choose to be ignorant or implicit.

about our parameterization or maybe even this is a nested model we made a governance decision in the lab to decide who gets to decide this and the constraints were the cost and the availability nested

modeling of the outer loop and what about infinite recursion and i think one um silver lining or saving grace or whatever or just strength of the bayesian graph or active inference framework is we know we can always dive in

and like build from a node.

Or we can just treat the edges of our base graph as like a Markov blanket with the unknown.

So we could say we made a simple model of decision-making here.

And then we could, this could be a multi-agent simulation and each of those could have another nested model.


SPEAKER_02:
And then we could do- Like if it fits, then maybe it's irrelevant or-

Maybe we want to know anyways, but what good will it?

I don't know.


SPEAKER_01:
Yeah.


SPEAKER_02:
Or it doesn't fit.


SPEAKER_01:
We do want to know.

Yeah.

Let's just say that the pragmatic value for us is high statistical power and sensitivity for diagnosis of a neuro condition.

So like we want the inner loop

to be in the point... We don't want to be spending money beyond diminishing returns on our experiment.

We want to actually diagnose and help the people in this limited situation.

Then one could do model comparison with just, I'm ignoring all of this.

I'm just going to parameterize.

I'm going to call it like I see it.

Then one could have another, but if one were to engage in a research program...

where we collected one data set.

Then, yeah, let's just call it like we see it.

But now we have the opportunity to design a second cohort of fMRI experiments.

Should we do two people for 100 sessions or 100 people for two sessions?

Who should those 100 people be?

So at that point, having a policy selection outer loop enabling seems pretty relevant.

Now, we could make that simple.

Maybe it's we always pick 30 participants.

Maybe it's we either do a small experiment with three or we do a big one with 30.

It's just simpler.

Those are the two pathways we have.

Or maybe it's a, you know, et cetera.

But then you could do model comparison and evaluation on those increasingly complex outer loop models.

none of that would be influencing the structure of the cognitive model of the patient so for all you blockference heads in the chat one could imagine that there's um human fmri cognitive model this is a versionable cognitive model open source

and then different lab groups could have open or closed models with open or closed data around their decision making about how they're using a core model this is also just a on a more um qualitative point and then Ali like this is where we can start to talk about

moving beyond implicit biases, not just like parching implicit biases or anything like that.

It's just like, we're doing investigation and exploration.

We're externalizing and exteriorizing our priors.

And we're working such that the relevant features that are in the box are all externalized in terms of the scientific apparatuses parameterization.

And that's at least something where we can move towards.

And someone could say, well, you haven't made your hyper, hyper, hyper priors explicit, but isn't that a better conversation to have?

Then you didn't say why you chose this many people for the experiment.

It could be like, you didn't state the meta governance of how you came to the decision to do 30 participants.


SPEAKER_02:
I mean, it makes it, it sounds like it makes it really a lot easier to think about comparing and counting, showing that you accounted for your methods.

It's the same, you're doing the exact same experiment, right?

Might be able to move the needle a little bit on some of the reproducible Earth replication crisis, things like in that vein.


SPEAKER_01:
Yeah, great point.

Um,

Ali?


SPEAKER_00:
Adding to what Brock just mentioned as a side note, I think approaching this kind of, I mean, this approach to model-based or

better the model-based data analysis takes the infinitism stance as an implicit somehow the philosophical stance because if we don't think in terms of infinitism

I don't think we were able to model adequately these kinds of meta-Bayesian reasoning because we'll get at some point to, as Brock said, just infinite regressive reasons.

So at least in my opinion, it somehow implicitly takes the infinitism stance here.

as proposed by Peter Klein, at least in large part, because as infinitism claims, we'll need somehow the subjectively and objectively available reasons to model or to construct or justify true beliefs.

In other words, the reason must be non-repeatingly and infinitely available to us in order for us to be able to construct that true belief.

So I'm not quite sure about it, but it looks very similar to infinitism stance.


SPEAKER_01:
Yeah, interesting introduction.

familiar with this area um like approximate Bayesian computation bounded rationality the composability of base graphs these are we get to eat our slice of cake and recognize that there's the rest of the cake and the table and the world outside of the restaurant and so on like we can just say the the GM that we made was the thermometer and a temperature value

someone can say but what about humidity and it's like there's a composability but um we're not within the positivist or the falsificationist like well my model of temperature and thermometer is positive evidence that that's all that's happening or it's the best model we have we're just waiting for it to be disproven we can have like a bayesian portfolio of models and all of them

can be understood as maps, as composable maps.

Could this lead to Bayes optimal experimental design of research programs?

like statistical power the the famous statistics quotation is like something about an autopsy like the statistician is not going to diagnose it it's they come in and they tell you what went wrong with the autopsy but that's also that's not fundamental to statistics that's actually just a little bit of like a joke about the practice of research which is like the biostatistician just speaking for my own research experience they're called in for assistance on a complex already collected data set

so it's like well we had three mice in march and then we had six mice in april how do we balance this versus the upfront conversation to help design a high statistical power experiment and so like statistics up front it's gonna increase the reproducibility and alignability of experiments

it is a sanity check if not a formal verification that the experiment is going to tell you something and it allows for um replicating experiments selecting experiments based upon like their statistical power

okay following sections unpack an example of generic inference that may be used for metabasian inference the variational laplace with hierarchical models then there's going to be a simple recipe that's related to the uh one of the other questions up here okay variational laplace

may be used for more generic likelihood functions than those encountered earlier, which were defined as Gaussian.

So previously, the family of functions that we were doing variational inference with was Gaussians.

Now, we can generalize beyond using only Gaussian distributions by saying whatever it is, we're going to do a Laplacian approximation.

So even if it's like something very strange, and as mentioned in cohort two discussion just an hour ago, Laplace approximation has two parameters, the mode, the center of the bell, and just the width of the parabola.

It's guaranteed to capture some of the variance of the distribution.

For distributions with a central tendency, Gaussian or otherwise, it can do well.

For distributions that are like multimodal,

It does not do well because it gets tricked to finding the mode, which may only capture like a minority of the overall bulk.

And again, the SPM textbook lays out side-by-side Laplace, variational Bayes, non-parametric sampling approaches.

From those Laplacian approximations, we're going to do variational inference.

So

The real distribution might be unfactorizable or we don't know what family of functions it even is.

But we know that the Laplace approximations that we make on those distributions for sure will be amenable to variational inference.

And because of the quadratic nature,

we can do gradient ascent.

So just like the ball going to the bottom of the bowl, this is just an inverted parabola.

So it's just gradient ascent on the mountain.

All right.

Parametric empirical base.

This usage

of a matrix to represent the experimental layout is heavily used in SPM.

This is looking a lot like a regression.

This is actually a little bit of a short section 9.4.

this part here comparing the evidence for a model where the second element is allowed to deviate from zero or the precise belief at zero that is actually very much again like regression testing like if you're gonna if you have um if you're testing for for the effect of a given um factor on height that is related to the p-value you get for height

is related to the model that includes height and the model that doesn't include height.

And then their sum of squares is compared within the appropriate statistical test family.

T test, Z test, F score, all giving you a p-value.

So this is very much like statistics on regressions.

One thing, maybe there's a deeper reading where this is clearer, but one way to talk about parametric empirical Bayes is that like, well, first off it's parametric, you're dealing with parameters.

So sometimes that's not too helpful, but the empirical part means like the way we're going to get our D matrix, our prior is from the data.

So like, let's just say that we have, we're measuring height in the classroom.

We might be able to use chapter six to specify and furnish the model.

But then as soon as we make that first, and then we might set our prior, we'll say, well, let's have a super loose prior.

Let's say that it could be like uniform across all, you know,

But of course, who knows what the maximum height is?

Maybe it's not a human classroom.

So we can't just specify a uniform distribution across every finite value.

And so a way to kind of break that a priori challenge where like you want your prior to be loose enough to accommodate the full range of the possible data.

but also sharp enough so that you're not just like starting like on, like it's just an absurdly uninformative prior.

All priors are informative.

Even a uniform prior is still informative.

The thing that matters is the strength, the weakness of the prior and the family of the prior.

So what you could do instead would be you could measure 10 heights or even one, and then use the mean and the variance of that sample empirically.

to set your prior.

So set just the mean of the prior on height as the mean that you got from 10, and then you could just over disperse the variance.

So if there's somebody who's outside of that bound, now it's like, you know, you can be more confident, but this is, so this is widely used to take an empirical data set

and then use the data sets empirical values and summary statistics to parameterize the generative model and then the ball goes from there um and also the expectation maximization algorithm where you have a given data set and you update the parameters in the generative model and then you generate data compatible with that model and continue that is um very close to peb okay

9.5 instructions for model-based analysis so let's copy these out because the question was like how is this a figure 9.2 i guess section 9.5 and it's summarized in 9.2 here's the six steps

Oh, someone added, wait.

Okay.

So this is a little bit of a review of the textbook, but we're starting with data collection.

Maybe the data already have been collected.

So we're putting aside the situation where we're doing the meta modeling on ourself and then doing experimental design and then collecting the data.

We're just going to take it from the data are collected.

A POMDP, again, discrete time in this case, is structurally prepared according to the recipe in chapter six.

POMDPs, would we say, are equivalent to specifying a likelihood function or they embody or they entail a likelihood function?

What is the arrow between two and three?

Does merely constructing a POMDP in this format uniquely identify a likelihood function?

Or what work has to happen right here?

Just a thought.

We can, you know, learn.

Ali, go ahead.


SPEAKER_00:
I don't think these arrows necessarily mean that

this particular component of the modeling process necessarily translates perfectly to the other stage, I think they're more likely to show the dependencies among each steps.

So in the case of the third stage or the construction of the likelihood function, obviously it depends on what our POMDP model functions.

I'm not sure if we can.

I mean, if they were 100% isomorphic, I guess they would be redundant, right?

So, yeah, I think those arrows show dependencies rather than translating into each other.


SPEAKER_01:
thanks I I I think I broadly agree and there might also be some other ways to draw it like the prior beliefs is this Theta I think this is using the uh figure 9-1 ontology

This theta is big theta.

These are the experimenter's parameters.

The model is the cognitive model of the entity.

Italic U tilde is the observed data.

So here's the observed data getting passed.

With the POMDP and associated likelihood function, we can, the likelihood, okay, the POMDP describes variables and their relationships.

The likelihood function allows us to create P, which is the distribution of observed behavior, U tilde, conditioned upon experimental parameters theta.

observations which are in this case the ones that are provided the stimuli experimental stimuli and the model then the parametric empirical base comes in when we actually get the data it comes in in six but we're getting there we're on the path when we have the data flowing from the experiment we can then do the model inversion

rather than describing the distribution of behavior that would be generated by theta O M, we're going to invert it so we can talk about the distribution of parameters, experimental parameters, conditioned upon M O U. Bayesian equation,

allows us to have a proportionality between what we really want to know, this top line, and a, I don't know if I can call it a joint distribution, but these two multiplied distributions, which is the probability of the experimental parameters given the model and separating out

the probability of U conditioned on all the rest.

So this is leveraging the sparsity of the inverted model to facilitate a form that is amenable to linear regression type equations.

This is like Y equals MX plus B. It's not, but it is.

This is like the same structure.

Y, M, X plus B. But then these bars and their associated variances, which come from Laplace, this is the p-value and the effect size.

So for this third modality, the effect size is zero and the variances, you know, such and such.

And then for this one, we could say the p-value or the base factor for this factor mattering

it's high.

The evidence for this mattering is extremely high.

So this is like, if this were a bar chart and we're, if we were in frequentist statistics land and it were like 10 plus or minus one, then we have a Z score of 10.

We're 10 standard deviations away from zero.

So the p-value of the effect size being greater than zero is,

whatever it is for a p-value of a z-score of 10 very low again this is like where the real data collection

is going to be happening and interfacing.

So now here's their description.

Collect behavioral data.

Formulate the POMDP.

It takes parameters as input, outputs a fully specified but not yet solved POMDP.

Specify a likelihood function.

specify prior beliefs often these will be centered on zero with precisions reflecting plausible ranges that's interesting note solve for posterior and model evidence

Newton is probably a reference there to some gradient derivative based gradient method.

I'm not exactly sure.

Group level analysis.

Treating the estimated parameters for each individual as if they were generated by a second level model.

This is exactly structurally ANOVA.

The group level analysis of variance.

Like we had people throw the ball.

There was type one and type two a person.

And then is there an effective type?

You make two models.

One of them is a model without type.

One of them is a model with two categories of type.

Then those two models can be compared in terms in the frequentist world

in terms of their hierarchical likelihood.

In the Bayesian world, however, a limitation of the hierarchical likelihood ratio test is that the models must be strictly structurally nested.

They have to reflect direct simplifications or elaborations of each other.

In contrast, Bayesian modeling allows us to use the Bayes factor

which can compare models that don't need to be strictly nested.

So you could test very structurally different models against each other.

It's kind of like information as a common currency in the Bayesian approach.

I considered variable one, two, and three.

And then someone else could compare variable four, five, six.

And you can be like, well, how good are those two different models against what we care about in terms of informational criterion, AIC, BIC, the Bayes factor?

Whereas if somebody said, well, I did model with 1, 2, 3, and the p-value was 0.01.

And I did this with 4, 5, 6, and the p-value was 0.01.

Those can't be directly compared because the p-value is against a local null hypothesis, and there's just probably other issues.

So that's one huge advantage of the Bayesian model.

We're 50 years past the Rubicon or 400 or whatever on Bayes, but Bayesian formulations allow for the generative recognition model, the tail of two densities.

So you can specify something that generates and recognizes data.

That's one advantage.

Another advantage is that you have access to all the statistical tools of frequentist statistics.

and you can also um access this like informational statistics i'm sure there's like way way way more rabbit holes there but like broadly speaking instead of taking experimental observations in frequentism and then mapping them onto a t distribution only and then saying well now because the p-value let me publish this paper in the bayes world

there are like richer comparisons and like more nuanced penalization functions of how many parameters versus how much variance explained you know just for example if anyone thought bayesians were too confident or if it wasn't the um leading modern strategy for dealing with uncertainty this design matrix for a linear model

The SPM design matrices are very interesting looking.

So the textbook has many, many, many of these matrices.

They're encoded on a grayscale.

So what's happening here?

Here are images through time.

Each row is an image.

Could be an image in a time series.

Could be different static images.

Here, third column is time.

So this is time point zero, one, two, three, four, five, six.

And then here's condition one and two.

First condition, whether one is white and black, it doesn't matter.

Here, condition, let's just say white is one.

Condition one was in effect.

Condition two is not, and vice versa.

But you could have overlapping conditions.

So it's like a graphical representation

And this is also common to see this kind of like waterfall.

Here's patient one, two, three, four, five.

Here's the three conditions in a randomized order for the one, two, three, four, five patients.

Here's their global blood flow.

Here's some other measurements, et cetera.

But it turns out, so this summarizes the total design of the experiment.

And it turns out like you just smash it like a matrix multiplication.

Like the design matrix multiplied by the observations.

And one can imagine like if the observations have no relationship with the design matrix, there's a null hypothesis that you could get from random matrix theory.

If the experimental design strongly influences the observations, there's some informational outcome.

9.6 in our final few minutes, examples of generative models.

Two experiments outlined in 9.5.

The details are not important.

Oh.

But it's two isocating models.

This is a continuous time.

Left tracking a moving target.

Right was a categorical eye tracking.

as we've mentioned and discussed like doing some kind of a webcam eye tracking cognitive model would be massive it's also very you know sensitive technology I think because it would be able to determine somebody's cognitive model and their attention in different ways

But it would be interesting.


SPEAKER_02:
I was going to say, they actually already do that specifically for championship first-person shooter games to try to catch people using aim bots.

So they try to predict where your eyes are going to be based on the data in the game.

And if you...


SPEAKER_01:
are shooting you know or you're you're acquiring the target faster than your eyes uh you know acquired could have acquired the target then wow yeah well yeah interesting maybe even some of the um behaviors already are out there so it's not like this is it yeah like maybe the data already exists it's like that moving moving target one yeah yeah

and like interpreting this as like an occupancy density but then having a cognitive model where that is like an uncertainty reduction or a salience or relevance observation which is implicitly what these usages are disney trailers um

Models of false inference.

Julius Caesar.

We had the Obama.

Now we have the Caesar.

Discussion of Bayes' optimality and disorders.

Different pathological and neurodiverse conditions.

Addiction, impulsivity, compulsivity.

Delusions, hallucinations.

Interpersonal personality disorders.

Ocular motor syndromes.

barmicotherapy, prefrontal syndromes, visual neglect, disorders of interoceptive inference.

So, if there's a parameters set that works well, there's going to be,

Various parameter sets that exhibit different outcomes.

And then as per Foucault, society is the definer of madness and all of that and the DSM.

And so, you know, asterisk, asterisk, asterisk.

But broadly speaking, these are models of pathology.

We outlined an approach that uses theoretical models described in previous chapters to pose questions to empirical data.

This lets us use active inference as a non-invasive tool to probe the computational processes that individuals use to make decisions.

So funny.

Well, the model is not invasive.

Okay.

I don't think any model is invasive.

Ultimately, the six steps in figure 9.1.

But it's not.

That's an error.

It's figure 9.2.

Six steps in figure 9.2 provide a generic method for designing experiments to non-invasively interrogate implicit generative models, people, or other systems used to drive behavior.

That's an opportunity to answer questions about the function of the nervous system in health and indices.

All right.

Thank you, fellows.

Looking forward to the conversation next week as well.


SPEAKER_00:
Thank you.


SPEAKER_01:
Peace bye.
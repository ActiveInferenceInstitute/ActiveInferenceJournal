start	end	sentNum	speaker	confidence	text
1210	1918	1	A	0.99979	Hey everyone.
2084	5710	2	A	1.0	It is October 21, 2022.
5860	9006	3	A	0.60124	We're in cohort one of the textbook group.
9108	15920	4	A	0.80324	It's meeting 21 and we're having our first discussion on chapter nine.
19570	22794	5	A	0.44797	Chapter nine is model based Data analysis.
22842	27800	6	A	0.99923	Let's look at what the section the headers are.
30250	46726	7	A	0.99674	There's a short introduction, a discussion of metabasian methods, which is going to be very interesting and in some ways is even like an entry point to thinking about entity modeling in active inference.
46918	48860	8	A	1.0	And so that'll be kind of fun.
49970	59150	9	A	0.52019	We'll return to variational LaPlace resonating with our just completed discussion on chapter four.
59300	81314	10	A	1.0	And LaPlace then section 9.49.5 are going to help us see where data in terms of like gigabytes of actual good day, Yakup, actual data from measurements and so on, where these come into play with models.
81442	96810	11	A	0.99998	So how does one go from a furnished trainable model to a parameterized specified model, which actually is explaining variance in real world data sets?
97150	103070	12	A	1.0	And then there's some examples of GMs and some models of false inference.
104690	115410	13	A	0.99695	Well, I added only one general question from a lighter reading.
116710	124420	14	A	0.99	I think as we all here move through this, we can generate a lot of other key points and questions.
128170	132760	15	A	0.9794	Ultimately, the models described in this book are only useful if they can answer scientific questions.
139670	144274	16	A	0.98356	We could rehearse the introduction again, but let's just go into it.
144392	146610	17	A	0.99648	So, metabasian methods.
146950	163270	18	A	0.99971	This chapter deals with the utility of active inference formulations in analyzing data from behavioral experiments so one could imagine all kinds of bodily, verbal, digital behavior.
164350	173340	19	A	0.99977	This goes beyond proof of principle simulations we've seen in previous chapters and instead exploits active inference in answering scientific questions.
176190	181258	20	A	0.9929	Broadly speaking, there's two related reasons for fitting a computational model to observe behavior.
181434	184240	21	A	0.88	The first is to estimate parameters of interest.
187090	190610	22	A	1.0	The second is to compare alternative hypotheses.
193780	227080	23	A	0.99929	So to parameterize from data is one opportunity, that is to make some model of brain function and then understand within one person or one group or two groups how you can consider those parameterizations to be phenotypes.
228800	234856	24	A	0.91525	Phenotype is something of a biological organism or system that's measurable.
234968	242972	25	A	0.99981	So like femur length is a phenotype, but also a phenotype doesn't have to just be something that's measurable on the body with a ruler.
243116	252690	26	A	0.99549	Like phenotype might be distance ran in the first 50 seconds after the hawk flies through the sky on a cloudy day.
253960	259030	27	A	1.0	And so then that is still a measurement that could be inferred or discussed or made.
259960	276810	28	A	0.56	And we're talking about computational phenotyping because the data and observations for sure are like a basal phenotype, like the button being clicked was measurable phenot to show.
277580	285020	29	A	1.0	But also we could talk about the precision variables in our cognitive model as being phenotypic.
287120	291890	30	A	1.0	And also model comparison can be used.
292660	302636	31	A	0.85379	So whereas this first modality of parameterization is like the data set is fixed, we collected 240 fMRI data sets.
302748	304316	32	A	1.0	Now we're going to parameterize.
304428	316810	33	A	0.99993	So we're going to give some plasticity to our model and we're going to fit parameters so that our model is like the best resembling it can to these 240 fMRI data sets we have.
317420	335470	34	A	0.99975	In the second modality, we're treating in some ways the models as fixed and then evaluating to what extent different models stack up.
336800	345010	35	A	1.0	And that can be used at a very fine scale to look for different models that fit better to a given data set.
345540	354352	36	A	0.99985	But also, this is where we can talk about actual biological explanations and predictions.
354416	357136	37	A	0.99938	So, like to give an example from yes, please, Ali.
357168	357750	38	A	1.0	First.
360520	381470	39	B	0.99409	Sorry, a related question to this whole discussion is I think it was in Section 9.2, it says this goes beyond the proof of principle simulations we have seen in previous chapters and instead exploits active inference in answering scientific questions.
382640	398092	40	B	1.0	I didn't quite understand this statement here because in the previous chapters, we were also engaged with modeling the real scientific phenomena.
398156	398672	41	B	0.99987	Right.
398806	413350	42	B	0.99987	So what does proof of principle simulation here exactly mean and how does it differ from the approach from answering real scientific questions?
415080	416664	43	A	0.9821	Yeah, good question.
416862	418184	44	A	0.99949	Does anyone want to give a thought?
418222	419690	45	A	0.99811	Or I can give a thought, too.
422780	433710	46	C	1.0	I mean, literally, just these following that the first and second reasons to use active inference in this way.
435120	455990	47	C	0.99741	Instead of just modeling something and saying, oh, that looks like what we intended to model, or oh, gosh, it looks really predictive, or whatever getting more rigorous with it to specifically enumerate parameters for a particular model.
458280	460864	48	C	0.68	And then compare multiple models.
460912	467936	49	C	0.91056	Like trying to say which one is more precise.
468048	472852	50	C	0.99987	That's a different thing than what was being done in the book previously.
472916	489070	51	C	0.75783	Like, we're just building up the model and seeing how it kind of worked before, and now we're like talking about putting it into a machinery that's going to select on models like which one's best fit or whatever.
492950	493602	52	A	0.99815	Well said.
493656	495460	53	A	0.93273	Yeah, I totally agree.
497910	510454	54	A	0.98861	Although arguably, like the chapter five Neurobiology, the tables of examples were answering scientific questions, but those generative models were not largely presented, but in the papers, one could have gone in.
510492	518150	55	A	0.99999	But yes, the previous generative models were like, built from the ground up and then like, oh, look at this intersting behavior.
518510	527434	56	A	0.96	Now we're building the machinery to take in real data and fit that.
527632	546370	57	A	1.0	And then there's the two uses of parameterization or the two uses of data fitting to find the best parameters for a given data set and model, and also to compare models, model comparison.
547110	552260	58	A	0.73982	All right, now we get to the very excellent figure 9.1.
553270	564230	59	A	0.99824	So 9.1 is the metabasian inference.
566730	570620	60	A	0.99964	What does anyone see in this figure?
577280	578300	61	A	0.77281	Ali.
582000	604848	62	B	0.99899	Well, as pointed out in the text, the inner box somehow represents the generative model of the phenomena we're observing, but the outer dashed box is somehow our generative model as the observers.
605024	630830	63	B	0.69666	So the term metabasian, I think used here to denote this kind of change of perspective to a higher level of viewpoint to add another layer of hierarchy to the already hierarchical generative models we had before.
633780	635136	64	A	0.91844	Yeah, great.
635318	636370	65	A	0.99432	Well said.
637780	640320	66	A	0.99955	We could have had an arbitrarily nested.
641380	647892	67	A	0.99916	Well, just to make a simple point, this is the discrete time formulation POMDP, Figure 4.3.
648026	662250	68	A	0.92	And that can be nested in an arbitrary way and composed and metabasian is like putting the wrapping paper on the gift because this outermost layer is us.
663900	664680	69	A	0.62089	Brock.
669020	677432	70	C	0.55608	Um, so the last time this came up was in the context.
677496	679656	71	C	1.0	I think it was in the other cohort.
679688	698160	72	C	0.87839	But we were talking about the provenance of the model, the impetus for it being as a tool for diagnosis of patients for nontypical sort of neuron.
701720	708476	73	C	0.96	I guess I'm curious maybe if there's a simple way to understand it from that perspective.
708608	733136	74	C	0.99998	But I also have another question, I guess, which is if this is us modeling, it seems like it would be like infinitely recursive because every time you add another layer, then there's another layer that's not being modeled of how we're actually modeling it.
733158	740960	75	C	0.86	And therefore we would have to but then if we accounted for that, then there would be another outer.
741860	752260	76	C	1.0	I don't know if that's just infinite regression towards the hidden state, one or the other or that's nonsense.
753480	755380	77	A	0.99842	Yeah, it's not.
755450	764760	78	A	0.69	I think let's talk about the setting of the clinical neuroimaging.
766140	779096	79	A	0.99721	So the inner model is going to be the cognitive behavioral model of the patient just speaking of the participant, the recipient of action as the patient.
779208	782220	80	A	0.51148	They're selecting green or blue coins or whatever.
782290	786690	81	A	0.99998	They're doing some behavioral paradigm isolating or choice or decision making.
787140	793890	82	A	0.69852	We're modeling their action, perception, attention and so on.
796120	810400	83	A	0.876	Whether we formalize it or not, the experimenter's behavior is influencing the parameterization of the patients.
811220	827620	84	A	0.98756	Like, we are choosing populations and sampling patients here is like the parameterization of the experiment.
829640	830772	85	A	0.51167	We'll have 20 people.
830826	832448	86	A	0.61492	They'll come in for two sessions.
832624	835930	87	A	0.64579	We're going to do eleven of this.
836860	849230	88	A	1.0	And here's, like the data coming out of so this O here is the observation that the patient sees and their inference on what they're seeing.
855180	860440	89	A	0.40935	Experimental stimuli are actually an action from the experimenter.
861100	864156	90	A	0.99992	We're pushing the O to their screen.
864258	867820	91	A	0.99995	So this O tilde is their O sequence.
870000	874540	92	A	0.99539	We're getting data from the experiment.
879030	894280	93	A	0.99567	We could choose to be ignorant or implicit about our parameterization or maybe even this is a nested model.
895610	902940	94	A	0.92033	We made a governance decision in the lab to decide who gets to decide this.
903310	917370	95	A	0.81	And the constraints were the cost and the availability nested modeling of the outer loop.
920730	922710	96	A	1.0	And what about infinite recursion?
923290	951550	97	A	1.0	And I think one silver lining or saving grace or whatever, or just strength of the Bayesian graph or active inference framework is we know we can always dive in and build from a node or we can just treat the edges of our base graph as like a Markov blanket with the unknown.
954050	967380	98	A	0.99982	So we could say we made a simple model of decision making here and then this could be a multi agent simulation and each of those could have another nested model and then we could.
967850	972722	99	C	0.99551	Then maybe it's irrelevant or maybe we want to know anyways.
972786	978166	100	C	0.99535	But what good will it I don't know or it doesn't fit.
978348	979640	101	C	0.98185	We do want to know.
980910	981274	102	A	0.9956	Yeah.
981312	994510	103	A	0.99282	Let's just say that the Pragmatic value for us is high statistical power and sensitivity for diagnosis of a neuro condition.
994930	1001598	104	A	0.53158	So we want the inner loop to be in the point.
1001764	1006210	105	A	0.9997	We don't want to be spending money beyond diminishing returns on our experiment.
1006950	1011540	106	A	0.99996	We want to actually diagnose and help the people in this limited situation.
1013670	1019010	107	A	0.99999	Then one could do model comparison with just I'm ignoring all of this.
1019160	1023000	108	A	0.76303	I'm just going to parameterize I'm going to call it like I see it.
1023530	1025174	109	A	0.99999	Then one could have another.
1025292	1035162	110	A	0.99998	But if one were to engage in a research program where we collected one data set, then yeah, let's just call like we see it.
1035296	1040090	111	A	1.0	But now we have the opportunity to design a second cohort of fMRI experiments.
1041070	1045370	112	A	1.0	Should we do two people for 100 sessions or 100 people for two sessions?
1045530	1047454	113	A	0.99999	Who should those hundred people be?
1047652	1057810	114	A	0.99993	So at that point, having a policy selection outer loop enabling seems pretty relevant.
1058710	1062210	115	A	1.0	Now, we could make that simple.
1062360	1065330	116	A	0.99953	Maybe we always pick 30 participants.
1066150	1070518	117	A	0.99991	Maybe it's we either do a small experiment with three or we do a big one with 30.
1070604	1071474	118	A	0.99472	It's just simpler.
1071522	1073720	119	A	0.99997	Those are the two pathways we have.
1074090	1077826	120	A	0.99937	Or maybe it's et cetera.
1077938	1086490	121	A	0.73564	But then you could do model comparison and evaluation on those increasingly complex outer loop models.
1087870	1097870	122	A	0.99588	None of that would be influencing the structure of the cognitive model of the patient.
1099170	1111220	123	A	0.99893	So for all you block, for instance, heads in the chat, one could imagine that there's human fMRI cognitive model.
1111750	1117170	124	A	1.0	This is a versionable cognitive model, open source.
1117850	1133900	125	A	1.0	And then different lab groups could have open or closed models with open or closed data around their decision making about how they're using a core model.
1140420	1143664	126	A	0.99999	This is also just on a more qualitative point.
1143702	1144770	127	A	0.96	And then I'll leave.
1145240	1150820	128	A	0.97122	This is where we can start to talk about moving beyond implicit biases.
1151880	1155156	129	A	0.99999	Not just like purging implicit biases or anything like that.
1155178	1159240	130	A	0.67598	It's just like we're doing investigation and exploration.
1159820	1181820	131	A	0.54264	We're externalizing and exteriorizing our priors and we're working such that the relevant features that are in the box are all externalized in terms of the scientific apparatuses parameterization and that's at least something where we can move towards.
1181890	1193360	132	A	1.0	And someone could say, well, you haven't made your hyper hyper hyper priors explicit, but isn't that a better conversation to have than you didn't say why you chose this many people for the experiment.
1194200	1202660	133	A	1.0	It could be like you didn't state the meta governance of how you came to the decision to do 30 participants.
1205800	1213172	134	C	1.0	It makes it really a lot easier to think about comparing and counting, showing that you accounted for your methods.
1213236	1218170	135	C	0.53	And it's the same you're doing the exact same experiment, right?
1219600	1229480	136	C	0.87674	Might be able to move the needle a little bit on some of the reproduced earth replication crisis, things like in that vein.
1229640	1237100	137	A	0.99978	Yeah, great point, Ollie.
1241680	1245510	138	B	0.98344	Adding to what well, just mentioned.
1246040	1273272	139	B	0.99999	As a side note, I think approaching this kind of I mean, this approach to model based or better, the model based data analysis takes the infinitism stance as an implicit, somehow philosophical stance.
1273336	1294576	140	B	0.99999	Because if we don't think in terms of infinitism, I don't think we were able to model adequately these kind of metabasian reasoning because we'll get at some point to, as Brock said, just infinite regressive reasons.
1294608	1333916	141	B	0.97682	So at least in my opinion, it somehow implicitly takes the infinitism stance here as proposed by Peter Klein, at least in large part because, as infinitism claims, will need somehow the subjectively and objectively available reasons to model or to construct justify true belief.
1334108	1346880	142	B	0.82898	In other words, the reason must be non repeatingly and infinitely available to us in order for us to be able to construct that true belief.
1347040	1355700	143	B	0.99033	So I'm not quite sure about it, but it looks very similar to infinitism stance.
1357340	1357704	144	A	0.88567	Yeah.
1357742	1360100	145	A	0.78219	Interesting introduction.
1360180	1363050	146	A	0.54296	Wasn't familiar with this area.
1364540	1371980	147	A	0.9865	Like approximate Bayesian computation, bounded rationality, the composability of Bayes graphs.
1372320	1385020	148	A	0.5514	These are we get to eat our slice of cake and recognize that there's the rest of the cake and the table and the world outside of the restaurant and so on.
1385090	1391170	149	A	0.52476	Like we can just say the GM that we made was the thermometer and a temperature value.
1391700	1394072	150	A	0.64	And someone can say, but what about humidity?
1394236	1404820	151	A	1.0	And it's like there's a composability, but we're not within the Positivist or the falsificationist.
1405640	1411880	152	A	0.96768	Like, well, my model of temperature and thermometer is positive evidence that that's all that's happening.
1411950	1413496	153	A	0.53437	Or it's the best model we have.
1413518	1415640	154	A	0.88525	We're just waiting for it to be disproven.
1415980	1428540	155	A	0.92604	We can have like a Bayesian portfolio of models and all of them can be understood as maps, as composable maps.
1435940	1448310	156	A	0.99941	Could this lead to Bayes optimal experimental design of research programs like statistical power?
1448680	1452992	157	A	1.0	The famous statistics quotation is like something about an autopsy.
1453136	1456116	158	A	0.76708	Like the statistician is not going to diagnose it.
1456218	1458836	159	A	0.99996	They come in and they tell you what went wrong with the autopsy.
1458948	1461780	160	A	0.99999	But that's not fundamental to statistics.
1461860	1468804	161	A	0.66047	That's actually just a little bit of like a joke about the practice of research, which is like the biostatistician.
1468852	1476190	162	A	1.0	Just speaking from my own research experience, they're called in for assistance on a complex, already collected data set.
1477200	1482640	163	A	1.0	And so it's like, well, we had three mice in March and then we had six mice in April.
1483700	1494880	164	A	1.0	How do we balance this versus the Upfront conversation to help design a high statistical power experiment?
1496100	1505220	165	A	0.99	And so, like statistics Upfront, it's going to increase the reproducibility and alignability of experiments.
1505880	1511770	166	A	1.0	It is a sanity check, if not a formal verification that the experiment is going to tell you something.
1514300	1523150	167	A	1.0	And it allows for replicating experiments, selecting experiments based upon their statistical power.
1527870	1537418	168	A	0.99624	Okay, following sections, unpack an example of generic inference that may be used for metabasian inference, the variational LaPlace with hierarchical models.
1537594	1543198	169	A	0.99998	Then there's going to be a simple recipe that's related to one of the other questions up here.
1543364	1544080	170	A	0.93414	Okay.
1549560	1557792	171	A	0.73822	Variational LaPlace may be used for more generic likelihood functions than those encountered earlier, which were defined as Gaussian.
1557936	1565480	172	A	0.99996	So previously, the family of functions that we are doing variational inference with was Gaussians.
1566780	1581340	173	A	1.0	Now, we can generalize beyond using only Gaussian distributions by saying, whatever it is, we're going to do a Laplacian approximation.
1581760	1598560	174	A	0.9992	So even if it's like something very strange, and as mentioned in cohort two discussion just an hour ago, LaPlace approximation has two parameters the mode, the center of the bell, and just the width of the parabola.
1599060	1603248	175	A	0.93402	It's guaranteed to capture some of the variance of the distribution.
1603424	1611300	176	A	1.0	For distributions with a central tendency, gaussian or otherwise, it can do well for distributions that are like multimodal.
1611720	1619284	177	A	1.0	It does not do well because it gets tricked to finding the mode, which may only capture, like, a minority of the overall bulk.
1619412	1628460	178	A	0.52	And again, the SPM textbook lays out side by side LaPlace variational bays, non parametric sampling approaches.
1634470	1639830	179	A	0.99999	From those Laplacian approximations, we're going to do variational inference.
1640410	1647478	180	A	0.6778	So the real distribution might be unfactorizable or not.
1647564	1650840	181	A	0.99999	We don't know what family of functions it even is.
1651690	1661770	182	A	0.99999	But we know that the LaPlace approximation that we make on those distributions for sure will be amenable to variational inference.
1665810	1670734	183	A	1.0	And because of the quadratic nature, we can do gradient ascent.
1670862	1676238	184	A	0.98606	So just like the ball going to the bottom of the bowl, this is just an inverted parabola.
1676334	1680178	185	A	0.99992	So it's just gradient ascent on the mountain.
1680354	1704750	186	A	0.78325	All right, parametric empirical baseage of a matrix to represent the experimental layout is heavily used in SPM.
1705570	1708190	187	A	1.0	This is looking a lot like a regression.
1712610	1716720	188	A	0.99993	This is actually a little bit of a short section 9.4.
1723910	1739110	189	A	0.99888	This part here comparing the evidence for a model where the second element is allowed to deviate from zero or the precise belief at zero, that is actually very much again like regression testing.
1741530	1775300	190	A	0.99996	If you're testing for the effect of a given factor on height that is related to the p value you get for height is related to the model that includes height and the model that doesn't include height, and then their sum of squares is compared within the appropriate statistical test, family t test, z, test, F score, all giving you a p value.
1775830	1781490	191	A	0.99921	So this is very much like statistics on regressions.
1783370	1795430	192	A	0.55	One thing, maybe there's a deeper reading where this is clearer, but one way to talk about parametric empirical Bayes, is that, well, first off, it's parametric.
1796270	1799494	193	A	0.94364	You're dealing with parameters, so sometimes that's not too helpful.
1799542	1809420	194	A	0.99982	But the empirical part means, like, the way we're going to get our D matrix, our prior, is from the data.
1810830	1815550	195	A	0.90982	So let's just say that we're measuring height in the classroom.
1816530	1821200	196	A	0.99999	We might be able to use chapter six to specify and furnish the model.
1822370	1829742	197	A	0.99966	But then as soon as we make that first and then we might set our prior, we say, well let's have a super loose prior.
1829806	1836694	198	A	0.99975	Let's say that it could be like uniform across all but of course who knows what the maximum height is.
1836732	1838162	199	A	0.99999	Maybe it's not a human classroom.
1838306	1845480	200	A	0.99996	So we can't just specify a uniform distribution across every finite value.
1846170	1848680	201	A	1.0	And so a way to kind of break that.
1850650	1868282	202	A	0.6	A priori challenge where you want your prior to be loose enough to accommodate the full range of the possible data but also sharp enough so that you're not just like starting on like it's just an absurdly uninformative prior.
1868426	1869850	203	A	0.99993	All priors are informative.
1869930	1872106	204	A	1.0	Even a uniform prior is still informative.
1872298	1877086	205	A	1.0	The thing that matters is the strength, the weakness of the prior and the family of the prior.
1877278	1892550	206	A	0.99349	So what you could do instead would be you could measure ten heights or even one and then use the mean and the variance of that sample empirically to set your prior.
1893690	1901450	207	A	0.99989	So set just the mean of the prior on height as the mean that you got from ten and then you could just over disperse the variance.
1903070	1912430	208	A	0.94919	So if there's somebody who's outside of that bound now it's like you can be more confident.
1912930	1924260	209	A	0.99866	So this is widely used to take an empirical data set and then use the data set's empirical values and summary statistics to parameterize the generative model.
1924630	1941494	210	A	1.0	And then the ball goes from there and also the expectation maximization algorithm where you have a given data set and you update the parameters in the generative model and then you generate data compatible with that model and continue.
1941692	1944550	211	A	0.99959	That is very close to PEB.
1944970	1950890	212	A	0.98905	Okay, 9.5 instructions for model based analysis.
1951950	1960174	213	A	0.92499	So let's copy these out because the question was like how is this figure 9.2?
1960212	1967330	214	A	1.0	I guess section 9.5 and it's summarized in 9.2.
1970580	1972240	215	A	0.99973	Here's the six steps.
1981400	1993900	216	A	0.96	Oh, someone added a great okay, so this is a little bit of a review of the textbook but we're starting with data collection.
1994240	1996760	217	A	0.99998	Maybe the data already have been collected.
1996920	2003392	218	A	0.99999	So we're putting aside the situation where we're doing the meta modeling on ourself and then doing experimental design and then collecting the data.
2003446	2007200	219	A	0.99775	We're just going to take it from the data are collected.
2009860	2021030	220	A	0.96	A POMDP again discrete time in this case is structurally prepared according to the recipe in chapter six.
2022920	2037020	221	A	0.15293	Pumdps would we say are equivalent to specifying a likelihood function or they embody or they entail a likelihood function.
2037710	2039900	222	A	0.99996	What is the arrow between two and three?
2042990	2054780	223	A	0.95912	Does merely constructing a POMDP in this format uniquely identify a likelihood function or what work has to happen right here?
2060660	2063456	224	A	0.99987	Just a thought we can learn.
2063638	2064530	225	A	0.68621	Go ahead.
2066600	2083268	226	B	1.0	I don't think these arrows necessarily mean that this particular component of the modeling process necessarily translates perfectly to the other stage.
2083444	2090488	227	B	0.9	I think they're more likely to show the dependencies among each steps.
2090664	2107324	228	B	0.90836	So in the case of the third stage or the likelihood, the construction of the likelihood function, obviously it depends on what our PMDP model functions.
2107372	2119440	229	B	0.98749	So I'm not sure if we can if they were 100% isomorphic, I guess they would be redundant.
2119520	2120150	230	B	0.99969	Right?
2120520	2129290	231	B	0.53598	So yeah, I think those arrows show dependencies rather than translating into each other.
2133130	2133880	232	A	0.99981	Thanks.
2134810	2138300	233	A	0.9	I think I broadly agree.
2141890	2145950	234	A	1.0	And there might also be some other ways to draw it, like the prior beliefs.
2152360	2153780	235	A	0.81462	Is this theta.
2156140	2164840	236	A	0.62	I think this is using the figure nine one ontology this theta is big theta.
2168310	2170630	237	A	0.99994	These are the experimenters parameters.
2171930	2178890	238	A	1.0	The model is the cognitive model of the entity.
2184210	2187840	239	A	0.94488	Italic u tilde is the observed data.
2188210	2197300	240	A	0.99963	So here's the observed data getting passed with the POMDP and associated likelihood function.
2202630	2207650	241	A	0.61135	We can the likelihood, okay, the PMDP describes variables in their relationships.
2208310	2231020	242	A	0.59	The likelihood function allows us to create p, which is the distribution of observed behavior u tilde conditioned upon experimental parameters, theta observations, which are in this case the ones that are provided the stimuli, experimental stimuli and the model.
2232830	2242302	243	A	0.60126	Then the parametric empirical Bayes comes in when we actually get the data, it comes in in six.
2242356	2244202	244	A	1.0	But we're getting there, we're on the path.
2244266	2252290	245	A	0.99998	When we have the data flowing from the experiment, we can then do the model inversion.
2253350	2273382	246	A	0.99999	Rather than describing the distribution of behavior that would be generated by theta o m, we're going to invert it so we can talk about the distribution of parameters.
2273526	2317000	247	A	0.97119	Experimental parameters conditioned upon MoU Bayesian equation allows us to have a proportionality between what we really want to know this top line and a I don't know if I can call it a joint distribution, but these two multiplied distributions, which is the probability of the experimental parameters given the model and separating out the probability of U conditioned on all the rest.
2318010	2332422	248	A	0.99988	So this is leveraging the sparsity of the inverted model to facilitate a form that is amenable to linear regression type equations.
2332566	2335838	249	A	0.71383	This is like y equals MX plus B.
2336004	2339518	250	A	0.99863	It's not, but it is.
2339604	2359540	251	A	0.99959	This is like the same structure y MX plus b, but then these bars and their associated variances which come from LaPlace, this is the p value and the effect size.
2360470	2366040	252	A	0.99991	So for this third modality, the effect size is zero and the variance is such and such.
2366570	2377418	253	A	1.0	And then for this one we could say the p value or the base factor for this factor mattering, it's high.
2377504	2380700	254	A	0.69	The evidence for this mattering is extremely high.
2383920	2396770	255	A	0.99894	So this is like if this were a bar chart and if we were in frequent statistics land and it were like ten plus or minus one, then we have a z score of ten.
2397140	2400644	256	A	0.77389	We're ten standard deviations away from zero.
2400842	2411130	257	A	0.99997	So the p value of the effect size being greater than zero is whatever it is for a p value of a z score of ten, very low.
2428000	2439520	258	A	0.94329	Again, this is like where the real data collection is going to be happening and interfacing.
2440340	2442560	259	A	0.99877	So now, here's their description.
2443140	2444770	260	A	0.99818	Collect behavioral data.
2446200	2448020	261	A	0.96285	Formulate the POMDP.
2450200	2468440	262	A	1.0	It takes parameters as input outputs a fully specified but not yet solved POMDP specify likely likelihood function, specify prior beliefs.
2470060	2473900	263	A	0.99988	Often these will be centered on zero with precisions reflecting plausible ranges.
2475200	2476076	264	A	0.99579	That's interesting.
2476178	2493180	265	A	0.5578	Note solve for posterior and model evidence.
2499120	2506076	266	A	0.94	Newton is probably a reference there to some gradient derivative based gradient method.
2506188	2507650	267	A	0.86436	I'm not exactly sure.
2508740	2515948	268	A	1.0	Group level Analysis treating the estimated parameters for each individual as if they were generated by a second level model.
2516134	2524900	269	A	0.99982	This is exactly structurally ANOVA the group level analysis of variance.
2526360	2529800	270	A	0.9941	Like we had people throw the ball.
2530540	2538090	271	A	1.0	There was type one and type two a person and then whether type two is there an effective type.
2539260	2540912	272	A	0.99999	You make two models.
2541076	2544030	273	A	1.0	One of them is a model without type.
2545120	2548190	274	A	1.0	One of them is a model with two categories of type.
2548720	2559840	275	A	0.66205	Then those two models can be compared in terms in the frequentist world, in terms of their hierarchical likelihood.
2561060	2571940	276	A	0.99993	In the Bayesian world, however, a limitation of the hierarchical likelihood ratio test is that the models must be strictly structurally nested.
2572280	2576950	277	A	1.0	They have to reflect direct, simplifications or elaborations of each other.
2577560	2591320	278	A	0.69862	In contrast, Bayesian modeling allows us to use the Bayes factor, which can compare models that don't need to be strictly nested.
2594000	2602670	279	A	0.99992	So you could test very structurally different models against each other.
2603380	2605920	280	A	0.69123	It's kind of like information as a common currency.
2608100	2615090	281	A	0.84791	In the Bayesian approach, I considered variable one, two and three.
2615460	2618290	282	A	1.0	And then someone else could compare variable four, five, six.
2618980	2627600	283	A	0.8	And you can be like, well, how, how much good are those two different models against what we care about in terms of informational criterion?
2627760	2630260	284	A	0.52775	AIC BIC the Bayes factor.
2631560	2651710	285	A	0.99997	Whereas if somebody said, well, I did model with one, two, three and the p value was zero one and I did this with four, five, six and the p value is zero one, those can't be directly compared because the p value is against like a local null hypothesis and there's just like, probably other issues.
2652720	2655120	286	A	0.991	So it's one huge advantage of the Bayesian.
2657220	2660796	287	A	0.9998	We're 50 years past the rubicon or 400 or whatever on Bayes.
2660908	2667648	288	A	0.99998	But Bayesian formulations allow for the generative recognition model, the tail of two densities.
2667824	2671430	289	A	0.99994	So you can specify something that generates and recognizes data.
2672120	2673600	290	A	0.77305	That's one advantage.
2673760	2688520	291	A	0.99998	Another advantage is that you have access to all the statistical tools of frequent statistics and you can also access this like informational statistics.
2689580	2692716	292	A	0.99983	I'm sure there's way more rabbit holes there.
2692738	2706590	293	A	0.99999	But like, broadly speaking, instead of taking experimental observations in frequentism and then mapping them onto a t distribution only and then saying well now, because of the p value, let me publish this paper.
2707120	2718020	294	A	1.0	In the Bayes world, there are richer comparisons and more nuanced penalization functions of how many parameters versus how much variance explained.
2721880	2733000	295	A	0.9335	Just for example, if anyone thought Bayesians were too confident or if it wasn't the leading modern strategy for dealing with uncertainty.
2735580	2746590	296	A	0.51258	This design matrix for a linear model, the SPM design matrices are like very interesting looking.
2748480	2752584	297	A	0.99621	So the textbook has many, many of these matrices.
2752632	2754960	298	A	0.57793	They're encoded on a gray scale.
2756660	2758690	299	A	0.99725	So like, what's happening here?
2759140	2760560	300	A	0.98627	Here are images through time.
2760630	2767220	301	A	1.0	Each row is an image, could be an image in a time series, could be like different static images.
2769160	2771236	302	A	0.99991	Here, third column is time.
2771418	2775316	303	A	0.99997	So this is time .0,123,456.
2775498	2777764	304	A	0.66	And then here's condition one and two.
2777962	2781610	305	A	1.0	First condition, whether one is white and black, it doesn't matter.
2782300	2785272	306	A	0.99767	Here condition, let's just say white is one.
2785406	2791008	307	A	0.99989	Condition one was in effect, condition two is not, and vice versa.
2791204	2793656	308	A	0.99935	But you could have overlapping conditions.
2793848	2796540	309	A	0.99915	So it's like a graphical representation.
2798480	2802328	310	A	0.99	And this is also common to see this kind of like waterfall.
2802504	2807310	311	A	0.99667	Here's patient 12345.
2807680	2816740	312	A	0.97909	Here's the three conditions in a randomized order for the 12345 patients.
2818120	2822500	313	A	0.99973	Here's their global blood flow, here's some other measurements, et cetera.
2824040	2830360	314	A	0.81507	But it turns out with so this summarizes the total design of the experiment.
2832460	2852380	315	A	1.0	And it turns out like you just smash it like a matrix multiplication, like the design matrix multiplied by the observations.
2854000	2863360	316	A	1.0	And one can imagine, like if the observations have no relationship with the design matrix, there's a null hypothesis that you could get from random matrix theory.
2865460	2873372	317	A	0.78978	If the experimental design strongly influences the observations, there's some informational.
2873436	2886126	318	A	0.99999	Outcome 96 in our final few minutes, examples of generative models, two experiments outlined in nine five.
2886308	2888014	319	A	1.0	The details are not important.
2888212	2895150	320	A	0.59	Oh, but it's two isocating models.
2895830	2901940	321	A	0.97725	This is a continuous time left.
2902790	2927380	322	A	0.99985	Tracking a moving target right was a categorical eye tracking, as we've mentioned and discussed, doing some kind of a webcam eye tracking cognitive model would be massive.
2930040	2943210	323	A	0.95978	It's also very sensitive technology, I think, because it would be able to determine somebody's cognitive model and their attention in different ways.
2945180	2948190	324	A	0.70255	But it would be interesting.
2950160	2966080	325	C	0.98	I was going to say they actually already do that specifically for championship, like first person shooter games to try to catch people using Aim bots.
2967780	2973764	326	C	0.97766	So they try to predict where your eyes are going to be based on the data in the game.
2973962	2984550	327	C	1.0	And if you are shooting or you're acquiring the target faster than your eyes could have acquired the target, then, wow.
2986300	2989320	328	A	0.8944	Yeah, interesting.
2989390	2994552	329	A	0.99999	Maybe even some of the behaviors already are out there.
2994606	2998960	330	A	0.63571	So it's not like this is maybe the data already exists.
2998980	3001372	331	C	0.77691	It's like that moving target one.
3001426	3002030	332	C	0.85951	Yeah.
3005360	3025990	333	A	1.0	And interpreting this as like an Occupancy density, but then having a cognitive model where that is like an uncertainty reduction or a salience or relevance observation, which is implicitly what these usages are.
3027960	3040290	334	A	0.99116	Disney trailers, models of false inference, julius Caesar, we had the Obama, now we have the Caesar.
3042870	3068570	335	A	0.99997	Discussion of phase optimality and disorders, different pathological and neurodiverse conditions, addiction, impulsivity, compulsivity, delusions, hallucinations, interpersonal personality disorders, ocular motor syndromes, pharmacotherapy prefrontal syndromes, visual neglect, disorders of interoceptive inference.
3069310	3083480	336	A	0.94192	So if there's a parameters set that works, well, there's going to be various parameter sets that exhibit different outcomes.
3083980	3089644	337	A	0.67	And then, as per Foucault, society is the definer of madness and all of that.
3089682	3098620	338	A	0.81	And the DSM asterisk but broadly speaking, these are models of pathology.
3102660	3109250	339	A	0.99967	We outlined an approach that uses theoretical models described in previous chapters to pose questions to empirical data.
3110340	3117300	340	A	0.99991	This lets us use active inference as a non invasive tool to probe the computational processes that individuals use to make decisions.
3119160	3120340	341	A	0.99985	So funny.
3121640	3128090	342	A	0.71786	Well, the model is not invasive, okay?
3128860	3131080	343	A	0.99	I don't think any model is invasive.
3132860	3141468	344	A	0.99967	Ultimately, the six steps in Figure 9.1 but it's not.
3141574	3142740	345	A	0.99199	That's an error.
3145320	3171014	346	A	0.99104	It's Figure 9.26.
3171052	3183110	347	A	0.9982	Steps in Figure 9.2 provide a generic method for designing experiments to non invasively, interrogate, implicit generative models people or other systems use to drive behavior.
3186920	3191580	348	A	0.99565	That's an opportunity to answer questions about the function of the nervous system in health and indices.
3192080	3194056	349	A	0.83541	All right, thank you, fellows.
3194088	3196670	350	A	0.99942	Looking forward to the conversation next week as well.
3198880	3199676	351	B	0.99946	Thank you.
3199778	3200232	352	A	0.99869	Peace.
3200296	3201020	353	A	0.92179	Bye.

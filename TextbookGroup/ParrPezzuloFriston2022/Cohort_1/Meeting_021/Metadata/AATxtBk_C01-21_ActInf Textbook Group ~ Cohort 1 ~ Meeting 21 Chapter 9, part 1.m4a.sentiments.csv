start	end	speaker	sentiment	confidence	text
1210	1918	A	0.5299751162528992	Hey everyone.
2084	5710	A	0.919599711894989	It is October 21, 2022.
5860	9006	A	0.8834571838378906	We're in cohort one of the textbook group.
9108	15920	A	0.8981147408485413	It's meeting 21 and we're having our first discussion on chapter nine.
19570	22794	A	0.8633393049240112	Chapter nine is model based Data analysis.
22842	27800	A	0.8870518207550049	Let's look at what the section the headers are.
30250	46726	A	0.9606507420539856	There's a short introduction, a discussion of metabasian methods, which is going to be very interesting and in some ways is even like an entry point to thinking about entity modeling in active inference.
46918	48860	A	0.9131831526756287	And so that'll be kind of fun.
49970	59150	A	0.5564237833023071	We'll return to variational LaPlace resonating with our just completed discussion on chapter four.
59300	81314	A	0.7671218514442444	And LaPlace then section 9.49.5 are going to help us see where data in terms of like gigabytes of actual good day, Yakup, actual data from measurements and so on, where these come into play with models.
81442	96810	A	0.804132878780365	So how does one go from a furnished trainable model to a parameterized specified model, which actually is explaining variance in real world data sets?
97150	103070	A	0.5422871708869934	And then there's some examples of GMs and some models of false inference.
104690	115410	A	0.8123785853385925	Well, I added only one general question from a lighter reading.
116710	124420	A	0.7342572212219238	I think as we all here move through this, we can generate a lot of other key points and questions.
128170	132760	A	0.6200747489929199	Ultimately, the models described in this book are only useful if they can answer scientific questions.
139670	144274	A	0.8505586981773376	We could rehearse the introduction again, but let's just go into it.
144392	146610	A	0.800908088684082	So, metabasian methods.
146950	163270	A	0.6322631239891052	This chapter deals with the utility of active inference formulations in analyzing data from behavioral experiments so one could imagine all kinds of bodily, verbal, digital behavior.
164350	173340	A	0.5996665954589844	This goes beyond proof of principle simulations we've seen in previous chapters and instead exploits active inference in answering scientific questions.
176190	181258	A	0.8521752953529358	Broadly speaking, there's two related reasons for fitting a computational model to observe behavior.
181434	184240	A	0.8024990558624268	The first is to estimate parameters of interest.
187090	190610	A	0.8037893176078796	The second is to compare alternative hypotheses.
193780	227080	A	0.808519721031189	So to parameterize from data is one opportunity, that is to make some model of brain function and then understand within one person or one group or two groups how you can consider those parameterizations to be phenotypes.
228800	234856	A	0.8079653978347778	Phenotype is something of a biological organism or system that's measurable.
234968	242972	A	0.8309935927391052	So like femur length is a phenotype, but also a phenotype doesn't have to just be something that's measurable on the body with a ruler.
243116	252690	A	0.8497537970542908	Like phenotype might be distance ran in the first 50 seconds after the hawk flies through the sky on a cloudy day.
253960	259030	A	0.8576560020446777	And so then that is still a measurement that could be inferred or discussed or made.
259960	276810	A	0.7813237905502319	And we're talking about computational phenotyping because the data and observations for sure are like a basal phenotype, like the button being clicked was measurable phenot to show.
277580	285020	A	0.8826910257339478	But also we could talk about the precision variables in our cognitive model as being phenotypic.
287120	291890	A	0.8431146740913391	And also model comparison can be used.
292660	302636	A	0.8667035698890686	So whereas this first modality of parameterization is like the data set is fixed, we collected 240 fMRI data sets.
302748	304316	A	0.7671983242034912	Now we're going to parameterize.
304428	316810	A	0.6219848990440369	So we're going to give some plasticity to our model and we're going to fit parameters so that our model is like the best resembling it can to these 240 fMRI data sets we have.
317420	335470	A	0.8945820927619934	In the second modality, we're treating in some ways the models as fixed and then evaluating to what extent different models stack up.
336800	345010	A	0.6944406628608704	And that can be used at a very fine scale to look for different models that fit better to a given data set.
345540	354352	A	0.818691611289978	But also, this is where we can talk about actual biological explanations and predictions.
354416	357136	A	0.7732713222503662	So, like to give an example from yes, please, Ali.
357168	357750	A	0.5376524925231934	First.
360520	381470	B	0.5999297499656677	Sorry, a related question to this whole discussion is I think it was in Section 9.2, it says this goes beyond the proof of principle simulations we have seen in previous chapters and instead exploits active inference in answering scientific questions.
382640	398092	B	0.6018143892288208	I didn't quite understand this statement here because in the previous chapters, we were also engaged with modeling the real scientific phenomena.
398156	398672	B	0.5664746165275574	Right.
398806	413350	B	0.8308108448982239	So what does proof of principle simulation here exactly mean and how does it differ from the approach from answering real scientific questions?
415080	416664	A	0.6520525217056274	Yeah, good question.
416862	418184	A	0.7860175371170044	Does anyone want to give a thought?
418222	419690	A	0.8288130760192871	Or I can give a thought, too.
422780	433710	C	0.8039774298667908	I mean, literally, just these following that the first and second reasons to use active inference in this way.
435120	455990	C	0.5494900941848755	Instead of just modeling something and saying, oh, that looks like what we intended to model, or oh, gosh, it looks really predictive, or whatever getting more rigorous with it to specifically enumerate parameters for a particular model.
458280	460864	C	0.7633245587348938	And then compare multiple models.
460912	467936	C	0.771277904510498	Like trying to say which one is more precise.
468048	472852	C	0.7573301792144775	That's a different thing than what was being done in the book previously.
472916	489070	C	0.8148168325424194	Like, we're just building up the model and seeing how it kind of worked before, and now we're like talking about putting it into a machinery that's going to select on models like which one's best fit or whatever.
492950	493602	A	0.7286635637283325	Well said.
493656	495460	A	0.7523435950279236	Yeah, I totally agree.
497910	510454	A	0.85746169090271	Although arguably, like the chapter five Neurobiology, the tables of examples were answering scientific questions, but those generative models were not largely presented, but in the papers, one could have gone in.
510492	518150	A	0.7954439520835876	But yes, the previous generative models were like, built from the ground up and then like, oh, look at this intersting behavior.
518510	527434	A	0.7704182863235474	Now we're building the machinery to take in real data and fit that.
527632	546370	A	0.8165956139564514	And then there's the two uses of parameterization or the two uses of data fitting to find the best parameters for a given data set and model, and also to compare models, model comparison.
547110	552260	A	0.9530625939369202	All right, now we get to the very excellent figure 9.1.
553270	564230	A	0.8612799048423767	So 9.1 is the metabasian inference.
566730	570620	A	0.8709263801574707	What does anyone see in this figure?
577280	578300	A	0.6077884435653687	Ali.
582000	604848	B	0.8465241193771362	Well, as pointed out in the text, the inner box somehow represents the generative model of the phenomena we're observing, but the outer dashed box is somehow our generative model as the observers.
605024	630830	B	0.8867420554161072	So the term metabasian, I think used here to denote this kind of change of perspective to a higher level of viewpoint to add another layer of hierarchy to the already hierarchical generative models we had before.
633780	635136	A	0.8782029747962952	Yeah, great.
635318	636370	A	0.7286635637283325	Well said.
637780	640320	A	0.7567323446273804	We could have had an arbitrarily nested.
641380	647892	A	0.8748094439506531	Well, just to make a simple point, this is the discrete time formulation POMDP, Figure 4.3.
648026	662250	A	0.7818166613578796	And that can be nested in an arbitrary way and composed and metabasian is like putting the wrapping paper on the gift because this outermost layer is us.
663900	664680	A	0.6659750938415527	Brock.
669020	677432	C	0.7662808299064636	Um, so the last time this came up was in the context.
677496	679656	C	0.8746361136436462	I think it was in the other cohort.
679688	698160	C	0.8815991282463074	But we were talking about the provenance of the model, the impetus for it being as a tool for diagnosis of patients for nontypical sort of neuron.
701720	708476	C	0.8506162166595459	I guess I'm curious maybe if there's a simple way to understand it from that perspective.
708608	733136	C	0.6232395172119141	But I also have another question, I guess, which is if this is us modeling, it seems like it would be like infinitely recursive because every time you add another layer, then there's another layer that's not being modeled of how we're actually modeling it.
733158	740960	C	0.842667818069458	And therefore we would have to but then if we accounted for that, then there would be another outer.
741860	752260	C	0.765788197517395	I don't know if that's just infinite regression towards the hidden state, one or the other or that's nonsense.
753480	755380	A	0.4841010868549347	Yeah, it's not.
755450	764760	A	0.8961866497993469	I think let's talk about the setting of the clinical neuroimaging.
766140	779096	A	0.9053536653518677	So the inner model is going to be the cognitive behavioral model of the patient just speaking of the participant, the recipient of action as the patient.
779208	782220	A	0.8925149440765381	They're selecting green or blue coins or whatever.
782290	786690	A	0.8288325667381287	They're doing some behavioral paradigm isolating or choice or decision making.
787140	793890	A	0.8437048196792603	We're modeling their action, perception, attention and so on.
796120	810400	A	0.8631485104560852	Whether we formalize it or not, the experimenter's behavior is influencing the parameterization of the patients.
811220	827620	A	0.8569088578224182	Like, we are choosing populations and sampling patients here is like the parameterization of the experiment.
829640	830772	A	0.8571271896362305	We'll have 20 people.
830826	832448	A	0.9058440923690796	They'll come in for two sessions.
832624	835930	A	0.8208871483802795	We're going to do eleven of this.
836860	849230	A	0.8780473470687866	And here's, like the data coming out of so this O here is the observation that the patient sees and their inference on what they're seeing.
855180	860440	A	0.871036171913147	Experimental stimuli are actually an action from the experimenter.
861100	864156	A	0.8191505670547485	We're pushing the O to their screen.
864258	867820	A	0.8734881281852722	So this O tilde is their O sequence.
870000	874540	A	0.8668685555458069	We're getting data from the experiment.
879030	894280	A	0.5298808813095093	We could choose to be ignorant or implicit about our parameterization or maybe even this is a nested model.
895610	902940	A	0.8573738932609558	We made a governance decision in the lab to decide who gets to decide this.
903310	917370	A	0.7979971766471863	And the constraints were the cost and the availability nested modeling of the outer loop.
920730	922710	A	0.8218149542808533	And what about infinite recursion?
923290	951550	A	0.5975550413131714	And I think one silver lining or saving grace or whatever, or just strength of the Bayesian graph or active inference framework is we know we can always dive in and build from a node or we can just treat the edges of our base graph as like a Markov blanket with the unknown.
954050	967380	A	0.8611300587654114	So we could say we made a simple model of decision making here and then this could be a multi agent simulation and each of those could have another nested model and then we could.
967850	972722	C	0.6177260279655457	Then maybe it's irrelevant or maybe we want to know anyways.
972786	978166	C	0.8095979690551758	But what good will it I don't know or it doesn't fit.
978348	979640	C	0.7729812860488892	We do want to know.
980910	981274	A	0.5491447448730469	Yeah.
981312	994510	A	0.5993962287902832	Let's just say that the Pragmatic value for us is high statistical power and sensitivity for diagnosis of a neuro condition.
994930	1001598	A	0.8318963050842285	So we want the inner loop to be in the point.
1001764	1006210	A	0.8156836628913879	We don't want to be spending money beyond diminishing returns on our experiment.
1006950	1011540	A	0.651652991771698	We want to actually diagnose and help the people in this limited situation.
1013670	1019010	A	0.6478750109672546	Then one could do model comparison with just I'm ignoring all of this.
1019160	1023000	A	0.8361660838127136	I'm just going to parameterize I'm going to call it like I see it.
1023530	1025174	A	0.7657209634780884	Then one could have another.
1025292	1035162	A	0.8303131461143494	But if one were to engage in a research program where we collected one data set, then yeah, let's just call like we see it.
1035296	1040090	A	0.5485718846321106	But now we have the opportunity to design a second cohort of fMRI experiments.
1041070	1045370	A	0.9229089021682739	Should we do two people for 100 sessions or 100 people for two sessions?
1045530	1047454	A	0.7833313941955566	Who should those hundred people be?
1047652	1057810	A	0.677598774433136	So at that point, having a policy selection outer loop enabling seems pretty relevant.
1058710	1062210	A	0.5120571851730347	Now, we could make that simple.
1062360	1065330	A	0.8524065613746643	Maybe we always pick 30 participants.
1066150	1070518	A	0.8797101378440857	Maybe it's we either do a small experiment with three or we do a big one with 30.
1070604	1071474	A	0.5713294744491577	It's just simpler.
1071522	1073720	A	0.8185545206069946	Those are the two pathways we have.
1074090	1077826	A	0.80411297082901	Or maybe it's et cetera.
1077938	1086490	A	0.815375566482544	But then you could do model comparison and evaluation on those increasingly complex outer loop models.
1087870	1097870	A	0.655683159828186	None of that would be influencing the structure of the cognitive model of the patient.
1099170	1111220	A	0.7380636930465698	So for all you block, for instance, heads in the chat, one could imagine that there's human fMRI cognitive model.
1111750	1117170	A	0.675177276134491	This is a versionable cognitive model, open source.
1117850	1133900	A	0.8784397840499878	And then different lab groups could have open or closed models with open or closed data around their decision making about how they're using a core model.
1140420	1143664	A	0.8261052370071411	This is also just on a more qualitative point.
1143702	1144770	A	0.7057853937149048	And then I'll leave.
1145240	1150820	A	0.5721424221992493	This is where we can start to talk about moving beyond implicit biases.
1151880	1155156	A	0.5405199527740479	Not just like purging implicit biases or anything like that.
1155178	1159240	A	0.763444721698761	It's just like we're doing investigation and exploration.
1159820	1181820	A	0.6643869876861572	We're externalizing and exteriorizing our priors and we're working such that the relevant features that are in the box are all externalized in terms of the scientific apparatuses parameterization and that's at least something where we can move towards.
1181890	1193360	A	0.4846615791320801	And someone could say, well, you haven't made your hyper hyper hyper priors explicit, but isn't that a better conversation to have than you didn't say why you chose this many people for the experiment.
1194200	1202660	A	0.7310442328453064	It could be like you didn't state the meta governance of how you came to the decision to do 30 participants.
1205800	1213172	C	0.9064426422119141	It makes it really a lot easier to think about comparing and counting, showing that you accounted for your methods.
1213236	1218170	C	0.6936358213424683	And it's the same you're doing the exact same experiment, right?
1219600	1229480	C	0.8841436505317688	Might be able to move the needle a little bit on some of the reproduced earth replication crisis, things like in that vein.
1229640	1237100	A	0.8362016081809998	Yeah, great point, Ollie.
1241680	1245510	B	0.871972382068634	Adding to what well, just mentioned.
1246040	1273272	B	0.843419075012207	As a side note, I think approaching this kind of I mean, this approach to model based or better, the model based data analysis takes the infinitism stance as an implicit, somehow philosophical stance.
1273336	1294576	B	0.6327731609344482	Because if we don't think in terms of infinitism, I don't think we were able to model adequately these kind of metabasian reasoning because we'll get at some point to, as Brock said, just infinite regressive reasons.
1294608	1333916	B	0.812481164932251	So at least in my opinion, it somehow implicitly takes the infinitism stance here as proposed by Peter Klein, at least in large part because, as infinitism claims, will need somehow the subjectively and objectively available reasons to model or to construct justify true belief.
1334108	1346880	B	0.73054438829422	In other words, the reason must be non repeatingly and infinitely available to us in order for us to be able to construct that true belief.
1347040	1355700	B	0.8029579520225525	So I'm not quite sure about it, but it looks very similar to infinitism stance.
1357340	1357704	A	0.5491447448730469	Yeah.
1357742	1360100	A	0.8895830512046814	Interesting introduction.
1360180	1363050	A	0.5729157328605652	Wasn't familiar with this area.
1364540	1371980	A	0.8133423328399658	Like approximate Bayesian computation, bounded rationality, the composability of Bayes graphs.
1372320	1385020	A	0.5720788836479187	These are we get to eat our slice of cake and recognize that there's the rest of the cake and the table and the world outside of the restaurant and so on.
1385090	1391170	A	0.865380048751831	Like we can just say the GM that we made was the thermometer and a temperature value.
1391700	1394072	A	0.7459630966186523	And someone can say, but what about humidity?
1394236	1404820	A	0.5220507383346558	And it's like there's a composability, but we're not within the Positivist or the falsificationist.
1405640	1411880	A	0.6720622181892395	Like, well, my model of temperature and thermometer is positive evidence that that's all that's happening.
1411950	1413496	A	0.7399399876594543	Or it's the best model we have.
1413518	1415640	A	0.6503430008888245	We're just waiting for it to be disproven.
1415980	1428540	A	0.6014125943183899	We can have like a Bayesian portfolio of models and all of them can be understood as maps, as composable maps.
1435940	1448310	A	0.7027811408042908	Could this lead to Bayes optimal experimental design of research programs like statistical power?
1448680	1452992	A	0.6449100375175476	The famous statistics quotation is like something about an autopsy.
1453136	1456116	A	0.5979206562042236	Like the statistician is not going to diagnose it.
1456218	1458836	A	0.6207730770111084	They come in and they tell you what went wrong with the autopsy.
1458948	1461780	A	0.5839409232139587	But that's not fundamental to statistics.
1461860	1468804	A	0.6883070468902588	That's actually just a little bit of like a joke about the practice of research, which is like the biostatistician.
1468852	1476190	A	0.7938054203987122	Just speaking from my own research experience, they're called in for assistance on a complex, already collected data set.
1477200	1482640	A	0.8506836891174316	And so it's like, well, we had three mice in March and then we had six mice in April.
1483700	1494880	A	0.86091548204422	How do we balance this versus the Upfront conversation to help design a high statistical power experiment?
1496100	1505220	A	0.6449198722839355	And so, like statistics Upfront, it's going to increase the reproducibility and alignability of experiments.
1505880	1511770	A	0.7749322056770325	It is a sanity check, if not a formal verification that the experiment is going to tell you something.
1514300	1523150	A	0.6366888284683228	And it allows for replicating experiments, selecting experiments based upon their statistical power.
1527870	1537418	A	0.9006204009056091	Okay, following sections, unpack an example of generic inference that may be used for metabasian inference, the variational LaPlace with hierarchical models.
1537594	1543198	A	0.9205945134162903	Then there's going to be a simple recipe that's related to one of the other questions up here.
1543364	1544080	A	0.584351658821106	Okay.
1549560	1557792	A	0.8419002294540405	Variational LaPlace may be used for more generic likelihood functions than those encountered earlier, which were defined as Gaussian.
1557936	1565480	A	0.8756229281425476	So previously, the family of functions that we are doing variational inference with was Gaussians.
1566780	1581340	A	0.7749632000923157	Now, we can generalize beyond using only Gaussian distributions by saying, whatever it is, we're going to do a Laplacian approximation.
1581760	1598560	A	0.8221675157546997	So even if it's like something very strange, and as mentioned in cohort two discussion just an hour ago, LaPlace approximation has two parameters the mode, the center of the bell, and just the width of the parabola.
1599060	1603248	A	0.8305265307426453	It's guaranteed to capture some of the variance of the distribution.
1603424	1611300	A	0.748142659664154	For distributions with a central tendency, gaussian or otherwise, it can do well for distributions that are like multimodal.
1611720	1619284	A	0.8737720251083374	It does not do well because it gets tricked to finding the mode, which may only capture, like, a minority of the overall bulk.
1619412	1628460	A	0.8690274357795715	And again, the SPM textbook lays out side by side LaPlace variational bays, non parametric sampling approaches.
1634470	1639830	A	0.8795889616012573	From those Laplacian approximations, we're going to do variational inference.
1640410	1647478	A	0.7860430479049683	So the real distribution might be unfactorizable or not.
1647564	1650840	A	0.5155640840530396	We don't know what family of functions it even is.
1651690	1661770	A	0.6051264405250549	But we know that the LaPlace approximation that we make on those distributions for sure will be amenable to variational inference.
1665810	1670734	A	0.7174956798553467	And because of the quadratic nature, we can do gradient ascent.
1670862	1676238	A	0.5880252122879028	So just like the ball going to the bottom of the bowl, this is just an inverted parabola.
1676334	1680178	A	0.8296016454696655	So it's just gradient ascent on the mountain.
1680354	1704750	A	0.8599420189857483	All right, parametric empirical baseage of a matrix to represent the experimental layout is heavily used in SPM.
1705570	1708190	A	0.6198101043701172	This is looking a lot like a regression.
1712610	1716720	A	0.8515720367431641	This is actually a little bit of a short section 9.4.
1723910	1739110	A	0.7803453803062439	This part here comparing the evidence for a model where the second element is allowed to deviate from zero or the precise belief at zero, that is actually very much again like regression testing.
1741530	1775300	A	0.8661205768585205	If you're testing for the effect of a given factor on height that is related to the p value you get for height is related to the model that includes height and the model that doesn't include height, and then their sum of squares is compared within the appropriate statistical test, family t test, z, test, F score, all giving you a p value.
1775830	1781490	A	0.7004811763763428	So this is very much like statistics on regressions.
1783370	1795430	A	0.785013735294342	One thing, maybe there's a deeper reading where this is clearer, but one way to talk about parametric empirical Bayes, is that, well, first off, it's parametric.
1796270	1799494	A	0.7959927916526794	You're dealing with parameters, so sometimes that's not too helpful.
1799542	1809420	A	0.8780157566070557	But the empirical part means, like, the way we're going to get our D matrix, our prior, is from the data.
1810830	1815550	A	0.6860072612762451	So let's just say that we're measuring height in the classroom.
1816530	1821200	A	0.909143328666687	We might be able to use chapter six to specify and furnish the model.
1822370	1829742	A	0.8489968180656433	But then as soon as we make that first and then we might set our prior, we say, well let's have a super loose prior.
1829806	1836694	A	0.8479938507080078	Let's say that it could be like uniform across all but of course who knows what the maximum height is.
1836732	1838162	A	0.6733635067939758	Maybe it's not a human classroom.
1838306	1845480	A	0.6383230686187744	So we can't just specify a uniform distribution across every finite value.
1846170	1848680	A	0.6992772221565247	And so a way to kind of break that.
1850650	1868282	A	0.5314767360687256	A priori challenge where you want your prior to be loose enough to accommodate the full range of the possible data but also sharp enough so that you're not just like starting on like it's just an absurdly uninformative prior.
1868426	1869850	A	0.5373833775520325	All priors are informative.
1869930	1872106	A	0.5155146718025208	Even a uniform prior is still informative.
1872298	1877086	A	0.7902892231941223	The thing that matters is the strength, the weakness of the prior and the family of the prior.
1877278	1892550	A	0.8983827829360962	So what you could do instead would be you could measure ten heights or even one and then use the mean and the variance of that sample empirically to set your prior.
1893690	1901450	A	0.8771985769271851	So set just the mean of the prior on height as the mean that you got from ten and then you could just over disperse the variance.
1903070	1912430	A	0.6080100536346436	So if there's somebody who's outside of that bound now it's like you can be more confident.
1912930	1924260	A	0.895763099193573	So this is widely used to take an empirical data set and then use the data set's empirical values and summary statistics to parameterize the generative model.
1924630	1941494	A	0.8659480810165405	And then the ball goes from there and also the expectation maximization algorithm where you have a given data set and you update the parameters in the generative model and then you generate data compatible with that model and continue.
1941692	1944550	A	0.5119832158088684	That is very close to PEB.
1944970	1950890	A	0.8647503852844238	Okay, 9.5 instructions for model based analysis.
1951950	1960174	A	0.896061897277832	So let's copy these out because the question was like how is this figure 9.2?
1960212	1967330	A	0.8579893112182617	I guess section 9.5 and it's summarized in 9.2.
1970580	1972240	A	0.8217007517814636	Here's the six steps.
1981400	1993900	A	0.7328640818595886	Oh, someone added a great okay, so this is a little bit of a review of the textbook but we're starting with data collection.
1994240	1996760	A	0.8528639674186707	Maybe the data already have been collected.
1996920	2003392	A	0.8637048602104187	So we're putting aside the situation where we're doing the meta modeling on ourself and then doing experimental design and then collecting the data.
2003446	2007200	A	0.8460375070571899	We're just going to take it from the data are collected.
2009860	2021030	A	0.9292876720428467	A POMDP again discrete time in this case is structurally prepared according to the recipe in chapter six.
2022920	2037020	A	0.8958300352096558	Pumdps would we say are equivalent to specifying a likelihood function or they embody or they entail a likelihood function.
2037710	2039900	A	0.8861044049263	What is the arrow between two and three?
2042990	2054780	A	0.8601313233375549	Does merely constructing a POMDP in this format uniquely identify a likelihood function or what work has to happen right here?
2060660	2063456	A	0.5793882012367249	Just a thought we can learn.
2063638	2064530	A	0.6500546336174011	Go ahead.
2066600	2083268	B	0.6258155703544617	I don't think these arrows necessarily mean that this particular component of the modeling process necessarily translates perfectly to the other stage.
2083444	2090488	B	0.771790623664856	I think they're more likely to show the dependencies among each steps.
2090664	2107324	B	0.871536910533905	So in the case of the third stage or the likelihood, the construction of the likelihood function, obviously it depends on what our PMDP model functions.
2107372	2119440	B	0.508417546749115	So I'm not sure if we can if they were 100% isomorphic, I guess they would be redundant.
2119520	2120150	B	0.7360090017318726	Right?
2120520	2129290	B	0.5787221193313599	So yeah, I think those arrows show dependencies rather than translating into each other.
2133130	2133880	A	0.6283750534057617	Thanks.
2134810	2138300	A	0.8055587410926819	I think I broadly agree.
2141890	2145950	A	0.8960602879524231	And there might also be some other ways to draw it, like the prior beliefs.
2152360	2153780	A	0.7714694738388062	Is this theta.
2156140	2164840	A	0.8610290288925171	I think this is using the figure nine one ontology this theta is big theta.
2168310	2170630	A	0.7740033864974976	These are the experimenters parameters.
2171930	2178890	A	0.7807984352111816	The model is the cognitive model of the entity.
2184210	2187840	A	0.832364022731781	Italic u tilde is the observed data.
2188210	2197300	A	0.9125673174858093	So here's the observed data getting passed with the POMDP and associated likelihood function.
2202630	2207650	A	0.8722658157348633	We can the likelihood, okay, the PMDP describes variables in their relationships.
2208310	2231020	A	0.8745439648628235	The likelihood function allows us to create p, which is the distribution of observed behavior u tilde conditioned upon experimental parameters, theta observations, which are in this case the ones that are provided the stimuli, experimental stimuli and the model.
2232830	2242302	A	0.8978819847106934	Then the parametric empirical Bayes comes in when we actually get the data, it comes in in six.
2242356	2244202	A	0.700887143611908	But we're getting there, we're on the path.
2244266	2252290	A	0.8585377931594849	When we have the data flowing from the experiment, we can then do the model inversion.
2253350	2273382	A	0.7599622011184692	Rather than describing the distribution of behavior that would be generated by theta o m, we're going to invert it so we can talk about the distribution of parameters.
2273526	2317000	A	0.8393420577049255	Experimental parameters conditioned upon MoU Bayesian equation allows us to have a proportionality between what we really want to know this top line and a I don't know if I can call it a joint distribution, but these two multiplied distributions, which is the probability of the experimental parameters given the model and separating out the probability of U conditioned on all the rest.
2318010	2332422	A	0.685143232345581	So this is leveraging the sparsity of the inverted model to facilitate a form that is amenable to linear regression type equations.
2332566	2335838	A	0.7497418522834778	This is like y equals MX plus B.
2336004	2339518	A	0.6432445645332336	It's not, but it is.
2339604	2359540	A	0.901868462562561	This is like the same structure y MX plus b, but then these bars and their associated variances which come from LaPlace, this is the p value and the effect size.
2360470	2366040	A	0.7437282204627991	So for this third modality, the effect size is zero and the variance is such and such.
2366570	2377418	A	0.857593834400177	And then for this one we could say the p value or the base factor for this factor mattering, it's high.
2377504	2380700	A	0.7428380846977234	The evidence for this mattering is extremely high.
2383920	2396770	A	0.8822696208953857	So this is like if this were a bar chart and if we were in frequent statistics land and it were like ten plus or minus one, then we have a z score of ten.
2397140	2400644	A	0.5356898307800293	We're ten standard deviations away from zero.
2400842	2411130	A	0.6271955966949463	So the p value of the effect size being greater than zero is whatever it is for a p value of a z score of ten, very low.
2428000	2439520	A	0.6407303214073181	Again, this is like where the real data collection is going to be happening and interfacing.
2440340	2442560	A	0.7738217115402222	So now, here's their description.
2443140	2444770	A	0.8507646918296814	Collect behavioral data.
2446200	2448020	A	0.7526162266731262	Formulate the POMDP.
2450200	2468440	A	0.7936817407608032	It takes parameters as input outputs a fully specified but not yet solved POMDP specify likely likelihood function, specify prior beliefs.
2470060	2473900	A	0.8840073347091675	Often these will be centered on zero with precisions reflecting plausible ranges.
2475200	2476076	A	0.9033102989196777	That's interesting.
2476178	2493180	A	0.855806827545166	Note solve for posterior and model evidence.
2499120	2506076	A	0.8906124830245972	Newton is probably a reference there to some gradient derivative based gradient method.
2506188	2507650	A	0.7124893069267273	I'm not exactly sure.
2508740	2515948	A	0.897993803024292	Group level Analysis treating the estimated parameters for each individual as if they were generated by a second level model.
2516134	2524900	A	0.8337950110435486	This is exactly structurally ANOVA the group level analysis of variance.
2526360	2529800	A	0.7219498157501221	Like we had people throw the ball.
2530540	2538090	A	0.8662603497505188	There was type one and type two a person and then whether type two is there an effective type.
2539260	2540912	A	0.7370086312294006	You make two models.
2541076	2544030	A	0.5957531332969666	One of them is a model without type.
2545120	2548190	A	0.7951429486274719	One of them is a model with two categories of type.
2548720	2559840	A	0.8726589679718018	Then those two models can be compared in terms in the frequentist world, in terms of their hierarchical likelihood.
2561060	2571940	A	0.668526291847229	In the Bayesian world, however, a limitation of the hierarchical likelihood ratio test is that the models must be strictly structurally nested.
2572280	2576950	A	0.8298318386077881	They have to reflect direct, simplifications or elaborations of each other.
2577560	2591320	A	0.5352135300636292	In contrast, Bayesian modeling allows us to use the Bayes factor, which can compare models that don't need to be strictly nested.
2594000	2602670	A	0.8505882024765015	So you could test very structurally different models against each other.
2603380	2605920	A	0.8458959460258484	It's kind of like information as a common currency.
2608100	2615090	A	0.8795576691627502	In the Bayesian approach, I considered variable one, two and three.
2615460	2618290	A	0.8478596210479736	And then someone else could compare variable four, five, six.
2618980	2627600	A	0.6300581693649292	And you can be like, well, how, how much good are those two different models against what we care about in terms of informational criterion?
2627760	2630260	A	0.8302860856056213	AIC BIC the Bayes factor.
2631560	2651710	A	0.601588249206543	Whereas if somebody said, well, I did model with one, two, three and the p value was zero one and I did this with four, five, six and the p value is zero one, those can't be directly compared because the p value is against like a local null hypothesis and there's just like, probably other issues.
2652720	2655120	A	0.6237651109695435	So it's one huge advantage of the Bayesian.
2657220	2660796	A	0.5948418378829956	We're 50 years past the rubicon or 400 or whatever on Bayes.
2660908	2667648	A	0.8297007083892822	But Bayesian formulations allow for the generative recognition model, the tail of two densities.
2667824	2671430	A	0.7881028056144714	So you can specify something that generates and recognizes data.
2672120	2673600	A	0.5269935727119446	That's one advantage.
2673760	2688520	A	0.8199495673179626	Another advantage is that you have access to all the statistical tools of frequent statistics and you can also access this like informational statistics.
2689580	2692716	A	0.5461333990097046	I'm sure there's way more rabbit holes there.
2692738	2706590	A	0.7039735317230225	But like, broadly speaking, instead of taking experimental observations in frequentism and then mapping them onto a t distribution only and then saying well now, because of the p value, let me publish this paper.
2707120	2718020	A	0.6553693413734436	In the Bayes world, there are richer comparisons and more nuanced penalization functions of how many parameters versus how much variance explained.
2721880	2733000	A	0.7175291776657104	Just for example, if anyone thought Bayesians were too confident or if it wasn't the leading modern strategy for dealing with uncertainty.
2735580	2746590	A	0.9238593578338623	This design matrix for a linear model, the SPM design matrices are like very interesting looking.
2748480	2752584	A	0.8488250970840454	So the textbook has many, many of these matrices.
2752632	2754960	A	0.5268117785453796	They're encoded on a gray scale.
2756660	2758690	A	0.7415053248405457	So like, what's happening here?
2759140	2760560	A	0.8641922473907471	Here are images through time.
2760630	2767220	A	0.8028998970985413	Each row is an image, could be an image in a time series, could be like different static images.
2769160	2771236	A	0.8062215447425842	Here, third column is time.
2771418	2775316	A	0.8570412397384644	So this is time .0,123,456.
2775498	2777764	A	0.8147549033164978	And then here's condition one and two.
2777962	2781610	A	0.6802842617034912	First condition, whether one is white and black, it doesn't matter.
2782300	2785272	A	0.7661401033401489	Here condition, let's just say white is one.
2785406	2791008	A	0.7215206623077393	Condition one was in effect, condition two is not, and vice versa.
2791204	2793656	A	0.7916590571403503	But you could have overlapping conditions.
2793848	2796540	A	0.7924832701683044	So it's like a graphical representation.
2798480	2802328	A	0.844087541103363	And this is also common to see this kind of like waterfall.
2802504	2807310	A	0.8811147212982178	Here's patient 12345.
2807680	2816740	A	0.9128172397613525	Here's the three conditions in a randomized order for the 12345 patients.
2818120	2822500	A	0.8994492292404175	Here's their global blood flow, here's some other measurements, et cetera.
2824040	2830360	A	0.828187882900238	But it turns out with so this summarizes the total design of the experiment.
2832460	2852380	A	0.6714473366737366	And it turns out like you just smash it like a matrix multiplication, like the design matrix multiplied by the observations.
2854000	2863360	A	0.7064273357391357	And one can imagine, like if the observations have no relationship with the design matrix, there's a null hypothesis that you could get from random matrix theory.
2865460	2873372	A	0.824385404586792	If the experimental design strongly influences the observations, there's some informational.
2873436	2886126	A	0.8066679835319519	Outcome 96 in our final few minutes, examples of generative models, two experiments outlined in nine five.
2886308	2888014	A	0.53258216381073	The details are not important.
2888212	2895150	A	0.7077525854110718	Oh, but it's two isocating models.
2895830	2901940	A	0.8217164278030396	This is a continuous time left.
2902790	2927380	A	0.7364579439163208	Tracking a moving target right was a categorical eye tracking, as we've mentioned and discussed, doing some kind of a webcam eye tracking cognitive model would be massive.
2930040	2943210	A	0.6436410546302795	It's also very sensitive technology, I think, because it would be able to determine somebody's cognitive model and their attention in different ways.
2945180	2948190	A	0.6894480586051941	But it would be interesting.
2950160	2966080	C	0.7673178911209106	I was going to say they actually already do that specifically for championship, like first person shooter games to try to catch people using Aim bots.
2967780	2973764	C	0.8274813294410706	So they try to predict where your eyes are going to be based on the data in the game.
2973962	2984550	C	0.5922315716743469	And if you are shooting or you're acquiring the target faster than your eyes could have acquired the target, then, wow.
2986300	2989320	A	0.6966834664344788	Yeah, interesting.
2989390	2994552	A	0.7182077169418335	Maybe even some of the behaviors already are out there.
2994606	2998960	A	0.7023244500160217	So it's not like this is maybe the data already exists.
2998980	3001372	C	0.6367886662483215	It's like that moving target one.
3001426	3002030	C	0.5491447448730469	Yeah.
3005360	3025990	A	0.8321179747581482	And interpreting this as like an Occupancy density, but then having a cognitive model where that is like an uncertainty reduction or a salience or relevance observation, which is implicitly what these usages are.
3027960	3040290	A	0.4916874170303345	Disney trailers, models of false inference, julius Caesar, we had the Obama, now we have the Caesar.
3042870	3068570	A	0.5921365022659302	Discussion of phase optimality and disorders, different pathological and neurodiverse conditions, addiction, impulsivity, compulsivity, delusions, hallucinations, interpersonal personality disorders, ocular motor syndromes, pharmacotherapy prefrontal syndromes, visual neglect, disorders of interoceptive inference.
3069310	3083480	A	0.8161632418632507	So if there's a parameters set that works, well, there's going to be various parameter sets that exhibit different outcomes.
3083980	3089644	A	0.6129938960075378	And then, as per Foucault, society is the definer of madness and all of that.
3089682	3098620	A	0.7066894173622131	And the DSM asterisk but broadly speaking, these are models of pathology.
3102660	3109250	A	0.7664305567741394	We outlined an approach that uses theoretical models described in previous chapters to pose questions to empirical data.
3110340	3117300	A	0.5574389100074768	This lets us use active inference as a non invasive tool to probe the computational processes that individuals use to make decisions.
3119160	3120340	A	0.884895384311676	So funny.
3121640	3128090	A	0.7481124997138977	Well, the model is not invasive, okay?
3128860	3131080	A	0.6074188351631165	I don't think any model is invasive.
3132860	3141468	A	0.7155455350875854	Ultimately, the six steps in Figure 9.1 but it's not.
3141574	3142740	A	0.6811224818229675	That's an error.
3145320	3171014	A	0.9033758640289307	It's Figure 9.26.
3171052	3183110	A	0.9053641557693481	Steps in Figure 9.2 provide a generic method for designing experiments to non invasively, interrogate, implicit generative models people or other systems use to drive behavior.
3186920	3191580	A	0.6838635206222534	That's an opportunity to answer questions about the function of the nervous system in health and indices.
3192080	3194056	A	0.8373454809188843	All right, thank you, fellows.
3194088	3196670	A	0.9678792953491211	Looking forward to the conversation next week as well.
3198880	3199676	B	0.8529649972915649	Thank you.
3199778	3200232	A	0.48897257447242737	Peace.
3200296	3201020	A	0.5137446522712708	Bye.

1
00:00:01,210 --> 00:00:04,654
Hey everyone. It is October 21,

2
00:00:04,692 --> 00:00:08,078
2022. We're in cohort one of the

3
00:00:08,084 --> 00:00:11,710
textbook group. It's meeting 21 and

4
00:00:11,780 --> 00:00:14,320
we're having our first discussion on

5
00:00:14,770 --> 00:00:15,920
chapter nine.

6
00:00:19,570 --> 00:00:22,110
Chapter nine is model based Data

7
00:00:22,180 --> 00:00:24,510
analysis. Let's look at what the section

8
00:00:25,370 --> 00:00:27,800
the headers are.

9
00:00:30,250 --> 00:00:32,070
There's a short introduction,

10
00:00:33,130 --> 00:00:35,474
a discussion of metabasian methods,

11
00:00:35,522 --> 00:00:37,810
which is going to be very interesting

12
00:00:37,980 --> 00:00:40,326
and in some ways is even like an entry

13
00:00:40,358 --> 00:00:44,342
point to thinking about entity

14
00:00:44,406 --> 00:00:47,386
modeling in active inference. And so

15
00:00:47,408 --> 00:00:51,294
that'll be kind of fun. We'll return to

16
00:00:51,412 --> 00:00:55,194
variational LaPlace resonating

17
00:00:55,242 --> 00:00:58,174
with our just completed discussion on

18
00:00:58,212 --> 00:01:02,080
chapter four. And LaPlace then

19
00:01:02,950 --> 00:01:06,546
section 9.49.5 are going to help

20
00:01:06,568 --> 00:01:10,034
us see where data in terms

21
00:01:10,072 --> 00:01:13,394
of like gigabytes of actual good

22
00:01:13,432 --> 00:01:16,454
day, Yakup, actual data from

23
00:01:16,492 --> 00:01:19,526
measurements and so on, where these come

24
00:01:19,548 --> 00:01:22,342
into play with models. So how does one

25
00:01:22,396 --> 00:01:27,080
go from a furnished trainable model

26
00:01:27,630 --> 00:01:31,098
to a parameterized specified model,

27
00:01:31,264 --> 00:01:34,678
which actually is explaining variance

28
00:01:34,854 --> 00:01:37,674
in real world data sets? And then

29
00:01:37,712 --> 00:01:41,022
there's some examples of GMs and some

30
00:01:41,076 --> 00:01:43,070
models of false inference.

31
00:01:44,690 --> 00:01:48,734
Well, I added only one

32
00:01:48,932 --> 00:01:53,874
general question from a

33
00:01:53,912 --> 00:01:57,282
lighter reading. I think

34
00:01:57,336 --> 00:02:01,282
as we all here move through this,

35
00:02:01,416 --> 00:02:03,314
we can generate a lot of other key

36
00:02:03,352 --> 00:02:04,420
points and questions.

37
00:02:08,170 --> 00:02:09,766
Ultimately, the models described in this

38
00:02:09,788 --> 00:02:11,494
book are only useful if they can answer

39
00:02:11,532 --> 00:02:12,760
scientific questions.

40
00:02:19,670 --> 00:02:21,726
We could rehearse the introduction

41
00:02:21,758 --> 00:02:24,770
again, but let's just go into it. So,

42
00:02:24,840 --> 00:02:28,046
metabasian methods. This chapter deals

43
00:02:28,078 --> 00:02:29,566
with the utility of active inference

44
00:02:29,598 --> 00:02:32,614
formulations in analyzing data from

45
00:02:32,652 --> 00:02:35,094
behavioral experiments so one could

46
00:02:35,132 --> 00:02:39,190
imagine all kinds of bodily,

47
00:02:39,610 --> 00:02:43,270
verbal, digital behavior.

48
00:02:44,350 --> 00:02:45,974
This goes beyond proof of principle

49
00:02:46,022 --> 00:02:47,462
simulations we've seen in previous

50
00:02:47,526 --> 00:02:50,486
chapters and instead exploits active

51
00:02:50,518 --> 00:02:52,662
inference in answering scientific

52
00:02:52,726 --> 00:02:53,340
questions.

53
00:02:56,190 --> 00:02:57,882
Broadly speaking, there's two related

54
00:02:57,946 --> 00:02:59,450
reasons for fitting a computational

55
00:02:59,530 --> 00:03:02,206
model to observe behavior. The first is

56
00:03:02,228 --> 00:03:04,240
to estimate parameters of interest.

57
00:03:07,090 --> 00:03:09,414
The second is to compare alternative

58
00:03:09,482 --> 00:03:10,610
hypotheses.

59
00:03:13,780 --> 00:03:17,890
So to parameterize from data

60
00:03:18,660 --> 00:03:20,610
is one opportunity,

61
00:03:22,020 --> 00:03:25,540
that is to make some model

62
00:03:25,610 --> 00:03:31,156
of brain function and then understand

63
00:03:31,338 --> 00:03:34,084
within one person or one group or two

64
00:03:34,122 --> 00:03:40,824
groups how you

65
00:03:40,862 --> 00:03:44,824
can consider those parameterizations to

66
00:03:44,862 --> 00:03:47,080
be phenotypes.

67
00:03:48,800 --> 00:03:51,916
Phenotype is something of a

68
00:03:51,938 --> 00:03:54,104
biological organism or system that's

69
00:03:54,152 --> 00:03:56,748
measurable. So like femur length is a

70
00:03:56,754 --> 00:03:59,688
phenotype, but also a phenotype doesn't

71
00:03:59,704 --> 00:04:00,868
have to just be something that's

72
00:04:00,904 --> 00:04:02,972
measurable on the body with a ruler.

73
00:04:03,116 --> 00:04:06,940
Like phenotype might be distance ran

74
00:04:07,020 --> 00:04:10,124
in the first 50 seconds after the hawk

75
00:04:10,172 --> 00:04:12,690
flies through the sky on a cloudy day.

76
00:04:13,960 --> 00:04:16,064
And so then that is still a measurement

77
00:04:16,112 --> 00:04:18,372
that could be inferred or discussed or

78
00:04:18,426 --> 00:04:21,572
made. And we're talking about

79
00:04:21,626 --> 00:04:24,980
computational phenotyping because the

80
00:04:25,050 --> 00:04:27,784
data and observations for sure are like

81
00:04:27,822 --> 00:04:31,784
a basal phenotype, like the button being

82
00:04:31,822 --> 00:04:35,028
clicked was measurable

83
00:04:35,124 --> 00:04:38,588
phenot to show. But also we

84
00:04:38,594 --> 00:04:40,904
could talk about the precision variables

85
00:04:40,952 --> 00:04:43,628
in our cognitive model as being

86
00:04:43,714 --> 00:04:45,020
phenotypic.

87
00:04:47,120 --> 00:04:51,056
And also model comparison can

88
00:04:51,078 --> 00:04:55,148
be used. So whereas this first modality

89
00:04:55,324 --> 00:04:58,512
of parameterization is like the data set

90
00:04:58,566 --> 00:05:02,064
is fixed, we collected 240 fMRI data

91
00:05:02,102 --> 00:05:04,316
sets. Now we're going to parameterize.

92
00:05:04,428 --> 00:05:08,064
So we're going to give some plasticity

93
00:05:08,112 --> 00:05:10,164
to our model and we're going to fit

94
00:05:10,202 --> 00:05:12,468
parameters so that our model is like the

95
00:05:12,474 --> 00:05:15,504
best resembling it can to these 240 fMRI

96
00:05:15,552 --> 00:05:18,104
data sets we have. In the second

97
00:05:18,142 --> 00:05:21,592
modality, we're treating in some ways

98
00:05:21,726 --> 00:05:26,584
the models as fixed and

99
00:05:26,622 --> 00:05:31,836
then evaluating to what extent different

100
00:05:31,938 --> 00:05:35,470
models stack up.

101
00:05:36,800 --> 00:05:38,732
And that can be used at a very fine

102
00:05:38,786 --> 00:05:41,984
scale to look for different models that

103
00:05:42,022 --> 00:05:45,010
fit better to a given data set.

104
00:05:45,540 --> 00:05:49,424
But also, this is where we can talk

105
00:05:49,462 --> 00:05:53,112
about actual biological

106
00:05:53,196 --> 00:05:54,756
explanations and predictions. So, like

107
00:05:54,778 --> 00:05:56,804
to give an example from yes, please,

108
00:05:56,842 --> 00:05:57,750
Ali. First.

109
00:06:00,520 --> 00:06:03,972
Sorry, a related question to this

110
00:06:04,026 --> 00:06:07,656
whole discussion is I

111
00:06:07,678 --> 00:06:11,432
think it was in Section 9.2, it says

112
00:06:11,566 --> 00:06:13,668
this goes beyond the proof of principle

113
00:06:13,764 --> 00:06:15,604
simulations we have seen in previous

114
00:06:15,652 --> 00:06:18,936
chapters and instead exploits active

115
00:06:18,968 --> 00:06:20,760
inference in answering scientific

116
00:06:20,840 --> 00:06:24,670
questions. I didn't quite understand

117
00:06:25,200 --> 00:06:29,276
this statement here because in the

118
00:06:29,298 --> 00:06:32,624
previous chapters, we were

119
00:06:32,662 --> 00:06:36,172
also engaged with modeling

120
00:06:36,316 --> 00:06:38,672
the real scientific phenomena. Right.

121
00:06:38,806 --> 00:06:41,020
So what does proof of principle

122
00:06:41,100 --> 00:06:44,372
simulation here exactly mean and

123
00:06:44,426 --> 00:06:47,684
how does it differ from the

124
00:06:47,722 --> 00:06:51,536
approach from answering

125
00:06:51,728 --> 00:06:53,350
real scientific questions?

126
00:06:55,080 --> 00:06:57,848
Yeah, good question. Does anyone want to

127
00:06:57,854 --> 00:06:59,064
give a thought? Or I can give a thought,

128
00:06:59,102 --> 00:06:59,690
too.

129
00:07:02,780 --> 00:07:05,796
I mean, literally, just these following

130
00:07:05,828 --> 00:07:09,070
that the first and second reasons to use

131
00:07:11,840 --> 00:07:13,710
active inference in this way.

132
00:07:15,120 --> 00:07:19,420
Instead of just modeling something and

133
00:07:19,490 --> 00:07:22,976
saying, oh, that looks like what

134
00:07:22,998 --> 00:07:25,216
we intended to model, or oh, gosh, it

135
00:07:25,238 --> 00:07:26,930
looks really predictive, or whatever

136
00:07:27,300 --> 00:07:29,760
getting more rigorous with it to

137
00:07:29,830 --> 00:07:33,760
specifically enumerate parameters

138
00:07:33,840 --> 00:07:35,990
for a particular model.

139
00:07:38,280 --> 00:07:41,364
And then compare multiple models. Like

140
00:07:41,482 --> 00:07:45,110
trying to say which one is

141
00:07:46,840 --> 00:07:49,224
more precise. That's a different thing

142
00:07:49,262 --> 00:07:52,088
than what was being done in the book

143
00:07:52,174 --> 00:07:54,216
previously. Like, we're just building up

144
00:07:54,238 --> 00:07:56,136
the model and seeing how it kind of

145
00:07:56,158 --> 00:07:58,250
worked before, and now we're like

146
00:07:59,600 --> 00:08:01,516
talking about putting it into a

147
00:08:01,538 --> 00:08:05,132
machinery that's going to select on

148
00:08:05,186 --> 00:08:08,316
models like which one's best fit

149
00:08:08,338 --> 00:08:09,070
or whatever.

150
00:08:12,950 --> 00:08:15,460
Well said. Yeah, I totally agree.

151
00:08:17,910 --> 00:08:20,130
Although arguably, like the chapter five

152
00:08:20,200 --> 00:08:22,302
Neurobiology, the tables of examples

153
00:08:22,366 --> 00:08:23,762
were answering scientific questions,

154
00:08:23,816 --> 00:08:25,266
but those generative models were not

155
00:08:25,288 --> 00:08:28,230
largely presented, but in the papers,

156
00:08:28,970 --> 00:08:31,206
one could have gone in. But yes, the

157
00:08:31,228 --> 00:08:32,854
previous generative models were like,

158
00:08:32,892 --> 00:08:35,174
built from the ground up and then like,

159
00:08:35,212 --> 00:08:38,150
oh, look at this intersting behavior.

160
00:08:38,510 --> 00:08:41,690
Now we're building the machinery to take

161
00:08:41,760 --> 00:08:46,010
in real data and

162
00:08:46,160 --> 00:08:49,690
fit that. And then there's the two

163
00:08:49,760 --> 00:08:53,214
uses of parameterization or the two

164
00:08:53,252 --> 00:08:56,622
uses of data fitting to find the best

165
00:08:56,676 --> 00:09:00,414
parameters for a given data set and

166
00:09:00,452 --> 00:09:04,798
model, and also to compare models,

167
00:09:04,974 --> 00:09:08,114
model comparison. All right,

168
00:09:08,312 --> 00:09:10,946
now we get to the very excellent figure

169
00:09:11,048 --> 00:09:21,334
9.1. So 9.1

170
00:09:21,532 --> 00:09:24,230
is the metabasian inference.

171
00:09:26,730 --> 00:09:30,620
What does anyone see in this figure?

172
00:09:37,280 --> 00:09:38,300
Ali.

173
00:09:42,000 --> 00:09:45,328
Well, as pointed out in the text, the

174
00:09:45,414 --> 00:09:48,828
inner box somehow represents

175
00:09:48,924 --> 00:09:53,520
the generative model of the phenomena

176
00:09:54,920 --> 00:09:58,464
we're observing, but the outer dashed

177
00:09:58,512 --> 00:10:02,592
box is somehow our generative

178
00:10:02,656 --> 00:10:06,416
model as the observers. So the term

179
00:10:06,448 --> 00:10:10,568
metabasian, I think used here

180
00:10:10,734 --> 00:10:13,176
to denote this kind of change of

181
00:10:13,278 --> 00:10:17,112
perspective to a

182
00:10:17,166 --> 00:10:22,140
higher level of viewpoint to

183
00:10:22,210 --> 00:10:26,268
add another layer of hierarchy to

184
00:10:26,434 --> 00:10:29,192
the already hierarchical generative

185
00:10:29,256 --> 00:10:30,830
models we had before.

186
00:10:33,780 --> 00:10:36,370
Yeah, great. Well said.

187
00:10:37,780 --> 00:10:40,320
We could have had an arbitrarily nested.

188
00:10:41,380 --> 00:10:43,536
Well, just to make a simple point, this

189
00:10:43,558 --> 00:10:46,464
is the discrete time formulation POMDP,

190
00:10:46,512 --> 00:10:50,116
Figure 4.3. And that can be nested in an

191
00:10:50,138 --> 00:10:52,692
arbitrary way and composed and

192
00:10:52,746 --> 00:10:56,436
metabasian is like putting the

193
00:10:56,458 --> 00:10:59,384
wrapping paper on the gift because this

194
00:10:59,422 --> 00:11:02,250
outermost layer is us.

195
00:11:03,900 --> 00:11:04,680
Brock.

196
00:11:09,020 --> 00:11:13,260
Um, so the

197
00:11:13,410 --> 00:11:16,716
last time this came up was in

198
00:11:16,738 --> 00:11:19,100
the context. I think it was in the other

199
00:11:19,170 --> 00:11:25,328
cohort. But we were talking about the

200
00:11:25,494 --> 00:11:28,096
provenance of the model, the impetus for

201
00:11:28,118 --> 00:11:32,050
it being as a tool for

202
00:11:32,820 --> 00:11:36,908
diagnosis of patients for nontypical

203
00:11:37,004 --> 00:11:38,160
sort of neuron.

204
00:11:41,720 --> 00:11:45,284
I guess I'm curious maybe if

205
00:11:45,322 --> 00:11:47,316
there's a simple way to understand it

206
00:11:47,338 --> 00:11:49,930
from that perspective. But I also have

207
00:11:53,260 --> 00:11:56,632
another question, I guess, which is if

208
00:11:56,686 --> 00:11:59,240
this is us modeling,

209
00:12:02,540 --> 00:12:04,392
it seems like it would be like

210
00:12:04,446 --> 00:12:06,664
infinitely recursive because every time

211
00:12:06,702 --> 00:12:08,996
you add another layer, then there's

212
00:12:09,028 --> 00:12:11,284
another layer that's not being modeled

213
00:12:11,412 --> 00:12:13,296
of how we're actually modeling it. And

214
00:12:13,318 --> 00:12:15,296
therefore we would have to but then if

215
00:12:15,318 --> 00:12:19,536
we accounted for that, then there

216
00:12:19,558 --> 00:12:22,528
would be another outer. I don't know if

217
00:12:22,534 --> 00:12:24,752
that's just infinite regression towards

218
00:12:24,806 --> 00:12:30,740
the hidden state, one or the other or

219
00:12:30,890 --> 00:12:34,230
that's nonsense. Yeah,

220
00:12:34,680 --> 00:12:38,628
it's not. I think let's

221
00:12:38,804 --> 00:12:42,648
talk about the setting of the

222
00:12:42,734 --> 00:12:44,760
clinical neuroimaging.

223
00:12:46,140 --> 00:12:49,832
So the inner model is

224
00:12:49,886 --> 00:12:52,600
going to be the cognitive behavioral

225
00:12:52,680 --> 00:12:56,012
model of the patient just speaking of

226
00:12:56,146 --> 00:12:58,120
the participant, the recipient of action

227
00:12:58,200 --> 00:13:01,004
as the patient. They're selecting green

228
00:13:01,042 --> 00:13:02,636
or blue coins or whatever. They're doing

229
00:13:02,658 --> 00:13:05,152
some behavioral paradigm isolating or

230
00:13:05,206 --> 00:13:07,596
choice or decision making. We're

231
00:13:07,628 --> 00:13:10,348
modeling their action,

232
00:13:10,444 --> 00:13:13,890
perception, attention and so on.

233
00:13:16,120 --> 00:13:19,910
Whether we formalize it or not,

234
00:13:22,800 --> 00:13:25,820
the experimenter's behavior is

235
00:13:25,890 --> 00:13:29,568
influencing the parameterization of

236
00:13:29,574 --> 00:13:33,612
the patients. Like, we are choosing

237
00:13:33,676 --> 00:13:37,164
populations and sampling

238
00:13:37,212 --> 00:13:44,484
patients here

239
00:13:44,522 --> 00:13:46,644
is like the parameterization of the

240
00:13:46,682 --> 00:13:47,620
experiment.

241
00:13:49,640 --> 00:13:51,348
We'll have 20 people. They'll come in

242
00:13:51,354 --> 00:13:53,412
for two sessions. We're going to do

243
00:13:53,466 --> 00:13:55,930
eleven of this.

244
00:13:56,860 --> 00:13:59,610
And here's, like the data coming out of

245
00:14:01,660 --> 00:14:05,416
so this O here is the observation that

246
00:14:05,438 --> 00:14:08,172
the patient sees and their inference on

247
00:14:08,226 --> 00:14:09,230
what they're seeing.

248
00:14:15,180 --> 00:14:17,912
Experimental stimuli are actually an

249
00:14:17,966 --> 00:14:21,604
action from the experimenter. We're

250
00:14:21,652 --> 00:14:25,132
pushing the O to their screen. So this O

251
00:14:25,186 --> 00:14:27,820
tilde is their O sequence.

252
00:14:30,000 --> 00:14:33,404
We're getting data from

253
00:14:33,442 --> 00:14:34,540
the experiment.

254
00:14:39,030 --> 00:14:42,914
We could choose to be

255
00:14:43,032 --> 00:14:46,898
ignorant or implicit about our

256
00:14:47,064 --> 00:14:51,926
parameterization or

257
00:14:52,108 --> 00:14:54,280
maybe even this is a nested model.

258
00:14:55,610 --> 00:14:58,840
We made a governance decision in the lab

259
00:14:59,450 --> 00:15:02,940
to decide who gets to decide this.

260
00:15:03,310 --> 00:15:05,466
And the constraints were the cost and

261
00:15:05,488 --> 00:15:14,310
the availability nested

262
00:15:15,370 --> 00:15:17,370
modeling of the outer loop.

263
00:15:20,730 --> 00:15:23,846
And what about infinite recursion? And I

264
00:15:23,868 --> 00:15:27,586
think one silver lining or saving grace

265
00:15:27,618 --> 00:15:30,162
or whatever, or just strength of the

266
00:15:30,236 --> 00:15:32,006
Bayesian graph or active inference

267
00:15:32,038 --> 00:15:36,666
framework is we

268
00:15:36,688 --> 00:15:40,380
know we can always dive in

269
00:15:41,470 --> 00:15:45,054
and build from a node or we can just

270
00:15:45,092 --> 00:15:48,702
treat the edges of our base graph as

271
00:15:48,756 --> 00:15:51,550
like a Markov blanket with the unknown.

272
00:15:54,050 --> 00:15:56,242
So we could say we made a simple model

273
00:15:56,296 --> 00:16:00,340
of decision making here and then

274
00:16:00,710 --> 00:16:02,734
this could be a multi agent simulation

275
00:16:02,782 --> 00:16:04,162
and each of those could have another

276
00:16:04,216 --> 00:16:07,380
nested model and then we could.

277
00:16:07,850 --> 00:16:11,766
Then maybe it's irrelevant or maybe we

278
00:16:11,788 --> 00:16:15,766
want to know anyways. But what good

279
00:16:15,788 --> 00:16:18,166
will it I don't know or it doesn't fit.

280
00:16:18,348 --> 00:16:21,274
We do want to know. Yeah.

281
00:16:21,312 --> 00:16:23,690
Let's just say that the Pragmatic value

282
00:16:23,760 --> 00:16:27,510
for us is high statistical

283
00:16:27,590 --> 00:16:31,580
power and sensitivity for diagnosis of a

284
00:16:32,770 --> 00:16:36,862
neuro condition. So we

285
00:16:36,916 --> 00:16:40,382
want the inner loop to be

286
00:16:40,516 --> 00:16:42,814
in the point. We don't want to be

287
00:16:42,932 --> 00:16:44,506
spending money beyond diminishing

288
00:16:44,538 --> 00:16:47,794
returns on our experiment. We want to

289
00:16:47,832 --> 00:16:49,986
actually diagnose and help the people in

290
00:16:50,008 --> 00:16:51,540
this limited situation.

291
00:16:53,670 --> 00:16:57,122
Then one could do model comparison with

292
00:16:57,176 --> 00:16:59,842
just I'm ignoring all of this. I'm just

293
00:16:59,896 --> 00:17:01,846
going to parameterize I'm going to call

294
00:17:01,868 --> 00:17:04,726
it like I see it. Then one could have

295
00:17:04,748 --> 00:17:06,678
another. But if one were to engage in a

296
00:17:06,684 --> 00:17:10,082
research program where we collected one

297
00:17:10,156 --> 00:17:13,834
data set, then yeah, let's just

298
00:17:13,872 --> 00:17:16,186
call like we see it. But now we have the

299
00:17:16,208 --> 00:17:18,474
opportunity to design a second cohort of

300
00:17:18,512 --> 00:17:22,282
fMRI experiments. Should we do two

301
00:17:22,336 --> 00:17:24,254
people for 100 sessions or 100 people

302
00:17:24,292 --> 00:17:26,206
for two sessions? Who should those

303
00:17:26,228 --> 00:17:29,150
hundred people be? So at that point,

304
00:17:29,300 --> 00:17:33,706
having a policy selection outer

305
00:17:33,738 --> 00:17:36,994
loop enabling seems pretty

306
00:17:37,032 --> 00:17:40,242
relevant. Now, we could

307
00:17:40,296 --> 00:17:43,970
make that simple. Maybe we always pick

308
00:17:44,040 --> 00:17:47,762
30 participants. Maybe it's we either do

309
00:17:47,896 --> 00:17:49,638
a small experiment with three or we do a

310
00:17:49,644 --> 00:17:51,474
big one with 30. It's just simpler.

311
00:17:51,522 --> 00:17:54,454
Those are the two pathways we have. Or

312
00:17:54,492 --> 00:17:57,826
maybe it's et cetera.

313
00:17:57,938 --> 00:18:00,094
But then you could do model comparison

314
00:18:00,162 --> 00:18:03,418
and evaluation on those

315
00:18:03,504 --> 00:18:06,490
increasingly complex outer loop models.

316
00:18:07,870 --> 00:18:14,654
None of that would be influencing the

317
00:18:14,692 --> 00:18:17,134
structure of the cognitive model of the

318
00:18:17,172 --> 00:18:21,434
patient. So for all you block,

319
00:18:21,482 --> 00:18:24,622
for instance, heads in the chat, one

320
00:18:24,676 --> 00:18:28,738
could imagine that there's human

321
00:18:28,904 --> 00:18:32,386
fMRI cognitive model. This is a

322
00:18:32,408 --> 00:18:34,900
versionable cognitive model,

323
00:18:35,910 --> 00:18:39,830
open source. And then different

324
00:18:39,900 --> 00:18:43,702
lab groups could have open

325
00:18:43,756 --> 00:18:45,762
or closed models with open or closed

326
00:18:45,826 --> 00:18:50,018
data around their decision making about

327
00:18:50,124 --> 00:18:53,900
how they're using a core model.

328
00:19:00,420 --> 00:19:03,372
This is also just on a more qualitative

329
00:19:03,436 --> 00:19:05,924
point. And then I'll leave. This is

330
00:19:05,962 --> 00:19:09,092
where we can start to talk about moving

331
00:19:09,146 --> 00:19:13,012
beyond implicit biases. Not just like

332
00:19:13,146 --> 00:19:14,996
purging implicit biases or anything like

333
00:19:15,018 --> 00:19:17,096
that. It's just like we're doing

334
00:19:17,198 --> 00:19:20,244
investigation and exploration. We're

335
00:19:20,292 --> 00:19:22,776
externalizing and exteriorizing our

336
00:19:22,798 --> 00:19:26,136
priors and we're working such that

337
00:19:26,158 --> 00:19:29,156
the relevant features that are in the

338
00:19:29,198 --> 00:19:33,240
box are all externalized

339
00:19:33,320 --> 00:19:35,592
in terms of the scientific apparatuses

340
00:19:35,656 --> 00:19:39,356
parameterization and that's at

341
00:19:39,378 --> 00:19:41,452
least something where we can move

342
00:19:41,506 --> 00:19:42,812
towards. And someone could say, well,

343
00:19:42,866 --> 00:19:44,604
you haven't made your hyper hyper hyper

344
00:19:44,652 --> 00:19:48,032
priors explicit, but isn't that a better

345
00:19:48,086 --> 00:19:50,736
conversation to have than you didn't say

346
00:19:50,758 --> 00:19:52,416
why you chose this many people for the

347
00:19:52,438 --> 00:19:55,664
experiment. It could be like you didn't

348
00:19:55,792 --> 00:19:59,636
state the meta governance of how

349
00:19:59,658 --> 00:20:01,588
you came to the decision to do 30

350
00:20:01,674 --> 00:20:02,660
participants.

351
00:20:05,800 --> 00:20:09,016
It makes it really a lot easier to think

352
00:20:09,038 --> 00:20:11,348
about comparing and counting, showing

353
00:20:11,364 --> 00:20:13,172
that you accounted for your methods.

354
00:20:13,236 --> 00:20:16,552
And it's the same you're doing the exact

355
00:20:16,606 --> 00:20:20,396
same experiment, right? Might be

356
00:20:20,418 --> 00:20:22,684
able to move the needle a little bit on

357
00:20:22,722 --> 00:20:26,024
some of the reproduced earth replication

358
00:20:26,072 --> 00:20:29,480
crisis, things like in that vein.

359
00:20:29,640 --> 00:20:31,550
Yeah, great point,

360
00:20:36,160 --> 00:20:37,100
Ollie.

361
00:20:41,680 --> 00:20:45,510
Adding to what well, just mentioned.

362
00:20:46,040 --> 00:20:50,832
As a side note, I think approaching

363
00:20:50,976 --> 00:20:54,100
this kind of I mean,

364
00:20:54,170 --> 00:20:58,330
this approach to model based or

365
00:20:59,180 --> 00:21:01,800
better, the model based data analysis

366
00:21:03,340 --> 00:21:06,932
takes the infinitism

367
00:21:06,996 --> 00:21:09,320
stance as an implicit,

368
00:21:10,540 --> 00:21:13,272
somehow philosophical stance.

369
00:21:13,336 --> 00:21:16,764
Because if we don't think in terms

370
00:21:16,802 --> 00:21:19,884
of infinitism, I don't think we were

371
00:21:19,922 --> 00:21:23,388
able to model adequately

372
00:21:23,484 --> 00:21:26,720
these kind of metabasian reasoning

373
00:21:27,060 --> 00:21:31,008
because we'll get at some point to,

374
00:21:31,174 --> 00:21:34,136
as Brock said, just infinite regressive

375
00:21:34,188 --> 00:21:37,504
reasons. So at least in my opinion,

376
00:21:37,632 --> 00:21:41,332
it somehow implicitly takes

377
00:21:41,386 --> 00:21:45,030
the infinitism stance here as

378
00:21:45,560 --> 00:21:49,688
proposed by Peter Klein, at least

379
00:21:49,854 --> 00:21:53,160
in large part because, as infinitism

380
00:21:55,020 --> 00:21:59,560
claims, will need somehow

381
00:22:00,000 --> 00:22:03,240
the subjectively and objectively

382
00:22:03,320 --> 00:22:10,028
available reasons to model

383
00:22:10,114 --> 00:22:13,916
or to construct justify true belief.

384
00:22:14,108 --> 00:22:18,796
In other words, the reason must be non

385
00:22:18,828 --> 00:22:21,936
repeatingly and infinitely available to

386
00:22:21,958 --> 00:22:24,788
us in order for us to be able to

387
00:22:24,954 --> 00:22:28,896
construct that true belief. So I'm

388
00:22:28,928 --> 00:22:32,884
not quite sure about it, but it looks

389
00:22:33,002 --> 00:22:35,700
very similar to infinitism stance.

390
00:22:37,340 --> 00:22:40,100
Yeah. Interesting introduction.

391
00:22:40,180 --> 00:22:43,050
Wasn't familiar with this area.

392
00:22:44,540 --> 00:22:47,508
Like approximate Bayesian computation,

393
00:22:47,604 --> 00:22:50,520
bounded rationality, the composability

394
00:22:50,600 --> 00:22:55,356
of Bayes graphs. These are we

395
00:22:55,378 --> 00:22:58,460
get to eat our slice of cake and

396
00:22:58,530 --> 00:23:00,348
recognize that there's the rest of the

397
00:23:00,354 --> 00:23:02,380
cake and the table and the world outside

398
00:23:02,450 --> 00:23:05,568
of the restaurant and so on. Like we can

399
00:23:05,574 --> 00:23:08,256
just say the GM that we made was the

400
00:23:08,278 --> 00:23:11,170
thermometer and a temperature value.

401
00:23:11,700 --> 00:23:13,152
And someone can say, but what about

402
00:23:13,206 --> 00:23:15,268
humidity? And it's like there's a

403
00:23:15,274 --> 00:23:19,936
composability, but we're

404
00:23:19,968 --> 00:23:23,396
not within the Positivist or the

405
00:23:23,418 --> 00:23:26,644
falsificationist. Like, well, my model

406
00:23:26,682 --> 00:23:29,912
of temperature and thermometer is

407
00:23:29,966 --> 00:23:31,556
positive evidence that that's all that's

408
00:23:31,588 --> 00:23:33,336
happening. Or it's the best model we

409
00:23:33,358 --> 00:23:34,648
have. We're just waiting for it to be

410
00:23:34,654 --> 00:23:37,684
disproven. We can have like a Bayesian

411
00:23:37,812 --> 00:23:41,196
portfolio of models and all

412
00:23:41,218 --> 00:23:45,420
of them can be understood as maps,

413
00:23:46,480 --> 00:23:48,540
as composable maps.

414
00:23:55,940 --> 00:24:00,220
Could this lead to Bayes optimal

415
00:24:00,300 --> 00:24:04,720
experimental design of research programs

416
00:24:06,040 --> 00:24:09,440
like statistical power? The famous

417
00:24:09,520 --> 00:24:11,604
statistics quotation is like something

418
00:24:11,642 --> 00:24:14,464
about an autopsy. Like the statistician

419
00:24:14,512 --> 00:24:16,596
is not going to diagnose it. They come

420
00:24:16,618 --> 00:24:17,728
in and they tell you what went wrong

421
00:24:17,754 --> 00:24:20,264
with the autopsy. But that's not

422
00:24:20,302 --> 00:24:22,116
fundamental to statistics. That's

423
00:24:22,148 --> 00:24:23,496
actually just a little bit of like a

424
00:24:23,518 --> 00:24:25,610
joke about the practice of research,

425
00:24:26,060 --> 00:24:29,064
which is like the biostatistician. Just

426
00:24:29,102 --> 00:24:30,132
speaking from my own research

427
00:24:30,206 --> 00:24:32,764
experience, they're called in for

428
00:24:32,802 --> 00:24:34,780
assistance on a complex, already

429
00:24:34,850 --> 00:24:38,220
collected data set. And so it's like,

430
00:24:38,290 --> 00:24:40,668
well, we had three mice in March and

431
00:24:40,674 --> 00:24:44,176
then we had six mice in April. How do

432
00:24:44,198 --> 00:24:48,560
we balance this versus the Upfront

433
00:24:50,580 --> 00:24:53,024
conversation to help design a high

434
00:24:53,062 --> 00:24:56,624
statistical power experiment? And so,

435
00:24:56,662 --> 00:24:59,440
like statistics Upfront,

436
00:25:00,280 --> 00:25:02,068
it's going to increase the

437
00:25:02,074 --> 00:25:04,164
reproducibility and alignability of

438
00:25:04,202 --> 00:25:07,592
experiments. It is a sanity check,

439
00:25:07,646 --> 00:25:09,656
if not a formal verification that the

440
00:25:09,678 --> 00:25:11,144
experiment is going to tell you

441
00:25:11,182 --> 00:25:11,770
something.

442
00:25:14,300 --> 00:25:17,012
And it allows for replicating

443
00:25:17,076 --> 00:25:20,252
experiments, selecting experiments based

444
00:25:20,306 --> 00:25:23,150
upon their statistical power.

445
00:25:27,870 --> 00:25:30,826
Okay, following sections, unpack an

446
00:25:30,848 --> 00:25:32,506
example of generic inference that may be

447
00:25:32,528 --> 00:25:34,026
used for metabasian inference, the

448
00:25:34,048 --> 00:25:36,442
variational LaPlace with hierarchical

449
00:25:36,506 --> 00:25:38,558
models. Then there's going to be a

450
00:25:38,564 --> 00:25:41,758
simple recipe that's related to one of

451
00:25:41,764 --> 00:25:44,080
the other questions up here. Okay.

452
00:25:49,560 --> 00:25:53,204
Variational LaPlace may be used for more

453
00:25:53,242 --> 00:25:54,964
generic likelihood functions than those

454
00:25:55,002 --> 00:25:56,816
encountered earlier, which were defined

455
00:25:56,848 --> 00:26:00,628
as Gaussian. So previously, the family

456
00:26:00,794 --> 00:26:02,536
of functions that we are doing

457
00:26:02,558 --> 00:26:04,168
variational inference with was

458
00:26:04,254 --> 00:26:07,530
Gaussians. Now,

459
00:26:09,020 --> 00:26:11,384
we can generalize beyond using only

460
00:26:11,422 --> 00:26:15,404
Gaussian distributions by

461
00:26:15,442 --> 00:26:18,956
saying, whatever it is, we're going to

462
00:26:18,978 --> 00:26:22,508
do a Laplacian approximation. So even if

463
00:26:22,514 --> 00:26:24,860
it's like something very strange, and as

464
00:26:24,930 --> 00:26:28,252
mentioned in cohort two discussion

465
00:26:28,316 --> 00:26:31,644
just an hour ago, LaPlace approximation

466
00:26:31,772 --> 00:26:34,672
has two parameters the mode, the center

467
00:26:34,726 --> 00:26:37,440
of the bell, and just the width of the

468
00:26:37,510 --> 00:26:40,732
parabola. It's guaranteed to capture

469
00:26:40,796 --> 00:26:42,436
some of the variance of the

470
00:26:42,458 --> 00:26:44,628
distribution. For distributions with a

471
00:26:44,634 --> 00:26:46,768
central tendency, gaussian or otherwise,

472
00:26:46,864 --> 00:26:49,556
it can do well for distributions that

473
00:26:49,578 --> 00:26:53,136
are like multimodal. It does not do well

474
00:26:53,258 --> 00:26:55,416
because it gets tricked to finding the

475
00:26:55,438 --> 00:26:57,256
mode, which may only capture, like, a

476
00:26:57,278 --> 00:26:59,944
minority of the overall bulk. And again,

477
00:26:59,982 --> 00:27:03,210
the SPM textbook lays out side by side

478
00:27:03,580 --> 00:27:07,192
LaPlace variational bays, non parametric

479
00:27:07,256 --> 00:27:08,460
sampling approaches.

480
00:27:14,470 --> 00:27:16,930
From those Laplacian approximations,

481
00:27:17,510 --> 00:27:19,830
we're going to do variational inference.

482
00:27:20,410 --> 00:27:24,390
So the real distribution

483
00:27:24,810 --> 00:27:27,938
might be unfactorizable or not. We don't

484
00:27:27,954 --> 00:27:30,182
know what family of functions it even

485
00:27:30,236 --> 00:27:33,926
is. But we know that the

486
00:27:33,948 --> 00:27:36,266
LaPlace approximation that we make on

487
00:27:36,288 --> 00:27:39,066
those distributions for sure will be

488
00:27:39,088 --> 00:27:41,770
amenable to variational inference.

489
00:27:45,810 --> 00:27:49,074
And because of the quadratic nature, we

490
00:27:49,112 --> 00:27:51,698
can do gradient ascent. So just like the

491
00:27:51,704 --> 00:27:53,598
ball going to the bottom of the bowl,

492
00:27:53,774 --> 00:27:56,546
this is just an inverted parabola. So

493
00:27:56,568 --> 00:27:59,366
it's just gradient ascent on the

494
00:27:59,388 --> 00:28:02,674
mountain. All right, parametric

495
00:28:02,722 --> 00:28:15,110
empirical baseage

496
00:28:15,950 --> 00:28:19,194
of a matrix to represent the

497
00:28:19,232 --> 00:28:22,802
experimental layout is heavily

498
00:28:22,886 --> 00:28:26,094
used in SPM. This is

499
00:28:26,132 --> 00:28:28,190
looking a lot like a regression.

500
00:28:32,610 --> 00:28:34,974
This is actually a little bit of a short

501
00:28:35,092 --> 00:28:36,720
section 9.4.

502
00:28:43,910 --> 00:28:47,054
This part here comparing the evidence

503
00:28:47,102 --> 00:28:50,066
for a model where the second element is

504
00:28:50,088 --> 00:28:52,418
allowed to deviate from zero or the

505
00:28:52,424 --> 00:28:55,814
precise belief at zero, that is actually

506
00:28:56,012 --> 00:28:59,110
very much again like regression testing.

507
00:29:01,530 --> 00:29:03,958
If you're testing for the effect of a

508
00:29:03,964 --> 00:29:08,230
given factor on height

509
00:29:09,530 --> 00:29:12,362
that is related to the p value you get

510
00:29:12,416 --> 00:29:16,026
for height is related to the

511
00:29:16,048 --> 00:29:18,254
model that includes height and the model

512
00:29:18,292 --> 00:29:21,600
that doesn't include height, and then

513
00:29:22,210 --> 00:29:26,430
their sum of squares is compared within

514
00:29:26,500 --> 00:29:29,470
the appropriate statistical test,

515
00:29:29,540 --> 00:29:33,246
family t test, z, test, F score,

516
00:29:33,438 --> 00:29:37,442
all giving you a p value. So this is

517
00:29:37,496 --> 00:29:40,418
very much like statistics on

518
00:29:40,504 --> 00:29:41,490
regressions.

519
00:29:43,370 --> 00:29:46,658
One thing, maybe there's a deeper

520
00:29:46,674 --> 00:29:49,254
reading where this is clearer, but one

521
00:29:49,292 --> 00:29:51,074
way to talk about parametric empirical

522
00:29:51,122 --> 00:29:54,038
Bayes, is that, well, first off,

523
00:29:54,204 --> 00:29:57,066
it's parametric. You're dealing with

524
00:29:57,088 --> 00:29:59,146
parameters, so sometimes that's not too

525
00:29:59,168 --> 00:30:01,850
helpful. But the empirical part means,

526
00:30:01,920 --> 00:30:04,202
like, the way we're going to get our D

527
00:30:04,256 --> 00:30:08,060
matrix, our prior, is from

528
00:30:08,430 --> 00:30:11,818
the data. So let's

529
00:30:11,834 --> 00:30:14,686
just say that we're measuring height in

530
00:30:14,708 --> 00:30:17,774
the classroom. We might be able to use

531
00:30:17,812 --> 00:30:20,574
chapter six to specify and furnish the

532
00:30:20,612 --> 00:30:24,706
model. But then as soon as

533
00:30:24,728 --> 00:30:27,266
we make that first and then we might set

534
00:30:27,288 --> 00:30:28,658
our prior, we say, well let's have a

535
00:30:28,664 --> 00:30:30,546
super loose prior. Let's say that it

536
00:30:30,568 --> 00:30:34,566
could be like uniform across all but

537
00:30:34,588 --> 00:30:36,258
of course who knows what the maximum

538
00:30:36,274 --> 00:30:37,462
height is. Maybe it's not a human

539
00:30:37,516 --> 00:30:40,006
classroom. So we can't just specify a

540
00:30:40,028 --> 00:30:44,342
uniform distribution across every

541
00:30:44,396 --> 00:30:47,606
finite value. And so a way to kind of

542
00:30:47,628 --> 00:30:51,990
break that. A priori

543
00:30:52,670 --> 00:30:56,422
challenge where you want your prior

544
00:30:56,486 --> 00:30:59,018
to be loose enough to accommodate the

545
00:30:59,024 --> 00:31:02,190
full range of the possible data but also

546
00:31:02,260 --> 00:31:03,774
sharp enough so that you're not just

547
00:31:03,812 --> 00:31:06,126
like starting on like it's just an

548
00:31:06,148 --> 00:31:09,114
absurdly uninformative prior. All priors

549
00:31:09,162 --> 00:31:11,086
are informative. Even a uniform prior is

550
00:31:11,108 --> 00:31:12,938
still informative. The thing that

551
00:31:12,964 --> 00:31:14,626
matters is the strength, the weakness of

552
00:31:14,648 --> 00:31:17,086
the prior and the family of the prior.

553
00:31:17,278 --> 00:31:19,586
So what you could do instead would be

554
00:31:19,688 --> 00:31:22,974
you could measure ten heights or even

555
00:31:23,032 --> 00:31:26,598
one and then use the mean and the

556
00:31:26,604 --> 00:31:31,334
variance of that sample empirically to

557
00:31:31,372 --> 00:31:35,046
set your prior. So set just the mean of

558
00:31:35,068 --> 00:31:37,434
the prior on height as the mean that you

559
00:31:37,472 --> 00:31:39,626
got from ten and then you could just

560
00:31:39,648 --> 00:31:41,450
over disperse the variance.

561
00:31:43,070 --> 00:31:45,386
So if there's somebody who's outside of

562
00:31:45,408 --> 00:31:49,360
that bound now it's like

563
00:31:50,690 --> 00:31:53,518
you can be more confident. So this is

564
00:31:53,524 --> 00:31:56,174
widely used to take an empirical data

565
00:31:56,212 --> 00:31:59,466
set and then use the data set's

566
00:31:59,498 --> 00:32:01,854
empirical values and summary statistics

567
00:32:01,982 --> 00:32:04,260
to parameterize the generative model.

568
00:32:04,630 --> 00:32:09,074
And then the ball goes from there and

569
00:32:09,112 --> 00:32:11,998
also the expectation maximization

570
00:32:12,094 --> 00:32:15,094
algorithm where you have a given data

571
00:32:15,132 --> 00:32:17,078
set and you update the parameters in the

572
00:32:17,084 --> 00:32:18,834
generative model and then you generate

573
00:32:18,882 --> 00:32:20,742
data compatible with that model and

574
00:32:20,796 --> 00:32:24,550
continue. That is very close to PEB.

575
00:32:24,970 --> 00:32:29,094
Okay, 9.5 instructions

576
00:32:29,142 --> 00:32:32,566
for model based analysis. So let's

577
00:32:32,598 --> 00:32:36,362
copy these out because

578
00:32:36,416 --> 00:32:39,582
the question was like how is this figure

579
00:32:39,636 --> 00:32:45,184
9.2? I guess section 9.5 and

580
00:32:45,222 --> 00:32:47,330
it's summarized in 9.2.

581
00:32:50,580 --> 00:32:52,240
Here's the six steps.

582
00:33:01,400 --> 00:33:04,488
Oh, someone added a great okay,

583
00:33:04,654 --> 00:33:08,444
so this is a little bit of a review of

584
00:33:08,482 --> 00:33:11,692
the textbook but we're starting with

585
00:33:11,746 --> 00:33:15,372
data collection. Maybe the data already

586
00:33:15,426 --> 00:33:17,596
have been collected. So we're putting

587
00:33:17,618 --> 00:33:19,084
aside the situation where we're doing

588
00:33:19,122 --> 00:33:21,024
the meta modeling on ourself and then

589
00:33:21,062 --> 00:33:22,704
doing experimental design and then

590
00:33:22,742 --> 00:33:24,144
collecting the data. We're just going to

591
00:33:24,182 --> 00:33:27,200
take it from the data are collected.

592
00:33:29,860 --> 00:33:33,556
A POMDP again discrete time

593
00:33:33,658 --> 00:33:37,940
in this case is structurally prepared

594
00:33:38,360 --> 00:33:41,030
according to the recipe in chapter six.

595
00:33:42,920 --> 00:33:48,634
Pumdps would

596
00:33:48,672 --> 00:33:51,738
we say are equivalent to specifying a

597
00:33:51,744 --> 00:33:54,986
likelihood function or they embody or

598
00:33:55,008 --> 00:33:58,026
they entail a likelihood function. What

599
00:33:58,048 --> 00:33:59,900
is the arrow between two and three?

600
00:34:02,990 --> 00:34:05,594
Does merely constructing a POMDP in this

601
00:34:05,632 --> 00:34:08,454
format uniquely identify a likelihood

602
00:34:08,502 --> 00:34:12,634
function or

603
00:34:12,752 --> 00:34:14,780
what work has to happen right here?

604
00:34:20,660 --> 00:34:24,530
Just a thought we can learn. Go ahead.

605
00:34:26,600 --> 00:34:30,624
I don't think these arrows necessarily

606
00:34:30,752 --> 00:34:34,264
mean that this particular

607
00:34:34,382 --> 00:34:38,264
component of the modeling process

608
00:34:38,462 --> 00:34:41,684
necessarily translates perfectly

609
00:34:41,732 --> 00:34:45,084
to the other stage. I think they're more

610
00:34:45,122 --> 00:34:48,600
likely to show the dependencies

611
00:34:48,680 --> 00:34:52,764
among each steps. So in the case

612
00:34:52,802 --> 00:34:55,832
of the third stage or the likelihood,

613
00:34:55,976 --> 00:34:57,532
the construction of the likelihood

614
00:34:57,596 --> 00:35:02,112
function, obviously it depends on what

615
00:35:02,246 --> 00:35:07,324
our PMDP model functions.

616
00:35:07,372 --> 00:35:10,390
So I'm not sure if we can

617
00:35:13,880 --> 00:35:17,060
if they were 100% isomorphic,

618
00:35:17,640 --> 00:35:20,150
I guess they would be redundant. Right?

619
00:35:20,520 --> 00:35:24,968
So yeah, I think those arrows show

620
00:35:25,054 --> 00:35:28,084
dependencies rather than translating

621
00:35:28,132 --> 00:35:29,290
into each other.

622
00:35:33,130 --> 00:35:36,794
Thanks. I think

623
00:35:36,832 --> 00:35:38,300
I broadly agree.

624
00:35:41,890 --> 00:35:43,614
And there might also be some other ways

625
00:35:43,652 --> 00:35:45,950
to draw it, like the prior beliefs.

626
00:35:52,360 --> 00:35:53,780
Is this theta.

627
00:35:56,140 --> 00:35:58,712
I think this is using the figure nine

628
00:35:58,766 --> 00:36:02,424
one ontology this

629
00:36:02,462 --> 00:36:04,840
theta is big theta.

630
00:36:08,310 --> 00:36:10,630
These are the experimenters parameters.

631
00:36:11,930 --> 00:36:16,714
The model is

632
00:36:16,752 --> 00:36:18,890
the cognitive model of the entity.

633
00:36:24,210 --> 00:36:27,840
Italic u tilde is the observed data.

634
00:36:28,210 --> 00:36:30,062
So here's the observed data getting

635
00:36:30,116 --> 00:36:33,694
passed with

636
00:36:33,732 --> 00:36:36,654
the POMDP and associated likelihood

637
00:36:36,702 --> 00:36:37,300
function.

638
00:36:42,630 --> 00:36:45,374
We can the likelihood, okay, the PMDP

639
00:36:45,422 --> 00:36:46,626
describes variables in their

640
00:36:46,648 --> 00:36:49,880
relationships. The likelihood function

641
00:36:50,410 --> 00:36:53,654
allows us to create p, which is the

642
00:36:53,692 --> 00:36:57,462
distribution of observed behavior u

643
00:36:57,516 --> 00:37:02,098
tilde conditioned upon experimental

644
00:37:02,114 --> 00:37:05,274
parameters, theta observations, which

645
00:37:05,312 --> 00:37:06,874
are in this case the ones that are

646
00:37:06,912 --> 00:37:08,646
provided the stimuli, experimental

647
00:37:08,678 --> 00:37:11,020
stimuli and the model.

648
00:37:12,830 --> 00:37:15,710
Then the parametric empirical Bayes

649
00:37:16,050 --> 00:37:18,720
comes in when we actually get the data,

650
00:37:21,170 --> 00:37:23,054
it comes in in six. But we're getting

651
00:37:23,092 --> 00:37:25,454
there, we're on the path. When we have

652
00:37:25,652 --> 00:37:27,890
the data flowing from the experiment,

653
00:37:29,110 --> 00:37:32,290
we can then do the model inversion.

654
00:37:33,350 --> 00:37:36,994
Rather than describing the

655
00:37:37,032 --> 00:37:40,726
distribution of behavior that would

656
00:37:40,748 --> 00:37:44,200
be generated by theta o m,

657
00:37:46,010 --> 00:37:48,934
we're going to invert it so we can talk

658
00:37:48,972 --> 00:37:53,382
about the distribution of parameters.

659
00:37:53,526 --> 00:37:56,214
Experimental parameters conditioned upon

660
00:37:56,262 --> 00:38:00,646
MoU Bayesian

661
00:38:00,838 --> 00:38:03,998
equation allows us to have a

662
00:38:04,004 --> 00:38:06,894
proportionality between what we really

663
00:38:06,932 --> 00:38:10,542
want to know this top

664
00:38:10,596 --> 00:38:15,120
line and a

665
00:38:18,710 --> 00:38:20,206
I don't know if I can call it a joint

666
00:38:20,238 --> 00:38:22,654
distribution, but these two multiplied

667
00:38:22,702 --> 00:38:25,810
distributions, which is the probability

668
00:38:26,310 --> 00:38:28,386
of the experimental parameters given the

669
00:38:28,408 --> 00:38:32,120
model and separating out

670
00:38:32,650 --> 00:38:36,086
the probability of U conditioned on

671
00:38:36,108 --> 00:38:39,446
all the rest. So this is leveraging the

672
00:38:39,468 --> 00:38:44,554
sparsity of the inverted model to

673
00:38:44,592 --> 00:38:48,742
facilitate a form that is amenable

674
00:38:48,886 --> 00:38:52,422
to linear regression type equations.

675
00:38:52,566 --> 00:38:55,838
This is like y equals MX plus B.

676
00:38:56,004 --> 00:38:59,966
It's not, but it is. This is

677
00:38:59,988 --> 00:39:05,086
like the same structure y

678
00:39:05,188 --> 00:39:07,060
MX plus b,

679
00:39:08,950 --> 00:39:12,446
but then these bars and their associated

680
00:39:12,478 --> 00:39:14,770
variances which come from LaPlace,

681
00:39:16,630 --> 00:39:19,540
this is the p value and the effect size.

682
00:39:20,470 --> 00:39:22,422
So for this third modality, the effect

683
00:39:22,476 --> 00:39:25,206
size is zero and the variance is such

684
00:39:25,228 --> 00:39:27,894
and such. And then for this one we could

685
00:39:27,932 --> 00:39:32,746
say the p value or the base factor for

686
00:39:32,768 --> 00:39:34,730
this factor mattering,

687
00:39:36,670 --> 00:39:38,634
it's high. The evidence for this

688
00:39:38,672 --> 00:39:40,700
mattering is extremely high.

689
00:39:43,920 --> 00:39:45,676
So this is like if this were a bar chart

690
00:39:45,708 --> 00:39:48,812
and if we were in frequent statistics

691
00:39:48,876 --> 00:39:52,624
land and it were like ten plus

692
00:39:52,662 --> 00:39:56,144
or minus one, then we have a z score of

693
00:39:56,182 --> 00:39:59,652
ten. We're ten standard deviations away

694
00:39:59,706 --> 00:40:03,156
from zero. So the p value of the

695
00:40:03,178 --> 00:40:05,670
effect size being greater than zero is

696
00:40:07,480 --> 00:40:09,396
whatever it is for a p value of a z

697
00:40:09,418 --> 00:40:11,130
score of ten, very low.

698
00:40:28,000 --> 00:40:34,496
Again, this is like where the real data

699
00:40:34,598 --> 00:40:38,160
collection is going to be happening

700
00:40:38,230 --> 00:40:41,072
and interfacing. So now,

701
00:40:41,126 --> 00:40:43,596
here's their description. Collect

702
00:40:43,628 --> 00:40:48,020
behavioral data. Formulate the POMDP.

703
00:40:50,200 --> 00:40:52,148
It takes parameters as input outputs a

704
00:40:52,154 --> 00:40:55,140
fully specified but not yet solved POMDP

705
00:41:00,700 --> 00:41:03,770
specify likely likelihood function,

706
00:41:06,380 --> 00:41:08,440
specify prior beliefs.

707
00:41:10,060 --> 00:41:11,532
Often these will be centered on zero

708
00:41:11,586 --> 00:41:13,144
with precisions reflecting plausible

709
00:41:13,192 --> 00:41:16,076
ranges. That's interesting.

710
00:41:16,178 --> 00:41:31,168
Note solve

711
00:41:31,204 --> 00:41:33,180
for posterior and model evidence.

712
00:41:39,120 --> 00:41:41,244
Newton is probably a reference there to

713
00:41:41,282 --> 00:41:44,652
some gradient derivative

714
00:41:44,716 --> 00:41:46,960
based gradient method. I'm not exactly

715
00:41:47,030 --> 00:41:51,196
sure. Group level Analysis treating

716
00:41:51,228 --> 00:41:52,784
the estimated parameters for each

717
00:41:52,822 --> 00:41:54,576
individual as if they were generated by

718
00:41:54,598 --> 00:41:57,270
a second level model. This is exactly

719
00:41:57,800 --> 00:42:02,484
structurally ANOVA the

720
00:42:02,522 --> 00:42:04,900
group level analysis of variance.

721
00:42:06,360 --> 00:42:09,800
Like we had people throw the ball.

722
00:42:10,540 --> 00:42:13,130
There was type one and type two a person

723
00:42:13,980 --> 00:42:16,968
and then whether type two is there an

724
00:42:16,974 --> 00:42:20,912
effective type. You make two models.

725
00:42:21,076 --> 00:42:24,030
One of them is a model without type.

726
00:42:25,120 --> 00:42:26,812
One of them is a model with two

727
00:42:26,866 --> 00:42:29,772
categories of type. Then those two

728
00:42:29,826 --> 00:42:33,644
models can be compared in

729
00:42:33,682 --> 00:42:36,944
terms in the frequentist world, in terms

730
00:42:36,982 --> 00:42:39,840
of their hierarchical likelihood.

731
00:42:41,060 --> 00:42:43,376
In the Bayesian world, however, a

732
00:42:43,398 --> 00:42:44,684
limitation of the hierarchical

733
00:42:44,732 --> 00:42:47,664
likelihood ratio test is that the models

734
00:42:47,712 --> 00:42:51,940
must be strictly structurally nested.

735
00:42:52,280 --> 00:42:54,004
They have to reflect direct,

736
00:42:54,122 --> 00:42:56,356
simplifications or elaborations of each

737
00:42:56,378 --> 00:42:58,820
other. In contrast,

738
00:42:59,260 --> 00:43:01,416
Bayesian modeling allows us to use the

739
00:43:01,438 --> 00:43:05,204
Bayes factor, which can compare models

740
00:43:05,252 --> 00:43:10,324
that don't need to be strictly

741
00:43:10,372 --> 00:43:11,320
nested.

742
00:43:14,000 --> 00:43:17,870
So you could test

743
00:43:18,320 --> 00:43:21,528
very structurally different models

744
00:43:21,544 --> 00:43:23,984
against each other. It's kind of like

745
00:43:24,022 --> 00:43:25,920
information as a common currency.

746
00:43:28,100 --> 00:43:29,920
In the Bayesian approach,

747
00:43:31,620 --> 00:43:34,464
I considered variable one, two and

748
00:43:34,502 --> 00:43:36,576
three. And then someone else could

749
00:43:36,598 --> 00:43:39,344
compare variable four, five, six. And

750
00:43:39,382 --> 00:43:40,644
you can be like, well, how, how much

751
00:43:40,762 --> 00:43:42,624
good are those two different models

752
00:43:42,672 --> 00:43:46,052
against what we care about in terms of

753
00:43:46,106 --> 00:43:49,236
informational criterion? AIC BIC the

754
00:43:49,258 --> 00:43:52,484
Bayes factor. Whereas if somebody

755
00:43:52,522 --> 00:43:54,664
said, well, I did model with one, two,

756
00:43:54,702 --> 00:43:57,976
three and the p value was zero one and I

757
00:43:57,998 --> 00:43:59,976
did this with four, five, six and the p

758
00:43:59,998 --> 00:44:03,704
value is zero one, those can't be

759
00:44:03,742 --> 00:44:06,776
directly compared because the p value is

760
00:44:06,798 --> 00:44:09,340
against like a local null hypothesis and

761
00:44:09,410 --> 00:44:11,052
there's just like, probably other

762
00:44:11,106 --> 00:44:14,256
issues. So it's one huge advantage of

763
00:44:14,278 --> 00:44:17,744
the Bayesian. We're 50

764
00:44:17,782 --> 00:44:19,488
years past the rubicon or 400 or

765
00:44:19,494 --> 00:44:22,860
whatever on Bayes. But Bayesian

766
00:44:22,940 --> 00:44:25,132
formulations allow for the generative

767
00:44:25,196 --> 00:44:26,916
recognition model, the tail of two

768
00:44:26,938 --> 00:44:29,252
densities. So you can specify something

769
00:44:29,306 --> 00:44:31,430
that generates and recognizes data.

770
00:44:32,120 --> 00:44:34,704
That's one advantage. Another advantage

771
00:44:34,752 --> 00:44:36,596
is that you have access to all the

772
00:44:36,618 --> 00:44:40,440
statistical tools of frequent statistics

773
00:44:41,820 --> 00:44:45,528
and you can also access

774
00:44:45,614 --> 00:44:48,520
this like informational statistics.

775
00:44:49,580 --> 00:44:52,536
I'm sure there's way more rabbit holes

776
00:44:52,568 --> 00:44:54,030
there. But like, broadly speaking,

777
00:44:54,560 --> 00:44:56,136
instead of taking experimental

778
00:44:56,168 --> 00:44:58,492
observations in frequentism and then

779
00:44:58,546 --> 00:45:01,230
mapping them onto a t distribution only

780
00:45:01,680 --> 00:45:03,308
and then saying well now, because of the

781
00:45:03,314 --> 00:45:06,590
p value, let me publish this paper.

782
00:45:07,120 --> 00:45:10,620
In the Bayes world, there are richer

783
00:45:10,700 --> 00:45:13,404
comparisons and more nuanced

784
00:45:13,452 --> 00:45:15,344
penalization functions of how many

785
00:45:15,382 --> 00:45:17,056
parameters versus how much variance

786
00:45:17,088 --> 00:45:18,020
explained.

787
00:45:21,880 --> 00:45:23,844
Just for example, if anyone thought

788
00:45:23,882 --> 00:45:25,796
Bayesians were too confident or if it

789
00:45:25,818 --> 00:45:30,500
wasn't the leading modern

790
00:45:30,580 --> 00:45:33,000
strategy for dealing with uncertainty.

791
00:45:35,580 --> 00:45:38,730
This design matrix for a linear model,

792
00:45:41,440 --> 00:45:44,860
the SPM design matrices are like very

793
00:45:44,930 --> 00:45:46,590
interesting looking.

794
00:45:48,480 --> 00:45:51,836
So the textbook has many, many of

795
00:45:51,858 --> 00:45:53,936
these matrices. They're encoded on a

796
00:45:53,958 --> 00:45:57,312
gray scale. So like,

797
00:45:57,366 --> 00:45:59,996
what's happening here? Here are images

798
00:46:00,028 --> 00:46:02,160
through time. Each row is an image,

799
00:46:02,980 --> 00:46:04,972
could be an image in a time series,

800
00:46:05,116 --> 00:46:07,220
could be like different static images.

801
00:46:09,160 --> 00:46:12,036
Here, third column is time. So this is

802
00:46:12,058 --> 00:46:15,316
time .0,123,456.

803
00:46:15,498 --> 00:46:17,764
And then here's condition one and two.

804
00:46:17,962 --> 00:46:20,232
First condition, whether one is white

805
00:46:20,286 --> 00:46:23,144
and black, it doesn't matter. Here

806
00:46:23,342 --> 00:46:25,272
condition, let's just say white is one.

807
00:46:25,406 --> 00:46:27,770
Condition one was in effect,

808
00:46:28,380 --> 00:46:31,008
condition two is not, and vice versa.

809
00:46:31,204 --> 00:46:32,664
But you could have overlapping

810
00:46:32,712 --> 00:46:35,400
conditions. So it's like a graphical

811
00:46:35,480 --> 00:46:39,788
representation. And this

812
00:46:39,794 --> 00:46:41,276
is also common to see this kind of like

813
00:46:41,298 --> 00:46:45,384
waterfall. Here's patient

814
00:46:45,432 --> 00:46:50,176
12345. Here's the

815
00:46:50,198 --> 00:46:53,490
three conditions in a randomized order

816
00:46:54,020 --> 00:46:56,740
for the 12345 patients.

817
00:46:58,120 --> 00:47:00,576
Here's their global blood flow, here's

818
00:47:00,608 --> 00:47:02,500
some other measurements, et cetera.

819
00:47:04,040 --> 00:47:07,264
But it turns out with so this summarizes

820
00:47:07,312 --> 00:47:10,360
the total design of the experiment.

821
00:47:12,460 --> 00:47:16,264
And it turns out like you

822
00:47:16,302 --> 00:47:21,664
just smash

823
00:47:21,712 --> 00:47:24,120
it like a matrix multiplication,

824
00:47:27,200 --> 00:47:30,908
like the design matrix multiplied by

825
00:47:31,074 --> 00:47:34,604
the observations. And one

826
00:47:34,642 --> 00:47:36,232
can imagine, like if the observations

827
00:47:36,296 --> 00:47:37,792
have no relationship with the design

828
00:47:37,846 --> 00:47:41,168
matrix, there's a null hypothesis that

829
00:47:41,174 --> 00:47:43,360
you could get from random matrix theory.

830
00:47:45,460 --> 00:47:47,852
If the experimental design strongly

831
00:47:47,916 --> 00:47:50,000
influences the observations,

832
00:47:51,700 --> 00:47:58,922
there's some informational. Outcome 96

833
00:47:58,976 --> 00:48:01,018
in our final few minutes, examples of

834
00:48:01,024 --> 00:48:04,666
generative models, two experiments

835
00:48:04,698 --> 00:48:07,198
outlined in nine five. The details are

836
00:48:07,204 --> 00:48:08,960
not important. Oh,

837
00:48:10,690 --> 00:48:15,150
but it's two isocating models.

838
00:48:15,830 --> 00:48:21,940
This is a continuous time left.

839
00:48:22,790 --> 00:48:26,900
Tracking a moving target right

840
00:48:29,860 --> 00:48:32,720
was a categorical eye tracking,

841
00:48:35,380 --> 00:48:37,600
as we've mentioned and discussed,

842
00:48:38,660 --> 00:48:42,216
doing some kind of a webcam eye

843
00:48:42,268 --> 00:48:46,244
tracking cognitive model would be

844
00:48:46,442 --> 00:48:47,380
massive.

845
00:48:50,040 --> 00:48:54,524
It's also very sensitive

846
00:48:54,592 --> 00:48:58,184
technology, I think, because it would

847
00:48:58,222 --> 00:48:59,716
be able to determine somebody's

848
00:48:59,748 --> 00:49:02,296
cognitive model and their attention in

849
00:49:02,318 --> 00:49:03,210
different ways.

850
00:49:05,180 --> 00:49:08,190
But it would be interesting.

851
00:49:10,160 --> 00:49:12,476
I was going to say they actually already

852
00:49:12,578 --> 00:49:18,072
do that specifically for championship,

853
00:49:18,136 --> 00:49:21,872
like first person shooter games to try

854
00:49:21,926 --> 00:49:26,080
to catch people using Aim bots.

855
00:49:27,780 --> 00:49:30,912
So they try to predict where your eyes

856
00:49:30,966 --> 00:49:33,076
are going to be based on the data in the

857
00:49:33,098 --> 00:49:36,784
game. And if you are shooting

858
00:49:36,912 --> 00:49:39,376
or you're acquiring the target faster

859
00:49:39,408 --> 00:49:42,544
than your eyes could have acquired

860
00:49:42,592 --> 00:49:44,550
the target, then, wow.

861
00:49:46,300 --> 00:49:49,976
Yeah, interesting. Maybe even some

862
00:49:49,998 --> 00:49:54,104
of the behaviors already are

863
00:49:54,142 --> 00:49:56,136
out there. So it's not like this is

864
00:49:56,318 --> 00:49:59,404
maybe the data already exists. It's like

865
00:49:59,442 --> 00:50:02,030
that moving target one. Yeah.

866
00:50:05,360 --> 00:50:09,308
And interpreting this as like

867
00:50:09,314 --> 00:50:10,960
an Occupancy density,

868
00:50:12,340 --> 00:50:15,584
but then having a cognitive model where

869
00:50:15,622 --> 00:50:17,856
that is like an uncertainty reduction or

870
00:50:17,878 --> 00:50:21,440
a salience or relevance observation,

871
00:50:22,680 --> 00:50:25,248
which is implicitly what these usages

872
00:50:25,344 --> 00:50:29,940
are. Disney trailers,

873
00:50:35,090 --> 00:50:37,278
models of false inference, julius

874
00:50:37,294 --> 00:50:39,266
Caesar, we had the Obama, now we have

875
00:50:39,288 --> 00:50:40,290
the Caesar.

876
00:50:42,870 --> 00:50:44,834
Discussion of phase optimality and

877
00:50:44,872 --> 00:50:45,810
disorders,

878
00:50:48,070 --> 00:50:51,394
different pathological and neurodiverse

879
00:50:51,442 --> 00:50:53,554
conditions, addiction, impulsivity,

880
00:50:53,602 --> 00:50:55,394
compulsivity, delusions,

881
00:50:55,442 --> 00:50:57,666
hallucinations, interpersonal

882
00:50:57,698 --> 00:50:59,714
personality disorders, ocular motor

883
00:50:59,762 --> 00:51:03,686
syndromes, pharmacotherapy prefrontal

884
00:51:03,718 --> 00:51:06,746
syndromes, visual neglect, disorders of

885
00:51:06,768 --> 00:51:13,876
interoceptive inference. So if

886
00:51:13,898 --> 00:51:15,752
there's a parameters set that works,

887
00:51:15,806 --> 00:51:19,252
well, there's going to be various

888
00:51:19,316 --> 00:51:21,450
parameter sets that exhibit different

889
00:51:22,380 --> 00:51:25,860
outcomes. And then, as per Foucault,

890
00:51:26,020 --> 00:51:29,228
society is the definer of madness and

891
00:51:29,234 --> 00:51:34,712
all of that. And the DSM asterisk

892
00:51:34,856 --> 00:51:37,416
but broadly speaking, these are models

893
00:51:37,448 --> 00:51:38,620
of pathology.

894
00:51:42,660 --> 00:51:44,524
We outlined an approach that uses

895
00:51:44,572 --> 00:51:46,876
theoretical models described in previous

896
00:51:46,908 --> 00:51:48,604
chapters to pose questions to empirical

897
00:51:48,652 --> 00:51:52,556
data. This lets us use active inference

898
00:51:52,588 --> 00:51:54,148
as a non invasive tool to probe the

899
00:51:54,154 --> 00:51:56,016
computational processes that individuals

900
00:51:56,048 --> 00:52:00,340
use to make decisions. So funny.

901
00:52:01,640 --> 00:52:04,020
Well, the model is not invasive,

902
00:52:07,340 --> 00:52:10,216
okay? I don't think any model is

903
00:52:10,238 --> 00:52:13,816
invasive. Ultimately, the six

904
00:52:13,838 --> 00:52:20,784
steps in Figure 9.1 but

905
00:52:20,822 --> 00:52:22,740
it's not. That's an error.

906
00:52:25,320 --> 00:52:51,014
It's Figure 9.26.

907
00:52:51,052 --> 00:52:53,026
Steps in Figure 9.2 provide a generic

908
00:52:53,058 --> 00:52:56,146
method for designing experiments to non

909
00:52:56,178 --> 00:52:57,698
invasively, interrogate, implicit

910
00:52:57,714 --> 00:53:01,014
generative models people or other

911
00:53:01,052 --> 00:53:03,110
systems use to drive behavior.

912
00:53:06,920 --> 00:53:08,580
That's an opportunity to answer

913
00:53:08,650 --> 00:53:09,820
questions about the function of the

914
00:53:09,850 --> 00:53:11,580
nervous system in health and indices.

915
00:53:12,080 --> 00:53:14,284
All right, thank you, fellows. Looking

916
00:53:14,322 --> 00:53:16,076
forward to the conversation next week as

917
00:53:16,098 --> 00:53:19,676
well. Thank you.

918
00:53:19,778 --> 00:53:20,232
Peace.



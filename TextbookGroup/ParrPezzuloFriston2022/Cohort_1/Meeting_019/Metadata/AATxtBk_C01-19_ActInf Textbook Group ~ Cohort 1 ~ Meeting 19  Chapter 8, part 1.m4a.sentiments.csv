start	end	speaker	sentiment	confidence	text
970	1840	A	0.5579421520233154	Hello everyone.
2530	5760	A	0.907856822013855	It's october 7, 2022.
6130	12400	A	0.901492714881897	We are in meeting 19 for cohort one and we're discussing chapter eight.
13490	20800	A	0.7297400236129761	So there's a bunch of questions and also a lot of things to talk about.
21650	24998	A	0.9085718989372253	Let us just begin with looking at chapter eight.
25044	25640	A	0.5307095050811768	Eight.
26170	36566	A	0.8675497770309448	So does anybody want to give any first thoughts on active inference in continuous time?
36668	40230	A	0.8358827829360962	Everything flows, nothing stands still, et cetera?
44330	45080	A	0.46103888750076294	Yes.
49970	51578	B	0.6831021904945374	Not on that specifically.
51674	55386	B	0.892601490020752	Okay, but I will have something to say for this chapter.
55498	60462	B	0.8786088824272156	I worked on this topic during my graduate time.
60516	63780	B	0.9023733735084534	So once we get to about 8.3, I'll weigh in.
65510	66450	A	0.5708939433097839	Excellent.
68310	73266	A	0.8796364068984985	Any comments like discrete continuous time?
73448	76440	A	0.7688882350921631	I know this is something we'll touch on again and again.
86400	94752	A	0.6989505887031555	It's kind of like another low road, high road duality with continuous and discrete time.
94806	120840	A	0.7662026286125183	And I think the textbook tries to give equal time and highlight the hybridization of these models in a way where realistically in the literature most are pure discrete models and continuous time models have been almost like domain bound.
122700	139512	A	0.866240382194519	Certain areas like motor reflexes, where there's more of a tradition of continuous models are heavily represented in them, whereas decision making models tend to use the discrete formalisms.
139656	161330	A	0.9190565943717957	And that difference between the sort of like continuous time traditionally motor associated and discrete categorical traditionally decision oriented models was addressed in the Folk Psychology paper and Live Stream with Ryan Smith et al.
161720	174696	A	0.8157407641410828	Okay, also I just started kind of an overview, got up through 83 of just understanding and just trying to lay out chapter eight.
174718	196988	A	0.6526016592979431	So for any chapter, everyone's always welcome to just make summaries and add their own notes so we can look at the questions, many of which are on the earlier part of the chapter.
197084	204930	A	0.678142249584198	And so maybe we can look at the earlier part of the chapter more this week and of course then have later time.
207800	220580	A	0.7199406027793884	Okay, so 8.1, it's a single paragraph, it's a complement to chapter seven, which was a discrete time approach towards generative model construction.
220740	230920	A	0.7312948107719421	The focus here is on continuous states based models which are well suited for physical fluctuations, sensory receptors and continuous motion of effectors.
233940	238450	A	0.8757681846618652	They start with the case of movement control 8.2.
239140	243600	A	0.8558200597763062	So let's get familiar with the notation.
244580	249430	A	0.7321135997772217	The states, hidden states are going to be X.
250040	253670	A	0.8206140995025635	The data, the observables are going to be Y.
254120	259208	A	0.8553551435470581	And then how states evolve over time depends on a static variable V.
259374	264680	A	0.7188571095466614	So that could be a hidden or slower changing cause or factor in the environment.
265660	272570	A	0.8920614719390869	Then we have the omega terms associated with each of the data and the hidden states change through time.
273040	278824	A	0.7368214726448059	And that variability is like the noise and this is like the signal or the flow.
278872	286560	A	0.9034085273742676	So it's kind of the length of an decomposition or a length of an approach towards dynamical systems modeling.
288900	296980	A	0.5028409361839294	One interesting thing to note is first that this is hugely forsaged in SPM.
298120	312730	A	0.8580014109611511	Then also that the data are a function of the states at that time, whereas the equation for the hidden states is a change in hidden states.
313740	321310	A	0.7505127191543579	X dot note that Action is absent from equation 8.1.
322880	327630	A	0.7506887316703796	This is because Action is part of the generative process, not the generative model.
329680	333250	A	0.8241952061653137	Thought that was kind of an interesting question.
334420	335536	A	0.7731212973594666	What does that mean?
335638	339410	A	0.6886193752288818	That Action is part of the generative process and not the generative model?
341060	351460	A	0.8335314393043518	Surely Action selection is part of the generative model, the cognitive process of policy inference.
354040	360090	A	0.7982696890830994	Active states are part of the particular states.
360540	362650	A	0.6065142154693604	Active states are blanket states.
365420	368090	A	0.8107792735099792	Particular states are the generative model.
371590	377700	A	0.8758388161659241	The generative process is referring to the external states, the hidden states.
379750	387430	A	0.9073496460914612	So is Action part of both the generative process and the generative model because it kind of sits in between intermediates?
388890	392950	A	0.7641885876655579	Or is this referring to the consequences?
396410	402570	A	0.8704140186309814	The generative model only deals with those variables directly influenced by states external to a Markov blanket.
404270	414960	A	0.8939862251281738	So is that saying that the generative model is kind of like the complement of the autonomous states, Action internal states?
415650	417630	A	0.7541261315345764	Or is this even more extreme?
418130	423326	A	0.8255170583724976	It's only dealing with the variables directly influenced by states external to a markup blanket.
423438	433410	A	0.6538195610046387	So then it must only deal with sense states, which is like a narrow perceptual control reading.
434490	448330	A	0.677567183971405	And in that sense, Action execution is outside of the generative model because the generative model is only dealing with observations.
455310	460122	A	0.8915622234344482	Does anyone have thoughts on how generative model, Action and generative process are being used?
460176	460890	A	0.6499149203300476	Eric.
463070	470670	B	0.8891261219978333	I wonder if you can consider your own actions part of what gets observed under that framework.
473730	497026	A	0.6365264654159546	As far as I would, the apologetics for the narrow view would be exactly that, which is like you have proprioceptors in your elbow, so you don't observe Action, but you are getting continuous updating of perceptions that are extremely influenced by Action.
497218	501350	A	0.7100839614868164	But the hidden state of the world is actually like the angle of your elbow.
502090	507994	A	0.5523194074630737	And so you're not directly observing the hidden state of the world nor how it changes.
508112	510422	A	0.7982478737831116	All you have is the proprioceptors.
510566	519390	A	0.6978492140769958	And of course, you engage in policies to change your appropriate receptors, bring them towards alignment, which is what we're going to be discussing in the chapter.
520370	522590	A	0.8125036954879761	Ali, I saw your hand raise.
525570	532820	B	0.7776362299919128	Yeah, I wanted to mention exactly the proprioception which you mentioned, so I lowered my hand.
534070	535700	A	0.8590751886367798	Okay, yeah, great.
537590	548578	B	0.8929924964904785	But then could you include the fact that you've got signals going to your muscles, your motor signals and motor neurons firing in there, making your muscles contract?
548674	559658	B	0.7840472459793091	Can that be part of your observation to say, well, okay, if this is happening, that these muscles are getting activated, then sure enough, the arm moves and appropriate section changes.
559744	578338	B	0.8163256645202637	So you can say, well, from the outside of that box, that's all part of the generative model, but not part of generating those muscle commands, but just seeing what the effect of them is.
578504	581170	B	0.8374906778335571	I wonder if that would be an acceptable viewpoint.
585620	586370	A	0.84200119972229	Cool.
586900	589532	A	0.8416522145271301	We'll return to this like where is Action?
589596	591056	A	0.7581243515014648	What is action.
591248	594436	A	0.6538594961166382	But I think it's just important to note while we're early here.
594618	602100	A	0.7558648586273193	Okay, so continuing a little bit on this part, I thought was also a little curious.
602940	603544	A	0.584351658821106	Okay.
603662	605352	A	0.863347053527832	So this is equation 8.1.
605406	607336	A	0.8656806945800781	We mentioned equation 82.
607438	611880	A	0.8599915504455566	So first question, italics versus bold GNF.
614060	616060	A	0.8294390439987183	Do you think that that matters?
620510	622570	A	0.5684624910354614	I don't think it matters.
626580	628160	A	0.693060576915741	Does anyone disagree?
628740	632500	A	0.8974407911300659	Do you think it's moving from one space to another space?
632570	636100	A	0.49404340982437134	Or do you think that's just a typographical error?
641080	644180	A	0.8136729001998901	I think it can be considered as an eradica.
646940	658024	B	0.775280773639679	One thing I did notice is that 8.3 and 8.4, the italics they call a generative model, and 8.4 they call a generative process without the italics.
658152	659660	B	0.8504908084869385	Could that be the distinction?
660880	663310	A	0.8070088624954224	Okay, 8.3 and four.
663840	664444	B	0.5491447448730469	Yeah.
664562	665230	A	0.584351658821106	Okay.
670140	672204	A	0.90410977602005	It's a very good question.
672322	685730	A	0.7781909108161926	Well, let's just say italics in 8.1 and three, bold in 8.2 and 8.4, unclear, if relevant difference.
691540	692290	A	0.584351658821106	Okay.
706910	710010	A	0.7817893028259277	If the meta model you say, frames it.
710160	714480	A	0.844868004322052	So if you consider the niche to be a metamodel, then sure.
723010	737010	A	0.7983163595199585	The difference between equation one and two, other than the bolding or not, is that V, the static variables have been replaced with U actions.
737830	748150	A	0.8626680970191956	So this takes us from, like, the sort of SPM where you'd be like, you have neuroimaging data Y as a function of underlying neural states x.
748300	762858	A	0.882967472076416	And then neural states changing through time are a function of neural states at that time and some conditional factor that you're testing for, like whether they were exposed to treatment one or treatment two.
763024	791460	A	0.7769331932067871	V so this is still a partially observable dynamical model, but then it gets moved very neatly into the control theoretic and the policy selectionist inference space by just saying, yeah, those slowly changing factors that influence how the hidden states change, we control those, or we have some ability to influence there.
792870	793620	A	0.584351658821106	Okay.
798310	798722	A	0.5491447448730469	Yeah.
798776	809306	A	0.7291887402534485	So this could be a cue towards the Bulls difference, which is, like, the functions GNF are not necessarily the same as those used to define the generative process.
809408	817606	A	0.8207350373268127	If they said not necessarily the same as those used to define the generative process, bold, G, and F, then it would be unambiguous.
817798	820090	A	0.6355233192443848	But it's a little ambiguous.
824690	829520	A	0.7862960696220398	We do not need to explicitly write down the dynamics of action in the generative model.
833560	834310	A	0.584351658821106	Okay.
836220	840090	A	0.9064198732376099	They emerge from the choices made for terms in equation 8.1.
846990	849660	A	0.6984812021255493	We start with a simple sort of generative model.
850450	861200	A	0.8867523670196533	So G, which is the function that's generating the data, is going to be x F.
863590	875140	A	0.8313366174697876	The function of how X changes through time is a function of X and V, the slower changing causes, and it's a difference between those two.
877110	880120	A	0.8478652834892273	The hidden state represents the expected value for the data.
880890	890970	A	0.8606032729148865	Or another way to say that would be equation three says that the expected value for the data are the hidden state, but those are the same, and it's a point attractor.
891790	899930	A	0.8844811916351318	Or in the Bayesian mechanics, we could say it's mode matching by attractor.
902450	903200	A	0.5491447448730469	Yeah.
907780	908240	A	0.46103888750076294	Yes.
908310	914470	A	0.9121639132499695	The the attractor here is the mode tracking that is being referred to here.
915000	922564	A	0.8836707472801208	When there's a difference between the hidden state and the hidden cause, V, then it moves towards it.
922602	926500	A	0.8331326842308044	So it's just moving towards a single fixed point attractor.
927580	937690	A	0.6645771861076355	Now the generative process is defined, so it looks almost identical, except V's are replaced with use.
938560	949260	A	0.6510363817214966	And this differential tracker, this kind of PID control motif, is replaced with action.
950660	962160	A	0.9007852673530579	It's an expression of the equilibrium point hypothesis, which treats motor control as enacted by reflex arcs that draw limbs towards equilibrium points set by descending motor signals.
969400	974470	A	0.8960427641868591	And then also this is related to the skilled performance live stream 23.
975160	983930	A	0.8411995768547058	The scheme does not require specification of inverse models that are widely used in other formulations of motor control.
986620	999950	A	0.6797930598258972	8.5 makes a slightly more sophisticated model that's based around not just position, because, of course, you can't just pick up your finger and move it to a different position.
1000980	1008640	A	0.8751115798950195	Really, the action affordances are in the space of changes in velocity, which is to say, acceleration.
1009220	1027610	A	0.866809606552124	So this is kind of a classical mechanical perspective on motor control, and it also points towards generalized coordinates of motion and that kind of PID control and the Bayesian mechanics and so on.
1029100	1033290	A	0.7056971788406372	Okay, now we get to 8.3.
1036620	1040780	A	0.6204311847686768	Continuous time formulations are well suited to characterization of movements.
1042400	1048924	A	0.8580163717269897	Well, certain kinds of movements on a chessboard, maybe not, but some kinds of movements, sure.
1049042	1049820	A	0.7154882550239563	Eric?
1052100	1066450	B	0.8053767085075378	Yeah, I like to point out with equation 8.5 just above that where he's got the they've got this spring mass.
1067140	1075236	B	0.7640427947044373	So that system oscillates forever, right?
1075258	1080730	B	0.6334354877471924	It's got a spring and a mass, and there's no dissipation of energy.
1081980	1092664	B	0.5959830284118652	So it's a step, I guess, better than more advanced than equation 8.3, in which you have instantaneous acceleration.
1092712	1099724	B	0.8089786767959595	So they recognize, okay, you can have instantaneous acceleration, but there's no damper here.
1099762	1109870	B	0.8884366154670715	So normally the true equilibrium point control formulation is viewed as a spring mass damper system.
1110880	1136948	B	0.8372843861579895	And, um, that way, if you if you remember from physics class, you know, depending upon your setting, your your mass, the spring tension and the amount of damping you can get over damping or under damping, and then you can get kind of an optimal settling to the or approach of the set point with maybe just a little bit of overshoot or no overshoot.
1137044	1142510	B	0.607124388217926	But if you don't have a damper in there, then it never settles in at the equilibrium point.
1144880	1163708	B	0.5575315356254578	So just to carry this a bit further, I would have to disagree with this statement that continuous time formulations are well suited to characterization of movements, because that's assuming that the equilibrium point control model is a good characterization of movement.
1163884	1166372	B	0.8534678220748901	I worked on this in grad school for about two years.
1166426	1173572	B	0.8840972185134888	When I first got to grad school, we worked on the Ilium point control model.
1173706	1176020	B	0.8855737447738647	I worked in the BT lab at MIT.
1177000	1182516	B	0.7826294898986816	We had an apparatus that mimicked an apparatus that was used in monkeys.
1182628	1199212	B	0.8332438468933105	We had one that we used on ourselves as human subjects, which would allow you to move your arm, and then it would kind of give you a little jolt, a little disruption to measure the amount of stiffness in.
1199266	1213208	B	0.9122081995010376	Your movement as you're moving your arm as a way of trying to deconstruct or work back the parameters of the equilibrium point control model for movement.
1213324	1228668	B	0.7145119309425354	And it's attractive because, as they say in the chapter, well, you don't have to have any very sophisticated knowledge of motor planning or inverse model of your plant.
1228704	1233576	B	0.7913868427276611	The system, in order to all you do is you say, well, this is where I want the arm to go.
1233678	1244604	B	0.8431748747825623	And passively, it takes care of itself, perhaps through a feedback loop, through the spinal cord, which I guess is what, 8.1 has.
1244802	1264112	B	0.45909932255744934	It's interesting that there's a paper from 2000 and 911 and Feldman that is reviewing this, but our conclusion was pretty much that equilibrium point control for motor control, it really is not a good model.
1264166	1265170	B	0.879893958568573	It doesn't work.
1267220	1284104	B	0.6356889605522156	In order to make it work, you have to have all kinds of play, all kinds of shenanigans, like, well, you not just have a fixed set point, you might overshoot and then you bring your set point back and you actually have to control the stiffness during movement in order to account for observed data.
1284302	1295736	B	0.7810849547386169	But still it falls short because many, if not most human movements or any motor movements are ballistic.
1295928	1311756	B	0.8406936526298523	So it's not that you set some conditions and then your spinal cord and motors and muscles act to fulfill those conditions that you set once from the top, from your brain.
1311948	1326900	B	0.7231866717338562	What really happens is your descending signals anticipate a whole trajectory or a whole plan in advance of how we're going to accelerate and then we're going to have to decelerate.
1329000	1333944	B	0.8423739075660706	And so the motor planning kind of knows what your plant is going to do.
1333982	1341160	B	0.7126708626747131	It doesn't wait for the plant to do something and then have to respond to that in kind of a passive way at the lower levels.
1341580	1347310	B	0.6495448350906372	So it's just like if you play the piano, you're not waiting for your finger to say, am I far enough down on the key?
1348640	1354290	B	0.6737282872200012	You learn how hard you have to press it and when you start to release before anything happens.
1355380	1357330	B	0.6652557253837585	Our movements are very fast.
1359540	1378820	B	0.5154444575309753	There's really no getting around having to model a plant and doing this sort of inverse modeling that they're concerned about in order to account for how any creature really moves or anything above an insect, and probably an insect too, is undoubtedly it's a motor plant.
1381420	1392540	B	0.856122612953186	I really find that this chapter, I don't think it advances a cause of active inference.
1396160	1403616	B	0.8483501672744751	It doesn't really fit with how motor control really has to work and doesn't add anything to the theory of it.
1403638	1406050	B	0.8740421533584595	I mean, it's using an old theory that doesn't work.
1408100	1410370	A	0.9160861372947693	Thanks for all those points.
1411380	1412720	B	0.743351936340332	That's my statement.
1413960	1414740	A	0.532121479511261	That's it.
1414810	1416100	A	0.6400642991065979	That's the tweet.
1417960	1425900	A	0.9156490564346313	Well, one live stream other than 49, which has just been like, amazing from the physics angle.
1426000	1433850	A	0.8860788941383362	We're preparing also for 50, which is using it's also by Feldman, Barrett and others.
1434700	1438336	A	0.8498663902282715	And it's specifically on interoceptive reflexes.
1438388	1445660	A	0.612732470035553	So less exploring like the spinal reflex arc and more on the vasopressive interception.
1446240	1478152	A	0.6693201065063477	But using those kinds of still classical, very differential, equation based equilibria control and showing that those types of formalisms can recapitulate like allostatic behavior, it's always difficult to know.
1478206	1482730	A	0.4829769730567932	Like, is that a very narrow and fragile range of parameter space?
1483500	1487480	A	0.8718324303627014	Is this a descriptive model that's making unique predictions?
1489340	1490836	A	0.8920634388923645	Let's look at figure eight.
1490878	1505810	A	0.8713029623031616	One, because it's speaking to the generative model, generative process distinction that was mentioned earlier, and it was addressed in a question.
1506180	1514628	A	0.9111378192901611	How is this representation of the reflux arc similar or different to what was being used in figure five or in chapter five?
1514714	1522352	A	0.9113679528236389	So in chapter five, here was the summary statistic or summary graphical abstract.
1522496	1528600	A	0.8065744042396545	There was a lot more focus on this kind of like policy selection and so on.
1528750	1539260	A	0.8721473217010498	But we we did have this kind of a reflex arc, y, E and so on.
1539410	1561660	A	0.8863449692726135	Okay, so now we're in 8.1 f is the action selection component.
1563360	1584550	A	0.876876711845398	So here is that simple attractor being shown where it just X is going to be converging to V, and this is the change in external states.
1606100	1612400	A	0.8447304964065552	So here we have Mu of X in an equation.
1613140	1620660	A	0.7172359228134155	In the lower one, is this one referring to this or is that kind of bleeding out from the bottom panel?
1622600	1625350	A	0.8839183449745178	Because here's U of X, and here's Mu of X.
1630460	1636600	A	0.8501322865486145	Here it looks like Mu of X is a prediction modulator on a differential.
1640580	1648180	A	0.5606561303138733	So it's like if the observation is where you want to be, then there shouldn't be any action.
1648600	1656840	A	0.8106837272644043	But as the observation diverges from where you want to be, you have a precision modulated action.
1663890	1685910	A	0.84140944480896	Is that active inference or is that just a notation, a different notational representation of classical equilibrium point motor control theory?
1688650	1694680	A	0.8983991742134094	Do those models integrate sensory and action together?
1696270	1698780	A	0.6616528630256653	I mean, in one sense they definitely do.
1700030	1707370	A	0.6951860189437866	Nobody was ever generating reflex arc models without some sensory feedback variable.
1709230	1711660	A	0.8004037737846375	So that's an open important question.
1712370	1734280	A	0.7566823959350586	Is this didactic with respect to active inference now just kind of saying, yeah, one special case of active inference is the traditionally understood and limited equilibrium point model of motor control, or is it like, this is our new way of doing motor point control?
1739850	1746620	A	0.8099807500839233	Okay, just kind of continuing on.
1751620	1752240	A	0.4896697998046875	All right.
1752310	1756080	A	0.6677045226097107	Precision, attention and sensory attenuation.
1759080	1762784	A	0.8114116191864014	So here's another curious note.
1762832	1770740	A	0.8616443872451782	So earlier we discussed whether motor control was or wasn't a natural setting for continuous models.
1771160	1778404	A	0.7731409668922424	In the box 81, they write, we address this the importance of precision in chapter seven, but it's worth recapping its role in continuous time systems.
1778532	1789420	A	0.5486879944801331	In many ways, this concept of precision is more naturally addressed in this setting as the Pi variable appears a direct consequence of LaPlace approximation.
1790560	1797010	A	0.8234804272651672	Okay, but can't you do a LaPlace approximation in discrete systems as well?
1797380	1822440	A	0.8585929870605469	LaPlace approximation is just mode determination followed by the parameterization of a parabola to create like a gaussian or at least a concave optimizable and two parametered approximation to an arbitrary distribution.
1822780	1836590	A	0.7920352220535278	I don't exactly see or is it referring to the LaPlace approximation is itself a continuous function, but you could apply a LaPlace approximation to a categorical distribution as well.
1841120	1847550	A	0.8996027708053589	So in what ways is precision more naturally addressed in a continuous model?
1850900	1854000	A	0.8540157079696655	Is it that precision is a continuous variable?
1854580	1868710	A	0.7469450831413269	Like precision isn't an integer valued variable so we can just leave that one.
1870440	1889790	A	0.8388113975524902	Okay, so we went into 83 dynamical systems, kind of looked at the spinal cord again, but it may or may not just be a sort of like trivialized or didactic representation of extremely traditional representations of motor control.
1891360	1896188	A	0.8013519048690796	Then box one, precision attenuation attention, sensory attenuation all.
1896194	1899980	A	0.880668580532074	Right, now we get into the lockvoltera dynamics.
1901060	1910880	A	0.9142408967018127	So what are some formal and qualitative implications of the use of lock of voltera dynamics and Lorenz systems?
1912500	1921796	A	0.9171728491783142	So I think there's a few interesting angles that us here now but others just more broadly may be interested in.
1921978	1929076	A	0.8854518532752991	So one is lock of voltera ties in of course with ecology and population ecologies.
1929268	1936980	A	0.8858968019485474	And so that is also very interesting to consider in terms of winnerless competitions and neural dynamics.
1937140	1956720	A	0.7932403087615967	And that is basically the analogy which is like just like even if you have a negative edge between the predator and the prey for any persistent which is to say oscillatory or manifold attracting population, at some point you have to let the prey regrow.
1957460	1973220	A	0.6850165128707886	So the analogy in neuroscience that's fleshed out in SPM is like if you have a brain region even with a unilaterally negative influence on another region, at some point it has to let off the brake cyclically.
1973560	1985812	A	0.853692352771759	So lock of volteris summarizes these kinds of winnerless dynamics that can give oscillation and time structure including with complex dynamics and recurrent dynamics, limit attractors.
1985876	1996030	A	0.510454535484314	All these kinds of dynamical processes can arise from a pretty simple starting point.
1996960	2019504	A	0.7597798109054565	And here we see manifolds and limits on the projection because it's a three dimensional space plant, herbivore carnivore or that could be the blood oxygenation of three brain regions in SPM and this is their neural trace or their inferred activities.
2019552	2023140	A	0.8459950685501099	And so then you can find like manifolds of lower dimensional activity.
2023480	2053890	A	0.9249259829521179	And then also one kind of related note is the Voltaire series and this is a pretty nice just technical note which is Taylor series does expansion of arbitrary functions from a starting point and the voltera series is able to include like a window of inputs also known as the memory effect.
2054820	2059564	A	0.8852710723876953	And so it's discussed in SPM.
2059612	2073220	A	0.8584593534469604	It has a continuous time and a discrete time variant and it is just a kind of kernel estimator that can be used for dynamical processes, input output dynamical processes with memory.
2075400	2077192	A	0.757519543170929	So it's kind of a classic in that area.
2077246	2092044	A	0.8778986930847168	And I think even in some live stream, maybe 45, I asked Friston about the Lockable Terra models and about why do we move away from them or something like that.
2092082	2095230	A	0.6859030723571777	But that'd be a good answer to kind of come back to.
2096480	2109360	A	0.6096044182777405	All right, so now those kinds of lockable terror dynamics are going to be applied, monochromatically as ever, to active inference.
2109940	2113820	A	0.6099716424942017	And that was one question, which is it didn't finish writing.
2113900	2119460	A	0.8398885726928711	How is the Lockable Terra model being used in the case of handwriting?
2125980	2134680	A	0.7972359657287598	So I think it's in the Friston and Herrero's paper, which I just brought in, but I didn't really investigate deeply.
2140610	2145214	A	0.7721776962280273	The left, there's two different applications here.
2145412	2148750	A	0.7085601091384888	Left eye blink, conditioning.
2152290	2163970	A	0.6508616209030151	So here it looks like a condition stimulus, unconditioned stimulus, where there's like where something happens and you blink.
2167860	2180820	A	0.681563675403595	As we've mentioned previously, it's a little bit difficult to tell exactly what these expectations are of or anything like that, but this is like some kind of stimulus response.
2181900	2186840	A	0.8290995359420776	Blink once if no, twice if yes, once if by land, twice if by sea.
2187260	2198408	A	0.9146884679794312	And then on the right side, sequential peaks using an attracting point, but selecting the specific attractor on the basis of which population a lockable terrace system is currently highest.
2198584	2202696	A	0.8945134878158569	This leads to sequential visiting of each points, giving rise to a type of handwriting.
2202808	2212160	A	0.9422739744186401	So this is kind of interesting, and I know for Yaakob and Ali also, with our mode tracking path matching discussion, Bayesian physics.
2213620	2222608	A	0.7958664894104004	So it's like I I must it's Bart Simpson at the chalkboard.
2222784	2224624	A	0.6636971831321716	This is I must rewrite the sentence.
2224672	2225744	A	0.6704207062721252	I must rewrite the sentence.
2225792	2226180	A	0.5680400133132935	I must.
2226250	2242840	A	0.8144027590751648	So your expectations are that you're going to be writing this sentence and then depending on which of those expectations are on top, that is going to set the motor imperative.
2243340	2246312	A	0.8407836556434631	So it'd be like if you were writing the letter D.
2246446	2253772	A	0.8668425679206848	And it'd be like well, it's like first there's like an arc down into the right, and then there's an arc down into the left, and then it's a straight line up.
2253826	2257632	A	0.8644863963127136	And then it's an arc down to the right, and then it's an arc down to left, and then it's a straight line up.
2257766	2273668	A	0.7231408953666687	And so here, it's kind of like doing this writing as it's picking a mode, pursuing the mode, and I think it's fair to say, Yakavarli, please add any more if you feel like it.
2273674	2285648	A	0.7223384976387024	But like, this is like shaking off the slumber of the classical physics and still having 1ft in both worlds.
2285824	2292204	A	0.6065244078636169	And the three faces of Bayesian mechanics are a much more advanced way to discuss us.
2292242	2309660	A	0.8261125087738037	This issued what do you think?
2310990	2337540	C	0.7640374898910522	I was just going to say that this well, not the case with writing specifically, but the case of motor motor control was discussed in Thomas Parr's lecture at the CPC 2022, which I don't know if it was recorded, actually, I think it was, but I don't know if it's public yet.
2338630	2356034	C	0.8682501912117004	But it's interesting that this example is in the continuous time chapter because you could think of it as a nested generative model with both discrete and continuous factors.
2356162	2365740	C	0.8553547859191895	Like if you partition it into three decisions, like first straight line up, then an arc down, then a second arc down.
2366510	2379200	C	0.907447874546051	Those are three discrete decisions that are at the top of the hierarchical generative model with the finer grain motor control below.
2384790	2385634	A	0.6283750534057617	Thanks.
2385832	2393090	A	0.866007924079895	Also just here's, that classic three faces figure mode tracking.
2393430	2407110	A	0.8948850631713867	This can be seen as using a lock of Ultera pseudo random number generator to spit out higher level predictions about which mode should be pursued.
2407530	2425742	A	0.8964207172393799	Then as to the dynamics of how that mode is approached, one can either use the equilibrium point methods discussed in this chapter or the more advanced Bayesian physics methods to describe that in terms of gauge theory and so on.
2425796	2427120	A	0.7482144832611084	Is that fair to say?
2440210	2441226	A	0.7739699482917786	This it Jakub.
2441258	2442960	A	0.9601938724517822	Thank you, Ali, for sending it.
2445250	2446014	C	0.8130888342857361	It's not the same.
2446052	2446946	C	0.5395411849021912	It's not the same one.
2446968	2452358	C	0.6646059155464172	But it looks like a lot of the slides are the same.
2452524	2459430	C	0.7713155150413513	I think the thing that I was referring to I'm just trying to find the slide.
2462270	2462842	C	0.5585339665412903	Oh, yeah.
2462896	2466026	C	0.884228527545929	I think it might be minute 41.
2466208	2466940	A	0.584351658821106	Okay.
2480130	2492610	A	0.8058028221130371	I really can't wait till there's some open source webcam software to do isocate analysis and pupil diameter.
2493190	2497598	A	0.5318456888198853	I don't think it would be that intractable.
2497774	2501400	A	0.8731754422187805	Maybe you'd have to be kind of close, or you'd have to use a 4K camera or something.
2502730	2504790	A	0.8491820096969604	But is there any software like that available?
2504860	2515930	A	0.586746096611023	I've never come across it just like move the mouse to where I'm looking and then click when I blink twice.
2518350	2522922	A	0.7798254489898682	Isn't that kind of like that does exist, but what is it called?
2522976	2524300	A	0.7914099097251892	Or where is it being used?
2527090	2535440	C	0.617303192615509	I know it's being used for people with physical impairments who can't control the computer.
2537590	2538146	A	0.5491447448730469	Yeah.
2538248	2540020	C	0.6446734666824341	Otherwise, yes.
2542310	2551074	C	0.7949121594429016	I haven't seen an analysis of pupil dilation, but eye tracking is definitely a thing.
2551192	2553460	A	0.6005154848098755	Okay, maybe we can explore it.
2554410	2557430	A	0.7150282263755798	All right, so, yeah, the blinking example was a puff of air.
2557580	2560566	A	0.6999111175537109	So it was just like a beep and then a puff of air.
2560748	2565770	A	0.8094900846481323	And so then they learned to preempt the air puff.
2568190	2568940	A	0.584351658821106	Okay.
2569710	2581834	A	0.8694024682044983	Learning incontinence models eight two continuous time domain accumulating evidence.
2581882	2585470	A	0.645060658454895	This works if we treat data in a series of small time intervals.
2586370	2589010	A	0.4888061285018921	Oh, no, don't discretize.
2592630	2602950	A	0.8882931470870972	This relates, I think, to some of the finer points on which Stochastic calculus is used, like the Edo calculus in the Bayesian mechanics.
2606090	2614070	A	0.6428552865982056	We won't go into it unless anybody really wants to say more, but this looks like important just to learn about the integrals and the gradients.
2617370	2625130	A	0.8211606740951538	P OMDP has largely superseded the use of the generalized Lotka volterra systems in active inference applications.
2626270	2631790	A	0.889717698097229	That was kind of, I think, related to asking Friston about why things moved away from the lockable Terra formulations.
2636370	2643300	A	0.9085102677345276	Lorenz system, also explored in Stochastic Chaos Markov blankets Livestream 32.
2647910	2649138	A	0.6753978133201599	It's it's almost.
2649224	2655894	A	0.7642425298690796	It's dealt with as minimally as you could possibly deal with the Lorenz system in 87.
2656012	2656726	A	0.584351658821106	Okay.
2656908	2659750	A	0.7993863224983215	84 generalized synchrony.
2660330	2665282	A	0.8712507486343384	So multi agent inference starts coming into play as well as agent environment synchronization.
2665346	2669100	A	0.5823478698730469	And we actually like, talked a little bit about the birdsong in the previous hour.
2670270	2683150	A	0.8740496039390564	Now the Run system comes back and yes, it's dealt with like sort of in the 2015 Frith and Friston paper, active inference communication and hermeneutics.
2683810	2687754	A	0.8363285660743713	And that's where the synchronization manifold is kind of displayed.
2687802	2694580	A	0.8903507590293884	And then more recently in live stream 32.
2696790	2698114	A	0.7746767401695251	And this one has a dot three.
2698152	2700980	A	0.8981682658195496	I think this might be the only dot three as of this point.
2702810	2706070	A	0.842887282371521	I think Connor Hines could only come at a later date.
2706140	2715450	A	0.8826630711555481	Yeah, so these are good discussions on Lorenz attractor a couple Lorenz systems.
2718270	2728640	A	0.5564275979995728	This is kind of very similar, if not identical to the birdsong synchronization manifold in first and fifth 2015.
2729730	2731470	A	0.6708680391311646	Birds, birds, birds.
2734210	2756840	A	0.6327728629112244	Bird synchronization isn't real then hybrid discrete models, hybrid mixed models, not that kind of mixed model allow inferences about sequential action plans and translations of those decisions into movements through a continuous model.
2757610	2764366	A	0.7619360685348511	So in that respect, it's extremely linked to Livestream.
2764418	2793090	A	0.802466869354248	46 active inference models do not contradict folk psychology, which is all about that continuous time motor, active inference, discrete state space decision active inference, mai and dai and using those motor and decision active inference models, while lower level model in the dashed box is the same form as the other models.
2796390	2818380	A	0.8586835265159607	In other words, it's continuous because we have V instead of Pi for the causal factors and F instead of B and G instead of A and ETA instead of G.
2821510	2841040	A	0.8173502683639526	Is it some mapping between well, in this case, I think just looking at it a little more narrowly, how outcomes map to slower causes of the world.
2842770	2846898	A	0.883874237537384	So here this is a discrete model up top with the G.
2846984	2849140	A	0.49513769149780273	Oh, yeah, that was one erotic that we found.
2849510	2851060	A	0.7413365840911865	Free floating G.
2852310	2856386	A	0.5846930742263794	Can't live with it, can't live without it, but it should be connected to the pi.
2856418	2875170	A	0.868479311466217	Of course there's a discrete time model on top and then it is kind of cascading its outcomes down to these continuous time models.
2876550	2884950	A	0.8707866072654724	So one could think about this like the Lock of Ultera handwriting example kind of could it's not the exact structure of this, but it's related.
2886330	2907920	A	0.8450137972831726	Okay, this is something I know Jakub has thought about how discrete and continuous models interface because we've talked about how the temperature could be continuous and the thermometer could be continuous, but then the internal generative model could be like, is it hot or not?
2908610	2925060	A	0.8992059826850891	So the question of how to move from generative models within and between different parts of the model, where are what kinds of transitions between continuous and discrete state spaces possible?
2928150	2944310	A	0.608203113079071	Okay, here's a continuous isocate ocular motor paradigm where the continuous lines are still confusing, but at least they are justified.
2948430	2953690	A	0.8376384377479553	83 combining categorical and continuous generative models.
2954110	2958110	A	0.8633283972740173	Outside of active inference has primarily been framed through the lens of clustering.
2959730	2963310	A	0.8726215958595276	The aim is to assign data points into a discrete cluster.
2966530	2968702	A	0.6679520606994629	I wasn't exactly sure about that.
2968756	2974980	A	0.842268168926239	I think clustering is one discretization clustering and thresholding is one discretization approach that's been used.
2975750	2980370	A	0.6666939854621887	Also, just binning seems like a simpler method.
2982710	2987110	A	0.8659129738807678	How do you take a continuous distribution of one through ten and make it categorical?
2987530	2989302	A	0.7324764132499695	Just bin it one through ten.
2989436	2991218	A	0.8570558428764343	Or you could cluster and do thresholds.
2991234	3003340	A	0.9227073788642883	So then you could have cluster one with 9.7 to 9.9 and then 9.7 to 9.5 and then 86.
3003710	3007738	A	0.6277503967285156	There's a big table here's.
3007754	3025940	A	0.7997249960899353	The songbirds this author Isamura, I believe, is the one who will join for 51 in November, and I think that'll be like our last probably one of our last paper streams for the year.
3028150	3037190	A	0.9272687435150146	We have just this last discussion with Dalton next week and then interception, and then the canonical neural networks perform active inference papers.
3038010	3040646	A	0.8964188694953918	But Isomer has worked with Par and Frisk and others.
3040828	3050010	A	0.7920016050338745	Ocular, motor reflex, eye pursuit, psychosis illusion, circade action, observation, attention, hybrid, and self organization models.
3055710	3076210	A	0.4453843832015991	Well, with four minutes left, kind of an interesting chapter, but I can only know it how I'm seeing it, and I have serious questions about what it does in the book.
3078020	3078880	A	0.6499149203300476	Eric.
3081780	3105416	B	0.6491429209709167	One thing I was puzzled by, maybe you guys can help me out with the bird song example is, were they implying that bird starts singing, another bird picks it up synchronizes using these dynamics, and the proof of that is that some other bird will just step in and take over and the first bird can stop.
3105598	3107912	B	0.5636112093925476	I don't believe I've ever heard that happening.
3108046	3111070	B	0.7127037644386292	Are they suggesting that that happens or what?
3115520	3117550	A	0.6685819625854492	Okay, I agree.
3119680	3126780	A	0.8138803839683533	Whether this is a bird improv conversation, in this case, they're singing from the same hymn sheets.
3127220	3147110	A	0.6123676300048828	So it's like somebody should be singing the bird anthem, and now we're going to be engaging in this sort of quasi dialogue where we're going to be reducing our surprise about hearing the song that we expect to hear, which goes like, insert your favorite song there.
3149500	3179730	A	0.7064968943595886	And then the phenomena that gets highlighted is the way in which one bird is singing, and then as it's singing, its precision starts to drift and then it stops singing, and then the other bird finds itself picking up at the same point to reduce its surprise about that song being sung at the time.
3181460	3206360	A	0.5595921874046326	But I think especially with with a bold title like, laying claim to hermeneutics more broadly, singing from the same pre coordinated hymn sheet is not the same thing as information updating or even just pure expressivity.
3210320	3222976	A	0.5289638042449951	So I think it leaves gaping areas for people to actually develop because it's like, well, we have what don't we have?
3222998	3228480	A	0.8254081606864929	Databases of like, millions of bird songs and natural soundscapes.
3230020	3245456	A	0.5609480738639832	So, like, we need some bird watching, active inference practitioners who want to actually develop the bird song motif because otherwise it is too easy and ironically become singing from the same hymn sheet.
3245568	3248170	A	0.7871100306510925	Oh, well, Hermeneutics has been addressed in 2015.
3249340	3251530	A	0.7933506369590759	Synchronization manifold 2015.
3253900	3274720	A	0.691001832485199	Whereas this is like a very fringe case of communication that is based upon most charitably, high reliability execution of preordained performance, which is a tiny edge case of communication.
3276740	3293056	B	0.6553203463554382	My understanding of bird songs is baby birds hear what their parents sing, they've got some predisposition to sing a certain kind of song, and for most birds, there are some that are very adaptable.
3293168	3295284	B	0.5220227241516113	Parents, parrots, stuff like that.
3295322	3305210	B	0.8268770575523376	But most birds learn the accent or the dialect that they are raised in, and that's the song that they always sing.
3306780	3308824	B	0.7740469574928284	And there's some variation on it.
3308862	3310812	B	0.7909576892852783	I mean, maybe they'll stop midway through.
3310946	3324012	B	0.8372727632522583	But what are the true facts about what I thought to be a claim here, where birds will pick up from each other or one leaves off?
3324146	3325470	B	0.622674286365509	Does that really happen?
3326080	3327890	B	0.6847118735313416	I'm not familiar with that happening.
3328260	3330176	B	0.4895702004432678	I don't think I've ever heard it happening.
3330358	3341252	B	0.7819718718528748	My impression of the way moosebirds sing is they'll sing their song all the way through and there might be a call in response, another one might pick up, might answer.
3341386	3350376	B	0.6666025519371033	But I don't think I have error here, synchronizing of songs, but I might be just not well enough informed about the ecology of those things.
3350558	3355290	A	0.7958540916442871	Yeah, very interesting questions.
3357580	3372296	A	0.6770439147949219	And the whole area with game theory of song, like, it's beneficial that somebody warns about the predator, but then you've revealed your location when you sing and things like that.
3372418	3386756	A	0.8144964575767517	That's the kind of decision making and uncertainty resolution and risk evaluation that I'd imagine an agent based birdsong model would orient towards.
3386938	3389668	A	0.8983680605888367	But those are some cool ways for us to go.
3389834	3402040	A	0.6699701547622681	So next week, I guess this is kind of a shorter chapter and it has some arcane symbolism, but also seemingly very skippable symbolism.
3402860	3419260	A	0.9010014533996582	So I'm glad that we could get through the whole thing and we'll return to any further thoughts on eight and then maybe we can have more of a seven plus eight synthesis.
3421040	3442420	A	0.7787380218505859	Or anyone can look into any of these citations key advances as we head into, like, chapter nine on empirical data, which is not super long, followed by ten, which is just a restatement.
3443160	3447364	A	0.6685687303543091	So onwards we go.
3447482	3448640	A	0.7386267781257629	Thanks, y'all.
3448800	3449620	A	0.5137446522712708	Bye.
3451080	3451650	C	0.8315965533256531	Thanks, everyone.

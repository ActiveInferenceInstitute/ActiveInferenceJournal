start	end	speaker	confidence	text
970	45080	A	0.9249508955223879	Hello everyone. It's october 7, 2022. We are in meeting 19 for cohort one and we're discussing chapter eight. So there's a bunch of questions and also a lot of things to talk about. Let us just begin with looking at chapter eight. Eight. So does anybody want to give any first thoughts on active inference in continuous time? Everything flows, nothing stands still, et cetera? Yes.
49970	63780	B	0.9434867647058823	Not on that specifically. Okay, but I will have something to say for this chapter. I worked on this topic during my graduate time. So once we get to about 8.3, I'll weigh in.
65510	460890	A	0.9267866057838674	Excellent. Any comments like discrete continuous time? I know this is something we'll touch on again and again. It's kind of like another low road, high road duality with continuous and discrete time. And I think the textbook tries to give equal time and highlight the hybridization of these models in a way where realistically in the literature most are pure discrete models and continuous time models have been almost like domain bound. Certain areas like motor reflexes, where there's more of a tradition of continuous models are heavily represented in them, whereas decision making models tend to use the discrete formalisms. And that difference between the sort of like continuous time traditionally motor associated and discrete categorical traditionally decision oriented models was addressed in the Folk Psychology paper and Live Stream with Ryan Smith et al. Okay, also I just started kind of an overview, got up through 83 of just understanding and just trying to lay out chapter eight. So for any chapter, everyone's always welcome to just make summaries and add their own notes so we can look at the questions, many of which are on the earlier part of the chapter. And so maybe we can look at the earlier part of the chapter more this week and of course then have later time. Okay, so 8.1, it's a single paragraph, it's a complement to chapter seven, which was a discrete time approach towards generative model construction. The focus here is on continuous states based models which are well suited for physical fluctuations, sensory receptors and continuous motion of effectors. They start with the case of movement control 8.2. So let's get familiar with the notation. The states, hidden states are going to be X. The data, the observables are going to be Y. And then how states evolve over time depends on a static variable V. So that could be a hidden or slower changing cause or factor in the environment. Then we have the omega terms associated with each of the data and the hidden states change through time. And that variability is like the noise and this is like the signal or the flow. So it's kind of the length of an decomposition or a length of an approach towards dynamical systems modeling. One interesting thing to note is first that this is hugely forsaged in SPM. Then also that the data are a function of the states at that time, whereas the equation for the hidden states is a change in hidden states. X dot note that Action is absent from equation 8.1. This is because Action is part of the generative process, not the generative model. Thought that was kind of an interesting question. What does that mean? That Action is part of the generative process and not the generative model? Surely Action selection is part of the generative model, the cognitive process of policy inference. Active states are part of the particular states. Active states are blanket states. Particular states are the generative model. The generative process is referring to the external states, the hidden states. So is Action part of both the generative process and the generative model because it kind of sits in between intermediates? Or is this referring to the consequences? The generative model only deals with those variables directly influenced by states external to a Markov blanket. So is that saying that the generative model is kind of like the complement of the autonomous states, Action internal states? Or is this even more extreme? It's only dealing with the variables directly influenced by states external to a markup blanket. So then it must only deal with sense states, which is like a narrow perceptual control reading. And in that sense, Action execution is outside of the generative model because the generative model is only dealing with observations. Does anyone have thoughts on how generative model, Action and generative process are being used? Eric.
463070	470670	B	0.9590788235294118	I wonder if you can consider your own actions part of what gets observed under that framework.
473730	522590	A	0.9371755752212392	As far as I would, the apologetics for the narrow view would be exactly that, which is like you have proprioceptors in your elbow, so you don't observe Action, but you are getting continuous updating of perceptions that are extremely influenced by Action. But the hidden state of the world is actually like the angle of your elbow. And so you're not directly observing the hidden state of the world nor how it changes. All you have is the proprioceptors. And of course, you engage in policies to change your appropriate receptors, bring them towards alignment, which is what we're going to be discussing in the chapter. Ali, I saw your hand raise.
525570	532820	B	0.9133187500000001	Yeah, I wanted to mention exactly the proprioception which you mentioned, so I lowered my hand.
534070	535700	A	0.9488366666666668	Okay, yeah, great.
537590	581170	B	0.9271866990291262	But then could you include the fact that you've got signals going to your muscles, your motor signals and motor neurons firing in there, making your muscles contract? Can that be part of your observation to say, well, okay, if this is happening, that these muscles are getting activated, then sure enough, the arm moves and appropriate section changes. So you can say, well, from the outside of that box, that's all part of the generative model, but not part of generating those muscle commands, but just seeing what the effect of them is. I wonder if that would be an acceptable viewpoint.
585620	644180	A	0.9222023999999998	Cool. We'll return to this like where is Action? What is action. But I think it's just important to note while we're early here. Okay, so continuing a little bit on this part, I thought was also a little curious. Okay. So this is equation 8.1. We mentioned equation 82. So first question, italics versus bold GNF. Do you think that that matters? I don't think it matters. Does anyone disagree? Do you think it's moving from one space to another space? Or do you think that's just a typographical error? I think it can be considered as an eradica.
646940	659660	B	0.9605396875	One thing I did notice is that 8.3 and 8.4, the italics they call a generative model, and 8.4 they call a generative process without the italics. Could that be the distinction?
660880	663310	A	0.93465	Okay, 8.3 and four.
663840	664444	B	0.96796	Yeah.
664562	1049820	A	0.9213650796178354	Okay. It's a very good question. Well, let's just say italics in 8.1 and three, bold in 8.2 and 8.4, unclear, if relevant difference. Okay. If the meta model you say, frames it. So if you consider the niche to be a metamodel, then sure. The difference between equation one and two, other than the bolding or not, is that V, the static variables have been replaced with U actions. So this takes us from, like, the sort of SPM where you'd be like, you have neuroimaging data Y as a function of underlying neural states x. And then neural states changing through time are a function of neural states at that time and some conditional factor that you're testing for, like whether they were exposed to treatment one or treatment two. V so this is still a partially observable dynamical model, but then it gets moved very neatly into the control theoretic and the policy selectionist inference space by just saying, yeah, those slowly changing factors that influence how the hidden states change, we control those, or we have some ability to influence there. Okay. Yeah. So this could be a cue towards the Bulls difference, which is, like, the functions GNF are not necessarily the same as those used to define the generative process. If they said not necessarily the same as those used to define the generative process, bold, G, and F, then it would be unambiguous. But it's a little ambiguous. We do not need to explicitly write down the dynamics of action in the generative model. Okay. They emerge from the choices made for terms in equation 8.1. We start with a simple sort of generative model. So G, which is the function that's generating the data, is going to be x F. The function of how X changes through time is a function of X and V, the slower changing causes, and it's a difference between those two. The hidden state represents the expected value for the data. Or another way to say that would be equation three says that the expected value for the data are the hidden state, but those are the same, and it's a point attractor. Or in the Bayesian mechanics, we could say it's mode matching by attractor. Yeah. Yes. The the attractor here is the mode tracking that is being referred to here. When there's a difference between the hidden state and the hidden cause, V, then it moves towards it. So it's just moving towards a single fixed point attractor. Now the generative process is defined, so it looks almost identical, except V's are replaced with use. And this differential tracker, this kind of PID control motif, is replaced with action. It's an expression of the equilibrium point hypothesis, which treats motor control as enacted by reflex arcs that draw limbs towards equilibrium points set by descending motor signals. And then also this is related to the skilled performance live stream 23. The scheme does not require specification of inverse models that are widely used in other formulations of motor control. 8.5 makes a slightly more sophisticated model that's based around not just position, because, of course, you can't just pick up your finger and move it to a different position. Really, the action affordances are in the space of changes in velocity, which is to say, acceleration. So this is kind of a classical mechanical perspective on motor control, and it also points towards generalized coordinates of motion and that kind of PID control and the Bayesian mechanics and so on. Okay, now we get to 8.3. Continuous time formulations are well suited to characterization of movements. Well, certain kinds of movements on a chessboard, maybe not, but some kinds of movements, sure. Eric?
1052100	1406050	B	0.9344428383705661	Yeah, I like to point out with equation 8.5 just above that where he's got the they've got this spring mass. So that system oscillates forever, right? It's got a spring and a mass, and there's no dissipation of energy. So it's a step, I guess, better than more advanced than equation 8.3, in which you have instantaneous acceleration. So they recognize, okay, you can have instantaneous acceleration, but there's no damper here. So normally the true equilibrium point control formulation is viewed as a spring mass damper system. And, um, that way, if you if you remember from physics class, you know, depending upon your setting, your your mass, the spring tension and the amount of damping you can get over damping or under damping, and then you can get kind of an optimal settling to the or approach of the set point with maybe just a little bit of overshoot or no overshoot. But if you don't have a damper in there, then it never settles in at the equilibrium point. So just to carry this a bit further, I would have to disagree with this statement that continuous time formulations are well suited to characterization of movements, because that's assuming that the equilibrium point control model is a good characterization of movement. I worked on this in grad school for about two years. When I first got to grad school, we worked on the Ilium point control model. I worked in the BT lab at MIT. We had an apparatus that mimicked an apparatus that was used in monkeys. We had one that we used on ourselves as human subjects, which would allow you to move your arm, and then it would kind of give you a little jolt, a little disruption to measure the amount of stiffness in. Your movement as you're moving your arm as a way of trying to deconstruct or work back the parameters of the equilibrium point control model for movement. And it's attractive because, as they say in the chapter, well, you don't have to have any very sophisticated knowledge of motor planning or inverse model of your plant. The system, in order to all you do is you say, well, this is where I want the arm to go. And passively, it takes care of itself, perhaps through a feedback loop, through the spinal cord, which I guess is what, 8.1 has. It's interesting that there's a paper from 2000 and 911 and Feldman that is reviewing this, but our conclusion was pretty much that equilibrium point control for motor control, it really is not a good model. It doesn't work. In order to make it work, you have to have all kinds of play, all kinds of shenanigans, like, well, you not just have a fixed set point, you might overshoot and then you bring your set point back and you actually have to control the stiffness during movement in order to account for observed data. But still it falls short because many, if not most human movements or any motor movements are ballistic. So it's not that you set some conditions and then your spinal cord and motors and muscles act to fulfill those conditions that you set once from the top, from your brain. What really happens is your descending signals anticipate a whole trajectory or a whole plan in advance of how we're going to accelerate and then we're going to have to decelerate. And so the motor planning kind of knows what your plant is going to do. It doesn't wait for the plant to do something and then have to respond to that in kind of a passive way at the lower levels. So it's just like if you play the piano, you're not waiting for your finger to say, am I far enough down on the key? You learn how hard you have to press it and when you start to release before anything happens. Our movements are very fast. There's really no getting around having to model a plant and doing this sort of inverse modeling that they're concerned about in order to account for how any creature really moves or anything above an insect, and probably an insect too, is undoubtedly it's a motor plant. I really find that this chapter, I don't think it advances a cause of active inference. It doesn't really fit with how motor control really has to work and doesn't add anything to the theory of it. I mean, it's using an old theory that doesn't work.
1408100	1410370	A	0.727678	Thanks for all those points.
1411380	1412720	B	0.9940899999999999	That's my statement.
1413960	2309660	A	0.9175652925353079	That's it. That's the tweet. Well, one live stream other than 49, which has just been like, amazing from the physics angle. We're preparing also for 50, which is using it's also by Feldman, Barrett and others. And it's specifically on interoceptive reflexes. So less exploring like the spinal reflex arc and more on the vasopressive interception. But using those kinds of still classical, very differential, equation based equilibria control and showing that those types of formalisms can recapitulate like allostatic behavior, it's always difficult to know. Like, is that a very narrow and fragile range of parameter space? Is this a descriptive model that's making unique predictions? Let's look at figure eight. One, because it's speaking to the generative model, generative process distinction that was mentioned earlier, and it was addressed in a question. How is this representation of the reflux arc similar or different to what was being used in figure five or in chapter five? So in chapter five, here was the summary statistic or summary graphical abstract. There was a lot more focus on this kind of like policy selection and so on. But we we did have this kind of a reflex arc, y, E and so on. Okay, so now we're in 8.1 f is the action selection component. So here is that simple attractor being shown where it just X is going to be converging to V, and this is the change in external states. So here we have Mu of X in an equation. In the lower one, is this one referring to this or is that kind of bleeding out from the bottom panel? Because here's U of X, and here's Mu of X. Here it looks like Mu of X is a prediction modulator on a differential. So it's like if the observation is where you want to be, then there shouldn't be any action. But as the observation diverges from where you want to be, you have a precision modulated action. Is that active inference or is that just a notation, a different notational representation of classical equilibrium point motor control theory? Do those models integrate sensory and action together? I mean, in one sense they definitely do. Nobody was ever generating reflex arc models without some sensory feedback variable. So that's an open important question. Is this didactic with respect to active inference now just kind of saying, yeah, one special case of active inference is the traditionally understood and limited equilibrium point model of motor control, or is it like, this is our new way of doing motor point control? Okay, just kind of continuing on. All right. Precision, attention and sensory attenuation. So here's another curious note. So earlier we discussed whether motor control was or wasn't a natural setting for continuous models. In the box 81, they write, we address this the importance of precision in chapter seven, but it's worth recapping its role in continuous time systems. In many ways, this concept of precision is more naturally addressed in this setting as the Pi variable appears a direct consequence of LaPlace approximation. Okay, but can't you do a LaPlace approximation in discrete systems as well? LaPlace approximation is just mode determination followed by the parameterization of a parabola to create like a gaussian or at least a concave optimizable and two parametered approximation to an arbitrary distribution. I don't exactly see or is it referring to the LaPlace approximation is itself a continuous function, but you could apply a LaPlace approximation to a categorical distribution as well. So in what ways is precision more naturally addressed in a continuous model? Is it that precision is a continuous variable? Like precision isn't an integer valued variable so we can just leave that one. Okay, so we went into 83 dynamical systems, kind of looked at the spinal cord again, but it may or may not just be a sort of like trivialized or didactic representation of extremely traditional representations of motor control. Then box one, precision attenuation attention, sensory attenuation all. Right, now we get into the lockvoltera dynamics. So what are some formal and qualitative implications of the use of lock of voltera dynamics and Lorenz systems? So I think there's a few interesting angles that us here now but others just more broadly may be interested in. So one is lock of voltera ties in of course with ecology and population ecologies. And so that is also very interesting to consider in terms of winnerless competitions and neural dynamics. And that is basically the analogy which is like just like even if you have a negative edge between the predator and the prey for any persistent which is to say oscillatory or manifold attracting population, at some point you have to let the prey regrow. So the analogy in neuroscience that's fleshed out in SPM is like if you have a brain region even with a unilaterally negative influence on another region, at some point it has to let off the brake cyclically. So lock of volteris summarizes these kinds of winnerless dynamics that can give oscillation and time structure including with complex dynamics and recurrent dynamics, limit attractors. All these kinds of dynamical processes can arise from a pretty simple starting point. And here we see manifolds and limits on the projection because it's a three dimensional space plant, herbivore carnivore or that could be the blood oxygenation of three brain regions in SPM and this is their neural trace or their inferred activities. And so then you can find like manifolds of lower dimensional activity. And then also one kind of related note is the Voltaire series and this is a pretty nice just technical note which is Taylor series does expansion of arbitrary functions from a starting point and the voltera series is able to include like a window of inputs also known as the memory effect. And so it's discussed in SPM. It has a continuous time and a discrete time variant and it is just a kind of kernel estimator that can be used for dynamical processes, input output dynamical processes with memory. So it's kind of a classic in that area. And I think even in some live stream, maybe 45, I asked Friston about the Lockable Terra models and about why do we move away from them or something like that. But that'd be a good answer to kind of come back to. All right, so now those kinds of lockable terror dynamics are going to be applied, monochromatically as ever, to active inference. And that was one question, which is it didn't finish writing. How is the Lockable Terra model being used in the case of handwriting? So I think it's in the Friston and Herrero's paper, which I just brought in, but I didn't really investigate deeply. The left, there's two different applications here. Left eye blink, conditioning. So here it looks like a condition stimulus, unconditioned stimulus, where there's like where something happens and you blink. As we've mentioned previously, it's a little bit difficult to tell exactly what these expectations are of or anything like that, but this is like some kind of stimulus response. Blink once if no, twice if yes, once if by land, twice if by sea. And then on the right side, sequential peaks using an attracting point, but selecting the specific attractor on the basis of which population a lockable terrace system is currently highest. This leads to sequential visiting of each points, giving rise to a type of handwriting. So this is kind of interesting, and I know for Yaakob and Ali also, with our mode tracking path matching discussion, Bayesian physics. So it's like I I must it's Bart Simpson at the chalkboard. This is I must rewrite the sentence. I must rewrite the sentence. I must. So your expectations are that you're going to be writing this sentence and then depending on which of those expectations are on top, that is going to set the motor imperative. So it'd be like if you were writing the letter D. And it'd be like well, it's like first there's like an arc down into the right, and then there's an arc down into the left, and then it's a straight line up. And then it's an arc down to the right, and then it's an arc down to left, and then it's a straight line up. And so here, it's kind of like doing this writing as it's picking a mode, pursuing the mode, and I think it's fair to say, Yakavarli, please add any more if you feel like it. But like, this is like shaking off the slumber of the classical physics and still having 1ft in both worlds. And the three faces of Bayesian mechanics are a much more advanced way to discuss us. This issued what do you think?
2310990	2379200	C	0.9119761111111109	I was just going to say that this well, not the case with writing specifically, but the case of motor motor control was discussed in Thomas Parr's lecture at the CPC 2022, which I don't know if it was recorded, actually, I think it was, but I don't know if it's public yet. But it's interesting that this example is in the continuous time chapter because you could think of it as a nested generative model with both discrete and continuous factors. Like if you partition it into three decisions, like first straight line up, then an arc down, then a second arc down. Those are three discrete decisions that are at the top of the hierarchical generative model with the finer grain motor control below.
2384790	2442960	A	0.9013786956521738	Thanks. Also just here's, that classic three faces figure mode tracking. This can be seen as using a lock of Ultera pseudo random number generator to spit out higher level predictions about which mode should be pursued. Then as to the dynamics of how that mode is approached, one can either use the equilibrium point methods discussed in this chapter or the more advanced Bayesian physics methods to describe that in terms of gauge theory and so on. Is that fair to say? This it Jakub. Thank you, Ali, for sending it.
2445250	2466026	C	0.9008397826086952	It's not the same. It's not the same one. But it looks like a lot of the slides are the same. I think the thing that I was referring to I'm just trying to find the slide. Oh, yeah. I think it might be minute 41.
2466208	2524300	A	0.9323850537634406	Okay. I really can't wait till there's some open source webcam software to do isocate analysis and pupil diameter. I don't think it would be that intractable. Maybe you'd have to be kind of close, or you'd have to use a 4K camera or something. But is there any software like that available? I've never come across it just like move the mouse to where I'm looking and then click when I blink twice. Isn't that kind of like that does exist, but what is it called? Or where is it being used?
2527090	2535440	C	0.942106	I know it's being used for people with physical impairments who can't control the computer.
2537590	2538146	A	0.72706	Yeah.
2538248	2551074	C	0.8922105882352941	Otherwise, yes. I haven't seen an analysis of pupil dilation, but eye tracking is definitely a thing.
2551192	3078880	A	0.926478458980047	Okay, maybe we can explore it. All right, so, yeah, the blinking example was a puff of air. So it was just like a beep and then a puff of air. And so then they learned to preempt the air puff. Okay. Learning incontinence models eight two continuous time domain accumulating evidence. This works if we treat data in a series of small time intervals. Oh, no, don't discretize. This relates, I think, to some of the finer points on which Stochastic calculus is used, like the Edo calculus in the Bayesian mechanics. We won't go into it unless anybody really wants to say more, but this looks like important just to learn about the integrals and the gradients. P OMDP has largely superseded the use of the generalized Lotka volterra systems in active inference applications. That was kind of, I think, related to asking Friston about why things moved away from the lockable Terra formulations. Lorenz system, also explored in Stochastic Chaos Markov blankets Livestream 32. It's it's almost. It's dealt with as minimally as you could possibly deal with the Lorenz system in 87. Okay. 84 generalized synchrony. So multi agent inference starts coming into play as well as agent environment synchronization. And we actually like, talked a little bit about the birdsong in the previous hour. Now the Run system comes back and yes, it's dealt with like sort of in the 2015 Frith and Friston paper, active inference communication and hermeneutics. And that's where the synchronization manifold is kind of displayed. And then more recently in live stream 32. And this one has a dot three. I think this might be the only dot three as of this point. I think Connor Hines could only come at a later date. Yeah, so these are good discussions on Lorenz attractor a couple Lorenz systems. This is kind of very similar, if not identical to the birdsong synchronization manifold in first and fifth 2015. Birds, birds, birds. Bird synchronization isn't real then hybrid discrete models, hybrid mixed models, not that kind of mixed model allow inferences about sequential action plans and translations of those decisions into movements through a continuous model. So in that respect, it's extremely linked to Livestream. 46 active inference models do not contradict folk psychology, which is all about that continuous time motor, active inference, discrete state space decision active inference, mai and dai and using those motor and decision active inference models, while lower level model in the dashed box is the same form as the other models. In other words, it's continuous because we have V instead of Pi for the causal factors and F instead of B and G instead of A and ETA instead of G. Is it some mapping between well, in this case, I think just looking at it a little more narrowly, how outcomes map to slower causes of the world. So here this is a discrete model up top with the G. Oh, yeah, that was one erotic that we found. Free floating G. Can't live with it, can't live without it, but it should be connected to the pi. Of course there's a discrete time model on top and then it is kind of cascading its outcomes down to these continuous time models. So one could think about this like the Lock of Ultera handwriting example kind of could it's not the exact structure of this, but it's related. Okay, this is something I know Jakub has thought about how discrete and continuous models interface because we've talked about how the temperature could be continuous and the thermometer could be continuous, but then the internal generative model could be like, is it hot or not? So the question of how to move from generative models within and between different parts of the model, where are what kinds of transitions between continuous and discrete state spaces possible? Okay, here's a continuous isocate ocular motor paradigm where the continuous lines are still confusing, but at least they are justified. 83 combining categorical and continuous generative models. Outside of active inference has primarily been framed through the lens of clustering. The aim is to assign data points into a discrete cluster. I wasn't exactly sure about that. I think clustering is one discretization clustering and thresholding is one discretization approach that's been used. Also, just binning seems like a simpler method. How do you take a continuous distribution of one through ten and make it categorical? Just bin it one through ten. Or you could cluster and do thresholds. So then you could have cluster one with 9.7 to 9.9 and then 9.7 to 9.5 and then 86. There's a big table here's. The songbirds this author Isamura, I believe, is the one who will join for 51 in November, and I think that'll be like our last probably one of our last paper streams for the year. We have just this last discussion with Dalton next week and then interception, and then the canonical neural networks perform active inference papers. But Isomer has worked with Par and Frisk and others. Ocular, motor reflex, eye pursuit, psychosis illusion, circade action, observation, attention, hybrid, and self organization models. Well, with four minutes left, kind of an interesting chapter, but I can only know it how I'm seeing it, and I have serious questions about what it does in the book. Eric.
3081780	3111070	B	0.9336422972972971	One thing I was puzzled by, maybe you guys can help me out with the bird song example is, were they implying that bird starts singing, another bird picks it up synchronizes using these dynamics, and the proof of that is that some other bird will just step in and take over and the first bird can stop. I don't believe I've ever heard that happening. Are they suggesting that that happens or what?
3115520	3274720	A	0.9280303731343281	Okay, I agree. Whether this is a bird improv conversation, in this case, they're singing from the same hymn sheets. So it's like somebody should be singing the bird anthem, and now we're going to be engaging in this sort of quasi dialogue where we're going to be reducing our surprise about hearing the song that we expect to hear, which goes like, insert your favorite song there. And then the phenomena that gets highlighted is the way in which one bird is singing, and then as it's singing, its precision starts to drift and then it stops singing, and then the other bird finds itself picking up at the same point to reduce its surprise about that song being sung at the time. But I think especially with with a bold title like, laying claim to hermeneutics more broadly, singing from the same pre coordinated hymn sheet is not the same thing as information updating or even just pure expressivity. So I think it leaves gaping areas for people to actually develop because it's like, well, we have what don't we have? Databases of like, millions of bird songs and natural soundscapes. So, like, we need some bird watching, active inference practitioners who want to actually develop the bird song motif because otherwise it is too easy and ironically become singing from the same hymn sheet. Oh, well, Hermeneutics has been addressed in 2015. Synchronization manifold 2015. Whereas this is like a very fringe case of communication that is based upon most charitably, high reliability execution of preordained performance, which is a tiny edge case of communication.
3276740	3350376	B	0.9315644632768358	My understanding of bird songs is baby birds hear what their parents sing, they've got some predisposition to sing a certain kind of song, and for most birds, there are some that are very adaptable. Parents, parrots, stuff like that. But most birds learn the accent or the dialect that they are raised in, and that's the song that they always sing. And there's some variation on it. I mean, maybe they'll stop midway through. But what are the true facts about what I thought to be a claim here, where birds will pick up from each other or one leaves off? Does that really happen? I'm not familiar with that happening. I don't think I've ever heard it happening. My impression of the way moosebirds sing is they'll sing their song all the way through and there might be a call in response, another one might pick up, might answer. But I don't think I have error here, synchronizing of songs, but I might be just not well enough informed about the ecology of those things.
3350558	3449620	A	0.9318136746987948	Yeah, very interesting questions. And the whole area with game theory of song, like, it's beneficial that somebody warns about the predator, but then you've revealed your location when you sing and things like that. That's the kind of decision making and uncertainty resolution and risk evaluation that I'd imagine an agent based birdsong model would orient towards. But those are some cool ways for us to go. So next week, I guess this is kind of a shorter chapter and it has some arcane symbolism, but also seemingly very skippable symbolism. So I'm glad that we could get through the whole thing and we'll return to any further thoughts on eight and then maybe we can have more of a seven plus eight synthesis. Or anyone can look into any of these citations key advances as we head into, like, chapter nine on empirical data, which is not super long, followed by ten, which is just a restatement. So onwards we go. Thanks, y'all. Bye.
3451080	3451650	C	0.8050999999999999	Thanks, everyone.

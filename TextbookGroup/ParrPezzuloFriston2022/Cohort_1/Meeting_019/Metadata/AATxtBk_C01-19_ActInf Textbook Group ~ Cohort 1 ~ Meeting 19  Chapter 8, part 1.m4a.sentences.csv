start	end	sentNum	speaker	confidence	text
970	1840	1	A	0.99999	Hello everyone.
2530	5760	2	A	0.97965	It's october 7, 2022.
6130	12400	3	A	0.9999	We are in meeting 19 for cohort one and we're discussing chapter eight.
13490	20800	4	A	0.99916	So there's a bunch of questions and also a lot of things to talk about.
21650	24998	5	A	0.9999	Let us just begin with looking at chapter eight.
25044	25640	6	A	0.45	Eight.
26170	36566	7	A	0.99908	So does anybody want to give any first thoughts on active inference in continuous time?
36668	40230	8	A	0.99744	Everything flows, nothing stands still, et cetera?
44330	45080	9	A	1.0	Yes.
49970	51578	10	B	0.96526	Not on that specifically.
51674	55386	11	B	0.54944	Okay, but I will have something to say for this chapter.
55498	60462	12	B	1.0	I worked on this topic during my graduate time.
60516	63780	13	B	0.99844	So once we get to about 8.3, I'll weigh in.
65510	66450	14	A	0.97389	Excellent.
68310	73266	15	A	0.99986	Any comments like discrete continuous time?
73448	76440	16	A	1.0	I know this is something we'll touch on again and again.
86400	94752	17	A	0.99978	It's kind of like another low road, high road duality with continuous and discrete time.
94806	120840	18	A	1.0	And I think the textbook tries to give equal time and highlight the hybridization of these models in a way where realistically in the literature most are pure discrete models and continuous time models have been almost like domain bound.
122700	139512	19	A	0.99992	Certain areas like motor reflexes, where there's more of a tradition of continuous models are heavily represented in them, whereas decision making models tend to use the discrete formalisms.
139656	161330	20	A	0.59	And that difference between the sort of like continuous time traditionally motor associated and discrete categorical traditionally decision oriented models was addressed in the Folk Psychology paper and Live Stream with Ryan Smith et al.
161720	174696	21	A	0.99633	Okay, also I just started kind of an overview, got up through 83 of just understanding and just trying to lay out chapter eight.
174718	196988	22	A	0.53117	So for any chapter, everyone's always welcome to just make summaries and add their own notes so we can look at the questions, many of which are on the earlier part of the chapter.
197084	204930	23	A	0.99	And so maybe we can look at the earlier part of the chapter more this week and of course then have later time.
207800	220580	24	A	0.98426	Okay, so 8.1, it's a single paragraph, it's a complement to chapter seven, which was a discrete time approach towards generative model construction.
220740	230920	25	A	0.82	The focus here is on continuous states based models which are well suited for physical fluctuations, sensory receptors and continuous motion of effectors.
233940	238450	26	A	0.99588	They start with the case of movement control 8.2.
239140	243600	27	A	0.9999	So let's get familiar with the notation.
244580	249430	28	A	1.0	The states, hidden states are going to be X.
250040	253670	29	A	1.0	The data, the observables are going to be Y.
254120	259208	30	A	1.0	And then how states evolve over time depends on a static variable V.
259374	264680	31	A	0.99998	So that could be a hidden or slower changing cause or factor in the environment.
265660	272570	32	A	0.88	Then we have the omega terms associated with each of the data and the hidden states change through time.
273040	278824	33	A	0.63	And that variability is like the noise and this is like the signal or the flow.
278872	286560	34	A	0.99996	So it's kind of the length of an decomposition or a length of an approach towards dynamical systems modeling.
288900	296980	35	A	1.0	One interesting thing to note is first that this is hugely forsaged in SPM.
298120	312730	36	A	1.0	Then also that the data are a function of the states at that time, whereas the equation for the hidden states is a change in hidden states.
313740	321310	37	A	1.0	X dot note that Action is absent from equation 8.1.
322880	327630	38	A	0.99979	This is because Action is part of the generative process, not the generative model.
329680	333250	39	A	0.99988	Thought that was kind of an interesting question.
334420	335536	40	A	0.99974	What does that mean?
335638	339410	41	A	0.9397	That Action is part of the generative process and not the generative model?
341060	351460	42	A	0.99956	Surely Action selection is part of the generative model, the cognitive process of policy inference.
354040	360090	43	A	0.55731	Active states are part of the particular states.
360540	362650	44	A	0.80368	Active states are blanket states.
365420	368090	45	A	0.9999	Particular states are the generative model.
371590	377700	46	A	1.0	The generative process is referring to the external states, the hidden states.
379750	387430	47	A	0.9999	So is Action part of both the generative process and the generative model because it kind of sits in between intermediates?
388890	392950	48	A	0.99992	Or is this referring to the consequences?
396410	402570	49	A	1.0	The generative model only deals with those variables directly influenced by states external to a Markov blanket.
404270	414960	50	A	0.99903	So is that saying that the generative model is kind of like the complement of the autonomous states, Action internal states?
415650	417630	51	A	0.99999	Or is this even more extreme?
418130	423326	52	A	0.99998	It's only dealing with the variables directly influenced by states external to a markup blanket.
423438	433410	53	A	0.99999	So then it must only deal with sense states, which is like a narrow perceptual control reading.
434490	448330	54	A	0.69	And in that sense, Action execution is outside of the generative model because the generative model is only dealing with observations.
455310	460122	55	A	0.99996	Does anyone have thoughts on how generative model, Action and generative process are being used?
460176	460890	56	A	0.99984	Eric.
463070	470670	57	B	0.97	I wonder if you can consider your own actions part of what gets observed under that framework.
473730	497026	58	A	0.97676	As far as I would, the apologetics for the narrow view would be exactly that, which is like you have proprioceptors in your elbow, so you don't observe Action, but you are getting continuous updating of perceptions that are extremely influenced by Action.
497218	501350	59	A	1.0	But the hidden state of the world is actually like the angle of your elbow.
502090	507994	60	A	1.0	And so you're not directly observing the hidden state of the world nor how it changes.
508112	510422	61	A	0.99999	All you have is the proprioceptors.
510566	519390	62	A	1.0	And of course, you engage in policies to change your appropriate receptors, bring them towards alignment, which is what we're going to be discussing in the chapter.
520370	522590	63	A	0.63083	Ali, I saw your hand raise.
525570	532820	64	B	0.99653	Yeah, I wanted to mention exactly the proprioception which you mentioned, so I lowered my hand.
534070	535700	65	A	0.99994	Okay, yeah, great.
537590	548578	66	B	0.99993	But then could you include the fact that you've got signals going to your muscles, your motor signals and motor neurons firing in there, making your muscles contract?
548674	559658	67	B	1.0	Can that be part of your observation to say, well, okay, if this is happening, that these muscles are getting activated, then sure enough, the arm moves and appropriate section changes.
559744	578338	68	B	0.99998	So you can say, well, from the outside of that box, that's all part of the generative model, but not part of generating those muscle commands, but just seeing what the effect of them is.
578504	581170	69	B	0.97	I wonder if that would be an acceptable viewpoint.
585620	586370	70	A	0.99993	Cool.
586900	589532	71	A	0.99801	We'll return to this like where is Action?
589596	591056	72	A	0.99992	What is action.
591248	594436	73	A	0.84289	But I think it's just important to note while we're early here.
594618	602100	74	A	0.99845	Okay, so continuing a little bit on this part, I thought was also a little curious.
602940	603544	75	A	0.67341	Okay.
603662	605352	76	A	0.74919	So this is equation 8.1.
605406	607336	77	A	0.99994	We mentioned equation 82.
607438	611880	78	A	0.99971	So first question, italics versus bold GNF.
614060	616060	79	A	0.99999	Do you think that that matters?
620510	622570	80	A	0.99	I don't think it matters.
626580	628160	81	A	0.99998	Does anyone disagree?
628740	632500	82	A	0.96454	Do you think it's moving from one space to another space?
632570	636100	83	A	0.99999	Or do you think that's just a typographical error?
641080	644180	84	A	1.0	I think it can be considered as an eradica.
646940	658024	85	B	1.0	One thing I did notice is that 8.3 and 8.4, the italics they call a generative model, and 8.4 they call a generative process without the italics.
658152	659660	86	B	0.99964	Could that be the distinction?
660880	663310	87	A	0.9986	Okay, 8.3 and four.
663840	664444	88	B	0.96796	Yeah.
664562	665230	89	A	0.99766	Okay.
670140	672204	90	A	0.956	It's a very good question.
672322	685730	91	A	0.65604	Well, let's just say italics in 8.1 and three, bold in 8.2 and 8.4, unclear, if relevant difference.
691540	692290	92	A	0.99671	Okay.
706910	710010	93	A	0.99605	If the meta model you say, frames it.
710160	714480	94	A	0.9993	So if you consider the niche to be a metamodel, then sure.
723010	737010	95	A	0.99	The difference between equation one and two, other than the bolding or not, is that V, the static variables have been replaced with U actions.
737830	748150	96	A	0.85811	So this takes us from, like, the sort of SPM where you'd be like, you have neuroimaging data Y as a function of underlying neural states x.
748300	762858	97	A	1.0	And then neural states changing through time are a function of neural states at that time and some conditional factor that you're testing for, like whether they were exposed to treatment one or treatment two.
763024	791460	98	A	0.98	V so this is still a partially observable dynamical model, but then it gets moved very neatly into the control theoretic and the policy selectionist inference space by just saying, yeah, those slowly changing factors that influence how the hidden states change, we control those, or we have some ability to influence there.
792870	793620	99	A	0.66123	Okay.
798310	798722	100	A	0.83113	Yeah.
798776	809306	101	A	0.99936	So this could be a cue towards the Bulls difference, which is, like, the functions GNF are not necessarily the same as those used to define the generative process.
809408	817606	102	A	0.99997	If they said not necessarily the same as those used to define the generative process, bold, G, and F, then it would be unambiguous.
817798	820090	103	A	0.89643	But it's a little ambiguous.
824690	829520	104	A	1.0	We do not need to explicitly write down the dynamics of action in the generative model.
833560	834310	105	A	0.9528	Okay.
836220	840090	106	A	0.55467	They emerge from the choices made for terms in equation 8.1.
846990	849660	107	A	0.99969	We start with a simple sort of generative model.
850450	861200	108	A	0.8398	So G, which is the function that's generating the data, is going to be x F.
863590	875140	109	A	1.0	The function of how X changes through time is a function of X and V, the slower changing causes, and it's a difference between those two.
877110	880120	110	A	1.0	The hidden state represents the expected value for the data.
880890	890970	111	A	0.87944	Or another way to say that would be equation three says that the expected value for the data are the hidden state, but those are the same, and it's a point attractor.
891790	899930	112	A	0.99983	Or in the Bayesian mechanics, we could say it's mode matching by attractor.
902450	903200	113	A	0.96604	Yeah.
907780	908240	114	A	0.58	Yes.
908310	914470	115	A	0.57	The the attractor here is the mode tracking that is being referred to here.
915000	922564	116	A	1.0	When there's a difference between the hidden state and the hidden cause, V, then it moves towards it.
922602	926500	117	A	0.99999	So it's just moving towards a single fixed point attractor.
927580	937690	118	A	1.0	Now the generative process is defined, so it looks almost identical, except V's are replaced with use.
938560	949260	119	A	1.0	And this differential tracker, this kind of PID control motif, is replaced with action.
950660	962160	120	A	0.63532	It's an expression of the equilibrium point hypothesis, which treats motor control as enacted by reflex arcs that draw limbs towards equilibrium points set by descending motor signals.
969400	974470	121	A	1.0	And then also this is related to the skilled performance live stream 23.
975160	983930	122	A	1.0	The scheme does not require specification of inverse models that are widely used in other formulations of motor control.
986620	999950	123	A	0.94	8.5 makes a slightly more sophisticated model that's based around not just position, because, of course, you can't just pick up your finger and move it to a different position.
1000980	1008640	124	A	0.99996	Really, the action affordances are in the space of changes in velocity, which is to say, acceleration.
1009220	1027610	125	A	0.9997	So this is kind of a classical mechanical perspective on motor control, and it also points towards generalized coordinates of motion and that kind of PID control and the Bayesian mechanics and so on.
1029100	1033290	126	A	0.99424	Okay, now we get to 8.3.
1036620	1040780	127	A	0.99992	Continuous time formulations are well suited to characterization of movements.
1042400	1048924	128	A	0.99945	Well, certain kinds of movements on a chessboard, maybe not, but some kinds of movements, sure.
1049042	1049820	129	A	0.99814	Eric?
1052100	1066450	130	B	0.93	Yeah, I like to point out with equation 8.5 just above that where he's got the they've got this spring mass.
1067140	1075236	131	B	0.55579	So that system oscillates forever, right?
1075258	1080730	132	B	0.56329	It's got a spring and a mass, and there's no dissipation of energy.
1081980	1092664	133	B	0.99869	So it's a step, I guess, better than more advanced than equation 8.3, in which you have instantaneous acceleration.
1092712	1099724	134	B	0.99995	So they recognize, okay, you can have instantaneous acceleration, but there's no damper here.
1099762	1109870	135	B	0.99921	So normally the true equilibrium point control formulation is viewed as a spring mass damper system.
1110880	1136948	136	B	0.84	And, um, that way, if you if you remember from physics class, you know, depending upon your setting, your your mass, the spring tension and the amount of damping you can get over damping or under damping, and then you can get kind of an optimal settling to the or approach of the set point with maybe just a little bit of overshoot or no overshoot.
1137044	1142510	137	B	0.99928	But if you don't have a damper in there, then it never settles in at the equilibrium point.
1144880	1163708	138	B	0.89883	So just to carry this a bit further, I would have to disagree with this statement that continuous time formulations are well suited to characterization of movements, because that's assuming that the equilibrium point control model is a good characterization of movement.
1163884	1166372	139	B	1.0	I worked on this in grad school for about two years.
1166426	1173572	140	B	0.99991	When I first got to grad school, we worked on the Ilium point control model.
1173706	1176020	141	B	0.69	I worked in the BT lab at MIT.
1177000	1182516	142	B	0.99998	We had an apparatus that mimicked an apparatus that was used in monkeys.
1182628	1199212	143	B	0.56209	We had one that we used on ourselves as human subjects, which would allow you to move your arm, and then it would kind of give you a little jolt, a little disruption to measure the amount of stiffness in.
1199266	1213208	144	B	0.99993	Your movement as you're moving your arm as a way of trying to deconstruct or work back the parameters of the equilibrium point control model for movement.
1213324	1228668	145	B	0.92	And it's attractive because, as they say in the chapter, well, you don't have to have any very sophisticated knowledge of motor planning or inverse model of your plant.
1228704	1233576	146	B	0.87	The system, in order to all you do is you say, well, this is where I want the arm to go.
1233678	1244604	147	B	0.68	And passively, it takes care of itself, perhaps through a feedback loop, through the spinal cord, which I guess is what, 8.1 has.
1244802	1264112	148	B	0.99966	It's interesting that there's a paper from 2000 and 911 and Feldman that is reviewing this, but our conclusion was pretty much that equilibrium point control for motor control, it really is not a good model.
1264166	1265170	149	B	1.0	It doesn't work.
1267220	1284104	150	B	0.99999	In order to make it work, you have to have all kinds of play, all kinds of shenanigans, like, well, you not just have a fixed set point, you might overshoot and then you bring your set point back and you actually have to control the stiffness during movement in order to account for observed data.
1284302	1295736	151	B	1.0	But still it falls short because many, if not most human movements or any motor movements are ballistic.
1295928	1311756	152	B	0.65676	So it's not that you set some conditions and then your spinal cord and motors and muscles act to fulfill those conditions that you set once from the top, from your brain.
1311948	1326900	153	B	0.95487	What really happens is your descending signals anticipate a whole trajectory or a whole plan in advance of how we're going to accelerate and then we're going to have to decelerate.
1329000	1333944	154	B	0.99	And so the motor planning kind of knows what your plant is going to do.
1333982	1341160	155	B	1.0	It doesn't wait for the plant to do something and then have to respond to that in kind of a passive way at the lower levels.
1341580	1347310	156	B	0.99888	So it's just like if you play the piano, you're not waiting for your finger to say, am I far enough down on the key?
1348640	1354290	157	B	1.0	You learn how hard you have to press it and when you start to release before anything happens.
1355380	1357330	158	B	0.81836	Our movements are very fast.
1359540	1378820	159	B	0.99541	There's really no getting around having to model a plant and doing this sort of inverse modeling that they're concerned about in order to account for how any creature really moves or anything above an insect, and probably an insect too, is undoubtedly it's a motor plant.
1381420	1392540	160	B	0.99	I really find that this chapter, I don't think it advances a cause of active inference.
1396160	1403616	161	B	1.0	It doesn't really fit with how motor control really has to work and doesn't add anything to the theory of it.
1403638	1406050	162	B	0.94	I mean, it's using an old theory that doesn't work.
1408100	1410370	163	A	0.64295	Thanks for all those points.
1411380	1412720	164	B	0.99842	That's my statement.
1413960	1414740	165	A	0.94907	That's it.
1414810	1416100	166	A	0.97118	That's the tweet.
1417960	1425900	167	A	0.99498	Well, one live stream other than 49, which has just been like, amazing from the physics angle.
1426000	1433850	168	A	0.84605	We're preparing also for 50, which is using it's also by Feldman, Barrett and others.
1434700	1438336	169	A	1.0	And it's specifically on interoceptive reflexes.
1438388	1445660	170	A	0.98306	So less exploring like the spinal reflex arc and more on the vasopressive interception.
1446240	1478152	171	A	0.99997	But using those kinds of still classical, very differential, equation based equilibria control and showing that those types of formalisms can recapitulate like allostatic behavior, it's always difficult to know.
1478206	1482730	172	A	0.85172	Like, is that a very narrow and fragile range of parameter space?
1483500	1487480	173	A	0.9984	Is this a descriptive model that's making unique predictions?
1489340	1490836	174	A	0.99717	Let's look at figure eight.
1490878	1505810	175	A	1.0	One, because it's speaking to the generative model, generative process distinction that was mentioned earlier, and it was addressed in a question.
1506180	1514628	176	A	0.99998	How is this representation of the reflux arc similar or different to what was being used in figure five or in chapter five?
1514714	1522352	177	A	0.9993	So in chapter five, here was the summary statistic or summary graphical abstract.
1522496	1528600	178	A	0.99996	There was a lot more focus on this kind of like policy selection and so on.
1528750	1539260	179	A	0.59309	But we we did have this kind of a reflex arc, y, E and so on.
1539410	1561660	180	A	0.57951	Okay, so now we're in 8.1 f is the action selection component.
1563360	1584550	181	A	0.99834	So here is that simple attractor being shown where it just X is going to be converging to V, and this is the change in external states.
1606100	1612400	182	A	0.99741	So here we have Mu of X in an equation.
1613140	1620660	183	A	0.99748	In the lower one, is this one referring to this or is that kind of bleeding out from the bottom panel?
1622600	1625350	184	A	0.97885	Because here's U of X, and here's Mu of X.
1630460	1636600	185	A	0.88687	Here it looks like Mu of X is a prediction modulator on a differential.
1640580	1648180	186	A	0.99816	So it's like if the observation is where you want to be, then there shouldn't be any action.
1648600	1656840	187	A	0.93551	But as the observation diverges from where you want to be, you have a precision modulated action.
1663890	1685910	188	A	0.90995	Is that active inference or is that just a notation, a different notational representation of classical equilibrium point motor control theory?
1688650	1694680	189	A	0.99998	Do those models integrate sensory and action together?
1696270	1698780	190	A	0.95	I mean, in one sense they definitely do.
1700030	1707370	191	A	0.99989	Nobody was ever generating reflex arc models without some sensory feedback variable.
1709230	1711660	192	A	0.99999	So that's an open important question.
1712370	1734280	193	A	0.39742	Is this didactic with respect to active inference now just kind of saying, yeah, one special case of active inference is the traditionally understood and limited equilibrium point model of motor control, or is it like, this is our new way of doing motor point control?
1739850	1746620	194	A	0.73833	Okay, just kind of continuing on.
1751620	1752240	195	A	0.85491	All right.
1752310	1756080	196	A	0.98963	Precision, attention and sensory attenuation.
1759080	1762784	197	A	0.9986	So here's another curious note.
1762832	1770740	198	A	0.99808	So earlier we discussed whether motor control was or wasn't a natural setting for continuous models.
1771160	1778404	199	A	0.9999	In the box 81, they write, we address this the importance of precision in chapter seven, but it's worth recapping its role in continuous time systems.
1778532	1789420	200	A	0.99999	In many ways, this concept of precision is more naturally addressed in this setting as the Pi variable appears a direct consequence of LaPlace approximation.
1790560	1797010	201	A	0.95434	Okay, but can't you do a LaPlace approximation in discrete systems as well?
1797380	1822440	202	A	0.53782	LaPlace approximation is just mode determination followed by the parameterization of a parabola to create like a gaussian or at least a concave optimizable and two parametered approximation to an arbitrary distribution.
1822780	1836590	203	A	0.99	I don't exactly see or is it referring to the LaPlace approximation is itself a continuous function, but you could apply a LaPlace approximation to a categorical distribution as well.
1841120	1847550	204	A	0.73104	So in what ways is precision more naturally addressed in a continuous model?
1850900	1854000	205	A	0.99926	Is it that precision is a continuous variable?
1854580	1868710	206	A	0.99109	Like precision isn't an integer valued variable so we can just leave that one.
1870440	1889790	207	A	0.98486	Okay, so we went into 83 dynamical systems, kind of looked at the spinal cord again, but it may or may not just be a sort of like trivialized or didactic representation of extremely traditional representations of motor control.
1891360	1896188	208	A	1.0	Then box one, precision attenuation attention, sensory attenuation all.
1896194	1899980	209	A	0.99924	Right, now we get into the lockvoltera dynamics.
1901060	1910880	210	A	0.99958	So what are some formal and qualitative implications of the use of lock of voltera dynamics and Lorenz systems?
1912500	1921796	211	A	0.99737	So I think there's a few interesting angles that us here now but others just more broadly may be interested in.
1921978	1929076	212	A	0.99436	So one is lock of voltera ties in of course with ecology and population ecologies.
1929268	1936980	213	A	1.0	And so that is also very interesting to consider in terms of winnerless competitions and neural dynamics.
1937140	1956720	214	A	0.76	And that is basically the analogy which is like just like even if you have a negative edge between the predator and the prey for any persistent which is to say oscillatory or manifold attracting population, at some point you have to let the prey regrow.
1957460	1973220	215	A	0.99807	So the analogy in neuroscience that's fleshed out in SPM is like if you have a brain region even with a unilaterally negative influence on another region, at some point it has to let off the brake cyclically.
1973560	1985812	216	A	0.99992	So lock of volteris summarizes these kinds of winnerless dynamics that can give oscillation and time structure including with complex dynamics and recurrent dynamics, limit attractors.
1985876	1996030	217	A	1.0	All these kinds of dynamical processes can arise from a pretty simple starting point.
1996960	2019504	218	A	1.0	And here we see manifolds and limits on the projection because it's a three dimensional space plant, herbivore carnivore or that could be the blood oxygenation of three brain regions in SPM and this is their neural trace or their inferred activities.
2019552	2023140	219	A	0.62	And so then you can find like manifolds of lower dimensional activity.
2023480	2053890	220	A	0.97	And then also one kind of related note is the Voltaire series and this is a pretty nice just technical note which is Taylor series does expansion of arbitrary functions from a starting point and the voltera series is able to include like a window of inputs also known as the memory effect.
2054820	2059564	221	A	0.66	And so it's discussed in SPM.
2059612	2073220	222	A	1.0	It has a continuous time and a discrete time variant and it is just a kind of kernel estimator that can be used for dynamical processes, input output dynamical processes with memory.
2075400	2077192	223	A	0.9999	So it's kind of a classic in that area.
2077246	2092044	224	A	1.0	And I think even in some live stream, maybe 45, I asked Friston about the Lockable Terra models and about why do we move away from them or something like that.
2092082	2095230	225	A	0.65222	But that'd be a good answer to kind of come back to.
2096480	2109360	226	A	0.55908	All right, so now those kinds of lockable terror dynamics are going to be applied, monochromatically as ever, to active inference.
2109940	2113820	227	A	1.0	And that was one question, which is it didn't finish writing.
2113900	2119460	228	A	0.99996	How is the Lockable Terra model being used in the case of handwriting?
2125980	2134680	229	A	0.99105	So I think it's in the Friston and Herrero's paper, which I just brought in, but I didn't really investigate deeply.
2140610	2145214	230	A	1.0	The left, there's two different applications here.
2145412	2148750	231	A	0.82667	Left eye blink, conditioning.
2152290	2163970	232	A	0.99929	So here it looks like a condition stimulus, unconditioned stimulus, where there's like where something happens and you blink.
2167860	2180820	233	A	0.52917	As we've mentioned previously, it's a little bit difficult to tell exactly what these expectations are of or anything like that, but this is like some kind of stimulus response.
2181900	2186840	234	A	0.60664	Blink once if no, twice if yes, once if by land, twice if by sea.
2187260	2198408	235	A	0.91	And then on the right side, sequential peaks using an attracting point, but selecting the specific attractor on the basis of which population a lockable terrace system is currently highest.
2198584	2202696	236	A	0.99998	This leads to sequential visiting of each points, giving rise to a type of handwriting.
2202808	2212160	237	A	0.99987	So this is kind of interesting, and I know for Yaakob and Ali also, with our mode tracking path matching discussion, Bayesian physics.
2213620	2222608	238	A	0.9986	So it's like I I must it's Bart Simpson at the chalkboard.
2222784	2224624	239	A	0.97058	This is I must rewrite the sentence.
2224672	2225744	240	A	1.0	I must rewrite the sentence.
2225792	2226180	241	A	0.55	I must.
2226250	2242840	242	A	0.99885	So your expectations are that you're going to be writing this sentence and then depending on which of those expectations are on top, that is going to set the motor imperative.
2243340	2246312	243	A	0.99998	So it'd be like if you were writing the letter D.
2246446	2253772	244	A	0.87	And it'd be like well, it's like first there's like an arc down into the right, and then there's an arc down into the left, and then it's a straight line up.
2253826	2257632	245	A	1.0	And then it's an arc down to the right, and then it's an arc down to left, and then it's a straight line up.
2257766	2273668	246	A	0.99	And so here, it's kind of like doing this writing as it's picking a mode, pursuing the mode, and I think it's fair to say, Yakavarli, please add any more if you feel like it.
2273674	2285648	247	A	0.99994	But like, this is like shaking off the slumber of the classical physics and still having 1ft in both worlds.
2285824	2292204	248	A	1.0	And the three faces of Bayesian mechanics are a much more advanced way to discuss us.
2292242	2309660	249	A	0.99573	This issued what do you think?
2310990	2337540	250	C	0.79	I was just going to say that this well, not the case with writing specifically, but the case of motor motor control was discussed in Thomas Parr's lecture at the CPC 2022, which I don't know if it was recorded, actually, I think it was, but I don't know if it's public yet.
2338630	2356034	251	C	0.99996	But it's interesting that this example is in the continuous time chapter because you could think of it as a nested generative model with both discrete and continuous factors.
2356162	2365740	252	C	0.97988	Like if you partition it into three decisions, like first straight line up, then an arc down, then a second arc down.
2366510	2379200	253	C	0.99989	Those are three discrete decisions that are at the top of the hierarchical generative model with the finer grain motor control below.
2384790	2385634	254	A	0.58735	Thanks.
2385832	2393090	255	A	0.99989	Also just here's, that classic three faces figure mode tracking.
2393430	2407110	256	A	0.99984	This can be seen as using a lock of Ultera pseudo random number generator to spit out higher level predictions about which mode should be pursued.
2407530	2425742	257	A	1.0	Then as to the dynamics of how that mode is approached, one can either use the equilibrium point methods discussed in this chapter or the more advanced Bayesian physics methods to describe that in terms of gauge theory and so on.
2425796	2427120	258	A	0.99982	Is that fair to say?
2440210	2441226	259	A	0.87316	This it Jakub.
2441258	2442960	260	A	0.9998	Thank you, Ali, for sending it.
2445250	2446014	261	C	0.52752	It's not the same.
2446052	2446946	262	C	0.58098	It's not the same one.
2446968	2452358	263	C	0.99993	But it looks like a lot of the slides are the same.
2452524	2459430	264	C	0.99	I think the thing that I was referring to I'm just trying to find the slide.
2462270	2462842	265	C	0.69	Oh, yeah.
2462896	2466026	266	C	1.0	I think it might be minute 41.
2466208	2466940	267	A	0.39039	Okay.
2480130	2492610	268	A	0.96	I really can't wait till there's some open source webcam software to do isocate analysis and pupil diameter.
2493190	2497598	269	A	0.65	I don't think it would be that intractable.
2497774	2501400	270	A	0.99999	Maybe you'd have to be kind of close, or you'd have to use a 4K camera or something.
2502730	2504790	271	A	0.99244	But is there any software like that available?
2504860	2515930	272	A	0.99425	I've never come across it just like move the mouse to where I'm looking and then click when I blink twice.
2518350	2522922	273	A	0.56919	Isn't that kind of like that does exist, but what is it called?
2522976	2524300	274	A	0.86521	Or where is it being used?
2527090	2535440	275	C	0.93	I know it's being used for people with physical impairments who can't control the computer.
2537590	2538146	276	A	0.72706	Yeah.
2538248	2540020	277	C	0.90478	Otherwise, yes.
2542310	2551074	278	C	1.0	I haven't seen an analysis of pupil dilation, but eye tracking is definitely a thing.
2551192	2553460	279	A	0.99967	Okay, maybe we can explore it.
2554410	2557430	280	A	0.86006	All right, so, yeah, the blinking example was a puff of air.
2557580	2560566	281	A	0.99988	So it was just like a beep and then a puff of air.
2560748	2565770	282	A	1.0	And so then they learned to preempt the air puff.
2568190	2568940	283	A	0.9836	Okay.
2569710	2581834	284	A	0.99844	Learning incontinence models eight two continuous time domain accumulating evidence.
2581882	2585470	285	A	0.99999	This works if we treat data in a series of small time intervals.
2586370	2589010	286	A	1.0	Oh, no, don't discretize.
2592630	2602950	287	A	0.99987	This relates, I think, to some of the finer points on which Stochastic calculus is used, like the Edo calculus in the Bayesian mechanics.
2606090	2614070	288	A	0.99959	We won't go into it unless anybody really wants to say more, but this looks like important just to learn about the integrals and the gradients.
2617370	2625130	289	A	0.56	P OMDP has largely superseded the use of the generalized Lotka volterra systems in active inference applications.
2626270	2631790	290	A	0.82782	That was kind of, I think, related to asking Friston about why things moved away from the lockable Terra formulations.
2636370	2643300	291	A	0.84105	Lorenz system, also explored in Stochastic Chaos Markov blankets Livestream 32.
2647910	2649138	292	A	0.86817	It's it's almost.
2649224	2655894	293	A	0.96097	It's dealt with as minimally as you could possibly deal with the Lorenz system in 87.
2656012	2656726	294	A	0.99763	Okay.
2656908	2659750	295	A	0.91	84 generalized synchrony.
2660330	2665282	296	A	0.99999	So multi agent inference starts coming into play as well as agent environment synchronization.
2665346	2669100	297	A	0.71	And we actually like, talked a little bit about the birdsong in the previous hour.
2670270	2683150	298	A	0.9	Now the Run system comes back and yes, it's dealt with like sort of in the 2015 Frith and Friston paper, active inference communication and hermeneutics.
2683810	2687754	299	A	0.56	And that's where the synchronization manifold is kind of displayed.
2687802	2694580	300	A	1.0	And then more recently in live stream 32.
2696790	2698114	301	A	0.99	And this one has a dot three.
2698152	2700980	302	A	1.0	I think this might be the only dot three as of this point.
2702810	2706070	303	A	0.99	I think Connor Hines could only come at a later date.
2706140	2715450	304	A	0.99943	Yeah, so these are good discussions on Lorenz attractor a couple Lorenz systems.
2718270	2728640	305	A	0.99978	This is kind of very similar, if not identical to the birdsong synchronization manifold in first and fifth 2015.
2729730	2731470	306	A	0.6422	Birds, birds, birds.
2734210	2756840	307	A	0.99907	Bird synchronization isn't real then hybrid discrete models, hybrid mixed models, not that kind of mixed model allow inferences about sequential action plans and translations of those decisions into movements through a continuous model.
2757610	2764366	308	A	0.9999	So in that respect, it's extremely linked to Livestream.
2764418	2793090	309	A	1.0	46 active inference models do not contradict folk psychology, which is all about that continuous time motor, active inference, discrete state space decision active inference, mai and dai and using those motor and decision active inference models, while lower level model in the dashed box is the same form as the other models.
2796390	2818380	310	A	0.99999	In other words, it's continuous because we have V instead of Pi for the causal factors and F instead of B and G instead of A and ETA instead of G.
2821510	2841040	311	A	0.99992	Is it some mapping between well, in this case, I think just looking at it a little more narrowly, how outcomes map to slower causes of the world.
2842770	2846898	312	A	0.9997	So here this is a discrete model up top with the G.
2846984	2849140	313	A	1.0	Oh, yeah, that was one erotic that we found.
2849510	2851060	314	A	0.97142	Free floating G.
2852310	2856386	315	A	0.99314	Can't live with it, can't live without it, but it should be connected to the pi.
2856418	2875170	316	A	1.0	Of course there's a discrete time model on top and then it is kind of cascading its outcomes down to these continuous time models.
2876550	2884950	317	A	0.81703	So one could think about this like the Lock of Ultera handwriting example kind of could it's not the exact structure of this, but it's related.
2886330	2907920	318	A	0.99968	Okay, this is something I know Jakub has thought about how discrete and continuous models interface because we've talked about how the temperature could be continuous and the thermometer could be continuous, but then the internal generative model could be like, is it hot or not?
2908610	2925060	319	A	0.99388	So the question of how to move from generative models within and between different parts of the model, where are what kinds of transitions between continuous and discrete state spaces possible?
2928150	2944310	320	A	0.85724	Okay, here's a continuous isocate ocular motor paradigm where the continuous lines are still confusing, but at least they are justified.
2948430	2953690	321	A	0.85	83 combining categorical and continuous generative models.
2954110	2958110	322	A	0.99999	Outside of active inference has primarily been framed through the lens of clustering.
2959730	2963310	323	A	1.0	The aim is to assign data points into a discrete cluster.
2966530	2968702	324	A	0.74	I wasn't exactly sure about that.
2968756	2974980	325	A	1.0	I think clustering is one discretization clustering and thresholding is one discretization approach that's been used.
2975750	2980370	326	A	0.99647	Also, just binning seems like a simpler method.
2982710	2987110	327	A	0.99999	How do you take a continuous distribution of one through ten and make it categorical?
2987530	2989302	328	A	0.99983	Just bin it one through ten.
2989436	2991218	329	A	0.99995	Or you could cluster and do thresholds.
2991234	3003340	330	A	0.99966	So then you could have cluster one with 9.7 to 9.9 and then 9.7 to 9.5 and then 86.
3003710	3007738	331	A	0.99989	There's a big table here's.
3007754	3025940	332	A	1.0	The songbirds this author Isamura, I believe, is the one who will join for 51 in November, and I think that'll be like our last probably one of our last paper streams for the year.
3028150	3037190	333	A	0.99974	We have just this last discussion with Dalton next week and then interception, and then the canonical neural networks perform active inference papers.
3038010	3040646	334	A	0.99941	But Isomer has worked with Par and Frisk and others.
3040828	3050010	335	A	0.5729	Ocular, motor reflex, eye pursuit, psychosis illusion, circade action, observation, attention, hybrid, and self organization models.
3055710	3076210	336	A	0.97829	Well, with four minutes left, kind of an interesting chapter, but I can only know it how I'm seeing it, and I have serious questions about what it does in the book.
3078020	3078880	337	A	0.95874	Eric.
3081780	3105416	338	B	1.0	One thing I was puzzled by, maybe you guys can help me out with the bird song example is, were they implying that bird starts singing, another bird picks it up synchronizes using these dynamics, and the proof of that is that some other bird will just step in and take over and the first bird can stop.
3105598	3107912	339	B	1.0	I don't believe I've ever heard that happening.
3108046	3111070	340	B	0.99717	Are they suggesting that that happens or what?
3115520	3117550	341	A	0.99655	Okay, I agree.
3119680	3126780	342	A	0.82777	Whether this is a bird improv conversation, in this case, they're singing from the same hymn sheets.
3127220	3147110	343	A	0.99914	So it's like somebody should be singing the bird anthem, and now we're going to be engaging in this sort of quasi dialogue where we're going to be reducing our surprise about hearing the song that we expect to hear, which goes like, insert your favorite song there.
3149500	3179730	344	A	1.0	And then the phenomena that gets highlighted is the way in which one bird is singing, and then as it's singing, its precision starts to drift and then it stops singing, and then the other bird finds itself picking up at the same point to reduce its surprise about that song being sung at the time.
3181460	3206360	345	A	0.99995	But I think especially with with a bold title like, laying claim to hermeneutics more broadly, singing from the same pre coordinated hymn sheet is not the same thing as information updating or even just pure expressivity.
3210320	3222976	346	A	0.98848	So I think it leaves gaping areas for people to actually develop because it's like, well, we have what don't we have?
3222998	3228480	347	A	0.99708	Databases of like, millions of bird songs and natural soundscapes.
3230020	3245456	348	A	0.99933	So, like, we need some bird watching, active inference practitioners who want to actually develop the bird song motif because otherwise it is too easy and ironically become singing from the same hymn sheet.
3245568	3248170	349	A	0.94	Oh, well, Hermeneutics has been addressed in 2015.
3249340	3251530	350	A	0.99982	Synchronization manifold 2015.
3253900	3274720	351	A	0.9998	Whereas this is like a very fringe case of communication that is based upon most charitably, high reliability execution of preordained performance, which is a tiny edge case of communication.
3276740	3293056	352	B	0.99999	My understanding of bird songs is baby birds hear what their parents sing, they've got some predisposition to sing a certain kind of song, and for most birds, there are some that are very adaptable.
3293168	3295284	353	B	0.57526	Parents, parrots, stuff like that.
3295322	3305210	354	B	0.89982	But most birds learn the accent or the dialect that they are raised in, and that's the song that they always sing.
3306780	3308824	355	B	0.55	And there's some variation on it.
3308862	3310812	356	B	0.78	I mean, maybe they'll stop midway through.
3310946	3324012	357	B	0.9999	But what are the true facts about what I thought to be a claim here, where birds will pick up from each other or one leaves off?
3324146	3325470	358	B	0.99995	Does that really happen?
3326080	3327890	359	B	0.99962	I'm not familiar with that happening.
3328260	3330176	360	B	1.0	I don't think I've ever heard it happening.
3330358	3341252	361	B	0.99994	My impression of the way moosebirds sing is they'll sing their song all the way through and there might be a call in response, another one might pick up, might answer.
3341386	3350376	362	B	0.5474	But I don't think I have error here, synchronizing of songs, but I might be just not well enough informed about the ecology of those things.
3350558	3355290	363	A	0.59395	Yeah, very interesting questions.
3357580	3372296	364	A	0.75	And the whole area with game theory of song, like, it's beneficial that somebody warns about the predator, but then you've revealed your location when you sing and things like that.
3372418	3386756	365	A	0.5971	That's the kind of decision making and uncertainty resolution and risk evaluation that I'd imagine an agent based birdsong model would orient towards.
3386938	3389668	366	A	0.99999	But those are some cool ways for us to go.
3389834	3402040	367	A	0.99027	So next week, I guess this is kind of a shorter chapter and it has some arcane symbolism, but also seemingly very skippable symbolism.
3402860	3419260	368	A	0.54115	So I'm glad that we could get through the whole thing and we'll return to any further thoughts on eight and then maybe we can have more of a seven plus eight synthesis.
3421040	3442420	369	A	0.99967	Or anyone can look into any of these citations key advances as we head into, like, chapter nine on empirical data, which is not super long, followed by ten, which is just a restatement.
3443160	3447364	370	A	0.50259	So onwards we go.
3447482	3448640	371	A	0.99984	Thanks, y'all.
3448800	3449620	372	A	0.93116	Bye.
3451080	3451650	373	C	0.77559	Thanks, everyone.

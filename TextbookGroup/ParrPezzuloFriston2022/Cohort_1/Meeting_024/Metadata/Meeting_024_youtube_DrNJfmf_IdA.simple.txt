SPEAKER_02:
Hello, greetings.

It's November 11th, 2022.

We're in cohort one of the textbook group and we're coming back to chapter 10, looking at the sections of 10, asking any questions about 10, thinking about any overview points.

Before next week, when it will be our final meeting,

And we'll talk about feedback, fill out the form in future textbook groups, talk about project ideas, next steps, talk about our plans for 2023 and so on.

But where do we begin with 10?

We covered many of the sections once last week.

There's a lot of sections.

The chapter kind of is like a symphonic presentation.

It has multiple movements and subsections that have their own rhythm and structure.

Let's go to the end.

I think this is a really important note about learning and active engagements, which is if you're not, even before skin in the game and psychological ownership and so on, if one is not making predictions,

not just about material like what's going to happen next in the tv show that i'm watching oh it surprised me but about the consequences of one's actions in the world they will not be able to update those models of action in the world so you could be surprised and end up being a good inferrer in a passive inference setting

However, doing is what actually allows you to learn in a totally different way, because whether you realize it at the beginning of the journey or not, or whether you even agree with this or not, those actions that you take

are chosen and exist on a trade-off frontier of epistemic and pragmatic value and so simply choosing to act is a strategy that opens up a space for pragmatic as well as epistemic action it's kind of like the hyper prior on active inference is how active

And one other note, just that anyone can add anything.

This was a discussion with JF and Kandon some days ago in our research meeting.

about the scaling debate in machine learning.

All you need is scaling, bigger natural language models and so on.

And like, how can active inference and how can we contextualize that and improve the state of things?

And we talked about how importantly GPT does not choose its own inputs.

And so, yes, it can engage within a perception action loop related to receiving inputs and sending outputs.

However, in terms of its own training and development, it's importantly constrained.

And that's not necessarily a bad thing.

It's just that to actively select what stimuli to sample

is very central to active inference from the ocular motor to the tactile and so all of the big data training algorithms do not select their own inputs

And then if they were to, it could be done in two ways.

It could be done in an ad hoc way or principled way.

The ad hoc way would be you do some kind of different architecture of a model that predicts what inputs would be valuable to train.

Or you could think of a unified imperative that would decide on both action and learning rules.

Eric and Brock.


SPEAKER_01:
Yeah, I suppose you could say that reinforcement learning, the machine is selecting its own samples to get feedback on.

Now, reinforcement learning is not GPT-3, it's not language learning, language modeling.

But I think an earlier paradigm, maybe I don't know, I think they still use reinforcement learning and deep learning models.

Although, you know, it was also, I think a lot of people are discouraged by that, because it's,

As they say, it's a very narrow, very low bandwidth signal you're getting back.

But it is an example of learning models using generating their own samples.

Thanks, Brock.


SPEAKER_03:
I was going to, I guess, take it to this biological place we left off in last

session of secading of like, being a kind of data cleaning process almost or stabilizing the, you know, the, like data set coming in, and that we do have some very, very crude, like, you know, built in labels for edges, kind of in our visual cortex, right?

That are

presumably evolutionary, um, and some decent evidence that like maybe one level higher, something gestalt like of like edges and heights, you know, um, or, uh, yeah, just certain kinds of like really, really, uh, generalized, um,

images or views that you might take that have like very pronounced, uh, definition edges of like, like a, a prayer, you know, like a landscape of, you know, our, our natural evolutionary landscape being like somewhat comforting or pleasing or something like this.

Right.

And walking over like a baby, like eight months or whatever, crawling over a glass, but it looks like, it looks like they're gonna fall and they're, they react to that.

right?

Like these sort of things where there's some sort of baked in like priors or again, like labeling, right?

There's no, I mean, are those hyper parameters, you know, there's, but then they're not controlling their hyper parameters in these models.

Like, so yeah, I mean,

there's no affordance there for that sort of like goal directed or selecting data sets in any possible way or any of that.

So


SPEAKER_02:
I'm thinking of a British English reading of how the world should be.

Like, we talk about expectations and preferences.

And if someone says, like, I should think so, it's not actually a normative stance.

It's like saying, I expect and prefer myself.

I should be there at eight.

Whereas often should is interpreted as purely normative.

Like I wish that you did this.

But should actually does kind of condense that expectation preference scenario.

And then to kind of come back to the inherited world models,

so yeah um sequential processing steps can be understood of within a hierarchical processing framework where higher levels have the interpretation as being hyper parameters on lower levels and i think we saw it a little bit at the end of um nine

which is basically for any, and this also speaks to the composability of Bayes graphs, which is that you can always have a variable as fixed, or you can have a hyper prior on that and an uncertainty on the hyper prior, or you could wrap it in another Bayes graph, but all of these can fall under the category of how things should be for that agents.

And a business that thinks that there should be a lot of interest next year and is wrong goes out of business.

The organism that thinks that it should be warm tomorrow night and doesn't plan for the cold dies.

And so the should almost reflects the whole cognitive stack and the action selection that makes it that way.

One area that Ali, please.


SPEAKER_00:
Yeah, I was reading the other day about the importance of the statistical regularity of any particular environment and the way it constrains

the evolutionary developments.

I mean, for instance, for terrestrial animals, the majority of the objects are solid objects.

And of course, that's not the case for sea animals.

But I was wondering that when an organism generates or constructs its generative model,

How exactly, especially in the case of visual perception, how exactly these kind of statistical regularities or let's say the statistical regularities as applied to their visual perception translates into their generative models?

Because for the most part, at least in this textbook, what we saw in generative model is just an abstract conception of generative model and, as you say, how the world should be.

But I don't see how these kinds of evolutionary constraints or statistical regularities of the environment takes place when a generative model is constructed for each specific task.

Because in my view, that would make a lot of difference in constructing the outer priors

according to those regularities.

I'm not sure if I expressed my question clearly enough, but yeah, I was wondering how that's done in active inference framework.


SPEAKER_02:
Yeah, good question.

Just on the visual note, and then the generative model.

On the visual note, there are some statistical regularities of natural visual scenes, like

pebbles on the ground, trees, like there's some autocorrelation functions and some spectral density analyses and some fractal similarity things that people have done so that like for quote natural scenes, so not necessarily a skyscraper, but again, like the bodies of animals and natural settings with trees and plants and

there are some like hyper parameters that make the scene natural, which is interesting to think about in these like image generating algorithms.

And then to the second point about the generative model, it reminds me of JF's approach towards society of minds, where it's like spinning up generative models.

Now, in that case, it's symbolic, logical generative models.

But also, we could imagine society of statistical generative models.

Eric?


SPEAKER_01:
Yeah, I would just echo Ali's question about how a gap in what we've

we've learned about active inference, which is how do these models get made in the first place?

For the examples in the book, well, he walked in with the team A's and a model for how the team A's works.

And that was probably the most sophisticated active inference model presented.

But when you get to real world scenarios, well, how are models of the world learned?

And that's a big open question.

You know, are they even, what sense are the generative models?

They're often predictive, I think.

But, you know, are they probabilistic generative models in the sense that we think of them mathematically?

That's, you know, some people model it that way, some people don't think so.

You know, certainly neural networks are discriminative in many cases.

So I just, I guess agreeing that that's, there's a lot of,

work unsaid to be done there are a lot of questions open.


SPEAKER_02:
I wonder if some people hope for perceptual control theory to kind of save the day, which is like, we don't need to make degenerative models for the bewildering possibilities of external states, but it might be enough to,

first to have bodies that can engage in interoception and homeostasis and allostasis.

And then with that machinery as enabling, maybe all you need to know about the external world is like, it's good or it's bad.

And just like run away from bad things and go towards good things.

and then you have to you know you can update your a matrix on whether you think good or bad states emit this or that kind of observation but um the the blanket facilitates the organism's hyper focus on its own states

and might allow it to engage less in this open-ended structure model fitting of the world not that you wouldn't do better with infinite resources and a better causal model of the world brock um what's something that we kind of discussed with jf actually too about um


SPEAKER_03:
Some of his generative models are going to be static and specified, just like the robot is literally specified by him.

It seems like if you wanted to have a more general explanation or process or whatever to get to whatever human level or something, things that start exhibiting those sort of behaviors or...

commensurate with active inference type models, you would need evolution.

You would need to evolve the system to that point.

And then you could ask a question like, if you did that, would you gain anything compared to just build Optimus and it's not as good, but it's a good first order approximation, first step.

of like towards something that would be constrained in such a way that it's more likely to produce generative models that are useful and whatever, could be human-like or whatever, you know?

Yeah.

Conscious, intelligent, something, whatever.

But I don't see how, yeah, I mean,

any possible explanations on the table for like how you get to something that's whatever that has some sort of central nervous system you know or or conscious or anything like that um that would all depend on something like evolution

And then that process is just necessarily like very long and laborious.

And if you're just, if you're looking for something novel, then maybe you have to do that.

But if you're just trying to get the first order approximations, something similar to what already exists, then why go through all that, you know, it's probably easier to just assume some priors, which is what JF is doing.

So


SPEAKER_02:
This reminds me of the problem in machine learning of one-shot learning, where a single image is shown.

And the goal is to learn cat as a category from one picture of a cat.

Now, there are algorithms that do better or worse.

But there's also some fundamental limits of any type of one-shot learning.

Like...

to say nothing of overgeneralizing, you know, overfitting, but of course.

So it's almost like this is statistics slowly retreating or advancing into biology because it's like, well, if we don't want to do just one shot learning on one data point or one data set, which is one datum still, it's one big datum, we want to have a learning model.

okay, now we want to have a learning model with active selection of inputs.

Now we want to have generators and basically like landscapes of models, portfolios of models, Bayesian model reduction and structure learning in the Bayesian setting as being across distributions of models.

And then that's where evolution gets us, which is,

populations of models existing in different niches and maybe if our data set is our generative process like maybe there are some models that are doing like really well in this part of the data set and there's other models that are doing really well another part of the data set you don't just compromise them or um you know ligate them together

their embodiment reflects the statistical regularities of the environment.

And I added this citation.

This is from Professor Gordon, my PhD advisor.

And it's like... And there were several subsequent papers that continued on this theme.

But...

it was like trying to get, without putting words into her mouth or anything like that, like trying to make this point that you couldn't, you can't just look at the collective behavior or the end of, you know, everything's collective all the way down.

So you can't just look at the behavior of an ant.

and just say like whether it's like good or bad or fit or not because there's landscapes with uniform and patchy distributions which is exactly the case for statistical distributions there's cases where the cost of stopping is high or low relative to the cost of operating and then there's situations where basically interruptions can be

easily recreated or not, among others.

And it just like, the generative process is what the generative model is fit to.

So it's an incomplete sentence or an incomplete expression to be like speculating on just generative model architectures.

without a little bit of at least an acknowledgement of the statistical regularities of the generative process.

If the generative process is a metronome that's alternating between states, then your generative model, you have some suggestions for how to design it.

If the generative process is another agent like you

then that's another, that's like the generative adversarial neural network case or the game theory case or the theory of mind case or the communications case.

Like those are different settings.

Returning to chapter six, what are we modeling here?

what is the appropriate form for the generative model, how to set it up, and then how to set up the generative process.

And I think we even discussed a little bit then, like, this is kind of a generative model first approach.

Whereas one could imagine step four moving to step one and a half.

Or, you know,

However, but it doesn't make sense.

Nothing in biology makes sense except in the light of evolution.

Famous Dobzhansky quote.

So will we have similar memes?

Nothing about the generative model is going to make sense except in light of a generative process.

The state transitions in the generative model, they are parameterized given the weight of the leg.

That's not part of the parameterization of the state space, but in fact, that parameterization makes sense in light of gravity, friction, the solidity of objects.

And so the disembodied general generative models, we lose sight of it because it feels like we can totally define the whole setting

therefore the parameters of the generative model should be like maximally interpretive but that might not even be true for the in silico case so especially when we're taking the meta bayesian perspective from chapter nine and we're thinking about doing inference on an inference doer there's a lot of

you know, X only makes sense of Y, which only makes sense in light of Z. And I think it's actually that more complex predicate.

I'm not sure if that's the accurate way to say it, but it's the more complex setup in the Metabasian model

that's one of the energy barriers to understanding not just act-inf but just good science and the map territory and all these other related topics especially because this dashed line that's our own gray zone

So our own parameterization is only going to make sense in light of things that we almost surely didn't specify.

Even our own parameterization about ourselves.

And so like understanding, I mean, then to kind of come all the way back,

I don't think it only leads us to an existentialist like, who are we?

What are we?

Where are we going?

Why are we doing that?

But of course, that's one of the places that you'll get to in this road network with the high road and the low road.

um one area we didn't go too much into which is totally understandable given just that it's a textbook that we've read over the last several months and all that is like the citations and just okay you know structure learning tenenbaum 2008 well we know tenenbaum is still active

So, and it's always useful to cite earlier papers that are more positional.

However, there's a lot of threads that could be connected to like, I don't know, a 2019 review on structure learning techniques so that the framing of the question and the comparators might be a little bit more modern.

learning to learn in machine learning 1949 formation of learning sets the seriously brain damaged monkey developed sets which allowed him to behave more efficiently than the untrained non-brain damaged monkey


SPEAKER_03:
This brain damage is adaptive.

Is that what this is?

Yeah, I'm sure it's not the conclusion, but I just can't help it.

Yeah.


SPEAKER_02:
I can see that the clickbait headlines already.


SPEAKER_03:
Maybe, I don't know, continuing the title of the chapter, and another question about categories from the last session here, like... What?

What?

Is there a specific active inference definition of sentient?

How does that relate to...

you know, attention, salience, epistemic dynamics, and not going down that existential rabbit hole we just left off on.


SPEAKER_02:
Ali.

Dr. Sentient.


SPEAKER_03:
Ali, what's... Is there an active...


SPEAKER_02:
specific uh I think he dropped off for a second he's back if you could talk Ali but I I just I joke because I know Ali was well um the upcoming live guest stream that we'll have with the um authors okay so that they recently had this papers in vitro neurons learn exhibit sentience

when embodied in a simulated game world they played pong and and then like people were um yeah okay people were freaking out because it was just like sentient as conscious but ali what is the take well uh i'm i'm not exactly sure what uh


SPEAKER_00:
I mean what was the controversy about because it was merely about just a lexical meaning of a term which is obviously something that you can look up in a dictionary and get those lexical meanings because the word sentient is defined in Oxford English Dictionary as

an organism that responds to stimuli, that's it.

So at least one of the meanings of the word sentient.

So yeah, I was not sure why the use of this word raised so much controversy on Twitter and around because that's basically how this word was used even in an 18th century or so.

I mean, before all these developments in artificial intelligence and philosophy of mind and so on.

So that's just minimal requirement for something to be described as sentient.


SPEAKER_02:
Yeah, I guess this framing, sentir, you know, in Spanish, me siento, I feel a certain way,

It's situated in not just a cognitivist, but a phenomenological space.

And so I believe that's some of the baggage people brought, but you're totally right.

Going to another mainstream source, it simply is the responsivity to sensations.


SPEAKER_03:
Is that like exhibiting agency?

Exhibiting policy selection, you know?


SPEAKER_02:
I don't even think it has to act.

Something could be sentient and not take actions.

Although one could then critique and say, if it doesn't take internal actions or covert actions of some kind...

then was the impression made or was it just you know teflon just sliding off of its perceptual apparatus so um but i mean it is what this chapter's title hinges on yeah it is i'm not sure i'm not sure that we


SPEAKER_03:
I don't know.

Uh, yeah, it's, it's still a little confusing in the light, in the context of active offense to me.

I think like it would have to be like the case that you, like you're saying, you know, some sort of action was taken.

Maybe it's a hidden state or something, you know, from the, but, uh, I don't see how, yeah, like it could be responsive to stimuli and not act like, um,

I mean, I don't know, like a rock rolling down a hill is responding to the stimuli of the gravitational geodesic there.

It's like, is that, you know what I mean?

That's not really... That's not a response to stimuli, right?

So that's probably outside of the definition there.

I don't know.

Maybe in Bayesian mechanics it's not, but...


SPEAKER_02:
Does sentient even add anything?

And are we only talking about organisms?

Organisms in the Kantian definition is the unit of biological organization that is the end in and of itself.

Not that that necessarily helps.

Whereas a more modern replicator evolutionary definition describes the organism as like the evolutionary unit or replicator.

Um...

like the ant colony as an organism.

And then Ali, I just saw you linked agential realism.

I mean, could this have just been... One of the benefits of Act-Inf is it provides... Again, we talked last week whether it's complete.

Provides a solution to the adaptive problems that agents have to solve.

Or inactive agents or active entities or active inferrers have to solve.

But not sure whether this...

brings in some lateral associations and over specifications that aren't necessarily entailed by simply active inference uh actually uh karen brought uh i mean uh uh the physicists to propose this um


SPEAKER_00:
ontological stance of agential realism defines agency as a kind of relationship between objects and not exactly as something that a subject or an organism has or a capacity of an organism or a cognitive agent, so to speak.

So in this sense, if we look at this definition of agency,

we can say that every kind of organism and every kind of even persistent objects in various time scales would necessarily have some kind of agencies.


SPEAKER_02:
Well, agency is another polysemous word, of course, because it can refer to the agent-like behavior

meaning something you'd specify more as like a mobile actor in a agent-based model, but also it is then used to specifically also mean taking action.

And like in the Bayesian physics, the textbook does not engage...

deeply with bayesian physics and it will be interesting to see in the coming one to three years as um dalton's et al's work becomes percolating through the space and potentially as reflected by second's textbook with friston and maybe others um at mit press

whether the different specified kinds of particles help us step outside of some of the previous framings of agent and agency.

So if agent is just going to mean thing, we have a way to talk about thingness in terms of its repeated measurements, quantum things,

Thermodynamic things, cognitive things.

So we have a good thing definition.

First in 2019, which is cited in this textbook, but it's not gone into too much.

And then we could talk about mode matching, mode tracking, and path tracking things.

And chaotic things and strange things.

And so a piece of wood floating down the river is following a path of least action that doesn't entail any cognitive model.

It is a thing.

It's agentic.

But it's not taking agency.

If it were a stick...

with um a different apparatus or increasingly different kinds of policy selection mechanisms that wouldn't change its thingness but it would be a different kind of thing

Also, I hope it's okay to share that and anyone who's watching.

On Monday at 6.30 a.m.

PT, Maxwell Ramstad is going to be giving a meeting at the Theoretical Neurobiology Group.

So email Active Inference if you want more information.

but he's going to be talking about some of these areas.

Basically, as part of this banner year of 2022, I believe there are some threads around... Ali, you can totally say it differently if you see it differently, but like...

kind of standing with two feet in act inf and fep as a framework and a filter as dean might say rather than this procrustean journey of trying to stretch and distort what fep is into other ontologies that might have false positives negatives

no quantitative definition in sight and so on and like just the way that so much ink and attention has been spent on framing FEP within past lineages of debate or trying to bring it to bear on one side or the other

i think that reflects a time coming when a true fep interpretation actually is possible rather than seeing fep as interpreted by others but that will be kind of interesting um okay well we have five more minutes what a year

it seems like going faster would have been challenging.

One chapter per week would have been a blitz.

Three chapters per week, I mean, three weeks per chapter.

Yeah, exactly.

Three weeks per chapter would have taken us

into the two thirds of a year range.

With some modifications and improvements that I hope everybody will engage in and have so much to contribute to, like in future textbook groups and other affordances, this is something like

a semester long course and that um like a four unit course or something where for two weeks reading a chapter with a few other selections so providing some other work and maybe starting slightly differently

Maybe, but these are all things that we can explore, just kind of like raising them.

I know that all of you are thinking about this too, but just wanted to like say it.

Within the context of a semester course.

And initially more like a seminar course, like we're working through it.

And some people like read and understand highly all the material.

Other people like fight through and,

get through some of it, but it all just is what it is before becoming more of a graded course.

Not even saying that we're going to grade.

I'm just saying there is a format archetype where challenging material is addressed, usually as a graduate seminar, and you basically pass if you participate.

And it doesn't require tests at all.

before we get into other kinds of assessments and everything.

Any thoughts in the last minutes here?


SPEAKER_03:
Thank you for doing this and having the cognitive range to the answer between all these disparate


SPEAKER_02:
gestures that we throw at you and uh yeah i really enjoyed it so thank you and also just one last night i maybe said it for some earlier but um we're in contact with the authors and with the publisher and um like i sent them some updates they're super excited um so we could schedule time to all talk potentially as like recorded but not live streamed

in early 23 with at least Thomas, if not others.

And we can return next week to some of the future steps and things like that.

But it's just like, it'll be cool.

All right.

Next week.

All right.

Thank you, fellows.

Farewell.

Bye.

Thank you, everyone.

Bye.
start	end	speaker	sentiment	confidence	text
650	1246	A	0.546256959438324	You.
1428	1918	B	0.5208579897880554	Hello.
2004	3962	B	0.9597113132476807	Thanks for all joining.
4106	13550	B	0.9252307415008545	It's active textbook group cohort one meeting 22, chapter nine on October 28.
16750	29170	B	0.8508115410804749	Okay, we're in the second discussion on chapter nine, model based data analysis.
32430	49710	B	0.91005939245224	Would anyone like to add anything they want or mention how the generative model discussion from the previous 1 hour relates to model based data analysis?
50130	52800	B	0.9077709913253784	How does chapter four relate to chapter nine?
67330	70320	B	0.907870888710022	What does chapter nine address?
72530	77070	B	0.7777726054191589	What do chapters six and nine address that chapter four doesn't?
84810	102340	A	0.6486007571220398	Um, I mean, six is about thinking about applying it in, like, incredibly kind of simplified step by step kind of way.
103670	112840	A	0.8335501551628113	And nine is very specifically about answering scientific questions with it and doing that with.
116170	117670	C	0.5474449396133423	This metabasian.
118330	121370	C	0.6493600010871887	It's like almost nested.
124460	127400	A	0.7527568340301514	Sort of schema of active inference.
128700	142556	A	0.741651177406311	So, I mean, that's the through line, I guess, but I'm not sure more.
142578	144030	C	0.6607948541641235	Than that what to say.
144740	146892	B	0.9186097979545593	That's really insightful.
146956	148320	B	0.8810703158378601	Thank you, Ali.
151540	166912	D	0.8127180337905884	Yeah, actually, I would compare it to a kind of learning how to solve problems in, I don't know, mathematics or other physics or other areas.
167056	190348	D	0.888534665107727	For example, in chapter four, we acquired some necessary knowledge or prerequisite about how to think about the phenomena or the problem we're dealing with.
190514	206450	D	0.8929039239883423	Chapter six kind of outlines a roadmap a roadmap about how exactly we should proceed to solve that problem in a kind of formulaic way.
207000	226536	D	0.7196081876754761	And chapter nine, actually, in chapter nine, we get our hands dirty and try to solve some of the actual or empirical problems with the tools or techniques we had learned in chapter four and six.
226718	247100	D	0.8535235524177551	So in this case, I somehow think of chapter nine as a kind of case of study well, kind of case of studies or a kind of solved problem section of this textbook.
247260	255440	D	0.5225231051445007	And, yeah, I don't know how much that captures their nature.
256420	257588	B	0.9799851179122925	That's great.
257754	258468	B	0.7093686461448669	Wow.
258634	260336	B	0.8913263082504272	Great thoughts.
260528	261910	B	0.7921646237373352	I totally agree.
265240	267780	B	0.9018975496292114	Let's look at the table of contents.
270220	273332	B	0.8060858845710754	Chapter one, special overview.
273396	276852	B	0.8325248956680298	Chapter mirror symmetry.
276996	278730	B	0.8339726328849792	Same, but different with ten.
280300	320200	B	0.7081778645515442	So now in the middle eight chapters, two chapters on the low road and the high road, helping two different approaches which can be developed, like in a lot more detail, to approach active inference from two fascinating and non controversial starting points Bayesian inference from the low road free energy principle and the repeated measurement persistence imperative.
324780	329800	B	0.9176567792892456	Chapter four is the first chapter on active inference.
330620	337420	B	0.8394138216972351	Active inference is about the generative models that are being specified and how they're being computed.
339760	356560	B	0.5614385008811951	After just one chapter on active inference, it goes to the area where the most research and modeling has been done in active inference, which is computational manalian neuroanatomy.
358520	364180	B	0.8805744051933289	One chapter on active, one chapter on the primary domain.
366200	372940	B	0.8402063250541687	First half of the book, second half of the book.
373470	383210	B	0.8077547550201416	This is the part where you're going to be in parallel, perhaps on your first, but perhaps not on your first reading.
383650	393230	B	0.8265982270240784	You're going to be doing playing through these applied aspects of modeling.
393970	397970	B	0.8467475771903992	First is the recipe for designing active inference models.
398470	405540	B	0.8942496180534363	In Chapter Six, as both of you have, very nicely set.
408070	426838	B	0.8831523060798645	Then the Figure 4.3 Dialectic discrete Continuous Time is revisited in two chapters because it probably it's one key difference.
426924	428566	B	0.8103192448616028	It's a minimum of two.
428748	431974	B	0.5893331170082092	So it prevents, like any like, oh, well, it has to be a PMDP.
432022	433820	B	0.609562873840332	It's like, well, obviously not.
435090	456930	B	0.8583592772483826	So it keeps the space of generative modeling open and it allows for discussion of the very interesting topic of hybrid active inference nested models with continuous actuators, for example, but discrete decision making apparatus.
460070	468598	B	0.7962567806243896	So it just is like wanting to cover these two key types of models in a bit more detail than was addressed in Chapter Four.
468764	473978	B	0.741723895072937	But this was the first possible moment to address it after 4.3.
474064	474886	B	0.6426146626472473	Just kidding.
474998	476060	B	0.6598692536354065	Maybe not.
477550	489920	B	0.798474133014679	Then Chapter Nine, yeah, where we are now, which is just, okay, you've built your pure discrete time pure continuous time or hybrid generative model.
490690	495440	B	0.7576860189437866	If it's a toy model, you can just generate data and just do whatever you want.
495890	515878	B	0.890975832939148	Or you might have data from sensors or real world or however that's in the data format that your generative model expected to receive, which was part of your design process, you could have determined what structure literally the data is going to be in.
516044	526106	B	0.8763789534568787	Is it going to be like, yes, data can be reshaped, but just broadly, is it like 256 sensors in a row of one?
526208	528380	B	0.9080483913421631	Or is it like eight by eight?
528750	530700	B	0.6694157719612122	Just those kinds of questions.
531390	556900	B	0.8990159630775452	So that when the data start coming in for the experiment, whether you generated the data from the generative model or whether you're using it in a purely recognition model capacity on real world data, what does it look like to do statistical modeling with these statistical models that we've been using?
558970	561640	B	0.8189003467559814	And then chapter ten is a review.
562490	577450	B	0.6907820105552673	It restates what each chapter does, provides some further thoughts, and just summarizes the unifying capacity of active inference modeling.
582500	588670	B	0.7747802734375	Then there are the appendices Ali.
591190	621050	D	0.8822161555290222	Also one in my opinion, very interesting section of Chapter Nine is the section on models of false inference, section 9.7, in which there's an interesting definition of the disorders, I mean the mental disorders, and how we can actually computationally model those different disorders.
622510	622970	D	0.509212851524353	Sorry.
623040	632202	D	0.6523988842964172	It says that here the disorder does not necessarily imply that the inferential mechanism is flawed.
632346	639410	D	0.7879083156585693	In most of the studies, the inferential mechanism operates normally but based on a flawed generative model.
639560	658790	D	0.8527566194534302	So that's basically what we had learned about how we can somehow update our beliefs or correct our perceptions based on either refining our generative model or modifying our generative model.
658860	670950	D	0.7743020057678223	Or somehow, sometimes we can just well, act upon those errors and try to reduce our uncertainties.
671110	689120	D	0.7196604013442993	But in this case, it's a very interesting point on the case of computational pathologies because oftentimes people ask, well, what does active inference actually explain?
689910	702182	D	0.977321445941925	So I think this is a very good example of its very meaningful and significant application in any sense of the word.
702316	730782	D	0.533885657787323	So if we can somehow computationally model these kind of disorders, we'll most probably have a much better understanding of its origin and the way to probable treatments of those disorders and many other applications can comes to mind.
730916	747060	D	0.9193804860115051	But yeah, I think this part of the chapter nine is something that was not addressed quite explicitly in the previous chapters and in my opinion is the most interesting part of this one.
749270	749874	B	0.7671424746513367	Great.
749992	750610	B	0.918424665927887	Awesome.
750760	751326	B	0.5491447448730469	Yeah.
751448	758310	B	0.7567237615585327	It reminds me of the table in chapter four, or sorry, chapter five with the neurotransmitters.
758730	763560	B	0.8994930982589722	So they were still at the organ systems level in chapter five.
764350	779694	B	0.7658520936965942	But five is very much related to nine, I guess in that sense as well, because it's like, here's what dopamine does, here's what acetylcholine does just at the computational model level.
779892	797314	B	0.8693416714668274	And then here it's like that computational model was fit, like between the phenomena, like attention or policy precision or however phenomena to a molecule in chapter five.
797512	805080	B	0.8225879669189453	Here, the phenomena is being connected to a real human clinical application.
808800	822530	B	0.7729209661483765	So it's not that we're saying or anyone is saying or anything like that, that the molecule is being directly linked to this paper, even if it's mentioned.
823220	844520	B	0.7728894948959351	The direct linkage approach is people in group X have more molecule, have less molecule, and even the neuroimaging approach can be seen as a variant of that, which is people in group X have more brain activation pattern.
847500	858030	B	0.8798272013664246	In computational pathology, those kinds of biochemical measurements and neuroimaging measurements can, and they are used.
859760	875280	B	0.8583119511604309	However, this type of computational modeling is referring to parametric modeling, where the parameters are related to the pathology.
877060	882630	B	0.7832245230674744	So it's not an a priori classification of those with and without.
883320	892040	B	0.8317356705665588	And then descriptive statistics showing that a biochemical or a brain imaging pattern are differentially expressed.
893020	895268	B	0.7921879291534424	That's descriptive statistics.
895444	897700	B	0.6871702671051025	It's not computational pathology.
897780	911180	B	0.8636879324913025	It might be precision pathology, but it could still be precision or big data or however, with just like purely doing data summaries, doing a random forest model to diagnose.
912000	923948	B	0.8788321018218994	These models have a cognitive structural hypothesis about the nature of these phenomena, like the ocular motor syndromes.
924044	926608	B	0.6906595826148987	Again, it's not just people with the diagnosis and without.
926774	932660	B	0.875314474105835	It's a neurocognitive model of eye behavior.
934920	954140	B	0.8738276958465576	And then explanations can be made from clinical data by rendering various aspects of the generative model conditionally independent of others, ocular motor syndrome, such as intranuclear optipomilegias, may be induced.
956080	967040	B	0.7611132860183716	So if you just were diagnosing these as different conditions, maybe they have similar ideologies how the condition arises, maybe they have different, maybe they have multiple.
968180	983380	B	0.8752428293228149	But these two conditions computationally can be understood as parametric modulating in a computational model of ocular motor behavior.
986120	1001240	B	0.5408225655555725	This isn't people with internuclear opto, DA, DA, DA have higher glucose or lower glucose or this brain region or this aspect of them, those are all absolutely valid approaches.
1002300	1009192	B	0.6101518869400024	That's like scientific methodological pluralism, which is like, it's okay that people want to do different approaches.
1009256	1040470	B	0.5795032382011414	Everybody's contributing to a common people who care about this and are creating high quality reproducible data, their data will always be useful and their perspectives are valid and we don't want to have a discourse space on inter nuclear optipomygia that's saying that one model is like simply the only correct model.
1042780	1073200	B	0.8648632764816284	This paper makes a positive contribution and points to some new ways of thinking about various pathologies and thinking beyond the ways that they've been worked on before, where one can imagine that before 2000 and so on, you had what health records.
1073700	1076320	B	0.5941842198371887	And you have some double blind experiments.
1077460	1085140	B	0.8891767263412476	And so you might do a meta analysis to find correlations with features of people and all of those things that we've been describing.
1085960	1096360	B	0.6096627116203308	But it was not plausible to actually implement computational models of these phenomena.
1097420	1121280	B	0.6015138030052185	Now, interestingly, that's something that can be worked on by researchers directly, and then somebody can a doctor could download Ocular motor syndromes programming, whatever it is, but this can be developed in a pure research context.
1122100	1144530	B	0.5965468883514404	And the Generative Model is like the interface for the kind of information that might not be available to the person developing the Generative Model because it relates to a lot of problems and issues about data availability.
1148470	1152594	B	0.6329742670059204	That's one kind of just side note, but dilly.
1152642	1166810	B	0.5753150582313538	Thanks for raising that point that I just wanted to emphasize and unpack, which is that this table, which is taking up two and a half pages of a pretty short book is describing.
1169090	1177786	B	0.6389907598495483	These are like the cases that can be brought up, which they're providing in a helpful table for what active inference describes.
1177978	1190100	B	0.7100270986557007	So if the principle of least action, gauge theory, et cetera, sophistication does not interest one, these are more pragmatic value.
1190790	1205660	B	0.6286603212356567	Now that will probably move the goalpost to show me where the computational modeling helped somebody with delusions and that's a fine place to want to go.
1207470	1216890	B	0.7443485856056213	But it's also a standard and an approach that it's not related to the substance of the theoretical development.
1223400	1228724	A	0.8436003923416138	I mean, I think you could probably the response to that question might be.
1228762	1256060	C	0.6032096743583679	Something like the difficulty of implementing that model in a way that provides actionable real time affordances to address the aberrant priors in a way that's causally.
1257280	1260830	C	0.6077415347099304	It's going to actually affect that system.
1262740	1272420	C	0.5566163659095764	We need either more compute or a particular kind of algorithmic medicine that doesn't exist yet or et cetera.
1272840	1284570	C	0.7158596515655518	But that's not a problem with the particular approach, it's just an insufficiency of ability to enact it, I guess.
1286300	1297464	C	0.82557612657547	But I wanted to ask a question about specifically this aberrant prior beliefs.
1297512	1301048	C	0.6749831438064575	It says ultimately the pathology is a consequence of aberrant prior beliefs.
1301144	1303980	C	0.6792399883270264	It's not the Generative Model, it's aberrant prior beliefs.
1307360	1321556	C	0.7022184133529663	I think this is like Ellie's point and everything that you just went through is correct that this is kind of where the rubber really meets the road for its value in demonstrating or whatever.
1321738	1325670	C	0.8097527623176575	But I'm wondering also.
1327480	1334200	A	0.5358974933624268	To me that there's like an easy confusion thing there about beliefs and priors.
1336220	1338250	C	0.7264748215675354	If they just believed the right thing.
1339340	1343384	A	0.552929162979126	Then they wouldn't have these that's not.
1343422	1345370	C	0.8336875438690186	Really what that means though, right?
1346880	1350620	A	0.8688795566558838	Is another way of saying this, like in the Ocular motor.
1352560	1365168	C	0.6142722964286804	Disorder case, like a kind of neurobiological bias in some kinds of cells or whatever, right.
1365334	1383108	C	0.6322826743125916	That's pathologically different than normal or whatever, like some receptor for a particular chemical or whatever, is that a kind of thing?
1383194	1388650	A	0.8382336497306824	That the way of saying that differently in that particular case that is.
1391420	1391784	D	0.5774549841880798	More.
1391822	1394010	C	0.8707730770111084	Commensurate with what they're actually saying there.
1394540	1412850	C	0.5986533761024475	Or like not more commensurate, but just not relying on jargon and or domain knowledge specific for what they mean by beliefs and priors or is that not right?
1415920	1418540	B	0.6918262839317322	Yeah, well, there's a lot there.
1418690	1442230	B	0.8815496563911438	Are you talking about the comparison of the terms beliefs and priors in relationship to the way people might naturally interpret such beliefs to be, and they may indeed be people's beliefs about the world in the conversational sense?
1444360	1454810	B	0.7248401641845703	Like some beliefs are just your retinal states but other states of the world, as if or really are your states on other things.
1456460	1457210	C	0.7360090017318726	Right?
1457580	1496620	C	0.7403208613395691	Yeah, I guess I'm asking if beliefs is in some cases it is beliefs, but it's also another way of stating if another way of stating that is a structural systemic bias for one thing or another or whatever in that context, is that physical or informational kind of non folk psychology?
1497520	1501710	C	0.8235717415809631	Is that commensurate with what they mean by beliefs there?
1504640	1509330	B	0.6025927662849426	Okay, let's try to yeah, Ali, please, first, sorry.
1510340	1516032	D	0.5588471293449402	I'm sorry, but you mean the statistical use of the word belief?
1516096	1526600	D	0.8128839731216431	Because in statistics and in folk psychology, as far as I know, this term is used in quite different ways.
1526750	1535812	D	0.7297111749649048	So I guess that what you're talking about is the statistical sense of the word belief.
1535876	1546590	D	0.7543259263038635	And if it maps onto the folk psychological sense of this term or am I missing something here?
1550000	1553120	A	0.7274783253669739	Well, let's just take two steps back for a second.
1553190	1573572	C	0.49724072217941284	And just in the text it says at the bottom of page 185 here at the top of that yeah, it says the infertile mechanism operates normally but based on a flawed generative model.
1573706	1577800	C	0.8032354712486267	This means that ultimately pathology is a consequence of aberrant prior beliefs.
1578620	1579370	B	0.5491447448730469	Yeah.
1580460	1591564	C	0.861709475517273	And I'm asking a kind of just to explicate different whatever right there in.
1591602	1595576	A	0.7597616910934448	The context of psychological disorders, which is not necessarily pathology.
1595608	1609810	C	0.7831825613975525	But I'm just you know, if you tell somebody, oh, yeah, it helps with or is the impetus or whatever for this whole thing was to think about specifically like schizophrenia and other but that.
1610980	1613956	A	0.7891675233840942	Immediately kind of puts it in that.
1613978	1617072	C	0.7235423922538757	Category or that context of definitions.
1617136	1621990	C	0.5759468078613281	And so just to kind of.
1624040	1626970	A	0.7579805850982666	Nuance the term their beliefs a little bit.
1628940	1629690	B	0.5491447448730469	Yeah.
1630300	1652370	C	0.7370765805244446	Is it commensurate with the rest of the text to say in the case of this specifically of this oculine motor disorder category, like you're saying beliefs in the retina or something that is like cones, like being sensitive to something.
1652980	1663764	C	0.822487473487854	Well, it's just what they are sensitive to and in this particular person they're just sensitive to or whatever the relevant structure is there.
1663802	1667108	C	0.7463252544403076	But it's sensitive to something.
1667274	1667700	C	0.5664746165275574	Right.
1667770	1671670	C	0.8492943644523621	And that's its belief in some sense.
1674920	1684056	C	0.5513808131217957	I'm not looking for what's the statistical definition or anything like that, but just is that completely off track or is.
1684078	1688410	B	0.8759170770645142	That I have a thought, but Ali, if you want to make a response please.
1690620	1691332	D	0.5491447448730469	Yeah.
1691486	1711392	D	0.7159715890884399	I think that this term belief and its different meanings in different contexts actually sort of bleed into each other and there cannot possibly be separated cleanly from each other.
1711446	1736280	D	0.8963306546211243	So in this particular case I believe that the word belief partly is this folk psychological sense of the word and partly it's based on the kind of philosophical epistemological sense of this word.
1736350	1755760	D	0.7280228137969971	Because especially in epistemology when philosophers talk about belief, usually they don't mean just the folk psychological sense of this term, although it relates to that concept, but it's much more nuanced than that.
1755830	1781130	D	0.7687103152275085	So in the case of Oculomotor pathology well, I believe that the word belief somehow can be interpreted in a little more abstract philosophical sense of or epistemological sense of this word than our folks psychological sense of it.
1782540	1793310	D	0.8838058710098267	But again, I think it also has some overlappings with the folk psychological sense of it as well.
1795920	1802000	B	0.6843858957290649	Great thought just to kind of restate that then say what I was planning to before.
1802070	1816020	B	0.5559965968132019	So here it's situational where people talk commonly about don't talk about beliefs like the eye, like where it believes it should be looking, people don't talk about that.
1816090	1835640	B	0.4950977861881256	But there might be another cognitive model that does align very well with an expressed felt cognitive belief and everything in between and bundles that are mixed like people who believe that they're accurate on a task but actually are low accuracy.
1836380	1842152	B	0.8509688973426819	So it's going to be very model specific, I think how it maps to different people's understanding.
1842296	1862800	B	0.6155568361282349	But that was what Ali's response kind of made me think about and that's why I didn't choose an example like Delusions to focus on because that is related to a felt or experienced or reported phenomena.
1865060	1884490	B	0.8419640064239502	Okay, so beliefs and priors and this question of disorders of inference which as Brock mentioned was like part of the early impetus in relating to studying for example, schizophrenia and fristin's earlier and continued work.
1888320	1904364	B	0.6196497082710266	Maybe there's a part of missing and maybe it's not the right context or way, but I think first we can narrowly interpret what they use to mean disorder and then have a more broad perspective.
1904412	1913036	B	0.7213973999023438	So I read this does not necessarily imply the inferential mechanisms flawed.
1913148	1924820	B	0.4981789290904999	Like the CPU has the right number of cycles, the machinery are not flawed, the car is operating at normal burning temperature.
1926380	1944430	B	0.7603223919868469	Given its generative model as specified hashtag chapter four it is doing appropriate variational, message passing, belief propagation, approximate Bayesian computation, Bayesian brain however you want to see it.
1944960	1954240	B	0.6491266489028931	Given what this person has experienced and their parameterization, it's not the case that they're making a fallacious estimate.
1955860	1961330	B	0.813690185546875	Now that's sort of in their situation where it's like, yeah, okay, but you're going to say that for whatever they do do.
1962420	1964912	B	0.7010931968688965	But that's kind of that tautology.
1965056	1990204	B	0.7032824754714966	It's like, right, that's our best estimate of the system as it is is just how it is in most of the studies reviewed in Table 9.1, which is to say the pathologies that are identified in Table 9.1, the inferential mechanism operates normally as per the previous meaning, but based upon a flawed generative model.
1990402	1992780	B	0.6722873449325562	And it's literally like there's the rub.
1993600	1996332	B	0.7211598753929138	Aberrant prior beliefs is like, what?
1996466	2004428	B	0.8755071759223938	And so that is where I would say there's two ways to zoom out from that and contextualize that statement.
2004524	2005516	B	0.5454474091529846	The first is aberrant.
2005548	2014560	B	0.7191101312637329	Prior beliefs are not intrinsically aberrant, whether they're subconsciously or consciously held.
2014720	2017540	B	0.6333123445510864	Belief in itself is not aberrant.
2017880	2025296	B	0.7944719791412354	But there's the niche and especially just the human cultural when we're talking about these felt beliefs.
2025488	2032040	B	0.7662912011146545	So in the niche, it's like there is ant colonies that were high foraging and low foraging.
2032620	2035764	B	0.6368225812911987	Those are not aberrances of foraging behavior.
2035892	2040940	B	0.8183422684669495	That's variation in foraging behavior with respect to variation in weather.
2041440	2045230	B	0.6521124243736267	And there was probably decades where one was better than the other.
2046720	2049100	B	0.7302083373069763	And that's why there's evolution.
2050720	2053476	B	0.689702570438385	So it's like, what's the purpose?
2053528	2078090	B	0.6302841305732727	Okay, so level one of aberrant is in a niche context and then especially for these expressively human experienced settings, that's where I think there's an opportunity, again, not saying that this paragraph is the right place in this textbook, but that's where it's important to also bring in.
2081660	2091000	B	0.7000845670700073	I'm not expert in this area, but like Foucault social descriptions of mental illness diagnosis.
2095260	2098110	A	0.616070568561554	Are only crazy if majority of people.
2101920	2105744	B	0.8598715662956238	This book, which I've mentioned on live streams before.
2105862	2108256	B	0.829143762588501	If I could just describe why I bring it.
2108278	2128068	B	0.834701657295227	Up and, like, how I think it's relevant to chapter nine is just like she's saying there's different methods and there's different theoretical interpretations and it's very clear and it's very pluralist.
2128244	2135416	B	0.6659150719642639	And though guided, everybody will also, I think, see like deeper aspects too.
2135438	2154290	B	0.8650190830230713	But just I studied with Helen in Stanford and she was just very like it was very interesting to see that coming from philosophy because in the scientific domains it's like not framed that way.
2158230	2180570	B	0.7914294600486755	We could have a generative model for these conditions and scenarios that integrates different types of information in a situational way with the sparse model and we could all work on that and then different people could have different data sets or different public or private data sets.
2184890	2222850	B	0.618595540523529	But that requires not just sort of like operational coordination and ontology driven design and other features, but it also requires basically personally held commitment to pluralism, at least in practice because there's too many methods and methods are too last mile to dictate models of phenomena.
2223430	2230610	B	0.8410816788673401	These are not measurements, but they're generated alongside and with and from measurements.
2233820	2266980	B	0.56901615858078	And so that's not descriptive statistics per se and that's a major difference in what you can do with the tail of two densities approaches these situations differently than one might see in a health journal showing the population level factors associated with a disease.
2268840	2279320	B	0.6876972913742065	This is a different kind of study but it could be done in a population context which has been done extensively in SPM and Neuroimaging.
2283310	2293070	B	0.8410959243774414	So you can still bring in features of the people and understand how different features of people are quite literally associated with the diagnosis.
2293810	2303140	B	0.8828675150871277	And that is seen as a label on a neurocognitive model, a generative model.
2303510	2305620	B	0.843966007232666	Now what's the generative model?
2306550	2322550	B	0.8799104690551758	Chapter Four chapter Six and then it's like, okay, we're just going to say that all of the people's ocular motor circuitry are structurally the same and we're going to explore how the two groups differ in the parameter estimates of these different features.
2322970	2328998	B	0.6081236004829407	Or you could say I'm studying a situation where this region has been severed.
2329174	2344590	B	0.8943358063697815	So the hypothesis I'm testing is actually whether this connectivity versus this connectivity is going to have this observable difference that's testing a structural hypothesis.
2345170	2352242	B	0.8853239417076111	So you could test parametric hypotheses within a generative model and or structural hypotheses across.
2352376	2361570	B	0.5205639004707336	But to kind of return to this point those are these two ways that the model can be quote in in their, you know, way aberrant or flawed.
2362010	2374806	B	0.6165521740913391	Quite literally it could be a parametric estimate that's aberrant which as discussed does not mean intrinsically bad number 4.2 or three 7.3 is not bad.
2374988	2380220	B	0.8667479157447815	It's about its relationship within a niche and a fitness context and like all of that.
2380590	2393098	B	0.8660435080528259	So it could be a parametric aberrance with respect to a state estimator like an expectation on a variable or precision.
2393274	2395390	B	0.8331423401832581	So that's the LaPlace approximation.
2395890	2413414	B	0.8431568741798401	It's the hierarchical predictive processing approximation structure, expectation variance, mean variance, mode variance, center of the path, width of the path those are the two.
2413452	2416706	B	0.7770664691925049	Gaussian distribution takes two parameters.
2416818	2419270	B	0.8144275546073914	LaPlace approximation takes two parameters.
2420090	2434570	B	0.8107552528381348	Hierarchical message passing has these kinds of variables or it can be structural differences which is quite open ended.
2437490	2438910	B	0.7135538458824158	How is this useful?
2440930	2445978	B	0.8723381757736206	An example is provided here, potentially.
2446074	2450260	B	0.5487815141677856	For example, let's just say that there's people who do and do not have a certain situation.
2451990	2454930	B	0.8059471845626831	Some moths go towards the light and some don't.
2455430	2475320	B	0.8291031718254089	Is that because of differences in their behavior where the statistical variance gets loaded onto differences in their realized preference vector or their habit e?
2476190	2499486	B	0.8572828769683838	So there could be a behavioral setup that would do the fibonacci pattern of variability or just whatever it happened to be which is extensively described in SPM how to time different behavioral stimuli, some natural observational setting or controlled behavioral setting both of which have pros and cons.
2499668	2503406	B	0.8705253005027771	Both quote controlled conditions in the lab and natural observation.
2503438	2504674	B	0.6889119744300842	They have pros and cons.
2504872	2515080	B	0.8344117403030396	But the idea in model based analysis is that those can be the observables and you may or may not control the observations, but at least you know what they are.
2515530	2537840	B	0.9031993746757507	And then you can explore whether differences in behavior end up being reflected in statistically differentiable parameter estimates for prior preference or for prior d or for the fixed form term e.
2543070	2544010	B	0.6077884435653687	Ali.
2547710	2548170	D	0.46103888750076294	Yes.
2548240	2565294	D	0.8556042909622192	For what it's worth, I also put a quote from this recent paper by Car Pristen from a couple of months ago, which is, by the way, is a great overview of this whole field of computation psychiatry through the lens of active inference.
2565422	2576126	D	0.8399077653884888	And here it provides a clear and explicit definition of the term belief used in this area.
2576248	2581880	D	0.7833887934684753	So I thought that might be helpful here.
2582410	2586354	D	0.6620932221412659	The kind of beliefs here are Bayesian beliefs.
2586402	2590860	D	0.8430365324020386	And then it goes on to describe what Bayesian beliefs mean.
2594600	2595350	B	0.918424665927887	Awesome.
2600120	2602420	A	0.7367233037948608	Um cool.
2602570	2605492	A	0.8830171227455139	That was super clarifying.
2605636	2609610	A	0.8087934255599976	I think it makes a lot of sense.
2616000	2616750	A	0.6050543785095215	Maybe.
2617360	2627310	A	0.6988883018493652	I don't know if the answer was already given exactly, or I'll just ask it a different, slightly different way about.
2628900	2632140	C	0.8016862273216248	The separation between the generative model and the priors.
2632220	2651810	C	0.8356325626373291	Here in this toy frog jumping example are the aberrant priors, the frog apple distribution on that belief beforehand, and.
2654780	2655288	A	0.5665541291236877	The.
2655374	2662212	C	0.5487974882125854	Observation, the updating, all of that being the generative model is working as intended.
2662356	2690480	C	0.8551509380340576	But it's just that the priors were so out of whack that when the generative model kind of observes and steps through or whatever, that it just ends up in the wrong state and or is the likelihood model I'm not sure.
2690550	2693060	A	0.8397602438926697	I guess in the toy example there.
2693210	2696132	C	0.8004037141799927	How you could have how it would be.
2696266	2699908	A	0.5911772847175598	I guess the part that's missing there is the context, maybe.
2699994	2715240	A	0.7433214783668518	But yeah, like if I had a prior belief that was different than your prior belief, but we didn't have a disorder or whatever, I would imagine our priors wouldn't be the same, but they would still kind of converge.
2718160	2723992	A	0.6623804569244385	So I'm not sure how you get there without an outside prior belief.
2724056	2727390	A	0.8990443348884583	Or something about the environment that's totally wrong, right?
2728000	2734850	A	0.8511788845062256	Or something else about the environment or the likelihood model is somehow really wrong.
2736580	2737440	B	0.6345008015632629	Yeah, thanks.
2737510	2738544	B	0.6860782504081726	Great question.
2738662	2741008	B	0.5556814074516296	Those are exactly the two greetings.
2741024	2741396	B	0.894907534122467	Welcome back.
2741418	2744308	B	0.7320789694786072	Ali those are exactly like the two.
2744394	2746980	B	0.8108585476875305	Let me unpack that in the pathology example.
2747130	2759288	B	0.8432395458221436	So society expects us and or we actually die or have consequences with when the frog when it's a frog, you're not supposed to eat it.
2759374	2761610	B	0.8819512724876404	Apples are okay to eat.
2763580	2767580	B	0.869882345199585	Now, if you turn down too many apples, you starve.
2768080	2770792	B	0.9446507096290588	If you eat too many frogs, you die.
2770936	2785916	B	0.6140681505203247	So you have to balance your type one and type two false positive and false negative error rates in this eat, do not eat, jump, does not jump observation setting.
2786108	2789504	B	0.7798393368721008	Because these are idle classifiers here.
2789622	2793488	B	0.8781856894493103	But we can imagine these as more action oriented classifiers.
2793664	2795072	B	0.7285488247871399	Apples are for eating.
2795136	2797300	B	0.5590054392814636	Frogs are for throwing.
2800120	2824350	B	0.6201602220535278	Now, society and or the niche defines aberrant behavior as one that does not conform to expectations or to a thriving state in the niche while recognizing that all parameter values that are actually observed are at least there.
2825360	2827756	B	0.6836151480674744	Now, that doesn't mean you want them to be there, they want them.
2827778	2832880	B	0.8099571466445923	But also then that's a belief an agent has about another system that also exists.
2835700	2842580	B	0.7780336737632751	So there would be many ways to see that aberrant behavior.
2843400	2863290	B	0.7526218295097351	So if you like speaking to the person who discards too many apples, maybe they believe that they have a strong prior on frogs 0.5 or their likelihood, which is also a parametric belief, is different.
2864540	2866480	B	0.8411182761192322	They have a different action mapping.
2866580	2867612	B	0.7049172520637512	Now that here's the question.
2867666	2872940	B	0.7561911344528198	Now you put them in the lab and you have jumping and non jumping frogs and apples.
2874320	2893030	B	0.8400915265083313	And then you can use those experimental techniques on a computational model narrowly to determine do they simply expect frogs to appear but they know what frogs and apples do?
2894520	2908360	B	0.5498447418212891	Or do they have an appropriate belief about the likelihood a priori of frogs and apples but they have an incorrect action mapping?
2909020	2912276	B	0.5423465371131897	I hope I said it correctly, but I think you get the picture.
2912388	2936450	B	0.5787749290466309	The point is you could then use experiments to determine whether groups of people who had effective or ineffective behavior and then if somebody, for example, had an overestimate of frogs, then the message to pass is frogs are not as common as you think.
2938260	2962490	B	0.6079606413841248	Whereas if there was somebody with a variance interpretable variance here in the likelihood model difference with some other preference again, keeping in mind that that's still like it's like outlier detection and attracting which isn't even necessarily a preferable thing.
2962940	2973390	B	0.7114204168319702	But just pointing out, just statistically, this person's model would be updated by saying it's not known that apples can jump, but actually they can.
2974080	2978670	B	0.6845094561576843	It's possible that apples jump, so next time something jumps, don't just throw it away.
2980580	3002688	B	0.63965904712677	And those are very different points to convey and this is a very simple example, but it's helpful to think about because those are the kinds of parameter if this was your model of schizophrenia, those would be your levers or your knobs.
3002864	3011876	B	0.5537089109420776	If you had a more complex model and the models are more complex, then there's a lot more associated challenge.
3011908	3012840	B	0.7963464856147766	Yeah, Brock.
3013500	3017500	A	0.8500492572784424	Oh, no, I was just going to say that was really helpful.
3021040	3025070	A	0.8813585042953491	I don't even know, it seems good.
3025520	3030160	A	0.5727372169494629	Obviously I know next to nothing, I guess, about psychiatry.
3033060	3036690	C	0.6069563031196594	I don't know how you do it though, without something like this.
3039220	3056344	C	0.7897198796272278	I guess they were doing something kind of like this approximately informally to some degree, in some cases, but it would seem like just guessing almost, I don't know.
3056382	3062410	A	0.5624101161956787	Yeah, well, I could infer about schizophrenia without something like this.
3064380	3080976	B	0.6297077536582947	Well, to simplify a complex area, there was studies that associated either a biomolecule or a genetic variant, but especially biomolecules in brain imaging directly to a diagnosis which from a health record perspective exactly makes sense.
3080998	3082240	B	0.7741411924362183	It's the data you have.
3082390	3088944	B	0.8416091799736023	However, this is an unobserved model of.
3088982	3121996	B	0.7921333312988281	The phenomena itself that can incorporate measurements, including health records, and that allows the development of an actual understanding of the ethiology and the causation and therefore could be used to justify traditionally understood interventions or suggest different behavior and interventions like maybe you don't just, oh, you're low on this, just take this thing that raises it.
3122018	3124076	B	0.49518662691116333	It's like maybe that's not exactly the.
3124098	3131088	C	0.8216904997825623	Move, but just more precise affordance to do that.
3131254	3136096	B	0.9301348328590393	Yeah, including the counterfactuals, like in a clinical setting that is going to be quite interesting.
3136198	3147700	B	0.8450295925140381	All right, let's just in the last minutes look to Ten Minsky active inference as a unified theory of sentient behavior.
3149480	3171640	B	0.49166804552078247	If anybody wants to do a little like Twitter thread recap on the sentient the sentience discourse over the last few weeks, I'm sure we can find some funny tweets, but with the sentient pong playing neurons and just wave after wave of semantic discussion about the term sentient.
3173280	3182288	B	0.913097620010376	In this chapter we wrap up active inference main theoretical points from the first part of the book, chapter one through five, and its practical implementations from the second part six through ten.
3182374	3183840	B	0.7475849986076355	Then we connect the dots.
3184260	3193010	B	0.7789322733879089	We abstract away from the specific active inference models discussed in previous chapters to focus on integrative aspects of the framework benefits of activ.
3194760	3209208	B	0.8545181155204773	Summary of the book 10.2, chapter one, chapter two, chapter three, chapter 4567 and eight, no love chapter nine.
3209374	3210810	B	0.6359970569610596	You just read it.
3211500	3212890	B	0.7511391639709473	Go read it again.
3214620	3215684	B	0.5038023591041565	Connecting the dots.
3215732	3226810	B	0.5872547030448914	The integrative perspective on active inference, interestingly framed in terms of some early dennett work.
3229440	3260390	B	0.6374049782752991	A call to model the whole iguana a complete cognitive creature, perhaps a simple one, and an environmental niche for it to cope with rather than single dimensional analyses and measurements on a complex system just too much like a Whirlwind to just measure and oh, well, brain regions correlated with this and this region is correlated with that.
3260760	3278730	B	0.5518837571144104	Those kinds of structural and behavioral, morphological any given measurement does not ever get at the underlying causal structure unless you specifically model it that way.
3280640	3288350	B	0.5106133818626404	Active inference helps with that issue by offering a first principle account of the ways in which organisms solve their adaptive problems.
3289040	3306980	B	0.6302417516708374	So by framing attention, memory, anticipation under common computational framings and a unified imperative it's possible to integrate different kinds of phenomena.
3307320	3313300	B	0.7213299870491028	Memory attention can be thought of as optimizing the same objective.
3317900	3321240	B	0.8257014155387878	Ten four predictive brains, predictive minds and predictive processing.
3323500	3327480	B	0.6901092529296875	Kind of a provocative selection.
3332160	3352864	B	0.803753674030304	Active inference creatures, or their brains are probabilistic inference machines connecting to the kind of focus of predictive mind, predictive brain, predictive processing, coding area, which is like the real time and anticipatory, especially real.
3352902	3390504	B	0.8866801857948303	Time unfolding anticipation self evidencing prediction predictive processing specifically Livestream 43 perception section 10.5 helmholtz perception is unconscious inference then how that was implemented in Bayesian brain Bayesian brain the inactive turn the pragmatic turn action control bringing action in end of Livestream.
3390552	3391612	B	0.8242025375366211	43 End of paper.
3391666	3417960	B	0.8107289671897888	43 Idea motor, cybernetic, some other ontologies that are kind of like easily functionally equated with parts of active idea motor theory, cybernetics, optimal control theory, utility, decision making, Bayesian decision, reinforcement, learning, planning as inference, behavior and bounded rationality.
3418700	3441500	B	0.7784302234649658	And then, specifically, what free energy brings into bounded rationality, valence, emotion, motivation, homeostasis alistasis and terraceptive processing, attention, salience, epistemic dynamics, rural learning, causal inference, fast generalization and next steps social machine learning and robotics.
3442160	3443032	B	0.6355422735214233	Summary.
3443176	3449230	B	0.7321406006813049	Lord of the Rings, we are confident that you will continue to pursue active inference in some form.
3452720	3454272	B	0.7642363905906677	So it's a quite long chap.
3454376	3455670	B	0.7700473070144653	There's a lot in it.
3457400	3459670	B	0.8160449266433716	We'll talk about it next time.
3460120	3461540	B	0.8902812004089355	Thank you, fellas.
3462520	3462880	B	0.5137446522712708	Bye.

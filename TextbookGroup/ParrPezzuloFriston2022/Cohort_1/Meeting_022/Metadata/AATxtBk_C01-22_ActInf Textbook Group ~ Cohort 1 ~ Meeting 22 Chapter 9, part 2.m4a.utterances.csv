start	end	speaker	confidence	text
650	1246	A	0.66803	You.
1428	77070	B	0.92917024691358	Hello. Thanks for all joining. It's active textbook group cohort one meeting 22, chapter nine on October 28. Okay, we're in the second discussion on chapter nine, model based data analysis. Would anyone like to add anything they want or mention how the generative model discussion from the previous 1 hour relates to model based data analysis? How does chapter four relate to chapter nine? What does chapter nine address? What do chapters six and nine address that chapter four doesn't?
84810	112840	A	0.9075497297297296	Um, I mean, six is about thinking about applying it in, like, incredibly kind of simplified step by step kind of way. And nine is very specifically about answering scientific questions with it and doing that with.
116170	121370	C	0.8141816666666667	This metabasian. It's like almost nested.
124460	142556	A	0.8697265	Sort of schema of active inference. So, I mean, that's the through line, I guess, but I'm not sure more.
142578	144030	C	0.9205260000000001	Than that what to say.
144740	148320	B	0.8551483333333333	That's really insightful. Thank you, Ali.
151540	255440	D	0.8997430769230766	Yeah, actually, I would compare it to a kind of learning how to solve problems in, I don't know, mathematics or other physics or other areas. For example, in chapter four, we acquired some necessary knowledge or prerequisite about how to think about the phenomena or the problem we're dealing with. Chapter six kind of outlines a roadmap a roadmap about how exactly we should proceed to solve that problem in a kind of formulaic way. And chapter nine, actually, in chapter nine, we get our hands dirty and try to solve some of the actual or empirical problems with the tools or techniques we had learned in chapter four and six. So in this case, I somehow think of chapter nine as a kind of case of study well, kind of case of studies or a kind of solved problem section of this textbook. And, yeah, I don't know how much that captures their nature.
256420	588670	B	0.9197772962962978	That's great. Wow. Great thoughts. I totally agree. Let's look at the table of contents. Chapter one, special overview. Chapter mirror symmetry. Same, but different with ten. So now in the middle eight chapters, two chapters on the low road and the high road, helping two different approaches which can be developed, like in a lot more detail, to approach active inference from two fascinating and non controversial starting points Bayesian inference from the low road free energy principle and the repeated measurement persistence imperative. Chapter four is the first chapter on active inference. Active inference is about the generative models that are being specified and how they're being computed. After just one chapter on active inference, it goes to the area where the most research and modeling has been done in active inference, which is computational manalian neuroanatomy. One chapter on active, one chapter on the primary domain. First half of the book, second half of the book. This is the part where you're going to be in parallel, perhaps on your first, but perhaps not on your first reading. You're going to be doing playing through these applied aspects of modeling. First is the recipe for designing active inference models. In Chapter Six, as both of you have, very nicely set. Then the Figure 4.3 Dialectic discrete Continuous Time is revisited in two chapters because it probably it's one key difference. It's a minimum of two. So it prevents, like any like, oh, well, it has to be a PMDP. It's like, well, obviously not. So it keeps the space of generative modeling open and it allows for discussion of the very interesting topic of hybrid active inference nested models with continuous actuators, for example, but discrete decision making apparatus. So it just is like wanting to cover these two key types of models in a bit more detail than was addressed in Chapter Four. But this was the first possible moment to address it after 4.3. Just kidding. Maybe not. Then Chapter Nine, yeah, where we are now, which is just, okay, you've built your pure discrete time pure continuous time or hybrid generative model. If it's a toy model, you can just generate data and just do whatever you want. Or you might have data from sensors or real world or however that's in the data format that your generative model expected to receive, which was part of your design process, you could have determined what structure literally the data is going to be in. Is it going to be like, yes, data can be reshaped, but just broadly, is it like 256 sensors in a row of one? Or is it like eight by eight? Just those kinds of questions. So that when the data start coming in for the experiment, whether you generated the data from the generative model or whether you're using it in a purely recognition model capacity on real world data, what does it look like to do statistical modeling with these statistical models that we've been using? And then chapter ten is a review. It restates what each chapter does, provides some further thoughts, and just summarizes the unifying capacity of active inference modeling. Then there are the appendices Ali.
591190	747060	D	0.9021438709677416	Also one in my opinion, very interesting section of Chapter Nine is the section on models of false inference, section 9.7, in which there's an interesting definition of the disorders, I mean the mental disorders, and how we can actually computationally model those different disorders. Sorry. It says that here the disorder does not necessarily imply that the inferential mechanism is flawed. In most of the studies, the inferential mechanism operates normally but based on a flawed generative model. So that's basically what we had learned about how we can somehow update our beliefs or correct our perceptions based on either refining our generative model or modifying our generative model. Or somehow, sometimes we can just well, act upon those errors and try to reduce our uncertainties. But in this case, it's a very interesting point on the case of computational pathologies because oftentimes people ask, well, what does active inference actually explain? So I think this is a very good example of its very meaningful and significant application in any sense of the word. So if we can somehow computationally model these kind of disorders, we'll most probably have a much better understanding of its origin and the way to probable treatments of those disorders and many other applications can comes to mind. But yeah, I think this part of the chapter nine is something that was not addressed quite explicitly in the previous chapters and in my opinion is the most interesting part of this one.
749270	1216890	B	0.9299151417525788	Great. Awesome. Yeah. It reminds me of the table in chapter four, or sorry, chapter five with the neurotransmitters. So they were still at the organ systems level in chapter five. But five is very much related to nine, I guess in that sense as well, because it's like, here's what dopamine does, here's what acetylcholine does just at the computational model level. And then here it's like that computational model was fit, like between the phenomena, like attention or policy precision or however phenomena to a molecule in chapter five. Here, the phenomena is being connected to a real human clinical application. So it's not that we're saying or anyone is saying or anything like that, that the molecule is being directly linked to this paper, even if it's mentioned. The direct linkage approach is people in group X have more molecule, have less molecule, and even the neuroimaging approach can be seen as a variant of that, which is people in group X have more brain activation pattern. In computational pathology, those kinds of biochemical measurements and neuroimaging measurements can, and they are used. However, this type of computational modeling is referring to parametric modeling, where the parameters are related to the pathology. So it's not an a priori classification of those with and without. And then descriptive statistics showing that a biochemical or a brain imaging pattern are differentially expressed. That's descriptive statistics. It's not computational pathology. It might be precision pathology, but it could still be precision or big data or however, with just like purely doing data summaries, doing a random forest model to diagnose. These models have a cognitive structural hypothesis about the nature of these phenomena, like the ocular motor syndromes. Again, it's not just people with the diagnosis and without. It's a neurocognitive model of eye behavior. And then explanations can be made from clinical data by rendering various aspects of the generative model conditionally independent of others, ocular motor syndrome, such as intranuclear optipomilegias, may be induced. So if you just were diagnosing these as different conditions, maybe they have similar ideologies how the condition arises, maybe they have different, maybe they have multiple. But these two conditions computationally can be understood as parametric modulating in a computational model of ocular motor behavior. This isn't people with internuclear opto, DA, DA, DA have higher glucose or lower glucose or this brain region or this aspect of them, those are all absolutely valid approaches. That's like scientific methodological pluralism, which is like, it's okay that people want to do different approaches. Everybody's contributing to a common people who care about this and are creating high quality reproducible data, their data will always be useful and their perspectives are valid and we don't want to have a discourse space on inter nuclear optipomygia that's saying that one model is like simply the only correct model. This paper makes a positive contribution and points to some new ways of thinking about various pathologies and thinking beyond the ways that they've been worked on before, where one can imagine that before 2000 and so on, you had what health records. And you have some double blind experiments. And so you might do a meta analysis to find correlations with features of people and all of those things that we've been describing. But it was not plausible to actually implement computational models of these phenomena. Now, interestingly, that's something that can be worked on by researchers directly, and then somebody can a doctor could download Ocular motor syndromes programming, whatever it is, but this can be developed in a pure research context. And the Generative Model is like the interface for the kind of information that might not be available to the person developing the Generative Model because it relates to a lot of problems and issues about data availability. That's one kind of just side note, but dilly. Thanks for raising that point that I just wanted to emphasize and unpack, which is that this table, which is taking up two and a half pages of a pretty short book is describing. These are like the cases that can be brought up, which they're providing in a helpful table for what active inference describes. So if the principle of least action, gauge theory, et cetera, sophistication does not interest one, these are more pragmatic value. Now that will probably move the goalpost to show me where the computational modeling helped somebody with delusions and that's a fine place to want to go. But it's also a standard and an approach that it's not related to the substance of the theoretical development.
1223400	1228724	A	0.9616057142857143	I mean, I think you could probably the response to that question might be.
1228762	1325670	C	0.8926821232876709	Something like the difficulty of implementing that model in a way that provides actionable real time affordances to address the aberrant priors in a way that's causally. It's going to actually affect that system. We need either more compute or a particular kind of algorithmic medicine that doesn't exist yet or et cetera. But that's not a problem with the particular approach, it's just an insufficiency of ability to enact it, I guess. But I wanted to ask a question about specifically this aberrant prior beliefs. It says ultimately the pathology is a consequence of aberrant prior beliefs. It's not the Generative Model, it's aberrant prior beliefs. I think this is like Ellie's point and everything that you just went through is correct that this is kind of where the rubber really meets the road for its value in demonstrating or whatever. But I'm wondering also.
1327480	1334200	A	0.8955857142857143	To me that there's like an easy confusion thing there about beliefs and priors.
1336220	1338250	C	0.9904785714285713	If they just believed the right thing.
1339340	1343384	A	0.9206685714285715	Then they wouldn't have these that's not.
1343422	1345370	C	0.9621233333333333	Really what that means though, right?
1346880	1350620	A	0.9509736363636364	Is another way of saying this, like in the Ocular motor.
1352560	1383108	C	0.9557784210526317	Disorder case, like a kind of neurobiological bias in some kinds of cells or whatever, right. That's pathologically different than normal or whatever, like some receptor for a particular chemical or whatever, is that a kind of thing?
1383194	1388650	A	0.8754207692307693	That the way of saying that differently in that particular case that is.
1391420	1391784	D	0.95	More.
1391822	1412850	C	0.9060755555555556	Commensurate with what they're actually saying there. Or like not more commensurate, but just not relying on jargon and or domain knowledge specific for what they mean by beliefs and priors or is that not right?
1415920	1454810	B	0.902536231884058	Yeah, well, there's a lot there. Are you talking about the comparison of the terms beliefs and priors in relationship to the way people might naturally interpret such beliefs to be, and they may indeed be people's beliefs about the world in the conversational sense? Like some beliefs are just your retinal states but other states of the world, as if or really are your states on other things.
1456460	1501710	C	0.892206031746032	Right? Yeah, I guess I'm asking if beliefs is in some cases it is beliefs, but it's also another way of stating if another way of stating that is a structural systemic bias for one thing or another or whatever in that context, is that physical or informational kind of non folk psychology? Is that commensurate with what they mean by beliefs there?
1504640	1509330	B	0.8249266666666667	Okay, let's try to yeah, Ali, please, first, sorry.
1510340	1546590	D	0.9093487878787878	I'm sorry, but you mean the statistical use of the word belief? Because in statistics and in folk psychology, as far as I know, this term is used in quite different ways. So I guess that what you're talking about is the statistical sense of the word belief. And if it maps onto the folk psychological sense of this term or am I missing something here?
1550000	1553120	A	0.885402	Well, let's just take two steps back for a second.
1553190	1577800	C	0.9278867391304347	And just in the text it says at the bottom of page 185 here at the top of that yeah, it says the infertile mechanism operates normally but based on a flawed generative model. This means that ultimately pathology is a consequence of aberrant prior beliefs.
1578620	1579370	B	0.99	Yeah.
1580460	1591564	C	0.9464478571428571	And I'm asking a kind of just to explicate different whatever right there in.
1591602	1595576	A	0.9221440000000001	The context of psychological disorders, which is not necessarily pathology.
1595608	1609810	C	0.894242857142857	But I'm just you know, if you tell somebody, oh, yeah, it helps with or is the impetus or whatever for this whole thing was to think about specifically like schizophrenia and other but that.
1610980	1613956	A	0.9110828571428572	Immediately kind of puts it in that.
1613978	1621990	C	0.8907141666666666	Category or that context of definitions. And so just to kind of.
1624040	1626970	A	0.9367799999999998	Nuance the term their beliefs a little bit.
1628940	1629690	B	0.91	Yeah.
1630300	1684056	C	0.9298417171717172	Is it commensurate with the rest of the text to say in the case of this specifically of this oculine motor disorder category, like you're saying beliefs in the retina or something that is like cones, like being sensitive to something. Well, it's just what they are sensitive to and in this particular person they're just sensitive to or whatever the relevant structure is there. But it's sensitive to something. Right. And that's its belief in some sense. I'm not looking for what's the statistical definition or anything like that, but just is that completely off track or is.
1684078	1688410	B	0.9972226666666667	That I have a thought, but Ali, if you want to make a response please.
1690620	1793310	D	0.9080074999999996	Yeah. I think that this term belief and its different meanings in different contexts actually sort of bleed into each other and there cannot possibly be separated cleanly from each other. So in this particular case I believe that the word belief partly is this folk psychological sense of the word and partly it's based on the kind of philosophical epistemological sense of this word. Because especially in epistemology when philosophers talk about belief, usually they don't mean just the folk psychological sense of this term, although it relates to that concept, but it's much more nuanced than that. So in the case of Oculomotor pathology well, I believe that the word belief somehow can be interpreted in a little more abstract philosophical sense of or epistemological sense of this word than our folks psychological sense of it. But again, I think it also has some overlappings with the folk psychological sense of it as well.
1795920	2091000	B	0.9203114634146348	Great thought just to kind of restate that then say what I was planning to before. So here it's situational where people talk commonly about don't talk about beliefs like the eye, like where it believes it should be looking, people don't talk about that. But there might be another cognitive model that does align very well with an expressed felt cognitive belief and everything in between and bundles that are mixed like people who believe that they're accurate on a task but actually are low accuracy. So it's going to be very model specific, I think how it maps to different people's understanding. But that was what Ali's response kind of made me think about and that's why I didn't choose an example like Delusions to focus on because that is related to a felt or experienced or reported phenomena. Okay, so beliefs and priors and this question of disorders of inference which as Brock mentioned was like part of the early impetus in relating to studying for example, schizophrenia and fristin's earlier and continued work. Maybe there's a part of missing and maybe it's not the right context or way, but I think first we can narrowly interpret what they use to mean disorder and then have a more broad perspective. So I read this does not necessarily imply the inferential mechanisms flawed. Like the CPU has the right number of cycles, the machinery are not flawed, the car is operating at normal burning temperature. Given its generative model as specified hashtag chapter four it is doing appropriate variational, message passing, belief propagation, approximate Bayesian computation, Bayesian brain however you want to see it. Given what this person has experienced and their parameterization, it's not the case that they're making a fallacious estimate. Now that's sort of in their situation where it's like, yeah, okay, but you're going to say that for whatever they do do. But that's kind of that tautology. It's like, right, that's our best estimate of the system as it is is just how it is in most of the studies reviewed in Table 9.1, which is to say the pathologies that are identified in Table 9.1, the inferential mechanism operates normally as per the previous meaning, but based upon a flawed generative model. And it's literally like there's the rub. Aberrant prior beliefs is like, what? And so that is where I would say there's two ways to zoom out from that and contextualize that statement. The first is aberrant. Prior beliefs are not intrinsically aberrant, whether they're subconsciously or consciously held. Belief in itself is not aberrant. But there's the niche and especially just the human cultural when we're talking about these felt beliefs. So in the niche, it's like there is ant colonies that were high foraging and low foraging. Those are not aberrances of foraging behavior. That's variation in foraging behavior with respect to variation in weather. And there was probably decades where one was better than the other. And that's why there's evolution. So it's like, what's the purpose? Okay, so level one of aberrant is in a niche context and then especially for these expressively human experienced settings, that's where I think there's an opportunity, again, not saying that this paragraph is the right place in this textbook, but that's where it's important to also bring in. I'm not expert in this area, but like Foucault social descriptions of mental illness diagnosis.
2095260	2098110	A	0.9202528571428573	Are only crazy if majority of people.
2101920	2544010	B	0.924765145502647	This book, which I've mentioned on live streams before. If I could just describe why I bring it. Up and, like, how I think it's relevant to chapter nine is just like she's saying there's different methods and there's different theoretical interpretations and it's very clear and it's very pluralist. And though guided, everybody will also, I think, see like deeper aspects too. But just I studied with Helen in Stanford and she was just very like it was very interesting to see that coming from philosophy because in the scientific domains it's like not framed that way. We could have a generative model for these conditions and scenarios that integrates different types of information in a situational way with the sparse model and we could all work on that and then different people could have different data sets or different public or private data sets. But that requires not just sort of like operational coordination and ontology driven design and other features, but it also requires basically personally held commitment to pluralism, at least in practice because there's too many methods and methods are too last mile to dictate models of phenomena. These are not measurements, but they're generated alongside and with and from measurements. And so that's not descriptive statistics per se and that's a major difference in what you can do with the tail of two densities approaches these situations differently than one might see in a health journal showing the population level factors associated with a disease. This is a different kind of study but it could be done in a population context which has been done extensively in SPM and Neuroimaging. So you can still bring in features of the people and understand how different features of people are quite literally associated with the diagnosis. And that is seen as a label on a neurocognitive model, a generative model. Now what's the generative model? Chapter Four chapter Six and then it's like, okay, we're just going to say that all of the people's ocular motor circuitry are structurally the same and we're going to explore how the two groups differ in the parameter estimates of these different features. Or you could say I'm studying a situation where this region has been severed. So the hypothesis I'm testing is actually whether this connectivity versus this connectivity is going to have this observable difference that's testing a structural hypothesis. So you could test parametric hypotheses within a generative model and or structural hypotheses across. But to kind of return to this point those are these two ways that the model can be quote in in their, you know, way aberrant or flawed. Quite literally it could be a parametric estimate that's aberrant which as discussed does not mean intrinsically bad number 4.2 or three 7.3 is not bad. It's about its relationship within a niche and a fitness context and like all of that. So it could be a parametric aberrance with respect to a state estimator like an expectation on a variable or precision. So that's the LaPlace approximation. It's the hierarchical predictive processing approximation structure, expectation variance, mean variance, mode variance, center of the path, width of the path those are the two. Gaussian distribution takes two parameters. LaPlace approximation takes two parameters. Hierarchical message passing has these kinds of variables or it can be structural differences which is quite open ended. How is this useful? An example is provided here, potentially. For example, let's just say that there's people who do and do not have a certain situation. Some moths go towards the light and some don't. Is that because of differences in their behavior where the statistical variance gets loaded onto differences in their realized preference vector or their habit e? So there could be a behavioral setup that would do the fibonacci pattern of variability or just whatever it happened to be which is extensively described in SPM how to time different behavioral stimuli, some natural observational setting or controlled behavioral setting both of which have pros and cons. Both quote controlled conditions in the lab and natural observation. They have pros and cons. But the idea in model based analysis is that those can be the observables and you may or may not control the observations, but at least you know what they are. And then you can explore whether differences in behavior end up being reflected in statistically differentiable parameter estimates for prior preference or for prior d or for the fixed form term e. Ali.
2547710	2590860	D	0.9392367415730338	Yes. For what it's worth, I also put a quote from this recent paper by Car Pristen from a couple of months ago, which is, by the way, is a great overview of this whole field of computation psychiatry through the lens of active inference. And here it provides a clear and explicit definition of the term belief used in this area. So I thought that might be helpful here. The kind of beliefs here are Bayesian beliefs. And then it goes on to describe what Bayesian beliefs mean.
2594600	2595350	B	0.99968	Awesome.
2600120	2627310	A	0.8885747222222222	Um cool. That was super clarifying. I think it makes a lot of sense. Maybe. I don't know if the answer was already given exactly, or I'll just ask it a different, slightly different way about.
2628900	2651810	C	0.890729655172414	The separation between the generative model and the priors. Here in this toy frog jumping example are the aberrant priors, the frog apple distribution on that belief beforehand, and.
2654780	2655288	A	0.82	The.
2655374	2690480	C	0.9334610714285713	Observation, the updating, all of that being the generative model is working as intended. But it's just that the priors were so out of whack that when the generative model kind of observes and steps through or whatever, that it just ends up in the wrong state and or is the likelihood model I'm not sure.
2690550	2693060	A	0.9957885714285714	I guess in the toy example there.
2693210	2696132	C	0.92475875	How you could have how it would be.
2696266	2734850	A	0.9143968965517241	I guess the part that's missing there is the context, maybe. But yeah, like if I had a prior belief that was different than your prior belief, but we didn't have a disorder or whatever, I would imagine our priors wouldn't be the same, but they would still kind of converge. So I'm not sure how you get there without an outside prior belief. Or something about the environment that's totally wrong, right? Or something else about the environment or the likelihood model is somehow really wrong.
2736580	3012840	B	0.9296778638941411	Yeah, thanks. Great question. Those are exactly the two greetings. Welcome back. Ali those are exactly like the two. Let me unpack that in the pathology example. So society expects us and or we actually die or have consequences with when the frog when it's a frog, you're not supposed to eat it. Apples are okay to eat. Now, if you turn down too many apples, you starve. If you eat too many frogs, you die. So you have to balance your type one and type two false positive and false negative error rates in this eat, do not eat, jump, does not jump observation setting. Because these are idle classifiers here. But we can imagine these as more action oriented classifiers. Apples are for eating. Frogs are for throwing. Now, society and or the niche defines aberrant behavior as one that does not conform to expectations or to a thriving state in the niche while recognizing that all parameter values that are actually observed are at least there. Now, that doesn't mean you want them to be there, they want them. But also then that's a belief an agent has about another system that also exists. So there would be many ways to see that aberrant behavior. So if you like speaking to the person who discards too many apples, maybe they believe that they have a strong prior on frogs 0.5 or their likelihood, which is also a parametric belief, is different. They have a different action mapping. Now that here's the question. Now you put them in the lab and you have jumping and non jumping frogs and apples. And then you can use those experimental techniques on a computational model narrowly to determine do they simply expect frogs to appear but they know what frogs and apples do? Or do they have an appropriate belief about the likelihood a priori of frogs and apples but they have an incorrect action mapping? I hope I said it correctly, but I think you get the picture. The point is you could then use experiments to determine whether groups of people who had effective or ineffective behavior and then if somebody, for example, had an overestimate of frogs, then the message to pass is frogs are not as common as you think. Whereas if there was somebody with a variance interpretable variance here in the likelihood model difference with some other preference again, keeping in mind that that's still like it's like outlier detection and attracting which isn't even necessarily a preferable thing. But just pointing out, just statistically, this person's model would be updated by saying it's not known that apples can jump, but actually they can. It's possible that apples jump, so next time something jumps, don't just throw it away. And those are very different points to convey and this is a very simple example, but it's helpful to think about because those are the kinds of parameter if this was your model of schizophrenia, those would be your levers or your knobs. If you had a more complex model and the models are more complex, then there's a lot more associated challenge. Yeah, Brock.
3013500	3030160	A	0.8767175862068964	Oh, no, I was just going to say that was really helpful. I don't even know, it seems good. Obviously I know next to nothing, I guess, about psychiatry.
3033060	3056344	C	0.9236148780487807	I don't know how you do it though, without something like this. I guess they were doing something kind of like this approximately informally to some degree, in some cases, but it would seem like just guessing almost, I don't know.
3056382	3062410	A	0.8107309090909091	Yeah, well, I could infer about schizophrenia without something like this.
3064380	3124076	B	0.935575701754386	Well, to simplify a complex area, there was studies that associated either a biomolecule or a genetic variant, but especially biomolecules in brain imaging directly to a diagnosis which from a health record perspective exactly makes sense. It's the data you have. However, this is an unobserved model of. The phenomena itself that can incorporate measurements, including health records, and that allows the development of an actual understanding of the ethiology and the causation and therefore could be used to justify traditionally understood interventions or suggest different behavior and interventions like maybe you don't just, oh, you're low on this, just take this thing that raises it. It's like maybe that's not exactly the.
3124098	3131088	C	0.8455866666666667	Move, but just more precise affordance to do that.
3131254	3462880	B	0.913810678899083	Yeah, including the counterfactuals, like in a clinical setting that is going to be quite interesting. All right, let's just in the last minutes look to Ten Minsky active inference as a unified theory of sentient behavior. If anybody wants to do a little like Twitter thread recap on the sentient the sentience discourse over the last few weeks, I'm sure we can find some funny tweets, but with the sentient pong playing neurons and just wave after wave of semantic discussion about the term sentient. In this chapter we wrap up active inference main theoretical points from the first part of the book, chapter one through five, and its practical implementations from the second part six through ten. Then we connect the dots. We abstract away from the specific active inference models discussed in previous chapters to focus on integrative aspects of the framework benefits of activ. Summary of the book 10.2, chapter one, chapter two, chapter three, chapter 4567 and eight, no love chapter nine. You just read it. Go read it again. Connecting the dots. The integrative perspective on active inference, interestingly framed in terms of some early dennett work. A call to model the whole iguana a complete cognitive creature, perhaps a simple one, and an environmental niche for it to cope with rather than single dimensional analyses and measurements on a complex system just too much like a Whirlwind to just measure and oh, well, brain regions correlated with this and this region is correlated with that. Those kinds of structural and behavioral, morphological any given measurement does not ever get at the underlying causal structure unless you specifically model it that way. Active inference helps with that issue by offering a first principle account of the ways in which organisms solve their adaptive problems. So by framing attention, memory, anticipation under common computational framings and a unified imperative it's possible to integrate different kinds of phenomena. Memory attention can be thought of as optimizing the same objective. Ten four predictive brains, predictive minds and predictive processing. Kind of a provocative selection. Active inference creatures, or their brains are probabilistic inference machines connecting to the kind of focus of predictive mind, predictive brain, predictive processing, coding area, which is like the real time and anticipatory, especially real. Time unfolding anticipation self evidencing prediction predictive processing specifically Livestream 43 perception section 10.5 helmholtz perception is unconscious inference then how that was implemented in Bayesian brain Bayesian brain the inactive turn the pragmatic turn action control bringing action in end of Livestream. 43 End of paper. 43 Idea motor, cybernetic, some other ontologies that are kind of like easily functionally equated with parts of active idea motor theory, cybernetics, optimal control theory, utility, decision making, Bayesian decision, reinforcement, learning, planning as inference, behavior and bounded rationality. And then, specifically, what free energy brings into bounded rationality, valence, emotion, motivation, homeostasis alistasis and terraceptive processing, attention, salience, epistemic dynamics, rural learning, causal inference, fast generalization and next steps social machine learning and robotics. Summary. Lord of the Rings, we are confident that you will continue to pursue active inference in some form. So it's a quite long chap. There's a lot in it. We'll talk about it next time. Thank you, fellas. Bye.

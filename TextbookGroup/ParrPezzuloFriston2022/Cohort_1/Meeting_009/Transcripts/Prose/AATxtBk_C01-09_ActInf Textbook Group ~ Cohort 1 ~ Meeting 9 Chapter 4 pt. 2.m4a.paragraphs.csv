start	end	paragNum	speaker	confidence	startTime	wordCount	text
250	20320	1	A	0.63	00:00	36	Of these questions. I think we talked about the oh, this is chapter two. Let's go to chapter four. Okay, so we talked about belief, policy, state. I'm not sure if we talked about this question.
21730	50860	2	A	0.99987	00:21	62	In the discussion of active inference in POMDP t belief updating about policies, we find that the posterior that minimizes the free energy does the posterior at time t. Oh, we did discuss this. Yeah, we did discuss this last time. So this question, I think we did not discuss pi being a policy or a model last time. Or did we?
51230	75118	3	A	1.0	00:51	70	I think we didn't get to this. Yeah, I don't think we discussed that. All right, so we can open it up and we'll start here, I guess, with the most upvoted. So the question reads, what is pi? On page 69, the authors write, at each time step, the current state is conditionally dependent on the state at the previous time and on the policy pi currently being pursued.
75214	113854	4	A	0.99669	01:15	97	Then on page 71, they write, thus we can interpret the priors of equation 4.6 combined with the likelihood of equation 4.5 as expressing a model pi of a behavioral sequence. So which is it, policy or model? And then they suggest a rewrite of the sentence. They say that, thus we can interpret the priors of equation 4.6 combined with the likelihood of equation 4.5 and the transition probabilities b sub tau pi as expressing a model for a behavioral sequence where the model is a function of policy pi. And then there's some discourse here.
114052	127400	5	A	0.88099	01:54	34	So in this reframing, can we say that the model simulates the agent in the environment as if it had taken the actions in the policy? And I don't know who asked that question.
129530	139900	6	A	0.83	02:09	32	Oh, Eric, that was you. That was your question. And then does anyone want to maybe take a stab at answering that question? Or we can continue reading what was written here.
142990	188230	7	A	0.9787	02:22	125	So what's written here says that on page 69, the authors also say policies here may be thought of as indexing alternative trajectories or sequences of actions that could be followed. On page 71, they define the likelihood in equation four five as a matrix A that expresses the probability of an outcome. They describe the priors of equation 4.6 as the prior over the initial state vector D, and beliefs about how the state at one time transitions to the state at the next time, matrix B. And they also say that the transitions are conditionally dependent on the policy chosen. Because of this conditional dependence, we can see how the policy influences the model and why the authors may use the terms interchangeably.
192990	244646	8	A	0.98963	03:12	69	So does anyone have any comments there's? I guess I would say that that answer in discourse kind of agrees with what I proposed, which is that strictly speaking, we should treat the pi as policy, but it behaves as a model. Once it's executed, it's implemented. Then the model has become a function of the policy that's been we've seen so far. So I'd say that's compatible interpretation.
244758	278360	9	C	0.99962	04:04	74	So I think strictly speaking, it would be better if the text was consistent and called Pi policy and then say, yeah, the model is a function of Pi. That'd be my interpretation. Yeah, I definitely agree with that. I think that the model is policy specific, but they could do a better job of I mean, the variables as we've seen are so ambiguous anyway, so it could definitely be a lot better.
284180	333570	10	B	0.71	04:44	126	And by the way, about the topic of consistency, actually I gave some thoughts about the issue we were discussing in yesterday's math learning session and I also consulted some other papers and active inference and in all of them the notation for matrices and vectors is used consistently, as in almost every linear algebra textbook. So I'm seriously beginning to suspect that every instance of a matrix or a vector not written in Bold face is a typo either in chapters or appendices because otherwise I really cannot find any justification behind using two different types of notation. So I'm going to use this assumption as my prior for the rest of the book, unless someone has any belief updating observation she wants to share.
336740	365080	11	A	0.7898	05:36	76	So just to summarize, for the people that weren't there yesterday, we had a big debate about what does the Bold typeface signify? And it wasn't so much a debate, but just like trying to come to a mutual understanding. So it's this question here. Does bold lettering mean this is a matrix, but seemingly also many non bold letters are a matrix? Or does bold lettering mean sufficient statistics of as on these page examples?
365580	393590	12	A	0.95	06:05	65	And it really was yeah, we couldn't come to a single unified answer there because there are many instances where it seems like it should be a matrix, but it's not bold and there are many instances where sufficient statistics are used but it's also not bold. So yeah, we couldn't really conclude that, but I think I'll lead it. That's a good assumption there.
412470	434986	13	A	0.60748	06:52	66	So we also talked about this a little bit last time, but we didn't really get fully into this question. What is the use of categorical distributions in equation 4.5? The second line is supposed to explain the cat notation, but I have problems to understand the advantage of the notation. Could someone explain it in simple words? Does anyone want to try to explain that?
435088	436490	14	A	1.0	07:15	6	I can pull up the equation.
443600	480280	15	B	0.99905	07:23	60	Well, I think every regular matrix and vector is by definition an array of ordered elements. But in a categorical distribution we don't necessarily assign any specific order to the elements. So the reason behind using the categorical distribution might be to just jettison any orderness in the elements and treat them as just an unordered set of the probabilities.
482880	499870	16	C	1.0	08:02	20	I wrote an answer to this one if you want to pull that up, so you can just read that's.
517460	519200	17	C	0.99581	08:37	6	So I hope this is helpful.
521540	580100	18	C	0.75489	08:41	133	It's basically the way I see it is you have the math, the math notation there, but when you need to actually implement that, you need to carry it out, then you need to turn that into operations you can perform. And so when you say that A is this categorical object, that means that you're turning it into an actual matrix with elements. And so the ordering is important, actually, because the ordering of the elements in the matrix says how that matrix is going to apply to the objects it applies to, which are the belief state S, and then your observation O. So basically it says the way you carry out inferring what O will be the probability of O given a belief state is you do a matrix multiply.
582200	610780	19	C	0.67	09:42	47	And because in this example, the states are not continuous, they're categorical, then you need a categorical matrix element in order to do map from the categories of belief states to the categories of observation states. And those are both distributions that gets transformed by the matrix.
613600	620770	20	A	0.50666	10:13	17	Thanks, Eric, that was super helpful. Does anybody have any further question or comment on this one?
625310	636270	21	B	0.99997	10:25	19	Sorry, Eric, but isn't the categorical notation special case of multinomial distribution? Categorical distribution special cases of multinomial distribution?
639090	703074	22	B	0.97	10:39	109	I don't know that. Yeah, because that was my understanding that the main difference between any categorical distribution and well, I mean, for instance, of course you're right that in any matrix we necessarily have some ordered elements in order for us to be able to compute the math on those matrices. But on the other hand, for categorical distributions, at least that was my understanding that we don't necessarily treat them as ordered elements, such as, I don't know, the ordered vectors or ordered matrices as the normal matrix. Because if that was the case, well, we could just designate this distribution as just a regular matrix. Right.
703192	720230	23	B	0.92993	11:43	25	So I don't get what's the reason behind using the notation here, cat A or cat, instead of just using the. Name of the matrix?
723150	726220	24	A	0.59967	12:03	9	Steven, can you mute you're super loud over there.
739450	740200	25	A	0.99831	12:19	1	Sorry.
742970	744840	26	C	0.99799	12:22	4	Yeah, I guess so.
748170	775940	27	C	1.0	12:28	61	The mathematical notation doesn't have an ordering. It just says distribution is related to another distribution through this relation. And when you get to the mechanics of how do you do that? That's where you care that you have the matrix with the vector ordering. So, yeah, I don't know what cat A actually means other than it's just describing it.
777030	782180	28	C	0.99	12:57	19	I don't think it's like an operator, I think it's just saying what it is. Does that make sense?
793790	796480	29	A	0.58009	13:13	9	Steven, if you're trying to talk, we can't hear.
798770	799520	30	D	0.53766	13:18	1	What.
815860	837160	31	A	0.99	13:35	48	I feel like Steven is trying to share an idea with us, but is unable to communicate effectively through the technological affordances he's been given at the time. Like the gain on your mic is way up or something, so it's picking up more background noise than you're.
882020	885810	32	A	0.99922	14:42	11	Just looking at this equation. Can you guys hear me okay?
893560	933920	33	A	0.60117	14:53	57	Yeah. Okay, so just looking at this equation, the second part, it's like the, it's like an ordered matrix A-I-J where the probability of the observations are equal to index I and the states equal to index j, something like that. I don't know, it looks like it is ordered. Yeah, the matrix itself is ordered. Yeah.
933990	944580	34	A	0.99	15:33	24	And so the second line is actually by how the authors are defining this cat A. So cat A represents this ordered matrix AIG?
946920	953780	35	A	0.99606	15:46	10	Yeah, that's at least what it says in the textbook.
978190	989680	36	A	0.92183	16:18	28	So it says the likelihood expressing the probability of observations conditioned on time tau given states s conditioned on time tau is equal to the categorical distribution A.
1005990	1043760	37	B	0.99496	16:45	95	Sorry. Yaku, go ahead. Sorry, my internet is lagging a bit, so there might be a delay and I might break off. But I just want to comment that I think that the cat notation is really just to reinforce the notion that it's in discrete time. But I think it's kind of redundant in this case because since we know that we are in discrete time, we could have just written a bolt to signify that it's a matrix and where it's a matrix, it's probably not going to be a continuous distribution anyway.
1045890	1051330	38	E	0.99927	17:25	13	So I think cat bold A and bold A is pretty much interchangeable.
1055470	1094760	39	A	0.89914	17:35	99	So I think that some of the new math is using some of the affordances from category theory more like some of the newer things. And I think the forward direction that Karl and Thomas and some of the people that really helped derive and advance some of this math, I think that they're going to be leveraging category theory methods like the renormalization group and the pullback attractor more in the future. And so that might be also part of the reason that they are moving toward this categorical notation. So just a little maybe foreshadowing there too.
1100810	1128290	40	B	0.69	18:20	59	May I just make one additional note about this categorical distribution? Well, the thing that confused me about this concept of categorical distribution is the Wikipedia's definition of it, because here it says there is no innate underlying ordering of these outcomes, but numerical labels are often attached for convenience in describing the distribution. So that was my confusion.
1135940	1154580	41	A	0.97	18:55	46	I don't think it would be the first time that we've seen traditional mathematical notation slightly and somewhat abused in active inference. Probably also won't be the last. But definitely I think it's a point that we can put forward to the authors for additional clarification.
1157020	1166490	42	A	0.99706	19:17	26	If there's no further anybody has anything else to say about the categorical distributions here, if not, we can move on to maybe the next question.
1175470	1210530	43	A	0.99344	19:35	87	So this next question reads in message passing, is there a decay of information as the distance between the variable X and individual Markov blanket constituents increases? Is implementation of information decay? In message passing an option for model implementation? I think that this is a super interesting question and it's something that I've thought about before a lot. But I think, yeah, let me open it up to anyone and see if they want to address it before we can get into some of the discourse.
1215750	1249860	44	C	0.98599	20:15	83	Well, doesn't that all depend upon the chain of loss between one stage of the message passing and another? And if you have no loss, then you don't have any decay. But the more stages you have where you get a little bit of diffusion or loss, then you're going to have decay, just a function of the parameters. Well, so are messages passed within the same Markov blanket or between things that are partitioned with their own Markov blanket? That's another question.
1264640	1304200	45	B	1.0	21:04	56	I also agree with Eric, as I mentioned yesterday. I also believe that, well, this information loss is not accounted for at least inherently in active inference formalism because it depends on the specific situations that those information propagate because otherwise we couldn't have any general or all encompassing formalism for accounting for these information loss.
1307580	1343190	46	A	0.99857	21:47	99	So we see in this box they talk about Markov blankets and the message passing. So here is variational message passing. This involves messages from all constituents of the Markov blanket of X, including the parents via the conditional probability of X given its parents and the children. The latter depends on the conditional probability of the children of X given all of their parents, which include X. Note the expectation includes the children and parents of the children as parents of the children x, we divide by Q of X to ensure the expectation includes the blanket only.
1344600	1387268	47	A	0.98808	22:24	100	So I think the messages are like all of the things inside of the blanket, send a message to something else in another blanket. At least that's how I interpret this way of active inference. And so it would make sense that from within the blanket, does everyone have the same is there like a high degree of mutual information shared in the Markov blanket? I think conditional dependence is what qualifies as conditional independence is what establishes the markup blanket. So within constituents under the blanket, do they all have all of the same information all of the time?
1387354	1434624	48	A	0.84639	23:07	91	That's something that's kind of confusing to me. One thing I'd point out about figure 4.3, so in the previous chapters we have this picture of there being the world and your model of the world and your model of the world is a Markov blanket itself and it interfaces with the world through the observation variables and the action variables. This picture doesn't have that breakdown. So it's got state there, but it doesn't say is that state all internal state? Is it external state or a mixture of them?
1434662	1466190	49	C	1.0	23:54	96	And it apparently is mixture because you've got policy that can affect state, which means that you're going to have action, is going to be part of the state transition as well as and then you have the observations coming out the bottom. But I think if we're going to talk about Markov blankets, then I would like to see this picture be exploded into the state of the machine versus the state of the external world and how those interact with one another. Because that's what the markup blanket does. Is it compartmentalizes the information?
1472930	1520378	50	A	0.69366	24:32	95	Well, that is the POMDP model and Figure 4.3 and this really Figure 4.4 makes me scratch my head quite a bit. This is the image that is going to represent message passing in a Bayesian framework. And so the caption says on the right dependencies between different variables in the belief updating scheme outlined in the main text. Intuitively, current beliefs about states under each policy at each time are compared with those that would be predicted given beliefs about states at other times, this is one. And current outcomes to calculate prediction errors.
1520474	1541320	51	A	0.99999	25:20	53	These errors then drive updating and beliefs, this is two. Given beliefs about states under each policy we can then calculate the gradients of the expected free energy. This is three. These are combined with the outcomes predicted under each policy omitted from the figure. To compute beliefs about policies, this is four.
1542410	1549960	52	A	1.0	25:42	21	And then using a Bayesian model average, we can then compute posterior beliefs about states averaged over policies. This is five.
1555050	1571440	53	A	0.99	25:55	44	And this kind of leads us into the next question. But I am not able to really interpret this Figure 4.4 in any better way. So if anybody has like a plain English description of how this works, I would love to hear it.
1588300	1621570	54	A	0.94775	26:28	79	We can talk a little bit about a question that was raised about this figure and also about this squiggle sigma because there's a great I don't know, we kind of had a hard time figuring out the equation and the squiggle sigma and what that actually stands for. So we could move to that discussion because I think it's maybe related to this message passing discussion. But let's read the discourse here first on the message passing question.
1624280	1649804	55	A	0.9777	27:04	57	So in the discourse, what is the mechanism for signal decay? So this is information decay, implementation of information decay. Is the message a thing behaving as an active inference agent itself or a special piece of information not behaving as an active inference system itself? Is there a blanket impedance mismatch? I think Brock contributed this.
1649842	1675148	56	A	0.99375	27:29	71	I'm not sure if he's here today. And then message passing as described through active inference usually is a hierarchy, not a lateral transfer like within constituents in the blanket. So the key thing here is how you define the markup blanket. Either there's a partition between particles, cells or whatever, or there's no partition in blanket and everything under the blanket is conditionally dependent. Does conditional dependence preclude message passing?
1675244	1696410	57	A	0.61534	27:55	61	Our message passed within nodes in the same blanket. I have not seen this and would love if someone pointed me to some references. So that was me. I wrote the last part because I haven't seen message passing under the same Markov blanket. It's always been like from one Markov blanket partitioned object to a different Markov blanket partitioned object.
1698460	1713330	58	A	0.79845	28:18	41	But that moves us into if anybody has any comments, feel free. Well, I would say message passing is used to do the inference. So it's within a markup blanket. That's pretty common, I think the way message passes is used.
1715460	1781028	59	A	0.99834	28:35	184	So here, as it's shown in Figure 4.4, although this is totally beyond me to describe here, this is a temporal message passing. So it's like from one time point to another time point and then on the left it depicts the hierarchical, like a hierarchical expansion collapsing over time steps but that a higher level network might predict the states and policies at the lower level and use these to draw inferences about the context in which these occur. So the way that I've always seen the message passing used is from one time point to another or from one layer of a hierarchy to the next layer of a hierarchy and I've not seen it passed within constituents under the same Markov blanket. So Eric, if you have references that depict some kind of message passing in active inference that's not through time steps or through hierarchical levels, I would love to see that because I have not seen it. Well, I would just go back to box 4.1 where they talk about the variational message passing and how you get information.
1781194	1812050	60	C	0.99	29:41	91	And I would say within a blanket you've got various nodes and they have this hierarchical parent child relations and in order to do inference about, hey, what's our belief in one of these parents? You got to kind of go up and down and say well, what are the children of that and what do the other parents think? So that's all within a single Markov blanket, the parents and children, the hierarchical relation and the message passing happens to circulate that information within the blanket. That's how I read.
1815380	1847400	61	B	1.0	30:15	60	And I also put a link here to a paper by Champion at all in which they have actually explicitly defined the active inference, or better say, reformulated the active inference formalism according to variation of message passing. And if you just give me a second, I can find exactly the place where they have stated this in plain English.
1868700	1908360	62	B	0.96	31:08	124	You can continue with the other questions if you like, but I need a moment to check this paper. Sure, yeah, I'll check it out also and yeah, thank you for that. So like when I'm reading this variational message passing, this involves messages from all constituents of the Markov blanket of X. So the message is coming from all the constituents in the blanket, but it doesn't say where the message is going. So it does make sense, I guess, that it's an interchangeable or within would be a better word than from within all constituents of the Markov blanket because I was reading it as like the message is coming from everything under that blanket, which I guess is an incorrect interpretation.
1912490	1916040	63	A	0.99675	31:52	13	So thank you Eric for pointing that out because I was like what?
1930380	1967408	64	A	0.99594	32:10	108	So this is a really hard question that I looked to try to answer. Actually a bunch of us looked yesterday a lot. So it says in equation four point ten, the sigma variable, squiggle sigma, we'll call it whatever, I'm not sure how to say it is used to describe the difference between the natural log of observations conditioned on policy and preferences. What is the function of this variable? It also comes up in figure 4.4, the message passing figure we're just looking at, and it would be great to have a verbal description of this figure, what other papers or equations use this squiggle sigma?
1967504	2002050	65	A	0.5	32:47	83	And we looked at length, like through the entire textbook, through the other chapters it refers to like we'll unpack this later in chapter seven. We looked through chapter seven, we looked through the appendices, we saw a variable that almost looks like that, but I think it's a gamma. It looks like a little bit more fancy than this squiggle sigma and could not find that at all. So we went to unpack equation 410. And so here is the equation itself.
2004500	2011780	66	A	0.49709	33:24	19	I'm not sure how to pull this up next to the actually maybe I'll put it up here, 419.
2015000	2043584	67	A	0.77	33:35	65	And this equation four point ten is a rewriting of equation 4.7 in linear algebraic form. So we tried to kind of unpack this a little bit. So the first line states the prior probability for each policy. This is that pi sub zero is equal to the soft maps function sigma times the negative expected free energy G. And so that's this first line.
2043702	2104284	68	A	1.0	34:03	160	The next line is the expected free energy conditioned on policy g sub pi is equal to the entropy or negative expected log probability h times the states conditioned on policy and time, which is S sub pi times tau or both of those sub pi and tau plus the observations conditioned on policy and time. This O sub pi times tau times the beliefs conditioned on policy and time, which is a squiggle sigma maybe sub pi times tau. So we were kind of unsure if beliefs is correct here. We kind of pulled that out of the legend for figure 4.4. But if anybody knows, and just to maybe unpack that a little bit more, the beliefs conditioned on policy and time, the squiggle sigma, sub pi times tau is equal to the difference between the observations conditioned on policy and time o sub pi times tau and preferences conditioned on time C sub pi or sub tau.
2104332	2121240	69	A	0.99791	35:04	44	So we were unsure if belief is correct here also. And so if anybody knows if beliefs is the difference between observations and preferences, that would be great to have some kind of feedback or input here because we couldn't really reach a conclusion.
2159770	2231500	70	C	0.99751	35:59	136	Well, I guess one tiny step toward figuring this out, I would say, is that that funny squiggle thing there has to do with the C. We've got, we've got this probability of observations given C and C is this mysterious object that expresses preferences which has also been under explained. So if you look at the two terms of the expected free energy G as a function of functional policy in 4.7, and then look at how it looks like in four point ten, that sigma thing there, we've got the dot, which I guess is a multiplier by observation. So that's the same as that's like saying your probability of observation given C. So that's, I guess, a matrix multiplier version of this log probability of sequence of observations given your preference prior C.
2250490	2308430	71	A	0.67365	37:30	143	Yeah, so they do unpack. It a little bit more in equation 4.7 or it seems to be like maybe we could extrapolate from 4.7 if this is beliefs, because it is this natural log of observations minus the natural log of preferences. But up in the top that just the free energy is the entropy or maybe the dot product of the entropy times the states plus the observations times or the dot product of the observations and maybe beliefs here I can't recall a mathematical representation of belief ever using this squiggle sigma before. And I looked through the recent Ryan Smith paper and I looked even at the message passing paper by Friston and I was not able to really get any additional references to this squiggle sigma at all. So why do you say belief as opposed to preferences?
2308930	2339800	72	A	0.94464	38:28	69	Well, so it says here that the squiggle sigma is equal to the difference between the natural log of the observations conditioned on policy and time and the natural log of the preferences at a certain time. So the sigma is defined by the natural log of the preferences, but it's more than just the preferences. Does that make sense? Eric? Yeah, but that's not claim as belief, right?
2340170	2353450	73	C	0.91834	39:00	21	That's observations versus preferences, right. From expected deviation or how much risk is being taken relative to the policy that's applied.
2356290	2378820	74	D	0.98	39:16	58	The dot product is between the observation and that wiggle sigma, right, which is in turn like, okay, this is what I've observed, this is what I expected. So it's like a divergence between two. So how much more risk am I taking and how far is this pushing me away from my free from minimizing free energy?
2381030	2382420	75	D	0.9997	39:41	4	So maybe it's something.
2384650	2389720	76	C	0.99995	39:44	15	Whereas belief is wrapped up in the s because that's your model of the world.
2392510	2404800	77	A	0.9761	39:52	37	So risk is maybe how the squiggle sigma is defined. But then if you look into this figure 4.4 like that's, that's why we used belief. So here, let's, let's I'll, I'll pull it up right now.
2428990	2452986	78	A	0.99062	40:28	60	So I'm not sure how easy this is to see. But before we get to number one here, we have like on the right hand side the second to lowest level is states conditioned on policy and time. I think I'm going to read it off this bigger screen. Yeah. States conditioned on policy and time at different time steps.
2453018	2488378	79	A	0.99855	40:53	86	So like the current time is in the middle, the forward time step is on the right and the backward time step is on the left. And so that's what we start with. So intuitively, current beliefs about states under each policy at each time are compared with those that would be predicted given beliefs about states at other times. So maybe beliefs is this f and that's kind of what Eric was saying because it's defined earlier. This epsilon here is a prediction error, right?
2488464	2512100	80	A	0.9523	41:28	65	So what we come to after step one, it says and current outcomes to calculate prediction errors. That's what we arrive at after number one. The errors then drive updating in the beliefs. That's number two. So here we go from the current state at the present time to a prediction error that drives the belief updating at this forward time step after number two.
2512790	2532118	81	A	0.8	41:52	45	And then it says given beliefs about states under each policy, we can then calculate the gradients of expected free energy. Three. So what gives us this gradient of expected free energy? Is it this s sub tau plus one? Or is that a tau?
2532214	2547120	82	A	1.0	42:12	36	I think that's a tau. Yeah. So is it a future state not conditioned on policy? Or here we get to this squiggle sigma. So it's a squiggle sigma conditioned on policy and time plus one.
2547650	2559540	83	A	0.99917	42:27	31	So it looks like a belief update there, which is why belief made sense. So is that a risk update? Is that a possible interpretation there in this message passing figure?
2565590	2615350	84	C	0.99928	42:45	89	Well, they say it's a gradient, which means it's saying how much do we have to change our policy, I think in order to get our objectives yeah. Gradient makes sense to me. It's how much information you've gained, right? So if you actually look at the change in the state with respect to so I apply some force, or let's just take a simple example. If you have an actuator that just applies force and all it does is it moves forward and has to follow a trajectory.
2615690	2639390	85	D	0.96476	43:35	65	So the error is at each time step. If it's a deterministic system, you expect it to follow a straight line and it's deviating up way. So that's epsilon. So you have to update, okay, this is how far up I am from the state that I'm supposed to be. So I need to apply slightly less force or maybe force in a different direction.
2643650	2677914	86	D	0.90115	44:03	88	It's also how much I have to overcorrect in the future depending on what I've done currently, right? It's not just if I've applied some force right now, I might deviate in the opposite direction and I have to come back, so I have to figure out how much force to apply. So that's why I said risk or. Yeah, gradient, you would have to take the change with respect to the state change with respect to the error. So yeah, that would be one interpretation of it.
2677952	2683740	87	D	0.9	44:37	16	Right, so it's a gradient of the state change with respect to what you have done.
2686210	2715842	88	A	0.84078	44:46	73	So I also heard the term information gain in there. Yeah, I'm just generally saying. So in neural networks, for example, we would have some sort of loss function and we would calculate the weight change with respect to the change of the loss function. Right. So, like, I have a weight vector and then this particular example is giving me a certain amount of gradient and I need to minimize this gradient.
2715906	2732780	89	D	0.99994	45:15	50	So I need to change the weights in turn so the gradient actually interfaces between this loss function and all these weights. So effectively, the gradient passes on information as to how much the weight has to change in order to minimize that loss function. That's why I said information.
2736450	2751410	90	B	0.99	45:36	26	And also, as a side note, I think I found this sigma in Ryan Smith's paper too, and it is defined as the expected prediction error.
2753510	2787520	91	A	0.84643	45:53	67	So the expected prediction error versus the actual prediction error. And they distinguish it from epsilon in the Ryan Smith paper because I did look at that, but I didn't find it in there. Yeah, it's in equation 27 under the section outcome prediction errors. You it may be that the expected means it's an expectation over the Q distribution, which is your distribution over belief states.
2808590	2825662	92	A	0.99765	46:48	43	Yeah, that's great. Ali and I and some others worked on this yesterday a lot and it's great that it kept you up late at night, clearly, Ali, so you went digging around for some more information. That's awesome. Yeah, great. Okay, perfect.
2825796	2832580	93	A	0.98358	47:05	15	Well, that resolves that very well. And then let's get into maybe the next question.
2845530	2870320	94	A	0.68152	47:25	53	In equation 4.16, the X has a dot over. It changed through time but not the Y data. So there's some discourse on this. Or maybe we should go to the next question, actually, because it's maybe more related. So, Figure 4.6 uses an epsilon for prediction error as described in equation 4.21.
2870770	2916886	95	A	0.99998	47:50	108	Is this predictive processing framing a part of the active inference model or is this presented for contrast to illustrate the similarities and differences between active inference and predictive processing? So this is equation 4.1 and this is the epsilon, but I think we see it way before equation 4.21. Like it's even there. In equation 4.4, it says this predictive coding schema is part of the active inference model illustrating the hierarchical structure of predictions and beliefs. The authors say one way to think about this is as if we had equipped a predictive coding scheme with classical reflex arcs at the lowest level of the hierarchy.
2916918	2939250	96	A	1.0	48:36	57	And they give this reference in this setting, active inference is just predictive coding plus reflex arcs. Does anybody know what a reflex arc is? Because I'm a little bit lost with respect to that or have any comments on this prediction area? Yeah, reflex arc is basically the stimuli that doesn't go through the sensory cortex.
2950000	2952640	97	A	0.99	49:10	8	I thought it was like a mathematical construct.
2959510	2971382	98	C	0.99998	49:19	25	Maybe you think of it as a control system. No thinking. It's just like you bang your knee. Your foot goes up. That makes sense.
2971436	2987660	99	A	1.0	49:31	47	And it says, we minimize free energy through action. And the only part of the free energy that depends on action is the lowest level of prediction error. In this hierarchical schema, action fulfills descending predictions by minimizing the error between the predicted and observed sensory data.
2999540	3004500	100	A	0.99999	49:59	10	Any additional comments here about predictive coding and active inference?
3024190	3032080	101	A	0.77	50:24	26	Or we can look at this equation 4.16. The x has the dot over. It changed through time, but not the y data. Y is this.
3034290	3056150	102	A	0.93	50:34	62	And here the discourse says, the top equation depends on f of x and v, which is a deterministic function describing how a hidden state changes over time. The bottom equation depends on g of x and v, which describes how data are generated from a single hidden state. The tilde indicates change through time. The dot indicates the first order derivative.
3061690	3065900	103	A	0.99714	51:01	9	Any comments here on this equation or your question?
3100790	3127100	104	A	0.93535	51:40	81	Maybe we can ask one more question, or maybe a couple more. So here it says on page 63, it says specifically, the Bayesian brain helps us frame the problems that an agent engaging in active inference must solve broadly. These are the problem of inferring states of the world perception and inferring a course of action planning. Other than perception and action planning, are there other tasks or challenges that the brain or organisms engage in? How would we know?
3128590	3132480	105	A	0.9806	52:08	15	There's no discourse here, but let's open it up and see what you guys think.
3153730	3165634	106	C	0.96567	52:33	25	Yeah, people wonder, hey, what's on that star out there? That's neither? I don't know. Is that inferring State of the world? I guess not.
3165672	3166530	107	C	0.99971	52:45	1	Planning.
3169610	3180600	108	D	0.78	52:49	28	And it could just be a hallucination of generative model. You're not really measuring anything, you're just measuring yourself over and over again. That could be one thing.
3199690	3210090	109	A	0.55736	53:19	23	Okay, so here there's another question for each of the graphical models in Figure 4.2. What is an intuitive example for each structure?
3230040	3239800	110	D	0.75	53:50	17	One would be disease diagnosis. So you have symptom that is causative or predictive of some condition.
3248550	3251620	111	A	0.7	54:08	13	I think that was the example that they gave in the text, right?
3254630	3257140	112	A	0.8	54:14	9	Or did we talk about this last week maybe?
3262250	3270520	113	B	0.70953	54:22	22	Yeah, I actually don't get the question because each of them we have an example, at least an example in the text.
3280310	3281540	114	A	0.98234	54:40	3	Yeah, that's true.
3285300	3289490	115	A	0.51	54:45	16	And I think we're going to maybe see more examples as we go through the book.
3300580	3316360	116	B	0.98767	55:00	38	But just in case, if anyone wants to see some more examples actually, that paper, I put the link in the chat, realizing Active Inference in Variation Message Passing contains some more examples for each of these diagrams.
3322640	3349540	117	A	1.0	55:22	62	This is a fun last question. Maybe it says this reiterates that active inference uses two constructs, variational free energy and expected free energy, which are mathematically related but play distinct and complementary roles. In your own words, what are the definitions of variational free energy and expected free energy? And what role do they play individually or together in active inference?
3365890	3396860	118	D	0.99454	56:05	66	My view of expected free energy was given a certain action that might be taken. It might produce this or reduce the free energy or something, or it talks about some future reduction on free energy or something like that. Variational free energy is more objective. I don't know how to put it. It's like, okay, actual kinetic energy versus perceived kinetic energy kind of thing.
3398670	3427220	119	D	0.99952	56:38	62	So if you're rolling down a hill, like rolling down a hill, we can actually measure the kinetic energy at the bottom of the hill, right? If you put the ball at the top, expected free energy would be like using a model to calculate that and then finding out actually corresponds if you look the model. At least that's my view.
3439070	3469800	120	A	0.8343	57:19	93	So I don't know. I'm probably like I think about this in a lot of different ways, but I think about variational free energy. I mean, this is one way to think about it, is maximizing the trade off between epistemic value and pragmatic value right now and expected for free energy is like maximizing that difference at some point in the future. So you can plan ahead. It might be cold out, so I'm going to take a jacket or like it's cold right now, I'm going to put my jacket on.
3472170	3485950	121	A	0.99967	57:52	36	That's how I think about the difference between those two. So it's ten. Thanks, everyone, for coming. I hope I didn't muddle this up too much. Trying to be substitute teacher for Daniel, and it's ten.
3486020	3502140	122	A	0.99971	58:06	47	So we're going to have tools right now in this room. But if anybody wants to continue discussing these ideas and gather, you're welcome to migrate up to one of the different spaces. Yeah, and I think we'll stop the recording now, Alex, if you haven't already.

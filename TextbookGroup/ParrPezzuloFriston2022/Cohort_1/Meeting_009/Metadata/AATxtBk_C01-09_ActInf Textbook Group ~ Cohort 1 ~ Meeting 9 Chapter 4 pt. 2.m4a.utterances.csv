start	end	speaker	confidence	text
250	53200	A	0.9293545714285711	Of these questions. I think we talked about the oh, this is chapter two. Let's go to chapter four. Okay, so we talked about belief, policy, state. I'm not sure if we talked about this question. In the discussion of active inference in POMDP t belief updating about policies, we find that the posterior that minimizes the free energy does the posterior at time t. Oh, we did discuss this. Yeah, we did discuss this last time. So this question, I think we did not discuss pi being a policy or a model last time. Or did we? I think we didn't get to this.
53650	55614	B	0.9981871428571429	Yeah, I don't think we discussed that.
55732	203290	A	0.9483681766381763	All right, so we can open it up and we'll start here, I guess, with the most upvoted. So the question reads, what is pi? On page 69, the authors write, at each time step, the current state is conditionally dependent on the state at the previous time and on the policy pi currently being pursued. Then on page 71, they write, thus we can interpret the priors of equation 4.6 combined with the likelihood of equation 4.5 as expressing a model pi of a behavioral sequence. So which is it, policy or model? And then they suggest a rewrite of the sentence. They say that, thus we can interpret the priors of equation 4.6 combined with the likelihood of equation 4.5 and the transition probabilities b sub tau pi as expressing a model for a behavioral sequence where the model is a function of policy pi. And then there's some discourse here. So in this reframing, can we say that the model simulates the agent in the environment as if it had taken the actions in the policy? And I don't know who asked that question. Oh, Eric, that was you. That was your question. And then does anyone want to maybe take a stab at answering that question? Or we can continue reading what was written here. So what's written here says that on page 69, the authors also say policies here may be thought of as indexing alternative trajectories or sequences of actions that could be followed. On page 71, they define the likelihood in equation four five as a matrix A that expresses the probability of an outcome. They describe the priors of equation 4.6 as the prior over the initial state vector D, and beliefs about how the state at one time transitions to the state at the next time, matrix B. And they also say that the transitions are conditionally dependent on the policy chosen. Because of this conditional dependence, we can see how the policy influences the model and why the authors may use the terms interchangeably. So does anyone have any comments there's?
204850	260990	C	0.9046451578947367	I guess I would say that that answer in discourse kind of agrees with what I proposed, which is that strictly speaking, we should treat the pi as policy, but it behaves as a model. Once it's executed, it's implemented. Then the model has become a function of the policy that's been we've seen so far. So I'd say that's compatible interpretation. So I think strictly speaking, it would be better if the text was consistent and called Pi policy and then say, yeah, the model is a function of Pi. That'd be my interpretation.
262370	278360	A	0.917831219512195	Yeah, I definitely agree with that. I think that the model is policy specific, but they could do a better job of I mean, the variables as we've seen are so ambiguous anyway, so it could definitely be a lot better.
284180	333570	B	0.914942936507936	And by the way, about the topic of consistency, actually I gave some thoughts about the issue we were discussing in yesterday's math learning session and I also consulted some other papers and active inference and in all of them the notation for matrices and vectors is used consistently, as in almost every linear algebra textbook. So I'm seriously beginning to suspect that every instance of a matrix or a vector not written in Bold face is a typo either in chapters or appendices because otherwise I really cannot find any justification behind using two different types of notation. So I'm going to use this assumption as my prior for the rest of the book, unless someone has any belief updating observation she wants to share.
336740	436490	A	0.9385276525821592	So just to summarize, for the people that weren't there yesterday, we had a big debate about what does the Bold typeface signify? And it wasn't so much a debate, but just like trying to come to a mutual understanding. So it's this question here. Does bold lettering mean this is a matrix, but seemingly also many non bold letters are a matrix? Or does bold lettering mean sufficient statistics of as on these page examples? And it really was yeah, we couldn't come to a single unified answer there because there are many instances where it seems like it should be a matrix, but it's not bold and there are many instances where sufficient statistics are used but it's also not bold. So yeah, we couldn't really conclude that, but I think I'll lead it. That's a good assumption there. So we also talked about this a little bit last time, but we didn't really get fully into this question. What is the use of categorical distributions in equation 4.5? The second line is supposed to explain the cat notation, but I have problems to understand the advantage of the notation. Could someone explain it in simple words? Does anyone want to try to explain that? I can pull up the equation.
443600	480280	B	0.8987518333333331	Well, I think every regular matrix and vector is by definition an array of ordered elements. But in a categorical distribution we don't necessarily assign any specific order to the elements. So the reason behind using the categorical distribution might be to just jettison any orderness in the elements and treat them as just an unordered set of the probabilities.
482880	610780	C	0.939195436893203	I wrote an answer to this one if you want to pull that up, so you can just read that's. So I hope this is helpful. It's basically the way I see it is you have the math, the math notation there, but when you need to actually implement that, you need to carry it out, then you need to turn that into operations you can perform. And so when you say that A is this categorical object, that means that you're turning it into an actual matrix with elements. And so the ordering is important, actually, because the ordering of the elements in the matrix says how that matrix is going to apply to the objects it applies to, which are the belief state S, and then your observation O. So basically it says the way you carry out inferring what O will be the probability of O given a belief state is you do a matrix multiply. And because in this example, the states are not continuous, they're categorical, then you need a categorical matrix element in order to do map from the categories of belief states to the categories of observation states. And those are both distributions that gets transformed by the matrix.
613600	620770	A	0.8918435294117648	Thanks, Eric, that was super helpful. Does anybody have any further question or comment on this one?
625310	718886	B	0.918158053691275	Sorry, Eric, but isn't the categorical notation special case of multinomial distribution? Categorical distribution special cases of multinomial distribution? I don't know that. Yeah, because that was my understanding that the main difference between any categorical distribution and well, I mean, for instance, of course you're right that in any matrix we necessarily have some ordered elements in order for us to be able to compute the math on those matrices. But on the other hand, for categorical distributions, at least that was my understanding that we don't necessarily treat them as ordered elements, such as, I don't know, the ordered vectors or ordered matrices as the normal matrix. Because if that was the case, well, we could just designate this distribution as just a regular matrix. Right. So I don't get what's the reason behind using the notation here, cat A or cat, instead of just using the.
718908	720230	D	0.9394075	Name of the matrix?
723150	740200	A	0.869342	Steven, can you mute you're super loud over there. Sorry.
742970	782180	C	0.9454582142857143	Yeah, I guess so. The mathematical notation doesn't have an ordering. It just says distribution is related to another distribution through this relation. And when you get to the mechanics of how do you do that? That's where you care that you have the matrix with the vector ordering. So, yeah, I don't know what cat A actually means other than it's just describing it. I don't think it's like an operator, I think it's just saying what it is. Does that make sense?
793790	796480	A	0.8733655555555555	Steven, if you're trying to talk, we can't hear.
798770	799520	D	0.53766	What.
815860	825510	A	0.9234867857142858	I feel like Steven is trying to share an idea with us, but is unable to communicate effectively through the technological affordances he's been given at the time.
826440	837160	C	0.8850835	Like the gain on your mic is way up or something, so it's picking up more background noise than you're.
882020	929100	A	0.9371052459016392	Just looking at this equation. Can you guys hear me okay? Yeah. Okay, so just looking at this equation, the second part, it's like the, it's like an ordered matrix A-I-J where the probability of the observations are equal to index I and the states equal to index j, something like that. I don't know, it looks like it is ordered.
930260	932880	B	0.94069	Yeah, the matrix itself is ordered.
933460	989680	A	0.9241726984126984	Yeah. And so the second line is actually by how the authors are defining this cat A. So cat A represents this ordered matrix AIG? Yeah, that's at least what it says in the textbook. So it says the likelihood expressing the probability of observations conditioned on time tau given states s conditioned on time tau is equal to the categorical distribution A.
1005990	1008962	B	0.9756024999999999	Sorry. Yaku, go ahead.
1009016	1051330	E	0.9253502884615382	Sorry, my internet is lagging a bit, so there might be a delay and I might break off. But I just want to comment that I think that the cat notation is really just to reinforce the notion that it's in discrete time. But I think it's kind of redundant in this case because since we know that we are in discrete time, we could have just written a bolt to signify that it's a matrix and where it's a matrix, it's probably not going to be a continuous distribution anyway. So I think cat bold A and bold A is pretty much interchangeable.
1055470	1094760	A	0.928388888888889	So I think that some of the new math is using some of the affordances from category theory more like some of the newer things. And I think the forward direction that Karl and Thomas and some of the people that really helped derive and advance some of this math, I think that they're going to be leveraging category theory methods like the renormalization group and the pullback attractor more in the future. And so that might be also part of the reason that they are moving toward this categorical notation. So just a little maybe foreshadowing there too.
1100810	1128290	B	0.9250003389830508	May I just make one additional note about this categorical distribution? Well, the thing that confused me about this concept of categorical distribution is the Wikipedia's definition of it, because here it says there is no innate underlying ordering of these outcomes, but numerical labels are often attached for convenience in describing the distribution. So that was my confusion.
1135940	1210530	A	0.9256615723270437	I don't think it would be the first time that we've seen traditional mathematical notation slightly and somewhat abused in active inference. Probably also won't be the last. But definitely I think it's a point that we can put forward to the authors for additional clarification. If there's no further anybody has anything else to say about the categorical distributions here, if not, we can move on to maybe the next question. So this next question reads in message passing, is there a decay of information as the distance between the variable X and individual Markov blanket constituents increases? Is implementation of information decay? In message passing an option for model implementation? I think that this is a super interesting question and it's something that I've thought about before a lot. But I think, yeah, let me open it up to anyone and see if they want to address it before we can get into some of the discourse.
1215750	1237470	C	0.9567986440677965	Well, doesn't that all depend upon the chain of loss between one stage of the message passing and another? And if you have no loss, then you don't have any decay. But the more stages you have where you get a little bit of diffusion or loss, then you're going to have decay, just a function of the parameters.
1238210	1249860	A	0.9340795833333334	Well, so are messages passed within the same Markov blanket or between things that are partitioned with their own Markov blanket? That's another question.
1264640	1304200	B	0.9270333928571429	I also agree with Eric, as I mentioned yesterday. I also believe that, well, this information loss is not accounted for at least inherently in active inference formalism because it depends on the specific situations that those information propagate because otherwise we couldn't have any general or all encompassing formalism for accounting for these information loss.
1307580	1389990	A	0.9503276328502415	So we see in this box they talk about Markov blankets and the message passing. So here is variational message passing. This involves messages from all constituents of the Markov blanket of X, including the parents via the conditional probability of X given its parents and the children. The latter depends on the conditional probability of the children of X given all of their parents, which include X. Note the expectation includes the children and parents of the children as parents of the children x, we divide by Q of X to ensure the expectation includes the blanket only. So I think the messages are like all of the things inside of the blanket, send a message to something else in another blanket. At least that's how I interpret this way of active inference. And so it would make sense that from within the blanket, does everyone have the same is there like a high degree of mutual information shared in the Markov blanket? I think conditional dependence is what qualifies as conditional independence is what establishes the markup blanket. So within constituents under the blanket, do they all have all of the same information all of the time? That's something that's kind of confusing to me.
1391320	1466190	C	0.947361284916201	One thing I'd point out about figure 4.3, so in the previous chapters we have this picture of there being the world and your model of the world and your model of the world is a Markov blanket itself and it interfaces with the world through the observation variables and the action variables. This picture doesn't have that breakdown. So it's got state there, but it doesn't say is that state all internal state? Is it external state or a mixture of them? And it apparently is mixture because you've got policy that can affect state, which means that you're going to have action, is going to be part of the state transition as well as and then you have the observations coming out the bottom. But I think if we're going to talk about Markov blankets, then I would like to see this picture be exploded into the state of the machine versus the state of the external world and how those interact with one another. Because that's what the markup blanket does. Is it compartmentalizes the information?
1472930	1702380	A	0.9375421906693718	Well, that is the POMDP model and Figure 4.3 and this really Figure 4.4 makes me scratch my head quite a bit. This is the image that is going to represent message passing in a Bayesian framework. And so the caption says on the right dependencies between different variables in the belief updating scheme outlined in the main text. Intuitively, current beliefs about states under each policy at each time are compared with those that would be predicted given beliefs about states at other times, this is one. And current outcomes to calculate prediction errors. These errors then drive updating and beliefs, this is two. Given beliefs about states under each policy we can then calculate the gradients of the expected free energy. This is three. These are combined with the outcomes predicted under each policy omitted from the figure. To compute beliefs about policies, this is four. And then using a Bayesian model average, we can then compute posterior beliefs about states averaged over policies. This is five. And this kind of leads us into the next question. But I am not able to really interpret this Figure 4.4 in any better way. So if anybody has like a plain English description of how this works, I would love to hear it. We can talk a little bit about a question that was raised about this figure and also about this squiggle sigma because there's a great I don't know, we kind of had a hard time figuring out the equation and the squiggle sigma and what that actually stands for. So we could move to that discussion because I think it's maybe related to this message passing discussion. But let's read the discourse here first on the message passing question. So in the discourse, what is the mechanism for signal decay? So this is information decay, implementation of information decay. Is the message a thing behaving as an active inference agent itself or a special piece of information not behaving as an active inference system itself? Is there a blanket impedance mismatch? I think Brock contributed this. I'm not sure if he's here today. And then message passing as described through active inference usually is a hierarchy, not a lateral transfer like within constituents in the blanket. So the key thing here is how you define the markup blanket. Either there's a partition between particles, cells or whatever, or there's no partition in blanket and everything under the blanket is conditionally dependent. Does conditional dependence preclude message passing? Our message passed within nodes in the same blanket. I have not seen this and would love if someone pointed me to some references. So that was me. I wrote the last part because I haven't seen message passing under the same Markov blanket. It's always been like from one Markov blanket partitioned object to a different Markov blanket partitioned object. But that moves us into if anybody has any comments, feel free.
1702450	1713330	C	0.9052617241379312	Well, I would say message passing is used to do the inference. So it's within a markup blanket. That's pretty common, I think the way message passes is used.
1715460	1772544	A	0.9399156790123447	So here, as it's shown in Figure 4.4, although this is totally beyond me to describe here, this is a temporal message passing. So it's like from one time point to another time point and then on the left it depicts the hierarchical, like a hierarchical expansion collapsing over time steps but that a higher level network might predict the states and policies at the lower level and use these to draw inferences about the context in which these occur. So the way that I've always seen the message passing used is from one time point to another or from one layer of a hierarchy to the next layer of a hierarchy and I've not seen it passed within constituents under the same Markov blanket. So Eric, if you have references that depict some kind of message passing in active inference that's not through time steps or through hierarchical levels, I would love to see that because I have not seen it.
1772662	1812050	C	0.9360098230088492	Well, I would just go back to box 4.1 where they talk about the variational message passing and how you get information. And I would say within a blanket you've got various nodes and they have this hierarchical parent child relations and in order to do inference about, hey, what's our belief in one of these parents? You got to kind of go up and down and say well, what are the children of that and what do the other parents think? So that's all within a single Markov blanket, the parents and children, the hierarchical relation and the message passing happens to circulate that information within the blanket. That's how I read.
1815380	1875372	B	0.9320193670886077	And I also put a link here to a paper by Champion at all in which they have actually explicitly defined the active inference, or better say, reformulated the active inference formalism according to variation of message passing. And if you just give me a second, I can find exactly the place where they have stated this in plain English. You can continue with the other questions if you like, but I need a moment to check this paper.
1875506	2121240	A	0.9375989949748754	Sure, yeah, I'll check it out also and yeah, thank you for that. So like when I'm reading this variational message passing, this involves messages from all constituents of the Markov blanket of X. So the message is coming from all the constituents in the blanket, but it doesn't say where the message is going. So it does make sense, I guess, that it's an interchangeable or within would be a better word than from within all constituents of the Markov blanket because I was reading it as like the message is coming from everything under that blanket, which I guess is an incorrect interpretation. So thank you Eric for pointing that out because I was like what? So this is a really hard question that I looked to try to answer. Actually a bunch of us looked yesterday a lot. So it says in equation four point ten, the sigma variable, squiggle sigma, we'll call it whatever, I'm not sure how to say it is used to describe the difference between the natural log of observations conditioned on policy and preferences. What is the function of this variable? It also comes up in figure 4.4, the message passing figure we're just looking at, and it would be great to have a verbal description of this figure, what other papers or equations use this squiggle sigma? And we looked at length, like through the entire textbook, through the other chapters it refers to like we'll unpack this later in chapter seven. We looked through chapter seven, we looked through the appendices, we saw a variable that almost looks like that, but I think it's a gamma. It looks like a little bit more fancy than this squiggle sigma and could not find that at all. So we went to unpack equation 410. And so here is the equation itself. I'm not sure how to pull this up next to the actually maybe I'll put it up here, 419. And this equation four point ten is a rewriting of equation 4.7 in linear algebraic form. So we tried to kind of unpack this a little bit. So the first line states the prior probability for each policy. This is that pi sub zero is equal to the soft maps function sigma times the negative expected free energy G. And so that's this first line. The next line is the expected free energy conditioned on policy g sub pi is equal to the entropy or negative expected log probability h times the states conditioned on policy and time, which is S sub pi times tau or both of those sub pi and tau plus the observations conditioned on policy and time. This O sub pi times tau times the beliefs conditioned on policy and time, which is a squiggle sigma maybe sub pi times tau. So we were kind of unsure if beliefs is correct here. We kind of pulled that out of the legend for figure 4.4. But if anybody knows, and just to maybe unpack that a little bit more, the beliefs conditioned on policy and time, the squiggle sigma, sub pi times tau is equal to the difference between the observations conditioned on policy and time o sub pi times tau and preferences conditioned on time C sub pi or sub tau. So we were unsure if belief is correct here also. And so if anybody knows if beliefs is the difference between observations and preferences, that would be great to have some kind of feedback or input here because we couldn't really reach a conclusion.
2159770	2231500	C	0.9050603676470587	Well, I guess one tiny step toward figuring this out, I would say, is that that funny squiggle thing there has to do with the C. We've got, we've got this probability of observations given C and C is this mysterious object that expresses preferences which has also been under explained. So if you look at the two terms of the expected free energy G as a function of functional policy in 4.7, and then look at how it looks like in four point ten, that sigma thing there, we've got the dot, which I guess is a multiplier by observation. So that's the same as that's like saying your probability of observation given C. So that's, I guess, a matrix multiplier version of this log probability of sequence of observations given your preference prior C.
2250490	2305710	A	0.9280304511278193	Yeah, so they do unpack. It a little bit more in equation 4.7 or it seems to be like maybe we could extrapolate from 4.7 if this is beliefs, because it is this natural log of observations minus the natural log of preferences. But up in the top that just the free energy is the entropy or maybe the dot product of the entropy times the states plus the observations times or the dot product of the observations and maybe beliefs here I can't recall a mathematical representation of belief ever using this squiggle sigma before. And I looked through the recent Ryan Smith paper and I looked even at the message passing paper by Friston and I was not able to really get any additional references to this squiggle sigma at all.
2305780	2308430	C	0.9757950000000001	So why do you say belief as opposed to preferences?
2308930	2335670	A	0.9369288524590161	Well, so it says here that the squiggle sigma is equal to the difference between the natural log of the observations conditioned on policy and time and the natural log of the preferences at a certain time. So the sigma is defined by the natural log of the preferences, but it's more than just the preferences. Does that make sense? Eric?
2336170	2343740	C	0.8676423076923077	Yeah, but that's not claim as belief, right? That's observations versus preferences, right.
2345630	2382420	D	0.9210462820512821	From expected deviation or how much risk is being taken relative to the policy that's applied. The dot product is between the observation and that wiggle sigma, right, which is in turn like, okay, this is what I've observed, this is what I expected. So it's like a divergence between two. So how much more risk am I taking and how far is this pushing me away from my free from minimizing free energy? So maybe it's something.
2384650	2389720	C	0.9991593333333334	Whereas belief is wrapped up in the s because that's your model of the world.
2392510	2559540	A	0.9302262222222224	So risk is maybe how the squiggle sigma is defined. But then if you look into this figure 4.4 like that's, that's why we used belief. So here, let's, let's I'll, I'll pull it up right now. So I'm not sure how easy this is to see. But before we get to number one here, we have like on the right hand side the second to lowest level is states conditioned on policy and time. I think I'm going to read it off this bigger screen. Yeah. States conditioned on policy and time at different time steps. So like the current time is in the middle, the forward time step is on the right and the backward time step is on the left. And so that's what we start with. So intuitively, current beliefs about states under each policy at each time are compared with those that would be predicted given beliefs about states at other times. So maybe beliefs is this f and that's kind of what Eric was saying because it's defined earlier. This epsilon here is a prediction error, right? So what we come to after step one, it says and current outcomes to calculate prediction errors. That's what we arrive at after number one. The errors then drive updating in the beliefs. That's number two. So here we go from the current state at the present time to a prediction error that drives the belief updating at this forward time step after number two. And then it says given beliefs about states under each policy, we can then calculate the gradients of expected free energy. Three. So what gives us this gradient of expected free energy? Is it this s sub tau plus one? Or is that a tau? I think that's a tau. Yeah. So is it a future state not conditioned on policy? Or here we get to this squiggle sigma. So it's a squiggle sigma conditioned on policy and time plus one. So it looks like a belief update there, which is why belief made sense. So is that a risk update? Is that a possible interpretation there in this message passing figure?
2565590	2585050	C	0.9634296428571429	Well, they say it's a gradient, which means it's saying how much do we have to change our policy, I think in order to get our objectives yeah.
2585120	2683740	D	0.9284804782608693	Gradient makes sense to me. It's how much information you've gained, right? So if you actually look at the change in the state with respect to so I apply some force, or let's just take a simple example. If you have an actuator that just applies force and all it does is it moves forward and has to follow a trajectory. So the error is at each time step. If it's a deterministic system, you expect it to follow a straight line and it's deviating up way. So that's epsilon. So you have to update, okay, this is how far up I am from the state that I'm supposed to be. So I need to apply slightly less force or maybe force in a different direction. It's also how much I have to overcorrect in the future depending on what I've done currently, right? It's not just if I've applied some force right now, I might deviate in the opposite direction and I have to come back, so I have to figure out how much force to apply. So that's why I said risk or. Yeah, gradient, you would have to take the change with respect to the state change with respect to the error. So yeah, that would be one interpretation of it. Right, so it's a gradient of the state change with respect to what you have done.
2686210	2690160	A	0.918121	So I also heard the term information gain in there.
2690770	2732780	D	0.9145070796460176	Yeah, I'm just generally saying. So in neural networks, for example, we would have some sort of loss function and we would calculate the weight change with respect to the change of the loss function. Right. So, like, I have a weight vector and then this particular example is giving me a certain amount of gradient and I need to minimize this gradient. So I need to change the weights in turn so the gradient actually interfaces between this loss function and all these weights. So effectively, the gradient passes on information as to how much the weight has to change in order to minimize that loss function. That's why I said information.
2736450	2751410	B	0.8807696153846153	And also, as a side note, I think I found this sigma in Ryan Smith's paper too, and it is defined as the expected prediction error.
2753510	2764738	A	0.9517749999999999	So the expected prediction error versus the actual prediction error. And they distinguish it from epsilon in the Ryan Smith paper because I did look at that, but I didn't find it in there.
2764824	2771810	B	0.8808790909090909	Yeah, it's in equation 27 under the section outcome prediction errors.
2772950	2787520	C	0.9106504545454548	You it may be that the expected means it's an expectation over the Q distribution, which is your distribution over belief states.
2808590	2930962	A	0.9370456870229006	Yeah, that's great. Ali and I and some others worked on this yesterday a lot and it's great that it kept you up late at night, clearly, Ali, so you went digging around for some more information. That's awesome. Yeah, great. Okay, perfect. Well, that resolves that very well. And then let's get into maybe the next question. In equation 4.16, the X has a dot over. It changed through time but not the Y data. So there's some discourse on this. Or maybe we should go to the next question, actually, because it's maybe more related. So, Figure 4.6 uses an epsilon for prediction error as described in equation 4.21. Is this predictive processing framing a part of the active inference model or is this presented for contrast to illustrate the similarities and differences between active inference and predictive processing? So this is equation 4.1 and this is the epsilon, but I think we see it way before equation 4.21. Like it's even there. In equation 4.4, it says this predictive coding schema is part of the active inference model illustrating the hierarchical structure of predictions and beliefs. The authors say one way to think about this is as if we had equipped a predictive coding scheme with classical reflex arcs at the lowest level of the hierarchy. And they give this reference in this setting, active inference is just predictive coding plus reflex arcs. Does anybody know what a reflex arc is? Because I'm a little bit lost with respect to that or have any comments on this prediction area?
2931016	2939250	B	0.8505664285714287	Yeah, reflex arc is basically the stimuli that doesn't go through the sensory cortex.
2950000	2952640	A	0.96838875	I thought it was like a mathematical construct.
2959510	2967250	C	0.9404777777777777	Maybe you think of it as a control system. No thinking. It's just like you bang your knee.
2967750	3132480	A	0.9303713229571986	Your foot goes up. That makes sense. And it says, we minimize free energy through action. And the only part of the free energy that depends on action is the lowest level of prediction error. In this hierarchical schema, action fulfills descending predictions by minimizing the error between the predicted and observed sensory data. Any additional comments here about predictive coding and active inference? Or we can look at this equation 4.16. The x has the dot over. It changed through time, but not the y data. Y is this. And here the discourse says, the top equation depends on f of x and v, which is a deterministic function describing how a hidden state changes over time. The bottom equation depends on g of x and v, which describes how data are generated from a single hidden state. The tilde indicates change through time. The dot indicates the first order derivative. Any comments here on this equation or your question? Maybe we can ask one more question, or maybe a couple more. So here it says on page 63, it says specifically, the Bayesian brain helps us frame the problems that an agent engaging in active inference must solve broadly. These are the problem of inferring states of the world perception and inferring a course of action planning. Other than perception and action planning, are there other tasks or challenges that the brain or organisms engage in? How would we know? There's no discourse here, but let's open it up and see what you guys think.
3153730	3166530	C	0.9175157692307694	Yeah, people wonder, hey, what's on that star out there? That's neither? I don't know. Is that inferring State of the world? I guess not. Planning.
3169610	3180600	D	0.9206517857142857	And it could just be a hallucination of generative model. You're not really measuring anything, you're just measuring yourself over and over again. That could be one thing.
3199690	3210090	A	0.8923404347826086	Okay, so here there's another question for each of the graphical models in Figure 4.2. What is an intuitive example for each structure?
3230040	3239800	D	0.8917376470588235	One would be disease diagnosis. So you have symptom that is causative or predictive of some condition.
3248550	3257140	A	0.9471613636363636	I think that was the example that they gave in the text, right? Or did we talk about this last week maybe?
3262250	3270520	B	0.9008613636363635	Yeah, I actually don't get the question because each of them we have an example, at least an example in the text.
3280310	3289490	A	0.9067600000000001	Yeah, that's true. And I think we're going to maybe see more examples as we go through the book.
3300580	3316360	B	0.9000899999999998	But just in case, if anyone wants to see some more examples actually, that paper, I put the link in the chat, realizing Active Inference in Variation Message Passing contains some more examples for each of these diagrams.
3322640	3349540	A	0.9414935483870968	This is a fun last question. Maybe it says this reiterates that active inference uses two constructs, variational free energy and expected free energy, which are mathematically related but play distinct and complementary roles. In your own words, what are the definitions of variational free energy and expected free energy? And what role do they play individually or together in active inference?
3365890	3427220	D	0.892344140625	My view of expected free energy was given a certain action that might be taken. It might produce this or reduce the free energy or something, or it talks about some future reduction on free energy or something like that. Variational free energy is more objective. I don't know how to put it. It's like, okay, actual kinetic energy versus perceived kinetic energy kind of thing. So if you're rolling down a hill, like rolling down a hill, we can actually measure the kinetic energy at the bottom of the hill, right? If you put the ball at the top, expected free energy would be like using a model to calculate that and then finding out actually corresponds if you look the model. At least that's my view.
3439070	3502140	A	0.9344177840909083	So I don't know. I'm probably like I think about this in a lot of different ways, but I think about variational free energy. I mean, this is one way to think about it, is maximizing the trade off between epistemic value and pragmatic value right now and expected for free energy is like maximizing that difference at some point in the future. So you can plan ahead. It might be cold out, so I'm going to take a jacket or like it's cold right now, I'm going to put my jacket on. That's how I think about the difference between those two. So it's ten. Thanks, everyone, for coming. I hope I didn't muddle this up too much. Trying to be substitute teacher for Daniel, and it's ten. So we're going to have tools right now in this room. But if anybody wants to continue discussing these ideas and gather, you're welcome to migrate up to one of the different spaces. Yeah, and I think we'll stop the recording now, Alex, if you haven't already.

start	end	speaker	sentiment	confidence	text
250	1920	A	0.6732448935508728	Of these questions.
3410	6720	A	0.8919735550880432	I think we talked about the oh, this is chapter two.
7650	9360	A	0.703356146812439	Let's go to chapter four.
10370	16960	A	0.8612033724784851	Okay, so we talked about belief, policy, state.
17810	20320	A	0.7214670777320862	I'm not sure if we talked about this question.
21730	32630	A	0.8453829884529114	In the discussion of active inference in POMDP t belief updating about policies, we find that the posterior that minimizes the free energy does the posterior at time t.
32700	34120	A	0.8376924991607666	Oh, we did discuss this.
34810	37080	A	0.8523204922676086	Yeah, we did discuss this last time.
37470	49834	A	0.5995962619781494	So this question, I think we did not discuss pi being a policy or a model last time.
49872	50860	A	0.7585627436637878	Or did we?
51230	53200	A	0.7166381478309631	I think we didn't get to this.
53650	55614	B	0.610296905040741	Yeah, I don't think we discussed that.
55732	61786	A	0.5950624346733093	All right, so we can open it up and we'll start here, I guess, with the most upvoted.
61898	64874	A	0.8771733641624451	So the question reads, what is pi?
64922	75118	A	0.9006317853927612	On page 69, the authors write, at each time step, the current state is conditionally dependent on the state at the previous time and on the policy pi currently being pursued.
75214	87250	A	0.8951966166496277	Then on page 71, they write, thus we can interpret the priors of equation 4.6 combined with the likelihood of equation 4.5 as expressing a model pi of a behavioral sequence.
87330	89414	A	0.7861818075180054	So which is it, policy or model?
89612	92918	A	0.5504360795021057	And then they suggest a rewrite of the sentence.
93014	109230	A	0.8867161870002747	They say that, thus we can interpret the priors of equation 4.6 combined with the likelihood of equation 4.5 and the transition probabilities b sub tau pi as expressing a model for a behavioral sequence where the model is a function of policy pi.
110610	113854	A	0.7904168367385864	And then there's some discourse here.
114052	121700	A	0.8963033556938171	So in this reframing, can we say that the model simulates the agent in the environment as if it had taken the actions in the policy?
123430	127400	A	0.5841636657714844	And I don't know who asked that question.
129530	130694	A	0.6698078513145447	Oh, Eric, that was you.
130732	131880	A	0.7536847591400146	That was your question.
132410	137078	A	0.8133772015571594	And then does anyone want to maybe take a stab at answering that question?
137164	139900	A	0.8547012209892273	Or we can continue reading what was written here.
142990	155258	A	0.9208185076713562	So what's written here says that on page 69, the authors also say policies here may be thought of as indexing alternative trajectories or sequences of actions that could be followed.
155354	162906	A	0.9264591932296753	On page 71, they define the likelihood in equation four five as a matrix A that expresses the probability of an outcome.
163018	174402	A	0.9065247178077698	They describe the priors of equation 4.6 as the prior over the initial state vector D, and beliefs about how the state at one time transitions to the state at the next time, matrix B.
174456	178914	A	0.8276252150535583	And they also say that the transitions are conditionally dependent on the policy chosen.
179042	188230	A	0.8792973756790161	Because of this conditional dependence, we can see how the policy influences the model and why the authors may use the terms interchangeably.
192990	203290	A	0.8671053647994995	So does anyone have any comments there's?
204850	225798	C	0.6669976711273193	I guess I would say that that answer in discourse kind of agrees with what I proposed, which is that strictly speaking, we should treat the pi as policy, but it behaves as a model.
225884	228354	C	0.79607093334198	Once it's executed, it's implemented.
228482	237970	C	0.8266791701316833	Then the model has become a function of the policy that's been we've seen so far.
238140	244646	C	0.5863564014434814	So I'd say that's compatible interpretation.
244758	258682	C	0.6527977585792542	So I think strictly speaking, it would be better if the text was consistent and called Pi policy and then say, yeah, the model is a function of Pi.
258826	260990	C	0.8298872113227844	That'd be my interpretation.
262370	263874	A	0.8166483044624329	Yeah, I definitely agree with that.
263912	278360	A	0.5593612194061279	I think that the model is policy specific, but they could do a better job of I mean, the variables as we've seen are so ambiguous anyway, so it could definitely be a lot better.
284180	309044	B	0.7103613615036011	And by the way, about the topic of consistency, actually I gave some thoughts about the issue we were discussing in yesterday's math learning session and I also consulted some other papers and active inference and in all of them the notation for matrices and vectors is used consistently, as in almost every linear algebra textbook.
309172	324792	B	0.9111108779907227	So I'm seriously beginning to suspect that every instance of a matrix or a vector not written in Bold face is a typo either in chapters or appendices because otherwise I really cannot find any justification behind using two different types of notation.
324936	333570	B	0.8545883297920227	So I'm going to use this assumption as my prior for the rest of the book, unless someone has any belief updating observation she wants to share.
336740	346416	A	0.657429575920105	So just to summarize, for the people that weren't there yesterday, we had a big debate about what does the Bold typeface signify?
346608	352272	A	0.7618865370750427	And it wasn't so much a debate, but just like trying to come to a mutual understanding.
352336	353588	A	0.8231990933418274	So it's this question here.
353674	359604	A	0.6301851868629456	Does bold lettering mean this is a matrix, but seemingly also many non bold letters are a matrix?
359652	365080	A	0.7501271367073059	Or does bold lettering mean sufficient statistics of as on these page examples?
365580	386320	A	0.5457110404968262	And it really was yeah, we couldn't come to a single unified answer there because there are many instances where it seems like it should be a matrix, but it's not bold and there are many instances where sufficient statistics are used but it's also not bold.
386900	391068	A	0.7398584485054016	So yeah, we couldn't really conclude that, but I think I'll lead it.
391094	393590	A	0.6564871072769165	That's a good assumption there.
412470	418434	A	0.741767942905426	So we also talked about this a little bit last time, but we didn't really get fully into this question.
418552	422694	A	0.900536060333252	What is the use of categorical distributions in equation 4.5?
422812	428402	A	0.6768614649772644	The second line is supposed to explain the cat notation, but I have problems to understand the advantage of the notation.
428546	431480	A	0.8171482682228088	Could someone explain it in simple words?
432190	434986	A	0.7636882066726685	Does anyone want to try to explain that?
435088	436490	A	0.8162937164306641	I can pull up the equation.
443600	452960	B	0.8280342817306519	Well, I think every regular matrix and vector is by definition an array of ordered elements.
453620	461840	B	0.8385269641876221	But in a categorical distribution we don't necessarily assign any specific order to the elements.
461920	480280	B	0.833466112613678	So the reason behind using the categorical distribution might be to just jettison any orderness in the elements and treat them as just an unordered set of the probabilities.
482880	499870	C	0.8400753140449524	I wrote an answer to this one if you want to pull that up, so you can just read that's.
517460	519200	C	0.9448930025100708	So I hope this is helpful.
521540	533190	C	0.8481979966163635	It's basically the way I see it is you have the math, the math notation there, but when you need to actually implement that, you need to carry it out, then you need to turn that into operations you can perform.
533720	548404	C	0.8005563616752625	And so when you say that A is this categorical object, that means that you're turning it into an actual matrix with elements.
548532	564510	C	0.867872953414917	And so the ordering is important, actually, because the ordering of the elements in the matrix says how that matrix is going to apply to the objects it applies to, which are the belief state S, and then your observation O.
565280	580100	C	0.8781253099441528	So basically it says the way you carry out inferring what O will be the probability of O given a belief state is you do a matrix multiply.
582200	605292	C	0.8126471042633057	And because in this example, the states are not continuous, they're categorical, then you need a categorical matrix element in order to do map from the categories of belief states to the categories of observation states.
605346	610780	C	0.8741443157196045	And those are both distributions that gets transformed by the matrix.
613600	616660	A	0.9837161302566528	Thanks, Eric, that was super helpful.
616840	620770	A	0.8631193041801453	Does anybody have any further question or comment on this one?
625310	630658	B	0.5932728052139282	Sorry, Eric, but isn't the categorical notation special case of multinomial distribution?
630774	636270	B	0.886630117893219	Categorical distribution special cases of multinomial distribution?
639090	640160	B	0.5536741018295288	I don't know that.
641810	669526	B	0.8394349813461304	Yeah, because that was my understanding that the main difference between any categorical distribution and well, I mean, for instance, of course you're right that in any matrix we necessarily have some ordered elements in order for us to be able to compute the math on those matrices.
669718	691754	B	0.8301641941070557	But on the other hand, for categorical distributions, at least that was my understanding that we don't necessarily treat them as ordered elements, such as, I don't know, the ordered vectors or ordered matrices as the normal matrix.
691802	702542	B	0.787671685218811	Because if that was the case, well, we could just designate this distribution as just a regular matrix.
702606	703074	B	0.5664746165275574	Right.
703192	718886	B	0.5257381200790405	So I don't get what's the reason behind using the notation here, cat A or cat, instead of just using the.
718908	720230	D	0.8250715732574463	Name of the matrix?
723150	726220	A	0.5475566387176514	Steven, can you mute you're super loud over there.
739450	740200	A	0.5092127919197083	Sorry.
742970	744840	C	0.6842135190963745	Yeah, I guess so.
748170	750326	C	0.6121252775192261	The mathematical notation doesn't have an ordering.
750358	757766	C	0.8822914361953735	It just says distribution is related to another distribution through this relation.
757878	762414	C	0.8068660497665405	And when you get to the mechanics of how do you do that?
762452	766826	C	0.849942684173584	That's where you care that you have the matrix with the vector ordering.
766938	775940	C	0.49835532903671265	So, yeah, I don't know what cat A actually means other than it's just describing it.
777030	780500	C	0.6851859092712402	I don't think it's like an operator, I think it's just saying what it is.
780950	782180	C	0.7228710055351257	Does that make sense?
793790	796480	A	0.5310500860214233	Steven, if you're trying to talk, we can't hear.
798770	799520	D	0.48124945163726807	What.
815860	825510	A	0.7345257997512817	I feel like Steven is trying to share an idea with us, but is unable to communicate effectively through the technological affordances he's been given at the time.
826440	837160	C	0.500153124332428	Like the gain on your mic is way up or something, so it's picking up more background noise than you're.
882020	884124	A	0.8496692776679993	Just looking at this equation.
884252	885810	A	0.8035325407981873	Can you guys hear me okay?
893560	894068	A	0.5491447448730469	Yeah.
894154	924030	A	0.8724605441093445	Okay, so just looking at this equation, the second part, it's like the, it's like an ordered matrix A-I-J where the probability of the observations are equal to index I and the states equal to index j, something like that.
925680	929100	A	0.6909353137016296	I don't know, it looks like it is ordered.
930260	932880	B	0.6622275114059448	Yeah, the matrix itself is ordered.
933460	933920	A	0.5491447448730469	Yeah.
933990	940370	A	0.848118007183075	And so the second line is actually by how the authors are defining this cat A.
940980	944580	A	0.8973264694213867	So cat A represents this ordered matrix AIG?
946920	953780	A	0.7439905405044556	Yeah, that's at least what it says in the textbook.
978190	989680	A	0.9025298357009888	So it says the likelihood expressing the probability of observations conditioned on time tau given states s conditioned on time tau is equal to the categorical distribution A.
1005990	1006594	B	0.5092127919197083	Sorry.
1006712	1008962	B	0.7465403079986572	Yaku, go ahead.
1009016	1015094	E	0.9186288118362427	Sorry, my internet is lagging a bit, so there might be a delay and I might break off.
1015132	1027322	E	0.7888143062591553	But I just want to comment that I think that the cat notation is really just to reinforce the notion that it's in discrete time.
1027376	1043760	E	0.5017886161804199	But I think it's kind of redundant in this case because since we know that we are in discrete time, we could have just written a bolt to signify that it's a matrix and where it's a matrix, it's probably not going to be a continuous distribution anyway.
1045890	1051330	E	0.7105361223220825	So I think cat bold A and bold A is pretty much interchangeable.
1055470	1064714	A	0.8848639726638794	So I think that some of the new math is using some of the affordances from category theory more like some of the newer things.
1064752	1082882	A	0.6104901432991028	And I think the forward direction that Karl and Thomas and some of the people that really helped derive and advance some of this math, I think that they're going to be leveraging category theory methods like the renormalization group and the pullback attractor more in the future.
1082936	1090674	A	0.9145835638046265	And so that might be also part of the reason that they are moving toward this categorical notation.
1090802	1094760	A	0.871547281742096	So just a little maybe foreshadowing there too.
1100810	1106010	B	0.9112029075622559	May I just make one additional note about this categorical distribution?
1106590	1125578	B	0.6333829760551453	Well, the thing that confused me about this concept of categorical distribution is the Wikipedia's definition of it, because here it says there is no innate underlying ordering of these outcomes, but numerical labels are often attached for convenience in describing the distribution.
1125754	1128290	B	0.597206175327301	So that was my confusion.
1135940	1144944	A	0.5364431142807007	I don't think it would be the first time that we've seen traditional mathematical notation slightly and somewhat abused in active inference.
1145072	1146900	A	0.7588402032852173	Probably also won't be the last.
1147050	1154580	A	0.6836255788803101	But definitely I think it's a point that we can put forward to the authors for additional clarification.
1157020	1166490	A	0.8153775930404663	If there's no further anybody has anything else to say about the categorical distributions here, if not, we can move on to maybe the next question.
1175470	1187710	A	0.8504305481910706	So this next question reads in message passing, is there a decay of information as the distance between the variable X and individual Markov blanket constituents increases?
1188130	1190522	A	0.5979946255683899	Is implementation of information decay?
1190586	1195390	A	0.9003216028213501	In message passing an option for model implementation?
1195810	1201380	A	0.9785046577453613	I think that this is a super interesting question and it's something that I've thought about before a lot.
1202710	1210530	A	0.8813353180885315	But I think, yeah, let me open it up to anyone and see if they want to address it before we can get into some of the discourse.
1215750	1224294	C	0.6238812804222107	Well, doesn't that all depend upon the chain of loss between one stage of the message passing and another?
1224332	1227846	C	0.6886833310127258	And if you have no loss, then you don't have any decay.
1228038	1237470	C	0.6473630666732788	But the more stages you have where you get a little bit of diffusion or loss, then you're going to have decay, just a function of the parameters.
1238210	1248606	A	0.8714811205863953	Well, so are messages passed within the same Markov blanket or between things that are partitioned with their own Markov blanket?
1248638	1249860	A	0.7374429702758789	That's another question.
1264640	1269420	B	0.7337089776992798	I also agree with Eric, as I mentioned yesterday.
1270580	1304200	B	0.5916502475738525	I also believe that, well, this information loss is not accounted for at least inherently in active inference formalism because it depends on the specific situations that those information propagate because otherwise we couldn't have any general or all encompassing formalism for accounting for these information loss.
1307580	1316984	A	0.8857457041740417	So we see in this box they talk about Markov blankets and the message passing.
1317032	1319272	A	0.7961171865463257	So here is variational message passing.
1319336	1328608	A	0.9234300851821899	This involves messages from all constituents of the Markov blanket of X, including the parents via the conditional probability of X given its parents and the children.
1328774	1334464	A	0.9072985053062439	The latter depends on the conditional probability of the children of X given all of their parents, which include X.
1334582	1343190	A	0.8598130941390991	Note the expectation includes the children and parents of the children as parents of the children x, we divide by Q of X to ensure the expectation includes the blanket only.
1344600	1352452	A	0.7903719544410706	So I think the messages are like all of the things inside of the blanket, send a message to something else in another blanket.
1352516	1357844	A	0.8014780879020691	At least that's how I interpret this way of active inference.
1357892	1370788	A	0.8643937706947327	And so it would make sense that from within the blanket, does everyone have the same is there like a high degree of mutual information shared in the Markov blanket?
1370904	1382028	A	0.8652898669242859	I think conditional dependence is what qualifies as conditional independence is what establishes the markup blanket.
1382124	1387268	A	0.7026574015617371	So within constituents under the blanket, do they all have all of the same information all of the time?
1387354	1389990	A	0.9444253444671631	That's something that's kind of confusing to me.
1391320	1424588	C	0.8879319429397583	One thing I'd point out about figure 4.3, so in the previous chapters we have this picture of there being the world and your model of the world and your model of the world is a Markov blanket itself and it interfaces with the world through the observation variables and the action variables.
1424764	1426828	C	0.5867541432380676	This picture doesn't have that breakdown.
1426924	1431616	C	0.7614852786064148	So it's got state there, but it doesn't say is that state all internal state?
1431718	1434624	C	0.8638718128204346	Is it external state or a mixture of them?
1434662	1447190	C	0.711399495601654	And it apparently is mixture because you've got policy that can affect state, which means that you're going to have action, is going to be part of the state transition as well as and then you have the observations coming out the bottom.
1448200	1462412	C	0.7448099255561829	But I think if we're going to talk about Markov blankets, then I would like to see this picture be exploded into the state of the machine versus the state of the external world and how those interact with one another.
1462466	1464140	C	0.7233712673187256	Because that's what the markup blanket does.
1464210	1466190	C	0.7637587785720825	Is it compartmentalizes the information?
1472930	1485858	A	0.7430875897407532	Well, that is the POMDP model and Figure 4.3 and this really Figure 4.4 makes me scratch my head quite a bit.
1485944	1494770	A	0.8580065369606018	This is the image that is going to represent message passing in a Bayesian framework.
1494930	1505100	A	0.904077410697937	And so the caption says on the right dependencies between different variables in the belief updating scheme outlined in the main text.
1505870	1515840	A	0.8565109372138977	Intuitively, current beliefs about states under each policy at each time are compared with those that would be predicted given beliefs about states at other times, this is one.
1516930	1520378	A	0.6141592860221863	And current outcomes to calculate prediction errors.
1520474	1524320	A	0.5252833962440491	These errors then drive updating and beliefs, this is two.
1524930	1530418	A	0.9020794034004211	Given beliefs about states under each policy we can then calculate the gradients of the expected free energy.
1530504	1531700	A	0.7997140288352966	This is three.
1533350	1538022	A	0.854817271232605	These are combined with the outcomes predicted under each policy omitted from the figure.
1538076	1541320	A	0.8450043797492981	To compute beliefs about policies, this is four.
1542410	1548674	A	0.8966939449310303	And then using a Bayesian model average, we can then compute posterior beliefs about states averaged over policies.
1548802	1549960	A	0.7168113589286804	This is five.
1555050	1557590	A	0.7902646660804749	And this kind of leads us into the next question.
1557660	1563566	A	0.7238404750823975	But I am not able to really interpret this Figure 4.4 in any better way.
1563668	1571440	A	0.6397010087966919	So if anybody has like a plain English description of how this works, I would love to hear it.
1588300	1607836	A	0.5316343307495117	We can talk a little bit about a question that was raised about this figure and also about this squiggle sigma because there's a great I don't know, we kind of had a hard time figuring out the equation and the squiggle sigma and what that actually stands for.
1607938	1614076	A	0.8386114835739136	So we could move to that discussion because I think it's maybe related to this message passing discussion.
1614108	1621570	A	0.8551937341690063	But let's read the discourse here first on the message passing question.
1624280	1631728	A	0.8247302174568176	So in the discourse, what is the mechanism for signal decay?
1631824	1636352	A	0.737611711025238	So this is information decay, implementation of information decay.
1636496	1645130	A	0.7106457352638245	Is the message a thing behaving as an active inference agent itself or a special piece of information not behaving as an active inference system itself?
1645660	1648276	A	0.7314335703849792	Is there a blanket impedance mismatch?
1648308	1649804	A	0.8981224298477173	I think Brock contributed this.
1649842	1651310	A	0.6489880681037903	I'm not sure if he's here today.
1651840	1660872	A	0.837306022644043	And then message passing as described through active inference usually is a hierarchy, not a lateral transfer like within constituents in the blanket.
1660936	1663736	A	0.8118458390235901	So the key thing here is how you define the markup blanket.
1663848	1671964	A	0.7141167521476746	Either there's a partition between particles, cells or whatever, or there's no partition in blanket and everything under the blanket is conditionally dependent.
1672092	1675148	A	0.7822796106338501	Does conditional dependence preclude message passing?
1675244	1677744	A	0.8592659831047058	Our message passed within nodes in the same blanket.
1677792	1681232	A	0.5412868857383728	I have not seen this and would love if someone pointed me to some references.
1681376	1682596	A	0.8057087063789368	So that was me.
1682618	1688628	A	0.6341143846511841	I wrote the last part because I haven't seen message passing under the same Markov blanket.
1688644	1696410	A	0.8051595091819763	It's always been like from one Markov blanket partitioned object to a different Markov blanket partitioned object.
1698460	1702380	A	0.8535895943641663	But that moves us into if anybody has any comments, feel free.
1702450	1705064	C	0.8460021615028381	Well, I would say message passing is used to do the inference.
1705112	1707100	C	0.7726296186447144	So it's within a markup blanket.
1707920	1713330	C	0.7418416142463684	That's pretty common, I think the way message passes is used.
1715460	1725456	A	0.6270278692245483	So here, as it's shown in Figure 4.4, although this is totally beyond me to describe here, this is a temporal message passing.
1725488	1744484	A	0.8402392268180847	So it's like from one time point to another time point and then on the left it depicts the hierarchical, like a hierarchical expansion collapsing over time steps but that a higher level network might predict the states and policies at the lower level and use these to draw inferences about the context in which these occur.
1744612	1759816	A	0.7415174841880798	So the way that I've always seen the message passing used is from one time point to another or from one layer of a hierarchy to the next layer of a hierarchy and I've not seen it passed within constituents under the same Markov blanket.
1759848	1772544	A	0.4563389718532562	So Eric, if you have references that depict some kind of message passing in active inference that's not through time steps or through hierarchical levels, I would love to see that because I have not seen it.
1772662	1781028	C	0.8562238812446594	Well, I would just go back to box 4.1 where they talk about the variational message passing and how you get information.
1781194	1794196	C	0.8024309277534485	And I would say within a blanket you've got various nodes and they have this hierarchical parent child relations and in order to do inference about, hey, what's our belief in one of these parents?
1794308	1799130	C	0.7884400486946106	You got to kind of go up and down and say well, what are the children of that and what do the other parents think?
1799580	1810228	C	0.8960372805595398	So that's all within a single Markov blanket, the parents and children, the hierarchical relation and the message passing happens to circulate that information within the blanket.
1810424	1812050	C	0.7848387956619263	That's how I read.
1815380	1835916	B	0.820881187915802	And I also put a link here to a paper by Champion at all in which they have actually explicitly defined the active inference, or better say, reformulated the active inference formalism according to variation of message passing.
1836048	1847400	B	0.578041672706604	And if you just give me a second, I can find exactly the place where they have stated this in plain English.
1868700	1875372	B	0.8148870468139648	You can continue with the other questions if you like, but I need a moment to check this paper.
1875506	1878348	A	0.9436596632003784	Sure, yeah, I'll check it out also and yeah, thank you for that.
1878434	1885216	A	0.8937047719955444	So like when I'm reading this variational message passing, this involves messages from all constituents of the Markov blanket of X.
1885318	1890930	A	0.6295626163482666	So the message is coming from all the constituents in the blanket, but it doesn't say where the message is going.
1891380	1908360	A	0.5891770720481873	So it does make sense, I guess, that it's an interchangeable or within would be a better word than from within all constituents of the Markov blanket because I was reading it as like the message is coming from everything under that blanket, which I guess is an incorrect interpretation.
1912490	1916040	A	0.7283435463905334	So thank you Eric for pointing that out because I was like what?
1930380	1935916	A	0.48337700963020325	So this is a really hard question that I looked to try to answer.
1936098	1938412	A	0.7720639109611511	Actually a bunch of us looked yesterday a lot.
1938546	1954560	A	0.7638481855392456	So it says in equation four point ten, the sigma variable, squiggle sigma, we'll call it whatever, I'm not sure how to say it is used to describe the difference between the natural log of observations conditioned on policy and preferences.
1954900	1956716	A	0.8437862992286682	What is the function of this variable?
1956828	1967408	A	0.5980079770088196	It also comes up in figure 4.4, the message passing figure we're just looking at, and it would be great to have a verbal description of this figure, what other papers or equations use this squiggle sigma?
1967504	1975352	A	0.9209675192832947	And we looked at length, like through the entire textbook, through the other chapters it refers to like we'll unpack this later in chapter seven.
1975406	1981336	A	0.8111822605133057	We looked through chapter seven, we looked through the appendices, we saw a variable that almost looks like that, but I think it's a gamma.
1981368	1988204	A	0.43633970618247986	It looks like a little bit more fancy than this squiggle sigma and could not find that at all.
1988322	1992016	A	0.8907491564750671	So we went to unpack equation 410.
1992018	2002050	A	0.7467743754386902	And so here is the equation itself.
2004500	2011780	A	0.7131389379501343	I'm not sure how to pull this up next to the actually maybe I'll put it up here, 419.
2015000	2023064	A	0.8939175605773926	And this equation four point ten is a rewriting of equation 4.7 in linear algebraic form.
2023262	2027432	A	0.8801748752593994	So we tried to kind of unpack this a little bit.
2027486	2032632	A	0.8696884512901306	So the first line states the prior probability for each policy.
2032766	2040590	A	0.8191878199577332	This is that pi sub zero is equal to the soft maps function sigma times the negative expected free energy G.
2041920	2043584	A	0.7625128030776978	And so that's this first line.
2043702	2065860	A	0.8281168341636658	The next line is the expected free energy conditioned on policy g sub pi is equal to the entropy or negative expected log probability h times the states conditioned on policy and time, which is S sub pi times tau or both of those sub pi and tau plus the observations conditioned on policy and time.
2065930	2076404	A	0.8760201930999756	This O sub pi times tau times the beliefs conditioned on policy and time, which is a squiggle sigma maybe sub pi times tau.
2076532	2079656	A	0.5680748820304871	So we were kind of unsure if beliefs is correct here.
2079758	2085228	A	0.9089598655700684	We kind of pulled that out of the legend for figure 4.4.
2085394	2104284	A	0.8405444025993347	But if anybody knows, and just to maybe unpack that a little bit more, the beliefs conditioned on policy and time, the squiggle sigma, sub pi times tau is equal to the difference between the observations conditioned on policy and time o sub pi times tau and preferences conditioned on time C sub pi or sub tau.
2104332	2107110	A	0.5952916145324707	So we were unsure if belief is correct here also.
2108360	2121240	A	0.6141384243965149	And so if anybody knows if beliefs is the difference between observations and preferences, that would be great to have some kind of feedback or input here because we couldn't really reach a conclusion.
2159770	2173610	C	0.6393430233001709	Well, I guess one tiny step toward figuring this out, I would say, is that that funny squiggle thing there has to do with the C.
2173760	2185550	C	0.8821559548377991	We've got, we've got this probability of observations given C and C is this mysterious object that expresses preferences which has also been under explained.
2186690	2213362	C	0.8774446845054626	So if you look at the two terms of the expected free energy G as a function of functional policy in 4.7, and then look at how it looks like in four point ten, that sigma thing there, we've got the dot, which I guess is a multiplier by observation.
2213426	2218214	C	0.8015208840370178	So that's the same as that's like saying your probability of observation given C.
2218252	2231500	C	0.8990374207496643	So that's, I guess, a matrix multiplier version of this log probability of sequence of observations given your preference prior C.
2250490	2252246	A	0.7891743779182434	Yeah, so they do unpack.
2252278	2266182	A	0.8774363994598389	It a little bit more in equation 4.7 or it seems to be like maybe we could extrapolate from 4.7 if this is beliefs, because it is this natural log of observations minus the natural log of preferences.
2266266	2291110	A	0.7763915657997131	But up in the top that just the free energy is the entropy or maybe the dot product of the entropy times the states plus the observations times or the dot product of the observations and maybe beliefs here I can't recall a mathematical representation of belief ever using this squiggle sigma before.
2291180	2305710	A	0.6562723517417908	And I looked through the recent Ryan Smith paper and I looked even at the message passing paper by Friston and I was not able to really get any additional references to this squiggle sigma at all.
2305780	2308430	C	0.6867666244506836	So why do you say belief as opposed to preferences?
2308930	2326580	A	0.8570196032524109	Well, so it says here that the squiggle sigma is equal to the difference between the natural log of the observations conditioned on policy and time and the natural log of the preferences at a certain time.
2327030	2333830	A	0.8470872640609741	So the sigma is defined by the natural log of the preferences, but it's more than just the preferences.
2334170	2335014	A	0.7228710055351257	Does that make sense?
2335052	2335670	A	0.7154882550239563	Eric?
2336170	2339800	C	0.7104187607765198	Yeah, but that's not claim as belief, right?
2340170	2343740	C	0.8000462651252747	That's observations versus preferences, right.
2345630	2353450	D	0.6547948122024536	From expected deviation or how much risk is being taken relative to the policy that's applied.
2356290	2368442	D	0.7573870420455933	The dot product is between the observation and that wiggle sigma, right, which is in turn like, okay, this is what I've observed, this is what I expected.
2368506	2371170	D	0.7520033121109009	So it's like a divergence between two.
2371320	2378820	D	0.5362786650657654	So how much more risk am I taking and how far is this pushing me away from my free from minimizing free energy?
2381030	2382420	D	0.7869989275932312	So maybe it's something.
2384650	2389720	C	0.6264140605926514	Whereas belief is wrapped up in the s because that's your model of the world.
2392510	2396886	A	0.7995585203170776	So risk is maybe how the squiggle sigma is defined.
2396918	2401814	A	0.8507314920425415	But then if you look into this figure 4.4 like that's, that's why we used belief.
2401942	2404800	A	0.7613639831542969	So here, let's, let's I'll, I'll pull it up right now.
2428990	2432620	A	0.5157648324966431	So I'm not sure how easy this is to see.
2433150	2447934	A	0.7204804420471191	But before we get to number one here, we have like on the right hand side the second to lowest level is states conditioned on policy and time.
2447972	2449694	A	0.6696285009384155	I think I'm going to read it off this bigger screen.
2449812	2450190	A	0.5491447448730469	Yeah.
2450260	2452986	A	0.8074370622634888	States conditioned on policy and time at different time steps.
2453018	2459540	A	0.9072904586791992	So like the current time is in the middle, the forward time step is on the right and the backward time step is on the left.
2460310	2462580	A	0.7007207274436951	And so that's what we start with.
2463210	2474200	A	0.8640908598899841	So intuitively, current beliefs about states under each policy at each time are compared with those that would be predicted given beliefs about states at other times.
2475370	2484214	A	0.8342840075492859	So maybe beliefs is this f and that's kind of what Eric was saying because it's defined earlier.
2484262	2488378	A	0.5105041265487671	This epsilon here is a prediction error, right?
2488464	2494074	A	0.7071865200996399	So what we come to after step one, it says and current outcomes to calculate prediction errors.
2494122	2496640	A	0.8761252760887146	That's what we arrive at after number one.
2497250	2500154	A	0.6293780207633972	The errors then drive updating in the beliefs.
2500202	2500974	A	0.7442811727523804	That's number two.
2501012	2512100	A	0.7088732719421387	So here we go from the current state at the present time to a prediction error that drives the belief updating at this forward time step after number two.
2512790	2521000	A	0.9064946174621582	And then it says given beliefs about states under each policy, we can then calculate the gradients of expected free energy.
2521450	2522294	A	0.5679553151130676	Three.
2522492	2526130	A	0.6911865472793579	So what gives us this gradient of expected free energy?
2526220	2530842	A	0.8786633610725403	Is it this s sub tau plus one?
2530896	2532118	A	0.7876920104026794	Or is that a tau?
2532214	2533530	A	0.7468327283859253	I think that's a tau.
2535150	2535900	A	0.5491447448730469	Yeah.
2536510	2539674	A	0.8511669039726257	So is it a future state not conditioned on policy?
2539792	2542506	A	0.6235256195068359	Or here we get to this squiggle sigma.
2542618	2547120	A	0.7717738747596741	So it's a squiggle sigma conditioned on policy and time plus one.
2547650	2552578	A	0.6205233335494995	So it looks like a belief update there, which is why belief made sense.
2552744	2554670	A	0.811335563659668	So is that a risk update?
2554750	2559540	A	0.8883497714996338	Is that a possible interpretation there in this message passing figure?
2565590	2585050	C	0.6478546261787415	Well, they say it's a gradient, which means it's saying how much do we have to change our policy, I think in order to get our objectives yeah.
2585120	2586860	D	0.6476608514785767	Gradient makes sense to me.
2587790	2591020	D	0.7141396999359131	It's how much information you've gained, right?
2592530	2604020	D	0.8606823086738586	So if you actually look at the change in the state with respect to so I apply some force, or let's just take a simple example.
2604390	2615350	D	0.8536388278007507	If you have an actuator that just applies force and all it does is it moves forward and has to follow a trajectory.
2615690	2620118	D	0.6930755376815796	So the error is at each time step.
2620204	2625334	D	0.674950897693634	If it's a deterministic system, you expect it to follow a straight line and it's deviating up way.
2625372	2626726	D	0.7949774861335754	So that's epsilon.
2626918	2632746	D	0.5780295133590698	So you have to update, okay, this is how far up I am from the state that I'm supposed to be.
2632768	2639390	D	0.7688857316970825	So I need to apply slightly less force or maybe force in a different direction.
2643650	2650402	D	0.7100390195846558	It's also how much I have to overcorrect in the future depending on what I've done currently, right?
2650456	2662870	D	0.5687559247016907	It's not just if I've applied some force right now, I might deviate in the opposite direction and I have to come back, so I have to figure out how much force to apply.
2662940	2666182	D	0.8141041398048401	So that's why I said risk or.
2666236	2674694	D	0.8127288222312927	Yeah, gradient, you would have to take the change with respect to the state change with respect to the error.
2674822	2677914	D	0.77447509765625	So yeah, that would be one interpretation of it.
2677952	2683740	D	0.8289375901222229	Right, so it's a gradient of the state change with respect to what you have done.
2686210	2690160	A	0.8478707075119019	So I also heard the term information gain in there.
2690770	2693614	D	0.776376485824585	Yeah, I'm just generally saying.
2693652	2705490	D	0.8697981238365173	So in neural networks, for example, we would have some sort of loss function and we would calculate the weight change with respect to the change of the loss function.
2705560	2705746	D	0.5664746165275574	Right.
2705768	2715842	D	0.5217472314834595	So, like, I have a weight vector and then this particular example is giving me a certain amount of gradient and I need to minimize this gradient.
2715906	2723918	D	0.8652828931808472	So I need to change the weights in turn so the gradient actually interfaces between this loss function and all these weights.
2724114	2731562	D	0.8152498602867126	So effectively, the gradient passes on information as to how much the weight has to change in order to minimize that loss function.
2731616	2732780	D	0.8074496388435364	That's why I said information.
2736450	2751410	B	0.6755327582359314	And also, as a side note, I think I found this sigma in Ryan Smith's paper too, and it is defined as the expected prediction error.
2753510	2759038	A	0.6545842289924622	So the expected prediction error versus the actual prediction error.
2759214	2764738	A	0.7162402868270874	And they distinguish it from epsilon in the Ryan Smith paper because I did look at that, but I didn't find it in there.
2764824	2771810	B	0.6649912595748901	Yeah, it's in equation 27 under the section outcome prediction errors.
2772950	2787520	C	0.8597666025161743	You it may be that the expected means it's an expectation over the Q distribution, which is your distribution over belief states.
2808590	2809900	A	0.9736766815185547	Yeah, that's great.
2811390	2822846	A	0.9181089401245117	Ali and I and some others worked on this yesterday a lot and it's great that it kept you up late at night, clearly, Ali, so you went digging around for some more information.
2822948	2823870	A	0.9799191951751709	That's awesome.
2824020	2824750	A	0.8782029747962952	Yeah, great.
2824820	2825662	A	0.8497672080993652	Okay, perfect.
2825796	2828240	A	0.7432472705841064	Well, that resolves that very well.
2829570	2832580	A	0.8385162353515625	And then let's get into maybe the next question.
2845530	2849814	A	0.9151890277862549	In equation 4.16, the X has a dot over.
2849852	2854040	A	0.7591012120246887	It changed through time but not the Y data.
2854970	2858780	A	0.8471540212631226	So there's some discourse on this.
2860270	2863478	A	0.8043267726898193	Or maybe we should go to the next question, actually, because it's maybe more related.
2863574	2870320	A	0.7142494320869446	So, Figure 4.6 uses an epsilon for prediction error as described in equation 4.21.
2870770	2883890	A	0.8454049229621887	Is this predictive processing framing a part of the active inference model or is this presented for contrast to illustrate the similarities and differences between active inference and predictive processing?
2884390	2891938	A	0.8705618977546692	So this is equation 4.1 and this is the epsilon, but I think we see it way before equation 4.21.
2892024	2893062	A	0.6180369853973389	Like it's even there.
2893116	2907698	A	0.9112452268600464	In equation 4.4, it says this predictive coding schema is part of the active inference model illustrating the hierarchical structure of predictions and beliefs.
2907794	2916886	A	0.7921324968338013	The authors say one way to think about this is as if we had equipped a predictive coding scheme with classical reflex arcs at the lowest level of the hierarchy.
2916918	2923802	A	0.7416154146194458	And they give this reference in this setting, active inference is just predictive coding plus reflex arcs.
2923946	2925934	A	0.8677859902381897	Does anybody know what a reflex arc is?
2925972	2930962	A	0.6090741157531738	Because I'm a little bit lost with respect to that or have any comments on this prediction area?
2931016	2939250	B	0.5898674130439758	Yeah, reflex arc is basically the stimuli that doesn't go through the sensory cortex.
2950000	2952640	A	0.7752020359039307	I thought it was like a mathematical construct.
2959510	2961780	C	0.730118989944458	Maybe you think of it as a control system.
2963590	2964386	C	0.6415844559669495	No thinking.
2964488	2967250	C	0.48911112546920776	It's just like you bang your knee.
2967750	2969140	A	0.7665908336639404	Your foot goes up.
2970490	2971382	A	0.5077060461044312	That makes sense.
2971436	2974978	A	0.5465020537376404	And it says, we minimize free energy through action.
2975074	2979606	A	0.5926278829574585	And the only part of the free energy that depends on action is the lowest level of prediction error.
2979638	2987660	A	0.8160425424575806	In this hierarchical schema, action fulfills descending predictions by minimizing the error between the predicted and observed sensory data.
2999540	3004500	A	0.918262779712677	Any additional comments here about predictive coding and active inference?
3024190	3026806	A	0.8877778053283691	Or we can look at this equation 4.16.
3026838	3027994	A	0.808735728263855	The x has the dot over.
3028032	3030474	A	0.7558819651603699	It changed through time, but not the y data.
3030672	3032080	A	0.5209016799926758	Y is this.
3034290	3043890	A	0.8879753351211548	And here the discourse says, the top equation depends on f of x and v, which is a deterministic function describing how a hidden state changes over time.
3044040	3050482	A	0.8598925471305847	The bottom equation depends on g of x and v, which describes how data are generated from a single hidden state.
3050616	3053142	A	0.8749650120735168	The tilde indicates change through time.
3053196	3056150	A	0.8071826100349426	The dot indicates the first order derivative.
3061690	3065900	A	0.8660606741905212	Any comments here on this equation or your question?
3100790	3103666	A	0.8481516242027283	Maybe we can ask one more question, or maybe a couple more.
3103768	3113554	A	0.7155930399894714	So here it says on page 63, it says specifically, the Bayesian brain helps us frame the problems that an agent engaging in active inference must solve broadly.
3113602	3119030	A	0.588817298412323	These are the problem of inferring states of the world perception and inferring a course of action planning.
3119190	3125642	A	0.8659572005271912	Other than perception and action planning, are there other tasks or challenges that the brain or organisms engage in?
3125776	3127100	A	0.739942729473114	How would we know?
3128590	3132480	A	0.8052140474319458	There's no discourse here, but let's open it up and see what you guys think.
3153730	3158800	C	0.7942823171615601	Yeah, people wonder, hey, what's on that star out there?
3159810	3160990	C	0.6531654596328735	That's neither?
3161670	3162514	C	0.5666733384132385	I don't know.
3162632	3163954	C	0.7897142171859741	Is that inferring State of the world?
3163992	3165634	C	0.4713563323020935	I guess not.
3165672	3166530	C	0.670235812664032	Planning.
3169610	3173430	D	0.6285080909729004	And it could just be a hallucination of generative model.
3173500	3177720	D	0.5097943544387817	You're not really measuring anything, you're just measuring yourself over and over again.
3179050	3180600	D	0.6926809549331665	That could be one thing.
3199690	3206262	A	0.8815348148345947	Okay, so here there's another question for each of the graphical models in Figure 4.2.
3206316	3210090	A	0.8881622552871704	What is an intuitive example for each structure?
3230040	3232580	D	0.5113275647163391	One would be disease diagnosis.
3233500	3239800	D	0.6818027496337891	So you have symptom that is causative or predictive of some condition.
3248550	3251620	A	0.873917281627655	I think that was the example that they gave in the text, right?
3254630	3257140	A	0.8998909592628479	Or did we talk about this last week maybe?
3262250	3270520	B	0.5763605237007141	Yeah, I actually don't get the question because each of them we have an example, at least an example in the text.
3280310	3281540	A	0.6264640092849731	Yeah, that's true.
3285300	3289490	A	0.7849692106246948	And I think we're going to maybe see more examples as we go through the book.
3300580	3316360	B	0.8285489082336426	But just in case, if anyone wants to see some more examples actually, that paper, I put the link in the chat, realizing Active Inference in Variation Message Passing contains some more examples for each of these diagrams.
3322640	3324140	A	0.8674360513687134	This is a fun last question.
3324210	3340028	A	0.818688154220581	Maybe it says this reiterates that active inference uses two constructs, variational free energy and expected free energy, which are mathematically related but play distinct and complementary roles.
3340124	3344980	A	0.9054495692253113	In your own words, what are the definitions of variational free energy and expected free energy?
3345050	3349540	A	0.9077247977256775	And what role do they play individually or together in active inference?
3365890	3372562	D	0.8937698006629944	My view of expected free energy was given a certain action that might be taken.
3372616	3384440	D	0.8491933345794678	It might produce this or reduce the free energy or something, or it talks about some future reduction on free energy or something like that.
3384810	3387970	D	0.6546620726585388	Variational free energy is more objective.
3388130	3389800	D	0.678744912147522	I don't know how to put it.
3391470	3396860	D	0.7774689793586731	It's like, okay, actual kinetic energy versus perceived kinetic energy kind of thing.
3398670	3412766	D	0.8862085938453674	So if you're rolling down a hill, like rolling down a hill, we can actually measure the kinetic energy at the bottom of the hill, right?
3412948	3425380	D	0.8250340819358826	If you put the ball at the top, expected free energy would be like using a model to calculate that and then finding out actually corresponds if you look the model.
3425750	3427220	D	0.7233264446258545	At least that's my view.
3439070	3439806	A	0.6297038793563843	So I don't know.
3439828	3445262	A	0.8245800733566284	I'm probably like I think about this in a lot of different ways, but I think about variational free energy.
3445316	3460610	A	0.7419697642326355	I mean, this is one way to think about it, is maximizing the trade off between epistemic value and pragmatic value right now and expected for free energy is like maximizing that difference at some point in the future.
3460680	3462840	A	0.6808028817176819	So you can plan ahead.
3463370	3469800	A	0.7873522639274597	It might be cold out, so I'm going to take a jacket or like it's cold right now, I'm going to put my jacket on.
3472170	3475206	A	0.8840353488922119	That's how I think about the difference between those two.
3475388	3476518	A	0.8010196089744568	So it's ten.
3476604	3477782	A	0.975451648235321	Thanks, everyone, for coming.
3477836	3481142	A	0.5221500992774963	I hope I didn't muddle this up too much.
3481276	3485950	A	0.8350524306297302	Trying to be substitute teacher for Daniel, and it's ten.
3486020	3488222	A	0.8678328990936279	So we're going to have tools right now in this room.
3488276	3497710	A	0.6186272501945496	But if anybody wants to continue discussing these ideas and gather, you're welcome to migrate up to one of the different spaces.
3498850	3502140	A	0.7087975740432739	Yeah, and I think we'll stop the recording now, Alex, if you haven't already.

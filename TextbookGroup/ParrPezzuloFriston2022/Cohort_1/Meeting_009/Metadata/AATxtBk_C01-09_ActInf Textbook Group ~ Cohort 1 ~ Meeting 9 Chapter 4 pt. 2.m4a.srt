1
00:00:00,250 --> 00:00:04,334
Of these questions. I think we talked

2
00:00:04,372 --> 00:00:06,720
about the oh, this is chapter two.

3
00:00:07,650 --> 00:00:11,120
Let's go to chapter four. Okay,

4
00:00:12,850 --> 00:00:16,014
so we talked about belief, policy,

5
00:00:16,212 --> 00:00:19,214
state. I'm not sure if we talked about

6
00:00:19,252 --> 00:00:22,926
this question. In the discussion of

7
00:00:22,948 --> 00:00:26,514
active inference in POMDP t belief

8
00:00:26,562 --> 00:00:28,326
updating about policies, we find that

9
00:00:28,348 --> 00:00:30,214
the posterior that minimizes the free

10
00:00:30,252 --> 00:00:32,630
energy does the posterior at time t.

11
00:00:32,700 --> 00:00:35,766
Oh, we did discuss this. Yeah, we did

12
00:00:35,788 --> 00:00:40,570
discuss this last time. So this

13
00:00:40,640 --> 00:00:45,594
question, I think we

14
00:00:45,632 --> 00:00:48,426
did not discuss pi being a policy or a

15
00:00:48,448 --> 00:00:51,866
model last time. Or did we? I think we

16
00:00:51,888 --> 00:00:54,446
didn't get to this. Yeah, I don't think

17
00:00:54,468 --> 00:00:57,086
we discussed that. All right, so we can

18
00:00:57,108 --> 00:00:59,726
open it up and we'll start here, I

19
00:00:59,748 --> 00:01:02,366
guess, with the most upvoted. So the

20
00:01:02,388 --> 00:01:06,034
question reads, what is pi? On page 69,

21
00:01:06,072 --> 00:01:07,762
the authors write, at each time step,

22
00:01:07,816 --> 00:01:09,454
the current state is conditionally

23
00:01:09,502 --> 00:01:11,278
dependent on the state at the previous

24
00:01:11,374 --> 00:01:13,570
time and on the policy pi currently

25
00:01:13,640 --> 00:01:16,818
being pursued. Then on page 71,

26
00:01:16,904 --> 00:01:18,886
they write, thus we can interpret the

27
00:01:18,908 --> 00:01:22,118
priors of equation 4.6 combined with the

28
00:01:22,124 --> 00:01:25,026
likelihood of equation 4.5 as expressing

29
00:01:25,058 --> 00:01:27,574
a model pi of a behavioral sequence. So

30
00:01:27,612 --> 00:01:30,134
which is it, policy or model? And then

31
00:01:30,172 --> 00:01:32,918
they suggest a rewrite of the sentence.

32
00:01:33,014 --> 00:01:34,826
They say that, thus we can interpret the

33
00:01:34,848 --> 00:01:37,546
priors of equation 4.6 combined with the

34
00:01:37,568 --> 00:01:40,286
likelihood of equation 4.5 and the

35
00:01:40,308 --> 00:01:43,950
transition probabilities b sub tau pi

36
00:01:44,450 --> 00:01:46,538
as expressing a model for a behavioral

37
00:01:46,554 --> 00:01:47,934
sequence where the model is a function

38
00:01:47,972 --> 00:01:52,286
of policy pi. And then there's some

39
00:01:52,388 --> 00:01:55,534
discourse here. So in this reframing,

40
00:01:55,582 --> 00:01:57,426
can we say that the model simulates the

41
00:01:57,448 --> 00:01:59,746
agent in the environment as if it had

42
00:01:59,768 --> 00:02:01,700
taken the actions in the policy?

43
00:02:03,430 --> 00:02:06,774
And I don't know who asked that

44
00:02:06,812 --> 00:02:10,694
question. Oh, Eric, that was you.

45
00:02:10,732 --> 00:02:13,254
That was your question. And then does

46
00:02:13,292 --> 00:02:15,814
anyone want to maybe take a stab at

47
00:02:15,852 --> 00:02:17,914
answering that question? Or we can

48
00:02:17,952 --> 00:02:19,900
continue reading what was written here.

49
00:02:22,990 --> 00:02:26,442
So what's written here says

50
00:02:26,496 --> 00:02:29,466
that on page 69, the authors also say

51
00:02:29,568 --> 00:02:31,210
policies here may be thought of as

52
00:02:31,280 --> 00:02:33,274
indexing alternative trajectories or

53
00:02:33,312 --> 00:02:34,686
sequences of actions that could be

54
00:02:34,708 --> 00:02:37,246
followed. On page 71, they define the

55
00:02:37,268 --> 00:02:39,566
likelihood in equation four five as a

56
00:02:39,588 --> 00:02:42,010
matrix A that expresses the probability

57
00:02:42,090 --> 00:02:44,154
of an outcome. They describe the priors

58
00:02:44,202 --> 00:02:46,466
of equation 4.6 as the prior over the

59
00:02:46,488 --> 00:02:49,166
initial state vector D, and beliefs

60
00:02:49,198 --> 00:02:51,010
about how the state at one time

61
00:02:51,080 --> 00:02:52,722
transitions to the state at the next

62
00:02:52,776 --> 00:02:55,426
time, matrix B. And they also say that

63
00:02:55,448 --> 00:02:57,026
the transitions are conditionally

64
00:02:57,058 --> 00:02:59,334
dependent on the policy chosen. Because

65
00:02:59,372 --> 00:03:01,126
of this conditional dependence, we can

66
00:03:01,148 --> 00:03:03,494
see how the policy influences the model

67
00:03:03,612 --> 00:03:06,998
and why the authors may use the terms

68
00:03:07,084 --> 00:03:08,230
interchangeably.

69
00:03:12,990 --> 00:03:17,046
So does anyone have any comments

70
00:03:17,158 --> 00:03:23,290
there's?

71
00:03:24,850 --> 00:03:27,982
I guess I would say that that answer

72
00:03:28,036 --> 00:03:31,886
in discourse kind of agrees with

73
00:03:32,068 --> 00:03:35,806
what I proposed, which is that strictly

74
00:03:35,838 --> 00:03:39,780
speaking, we should treat the

75
00:03:40,870 --> 00:03:44,514
pi as policy, but it

76
00:03:44,552 --> 00:03:47,266
behaves as a model. Once it's executed,

77
00:03:47,298 --> 00:03:53,286
it's implemented. Then the

78
00:03:53,308 --> 00:03:55,686
model has become a function of the

79
00:03:55,708 --> 00:03:57,970
policy that's been we've seen so far.

80
00:03:58,140 --> 00:04:02,410
So I'd say that's compatible

81
00:04:03,550 --> 00:04:05,926
interpretation. So I think strictly

82
00:04:05,958 --> 00:04:06,540
speaking,

83
00:04:09,310 --> 00:04:11,342
it would be better if the text was

84
00:04:11,396 --> 00:04:15,534
consistent and called Pi policy and

85
00:04:15,652 --> 00:04:17,902
then say, yeah, the model is a function

86
00:04:17,956 --> 00:04:20,990
of Pi. That'd be my interpretation.

87
00:04:22,370 --> 00:04:24,066
Yeah, I definitely agree with that. I

88
00:04:24,088 --> 00:04:26,660
think that the model is policy specific,

89
00:04:28,950 --> 00:04:31,506
but they could do a better job of I

90
00:04:31,528 --> 00:04:35,022
mean, the variables as we've seen are so

91
00:04:35,176 --> 00:04:37,222
ambiguous anyway, so it could definitely

92
00:04:37,276 --> 00:04:38,360
be a lot better.

93
00:04:44,180 --> 00:04:47,024
And by the way, about the topic of

94
00:04:47,062 --> 00:04:49,856
consistency, actually I gave some

95
00:04:49,878 --> 00:04:51,708
thoughts about the issue we were

96
00:04:51,734 --> 00:04:54,020
discussing in yesterday's math learning

97
00:04:54,090 --> 00:04:56,932
session and I also consulted some other

98
00:04:56,986 --> 00:05:00,836
papers and active inference and in

99
00:05:00,858 --> 00:05:02,992
all of them the notation for matrices

100
00:05:03,056 --> 00:05:06,180
and vectors is used consistently,

101
00:05:06,260 --> 00:05:08,276
as in almost every linear algebra

102
00:05:08,308 --> 00:05:10,936
textbook. So I'm seriously beginning to

103
00:05:10,958 --> 00:05:13,716
suspect that every instance of a matrix

104
00:05:13,828 --> 00:05:16,824
or a vector not written in Bold face is

105
00:05:16,862 --> 00:05:19,640
a typo either in chapters or appendices

106
00:05:19,800 --> 00:05:21,644
because otherwise I really cannot find

107
00:05:21,682 --> 00:05:23,356
any justification behind using two

108
00:05:23,378 --> 00:05:25,656
different types of notation. So I'm

109
00:05:25,688 --> 00:05:27,512
going to use this assumption as my prior

110
00:05:27,576 --> 00:05:29,920
for the rest of the book, unless someone

111
00:05:29,990 --> 00:05:32,576
has any belief updating observation she

112
00:05:32,598 --> 00:05:33,570
wants to share.

113
00:05:36,740 --> 00:05:38,912
So just to summarize, for the people

114
00:05:38,966 --> 00:05:41,056
that weren't there yesterday, we had a

115
00:05:41,078 --> 00:05:43,940
big debate about what does the Bold

116
00:05:44,520 --> 00:05:47,636
typeface signify? And it wasn't so much

117
00:05:47,658 --> 00:05:50,804
a debate, but just like trying to come

118
00:05:50,842 --> 00:05:52,884
to a mutual understanding. So it's this

119
00:05:52,922 --> 00:05:55,272
question here. Does bold lettering mean

120
00:05:55,406 --> 00:05:57,864
this is a matrix, but seemingly also

121
00:05:57,902 --> 00:05:59,816
many non bold letters are a matrix? Or

122
00:05:59,838 --> 00:06:01,364
does bold lettering mean sufficient

123
00:06:01,412 --> 00:06:05,080
statistics of as on these page examples?

124
00:06:05,580 --> 00:06:09,668
And it really was yeah,

125
00:06:09,774 --> 00:06:12,920
we couldn't come to a single unified

126
00:06:13,000 --> 00:06:15,484
answer there because there are many

127
00:06:15,522 --> 00:06:16,876
instances where it seems like it should

128
00:06:16,898 --> 00:06:19,408
be a matrix, but it's not bold and there

129
00:06:19,414 --> 00:06:22,732
are many instances where sufficient

130
00:06:22,796 --> 00:06:25,584
statistics are used but it's also not

131
00:06:25,622 --> 00:06:28,512
bold. So yeah, we couldn't really

132
00:06:28,566 --> 00:06:31,068
conclude that, but I think I'll lead it.

133
00:06:31,094 --> 00:06:33,590
That's a good assumption there.

134
00:06:52,470 --> 00:06:54,866
So we also talked about this a little

135
00:06:54,888 --> 00:06:56,642
bit last time, but we didn't really get

136
00:06:56,696 --> 00:06:59,186
fully into this question. What is the

137
00:06:59,208 --> 00:07:01,346
use of categorical distributions in

138
00:07:01,368 --> 00:07:03,814
equation 4.5? The second line is

139
00:07:03,852 --> 00:07:05,394
supposed to explain the cat notation,

140
00:07:05,442 --> 00:07:07,046
but I have problems to understand the

141
00:07:07,068 --> 00:07:09,222
advantage of the notation. Could someone

142
00:07:09,276 --> 00:07:12,938
explain it in simple words? Does anyone

143
00:07:13,024 --> 00:07:15,578
want to try to explain that? I can pull

144
00:07:15,584 --> 00:07:16,490
up the equation.

145
00:07:23,600 --> 00:07:26,944
Well, I think every regular matrix and

146
00:07:26,982 --> 00:07:31,104
vector is by definition an

147
00:07:31,142 --> 00:07:34,576
array of ordered elements. But in a

148
00:07:34,598 --> 00:07:37,404
categorical distribution we don't

149
00:07:37,452 --> 00:07:40,852
necessarily assign any specific order

150
00:07:40,906 --> 00:07:43,988
to the elements. So the reason behind

151
00:07:44,074 --> 00:07:47,412
using the categorical distribution might

152
00:07:47,466 --> 00:07:50,540
be to just jettison

153
00:07:50,640 --> 00:07:54,152
any orderness in the elements and

154
00:07:54,286 --> 00:07:58,024
treat them as just an

155
00:07:58,062 --> 00:08:00,280
unordered set of the probabilities.

156
00:08:02,880 --> 00:08:04,988
I wrote an answer to this one if you

157
00:08:04,994 --> 00:08:07,724
want to pull that up, so you can just

158
00:08:07,762 --> 00:08:19,870
read that's.

159
00:08:37,460 --> 00:08:39,200
So I hope this is helpful.

160
00:08:41,540 --> 00:08:44,816
It's basically the way I see it is you

161
00:08:44,838 --> 00:08:47,024
have the math, the math notation there,

162
00:08:47,142 --> 00:08:48,876
but when you need to actually implement

163
00:08:48,908 --> 00:08:50,628
that, you need to carry it out, then you

164
00:08:50,634 --> 00:08:52,388
need to turn that into operations you

165
00:08:52,394 --> 00:08:56,276
can perform. And so when you

166
00:08:56,298 --> 00:09:00,660
say that A is this categorical

167
00:09:02,620 --> 00:09:04,548
object, that means that you're turning

168
00:09:04,564 --> 00:09:08,404
it into an actual matrix with elements.

169
00:09:08,532 --> 00:09:10,008
And so the ordering is important,

170
00:09:10,094 --> 00:09:12,616
actually, because the ordering of the

171
00:09:12,638 --> 00:09:14,524
elements in the matrix says how that

172
00:09:14,562 --> 00:09:17,528
matrix is going to apply to the objects

173
00:09:17,544 --> 00:09:20,456
it applies to, which are the belief

174
00:09:20,488 --> 00:09:24,510
state S, and then your observation O.

175
00:09:25,280 --> 00:09:28,800
So basically

176
00:09:28,870 --> 00:09:33,612
it says the way you carry out inferring

177
00:09:33,676 --> 00:09:36,464
what O will be the probability of O

178
00:09:36,502 --> 00:09:39,288
given a belief state is you do a matrix

179
00:09:39,324 --> 00:09:40,100
multiply.

180
00:09:42,200 --> 00:09:46,036
And because in

181
00:09:46,058 --> 00:09:46,870
this example,

182
00:09:49,240 --> 00:09:52,356
the states are not continuous, they're

183
00:09:52,388 --> 00:09:55,252
categorical, then you need a categorical

184
00:09:55,396 --> 00:09:59,156
matrix element in order to do map

185
00:09:59,188 --> 00:10:02,536
from the categories of belief states to

186
00:10:02,558 --> 00:10:05,292
the categories of observation states.

187
00:10:05,346 --> 00:10:08,124
And those are both distributions that

188
00:10:08,162 --> 00:10:10,780
gets transformed by the matrix.

189
00:10:13,600 --> 00:10:16,660
Thanks, Eric, that was super helpful.

190
00:10:16,840 --> 00:10:19,008
Does anybody have any further question

191
00:10:19,094 --> 00:10:20,770
or comment on this one?

192
00:10:25,310 --> 00:10:27,974
Sorry, Eric, but isn't the categorical

193
00:10:28,022 --> 00:10:29,974
notation special case of multinomial

194
00:10:30,022 --> 00:10:33,018
distribution? Categorical distribution

195
00:10:33,194 --> 00:10:35,258
special cases of multinomial

196
00:10:35,354 --> 00:10:36,270
distribution?

197
00:10:39,090 --> 00:10:42,414
I don't know that. Yeah,

198
00:10:42,532 --> 00:10:44,498
because that was my understanding that

199
00:10:44,584 --> 00:10:46,802
the main difference between any

200
00:10:46,856 --> 00:10:51,540
categorical distribution and well,

201
00:10:52,150 --> 00:10:52,980
I mean,

202
00:10:56,410 --> 00:10:59,414
for instance, of course you're right

203
00:10:59,452 --> 00:11:03,078
that in any matrix we necessarily have

204
00:11:03,164 --> 00:11:06,038
some ordered elements in order for us to

205
00:11:06,044 --> 00:11:08,554
be able to compute the math on those

206
00:11:08,592 --> 00:11:11,420
matrices. But on the other hand,

207
00:11:12,350 --> 00:11:15,706
for categorical distributions, at least

208
00:11:15,728 --> 00:11:19,030
that was my understanding that we don't

209
00:11:19,110 --> 00:11:22,718
necessarily treat them as

210
00:11:22,884 --> 00:11:25,534
ordered elements, such as, I don't know,

211
00:11:25,572 --> 00:11:28,730
the ordered vectors or ordered matrices

212
00:11:28,810 --> 00:11:31,754
as the normal matrix.

213
00:11:31,802 --> 00:11:34,980
Because if that was the case, well,

214
00:11:35,590 --> 00:11:38,914
we could just designate this

215
00:11:38,952 --> 00:11:42,542
distribution as just a regular matrix.

216
00:11:42,606 --> 00:11:46,310
Right. So I don't get what's the reason

217
00:11:46,380 --> 00:11:50,034
behind using the notation

218
00:11:50,162 --> 00:11:53,720
here, cat A or cat,

219
00:11:56,970 --> 00:11:59,366
instead of just using the. Name of the

220
00:11:59,388 --> 00:12:00,230
matrix?

221
00:12:03,150 --> 00:12:05,446
Steven, can you mute you're super loud

222
00:12:05,478 --> 00:12:06,220
over there.

223
00:12:19,450 --> 00:12:20,200
Sorry.

224
00:12:22,970 --> 00:12:24,840
Yeah, I guess so.

225
00:12:28,170 --> 00:12:29,818
The mathematical notation doesn't have

226
00:12:29,824 --> 00:12:31,814
an ordering. It just says distribution

227
00:12:31,862 --> 00:12:33,750
is related to another distribution

228
00:12:33,910 --> 00:12:37,194
through this

229
00:12:37,232 --> 00:12:39,306
relation. And when you get to the

230
00:12:39,328 --> 00:12:42,698
mechanics of how do you do that? That's

231
00:12:42,714 --> 00:12:44,458
where you care that you have the matrix

232
00:12:44,554 --> 00:12:47,262
with the vector ordering. So,

233
00:12:47,316 --> 00:12:51,150
yeah, I don't know what cat A

234
00:12:51,220 --> 00:12:54,754
actually means other than it's just

235
00:12:54,792 --> 00:12:57,986
describing it. I don't think it's like

236
00:12:58,008 --> 00:12:59,474
an operator, I think it's just saying

237
00:12:59,512 --> 00:13:02,180
what it is. Does that make sense?

238
00:13:13,790 --> 00:13:15,470
Steven, if you're trying to talk, we

239
00:13:15,540 --> 00:13:16,480
can't hear.

240
00:13:18,770 --> 00:13:19,520
What.

241
00:13:35,860 --> 00:13:38,004
I feel like Steven is trying to share an

242
00:13:38,042 --> 00:13:40,644
idea with us, but is unable to

243
00:13:40,762 --> 00:13:42,644
communicate effectively through the

244
00:13:42,682 --> 00:13:44,436
technological affordances he's been

245
00:13:44,458 --> 00:13:47,716
given at the time. Like the gain on your

246
00:13:47,738 --> 00:13:49,988
mic is way up or something, so it's

247
00:13:50,004 --> 00:13:51,416
picking up more background noise than

248
00:13:51,438 --> 00:13:57,160
you're.

249
00:14:42,020 --> 00:14:44,608
Just looking at this equation. Can you

250
00:14:44,614 --> 00:14:45,810
guys hear me okay?

251
00:14:53,560 --> 00:14:56,804
Yeah. Okay, so just looking at this

252
00:14:56,842 --> 00:14:59,830
equation, the second part,

253
00:15:02,600 --> 00:15:03,910
it's like the,

254
00:15:06,700 --> 00:15:14,440
it's like an ordered matrix A-I-J where

255
00:15:14,510 --> 00:15:17,950
the probability of the observations are

256
00:15:18,480 --> 00:15:21,404
equal to index I and the states equal to

257
00:15:21,442 --> 00:15:24,030
index j, something like that.

258
00:15:25,680 --> 00:15:28,284
I don't know, it looks like it is

259
00:15:28,322 --> 00:15:31,984
ordered. Yeah, the matrix itself is

260
00:15:32,022 --> 00:15:36,256
ordered. Yeah. And so the second line is

261
00:15:36,438 --> 00:15:39,004
actually by how the authors are defining

262
00:15:39,052 --> 00:15:42,964
this cat A. So cat A represents this

263
00:15:43,002 --> 00:15:44,580
ordered matrix AIG?

264
00:15:46,920 --> 00:15:47,670
Yeah,

265
00:15:50,920 --> 00:15:52,948
that's at least what it says in the

266
00:15:52,954 --> 00:15:53,780
textbook.

267
00:16:18,190 --> 00:16:20,266
So it says the likelihood expressing the

268
00:16:20,288 --> 00:16:22,166
probability of observations conditioned

269
00:16:22,198 --> 00:16:25,650
on time tau given states s conditioned

270
00:16:25,670 --> 00:16:28,154
on time tau is equal to the categorical

271
00:16:28,202 --> 00:16:29,680
distribution A.

272
00:16:45,990 --> 00:16:48,962
Sorry. Yaku, go ahead.

273
00:16:49,016 --> 00:16:52,382
Sorry, my internet is lagging a bit, so

274
00:16:52,456 --> 00:16:54,822
there might be a delay and I might break

275
00:16:54,876 --> 00:16:56,918
off. But I just want to comment that I

276
00:16:56,924 --> 00:17:00,342
think that the cat notation is really

277
00:17:00,396 --> 00:17:04,390
just to reinforce

278
00:17:05,290 --> 00:17:07,322
the notion that it's in discrete time.

279
00:17:07,376 --> 00:17:10,826
But I think it's kind of redundant in

280
00:17:10,848 --> 00:17:13,338
this case because since we know that we

281
00:17:13,344 --> 00:17:14,874
are in discrete time, we could have just

282
00:17:14,912 --> 00:17:18,558
written a bolt to

283
00:17:18,724 --> 00:17:20,318
signify that it's a matrix and where

284
00:17:20,324 --> 00:17:21,758
it's a matrix, it's probably not going

285
00:17:21,764 --> 00:17:23,760
to be a continuous distribution anyway.

286
00:17:25,890 --> 00:17:29,514
So I think cat bold A and bold

287
00:17:29,562 --> 00:17:31,330
A is pretty much interchangeable.

288
00:17:35,470 --> 00:17:38,842
So I think that some of the new math is

289
00:17:38,896 --> 00:17:41,050
using some of the affordances from

290
00:17:41,120 --> 00:17:43,978
category theory more like some of the

291
00:17:44,064 --> 00:17:46,560
newer things. And I think the forward

292
00:17:47,090 --> 00:17:50,766
direction that Karl and Thomas and some

293
00:17:50,788 --> 00:17:52,458
of the people that really helped derive

294
00:17:52,554 --> 00:17:55,006
and advance some of this math, I think

295
00:17:55,028 --> 00:17:56,650
that they're going to be leveraging

296
00:17:56,730 --> 00:17:59,026
category theory methods like the

297
00:17:59,048 --> 00:18:00,814
renormalization group and the pullback

298
00:18:00,862 --> 00:18:03,314
attractor more in the future. And so

299
00:18:03,352 --> 00:18:06,290
that might be also part of the reason

300
00:18:06,360 --> 00:18:08,894
that they are moving toward this

301
00:18:08,952 --> 00:18:11,782
categorical notation. So just a little

302
00:18:11,836 --> 00:18:14,760
maybe foreshadowing there too.

303
00:18:20,810 --> 00:18:23,478
May I just make one additional note

304
00:18:23,494 --> 00:18:26,010
about this categorical distribution?

305
00:18:26,590 --> 00:18:29,340
Well, the thing that confused me about

306
00:18:29,950 --> 00:18:31,974
this concept of categorical distribution

307
00:18:32,022 --> 00:18:34,750
is the Wikipedia's definition of it,

308
00:18:34,820 --> 00:18:37,466
because here it says there is no innate

309
00:18:37,498 --> 00:18:39,610
underlying ordering of these outcomes,

310
00:18:39,690 --> 00:18:41,946
but numerical labels are often attached

311
00:18:41,978 --> 00:18:44,654
for convenience in describing the

312
00:18:44,692 --> 00:18:48,290
distribution. So that was my confusion.

313
00:18:55,940 --> 00:18:57,840
I don't think it would be the first time

314
00:18:57,910 --> 00:19:00,732
that we've seen traditional mathematical

315
00:19:00,796 --> 00:19:03,824
notation slightly and somewhat abused in

316
00:19:03,862 --> 00:19:06,196
active inference. Probably also won't be

317
00:19:06,218 --> 00:19:09,456
the last. But definitely I think it's

318
00:19:09,488 --> 00:19:12,468
a point that we can put forward to the

319
00:19:12,474 --> 00:19:14,580
authors for additional clarification.

320
00:19:17,020 --> 00:19:19,864
If there's no further anybody has

321
00:19:19,902 --> 00:19:21,304
anything else to say about the

322
00:19:21,342 --> 00:19:24,456
categorical distributions here, if not,

323
00:19:24,478 --> 00:19:25,864
we can move on to maybe the next

324
00:19:25,902 --> 00:19:26,490
question.

325
00:19:35,470 --> 00:19:38,170
So this next question reads in message

326
00:19:38,240 --> 00:19:40,778
passing, is there a decay of information

327
00:19:40,944 --> 00:19:43,658
as the distance between the variable X

328
00:19:43,824 --> 00:19:45,994
and individual Markov blanket

329
00:19:46,042 --> 00:19:48,590
constituents increases? Is

330
00:19:48,660 --> 00:19:51,006
implementation of information decay? In

331
00:19:51,108 --> 00:19:54,270
message passing an option for model

332
00:19:54,340 --> 00:19:57,006
implementation? I think that this is a

333
00:19:57,028 --> 00:19:58,602
super interesting question and it's

334
00:19:58,666 --> 00:20:00,394
something that I've thought about before

335
00:20:00,532 --> 00:20:03,860
a lot. But I think,

336
00:20:04,310 --> 00:20:06,466
yeah, let me open it up to anyone and

337
00:20:06,488 --> 00:20:08,066
see if they want to address it before we

338
00:20:08,088 --> 00:20:10,530
can get into some of the discourse.

339
00:20:15,750 --> 00:20:19,160
Well, doesn't that all depend upon the

340
00:20:20,970 --> 00:20:23,398
chain of loss between one stage of the

341
00:20:23,404 --> 00:20:24,646
message passing and another? And if you

342
00:20:24,668 --> 00:20:26,954
have no loss, then you don't have any

343
00:20:26,992 --> 00:20:30,074
decay. But the more stages you have

344
00:20:30,112 --> 00:20:33,254
where you get a little bit of diffusion

345
00:20:33,302 --> 00:20:34,842
or loss, then you're going to have

346
00:20:34,896 --> 00:20:36,718
decay, just a function of the

347
00:20:36,724 --> 00:20:40,202
parameters. Well, so are messages passed

348
00:20:40,266 --> 00:20:43,358
within the same Markov blanket or

349
00:20:43,444 --> 00:20:47,374
between things that are partitioned with

350
00:20:47,412 --> 00:20:49,138
their own Markov blanket? That's another

351
00:20:49,224 --> 00:20:49,860
question.

352
00:21:04,640 --> 00:21:08,284
I also agree with Eric, as I mentioned

353
00:21:08,482 --> 00:21:12,032
yesterday. I also believe that,

354
00:21:12,166 --> 00:21:16,540
well, this information loss

355
00:21:16,620 --> 00:21:19,660
is not accounted for at least inherently

356
00:21:19,740 --> 00:21:22,930
in active inference formalism because

357
00:21:23,320 --> 00:21:26,752
it depends on the specific situations

358
00:21:26,816 --> 00:21:29,972
that those

359
00:21:30,026 --> 00:21:32,820
information propagate because otherwise

360
00:21:33,800 --> 00:21:37,816
we couldn't have any general

361
00:21:37,918 --> 00:21:41,256
or all encompassing formalism for

362
00:21:41,358 --> 00:21:44,200
accounting for these information loss.

363
00:21:47,580 --> 00:21:51,116
So we see in this box they

364
00:21:51,138 --> 00:21:56,044
talk about Markov blankets and the

365
00:21:56,082 --> 00:21:58,376
message passing. So here is variational

366
00:21:58,408 --> 00:22:00,552
message passing. This involves messages

367
00:22:00,616 --> 00:22:02,808
from all constituents of the Markov

368
00:22:02,824 --> 00:22:05,516
blanket of X, including the parents via

369
00:22:05,548 --> 00:22:07,104
the conditional probability of X given

370
00:22:07,142 --> 00:22:09,612
its parents and the children. The latter

371
00:22:09,676 --> 00:22:11,228
depends on the conditional probability

372
00:22:11,324 --> 00:22:12,976
of the children of X given all of their

373
00:22:12,998 --> 00:22:15,136
parents, which include X. Note the

374
00:22:15,158 --> 00:22:16,596
expectation includes the children and

375
00:22:16,618 --> 00:22:18,116
parents of the children as parents of

376
00:22:18,138 --> 00:22:20,196
the children x, we divide by Q of X to

377
00:22:20,218 --> 00:22:21,956
ensure the expectation includes the

378
00:22:21,978 --> 00:22:25,604
blanket only. So I think

379
00:22:25,642 --> 00:22:28,232
the messages are like all of the things

380
00:22:28,286 --> 00:22:30,344
inside of the blanket, send a message to

381
00:22:30,382 --> 00:22:32,648
something else in another blanket. At

382
00:22:32,654 --> 00:22:36,408
least that's how I interpret this

383
00:22:36,574 --> 00:22:40,204
way of active inference. And so it would

384
00:22:40,242 --> 00:22:43,964
make sense that from

385
00:22:44,002 --> 00:22:45,644
within the blanket, does everyone have

386
00:22:45,682 --> 00:22:47,676
the same is there like a high degree of

387
00:22:47,698 --> 00:22:50,136
mutual information shared in the Markov

388
00:22:50,168 --> 00:22:53,020
blanket? I think conditional dependence

389
00:22:53,100 --> 00:22:57,010
is what qualifies as

390
00:22:57,860 --> 00:23:00,048
conditional independence is what

391
00:23:00,214 --> 00:23:02,432
establishes the markup blanket. So

392
00:23:02,486 --> 00:23:04,624
within constituents under the blanket,

393
00:23:04,672 --> 00:23:05,924
do they all have all of the same

394
00:23:05,962 --> 00:23:07,616
information all of the time? That's

395
00:23:07,648 --> 00:23:09,364
something that's kind of confusing to

396
00:23:09,402 --> 00:23:13,556
me. One thing I'd point out about

397
00:23:13,738 --> 00:23:18,730
figure 4.3,

398
00:23:21,100 --> 00:23:24,744
so in the previous chapters we

399
00:23:24,782 --> 00:23:27,384
have this picture of there being the

400
00:23:27,422 --> 00:23:32,684
world and your model of the world and

401
00:23:32,722 --> 00:23:36,216
your model of the world is a Markov

402
00:23:36,248 --> 00:23:38,396
blanket itself and it interfaces with

403
00:23:38,418 --> 00:23:41,204
the world through the observation

404
00:23:41,272 --> 00:23:44,588
variables and the action variables.

405
00:23:44,764 --> 00:23:46,224
This picture doesn't have that

406
00:23:46,262 --> 00:23:48,736
breakdown. So it's got state there, but

407
00:23:48,758 --> 00:23:50,672
it doesn't say is that state all

408
00:23:50,726 --> 00:23:53,824
internal state? Is it external state or

409
00:23:53,862 --> 00:23:55,664
a mixture of them? And it apparently is

410
00:23:55,702 --> 00:23:57,764
mixture because you've got policy that

411
00:23:57,802 --> 00:23:59,636
can affect state, which means that

412
00:23:59,658 --> 00:24:01,576
you're going to have action, is going to

413
00:24:01,578 --> 00:24:03,924
be part of the state transition as well

414
00:24:03,962 --> 00:24:05,712
as and then you have the observations

415
00:24:05,776 --> 00:24:09,256
coming out the bottom. But I

416
00:24:09,278 --> 00:24:10,264
think if we're going to talk about

417
00:24:10,302 --> 00:24:12,456
Markov blankets, then I would like to

418
00:24:12,478 --> 00:24:16,456
see this picture be exploded into the

419
00:24:16,478 --> 00:24:19,164
state of the machine versus the state of

420
00:24:19,202 --> 00:24:21,180
the external world and how those

421
00:24:21,330 --> 00:24:22,684
interact with one another. Because

422
00:24:22,722 --> 00:24:24,396
that's what the markup blanket does. Is

423
00:24:24,418 --> 00:24:26,190
it compartmentalizes the information?

424
00:24:32,930 --> 00:24:37,074
Well, that is the POMDP model and

425
00:24:37,112 --> 00:24:40,850
Figure 4.3 and this really Figure

426
00:24:40,920 --> 00:24:44,846
4.4 makes me scratch

427
00:24:44,878 --> 00:24:47,166
my head quite a bit. This is the image

428
00:24:47,198 --> 00:24:52,118
that is going to represent message

429
00:24:52,204 --> 00:24:55,800
passing in a Bayesian framework. And so

430
00:24:56,810 --> 00:24:58,902
the caption says on the right

431
00:24:59,036 --> 00:25:01,174
dependencies between different variables

432
00:25:01,222 --> 00:25:03,462
in the belief updating scheme outlined

433
00:25:03,526 --> 00:25:07,002
in the main text. Intuitively, current

434
00:25:07,056 --> 00:25:08,858
beliefs about states under each policy

435
00:25:08,944 --> 00:25:10,794
at each time are compared with those

436
00:25:10,832 --> 00:25:13,146
that would be predicted given beliefs

437
00:25:13,178 --> 00:25:15,214
about states at other times, this is

438
00:25:15,252 --> 00:25:19,242
one. And current outcomes to calculate

439
00:25:19,306 --> 00:25:21,454
prediction errors. These errors then

440
00:25:21,492 --> 00:25:24,320
drive updating and beliefs, this is two.

441
00:25:24,930 --> 00:25:27,074
Given beliefs about states under each

442
00:25:27,112 --> 00:25:28,626
policy we can then calculate the

443
00:25:28,648 --> 00:25:30,418
gradients of the expected free energy.

444
00:25:30,504 --> 00:25:34,174
This is three. These are combined

445
00:25:34,222 --> 00:25:36,322
with the outcomes predicted under each

446
00:25:36,376 --> 00:25:38,246
policy omitted from the figure. To

447
00:25:38,268 --> 00:25:40,598
compute beliefs about policies, this is

448
00:25:40,684 --> 00:25:44,006
four. And then using a Bayesian model

449
00:25:44,108 --> 00:25:45,906
average, we can then compute posterior

450
00:25:45,938 --> 00:25:47,862
beliefs about states averaged over

451
00:25:47,916 --> 00:25:49,960
policies. This is five.

452
00:25:55,050 --> 00:25:57,190
And this kind of leads us into the next

453
00:25:57,260 --> 00:25:59,642
question. But I am not able to really

454
00:25:59,696 --> 00:26:03,070
interpret this Figure 4.4 in any better

455
00:26:03,140 --> 00:26:05,194
way. So if anybody has like a plain

456
00:26:05,242 --> 00:26:08,590
English description of how

457
00:26:08,740 --> 00:26:11,440
this works, I would love to hear it.

458
00:26:28,300 --> 00:26:31,496
We can talk a little bit about a

459
00:26:31,518 --> 00:26:33,064
question that was raised about this

460
00:26:33,102 --> 00:26:35,684
figure and also about this squiggle

461
00:26:35,732 --> 00:26:39,336
sigma because there's a great I

462
00:26:39,358 --> 00:26:42,300
don't know, we kind of had a hard time

463
00:26:42,370 --> 00:26:44,876
figuring out the equation and the

464
00:26:44,898 --> 00:26:47,020
squiggle sigma and what that actually

465
00:26:47,090 --> 00:26:49,420
stands for. So we could move to that

466
00:26:49,570 --> 00:26:51,024
discussion because I think it's maybe

467
00:26:51,062 --> 00:26:53,404
related to this message passing

468
00:26:53,532 --> 00:26:55,596
discussion. But let's read the discourse

469
00:26:55,628 --> 00:26:59,840
here first on the message

470
00:26:59,910 --> 00:27:01,570
passing question.

471
00:27:04,280 --> 00:27:07,380
So in the discourse,

472
00:27:09,160 --> 00:27:11,728
what is the mechanism for signal decay?

473
00:27:11,824 --> 00:27:14,384
So this is information decay,

474
00:27:14,432 --> 00:27:16,816
implementation of information decay. Is

475
00:27:16,858 --> 00:27:18,696
the message a thing behaving as an

476
00:27:18,718 --> 00:27:21,384
active inference agent itself or a

477
00:27:21,422 --> 00:27:22,824
special piece of information not

478
00:27:22,862 --> 00:27:24,440
behaving as an active inference system

479
00:27:24,510 --> 00:27:27,620
itself? Is there a blanket impedance

480
00:27:27,700 --> 00:27:29,568
mismatch? I think Brock contributed

481
00:27:29,604 --> 00:27:31,310
this. I'm not sure if he's here today.

482
00:27:31,840 --> 00:27:34,248
And then message passing as described

483
00:27:34,264 --> 00:27:35,916
through active inference usually is a

484
00:27:35,938 --> 00:27:38,684
hierarchy, not a lateral transfer like

485
00:27:38,722 --> 00:27:41,212
within constituents in the blanket. So

486
00:27:41,266 --> 00:27:42,748
the key thing here is how you define the

487
00:27:42,754 --> 00:27:44,984
markup blanket. Either there's a

488
00:27:45,122 --> 00:27:47,696
partition between particles, cells or

489
00:27:47,718 --> 00:27:49,088
whatever, or there's no partition in

490
00:27:49,094 --> 00:27:50,476
blanket and everything under the blanket

491
00:27:50,508 --> 00:27:52,480
is conditionally dependent. Does

492
00:27:52,550 --> 00:27:54,512
conditional dependence preclude message

493
00:27:54,566 --> 00:27:56,876
passing? Our message passed within nodes

494
00:27:56,908 --> 00:27:58,436
in the same blanket. I have not seen

495
00:27:58,458 --> 00:27:59,936
this and would love if someone pointed

496
00:27:59,968 --> 00:28:02,596
me to some references. So that was me.

497
00:28:02,618 --> 00:28:05,452
I wrote the last part because I haven't

498
00:28:05,536 --> 00:28:07,864
seen message passing under the same

499
00:28:07,902 --> 00:28:09,464
Markov blanket. It's always been like

500
00:28:09,502 --> 00:28:12,852
from one Markov blanket partitioned

501
00:28:12,916 --> 00:28:15,156
object to a different Markov blanket

502
00:28:15,188 --> 00:28:19,144
partitioned object. But that

503
00:28:19,182 --> 00:28:21,164
moves us into if anybody has any

504
00:28:21,202 --> 00:28:23,244
comments, feel free. Well, I would say

505
00:28:23,282 --> 00:28:24,636
message passing is used to do the

506
00:28:24,658 --> 00:28:26,264
inference. So it's within a markup

507
00:28:26,312 --> 00:28:29,790
blanket. That's pretty common, I think

508
00:28:31,460 --> 00:28:33,330
the way message passes is used.

509
00:28:35,460 --> 00:28:38,912
So here, as it's shown in Figure 4.4,

510
00:28:39,046 --> 00:28:41,984
although this is totally beyond me to

511
00:28:42,022 --> 00:28:44,592
describe here, this is a temporal

512
00:28:44,656 --> 00:28:46,324
message passing. So it's like from one

513
00:28:46,362 --> 00:28:49,316
time point to another time point and

514
00:28:49,338 --> 00:28:51,156
then on the left it depicts the

515
00:28:51,178 --> 00:28:54,032
hierarchical, like a hierarchical

516
00:28:54,096 --> 00:28:56,724
expansion collapsing over time steps but

517
00:28:56,762 --> 00:28:58,356
that a higher level network might

518
00:28:58,378 --> 00:28:59,928
predict the states and policies at the

519
00:28:59,934 --> 00:29:01,752
lower level and use these to draw

520
00:29:01,806 --> 00:29:03,624
inferences about the context in which

521
00:29:03,662 --> 00:29:05,992
these occur. So the way that I've always

522
00:29:06,046 --> 00:29:08,184
seen the message passing used is from

523
00:29:08,222 --> 00:29:10,124
one time point to another or from one

524
00:29:10,162 --> 00:29:13,176
layer of a hierarchy to the next layer

525
00:29:13,208 --> 00:29:15,164
of a hierarchy and I've not seen it

526
00:29:15,202 --> 00:29:18,716
passed within constituents under the

527
00:29:18,738 --> 00:29:20,556
same Markov blanket. So Eric, if you

528
00:29:20,578 --> 00:29:23,904
have references that depict some kind of

529
00:29:23,942 --> 00:29:25,836
message passing in active inference

530
00:29:25,868 --> 00:29:28,304
that's not through time steps or through

531
00:29:28,342 --> 00:29:30,864
hierarchical levels, I would love to see

532
00:29:30,902 --> 00:29:32,944
that because I have not seen it. Well,

533
00:29:32,982 --> 00:29:36,564
I would just go back to box 4.1 where

534
00:29:36,602 --> 00:29:38,132
they talk about the variational message

535
00:29:38,186 --> 00:29:41,028
passing and how you get information.

536
00:29:41,194 --> 00:29:43,376
And I would say within a blanket you've

537
00:29:43,408 --> 00:29:46,804
got various nodes and they have this

538
00:29:46,842 --> 00:29:49,572
hierarchical parent child relations and

539
00:29:49,626 --> 00:29:52,068
in order to do inference about, hey,

540
00:29:52,234 --> 00:29:53,544
what's our belief in one of these

541
00:29:53,582 --> 00:29:55,496
parents? You got to kind of go up and

542
00:29:55,518 --> 00:29:57,032
down and say well, what are the children

543
00:29:57,086 --> 00:29:58,484
of that and what do the other parents

544
00:29:58,532 --> 00:30:02,104
think? So that's all within a single

545
00:30:02,142 --> 00:30:03,576
Markov blanket, the parents and

546
00:30:03,598 --> 00:30:05,324
children, the hierarchical relation and

547
00:30:05,362 --> 00:30:08,216
the message passing happens to circulate

548
00:30:08,248 --> 00:30:10,228
that information within the blanket.

549
00:30:10,424 --> 00:30:12,050
That's how I read.

550
00:30:15,380 --> 00:30:18,880
And I also put a link here to a paper by

551
00:30:19,030 --> 00:30:22,608
Champion at all in which they have

552
00:30:22,694 --> 00:30:26,064
actually explicitly defined the active

553
00:30:26,112 --> 00:30:29,664
inference, or better say, reformulated

554
00:30:29,712 --> 00:30:32,176
the active inference formalism according

555
00:30:32,208 --> 00:30:35,916
to variation of message passing.

556
00:30:36,048 --> 00:30:38,744
And if you just give me a second, I can

557
00:30:38,782 --> 00:30:42,410
find exactly the place where

558
00:30:42,780 --> 00:30:46,344
they have stated this in

559
00:30:46,382 --> 00:30:47,400
plain English.

560
00:31:08,700 --> 00:31:10,284
You can continue with the other

561
00:31:10,322 --> 00:31:13,196
questions if you like, but I need a

562
00:31:13,218 --> 00:31:16,380
moment to check this paper. Sure, yeah,

563
00:31:16,450 --> 00:31:17,836
I'll check it out also and yeah, thank

564
00:31:17,858 --> 00:31:19,576
you for that. So like when I'm reading

565
00:31:19,608 --> 00:31:21,564
this variational message passing, this

566
00:31:21,602 --> 00:31:23,548
involves messages from all constituents

567
00:31:23,564 --> 00:31:25,776
of the Markov blanket of X. So the

568
00:31:25,798 --> 00:31:27,456
message is coming from all the

569
00:31:27,478 --> 00:31:29,136
constituents in the blanket, but it

570
00:31:29,158 --> 00:31:30,930
doesn't say where the message is going.

571
00:31:31,380 --> 00:31:34,676
So it does make sense, I guess, that

572
00:31:34,698 --> 00:31:37,476
it's an interchangeable or within would

573
00:31:37,498 --> 00:31:40,004
be a better word than from within all

574
00:31:40,042 --> 00:31:41,776
constituents of the Markov blanket

575
00:31:41,808 --> 00:31:42,948
because I was reading it as like the

576
00:31:42,954 --> 00:31:44,824
message is coming from everything under

577
00:31:44,862 --> 00:31:46,776
that blanket, which I guess is an

578
00:31:46,798 --> 00:31:48,360
incorrect interpretation.

579
00:31:52,490 --> 00:31:54,694
So thank you Eric for pointing that out

580
00:31:54,732 --> 00:31:56,040
because I was like what?

581
00:32:10,380 --> 00:32:13,836
So this is a really hard question that

582
00:32:13,858 --> 00:32:16,748
I looked to try to answer. Actually a

583
00:32:16,754 --> 00:32:19,036
bunch of us looked yesterday a lot. So

584
00:32:19,138 --> 00:32:22,364
it says in equation four point ten, the

585
00:32:22,402 --> 00:32:25,964
sigma variable, squiggle sigma,

586
00:32:26,012 --> 00:32:27,696
we'll call it whatever, I'm not sure how

587
00:32:27,718 --> 00:32:29,776
to say it is used to describe the

588
00:32:29,798 --> 00:32:31,456
difference between the natural log of

589
00:32:31,478 --> 00:32:33,680
observations conditioned on policy and

590
00:32:33,750 --> 00:32:35,936
preferences. What is the function of

591
00:32:35,958 --> 00:32:37,904
this variable? It also comes up in

592
00:32:37,942 --> 00:32:39,924
figure 4.4, the message passing figure

593
00:32:39,962 --> 00:32:41,796
we're just looking at, and it would be

594
00:32:41,818 --> 00:32:43,316
great to have a verbal description of

595
00:32:43,338 --> 00:32:45,044
this figure, what other papers or

596
00:32:45,082 --> 00:32:48,004
equations use this squiggle sigma? And

597
00:32:48,122 --> 00:32:49,956
we looked at length, like through the

598
00:32:49,978 --> 00:32:52,424
entire textbook, through the other

599
00:32:52,462 --> 00:32:54,196
chapters it refers to like we'll unpack

600
00:32:54,228 --> 00:32:55,704
this later in chapter seven. We looked

601
00:32:55,742 --> 00:32:57,096
through chapter seven, we looked through

602
00:32:57,118 --> 00:32:59,224
the appendices, we saw a variable that

603
00:32:59,262 --> 00:33:00,868
almost looks like that, but I think it's

604
00:33:00,884 --> 00:33:02,364
a gamma. It looks like a little bit more

605
00:33:02,402 --> 00:33:05,004
fancy than this squiggle sigma and could

606
00:33:05,042 --> 00:33:08,204
not find that at all.

607
00:33:08,322 --> 00:33:12,016
So we went to unpack equation 410.

608
00:33:12,018 --> 00:33:17,184
And

609
00:33:17,222 --> 00:33:21,404
so here is the equation

610
00:33:21,452 --> 00:33:25,056
itself. I'm not

611
00:33:25,078 --> 00:33:26,384
sure how to pull this up next to the

612
00:33:26,422 --> 00:33:28,070
actually maybe I'll put it up here,

613
00:33:30,600 --> 00:33:31,780
419.

614
00:33:35,000 --> 00:33:38,776
And this equation four point ten is a

615
00:33:38,798 --> 00:33:41,572
rewriting of equation 4.7 in linear

616
00:33:41,636 --> 00:33:45,400
algebraic form. So we tried to

617
00:33:45,550 --> 00:33:47,816
kind of unpack this a little bit. So the

618
00:33:47,838 --> 00:33:51,380
first line states the prior probability

619
00:33:51,540 --> 00:33:53,804
for each policy. This is that pi sub

620
00:33:53,842 --> 00:33:56,636
zero is equal to the soft maps function

621
00:33:56,738 --> 00:33:59,244
sigma times the negative expected free

622
00:33:59,282 --> 00:34:02,716
energy G. And so that's

623
00:34:02,748 --> 00:34:05,984
this first line. The next line is the

624
00:34:06,022 --> 00:34:07,984
expected free energy conditioned on

625
00:34:08,022 --> 00:34:10,684
policy g sub pi is equal to the entropy

626
00:34:10,732 --> 00:34:14,416
or negative expected log probability h

627
00:34:14,518 --> 00:34:16,492
times the states conditioned on policy

628
00:34:16,566 --> 00:34:19,984
and time, which is S sub pi times tau

629
00:34:20,032 --> 00:34:23,364
or both of those sub pi and tau plus the

630
00:34:23,402 --> 00:34:25,524
observations conditioned on policy and

631
00:34:25,562 --> 00:34:28,920
time. This O sub pi times tau times

632
00:34:28,990 --> 00:34:31,544
the beliefs conditioned on policy and

633
00:34:31,582 --> 00:34:34,712
time, which is a squiggle sigma maybe

634
00:34:34,846 --> 00:34:37,416
sub pi times tau. So we were kind of

635
00:34:37,438 --> 00:34:40,024
unsure if beliefs is correct here. We

636
00:34:40,062 --> 00:34:43,112
kind of pulled that out of the legend

637
00:34:43,176 --> 00:34:47,740
for figure 4.4. But if anybody knows,

638
00:34:48,160 --> 00:34:49,676
and just to maybe unpack that a little

639
00:34:49,698 --> 00:34:51,436
bit more, the beliefs conditioned on

640
00:34:51,458 --> 00:34:53,176
policy and time, the squiggle sigma,

641
00:34:53,208 --> 00:34:55,488
sub pi times tau is equal to the

642
00:34:55,494 --> 00:34:56,940
difference between the observations

643
00:34:57,020 --> 00:34:59,116
conditioned on policy and time o sub pi

644
00:34:59,148 --> 00:35:01,296
times tau and preferences conditioned on

645
00:35:01,318 --> 00:35:05,056
time C sub pi or sub tau. So we were

646
00:35:05,078 --> 00:35:07,110
unsure if belief is correct here also.

647
00:35:08,360 --> 00:35:11,476
And so if anybody knows if beliefs is

648
00:35:11,658 --> 00:35:13,716
the difference between observations and

649
00:35:13,738 --> 00:35:15,444
preferences, that would be great to have

650
00:35:15,482 --> 00:35:17,992
some kind of feedback or input here

651
00:35:18,046 --> 00:35:20,296
because we couldn't really reach a

652
00:35:20,318 --> 00:35:21,240
conclusion.

653
00:35:59,770 --> 00:36:03,954
Well, I guess one tiny

654
00:36:04,002 --> 00:36:07,706
step toward figuring this out, I would

655
00:36:07,728 --> 00:36:10,794
say, is that that funny squiggle thing

656
00:36:10,832 --> 00:36:13,610
there has to do with the C.

657
00:36:13,760 --> 00:36:17,510
We've got, we've got this probability

658
00:36:17,590 --> 00:36:20,126
of observations given C and C is this

659
00:36:20,148 --> 00:36:21,994
mysterious object that expresses

660
00:36:22,042 --> 00:36:24,478
preferences which has also been under

661
00:36:24,564 --> 00:36:27,854
explained. So if you look at the two

662
00:36:27,892 --> 00:36:31,466
terms of the expected

663
00:36:31,498 --> 00:36:34,254
free energy G as a function of

664
00:36:34,372 --> 00:36:37,746
functional policy in 4.7, and then look

665
00:36:37,768 --> 00:36:39,666
at how it looks like in four point ten,

666
00:36:39,768 --> 00:36:41,380
that sigma thing there,

667
00:36:48,330 --> 00:36:50,454
we've got the dot, which I guess is a

668
00:36:50,572 --> 00:36:54,166
multiplier by observation. So that's the

669
00:36:54,188 --> 00:36:56,294
same as that's like saying your

670
00:36:56,332 --> 00:36:58,406
probability of observation given C. So

671
00:36:58,428 --> 00:37:00,406
that's, I guess, a matrix multiplier

672
00:37:00,438 --> 00:37:04,700
version of this log probability of

673
00:37:05,230 --> 00:37:07,500
sequence of observations given your

674
00:37:08,510 --> 00:37:11,500
preference prior C.

675
00:37:30,490 --> 00:37:32,874
Yeah, so they do unpack. It a little bit

676
00:37:32,912 --> 00:37:36,542
more in equation 4.7 or it seems

677
00:37:36,596 --> 00:37:38,474
to be like maybe we could extrapolate

678
00:37:38,522 --> 00:37:41,726
from 4.7 if this is beliefs, because it

679
00:37:41,748 --> 00:37:43,930
is this natural log of observations

680
00:37:44,010 --> 00:37:46,182
minus the natural log of preferences.

681
00:37:46,266 --> 00:37:48,082
But up in the top that just the free

682
00:37:48,136 --> 00:37:51,890
energy is the entropy or

683
00:37:51,960 --> 00:37:53,742
maybe the dot product of the entropy

684
00:37:53,806 --> 00:37:57,438
times the states plus the observations

685
00:37:57,614 --> 00:37:59,286
times or the dot product of the

686
00:37:59,308 --> 00:38:03,560
observations and maybe beliefs here

687
00:38:05,610 --> 00:38:07,762
I can't recall a mathematical

688
00:38:07,826 --> 00:38:09,894
representation of belief ever using this

689
00:38:09,932 --> 00:38:11,802
squiggle sigma before. And I looked

690
00:38:11,856 --> 00:38:14,666
through the recent Ryan Smith paper and

691
00:38:14,688 --> 00:38:16,806
I looked even at the message passing

692
00:38:16,838 --> 00:38:19,914
paper by Friston and I was not able to

693
00:38:19,952 --> 00:38:23,538
really get any additional references

694
00:38:23,654 --> 00:38:26,398
to this squiggle sigma at all. So why do

695
00:38:26,404 --> 00:38:27,646
you say belief as opposed to

696
00:38:27,668 --> 00:38:31,086
preferences? Well, so it says here

697
00:38:31,188 --> 00:38:34,510
that the squiggle sigma

698
00:38:34,850 --> 00:38:37,826
is equal to the difference between the

699
00:38:37,848 --> 00:38:39,486
natural log of the observations

700
00:38:39,598 --> 00:38:43,026
conditioned on policy and time and the

701
00:38:43,048 --> 00:38:45,666
natural log of the preferences at a

702
00:38:45,688 --> 00:38:49,162
certain time. So the sigma is defined

703
00:38:49,246 --> 00:38:51,250
by the natural log of the preferences,

704
00:38:51,410 --> 00:38:53,830
but it's more than just the preferences.

705
00:38:54,170 --> 00:38:56,920
Does that make sense? Eric? Yeah,

706
00:38:57,290 --> 00:38:59,800
but that's not claim as belief, right?

707
00:39:00,170 --> 00:39:02,410
That's observations versus preferences,

708
00:39:02,990 --> 00:39:03,740
right.

709
00:39:05,630 --> 00:39:09,126
From expected deviation or how much risk

710
00:39:09,158 --> 00:39:12,218
is being taken relative to the policy

711
00:39:12,304 --> 00:39:13,450
that's applied.

712
00:39:16,290 --> 00:39:18,046
The dot product is between the

713
00:39:18,068 --> 00:39:21,674
observation and that wiggle sigma,

714
00:39:21,722 --> 00:39:24,750
right, which is in turn like,

715
00:39:24,900 --> 00:39:27,406
okay, this is what I've observed, this

716
00:39:27,428 --> 00:39:29,374
is what I expected. So it's like a

717
00:39:29,412 --> 00:39:32,274
divergence between two. So how much more

718
00:39:32,312 --> 00:39:34,594
risk am I taking and how far is this

719
00:39:34,632 --> 00:39:37,186
pushing me away from my free from

720
00:39:37,288 --> 00:39:38,820
minimizing free energy?

721
00:39:41,030 --> 00:39:42,420
So maybe it's something.

722
00:39:44,650 --> 00:39:46,822
Whereas belief is wrapped up in the s

723
00:39:46,876 --> 00:39:49,720
because that's your model of the world.

724
00:39:52,510 --> 00:39:56,358
So risk is maybe how the squiggle sigma

725
00:39:56,374 --> 00:39:57,962
is defined. But then if you look into

726
00:39:58,016 --> 00:40:00,554
this figure 4.4 like that's, that's why

727
00:40:00,592 --> 00:40:03,094
we used belief. So here, let's, let's

728
00:40:03,142 --> 00:40:04,800
I'll, I'll pull it up right now.

729
00:40:28,990 --> 00:40:32,620
So I'm not sure how easy this is to see.

730
00:40:33,150 --> 00:40:37,002
But before we get to number one

731
00:40:37,056 --> 00:40:39,294
here, we have like on the right hand

732
00:40:39,332 --> 00:40:43,360
side the second to lowest level is

733
00:40:44,530 --> 00:40:47,934
states conditioned on policy and time.

734
00:40:47,972 --> 00:40:49,006
I think I'm going to read it off this

735
00:40:49,028 --> 00:40:51,066
bigger screen. Yeah. States conditioned

736
00:40:51,098 --> 00:40:52,606
on policy and time at different time

737
00:40:52,628 --> 00:40:55,058
steps. So like the current time is in

738
00:40:55,064 --> 00:40:57,138
the middle, the forward time step is on

739
00:40:57,144 --> 00:40:58,786
the right and the backward time step is

740
00:40:58,808 --> 00:41:01,890
on the left. And so that's what we start

741
00:41:01,960 --> 00:41:06,194
with. So intuitively, current beliefs

742
00:41:06,242 --> 00:41:09,174
about states under each policy at each

743
00:41:09,212 --> 00:41:10,966
time are compared with those that would

744
00:41:10,988 --> 00:41:13,110
be predicted given beliefs about states

745
00:41:13,180 --> 00:41:17,190
at other times. So maybe

746
00:41:17,260 --> 00:41:19,786
beliefs is this f and that's kind of

747
00:41:19,808 --> 00:41:23,286
what Eric was saying because it's

748
00:41:23,318 --> 00:41:26,986
defined earlier. This epsilon here is a

749
00:41:27,008 --> 00:41:29,322
prediction error, right? So what we come

750
00:41:29,376 --> 00:41:32,094
to after step one, it says and current

751
00:41:32,132 --> 00:41:34,074
outcomes to calculate prediction errors.

752
00:41:34,122 --> 00:41:36,014
That's what we arrive at after number

753
00:41:36,052 --> 00:41:39,486
one. The errors then drive updating in

754
00:41:39,508 --> 00:41:41,426
the beliefs. That's number two. So here

755
00:41:41,448 --> 00:41:43,794
we go from the current state at the

756
00:41:43,832 --> 00:41:46,674
present time to a prediction error that

757
00:41:46,712 --> 00:41:49,714
drives the belief updating at this

758
00:41:49,832 --> 00:41:53,154
forward time step after number two. And

759
00:41:53,192 --> 00:41:55,990
then it says given beliefs about states

760
00:41:56,060 --> 00:41:58,786
under each policy, we can then calculate

761
00:41:58,818 --> 00:42:01,000
the gradients of expected free energy.

762
00:42:01,450 --> 00:42:04,710
Three. So what gives us this gradient of

763
00:42:04,860 --> 00:42:09,674
expected free energy? Is it this s

764
00:42:09,712 --> 00:42:12,378
sub tau plus one? Or is that a tau? I

765
00:42:12,384 --> 00:42:15,900
think that's a tau. Yeah.

766
00:42:16,510 --> 00:42:19,046
So is it a future state not conditioned

767
00:42:19,078 --> 00:42:21,310
on policy? Or here we get to this

768
00:42:21,380 --> 00:42:24,554
squiggle sigma. So it's a squiggle sigma

769
00:42:24,602 --> 00:42:27,120
conditioned on policy and time plus one.

770
00:42:27,650 --> 00:42:30,270
So it looks like a belief update there,

771
00:42:30,340 --> 00:42:33,266
which is why belief made sense. So is

772
00:42:33,288 --> 00:42:36,260
that a risk update? Is that a possible

773
00:42:36,630 --> 00:42:38,482
interpretation there in this message

774
00:42:38,536 --> 00:42:39,540
passing figure?

775
00:42:45,590 --> 00:42:47,330
Well, they say it's a gradient,

776
00:42:50,180 --> 00:42:53,296
which means it's saying how much do we

777
00:42:53,318 --> 00:42:54,530
have to change our policy,

778
00:43:00,990 --> 00:43:03,850
I think in order to get our objectives

779
00:43:04,590 --> 00:43:06,860
yeah. Gradient makes sense to me.

780
00:43:07,790 --> 00:43:10,246
It's how much information you've gained,

781
00:43:10,358 --> 00:43:13,950
right? So if you actually

782
00:43:14,020 --> 00:43:15,934
look at the change in the state with

783
00:43:15,972 --> 00:43:21,214
respect to so I apply some force,

784
00:43:21,262 --> 00:43:24,020
or let's just take a simple example.

785
00:43:24,390 --> 00:43:27,380
If you have an actuator that just

786
00:43:27,750 --> 00:43:31,086
applies force and all

787
00:43:31,128 --> 00:43:34,086
it does is it moves forward and has to

788
00:43:34,108 --> 00:43:37,602
follow a trajectory. So the error

789
00:43:37,666 --> 00:43:41,110
is at each time step. If it's a

790
00:43:41,180 --> 00:43:43,046
deterministic system, you expect it to

791
00:43:43,068 --> 00:43:44,338
follow a straight line and it's

792
00:43:44,354 --> 00:43:47,274
deviating up way. So that's epsilon. So

793
00:43:47,312 --> 00:43:49,066
you have to update, okay, this is how

794
00:43:49,088 --> 00:43:52,166
far up I am from the state that I'm

795
00:43:52,198 --> 00:43:53,946
supposed to be. So I need to apply

796
00:43:54,128 --> 00:43:58,026
slightly less force or maybe force

797
00:43:58,058 --> 00:43:59,390
in a different direction.

798
00:44:03,650 --> 00:44:06,426
It's also how much I have to overcorrect

799
00:44:06,458 --> 00:44:08,358
in the future depending on what I've

800
00:44:08,394 --> 00:44:14,706
done currently, right? It's not just if

801
00:44:14,728 --> 00:44:17,074
I've applied some force right now, I

802
00:44:17,112 --> 00:44:19,246
might deviate in the opposite direction

803
00:44:19,278 --> 00:44:21,366
and I have to come back, so I have to

804
00:44:21,388 --> 00:44:23,560
figure out how much force to apply. So

805
00:44:24,010 --> 00:44:26,502
that's why I said risk or. Yeah,

806
00:44:26,556 --> 00:44:29,880
gradient, you would have to take the

807
00:44:30,250 --> 00:44:33,254
change with respect to the state change

808
00:44:33,292 --> 00:44:35,514
with respect to the error. So yeah,

809
00:44:35,552 --> 00:44:37,914
that would be one interpretation of it.

810
00:44:37,952 --> 00:44:41,002
Right, so it's a gradient of the state

811
00:44:41,056 --> 00:44:43,018
change with respect to what you have

812
00:44:43,104 --> 00:44:46,974
done. So I

813
00:44:47,012 --> 00:44:49,566
also heard the term information gain in

814
00:44:49,588 --> 00:44:51,520
there. Yeah,

815
00:44:52,290 --> 00:44:54,618
I'm just generally saying. So in neural

816
00:44:54,634 --> 00:44:57,518
networks, for example, we would have

817
00:44:57,604 --> 00:45:00,242
some sort of loss function and we would

818
00:45:00,296 --> 00:45:03,214
calculate the weight change with respect

819
00:45:03,262 --> 00:45:05,490
to the change of the loss function.

820
00:45:05,560 --> 00:45:07,262
Right. So, like, I have a weight vector

821
00:45:07,326 --> 00:45:11,026
and then this particular example

822
00:45:11,128 --> 00:45:13,118
is giving me a certain amount of

823
00:45:13,144 --> 00:45:15,414
gradient and I need to minimize this

824
00:45:15,452 --> 00:45:17,286
gradient. So I need to change the

825
00:45:17,308 --> 00:45:20,166
weights in turn so the gradient actually

826
00:45:20,348 --> 00:45:22,342
interfaces between this loss function

827
00:45:22,396 --> 00:45:25,414
and all these weights. So effectively,

828
00:45:25,542 --> 00:45:27,914
the gradient passes on information as to

829
00:45:27,952 --> 00:45:30,106
how much the weight has to change in

830
00:45:30,128 --> 00:45:31,562
order to minimize that loss function.

831
00:45:31,616 --> 00:45:32,780
That's why I said information.

832
00:45:36,450 --> 00:45:39,982
And also, as a side note, I think

833
00:45:40,036 --> 00:45:43,934
I found this sigma in

834
00:45:44,052 --> 00:45:47,534
Ryan Smith's paper too, and it is

835
00:45:47,572 --> 00:45:50,542
defined as the expected prediction

836
00:45:50,606 --> 00:45:51,410
error.

837
00:45:53,510 --> 00:45:57,134
So the expected prediction error versus

838
00:45:57,182 --> 00:45:59,714
the actual prediction error. And they

839
00:45:59,752 --> 00:46:01,406
distinguish it from epsilon in the Ryan

840
00:46:01,438 --> 00:46:03,026
Smith paper because I did look at that,

841
00:46:03,048 --> 00:46:05,218
but I didn't find it in there. Yeah,

842
00:46:05,304 --> 00:46:09,122
it's in equation 27 under

843
00:46:09,176 --> 00:46:11,810
the section outcome prediction errors.

844
00:46:12,950 --> 00:46:16,522
You it may be that the expected means

845
00:46:16,576 --> 00:46:20,282
it's an expectation over the Q

846
00:46:20,336 --> 00:46:22,550
distribution, which is your distribution

847
00:46:22,630 --> 00:46:26,874
over belief

848
00:46:26,922 --> 00:46:27,520
states.

849
00:46:48,590 --> 00:46:52,154
Yeah, that's great. Ali and I

850
00:46:52,192 --> 00:46:54,710
and some others worked on this yesterday

851
00:46:54,870 --> 00:46:58,174
a lot and it's great that it kept you up

852
00:46:58,212 --> 00:46:59,886
late at night, clearly, Ali, so you went

853
00:46:59,908 --> 00:47:02,382
digging around for some more

854
00:47:02,436 --> 00:47:04,382
information. That's awesome. Yeah,

855
00:47:04,436 --> 00:47:06,702
great. Okay, perfect. Well, that

856
00:47:06,756 --> 00:47:10,290
resolves that very well. And then

857
00:47:10,360 --> 00:47:12,580
let's get into maybe the next question.

858
00:47:25,530 --> 00:47:28,982
In equation 4.16, the X

859
00:47:29,036 --> 00:47:31,206
has a dot over. It changed through time

860
00:47:31,308 --> 00:47:34,040
but not the Y data.

861
00:47:34,970 --> 00:47:38,780
So there's some discourse on this.

862
00:47:40,270 --> 00:47:41,594
Or maybe we should go to the next

863
00:47:41,632 --> 00:47:42,794
question, actually, because it's maybe

864
00:47:42,832 --> 00:47:46,058
more related. So, Figure 4.6 uses an

865
00:47:46,064 --> 00:47:47,774
epsilon for prediction error as

866
00:47:47,812 --> 00:47:51,502
described in equation 4.21. Is this

867
00:47:51,556 --> 00:47:53,678
predictive processing framing a part of

868
00:47:53,684 --> 00:47:56,254
the active inference model or is this

869
00:47:56,292 --> 00:47:58,910
presented for contrast to illustrate the

870
00:47:59,060 --> 00:48:01,150
similarities and differences between

871
00:48:01,220 --> 00:48:02,926
active inference and predictive

872
00:48:02,958 --> 00:48:06,770
processing? So this is equation 4.1

873
00:48:06,840 --> 00:48:09,010
and this is the epsilon, but I think we

874
00:48:09,080 --> 00:48:12,274
see it way before equation 4.21. Like

875
00:48:12,312 --> 00:48:14,920
it's even there. In equation 4.4,

876
00:48:19,850 --> 00:48:22,726
it says this predictive coding schema is

877
00:48:22,748 --> 00:48:24,262
part of the active inference model

878
00:48:24,316 --> 00:48:26,066
illustrating the hierarchical structure

879
00:48:26,098 --> 00:48:28,434
of predictions and beliefs. The authors

880
00:48:28,482 --> 00:48:30,826
say one way to think about this is as if

881
00:48:30,848 --> 00:48:32,646
we had equipped a predictive coding

882
00:48:32,678 --> 00:48:35,386
scheme with classical reflex arcs at the

883
00:48:35,408 --> 00:48:37,226
lowest level of the hierarchy. And they

884
00:48:37,248 --> 00:48:39,414
give this reference in this setting,

885
00:48:39,462 --> 00:48:41,242
active inference is just predictive

886
00:48:41,306 --> 00:48:44,510
coding plus reflex arcs. Does anybody

887
00:48:44,580 --> 00:48:46,638
know what a reflex arc is? Because I'm a

888
00:48:46,644 --> 00:48:48,942
little bit lost with respect to that or

889
00:48:48,996 --> 00:48:50,654
have any comments on this prediction

890
00:48:50,702 --> 00:48:55,474
area? Yeah, reflex arc is basically

891
00:48:55,672 --> 00:48:57,794
the stimuli that doesn't go through the

892
00:48:57,832 --> 00:48:59,250
sensory cortex.

893
00:49:10,000 --> 00:49:11,804
I thought it was like a mathematical

894
00:49:11,852 --> 00:49:12,640
construct.

895
00:49:19,510 --> 00:49:21,090
Maybe you think of it as a control

896
00:49:21,160 --> 00:49:24,386
system. No thinking.

897
00:49:24,488 --> 00:49:27,250
It's just like you bang your knee.

898
00:49:27,750 --> 00:49:31,094
Your foot goes up. That makes

899
00:49:31,132 --> 00:49:33,766
sense. And it says, we minimize free

900
00:49:33,788 --> 00:49:35,926
energy through action. And the only part

901
00:49:35,948 --> 00:49:37,506
of the free energy that depends on

902
00:49:37,548 --> 00:49:39,334
action is the lowest level of prediction

903
00:49:39,382 --> 00:49:41,382
error. In this hierarchical schema,

904
00:49:41,526 --> 00:49:43,654
action fulfills descending predictions

905
00:49:43,702 --> 00:49:45,434
by minimizing the error between the

906
00:49:45,472 --> 00:49:47,660
predicted and observed sensory data.

907
00:49:59,540 --> 00:50:01,572
Any additional comments here about

908
00:50:01,626 --> 00:50:04,500
predictive coding and active inference?

909
00:50:24,190 --> 00:50:26,806
Or we can look at this equation 4.16.

910
00:50:26,838 --> 00:50:28,518
The x has the dot over. It changed

911
00:50:28,534 --> 00:50:31,422
through time, but not the y data. Y is

912
00:50:31,476 --> 00:50:35,022
this. And here

913
00:50:35,076 --> 00:50:37,690
the discourse says, the top equation

914
00:50:37,770 --> 00:50:40,158
depends on f of x and v, which is a

915
00:50:40,164 --> 00:50:42,158
deterministic function describing how a

916
00:50:42,164 --> 00:50:44,306
hidden state changes over time. The

917
00:50:44,328 --> 00:50:47,074
bottom equation depends on g of x and v,

918
00:50:47,112 --> 00:50:48,974
which describes how data are generated

919
00:50:49,022 --> 00:50:51,502
from a single hidden state. The tilde

920
00:50:51,566 --> 00:50:53,714
indicates change through time. The dot

921
00:50:53,762 --> 00:50:56,150
indicates the first order derivative.

922
00:51:01,690 --> 00:51:05,126
Any comments here on this equation or

923
00:51:05,148 --> 00:51:05,900
your question?

924
00:51:40,790 --> 00:51:42,786
Maybe we can ask one more question, or

925
00:51:42,808 --> 00:51:44,866
maybe a couple more. So here it says on

926
00:51:44,888 --> 00:51:47,506
page 63, it says specifically, the

927
00:51:47,528 --> 00:51:49,734
Bayesian brain helps us frame the

928
00:51:49,772 --> 00:51:51,446
problems that an agent engaging in

929
00:51:51,468 --> 00:51:53,554
active inference must solve broadly.

930
00:51:53,602 --> 00:51:55,186
These are the problem of inferring

931
00:51:55,218 --> 00:51:56,854
states of the world perception and

932
00:51:56,892 --> 00:51:59,030
inferring a course of action planning.

933
00:51:59,190 --> 00:52:00,774
Other than perception and action

934
00:52:00,822 --> 00:52:02,666
planning, are there other tasks or

935
00:52:02,688 --> 00:52:04,662
challenges that the brain or organisms

936
00:52:04,726 --> 00:52:07,100
engage in? How would we know?

937
00:52:08,590 --> 00:52:10,446
There's no discourse here, but let's

938
00:52:10,468 --> 00:52:12,480
open it up and see what you guys think.

939
00:52:33,730 --> 00:52:36,670
Yeah, people wonder, hey,

940
00:52:36,820 --> 00:52:38,800
what's on that star out there?

941
00:52:39,810 --> 00:52:42,994
That's neither? I don't know. Is that

942
00:52:43,032 --> 00:52:44,740
inferring State of the world? I guess

943
00:52:45,270 --> 00:52:46,530
not. Planning.

944
00:52:49,610 --> 00:52:52,278
And it could just be a hallucination of

945
00:52:52,444 --> 00:52:54,502
generative model. You're not really

946
00:52:54,556 --> 00:52:55,654
measuring anything, you're just

947
00:52:55,692 --> 00:52:57,720
measuring yourself over and over again.

948
00:52:59,050 --> 00:53:00,600
That could be one thing.

949
00:53:19,690 --> 00:53:22,310
Okay, so here there's another question

950
00:53:22,460 --> 00:53:24,774
for each of the graphical models in

951
00:53:24,812 --> 00:53:28,134
Figure 4.2. What is an intuitive example

952
00:53:28,252 --> 00:53:30,090
for each structure?

953
00:53:50,040 --> 00:53:52,580
One would be disease diagnosis.

954
00:53:53,500 --> 00:53:57,624
So you have symptom that is causative or

955
00:53:57,662 --> 00:53:59,800
predictive of some condition.

956
00:54:08,550 --> 00:54:10,226
I think that was the example that they

957
00:54:10,248 --> 00:54:11,620
gave in the text, right?

958
00:54:14,630 --> 00:54:16,514
Or did we talk about this last week

959
00:54:16,552 --> 00:54:17,140
maybe?

960
00:54:22,250 --> 00:54:24,742
Yeah, I actually don't get the question

961
00:54:24,876 --> 00:54:28,120
because each of them we have an example,

962
00:54:28,490 --> 00:54:30,520
at least an example in the text.

963
00:54:40,310 --> 00:54:41,540
Yeah, that's true.

964
00:54:45,300 --> 00:54:46,784
And I think we're going to maybe see

965
00:54:46,822 --> 00:54:49,490
more examples as we go through the book.

966
00:55:00,580 --> 00:55:03,460
But just in case, if anyone wants to see

967
00:55:03,530 --> 00:55:06,388
some more examples actually,

968
00:55:06,474 --> 00:55:09,392
that paper, I put the link in the chat,

969
00:55:09,456 --> 00:55:11,392
realizing Active Inference in Variation

970
00:55:11,456 --> 00:55:13,944
Message Passing contains some more

971
00:55:13,982 --> 00:55:16,360
examples for each of these diagrams.

972
00:55:22,640 --> 00:55:27,500
This is a fun last question. Maybe it

973
00:55:27,570 --> 00:55:31,436
says this reiterates that

974
00:55:31,458 --> 00:55:33,484
active inference uses two constructs,

975
00:55:33,532 --> 00:55:35,276
variational free energy and expected

976
00:55:35,308 --> 00:55:37,084
free energy, which are mathematically

977
00:55:37,132 --> 00:55:38,784
related but play distinct and

978
00:55:38,822 --> 00:55:41,040
complementary roles. In your own words,

979
00:55:41,110 --> 00:55:43,368
what are the definitions of variational

980
00:55:43,404 --> 00:55:44,980
free energy and expected free energy?

981
00:55:45,050 --> 00:55:46,944
And what role do they play individually

982
00:55:46,992 --> 00:55:49,540
or together in active inference?

983
00:56:05,890 --> 00:56:08,740
My view of expected free energy was

984
00:56:09,350 --> 00:56:12,274
given a certain action that might be

985
00:56:12,312 --> 00:56:15,966
taken. It might produce this or reduce

986
00:56:15,998 --> 00:56:19,206
the free energy or something, or it

987
00:56:19,228 --> 00:56:22,614
talks about some future reduction on

988
00:56:22,652 --> 00:56:24,440
free energy or something like that.

989
00:56:24,810 --> 00:56:26,870
Variational free energy is more

990
00:56:27,020 --> 00:56:29,800
objective. I don't know how to put it.

991
00:56:31,470 --> 00:56:33,690
It's like, okay, actual kinetic energy

992
00:56:33,760 --> 00:56:36,298
versus perceived kinetic energy kind of

993
00:56:36,304 --> 00:56:39,418
thing. So if

994
00:56:39,424 --> 00:56:40,970
you're rolling down a hill,

995
00:56:43,310 --> 00:56:46,606
like rolling down a hill, we can

996
00:56:46,628 --> 00:56:50,350
actually measure the kinetic energy at

997
00:56:50,420 --> 00:56:53,438
the bottom of the hill, right? If you

998
00:56:53,444 --> 00:56:56,266
put the ball at the top, expected free

999
00:56:56,308 --> 00:56:58,754
energy would be like using a model to

1000
00:56:58,792 --> 00:57:00,980
calculate that and then finding out

1001
00:57:01,750 --> 00:57:04,818
actually corresponds if you look the

1002
00:57:04,824 --> 00:57:07,220
model. At least that's my view.

1003
00:57:19,070 --> 00:57:20,638
So I don't know. I'm probably like I

1004
00:57:20,644 --> 00:57:21,774
think about this in a lot of different

1005
00:57:21,812 --> 00:57:24,974
ways, but I think about variational free

1006
00:57:25,012 --> 00:57:26,606
energy. I mean, this is one way to think

1007
00:57:26,628 --> 00:57:30,610
about it, is maximizing

1008
00:57:31,110 --> 00:57:33,682
the trade off between epistemic value

1009
00:57:33,736 --> 00:57:36,194
and pragmatic value right now and

1010
00:57:36,232 --> 00:57:37,938
expected for free energy is like

1011
00:57:38,024 --> 00:57:40,002
maximizing that difference at some point

1012
00:57:40,056 --> 00:57:42,840
in the future. So you can plan ahead.

1013
00:57:43,370 --> 00:57:45,654
It might be cold out, so I'm going to

1014
00:57:45,692 --> 00:57:48,038
take a jacket or like it's cold right

1015
00:57:48,044 --> 00:57:49,800
now, I'm going to put my jacket on.

1016
00:57:52,170 --> 00:57:53,670
That's how I think about the difference

1017
00:57:53,740 --> 00:57:56,902
between those two. So it's ten. Thanks,

1018
00:57:56,956 --> 00:57:58,950
everyone, for coming. I hope I didn't

1019
00:57:59,450 --> 00:58:02,038
muddle this up too much. Trying to be

1020
00:58:02,124 --> 00:58:05,594
substitute teacher for Daniel, and it's

1021
00:58:05,642 --> 00:58:07,326
ten. So we're going to have tools right

1022
00:58:07,348 --> 00:58:09,214
now in this room. But if anybody wants

1023
00:58:09,252 --> 00:58:12,494
to continue discussing these ideas and

1024
00:58:12,532 --> 00:58:15,422
gather, you're welcome to migrate up to

1025
00:58:15,476 --> 00:58:17,710
one of the different spaces.

1026
00:58:18,850 --> 00:58:20,606
Yeah, and I think we'll stop the

1027
00:58:20,628 --> 00:58:22,026
recording now, Alex, if you haven't



SPEAKER_00:
hello it's act inf textbook group cohort one meeting 23 on no 4 22 and we are in our first of two discussions on chapter 10. it's the final chapter of the book and so let's take a look at the table of contents and just does anybody from their reading or experience want to share an initial note on

where and how we've come where do we sit in a satisfaction space after chapter nine does chapter 10 get us where we want to be as we then complete the book

Yeah, Ali, and then anyone else.


SPEAKER_02:
Actually, chapter 10 is a pretty interesting chapter with all its comparative analysis provided and comparing active inference framework with other similar methodologies.

and unsurprisingly emphasizing almost always the advantages of using active inference over all the other frameworks.

But yeah, these comparative analyses were really helpful, in my opinion.

in order to distinguish what exactly active inference is and how it relates to other disciplinary areas.

But I think that some of these materials could have been

introduced much earlier, I mean, even in chapters 1 or 2, because most, some of the confusions at least, sometimes

arises from not being too clear enough about what active inference is and what it does and how it compares to other related areas.

But otherwise, it's a very, very interesting chapter in my opinion.


SPEAKER_03:
Yeah, I kind of feel similarly here that the way I would word it is that it's almost like a survey chapter, but it's got a lot of details and it's like, that would have been a good, maybe not all of that up front in the same way, but it just helps delineate active inference a little bit more.

Just the textbook overall, like,

I feel like we could make this no, you know, no exercises or something and lack of implementation comment.

It's like, but the fact that it's in the structure of the way that the, you know,

that it is currently understood at the frontier is almost like a little puzzle that will probably result in significantly better understanding of the domain.

So I feel like it's...

it's just the right amount of challenge for me to like really ground what is being said here.

And it's not like just given to you.

So, um, usually that's not done very well.

Once you have some pedagogical sort of resource, it's like, this is what it is.

Don't think about it.

Just remember that.

And that's not very helpful either.

So, um,

I wish there were maybe other libraries or something of software that explained a little more how this was working in a cogent fashion.

But also, there's an opportunity to help write this.

So that's, to me, I don't know, more interesting than just a rote definition explanation.


SPEAKER_00:
yes this isn't the final version of the textbook nor the final textbook ecosystem and i agree yes chapter 10 provides a nice overview it's it is long and it also may make sense as like chapter 1.1 even just to understand like the unifying theory of sentient behavior

angle before even talking about how to drive to active inference with a high road and a low road you know here's the overview in chapter one and here's maybe here's how hawaii is different than this other destination now here are two ways to get there now here's the generative model and empirical examples

So many ways to read it.

Eric, any overall thoughts?

Or let's look at their key points.


SPEAKER_01:
I felt like this was kind of a kitchen sink chapter, trying to take just a whole conglomeration of perspectives and lines of research and ideas in the whole enterprise of computational intelligence

cognitive science, neuroscience, psychology, and so forth, and say, okay, this is how we view it through the active inference lens.

So I think that's the purpose, kind of the purpose of the chapter, at least the second part of the chapter.

The first part of the chapter is kind of the summary of, again, the overview of what active inference is about, and tries to tie the threads together.

And I don't think it,

It's good to see the reminders of what the journey has been through the book.

I appreciated that refresh.

There are a number of threads that go in different directions.

Things like the notion of

Everything being a form of inference, but planning is a different form of inference because it's a different functional energy functional than pure perceptual inference.

The notion of the exploitation exploration trade-off, the notion of not being able to solve for the base and optimal policy, but therefore having to use

a variational energy approximation.

That was mentioned, but I felt like the book never in this chapter didn't say, well, how do we really leverage that?

I mean, how does that really come into play, you know, operationally?

It reviewed the examples that were used earlier in the book, such as the TMAs and so forth, which I found underwhelming all the way through.

And so the chapter ends with promises.

It says, wow, this is this wonderful hammer we have.

Let's see what nails we can find out in the world.

And we're going to hammer, hammer, hammer.

And let's see if they behave like nails.

We're going to treat them as they behave like nails, even if they're screws and need a different instrument to turn them.

so i think that as far as you know what if you're a devotee of active inference then this chapter is perhaps inspirational um if you're a skeptic then it's um it's like well okay that's nice move on yeah a lot there too let's let's look at some of these strong claims


SPEAKER_00:
One benefit of active inference is that it provides a complete solution to the adaptive problems that sentient organisms have to solve.

Of course, more could be said, but does it provide a complete solution?

Or maybe a little bit, still generously, but perhaps less excessively, it provides a complete framing or accounting.

which is different than a solution for the problems this makes it sound like just you know install actinf.py and the problem is solved which is obviously going to be situational depending on the generative model which is under no guarantee to be effective just like no linear model is effective by principle

So pretty obvious point, but something where as Eric just suggested, like an adherent might see this as a slam dunk, a fractal slam dunk.

Whereas somebody who's approaching this more skeptically is actually not going to have seen a solution at least presented in this book.

It offers a unified perspective

on problems like perception, action selection, attention, emotional regulation, which are usually treated in isolation in psych and neuro and addressed using distinct computational approaches and artificial intelligence.

This much more coherently is true because it is a unified perspective on perception, learning, but through the composability of generative models.

So merely the particular physics

markov blanket itself is not a unified model of perception action selection attention emotion regulation etc etc etc however it's a composable framework under which these phenomena can be explored but just the active inference kernel figure 1.1

is not that but it does offer a perspective that allows us to integrate it in a way that linear models aren't going to get us there and indeed these fields do work in isolation across multiple levels of mars hierarchy and so unified perspective is really important to start having some discourse about like hey there are two computational functions from the same brain region

or other kinds of more integrative work.

So that's nice.

Here, they're going to contextualize Active Inference in light of the following other fields.

Eric first though, go ahead.


SPEAKER_01:
Yeah, I'll just add one thing.

Since you mentioned Marr,

You know, I actually met David Maher once when I was applying for grad school.

And so I was a big part of the, you know, I was a big enthusiast about the whole school of thought that he was the founder of at MIT, first in the UK and then at MIT.

And one of the things that some of my fellow students and I observed was in that era was a phenomenon we call physics envy.

which is that an observation that physics is the grand science to explain the universe,

And it manages to boil things down to very pithy equations, simple equations that explain a lot.

You've got E equals MC squared.

You've got Schrodinger's equation, things like that.

And people in computer science and cognitive science and artificial intelligence want to have a grand theory because we know that the even more interesting problem

for civilization is not how the universe works, it's how do minds work.

So we wanted to, so at least our professors and our mentors

We're always looking for these grand unified theories, little equations that will explain everything.

And it's important.

Math is important.

So, you know, you can find like equations of optical flow.

And it's really important to model things using math.

But it seems that the free energy principle is is one of these things.

physics envy type of approaches where we were looking for a single equation to explain everything.

And by the way, it does borrow from statistical physics.

So it really is, I guess, Friston himself is his background.

So from that point of view, you know, this is again, you know, looking for a hammer that will,

treat the mysteries of nature as a nail and you can hit it with a hammer and presto, you've got it.

And, you know, it may be more like biology where there are,

Lots of principles, there's lots of math to be doing, lots of modeling to be doing, but by golly, it's really, really complicated and complex, and you're never gonna boil it down to the details of how it actually works using any simple equations.

Physics does not predict.

the shape of my coffee cup.

I don't care how much you understand Schroeder's equation or the laws of physics.

There's a lot other levels of complexity that have their own independent principles and ways of modeling them.

that are required to really understand the phenomena.

So my concern is that these over-encompassing claims about active inference are falling susceptible to the physics envy trap.


SPEAKER_00:
so sorry that's my oh awesome thank you yeah well it surely recalls the earlier um distinction of needs and scruffies from like chapter one in the book or i believe um whether the complexity of any realized system is so in the weeds and massively factorial that

transcendent generalizations are ineffectual um to the physics envy so this is the classic xkcd comic much memed many variants and um

it also surely represents some sociological settings like at Santa Fe Institute.

And it's like, well, we can talk, we can have the physicists speculate about sociology, but you wouldn't have the sociologists speculate about physics, right?

And there's cases where that conversation in both directions is helpful or not, but it just isn't this projected down number line.

But as a sort of rejoinder to the physics envy, is the FEP carrying on this lineage of physics envy?

We may be in a lineage of physics annexation because the cognitive physics...

is actually expanding to tackle the scope of classical statistical thermodynamic and quantum physics under special cases of cognitive systems and so i do agree that an individual might experience physics envy or feel like they're getting closer and closer to the real thing as more and more equations and and inscrutable symbols appear

however I wonder if on a deeper supra personal level we're actually aligning physics more with cognitive science than vice versa which would completely invert the traditionally understood um physics envy they're gonna have neuro envy or they'll have they'll have miramiko envy Ali


SPEAKER_02:
Yeah, one other case in point in this physics-envy endeavor is Karen Barad, who's a physicist which tries to apply her expertise onto some of the cultural phenomena, especially feminism.

And she has a very well-known book on that subject called Meeting the Universe Halfway.

And she's also the founder of a kind of realism called agential realism, which is very isomorphic with some of the fundamental concepts of FEP here as well.

So yeah, there are lots of interminglings between humanities and sciences or especially physics these days.


SPEAKER_00:
Yeah.

One image I kind of get is like there's a micro ratchet towards technicality, applications, visibility, tangibility, but there's a macro ratchet

towards holism, systems thinking, humanities, and so on.

And without broader awareness of that situation, there can be whiplash.

Like, are we doing philosophy on what things are and how they become and becoming and unbecoming?

Or are we calculating a particular form of a POMDP's expected free energy functional?

And we need to educate and develop our information environment to understand both and to include many perspectives yet unsaid.

So 10.2, wrap up.


SPEAKER_03:
Just really quickly, I'm going to also say there is an interesting discussion on Lex Friedman's podcast with Andrej Karpathy on attention is all you need, the way that they framed it.

This is just kind of this underwhelming kind of mimetic kind of pithy thing.

But they didn't make this claim about transformers being kind of some significantly a more general trainable architecture that was just kind of in the paper slipped in and they were just framing it as here's a better, you know, language translation slash prediction, right?

And he didn't like that.

He was like, oh, it was so transformational.

They should have named it the Transformers, the new whatever.

But then also had this conversation of, well, maybe it worked and it spread because it was this pithy meme thing that was under-promising and then eventually over-delivering.

I think in Active Inference, I don't know

I don't see, like, I see how this, like, FEP framing of it is, like, implicitly overpromising.

But I don't see a lot of people explicitly constantly kind of making this, like,

FEP is all you need.

Actives will explain everything sort of framing of it.

But there's this, I guess, yeah, it's like implicitly somehow because FEP is this sort of physics framing of it.

It just it seems like it's murky because of that relationship.

But ultimately, we'll just find where it's

well suited and not that there are really, in fact, a lot of nails out there.

And there may be a lot more screws or an equal amount of screws or we don't know, but we're going to get, you know, more complex and more precise models.

So how that exactly works in implementation is up for debate.

But or as remains to be seen, but I

I don't know if there's something that can be done there.

It's already modeled as FEP as some sort of containing background foundational epistemic prior there.

And so they've already said the transformer is the amazing thing.

So it's just it will deliver until it meets that and or not.

Yep.


SPEAKER_00:
Nicely said.

like we have the worked example we have canonical neural networks are active inference agents we have the the initial tracers you know the grappling hooks have been thrown the bugle has been sounded now let's see what really happens so good comments 10.2 wrapping up oh Ali please


SPEAKER_02:
Sorry, just one point about this claim in the book that says the active inference provides a complete solution to the adaptive problems.

Well, I don't know.

It seems to me like a kind of dubious claim because especially about the word complete here, maybe they meant that to be

an adequate solution to the adaptive problems because you see there's a very significant and fundamental difference between being adequate and between complete in your solutions right so in this case i'm not sure what they mean by a complete solution to the adaptive problems because

as we all know, even at this stage of its development, Active Inference has some shortcomings and there are some, well, gaps in the theory and so on, and so using this strong claim here seems to be

a little bit out of place.

But I would be fine if I have an adequate solution to some problems in some situations.


SPEAKER_00:
Yeah.

10.2 might be a nice reading early.

It summarizes chapters.

Summarizing chapter 1.

Adaptive exchange, the high road and the low road.

Chapter two, the low road.

Bayesian inference and generative models that can be inverted and heuristics thereof, as well as decompositions associated with those heuristics.

Chapter three, the high road, starting from the deflationary imperative to preserve integrity and avoid dissipation, which we may have a more sophisticated realization of in the Bayesian physics.

Chapter five, neurobiology, the most well-studied cases and systems of interest for many.

Cortical, subcortical, motor architectures supporting the integrative work, pointing to empirical examples on neurotransmitters, composability of neural systems, artificial systems.

That's part one of the book.

Chapter six heads into part two of the book.

There's a recipe for designing systems seven and eight.

are currently summarized as just the continuous and discrete time settings.

And chapter nine talks about model-based data analysis in terms of parametric recovery of an individual's generative model in the context of meta-Bayesian inference.

Connecting the dots, 10.3.

Dennett and the whole iguana.

guess usually we would say what the whole enchilada but that's not sentient enough so dennett was out there um it's a whole cognitive iguana it's a metabasian iguana we're imagining that we're building the whole iguana but we don't no one's actually built the whole iguana but also this is so representative how like threads of research just serve to be like

It's like spark after spark with no fire.

What is it like to be a bat?

Spawned tens of thousands of directions from 1954.

But where can you go find what it's like to be a bat?

oh that's right it's still the exact same speculation that existed in 1954 which is if you had the motor apparatus and the evolutionary heritage of being a bat then you'd know what it was like and it'd be whatever it is it is like like but if you're not you're not and so you don't know and i don't know if anyone has actually pierced that veil

so at least it reflects following up diligently or more diligently on provocations from our philosopher colleagues eric yeah i'm sorry this is just a digression you sent me on uh well maybe the metaverse will tell us because then you can have your virtual reality


SPEAKER_01:
environment that gives you only the abilities the sensory abilities and motor abilities of a bat and you live in that for a week and you learn really what a bat you know how to think like a bat what a bat can do can't do um i don't know just just sorry about that you no no you started it that's what i think is the brain machine interface like where it literally you'd rewire your brain


SPEAKER_03:
to feel it yeah and then then we'd know but you'd have to recover that too so there's like a difficulty of like bringing that sensory experience back to the human brain i don't all right well yeah might be a no cloning theorem sort of situation there all right this is a close enough approach i'm gonna


SPEAKER_01:
I'm going to advertise a piece of work I did many years ago.

It's called The Consciousness Machine.

And it's an account for how to solve a hard problem of consciousness.

The hard problem being, why do we have consciousness?

How does it work?

How does the brain produce it?

But your own subjective experience of consciousness.

is the mystery that it's hard to penetrate through pure science and explanations.

So how do you do that?

You have to have a user experience.

You basically have to make a cross between a video game and a simulation of your own mind, the mind of the user.

And in that environment,

then you can not only see intellectually, well, how is my brain working, but you can feel it, experience it, and make it real to yourself in the feedback sort of sense, the motor feedback.

And so once we have a consciousness machine, which we don't know how to build yet, but we're getting closer, and all the VR technology is an enabler for that, then we'll be able to

to um you know transform our conscious experience in various ways including uh becoming bats or ants or or whatever by learning to live those kind of lives in certain contexts it's on my website awesome


SPEAKER_00:
short on that somebody will always say well you're not a bat you're a person thinking you're a bat but that's where you go to the meta turing test which is now you're in the metaverse can you tell the difference between the true bat and the person thinking that they're about now if you can't design a setting to differentiate it from the functional ecological pragmatic view

It's as bat-like in the metaverse space as you can get.

So yeah, I mean, Act-Inf in the metaverse and Internet of Things and the way that we can even just today qualitatively talk about cyber-physical systems, composability, intelligence augmentation, distributed cognitive systems.

These are all so valuable and they are not...

Features that come from some of the competitors that we're going to be discussing today.

Okay.

The whole iguana thing.

some connections to analyses of cognition and generalized challenges in cognitive systems like exploration exploitation we talked earlier about how the planning mediated resolution of explore exploit was not quite that but certainly other papers do actually carry that out

that doesn't mean that they outperform any other given model but at least we know that we can frame explore exploit in single step and in multi-step scenarios with active um and active offers a means of understanding corresponding neural computations the connection between cognitive processing and expected neural dynamics was actually very very interestingly characterized in the recent live stream number 51.1

which I highly recommend because this could be the dam breaking between artificial neural networks and active inference POMDP models.

So it's quite exciting to hear.

We will explore in the rest of this chapter some implications for psychological functions as if we were sketching a psychology textbook.

This is kind of like that Gertl Escher Bach style where it's like, am I going to end the book in the middle of the book and then fill it with filler?

I don't know.

Has that already happened?

I don't know.

Am I sketching a book or is this the book itself?

It's up to you.

But that's just such a funny note.

predictive brains bayesian brain predictive coding predictive processing architecture um these are popular frameworks in computational cognitive neurosciences yet also at a deeper level these predictive approaches

still struggle for recognition among somehow default non-predictive approaches, like it always is needing to be justified in some people's eyes.

Whereas Active Inference takes it several steps further, first by incorporating action over multiple timescales.

although predictive processing via Andy Clark and others also was making directions in that way, but in a less quantitative way, and also introduces this variational physics grounding.

Like 4E cognition plus predictive processing with inference and action, simply smashing those two together in your theory collider

would never reveal that classical statistical thermodynamic and quantum mechanics can be understood as special cases of a cognitive physics.

that's sort of this like third vector that is exceptionally technical but does more than or makes active inference more than merely for e-cognition plus predictive processing as qualitative syntheses with a tractable implementation approach in the pomdp for example

Perception and learning.

We've talked about how there are similar inferential processes playing out over different timescales, and that maps to some biologically plausible physiological updates in terms of synaptic plasticity for learning and neural firing rates for perception.

And it's a very strong mapping indeed, because the imperative in terms of the loss function of the neural network being trained includes the firing and the plasticity term.

And that actually brings a very strong synthesis between the loss function in the neural networks training and the variational free energy descent in the POMDP.

So figure one,

in isomera at all and the loss function on a neural network as a function of observations and parameterization is concordant with the variational free energy in terms of observations and parameterizations and we can model one interesting piece here i know this is not the textbook itself but this fictive causality causality is like the opposite of the expected free energy

So it's almost like variational free energy is the now casting.

Expected free energy is anticipated decision-making and fictive causality is looking only at the past.

And it has a computational implementation in terms of a risk variable and a neuromodulatory interpretation with respect to dopamine modulation.

So...

Very interesting and tractable.

Bayesian brain, I don't know what more can be said.

It's about time that the Bayesian brain got formal.

But there's a lot of prose on the Bayesian brain and a lot of reprinting of the Bayes equation, but not much in the way of meaningful contributions beyond that.

And where there are meaningful contributions, they're not carried out in the spirit of the unified sentient behavioral model.

It's just like, oh, this brain region can be understood as doing prior updating on this inputs.

So it's using Bayesian framework, Bayesian statistics to describe neurophysiology, but that is unless super intentionally otherwise on the scruffy end of the spectrum.

Whereas Act Dymph is like coming from the neat end of the spectrum and connecting some of those scruffy findings.

Action.

Both action processing and perceptual processing are guided by forward predictions.

many contact points with various motor embodied embedded perspectives that we've discussed ecological psychology affordances cups are for grasping change the world change your mind we're talking about action the consequences of past action what to do now planning in the future the

Idea motor theory.

That's an interesting thread with William James and beyond.

And it is about connecting ideas with behavior.

That is getting formalized here.

cybernetics, the TOTE as well as the OODA loop, Observe, Orient, Decide, Act, and the Test, Operate, Test, Exit model.

And cybernetics brings in the concept of the goal-oriented teleos.

And in certain cybernetics settings, that was operationalized further as perceptual control theory, which is that what is being bounded is surprise about perception.

Interestingly, and as Dr. Isamura described in our conversation, the Bayesian risk is calculated about uncertainty on hidden states, which allows the Bayesian risk to be fit monotonically and with a heuristic approximation.

And in doing so, prevents overfitting of observables.

So in the fully observable chessboard situation, of course, observations and hidden states are one and the same, and the A matrix is just an identity matrix, mixing our metaphors.

Whereas when there's uncertainty between the hidden state and the outcome, one may be down a fallacious cul-de-sac if they overfit on the observations, because there might be like a many-many mapping or a stochastic mapping between hidden states and observations.

However,

under an appropriately structured partition the particular partition in particular one may fit a central tendency and variance attribute on an unobserved state and by doing so resists overfitting categorically to outcomes

and also enables fitting of large or sparse data sets of observables.

Optimal control theory discussed in various previous live streams and cases on how our inverse and forward control theoretic

In the control theory ontology, how does it map to the way that we talk about message passing and active inference?

General key point, the cost or value function is a special case or part of the decomposition of a variational or expected free energy.

However, we're not shoehorning everything into cost or value function.

However, again, components or special scenarios of the free energy can be understood to have a pragmatic value-like or a cost-like component.

Utility and decision-making.

Dynamic programming and the Bellman work.

Some of these ad hoc bonuses, novelty bonus, intrinsic reward, and so on,

are being approached in a first principles way through unified imperatives like the expected and variational free energy, which can be decomposed into pragmatic and epistemic components.

So, whence utility?

Oh utility, where art thy sting?

How am I going to get utility by learning about active inference?

Bayesian decision-making coming closer from the cybernetic and utilitarian decision-making space into specifically the Bayesian brain and Bayesian decision-making space.

This bears very, very heavily on the complete class theorem.

And so Ali, thanks a lot for raising this in the preparations for 51, because the Bayesian beliefs can be framed in terms of willingness to bet,

willingness to pay but importantly from the complete class theorem perspective as basically like an action ability and the specific set of strategies that they are interested in and restrict their analysis to are the admissible decision rules which means that that decision rule is as good as other decision rules everywhere or at least somewhere

So by staying in subspaces of admissible rules, one has strategies that are in some way functional, meaning more functional than alternatives, which is very interesting because that is still a large space with many bizarre strategies, some of which are very fragile.

but their admissibility is a constraint that is equivalent to saying that the calculation of perception is going to be Bayes optimal.

Like there's some signal input for which the Kalman filter of time horizon three, seven, 11 is going to be the Bayes optimal solution.

Or for whatever length of Kalman filter or Bayesian filtering approach you're taking, you can calculate it Bayes optimally.

And that's in the inference space.

And then analogously, we can think about the admissible strategies in the strategy space.

Reinforcement learning addressed, but subtle.

Reinforcement and reward are decompositions of expected and variational free energies.

And

so it's not that we don't reinforce high value behaviors it's just that the model architecture is quite distinct they then go into these families of rl model free reinforcement learning model based and policy gradient some similarities and differences with all of these

Planning is inference, which we may or may not have seen in the book, depending on how you think about it.

But the key point is that as future outcomes haven't happened, even hypothetically, one has to take a different approach towards planning and the consequences of hypothesized consequences of potential actions or policies.

It's a slightly different topic or question than inferring the relationship between actions taken in the world and the consequences on how things change.

And it actually is captured very interestingly again with the fictive causality, which can be understood as like a retrospective learning

from just starting with the first data point, already you're in the game, but starting from even the first round and then accumulating through training, learning the relationship of actions and transitions in the world and allowing reevaluation thereof, because some actions have delayed consequences and so on, learning that in an open-ended way

while actions truly do have consequence in the real world, in the generative process.

So active inference takes a unique approach for planning.

First, we use variational inference.

Second, it's model-based planning.

Third, we use an integrated expected free energy functional that subsumes other schemes like KL optimal control.

so at worst it should fit these other schemes as well as they do I'll be it with some computational overhead at best we should strictly be better behavior and bounded rationality Cicero bounded rationality in free energy interestingly free energy theory

More of the psychology.

Valence, emotion, motivation.

Dante.

Homeostasis, allostasis, interoceptive processing.

We just completed 50.2 yesterday.

Interoception as modeling, allostasis as control.

This is an important area.

And these models are fit.

Attention, salience, and epistemic dynamics.

So we saw kind of like utility oriented above with the bounded rationality and the reinforcement learning and so on.

And now we're moving towards the epistemic components being addressed in a first principles way.

Salience and attention.

Now into the structural learning.

Rule learning, causal inference, fast generalization,

building on the bayes graph causal architectures and thinking about inference and inferring fast and slow meta learning active inference in other fields open directions active inference can be applied to other domains two domains social cultural dynamics and ml and robots

social and cultural thinking through other minds the anticipating brain is not a scientist ecological cognitive processes birdsong which as we've discussed isn't also the communication example that people think it is it's an example of dyadically embodied pre-scripted monologue that's why it's called a duet for one

This is not open-ended symbolic message passing.

So much, much more needs to come into play before we can say that we have active inference models of like improvising entities.

But some of the turn-taking architecture is adequate.

And for example, these birds expect to hear a certain song, but we could use this model to say, well, we just expect there to be a conversation.

we expect little dead air.

So we can use this mechanics to simulate the dynamics of turn-taking, but we need a different payload for the actual communication if we want those entities, those birds to be doing anything other than just saying what they already expect each other to say.

and in the machine and cumulative culture and all of these complexities, extended cognition.

Machine learning and robotics, JF is the one to speak to on this topic.

And it's really interesting with the cognitive robotics angle, how like it's helping get action fully into the loop

And by doing so, showing which aspects of the inference and action are embodied, embedded, and cultured, etc., etc., etc.

And we end with Lord of the Rings quote.

Perhaps referencing the One Ring on the cover of the textbook, perhaps not.

they started by daring to be naive and ask whether it is possible to understand brain and behavior from first principles and brought active inference from off stage to address that question they hope expect and prefer that the reader has been convinced that the answer to the original question is yes it is possible to understand brain and behavior from first principles

The book is a useful compliment to philosophy and physics.

A Physics for Particular Systems, 2019 monograph, currently under contract at the same MIT Press.

And speculatively, we can say that this will be the active inference textbook, and there is forthcoming a physics textbook that will be more about the FEP.

The journey ends.

It was an introduction.

You can't just learn it in theory.

In theoretical neurobiology,

You have to get your hands on the models and their misbehaviors.

So like the classic Turing quip, when someone said, how can computers be creative when they only put out what we put in?

They never surprise us.

We're just getting what we put in.

We hear that today.

And Turing said, I'm surprised by computers every day.

Whether or not you're going to be modeling computationally, you may reflect on it day to day.

It might be about where you look.

It might be about where you lick.

It might be about what you do.

Ultimately, we are confident that you will continue to pursue active inference in some form.

Bravo to those who stay through the cohort.

Do you have any last thoughts for today?

Yeah.


SPEAKER_01:
Well, excellent reading.

Thank you for going through it in such detail.

That's a great summary of your part.


SPEAKER_00:
trying to you know balance the um the epistemic and the pragmatic which is to say the comedic value um we'll talk again about chapter 10 maybe go into a few more details and then probably highlight projects and the ways that we can return in 2023 for cohort three

with something like a cleaner slate as we welcome the next set of people in we accreted a lot we got all the equations in we got all the figures in all the boxes are in now we have the the actual task laid out in terms of the space so we'll be ready to proceed

All right, going to finish the recording now.

Thank you guys.
start	end	paragNum	speaker	confidence	startTime	wordCount	text
90	17930	1	A	0.12549	00:00	19	You. Hello everyone. It's october 14, 2022. We're in meeting 20. We're in our second discussion of chapter eight.
18080	71400	2	A	0.99992	00:18	84	We have a few topics and points in chapter eight and then maybe we can preview some chapter nine and have a lot of space for any other foster questions people are having so we can jump to the questions and look at the written chapter eight questions. Or does anyone want to suggest a point for us to go to last time? We know that some of the areas we wanted to pick up on involved learning and hierarchical modeling and hybrid modeling.
96020	161540	3	A	0.96	01:36	129	One question that came up, I think here, but maybe we can revisit is the birdsong model is widely pointed at as an example of how communication is modeled in active inference or for active inference entities. However, it's quite literally singing from the same hymn sheet. And the turn taking is based upon a joint expectation of that song being sung that way. And then the actions that the birds can take, the kind of turn taking that they enact is again related to whether it's like, well, somebody has to say it is it going to be me or is it going to be them? So how do you think we would broaden that scope of communication to account for situations where there isn't a prescripted song?
165580	183660	4	A	0.96879	02:45	21	How do other frameworks for communication cybernetic ish or otherwise deal with the semantics of conversation, improvisation, open endedness in conversation?
193470	275810	5	B	0.99887	03:13	122	Actually, I had somewhat more general question about the bird songs and its relation to these particular systems that they've developed in the textbook. You see, some researchers believe that I'm sure you're much qualified to talk about this than me, but there are lots of research going around about the territorialization function of birdsong. So if we take a bird song as its territorializing mechanism, at least one of its territorializing mechanism, can we somehow put the dynamic I mean, the dynamic boundary of the markup blanket, I think would be affected from this point of view. And then I'm not sure how that would translate into this specific model if we take that dynamic nature of the markup blanket boundary.
282070	306250	6	C	0.9991	04:42	55	Yeah, I guess I would just build on that and say that communication serves a purpose. And so I think a model should incorporate what that purpose is and how it's served by the communication. And then whether it happens to be syncing up with some predefined pattern or whatever is a follow on consideration.
311580	345760	7	A	0.61	05:11	56	The generative model in the singing from the same hymn sheet is very much like the sheet music example provided in the textbook. There's a known sheet music script such that even when there's like an error in the playing of the music, still the sheet music can be heard. So just coarsely what generative models?
347860	364570	8	A	1.0	05:47	40	I think it's pretty straightforward to see how a generative model could have the notation or the sheet music and then use the sheet music as its prior to kind of converge reality. To what extent is communication that way?
367020	380110	9	A	0.99991	06:07	47	Is it totally disparate and differently structured? Or is it a little bit like a hybrid? Like you're doing inference on the sheet music that they're about to say and then you use your inference about what they're about to say to converge what they do say?
382560	418570	10	C	0.98775	06:22	84	Well, you asked a good question, which is, well, what are some alternative frameworks? And I know, I know some or should know some, and they're not clean to mind. What comes to mind most immediately in this example is a phase lock loop in electronics which is simply a way of synchronizing one circuit's oscillation with another. And this is like a phase lock loop with a bigger vocabulary. So figure out where we are in the pattern and then synchronize with it.
422880	426380	11	C	1.0	07:02	7	But the question of what other frameworks?
431200	467640	12	C	0.94	07:11	55	I mean, there's this whole field of conversation analysis. Which. The field is actually called CA conversation analysis. And they have identified patterns of how people communicate through certain structure of I guess we call them rhetorical devices for achieving the purpose of the conversation. So the purpose of the communication is kind of bedrock.
468300	474190	13	C	1.0	07:48	15	And there are multiple purposes, some of which are instrumental, some are which are social.
477280	508660	14	C	0.76	07:57	73	And then they have identified patterns for the way human conversation tends to proceed which are culturally influenced. There's some cross cultural aspects, but then there are certain different cultures that have different takes on it. So they identify things like, okay, now we're in the conversation opening phase. Now we're in a phase of where we're exchanging information and we have turntaking. Now we have notions of epistemic leadership or epistemic authority.
509740	519930	15	C	0.92701	08:29	26	Who's the person who's asking? Who's the person who's answering or taking those roles in the conversation? And there are certain rhetorical stances that are taken.
522640	556880	16	C	0.99752	08:42	68	That's a pretty well worked out, I guess, even formalized system because there are well identified patterns. It's not strictly worked out because it seems like every domain a conversation analyst goes to and every every group has their own kind of flavor of it. But it does have this aspect of synchronizing with respect to the underlying known patterns of each of the parties in the conversation.
572810	659160	17	A	0.47023	09:32	138	Bird Song is also ollie, please. Sorry. Adding to what Eric just mentioned that reminded me of a mechanism on a much smaller scale, namely the scale of cells and so on, and particularly the model proposed by Stuart Kaufman because he talks about the pluripotent, potent nature of cells and how they communicate with each other in the context of phenomena of induction, biological induction. I mean, in the embryological development stage of the cells, almost all cells are pluripotent and they're capable of becoming any of the different types of cells which characterize the adult individual. So that also, from Kaufman's model perspective, these kind of inductive signals, or so to speak, communicative signals between cells can act as a kind of nonspecific stimulus which switch a cell among a variety of internally available stable states.
664990	681440	18	B	0.98731	11:04	38	Or let's put it this way the basic idea in this model is that the regulatory genes within a cell form a complex network in which genes interacting by their products can turn one another on or off.
684130	702680	19	B	0.99947	11:24	28	These patterns exhibit the kind of homeostatic stability associated with attractors of the systems. So I think that translates perfectly into this scale as well. This is small.
705050	755510	20	A	0.99613	11:45	69	Yeah. The mapping of communication as generalized synchrony may be compatible with a variety of communication mechanisms. And so it abstracts us from the media of communication per se because we can talk about the informational synchronizations between, for example, the two internal states of the two entities or multilateral synchronizations with environmental states or so on. Um, like thinking about the Bayesian mechanics, Bayesian physics, the synchronizing metronomes.
758570	774490	21	A	0.99859	12:38	17	They don't have a hymn sheet, they don't have sheet music. Their embodiment and coupling enables synchronization.
777490	791540	22	C	0.89	12:57	31	One thing I might note, I think you asked a good question here, which is how would you translate something like conversation analysis or human conversation into an active inference model?
793590	843762	23	C	0.99182	13:13	91	That kind of reminds me that the continuous valued models because they're limited by what you can do in these differential equations, they seem to be fairly simple. Whereas if I were to do something like thinking about human conversation in terms of active inference, I would want something more a very highly high dimensional or multistate type of system where I can have more complex patterns and they may not make these loops, these voltera loops. Is that what they call voltera? Is that right? Or is this Lawrence something?
843816	892340	24	C	0.56801	14:03	90	That dynamics and the Lawrence yeah, so they may not make those continuous loops, but you'd still get good structured patterns out of discrete systems. So I'd kind of be curious and I think this chapter would be cool if it had something about communication and synchrony, pattern synchrony, but in terms of discrete valued active inference models instead of the continuous valued ones. Yeah, very interesting. That's what I was wondering. Which is why is communication continuous time models are argued to be a natural fit for motor control?
893350	928970	25	A	0.66254	14:53	69	Are communication situations naturally fit for one or the other? Seems like no, you could imagine it either way. And then also the synchronization means the internal state of one creature should come to resolve the resemble internal state of another. A primitive kind of theory of mind, I think. Not as a fatal critique, but these examples put the rabbit in the hat a little bit too keenly.
929130	986320	26	A	0.9956	15:29	114	Like the planning example we explored wasn't truly planning. It was like a parameterized subspace where single step optimization emulated planning. Now, one could argue that they believe that that's actually like a better approach than true explicit planning, but that's not what was on the COVID And then similarly, theory of mind, though qualitatively, may be a framing for communication. We're talking still about the birdsong where they have the same shared a prior regenerative model. So this doesn't in other words, theory of mind is axiomatically and implicitly embedded within this model, rather than active inference being used as in a priori first principles to generate some realized computational theory of mind.
986710	1013160	27	C	0.62	16:26	65	I feel like, yeah, it's like another aspect of the continuous valued formulations is there's really not that many degrees of freedom, but when you're communicating anything other than a fixed song, there's a lot of degrees of freedom. And that's why we have symbol systems. But symbol systems is where you would use a discrete model framework for not a continuous value to win.
1018080	1071870	28	A	0.99815	16:58	77	Okay, let's talk about hybrid models, discrete and continuous models and learning and updating. So these are two features. Chapter eight is within a continuous time framework, just like chapter seven, layered in these kind of patterns or archetypes of discrete time models in chapter eight, analogously, the most kernel continuous time formalism is introduced with movement control and then we're going to see some kind of extensions or patterns like design patterns that include continuous time.
1074960	1078400	29	A	0.99997	17:54	5	This is the message passing.
1086720	1145170	30	A	0.99922	18:06	95	Okay, so missing line here, but we have here, this top half is like the bottom of figure 4.3, or the top, I guess, discrete time. So we have B matrix as a transition matrix. We have discrete time and we have discrete time. T plus one, t minus one. But within each time point we see the continuous time e whatever we want to call this motif where the B transition matrix in discrete time is functionally substituted with a derivative map such that rather than actually retro or forecasting state values through time.
1145620	1166950	31	A	1.0	19:05	42	This is much more like a Taylor Series approximation, where extrapolation from the present, under the auspices of slower changing causes v, which play a functional role of policies, higher derivatives capture more and more variance further and further from the origin.
1173100	1178570	32	A	0.99952	19:33	5	What is the Ita doing?
1187910	1210220	33	A	0.99546	19:47	44	It's playing some functional role to g again, allowing for a line here in the sense that it is selecting policy, but it's not policy that's being selected or discussed this way, but these slower changing factors that influence how the derivatives are made.
1212910	1216670	34	A	0.81	20:12	7	But is EDA a free energy variable?
1219090	1243210	35	A	0.97713	20:19	14	Let's just ask, okay, what do people think about this type of hybrid modeling?
1245630	1265460	36	A	0.972	20:45	26	How does this align with the previous decades of analog digital signals engineering and the interfaces, the digitalization of analog noise and signals and so on?
1271510	1276680	37	C	0.99637	21:11	10	Well, I'll lay off one thought, if it's all right.
1279770	1352080	38	C	0.86886	21:19	134	So one of the things that active inference lets you do is look at a past trajectory and figure out how to interpret it in terms of your variable, your state variables. So essentially, you look for the probability distribution over variables that optimizes the free energy and that allows you to look back in time and reinterpret evidence because you get something new. And in light of new evidence and in some of the earlier chapters, they had this kind of example and they had the example of a dynamic programming kind of a model for this. So I would expect that to be the case also in communication systems. And I guess I'm going back again to communication where to some extent you're able to reinterpret what you thought you heard before.
1353650	1391562	39	C	0.99484	22:33	94	It's kind of rare in language, but it happens. There are garden path sentences, but usually most communication systems try to be relatively hierarchical in that once you interpret your continuous value signal, then you don't leave a lot of ambiguity. You kind of fix on that and then you have to deal with it later and sometimes you go back and do repair. So I'm wondering if this figure 8.6, if that allows that kind of dynamics where you're kind of tracking something at the continuous value level, you're tracking a signal. Why?
1391616	1441186	40	C	0.84902	23:11	90	Effectively by making predictions of what you're going to hear, what you observe and then is that able in this framework to go and influence the symbol level, the S variables above and how far back in time is it able to do that? Nice. That makes me think about phonemes down here and audio. And then here are the spellings. So in a language or someone is speaking unclearly or you're not super familiar with a language, then you really are like doing inference on the whole time series.
1441298	1522478	41	A	0.86832	24:01	143	It's like that kind of retrocasting which can include even revisionism and delusions and false memories and all of that. Now casting including the tension between representing things quote as they are versus like having delusions in the presence and then anticipation, which of course hasn't even happened unless you have a vision of time as something that always was or however. So it's really interesting that although this time T is an index, we are doing inference across these and maybe it would speak to some measure of information processing and memory and anticipation. The extent to which meaning is able to be the closer meaning is to a critical point. Like the more this graph structure if someone says but I was just kidding, you would want that to apply to be like oh, I didn't know I was in Apprentices.
1522574	1525060	42	A	1.0	25:22	9	And now that they were just kidding about that.
1527530	1550720	43	A	0.99933	25:27	28	So it's actually like delineating those structural markers and then reinterpreting meaning is perhaps moving towards the kind of linguistic modeling. So yeah, to come back to it.
1552770	1561710	44	A	0.99974	25:52	21	So this could be a waveform. The observables down here could be just a Fourier analysis or a pure auditory waveform.
1569940	1573490	45	C	0.99165	26:09	11	Yeah, I think phonemes is that's what comes to mind? Also.
1581250	1584110	46	A	0.81246	26:21	4	Okay. Ocular motor tasks.
1587890	1597650	47	A	0.99983	26:27	16	So selecting a target location and using motor behavior functionally eyes to fove eight the target.
1599910	1606680	48	A	0.95584	26:39	14	Figure 87. Okay, so figure eight seven is a hybrid or mixed generative model.
1610350	1615930	49	A	0.99618	26:50	15	Higher level is this is kind of like the decision making motor active inference fusion.
1625110	1635300	50	A	0.99958	27:05	25	Here we have a little bit of the italics and bold, which we could dive into, but don't need to do so at this moment.
1637510	1654934	51	C	0.93131	27:17	48	So my recollection for last week was that that corresponded to generative model versus process model. You think that might map again here? I know you didn't want to dive into it. I believe it does. Let's just remind ourselves where it came up earlier in the chapter.
1654982	1665710	52	A	0.72896	27:34	18	Yeah, it was always the very beginning right here with italics versus process in bold model in italics.
1669490	1702506	53	A	0.75385	27:49	60	So 87. So now here the discrete model is on a time period of 200 milliseconds. Continuous model operates continuously. I think this speaks to how biological knowledge can be encoded in the model structure. I mean, a more comprehensive approach would have done parameter sweeps for what the optimal number of milliseconds is to do a discrete model on.
1702688	1752170	54	A	0.99997	28:22	105	However, one could say, like, well, the physiology that we're interested in is accomplished with a gamma EEG cycle. The gamma EEG cycle is this many milliseconds. So we're just going to treat that as like a discrete rhythm. Like, one can imagine if the discrete time were one millisecond here, you would be losing computational advantage and you'd be kind of like overslicing. Because if eye movement does happen on a timescale of hundreds of milliseconds, discretizing to one millisecond is going to be too fine, and discretizing to 1 second is not going to enable multiple actions to be taken within that frame.
1754190	1768750	55	A	0.99819	29:14	25	So timescale and spatial scale of a system might be able to be encoded at the phase of its construction or structure learning more generally.
1771090	1798600	56	A	0.94986	29:31	49	Okay, doubtsian mixture model and the hierarchical. Yeah, sorry, before we move on, just about the isochods, I don't understand what is gained by an active inference model for this. I mean, why not just say you got a controller and it's a control loop and that's good enough?
1804600	1827400	57	A	0.81446	30:04	32	Yes. In live stream number 50, which we're just preparing for, it's like very classical cybernetic control loop, homeostatic dynamics. And it's interesting to ask again, what makes the model active inference?
1829440	1890860	58	A	1.0	30:29	83	One could say that there's a Markov blanket, but between every one of these variables, it is a model of action and perception, like so many other models of action and perception. So what is being shown here is it that the single imperative that energy based imperatives just speaking narrowly here, energy based imperatives can mediate decision making in discrete continuous hybrid models, and that it's possible to have a categorical decision making model in feedback with a continuous ocular motor model.
1899160	1911640	59	A	1.0	31:39	19	At the very least, it places this type of model within the domain and grammar of active inference modeling.
1932450	1942900	60	A	0.63683	32:12	19	Data clustering. We may have touched upon also hierarchical Gaussian filter that I know some people have worked on.
1946710	1954520	61	A	0.99753	32:26	19	Okay, let's look at some of the summary advances in continuous time models. Synthetic birdsong. We talked about this.
1956570	1988762	62	A	0.9985	32:36	56	Ocular motor delays conditioned reflex. Okay, so this is in the chapter. This is in the chapter. I don't know about this exact paper, perhaps, but Ocular motor is in the chapter and smooth eye pursuits, conditioned reflexes, that is in this chapter. The blank, the conditioned and the unconditioned stimuli building on the songbird model.
1988816	1990940	63	A	1.0	33:08	7	But the songbird model was in 2015.
1996490	2003190	64	A	0.70173	33:16	11	False inference from suboptimal prior beliefs. Can prior beliefs be suboptimal?
2005470	2033300	65	B	0.998	33:25	44	Sorry. Actually, Friston has talked about bird songs before 2015 in one of his papers, I think from 2013 or twelve. Yeah, it's from 2013, which is co authored with his son, I guess. Dominic A. Friston 2015 paper was not the first paper.
2033750	2043750	66	B	1.0	33:53	21	The name of the paper is the Free Energy Formulation of Music Generation and Perception. Thank you. Thank you. Yeah. Wow.
2043820	2048200	67	A	0.99733	34:03	5	Yeah, good find reminder. Interesting.
2050250	2061260	68	A	0.80126	34:10	21	Looks very much like the textbook, you know, opening quotes. I can see how this is related to your researching too.
2063390	2077870	69	A	0.92139	34:23	16	Okay. Maybe even 2009. Birdsong chaotic Attractors. Wow. It's ten plus years on the birdsong paradigm.
2081270	2105270	70	A	0.97874	34:41	38	Okay, so deviations in perception, more eye movement. It's kind of fun. Like how we've talked about babbling and that across different domains like cicading. Now there's the Ocular motor cicade, but are there other kinds of cicadings?
2108350	2128682	71	A	0.99997	35:08	48	For example, I think that knowledge management niches facilitate semantic cicading. Yes. Enabled by visual cicading as well because you have to be like looking around on the page, but it's just like what was chapter four? Okay, that's what we were talking about, chapter four. What's notation?
2128826	2141300	72	A	0.5	35:28	29	I think it's just sort of like the analog to scanning around a little. Bit, but just have in conversational analysis that's a very important topic is topic control.
2146410	2163530	73	C	0.99897	35:46	39	If you look at any, most conversations have multiple threads going. So it's like a fugue in Bach. Okay, which voice is this? Every note is in a voice and the voice has got some topic and they intertwine.
2166750	2173620	74	A	1.0	36:06	6	In Godel, usherbach, prelude, ant fugue.
2176440	2218130	75	A	0.99963	36:16	82	Nice. This is very interesting and I think it starts to plant some seeds towards, again, semantic circading, computational conversation analysis and generation adaptive knowledge environments like visual accessibility is one topic. If somebody has a visual difference, then making sure that they can see it is the enabling system. But let's just assume that we're within a space where we can talk about what is being seen. So then how does someone get a clear picture of what we're talking about?
2218900	2257840	76	A	1.0	36:58	93	That would be through some sort of circade where you would recognize more uncertainty at the periphery and the phobia in this case is paying attention to the conversation. And that's why with a language we're unfamiliar with, we get a little bit lost because our cicade, we're too slow, we don't have the fluency to circade. And so the video is moving too fast to track. So we're just kind of like randomly listening and trying to compute. But what if somebody were on a guided cicade journey through their semantic landscape.
2258900	2295096	77	C	0.49	37:38	107	I mean, just to take this a little bit further, this is also maybe it's a critical element of cognitive architecture, which is the ability to topic switch. And in psychology and cognitive science, there's this notion of executive control. And one of the fundamentals of executive control is to be able to topic switch. So you're on some task, you get an interrupt, you have to put that on a stack, deal with your interrupt. Then when you're done, maybe the interrupt is a goal, it could be a sub goal, it could be some other topic that comes in, something you have to deal with.
2295198	2325510	78	C	1.0	38:15	95	Then when you're done, you have to be able to pull that back, push that back off the stack and get back to where you were. So all that context switching is really I mean, there's measures of how people do this and there's debates about whether people can multitask or not, whether they're actually really multitasking or they're just multithreading, but they're dividing the amount of computational process by the number of processes you have going. So you're actually not doing multiple things at once as effectively as if you just stuck with one.
2328680	2343240	79	A	0.89	38:48	25	And then it's like, do we have that intel hyperthreading capacity? Is it a virtualized hyper thread? On one thread? Are there actually different threads?
2347760	2367600	80	A	0.94001	39:07	36	Yeah, I mean the conversation analysis, other than us being right in the thick of it and it being highly engaging, support an area. Let's just continue through this cases here. Action observation. Okay. Mirror neurons.
2370420	2381700	81	A	0.99999	39:30	24	This may be very much related to the birdsong singing from the same hymn sheet or whatever and then doing the same dance. Attention.
2383900	2442440	82	A	0.99943	39:43	96	This is a super fascinating area and I think a lot of notebooks and applications like just taking a trace of somebody's browsing on Wikipedia or across websites and then reconstructing components of their generative model based upon their salience revealed choice is a huge area. And then doing again, that with Ocular motor, with Webcams, and doing it with semantic landscapes, hybrid modeling, self organization, arguably with some pretty interesting advances in the last several months and days from Fields, Levin and others on the And Forestaged earlier, but on the morphogenesis as Bayesian inference.
2448510	2456720	83	A	0.58024	40:48	22	Okay, in our last few minutes here, let's just look ahead to nine because I think this is setting us up well.
2464050	2520980	84	A	0.87048	41:04	107	Okay, model based data analysis or databased modeling, or however we want to say this is about parameterizing structured models with observations. So rather than just by fiat dictate saying this is the structure of the model and it spits out observations like so and so. We want to be able to actually run it backwards, not as a generative model, but as a recognition model, so to speak, in this expectation maximization framework and parameterize our generative model from sparse or dense observations. And that's also called parametric empirical base because you're parameterizing the prior based upon the mean invariance and patterns of some empirical data.
2523750	2531110	85	A	0.92213	42:03	6	Okay? The metabasian approach, base theorem.
2533450	2541690	86	A	0.99998	42:13	15	This may even be something that could be introduced earlier, even though it's also meta.
2543870	2581400	87	A	0.99991	42:23	73	This is like the map territory fallacy, fallacy graphical abstract, which is we're making a map here's us in the bigger box, even though there's a dashed box even beyond us. We're making a map of a cognitive entity in its map making of its generative process. And that was the subject also of some very rich discussion several days ago with Yaakov and Dali and Dalton Ali, anything to add on this?
2587190	2659478	88	B	0.9968	43:07	113	No, nothing much. But. One general comment I wanted to make about chapter nine is that, you see, because of my project that I've been working on for the past several months, this chapter nine has been quite helpful in developing some of my ideas around how to exactly model my music emotional recognition system that I've been working on. But I wanted to point out one small caveat here, which is that you see, at least in my opinion, chapter nine, or this so called model based data analysis, is not conducive to generate on every model that's been described throughout the book. It's just focused on some specific kinds of models.
2659654	2668030	89	B	0.99993	44:19	19	So it might not be generalizable to every kind of model that we've seen throughout all the previous chapters.
2671170	2683890	90	A	0.99986	44:31	28	Good point. Some models might be just plug and play, so to speak, and other ones might take a little bit more work to get to use empirically.
2685910	2693560	91	A	0.99971	44:45	21	Okay, let's just continued in our last minutes to scan through so metabasian process. We're drawing inferences about an inferential process.
2696330	2758090	92	A	0.98339	44:56	91	It's a key point. And that's the two levels of mapping. It's like the map of the map variational, LaPlace variational, bayes factorized models for computational simplicity and interpretability based upon the sparsity of coupling of variables in a Bayes graph. And LaPlace using a parabolic approximator of the mode and a variance to ensure a simple optimizable heuristic. Not that it perfectly recapitulates capitulates the shape of the posterior, which doesn't have to be inverted quadratic, but the LaPlace approximation is smoothly optimizable for both the mode and the variance.
2758830	2849206	93	A	1.0	45:58	150	And here's where they mentioned that's the parametric empirical Bayesian approach and basically LaPlace approximation can be thought of as making a commitment to the quadratic form. So choosing the family that your prior is going to be of and then bootstrapping your initial prior with your observed data bootstrapping not in the resampling sense, but in the starting oneself out sense. This was explored more in other works, but you have some likelihood function of all the parameters l of data and optimizing action involves computing beliefs about policy such that policies are proportionally taking up slices of the pie monotonically with respect to their free energy. And then there's a precision on action parameter where you can turn it all the way down to sort of like don't know, don't care, where even large differences in free energy between policies get equilibrated out. That's like hot decision making.
2849388	2883170	94	A	0.85	47:29	69	And then absolute zero decision making would be a policy that's even 51% more likely would be always selected. So the variation of free energy, expected free energy, it's a bound on surprise due to Jensen's inequality and evidence lower bound. And all this other work in variational inference. And then that pi slicing can be tuned up or down with a shaky hand parameter. Soft max temperature parameter.
2887990	2905260	95	A	0.99999	48:07	31	If the soft max is one, the pie is undistorted. If the soft max is large, you have a relentless bias for better policies. Or it can be less than one.
2907710	2910890	96	A	0.77556	48:27	8	Here's some technical details of the LaPlace approximation.
2914190	2922990	97	A	0.99917	48:34	8	Parametric empirical base, generalized linear modeling, hashtag SPM.
2926370	2938340	98	A	0.88128	48:46	37	Here's recipe. So this is kind of like chapter six, but now it's model based data analysis. Collect behavioral data. Maybe the data has already been collected. Maybe there's some data sets that we already can use.
2939430	2969760	99	A	0.99355	48:59	48	Formulate a POMDP. I wonder if this is as chapter six, not as chapter seven. It could be chapter seven as well. If it's discrete time, specify likelihood, perhaps we can, in the coming weeks, check out SPM Mdpvx, and they mention the scripts, data. This is nice.
2971330	2982260	100	A	0.99967	49:31	22	This will be fun. Chapter seven. I mean, it is the correct if they're using this type four. Chapter seven. Chapter four.
2983350	2990930	101	A	0.9929	49:43	13	So this looks pretty cool, pretty interesting. We could walk through some examples.
2998500	3003040	102	A	0.96623	49:58	4	False inference, computational pathology.
3005220	3031960	103	A	0.99996	50:05	43	So it kind of ends like chapter eight with a table of references and a summary. So it's a pretty short chapter and has some formalisms, but it's only from page 179 of the PDF to 196, so it's less than 20 pages.
3035360	3042110	104	B	0.98	50:35	20	The next chapter, I think, is the longest chapter in the book. Which one is chapter ten? Chapter ten, 197.
3046180	3055230	105	A	0.88792	50:46	14	Yeah. That one's over 30 pages. Yeah, but it ends on a high note.
3060390	3108650	106	A	0.81276	51:00	79	All right, well, interesting conversations. Thanks for this. We will come back to chapter nine, look through it, and then peep into ten, go through ten, and really solidify our understanding of the map and I guess indirectly of the territory. And then for those who want to join, we'll probably continue the discussion and especially start looking towards those project ideas again. Like we talked just a little earlier about making a dot zero sessions for the textbook.
3109110	3124370	107	C	0.36189	51:49	39	It so I have to apologize. I'm traveling for a few weeks, so I'm going to miss the next several sessions. It's all good. Thank you, Eric. Any other comments people want to make or I'll close the recording.
3129560	3131330	108	A	0.98681	52:09	3	Okay, thank you.

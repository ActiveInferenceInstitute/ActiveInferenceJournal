start	end	speaker	confidence	text
90	183660	A	0.9276592490118578	You. Hello everyone. It's october 14, 2022. We're in meeting 20. We're in our second discussion of chapter eight. We have a few topics and points in chapter eight and then maybe we can preview some chapter nine and have a lot of space for any other foster questions people are having so we can jump to the questions and look at the written chapter eight questions. Or does anyone want to suggest a point for us to go to last time? We know that some of the areas we wanted to pick up on involved learning and hierarchical modeling and hybrid modeling. One question that came up, I think here, but maybe we can revisit is the birdsong model is widely pointed at as an example of how communication is modeled in active inference or for active inference entities. However, it's quite literally singing from the same hymn sheet. And the turn taking is based upon a joint expectation of that song being sung that way. And then the actions that the birds can take, the kind of turn taking that they enact is again related to whether it's like, well, somebody has to say it is it going to be me or is it going to be them? So how do you think we would broaden that scope of communication to account for situations where there isn't a prescripted song? How do other frameworks for communication cybernetic ish or otherwise deal with the semantics of conversation, improvisation, open endedness in conversation?
193470	275810	B	0.8707732786885243	Actually, I had somewhat more general question about the bird songs and its relation to these particular systems that they've developed in the textbook. You see, some researchers believe that I'm sure you're much qualified to talk about this than me, but there are lots of research going around about the territorialization function of birdsong. So if we take a bird song as its territorializing mechanism, at least one of its territorializing mechanism, can we somehow put the dynamic I mean, the dynamic boundary of the markup blanket, I think would be affected from this point of view. And then I'm not sure how that would translate into this specific model if we take that dynamic nature of the markup blanket boundary.
282070	306250	C	0.9358219999999996	Yeah, I guess I would just build on that and say that communication serves a purpose. And so I think a model should incorporate what that purpose is and how it's served by the communication. And then whether it happens to be syncing up with some predefined pattern or whatever is a follow on consideration.
311580	380110	A	0.9292448951048952	The generative model in the singing from the same hymn sheet is very much like the sheet music example provided in the textbook. There's a known sheet music script such that even when there's like an error in the playing of the music, still the sheet music can be heard. So just coarsely what generative models? I think it's pretty straightforward to see how a generative model could have the notation or the sheet music and then use the sheet music as its prior to kind of converge reality. To what extent is communication that way? Is it totally disparate and differently structured? Or is it a little bit like a hybrid? Like you're doing inference on the sheet music that they're about to say and then you use your inference about what they're about to say to converge what they do say?
382560	437760	C	0.9345315999999998	Well, you asked a good question, which is, well, what are some alternative frameworks? And I know, I know some or should know some, and they're not clean to mind. What comes to mind most immediately in this example is a phase lock loop in electronics which is simply a way of synchronizing one circuit's oscillation with another. And this is like a phase lock loop with a bigger vocabulary. So figure out where we are in the pattern and then synchronize with it. But the question of what other frameworks? I mean, there's this whole field of conversation analysis.
439540	440290	A	0.64091	Which.
442100	556880	C	0.9260322026431711	The field is actually called CA conversation analysis. And they have identified patterns of how people communicate through certain structure of I guess we call them rhetorical devices for achieving the purpose of the conversation. So the purpose of the communication is kind of bedrock. And there are multiple purposes, some of which are instrumental, some are which are social. And then they have identified patterns for the way human conversation tends to proceed which are culturally influenced. There's some cross cultural aspects, but then there are certain different cultures that have different takes on it. So they identify things like, okay, now we're in the conversation opening phase. Now we're in a phase of where we're exchanging information and we have turntaking. Now we have notions of epistemic leadership or epistemic authority. Who's the person who's asking? Who's the person who's answering or taking those roles in the conversation? And there are certain rhetorical stances that are taken. That's a pretty well worked out, I guess, even formalized system because there are well identified patterns. It's not strictly worked out because it seems like every domain a conversation analyst goes to and every every group has their own kind of flavor of it. But it does have this aspect of synchronizing with respect to the underlying known patterns of each of the parties in the conversation.
572810	576160	A	0.7703966666666666	Bird Song is also ollie, please.
577810	702680	B	0.8992635858585855	Sorry. Adding to what Eric just mentioned that reminded me of a mechanism on a much smaller scale, namely the scale of cells and so on, and particularly the model proposed by Stuart Kaufman because he talks about the pluripotent, potent nature of cells and how they communicate with each other in the context of phenomena of induction, biological induction. I mean, in the embryological development stage of the cells, almost all cells are pluripotent and they're capable of becoming any of the different types of cells which characterize the adult individual. So that also, from Kaufman's model perspective, these kind of inductive signals, or so to speak, communicative signals between cells can act as a kind of nonspecific stimulus which switch a cell among a variety of internally available stable states. Or let's put it this way the basic idea in this model is that the regulatory genes within a cell form a complex network in which genes interacting by their products can turn one another on or off. These patterns exhibit the kind of homeostatic stability associated with attractors of the systems. So I think that translates perfectly into this scale as well. This is small.
705050	774490	A	0.9164201162790696	Yeah. The mapping of communication as generalized synchrony may be compatible with a variety of communication mechanisms. And so it abstracts us from the media of communication per se because we can talk about the informational synchronizations between, for example, the two internal states of the two entities or multilateral synchronizations with environmental states or so on. Um, like thinking about the Bayesian mechanics, Bayesian physics, the synchronizing metronomes. They don't have a hymn sheet, they don't have sheet music. Their embodiment and coupling enables synchronization.
777490	840638	C	0.9295089473684204	One thing I might note, I think you asked a good question here, which is how would you translate something like conversation analysis or human conversation into an active inference model? That kind of reminds me that the continuous valued models because they're limited by what you can do in these differential equations, they seem to be fairly simple. Whereas if I were to do something like thinking about human conversation in terms of active inference, I would want something more a very highly high dimensional or multistate type of system where I can have more complex patterns and they may not make these loops, these voltera loops. Is that what they call voltera?
840654	841026	B	0.8570533333333333	Is that right?
841048	876240	C	0.9000173913043477	Or is this Lawrence something? That dynamics and the Lawrence yeah, so they may not make those continuous loops, but you'd still get good structured patterns out of discrete systems. So I'd kind of be curious and I think this chapter would be cool if it had something about communication and synchrony, pattern synchrony, but in terms of discrete valued active inference models instead of the continuous valued ones.
877490	986320	A	0.9209783732057406	Yeah, very interesting. That's what I was wondering. Which is why is communication continuous time models are argued to be a natural fit for motor control? Are communication situations naturally fit for one or the other? Seems like no, you could imagine it either way. And then also the synchronization means the internal state of one creature should come to resolve the resemble internal state of another. A primitive kind of theory of mind, I think. Not as a fatal critique, but these examples put the rabbit in the hat a little bit too keenly. Like the planning example we explored wasn't truly planning. It was like a parameterized subspace where single step optimization emulated planning. Now, one could argue that they believe that that's actually like a better approach than true explicit planning, but that's not what was on the COVID And then similarly, theory of mind, though qualitatively, may be a framing for communication. We're talking still about the birdsong where they have the same shared a prior regenerative model. So this doesn't in other words, theory of mind is axiomatically and implicitly embedded within this model, rather than active inference being used as in a priori first principles to generate some realized computational theory of mind.
986710	1013160	C	0.9040347692307691	I feel like, yeah, it's like another aspect of the continuous valued formulations is there's really not that many degrees of freedom, but when you're communicating anything other than a fixed song, there's a lot of degrees of freedom. And that's why we have symbol systems. But symbol systems is where you would use a discrete model framework for not a continuous value to win.
1018080	1265460	A	0.9342554920634916	Okay, let's talk about hybrid models, discrete and continuous models and learning and updating. So these are two features. Chapter eight is within a continuous time framework, just like chapter seven, layered in these kind of patterns or archetypes of discrete time models in chapter eight, analogously, the most kernel continuous time formalism is introduced with movement control and then we're going to see some kind of extensions or patterns like design patterns that include continuous time. This is the message passing. Okay, so missing line here, but we have here, this top half is like the bottom of figure 4.3, or the top, I guess, discrete time. So we have B matrix as a transition matrix. We have discrete time and we have discrete time. T plus one, t minus one. But within each time point we see the continuous time e whatever we want to call this motif where the B transition matrix in discrete time is functionally substituted with a derivative map such that rather than actually retro or forecasting state values through time. This is much more like a Taylor Series approximation, where extrapolation from the present, under the auspices of slower changing causes v, which play a functional role of policies, higher derivatives capture more and more variance further and further from the origin. What is the Ita doing? It's playing some functional role to g again, allowing for a line here in the sense that it is selecting policy, but it's not policy that's being selected or discussed this way, but these slower changing factors that influence how the derivatives are made. But is EDA a free energy variable? Let's just ask, okay, what do people think about this type of hybrid modeling? How does this align with the previous decades of analog digital signals engineering and the interfaces, the digitalization of analog noise and signals and so on?
1271510	1412720	C	0.9257021631205669	Well, I'll lay off one thought, if it's all right. So one of the things that active inference lets you do is look at a past trajectory and figure out how to interpret it in terms of your variable, your state variables. So essentially, you look for the probability distribution over variables that optimizes the free energy and that allows you to look back in time and reinterpret evidence because you get something new. And in light of new evidence and in some of the earlier chapters, they had this kind of example and they had the example of a dynamic programming kind of a model for this. So I would expect that to be the case also in communication systems. And I guess I'm going back again to communication where to some extent you're able to reinterpret what you thought you heard before. It's kind of rare in language, but it happens. There are garden path sentences, but usually most communication systems try to be relatively hierarchical in that once you interpret your continuous value signal, then you don't leave a lot of ambiguity. You kind of fix on that and then you have to deal with it later and sometimes you go back and do repair. So I'm wondering if this figure 8.6, if that allows that kind of dynamics where you're kind of tracking something at the continuous value level, you're tracking a signal. Why? Effectively by making predictions of what you're going to hear, what you observe and then is that able in this framework to go and influence the symbol level, the S variables above and how far back in time is it able to do that?
1414230	1561710	A	0.9210384615384614	Nice. That makes me think about phonemes down here and audio. And then here are the spellings. So in a language or someone is speaking unclearly or you're not super familiar with a language, then you really are like doing inference on the whole time series. It's like that kind of retrocasting which can include even revisionism and delusions and false memories and all of that. Now casting including the tension between representing things quote as they are versus like having delusions in the presence and then anticipation, which of course hasn't even happened unless you have a vision of time as something that always was or however. So it's really interesting that although this time T is an index, we are doing inference across these and maybe it would speak to some measure of information processing and memory and anticipation. The extent to which meaning is able to be the closer meaning is to a critical point. Like the more this graph structure if someone says but I was just kidding, you would want that to apply to be like oh, I didn't know I was in Apprentices. And now that they were just kidding about that. So it's actually like delineating those structural markers and then reinterpreting meaning is perhaps moving towards the kind of linguistic modeling. So yeah, to come back to it. So this could be a waveform. The observables down here could be just a Fourier analysis or a pure auditory waveform.
1569940	1573490	C	0.9446372727272728	Yeah, I think phonemes is that's what comes to mind? Also.
1581250	1635300	A	0.9025901351351351	Okay. Ocular motor tasks. So selecting a target location and using motor behavior functionally eyes to fove eight the target. Figure 87. Okay, so figure eight seven is a hybrid or mixed generative model. Higher level is this is kind of like the decision making motor active inference fusion. Here we have a little bit of the italics and bold, which we could dive into, but don't need to do so at this moment.
1637510	1650620	C	0.9212340625000001	So my recollection for last week was that that corresponded to generative model versus process model. You think that might map again here? I know you didn't want to dive into it.
1651070	1781730	A	0.9079015584415583	I believe it does. Let's just remind ourselves where it came up earlier in the chapter. Yeah, it was always the very beginning right here with italics versus process in bold model in italics. So 87. So now here the discrete model is on a time period of 200 milliseconds. Continuous model operates continuously. I think this speaks to how biological knowledge can be encoded in the model structure. I mean, a more comprehensive approach would have done parameter sweeps for what the optimal number of milliseconds is to do a discrete model on. However, one could say, like, well, the physiology that we're interested in is accomplished with a gamma EEG cycle. The gamma EEG cycle is this many milliseconds. So we're just going to treat that as like a discrete rhythm. Like, one can imagine if the discrete time were one millisecond here, you would be losing computational advantage and you'd be kind of like overslicing. Because if eye movement does happen on a timescale of hundreds of milliseconds, discretizing to one millisecond is going to be too fine, and discretizing to 1 second is not going to enable multiple actions to be taken within that frame. So timescale and spatial scale of a system might be able to be encoded at the phase of its construction or structure learning more generally. Okay, doubtsian mixture model and the hierarchical.
1782310	1798600	C	0.9251999999999999	Yeah, sorry, before we move on, just about the isochods, I don't understand what is gained by an active inference model for this. I mean, why not just say you got a controller and it's a control loop and that's good enough?
1804600	2003190	A	0.9301276016260162	Yes. In live stream number 50, which we're just preparing for, it's like very classical cybernetic control loop, homeostatic dynamics. And it's interesting to ask again, what makes the model active inference? One could say that there's a Markov blanket, but between every one of these variables, it is a model of action and perception, like so many other models of action and perception. So what is being shown here is it that the single imperative that energy based imperatives just speaking narrowly here, energy based imperatives can mediate decision making in discrete continuous hybrid models, and that it's possible to have a categorical decision making model in feedback with a continuous ocular motor model. At the very least, it places this type of model within the domain and grammar of active inference modeling. Data clustering. We may have touched upon also hierarchical Gaussian filter that I know some people have worked on. Okay, let's look at some of the summary advances in continuous time models. Synthetic birdsong. We talked about this. Ocular motor delays conditioned reflex. Okay, so this is in the chapter. This is in the chapter. I don't know about this exact paper, perhaps, but Ocular motor is in the chapter and smooth eye pursuits, conditioned reflexes, that is in this chapter. The blank, the conditioned and the unconditioned stimuli building on the songbird model. But the songbird model was in 2015. False inference from suboptimal prior beliefs. Can prior beliefs be suboptimal?
2005470	2039694	B	0.8898906779661018	Sorry. Actually, Friston has talked about bird songs before 2015 in one of his papers, I think from 2013 or twelve. Yeah, it's from 2013, which is co authored with his son, I guess. Dominic A. Friston 2015 paper was not the first paper. The name of the paper is the Free Energy Formulation of Music Generation and Perception.
2039822	2133266	A	0.9077525000000001	Thank you. Thank you. Yeah. Wow. Yeah, good find reminder. Interesting. Looks very much like the textbook, you know, opening quotes. I can see how this is related to your researching too. Okay. Maybe even 2009. Birdsong chaotic Attractors. Wow. It's ten plus years on the birdsong paradigm. Okay, so deviations in perception, more eye movement. It's kind of fun. Like how we've talked about babbling and that across different domains like cicading. Now there's the Ocular motor cicade, but are there other kinds of cicadings? For example, I think that knowledge management niches facilitate semantic cicading. Yes. Enabled by visual cicading as well because you have to be like looking around on the page, but it's just like what was chapter four? Okay, that's what we were talking about, chapter four. What's notation? I think it's just sort of like the analog to scanning around a little.
2133288	2163530	C	0.9156637037037039	Bit, but just have in conversational analysis that's a very important topic is topic control. If you look at any, most conversations have multiple threads going. So it's like a fugue in Bach. Okay, which voice is this? Every note is in a voice and the voice has got some topic and they intertwine.
2166750	2257840	A	0.9062029834254137	In Godel, usherbach, prelude, ant fugue. Nice. This is very interesting and I think it starts to plant some seeds towards, again, semantic circading, computational conversation analysis and generation adaptive knowledge environments like visual accessibility is one topic. If somebody has a visual difference, then making sure that they can see it is the enabling system. But let's just assume that we're within a space where we can talk about what is being seen. So then how does someone get a clear picture of what we're talking about? That would be through some sort of circade where you would recognize more uncertainty at the periphery and the phobia in this case is paying attention to the conversation. And that's why with a language we're unfamiliar with, we get a little bit lost because our cicade, we're too slow, we don't have the fluency to circade. And so the video is moving too fast to track. So we're just kind of like randomly listening and trying to compute. But what if somebody were on a guided cicade journey through their semantic landscape.
2258900	2325510	C	0.9408959405940592	I mean, just to take this a little bit further, this is also maybe it's a critical element of cognitive architecture, which is the ability to topic switch. And in psychology and cognitive science, there's this notion of executive control. And one of the fundamentals of executive control is to be able to topic switch. So you're on some task, you get an interrupt, you have to put that on a stack, deal with your interrupt. Then when you're done, maybe the interrupt is a goal, it could be a sub goal, it could be some other topic that comes in, something you have to deal with. Then when you're done, you have to be able to pull that back, push that back off the stack and get back to where you were. So all that context switching is really I mean, there's measures of how people do this and there's debates about whether people can multitask or not, whether they're actually really multitasking or they're just multithreading, but they're dividing the amount of computational process by the number of processes you have going. So you're actually not doing multiple things at once as effectively as if you just stuck with one.
2328680	2581400	A	0.9096230940594056	And then it's like, do we have that intel hyperthreading capacity? Is it a virtualized hyper thread? On one thread? Are there actually different threads? Yeah, I mean the conversation analysis, other than us being right in the thick of it and it being highly engaging, support an area. Let's just continue through this cases here. Action observation. Okay. Mirror neurons. This may be very much related to the birdsong singing from the same hymn sheet or whatever and then doing the same dance. Attention. This is a super fascinating area and I think a lot of notebooks and applications like just taking a trace of somebody's browsing on Wikipedia or across websites and then reconstructing components of their generative model based upon their salience revealed choice is a huge area. And then doing again, that with Ocular motor, with Webcams, and doing it with semantic landscapes, hybrid modeling, self organization, arguably with some pretty interesting advances in the last several months and days from Fields, Levin and others on the And Forestaged earlier, but on the morphogenesis as Bayesian inference. Okay, in our last few minutes here, let's just look ahead to nine because I think this is setting us up well. Okay, model based data analysis or databased modeling, or however we want to say this is about parameterizing structured models with observations. So rather than just by fiat dictate saying this is the structure of the model and it spits out observations like so and so. We want to be able to actually run it backwards, not as a generative model, but as a recognition model, so to speak, in this expectation maximization framework and parameterize our generative model from sparse or dense observations. And that's also called parametric empirical base because you're parameterizing the prior based upon the mean invariance and patterns of some empirical data. Okay? The metabasian approach, base theorem. This may even be something that could be introduced earlier, even though it's also meta. This is like the map territory fallacy, fallacy graphical abstract, which is we're making a map here's us in the bigger box, even though there's a dashed box even beyond us. We're making a map of a cognitive entity in its map making of its generative process. And that was the subject also of some very rich discussion several days ago with Yaakov and Dali and Dalton Ali, anything to add on this?
2587190	2588466	B	0.9960233333333334	No, nothing much.
2588568	2589220	C	1.0	But.
2591050	2668030	B	0.9466328125000001	One general comment I wanted to make about chapter nine is that, you see, because of my project that I've been working on for the past several months, this chapter nine has been quite helpful in developing some of my ideas around how to exactly model my music emotional recognition system that I've been working on. But I wanted to point out one small caveat here, which is that you see, at least in my opinion, chapter nine, or this so called model based data analysis, is not conducive to generate on every model that's been described throughout the book. It's just focused on some specific kinds of models. So it might not be generalizable to every kind of model that we've seen throughout all the previous chapters.
2671170	3031960	A	0.9119819197207686	Good point. Some models might be just plug and play, so to speak, and other ones might take a little bit more work to get to use empirically. Okay, let's just continued in our last minutes to scan through so metabasian process. We're drawing inferences about an inferential process. It's a key point. And that's the two levels of mapping. It's like the map of the map variational, LaPlace variational, bayes factorized models for computational simplicity and interpretability based upon the sparsity of coupling of variables in a Bayes graph. And LaPlace using a parabolic approximator of the mode and a variance to ensure a simple optimizable heuristic. Not that it perfectly recapitulates capitulates the shape of the posterior, which doesn't have to be inverted quadratic, but the LaPlace approximation is smoothly optimizable for both the mode and the variance. And here's where they mentioned that's the parametric empirical Bayesian approach and basically LaPlace approximation can be thought of as making a commitment to the quadratic form. So choosing the family that your prior is going to be of and then bootstrapping your initial prior with your observed data bootstrapping not in the resampling sense, but in the starting oneself out sense. This was explored more in other works, but you have some likelihood function of all the parameters l of data and optimizing action involves computing beliefs about policy such that policies are proportionally taking up slices of the pie monotonically with respect to their free energy. And then there's a precision on action parameter where you can turn it all the way down to sort of like don't know, don't care, where even large differences in free energy between policies get equilibrated out. That's like hot decision making. And then absolute zero decision making would be a policy that's even 51% more likely would be always selected. So the variation of free energy, expected free energy, it's a bound on surprise due to Jensen's inequality and evidence lower bound. And all this other work in variational inference. And then that pi slicing can be tuned up or down with a shaky hand parameter. Soft max temperature parameter. If the soft max is one, the pie is undistorted. If the soft max is large, you have a relentless bias for better policies. Or it can be less than one. Here's some technical details of the LaPlace approximation. Parametric empirical base, generalized linear modeling, hashtag SPM. Here's recipe. So this is kind of like chapter six, but now it's model based data analysis. Collect behavioral data. Maybe the data has already been collected. Maybe there's some data sets that we already can use. Formulate a POMDP. I wonder if this is as chapter six, not as chapter seven. It could be chapter seven as well. If it's discrete time, specify likelihood, perhaps we can, in the coming weeks, check out SPM Mdpvx, and they mention the scripts, data. This is nice. This will be fun. Chapter seven. I mean, it is the correct if they're using this type four. Chapter seven. Chapter four. So this looks pretty cool, pretty interesting. We could walk through some examples. False inference, computational pathology. So it kind of ends like chapter eight with a table of references and a summary. So it's a pretty short chapter and has some formalisms, but it's only from page 179 of the PDF to 196, so it's less than 20 pages.
3035360	3038524	B	0.9479974999999999	The next chapter, I think, is the longest chapter in the book.
3038642	3108650	A	0.9508810891089104	Which one is chapter ten? Chapter ten, 197. Yeah. That one's over 30 pages. Yeah, but it ends on a high note. All right, well, interesting conversations. Thanks for this. We will come back to chapter nine, look through it, and then peep into ten, go through ten, and really solidify our understanding of the map and I guess indirectly of the territory. And then for those who want to join, we'll probably continue the discussion and especially start looking towards those project ideas again. Like we talked just a little earlier about making a dot zero sessions for the textbook.
3109110	3117650	C	0.926720476190476	It so I have to apologize. I'm traveling for a few weeks, so I'm going to miss the next several sessions.
3118310	3131330	A	0.9224347619047618	It's all good. Thank you, Eric. Any other comments people want to make or I'll close the recording. Okay, thank you.

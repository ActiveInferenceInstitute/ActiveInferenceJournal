SPEAKER_00:
Hello, everyone.

It's October 14, 2022.

We're in

meeting 20 we're in our second discussion of chapter 8 we have a few topics and points in chapter 8 and then maybe we can preview some chapter 9 and have a lot of space for any other thoughts or questions people are having so we can jump to the questions and look at the um

written chapter eight questions or does anyone want to suggest a point for us to go to last time we know that some of the areas we wanted to pick up on um involved learning and hierarchical modeling and hybrid modeling

One question that came up, I think, here, but maybe we can revisit, is the birdsong model is widely pointed at as an example of how communication is modeled in active inference or for active inference entities.

However, it's quite literally singing from the same hymn sheet,

And the turn-taking is based upon a joint expectation of that song being sung that way.

And then the actions that the birds can take, the kind of turn-taking that they enact is, again, it's related to whether it's like, well, somebody has to say it.

Is it going to be me or is it going to be them?

So how do you think we would broaden that scope of communication to account for situations where there isn't a pre-scripted song?

How do other frameworks for communication, cybernetic-ish or otherwise,

deal with the semantics of conversation, improvisation, open-endedness in conversation


SPEAKER_01:
Actually, I had a somewhat more general question about the bird songs and its relation to these particular systems that they've developed in the textbook.

You see,

Some researchers believe that, I'm sure you're much qualified to talk about this than me, but there are lots of research going around about the territorialization function of birdsongs.

So if we take Birdsong as its territorializing mechanism, at least one of its territorializing mechanism, can we

somehow put the dynamic I mean the dynamic boundary of the Markov blanket I think would be affected from this point of view and then I'm not sure how that would translate into this specific model if we take that dynamic nature of the Markov blanket boundary


SPEAKER_02:
Yeah, I guess I would just build on that and say that communication serves a purpose.

And so I think a model should incorporate what that purpose is and how it's served by the communication.

And then whether it happens to be, you know, syncing up with some predefined pattern or whatever is a follow-on consideration.


SPEAKER_00:
The generative model in the singing from the same hymn sheet is very much like the sheet music example provided in the textbook.

There's a known sheet music script such that even when there's an error in the playing of the music, still the sheet music can be heard.

So just coarsely

What generative models?

I think it's pretty straightforward to see how a generative model could have the notation or the sheet music and then use the sheet music as its prior to kind of converge reality.

To what extent is communication that way?

Is it totally disparate and differently structured or is it a little bit like a hybrid?

Like you're doing inference on the sheet music that they're about to say, and then you use your inference about what they're about to say to converge what they do say.


SPEAKER_02:
Well, you asked a good question, which is, well, what are some alternative frameworks?

And, um,

I know I know some or should know some, and they're not coming to mind.

But what comes to mind most immediately in this example is a phase lock loop in electronics, which is simply a way of synchronizing one circuit's oscillation with another.

And this is like a phase lock loop with a bigger vocabulary.

So figure out where we are in the pattern and then synchronize with it.

And but the question of what other frameworks?

I mean, there's this whole field of conversation analysis.

which, well, the field's actually called CA, Conversation Analysis, and they have identified patterns of how people communicate through a certain structure of, I guess we call them rhetorical devices, for achieving the purpose of the conversation.

So the purpose is,

The communication is kind of bedrock, and there are multiple purposes, some of which are instrumental, some of which are social.

And then they have identified patterns for the way human conversation tends to proceed, which are culturally influenced.

You know, there's some cross-cultural aspects, but then there's certain different cultures that have different takes on it.

So they identify things like, okay, now we're in the conversation opening phase.

Now we're in a phase where we're exchanging information and we have turn-taking.

Now we have notions of epistemic leadership or epistemic authority.

Who's the person who's asking?

Who's the person who's answering or taking those roles in the conversation?

And there's certain rhetorical stances that are taken.

So that's a pretty well worked out

I guess even formalized system because there are well-identified patterns.

It's not strictly worked out because it seems like every domain a conversation analyst goes to and every group has their own kind of flavor of it.

But it is kind of...

It does have this aspect of synchronizing with respect to the underlying known patterns of each of the parties in the conversation.


SPEAKER_00:
The third song is also... Ali, please.


SPEAKER_01:
Sorry, adding to what Eric just mentioned, that reminded me of a mechanism on a much smaller scale, namely the scale of cells and so on, and particularly the model proposed by Stuart Kaufman, because he talks about

the pluripotent nature of cells and how they communicate with each other in the context of phenomena of induction, biological induction.

I mean, in the embryological development stage of the cells, almost all cells are pluripotent and they're capable of becoming any of the different types of cells which characterize the adult individual.

So that also, from Kaufman's model perspective,

these kind of inductive signals or so to speak communicative signals between cells can act as a kind of non-specific stimulus which switch a cell among a variety of internally available stable states.

So I mean a cell

Or let's put it this way.

The basic idea in this model is that the regulatory genes within a cell form a complex network in which genes interacting by their products can turn one another on or off.

You see?

So yeah, these patterns exhibit the kind of homeostatic stability associated with attractors of the systems.

So I think that translates perfectly into this scale as well, this small scale.


SPEAKER_00:
Yeah.

the mapping of communication as generalized synchrony may be compatible with a variety of communication mechanisms.

And so it abstracts us from the media of communication per se, because we can talk about the informational synchronizations

between, for example, the two internal states of the two entities or multilateral synchronizations with environmental states or so on.

Like thinking about the Bayesian mechanics, Bayesian physics, the synchronizing metronomes,

don't have a hymn sheet they don't have sheet music their embodiment and coupling um enables synchronization now one thing i might know you know you i i think you asked a good question here which is you know how would you transfer translate something like um um conversation analysis


SPEAKER_02:
or human conversation into an active inference model.

So that kind of reminds me that the continuous valued models, because they're limited by what you can do in these differential equations, they seem to be fairly simple.

Whereas if I were to do something like thinking about human conversation in terms of active inference, I would want something more very highly high dimensional or multi-state type of system where I can have more complex patterns.

And they may not make these loops, these Volterra loops, or is that what they're called?

Volterra, is that right?

Or is this Lawrence, something in the dynamics?


SPEAKER_00:
Yeah, Volterra and the Lawrence.


SPEAKER_02:
Yeah.

Yeah, so it may not make those continuous loops, but you'd still get good structured patterns out of discrete systems.

So I'd kind of be curious, and I think this chapter would be cool if it had something about communication and synchrony, pattern synchrony, but in terms of discrete valued active inference models instead of the continuous valued ones.


SPEAKER_00:
Yeah.

Yeah, very interesting.

That's what I was wondering, which is why is communication... Continuous time models are argued to be a natural fit for motor control.

Are communication situations naturally fit for one or the other?

seems like no, you could imagine it either way.

And then also like the synchronization means the internal state of one creature should come to resolve the, resemble internal states of another, a primitive kind of theory of mind.

I think not as a fatal critique, but these examples put the rabbit in the hat

a little bit too keenly.

Like the planning example we explored wasn't truly planning.

It was like a parameterized subspace where single step optimization emulated planning.

And one could argue that they believe that that's actually like a better approach than true explicit planning, but that's not what was on the cover.

And then similarly,

theory of mind though qualitatively may be a framing for communication we're talking still about the bird song where they have the same shared a priori generative model so this doesn't um in other words theory of mind is is um is axiomatically and implicitly embedded within this model

rather than active inference being used as an a priori first principles to generate some realized computational theory of mind i feel like yeah yeah it's like um you know another aspect of this the continuous valued formulations is there's really not that many degrees of freedom


SPEAKER_02:
But when you're communicating anything other than a fixed song, there's a lot of degrees of freedom.

And that's why we have symbol systems.

But symbol systems is where you would use a discrete model framework for not a continuous value line.


SPEAKER_00:
OK.

Let's talk about hybrid models, discrete and continuous models.

and learning and updating so these are two features um chapter eight is um within a continuous time framework just like chapter seven layered in these kind of patterns or archetypes of discrete time models in chapter eight analogously the most

kernel continuous time formalism is introduced with movement control and then we're going to see some kind of extensive extensions or um patterns like design patterns that include continuous time this is the um message passing

Okay, so missing line here, but we have here, this top half is like the bottom of figure 4.3 or the top, I guess, discrete time.

So we have B matrix as a transition matrix.

We have discrete time and we have, yeah, it's discrete time, T plus one, T minus one.

But within each time point,

we see the continuous time, E, whatever we want to call this motif, where the B transition matrix in discrete time is functionally substituted with a derivative map, such that rather than actually retro or forecasting

state values through time this is much more like a taylor series approximation where extrapolation from the present under the auspices of slower changing causes v which play a functional role of policies higher derivatives capture more and more variance further and further from the origin

What is the ETA doing?

It's playing some functional role to G. Again, allowing for a line here in the sense that it is selecting policy.

But it's not policy that's being selected or discussed this way, but the slower changing factors that influence how the derivatives are made.

But is eta a free energy variable?

Let's just ask.

Okay.

What do people think about this type of hybrid modeling?

How does this align with the previous decades of analog digital signals engineering and the interfaces, the digitalization of analog noise and signals and so on?


SPEAKER_02:
Well, let me offer one thought, if it's all right.

So one of the things that active inference lets you do is look at a past trajectory and figure out how to interpret it in terms of your state variables.

So you can essentially you look for the the probability distribution over variables that optimizes the free energy and that allows you to look back in time and reinterpret evidence because you get something new and you get in light of new evidence.

And then some of the earlier chapters they had this kind of example and they had the example of the

of a dynamic programming kind of a model for this.

So I would expect that to be the case also in communication systems.

I guess I'm going back again to communication, where to some extent, you're able to reinterpret what you thought you heard before.

It's kind of rare in language, but it happens.

There are garden path sentences.

but usually you know you most communication systems try to be relatively hierarchical in that once you was once you interpret your continuous value signal then you don't leave a lot of ambiguity you kind of you know fix on that and then you have to deal with it later and sometimes you go back and do repair so i'm wondering if this figure 8.6 if that allows that kind of dynamics where

You know, you're kind of tracking something at the continuous value level.

You're tracking a signal, Y effectively, by making predictions of what you're going to hear, what you observe.

And then is that able, in this framework, to go and influence the symbol level, the S variables above, and how far back in time is it able to do that?


SPEAKER_00:
nice that makes me think about phonemes down here and audio and then here are the um spellings so in a language or someone is speaking unclearly or you're not super familiar with the language um then you really are like doing inference on the on the whole time series like it's like that kind of um

retrocasting, which can include even revisionism and delusions and false memories and all of that.

Now casting, including the tension between representing things, quote, as they are versus like having delusions in the present.

And then anticipation, which of course hasn't even happened unless you have a vision of time as, you know,

something that always was or however.

So it's really interesting that although this time t is an index, we are doing inference across these.

And maybe it would speak to some measure of information processing and memory and anticipation, the extent to which

um like meaning is able to be the closer meaning is to a critical point like the more this graph structure if someone says but i was just kidding you would want that to apply to to be like oh i didn't know i was in a parentheses and now that they were just kidding about that um

So it's actually like delineating those structural markers and then reinterpreting meaning is perhaps moving towards the kind of linguistic modeling.

So yeah, to come back to it.

So this could be a waveform.

observables down here could could be just a four-year analysis or a pure auditory waveform yeah i think phonemes is that that's what comes to mind also okay okay ocular motor tasks

So selecting a target location and using motor behavior functionally eyes to foveate the target.

Figure eight, seven.

Okay, so figure eight, seven is a hybrid or mixed generative model.

Higher level, this is kind of like the decision-making motor active inference fusion.

Here we have a little bit of the italics and bold, which we could dive into but don't need to do so at this moment.


SPEAKER_02:
So my recollection for last week was that that corresponded to generative model versus process model.

You think that might map again here?

I know you didn't want to dive into it, but you did.


SPEAKER_00:
i believe it does let's just remind ourselves where it came up earlier in the chapter yeah it was it was the very beginning right here yeah with italics versus process in bold model in italics okay so eight seven so now here the um discrete model is on a time period of 200 milliseconds

Continuous model operates continuously.

I think this speaks to how biological knowledge can be encoded in the model structure

I mean, a more comprehensive approach would have done parameter sweeps for what the optimal number of milliseconds is to do a discrete model on.

However, one could say like, well, the physiology that we're interested in is accomplished with a gamma EEG cycle.

The gamma EEG cycle is this many milliseconds.

So we're just going to treat

that as like a discrete rhythm like one can imagine if the discrete time were one millisecond here you would be losing computational advantage and you'd be kind of like over slicing because if eye movement does happen on a time scale of hundreds of milliseconds

discretizing to one millisecond is going to be too fine.

And discretizing to one second is not going to enable multiple actions to be taken within that frame.

So time scale and spatial scale of a system might be able to be encoded at the phase of its construction or structured learning more generally.

Okay.

Gaussian mixture model and the hierarchical.


SPEAKER_02:
Yeah.

Sorry, before we move on, just about the isochodes.

I don't understand what is gained by an active inference model for this.

I mean, why not just say you've got a controller and it's a control loop and that's good enough.


SPEAKER_00:
Yes.

in live stream number 50, which we're just preparing for, it's like very classical cybernetic control loop, homeostatic dynamics.

And it's interesting to ask like, again, what makes the model active inference?

One could say that there's a Markov blanket, but we're, you know, between every one of these variables.

It is a model of action and perception like so many other models of action and perception.

So what is being shown here?

Is it that the single imperative, that energy-based imperatives, just speaking narrowly here, energy-based imperatives can mediate

decision-making in discrete continuous hybrid models.

And that it's possible to have a categorical decision-making model in feedback with a continuous ocular motor model

At the very least, it places this type of model within the domain and grammar of active inference modeling.

Okay, data clustering we may have touched upon, also hierarchical Gaussian filter that I know some people have worked on.

Okay, let's look at some of the summary advances in continuous time models.

Synthetic birdsong, we talked about this.

Ocular motor delays.

Conditioned reflex.

Okay, so this is in the chapter.

This is in the chapter.

I don't know about this exact paper perhaps, but ocular motor is in the chapter.

And smooth eye pursuits.

Conditioned reflexes.

That is in this chapter.

The blink, the conditioned and the unconditioned stimuli.

Building on the Songbird model, but the Songbird model was in 2015.

False inference from suboptimal prior beliefs.

Can prior beliefs be suboptimal?


SPEAKER_01:
I'm sorry.

Actually, Friston has talked about Birdsongs before 2015 in one of his papers, I think from 2013 or 12.

Yeah, it's from 2013, which is co-authored with...

uh his son i guess dominic a friston so yeah i mean the 2015 paper was not the first paper the name of the paper is the is the free energy formulation of music generation and perception thank you thank you yeah this one wow yeah good um fine reminder interesting


SPEAKER_00:
Looks very much like the textbook, you know, opening quotes.

I can see how this is related to your researching too.

Okay.

Oh, maybe even 2009, Birdsong, Chaotic Attractors.

Wow, you know, it's 10 plus years on the Birdsong paradigm.

Okay, so deviations in perception.

More eye movement.

It's kind of fun, like how we've talked about babbling and that across different domains, like cicading.

Now there's the ocular motor cicade, but are there other kinds of cicadings?

For example, I think that

knowledge management niches facilitate semantic circadian yes enabled by visual circadian as well because you have to be like looking around on the page but it's just like what was chapter four okay we were talking okay that's what we were talking about chapter four what's notation i think it's just sort of like the analog to scanning around a little bit


SPEAKER_02:
But just having conversation analysis, that's, you know, that's a very important topic is topic control.

And, or, you know, if you're, if you look at any, you know, most conversations have multiple threads going.

So it's like a fugue in, you know, Bach, okay, you know, which voice is this?

You know, every note is in a voice.

And the voice has got some topic, and they intertwine.


SPEAKER_00:
in girdle escherbach prelude ant fugue nice this is very um interesting i think it starts to um plant some seeds towards again semantic circadian computational conversation analysis and generation adaptive knowledge environments

like visual accessibility is one topic.

You know, if somebody has a visual difference, then making sure that they can see it is the enabling system.

But let's just assume that we're within a space where we can talk about what is being seen.

So then how does someone get a clear picture of what we're talking about?

That would be through some sort of saccade.

where you'd recognize more uncertainty at the periphery.

And like the fovea in this case is paying attention to the conversation.

And that's why like with a language we're unfamiliar with, we get a little bit lost because like our saccade, we're too slow.

We don't have the fluency to saccade.

And so the video is moving too fast to track.

So we're just kind of like randomly listening and like trying to like compute.

but what if somebody were on a guided, saccade journey through their semantic landscape?


SPEAKER_02:
I mean, just to take this a little bit further, this is also maybe, it's a critical element of,

cognitive architecture, which is the ability to topic switch.

And in psychology and cognitive science, there's this notion of executive control.

And one of the fundamentals of executive control is to be able to topic switch.

So you're on some task, you get an interrupt, you have to put that on a stack, deal with your interrupt.

Then when you're done,

maybe the interrupt is a goal it could be a sub goal it could be some other topic that comes in something you have to deal with then when you're done you have to be able to pull that back push that back off the stack

and get back to where you were.

So all that context switching is really, I mean, there's measures of how people do this.

And there's debates about whether people can multitask or not, whether they're actually really multitasking or they're just multi-threading, but they're dividing the amount of computational process by the number of processes you have going.

So you're actually not doing multiple things at once as effectively as if you just stuck with one.

So yeah, anyway.


SPEAKER_00:
and then it's like do we have that intel hyper threading capacity is it a virtualized hyper thread on one thread are there actually different threads yeah i mean the conversation analysis other than us being right in the thick of it and it being highly engaging

it's important area let's just continue through this cases here action observation okay mirror neurons this may be very much related to the um bird song singing from the same hymn sheet or whatever and then like you know doing the same dance okay attention this is a super fascinating area

And I think a lot of notebooks and applications, like just taking a trace of somebody's browsing on Wikipedia or across websites and then reconstructing components of their generative model based upon their salience revealed choice is a huge area.

And then doing again that with the ocular motor, with webcams,

uh and doing it with uh semantic landscapes hybrid modeling self-organization arguably with some pretty interesting advances in the last several months and days from fields and levin and others on the uh and four-staged earlier but on the morphogenesis as bayesian inference

Okay, in our last few minutes here, let's just look ahead to nine because I think this is setting us up well.

Okay.

Model-based data analysis or data-based modeling or however we want to say this is about parameterizing

structured models with observations so rather than just by fiat dictate saying this is the structure of the model and it spits out observations like so and so we want to be able to actually run it backwards not as a generative model but as a recognition model

so to speak in this expectation maximization framework and parameterize our generative model from sparse or dense observations.

And that's also called parametric empirical base because you're parameterizing the prior based upon the mean and variance and patterns of some empirical data.

Okay, the Metabasian approach.

base theorem this may even be something that could be introduced earlier even though it's also meta this is like the map territory fallacy fallacy graphical abstract which is we're making a map here's us in the bigger box even though there's a dashed box even beyond us

were making a map of a cognitive entity in its map making of its generative process and that was the subject also of some very rich discussion several days ago with Jakob and Ali and Dalton Ali anything to add on this


SPEAKER_01:
I know nothing much but one general comment I wanted to make about chapter nine is that you see because of my project that I've been working on for the past several months this chapter nine has been quite helpful in developing some of my ideas around how to how to exactly model my

I mean, the music emotional recognition system that I've been working on.

But I wanted to point out one small caveat here, which is that, you see, at least in my opinion, Chapter 9, or this so-called model-based data analysis, is not conducive to...

generate every, I mean, on every model that's been described throughout the throughout the book, I mean, it's just focused on some specific kinds of models.

So it might not be generalizable to every kind of model that we've seen throughout all the previous chapters.


SPEAKER_00:
Good point.

Some models might be just plug and play, so to speak, and other ones might take a little bit more work to get to use empirically.

Okay, let's just continue in our last minutes to scan through.

So metabasian process, we're drawing inferences about an inferential process.

it's a key point and that's the two levels of mapping it's like the map of the map variational applause variational base factorized models for computational simplicity and interpretability based upon the sparsity of coupling of variables in a base graph

and laplace using a parabolic approximator of the mode and a variance to ensure a simple optimizable heuristic not that it perfectly recapitulates the shape of the posterior which doesn't have to be inverted quadratic

but the Laplace approximation is smoothly optimizable for both the mode and the variance and here's where they mentioned that's the parametric empirical Bayesian approach and basically Laplace approximation can be thought of as making a commitment to um the quadratic form

So choosing the family that your prior is going to be of.

And then bootstrapping your initial prior with your observed data.

Bootstrapping not in the resampling sense, but in the starting oneself out sense.

This was explored more in other works.

but you have some likelihood function of all the parameters L of data and optimizing action involves computing beliefs about policy such that policies are proportionally taking up slices of the pie monotonically with respect to their free energy.

and then there's a precision on action parameter where you can turn it all the way down to sort of like don't know don't care where even large differences in free energy between policies get equilibrated out that's like hot decision making and then absolute zero decision making would be a policy that's even 51 more likely would be always selected

So the variational free energy, expected free energy, it's a bound on surprise due to Jensen's inequality and evidence lower bound and all this other work in variational inference.

And then that pi slicing can be tuned up or down with a shaky hand parameter, soft max temperature parameter.

If the soft max is one,

the pie is undistorted if the softmax is large you have a relentless bias for better policies or it could be less than one here's some technical details of the Laplace approximation parametric empirical base generalized linear modeling

Hashtag SPM.

Here's recipe.

So this is kind of like chapter six, but now it's model-based data analysis, collect behavioral data.

Maybe the data has already been collected.

Maybe there's some data sets that we already can use.

Formulate a POMDP.

I wonder if this is as chapter six, not as chapter seven.

It could be chapter seven as well, if it's discrete time.

specify likelihood perhaps we can um in the coming weeks check out spm mdp vbx and they mentioned the scripts data this is nice this will be fun chapter seven i mean it is the correct if they're using this type four chapter seven chapter four

So this looks pretty cool, pretty interesting.

We could walk through some examples.

False inference.

Computational pathology.

So it kind of ends like chapter eight.

a table of references and a summary so it's a pretty short chapter and has some formalisms but it's only from page 179 in the pdf to 196. so it's less than 20 pages


SPEAKER_01:
The next chapter, I think, is the longest chapter in the book.


SPEAKER_00:
Which one is?

Chapter 10.

Chapter 10, 197.

Yeah, that one's over 30 pages.

Yeah.

But it ends on a high note.

All right.

Well, interesting.

conversations thanks for this we will come um back to chapter nine look through it and then peep into ten go through ten and really solidify our understanding of the map and i guess indirectly of the territory and then for those who want to join

we'll probably continue the discussion and especially start looking towards those project ideas again.

Like we talked just a little earlier about making a dot zero sessions for the textbook.


SPEAKER_02:
So I have to apologize.

I'm traveling for a few weeks, so I'm going to miss the next several sessions.


SPEAKER_00:
It's all good.

Thank you, Eric.

Any other comments people want to make or I'll close the recording.

Okay.

Thank you.
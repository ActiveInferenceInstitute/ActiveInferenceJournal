[
  {
    "start": 2.605,
    "end": 31.21,
    "text": " all right hello everyone it is week 11. and we're in chapter five part two of the active textbook group first cohort we'll be now turning to some of the more direct questions on chapter five after having more of a overview last week we'll also um",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 33.527,
    "end": 49.769,
    "text": " be concluding our regular chapter discussions for the first part of this book and in the coming two weeks we'll be having some review and synthesis and connecting some dots",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 51.135,
    "end": 73.553,
    "text": " asking overarching questions, returning to questions that were based in a specific chapter, and hopefully people on their own or wherever else they're seeing as relevant, or requesting new pages if they see something that they want, providing some syntheses on this first half of the textbook, which is the epistemic half.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 73.873,
    "end": 80.619,
    "text": "And then for people who want to continue in the textbook group,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 81.974,
    "end": 105.821,
    "text": " go to the onboarding page and there's two columns there's yes for part two of cohort one chapter six through ten so check that box if you want to be included in starting in september going through chapters six through ten which has like the recipe for active models and that's when we're going to be getting more hands-on with the modeling",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 107.717,
    "end": 126.19,
    "text": " also everyone is welcome to join for part one chapters one through five cohort two also starting in september so it could be um a fun experience just to get another coat of paint on the material",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 128.114,
    "end": 134.017,
    "text": " play a more active role, ask another set of questions informed by what you've seen in the first cohort.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 134.477,
    "end": 137.138,
    "text": "So feel free to check those boxes.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 138.038,
    "end": 138.539,
    "text": "And then also,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 140.814,
    "end": 166.524,
    "text": " keep incubating as we head into chapter um six and beyond in the second half of the textbook um like the project ideas several um of the project ideas like relate to what mike and noah have added we've started to address and explore in the active blockference meetings on wednesday",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 167.423,
    "end": 175.706,
    "text": " especially related to like cyber physical systems, simulations, taxonomies for governance, these kinds of ideas.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 177.246,
    "end": 185.349,
    "text": "Brock's added some great notes about like what is the ramp that gets someone to the low road or the high road?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 186.87,
    "end": 193.972,
    "text": "And then I've recorded the first chapter and the first segments as an audio book",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 195.746,
    "end": 214.773,
    "text": " first chapter doesn't have any equations so it's straightforward to read but the subsequent chapters do have equations which is um motivating having the natural language representations of the equations",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 215.72,
    "end": 241.675,
    "text": " there's still a ton of um discourse questions that can be asked why things are one way why things are a different way if it can be placed into the equation itself that's great if it needs to be somewhere else hopefully the space exists and people can request if not but um for example when reading the textbook the equations can be read this way so those are just some of the um projects that",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 242.506,
    "end": 264.602,
    "text": " people can look forward to and the affordances to continue on which is again going to the onboarding and participating in the first five chapters again cohort two or going to continue on with chapters six through ten starting in september for both of these so that we can like complete",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 265.342,
    "end": 292.215,
    "text": " one pass of the book by the end of the year and also probably make a lot of progress on different project ideas and project ideas arising as we start to see and build templates for the models and also like ali has some nice interactive notebooks so there will be a lot of great things um coming together if anyone wants to raise their hand of course just go for it",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 294.557,
    "end": 321.091,
    "text": " otherwise we will be continuing with the questions asked about chapter five before we go to the questions is there any general comment that somebody wants to add about chapter five their reading of it their re-reading of it listening to the video from last week how is chapter five um",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 323.652,
    "end": 347.922,
    "text": " hitting them today okay um to kind of maybe bring that one step closer to a specific question uh i'm not going to",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 349.201,
    "end": 371.779,
    "text": " read this whole quote but this question says in the preface they wrote in chapter 5 we will move from formal treatments to biological implications of active inference this aids in mapping the abstract computational principles of active inference to specific neural computations that can be executed by physiological substrates",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 373.715,
    "end": 379.638,
    "text": " This is important in forming hypotheses under this framework and ensures that these are answerable to measured data.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 380.379,
    "end": 384.161,
    "text": "In other words, Chapter 5 sets out the process theory associated with active inference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 386.122,
    "end": 388.924,
    "text": "Is this how people saw Chapter 5?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 390.216,
    "end": 393.418,
    "text": " Do they feel like it did something different?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 393.718,
    "end": 405.827,
    "text": "Did it succeed at making the abstract computational principles linked or mapped to specific neural computations executed by physiological substrates?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 424.707,
    "end": 425.187,
    "text": " Yes, Eric?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 426.348,
    "end": 427.828,
    "text": "You know, first of all, I have to apologize.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 427.848,
    "end": 432.19,
    "text": "I only watched half of last week's discussion.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 433.01,
    "end": 438.232,
    "text": "So I may be talking about ground that's been covered before.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 438.252,
    "end": 444.255,
    "text": "It just seems to me that, you know, I've seen many, many...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 447.671,
    "end": 455.12,
    "text": " papers trying to map theories onto brain circuitry and physiology.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 456.262,
    "end": 459.125,
    "text": "And it's I'm not well enough.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 459.145,
    "end": 463.671,
    "text": "You know, I spent I'm only loosely associated with neuroscience and, you know,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 464.953,
    "end": 465.954,
    "text": " don't follow it very closely.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 466.014,
    "end": 469.437,
    "text": "So it's hard for me to cite specific instances along the way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 469.497,
    "end": 473.822,
    "text": "I know you mentioned Numenta, which is, you know, one of the more recent ones.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 473.882,
    "end": 477.566,
    "text": "It's trying to say this is what the layers are doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 477.586,
    "end": 477.626,
    "text": "So",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 481.087,
    "end": 488.769,
    "text": " This chapter, to me, didn't seem any very qualitatively different from anybody else's.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 489.529,
    "end": 501.553,
    "text": "Very hand-wavy way to say, here's what the computations are all about, and here's how they map to what brain circuitry could map to what brain circuitry might be doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 502.253,
    "end": 508.835,
    "text": "It's all quite hand-wavy, I find, and I guess plausible.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 511.084,
    "end": 513.846,
    "text": " It's certainly not definitive, nothing close to that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 518.369,
    "end": 534.922,
    "text": "Maybe one other comment I would say is, again, in this chapter, they use this word prediction in a way that conforms to the theory, but to say that what's happening in",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 537.051,
    "end": 548.56,
    "text": " say motor control, where top-down signals from the cortex go down to the ganglia in the spinal cord in order to invoke motor patterns.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 550.782,
    "end": 560.348,
    "text": "To call that a prediction is to me a kind of, well, that's taking extreme measures to map the concepts of the theory onto",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 561.59,
    "end": 562.571,
    "text": " what neurons are doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 563.613,
    "end": 573.668,
    "text": "And if the theory works out to be very useful in some way, and that's what you have to do to make that happen,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 574.872,
    "end": 592.185,
    "text": " Okay, I guess I can use that language, but a priori, it just does not seem like the right language to use to say, yeah, we're trying to set up some motor patterns, set up some context and situations for what we want the local reflexes to be doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 593.846,
    "end": 597.108,
    "text": "But to cast that as a prediction just seems a little unnatural to me.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 598.029,
    "end": 600.571,
    "text": "So those are my two reactions to this chapter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 602.012,
    "end": 602.272,
    "text": "Thank you.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 605.993,
    "end": 607.453,
    "text": " Anyone else with a raised hand?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 613.755,
    "end": 634.922,
    "text": "This is a really good point about what the linkage or mapping is between, without going into like, is anything physical or et cetera, like physical brains in their niche and any type of equations, would a well-fitting linear model say that the brain is a linear model?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 635.95,
    "end": 640.032,
    "text": " Does a well-fitting Bayes graph imply that the brain is a Bayes graph?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 641.253,
    "end": 652.058,
    "text": "Does a well-fitting Bayes graph plus a philosophical belief that we're just making a map, we're not describing the territory, equate to what beyond a linear model or a verbal model?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 652.599,
    "end": 652.859,
    "text": "Rohan?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 655.44,
    "end": 657.381,
    "text": "Yeah, building on the previous point,",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 659.361,
    "end": 665.123,
    "text": " I'm still unsure about what active inference is considering a signal.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 665.183,
    "end": 669.284,
    "text": "There's no effective, well, there's no noise model.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 669.485,
    "end": 677.467,
    "text": "It doesn't specify that here's, because if you say that I'm predicting something, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 677.507,
    "end": 684.93,
    "text": "So there must be something that is substantially different, statistically different from some noise distribution.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 686.243,
    "end": 687.764,
    "text": " that you can differentiate, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 688.444,
    "end": 696.828,
    "text": "So it's basically trying to estimate some hidden probabilities, which is not really signal.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 698.568,
    "end": 699.289,
    "text": "Does that make sense?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 699.549,
    "end": 703.691,
    "text": "Because even the internal states, unless they're actually",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 705.153,
    "end": 712.237,
    "text": " instantiated prior, like when we set up the model, we say, okay, these are the only states that you can actually traverse.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 713.678,
    "end": 720.302,
    "text": "It does not make, you still have to infer the states from your measurements of reality.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 724.244,
    "end": 724.584,
    "text": "Thank you.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 730.928,
    "end": 731.208,
    "text": "Jakob?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 732.199,
    "end": 743.044,
    "text": " Yeah, I just want to ask if the noise isn't kind of implied in the POMDP formalism.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 744.665,
    "end": 760.272,
    "text": "Like, I think in figure 5.4, where it shows the direct and invert pathways of message passing, it describes actinth and the basic ganglia",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 761.388,
    "end": 763.75,
    "text": " as a viewMDP generative model.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 764.411,
    "end": 768.835,
    "text": "So if it's partially observable, doesn't that imply some kind of noise?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 772.678,
    "end": 781.005,
    "text": "My counter to this would be there is missing information that needs to be filled out that's not really noise, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 781.586,
    "end": 784.328,
    "text": "So noise would be something like if we",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 788.478,
    "end": 792.32,
    "text": " So even if we, let's not use this, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 792.36,
    "end": 795.903,
    "text": "So we have like returns of a stock.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 796.994,
    "end": 801.917,
    "text": " And looking at the distribution of the prior returns, we know what the spread is.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 802.117,
    "end": 813.643,
    "text": "And in case we want to measure whether your portfolio is substantially different from this distribution, that's where we would have some sort of like sharp ratio or a signal to noise ratio.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 814.223,
    "end": 821.427,
    "text": "Noise ratio being the complete set of distributions that are possible, the set of ranges that are possible.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 823.841,
    "end": 827.945,
    "text": " That is not really here, because that is missing information.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 827.965,
    "end": 839.374,
    "text": "So we can only observe part... The fact that we have only partial observations does not excuse the fact that we don't have even noise models of the sensors.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 839.454,
    "end": 843.297,
    "text": "So how good are the sensors, for example?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 843.697,
    "end": 845.579,
    "text": "Do we have some idea of that?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 847.68,
    "end": 851.263,
    "text": "Yes, Jakub, but then I'll add a point about these Bayesian terminology.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 852.244,
    "end": 852.504,
    "text": "Yeah.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 859.733,
    "end": 881.657,
    "text": " What's the practical difference between partial observation, where the noise implies some kind of information that is missing, versus a distribution that's added to full observation that adds noise to the sensory processing?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 887.839,
    "end": 898.815,
    "text": " So if I use an actual physical sensor, it would be something like what are the ranges of values that it can go up to because we've designed the sensor.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 899.758,
    "end": 905.503,
    "text": " So we basically know what the bias is, what the variance of the sensor is under different conditions.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 908.085,
    "end": 911.508,
    "text": "There are actual physical processes implied in that measurement.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 911.888,
    "end": 922.197,
    "text": "So in the case of an ultrasonic sensor, it's probably like some current value or some voltage value that's being measured by some circuitry somewhere.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 922.757,
    "end": 923.998,
    "text": "So there are ranges for that.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 925.02,
    "end": 927.302,
    "text": " So there's no missing information there.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 928.143,
    "end": 929.864,
    "text": "So you basically know.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 930.885,
    "end": 933.407,
    "text": "So it's already a prior in this system.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 933.607,
    "end": 935.609,
    "text": "So you know what the ranges of values are going to be.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 936.049,
    "end": 939.973,
    "text": "And from there, you need to infer some state in the world.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 940.033,
    "end": 942.455,
    "text": "So is there an obstacle in the way, or is there?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 942.475,
    "end": 943.375,
    "text": "Yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 946.398,
    "end": 951.762,
    "text": "OK, let me just make a comment about some Bayesian terminology, then Jessica and Brock.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 952.628,
    "end": 954.449,
    "text": " So let's look at figure 4.3.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 955.59,
    "end": 971.48,
    "text": "This is the partially observable Markov decision process that Jakob talked about, but we're gonna be just talking about the perceptive part, not the action intervention into how things are changing, yet just focusing on this motif on the left, like step-by-step guide uses.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 972.321,
    "end": 976.143,
    "text": "So you mentioned about the bias related to measurements.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 976.343,
    "end": 981.507,
    "text": "So if the measurables are the observable sense states,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 982.406,
    "end": 1008.167,
    "text": " then the bias and the covariance and the variance about how like the reading on the thermometer relates to the underlying variable the the hidden state the temperature of the room it's embodied in the a matrix because that is actually the learned or fixed variable that is playing a role of a noise model",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1009.403,
    "end": 1019.93,
    "text": " Then you mentioned when talking about returns on stocks, testing whether there's like a substantial difference from some null expectation, like is there a signal that's different from the noise?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1021.207,
    "end": 1048.085,
    "text": " um this is like a frequentist approach it's based upon determining whether there's some statistically significant level doesn't have to be it's not the only way to do it um but one can do like a p-value on whether a given signal is statistically different from given a noise model different from noise um and then like the sharp ratio and other types of things are like summary statistics they're like descriptive statistics um",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1050.766,
    "end": 1068.217,
    "text": " noisy measurements are addressed in the context of the POMDP by having an A matrix that accommodates some of the features of denoising a signal, and then precision variables.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1069.859,
    "end": 1075.222,
    "text": "And those precisions can, again, be learnt or fixed, but the functional...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1078.803,
    "end": 1092.487,
    "text": " aspects of what is being referred to as noisy measurement are accommodated within this kind of Bayesian approach, which is why Kalman filtering, generalized Bayesian filtering, why these techniques are used.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1093.167,
    "end": 1100.93,
    "text": "Because we can have a Bayesian update scheme implemented with variational approaches, message passing, sampling, all these things.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1102.205,
    "end": 1108.49,
    "text": " that does denoise signal, and it does embody a noise model and a signal model.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1109.111,
    "end": 1113.034,
    "text": "And that's all part of the generative model of the cognitive entity.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1114.055,
    "end": 1120.54,
    "text": "So it just isn't being addressed from a frequentist statistics or a summary statistics way.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1121.32,
    "end": 1123.682,
    "text": "Yeah, I shouldn't have brought up the Sharpe ratio.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1123.722,
    "end": 1124.963,
    "text": "Maybe that's not the best.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1125.424,
    "end": 1127.005,
    "text": "But the Sharpe ratio is...",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1128.743,
    "end": 1133.446,
    "text": " is effectively some way to say that some portfolio choice is different from others.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1133.987,
    "end": 1138.55,
    "text": "Or in communication systems, we would use something like EB over N0.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1139.09,
    "end": 1144.594,
    "text": "That's the signal-to-noise ratio for any sort of communication system.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1145.791,
    "end": 1147.531,
    "text": " So yeah, yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1147.971,
    "end": 1151.332,
    "text": "But they're not necessarily only frequentist, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1151.352,
    "end": 1155.073,
    "text": "So there are patient ways to estimate these things.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1155.113,
    "end": 1163.095,
    "text": "In fact, that's pretty much how we do it in practice, is we don't assume that we know the complete distribution.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1163.115,
    "end": 1165.615,
    "text": "We update priors in each of these cases.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1165.915,
    "end": 1167.656,
    "text": "Because we have, yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1170.385,
    "end": 1181.928,
    "text": " Coming back to your point that the A matrix does address this, the A matrix would be something that you said it would denoise and filter out some stray observations.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1182.748,
    "end": 1193.07,
    "text": "But okay, so the question still remains is when you're saying prediction, is it prediction of the signal over here?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1194.517,
    "end": 1218.533,
    "text": " as in what is the future state of the sensor but i think there it seems seem to indicate that the prediction relates to that relates to the free energy minimization itself right so you choose policies that would minimize your free energy and keep yourself into some set point so which means that there should be some sort of uh",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1219.798,
    "end": 1230.184,
    "text": " an idea of what ranges are there for this value, or how far can these go before you're not in that set point anymore.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1231.145,
    "end": 1235.548,
    "text": "So even if you do want to make that prediction, it's not just the signal value.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1235.588,
    "end": 1240.571,
    "text": "So signal values, effectively, through the Markov blanket, they affect some internal state.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1241.431,
    "end": 1247.695,
    "text": "And I'm assuming that's what we're trying to predict, that the trajectory of the future internal states",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1250.153,
    "end": 1257.096,
    "text": " the expectation that's being minimized is over outcome observations, not about internal states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1258.377,
    "end": 1258.837,
    "text": "Oh, okay.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1258.857,
    "end": 1260.258,
    "text": "Yeah, I'm sorry.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1261.879,
    "end": 1277.646,
    "text": "So even then, we don't have... So the signals may have well-defined precision and bias, but the entire system itself doesn't have any sort of... Well, it doesn't know how wrong it can go.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1279.096,
    "end": 1280.498,
    "text": " It doesn't have any error estimate.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1281.159,
    "end": 1283.562,
    "text": "So there's no noise model as such.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1283.702,
    "end": 1285.164,
    "text": "So we can't apply any.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1285.945,
    "end": 1298.0,
    "text": "So we would have to assume that there is some distribution of states and then use something like least squares or some other optimization routine to actually bring this in practice.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1300.282,
    "end": 1301.402,
    "text": " Great, totally possible.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1302.643,
    "end": 1310.745,
    "text": "It would be interesting to sketch out a Bayes graph for what you believe a signal-to-noise or some type of noise model would embody.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1311.785,
    "end": 1315.566,
    "text": "And maybe we will see some analogies or some mappings.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1316.386,
    "end": 1317.487,
    "text": "Jessica, and then Brock.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1320.908,
    "end": 1328.53,
    "text": "I think someone, like what I was going to say, probably already been covered by both you and Rohan in the new comments.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1329.926,
    "end": 1344.099,
    "text": " I was going to say that, um, with the, like the filtering and sort of like a lot of like the biases and things that like, that kind of like made sense of it is like, I see the, cause I think it was like chapter two or something like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1344.119,
    "end": 1344.679,
    "text": "I said like, right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1344.699,
    "end": 1350.624,
    "text": "Like, so the why is this, you know, quote unquote, um,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1351.821,
    "end": 1376.5,
    "text": " know uh like a fat thing like you know that we can observe from the hidden state or like the process of something that we don't know and so for everybody like we can see the same thing but how we process it internally is going to vary like it's going to be very subjective so the why is going to look different for daniel then it's going to look for me based on all this filtering",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1377.33,
    "end": 1388.957,
    "text": " And so the blanket states, I imagine sort of like what have like those filters that encode the biases and other things that allow us to process things differently.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1390.078,
    "end": 1392.059,
    "text": "And so like then I generate a model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1393.3,
    "end": 1395.681,
    "text": " And it's going to be updated differently.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1396.442,
    "end": 1414.011,
    "text": "Also encoded, I think, in the general model will be a lot of heuristic and things like that that embody, I think, probably a lot of biases and things of how we understand things, which also influence how we interpret what we observe.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1415.38,
    "end": 1418.921,
    "text": " And so for me, it's like a lot of these things are there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1418.981,
    "end": 1431.563,
    "text": "In terms of statistics, I don't know much, but that's how conceptually I understood a lot of these things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1431.863,
    "end": 1437.705,
    "text": "So basically, in the blanket states of the Marco Blanket, it would have a lot of this filtering.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1440.049,
    "end": 1452.403,
    "text": " Yes, like if you're going to try to, you know, let's say like you observe something that's different than what you predicted in your model, then you would...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1453.41,
    "end": 1459.511,
    "text": " take action and to forage and like find information and to figure it out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1459.611,
    "end": 1460.011,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1460.031,
    "end": 1462.092,
    "text": "Like, what am I missing?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1462.392,
    "end": 1467.913,
    "text": "Like, why did I make this mistake or like this error in my prediction?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1467.933,
    "end": 1474.054,
    "text": "And so that I can like have more information and update the model so that it aligns more closely.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1474.114,
    "end": 1480.096,
    "text": "So then you will basically take action to the noise and basically get closer to like the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1482.396,
    "end": 1484.737,
    "text": " quote unquote, the reality that's out there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1485.978,
    "end": 1490.78,
    "text": "Yeah, I don't know if it's official, I think, but that's how it kind of makes sense to me.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1491.7,
    "end": 1492.341,
    "text": "Thanks, Jessica.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1492.621,
    "end": 1492.921,
    "text": "Brock?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1496.463,
    "end": 1508.068,
    "text": "Yeah, I just wanted to, I guess, speak to this, like, filter noise model, I don't, like,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1508.883,
    "end": 1531.628,
    "text": " in practice, in reality, like when we're talking about sensors, like in a physical system, like, like a digital signal processing kind of context, like the sensors are tested under some conditions for things like, you know, jitter and spurious kind of noise conditions and stuff like that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1531.688,
    "end": 1534.049,
    "text": "But they're tested under some conditions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1535.967,
    "end": 1544.194,
    "text": " meaning that there's never ever going to be a time when you have an actual model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1545.415,
    "end": 1557.185,
    "text": "You're just gonna have an approximation, whether that's expressed in a frequentist or Bayesian way, like there's going to be some uncertainty in the model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1558.406,
    "end": 1563.35,
    "text": "And especially if you start connecting it up to a larger system,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1565.069,
    "end": 1587.62,
    "text": " doing the same kind of like is the jitter in that sensor the same when you know when it's completely isolated as it is when it's on a you know six by nine pcb board with you know um 12 volts on one side and a oscillator next to that that's you know like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1589.006,
    "end": 1590.948,
    "text": " Are those now, is it the same thing?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1590.968,
    "end": 1609.661,
    "text": "You know, you have to do these sort of electromagnetic compatibility, you know, resonance sort of testing on it, like, there's just that it's, it's, in practice, it's Bayesian, like you're doing the same sort of bounding and, you know,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1610.543,
    "end": 1617.905,
    "text": " belief, testing and, you know, finding your errors and correcting them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1618.865,
    "end": 1619.726,
    "text": "Even in practice.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1622.847,
    "end": 1637.171,
    "text": "I don't see how you could possibly have like even like physically a system like that could exist where you had a noise model that you could write down that was actually the noise model under all conditions or",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1639.95,
    "end": 1651.138,
    "text": " Yeah, it doesn't seem like a approachable, I don't know, it's not even not tractable, which is the wrong thing to do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1651.378,
    "end": 1651.739,
    "text": "I don't know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1653.72,
    "end": 1654.16,
    "text": "Thanks, Brock.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1656.482,
    "end": 1665.889,
    "text": "In the case of engineering and designing models, it's up to the designer or the engineering team to understand what is adequate or not.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1666.089,
    "end": 1667.41,
    "text": "And for natural systems,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1668.723,
    "end": 1672.344,
    "text": " outside of their bounds, natural selection sweeps them off the table.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1673.164,
    "end": 1696.672,
    "text": "So some of the adequacy questions are either based upon specified or implicit human standards, or the failure to resist dissipation, and therefore the failure to realize repeated measurement of oneself or from the external, and then that system is no longer",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1697.995,
    "end": 1700.616,
    "text": " Let's just continue to move through some of the questions.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1702.157,
    "end": 1702.898,
    "text": "Yes, Rohan, go for it.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1705.579,
    "end": 1709.441,
    "text": "Yeah, I completely agree with what Brock said, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1709.541,
    "end": 1721.768,
    "text": "But my point was when you say that you're predicting something, it means that you either know the quantity that you're trying to predict,",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1723.466,
    "end": 1735.475,
    "text": " And, or, and it corresponds to, so in this case, like some action results in some reward, I guess we could predict the reward, but without knowing how wrong you're going to be in the future.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1736.395,
    "end": 1744.321,
    "text": "So very exactly is that any living system would have to have be aware of its own bounds, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1744.361,
    "end": 1747.723,
    "text": "That's, that's what I meant by noise model.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1747.783,
    "end": 1752.647,
    "text": "So it has to know that it has to stay within some bounds and as it starts coming.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1753.254,
    "end": 1761.98,
    "text": " closer to one of the upper or lower bounds, there should be something like pulling it back down or some feedback mechanism.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1762.56,
    "end": 1771.326,
    "text": "But that is not very clear from what I've read so far on active inference as to how exactly it would estimate that.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1772.326,
    "end": 1774.628,
    "text": "You can just say free energy minimization",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1776.169,
    "end": 1778.533,
    "text": " But what is a minimum in this case?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1778.754,
    "end": 1780.537,
    "text": "That would be different for different systems.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1780.938,
    "end": 1785.306,
    "text": "It should have some idea of where the surface is or which of the local surfaces it's lying on.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1785.586,
    "end": 1786.268,
    "text": "That was my point.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1787.642,
    "end": 1789.323,
    "text": " So for sure, it's different for different systems.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1789.524,
    "end": 1792.326,
    "text": "It's different as every single generative model is.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1793.807,
    "end": 1796.929,
    "text": "And then you mentioned like a pullback attractor.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1797.65,
    "end": 1802.314,
    "text": "And so this has been treated extensively in the context of physiological measurements.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1802.914,
    "end": 1813.743,
    "text": "And then again, where those priors on the physiological measurements are, the tolerable range are either provided by the human engineering team or through evolution by natural selection.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1814.504,
    "end": 1816.185,
    "text": "And then, yeah.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1817.287,
    "end": 1843.117,
    "text": " the quantity like another related topic would be expectation maximization models which is what is happening essentially in the dialectic between the observation and the hidden state update is an expectation maximization like process where it's like given the hyper priors or we can just simplify just given the priors on s what are the most likely or the distribution of observables",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1843.973,
    "end": 1849.756,
    "text": " And then given the observables, what should be updated about the hidden state priors?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1850.557,
    "end": 1853.578,
    "text": "And when those are at convergence, the model is stationary.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1853.918,
    "end": 1867.406,
    "text": "And then as things change, either from the top via learning or context shift or through changed measurements, the expectation maximization algorithm is just able to track those changes.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1867.746,
    "end": 1872.769,
    "text": "And that can, in the context of a fixed prior, have the function of a pullback attractor.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1874.887,
    "end": 1878.814,
    "text": " Um, and that's even before getting into action specifically, but yes,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1880.086,
    "end": 1906.641,
    "text": " definitely considering like the role of comparing future plans of action and which plans of actions will have the least expected free energy based upon their um pragmatic and epistemic value like which the epistemic just being clarity around how it's going to be achieved and the pragmatic value being that kl divergence between expectation preferences and the expected observations",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1907.501,
    "end": 1919.81,
    "text": " So if we expect slash prefer to be in homeostasis, the pragmatic value is going to come or be loaded onto policies that keep us in homeostasis, even if some other one is super informative.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1920.87,
    "end": 1925.894,
    "text": "How that model gets tuned is quite literally the details of how the model is trained.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1926.454,
    "end": 1927.175,
    "text": " Right.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1927.255,
    "end": 1929.997,
    "text": "My point was that active inference doesn't help us find this.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1930.037,
    "end": 1931.478,
    "text": "We have to find this ourselves.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1931.719,
    "end": 1938.284,
    "text": "And then hopefully, once we have instantiated these bounds, I think active inference works very well to keep everything this.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1939.535,
    "end": 1951.3,
    "text": " If you can't discover this de novo, like I can't have like what you would say tabula rasa type system, like a blank slate system, go out in the world and discover your bounds.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1952.08,
    "end": 1957.022,
    "text": "That's not going to happen with this kind of method is what I was making it out.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1957.463,
    "end": 1964.586,
    "text": "Whereas taking something equivalent without any... So deep learning is another...",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1966.276,
    "end": 1966.897,
    "text": " method, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1966.917,
    "end": 1968.437,
    "text": "So you just feed it a lot of data.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1968.557,
    "end": 1970.178,
    "text": "It doesn't have to know anything about the data.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1970.238,
    "end": 1977.262,
    "text": "Eventually it forms some opinion about the data it's seen, and then it's able to do some classification or depending on how much data it's seen.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1977.702,
    "end": 1980.143,
    "text": "So it's able to build certain amount of bounds.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1980.803,
    "end": 1992.269,
    "text": "Whether that's generalizable is a different question, but here there's the learning part does not say much about how exactly it goes about discovering",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1994.045,
    "end": 1995.206,
    "text": " where its limits are.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1995.226,
    "end": 1995.867,
    "text": "That's my point.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1996.668,
    "end": 1996.888,
    "text": "Sure.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1997.188,
    "end": 1997.509,
    "text": "Thank you.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1998.27,
    "end": 1998.51,
    "text": "Eric?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2001.193,
    "end": 2012.064,
    "text": "You know, I would love to dive just a little bit more into the thing you were just talking about, how expectation maximization is a mathematical",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2014.841,
    "end": 2030.996,
    "text": " formulation for how to infer belief states from observations because you basically go through this iterative process of trying to find alignment between the states of the belief and the observables.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2031.136,
    "end": 2037.422,
    "text": "And then you can track that over time between that concept and the concept of message passing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2038.323,
    "end": 2057.919,
    "text": " When I think of message passing, I think of some sort of a distributed system where you have some local computation, and you have, again, either synchronous or asynchronous process for communicating between these different centers of belief state.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2058.78,
    "end": 2063.784,
    "text": "And then, again, through message passing, you have often an iterative process of convergence.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2064.725,
    "end": 2089.623,
    "text": " so um those are not the same thing and i think you know message passing to my understanding would be an em algorithm under certain types of messages and states at each of the nodes that you're sending the messages around but not all message passing is going to be an em algorithm and similarly when i think of em i don't think about message passing i think about making an arrays of you know of um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2090.99,
    "end": 2104.958,
    "text": " belief and matrices, mapping between belief and observations, and then trying to compute an expectation for what my predictions would be, and then making this iterative convergence.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2104.978,
    "end": 2106.539,
    "text": "So I don't think of that in terms of message passing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2106.9,
    "end": 2112.183,
    "text": "So I'd just love to hear other people's ideas about how to tie these two ideas together.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2113.924,
    "end": 2114.744,
    "text": "Yeah.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2115.225,
    "end": 2118.407,
    "text": "Brock, your hand is still raised?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2120.413,
    "end": 2120.593,
    "text": " Okay.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2121.053,
    "end": 2121.754,
    "text": "Nope, sorry.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2122.254,
    "end": 2122.934,
    "text": "Yeah, all good, all good.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2127.037,
    "end": 2128.638,
    "text": "Does anyone have thoughts on this?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2130.679,
    "end": 2135.842,
    "text": "There's a lot to say about the expectation maximization algorithms.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2136.582,
    "end": 2141.745,
    "text": "And just to kind of give a little context, this is the step-by-step paper.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2142.918,
    "end": 2147.246,
    "text": " So this is the model stream one, it's four parts.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2149.03,
    "end": 2152.877,
    "text": "And this is like really a quite relevant figure, which is not in the textbook.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2153.683,
    "end": 2154.464,
    "text": " but helps a lot.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2155.084,
    "end": 2160.769,
    "text": "Like this is the essence of the partially observable Bayesian inference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2161.09,
    "end": 2179.386,
    "text": "There's a prior D and then there's an A ambiguity matrix that is mapping in a generative capacity between hidden states S and observation measurements O. This is the tale of two densities because O through A can give you the most likely S.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2180.546,
    "end": 2191.833,
    "text": " And S through A can give you the most likely O. And it turns out that by alternating those procedures in a Bayesian update context,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2193.368,
    "end": 2196.489,
    "text": " That two-stroke engine is called expectation maximization.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2197.03,
    "end": 2204.933,
    "text": "Again, because given expectations on the summary statistics of a distribution, the expected sensory observations can be generated.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2205.573,
    "end": 2214.136,
    "text": "And then given those observations, a likelihood function can be maximized that then updates the hidden states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2214.737,
    "end": 2222.62,
    "text": "So this motif, this kind of elbow motif is then extended into a caterpillar with this B",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2223.902,
    "end": 2225.864,
    "text": " which is how the hidden state changes through time.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2226.685,
    "end": 2232.13,
    "text": "So importantly, like note what's not there, which is like the observations being chained through time.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2233.091,
    "end": 2236.995,
    "text": "So it's not that the temperature reading at one moment influences the temperature reading at the next moment.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2237.496,
    "end": 2246.685,
    "text": "The temperature readings are continually linked synchronously, like in one time slice to their hidden states.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2247.577,
    "end": 2270.971,
    "text": " so then when action gets into the picture we've talked about like several sources that's like the equation 2.5 to equation 2.6 phase change because action brings in several kinds of uncertainty first off you're reducing your uncertainty about observations which haven't happened yet you're also having the unknown consequences of your actions",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2272.111,
    "end": 2284.382,
    "text": " And in order to have action selection that's relevant, one has to have a preference distribution over what kinds of observations they would like to be seeing so that action can guide it in that direction.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2285.924,
    "end": 2287.105,
    "text": "Okay.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2287.905,
    "end": 2296.193,
    "text": "Then, so that's the expectation maximization two-stroke engine is basically graphically",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2298.319,
    "end": 2317.024,
    "text": " Daniel Katz- proposed here D is just the initiating set of parameters and then in a one shot way like you can have a folder with 1000 images and do expectation maximization on it, or you could have something that's dynamical and do that type of expectation maximization through time um.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2321.604,
    "end": 2327.227,
    "text": " Eric asked, how is it related to message passing on graphs?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2327.867,
    "end": 2344.895,
    "text": "So I'm not familiar with all the details of this model, but you basically said it, which was that like certain message passing systems implement EM, but of course not all message passing architectures implement EM.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2345.115,
    "end": 2348.657,
    "text": "So message passing is more general than EM, also EM,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2350.316,
    "end": 2353.678,
    "text": " wouldn't have to be implemented through message passing.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2353.958,
    "end": 2368.088,
    "text": "But there's like an area of intersection where the EM algorithm can be seen as a type of message passing under certain compute rules in a factor graph.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2369.469,
    "end": 2373.571,
    "text": "And EM, at least from here, may be used to break cycles in a factor graph.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2376.073,
    "end": 2376.934,
    "text": "So it's always like...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2377.93,
    "end": 2398.455,
    "text": " how close to the kernel and like the platonic ideal of active inference are we talking and then how many heuristics and just ways of connecting things are possible so i i i will now read the chat",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2406.943,
    "end": 2408.164,
    "text": " That's from a previous context.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2408.605,
    "end": 2411.087,
    "text": "Okay, okay, thank you.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2413.009,
    "end": 2430.265,
    "text": "Like, the Statistical Parametric Mapping textbook and documentation SPM has a lot on EM, variational inference, and a lot of the parts that bring one to understand this",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2432.307,
    "end": 2453.315,
    "text": " a lot better spm is like almost like it doesn't include action that much because it's a neuroimaging so the observations are neuroimaging sensor fusion different error modalities like the whole panoply and then spm started to incorporate participant actions",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2454.753,
    "end": 2478.494,
    "text": " in this partially observable meta bayesian way and potentially that is what led friston and colleagues towards a grander synthesis of inference and action under a statistically principled framework let's just see if there's any other these are all you know",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2479.878,
    "end": 2498.755,
    "text": " important questions and like there's a lot to get up to them and then there's like a lot to go from the question but these are all important things to raise okay um if anyone has a thought on this how should we think about redundancy in neural systems in the context of active inference",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2499.895,
    "end": 2509.341,
    "text": " So redundancy would be like if there's something that's playing a functional role, but its removal is not damaging the function of the system.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2510.121,
    "end": 2514.163,
    "text": "There's 10 pillars, knocking out one of them, the building stays up.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2514.984,
    "end": 2516.165,
    "text": "So there's redundancy in the pillars.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2517.205,
    "end": 2519.987,
    "text": "What about the cases where the same neural circuits serve multiple functions?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2522.202,
    "end": 2530.785,
    "text": " So how do we deal with the fact that there's like a many to many potentially, or a complex mapping between system elements and system functions?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2532.466,
    "end": 2540.669,
    "text": "Where sometimes removing node one does nothing, but removing node one and two does a lot of things, but then removing one, two, and three, and it's going back to being fine.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2541.269,
    "end": 2541.569,
    "text": "Jessica?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2545.239,
    "end": 2551.461,
    "text": " I guess my comment is more like a question and maybe, like, ask Blue about it because this is what I feel.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2551.501,
    "end": 2565.026,
    "text": "But this question made me think about, like, it's not only, like, that the brain has, like, this redundancy of things, but it, like, would... Like, it would say, like, if you have some kind of damage to your brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2565.106,
    "end": 2568.447,
    "text": "Like, another part of your brain that maybe was not...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2569.68,
    "end": 2591.677,
    "text": " use for something would develop that capacity and so it would create that you know ability and so neuroplasticity like brand new even though it was not a redundancy to begin with so i guess i'm more curious to understand this and so other people who know more like blue and stuff um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2592.722,
    "end": 2614.543,
    "text": " about this um basically that's kind of what this question made me think about and kind of wanting to understand more nice good question yeah blue wrote depends on your age yeah like different neuroplasticity mechanisms are differentially available throughout life for different organisms um",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2615.612,
    "end": 2622.618,
    "text": " Just one thought on this would be like, the same neural circuit serving multiple functions might be totally the case.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2622.898,
    "end": 2630.664,
    "text": "For example, is the function of a heart to provide one pound of weight to the torso?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2631.344,
    "end": 2633.106,
    "text": "It is a function of the heart.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2634.047,
    "end": 2638.89,
    "text": "So identifying what function is being modeled is what's being done here.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2639.091,
    "end": 2640.872,
    "text": "So let's just go to the example of figure 5-3.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2640.892,
    "end": 2641.132,
    "text": "This neuron...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2646.59,
    "end": 2673.16,
    "text": " modifying it like a loss of function experiment or having it injured might influence more things than just the lower motor neuron descending message so that'd be a case where like it serves multiple functions um this is just one statistical model of this function of this circuit",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2674.184,
    "end": 2685.807,
    "text": " And that returns to the earliest comments of Eric with like, okay, this is like tantalizingly seeming like it's actually going to be describing the neuroanatomy.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2686.608,
    "end": 2690.809,
    "text": "But then they say things like, note the absence of dot dot dot dot dot dot dot.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2692.189,
    "end": 2698.051,
    "text": "Note that there's a discrepancy between, for example, the reality of the anatomy and the Bayes graph.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2699.892,
    "end": 2704.555,
    "text": " This highlights that the connections implied by message passing schemes may not manifest as single synapses.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2708.098,
    "end": 2713.002,
    "text": "So, okay, there's kind of false positives and false negatives.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2713.762,
    "end": 2715.884,
    "text": "The Bayes graph isn't just the anatomy.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2717.045,
    "end": 2717.525,
    "text": "That's fine.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2718.886,
    "end": 2722.429,
    "text": "The linear regression between height and weight isn't the actual relationship between height and weight.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2723.269,
    "end": 2725.331,
    "text": "And the structural equation model of...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2726.656,
    "end": 2730.117,
    "text": " Inequality is not the generator of inequality.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2730.997,
    "end": 2744.041,
    "text": "So it's totally fair and shouldn't be expected to be otherwise that the Bayes graph, the best fitting Bayes graph, the most didactic Bayes graph, the simplest Bayes graph, none of those recapitulate the anatomy.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2744.601,
    "end": 2749.842,
    "text": "And the Bayes graph that recapitulates the anatomy would not necessarily even be the best fitting.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2751.243,
    "end": 2752.303,
    "text": "It's map and territory.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2753.308,
    "end": 2760.894,
    "text": " And if somebody has a special equation to break through that blanket, everybody would love to see it.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2761.935,
    "end": 2774.065,
    "text": "However, sometimes it's easy to gloss over that in principle challenge of mapping formalisms to biological systems.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2774.745,
    "end": 2778.128,
    "text": "Because we see the cell and then the blanket is the membrane.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2778.148,
    "end": 2780.65,
    "text": "And we see the brain and it's like vision and action.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2780.69,
    "end": 2782.652,
    "text": "And it seems like it maps onto...",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2784.81,
    "end": 2811.81,
    "text": " physical or the anatomical structure of the world or the causal structure of the world and um it doesn't so i hope people can add more thoughts about like how we consider redundancy because it's a great question um but we'll leave it there um how are reflexes modeled in active inference",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2813.482,
    "end": 2835.576,
    "text": " they exist in an active inference module that is distinct from proprioception would they operate on different time scales um if anybody wants to add like some some context on it um a reflex arc",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2837.185,
    "end": 2848.221,
    "text": " relating to um where reflex is defined as um a function that is being relayed through the spinal cord of a mammal",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2849.255,
    "end": 2878.901,
    "text": " and not passing like you could have a nerve block in the cervical vertebra and it still is able to like implement like that's one definition of reflex often it's also used slightly more broadly to mean like um stimulus action reproducible outcomes but not every reflex has a perfect reproducibility etc etc so it's kind of a continuum um yes it would involve proprioceptive input that proprioceptive input",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2880.305,
    "end": 2890.731,
    "text": " would be combined or juxtaposed with the descending prediction, resulting in an error.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2892.352,
    "end": 2897.555,
    "text": "And then that directionality and magnitude of the error in this model drives the reflex.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2899.656,
    "end": 2904.499,
    "text": "There can also be multiple timescales, but we haven't seen any nested modeling yet.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2906.0,
    "end": 2908.381,
    "text": "This is just the one layer model.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2913.7,
    "end": 2922.727,
    "text": " The schematic on the left in figure 5-1 shows that layer 5 of cortex projects to spinal pyramidal neurons, and this can be interpreted as a prediction.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2925.369,
    "end": 2926.871,
    "text": "Okay, so this is this one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2926.891,
    "end": 2934.777,
    "text": "Okay, helpful context.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2936.819,
    "end": 2942.123,
    "text": "And it relates to this earlier highlighted section, which is like,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2944.58,
    "end": 2960.213,
    "text": " It's the interpretation, the interpretive link between some anatomical or biological feature, phenotype, and some parametric resonance with a model.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2963.776,
    "end": 2971.543,
    "text": "The validity of that ranges from pretty clear, pretty uncontroversial, pretty useful, pretty effective.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2973.413,
    "end": 2984.167,
    "text": " to none of the above and it's hard to know like what given interpretive links are doing",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2985.475,
    "end": 2996.984,
    "text": " like saying that this edge on this Bayes graph can be interpreted as a descending excitatory connection, or that might be used to generate specific testable hypotheses.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2997.665,
    "end": 3003.269,
    "text": "Like if we measure it during a period of excitation, the activity of this neuron, we expect it to be increased.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3003.91,
    "end": 3007.273,
    "text": "Whereas if it was a descending inhibitory connection, et cetera.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3009.409,
    "end": 3031.153,
    "text": " that's a great example of active inference being used in a proactive way to generate hypotheses about biological systems that are going to be as they said in the preface um answerable to measured data by grounding in",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3032.64,
    "end": 3059.282,
    "text": " computational model that is related to the neuroanatomy but isn't trying to be like a digital twin of the neuroanatomy we can generate predictions about gain loss of function different measurements to make if they don't already exist and then that can be used to increase our confidence or reliability or falsify even a specific generative model as proposed so there's so much",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3060.222,
    "end": 3084.315,
    "text": " discourse like continuing to the present day and surely beyond as like all these questions and recently posted in the discord about like active inference or free energy principle isn't falsifiable now the funny link is it's not falsifiable so it's been falsified it can't be falsified so it's incorrect",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3085.267,
    "end": 3087.168,
    "text": " That's an interesting connection that some people make.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3087.908,
    "end": 3089.509,
    "text": "But a linear model cannot be falsified.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3089.989,
    "end": 3091.41,
    "text": "A Bayes graph cannot be falsified.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3092.25,
    "end": 3105.097,
    "text": "Once any given linear model is presented in a context with constraints for a certain data type, then it's an empirical question of its accuracy and adequacy relative to other models.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3107.258,
    "end": 3113.401,
    "text": "But at the abstract level of a neural network or a linear model or active inference, falsification",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3114.58,
    "end": 3138.179,
    "text": " simply doesn't apply but there's a lot that could be clarified um about like what are the utilities and some of the pitfalls of mapping biological or cyber physical systems to active inference models",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3140.29,
    "end": 3152.658,
    "text": " If it's a purely digital system or potentially even a cyber-physical system that's been designed a really certain way, it might be compatible with Act-Inf, like essentially by fiat or by design.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3154.78,
    "end": 3160.544,
    "text": "One could make an artificial creature like an InferAnt that is implementing active inference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3162.605,
    "end": 3164.847,
    "text": "One could also model an ant in the field",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3166.093,
    "end": 3189.999,
    "text": " using a linear model or a levy flight or some other model or an active inference model but those are two very different settings one in which sort of the rabbit was placed into the hat and then we have a rabbit analyzer and the other one being like we don't know what's in the hat but we have a rabbit analyzer that we think applies to what is in this hat um",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3201.038,
    "end": 3223.125,
    "text": " chapter five includes some um evidence presented on different neurotransmitters so um it'd be interesting to see like what other frameworks are able to be compatible or even provide unique explanations predictions Etc related to neurochemistry",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3224.534,
    "end": 3247.894,
    "text": " neural networks ever been used to derive unique predictions or found compatibility with neural hormones neurochemicals um and then again they have a final figure in chapter five that's like a graphical overview of several of the systems that are described in the chapter specifically relating to like the cortical cognitive functions",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3249.145,
    "end": 3277.917,
    "text": " and then the dialectic between habit and free energy driven planning here with dopamine as a precision modulator with like extensive further modeling presented in other papers and this like even more basal motor selection mechanism based upon reflex arcs and proprioceptive error minimization and all of that",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3279.759,
    "end": 3304.657,
    "text": " um we raise these questions not to offer any answers but to highlight some of the exciting avenues of future research in theoretical neurobiology so a lot of the questions that are asked here in this paragraph but also above at the very beginning they said we're not saying how it is we're just giving like the current process model understanding and then especially at the end",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3306.172,
    "end": 3334.006,
    "text": " they raise even more speculative questions so um that takes us to the end of chapter five that's the first half of the book that's the uh epistemic component of the book well a lot of it is epistemic but that's the first five chapters um the second five chapters which are longer",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3335.834,
    "end": 3357.356,
    "text": " partially because of having more figures and things like that but you can see there's more pages um slightly in the second part of the book though we've also read the um at least some of the appendices that's where we're heading which is picking up on uh chapter six",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3358.976,
    "end": 3362.138,
    "text": " in just a few weeks with the recipe for designing active inference models.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3362.418,
    "end": 3370.203,
    "text": "So for those who stuck with the uncertainty for the first five chapters and stay in the game,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3372.108,
    "end": 3400.482,
    "text": " more will become clear when the recipe for the dish is seen and then more will be clarified when the dish is prepared and then when you cut the vegetables and then when you design the dish and the recipe and all of that and that's like a journey that we're all going to be on the tools are not finalized and the kitchen is not completed etc etc etc so it's just a call for us to go to onboarding",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3402.08,
    "end": 3414.27,
    "text": " and indicate our interest in continuing with one or the other cohort or sharing with any colleagues who you think might like to jump in to the second cohort of part one.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3415.411,
    "end": 3420.875,
    "text": "And then in the coming two weeks, we'll be able to take a step back",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3422.165,
    "end": 3448.736,
    "text": " look over the chapter questions but also think about more general questions that we're having basic questions meso questions advanced questions research avenues and also turn to the project ideas where multiple of these are already active and there's spaces for people who want to facilitate or catalyze some other direction so",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3450.97,
    "end": 3452.21,
    "text": " fun meeting.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3454.491,
    "end": 3464.533,
    "text": "Everyone's welcome to stay on for tools or head up to a room above if they just want to talk to other people about anything else.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3465.354,
    "end": 3470.895,
    "text": "So thanks again for joining and see you in just a few minutes for tools.",
    "speaker": "SPEAKER_05"
  }
]
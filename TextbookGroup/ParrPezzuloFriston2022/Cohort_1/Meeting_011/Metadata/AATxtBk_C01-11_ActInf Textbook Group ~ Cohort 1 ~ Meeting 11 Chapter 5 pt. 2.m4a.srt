1
00:00:02,570 --> 00:00:05,680
Alright, hello everyone. It is week

2
00:00:06,370 --> 00:00:10,110
eleven and we're in

3
00:00:10,260 --> 00:00:13,438
chapter five, part two of the

4
00:00:13,524 --> 00:00:16,750
active textbook group first cohort.

5
00:00:17,890 --> 00:00:21,630
We'll be now turning to some of the more

6
00:00:21,700 --> 00:00:24,918
direct questions on chapter five after

7
00:00:25,044 --> 00:00:29,160
having more of an overview last week.

8
00:00:30,250 --> 00:00:33,654
We'll also be

9
00:00:33,692 --> 00:00:36,546
concluding our regular chapter

10
00:00:36,658 --> 00:00:39,850
discussions for the first part of this

11
00:00:39,920 --> 00:00:43,130
book. And in the coming two weeks,

12
00:00:43,280 --> 00:00:46,474
we'll be having some review and

13
00:00:46,512 --> 00:00:50,170
synthesis and connecting some dots,

14
00:00:50,930 --> 00:00:53,146
asking overarching questions, returning

15
00:00:53,178 --> 00:00:54,606
to questions that were based in a

16
00:00:54,628 --> 00:00:57,230
specific chapter and hopefully people

17
00:00:57,300 --> 00:00:59,486
like on their own or wherever else

18
00:00:59,508 --> 00:01:02,854
they're seeing as relevant or requesting

19
00:01:02,922 --> 00:01:05,426
new pages if they see something that

20
00:01:05,448 --> 00:01:08,738
they want. Providing some synthesis on

21
00:01:08,744 --> 00:01:11,394
this first half of the textbook, which

22
00:01:11,432 --> 00:01:16,614
is the epistemic half. And then for

23
00:01:16,732 --> 00:01:19,926
people who want to continue in the

24
00:01:19,948 --> 00:01:23,442
textbook group, go to the onboarding

25
00:01:23,586 --> 00:01:27,202
page and there's two columns

26
00:01:27,266 --> 00:01:29,926
there's. Yes, for part two of cohort

27
00:01:29,958 --> 00:01:32,234
one, chapter six through ten. So check

28
00:01:32,272 --> 00:01:35,146
that box if you want to be included in

29
00:01:35,248 --> 00:01:38,474
starting in September going through

30
00:01:38,512 --> 00:01:40,526
chapters six through ten, which has like

31
00:01:40,548 --> 00:01:42,666
the recipe for active models and that's

32
00:01:42,698 --> 00:01:44,174
when we're going to be getting more

33
00:01:44,212 --> 00:01:46,270
hands on with modeling.

34
00:01:47,570 --> 00:01:51,130
Also, everyone is welcome to join

35
00:01:51,290 --> 00:01:54,706
for part one, chapters one through five,

36
00:01:54,888 --> 00:01:58,050
cohort two also starting in September.

37
00:01:59,190 --> 00:02:03,170
So it could be a fun experience just to

38
00:02:03,240 --> 00:02:05,542
get another coat of paint on the

39
00:02:05,676 --> 00:02:09,794
material, play a more active role,

40
00:02:09,922 --> 00:02:12,326
ask another set of questions informed by

41
00:02:12,348 --> 00:02:14,886
what you've seen in the first cohort so

42
00:02:14,988 --> 00:02:18,106
feel free to check those boxes and

43
00:02:18,128 --> 00:02:22,438
then also keep incubating

44
00:02:22,534 --> 00:02:26,842
as we head into chapter six

45
00:02:26,896 --> 00:02:29,018
and beyond. In the second half of the

46
00:02:29,024 --> 00:02:32,746
textbook, like the project ideas,

47
00:02:32,938 --> 00:02:36,318
several of the project ideas like

48
00:02:36,484 --> 00:02:39,054
related to what Mike and Noah have

49
00:02:39,092 --> 00:02:43,122
added, we've started to address and

50
00:02:43,176 --> 00:02:45,026
explore in the active block, for

51
00:02:45,048 --> 00:02:46,930
instance, meetings on Wednesday,

52
00:02:47,270 --> 00:02:49,742
especially related to cyber, physical

53
00:02:49,806 --> 00:02:53,122
systems, simulations, taxonomies for

54
00:02:53,176 --> 00:02:56,150
governance, these kinds of ideas.

55
00:02:56,970 --> 00:03:00,200
Brock's added some great notes about

56
00:03:01,370 --> 00:03:03,798
what is the ramp that gets someone to

57
00:03:03,964 --> 00:03:07,400
the low road or the high road. And then

58
00:03:08,590 --> 00:03:11,706
I've recorded the first chapter and the

59
00:03:11,728 --> 00:03:14,330
first segments as an audiobook.

60
00:03:15,390 --> 00:03:17,210
The first chapter doesn't have any

61
00:03:17,280 --> 00:03:19,866
equations so it's straightforward to

62
00:03:19,888 --> 00:03:22,494
read, but the subsequent chapters do

63
00:03:22,532 --> 00:03:28,990
have equations, which is motivating

64
00:03:30,210 --> 00:03:32,202
having the natural language

65
00:03:32,266 --> 00:03:35,666
representations of the equations. So

66
00:03:35,688 --> 00:03:39,070
there's still a ton of discourse

67
00:03:39,150 --> 00:03:40,786
questions that can be asked why things

68
00:03:40,808 --> 00:03:42,242
are one way, why things are a different

69
00:03:42,296 --> 00:03:44,834
way. If it can be placed into the

70
00:03:44,872 --> 00:03:47,798
equation itself, that's great. If it

71
00:03:47,804 --> 00:03:49,474
needs to be somewhere else, hopefully

72
00:03:49,522 --> 00:03:51,426
the space exists and people can request,

73
00:03:51,458 --> 00:03:54,818
if not, but for example, when reading

74
00:03:54,834 --> 00:03:57,458
the textbook, the equations can be read

75
00:03:57,564 --> 00:03:59,930
this way. So those are just some of the

76
00:04:00,080 --> 00:04:03,594
projects that people can look

77
00:04:03,632 --> 00:04:05,722
forward to and the affordances to

78
00:04:05,776 --> 00:04:09,370
continue on, which is again going to

79
00:04:09,440 --> 00:04:12,646
the onboarding and participating in

80
00:04:12,768 --> 00:04:16,318
first five chapters. Again cohort two or

81
00:04:16,404 --> 00:04:19,662
going to continue? On with

82
00:04:19,716 --> 00:04:21,886
chapters six through ten starting in

83
00:04:21,908 --> 00:04:23,826
September for both of these so that we

84
00:04:23,848 --> 00:04:26,946
can complete one pass of the book by the

85
00:04:26,968 --> 00:04:29,938
end of the year and also probably make a

86
00:04:29,944 --> 00:04:32,098
lot of progress on different project

87
00:04:32,184 --> 00:04:35,634
ideas and project ideas arising as

88
00:04:35,672 --> 00:04:38,258
we start to see and build templates for

89
00:04:38,344 --> 00:04:42,294
the models. And also Ali has

90
00:04:42,332 --> 00:04:44,710
some nice interactive notebooks, so

91
00:04:44,780 --> 00:04:46,694
there will be a lot of great things

92
00:04:46,812 --> 00:04:50,618
coming together. If anyone wants to

93
00:04:50,624 --> 00:04:52,154
raise their hand, of course, just go for

94
00:04:52,192 --> 00:04:56,310
it. Otherwise we will be continuing

95
00:04:56,390 --> 00:04:59,990
with the questions

96
00:05:00,080 --> 00:05:01,840
asked about Chapter Five.

97
00:05:03,170 --> 00:05:05,998
Before we go to the questions, is there

98
00:05:06,084 --> 00:05:08,734
any general comment that somebody wants

99
00:05:08,772 --> 00:05:12,334
to add about Chapter Five?

100
00:05:12,532 --> 00:05:14,866
Their reading of it, their rereading of

101
00:05:14,888 --> 00:05:16,594
it, listening to the video from last

102
00:05:16,632 --> 00:05:20,260
week? How is chapter Five

103
00:05:23,510 --> 00:05:32,440
hitting them today's?

104
00:05:41,260 --> 00:05:43,532
To kind of maybe bring that one step

105
00:05:43,586 --> 00:05:45,790
closer to a specific question.

106
00:05:47,120 --> 00:05:50,408
I'm not going to read this whole quote,

107
00:05:50,504 --> 00:05:52,840
but this question says in the preface

108
00:05:53,000 --> 00:05:55,664
they wrote in Chapter Five, we will move

109
00:05:55,702 --> 00:05:57,852
from formal treatments to biological

110
00:05:57,916 --> 00:06:00,000
implications of active inference.

111
00:06:01,860 --> 00:06:04,060
This AIDS in mapping the abstract

112
00:06:04,140 --> 00:06:06,156
computational principles of active

113
00:06:06,188 --> 00:06:07,928
inference to specific neural

114
00:06:07,964 --> 00:06:10,324
computations that can be executed by

115
00:06:10,362 --> 00:06:13,956
physiological substrates. This is

116
00:06:13,978 --> 00:06:15,940
important in forming hypotheses under

117
00:06:16,010 --> 00:06:18,196
this framework and ensures that these

118
00:06:18,218 --> 00:06:20,616
are answerable to measure data. In other

119
00:06:20,638 --> 00:06:22,280
words, Chapter Five sets out the process

120
00:06:22,350 --> 00:06:24,680
theory associated with active inference.

121
00:06:25,820 --> 00:06:29,610
Is this how people saw chapter five?

122
00:06:29,980 --> 00:06:32,250
Do they feel like it did something

123
00:06:32,880 --> 00:06:36,204
different? Did it succeed at

124
00:06:36,402 --> 00:06:38,440
making the abstract computational

125
00:06:38,520 --> 00:06:42,300
principles linked or mapped to specific

126
00:06:42,370 --> 00:06:44,716
neural computations executed by

127
00:06:44,738 --> 00:06:46,400
physiological substrates?

128
00:07:04,480 --> 00:07:07,356
Yes. Eric, first of all, I have to

129
00:07:07,378 --> 00:07:10,592
apologize. Only watched half of last

130
00:07:10,646 --> 00:07:14,176
week's discussion, so I may

131
00:07:14,278 --> 00:07:17,536
be talking about ground that's been

132
00:07:17,558 --> 00:07:18,530
covered before.

133
00:07:20,840 --> 00:07:23,972
It just seems to me that I've seen many,

134
00:07:24,026 --> 00:07:28,580
many papers

135
00:07:29,960 --> 00:07:33,684
trying to map theories onto brain

136
00:07:33,732 --> 00:07:38,388
circuitry and physiology, and I'm

137
00:07:38,404 --> 00:07:41,924
not well enough. I'm only loosely

138
00:07:41,972 --> 00:07:45,076
associated with neuroscience and don't

139
00:07:45,108 --> 00:07:47,096
follow it very closely. So it's hard for

140
00:07:47,118 --> 00:07:49,308
me to cite specific instances along the

141
00:07:49,314 --> 00:07:51,304
way. I know you mentioned no Menta,

142
00:07:51,432 --> 00:07:53,852
which is one of the more recent ones.

143
00:07:53,906 --> 00:07:56,908
It's trying to say these are what the

144
00:07:56,914 --> 00:08:01,536
layers are doing. So this

145
00:08:01,638 --> 00:08:05,456
chapter to me didn't seem any

146
00:08:05,638 --> 00:08:07,712
very qualitatively different from

147
00:08:07,766 --> 00:08:11,652
anybody else's very hand wavy way to say

148
00:08:11,786 --> 00:08:13,476
here's what the computations are all

149
00:08:13,498 --> 00:08:17,140
about and here's how they map to what

150
00:08:17,210 --> 00:08:20,096
brain circuitry could map to what brain

151
00:08:20,128 --> 00:08:23,528
circuitry might be doing. It's all quite

152
00:08:23,614 --> 00:08:28,056
hand wavy, I find, and I

153
00:08:28,078 --> 00:08:31,912
guess plausible, but it's certainly

154
00:08:32,046 --> 00:08:34,330
not definitive, nothing close to that.

155
00:08:38,220 --> 00:08:41,724
Maybe one other comment I would

156
00:08:41,842 --> 00:08:45,128
say is, again, in this chapter,

157
00:08:45,224 --> 00:08:48,832
they use this word prediction in a way

158
00:08:48,886 --> 00:08:52,704
that conforms to the theory. But to say

159
00:08:52,742 --> 00:08:55,410
that what's happening in,

160
00:08:56,900 --> 00:08:59,410
say, motor control,

161
00:08:59,780 --> 00:09:02,272
where top down signals from the cortex

162
00:09:02,336 --> 00:09:05,168
go down to the ganglia in the spinal

163
00:09:05,184 --> 00:09:09,060
cord in order to invoke motor patterns.

164
00:09:10,520 --> 00:09:13,476
To call that a prediction is, to me, a

165
00:09:13,498 --> 00:09:16,516
kind of well, that's taking extreme

166
00:09:16,548 --> 00:09:19,496
measures to map the concepts of the

167
00:09:19,518 --> 00:09:24,010
theory onto what neurons are doing and

168
00:09:25,420 --> 00:09:29,676
if the theory works out to be very

169
00:09:29,778 --> 00:09:31,836
useful in some way and that's what you

170
00:09:31,858 --> 00:09:33,950
have to do to make that happen.

171
00:09:34,640 --> 00:09:37,864
Okay, I guess I can use that language,

172
00:09:37,912 --> 00:09:40,016
but Operaria just does not seem like the

173
00:09:40,038 --> 00:09:42,192
right language to use to say, yeah,

174
00:09:42,246 --> 00:09:43,996
we're trying to set up some motor

175
00:09:44,028 --> 00:09:45,936
patterns, set up some context and

176
00:09:45,958 --> 00:09:48,850
situations for what we want the local

177
00:09:49,220 --> 00:09:52,550
reflexes to be doing.

178
00:09:53,560 --> 00:09:55,716
But to cast that as a prediction just

179
00:09:55,738 --> 00:09:58,276
seems a little unnatural to me. So those

180
00:09:58,298 --> 00:10:00,980
are my two reactions to this chapter.

181
00:10:01,800 --> 00:10:04,090
Thank you.

182
00:10:05,740 --> 00:10:07,850
Anyone else with a raised hand?

183
00:10:13,580 --> 00:10:17,550
This is a really good point about what

184
00:10:18,080 --> 00:10:20,990
the linkage or mapping is between

185
00:10:21,840 --> 00:10:23,516
without going into, like, you know, is

186
00:10:23,538 --> 00:10:25,836
anything physical or, et cetera, like

187
00:10:26,018 --> 00:10:29,132
physical brains in their niche and any

188
00:10:29,186 --> 00:10:32,284
type of equations. Would a well fitting

189
00:10:32,332 --> 00:10:34,528
linear model say that the brain is a

190
00:10:34,534 --> 00:10:37,036
linear model? Does a well fitting Bayes

191
00:10:37,068 --> 00:10:39,756
graph imply that the brain is a Bayes

192
00:10:39,788 --> 00:10:42,800
graph? Does a well fitting Bayes graph

193
00:10:42,880 --> 00:10:44,768
plus a philosophical belief that we're

194
00:10:44,784 --> 00:10:46,176
just making a map, we're not describing

195
00:10:46,208 --> 00:10:50,016
the territory equate to what beyond

196
00:10:50,048 --> 00:10:52,324
a linear model or a verbal model.

197
00:10:52,442 --> 00:10:56,296
Rohan. Yeah. Building on

198
00:10:56,318 --> 00:11:00,120
the previous point right. I'm still

199
00:11:00,190 --> 00:11:03,976
unsure about what Active inference is,

200
00:11:03,998 --> 00:11:05,772
considering a signal, there's no

201
00:11:05,826 --> 00:11:09,292
effective well, there's no noise model

202
00:11:09,346 --> 00:11:12,600
or it doesn't specify that here's,

203
00:11:12,680 --> 00:11:16,776
because if you say that I'm predicting

204
00:11:16,808 --> 00:11:18,464
something, right. So there must be

205
00:11:18,502 --> 00:11:21,452
something that is substantially

206
00:11:21,516 --> 00:11:23,568
different, statistically different from

207
00:11:23,654 --> 00:11:26,816
some noise distribution that you can

208
00:11:26,838 --> 00:11:31,044
differentiate. Right. So it's basically

209
00:11:31,242 --> 00:11:34,016
trying to estimate some hidden

210
00:11:34,048 --> 00:11:36,436
probabilities, which is not really a

211
00:11:36,458 --> 00:11:39,428
signal. Does that make sense?

212
00:11:39,514 --> 00:11:42,324
Because even the internal states,

213
00:11:42,442 --> 00:11:46,436
unless they're actually instantiated

214
00:11:46,628 --> 00:11:49,352
prior, like when we set up the model,

215
00:11:49,406 --> 00:11:50,920
we say, okay, these are the only states

216
00:11:50,990 --> 00:11:52,760
that you can actually traverse.

217
00:11:55,600 --> 00:11:58,556
You still have to infer the states from

218
00:11:58,658 --> 00:12:00,620
your measurements of reality.

219
00:12:04,080 --> 00:12:05,070
Thank you.

220
00:12:10,660 --> 00:12:14,544
Jacob? Yeah. I just want

221
00:12:14,582 --> 00:12:17,104
to ask if the noise isn't kind of

222
00:12:17,142 --> 00:12:20,508
implied in the

223
00:12:20,694 --> 00:12:23,380
POMDP formalism,

224
00:12:24,520 --> 00:12:27,350
like, I think in Figure 5.4,

225
00:12:28,600 --> 00:12:30,416
where it shows the direct and indirect

226
00:12:30,448 --> 00:12:32,260
pathways of message passing,

227
00:12:35,400 --> 00:12:39,456
it describes Active and the basic

228
00:12:39,648 --> 00:12:43,424
ganglia as a POMDP generative

229
00:12:43,472 --> 00:12:46,584
model. So it's partially observable.

230
00:12:46,632 --> 00:12:49,260
Doesn't that imply some kind of noise?

231
00:12:52,480 --> 00:12:56,364
My counter to this would be that is

232
00:12:56,482 --> 00:12:58,416
missing information that needs to be

233
00:12:58,438 --> 00:13:00,652
filled out. That's not really noise.

234
00:13:00,716 --> 00:13:10,556
Right. So noise would be something let's

235
00:13:10,588 --> 00:13:14,310
not use this. So we have

236
00:13:14,680 --> 00:13:17,988
returns of a stock, and looking at

237
00:13:17,994 --> 00:13:20,432
the distribution of the prior returns,

238
00:13:20,496 --> 00:13:23,110
we know what the spread is, and in case

239
00:13:23,480 --> 00:13:25,032
we want to measure whether your

240
00:13:25,086 --> 00:13:27,560
portfolio is substantially different

241
00:13:27,630 --> 00:13:30,376
from this distribution, that's where we

242
00:13:30,398 --> 00:13:31,956
would have some sort of, like, sharp

243
00:13:31,988 --> 00:13:34,436
ratio or a signal noise ratio. Noise

244
00:13:34,468 --> 00:13:36,364
ratio being the complete set of

245
00:13:36,402 --> 00:13:39,244
distributions that are possible, the set

246
00:13:39,282 --> 00:13:41,630
of ranges that are possible.

247
00:13:43,680 --> 00:13:46,844
That is not really here, because that is

248
00:13:46,962 --> 00:13:48,752
missing information. So we can only

249
00:13:48,806 --> 00:13:52,144
observe the fact that

250
00:13:52,182 --> 00:13:54,736
we have only partial observations does

251
00:13:54,758 --> 00:13:56,930
not excuse the fact that we don't have

252
00:13:57,620 --> 00:14:00,224
even noise models of the sensors. So how

253
00:14:00,262 --> 00:14:04,004
good are the sensors, for example? Do we

254
00:14:04,042 --> 00:14:05,990
have some idea of that?

255
00:14:07,400 --> 00:14:09,444
Yes. Yaku, but then I'll add a point

256
00:14:09,482 --> 00:14:11,460
about the Espasian terminology.

257
00:14:12,120 --> 00:14:12,870
Yeah.

258
00:14:19,500 --> 00:14:21,864
What's the practical difference between

259
00:14:22,062 --> 00:14:25,880
partial observation where the noise

260
00:14:26,480 --> 00:14:29,676
implies some kind of information that is

261
00:14:29,698 --> 00:14:33,624
missing, versus a distribution that's

262
00:14:33,672 --> 00:14:39,196
added to full observation that adds

263
00:14:39,228 --> 00:14:42,080
noise to the sensory processing?

264
00:14:45,140 --> 00:14:45,890
Great.

265
00:14:49,220 --> 00:14:51,612
So if I use an actual physical sensor,

266
00:14:51,676 --> 00:14:54,980
right, it would be something like

267
00:14:55,050 --> 00:14:56,756
what are the ranges of values that it

268
00:14:56,778 --> 00:14:58,436
can go up to? Because we've designed the

269
00:14:58,458 --> 00:15:01,476
sensor, so we basically know what the

270
00:15:01,498 --> 00:15:03,636
bias is, what the variance of the sensor

271
00:15:03,668 --> 00:15:05,960
is under different conditions.

272
00:15:07,900 --> 00:15:10,052
There are actual physical processes

273
00:15:10,116 --> 00:15:12,936
implied in that measurement. So in the

274
00:15:12,958 --> 00:15:16,056
case of ultrasonic sensor,

275
00:15:16,088 --> 00:15:18,652
it's probably like some current value or

276
00:15:18,706 --> 00:15:20,856
some voltage value that's being measured

277
00:15:20,888 --> 00:15:23,036
by some circuitry somewhere. So there

278
00:15:23,058 --> 00:15:26,124
are ranges for that. So there's no

279
00:15:26,162 --> 00:15:28,924
missing information there. So you

280
00:15:28,962 --> 00:15:32,392
basically know it's already a prior

281
00:15:32,456 --> 00:15:34,176
in this system, right. So you know what

282
00:15:34,198 --> 00:15:35,888
the ranges of values are going to be.

283
00:15:35,974 --> 00:15:39,196
And from there you need to infer the sum

284
00:15:39,228 --> 00:15:40,676
state in the world. So is there an

285
00:15:40,698 --> 00:15:42,870
obstacle in the way or is there?

286
00:15:46,200 --> 00:15:48,644
Okay, let me just make a comment about

287
00:15:48,682 --> 00:15:51,376
some basing terminology then. Jessica

288
00:15:51,408 --> 00:15:55,844
and Brock so let's look at figure 4.3.

289
00:15:55,882 --> 00:15:59,296
This is the partially observable Markov

290
00:15:59,328 --> 00:16:00,984
decision process that Jacob talked

291
00:16:01,022 --> 00:16:02,664
about. But we're going to be just

292
00:16:02,702 --> 00:16:04,360
talking about the perceptive part, not

293
00:16:04,430 --> 00:16:06,316
the action intervention into how things

294
00:16:06,338 --> 00:16:08,076
are changing, yet just focusing on this

295
00:16:08,098 --> 00:16:10,492
motif on the left, like step by step

296
00:16:10,546 --> 00:16:14,140
guide uses. So you mentioned

297
00:16:14,210 --> 00:16:16,280
about the bias related to measurements.

298
00:16:16,360 --> 00:16:20,428
So if the measurables are the observable

299
00:16:20,604 --> 00:16:23,776
sense states, then the bias and the

300
00:16:23,798 --> 00:16:26,912
covariance and the variance about how

301
00:16:27,046 --> 00:16:29,404
the reading on the thermometer relates

302
00:16:29,452 --> 00:16:32,416
to the underlying variable, the hidden

303
00:16:32,448 --> 00:16:34,148
state, the temperature of the room,

304
00:16:34,314 --> 00:16:38,100
it's embodied in the a matrix because

305
00:16:38,170 --> 00:16:41,232
that is actually the learned or fixed

306
00:16:41,376 --> 00:16:45,524
variable that is playing

307
00:16:45,572 --> 00:16:48,490
a role of a noise model.

308
00:16:49,260 --> 00:16:51,928
Then you mentioned when talking about

309
00:16:52,014 --> 00:16:54,344
returns on stocks, testing whether

310
00:16:54,382 --> 00:16:55,652
there's like a substantial difference

311
00:16:55,726 --> 00:16:58,364
from some null expectation, like is

312
00:16:58,402 --> 00:16:59,708
there a signal that's different from the

313
00:16:59,714 --> 00:17:02,824
noise? This is like a frequentist

314
00:17:02,872 --> 00:17:05,256
approach. It's based upon determining

315
00:17:05,288 --> 00:17:06,552
whether there's some statistically

316
00:17:06,616 --> 00:17:08,716
significant level. Doesn't have to be,

317
00:17:08,738 --> 00:17:11,792
it's not the only way to do it, but one

318
00:17:11,846 --> 00:17:14,912
can do like a p value on whether a given

319
00:17:14,966 --> 00:17:17,520
signal is statistically different from

320
00:17:17,670 --> 00:17:19,632
given a noise model, different from

321
00:17:19,766 --> 00:17:22,464
noise. And then like, the sharp ratio

322
00:17:22,512 --> 00:17:24,324
and other types of things are like

323
00:17:24,362 --> 00:17:25,716
summary statistics. They're like

324
00:17:25,738 --> 00:17:27,140
descriptive statistics.

325
00:17:30,520 --> 00:17:34,084
Noisy measurements are addressed

326
00:17:34,212 --> 00:17:38,312
in the context of the POMDP by

327
00:17:38,366 --> 00:17:41,812
having an a matrix that accommodates

328
00:17:41,876 --> 00:17:44,824
some of the features of denoising a

329
00:17:44,862 --> 00:17:48,700
signal and then precision variables

330
00:17:49,520 --> 00:17:51,276
and those precisions can again be

331
00:17:51,298 --> 00:17:54,764
learned or fixed. But like the

332
00:17:54,802 --> 00:17:59,404
functional aspects

333
00:17:59,452 --> 00:18:02,316
of what is being referred to as noisy

334
00:18:02,348 --> 00:18:05,488
measurements are accommodated within

335
00:18:05,574 --> 00:18:07,376
this kind of Bayesian approach. Which is

336
00:18:07,398 --> 00:18:09,084
why common filtering, generalized

337
00:18:09,132 --> 00:18:11,776
Bayesian filtering, why these techniques

338
00:18:11,888 --> 00:18:14,912
are used because we can have a Bayesian

339
00:18:14,976 --> 00:18:17,204
update scheme implemented with

340
00:18:17,322 --> 00:18:19,680
variational approaches, message passing,

341
00:18:19,760 --> 00:18:23,896
sampling, all these things that does

342
00:18:24,078 --> 00:18:26,536
denoise signal, and it does embody a

343
00:18:26,558 --> 00:18:29,176
noise model and a signal model. And

344
00:18:29,198 --> 00:18:31,336
that's all part of the generative model

345
00:18:31,518 --> 00:18:34,984
of the cognitive entity. So it just

346
00:18:35,022 --> 00:18:37,096
isn't being addressed from a frequentist

347
00:18:37,128 --> 00:18:38,910
statistics or.

348
00:18:41,120 --> 00:18:42,892
Yeah, I shouldn't have brought up the

349
00:18:42,946 --> 00:18:45,132
Sharp ratio. Maybe that's not the best.

350
00:18:45,266 --> 00:18:48,800
But the Sharp ratio is

351
00:18:48,870 --> 00:18:51,136
effectively some way to say that some

352
00:18:51,238 --> 00:18:53,120
portfolio choice is different from

353
00:18:53,190 --> 00:18:56,256
others. Or in communication systems you

354
00:18:56,278 --> 00:18:58,800
would use something like EB over N zero.

355
00:18:58,950 --> 00:19:02,390
That's the signal to noise ratio for

356
00:19:02,840 --> 00:19:04,950
any sort of communication system,

357
00:19:07,720 --> 00:19:10,292
but they're not necessarily only

358
00:19:10,346 --> 00:19:13,024
frequentist. Right. So there are patient

359
00:19:13,072 --> 00:19:15,492
ways to estimate these things. In fact,

360
00:19:15,546 --> 00:19:17,156
that's pretty much how we do it in

361
00:19:17,178 --> 00:19:21,016
practice, is we don't assume that we

362
00:19:21,038 --> 00:19:23,352
know the complete distribution. We

363
00:19:23,406 --> 00:19:26,040
update priors in each of these cases.

364
00:19:30,160 --> 00:19:31,916
Coming back to your point that the A

365
00:19:31,938 --> 00:19:34,344
matrix does address this. The A matrix

366
00:19:34,392 --> 00:19:37,516
would be something that you

367
00:19:37,538 --> 00:19:40,432
said it would denoise and filter out

368
00:19:40,486 --> 00:19:43,810
some stray observations. But okay,

369
00:19:45,940 --> 00:19:48,176
the question still remains is when

370
00:19:48,198 --> 00:19:50,652
you're saying prediction, is it

371
00:19:50,726 --> 00:19:53,430
prediction of the signal over here?

372
00:19:54,280 --> 00:19:56,596
As in what is the future state of the

373
00:19:56,618 --> 00:20:00,228
sensor? But I think there, it seems

374
00:20:00,314 --> 00:20:02,740
seem to indicate that the prediction

375
00:20:05,580 --> 00:20:08,020
relates to the free energy minimization

376
00:20:08,100 --> 00:20:10,692
itself. Right. So you choose policies

377
00:20:10,756 --> 00:20:12,712
that would minimize your free energy and

378
00:20:12,766 --> 00:20:14,890
keep yourself into some set point,

379
00:20:15,760 --> 00:20:17,772
which means that there should be some

380
00:20:17,826 --> 00:20:21,260
sort of an idea of what

381
00:20:21,410 --> 00:20:25,660
ranges are there for this value or how

382
00:20:25,730 --> 00:20:28,888
far can these go before you're

383
00:20:28,904 --> 00:20:31,448
not in that set point anymore? So even

384
00:20:31,474 --> 00:20:32,944
if you, if you do want to make that

385
00:20:32,982 --> 00:20:34,848
prediction, it's, it's not just the

386
00:20:34,854 --> 00:20:36,844
signal value. So signal values

387
00:20:36,972 --> 00:20:38,844
effectively through the Marco blanket,

388
00:20:38,892 --> 00:20:43,648
they affect some internal state. And I'm

389
00:20:43,664 --> 00:20:44,916
assuming that's what we are trying to

390
00:20:44,938 --> 00:20:46,404
predict, that the trajectory of. The

391
00:20:46,442 --> 00:20:50,372
future internal states actually the

392
00:20:50,426 --> 00:20:54,164
expectation that's being minimized

393
00:20:54,212 --> 00:20:56,312
is over outcome observations, not about

394
00:20:56,366 --> 00:21:00,090
internal states. Okay? Yeah. I'm sorry.

395
00:21:02,460 --> 00:21:03,530
Even then,

396
00:21:05,680 --> 00:21:08,248
the signals may have well defined

397
00:21:08,264 --> 00:21:10,780
precision and bias, but the entire

398
00:21:10,850 --> 00:21:13,710
system itself doesn't have any sort of

399
00:21:15,840 --> 00:21:17,692
well, it doesn't know how wrong it can

400
00:21:17,746 --> 00:21:20,824
go. It doesn't have any error estimate.

401
00:21:20,952 --> 00:21:23,950
So there's no noise model. As such,

402
00:21:25,840 --> 00:21:28,364
we would have to assume that there is

403
00:21:28,482 --> 00:21:30,388
some distribution of states and then

404
00:21:30,434 --> 00:21:32,644
then use something like lee squares or

405
00:21:32,762 --> 00:21:35,652
some other optimization routine to

406
00:21:35,706 --> 00:21:38,420
actually bring this in practice.

407
00:21:40,040 --> 00:21:43,044
Great. Totally possible. It would be

408
00:21:43,082 --> 00:21:46,004
interesting to sketch out a Bayes graph

409
00:21:46,132 --> 00:21:48,516
for what you believe a signal to noise

410
00:21:48,628 --> 00:21:51,080
or some type of noise model would embody

411
00:21:51,500 --> 00:21:54,692
and maybe we will see some analogies

412
00:21:54,756 --> 00:21:57,224
or some mappings. Jessica and then

413
00:21:57,262 --> 00:21:57,880
Brock.

414
00:22:00,620 --> 00:22:03,276
Hi. I think someone like what I was

415
00:22:03,298 --> 00:22:04,764
going to say probably already been

416
00:22:04,802 --> 00:22:08,144
covered by both you and Rohan in the new

417
00:22:08,182 --> 00:22:11,250
comments. But I was going to say that

418
00:22:12,020 --> 00:22:15,408
with this filtering and a lot of the

419
00:22:15,414 --> 00:22:18,512
biases and things, I kind of made sense

420
00:22:18,566 --> 00:22:19,250
of it.

421
00:22:21,880 --> 00:22:23,636
I think it was like chapter two or

422
00:22:23,658 --> 00:22:28,724
something like that. So the why is this

423
00:22:28,922 --> 00:22:34,280
quote unquote thing

424
00:22:34,430 --> 00:22:36,724
that we can observe from the hidden

425
00:22:36,772 --> 00:22:39,096
state or the process of something that

426
00:22:39,198 --> 00:22:42,410
we don't know for everybody.

427
00:22:42,860 --> 00:22:45,992
We can see the same thing, but how we

428
00:22:46,046 --> 00:22:48,900
process it internally is going to vary.

429
00:22:49,060 --> 00:22:51,064
It's going to be very subjective. So the

430
00:22:51,102 --> 00:22:52,924
why is going to look different for

431
00:22:52,962 --> 00:22:54,844
Daniel than it's going to look for me

432
00:22:54,962 --> 00:22:57,896
based on all this filtering. And so the

433
00:22:57,938 --> 00:23:00,624
blanket states, I imagine, sort of like

434
00:23:00,662 --> 00:23:04,096
what have those filters that encode the

435
00:23:04,118 --> 00:23:07,090
biases and other things that allow us to

436
00:23:07,540 --> 00:23:11,248
process things differently than our

437
00:23:11,254 --> 00:23:14,436
generative model. It's going to be

438
00:23:14,458 --> 00:23:17,668
updated differently, also encoded, I

439
00:23:17,674 --> 00:23:19,444
think in the general model will be a lot

440
00:23:19,482 --> 00:23:22,004
of cures and things like that that

441
00:23:22,122 --> 00:23:23,976
embody, I think, probably a lot of

442
00:23:23,998 --> 00:23:27,064
biases and things of how we understand

443
00:23:27,182 --> 00:23:31,210
things which also influence how we

444
00:23:31,740 --> 00:23:34,520
interpret what we observe.

445
00:23:36,400 --> 00:23:38,076
So for me, it's like a lot of these

446
00:23:38,098 --> 00:23:40,504
things are there in terms of statistics.

447
00:23:40,632 --> 00:23:43,630
I don't know how much, but that's how

448
00:23:44,320 --> 00:23:50,392
conceptually, I understood

449
00:23:50,536 --> 00:23:52,988
a lot of these things. Basically, in the

450
00:23:52,994 --> 00:23:55,824
blanket days of the Marco blanket, it

451
00:23:55,862 --> 00:23:58,000
would have a lot of disfaltering.

452
00:23:59,860 --> 00:24:03,860
Yes. If you're going to try

453
00:24:03,930 --> 00:24:07,060
to, let's say you observe something

454
00:24:07,210 --> 00:24:10,000
that's different than what you predicted

455
00:24:10,080 --> 00:24:13,720
in your model, then you would take

456
00:24:13,790 --> 00:24:16,472
action and to forage and find

457
00:24:16,526 --> 00:24:20,170
information and to figure it out, okay,

458
00:24:20,540 --> 00:24:23,752
what am I missing? Why did I make this

459
00:24:23,806 --> 00:24:27,148
mistake or like this error in my

460
00:24:27,234 --> 00:24:30,364
prediction so that I can have more

461
00:24:30,402 --> 00:24:32,316
information and update the model so that

462
00:24:32,338 --> 00:24:34,764
it aligns more closely? So then you will

463
00:24:34,802 --> 00:24:38,304
basically take action to the noise and

464
00:24:38,342 --> 00:24:43,184
basically get closer to the

465
00:24:43,222 --> 00:24:46,528
reality that's out there. Yeah. I don't

466
00:24:46,534 --> 00:24:48,976
know if this official lightning, but

467
00:24:48,998 --> 00:24:51,316
that's how it kind of makes sense to me.

468
00:24:51,498 --> 00:24:53,300
Thanks, Jessica Brock.

469
00:24:56,280 --> 00:25:00,292
Yeah, I just wanted

470
00:25:00,346 --> 00:25:05,584
to, I guess, speak to this sensor

471
00:25:05,712 --> 00:25:09,284
noise model in

472
00:25:09,402 --> 00:25:12,024
practice. In reality, when we're talking

473
00:25:12,062 --> 00:25:13,492
about sensors, like in a physical

474
00:25:13,556 --> 00:25:16,596
system, like, like a digital signal

475
00:25:16,628 --> 00:25:19,836
processing kind of context, like, the

476
00:25:19,858 --> 00:25:22,700
sensors are tested under some conditions

477
00:25:23,280 --> 00:25:27,908
for things like jitter and spurious

478
00:25:28,024 --> 00:25:31,296
kind of noise conditions and stuff

479
00:25:31,318 --> 00:25:33,440
like that. But they're tested under some

480
00:25:33,510 --> 00:25:37,424
conditions, meaning that

481
00:25:37,542 --> 00:25:40,804
there's never ever going to be a

482
00:25:40,842 --> 00:25:44,470
time when you have an actual model.

483
00:25:45,160 --> 00:25:46,644
You're just going to have an

484
00:25:46,682 --> 00:25:48,896
approximation whether that's expressed

485
00:25:48,928 --> 00:25:53,530
in a frequentness or Bayesian way.

486
00:25:53,980 --> 00:25:56,776
There's going to be some uncertainty in

487
00:25:56,798 --> 00:26:00,120
the model and especially if you start

488
00:26:00,190 --> 00:26:03,610
connecting it up to a larger system

489
00:26:04,940 --> 00:26:08,332
doing the same. Kind of like is the

490
00:26:08,466 --> 00:26:12,604
jitter in that sensor the same when

491
00:26:12,642 --> 00:26:14,876
it's completely isolated as it is when

492
00:26:14,898 --> 00:26:19,088
it's on a six x nine PCB board

493
00:26:19,174 --> 00:26:22,972
with 12 volts

494
00:26:23,036 --> 00:26:26,176
on one side and an oscillator next to

495
00:26:26,198 --> 00:26:26,770
that.

496
00:26:30,020 --> 00:26:33,508
Is it the same thing? You have to

497
00:26:33,514 --> 00:26:35,120
do these sort of electromagnetic

498
00:26:35,200 --> 00:26:37,776
compatibility resonance sort of testing

499
00:26:37,808 --> 00:26:38,550
on it.

500
00:26:41,640 --> 00:26:44,356
In practice it's Bayesian like you're

501
00:26:44,388 --> 00:26:48,890
doing the same sort of bounding and

502
00:26:50,380 --> 00:26:53,850
belief testing and

503
00:26:54,380 --> 00:26:57,710
finding your errors and correcting them.

504
00:26:58,640 --> 00:27:00,060
Even in practice.

505
00:27:02,560 --> 00:27:05,230
I don't see how you could possibly have,

506
00:27:06,480 --> 00:27:09,424
even physically a system that could

507
00:27:09,462 --> 00:27:12,256
exist where you had a noise model that

508
00:27:12,278 --> 00:27:13,888
you could write down that was actually

509
00:27:13,974 --> 00:27:17,570
the noise model under all conditions or

510
00:27:19,460 --> 00:27:20,210
yeah,

511
00:27:23,140 --> 00:27:26,548
it doesn't seem like a approachable. I

512
00:27:26,554 --> 00:27:28,980
don't know. It's not even not tractable,

513
00:27:29,400 --> 00:27:31,510
but just a wrong thing to do.

514
00:27:33,340 --> 00:27:36,872
Thanks, Brock. In the

515
00:27:36,926 --> 00:27:39,124
case of engineering and designing

516
00:27:39,172 --> 00:27:42,600
models, it's up to the designer or the

517
00:27:42,670 --> 00:27:45,256
engineering team to understand what is

518
00:27:45,278 --> 00:27:47,900
adequate or not. And for natural systems

519
00:27:48,480 --> 00:27:51,096
outside of their bounds, natural

520
00:27:51,128 --> 00:27:53,710
selection sweeps them off the table. So

521
00:27:54,320 --> 00:27:56,364
some of the adequacy questions are

522
00:27:56,402 --> 00:28:00,432
either based upon specified or

523
00:28:00,486 --> 00:28:04,384
implicit human standards or the

524
00:28:04,422 --> 00:28:08,352
failure to resist dissipation and

525
00:28:08,406 --> 00:28:10,012
therefore the failure to realize

526
00:28:10,076 --> 00:28:12,996
repeated measurement of oneself or from

527
00:28:13,018 --> 00:28:16,436
the external. And then that system is no

528
00:28:16,458 --> 00:28:19,444
longer let's just continue to move

529
00:28:19,482 --> 00:28:22,196
through some of the questions. Yes,

530
00:28:22,218 --> 00:28:23,510
Rohan. Go for it.

531
00:28:25,420 --> 00:28:28,884
Yeah, I completely agree with what Brock

532
00:28:28,932 --> 00:28:33,032
said. Right. But my point was,

533
00:28:33,166 --> 00:28:35,376
when you say that you're predicting

534
00:28:35,428 --> 00:28:39,564
something, it means that you either know

535
00:28:39,602 --> 00:28:41,372
the quantity that you're trying to

536
00:28:41,426 --> 00:28:44,684
predict and

537
00:28:44,722 --> 00:28:48,140
it corresponds to so in this case, some

538
00:28:48,210 --> 00:28:51,196
action results in some reward. I guess

539
00:28:51,298 --> 00:28:53,104
you could predict the reward, but

540
00:28:53,142 --> 00:28:54,656
without knowing how wrong you're going

541
00:28:54,678 --> 00:28:58,690
to be in the future. So where exactly

542
00:28:59,860 --> 00:29:03,128
any living system would have to be aware

543
00:29:03,164 --> 00:29:05,748
of its own bounds. Right? That's what I

544
00:29:05,914 --> 00:29:08,964
meant by noise model. So it has to know

545
00:29:09,002 --> 00:29:11,120
that it has to stay within some bounds

546
00:29:11,200 --> 00:29:14,456
and as it starts coming closer to one of

547
00:29:14,478 --> 00:29:16,216
the upper or lower bounds, there should

548
00:29:16,238 --> 00:29:20,090
be something like pulling it back down

549
00:29:20,540 --> 00:29:23,224
or some feedback mechanism. But that is

550
00:29:23,262 --> 00:29:26,396
not very clear from what I've read so

551
00:29:26,418 --> 00:29:30,412
far on active inferences, how exactly

552
00:29:30,466 --> 00:29:33,132
it would estimate that. You can just say

553
00:29:33,186 --> 00:29:36,636
free energy minimization. But what

554
00:29:36,658 --> 00:29:39,356
is a minimum in this case that would be

555
00:29:39,378 --> 00:29:41,004
different for different systems? It

556
00:29:41,042 --> 00:29:42,592
should have some idea of where the

557
00:29:42,646 --> 00:29:44,476
surface is of whichever local surface

558
00:29:44,508 --> 00:29:47,936
it's lying on. That was my point. So for

559
00:29:47,958 --> 00:29:48,944
sure it's different for different

560
00:29:48,982 --> 00:29:51,012
systems. It's different as every single

561
00:29:51,066 --> 00:29:54,116
generative model is. And then you

562
00:29:54,138 --> 00:29:57,264
mentioned like a pullback attractor.

563
00:29:57,392 --> 00:29:59,872
And so this has been treated extensively

564
00:30:00,016 --> 00:30:01,824
in the context of physiological

565
00:30:01,872 --> 00:30:03,864
measurements and then again where those

566
00:30:03,902 --> 00:30:05,924
priors on the physiological measurements

567
00:30:05,972 --> 00:30:08,520
are, the Tolerable range are either

568
00:30:08,590 --> 00:30:10,888
provided by the human engineering team

569
00:30:11,054 --> 00:30:13,396
or through evolution by natural

570
00:30:13,428 --> 00:30:18,120
selection and then the quantity.

571
00:30:18,640 --> 00:30:21,132
Another related topic would be

572
00:30:21,186 --> 00:30:23,884
expectation maximization models, which

573
00:30:23,922 --> 00:30:25,788
is what is happening essentially in the

574
00:30:25,794 --> 00:30:29,312
dialectic between the observation and

575
00:30:29,366 --> 00:30:31,856
the hidden state update is an

576
00:30:31,878 --> 00:30:33,984
expectation maximization like process

577
00:30:34,102 --> 00:30:36,480
where it's like given the hyper priors

578
00:30:36,820 --> 00:30:38,608
or we can just simplify, given the

579
00:30:38,614 --> 00:30:41,636
priors on S, what are the most likely or

580
00:30:41,658 --> 00:30:44,084
the distribution of observables. And

581
00:30:44,122 --> 00:30:46,676
then given the observables, what should

582
00:30:46,698 --> 00:30:49,300
be updated about the hidden state

583
00:30:49,370 --> 00:30:51,812
priors. And when those are at

584
00:30:51,866 --> 00:30:54,056
convergence, the model is stationary and

585
00:30:54,078 --> 00:30:56,808
then as things change, either from the

586
00:30:56,814 --> 00:30:59,752
top via learning or context shift or

587
00:30:59,806 --> 00:31:02,616
through changed measurements, the

588
00:31:02,638 --> 00:31:05,604
expectation maximization algorithm is

589
00:31:05,662 --> 00:31:07,836
just able to track those changes. And

590
00:31:07,858 --> 00:31:09,976
that can, in the context of a fixed

591
00:31:10,008 --> 00:31:12,376
prior, have the function of a pullback

592
00:31:12,408 --> 00:31:15,724
attractor. Okay,

593
00:31:15,842 --> 00:31:17,452
and that's even before getting into

594
00:31:17,506 --> 00:31:19,330
action specifically. But yes,

595
00:31:19,940 --> 00:31:22,064
definitely considering the role of

596
00:31:22,102 --> 00:31:25,424
comparing future plans of action and

597
00:31:25,462 --> 00:31:27,440
which plans of actions will have the

598
00:31:27,510 --> 00:31:30,492
least expected free energy based upon

599
00:31:30,636 --> 00:31:34,448
their pragmatic and Epistemic value.

600
00:31:34,614 --> 00:31:37,824
Like the Epistemic just being clarity

601
00:31:37,872 --> 00:31:39,476
around how it's going to be achieved and

602
00:31:39,498 --> 00:31:41,216
the pragmatic value being that KL

603
00:31:41,248 --> 00:31:43,904
divergence between the expectation slash

604
00:31:43,952 --> 00:31:46,016
preferences and the expected

605
00:31:46,048 --> 00:31:48,428
observations. So if we expect slash

606
00:31:48,464 --> 00:31:51,384
preferred to be in homeostasis, the

607
00:31:51,422 --> 00:31:54,264
pragmatic value is going to come or be

608
00:31:54,302 --> 00:31:56,748
loaded onto policies that keep us in

609
00:31:56,754 --> 00:31:59,116
homeostasis, even if some other one is

610
00:31:59,138 --> 00:32:01,804
super informative. How that model gets

611
00:32:01,842 --> 00:32:03,948
tuned is quite literally the details of

612
00:32:03,954 --> 00:32:06,988
how the model is trained. Right.

613
00:32:07,154 --> 00:32:08,984
My point was that active inference

614
00:32:09,032 --> 00:32:10,496
doesn't help us find this. We have to

615
00:32:10,518 --> 00:32:12,860
find this ourselves and then hopefully

616
00:32:12,940 --> 00:32:15,036
once we have instantiated these bounds,

617
00:32:15,068 --> 00:32:17,152
I think active inference works very well

618
00:32:17,286 --> 00:32:20,124
to keep it within this. If you can't

619
00:32:20,172 --> 00:32:23,660
discover this de novo, I can't

620
00:32:23,740 --> 00:32:27,216
have what you'd say table or asset

621
00:32:27,248 --> 00:32:29,012
type system, like a blank slate system,

622
00:32:29,066 --> 00:32:30,964
go out in the world and discover your

623
00:32:31,002 --> 00:32:33,704
bounds. That's not going to happen. But

624
00:32:33,742 --> 00:32:36,712
this kind of method is what I was making

625
00:32:36,766 --> 00:32:40,600
it out. Whereas taking something

626
00:32:40,670 --> 00:32:43,960
equivalent without any so deep learning

627
00:32:44,030 --> 00:32:47,256
is another method. Right? So you

628
00:32:47,278 --> 00:32:48,868
just feed it a lot of data, it doesn't

629
00:32:48,884 --> 00:32:50,172
have to know anything about the data.

630
00:32:50,226 --> 00:32:52,204
Eventually it forms some opinion about

631
00:32:52,242 --> 00:32:53,916
the data it's seen and then it's able to

632
00:32:53,938 --> 00:32:56,476
do some classification depending on how

633
00:32:56,498 --> 00:32:58,576
much data it's seen. So it's able to

634
00:32:58,598 --> 00:33:01,040
build certain amount of bounds. Whether

635
00:33:01,110 --> 00:33:03,456
that's generalizable is a different

636
00:33:03,558 --> 00:33:06,912
question. But here the

637
00:33:06,966 --> 00:33:10,364
learning part does not say much about

638
00:33:10,422 --> 00:33:12,740
how exactly it goes about discovering

639
00:33:13,800 --> 00:33:16,356
where its limits are. That's my point.

640
00:33:16,538 --> 00:33:18,900
Sure. Thank you. Eric.

641
00:33:22,280 --> 00:33:24,664
I would love to dive just a little bit

642
00:33:24,702 --> 00:33:27,624
more into the thing you were just

643
00:33:27,662 --> 00:33:29,380
talking about, how expectation

644
00:33:29,460 --> 00:33:33,032
maximization is a

645
00:33:33,086 --> 00:33:36,684
mathematical formulation for how

646
00:33:36,722 --> 00:33:39,772
to infer belief states from

647
00:33:39,826 --> 00:33:44,110
observations. Because you basically

648
00:33:44,800 --> 00:33:46,812
go through this iterative process of

649
00:33:46,866 --> 00:33:48,716
trying to find alignment between the

650
00:33:48,738 --> 00:33:51,036
states of the belief and the observables

651
00:33:51,068 --> 00:33:52,930
and then you contract that over time

652
00:33:53,300 --> 00:33:56,544
between that concept and the concept of

653
00:33:56,582 --> 00:33:59,280
message passing. When I think of message

654
00:33:59,350 --> 00:34:01,556
passing, I think of some sort of a

655
00:34:01,578 --> 00:34:03,444
distributed system where you have some

656
00:34:03,482 --> 00:34:06,036
local computation and then it sends and

657
00:34:06,058 --> 00:34:08,276
you have again either synchronous or

658
00:34:08,378 --> 00:34:11,552
asynchronous process for communicating

659
00:34:11,616 --> 00:34:14,996
between these kind of different centers

660
00:34:15,028 --> 00:34:18,344
of belief state.

661
00:34:18,542 --> 00:34:21,076
And then again, through message passing,

662
00:34:21,108 --> 00:34:23,224
you have often an iterative process of

663
00:34:23,262 --> 00:34:26,744
convergence. So those are not the same

664
00:34:26,782 --> 00:34:29,436
thing. And I think message passing, to

665
00:34:29,458 --> 00:34:31,628
my understanding, would be an Em

666
00:34:31,714 --> 00:34:34,396
algorithm under certain types of

667
00:34:34,418 --> 00:34:36,616
messages and states at each of the nodes

668
00:34:36,648 --> 00:34:38,444
that you're sending the messages around.

669
00:34:38,642 --> 00:34:40,976
But not all message passing is going to

670
00:34:40,998 --> 00:34:44,016
be an Em algorithm. And similarly, when

671
00:34:44,038 --> 00:34:45,424
I think of Em, I don't think about

672
00:34:45,462 --> 00:34:47,616
message passing. I think about making an

673
00:34:47,638 --> 00:34:51,392
arrays of belief

674
00:34:51,456 --> 00:34:54,068
and matrices between mapping between

675
00:34:54,154 --> 00:34:58,532
belief and observations and then trying

676
00:34:58,586 --> 00:35:02,036
to compute an expectation for what my

677
00:35:02,058 --> 00:35:03,844
predictions would be and then making

678
00:35:03,882 --> 00:35:05,288
this iterative convergence. So I don't

679
00:35:05,294 --> 00:35:06,232
think of that in terms of message

680
00:35:06,286 --> 00:35:08,456
passing. So I just love to hear other

681
00:35:08,478 --> 00:35:10,344
people's ideas about how to tie these

682
00:35:10,382 --> 00:35:12,410
two ideas together.

683
00:35:13,740 --> 00:35:17,256
Yeah, Brock, if your

684
00:35:17,278 --> 00:35:20,910
hand is still raised. Okay,

685
00:35:21,440 --> 00:35:23,470
sorry. Yeah, all good.

686
00:35:26,800 --> 00:35:29,090
Does anyone have thoughts on this?

687
00:35:30,420 --> 00:35:32,850
There's a lot to say about the

688
00:35:33,540 --> 00:35:36,172
expectation maximization algorithms.

689
00:35:36,316 --> 00:35:38,240
And just to kind of give a little

690
00:35:38,310 --> 00:35:41,990
context, this is the step by step paper.

691
00:35:42,680 --> 00:35:46,260
So this is the model stream one,

692
00:35:46,330 --> 00:35:50,212
it's four parts. And this is like really

693
00:35:50,346 --> 00:35:52,356
a quite relevant figure, which is not in

694
00:35:52,378 --> 00:35:54,840
the textbook but helps a lot.

695
00:35:54,990 --> 00:35:57,816
Like this is the essence of the

696
00:35:57,838 --> 00:36:00,932
partially observable Bayesian inference.

697
00:36:00,996 --> 00:36:03,880
There's a prior D and then there's an a

698
00:36:03,950 --> 00:36:07,804
ambiguity matrix that is mapping in

699
00:36:07,842 --> 00:36:09,496
a generative capacity between hidden

700
00:36:09,528 --> 00:36:11,916
states s and observation measurements O.

701
00:36:12,018 --> 00:36:14,008
This is the tail of two densities

702
00:36:14,184 --> 00:36:17,688
because O through A

703
00:36:17,874 --> 00:36:21,456
can give you the most likely S and S

704
00:36:21,638 --> 00:36:23,884
through A can give you the most likely

705
00:36:23,932 --> 00:36:27,532
O. And it turns out that by alternating

706
00:36:27,596 --> 00:36:31,264
those procedures in a Bayesian update

707
00:36:31,312 --> 00:36:34,692
context, that two stroke engine

708
00:36:34,746 --> 00:36:36,720
is called expectation maximization.

709
00:36:36,880 --> 00:36:40,116
Again, because given expectations on the

710
00:36:40,138 --> 00:36:41,968
summary statistics of A distribution,

711
00:36:42,144 --> 00:36:44,536
the expected sensory observations can be

712
00:36:44,558 --> 00:36:47,768
generated and then given those

713
00:36:47,854 --> 00:36:50,616
observations, a likelihood function can

714
00:36:50,638 --> 00:36:53,544
be maximized that then updates the

715
00:36:53,582 --> 00:36:56,748
hidden states. So this motif, this kind

716
00:36:56,754 --> 00:37:00,504
of elbow motif is then extended

717
00:37:00,552 --> 00:37:03,996
into a caterpillar with this be, which

718
00:37:04,018 --> 00:37:05,724
is how the hidden state changes through

719
00:37:05,762 --> 00:37:08,684
time. So importantly, note what's not

720
00:37:08,722 --> 00:37:10,620
there, which is like the observations

721
00:37:10,700 --> 00:37:13,408
being chained through time. So it's not

722
00:37:13,414 --> 00:37:14,736
that the temperature reading at one

723
00:37:14,758 --> 00:37:16,044
moment influences the temperature

724
00:37:16,092 --> 00:37:17,536
reading at the next moment. The

725
00:37:17,558 --> 00:37:19,372
temperature readings are continually

726
00:37:19,436 --> 00:37:23,220
linked, synchronously like in one

727
00:37:23,290 --> 00:37:27,030
time slice to their hidden states.

728
00:37:27,400 --> 00:37:30,196
So then when action gets into the

729
00:37:30,218 --> 00:37:32,416
picture, we've talked about like several

730
00:37:32,448 --> 00:37:34,536
sources that's like the equation 2.5 to

731
00:37:34,558 --> 00:37:37,592
equation 2.6 phase change, because

732
00:37:37,646 --> 00:37:39,656
action brings in several kinds of

733
00:37:39,678 --> 00:37:42,260
uncertainty. First off, you're reducing

734
00:37:42,340 --> 00:37:44,884
your uncertainty about observations

735
00:37:44,932 --> 00:37:48,044
which haven't happened yet. You're also

736
00:37:48,242 --> 00:37:50,636
having the unknown consequences of your

737
00:37:50,658 --> 00:37:53,464
actions. And in order to have action

738
00:37:53,512 --> 00:37:56,156
selection that's relevant, one has to

739
00:37:56,178 --> 00:37:59,296
have a preference distribution over

740
00:37:59,398 --> 00:38:01,344
what kinds of observations they would

741
00:38:01,382 --> 00:38:03,152
like to be seeing so that action can

742
00:38:03,206 --> 00:38:08,352
guide it in that direction. Okay then.

743
00:38:08,486 --> 00:38:11,392
So that's the expectation maximization

744
00:38:11,536 --> 00:38:14,710
two stroke engine is basically

745
00:38:15,400 --> 00:38:19,184
graphically proposed

746
00:38:19,232 --> 00:38:21,796
here. D is just the initiating set of

747
00:38:21,818 --> 00:38:25,172
parameters and then in a one shot way,

748
00:38:25,226 --> 00:38:27,040
like you can have a folder with a

749
00:38:27,050 --> 00:38:28,964
thousand images and do expectation

750
00:38:29,012 --> 00:38:31,720
maximization on it, or you could have

751
00:38:31,790 --> 00:38:33,896
something that's dynamical and do that

752
00:38:33,918 --> 00:38:35,724
type of expectation maximization through

753
00:38:35,762 --> 00:38:36,350
time.

754
00:38:41,280 --> 00:38:44,988
Eric asked how is it related to message

755
00:38:45,074 --> 00:38:49,004
passing on graphs? So I'm

756
00:38:49,052 --> 00:38:50,944
not familiar with all the details of

757
00:38:50,982 --> 00:38:54,752
this model, but you basically said

758
00:38:54,806 --> 00:38:57,440
it, which was that certain message

759
00:38:57,510 --> 00:39:01,204
passing systems implement Em.

760
00:39:01,402 --> 00:39:03,504
But of course not all message passing

761
00:39:03,552 --> 00:39:05,492
architectures implement Em. So message

762
00:39:05,546 --> 00:39:08,580
passing is more general than Em. Also,

763
00:39:08,650 --> 00:39:11,952
Em wouldn't have to be implemented

764
00:39:12,016 --> 00:39:14,436
through message passing, but there's

765
00:39:14,468 --> 00:39:17,864
like an area of intersection where the

766
00:39:17,902 --> 00:39:21,272
Em algorithm can be seen as a type

767
00:39:21,326 --> 00:39:25,300
of message passing under certain

768
00:39:25,390 --> 00:39:28,540
compute rules in a factor graph.

769
00:39:29,120 --> 00:39:31,964
And Em, at least from here, may be used

770
00:39:32,002 --> 00:39:34,060
to break cycles in a factor graph.

771
00:39:35,760 --> 00:39:39,136
So it's always like how close to the

772
00:39:39,158 --> 00:39:42,588
kernel and the Platonic ideal of active

773
00:39:42,604 --> 00:39:45,584
inference are we talking? And then how

774
00:39:45,622 --> 00:39:50,804
many heuristics and just ways

775
00:39:50,842 --> 00:39:52,870
of connecting things are possible.

776
00:39:53,640 --> 00:39:57,156
So I

777
00:39:57,178 --> 00:39:58,980
will now read the chat.

778
00:40:06,610 --> 00:40:09,006
That'S from previous context. Okay,

779
00:40:09,108 --> 00:40:10,000
thank you.

780
00:40:13,750 --> 00:40:15,934
The statistical Parametric mapping

781
00:40:16,062 --> 00:40:19,542
textbook and documentation. SPM has

782
00:40:19,596 --> 00:40:23,670
a lot on Em variational inference

783
00:40:24,010 --> 00:40:28,280
and a lot of the parts that bring one

784
00:40:29,290 --> 00:40:32,602
to understand this a lot

785
00:40:32,656 --> 00:40:35,546
better. SPM is like almost like it

786
00:40:35,568 --> 00:40:38,442
doesn't include action that much because

787
00:40:38,496 --> 00:40:40,582
it's a neuroimaging. So the observations

788
00:40:40,646 --> 00:40:42,630
are neuroimaging, sensor fusion,

789
00:40:42,790 --> 00:40:44,926
different error modalities, like the

790
00:40:44,948 --> 00:40:48,558
whole monopoly. And then

791
00:40:48,644 --> 00:40:52,778
SPM started to incorporate participant

792
00:40:52,874 --> 00:40:56,522
actions in this partially observable

793
00:40:56,666 --> 00:41:00,218
metabasian way and potentially

794
00:41:00,394 --> 00:41:03,550
that is what led Friston and colleagues

795
00:41:03,630 --> 00:41:06,446
towards a grander synthesis of inference

796
00:41:06,478 --> 00:41:10,374
and action under a

797
00:41:10,412 --> 00:41:12,390
statistically principled framework.

798
00:41:16,090 --> 00:41:17,558
Let's just see if there's any other

799
00:41:17,644 --> 00:41:20,822
these are all important questions and

800
00:41:20,876 --> 00:41:22,538
there's a lot to get up to them and then

801
00:41:22,544 --> 00:41:24,586
there's like a lot to go from the

802
00:41:24,608 --> 00:41:26,090
question, but these are all important

803
00:41:26,160 --> 00:41:33,194
things to raise if

804
00:41:33,232 --> 00:41:35,110
anyone has a thought on this. How should

805
00:41:35,120 --> 00:41:36,906
we think about redundancy in neural

806
00:41:36,938 --> 00:41:38,426
systems in the context of active

807
00:41:38,458 --> 00:41:41,374
inference? So redundancy would be like

808
00:41:41,412 --> 00:41:42,478
if there's something that's playing a

809
00:41:42,484 --> 00:41:46,174
functional role but its removal is

810
00:41:46,292 --> 00:41:49,714
not damaging the function of the system.

811
00:41:49,912 --> 00:41:52,546
There's ten pillars knocking out one of

812
00:41:52,568 --> 00:41:55,198
them, the building stays up, so there's

813
00:41:55,214 --> 00:41:57,474
redundancy in the pillars. What about

814
00:41:57,512 --> 00:41:58,970
the cases where the same neural circuits

815
00:41:58,990 --> 00:42:00,550
serve multiple functions?

816
00:42:02,010 --> 00:42:04,086
So how do we deal with the fact that

817
00:42:04,108 --> 00:42:05,810
there's like a many to many potentially

818
00:42:05,890 --> 00:42:08,710
or a complex mapping between system

819
00:42:08,780 --> 00:42:12,614
elements and system functions where

820
00:42:12,652 --> 00:42:14,266
sometimes removing node one does

821
00:42:14,288 --> 00:42:16,458
nothing, but removing node one and two

822
00:42:16,624 --> 00:42:18,534
does a lot of things, but then removing

823
00:42:18,582 --> 00:42:19,994
one, two and three and it's going back

824
00:42:20,032 --> 00:42:22,090
to being fine. Jessica.

825
00:42:24,990 --> 00:42:28,014
I guess my comment is more like a

826
00:42:28,052 --> 00:42:30,174
question and maybe like ask Blue about

827
00:42:30,212 --> 00:42:33,902
it because this is what I feel. But this

828
00:42:33,956 --> 00:42:36,206
question made me think. About. It's not

829
00:42:36,228 --> 00:42:38,494
only like that the brain has this

830
00:42:38,532 --> 00:42:42,586
redundancy of things, but it's

831
00:42:42,618 --> 00:42:44,546
like if you have some kind of damage to

832
00:42:44,568 --> 00:42:47,214
your brain, another part of your brain

833
00:42:47,262 --> 00:42:50,726
that maybe was not used for something

834
00:42:50,828 --> 00:42:54,134
would develop that capacity and so

835
00:42:54,172 --> 00:42:56,950
it would create that ability.

836
00:42:58,250 --> 00:43:01,746
Neuroplasticity brand

837
00:43:01,778 --> 00:43:03,782
new even though it was not a redundancy

838
00:43:03,846 --> 00:43:06,154
to begin with. So I guess I'm more

839
00:43:06,192 --> 00:43:08,874
curious to understand this. So other

840
00:43:08,912 --> 00:43:11,660
people who know more like Blue and stuff

841
00:43:12,510 --> 00:43:15,326
about this. Basically that's kind of

842
00:43:15,348 --> 00:43:17,310
what this question made me think about

843
00:43:17,460 --> 00:43:19,840
and kind of wanting to understand more.

844
00:43:22,050 --> 00:43:24,714
Nice. Good question. Yeah. Blue wrote,

845
00:43:24,762 --> 00:43:26,560
Depends on your age. Different

846
00:43:26,870 --> 00:43:28,754
neuroplasticity mechanisms are

847
00:43:28,792 --> 00:43:31,778
differentially available throughout life

848
00:43:31,944 --> 00:43:33,890
for different organisms.

849
00:43:35,430 --> 00:43:38,322
Just one thought on this would be like

850
00:43:38,456 --> 00:43:39,986
the same neural circuits serving

851
00:43:40,018 --> 00:43:42,406
multiple functions might be totally the

852
00:43:42,428 --> 00:43:46,566
case. For example, and is

853
00:43:46,588 --> 00:43:48,662
the function of the heart to provide one

854
00:43:48,716 --> 00:43:52,042
pound of weight to the torso. It is a

855
00:43:52,096 --> 00:43:55,494
function of the heart. So identifying

856
00:43:55,542 --> 00:43:58,246
what function is being modeled is what's

857
00:43:58,278 --> 00:44:00,186
being done here. So let's just go to the

858
00:44:00,208 --> 00:44:03,180
example of figure five three. This

859
00:44:03,970 --> 00:44:07,294
neuron modifying it

860
00:44:07,332 --> 00:44:09,934
like a loss of function experiment or

861
00:44:10,052 --> 00:44:13,070
having it injured might influence more

862
00:44:13,140 --> 00:44:16,518
things than just the lower motor neuron

863
00:44:16,554 --> 00:44:18,914
descending message. So that'd be a case

864
00:44:18,952 --> 00:44:21,330
where it serves multiple functions.

865
00:44:25,810 --> 00:44:30,080
This is just one statistical model of

866
00:44:31,190 --> 00:44:34,562
this function, of this circuit. And that

867
00:44:34,616 --> 00:44:38,494
returns to the earliest comments of Eric

868
00:44:38,542 --> 00:44:40,642
with like, okay, this is like

869
00:44:40,696 --> 00:44:43,858
Tantalizingly seeming like it's actually

870
00:44:43,944 --> 00:44:46,242
going to be describing the neuroanatomy.

871
00:44:46,386 --> 00:44:48,806
But then they say things like note the

872
00:44:48,828 --> 00:44:51,350
absence of dot dot dot dot dot dot dot.

873
00:44:51,930 --> 00:44:53,590
Note that there's a discrepancy between,

874
00:44:53,660 --> 00:44:56,930
for example, the reality of the anatomy

875
00:44:57,090 --> 00:45:00,538
and the base graph. This highlights that

876
00:45:00,544 --> 00:45:01,882
the connections implied by message

877
00:45:01,936 --> 00:45:03,754
passing schemes may not manifest as

878
00:45:03,792 --> 00:45:04,970
single synapses.

879
00:45:07,910 --> 00:45:10,200
So, okay,

880
00:45:10,810 --> 00:45:12,278
there's kind of false positives and

881
00:45:12,284 --> 00:45:14,914
false negatives. The Bayes graph isn't

882
00:45:14,962 --> 00:45:17,960
just the anatomy. That's fine.

883
00:45:18,570 --> 00:45:20,278
The linear regression between height and

884
00:45:20,284 --> 00:45:21,570
weight isn't the actual relationship

885
00:45:21,660 --> 00:45:23,786
between height and weight. And the

886
00:45:23,808 --> 00:45:27,318
structural equation model of inequality

887
00:45:27,414 --> 00:45:30,566
is not the generator of inequality.

888
00:45:30,758 --> 00:45:33,642
So it's totally fair and shouldn't be

889
00:45:33,696 --> 00:45:36,506
expected to be otherwise. That the Bayes

890
00:45:36,538 --> 00:45:38,490
graph, the best fitting Bayes graph,

891
00:45:38,570 --> 00:45:40,078
the most didactic bays graph, the

892
00:45:40,084 --> 00:45:42,590
simplest Bayes graph, none of those

893
00:45:42,660 --> 00:45:44,986
recapitulate the anatomy. And the Bayes

894
00:45:45,018 --> 00:45:47,078
graph that recapitulates the anatomy

895
00:45:47,274 --> 00:45:49,522
would not necessarily even be the best

896
00:45:49,576 --> 00:45:52,766
fitting. It's map and territory.

897
00:45:52,958 --> 00:45:56,014
And if somebody has a special equation

898
00:45:56,062 --> 00:45:58,850
to break through that blanket,

899
00:45:59,450 --> 00:46:01,574
everybody would love to see it.

900
00:46:01,772 --> 00:46:05,410
However, sometimes it's easy to gloss

901
00:46:05,490 --> 00:46:09,014
over that in

902
00:46:09,052 --> 00:46:12,082
principle challenge of mapping

903
00:46:12,146 --> 00:46:14,482
formalisms to biological systems.

904
00:46:14,626 --> 00:46:16,938
Because we see the cell and then the

905
00:46:16,944 --> 00:46:18,746
blanket is the membrane and we see the

906
00:46:18,768 --> 00:46:20,614
brain and it's like vision and action.

907
00:46:20,662 --> 00:46:24,766
And it seems like it maps onto the

908
00:46:24,788 --> 00:46:26,686
physical or the anatomical structure of

909
00:46:26,708 --> 00:46:28,446
the world or the causal structure of the

910
00:46:28,468 --> 00:46:32,526
world, and it

911
00:46:32,628 --> 00:46:36,446
doesn't. So I hope people can add more

912
00:46:36,468 --> 00:46:37,938
thoughts about how we consider

913
00:46:38,024 --> 00:46:39,906
redundancy because it's a great

914
00:46:40,088 --> 00:46:40,820
question,

915
00:46:44,470 --> 00:46:46,020
but we'll leave it there.

916
00:46:48,310 --> 00:46:51,506
How are reflexes modeled in active

917
00:46:51,538 --> 00:46:54,546
inference? Would they exist in an active

918
00:46:54,578 --> 00:46:56,166
inference module that is distinct from

919
00:46:56,188 --> 00:46:58,454
proprioception? Would they operate on

920
00:46:58,492 --> 00:47:01,666
different timescales? If anybody wants

921
00:47:01,708 --> 00:47:05,754
to add some

922
00:47:05,792 --> 00:47:07,180
context on it,

923
00:47:14,470 --> 00:47:20,422
a reflex arc relating to where

924
00:47:20,476 --> 00:47:24,246
reflex is defined as a

925
00:47:24,268 --> 00:47:26,966
function that is being relayed through

926
00:47:26,988 --> 00:47:29,910
the spinal cord of a mammal and not

927
00:47:29,980 --> 00:47:31,494
passing. Like you could have a nerve

928
00:47:31,542 --> 00:47:34,346
block in the cervical vertebra and it

929
00:47:34,368 --> 00:47:36,794
still is able to implement that's one

930
00:47:36,832 --> 00:47:39,402
definition of reflex. Often it's also

931
00:47:39,456 --> 00:47:42,140
used slightly more broadly to mean like

932
00:47:42,690 --> 00:47:45,994
stimulus action, reproducible outcomes,

933
00:47:46,122 --> 00:47:47,854
but not every reflex has a perfect

934
00:47:47,892 --> 00:47:49,818
reproducibility, et cetera, et cetera.

935
00:47:49,994 --> 00:47:52,974
So it's kind of a continuum. Yes,

936
00:47:53,092 --> 00:47:56,690
it would involve proprioceptive input.

937
00:47:57,350 --> 00:48:00,946
That proprioceptive input would be

938
00:48:01,048 --> 00:48:06,206
combined or juxtaposed

939
00:48:06,318 --> 00:48:08,150
with the descending prediction,

940
00:48:09,370 --> 00:48:13,174
resulting in an error. And then that

941
00:48:13,292 --> 00:48:14,886
directionality and magnitude of the

942
00:48:14,908 --> 00:48:17,910
error in this model drives the reflex.

943
00:48:19,450 --> 00:48:21,686
There can also be multiple timescales,

944
00:48:21,718 --> 00:48:24,262
but we haven't seen any nested modeling

945
00:48:24,326 --> 00:48:27,562
yet. This is just the one

946
00:48:27,616 --> 00:48:28,700
layer model.

947
00:48:33,490 --> 00:48:35,422
The schematic on the left in figure

948
00:48:35,476 --> 00:48:38,206
five, one shows that layer five of

949
00:48:38,228 --> 00:48:40,154
cortex projects to spinal parameter

950
00:48:40,202 --> 00:48:42,174
neurons. And this can be interpreted as

951
00:48:42,212 --> 00:48:45,746
a prediction. Okay,

952
00:48:45,848 --> 00:48:54,012
so this is this one helpful

953
00:48:54,156 --> 00:48:58,092
context, and it relates

954
00:48:58,156 --> 00:49:00,740
to this earlier highlighted section,

955
00:49:01,320 --> 00:49:05,940
which is like it's the interpretation,

956
00:49:06,520 --> 00:49:09,860
the interpretive link between some

957
00:49:09,930 --> 00:49:13,620
anatomical or biological feature

958
00:49:13,700 --> 00:49:17,480
phenotype and some parametric

959
00:49:19,020 --> 00:49:20,570
resonance with a model.

960
00:49:23,530 --> 00:49:27,162
The validity of that ranges from pretty

961
00:49:27,216 --> 00:49:29,914
clear, pretty uncontroversial, pretty

962
00:49:29,952 --> 00:49:31,930
useful, pretty effective,

963
00:49:33,230 --> 00:49:35,930
to none of the above.

964
00:49:37,250 --> 00:49:40,766
And it's hard to know what given

965
00:49:40,868 --> 00:49:44,400
interpretive links are doing.

966
00:49:45,250 --> 00:49:47,582
Like saying that this edge on this base

967
00:49:47,636 --> 00:49:50,198
graph can be interpreted as a descending

968
00:49:50,234 --> 00:49:53,586
excitatory connection or that

969
00:49:53,608 --> 00:49:55,714
might be used to generate specific

970
00:49:55,832 --> 00:49:58,526
testable hypotheses. Like if we measured

971
00:49:58,558 --> 00:50:00,786
it during a period of excitation the

972
00:50:00,808 --> 00:50:02,678
activity of this neuron, we expect it to

973
00:50:02,684 --> 00:50:04,678
be increased, whereas if it was a

974
00:50:04,684 --> 00:50:07,058
descending inhibitory connection, et

975
00:50:07,074 --> 00:50:11,638
cetera. That's a great example of

976
00:50:11,804 --> 00:50:13,798
active inference being used in in a

977
00:50:13,804 --> 00:50:16,582
proactive way to generate hypotheses

978
00:50:16,646 --> 00:50:19,674
about biological systems that are going

979
00:50:19,712 --> 00:50:21,770
to be, as they said in the preface,

980
00:50:25,070 --> 00:50:27,260
answerable to measured data.

981
00:50:29,570 --> 00:50:33,242
By grounding in a computational

982
00:50:33,306 --> 00:50:35,838
model that is related to the

983
00:50:35,844 --> 00:50:37,518
neuroanatomy but isn't trying to be like

984
00:50:37,524 --> 00:50:40,206
a digital twin of the neuroanatomy. We

985
00:50:40,228 --> 00:50:42,542
can generate predictions about gain,

986
00:50:42,606 --> 00:50:44,174
loss of function, different measurements

987
00:50:44,222 --> 00:50:46,050
to make if they don't already exist.

988
00:50:46,470 --> 00:50:48,894
And then that can be used to increase

989
00:50:48,942 --> 00:50:52,650
our confidence or reliability or falsify

990
00:50:52,750 --> 00:50:55,750
even a specific generative model as

991
00:50:55,820 --> 00:50:59,814
proposed. So there's so much

992
00:51:00,012 --> 00:51:02,694
discourse like continuing to the present

993
00:51:02,732 --> 00:51:05,554
day and surely beyond, as like all these

994
00:51:05,612 --> 00:51:07,386
questions and recently posted in the

995
00:51:07,408 --> 00:51:11,078
Discord about active

996
00:51:11,094 --> 00:51:12,886
inference or free energy principle isn't

997
00:51:12,918 --> 00:51:16,938
falsifiable. Now, the funny link is

998
00:51:17,024 --> 00:51:19,214
it's not falsifiable. So it's been

999
00:51:19,252 --> 00:51:22,830
falsified, it can't be falsified,

1000
00:51:23,250 --> 00:51:25,918
so it's incorrect. That's an interesting

1001
00:51:26,004 --> 00:51:28,078
connection that some people make. But a

1002
00:51:28,084 --> 00:51:30,234
linear model cannot be falsified. A base

1003
00:51:30,292 --> 00:51:32,882
graph cannot be falsified. Once any

1004
00:51:32,936 --> 00:51:36,146
given linear model is presented in a

1005
00:51:36,168 --> 00:51:38,482
context with constraints for a certain

1006
00:51:38,536 --> 00:51:40,862
data type, then it's an empirical

1007
00:51:40,926 --> 00:51:43,762
question of its accuracy and adequacy

1008
00:51:43,906 --> 00:51:47,686
relative to other models. But at

1009
00:51:47,708 --> 00:51:49,698
the abstract level of a neural network

1010
00:51:49,794 --> 00:51:52,306
or linear model or active inference,

1011
00:51:52,498 --> 00:51:55,820
falsification simply doesn't apply.

1012
00:52:00,880 --> 00:52:02,732
But there's a lot that could be

1013
00:52:02,786 --> 00:52:06,492
clarified about

1014
00:52:06,626 --> 00:52:09,468
what are the utilities and some of the

1015
00:52:09,554 --> 00:52:13,840
pitfalls of mapping

1016
00:52:14,580 --> 00:52:17,184
biological or cyberphysical systems to

1017
00:52:17,222 --> 00:52:20,608
active inference models. If it's a

1018
00:52:20,614 --> 00:52:22,204
purely digital system or potentially

1019
00:52:22,252 --> 00:52:24,164
even a cyberphysical system that's been

1020
00:52:24,202 --> 00:52:27,044
designed a really certain way, it might

1021
00:52:27,082 --> 00:52:30,468
be compatible with activ like

1022
00:52:30,554 --> 00:52:32,950
essentially by fiat or by design.

1023
00:52:34,440 --> 00:52:36,628
One could make an artificial creature

1024
00:52:36,724 --> 00:52:39,796
like an infer ant that is implementing

1025
00:52:39,828 --> 00:52:43,336
active inference. One could also

1026
00:52:43,438 --> 00:52:46,676
model an ant in the field using a linear

1027
00:52:46,708 --> 00:52:49,736
model or a levee flight or some other

1028
00:52:49,758 --> 00:52:53,116
model or an active inference model. But

1029
00:52:53,138 --> 00:52:54,712
those are two very different settings.

1030
00:52:54,776 --> 00:52:56,764
One in which sort of the rabbit was

1031
00:52:56,802 --> 00:52:59,068
placed into the hat and then we have a

1032
00:52:59,074 --> 00:53:01,712
rabbit analyzer and the other one being

1033
00:53:01,766 --> 00:53:04,284
like we don't know what's in the hat,

1034
00:53:04,412 --> 00:53:06,656
but we have a rabbit analyzer that we

1035
00:53:06,678 --> 00:53:08,560
think applies to what is in this hat.

1036
00:53:20,840 --> 00:53:25,184
Chapter five includes some evidence

1037
00:53:25,232 --> 00:53:26,452
presented on different

1038
00:53:26,586 --> 00:53:29,728
neurotransmitters. So it'd

1039
00:53:29,744 --> 00:53:32,568
be interesting to see like, what other

1040
00:53:32,654 --> 00:53:36,600
frameworks are able to be compatible

1041
00:53:37,180 --> 00:53:39,300
or even provide unique explanations,

1042
00:53:39,380 --> 00:53:42,168
predictions, et cetera, related to

1043
00:53:42,254 --> 00:53:45,128
neurochemistry. Have neural networks

1044
00:53:45,144 --> 00:53:47,224
ever been used to derive unique

1045
00:53:47,272 --> 00:53:49,564
predictions or found compatibility with

1046
00:53:49,682 --> 00:53:52,540
neurohormones neurochemicals?

1047
00:53:54,400 --> 00:53:56,712
And then again, they have a final figure

1048
00:53:56,786 --> 00:53:58,892
in chapter five that's like a graphical

1049
00:53:58,956 --> 00:54:00,848
overview of several of the systems that

1050
00:54:00,854 --> 00:54:02,400
are described in the chapter

1051
00:54:02,900 --> 00:54:05,440
specifically relating to the cortical

1052
00:54:06,740 --> 00:54:10,230
cognitive functions and then

1053
00:54:10,680 --> 00:54:14,310
the dialectic between habit and

1054
00:54:14,760 --> 00:54:18,230
free energy driven planning here

1055
00:54:18,680 --> 00:54:21,024
with Dopamine as a precision modulator

1056
00:54:21,152 --> 00:54:23,444
with extensive further modeling

1057
00:54:23,492 --> 00:54:27,400
presented in other papers. And this even

1058
00:54:27,470 --> 00:54:31,268
more basal motor selection mechanism

1059
00:54:31,444 --> 00:54:34,392
based upon reflex arcs and

1060
00:54:34,446 --> 00:54:37,432
proprioceptive error minimization and

1061
00:54:37,486 --> 00:54:38,410
all of that.

1062
00:54:42,140 --> 00:54:43,724
We raised these questions not to offer

1063
00:54:43,762 --> 00:54:44,988
any answers, but to highlight some of

1064
00:54:44,994 --> 00:54:46,936
the exciting avenues of future research

1065
00:54:47,058 --> 00:54:50,048
in theoretical neurobiology. So a lot of

1066
00:54:50,054 --> 00:54:55,408
the questions that are asked here

1067
00:54:55,574 --> 00:54:58,608
in this paragraph, but also above, at

1068
00:54:58,614 --> 00:54:59,876
the very beginning they said we're not

1069
00:54:59,898 --> 00:55:01,156
saying how it is, we're just giving like

1070
00:55:01,178 --> 00:55:03,216
the current process model understanding.

1071
00:55:03,328 --> 00:55:06,420
And then especially at the end, they

1072
00:55:06,490 --> 00:55:08,710
raise even more speculative questions.

1073
00:55:09,080 --> 00:55:12,264
So that

1074
00:55:12,382 --> 00:55:14,490
takes us to the end of chapter five.

1075
00:55:15,660 --> 00:55:17,368
That's the first half of the book.

1076
00:55:17,454 --> 00:55:20,296
That's the epistemic component of the

1077
00:55:20,318 --> 00:55:24,128
book. Well, a lot of it is epistemic,

1078
00:55:24,244 --> 00:55:26,620
but that's the first five chapters.

1079
00:55:29,440 --> 00:55:33,612
The second five chapters, which are

1080
00:55:33,666 --> 00:55:36,924
longer partially because of having

1081
00:55:36,962 --> 00:55:38,672
more figures and things like that. But

1082
00:55:38,726 --> 00:55:41,916
you can see there's more pages slightly

1083
00:55:41,948 --> 00:55:44,304
in the second part of the book, though

1084
00:55:44,342 --> 00:55:47,856
we've also read at least some of

1085
00:55:47,878 --> 00:55:49,280
the appendices.

1086
00:55:51,240 --> 00:55:54,068
That's where we're heading, which is

1087
00:55:54,154 --> 00:55:58,230
picking up on chapter six

1088
00:55:58,680 --> 00:56:00,756
in just a few weeks with a recipe for

1089
00:56:00,778 --> 00:56:03,048
designing active inference models. So

1090
00:56:03,214 --> 00:56:06,360
for those who stuck with the uncertainty

1091
00:56:07,340 --> 00:56:09,816
for the first five chapters and stay in

1092
00:56:09,838 --> 00:56:13,256
the game, more will become clear when

1093
00:56:13,278 --> 00:56:15,884
the recipe for the dish is seen. And

1094
00:56:15,922 --> 00:56:19,628
then more will be clarified when

1095
00:56:19,714 --> 00:56:23,212
the dish is prepared, and then when you

1096
00:56:23,266 --> 00:56:24,972
cut the vegetables, and then when you

1097
00:56:25,026 --> 00:56:27,116
design the dish and the recipe and all

1098
00:56:27,138 --> 00:56:29,056
of that. And that's like a journey that

1099
00:56:29,078 --> 00:56:31,456
we're all going to be on. The tools are

1100
00:56:31,478 --> 00:56:33,280
not finalized and the kitchen is not

1101
00:56:33,350 --> 00:56:34,748
completed, et cetera, et cetera, et

1102
00:56:34,764 --> 00:56:37,836
cetera. So it's

1103
00:56:37,868 --> 00:56:40,980
just a call for us to go to Onboarding

1104
00:56:41,800 --> 00:56:45,392
and indicate our interest in continuing

1105
00:56:45,456 --> 00:56:48,992
with one or the other cohort or sharing

1106
00:56:49,056 --> 00:56:51,604
with any colleagues who you think might

1107
00:56:51,642 --> 00:56:53,848
like to jump in to the second cohort of

1108
00:56:53,854 --> 00:56:57,304
part one. And then in the coming two

1109
00:56:57,342 --> 00:57:00,456
weeks, we'll be able to take a

1110
00:57:00,478 --> 00:57:03,252
step back, look over the chapter

1111
00:57:03,316 --> 00:57:05,164
questions, but also think about more

1112
00:57:05,202 --> 00:57:07,116
general questions that we're having.

1113
00:57:07,298 --> 00:57:10,270
Basic questions, meso questions,

1114
00:57:10,640 --> 00:57:13,288
advanced questions, research avenues,

1115
00:57:13,464 --> 00:57:17,504
and also turn to the project ideas where

1116
00:57:17,622 --> 00:57:20,864
multiple of these are already active and

1117
00:57:20,902 --> 00:57:23,170
there's spaces for people who want to

1118
00:57:23,780 --> 00:57:27,024
facilitate or catalyze some other

1119
00:57:27,142 --> 00:57:31,616
direction. So fun

1120
00:57:31,798 --> 00:57:34,876
meeting everyone's.

1121
00:57:34,908 --> 00:57:38,400
Welcome to stay on for

1122
00:57:38,470 --> 00:57:41,784
Tools or head up to a room above if they

1123
00:57:41,862 --> 00:57:43,912
just want to talk to other people about

1124
00:57:43,966 --> 00:57:47,272
anything else. So thanks again for

1125
00:57:47,326 --> 00:57:49,896
joining and see you in just a few

1126
00:57:49,918 --> 00:57:51,400
minutes for Tools.



[
  {
    "start": 1.547,
    "end": 30.241,
    "text": " hello everyone it's may 26 2022 it's the fourth week of the first cohort of the active textbook group we're in our first discussion of chapter two today we're going to go over the questions that are posted for chapter two then we'll see where that takes us and what ideas of the book have been explored and that'll be the main focus",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 31.587,
    "end": 50.243,
    "text": " also just a note on the math learning group those meetings have been occurring on 19 UTC on Wednesdays and so in the math learning group you'll find information about how to participate you'll find and be able to edit resources on learning different topics",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 51.092,
    "end": 61.417,
    "text": " So this is a really helpful activity if people want to share the resources that help them learn about different ideas, and that'll help people search for a video about this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 63.038,
    "end": 68.821,
    "text": "We have a notation table that will help us as we start to look at the notation more.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 70.122,
    "end": 78.466,
    "text": "There's some math learning basic questions like around math, anything related to the textbook content.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 80.714,
    "end": 108.065,
    "text": " can go in the main questions math learning group is part of the real textbook group so these are not the questions to be answered like the other ones that we're going to go to are but this is just about math and then also there's some math oriented overviews like i wrote this summary of the first two-thirds of chapter two so if people want to be like continuing to distill and represent what they're learning and what they're asking that's all good",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 109.125,
    "end": 133.313,
    "text": " okay any other um notes that anybody wants to add before we start to go into the questions you can raise your hand or you can type it in the chat okay so we're in the first week of discussing chapter two the low road to active inference",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 134.269,
    "end": 139.496,
    "text": " So just to kind of warm us up, what is the low road?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 169.95,
    "end": 198.099,
    "text": " um yeah the low road is uh the bayesian brain approach right here um is the like proximate next like big concept i guess previous but it's kind of based on i guess yeah i mean you can see right in the picture like it's based on basing it's like mathematical but i the way i conceptualize it is like",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 199.269,
    "end": 226.433,
    "text": " Literally, the low road meaning very primitive, very mathematical, very hyper-analytical, foundational kind of an analysis way of looking at it, I guess, that's not necessarily living or anything like that.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 232.022,
    "end": 254.142,
    "text": " whereas the high road is kind of more about the other side of that okay so we're going to be learning about the high road and the low road those are the two paths that are offered in chapter two and three of the book so hopefully again like",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 255.353,
    "end": 264.016,
    "text": " It would be awesome to have as many people as possible speak and feel welcome to share their views during this session.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 265.017,
    "end": 278.642,
    "text": "So everyone is especially encouraged to put questions or comments in the chat or raise their hand to speak or find other times to have synchronous communications and especially to make contributions to the coda.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 278.982,
    "end": 284.104,
    "text": "But really appreciate that everyone has come out to go through these discussion questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 285.086,
    "end": 290.669,
    "text": " So we're going to now go to the questions that have been placed into the questions table.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 291.65,
    "end": 296.053,
    "text": "And just the more questions the better, they can be super short.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 296.593,
    "end": 298.774,
    "text": "It can be anything that you're having uncertainty about.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 299.595,
    "end": 306.259,
    "text": "If you come across a table or a figure or a sentence in the text and you have a question, please write the question down.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 307.259,
    "end": 309.681,
    "text": "It will be extremely, extremely helpful.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 311.642,
    "end": 314.304,
    "text": "It doesn't have to be with extensive",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 315.879,
    "end": 338.94,
    "text": " um references or citations it can just be what did this mean if you're uncertain about something and then if you were certain about it you could still write a question like what would you ask somebody to test or to understand what they understood about it okay so we're gonna start with this first most upvoted question but there's still time",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 339.776,
    "end": 364.161,
    "text": " this session and also till next week in our next session on chapter two to add more questions if you wrote them down somewhere else and also to upvote questions but we're just going to start with the most interestingly voted questions what is the relevance of the support and surprise for the different probability distributions in table 2.1",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 368.362,
    "end": 374.929,
    "text": " So what does anyone see in table 2.1 and what are support and surprise?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 388.645,
    "end": 389.666,
    "text": "People, Mike, yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 394.73,
    "end": 402.635,
    "text": " So table 2.1 looks to me like a guide for selecting distributions.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 403.356,
    "end": 420.187,
    "text": "If you're building out one of these models with support saying what set of numbers would fit within this distribution selection, and then if you chose this distribution, how would you look at the support equation for it?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 423.817,
    "end": 426.919,
    "text": " Can you unpack what you mentioned with the surprise part?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 427.439,
    "end": 428.54,
    "text": "Yeah.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 429.02,
    "end": 432.422,
    "text": "If you chose this distribution, how would you look at the surprise component of it?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 442.647,
    "end": 447.19,
    "text": "And we're going to get to surprise and Bayesian surprise in the coming questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 448.31,
    "end": 449.151,
    "text": "So support.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 450.894,
    "end": 453.755,
    "text": " using the definition pulled here from Wikipedia.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 454.435,
    "end": 463.338,
    "text": "The support of the function is the subset of the X axis, like the domain, where there's a Y value that's not zero.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 463.738,
    "end": 466.619,
    "text": "So it's where the function is non-zero defined.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 468.299,
    "end": 479.783,
    "text": "The Gaussian, the normal bell curve distribution, has non-zero support at all numbers, fancy R.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 481.26,
    "end": 489.108,
    "text": " And then other distributions have different values that they're having non-zero values at.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 489.408,
    "end": 494.073,
    "text": "Like the gamma distribution is defined over the interval from zero to infinity.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 494.814,
    "end": 497.437,
    "text": "So if it was like, how many of this thing do you have?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 498.378,
    "end": 502.982,
    "text": "That might be a reason to use a distribution that's only positive numbers.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 504.244,
    "end": 528.562,
    "text": " or if you were wanting something that could have positive or negative numbers of any size the Gaussian is a distribution and then another one that's like a relevant support difference would be like the Dirichlet being defined only within the zero to one interval which helps connect it to probabilities so the support is where",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 529.775,
    "end": 533.605,
    "text": " That function, that distribution family is defined.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 534.548,
    "end": 537.555,
    "text": "And then what is the surprise?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 557.708,
    "end": 561.13,
    "text": " Does anyone know the name of the little squiggle they are using to represent surprise?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 562.611,
    "end": 564.613,
    "text": "I think it's called Fracture Eye.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 587.085,
    "end": 607.218,
    "text": " aka fancy eye um the first we're going to come to the definition of surprise in a coming question but um and without going into every variable and trying to",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 608.402,
    "end": 628.413,
    "text": " overlearn something the surprise of a given observation like a data point let's just think about the gaussian x is the observed data point and mu is like the mean of that normal distribution which is also the mode so it's like the center of that distribution",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 629.434,
    "end": 634.718,
    "text": " And then this capital Pi is a variance estimator, I believe here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 635.678,
    "end": 645.005,
    "text": "So how surprised one should be by a new data point coming in is related to the difference between that new data point and the mean.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 646.366,
    "end": 650.308,
    "text": "So if the new data point were exactly on the center of the distribution,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 651.377,
    "end": 653.058,
    "text": " this whole term is going to go to zero.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 653.779,
    "end": 659.843,
    "text": "So one should not be surprised at all by exact data points on the mean of the distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 660.023,
    "end": 664.747,
    "text": "But any data point that has not exactly on the mean is going to have some non-zero value.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 665.787,
    "end": 674.173,
    "text": "And so it's going to be a function of how far the data point is from the mean and some scaling by the variance.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 675.254,
    "end": 678.937,
    "text": "So if we were expecting 100 with a variance of 10,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 680.819,
    "end": 682.38,
    "text": " versus 100 with a variance of one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 683.661,
    "end": 687.423,
    "text": "In one case, like getting 101 is like a one sigma event.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 687.523,
    "end": 691.546,
    "text": "It has a Z score of one, just speaking loosely here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 692.787,
    "end": 699.251,
    "text": "Whereas in the case of it having a variance that's very small, then that might be more surprising.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 705.395,
    "end": 709.517,
    "text": "Any other comments that people have on table 2.1?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 712.565,
    "end": 714.006,
    "text": " I have a question.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 715.487,
    "end": 715.667,
    "text": "Yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 717.709,
    "end": 725.795,
    "text": "So while I was reading through the chapter, so, okay.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 725.835,
    "end": 727.516,
    "text": "Going back to section 2.2, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 727.556,
    "end": 728.697,
    "text": "Perception as inference.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 729.818,
    "end": 737.804,
    "text": "So when they say that the brain is a predictive machine or a statistical organ that infers external states of the world.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 739.429,
    "end": 763.63,
    "text": " uh okay here's my question because i'm primarily from a telecom background so most of our things are to get messages across between two distant transmitters uh receiver right so we build noise models so without any noise you don't get signal so you need to know what a noise model looks like",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 764.488,
    "end": 775.132,
    "text": " And then, you know, so for example, if the power spectrum of the signal is what you expect from, let's say you're using a Gaussian distribution of noise, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 775.732,
    "end": 781.175,
    "text": "So if it's the same as you expect from the Gaussian, you do nothing.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 781.295,
    "end": 782.135,
    "text": "So that's just noise.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 782.355,
    "end": 784.456,
    "text": "So you don't classify the data that's coming in.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 786.149,
    "end": 789.352,
    "text": " If it is more than this, then okay.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 789.372,
    "end": 792.115,
    "text": "So there might be something interesting about this.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 792.135,
    "end": 793.236,
    "text": "There might be some signals here.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 793.297,
    "end": 796.7,
    "text": "So we need to find that signal to noise ratio.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 796.72,
    "end": 805.369,
    "text": "A lot of this chapter seems to be dealing with the fact that we by default know what the signal looks like.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 806.49,
    "end": 808.853,
    "text": "So there's no attempt to build a noise model here.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 809.95,
    "end": 839.27,
    "text": " would be like okay so uh maybe the sensor in this case our eyes for example or uh has some parameters right right it can detect between uh say a bandwidth well okay so it can detect light between certain frequency ranges but maybe it performs very well in a certain frequency range and not others there's more error in other frequency ranges right so the noise model should incorporate such things",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 840.27,
    "end": 846.534,
    "text": " imperfections in the sensor, imperfections in the connections between the sensor and processing equipment.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 847.135,
    "end": 849.796,
    "text": "But there's no attempt to do that in this entire chapter.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 849.856,
    "end": 855.28,
    "text": "So I'm just curious as to what exactly is the predictive machine here?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 856.12,
    "end": 857.261,
    "text": "What is being predicted here?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 858.951,
    "end": 859.632,
    "text": " Great question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 859.912,
    "end": 866.278,
    "text": "And in general, every question that people have, they're going to be 100 times more impactful if they can be written down.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 868.08,
    "end": 869.541,
    "text": "This is just the second chapter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 869.841,
    "end": 874.205,
    "text": "It's introducing us to the essence of starting with just Bayes' equation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 875.106,
    "end": 876.227,
    "text": "It's the first equation here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 876.848,
    "end": 879.33,
    "text": "Starting with the foundation of...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 880.293,
    "end": 901.286,
    "text": " increasingly advanced noise filtering approaches so check out live stream 43 to see how common filter and advanced multi-level bayesian filtering schemes are used and learned and fit with free energy minimization and so on but you're right that is not approached in this chapter because it's starting with the essence of bay's equation",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 902.498,
    "end": 910.8,
    "text": " and then going to build towards another kernel, which is going to be the kernel of the variational free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 912.88,
    "end": 926.644,
    "text": "And then that in this one layer case is going to be able to be elaborated and parameterized and fit within nested models to accommodate like learning on hyperparameters, learning of noise models and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 927.084,
    "end": 928.284,
    "text": "But it is not in this chapter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 930.137,
    "end": 930.357,
    "text": " Okay.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 930.717,
    "end": 930.957,
    "text": "Yeah.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 931.017,
    "end": 934.039,
    "text": "So I was just, I'm sorry if I threw us off track.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 934.619,
    "end": 937.941,
    "text": "No, so it's perfect, but they're, they're good questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 937.981,
    "end": 945.984,
    "text": "And then Jakob asked, what do you think of the surprise, the distributions involving a sum product of the product probability distributions, except for gamma.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 952.227,
    "end": 954.668,
    "text": "Actually, I guess the Gaussian kind of as well.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 954.688,
    "end": 954.728,
    "text": "Um,",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 957.165,
    "end": 969.93,
    "text": " Yeah, maybe I'm thinking about this wrong, but I thought that the surprise was the information gain or equivalent.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 970.71,
    "end": 981.975,
    "text": "And so for the Gaussian and the gamma distributions, it's always for one single element of the distribution, for one x.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 982.887,
    "end": 990.292,
    "text": " but for the multinomial and Dirichlet, there's something over all of the possible probabilities which makes it, I guess, an entropy function.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 991.553,
    "end": 994.875,
    "text": "But is there a difference?",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1004.641,
    "end": 1009.324,
    "text": "For example, one could have a multi-dimensional Gaussian, a two-dimensional Gaussian,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1010.618,
    "end": 1019.222,
    "text": " And then the surprise for the two-dimensional Gaussian would be represented as a sum over the different dimensions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1019.682,
    "end": 1027.285,
    "text": "So they've shown the Gaussian and the gamma in the unidimensional case and the multinomial and the Dirichlet in a multidimensional case.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1028.545,
    "end": 1036.749,
    "text": "So in that case, is it... I thought that x was just one element of the domain.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1037.349,
    "end": 1038.329,
    "text": "So that's not necessary.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1038.409,
    "end": 1040.01,
    "text": "So x is just...",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1040.834,
    "end": 1054.88,
    "text": " If we think of the Gaussian as this kind of bell-shaped curve on the x and y-axis, we really want the f value to get the Gaussian value, but x is just the input value.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1058.062,
    "end": 1058.442,
    "text": "Is that correct?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1058.462,
    "end": 1064.905,
    "text": "I believe that is accurate, because yes, x is the domain of the function, and then the sum is over i's.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1066.96,
    "end": 1074.302,
    "text": " So if we had a multi-dimensional Gaussian in i dimensions, then it would have a sum over i's.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1077.603,
    "end": 1080.384,
    "text": "Yeah.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1082.244,
    "end": 1088.066,
    "text": "I guess I still have uncertainty about why we don't do that for the single-dimensional Gaussian as well.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1088.106,
    "end": 1095.828,
    "text": "Because if we take just one point on the Gaussian, I understand why that",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1096.584,
    "end": 1109.406,
    "text": " would have a certain surprise, but then in the same way, if we take one point in the Dirichlet distribution, shouldn't that also have an equivalent equation for surprise?",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1110.626,
    "end": 1122.428,
    "text": "Why in the Dirichlet distribution are we summing over all the i's, but in the Gaussian we are considering just one point on the probability distribution?",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1123.929,
    "end": 1125.749,
    "text": "Or maybe Dirichlet is...",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 1127.136,
    "end": 1152.056,
    "text": " multi multi-dimensional and that's the reason i think this the uni-dimensional case would be just without this sigma i it would just be this but if anyone has any other thoughts especially if they want to contribute them in like writing to improve our understanding that'd be awesome let's continue to the next question",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1155.734,
    "end": 1160.617,
    "text": " In section 2.4, they write, we now introduce the simple but fundamental advance offered by active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1161.218,
    "end": 1166.442,
    "text": "This starts from the same inferential perspective discussed above, but extends it to consider action as inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1168.283,
    "end": 1172.146,
    "text": "In your own words, what is the simple but fundamental advance offered by active inference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1174.567,
    "end": 1179.19,
    "text": "So what would anyone write or like to share?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1181.112,
    "end": 1182.533,
    "text": "What is the advance of active inference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1197.857,
    "end": 1200.738,
    "text": " Sorry, is it just bringing action into the equation?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1201.499,
    "end": 1210.284,
    "text": "So predictive coding and Bayesian brain are specifically about perception, visual perception.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1212.185,
    "end": 1225.254,
    "text": "I think the key thing with active inference is that it's not just that it brings action into the equation, it's that it treats them as essentially the same thing, that they're kind of united as part of the same process.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1226.343,
    "end": 1229.088,
    "text": " which I think is kind of a new development.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1229.108,
    "end": 1232.894,
    "text": "Thanks for the awesome answer.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1235.278,
    "end": 1235.498,
    "text": "Alik?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1240.257,
    "end": 1254.466,
    "text": " Yeah, I think Alan Barthow, or Barthow's in The Brain's Sense of Movement, has a particularly relevant description of action and perception.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1255.046,
    "end": 1259.309,
    "text": "And he writes that perception is simulated action.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1260.209,
    "end": 1267.616,
    "text": " And I think that really speaks to the advance made by active inference.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1267.676,
    "end": 1286.652,
    "text": "I mean, the integration of action and perception, or in other words, as Barthold claims, the simulation of action and perception is, I guess, a really novel development.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1291.212,
    "end": 1295.536,
    "text": " I think I read something similar to that, but I can't remember where I read it.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1295.716,
    "end": 1310.649,
    "text": "But I think it was roughly that on these existing Bayesian brain theories, an agent perceives in order to act, whereas under active inference, an agent acts, action kind of precedes perception.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1311.53,
    "end": 1318.736,
    "text": "So active inference essentially flips this traditional schema in terms of which one feeds into the other.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1321.638,
    "end": 1328.543,
    "text": " Yeah, I was just going to say that, yeah, adding on to, bringing action into the equation, it flips it.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1328.663,
    "end": 1333.647,
    "text": "Like, there's no, like, where do your priors come from?",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1333.667,
    "end": 1335.688,
    "text": "Where do the hyperparameters get specified?",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1335.769,
    "end": 1338.411,
    "text": "Like, where does the model start?",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1338.451,
    "end": 1342.053,
    "text": "Where is the agency, kind of, in all of these other models?",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1342.774,
    "end": 1345.276,
    "text": "There's really no affordance for that.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1346.325,
    "end": 1355.031,
    "text": " where it's hard to, it's like very emergent, whereas here it's like very explicitly specified, some affordance for that.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1356.272,
    "end": 1356.812,
    "text": "Thanks, Brock.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1357.553,
    "end": 1360.635,
    "text": "Mike, and then anyone else who raises their hand.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1361.896,
    "end": 1375.525,
    "text": "Yeah, so I'm thinking if you have priors, you have some prior assumptions, then do you necessarily need to take action in order to sort of envision what your perception might be",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1376.018,
    "end": 1378.48,
    "text": " So it's almost like getting into imagination or something like that.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1378.5,
    "end": 1386.025,
    "text": "You have a set of priors that can drive what you imagine your perception to be before any action is taken.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1386.045,
    "end": 1392.41,
    "text": "Great.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1392.77,
    "end": 1393.09,
    "text": "Question?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1393.551,
    "end": 1393.731,
    "text": "Blue?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1398.132,
    "end": 1409.241,
    "text": " So I think that this was like elaborated on a lot in section 2.7, when they talk about pursuing a policy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1410.803,
    "end": 1420.871,
    "text": "Because there's consequences, like when we're planning and making predictions that has consequences for both our perception and our action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1422.312,
    "end": 1426.256,
    "text": "Like when, I mean, even just planning, even in a conversation,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1426.758,
    "end": 1434.4,
    "text": " when someone says, oh, have a nice day is expected at the end of that, but like have a nice rabbit would be like, what?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1434.82,
    "end": 1439.601,
    "text": "Like you're not expecting that to come at the end of that sentence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1440.181,
    "end": 1446.662,
    "text": "So when you plan, it interferes with your perception because you didn't plan for them to say, have a nice rabbit, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1447.282,
    "end": 1450.363,
    "text": "But it also interferes with your action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1450.423,
    "end": 1451.423,
    "text": "And so I think like this,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1455.12,
    "end": 1464.809,
    "text": " like novelty or enhanced aspect of active inference loops into the fact that perception and action are continually impacting one another?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1468.152,
    "end": 1468.432,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1469.273,
    "end": 1469.674,
    "text": "Thanks, Blue.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1472.97,
    "end": 1482.952,
    "text": " These are some of the core memes and themes, and it really is important to understand how active inference is similar and different than other work in this area.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1483.093,
    "end": 1487.594,
    "text": "Like, variational Bayesian inference is not introduced by active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1488.774,
    "end": 1492.295,
    "text": "Bayesian models of perception is not introduced by active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1493.015,
    "end": 1496.316,
    "text": "Bayesian models of action is not introduced by active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1497.242,
    "end": 1509.936,
    "text": " so it's about finding what has been done to understand what is being offered and then whether one chooses to take this like history of science development of science",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1510.799,
    "end": 1536.925,
    "text": " view or we kind of just want to state plainly what it is without wondering what the advance relative to other frameworks is these are all really critical ideas like the signal processing framework was brought up earlier and predictive processing predictive coding anticipatory systems are often purely about sense and",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1537.898,
    "end": 1546.923,
    "text": " Just to give one thought on that, it kind of makes sense for a video encoding algorithm, for example, because every piece of the camera is in focus.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1547.743,
    "end": 1550.545,
    "text": "But vision requires action.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1552.156,
    "end": 1577.144,
    "text": " And so that's where we're going to be bringing in all these other important concepts like attention, sensory attenuation, and the active decision-making components of vision to resolve uncertainty, which is what gives rise to a generative visual field that seems like there's color everywhere and seems like there's high resolution everywhere.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1578.228,
    "end": 1581.449,
    "text": " though that is not the incoming sensory information.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1582.389,
    "end": 1599.713,
    "text": "So by fully taking this generative modeling perspective on perception, which in live stream number 43.0, Maria did an awesome job of connecting this even before Helmholtz and Kant, it's Plato's cave.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1600.673,
    "end": 1607.675,
    "text": "And it's part of this long discussion about perception and action could be a variable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1609.059,
    "end": 1633.685,
    "text": " so there's perceiving and then if action is a variable then active inference is providing not just a unified framework like a bayesian graph framework for some variables that are interpreted as perception and some variables that can be interpreted as action selection or policy planning but there's a tractable approximation",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1635.919,
    "end": 1660.497,
    "text": " by shifting from the in reading this answer by shifting from an exact solution to the mathematical problem Bayesian inference to an approximate solution variational free energy which is going to provide a bound on some quantity that might be intractable to compute so it's going to give a heuristic for the action perception cycle",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1664.261,
    "end": 1673.927,
    "text": " But this is a really great question, so people can continue to return to it because people will probably ask us for a long time to come.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1674.807,
    "end": 1676.728,
    "text": "How is this different than blank?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1677.309,
    "end": 1678.83,
    "text": "Or what is active inference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1679.47,
    "end": 1682.092,
    "text": "These are some of the things that we would want to have in mind.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1682.112,
    "end": 1682.592,
    "text": "Ali?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1686.651,
    "end": 1699.747,
    "text": " Again, going back to Bartho and his definition of perception as simulated action, I think it also relates nicely to Merleau-Ponty's phenomenology.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1699.807,
    "end": 1701.529,
    "text": "Merleau-Ponty has a famous",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1702.723,
    "end": 1709.267,
    "text": " and has a famous insight as vision is the brain's way of touching.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1710.567,
    "end": 1726.296,
    "text": "And so, you see, for instance, as we move around space, as we construct our sense of spatiality, I mean, or our sense of temporality,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1726.829,
    "end": 1733.232,
    "text": " or everything else, we don't just begin with the representation of space or time.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1734.112,
    "end": 1743.676,
    "text": "I mean, the image of movement constructs because we move and not as a consequence of our movement.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1746.478,
    "end": 1748.499,
    "text": "Thanks, and we'll...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1750.595,
    "end": 1760.519,
    "text": " continue with the questions, but that's an awesome area to go into with phenomenology, with 4E cognition, extended, embedded in culture, et cetera, et cetera.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1761.539,
    "end": 1765.421,
    "text": "And many, many of the live streams and papers have been on those areas.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1766.101,
    "end": 1775.404,
    "text": "So it's cool to bridge from formal models of perception, cognition, and action into qualitative and philosophical areas.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1776.745,
    "end": 1776.925,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1783.954,
    "end": 1809.507,
    "text": " then oh thanks for adding that the idea of interrelated action and perception has been around in cognitive science for a long time like dynamical systems theory and an activism emphasize that but active inference brings a coherent formalism to the table which is a nice advance well said next question figure 2.2",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1820.06,
    "end": 1828.562,
    "text": " Figure 2.2 illustrates Y as a result of an internal generative model, X, and an external generative process, X star.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1834.183,
    "end": 1837.743,
    "text": "In order to measure surprise, wouldn't we need another value of Y, i.e.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1837.783,
    "end": 1839.664,
    "text": "a separate Y that encodes prior beliefs?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1842.124,
    "end": 1847.345,
    "text": "If Y can be objectively measured from external signals, is there a third Y that's considered the observation?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1855.54,
    "end": 1876.995,
    "text": " first i'd like to actually go to this question about two notions of surprise so um because they might come to bear on this action perception loop once we move from just bayes equation and calculating surprise on parametric distributions",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1877.987,
    "end": 1893.788,
    "text": " to thinking about prior updating and cognitive entities with a generative model that constitutes a prior with incoming information coming in, we're going to start to see why this notion of surprise and Bayesian surprise are similar and different.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1895.828,
    "end": 1914.616,
    "text": " using the apple frog example so the unobserved hidden state the latent state of the world is whether there's an apple or a frog in a bag for example or just in this person's area and then what can be observed just speaking coarsely is it can jump or not",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1915.56,
    "end": 1918.941,
    "text": " And we can totally go down the rabbit hole with what is truly observed and things like that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1919.301,
    "end": 1922.282,
    "text": "But this is just what are in the context of this model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1922.482,
    "end": 1925.063,
    "text": "The unobserved state is the actual identity of the object.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1925.563,
    "end": 1928.643,
    "text": "And the observation is going to be the action or not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1929.544,
    "end": 1933.545,
    "text": "We take the opportunity to unpack two different notions of surprise, both of which are important.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1934.785,
    "end": 1937.386,
    "text": "The first we refer to simply as surprise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1938.502,
    "end": 1943.463,
    "text": " It's the negative log evidence where evidence is the marginal probability of observations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1944.263,
    "end": 1951.225,
    "text": "So we saw that first sense of surprise with the fracture I in the previous table we looked at.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1952.525,
    "end": 1956.166,
    "text": "The second notion of surprise is referred to as Bayesian surprise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1957.167,
    "end": 1962.548,
    "text": "This is a measure of how much we have to update our beliefs following an observation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1963.635,
    "end": 1968.779,
    "text": " In other words, Bayesian surprise quantifies the difference between a prior and a posterior probability.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1970.2,
    "end": 1973.022,
    "text": "What are the similarities and differences between the two notions of surprise?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1973.042,
    "end": 1980.387,
    "text": "Okay, we'll see what has been said and then hear what other people are thinking.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1982.969,
    "end": 1988.133,
    "text": "So, page 20, they wrote that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1992.844,
    "end": 2001.538,
    "text": " Bayesian surprise scores the amount of belief updating as opposed to surprise, which is simply how unlikely or likely that observation was.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2004.966,
    "end": 2029.527,
    "text": " so in that Gaussian case the surprise is like about one data point coming in given how the distributions parameterized right now how surprising was that one data point and then Bayesian surprise is going to be about how much that distribution is updated after processing that data point similarities they both depend on how well the agent's generative model matches the external world",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2030.97,
    "end": 2039.114,
    "text": " and they're both measuring surprise or Bayesian surprise in the same units, which are information-theoretic quantities of information, i.e.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2039.254,
    "end": 2040.115,
    "text": "nats or bits.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2040.775,
    "end": 2049.26,
    "text": "Differences, plain surprise marginalizes over all the model's degrees of freedom under the model's prior distribution over its adjustable parameters",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2050.516,
    "end": 2072.039,
    "text": " would be great for someone to unpack what does it mean to marginalize over the model's degrees of freedoms and then basing surprise lets the model choose the best set of parameters it can to fit the observed data and then measures how much of an update that was um mike and then any and then blue and then only",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2073.769,
    "end": 2086.332,
    "text": " Yeah, under the first similarity, I'm wondering if it's this dependency on how well the agent's generative model matches their perception of the external world as opposed to matches the external world.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2086.352,
    "end": 2090.373,
    "text": "I agree with that, great addition.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2094.814,
    "end": 2096.554,
    "text": "Blue and then Ali, okay?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2096.714,
    "end": 2098.255,
    "text": "I think my hand was just left over, left up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2098.295,
    "end": 2098.995,
    "text": "Okay, Ali?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2104.185,
    "end": 2109.991,
    "text": " Well, I believe that the plane surprise is a kind of raw statistical surprise.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2110.091,
    "end": 2117.277,
    "text": "But Bayesian surprise is a kind of surprised and is a kind of processed surprise.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2117.517,
    "end": 2123.263,
    "text": "Or I'm not sure if I'm right in saying that plane surprise can...",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2124.884,
    "end": 2133.508,
    "text": " probably be described as an objective surprise as opposed to Bayesian surprise as a subjective surprise.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2133.988,
    "end": 2142.452,
    "text": "But as I said, I'm not sure about the plausibility of these descriptions.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2144.493,
    "end": 2144.773,
    "text": "Great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2145.193,
    "end": 2145.573,
    "text": "Comments?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2148.154,
    "end": 2153.557,
    "text": "Even the surprise alone still depends on the parameters",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2154.787,
    "end": 2176.006,
    "text": " of the generative model so it still is within a processing or filtering frame albeit a fixed one so um they talk about like uh let's look at the figure where there's a graphical overview of this apple and the frog so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2177.529,
    "end": 2184.013,
    "text": " And this is kind of giving some graphics and words to fill in this apple frog example.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2184.613,
    "end": 2198.601,
    "text": "So initially, the person has this likelihood model in the back of their head, where they have beliefs about how likely apples are to jump, they do it 1% of the time, and how likely frogs are to jump, they do it 81% of the time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2200.262,
    "end": 2201.043,
    "text": "That's the likelihood.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2201.583,
    "end": 2206.606,
    "text": "That's about how observations depend on hidden states of the world.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2209.144,
    "end": 2214.226,
    "text": " Their prior beliefs are that there's a 10% chance that the entity is a frog.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2215.406,
    "end": 2217.767,
    "text": "And these are mutually exclusive options.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2217.787,
    "end": 2219.427,
    "text": "There's no third option here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2220.167,
    "end": 2224.949,
    "text": "That would be like another category model, structure learning on the model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2224.989,
    "end": 2227.57,
    "text": "So we're staying within this model for now.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2228.83,
    "end": 2230.63,
    "text": "And they sum to one because they're a probability.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2231.411,
    "end": 2233.511,
    "text": "Then there's an observation, which is jumping.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2234.272,
    "end": 2238.293,
    "text": "And then the posterior reflects the updated beliefs",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2239.511,
    "end": 2240.812,
    "text": " about what the entity is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2241.712,
    "end": 2252.758,
    "text": "And so it's like, it's so much overwhelmingly more likely that frogs jump that seeing something jump updates the prior from here to here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2255.379,
    "end": 2263.083,
    "text": "So in this case, one can calculate the surprise of the observation without doing any updating at all.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2263.844,
    "end": 2266.065,
    "text": "One could just stay fixed in their prior belief",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2267.611,
    "end": 2273.476,
    "text": " and then could describe how surprised they are in NATS by given observation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2275.458,
    "end": 2289.55,
    "text": "In this full Bayesian cycle, there is an updating of the prior to the posterior, and then that updating can be described in terms of how much the prior was updated.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2291.075,
    "end": 2296.776,
    "text": " And so that's like here, the model is updated to the observed data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2296.876,
    "end": 2298.336,
    "text": "This is now the best fitting model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2300.037,
    "end": 2303.417,
    "text": "And then we're calculating the difference between these two distributions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2305.218,
    "end": 2312.839,
    "text": "So regular surprise being zero means the data point was exactly as you expected.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2314.139,
    "end": 2317.78,
    "text": "Bayesian surprise being zero means the distribution was not updated.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2319.181,
    "end": 2323.244,
    "text": " High surprise means that the data point was extremely unpredicted.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2323.264,
    "end": 2327.908,
    "text": "It was extremely unlikely whether or not you update your model at all.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2329.049,
    "end": 2335.574,
    "text": "High Bayesian surprise means that the distribution was changed a lot as a function of seeing that happen.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2337.015,
    "end": 2337.195,
    "text": "Ali?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2340.785,
    "end": 2368.566,
    "text": " Perhaps I didn't understand it correctly, but isn't it the case that in the plane surprise, or more generally in the Bayesian formulation of the probability of events, the likelihood and the priors are somehow inherent to the phenomena, inherent to the events, independent of the observers?",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2372.315,
    "end": 2373.056,
    "text": " Yeah, great question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2373.076,
    "end": 2381.702,
    "text": "I hope I'm not going off on a branch here, but this is related to the difference between frequentist and Bayesian approaches to statistics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2381.723,
    "end": 2389.188,
    "text": "So frequentism does have, viewed from the Bayesian perspective, frequentism does have priors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2389.949,
    "end": 2393.13,
    "text": " They're uniform priors, which are not uninformative priors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2393.51,
    "end": 2394.77,
    "text": "They just are uniform.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2395.451,
    "end": 2418.437,
    "text": "And so in frequentism, we come across like the maximum likelihood solution or the maximum likelihood parameter, which is just like, well, if all outcomes were equally a priori likely, uniform prior, then we would just need to evaluate the likelihood and find the model with the maximum likelihood solution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2419.898,
    "end": 2422.66,
    "text": " Bayesian offers another degree of freedom.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2422.68,
    "end": 2437.112,
    "text": "It says, well, sure, you could pick a uniform prior that would give you the maximum likelihood solution, or you might want to have a prior distribution over that space.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2438.553,
    "end": 2448.721,
    "text": "So that if something is twice as likely a priori, and then you observe less than 2x evidence for it, you still might want the Bayes factor or your posterior to reflect",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2449.693,
    "end": 2478.393,
    "text": " that one rather than just jumping instantly to a different maximum likelihood solution so yes priors and likelihood are implicit but they're using a different ontology than bayesian statistics but we're in the bays or the post base area now but there's so many connections to classical statistics and in spm the textbook there is",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2479.793,
    "end": 2486.617,
    "text": " parametric classical statistics, non-parametric classical statistics, and Bayesian statistics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2487.878,
    "end": 2491.18,
    "text": "So they're more similar than not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2492.36,
    "end": 2498.244,
    "text": "It just is about seeing where one of them is like a special case of another or a generalization of another.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2505.245,
    "end": 2511.407,
    "text": " Okay, let's see if we can do one or two more questions during this session.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2514.289,
    "end": 2516.149,
    "text": "Okay, so let's return to the previous question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2517.25,
    "end": 2532.396,
    "text": "So we're looking at Figure 2-2, which is something we're going to see different representations of this entity as generative model, world or niche as generative process",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2533.936,
    "end": 2538.957,
    "text": " And then here, the hidden states are those that are unobserved as data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2540.337,
    "end": 2543.518,
    "text": "Y is referring to data points that are observed as data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2545.438,
    "end": 2550.639,
    "text": "There's a cognitive hidden state, which is like a prior in this generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2551.239,
    "end": 2560.661,
    "text": "And then there's some hidden state X star, but it could have been any letter or any shape about the hidden state in the world.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2562.441,
    "end": 2563.181,
    "text": "The observation,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2564.784,
    "end": 2587.982,
    "text": " is going to everything that happens in between here is cognition um like the sandwich model like sense think act that type of model is just referring to that little boomerang data come in cognitive processing action selection so we're in that figure the question was",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2589.369,
    "end": 2593.471,
    "text": " In order to measure surprise, wouldn't we need another value of y, i.e.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2593.511,
    "end": 2595.832,
    "text": "a separate y that includes prior belief?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2596.652,
    "end": 2597.873,
    "text": "Great question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2599.774,
    "end": 2604.496,
    "text": "Let's just assume that there is a Gaussian generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2605.436,
    "end": 2610.939,
    "text": "So the entity has like two parameters in its cognitive model, the mean and the variance.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2612.82,
    "end": 2617.802,
    "text": "Then y comes in, and given the parameterization of the generative model,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2619.013,
    "end": 2624.875,
    "text": " All that's needed is the data point to come in for the surprise to be calculated.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2625.756,
    "end": 2636.34,
    "text": "So another value is needed, but whether we call it ABCXYZ is just a mathematical abstraction.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2637.18,
    "end": 2640.601,
    "text": "So yes, prior beliefs are important.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2641.382,
    "end": 2647.724,
    "text": "The prior beliefs that can be interpreted as the parameterizations of the cognitive model are required.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2650.242,
    "end": 2673.199,
    "text": " if y can be objectively measured from external signals is there a third y that is considered the observation so one could imagine a lot of like real world scenarios where a more complex model would be required two people looking at the thermometer or all these different sorts of situations but um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2678.089,
    "end": 2686.633,
    "text": " Unless somebody can unpack this a little more or explain what they were asking about, then I don't think a third why is required.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2686.913,
    "end": 2687.073,
    "text": "Mike?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2687.093,
    "end": 2691.716,
    "text": "Yeah, that was my question, so I can try and unpack it.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2693.196,
    "end": 2702.281,
    "text": "And so sticking with the thermometer example at the heart of that last question is, if we have a thermometer that's registering the temperature,",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2705.014,
    "end": 2718.481,
    "text": " Can we consider that as sort of the true observation, that is the actual Y as measured by this instrument, and then therefore we can compare our sort of internal Y with the actual Y?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2718.501,
    "end": 2723.444,
    "text": "Okay.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2724.524,
    "end": 2731.548,
    "text": "So I have a really similar and related question that's like two questions down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2731.568,
    "end": 2731.668,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2732.896,
    "end": 2737.041,
    "text": " Um, but it's, it's in a different section, but let me just like place some things on here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2737.081,
    "end": 2742.268,
    "text": "And it goes back to like what we were saying about like, what exactly is the hidden state?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2742.829,
    "end": 2750.238,
    "text": "So is the hidden state, the temperature and the data is the reading from the thermometer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2751.337,
    "end": 2753.8,
    "text": " that like, that's like the data why, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2753.86,
    "end": 2758.966,
    "text": "Like the out this, this why in the middle, but then like, what is how I feel hot or cold?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2759.326,
    "end": 2762.43,
    "text": "Like, I mean, I'm perfectly positioned to be at 75 all the time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2762.47,
    "end": 2768.357,
    "text": "So like, I know if it's like one degree too cold or one degree too hot, like I've got to turn up the heat or turn it down or, or, or whatever.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2768.937,
    "end": 2794.553,
    "text": " um but like my registration my sensory input is not at 75 degrees like my sensory input is i'm hotter i'm cold and so this this is like where i mean we were talking about this yesterday in the math group like there's this really fuzzy line for me like i would love for someone to clarify that so i do get what you mean about like this in like shouldn't there be another why like there's the why out there in the world 75 degrees and then there's how i feel about that why",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2797.153,
    "end": 2816.683,
    "text": " okay thanks um so variables are not like innately tagged with being observables or hidden states it's a model specific framing of what is going to be modeled as generated data and what is going to be modeled as a bayesian prior",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2817.759,
    "end": 2828.289,
    "text": " In multi-level Bayesian modeling, the priors themselves are generated from a higher or deeper level of the... So one can be serving, in fact, multiple roles.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2828.709,
    "end": 2838.818,
    "text": "This is the minimal prior generated data expectation maximization type single layer Bayesian kernel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2839.519,
    "end": 2841.441,
    "text": "So we're going to think about this temperature example.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2842.827,
    "end": 2850.17,
    "text": " There's a hidden state of the generative process, which is going to be the temperature of the world, latent unmodeled.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2850.45,
    "end": 2853.071,
    "text": "So this isn't even claiming that there is such a thing as temperature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2853.191,
    "end": 2862.435,
    "text": "This is just the latent unobserved temperature that is giving rise to thermometer readings, which might have like different sorts of noise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2864.236,
    "end": 2868.578,
    "text": "Then there's another hidden state, which is the evaluation of temperature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2870.879,
    "end": 2871.199,
    "text": "And so,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2873.706,
    "end": 2901.606,
    "text": " this is just a schematic that's and also the whether one is labeled X or Y or L or triangle is like a norm and a convenience but the letter doesn't matter itself it it will be used mostly consistently within the textbook but there's only so many letters and there's not a lot of coherence on notation use",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2906.202,
    "end": 2916.265,
    "text": " um what else could be explored here yeah um mike and then ali",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2919.542,
    "end": 2926.89,
    "text": " So as I keep ruminating on this, which is probably more than I should, it seems like there can be infinite whys, right?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2927.01,
    "end": 2940.243,
    "text": "So to the extent that you are taking action, which will change your perception, then you are, as a result, triggering potentially new whys in the system, right?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2942.029,
    "end": 2944.51,
    "text": " So you just have this infinite potential for what Y can be.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2945.251,
    "end": 2948.873,
    "text": "This speaks to the composability and the flexibility of active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2949.114,
    "end": 2952.476,
    "text": "So like Y could be one pixel of visual input.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2952.936,
    "end": 2954.458,
    "text": "It could be a 4K video.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2954.818,
    "end": 2957.4,
    "text": "Y could be smell and a 4K video.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2958.08,
    "end": 2960.342,
    "text": "Y could be LIDAR and this and that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2960.442,
    "end": 2962.764,
    "text": "So Y is just the generalized input.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2963.624,
    "end": 2964.405,
    "text": " of sense.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2965.266,
    "end": 2970.43,
    "text": "And then action might be related to this in a very direct way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2970.71,
    "end": 2974.753,
    "text": "Like why could be visual input and action could be your eye movement.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2975.294,
    "end": 2977.996,
    "text": "So that case is going to be explored a lot in the book.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2979.137,
    "end": 2984.762,
    "text": "However, it could also be you're getting something in and then the actions are just totally unrelated.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2985.342,
    "end": 2989.786,
    "text": "Maybe they don't affect the hidden state, the causal process in the world at all.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2991.127,
    "end": 2994.711,
    "text": " Or maybe actions enable different Ys to enter the picture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2994.731,
    "end": 2999.355,
    "text": "This is just the total essence kernel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3000.496,
    "end": 3006.422,
    "text": "And then even trivial cases require some more apparatus.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3008.144,
    "end": 3008.304,
    "text": "Ali?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3011.704,
    "end": 3012.665,
    "text": " Yeah, I have a question.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3014.107,
    "end": 3026.4,
    "text": "Is it correct to say that the minimization of variation of free energy aka the surprise is exactly equivalent to the maximization of expectation?",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3027.64,
    "end": 3042.283,
    "text": " I mean, is it the wholly linear relationship between these two, or possibly we have some kind of plateaued areas between these two extremes?",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3046.324,
    "end": 3047.204,
    "text": "Great question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3047.924,
    "end": 3053.825,
    "text": "So this is something that we'll come to next week in our discussions on Chapter 2.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3054.265,
    "end": 3056.046,
    "text": "So we have not...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3056.972,
    "end": 3080.803,
    "text": " even discussed variational free energy yet today we talked about starting with a low road on some of the atomic calculations that are going to come into play like surprise and bayesian surprise in a bayesian framework and beginning to partition active entities and their action perception loops",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3082.259,
    "end": 3095.249,
    "text": " in terms of a Bayesian graph that's going to be amenable to flexible modeling of perceptive, cognitive, active, and out there in the world variables, variables with those interpretations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3096.37,
    "end": 3103.375,
    "text": "And then variational free energy is going to come into play as a way to bound surprise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3104.435,
    "end": 3109.479,
    "text": "So let's have some questions and discourse and we'll come to it next week, but it's an awesome question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3109.98,
    "end": 3110.32,
    "text": "Jessica?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3113.666,
    "end": 3118.632,
    "text": " Hi, yes, I have a question, I guess related to this figure 2.2.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3120.454,
    "end": 3126.601,
    "text": "Let's say like in regards to like how we interpret and like what we observe.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3126.801,
    "end": 3129.905,
    "text": "So like I understand like you have a prior that might, you know,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3131.903,
    "end": 3142.309,
    "text": " you're anticipating, you know, something like some other people's behavior or like how things should, that might be different from what you actually observe.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3143.289,
    "end": 3147.612,
    "text": "But a lot of times, at least in terms of like, when we're thinking like human beings, like I say, like,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3149.045,
    "end": 3159.251,
    "text": " an action in the world, like, how you interpret it has to do a lot with your own, like, views or, like, your own experiences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3159.751,
    "end": 3162.273,
    "text": "And maybe that connects to the priors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3162.633,
    "end": 3165.675,
    "text": "But, like, the interpretation might be very different.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3165.695,
    "end": 3173.559,
    "text": "Like, from people to people, the interpretation will vary a lot because everybody has, like, a lot of different experiences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3173.979,
    "end": 3174.14,
    "text": "So...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3175.14,
    "end": 3200.009,
    "text": " those could be like encoded i guess in the priors um but it's not connected per se to the prediction part or maybe it is so i'm trying to kind of connect that idea on of how like our personal interpretation based on our experiences which could be priors make us see the um like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3201.127,
    "end": 3225.19,
    "text": " what's happening in the world differently than other people and so that why will look very different for you than for me just because we have lived different experiences so it's not like a like an actual why like it's not like you cannot really say like this is a fact and because it's an interpretation at the end of the day",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3228.991,
    "end": 3230.572,
    "text": " Great question and points.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3231.473,
    "end": 3245.482,
    "text": "Yeah, understanding how the individual setup of a given entity is related to its past experiences and how that could be modeled by priors is an important area.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3246.642,
    "end": 3253.747,
    "text": "And some of those rich dynamics are not in this minimal nucleus.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3255.404,
    "end": 3266.253,
    "text": " but it'll be awesome to start to think about what does have to be in the box here to give rise to those kinds of dynamics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3270.436,
    "end": 3273.198,
    "text": "So that's going to end this discussion.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3276.101,
    "end": 3281.966,
    "text": "Next week, we will also be staying in chapter two and taking it more towards",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3283.188,
    "end": 3285.694,
    "text": " Variational free energy and expected free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3286.756,
    "end": 3289.422,
    "text": "So that should be fun.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3290.825,
    "end": 3292.97,
    "text": "We're going to, in this room...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3294.54,
    "end": 3319.331,
    "text": " transfer immediately to the dot tools meeting so if you want to join for active lab dot tools meeting everyone is welcome and there's no like prerequisites or anything like that if you want to hang around and join dot tools stay in this exact gather space if you want to continue talking with other people about the book or anything else just head up into one of the rooms above",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3320.396,
    "end": 3324.122,
    "text": " And if anyone who wants to can just continue any discussion that they want.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3325.144,
    "end": 3327.167,
    "text": "But in this space, we're going to continue now with tools.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3328.069,
    "end": 3329.371,
    "text": "So thanks, everyone.",
    "speaker": "SPEAKER_02"
  }
]
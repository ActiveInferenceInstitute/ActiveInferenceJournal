1
00:00:01,450 --> 00:00:04,990
Hello, everyone. It's May 26, 2022.

2
00:00:05,060 --> 00:00:08,266
It's the fourth week of the first cohort

3
00:00:08,298 --> 00:00:11,006
of the active textbook group. We're in

4
00:00:11,028 --> 00:00:13,920
our first discussion of chapter two

5
00:00:15,250 --> 00:00:17,214
today. We're going to go over the

6
00:00:17,252 --> 00:00:20,634
questions that are posted for chapter

7
00:00:20,682 --> 00:00:23,886
two. Then we'll see where that takes us

8
00:00:23,988 --> 00:00:26,678
and what ID of the book have been

9
00:00:26,764 --> 00:00:30,520
explored. And that'll be the main focus.

10
00:00:31,450 --> 00:00:33,670
Also, just a note on the math learning

11
00:00:33,740 --> 00:00:35,654
group, those meetings have been

12
00:00:35,692 --> 00:00:39,286
occurring on 19 UTC on Wednesdays.

13
00:00:39,478 --> 00:00:40,954
And so in the math learning group,

14
00:00:40,992 --> 00:00:43,194
you'll find information about how to

15
00:00:43,232 --> 00:00:46,602
participate. You'll find

16
00:00:46,656 --> 00:00:49,194
and be able to edit resources on

17
00:00:49,232 --> 00:00:51,806
learning different topics. So this is a

18
00:00:51,828 --> 00:00:53,966
really helpful activity if people want

19
00:00:53,988 --> 00:00:56,062
to share the resources that help them

20
00:00:56,116 --> 00:00:57,994
learn about different ideas and that'll

21
00:00:58,042 --> 00:01:01,086
help people search for a video about

22
00:01:01,188 --> 00:01:05,186
this. We have a notation table that will

23
00:01:05,208 --> 00:01:07,794
help us as we start to look at the

24
00:01:07,832 --> 00:01:11,886
notation more. There's some math

25
00:01:11,918 --> 00:01:15,378
learning basic questions, like around

26
00:01:15,464 --> 00:01:17,890
math, anything related to the textbook

27
00:01:17,970 --> 00:01:21,878
content can go in the main questions.

28
00:01:21,964 --> 00:01:23,750
Math learning group is part of the real

29
00:01:23,820 --> 00:01:25,974
textbook group. So these are not the

30
00:01:26,012 --> 00:01:28,134
questions to be answered like the other

31
00:01:28,172 --> 00:01:29,786
ones that we're going to go to are, but

32
00:01:29,808 --> 00:01:32,650
this is just about math. And then also

33
00:01:32,800 --> 00:01:35,414
there's some math oriented overviews.

34
00:01:35,542 --> 00:01:37,850
Like, I wrote this summary of the first

35
00:01:38,000 --> 00:01:40,830
two thirds of chapter two. So if people

36
00:01:40,900 --> 00:01:44,106
want to be like, continuing to distill

37
00:01:44,298 --> 00:01:46,526
and represent what they're learning and

38
00:01:46,548 --> 00:01:48,560
what they're asking, that's all good.

39
00:01:48,930 --> 00:01:52,706
Okay, any other notes that

40
00:01:52,728 --> 00:01:55,986
anybody wants to add before we start to

41
00:01:56,008 --> 00:01:59,326
go into the questions? You can raise

42
00:01:59,358 --> 00:02:00,786
your hand or you can type it in the

43
00:02:00,808 --> 00:02:01,410
chat.

44
00:02:05,450 --> 00:02:09,618
Okay, so we're

45
00:02:09,634 --> 00:02:11,426
in the first week of discussing chapter

46
00:02:11,458 --> 00:02:13,858
two, the low Road to active inference.

47
00:02:14,034 --> 00:02:16,440
So just to kind of warm us up,

48
00:02:17,770 --> 00:02:19,960
what is the low road?

49
00:02:54,160 --> 00:02:58,464
Got? A low road is the

50
00:02:58,502 --> 00:03:01,504
Bayesian Brain approach. Right here is

51
00:03:01,542 --> 00:03:06,016
the Proximate next

52
00:03:06,198 --> 00:03:08,880
big concept, I guess previous,

53
00:03:09,860 --> 00:03:12,164
but it's kind of based on I guess.

54
00:03:12,202 --> 00:03:13,428
Yeah, I mean, you can see right in the

55
00:03:13,434 --> 00:03:15,076
picture, it's based on basing it's like

56
00:03:15,098 --> 00:03:17,876
mathematical. The way I conceptualize it

57
00:03:17,898 --> 00:03:23,236
is like literally the

58
00:03:23,258 --> 00:03:27,320
low road, meaning very primitive,

59
00:03:28,140 --> 00:03:31,732
very mathematical, very hyper

60
00:03:31,796 --> 00:03:35,630
analytical, foundational, kind of

61
00:03:38,480 --> 00:03:40,876
an analysis way of looking at it, I

62
00:03:40,898 --> 00:03:44,488
guess that's not necessarily

63
00:03:44,664 --> 00:03:46,910
living or anything like that. Right?

64
00:03:51,690 --> 00:03:53,478
Whereas the high road is kind of more

65
00:03:53,564 --> 00:03:56,280
about the other side of that.

66
00:03:59,850 --> 00:04:03,238
All right. Okay,

67
00:04:03,404 --> 00:04:06,246
so we're going to be learning about the

68
00:04:06,268 --> 00:04:07,786
high road and low road. Those are the

69
00:04:07,808 --> 00:04:09,526
two paths that are offered in chapter

70
00:04:09,558 --> 00:04:13,130
two and three of the book. So hopefully,

71
00:04:13,470 --> 00:04:16,446
again, it would be awesome to have as

72
00:04:16,468 --> 00:04:19,150
many people as possible speak and feel

73
00:04:19,300 --> 00:04:22,960
welcome to share their views during this

74
00:04:23,490 --> 00:04:26,478
session. So everyone is especially

75
00:04:26,564 --> 00:04:28,926
encouraged to put questions or comments

76
00:04:28,958 --> 00:04:31,220
in the chat or raise their hand to speak

77
00:04:32,630 --> 00:04:35,086
or find other times to have synchronous

78
00:04:35,118 --> 00:04:37,330
communications and especially to make

79
00:04:37,400 --> 00:04:39,314
contributions to the coda. But really

80
00:04:39,352 --> 00:04:41,638
appreciate that everyone has come out to

81
00:04:41,724 --> 00:04:44,280
go through these discussion questions.

82
00:04:44,810 --> 00:04:46,966
So we're going to now go to the

83
00:04:46,988 --> 00:04:49,926
questions that have been placed into the

84
00:04:49,948 --> 00:04:53,238
questions table. And just the more

85
00:04:53,324 --> 00:04:55,638
questions, the better they can be super

86
00:04:55,724 --> 00:04:57,638
short. It can be anything that you're

87
00:04:57,654 --> 00:04:59,946
having uncertainty about. If you come

88
00:04:59,968 --> 00:05:02,694
across a table or a figure or a sentence

89
00:05:02,742 --> 00:05:04,426
in the text and you have a question,

90
00:05:04,608 --> 00:05:07,486
please write the question down. It will

91
00:05:07,508 --> 00:05:10,030
be extremely, extremely helpful.

92
00:05:11,410 --> 00:05:14,510
It doesn't have to be with extensive

93
00:05:16,370 --> 00:05:18,606
references or citations. It can just be

94
00:05:18,628 --> 00:05:19,978
what does this mean if you're uncertain

95
00:05:19,994 --> 00:05:21,954
about something and then if you were

96
00:05:21,992 --> 00:05:23,954
certain about it, you could still write

97
00:05:23,992 --> 00:05:25,762
a question like what would you ask

98
00:05:25,816 --> 00:05:28,354
somebody to test or to understand what

99
00:05:28,392 --> 00:05:31,158
they understood about it? Okay, so we're

100
00:05:31,164 --> 00:05:34,662
going to start with

101
00:05:34,716 --> 00:05:37,654
this first most upvoted question. But

102
00:05:37,692 --> 00:05:40,582
there's still time in this session and

103
00:05:40,636 --> 00:05:43,346
also till next week in our next session

104
00:05:43,378 --> 00:05:45,626
on chapter two, to add more questions if

105
00:05:45,648 --> 00:05:47,194
you wrote them down somewhere else and

106
00:05:47,232 --> 00:05:49,226
also to upvote questions. But we're just

107
00:05:49,248 --> 00:05:50,202
going to start with the most

108
00:05:50,256 --> 00:05:52,540
interestingly voted questions.

109
00:05:55,970 --> 00:05:59,774
What is the relevance of the support

110
00:05:59,972 --> 00:06:02,222
and surprise for the different

111
00:06:02,276 --> 00:06:05,200
probability distributions in table 2.1?

112
00:06:08,070 --> 00:06:11,460
So what does anyone see in table 2.1

113
00:06:12,310 --> 00:06:15,330
and what are support and surprise

114
00:06:28,410 --> 00:06:30,140
people? Mike yes.

115
00:06:34,430 --> 00:06:39,900
So table 2.1 looks to me like

116
00:06:40,290 --> 00:06:43,486
a guide for selecting distributions. If

117
00:06:43,508 --> 00:06:46,190
you're building out one of these models

118
00:06:46,530 --> 00:06:50,142
with support saying what

119
00:06:50,196 --> 00:06:53,490
set of numbers would fit within

120
00:06:53,560 --> 00:06:55,586
this distribution selection? And then if

121
00:06:55,608 --> 00:06:57,746
you chose this distribution, how would

122
00:06:57,768 --> 00:07:00,740
you look at the support equation for it?

123
00:07:03,590 --> 00:07:07,222
Can you unpack with a surprise part?

124
00:07:07,356 --> 00:07:10,162
Yeah. If you chose this distribution,

125
00:07:10,226 --> 00:07:11,554
how would you look at the surprise

126
00:07:11,602 --> 00:07:13,000
component of it?

127
00:07:22,360 --> 00:07:24,324
And we're going to get to surprise and

128
00:07:24,362 --> 00:07:26,740
Bayesian surprise in the coming

129
00:07:26,810 --> 00:07:31,140
questions. So support using

130
00:07:31,210 --> 00:07:33,236
the definition pulled here from

131
00:07:33,258 --> 00:07:36,090
Wikipedia, the support of the function

132
00:07:36,460 --> 00:07:39,304
is the subset of the x axis, like the

133
00:07:39,342 --> 00:07:42,756
domain where there's a y value that's

134
00:07:42,788 --> 00:07:45,112
not zero. So it's where the function is

135
00:07:45,166 --> 00:07:47,080
nonzero defined.

136
00:07:48,060 --> 00:07:51,220
The Gaussian, the normal

137
00:07:51,380 --> 00:07:55,232
Belker of distribution, has nonzero

138
00:07:55,316 --> 00:07:58,844
support at all numbers.

139
00:07:59,042 --> 00:08:03,132
Fancy r and then other distributions

140
00:08:03,196 --> 00:08:05,520
have different variable, different

141
00:08:05,590 --> 00:08:08,368
values that they're having non zero

142
00:08:08,454 --> 00:08:10,880
values at. Like the gamma distribution

143
00:08:11,220 --> 00:08:13,392
is defined over the interval from zero

144
00:08:13,446 --> 00:08:16,164
to infinity. So if it was like how many

145
00:08:16,202 --> 00:08:18,756
of this thing do you have that might be

146
00:08:18,778 --> 00:08:21,568
a reason to use a distribution that's

147
00:08:21,664 --> 00:08:24,932
only positive numbers? Or if you were

148
00:08:24,986 --> 00:08:26,184
wanting something that could have

149
00:08:26,222 --> 00:08:27,864
positive or negative numbers of any

150
00:08:27,902 --> 00:08:31,736
size, the Gaussian is a distribution and

151
00:08:31,758 --> 00:08:35,300
then another one that's like a relevant

152
00:08:35,380 --> 00:08:37,416
support difference would be like the

153
00:08:37,438 --> 00:08:40,812
deer Schlee being defined only within

154
00:08:40,866 --> 00:08:44,792
the zero to one interval, which helps

155
00:08:44,856 --> 00:08:47,436
connect it to probabilities. So the

156
00:08:47,458 --> 00:08:50,592
support is where that function,

157
00:08:50,726 --> 00:08:54,156
that distribution family is defined.

158
00:08:54,348 --> 00:08:57,920
And then what is the surprise?

159
00:09:17,450 --> 00:09:19,094
Does anyone know the name of the little

160
00:09:19,132 --> 00:09:20,674
squiggle they are using to represent

161
00:09:20,722 --> 00:09:24,166
surprise? I think it's called Fracture

162
00:09:24,278 --> 00:09:24,940
i,

163
00:09:46,740 --> 00:09:48,560
aka fancy eye.

164
00:09:58,200 --> 00:09:59,924
We're going to come to the definition of

165
00:09:59,962 --> 00:10:04,516
surprise in a coming question and

166
00:10:04,538 --> 00:10:06,984
without going into every variable and

167
00:10:07,022 --> 00:10:10,024
trying to overlearn something, the

168
00:10:10,062 --> 00:10:12,728
surprise of a given observation, like a

169
00:10:12,734 --> 00:10:14,216
data point, let's just think about the

170
00:10:14,238 --> 00:10:18,004
Gaussian. X is the observed

171
00:10:18,052 --> 00:10:22,060
data point and Mu is like the mean

172
00:10:22,210 --> 00:10:25,164
of that normal distribution, which is

173
00:10:25,202 --> 00:10:27,420
also the mode. So it's like the center

174
00:10:27,490 --> 00:10:30,828
of that distribution. And then this

175
00:10:30,994 --> 00:10:34,044
capital pi is a variance estimator,

176
00:10:34,092 --> 00:10:37,184
I believe here. So how surprised one

177
00:10:37,222 --> 00:10:39,890
should be by a new data point coming in

178
00:10:40,260 --> 00:10:42,704
is related to the difference between

179
00:10:42,742 --> 00:10:45,250
that new data point and the mean.

180
00:10:46,100 --> 00:10:49,076
So if the new data point were exactly on

181
00:10:49,098 --> 00:10:51,524
the center of the distribution, this

182
00:10:51,562 --> 00:10:53,972
whole term is going to go to zero. So

183
00:10:54,026 --> 00:10:56,340
one should not be surprised at all by

184
00:10:56,410 --> 00:10:59,396
exact data points on the mean of the

185
00:10:59,418 --> 00:11:01,704
distribution. But any data point that

186
00:11:01,742 --> 00:11:03,608
has not exactly on the mean is going to

187
00:11:03,614 --> 00:11:06,148
have some non zero value and so it's

188
00:11:06,164 --> 00:11:08,136
going to be a function of how far the

189
00:11:08,158 --> 00:11:12,076
data point is from the mean and some

190
00:11:12,178 --> 00:11:14,620
scaling by the variance.

191
00:11:15,040 --> 00:11:18,364
So if we were expecting 100 with a

192
00:11:18,402 --> 00:11:21,628
variance of ten versus 100 with

193
00:11:21,634 --> 00:11:24,864
a variance of one in one case, like

194
00:11:24,982 --> 00:11:27,392
getting 101 is like a one sigma event.

195
00:11:27,446 --> 00:11:30,768
It has a z score of one. Just speaking

196
00:11:30,854 --> 00:11:34,532
loosely here, whereas in the case

197
00:11:34,586 --> 00:11:36,484
of it having a variance that's very

198
00:11:36,522 --> 00:11:38,548
small, then that might be more

199
00:11:38,634 --> 00:11:39,620
surprising.

200
00:11:45,130 --> 00:11:48,214
Any other comments that

201
00:11:48,252 --> 00:11:50,940
people have on Table 2.1?

202
00:11:52,270 --> 00:11:54,140
I have a question.

203
00:11:55,310 --> 00:11:59,034
Yeah. So while

204
00:11:59,072 --> 00:12:06,014
I was reading through the chapter going

205
00:12:06,052 --> 00:12:08,106
back to section 2.2, right, perception

206
00:12:08,138 --> 00:12:11,326
as inference. So when he says, when they

207
00:12:11,348 --> 00:12:12,954
say that the brain is a predictive

208
00:12:13,002 --> 00:12:14,994
machine or a statistical organ, that

209
00:12:15,032 --> 00:12:18,180
infers external states of the world.

210
00:12:19,830 --> 00:12:22,686
Okay, here's my question, because I'm

211
00:12:22,718 --> 00:12:25,940
primarily from a telecom background. So

212
00:12:26,470 --> 00:12:30,274
most of our things are to get messages

213
00:12:30,322 --> 00:12:33,778
crossed between two distant transmitters

214
00:12:33,954 --> 00:12:36,594
receiver, right? So we build noise

215
00:12:36,642 --> 00:12:39,950
models. So without any noise you don't

216
00:12:39,970 --> 00:12:42,298
get signal. So you need to know what

217
00:12:42,384 --> 00:12:44,060
noise model looks like.

218
00:12:46,670 --> 00:12:48,586
For example, if the power spectrum of

219
00:12:48,608 --> 00:12:51,210
the signal is what you expect from,

220
00:12:51,280 --> 00:12:53,050
let's say we are using a Gaussian

221
00:12:53,550 --> 00:12:56,346
distribution of noise, right? So if it's

222
00:12:56,378 --> 00:12:59,006
the same as you expect from the

223
00:12:59,028 --> 00:13:01,854
Gaussian, you do nothing. So that's just

224
00:13:01,892 --> 00:13:03,614
noise. So you don't classify the data

225
00:13:03,652 --> 00:13:07,074
point that's coming in. If it is more

226
00:13:07,112 --> 00:13:10,706
than this, then okay, so there

227
00:13:10,728 --> 00:13:11,954
might be something interesting about

228
00:13:11,992 --> 00:13:13,234
this. There might be some signal here.

229
00:13:13,272 --> 00:13:15,810
So we define that signal to noise ratio.

230
00:13:16,470 --> 00:13:18,550
A lot of this chapter seems to be

231
00:13:18,620 --> 00:13:22,838
dealing with the fact that we

232
00:13:22,924 --> 00:13:25,174
by default know what the signal looks

233
00:13:25,212 --> 00:13:27,894
like. So there's no attempt to build a

234
00:13:27,932 --> 00:13:30,474
noise model here. That would be like,

235
00:13:30,512 --> 00:13:33,482
okay, so maybe the sensor in this case,

236
00:13:33,536 --> 00:13:37,178
our eyes for example, has some

237
00:13:37,344 --> 00:13:40,380
parameters it can detect between,

238
00:13:41,870 --> 00:13:43,310
say, a bandwidth,

239
00:13:45,090 --> 00:13:46,942
okay, so it can detect light between

240
00:13:46,996 --> 00:13:49,326
certain frequency ranges, but maybe it

241
00:13:49,348 --> 00:13:50,942
performs very well in a certain

242
00:13:50,996 --> 00:13:52,826
frequency range and not others. There's

243
00:13:52,858 --> 00:13:56,062
more error in other frequency ranges,

244
00:13:56,126 --> 00:13:58,034
right? So the noise model should

245
00:13:58,072 --> 00:14:00,894
incorporate such things, imperfections

246
00:14:00,942 --> 00:14:02,946
in the sensor, imperfections in the

247
00:14:02,968 --> 00:14:05,442
connections between the sensor and

248
00:14:05,496 --> 00:14:07,926
processing equipment. But there's no

249
00:14:07,948 --> 00:14:09,382
attempt to do that in this entire

250
00:14:09,436 --> 00:14:13,030
chapter. So I'm just curious as to what

251
00:14:13,100 --> 00:14:15,734
exactly is the predictive machine here?

252
00:14:15,932 --> 00:14:17,800
What is being predicted here?

253
00:14:18,810 --> 00:14:21,254
Great question. And in general, every

254
00:14:21,292 --> 00:14:23,098
question that people have, they're going

255
00:14:23,104 --> 00:14:25,466
to be 100 times more impactful if they

256
00:14:25,488 --> 00:14:28,714
can be written down. This is just the

257
00:14:28,752 --> 00:14:30,906
second chapter. It's introducing us to

258
00:14:30,928 --> 00:14:33,802
the essence of starting with just Bayes

259
00:14:33,866 --> 00:14:36,622
equation. It's the first equation here,

260
00:14:36,756 --> 00:14:39,760
starting with the foundation of

261
00:14:40,210 --> 00:14:42,698
increasingly advanced noise filtering

262
00:14:42,714 --> 00:14:45,230
approaches. So check out Livestream 43

263
00:14:45,380 --> 00:14:48,686
to see how colman filter and advanced

264
00:14:48,718 --> 00:14:51,022
multilevel Bayesian filtering schemes

265
00:14:51,086 --> 00:14:53,426
are used and learned and fit with free

266
00:14:53,448 --> 00:14:54,946
energy, minimization and so on. But

267
00:14:54,968 --> 00:14:56,706
you're right, that is not approached in

268
00:14:56,728 --> 00:14:59,286
this chapter because it's starting with

269
00:14:59,308 --> 00:15:02,662
the essence of Bayes equation and

270
00:15:02,716 --> 00:15:06,022
then going to build towards another

271
00:15:06,156 --> 00:15:09,234
kernel, which is going to be the kernel

272
00:15:09,282 --> 00:15:13,242
of the variational free energy. And then

273
00:15:13,296 --> 00:15:16,714
that in this one layer case is

274
00:15:16,752 --> 00:15:18,794
going to be able to be elaborated and

275
00:15:18,832 --> 00:15:20,614
parameterized and fit within nested

276
00:15:20,662 --> 00:15:23,774
models to accommodate, like, learning on

277
00:15:23,812 --> 00:15:25,754
hyperparameters, learning of noise

278
00:15:25,802 --> 00:15:28,014
models and so on. But it is not in this

279
00:15:28,052 --> 00:15:31,280
chapter. Okay, yeah,

280
00:15:31,810 --> 00:15:34,878
I'm sorry if I threw us off track. No,

281
00:15:34,964 --> 00:15:37,666
it's perfect. But they're good

282
00:15:37,688 --> 00:15:40,018
questions. And then Jacob asked, what do

283
00:15:40,024 --> 00:15:41,186
you think of the surprise, the

284
00:15:41,208 --> 00:15:43,298
distributions involving a sum product of

285
00:15:43,304 --> 00:15:45,166
the project, probability distributions,

286
00:15:45,198 --> 00:15:46,530
except for gamma.

287
00:15:51,990 --> 00:15:54,418
Actually, I guess the Gaussian kind of

288
00:15:54,424 --> 00:15:57,966
as well. Yeah, maybe I'm

289
00:15:57,998 --> 00:15:59,666
thinking about this wrong, but I thought

290
00:15:59,688 --> 00:16:03,940
that the surprise was like

291
00:16:04,490 --> 00:16:09,046
the information gain or

292
00:16:09,228 --> 00:16:12,866
equivalent. And so for the Gaussian

293
00:16:12,978 --> 00:16:15,590
and the gamma distributions, it's always

294
00:16:15,660 --> 00:16:19,290
for one single element

295
00:16:20,030 --> 00:16:23,546
of the distribution for one x. But for

296
00:16:23,568 --> 00:16:24,954
the multinomial and derivative lay,

297
00:16:24,992 --> 00:16:26,842
there's something over all of the

298
00:16:26,896 --> 00:16:28,710
possible probabilities, which makes it,

299
00:16:28,800 --> 00:16:30,560
I guess, an entropy function.

300
00:16:31,330 --> 00:16:35,120
But is there a difference?

301
00:16:44,440 --> 00:16:46,148
For example, one could have a

302
00:16:46,234 --> 00:16:48,456
multidimensional Gaussian, a two

303
00:16:48,478 --> 00:16:51,144
dimensional Gaussian, and then the

304
00:16:51,182 --> 00:16:52,468
surprise for the two dimensional

305
00:16:52,484 --> 00:16:55,684
Gaussian would be represented as a sum

306
00:16:55,732 --> 00:16:59,396
over the different dimensions.

307
00:16:59,508 --> 00:17:01,116
So they've shown the Gaussian and the

308
00:17:01,138 --> 00:17:04,396
gamma in the uni dimensional case and

309
00:17:04,418 --> 00:17:06,428
the multinomial and the zero schlay in a

310
00:17:06,434 --> 00:17:09,772
multidimensional case. So in that case,

311
00:17:09,826 --> 00:17:10,590
is it.

312
00:17:13,280 --> 00:17:16,056
I. Thought that x was just one element

313
00:17:16,088 --> 00:17:19,564
of the domain. So x

314
00:17:19,602 --> 00:17:23,010
is just if we think of the Gaussian as

315
00:17:23,540 --> 00:17:26,356
this kind of bell shaped curve on the x

316
00:17:26,378 --> 00:17:29,604
and y axis, we really want the f of x

317
00:17:29,642 --> 00:17:31,940
value to get like the Gaussian value,

318
00:17:32,010 --> 00:17:35,190
but x is just the input value.

319
00:17:37,720 --> 00:17:39,016
Is that correct? Believe I. That is

320
00:17:39,038 --> 00:17:42,068
accurate because, yes, x is the domain

321
00:17:42,164 --> 00:17:44,520
of the function and then the sum is over

322
00:17:44,590 --> 00:17:48,372
eyes. So if we had a multinomial

323
00:17:48,436 --> 00:17:51,400
Gaussian or multi dimensional Gaussian

324
00:17:51,480 --> 00:17:53,436
in I dimensions, then it would have a

325
00:17:53,458 --> 00:17:54,670
sum over eyes.

326
00:17:57,360 --> 00:17:58,110
Yeah,

327
00:18:02,000 --> 00:18:03,888
I guess I still have uncertainty about

328
00:18:03,974 --> 00:18:06,784
why we don't do that for the single

329
00:18:06,822 --> 00:18:11,424
dimensional Gaussian as well. Because if

330
00:18:11,462 --> 00:18:14,412
we take just one point on the Gaussian,

331
00:18:14,476 --> 00:18:17,076
I understand why that would have a

332
00:18:17,098 --> 00:18:19,732
certain surprise. But then in the same

333
00:18:19,786 --> 00:18:22,436
way, if we take one point in the

334
00:18:22,458 --> 00:18:23,860
Dirichlet distribution,

335
00:18:25,560 --> 00:18:27,700
shouldn't that also have an equivalent

336
00:18:28,200 --> 00:18:31,830
equation for surprise? Why in the

337
00:18:32,600 --> 00:18:34,564
Dirichlet distribution are we summing

338
00:18:34,612 --> 00:18:36,490
over all the eyes,

339
00:18:38,620 --> 00:18:40,436
but in the Gaussian we are considering

340
00:18:40,468 --> 00:18:41,800
just one point on the probability

341
00:18:41,880 --> 00:18:45,688
distribution? Or maybe Derishley

342
00:18:45,704 --> 00:18:48,504
is multidimensional.

343
00:18:48,552 --> 00:18:51,852
And that's the reason I think the uni

344
00:18:51,906 --> 00:18:55,520
dimensional case would be just without

345
00:18:55,590 --> 00:18:59,104
this sigma I. It would just

346
00:18:59,142 --> 00:18:59,970
be this.

347
00:19:02,740 --> 00:19:05,036
But if anyone has any other thoughts,

348
00:19:05,068 --> 00:19:06,336
especially if they want to contribute

349
00:19:06,368 --> 00:19:09,044
them in writing to improve our

350
00:19:09,082 --> 00:19:11,024
understanding, that'd be awesome. Let's

351
00:19:11,072 --> 00:19:12,470
continue to the next question.

352
00:19:15,400 --> 00:19:17,444
In Section 2.4 they write we now

353
00:19:17,482 --> 00:19:19,024
introduce the simple but fundamental

354
00:19:19,072 --> 00:19:20,948
advance offered by active inference.

355
00:19:21,044 --> 00:19:22,596
This starts from the same inferential

356
00:19:22,628 --> 00:19:24,468
perspective discussed above, but extends

357
00:19:24,484 --> 00:19:26,920
it to consider action as inference.

358
00:19:27,980 --> 00:19:29,832
In your own words, what is the simple

359
00:19:29,886 --> 00:19:31,516
but fundamental advance offered by

360
00:19:31,538 --> 00:19:35,004
active inference? So what would

361
00:19:35,042 --> 00:19:39,470
anyone write or like to share?

362
00:19:40,800 --> 00:19:43,100
What is the advance of active inference?

363
00:19:57,660 --> 00:20:00,236
Sorry, is it just bringing action into

364
00:20:00,258 --> 00:20:04,504
the equation? So predictive

365
00:20:04,552 --> 00:20:06,844
coding and Bayesian brain are

366
00:20:06,882 --> 00:20:09,784
specifically about perception, visual

367
00:20:09,832 --> 00:20:13,024
perception. I think the key thing with

368
00:20:13,062 --> 00:20:15,456
active inference is that it's well,

369
00:20:15,478 --> 00:20:16,976
it's not just that it brings action into

370
00:20:16,998 --> 00:20:18,416
the equation, it's that it treats them

371
00:20:18,438 --> 00:20:19,824
as essentially the same thing, that

372
00:20:19,862 --> 00:20:24,164
they're kind of united as

373
00:20:24,202 --> 00:20:27,876
part of the same process, which I

374
00:20:27,898 --> 00:20:29,540
think is kind of a new development.

375
00:20:31,420 --> 00:20:33,210
Thanks for the awesome answer,

376
00:20:35,020 --> 00:20:35,960
Ali.

377
00:20:40,050 --> 00:20:43,546
Yeah, I think Alan Bartheau, or Bartho's

378
00:20:43,578 --> 00:20:46,050
in the brain's sense of movement,

379
00:20:47,670 --> 00:20:51,250
has a particularly relevant

380
00:20:52,230 --> 00:20:54,702
description of action and perception.

381
00:20:54,846 --> 00:20:57,758
And he writes that perception is

382
00:20:57,864 --> 00:21:01,014
simulated action. And I think

383
00:21:01,132 --> 00:21:04,374
that really speaks to the

384
00:21:04,412 --> 00:21:07,634
advance made by active inference.

385
00:21:07,682 --> 00:21:11,162
I mean the integration of action and

386
00:21:11,216 --> 00:21:15,002
perception. Or in other words, as

387
00:21:15,056 --> 00:21:19,626
Bartheau claims, the simulation of

388
00:21:19,808 --> 00:21:23,260
action and perception is,

389
00:21:23,790 --> 00:21:26,990
I guess, really novel development.

390
00:21:30,930 --> 00:21:34,074
I think I read something, but I can't

391
00:21:34,122 --> 00:21:36,866
remember where I read it, but I think it

392
00:21:36,888 --> 00:21:39,054
was roughly that. On these existing

393
00:21:39,102 --> 00:21:40,610
Bayesian brain theories,

394
00:21:42,390 --> 00:21:44,898
an agent perceives in order to act,

395
00:21:44,984 --> 00:21:47,074
whereas under active inference an agent

396
00:21:47,192 --> 00:21:50,002
acts. Action kind of precedes

397
00:21:50,066 --> 00:21:52,626
perception. So active inference

398
00:21:52,658 --> 00:21:55,170
essentially flips this traditional

399
00:21:55,250 --> 00:21:58,454
schema in terms of which one feeds into

400
00:21:58,492 --> 00:22:02,058
the other. Yeah,

401
00:22:02,144 --> 00:22:05,066
I was just going to say that adding on

402
00:22:05,088 --> 00:22:07,638
to bringing action into the equation,

403
00:22:07,734 --> 00:22:09,020
it flips it.

404
00:22:12,430 --> 00:22:13,626
Where do your priorities come from?

405
00:22:13,648 --> 00:22:15,166
Where do the hyper parameters get

406
00:22:15,188 --> 00:22:18,590
specified? Where does the model start?

407
00:22:18,740 --> 00:22:21,278
Where is the agency kind of in all of

408
00:22:21,284 --> 00:22:24,874
these other models, really no affordance

409
00:22:24,922 --> 00:22:29,294
for that or it's hard to very emergent

410
00:22:29,422 --> 00:22:32,610
as here it's very explicitly specified.

411
00:22:33,830 --> 00:22:37,230
Some affordance for that. Thanks brock,

412
00:22:37,390 --> 00:22:39,074
Mike and then anyone else who raises

413
00:22:39,122 --> 00:22:43,062
their hand. Yeah, so I'm thinking

414
00:22:43,116 --> 00:22:45,394
if you have priors you have some prior

415
00:22:45,442 --> 00:22:49,046
assumptions, then do

416
00:22:49,068 --> 00:22:51,218
you necessarily need to take action in

417
00:22:51,244 --> 00:22:54,554
order to sort of envision what your

418
00:22:54,592 --> 00:22:56,746
perception might be? So it's almost like

419
00:22:56,768 --> 00:22:58,074
getting into imagination or something

420
00:22:58,112 --> 00:23:00,490
like that. You have a set of priors that

421
00:23:00,560 --> 00:23:03,934
can drive what you imagine your

422
00:23:03,972 --> 00:23:05,854
perception to be before any action is

423
00:23:05,892 --> 00:23:06,480
taken.

424
00:23:12,010 --> 00:23:14,230
Great question, Blue.

425
00:23:17,930 --> 00:23:21,482
So I think that this was

426
00:23:21,536 --> 00:23:26,138
elaborated on a lot in Section 2.7

427
00:23:26,224 --> 00:23:29,260
when they talk about pursuing a policy

428
00:23:30,590 --> 00:23:33,786
because there's consequences when we're

429
00:23:33,818 --> 00:23:35,646
planning and making predictions, that

430
00:23:35,668 --> 00:23:39,258
has consequences for both our perception

431
00:23:39,354 --> 00:23:41,230
and our action.

432
00:23:44,610 --> 00:23:47,330
Even in a conversation when someone

433
00:23:47,400 --> 00:23:50,882
says, oh, have a nice day

434
00:23:50,936 --> 00:23:52,578
is expected at the end of that, but have

435
00:23:52,584 --> 00:23:54,740
a nice rabbit would be like, what?

436
00:23:55,270 --> 00:23:58,034
You're not expecting that to come at the

437
00:23:58,072 --> 00:24:01,350
end of that sentence. So when you plan,

438
00:24:01,500 --> 00:24:04,130
it interferes with your perception

439
00:24:04,210 --> 00:24:05,526
because you didn't plan for them to say

440
00:24:05,548 --> 00:24:07,782
have a nice rabbit. Right, but it also

441
00:24:07,836 --> 00:24:10,858
interferes with your action. And so I

442
00:24:10,864 --> 00:24:15,018
think the

443
00:24:15,184 --> 00:24:18,358
novelty or enhanced aspect of active

444
00:24:18,374 --> 00:24:21,334
inference loops into the fact that

445
00:24:21,472 --> 00:24:23,482
perception and action are continually

446
00:24:23,626 --> 00:24:25,040
impacting one another.

447
00:24:27,970 --> 00:24:30,270
Yes. Thanks, Blue.

448
00:24:32,770 --> 00:24:35,326
These are some of the core memes and

449
00:24:35,348 --> 00:24:37,506
themes, and it really is important to

450
00:24:37,528 --> 00:24:39,234
understand how active inference is

451
00:24:39,272 --> 00:24:42,386
similar and different than other work in

452
00:24:42,408 --> 00:24:44,894
this area. Like variational. Bayesian

453
00:24:44,942 --> 00:24:47,198
inference is not introduced by active

454
00:24:47,214 --> 00:24:50,434
inference. Bayesian models of perception

455
00:24:50,482 --> 00:24:52,674
is not introduced by active inference.

456
00:24:52,802 --> 00:24:54,854
Bayesian models of action is not

457
00:24:54,892 --> 00:24:57,858
introduced by active inference. So it's

458
00:24:57,874 --> 00:25:00,634
about finding what has been done to

459
00:25:00,672 --> 00:25:04,106
understand what is being offered and

460
00:25:04,128 --> 00:25:05,834
then whether one chooses to take this,

461
00:25:05,872 --> 00:25:09,338
like, history of science, development of

462
00:25:09,424 --> 00:25:12,778
science view or we kind of

463
00:25:12,784 --> 00:25:15,022
just want to state plainly what it is

464
00:25:15,076 --> 00:25:17,322
without wondering what the advance

465
00:25:17,386 --> 00:25:19,646
relative to other frameworks is. These

466
00:25:19,668 --> 00:25:22,606
are all really critical ideas. Like the

467
00:25:22,628 --> 00:25:24,298
signal processing framework was brought

468
00:25:24,314 --> 00:25:27,134
up earlier and predictive processing,

469
00:25:27,262 --> 00:25:28,610
predictive coding,

470
00:25:29,510 --> 00:25:33,154
anticipatory systems are often

471
00:25:33,352 --> 00:25:35,860
purely about sense.

472
00:25:36,550 --> 00:25:39,318
And just to give one thought on that,

473
00:25:39,404 --> 00:25:41,302
it kind of makes sense for a video

474
00:25:41,356 --> 00:25:44,070
encoding algorithm, for example,

475
00:25:44,220 --> 00:25:46,454
because every piece of the camera is in

476
00:25:46,492 --> 00:25:50,174
focus, but vision requires

477
00:25:50,242 --> 00:25:53,738
action. And so that's where we're going

478
00:25:53,744 --> 00:25:55,610
to be bringing in all these other

479
00:25:55,680 --> 00:25:59,142
important concepts like attention,

480
00:25:59,286 --> 00:26:03,182
sensory attenuation, and the

481
00:26:03,236 --> 00:26:06,078
active decision making component of

482
00:26:06,164 --> 00:26:09,246
vision to resolve uncertainty, which is

483
00:26:09,268 --> 00:26:11,850
what gives rise to a generative visual

484
00:26:11,930 --> 00:26:14,510
field that seems like there's color

485
00:26:14,580 --> 00:26:16,146
everywhere and seems like there's high

486
00:26:16,168 --> 00:26:18,994
resolution everywhere. Though that is

487
00:26:19,032 --> 00:26:21,540
not the incoming sensory information.

488
00:26:22,150 --> 00:26:25,390
So by fully taking this generative

489
00:26:25,470 --> 00:26:27,970
modeling perspective on perception,

490
00:26:28,330 --> 00:26:32,214
which in live stream number 43.0,

491
00:26:32,332 --> 00:26:35,106
maria did an awesome job of connecting

492
00:26:35,138 --> 00:26:38,338
this even before Helmholtz and Kant.

493
00:26:38,514 --> 00:26:42,074
It's Plato's cave and it's part of this

494
00:26:42,192 --> 00:26:44,970
long discussion about perception.

495
00:26:45,310 --> 00:26:48,170
And action could be a variable.

496
00:26:48,830 --> 00:26:52,166
So there's perceiving and then if action

497
00:26:52,198 --> 00:26:54,446
is a variable, then active inference is

498
00:26:54,468 --> 00:26:58,106
providing not just a unified framework,

499
00:26:58,298 --> 00:27:01,134
like a Bayesian graph framework, for

500
00:27:01,172 --> 00:27:03,214
some variables that are interpreted as

501
00:27:03,252 --> 00:27:04,766
perception and some variables that can

502
00:27:04,788 --> 00:27:06,962
be interpreted as action selection or

503
00:27:07,016 --> 00:27:08,210
policy planning.

504
00:27:10,470 --> 00:27:14,050
But there's a tractable approximation

505
00:27:15,670 --> 00:27:17,534
by shifting from the in reading this

506
00:27:17,592 --> 00:27:19,222
answer, by shifting from an exact

507
00:27:19,276 --> 00:27:21,526
solution to the mathematical problem

508
00:27:21,708 --> 00:27:24,754
Bayesian inference to an approximate

509
00:27:24,802 --> 00:27:27,686
solution variational free energy, which

510
00:27:27,708 --> 00:27:31,020
is going to provide a bound on some

511
00:27:31,470 --> 00:27:34,554
quantity that might be intractable to

512
00:27:34,592 --> 00:27:36,938
compute. So it's going to give a

513
00:27:36,944 --> 00:27:40,022
heuristic for the action perception

514
00:27:40,086 --> 00:27:40,890
cycle.

515
00:27:44,050 --> 00:27:47,870
But this is a really great question. So

516
00:27:47,940 --> 00:27:50,270
people can continue to return to it

517
00:27:50,420 --> 00:27:52,526
because people will probably ask us for

518
00:27:52,548 --> 00:27:55,346
a long time to come, how is this

519
00:27:55,368 --> 00:27:58,446
different than blank? Or what is active

520
00:27:58,478 --> 00:28:00,514
inference? These are some of the things

521
00:28:00,552 --> 00:28:02,306
that we would want to have in mind.

522
00:28:02,408 --> 00:28:03,250
Ali.

523
00:28:06,230 --> 00:28:09,910
Yeah, again, going back to Bartheau and

524
00:28:10,060 --> 00:28:12,422
his definition of perception as

525
00:28:12,556 --> 00:28:15,750
simulated action, I think it also

526
00:28:15,820 --> 00:28:18,738
relates nicely to Merlopanti's

527
00:28:18,834 --> 00:28:21,930
phenomenology. Merlopanti has a famous

528
00:28:22,430 --> 00:28:25,606
statement and has a famous

529
00:28:25,638 --> 00:28:28,794
insight as vision is the brain's way of

530
00:28:28,832 --> 00:28:32,234
touching. And so you

531
00:28:32,272 --> 00:28:36,622
see, for instance, as we

532
00:28:36,756 --> 00:28:40,218
move around space, as we construct

533
00:28:40,314 --> 00:28:44,526
our sense of spatiality or

534
00:28:44,708 --> 00:28:47,330
our sense of temporality or everything

535
00:28:47,400 --> 00:28:50,834
else, we don't just begin

536
00:28:50,952 --> 00:28:52,914
with the representation of space or

537
00:28:52,952 --> 00:28:57,090
time. I mean, the image of movement

538
00:28:58,170 --> 00:29:01,830
constructs because we move and not

539
00:29:01,900 --> 00:29:04,070
as a consequence of our movement.

540
00:29:06,330 --> 00:29:10,890
Thanks. And we'll continue

541
00:29:10,960 --> 00:29:12,058
with the questions, but that's an

542
00:29:12,064 --> 00:29:14,458
awesome area to go into with

543
00:29:14,624 --> 00:29:17,398
phenomenology with four E cognition

544
00:29:17,494 --> 00:29:19,718
extended, embedded in culture, et

545
00:29:19,734 --> 00:29:22,686
cetera, et cetera. And many, many of the

546
00:29:22,708 --> 00:29:24,574
live streams and papers have been on

547
00:29:24,612 --> 00:29:27,530
those areas. So it's cool. It's a bridge

548
00:29:27,610 --> 00:29:30,394
from formal models of perception,

549
00:29:30,442 --> 00:29:34,042
cognition, and action into qualitative

550
00:29:34,106 --> 00:29:35,890
and philosophical areas.

551
00:29:36,550 --> 00:29:37,300
Okay.

552
00:29:43,590 --> 00:29:46,606
And then oh, thanks, Morris, for adding

553
00:29:46,638 --> 00:29:51,684
that the

554
00:29:51,722 --> 00:29:53,076
idea of interrelated action and

555
00:29:53,098 --> 00:29:54,704
perception has been around in cognitive

556
00:29:54,752 --> 00:29:58,144
science for a long time, like dynamical

557
00:29:58,192 --> 00:30:00,832
systems theory and inactivism emphasize

558
00:30:00,896 --> 00:30:02,696
that. But active inference brings a

559
00:30:02,718 --> 00:30:04,168
coherent formalism to the table, which

560
00:30:04,174 --> 00:30:08,072
is a nice advance. Well said. Next

561
00:30:08,126 --> 00:30:10,730
question. Figure 2.2.

562
00:30:19,820 --> 00:30:22,076
Figure two. Two point illustrates Y as a

563
00:30:22,098 --> 00:30:25,212
result of an internal generative model X

564
00:30:25,266 --> 00:30:28,140
and an external generative process x

565
00:30:28,210 --> 00:30:28,830
star.

566
00:30:33,940 --> 00:30:35,996
In order to measure surprise, wouldn't

567
00:30:36,028 --> 00:30:37,888
we need another value of Y-I-E-A

568
00:30:37,894 --> 00:30:40,240
separate Y that encodes prior beliefs,

569
00:30:41,860 --> 00:30:43,984
if Y can be objectively measured from

570
00:30:44,022 --> 00:30:45,844
external signals? Is there a third Y

571
00:30:45,882 --> 00:30:47,700
that's considered the observation?

572
00:30:55,400 --> 00:30:58,792
First, I'd like to actually go to

573
00:30:58,846 --> 00:31:01,544
this question about two notions of

574
00:31:01,582 --> 00:31:06,250
surprise because

575
00:31:07,660 --> 00:31:09,636
they might come to bear on this action

576
00:31:09,668 --> 00:31:12,090
perception loop. Once we move from just

577
00:31:12,460 --> 00:31:15,464
Bayes equation and calculating surprise

578
00:31:15,512 --> 00:31:18,700
on parametric distributions to thinking

579
00:31:18,770 --> 00:31:22,472
about prior updating and cognitive

580
00:31:22,536 --> 00:31:25,184
entities with a generative model that

581
00:31:25,222 --> 00:31:27,436
constitutes a prior, with incoming

582
00:31:27,468 --> 00:31:29,376
information coming in, we're going to

583
00:31:29,398 --> 00:31:31,404
start to see why this notion of surprise

584
00:31:31,452 --> 00:31:33,568
and Bayesian surprise are similar and

585
00:31:33,574 --> 00:31:37,064
different. So using the apple frog

586
00:31:37,132 --> 00:31:40,672
example so the unobserved hidden

587
00:31:40,736 --> 00:31:42,372
state, the latent state of the world,

588
00:31:42,426 --> 00:31:44,912
is whether there is an apple or a frog

589
00:31:45,056 --> 00:31:47,252
in a bag, for example, or just in this

590
00:31:47,306 --> 00:31:50,688
person's area. And then what can be

591
00:31:50,714 --> 00:31:53,624
observed, just speaking coarsely, is it

592
00:31:53,662 --> 00:31:56,456
can jump or not, and we can totally go

593
00:31:56,478 --> 00:31:57,956
down the rabbit hole with what is truly

594
00:31:57,988 --> 00:31:59,576
observed and things like that. But this

595
00:31:59,598 --> 00:32:01,976
is just what are in the context of this

596
00:32:01,998 --> 00:32:03,756
model. The unobserved state is the

597
00:32:03,778 --> 00:32:05,804
actual identity of the object and the

598
00:32:05,842 --> 00:32:08,476
observation is going to be the action or

599
00:32:08,498 --> 00:32:11,176
not. We take the opportunity to unpack

600
00:32:11,208 --> 00:32:12,736
two different notions of surprise, both

601
00:32:12,758 --> 00:32:15,744
of which are important. The first we

602
00:32:15,782 --> 00:32:18,736
refer to simply as surprise. It's the

603
00:32:18,758 --> 00:32:21,456
negative log evidence where evidence is

604
00:32:21,478 --> 00:32:22,864
the marginal probability of

605
00:32:22,902 --> 00:32:25,652
observations. So we saw that first sense

606
00:32:25,706 --> 00:32:29,716
of surprise with the fracture I in

607
00:32:29,818 --> 00:32:32,596
the previous table we looked at the

608
00:32:32,618 --> 00:32:35,012
second notion of surprise is referred to

609
00:32:35,066 --> 00:32:38,164
as Bayesian Surprise. This is a measure

610
00:32:38,292 --> 00:32:40,536
of how much we have to update our

611
00:32:40,558 --> 00:32:43,704
beliefs following an observation. In

612
00:32:43,742 --> 00:32:45,252
other words, Bayesian Surprise

613
00:32:45,316 --> 00:32:46,808
quantifies the difference between a

614
00:32:46,814 --> 00:32:48,920
prior and a posterior of probability.

615
00:32:50,000 --> 00:32:51,228
What are the similarities and

616
00:32:51,234 --> 00:32:52,716
differences between the two notions of

617
00:32:52,738 --> 00:32:55,900
surprise? Okay,

618
00:32:55,970 --> 00:32:58,876
we'll see what has been said and then

619
00:32:59,058 --> 00:33:00,830
hear what other people are thinking.

620
00:33:02,740 --> 00:33:03,490
So,

621
00:33:06,980 --> 00:33:10,290
page 20, they wrote that

622
00:33:12,580 --> 00:33:14,948
Bayesian Surprise scores the amount of

623
00:33:14,954 --> 00:33:18,000
belief updating as opposed to surprise,

624
00:33:18,080 --> 00:33:20,496
which is simply how unlikely or likely

625
00:33:20,528 --> 00:33:22,230
that observation was.

626
00:33:24,760 --> 00:33:27,584
So in that Gaussian case, the surprise

627
00:33:27,632 --> 00:33:30,152
is like about one data point coming in.

628
00:33:30,286 --> 00:33:32,056
Given how the distribution is

629
00:33:32,078 --> 00:33:34,948
parameterized right now, how surprising

630
00:33:35,044 --> 00:33:37,336
was that one data point? And then

631
00:33:37,358 --> 00:33:38,824
Bayesian Surprise is going to be about

632
00:33:38,862 --> 00:33:42,140
how much that distribution is updated

633
00:33:42,720 --> 00:33:45,116
after processing that data point.

634
00:33:45,298 --> 00:33:47,036
Similarities? They both depend on how

635
00:33:47,058 --> 00:33:48,380
well the Agent's Generative model

636
00:33:48,450 --> 00:33:51,256
matches the external world. And they're

637
00:33:51,288 --> 00:33:53,756
both measuring Surprise or Bayesian

638
00:33:53,788 --> 00:33:55,984
Surprise in the same units, which are

639
00:33:56,022 --> 00:33:58,064
information theoretic quantities of

640
00:33:58,102 --> 00:34:00,524
information, i, e. Nats or bits.

641
00:34:00,652 --> 00:34:03,564
Differences. Plain Surprise marginalizes

642
00:34:03,612 --> 00:34:05,648
over all the model's degrees of freedom

643
00:34:05,744 --> 00:34:07,680
under the model's prior distribution

644
00:34:07,760 --> 00:34:10,484
over its adjustable parameters that

645
00:34:10,522 --> 00:34:12,784
would be great for someone to unpack.

646
00:34:12,832 --> 00:34:14,804
What does it mean to marginalize over

647
00:34:14,842 --> 00:34:17,216
the model's degrees of freedoms? And

648
00:34:17,258 --> 00:34:18,712
then Bayesian Surprise lets the model

649
00:34:18,766 --> 00:34:20,504
choose the best set of parameters it can

650
00:34:20,542 --> 00:34:23,400
to fit the observed data and then

651
00:34:23,550 --> 00:34:27,204
measures how much of an update

652
00:34:27,252 --> 00:34:30,904
that was Mike and then

653
00:34:30,942 --> 00:34:34,684
Blue and then Ali. Yeah. Under the

654
00:34:34,722 --> 00:34:38,540
first similarity, I'm wondering if it's

655
00:34:38,960 --> 00:34:40,940
this dependency on how well the Agents

656
00:34:41,010 --> 00:34:42,512
Generative model matches their

657
00:34:42,566 --> 00:34:44,448
perception of the external world as

658
00:34:44,454 --> 00:34:46,690
opposed to matches the external world.

659
00:34:48,260 --> 00:34:55,152
Agree with that. Great addition blue

660
00:34:55,216 --> 00:34:57,316
and then Ali. Okay. I think my hand was

661
00:34:57,338 --> 00:34:59,540
just left over. Left up. Okay. Ali?

662
00:35:04,040 --> 00:35:06,752
Well, I believe that the plain Surprise

663
00:35:06,816 --> 00:35:10,004
is a kind of raw statistical surprise,

664
00:35:10,052 --> 00:35:12,968
but Bayesian Surprise is a kind of

665
00:35:13,134 --> 00:35:16,580
surprise is a kind of processed

666
00:35:16,660 --> 00:35:20,220
surprise. Or I'm not sure if I'm right

667
00:35:20,290 --> 00:35:24,350
in saying that plain Surprise can

668
00:35:24,720 --> 00:35:27,784
probably be described as an objective

669
00:35:27,832 --> 00:35:30,728
surprise as opposed to Bayesian surprise

670
00:35:30,744 --> 00:35:33,772
prized as a subjective surprise.

671
00:35:33,916 --> 00:35:39,360
But as I said, I'm not sure about

672
00:35:39,430 --> 00:35:42,800
the plausibility of these descriptions.

673
00:35:44,340 --> 00:35:45,940
Great comments.

674
00:35:47,960 --> 00:35:51,408
Even the surprise alone

675
00:35:51,584 --> 00:35:54,804
still depends on the parameters of

676
00:35:54,842 --> 00:35:58,232
the generative model. So it still is

677
00:35:58,286 --> 00:36:00,484
within a processing or a filtering

678
00:36:00,532 --> 00:36:03,610
frame, albeit a fixed one.

679
00:36:04,620 --> 00:36:08,010
So they talk about like,

680
00:36:08,860 --> 00:36:10,668
let's look at the figure where there's a

681
00:36:10,674 --> 00:36:12,956
graphical overview of this apple and the

682
00:36:12,978 --> 00:36:13,740
frog.

683
00:36:17,200 --> 00:36:20,328
And this is kind of giving some graphics

684
00:36:20,424 --> 00:36:23,672
and words to fill in this apple frog

685
00:36:23,736 --> 00:36:26,784
example. So initially the person has

686
00:36:26,822 --> 00:36:28,656
this likelihood model in the back of

687
00:36:28,678 --> 00:36:31,248
their head where they have beliefs about

688
00:36:31,334 --> 00:36:34,016
how likely apples are to jump. They do

689
00:36:34,038 --> 00:36:36,656
it 1% of the time and how likely frogs

690
00:36:36,688 --> 00:36:39,110
are to jump. They do it 81% of the time.

691
00:36:39,960 --> 00:36:42,580
That's the likelihood. That's about how

692
00:36:42,650 --> 00:36:46,212
observations depend on hidden states

693
00:36:46,266 --> 00:36:49,604
of the world. Their prior

694
00:36:49,652 --> 00:36:52,280
beliefs are that there's a 10% chance

695
00:36:52,430 --> 00:36:55,752
that the entity is a frog and

696
00:36:55,886 --> 00:36:57,720
these are mutually exclusive options.

697
00:36:57,790 --> 00:37:00,420
There's no third option here. That would

698
00:37:00,430 --> 00:37:03,976
be like another category model structure

699
00:37:04,008 --> 00:37:06,264
learning on the model. So we're staying

700
00:37:06,312 --> 00:37:09,256
within this model for now and they sum

701
00:37:09,288 --> 00:37:10,860
to one because they're a probability.

702
00:37:11,200 --> 00:37:13,136
Then there's an observation which is

703
00:37:13,158 --> 00:37:16,716
jumping and then the posterior reflects

704
00:37:16,748 --> 00:37:20,176
the updated beliefs about what the

705
00:37:20,198 --> 00:37:22,832
entity is. And so it's like, it's so

706
00:37:22,886 --> 00:37:24,964
much overwhelmingly more likely that

707
00:37:25,002 --> 00:37:28,260
frogs jump that seeing something jump

708
00:37:29,000 --> 00:37:31,910
updates the prior from here to here.

709
00:37:35,080 --> 00:37:38,264
So in this case, one can calculate the

710
00:37:38,302 --> 00:37:41,528
surprise of the observation without

711
00:37:41,614 --> 00:37:44,136
doing any updating at all. One could

712
00:37:44,158 --> 00:37:46,600
just stay fixed in their prior belief

713
00:37:47,340 --> 00:37:49,984
and then could describe how surprised

714
00:37:50,052 --> 00:37:53,820
they are in Nats by given observation.

715
00:37:55,200 --> 00:37:58,204
In this full Bayesian cycle, there is an

716
00:37:58,242 --> 00:38:01,260
updating of the prior to the posterior

717
00:38:01,760 --> 00:38:05,612
and then that updating can be described

718
00:38:05,676 --> 00:38:09,040
in terms of how much the prior was

719
00:38:09,110 --> 00:38:12,864
updated. And so that's like here, the

720
00:38:12,902 --> 00:38:16,036
model is updated to the

721
00:38:16,058 --> 00:38:17,684
observed data. This is now the best

722
00:38:17,722 --> 00:38:20,736
fitting model. And then we're

723
00:38:20,768 --> 00:38:22,484
calculating the difference between these

724
00:38:22,522 --> 00:38:27,500
two distributions. So regular

725
00:38:27,600 --> 00:38:30,504
surprise being zero means the data point

726
00:38:30,542 --> 00:38:33,240
was exactly as you expected.

727
00:38:33,900 --> 00:38:36,184
Bayesian surprise being zero means the

728
00:38:36,222 --> 00:38:39,480
distribution was not updated. High

729
00:38:39,550 --> 00:38:41,884
surprise means that the data point was

730
00:38:41,922 --> 00:38:44,024
extremely unpredicted. It was extremely

731
00:38:44,072 --> 00:38:47,276
unlikely whether or not you update your

732
00:38:47,298 --> 00:38:50,408
model at all. High Bayesian surprise

733
00:38:50,504 --> 00:38:52,876
means that the distribution was changed

734
00:38:52,908 --> 00:38:55,184
a lot as a function of seeing that

735
00:38:55,222 --> 00:38:57,680
happen. Ali.

736
00:39:00,260 --> 00:39:02,416
Well, perhaps I didn't understand it

737
00:39:02,438 --> 00:39:06,324
correctly, but isn't it the case that in

738
00:39:06,442 --> 00:39:10,192
the plain surprise or more generally

739
00:39:10,256 --> 00:39:13,556
in the Bayesian formulation of the

740
00:39:13,578 --> 00:39:17,500
probability of events, the likelihood

741
00:39:17,600 --> 00:39:21,348
and both the likelihood and the priors

742
00:39:21,524 --> 00:39:25,144
are somehow inherent to

743
00:39:25,182 --> 00:39:27,124
the phenomena inherent to the events,

744
00:39:27,172 --> 00:39:28,920
independent of the observers?

745
00:39:31,990 --> 00:39:33,938
Yeah, great question. I hope I'm not

746
00:39:34,024 --> 00:39:36,242
going off on a branch here, but this is

747
00:39:36,296 --> 00:39:37,570
related to the difference between

748
00:39:37,640 --> 00:39:40,146
frequentist and Bayesian approaches to

749
00:39:40,168 --> 00:39:43,862
statistics. So Frequentism does

750
00:39:43,916 --> 00:39:46,194
have viewed from the Bayesian

751
00:39:46,242 --> 00:39:48,694
perspective, frequentism does have

752
00:39:48,732 --> 00:39:51,606
priors. They're uniform priors, which

753
00:39:51,628 --> 00:39:53,846
are not uninformative priors, they just

754
00:39:53,868 --> 00:39:57,230
are uniform. And so in frequentism

755
00:39:57,410 --> 00:39:59,174
we come across like the maximum

756
00:39:59,222 --> 00:40:01,606
likelihood solution or the maximum

757
00:40:01,638 --> 00:40:04,026
likelihood parameter, which is just

758
00:40:04,048 --> 00:40:07,334
like, well, if all outcomes were equally

759
00:40:07,382 --> 00:40:10,350
a priori likely uniform prior,

760
00:40:11,250 --> 00:40:14,254
then we would just need to evaluate the

761
00:40:14,292 --> 00:40:17,006
likelihood and find the model with the

762
00:40:17,028 --> 00:40:18,830
maximum likelihood solution.

763
00:40:19,650 --> 00:40:22,418
Bayesian offers another degree of

764
00:40:22,424 --> 00:40:24,002
freedom and says, well, sure, you could

765
00:40:24,056 --> 00:40:27,794
pick a uniform prior that would give you

766
00:40:27,832 --> 00:40:31,874
the maximum likelihood solution. Or you

767
00:40:31,912 --> 00:40:34,870
might want to have a prior distribution

768
00:40:36,010 --> 00:40:39,286
over that space so that if something is

769
00:40:39,308 --> 00:40:41,846
twice as likely a priori and then you

770
00:40:41,868 --> 00:40:44,918
observe less than two X evidence for it,

771
00:40:45,084 --> 00:40:46,726
you still might want like the Bayes

772
00:40:46,758 --> 00:40:49,882
factor or your posterior to reflect that

773
00:40:49,936 --> 00:40:53,766
one rather than just jumping instantly

774
00:40:53,798 --> 00:40:55,334
to a different maximum likelihood

775
00:40:55,382 --> 00:40:59,130
solution. So yes, priors and likelihood

776
00:40:59,210 --> 00:41:01,598
are implicit, but they're using a

777
00:41:01,604 --> 00:41:03,034
different ontology than Bayesian

778
00:41:03,082 --> 00:41:06,238
statistics. But we're in the bays or the

779
00:41:06,244 --> 00:41:08,320
post bays area now.

780
00:41:11,830 --> 00:41:13,426
But there's so many connections to

781
00:41:13,448 --> 00:41:16,306
classical statistics and in SPM, the

782
00:41:16,328 --> 00:41:20,382
textbook there is parametric

783
00:41:20,446 --> 00:41:23,482
classical statistics, non parametric

784
00:41:23,566 --> 00:41:26,002
classical statistics, and Bayesian

785
00:41:26,066 --> 00:41:29,622
statistics. So they're more

786
00:41:29,756 --> 00:41:33,014
similar than not. It just is

787
00:41:33,052 --> 00:41:34,886
about seeing where one of them is like a

788
00:41:34,908 --> 00:41:37,146
special case of another or a

789
00:41:37,168 --> 00:41:38,620
generalization of another.

790
00:41:45,070 --> 00:41:48,742
Okay, let's see if we can do one or two

791
00:41:48,816 --> 00:41:51,870
more questions during this session.

792
00:41:54,050 --> 00:41:55,882
Okay, so let's return to the previous

793
00:41:55,946 --> 00:41:58,510
question. So we're looking at figure

794
00:41:58,580 --> 00:42:03,362
two, which is something

795
00:42:03,416 --> 00:42:04,482
we're going to see different

796
00:42:04,536 --> 00:42:08,190
representations of this entity

797
00:42:08,270 --> 00:42:11,474
as generative model, world or niche as

798
00:42:11,512 --> 00:42:15,446
generative process. And then here

799
00:42:15,628 --> 00:42:17,494
the hidden states are those that are

800
00:42:17,532 --> 00:42:20,774
unobserved as data. Y is

801
00:42:20,812 --> 00:42:22,294
referring to data points that are

802
00:42:22,332 --> 00:42:25,798
observed as data. There's a

803
00:42:25,804 --> 00:42:28,554
cognitive hidden state which is like a

804
00:42:28,592 --> 00:42:31,642
prior in this generative model. And then

805
00:42:31,696 --> 00:42:35,066
there's some hidden state x star. But it

806
00:42:35,088 --> 00:42:37,530
could have been any letter or any shape

807
00:42:38,190 --> 00:42:41,040
about the hidden state in the world.

808
00:42:42,210 --> 00:42:45,534
The observation is going to everything

809
00:42:45,572 --> 00:42:46,974
that happens in between. Here is

810
00:42:47,012 --> 00:42:50,960
cognition, like the sandwich model,

811
00:42:51,430 --> 00:42:54,980
like sense, think, act.

812
00:42:55,430 --> 00:42:57,506
That type of model is just referring to

813
00:42:57,528 --> 00:43:00,114
that little boomerang. Data come in

814
00:43:00,312 --> 00:43:03,650
cognitive processing, action selection.

815
00:43:04,330 --> 00:43:07,750
So we're in that figure. The question

816
00:43:07,820 --> 00:43:10,978
was, in order to measure surprise,

817
00:43:11,154 --> 00:43:12,774
wouldn't we need another value of

818
00:43:12,812 --> 00:43:15,506
Y-I-E-A separate Y that enclose prior

819
00:43:15,538 --> 00:43:18,060
belief? Great question.

820
00:43:19,550 --> 00:43:21,658
Let's just assume that there is a

821
00:43:21,664 --> 00:43:24,780
Gaussian generative model.

822
00:43:25,150 --> 00:43:28,426
So the entity has like two parameters in

823
00:43:28,448 --> 00:43:30,526
its cognitive model, the mean and the

824
00:43:30,548 --> 00:43:33,982
variance. Then Y comes

825
00:43:34,036 --> 00:43:37,038
in and given the parameterization of the

826
00:43:37,044 --> 00:43:40,302
generative model, all that's needed is

827
00:43:40,436 --> 00:43:42,946
the data point to come in for the

828
00:43:42,968 --> 00:43:46,930
surprise to be calculated. So another

829
00:43:47,000 --> 00:43:50,866
value is needed. But whether we call

830
00:43:50,888 --> 00:43:54,262
it ABC XYZ is

831
00:43:54,316 --> 00:43:57,974
just a mathematical abstraction. So yes,

832
00:43:58,092 --> 00:44:01,746
prior beliefs are important. The prior

833
00:44:01,778 --> 00:44:04,006
beliefs that can be interpreted as the

834
00:44:04,028 --> 00:44:06,970
parameterizations of the cognitive model

835
00:44:07,040 --> 00:44:10,314
are required if

836
00:44:10,352 --> 00:44:12,122
Y can be objectively measured from

837
00:44:12,176 --> 00:44:15,226
external signals. Is there a

838
00:44:15,248 --> 00:44:17,146
third Y. That is considered the

839
00:44:17,168 --> 00:44:20,634
observation. So one could

840
00:44:20,672 --> 00:44:23,710
imagine a lot of like, real world

841
00:44:23,780 --> 00:44:26,542
scenarios where a more complex model

842
00:44:26,596 --> 00:44:28,846
would be required, two people looking at

843
00:44:28,868 --> 00:44:31,002
the thermometer or all these different

844
00:44:31,076 --> 00:44:38,342
sorts of situations. But unless

845
00:44:38,406 --> 00:44:40,940
somebody can unpack this a little more

846
00:44:41,790 --> 00:44:44,314
or explain what they were asking about,

847
00:44:44,432 --> 00:44:46,286
then I don't think a third why is

848
00:44:46,308 --> 00:44:49,726
required. Mike yeah, it was

849
00:44:49,748 --> 00:44:52,240
my question, so I can try and unpack it.

850
00:44:52,930 --> 00:44:55,498
And so sticking with the thermometer

851
00:44:55,594 --> 00:44:57,786
example at the heart of that last

852
00:44:57,828 --> 00:45:00,302
question is, if we have a thermometer

853
00:45:00,366 --> 00:45:02,610
that's registering the temperature,

854
00:45:04,870 --> 00:45:08,130
can we consider that as sort of the true

855
00:45:08,280 --> 00:45:11,782
observation that is the actual Y as

856
00:45:11,836 --> 00:45:14,262
measured by this instrument? And then

857
00:45:14,316 --> 00:45:16,534
therefore we can compare our sort of

858
00:45:16,572 --> 00:45:18,840
internal Y with the actual Y?

859
00:45:23,070 --> 00:45:26,346
Okay, so I have a really similar and

860
00:45:26,448 --> 00:45:30,010
related question that's like

861
00:45:30,080 --> 00:45:33,958
two questions down, but it's

862
00:45:33,974 --> 00:45:35,690
in a different section. But let me just

863
00:45:35,840 --> 00:45:37,454
place some things on here and it goes

864
00:45:37,492 --> 00:45:39,998
back to what we were saying about what

865
00:45:40,164 --> 00:45:43,486
exactly is the hidden state. So is the

866
00:45:43,508 --> 00:45:46,574
hidden state the temperature and the

867
00:45:46,612 --> 00:45:50,670
data is reading from the thermometer

868
00:45:51,490 --> 00:45:55,054
that's like the data Y, right? Like this

869
00:45:55,092 --> 00:45:57,826
Y in the middle. But then what is how I

870
00:45:57,848 --> 00:46:00,574
feel hot or cold? I'm perfectly

871
00:46:00,622 --> 00:46:03,106
positioned to be at 75 all the time. So

872
00:46:03,288 --> 00:46:05,058
I know if it's like one degree too cold

873
00:46:05,074 --> 00:46:06,566
or one degree too hot, I've got to turn

874
00:46:06,588 --> 00:46:08,760
up the heat or turn it down or whatever.

875
00:46:09,210 --> 00:46:12,806
But my registration, my sensory input is

876
00:46:12,828 --> 00:46:15,366
not at 75 degrees. My sensory input is

877
00:46:15,388 --> 00:46:18,186
I'm hot or I'm cold. And so this is like

878
00:46:18,208 --> 00:46:20,026
where I mean, we were talking about this

879
00:46:20,048 --> 00:46:21,686
yesterday in the math group. There's

880
00:46:21,718 --> 00:46:24,346
this really fuzzy line for me. I would

881
00:46:24,368 --> 00:46:26,426
love for someone to clarify that. So I

882
00:46:26,448 --> 00:46:28,240
do get what you mean about this.

883
00:46:28,850 --> 00:46:30,666
Shouldn't there be another why? There's

884
00:46:30,698 --> 00:46:32,254
the why out there in the world, 75

885
00:46:32,292 --> 00:46:33,982
degrees and then there's how I feel

886
00:46:34,036 --> 00:46:35,040
about that why.

887
00:46:36,850 --> 00:46:41,502
Okay, thanks. So variables

888
00:46:41,566 --> 00:46:44,402
are not like, innately tagged with being

889
00:46:44,456 --> 00:46:46,946
observables or hidden states. It's a

890
00:46:46,968 --> 00:46:50,486
model specific framing of what is going

891
00:46:50,508 --> 00:46:54,422
to be modeled as generated data and

892
00:46:54,476 --> 00:46:56,006
what is going to be modeled as a

893
00:46:56,028 --> 00:46:58,994
Bayesian prior. In multilevel Bayesian

894
00:46:59,042 --> 00:47:02,214
modeling, the priors themselves are

895
00:47:02,252 --> 00:47:04,762
generated from a higher or deeper level

896
00:47:04,816 --> 00:47:07,482
of the. So one can be serving, in fact,

897
00:47:07,536 --> 00:47:10,090
multiple roles. This is the minimal

898
00:47:11,630 --> 00:47:14,950
prior generated data expectation

899
00:47:15,030 --> 00:47:18,394
maximization type, single layer Bayesian

900
00:47:18,442 --> 00:47:20,414
kernel. So we're going to think about

901
00:47:20,452 --> 00:47:23,278
this temperature example. There's a

902
00:47:23,284 --> 00:47:25,280
hidden state of the generative process

903
00:47:25,730 --> 00:47:27,774
which is going to be the temperature of

904
00:47:27,812 --> 00:47:30,706
the world latent unmodeled. So this

905
00:47:30,728 --> 00:47:32,418
isn't even claiming that there is such a

906
00:47:32,424 --> 00:47:33,778
thing as temperature. This is just a

907
00:47:33,784 --> 00:47:37,106
latent unobserved temperature that

908
00:47:37,128 --> 00:47:40,154
is giving rise to thermometer readings,

909
00:47:40,302 --> 00:47:41,986
which might have, like, different sorts

910
00:47:42,018 --> 00:47:45,794
of noise. Then there's another hidden

911
00:47:45,842 --> 00:47:48,134
state, which is the evaluation of

912
00:47:48,172 --> 00:47:53,866
temperature. And so this

913
00:47:53,888 --> 00:47:57,946
is just a schematic. And also,

914
00:47:58,048 --> 00:48:01,738
whether one is labeled x or Y or L or

915
00:48:01,824 --> 00:48:04,506
triangle is like a norm and a

916
00:48:04,528 --> 00:48:08,734
convenience. But the

917
00:48:08,772 --> 00:48:11,760
letter doesn't matter itself.

918
00:48:13,250 --> 00:48:15,274
It will be used mostly consistently

919
00:48:15,322 --> 00:48:17,906
within the textbook. But there's only so

920
00:48:17,928 --> 00:48:20,066
many letters and there's not a lot of

921
00:48:20,088 --> 00:48:22,020
coherence on notation use.

922
00:48:28,740 --> 00:48:31,010
What else could be explored here?

923
00:48:34,680 --> 00:48:36,820
Yeah. Mike and then Ali.

924
00:48:39,320 --> 00:48:41,236
So as I keep ruminating on this, which

925
00:48:41,258 --> 00:48:44,164
is probably more than I should, it seems

926
00:48:44,202 --> 00:48:46,912
like there can be infinite wise, right?

927
00:48:46,986 --> 00:48:50,648
So to the extent that you are taking

928
00:48:50,734 --> 00:48:53,752
action which will change your

929
00:48:53,806 --> 00:48:56,920
perception, then you are as a result

930
00:48:56,990 --> 00:48:59,656
triggering potentially new Y's in the

931
00:48:59,678 --> 00:49:02,716
system, right? So you just have this

932
00:49:02,738 --> 00:49:04,988
infinite potential for what Y can be.

933
00:49:05,154 --> 00:49:07,356
This speaks to the composability and the

934
00:49:07,378 --> 00:49:10,044
flexibility of active inference. So like

935
00:49:10,162 --> 00:49:12,728
Y could be one pixel of visual input,

936
00:49:12,824 --> 00:49:16,076
it could be a 4K video, y could be smell

937
00:49:16,188 --> 00:49:19,340
and a 4K video, y could be LiDAR

938
00:49:19,420 --> 00:49:21,664
and this and that. So Y is just the

939
00:49:21,702 --> 00:49:25,344
generalized input of sense and

940
00:49:25,382 --> 00:49:28,916
then action might be related to this in

941
00:49:28,938 --> 00:49:31,508
a very direct way. Like Y could be

942
00:49:31,514 --> 00:49:34,004
visual input and action could be your

943
00:49:34,042 --> 00:49:36,468
eye movement. So that case is going to

944
00:49:36,474 --> 00:49:39,736
be explored a lot in the book. However,

945
00:49:39,918 --> 00:49:41,496
it could also be you're getting

946
00:49:41,598 --> 00:49:43,736
something in and then the actions are

947
00:49:43,758 --> 00:49:46,436
just totally unrelated. Maybe they don't

948
00:49:46,628 --> 00:49:48,724
affect the hidden state, the causal

949
00:49:48,772 --> 00:49:51,948
process in the world at all. Or maybe

950
00:49:52,114 --> 00:49:54,204
actions enable different wise to enter

951
00:49:54,242 --> 00:49:57,790
the picture. This is just the total

952
00:49:58,160 --> 00:50:02,672
essence kernel and then even

953
00:50:02,806 --> 00:50:05,744
trivial cases require some more

954
00:50:05,782 --> 00:50:08,800
apparatus. Ali?

955
00:50:11,460 --> 00:50:15,220
Yeah. I have a question. Is it correct

956
00:50:15,290 --> 00:50:17,828
to say that the minimization of

957
00:50:17,914 --> 00:50:21,188
variation of free energy, aka the

958
00:50:21,274 --> 00:50:24,596
Surprise is exactly equivalent to

959
00:50:24,618 --> 00:50:26,660
the maximization of expectation,

960
00:50:30,380 --> 00:50:32,680
the wholly linear relationship between

961
00:50:32,750 --> 00:50:37,336
these two? Or possibly we have some kind

962
00:50:37,358 --> 00:50:41,070
of plateaued areas between these two

963
00:50:41,600 --> 00:50:42,780
extremes?

964
00:50:46,130 --> 00:50:49,022
Great question. So this is something

965
00:50:49,076 --> 00:50:52,302
that we'll come to next week

966
00:50:52,356 --> 00:50:55,602
in our discussions on chapter two. So we

967
00:50:55,656 --> 00:50:59,042
have not even discussed variational free

968
00:50:59,096 --> 00:51:02,722
energy yet today. We talked about

969
00:51:02,856 --> 00:51:05,938
starting with a low road on some of

970
00:51:05,944 --> 00:51:07,846
the atomic calculations that are going

971
00:51:07,868 --> 00:51:11,446
to come into play, like Surprise and

972
00:51:11,468 --> 00:51:13,810
Bayesian. Surprise in a Bayesian

973
00:51:13,890 --> 00:51:17,270
framework and beginning to partition

974
00:51:18,010 --> 00:51:19,926
active entities and their action

975
00:51:19,958 --> 00:51:23,178
perception. Loops in terms of a

976
00:51:23,184 --> 00:51:25,226
Bayesian graph that's going to be

977
00:51:25,248 --> 00:51:27,786
amenable to flexible modeling of

978
00:51:27,888 --> 00:51:31,246
perceptive, cognitive active and out

979
00:51:31,268 --> 00:51:34,218
there in the world variables. Variables

980
00:51:34,234 --> 00:51:37,120
with those interpretations. And then

981
00:51:37,730 --> 00:51:39,726
variational free energy is going to come

982
00:51:39,748 --> 00:51:43,710
into play as a way to bound surprise.

983
00:51:44,150 --> 00:51:46,594
So let's have some questions and

984
00:51:46,632 --> 00:51:48,306
discourse and we'll come to it next

985
00:51:48,328 --> 00:51:49,682
week. But it's an awesome question.

986
00:51:49,816 --> 00:51:50,850
Jessica?

987
00:51:53,430 --> 00:51:56,550
Hi. Yes, I have a question, I guess

988
00:51:56,620 --> 00:51:59,000
related to this figure 2.2,

989
00:52:00,250 --> 00:52:03,414
let's say, in regards to how we

990
00:52:03,452 --> 00:52:06,674
interpret what we observe.

991
00:52:06,722 --> 00:52:09,034
So I understand you have a prior that

992
00:52:09,072 --> 00:52:13,050
might you're anticipating

993
00:52:14,590 --> 00:52:16,294
something like some other people's

994
00:52:16,342 --> 00:52:19,834
behavior or how things should be that

995
00:52:19,872 --> 00:52:21,534
might be different from what you

996
00:52:21,572 --> 00:52:24,030
actually observe. But a lot of times,

997
00:52:24,100 --> 00:52:25,662
at least in terms of when we're thinking

998
00:52:25,716 --> 00:52:29,674
like human beings, an action

999
00:52:29,722 --> 00:52:33,514
in the world, how you interpret it has

1000
00:52:33,572 --> 00:52:37,234
to do a lot with your own

1001
00:52:37,432 --> 00:52:40,114
views or your own experiences. And maybe

1002
00:52:40,152 --> 00:52:43,666
that connects to the priors, but the

1003
00:52:43,688 --> 00:52:45,790
interpretation might be very different

1004
00:52:45,960 --> 00:52:47,206
from people to people. The

1005
00:52:47,228 --> 00:52:49,862
interpretation will vary a lot because

1006
00:52:49,916 --> 00:52:53,778
everybody has different experiences.

1007
00:52:53,874 --> 00:52:57,158
So those could be encoded, I guess, in

1008
00:52:57,164 --> 00:52:59,866
the priors, but it's not connected per

1009
00:52:59,888 --> 00:53:03,434
se to the prediction part. Or maybe it

1010
00:53:03,472 --> 00:53:07,130
is. So I'm trying to kind of connect

1011
00:53:07,200 --> 00:53:11,062
that idea of our personal

1012
00:53:11,136 --> 00:53:13,658
interpretation based on our experiences,

1013
00:53:13,754 --> 00:53:17,760
which could be priors, make us see

1014
00:53:20,930 --> 00:53:22,398
what's happening in the world

1015
00:53:22,484 --> 00:53:24,980
differently than other people.

1016
00:53:25,350 --> 00:53:27,906
So that why will look very different for

1017
00:53:27,928 --> 00:53:31,362
you than for me, just because we have

1018
00:53:31,416 --> 00:53:34,546
lived different experiences. So it's not

1019
00:53:34,568 --> 00:53:38,242
like an actual why. You cannot

1020
00:53:38,306 --> 00:53:42,646
really say this is a fact because

1021
00:53:42,748 --> 00:53:44,486
it's an interpretation. At the end of

1022
00:53:44,508 --> 00:53:45,560
the day.

1023
00:53:48,830 --> 00:53:51,642
Great question and points. Yeah.

1024
00:53:51,696 --> 00:53:55,402
Understanding how the

1025
00:53:55,456 --> 00:53:58,954
individual setup of a given entity is

1026
00:53:58,992 --> 00:54:01,646
related to its past experiences and how

1027
00:54:01,668 --> 00:54:04,014
that could be modeled by priors is an

1028
00:54:04,052 --> 00:54:08,334
important area. And some

1029
00:54:08,372 --> 00:54:12,206
of those rich dynamics are not in this

1030
00:54:12,388 --> 00:54:14,190
minimal nucleus.

1031
00:54:15,190 --> 00:54:17,794
But it'll be awesome to start to think

1032
00:54:17,832 --> 00:54:21,410
about what does have to be in the box

1033
00:54:21,560 --> 00:54:25,186
here to give rise to

1034
00:54:25,208 --> 00:54:26,690
those kinds of dynamics.

1035
00:54:30,170 --> 00:54:33,590
So that's going to end this discussion

1036
00:54:35,850 --> 00:54:38,746
next week. We will also be staying in

1037
00:54:38,768 --> 00:54:42,140
chapter two and taking it more towards

1038
00:54:42,990 --> 00:54:45,046
variational free energy and expected

1039
00:54:45,078 --> 00:54:49,740
free energy. So that should be fun.

1040
00:54:50,590 --> 00:54:55,070
We're going to in this room transfer

1041
00:54:55,220 --> 00:54:57,966
immediately to the Tools meeting. So if

1042
00:54:57,988 --> 00:55:00,874
you want to join for active Lab Tools

1043
00:55:00,922 --> 00:55:02,238
meeting, everyone is welcome. And

1044
00:55:02,244 --> 00:55:04,222
there's no prerequisites or anything

1045
00:55:04,276 --> 00:55:06,658
like that. If you want to hang around

1046
00:55:06,824 --> 00:55:09,970
and join Tools, stay in this exact

1047
00:55:10,040 --> 00:55:12,610
gather space. If you want to continue

1048
00:55:12,680 --> 00:55:15,298
talking with other people about the book

1049
00:55:15,464 --> 00:55:17,938
or anything else, just head up into one

1050
00:55:17,944 --> 00:55:21,538
of the rooms above. And if anyone who

1051
00:55:21,544 --> 00:55:22,962
wants to can just continue any

1052
00:55:23,016 --> 00:55:25,506
discussion that they want. But in this

1053
00:55:25,528 --> 00:55:26,914
space we're going to continue now with

1054
00:55:26,952 --> 00:55:29,740
Tools. So thanks everyone.



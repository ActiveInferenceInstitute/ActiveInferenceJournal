start	end	speaker	sentiment	confidence	text
1450	2254	A	0.5148600339889526	Hello, everyone.
2372	4990	A	0.864296555519104	It's May 26, 2022.
5060	10238	A	0.9154766798019409	It's the fourth week of the first cohort of the active textbook group.
10404	15710	A	0.8816038370132446	We're in our first discussion of chapter two today.
15780	21280	A	0.8998879790306091	We're going to go over the questions that are posted for chapter two.
21810	27826	A	0.9039573669433594	Then we'll see where that takes us and what ID of the book have been explored.
28018	30520	A	0.7117183804512024	And that'll be the main focus.
31450	39286	A	0.934112548828125	Also, just a note on the math learning group, those meetings have been occurring on 19 UTC on Wednesdays.
39478	44170	A	0.8180490732192993	And so in the math learning group, you'll find information about how to participate.
45870	50678	A	0.5250394344329834	You'll find and be able to edit resources on learning different topics.
50854	61840	A	0.9511428475379944	So this is a really helpful activity if people want to share the resources that help them learn about different ideas and that'll help people search for a video about this.
62790	69220	A	0.7300413846969604	We have a notation table that will help us as we start to look at the notation more.
69910	81878	A	0.8808097839355469	There's some math learning basic questions, like around math, anything related to the textbook content can go in the main questions.
81964	84854	A	0.8692957162857056	Math learning group is part of the real textbook group.
84972	91606	A	0.5951036810874939	So these are not the questions to be answered like the other ones that we're going to go to are, but this is just about math.
91718	95414	A	0.8723325729370117	And then also there's some math oriented overviews.
95542	99798	A	0.8807497620582581	Like, I wrote this summary of the first two thirds of chapter two.
99984	108560	A	0.6593542098999023	So if people want to be like, continuing to distill and represent what they're learning and what they're asking, that's all good.
108930	117140	A	0.9084571599960327	Okay, any other notes that anybody wants to add before we start to go into the questions?
118470	121410	A	0.8738597631454468	You can raise your hand or you can type it in the chat.
125450	133858	A	0.8916333913803101	Okay, so we're in the first week of discussing chapter two, the low Road to active inference.
134034	139960	A	0.8789002299308777	So just to kind of warm us up, what is the low road?
174160	174524	B	0.6625792980194092	Got?
174562	179740	B	0.8040109276771545	A low road is the Bayesian Brain approach.
179820	192164	B	0.8880826830863953	Right here is the Proximate next big concept, I guess previous, but it's kind of based on I guess.
192202	196080	B	0.7956519722938538	Yeah, I mean, you can see right in the picture, it's based on basing it's like mathematical.
196240	226188	B	0.7166586518287659	The way I conceptualize it is like literally the low road, meaning very primitive, very mathematical, very hyper analytical, foundational, kind of an analysis way of looking at it, I guess that's not necessarily living or anything like that.
226274	226910	B	0.7360090017318726	Right?
231690	236280	B	0.869617223739624	Whereas the high road is kind of more about the other side of that.
239850	240760	B	0.4896698594093323	All right.
242490	247322	A	0.8214039206504822	Okay, so we're going to be learning about the high road and low road.
247376	251594	A	0.9203340411186218	Those are the two paths that are offered in chapter two and three of the book.
251712	264430	A	0.9804410934448242	So hopefully, again, it would be awesome to have as many people as possible speak and feel welcome to share their views during this session.
264850	278814	A	0.7475209832191467	So everyone is especially encouraged to put questions or comments in the chat or raise their hand to speak or find other times to have synchronous communications and especially to make contributions to the coda.
278862	284280	A	0.9008767008781433	But really appreciate that everyone has come out to go through these discussion questions.
284810	291186	A	0.9192563891410828	So we're going to now go to the questions that have been placed into the questions table.
291378	296274	A	0.7002063989639282	And just the more questions, the better they can be super short.
296412	299178	A	0.6783305406570435	It can be anything that you're having uncertainty about.
299344	306560	A	0.8565199375152588	If you come across a table or a figure or a sentence in the text and you have a question, please write the question down.
307010	310030	A	0.9496765732765198	It will be extremely, extremely helpful.
311410	317898	A	0.7883785367012024	It doesn't have to be with extensive references or citations.
317994	329774	A	0.820275068283081	It can just be what does this mean if you're uncertain about something and then if you were certain about it, you could still write a question like what would you ask somebody to test or to understand what they understood about it?
329912	337222	A	0.8701030015945435	Okay, so we're going to start with this first most upvoted question.
337356	348618	A	0.7091231942176819	But there's still time in this session and also till next week in our next session on chapter two, to add more questions if you wrote them down somewhere else and also to upvote questions.
348704	352540	A	0.7584662437438965	But we're just going to start with the most interestingly voted questions.
355970	365200	A	0.9066937565803528	What is the relevance of the support and surprise for the different probability distributions in table 2.1?
368070	388978	A	0.8845059275627136	So what does anyone see in table 2.1 and what are support and surprise people?
389084	390140	A	0.5074729323387146	Mike yes.
394430	403050	C	0.8460605144500732	So table 2.1 looks to me like a guide for selecting distributions.
403210	415054	C	0.9209095239639282	If you're building out one of these models with support saying what set of numbers would fit within this distribution selection?
415102	420740	C	0.8978812098503113	And then if you chose this distribution, how would you look at the support equation for it?
423590	427222	A	0.861257016658783	Can you unpack with a surprise part?
427356	428040	C	0.5491447448730469	Yeah.
428730	433000	C	0.894404411315918	If you chose this distribution, how would you look at the surprise component of it?
442360	447430	A	0.5374181270599365	And we're going to get to surprise and Bayesian surprise in the coming questions.
448040	463464	A	0.8319612741470337	So support using the definition pulled here from Wikipedia, the support of the function is the subset of the x axis, like the domain where there's a y value that's not zero.
463582	467080	A	0.7558575868606567	So it's where the function is nonzero defined.
468060	478844	A	0.7707902193069458	The Gaussian, the normal Belker of distribution, has nonzero support at all numbers.
479042	489280	A	0.8371380567550659	Fancy r and then other distributions have different variable, different values that they're having non zero values at.
489350	494424	A	0.8547399640083313	Like the gamma distribution is defined over the interval from zero to infinity.
494572	503270	A	0.8460549712181091	So if it was like how many of this thing do you have that might be a reason to use a distribution that's only positive numbers?
503960	526460	A	0.8061790466308594	Or if you were wanting something that could have positive or negative numbers of any size, the Gaussian is a distribution and then another one that's like a relevant support difference would be like the deer Schlee being defined only within the zero to one interval, which helps connect it to probabilities.
526880	534156	A	0.8271497488021851	So the support is where that function, that distribution family is defined.
534348	537920	A	0.7835462093353271	And then what is the surprise?
557450	561510	A	0.8785325884819031	Does anyone know the name of the little squiggle they are using to represent surprise?
562330	588560	A	0.8182380199432373	I think it's called Fracture i, aka fancy eye.
598200	615080	A	0.7607255578041077	We're going to come to the definition of surprise in a coming question and without going into every variable and trying to overlearn something, the surprise of a given observation, like a data point, let's just think about the Gaussian.
615740	626152	A	0.8687911033630371	X is the observed data point and Mu is like the mean of that normal distribution, which is also the mode.
626216	628780	A	0.8440132737159729	So it's like the center of that distribution.
629200	635296	A	0.8911688327789307	And then this capital pi is a variance estimator, I believe here.
635478	645250	A	0.7810176014900208	So how surprised one should be by a new data point coming in is related to the difference between that new data point and the mean.
646100	653428	A	0.5685380697250366	So if the new data point were exactly on the center of the distribution, this whole term is going to go to zero.
653594	659920	A	0.6488406658172607	So one should not be surprised at all by exact data points on the mean of the distribution.
660000	674620	A	0.7598146200180054	But any data point that has not exactly on the mean is going to have some non zero value and so it's going to be a function of how far the data point is from the mean and some scaling by the variance.
675040	687392	A	0.8420193791389465	So if we were expecting 100 with a variance of ten versus 100 with a variance of one in one case, like getting 101 is like a one sigma event.
687446	689090	A	0.8284581899642944	It has a z score of one.
689620	699620	A	0.6875083446502686	Just speaking loosely here, whereas in the case of it having a variance that's very small, then that might be more surprising.
705130	710940	A	0.8987060785293579	Any other comments that people have on Table 2.1?
712270	714140	D	0.6957666277885437	I have a question.
715310	716060	A	0.5491447448730469	Yeah.
717550	729150	D	0.9015594720840454	So while I was reading through the chapter going back to section 2.2, right, perception as inference.
729570	738180	D	0.7575438618659973	So when he says, when they say that the brain is a predictive machine or a statistical organ, that infers external states of the world.
739830	744850	D	0.8575977087020874	Okay, here's my question, because I'm primarily from a telecom background.
745190	755094	D	0.7371208667755127	So most of our things are to get messages crossed between two distant transmitters receiver, right?
755212	757106	D	0.6702587604522705	So we build noise models.
757138	760806	D	0.5984171628952026	So without any noise you don't get signal.
760918	764060	D	0.7554247975349426	So you need to know what noise model looks like.
766670	775454	D	0.7717528939247131	For example, if the power spectrum of the signal is what you expect from, let's say we are using a Gaussian distribution of noise, right?
775572	781182	D	0.6242651343345642	So if it's the same as you expect from the Gaussian, you do nothing.
781236	782202	D	0.5458080768585205	So that's just noise.
782266	785040	D	0.6319053769111633	So you don't classify the data point that's coming in.
785890	792146	D	0.8883795142173767	If it is more than this, then okay, so there might be something interesting about this.
792168	793234	D	0.7562769055366516	There might be some signal here.
793272	795810	D	0.8351356983184814	So we define that signal to noise ratio.
796470	805800	D	0.6946947574615479	A lot of this chapter seems to be dealing with the fact that we by default know what the signal looks like.
806250	809240	D	0.768427312374115	So there's no attempt to build a noise model here.
809630	832478	D	0.6665465235710144	That would be like, okay, so maybe the sensor in this case, our eyes for example, has some parameters it can detect between, say, a bandwidth, okay, so it can detect light between certain frequency ranges, but maybe it performs very well in a certain frequency range and not others.
832564	836498	D	0.7074897289276123	There's more error in other frequency ranges, right?
836584	847006	D	0.5942375659942627	So the noise model should incorporate such things, imperfections in the sensor, imperfections in the connections between the sensor and processing equipment.
847198	849826	D	0.6399953365325928	But there's no attempt to do that in this entire chapter.
849858	855734	D	0.8603127002716064	So I'm just curious as to what exactly is the predictive machine here?
855932	857800	D	0.8695257306098938	What is being predicted here?
858810	859686	A	0.6860781908035278	Great question.
859788	866700	A	0.6868748664855957	And in general, every question that people have, they're going to be 100 times more impactful if they can be written down.
867870	869622	A	0.754821240901947	This is just the second chapter.
869686	874666	A	0.8004028797149658	It's introducing us to the essence of starting with just Bayes equation.
874858	883194	A	0.7754760980606079	It's the first equation here, starting with the foundation of increasingly advanced noise filtering approaches.
883242	894754	A	0.7450433373451233	So check out Livestream 43 to see how colman filter and advanced multilevel Bayesian filtering schemes are used and learned and fit with free energy, minimization and so on.
894792	911100	A	0.7363781929016113	But you're right, that is not approached in this chapter because it's starting with the essence of Bayes equation and then going to build towards another kernel, which is going to be the kernel of the variational free energy.
912670	926878	A	0.7859821319580078	And then that in this one layer case is going to be able to be elaborated and parameterized and fit within nested models to accommodate, like, learning on hyperparameters, learning of noise models and so on.
926964	928830	A	0.5894176363945007	But it is not in this chapter.
929890	934334	D	0.6284075379371643	Okay, yeah, I'm sorry if I threw us off track.
934452	935860	A	0.6406206488609314	No, it's perfect.
936310	937922	A	0.6102379560470581	But they're good questions.
937976	946530	A	0.8733943700790405	And then Jacob asked, what do you think of the surprise, the distributions involving a sum product of the project, probability distributions, except for gamma.
951990	955140	E	0.8585332036018372	Actually, I guess the Gaussian kind of as well.
956950	970290	E	0.5643265247344971	Yeah, maybe I'm thinking about this wrong, but I thought that the surprise was like the information gain or equivalent.
970450	982506	E	0.8524086475372314	And so for the Gaussian and the gamma distributions, it's always for one single element of the distribution for one x.
982688	990560	E	0.8191322684288025	But for the multinomial and derivative lay, there's something over all of the possible probabilities, which makes it, I guess, an entropy function.
991330	995120	E	0.7901498675346375	But is there a difference?
1004440	1019396	A	0.8462311625480652	For example, one could have a multidimensional Gaussian, a two dimensional Gaussian, and then the surprise for the two dimensional Gaussian would be represented as a sum over the different dimensions.
1019508	1027790	A	0.8788650035858154	So they've shown the Gaussian and the gamma in the uni dimensional case and the multinomial and the zero schlay in a multidimensional case.
1028320	1030590	E	0.6915220618247986	So in that case, is it.
1033280	1033596	A	0.5497900247573853	I.
1033618	1037100	E	0.7806390523910522	Thought that x was just one element of the domain.
1038320	1055190	E	0.8368163108825684	So x is just if we think of the Gaussian as this kind of bell shaped curve on the x and y axis, we really want the f of x value to get like the Gaussian value, but x is just the input value.
1057720	1058356	E	0.8415143489837646	Is that correct?
1058362	1058600	A	0.5708538293838501	Believe I.
1058670	1065210	A	0.6673975586891174	That is accurate because, yes, x is the domain of the function and then the sum is over eyes.
1066700	1074670	A	0.8766024112701416	So if we had a multinomial Gaussian or multi dimensional Gaussian in I dimensions, then it would have a sum over eyes.
1077360	1088144	E	0.5893251299858093	Yeah, I guess I still have uncertainty about why we don't do that for the single dimensional Gaussian as well.
1088182	1097824	E	0.7539536952972412	Because if we take just one point on the Gaussian, I understand why that would have a certain surprise.
1097872	1109860	E	0.8121195435523987	But then in the same way, if we take one point in the Dirichlet distribution, shouldn't that also have an equivalent equation for surprise?
1110760	1122780	E	0.7252932190895081	Why in the Dirichlet distribution are we summing over all the eyes, but in the Gaussian we are considering just one point on the probability distribution?
1123600	1128504	E	0.8700857162475586	Or maybe Derishley is multidimensional.
1128552	1137090	A	0.7844530344009399	And that's the reason I think the uni dimensional case would be just without this sigma I.
1138420	1139970	A	0.7573757767677307	It would just be this.
1142740	1150468	A	0.8817599415779114	But if anyone has any other thoughts, especially if they want to contribute them in writing to improve our understanding, that'd be awesome.
1150634	1152470	A	0.8348679542541504	Let's continue to the next question.
1155400	1160948	A	0.7814595699310303	In Section 2.4 they write we now introduce the simple but fundamental advance offered by active inference.
1161044	1166920	A	0.8439461588859558	This starts from the same inferential perspective discussed above, but extends it to consider action as inference.
1167980	1172700	A	0.8539544343948364	In your own words, what is the simple but fundamental advance offered by active inference?
1174240	1179470	A	0.9172936081886292	So what would anyone write or like to share?
1180800	1183100	A	0.8953737616539001	What is the advance of active inference?
1197660	1201176	F	0.6092166304588318	Sorry, is it just bringing action into the equation?
1201368	1210700	F	0.8620577454566956	So predictive coding and Bayesian brain are specifically about perception, visual perception.
1211940	1229540	F	0.5917028784751892	I think the key thing with active inference is that it's well, it's not just that it brings action into the equation, it's that it treats them as essentially the same thing, that they're kind of united as part of the same process, which I think is kind of a new development.
1231420	1235960	A	0.9839863777160645	Thanks for the awesome answer, Ali.
1240050	1254702	G	0.5041332840919495	Yeah, I think Alan Bartheau, or Bartho's in the brain's sense of movement, has a particularly relevant description of action and perception.
1254846	1259746	G	0.7190845012664795	And he writes that perception is simulated action.
1259938	1267634	G	0.5661290287971497	And I think that really speaks to the advance made by active inference.
1267682	1272170	G	0.8269068598747253	I mean the integration of action and perception.
1272510	1286990	G	0.49442288279533386	Or in other words, as Bartheau claims, the simulation of action and perception is, I guess, really novel development.
1290930	1297874	F	0.842559814453125	I think I read something, but I can't remember where I read it, but I think it was roughly that.
1297992	1308050	F	0.8625756502151489	On these existing Bayesian brain theories, an agent perceives in order to act, whereas under active inference an agent acts.
1308410	1311106	F	0.6904371380805969	Action kind of precedes perception.
1311298	1319160	F	0.8386478424072266	So active inference essentially flips this traditional schema in terms of which one feeds into the other.
1321550	1329020	B	0.8060020804405212	Yeah, I was just going to say that adding on to bringing action into the equation, it flips it.
1332430	1333626	B	0.7424543499946594	Where do your priorities come from?
1333648	1335754	B	0.7601415514945984	Where do the hyper parameters get specified?
1335882	1338590	B	0.8358424305915833	Where does the model start?
1338740	1352610	B	0.5914744734764099	Where is the agency kind of in all of these other models, really no affordance for that or it's hard to very emergent as here it's very explicitly specified.
1353830	1355460	B	0.7060860991477966	Some affordance for that.
1356150	1359880	A	0.8328459858894348	Thanks brock, Mike and then anyone else who raises their hand.
1361690	1375738	C	0.8291234970092773	Yeah, so I'm thinking if you have priors you have some prior assumptions, then do you necessarily need to take action in order to sort of envision what your perception might be?
1375824	1378426	C	0.7952739000320435	So it's almost like getting into imagination or something like that.
1378448	1386480	C	0.7773786187171936	You have a set of priors that can drive what you imagine your perception to be before any action is taken.
1392010	1394230	A	0.8386445045471191	Great question, Blue.
1397930	1421230	H	0.7588959336280823	So I think that this was elaborated on a lot in Section 2.7 when they talk about pursuing a policy because there's consequences when we're planning and making predictions, that has consequences for both our perception and our action.
1424610	1434740	H	0.5709373354911804	Even in a conversation when someone says, oh, have a nice day is expected at the end of that, but have a nice rabbit would be like, what?
1435270	1439890	H	0.6026911735534668	You're not expecting that to come at the end of that sentence.
1440050	1446434	H	0.5048006176948547	So when you plan, it interferes with your perception because you didn't plan for them to say have a nice rabbit.
1446482	1450374	H	0.6971754431724548	Right, but it also interferes with your action.
1450422	1465040	H	0.8058035969734192	And so I think the novelty or enhanced aspect of active inference loops into the fact that perception and action are continually impacting one another.
1467970	1468814	A	0.46103888750076294	Yes.
1469012	1470270	A	0.7864035367965698	Thanks, Blue.
1472770	1482978	A	0.5139392614364624	These are some of the core memes and themes, and it really is important to understand how active inference is similar and different than other work in this area.
1483064	1484366	A	0.7398562431335449	Like variational.
1484398	1488050	A	0.6742706894874573	Bayesian inference is not introduced by active inference.
1488570	1492674	A	0.7119236588478088	Bayesian models of perception is not introduced by active inference.
1492802	1496866	A	0.6864306926727295	Bayesian models of action is not introduced by active inference.
1497058	1519262	A	0.8500396609306335	So it's about finding what has been done to understand what is being offered and then whether one chooses to take this, like, history of science, development of science view or we kind of just want to state plainly what it is without wondering what the advance relative to other frameworks is.
1519396	1521418	A	0.7588493824005127	These are all really critical ideas.
1521514	1535860	A	0.8346431851387024	Like the signal processing framework was brought up earlier and predictive processing, predictive coding, anticipatory systems are often purely about sense.
1536550	1551050	A	0.6263333559036255	And just to give one thought on that, it kind of makes sense for a video encoding algorithm, for example, because every piece of the camera is in focus, but vision requires action.
1551870	1577570	A	0.49725911021232605	And so that's where we're going to be bringing in all these other important concepts like attention, sensory attenuation, and the active decision making component of vision to resolve uncertainty, which is what gives rise to a generative visual field that seems like there's color everywhere and seems like there's high resolution everywhere.
1578070	1581540	A	0.7179404497146606	Though that is not the incoming sensory information.
1582150	1598338	A	0.9328771829605103	So by fully taking this generative modeling perspective on perception, which in live stream number 43.0, maria did an awesome job of connecting this even before Helmholtz and Kant.
1598514	1604970	A	0.8734512329101562	It's Plato's cave and it's part of this long discussion about perception.
1605310	1608170	A	0.7529064416885376	And action could be a variable.
1608830	1628210	A	0.879321277141571	So there's perceiving and then if action is a variable, then active inference is providing not just a unified framework, like a Bayesian graph framework, for some variables that are interpreted as perception and some variables that can be interpreted as action selection or policy planning.
1630470	1655450	A	0.7357847094535828	But there's a tractable approximation by shifting from the in reading this answer, by shifting from an exact solution to the mathematical problem Bayesian inference to an approximate solution variational free energy, which is going to provide a bound on some quantity that might be intractable to compute.
1655950	1660890	A	0.8880780935287476	So it's going to give a heuristic for the action perception cycle.
1664050	1667214	A	0.8782256841659546	But this is a really great question.
1667412	1677006	A	0.8147218823432922	So people can continue to return to it because people will probably ask us for a long time to come, how is this different than blank?
1677118	1679214	A	0.8527169227600098	Or what is active inference?
1679342	1682306	A	0.6244781613349915	These are some of the things that we would want to have in mind.
1682408	1683250	A	0.6077883839607239	Ali.
1686230	1699746	G	0.8267490863800049	Yeah, again, going back to Bartheau and his definition of perception as simulated action, I think it also relates nicely to Merlopanti's phenomenology.
1699778	1709610	G	0.7873049378395081	Merlopanti has a famous statement and has a famous insight as vision is the brain's way of touching.
1710270	1733540	G	0.8426729440689087	And so you see, for instance, as we move around space, as we construct our sense of spatiality or our sense of temporality or everything else, we don't just begin with the representation of space or time.
1733910	1744070	G	0.8435897827148438	I mean, the image of movement constructs because we move and not as a consequence of our movement.
1746330	1746934	A	0.6283750534057617	Thanks.
1747052	1761046	A	0.905703067779541	And we'll continue with the questions, but that's an awesome area to go into with phenomenology with four E cognition extended, embedded in culture, et cetera, et cetera.
1761238	1765754	A	0.8626745939254761	And many, many of the live streams and papers have been on those areas.
1765882	1766654	A	0.939627468585968	So it's cool.
1766692	1775890	A	0.8022275567054749	It's a bridge from formal models of perception, cognition, and action into qualitative and philosophical areas.
1776550	1777300	A	0.584351658821106	Okay.
1783590	1801220	A	0.6107240915298462	And then oh, thanks, Morris, for adding that the idea of interrelated action and perception has been around in cognitive science for a long time, like dynamical systems theory and inactivism emphasize that.
1801290	1805428	A	0.8956731557846069	But active inference brings a coherent formalism to the table, which is a nice advance.
1805604	1806570	A	0.7286635637283325	Well said.
1807660	1808824	A	0.6033201217651367	Next question.
1809022	1810730	A	0.751184344291687	Figure 2.2.
1819820	1820427	A	0.7339675426483154	Figure two.
1820427	1828830	A	0.9115797877311707	Two point illustrates Y as a result of an internal generative model X and an external generative process x star.
1833940	1844944	A	0.8126428127288818	In order to measure surprise, wouldn't we need another value of Y-I-E-A separate Y that encodes prior beliefs, if Y can be objectively measured from external signals?
1844992	1847700	A	0.8977228999137878	Is there a third Y that's considered the observation?
1855400	1870404	A	0.8471432328224182	First, I'd like to actually go to this question about two notions of surprise because they might come to bear on this action perception loop.
1870452	1893936	A	0.8209460377693176	Once we move from just Bayes equation and calculating surprise on parametric distributions to thinking about prior updating and cognitive entities with a generative model that constitutes a prior, with incoming information coming in, we're going to start to see why this notion of surprise and Bayesian surprise are similar and different.
1894038	1908470	A	0.8517847657203674	So using the apple frog example so the unobserved hidden state, the latent state of the world, is whether there is an apple or a frog in a bag, for example, or just in this person's area.
1908920	1919128	A	0.6792523860931396	And then what can be observed, just speaking coarsely, is it can jump or not, and we can totally go down the rabbit hole with what is truly observed and things like that.
1919214	1922280	A	0.7721927762031555	But this is just what are in the context of this model.
1922350	1929116	A	0.8343315720558167	The unobserved state is the actual identity of the object and the observation is going to be the action or not.
1929298	1933810	A	0.5504566431045532	We take the opportunity to unpack two different notions of surprise, both of which are important.
1934500	1937840	A	0.7987893223762512	The first we refer to simply as surprise.
1938180	1943836	A	0.6524014472961426	It's the negative log evidence where evidence is the marginal probability of observations.
1944028	1956736	A	0.864798367023468	So we saw that first sense of surprise with the fracture I in the previous table we looked at the second notion of surprise is referred to as Bayesian Surprise.
1956928	1962920	A	0.890439510345459	This is a measure of how much we have to update our beliefs following an observation.
1963340	1968920	A	0.8392162919044495	In other words, Bayesian Surprise quantifies the difference between a prior and a posterior of probability.
1970000	1973500	A	0.887897789478302	What are the similarities and differences between the two notions of surprise?
1975440	1980830	A	0.8634470105171204	Okay, we'll see what has been said and then hear what other people are thinking.
1982740	2002230	A	0.6619299650192261	So, page 20, they wrote that Bayesian Surprise scores the amount of belief updating as opposed to surprise, which is simply how unlikely or likely that observation was.
2004760	2010152	A	0.7868885397911072	So in that Gaussian case, the surprise is like about one data point coming in.
2010286	2016792	A	0.8051851987838745	Given how the distribution is parameterized right now, how surprising was that one data point?
2016926	2025116	A	0.887832760810852	And then Bayesian Surprise is going to be about how much that distribution is updated after processing that data point.
2025298	2026024	A	0.7477992177009583	Similarities?
2026072	2030030	A	0.8596318364143372	They both depend on how well the Agent's Generative model matches the external world.
2030640	2039152	A	0.8575678467750549	And they're both measuring Surprise or Bayesian Surprise in the same units, which are information theoretic quantities of information, i, e.
2039206	2040524	A	0.724517822265625	Nats or bits.
2040652	2041292	A	0.6981911063194275	Differences.
2041356	2052784	A	0.49574393033981323	Plain Surprise marginalizes over all the model's degrees of freedom under the model's prior distribution over its adjustable parameters that would be great for someone to unpack.
2052832	2056720	A	0.6618239283561707	What does it mean to marginalize over the model's degrees of freedoms?
2056880	2072600	A	0.8180392980575562	And then Bayesian Surprise lets the model choose the best set of parameters it can to fit the observed data and then measures how much of an update that was Mike and then Blue and then Ali.
2073520	2073932	A	0.5491447448730469	Yeah.
2073986	2086690	C	0.8659684062004089	Under the first similarity, I'm wondering if it's this dependency on how well the Agents Generative model matches their perception of the external world as opposed to matches the external world.
2088260	2089072	A	0.6654933094978333	Agree with that.
2089126	2096096	A	0.9041651487350464	Great addition blue and then Ali.
2096208	2096532	E	0.584351658821106	Okay.
2096586	2097844	H	0.7801937460899353	I think my hand was just left over.
2097882	2098324	H	0.645361065864563	Left up.
2098362	2098708	A	0.584351658821106	Okay.
2098794	2099540	A	0.6637656092643738	Ali?
2104040	2117300	G	0.7971010208129883	Well, I believe that the plain Surprise is a kind of raw statistical surprise, but Bayesian Surprise is a kind of surprise is a kind of processed surprise.
2117380	2133772	G	0.665858805179596	Or I'm not sure if I'm right in saying that plain Surprise can probably be described as an objective surprise as opposed to Bayesian surprise prized as a subjective surprise.
2133916	2142800	G	0.5012024641036987	But as I said, I'm not sure about the plausibility of these descriptions.
2144340	2145940	A	0.8596616983413696	Great comments.
2147960	2156150	A	0.8690224885940552	Even the surprise alone still depends on the parameters of the generative model.
2156600	2163610	A	0.8833524584770203	So it still is within a processing or a filtering frame, albeit a fixed one.
2164620	2173740	A	0.8983628749847412	So they talk about like, let's look at the figure where there's a graphical overview of this apple and the frog.
2177200	2184252	A	0.7188080549240112	And this is kind of giving some graphics and words to fill in this apple frog example.
2184386	2193420	A	0.8113967776298523	So initially the person has this likelihood model in the back of their head where they have beliefs about how likely apples are to jump.
2193580	2197216	A	0.8171261548995972	They do it 1% of the time and how likely frogs are to jump.
2197248	2199110	A	0.8036892414093018	They do it 81% of the time.
2199960	2201344	A	0.7802634835243225	That's the likelihood.
2201472	2207110	A	0.8521232604980469	That's about how observations depend on hidden states of the world.
2208860	2217720	A	0.7006093859672546	Their prior beliefs are that there's a 10% chance that the entity is a frog and these are mutually exclusive options.
2217790	2219768	A	0.622799813747406	There's no third option here.
2219934	2224940	A	0.8063852190971375	That would be like another category model structure learning on the model.
2225010	2230860	A	0.8750056028366089	So we're staying within this model for now and they sum to one because they're a probability.
2231200	2241248	A	0.904789388179779	Then there's an observation which is jumping and then the posterior reflects the updated beliefs about what the entity is.
2241414	2251910	A	0.6290000081062317	And so it's like, it's so much overwhelmingly more likely that frogs jump that seeing something jump updates the prior from here to here.
2255080	2263464	A	0.8303107619285583	So in this case, one can calculate the surprise of the observation without doing any updating at all.
2263582	2273820	A	0.8604809045791626	One could just stay fixed in their prior belief and then could describe how surprised they are in Nats by given observation.
2275200	2289840	A	0.8950265049934387	In this full Bayesian cycle, there is an updating of the prior to the posterior and then that updating can be described in terms of how much the prior was updated.
2290820	2296820	A	0.8733675479888916	And so that's like here, the model is updated to the observed data.
2296890	2298710	A	0.9253101348876953	This is now the best fitting model.
2299800	2303860	A	0.8415864706039429	And then we're calculating the difference between these two distributions.
2305000	2313240	A	0.6984872817993164	So regular surprise being zero means the data point was exactly as you expected.
2313900	2318120	A	0.4961588382720947	Bayesian surprise being zero means the distribution was not updated.
2319020	2323256	A	0.5899940133094788	High surprise means that the data point was extremely unpredicted.
2323288	2328430	A	0.5179792046546936	It was extremely unlikely whether or not you update your model at all.
2328880	2335810	A	0.7583684325218201	High Bayesian surprise means that the distribution was changed a lot as a function of seeing that happen.
2336740	2337680	A	0.6077883839607239	Ali.
2340260	2368920	G	0.6601216197013855	Well, perhaps I didn't understand it correctly, but isn't it the case that in the plain surprise or more generally in the Bayesian formulation of the probability of events, the likelihood and both the likelihood and the priors are somehow inherent to the phenomena inherent to the events, independent of the observers?
2371990	2373042	A	0.7677177786827087	Yeah, great question.
2373096	2381118	A	0.6887062788009644	I hope I'm not going off on a branch here, but this is related to the difference between frequentist and Bayesian approaches to statistics.
2381294	2389538	A	0.8849108815193176	So Frequentism does have viewed from the Bayesian perspective, frequentism does have priors.
2389714	2395026	A	0.7504149675369263	They're uniform priors, which are not uninformative priors, they just are uniform.
2395218	2418830	A	0.7799776196479797	And so in frequentism we come across like the maximum likelihood solution or the maximum likelihood parameter, which is just like, well, if all outcomes were equally a priori likely uniform prior, then we would just need to evaluate the likelihood and find the model with the maximum likelihood solution.
2419650	2429810	A	0.5586516857147217	Bayesian offers another degree of freedom and says, well, sure, you could pick a uniform prior that would give you the maximum likelihood solution.
2430390	2456170	A	0.7986254692077637	Or you might want to have a prior distribution over that space so that if something is twice as likely a priori and then you observe less than two X evidence for it, you still might want like the Bayes factor or your posterior to reflect that one rather than just jumping instantly to a different maximum likelihood solution.
2456670	2464058	A	0.7899110317230225	So yes, priors and likelihood are implicit, but they're using a different ontology than Bayesian statistics.
2464234	2468320	A	0.8832879662513733	But we're in the bays or the post bays area now.
2471830	2486950	A	0.7884628772735596	But there's so many connections to classical statistics and in SPM, the textbook there is parametric classical statistics, non parametric classical statistics, and Bayesian statistics.
2487690	2491560	A	0.6531229615211487	So they're more similar than not.
2492090	2498620	A	0.7966036200523376	It just is about seeing where one of them is like a special case of another or a generalization of another.
2505070	2511870	A	0.7904308438301086	Okay, let's see if we can do one or two more questions during this session.
2514050	2516560	A	0.8281826972961426	Okay, so let's return to the previous question.
2517010	2532580	A	0.8782776594161987	So we're looking at figure two, which is something we're going to see different representations of this entity as generative model, world or niche as generative process.
2533770	2539240	A	0.6906264424324036	And then here the hidden states are those that are unobserved as data.
2539930	2543800	A	0.8835005760192871	Y is referring to data points that are observed as data.
2545210	2550890	A	0.8148035407066345	There's a cognitive hidden state which is like a prior in this generative model.
2551040	2554394	A	0.8267399668693542	And then there's some hidden state x star.
2554592	2561040	A	0.8260379433631897	But it could have been any letter or any shape about the hidden state in the world.
2562210	2566574	A	0.828690230846405	The observation is going to everything that happens in between.
2566612	2574980	A	0.8219582438468933	Here is cognition, like the sandwich model, like sense, think, act.
2575430	2578782	A	0.7805488705635071	That type of model is just referring to that little boomerang.
2578846	2583650	A	0.832800030708313	Data come in cognitive processing, action selection.
2584330	2585960	A	0.8408393859863281	So we're in that figure.
2587050	2596350	A	0.8148293495178223	The question was, in order to measure surprise, wouldn't we need another value of Y-I-E-A separate Y that enclose prior belief?
2596530	2598060	A	0.6860781908035278	Great question.
2599550	2604780	A	0.8160848617553711	Let's just assume that there is a Gaussian generative model.
2605150	2611390	A	0.8560358881950378	So the entity has like two parameters in its cognitive model, the mean and the variance.
2612610	2625326	A	0.8801479339599609	Then Y comes in and given the parameterization of the generative model, all that's needed is the data point to come in for the surprise to be calculated.
2625518	2628500	A	0.7638352513313293	So another value is needed.
2629190	2636818	A	0.8453673124313354	But whether we call it ABC XYZ is just a mathematical abstraction.
2636994	2640600	A	0.7966354489326477	So yes, prior beliefs are important.
2641130	2653370	A	0.9019941687583923	The prior beliefs that can be interpreted as the parameterizations of the cognitive model are required if Y can be objectively measured from external signals.
2654590	2655946	A	0.8586565852165222	Is there a third Y.
2656048	2658010	A	0.8554201722145081	That is considered the observation.
2658510	2672446	A	0.7994726896286011	So one could imagine a lot of like, real world scenarios where a more complex model would be required, two people looking at the thermometer or all these different sorts of situations.
2672638	2686778	A	0.5791244506835938	But unless somebody can unpack this a little more or explain what they were asking about, then I don't think a third why is required.
2686874	2692240	C	0.8249436616897583	Mike yeah, it was my question, so I can try and unpack it.
2692930	2713346	C	0.8909833431243896	And so sticking with the thermometer example at the heart of that last question is, if we have a thermometer that's registering the temperature, can we consider that as sort of the true observation that is the actual Y as measured by this instrument?
2713458	2718840	C	0.8642843961715698	And then therefore we can compare our sort of internal Y with the actual Y?
2723070	2734726	H	0.7873154878616333	Okay, so I have a really similar and related question that's like two questions down, but it's in a different section.
2734758	2742510	H	0.8787712454795837	But let me just place some things on here and it goes back to what we were saying about what exactly is the hidden state.
2742660	2753822	H	0.8843807578086853	So is the hidden state the temperature and the data is reading from the thermometer that's like the data Y, right?
2753876	2756050	H	0.7772151827812195	Like this Y in the middle.
2756200	2759410	H	0.8238738775253296	But then what is how I feel hot or cold?
2759750	2762434	H	0.5782747864723206	I'm perfectly positioned to be at 75 all the time.
2762472	2768760	H	0.5555068850517273	So I know if it's like one degree too cold or one degree too hot, I've got to turn up the heat or turn it down or whatever.
2769210	2774162	H	0.6412435173988342	But my registration, my sensory input is not at 75 degrees.
2774226	2776658	H	0.8305932283401489	My sensory input is I'm hot or I'm cold.
2776754	2781194	H	0.8570644855499268	And so this is like where I mean, we were talking about this yesterday in the math group.
2781312	2783898	H	0.6628977060317993	There's this really fuzzy line for me.
2783984	2785930	H	0.5837506055831909	I would love for someone to clarify that.
2786000	2788240	H	0.7927858829498291	So I do get what you mean about this.
2788850	2790286	H	0.7485878467559814	Shouldn't there be another why?
2790388	2795040	H	0.5744845867156982	There's the why out there in the world, 75 degrees and then there's how I feel about that why.
2796850	2797920	A	0.6231660842895508	Okay, thanks.
2798550	2806322	A	0.7292802333831787	So variables are not like, innately tagged with being observables or hidden states.
2806456	2817190	A	0.877415120601654	It's a model specific framing of what is going to be modeled as generated data and what is going to be modeled as a Bayesian prior.
2817530	2825290	A	0.8572326898574829	In multilevel Bayesian modeling, the priors themselves are generated from a higher or deeper level of the.
2825360	2828518	A	0.8091906905174255	So one can be serving, in fact, multiple roles.
2828614	2839162	A	0.8651321530342102	This is the minimal prior generated data expectation maximization type, single layer Bayesian kernel.
2839306	2841760	A	0.909616231918335	So we're going to think about this temperature example.
2842610	2850302	A	0.8311230540275574	There's a hidden state of the generative process which is going to be the temperature of the world latent unmodeled.
2850366	2853102	A	0.5927680134773254	So this isn't even claiming that there is such a thing as temperature.
2853166	2862870	A	0.7823033332824707	This is just a latent unobserved temperature that is giving rise to thermometer readings, which might have, like, different sorts of noise.
2864090	2868950	A	0.7871819734573364	Then there's another hidden state, which is the evaluation of temperature.
2870570	2875210	A	0.4849557876586914	And so this is just a schematic.
2877230	2885370	A	0.878694474697113	And also, whether one is labeled x or Y or L or triangle is like a norm and a convenience.
2886370	2891760	A	0.528872013092041	But the letter doesn't matter itself.
2893250	2896618	A	0.5951680541038513	It will be used mostly consistently within the textbook.
2896794	2902020	A	0.5823155045509338	But there's only so many letters and there's not a lot of coherence on notation use.
2908740	2911010	A	0.8934413194656372	What else could be explored here?
2914680	2915284	A	0.5491447448730469	Yeah.
2915402	2916820	A	0.7641675472259521	Mike and then Ali.
2919320	2926912	C	0.613796055316925	So as I keep ruminating on this, which is probably more than I should, it seems like there can be infinite wise, right?
2926986	2940650	C	0.7130919098854065	So to the extent that you are taking action which will change your perception, then you are as a result triggering potentially new Y's in the system, right?
2941760	2944988	C	0.7343190312385559	So you just have this infinite potential for what Y can be.
2945154	2949032	A	0.636911928653717	This speaks to the composability and the flexibility of active inference.
2949096	2960384	A	0.8513811230659485	So like Y could be one pixel of visual input, it could be a 4K video, y could be smell and a 4K video, y could be LiDAR and this and that.
2960422	2970580	A	0.9014840126037598	So Y is just the generalized input of sense and then action might be related to this in a very direct way.
2970650	2975024	A	0.8681464195251465	Like Y could be visual input and action could be your eye movement.
2975152	2978330	A	0.8312126994132996	So that case is going to be explored a lot in the book.
2978940	2985044	A	0.673633873462677	However, it could also be you're getting something in and then the actions are just totally unrelated.
2985172	2990350	A	0.7011215686798096	Maybe they don't affect the hidden state, the causal process in the world at all.
2990880	2995100	A	0.8582603335380554	Or maybe actions enable different wise to enter the picture.
2996240	3006800	A	0.7304430603981018	This is just the total essence kernel and then even trivial cases require some more apparatus.
3007860	3008800	A	0.6637656092643738	Ali?
3011460	3011824	A	0.5491447448730469	Yeah.
3011862	3012930	G	0.6957666277885437	I have a question.
3013860	3033610	G	0.7862057685852051	Is it correct to say that the minimization of variation of free energy, aka the Surprise is exactly equivalent to the maximization of expectation, the wholly linear relationship between these two?
3033980	3042780	G	0.6831148266792297	Or possibly we have some kind of plateaued areas between these two extremes?
3046130	3047360	A	0.6860781908035278	Great question.
3047730	3054210	A	0.8114554286003113	So this is something that we'll come to next week in our discussions on chapter two.
3054280	3061506	A	0.49479779601097107	So we have not even discussed variational free energy yet today.
3061608	3071954	A	0.8787911534309387	We talked about starting with a low road on some of the atomic calculations that are going to come into play, like Surprise and Bayesian.
3072002	3080454	A	0.5683490633964539	Surprise in a Bayesian framework and beginning to partition active entities and their action perception.
3080502	3093690	A	0.6774737238883972	Loops in terms of a Bayesian graph that's going to be amenable to flexible modeling of perceptive, cognitive active and out there in the world variables.
3093770	3095550	A	0.7182566523551941	Variables with those interpretations.
3096130	3103710	A	0.8146700859069824	And then variational free energy is going to come into play as a way to bound surprise.
3104150	3108514	A	0.8185167908668518	So let's have some questions and discourse and we'll come to it next week.
3108552	3109682	A	0.829745888710022	But it's an awesome question.
3109816	3110850	A	0.7002503275871277	Jessica?
3113430	3113794	I	0.5325649380683899	Hi.
3113832	3126674	I	0.9012717008590698	Yes, I have a question, I guess related to this figure 2.2, let's say, in regards to how we interpret what we observe.
3126722	3142858	I	0.7911884784698486	So I understand you have a prior that might you're anticipating something like some other people's behavior or how things should be that might be different from what you actually observe.
3143034	3159534	I	0.820805549621582	But a lot of times, at least in terms of when we're thinking like human beings, an action in the world, how you interpret it has to do a lot with your own views or your own experiences.
3159662	3166982	I	0.8185691237449646	And maybe that connects to the priors, but the interpretation might be very different from people to people.
3167036	3173778	I	0.8076155781745911	The interpretation will vary a lot because everybody has different experiences.
3173874	3182570	I	0.7817816734313965	So those could be encoded, I guess, in the priors, but it's not connected per se to the prediction part.
3182640	3183962	I	0.742197573184967	Or maybe it is.
3184096	3204980	I	0.8330148458480835	So I'm trying to kind of connect that idea of our personal interpretation based on our experiences, which could be priors, make us see what's happening in the world differently than other people.
3205350	3213170	I	0.6789257526397705	So that why will look very different for you than for me, just because we have lived different experiences.
3213830	3217160	I	0.5685147643089294	So it's not like an actual why.
3217610	3223842	I	0.5487882494926453	You cannot really say this is a fact because it's an interpretation.
3223906	3225560	I	0.6719300150871277	At the end of the day.
3228830	3231034	A	0.825541615486145	Great question and points.
3231232	3231642	A	0.5491447448730469	Yeah.
3231696	3245760	A	0.8374389410018921	Understanding how the individual setup of a given entity is related to its past experiences and how that could be modeled by priors is an important area.
3246370	3254190	A	0.6024373769760132	And some of those rich dynamics are not in this minimal nucleus.
3255190	3266690	A	0.7585121989250183	But it'll be awesome to start to think about what does have to be in the box here to give rise to those kinds of dynamics.
3270170	3276502	A	0.8610420823097229	So that's going to end this discussion next week.
3276556	3285900	A	0.5032269358634949	We will also be staying in chapter two and taking it more towards variational free energy and expected free energy.
3286510	3289740	A	0.9672176241874695	So that should be fun.
3290590	3297498	A	0.9215796589851379	We're going to in this room transfer immediately to the Tools meeting.
3297594	3302062	A	0.9253777861595154	So if you want to join for active Lab Tools meeting, everyone is welcome.
3302116	3304914	A	0.7540414929389954	And there's no prerequisites or anything like that.
3305032	3311074	A	0.8241737484931946	If you want to hang around and join Tools, stay in this exact gather space.
3311272	3319730	A	0.897367000579834	If you want to continue talking with other people about the book or anything else, just head up into one of the rooms above.
3320070	3324500	A	0.7094593644142151	And if anyone who wants to can just continue any discussion that they want.
3324870	3327710	A	0.8910371661186218	But in this space we're going to continue now with Tools.
3327870	3329740	A	0.9435054659843445	So thanks everyone.
3330270	3330550	A	0.5137447118759155	Bye.

start	end	startTime	summary	headline	gist
1450	121410	00:01	We're in the fourth week of the first cohort of the active textbook group. We're going to go over the questions that are posted for chapter two. Also, just a note on the math learning group, those meetings have been occurring on 19 UTC on Wednesdays.	This is the fourth week of the first cohort of the active textbook group	The Math Learning Group
125450	284280	02:05	We're going to be learning about the high road and low road. Those are the two paths that are offered in chapter two and three of the book. Everyone is especially encouraged to put questions or comments in the chat or raise their hand to speak.	This week we will discuss the low Road to active inference	The Low Road to Bayesian Inference
284810	352540	04:44	If you come across a table or a figure or a sentence in the text and you have a question, please write the question down. It can be anything that you're having uncertainty about. There's still time in this session and also till next week in our next session on chapter two to add more questions.	Please write down questions that you have uncertainty about in chapter two	Questions for Chapter 2
355970	699620	05:55	Table 2.1 looks to me like a guide for selecting distributions. If you chose this distribution, how would you look at the surprise component of it? We're going to get to surprise and Bayesian surprise in the coming questions.	Table 2.1 looks to me like a guide for selecting distributions	The Support and Surprise for Probability Distributions
705130	937922	11:45	A lot of this chapter seems to be dealing with the fact that we by default know what the signal looks like. There's no attempt to build a noise model here. So I'm just curious as to what exactly is the predictive machine here?	There's no attempt to build a noise model here. So without any noise you don't get signal	Phenomenology of Perception as Inference (4)
937976	1150468	15:37	For the Gaussian and the gamma distributions, it's always for one single element of the distribution for one x. But for the multinomial and derivative lay, there's something over all of the possible probabilities. Is there a difference? If anyone has any other thoughts, especially if they want to contribute them in writing to improve our understanding.	Jacob asked what do you think of the surprise in probability distributions	Seems like the Gaussian and Gamma distributions have the same
1150634	1805428	19:10	Active inference treats action and perception as essentially the same thing. Action kind of precedes perception. When we're planning and making predictions, that has consequences for both our perception and our action. The novelty of active inference loops into the fact that perception and action are continually impacting one another.	Blue Helm: What is the fundamental advance offered by active inference	What is the Advance Made by Active Inference?
1805604	1929116	30:05	In order to measure surprise, wouldn't we need another value of Y-I-E-A separate Y that encodes prior beliefs? Is there a third Y that's considered the observation? Once we move from just Bayes equation to thinking about prior updating, we'll see why surprise and Bayesian surprise are similar and different.	Question about two notions of surprise that might come to bear on action perception loop	Bayesian and Prior Knowledge
1929298	2337680	32:09	Bayesian Surprise quantifies the difference between a prior and a posterior of probability. Both depend on how well the Agent's Generative model matches the external world. Even the surprise alone still depends on the parameters of the generative model.	Bayesian Surprise quantifies difference between a prior and a posterior of probability	Plain Surprise vs Bayesian Surprise
2340260	2511870	39:00	The difference between frequentist and Bayesian approaches to statistics. Frequentism does have priors, which are not uninformative priors. But they're using a different ontology than Bayesian statistics. But there's so many connections to classical statistics.	There's a difference between frequentist and Bayesian approaches to statistics	Frequentism and Bayesian Statistics
2514050	2718840	41:54	The question was, in order to measure surprise, wouldn't we need another value of Y-I-E-A separate Y that enclose prior belief? Is there a third Y that is considered the observation?	Y is referring to data points that are observed as data	Cognitive neuroscience and the surprise of data
2723070	3054210	45:23	In multilevel Bayesian modeling, the priors themselves are generated from a higher or deeper level of the. One can be serving, in fact, multiple roles. This is the minimal prior generated data expectation maximization type, single layer Bayesian kernel. What else could be explored here?	Ali: I have a question about what exactly is the hidden state	Bayesian inference: The Hidden State
3054280	3110850	50:54	Surprise in a Bayesian framework and beginning to partition active entities and their action perception. Variational free energy is going to come into play as a way to bound surprise. Let's have some questions and discourse and we'll come to it next week.	So we have not even discussed variational free energy yet today	Exploring Variational Free Energy in Cognitive Science
3113430	3330550	51:53	How you interpret an action in the world has to do with your own views or your own experiences. The interpretation will vary a lot because everybody has different experiences. Understanding how the individual setup of a given entity is related to its past experiences is an important area. We will end this discussion next week with Tools.	How we interpret what we observe depends on our own experiences and priors	Prelims and Interpretations in Nature

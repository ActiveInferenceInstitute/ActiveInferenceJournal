start	end	speaker	confidence	text
650	81610	A	0.9320900909090907	Hello, cohort one. It's meeting 18. We're in our second discussion on chapter seven. Let's go to the questions and see what we can explore today or see where else to go and or look towards chapter eight. But there's any number of ways we can do it. So go first, just on chapter seven. Does anyone want to turn to any of these questions, add another question, add another reflection or a thought that arose in the last week? One question that I had was, are there any ways or suites for, for example, taking in an analytical expression and then providing equivalent phrasings that might have other advantages?
86360	93130	B	0.9348009999999999	Ali you mean by translating it into natural language or.
96060	138680	A	0.9112601470588234	Like what we saw in equation 2.5 and 2.6, like, to take in an expression and then output isomorphic or I guess not isomorphic, but equivalent expressions, expressions that have the exact same value as calculated. And when we look through the derivation, sometimes it's possible to trace the trail. But to know which representations of perhaps even the same functional or same term, it seems quite relevant.
140620	160910	B	0.8864535	Actually. There's an AI assistant for deriving formal proofs for mathematical theorems, but I haven't used it myself, but I'm not sure if that's what you mean. I'll look at the name of that and just 1 second, I'll post.
162980	163632	C	0.99898	Maybe.
163766	191530	D	0.9143343243243244	There'S a way to reformulate these specific terms from, like, a Bayesian mechanics formalism, like posterior predictive, predicted entropy. That looks like something that Dalton probably described as well. Question is whether you can just interchange it.
195380	433480	A	0.9107928825622766	Yeah. Does working with the particular partition enable equations to be operated with more composably? Because we can know that there are certain operations that are always, sometimes never going to be valid. I'm just kind of asking. I don't even know if that's the right way to have it. Um, so last time, we talked primarily about the mouse in the maze, and we are going through the way that the chapter is layering on features of the model. So first we saw the mouse just go for it, and now we're going to be where it gets the queue. And this comes to our earlier points about the resolution of the Explore exploit tradeoff. I'm going to add a question. Anyone can give a thought while I'm adding it. What are posterior predictive entropy and expected ambiguity? They're part of the decomposition of epistemic value posterior. Let me just make sure to say the posterior predictive entropy is the expected surprise or the entropy of the distribution of observations conditioned on a policy. So we're in expected free energy world. We're talking about evaluating policies with respect to now, putting aside pragmatic value. We're talking about decompositions of how the informational or the epistemic value of a policy is evaluated. So there's two terms here. First, how dispersed is your distribution of outcomes for that policy? One can imagine that, all things being equal, you would want to select policies that have a tighter distribution of outcomes. Here we have an expectation. Interestingly, this is italics e, but it's not a fancy e. Do people think there's a difference that matters or do they think that's a slight error?
438300	440068	B	0.9301542857142858	I think that should be a typo.
440164	525800	A	0.9339405494505496	Yeah, I'm going to add it to erata e is TALIX should be fancy e. It's the expectation over our hidden state estimates condition on a policy and it's the expectation of the expected ambiguity. So entropy of the A matrix, functionally how outcomes depend on states. So this is saying I want to be I want to be more is it the case that it says, I want to be more certain about observations and how they map to policy and I want to have a tighter A matrix? Eric.
528140	533390	E	0.9039562500000002	So, nice question. Why is there an expectation on the right and not on the left?
534560	542560	A	0.9594992857142858	I was also going to ask this isn't entropy already the expectation of surprise?
547860	616150	E	0.8812770999999998	Just another point of I'm sorry. So another point of clarification for myself that want to make sure this is right is the O tilde is observations. And what we're looking at here is a functional Q. So that's a distribution of observations that will get over policy and that's what's being adjusted in the free energy. So we're going to explore the distributions Q and the O Tilde's are going to be past observations. We're not looking forward there. We're looking past because those are actual observations. They're not predicted through the hidden states s. So straighten that out.
617340	639420	A	0.9537909090909089	Are we sure that O are only past observations? I believe that past observations are accounted for with respect to how they influence the prior. But does O tilde also include future observations?
641620	645730	E	0.922132857142857	Well, if so, then it has to be an expectation over something, mustn't it?
646900	675960	A	0.9204288235294118	But this is conditioned upon a given policy. Then this is the time series of observations expected under that policy. Is the left term the Q expected distribution of observations conditioned on policy. Jacob.
678460	693280	D	0.9100344736842105	I was just going to say that I think I've seen this in other parts of the book as well, and I think they use the h and then just the expectation notation for entropy kind of interchangeably.
697570	713090	A	0.9347512903225806	Okay, if they do, but then isn't this like a double expectation? This is the expectation of surprise on the queue, but this is the expectation of the expectation of surprise.
717830	718610	C	0.49685	That'S.
721130	725800	D	0.989316	Yeah, that I don't know.
731760	743110	A	0.9010555555555554	Eric okay, but the left term just Ali, please.
746040	774000	B	0.892800487804878	Well, shouldn't that first line, I mean, shouldn't both of them be at the argument of the expectation according to equation 7.4? Because in equation 7.4 we have the negative Epistemic value, but it's somehow different from this equation. Yeah, that's.
785060	859630	A	0.9169631111111106	Okay. So in 7.4 we see a restatement of expected free energy. Equation two, six. Let's leave pragmatic value to the side for now. Here we see fancy E-Q-S tilde pi. So this is looking like this probably could. Or should we also see Hposhpos minus H-Q-O pi? Okay, so let's take the negative of the first line. The negative of the first line. You can kind of just flip the terms. It's the expected ambiguity. Now minus the posterior predictive entropy. Then an expectation is being taken over. That.
861620	870930	B	0.9335313333333333	Sorry, my bad. I thought the square bracket was encompassing the whole expectation term. Sorry.
872500	882310	A	0.9385400000000002	No, you're right, actually. Well, there's a lot of layers of it, actually.
882380	883990	B	0.806944	It's right as it's written.
891040	927688	A	0.9415642352941174	Okay, wait. I always color my brackets when I'm trying to do this. Okay. The outermost bracket there actually may be a bracket. I don't even know. Let's try to figure it out. The outer bracket starts here on the left, and then it closes. Okay. Pragmatic value. If we take that as a standalone if we take that as standalone, is anyone else seeing a bracket sort of scenario? If we look at the brackets as a whole, this first bracket closes this, right?
927774	938484	B	0.8601733333333335	No, I don't think so. The first bracket closes at the end of the S tilde parentheses bracket.
938532	941228	D	0.8473342857142858	Yeah, that's why I was confused, too.
941394	950830	A	0.9648611764705881	I agree. I was thinking, should we evaluate the brackets locally? But yes, they are. Okay, good.
953380	958064	B	0.923085	So it's exactly the negative of equation 7.8.
958182	1072600	A	0.9293919480519476	Left side here is the negative of the first term. Let's just say it in terms of picking good policies. We want policies are better, with more clarity about observation outcomes. That's the first term. And a tighter a matrix. That's the second term. Okay. Middle line. This is a KL divergence between the product two products of distributions. Both of them are dealing with O in the first and S in the second. The Q distribution in the second part is the same for both. So maybe there's some interactions in how distributions of different kinds multiply, but it may be fair to interpret this as the POS is the A matrix and the Q O. Pi is how observations are shaped by policy. Or another way to say it might be POS is outcome expectations conditioned upon hidden states of the world, whereas this is outcome distributions conditioned on what we do.
1073930	1107250	E	0.9196969811320754	To me, that just looks like base law there because you've got at least in the left side a little bit, because you've got conditioned on S and then S conditioned on pi. So you're just basically saying you're trying to figure out what O is from pi through this intermediary S delta.
1117430	1494010	A	0.9271418691588791	Another angle would be where is this zero? Well, this is zero when these two are identical. When the epistemic value of a policy is low, when the resulting O distribution is being unconditioned by the policy that we choose. Whereas for policies that change how we think about the A matrix, those policies have higher informational value. Because POS is kind of like a policy agnostic mapping, but we're like drilling down to kind of subvariance partitions of informational value. These are different ways to look at what makes a policy have epistemic value. The first one line was coming down to clarity around the tightness of outcome distributions. The second one is related to how policy deviates our understanding of the A matrix. Now let's turn to here's another. Italic e. Not a fancy e. Thank you, Ollie, for this link. I've heard of this one, the Cock proof assistant, but I've never seen it used or anything. Yeah, this looks pretty interesting. I wonder if anyone has done that kind of formal work on proving certain parts of FEP. And then a more distal question would be if we have formal ontology representations, for example with Sumo or otherwise, could we make proofs around the ontology and the Sumo suggested upper merged ontology? We explored some of those things but didn't go super, super far. It was just take somebody who knows that area. Okay, the bottom term bottom line an expectation over Q. But now our Q is on outcomes not focusing on the hidden states. Then it's an expectation about outcomes of a KL divergence between this is about hidden states. This KL divergence is zero when outcomes are not when in addition to policy, outcomes don't change how we think about hidden states. So when outcomes influence how we think about hidden states, this is going to be higher. And what does on the right? This is a triangle, meaning defined as so QS pi, O is defined as and this looks like a Bayes theorem situation. Like kind of that chaining that Eric just mentioned. Like you have P of O conditioned on S and then you have S conditioned on pi. So then those kind of like it's like a way to get from O through S to pi. Whereas this one on the bottom is just like going directly about O conditioned on pi. Which interestingly is I don't know if this is like in one of these or in some subsets of them. One can think about that kind of structural model like the minimum hidden Markov model, not even in its full framing, but just like what are the three pillars of the hidden Markov model? Observations, hidden states, policy, yes, transitions and so on. And so what is gained by going through an intermediary hidden state? There's going to be situations where the observable contains 100% of the information for policy. Or we can imagine situations where the ability to choose policies based upon S, which is where the actual POMDP is intervening is better. Whereas in a fully observable Markov decision process, you don't have S, you're just making decisions on O.
1500730	1546120	E	0.9434145544554458	Kind of just revisiting the point I raised earlier about what Otilda is. It seems to me that this only works when you have the observations O tilde. So I don't see how it could be anything like looking toward the future. Because if you actually want to carry out that expectation, you want to work out what Q is or use it, you have to compute it and you have to have the observations to compute it. And the S tilde you have to have actually pipe through and use them. So help me understand if I'm wrong with that.
1556640	1621910	A	0.9324018461538464	Okay, here would be one thought. The A matrix captures how hidden states mapped observations. So whether we have a really tight A matrix or whether we have a super dispersed A matrix, using the Gaussian form for the A matrix ensures that we can always say what the most likely observation is for a given hidden state. If we had a bimodal A matrix, then there are some issues. But if we use a Gaussian form, which is the Laplacian approximation, it's more tractable, it's monotonic, it's more optimizable. So for a given S sequences of S, we can always derive the expected observations. Again, those might have a lot of uncertainty around them, but we can always make a sequence of O's corresponding to any sequence of S.
1623640	1642010	E	0.9420673333333334	But this equation makes no expectation at all. Nothing says nothing about what the form of the distribution is that you use, whether it's parametric or not parametric or whatever. It's just saying you have a distribution Q. So this is how we evaluate it?
1649190	1658260	A	0.83324	Yes. Is this true for all families of distributions q Ali, what do you think about that or anything?
1660390	1690750	B	0.9221822641509433	Actually, it's interesting that in the step by step active inference paper, this epistemic value is defined somewhat differently. I'm not sure if they're exactly equivalent with each other or there are some minor differences between them, but they've just defined the epistemic value only in terms of the expectation over the surprisals.
1692450	1692814	A	0.66	The.
1692852	1724060	B	0.8798803773584905	Difference between the Q and P surprisals. So we don't have this kind of expression as conditioned on the S tilde here. I don't know if they're the same or not. I don't know how to copy the image into this chat, but chat of the gatherer or just put it in here.
1724430	1727100	A	0.924482	Put it in the question.
1728110	1728666	B	1.0	Okay.
1728768	1844490	A	0.9307073076923075	Yeah, well, one thought is policy is not an open ended policy in its specification is always going to be of a finite horizon. So if we're considering policies of length two so interestingly, this is a policy that includes leading up to the Presence. If this is the presence, I think we had one meme that was like what if my T minus one was T or something like that. But this is a policy that is influencing and this is just the variant of 43. But let's just say that we're considering policy of true future looking length two, then we're exploring different ways in which S is going to unfold with respect to basically B one and B two. Not using that in the SuperScript way to mean like two parallel B's, but just like there's our current, there's our estimate now and then there's the way that we have, let's just say four affordances. So then there's four B ones, and then now there's going to be four options for S. And then each of those four we could apply the second affordance again. So there's 16 policies to evaluate. Each one is defined by either taking, like, B one, B three, B one, B one, B four, B four, and for each of those S, in the next two time steps, we can emit observations. Ali and then rohan.
1848020	1850984	B	0.803435	Sorry, my hand. Just stay away.
1851102	1854120	A	0.81595	Sorry. Rohan.
1863160	1863572	B	0.99997	Hello.
1863626	1864820	A	0.902605	Yes, greetings.
1865900	1898064	C	0.9112794736842103	Yeah. So coming back to what the posterior distribution is, right? Isn't it just that we assume there's a posterior distribution of states, like an ideal posterior distribution that we want to that the free energy principle moves the current empirical distribution towards? Isn't that? I thought that's what we were doing from context in chapter one.
1898182	1902770	A	0.97317	Yes, go ahead.
1905080	1949490	C	0.894758709677419	Yeah, sorry, cohort one, I think that's what we discussed. Right? So it's essentially trying to maintain some form of homeostasis. So whatever policies you have performed should eventually bring the system to that ideal homeostatic perspective. Right. So why couldn't we just assume, like, a normal distribution? So even if it is fat tailed, for example, the empirical distribution is like the P distribution in this formula has a heavy left tail, you can essentially bring it back towards that by essentially modifying the policy. That was the context of this. Right?
1950980	1983550	A	0.9142077464788736	Yes, broadly, you're right. So when we look at the full free energy, expected free energy formalization so here's equation 7.4 or equation 2.6, you're absolutely right that there's a pragmatic value that's based around reducing divergence between observations and preferences. But in equation one that we've been focusing on and trying to really unpack, because, again, also like O tilde is used in pragmatic value. We're only talking here about.
1988400	2019530	C	0.9146526760563379	But that would make sense, right? Because you're conditioning on your actions, having some effect on the state, on your own state and your state in the world or maybe some state in the world. So you have to continuously monitor until it comes down to the expected observation. That's what the generative model would spit out. Right, so it makes perfect sense and it's completely coherent in that way, right?
2020620	2033144	A	0.9387346153846156	Yes. I hope and expect and prefer that to be true. What you're describing about bringing the homeostatic variable closer to preferences expectations, that is pragmatic.
2033192	2043552	C	0.852771111111111	Yeah. It wouldn't be surprising that Hotel does that in that P distribution as well as what I'm saying. No, it would need it to be there.
2043686	2070840	A	0.9108462962962963	You're right, it is needed to be there because these are surprises and entropies and so on about observations. And then this part is really digging into decompositions of epistemic value. So there might be two policies that are both expected to bring your temperature down or whatever, but they have different epistemic values.
2073180	2098364	C	0.9478705797101448	Does it matter if it just brings your temperature down? It would look the same. Essentially, it would have the same effect. So why does it matter? Why did we have to have this kind of yeah, so there would be multiple policies that would bring it down, bring the temperature down. If the preference is to bring the temperature down, it does not matter which policy is chosen.
2098412	2098576	A	0.90945	Right.
2098598	2105292	C	0.9200494117647057	So that's essentially what I think it's being said here. Okay, so as long as you minimize.
2105356	2115590	A	0.9582747826086957	The expected here's two investment opportunities from $100 150 plus or minus one or 150 plus or -200 which one do you want.
2117480	2135260	C	0.9381272549019607	That'S not what you were discussing it is the payoff space is very different. No, it's not the same thing. They are the payoff space expected ambiguity in the $200 plus or minus $200 is very different and your mutual information will be very different because the payoff space is different.
2135330	2135660	A	0.99999	Exactly.
2135730	2153404	C	0.9641289130434781	You are essentially taking yeah, but if something is going to bring down temperature plus or minus two, plus or minus four, it's not a huge difference as long as it brings it down. Right. So that's essentially what I'm getting at, why does it matter?
2153542	2168040	A	0.9110726666666668	Okay, so in this modeling, the body's expected temperature is like the expected return on investment and the variance around the expected temperature is like the variance around the investments.
2169740	2221350	C	0.9070016129032255	Right, but they wouldn't be the same policy is what I'm saying. Sorry. They wouldn't be the same payoff space in the second case. So if something generates $100 but has a low probability of going bust so that there's a high variance, that's not the same payoff as something that produces the same $100, but that has a lower variance, the payoff spaces are different and that will be captured in that callback diversions that you have there. It would prefer the one with the lower that's right, because the payoffs yeah, but I think the discussion was around if multiple things lead to the same epistemic value and they have the same payoff, why does it matter which one is taken? Right.
2222200	2236780	A	0.8769665624999998	There's that last piece. There may be multiple policies with the same pragmatic value and then epistemic value is the difference maker that favors policies that have more clarity about observation outcomes.
2238640	2239390	C	0.99961	Right.
2240800	2243950	A	0.89185	So pragmatic value would say.
2246100	2275976	C	0.8864798507462686	That'S not necessarily true. Let's say that. Okay. When the payoff space is completely unknown that the steps that you're taking at T equals one, two, three will bring down the will have the same pragmatic value. But one of these actions will lead to some very bad consequences at step T equals 15 because you're giving up something else. It will be like paying space because.
2275998	2278004	A	0.9986118181818182	If we're talking about temperature, we're in the space of temperature.
2278052	2283160	C	0.9486891666666667	Okay, all right, so let's just bring it back to the finance.
2283320	2286590	A	0.9395275	The space is dollars. That's the dollars space.
2289600	2336044	C	0.913449464285714	Yeah. Okay, so we could use dollar as numerator. So we benchmarked the dollars. How many dollars we're making profit. So I have two investments, both of them generate $100, but one of them requires me to borrow $100 in order to generate $100. The other one would generate $100 over, say, 50 times steps. This would be quicker. The second one where you borrow $100. So it would be quicker because you're borrowing $100. But there's an interest rate that you have to pay out. So you will have to generate much more than $100 in order for this to be viable because you have an interest rate to pay off.
2336242	2348850	A	0.8930224999999999	Okay, try to follow up here. Policies are defined in terms of sequences of affordances to take. So the affordance here that you're highlighting is borrowing or not?
2350980	2403196	C	0.9350571111111112	Yes, but there are actions that would be like if you have a very high fever, in order to bring down that high fever, sometimes it makes sense to borrow that $100. Whatever metaphorical variation of that borrowing that you have to bring it down to something more manageable. Right. So it depends on how close you are to some sort of threshold where it's intolerable. I mean, putting it in more mundane terms that, hey, I have a bad toothache. It doesn't make sense for me to borrow $100 to fix my tooth. I could just brush my teeth and hope that it goes away, but I have a knife in my back, I need to get to the hospital. It makes sense to pay $100 because you're going to die if you don't.
2403308	2429876	A	0.9191199999999998	Okay, let me try to try to see this is how I'm seeing that is when we're near homeostasis, then we take actions that basically keep us there, and we try not to add risk to the situation. Whereas near the limits of our homeostatic tolerance, we may engage in high risk behavior because there's already a non negligible.
2430068	2439550	C	0.9145109090909092	Risk because the payoff space is much larger. Yeah, the payoff space, the positive payoff from your own life. No, it's not.
2439920	2441288	A	0.8525325	The space is no.
2441394	2479288	C	0.9358808791208789	So when you have the knife in your back, the payoff from borrowing $100 will save your life. When you have a toothache, borrowing $100 might cause more risk that you go bankrupt. Right. But you will still survive. If you have a toothache, you're just short $100, which you could have used elsewhere. There is no way you can use that $100 if you're dead. Yeah, there's a much higher payoff borrowing at that point than it is with the toothache example. It depends on where you take these actions.
2479464	2488748	A	0.9557166666666667	I got you. I'm seeing the space are the axes. So here we have dollars and life. And so you're saying that yeah, but.
2488834	2565750	C	0.9363434463276833	It is a multidimensional, this one. So if you think about it as, let's say yeah, so let's make it more concrete. Right? So if you have a drone and you have a trade off between battery life and staying in the air, and let's say one of your propellers goes out, just redistributing power to the other motors so that you stay in the air even though it reduces your hover time. Makes more sense because you could land versus trying to fly around and then crash eventually because you lost one motor. But whereas let's say that the other option would be let's just go faster to a certain destination when it doesn't really make sense to do so, you're just reducing your battery life to get to a destination faster. But if something happens on the wave which causes you to lose your motor, then you don't have enough battery in order to enough battery power to distribute to the automotive so that you can hover and come down safely. Does that make sense?
2567180	2568040	A	0.99934	Eric.
2570220	2738124	E	0.9488278294573651	I wonder if we could table this particular discussion. It seems to be going and get back to some of the other questions in the chapter. We had a question that was pending from last week that I think is kind of critical to the understanding of the chapter, and I wonder if we could just want to make sure we have time for it, because we're running out of time here. Yeah, and that's the question about what the relative role is of how it actually operates, that the rat learns some information prior to deciding which of the two branches of the maze to take. And the way they seem to frame the chapter is that, well, they set it up so that the rat has its preference for epistemic value. Because of epistemic value of learning which of the two branches to take is a bigger term. It'll first go to the bottom branch and learn which of the two top branches has the payoff. Then once that's resolved, it knows that, then it decides which of the top branches to go to and it gets its reward. So my understanding of that is that and this is my claim in this question, is that the rat has a preference to resolve epistemic uncertainty regardless of whether it's useful or not. So if you added more uncertain questions like, what color is my apple today? But there's no apple to be gotten, it'll still want to resolve that question and any other questions because those are uncertainties. So epistemic value, it'll collect that epistemic value, and then finally it gets around to collecting its reward. That's in contrast to a model where the purpose of resolving epistemic uncertainty is to gain the reward, which requires look ahead. So in other words, the planner or the policy would be, I try to learn what's needed to collect the reward. I do that, and that's posted as epistemic value with a purpose, then the order of operation is, okay, I figure out what I need to know, I learn what I need to know, then I go and do the exploitation. So that's what I would expect that this kind of framing would give us, but I don't see that that's what you're actually giving us.
2738322	2940390	A	0.9322549470899475	All right, here's how I see that. The example that we would want to see would have a bunch of uncertainty resolving this one picks a number between one and ten and tells you what it is. This one's a number between one and a million. So it's an incredible information resolving button. Then the question is can the relevant sources of information be sought after? I think the examples is prepared in several ways. For example, the rat already knows the semantics that this is related to here that's been just the structure of the model is already preparing that any uncertainty reduction about this basically it's already encoded in the model that this bottom queue is information about this. So this is kind of like saying and this relates earlier, not only are those implicit structurally encoded aspects of knowledge but also there's parameterization questions like we discussed if it has a dire urgency for food to the earlier questions about like the urgency of the imperative it may just go for it. And so I'd rather have a 50 50 shot now than a 98% in two timesteps. So it depends on how it's parameterized but even more deeply it depends on the construction of the model, what kinds of relationships are implicitly and explicitly being linked and it's like a deep level of modeling to understand what would be like a more neutral way to frame this question. And then it's like well, how many layers back do you want to pull? Like you could have a preplay where a rat has an association matrix, there's three information sources and it's allowed to freely explore without any shock or food and determine which one of these information sources has like a causal relationship to these edges. And then there's a learnt queue that now we're in game time and now it's going to rely on its past learning about which information resource is relevant for the Pragmatic value and then it's I mean there's just how many layers back does one need to pull before the rabbit is not in the hat? I think is going to be a serious question because even toy demonstrations have been seen as slam dunk and they're not.
2943510	2958360	E	0.9156700000000003	Well since they're bringing up Palm DPS here palmdps are really good for planning but they don't have this rat do any planning. It seems like they're really not aspiring very strongly to build a smart rat here and they're not achieving it either.
2960830	3007590	A	0.9374588405797101	Yeah, it's true because here the first movement again presuming that the parameterization is such that epistemic value is salient. The first move is dominated by epistemic value and they say well, now that that value has been tapped, now it can step into pursuing Pragmatic value with even increased confidence. But it's not considering the set of time horizon two policies that is not being explicitly encoded or.
3012460	3013256	E	0.8491633333333333	That means that.
3013278	3079850	A	0.9585254794520547	Even implicit yeah, these are transition matrices. These are just the four options depending on where you are. You can stay, you can go down. Yeah. Does section. 73 address planning. And if it doesn't, why do we need g? Why can't we just use a one step variational free energy approximation? It's saying, what's the most likely thing for me to do right now? Is a question about now in the past.
3082780	3110896	E	0.9100165333333333	Well, I think the answer to the question is exactly as you said it before, which is they baked in this particular hardwired rat, for which in this case, the best thing to do is to first explore, learn, and then exploit. So they designed it to do only this by rote to do the right thing, as opposed to the rat actually doing any look ahead or any intelligence to do the right thing.
3110998	3381150	A	0.9163407674943569	Yeah, it's structurally implicitly hardwired, and then it's fine tuned because again, there's structures where it wouldn't do this and there's tunings where it wouldn't do this, but in the tens place and in the decimal point. This example exists in a very limited manifold of models and parameterizations where one step optimal policies emulate two step planning. Like, I just happen to love sacrificing pawns and taking castles, so I'm willing to sacrifice a pawn so that later I can do a castle. But of course, you can't walk around with the belief that you love sacrificing pawns. Yeah, I actually wondered why there wasn't enumeration of policies in the planning section. Okay, learning hyperpriers hidden states on hidden states, seeing hidden states as outcomes of other hidden states so that they can be learned or fixed. Little bit of technical details on multi parameter minimization theta being just the vector of parameters for the generative model and deer Schlee distributions, which I think could be gotten into, but that's kind of distribution specific creatures select the most appropriate data to improve their generative models. I think there's more that we can dig into and explore on similar wavelengths like risk plus ambiguity, but then now there's an information gain. I thought we just had risk plus ambiguity. Then we didn't really yeah, people point to structure learning, but don't they always as a way to get around these questions. But then it's the structure of the structure learner, and then people end up with these turing machines that don't plan. People hope that Bayesian model reduction will be attractable and provide heuristics for structure learning. But I haven't seen any empirical examples that come to mind where Bayesian model reduction was used to identify actionable lower dimensional useful models. But we know that structure learning on the state space of hierarchical models is going to be essential active inference or beyond, because people thought that the explosions, the computational complexity class of just branching time, active inference or anything within a model. This is going to be like exploding upon that by several exponents. And we have these fundamental questions about the continuous time interpolations. Why are there continuous interpolations in chapter seven when it's the discrete time chapter? Okay, next time we come to chapter eight, I think this will be quite interesting. We're going to talk about dynamical systems, motor control, lock of Ultera. We'll have some justified continuous lines, LaPlace assumption, Lorenz stream number 32, stochastic chaos and Markov blankets. Hybrid models with discrete continuous fusion and some advances in continuous time modeling. Thanks everybody. See you soon.
3382480	3383050	D	0.729825	Thanks everyone.

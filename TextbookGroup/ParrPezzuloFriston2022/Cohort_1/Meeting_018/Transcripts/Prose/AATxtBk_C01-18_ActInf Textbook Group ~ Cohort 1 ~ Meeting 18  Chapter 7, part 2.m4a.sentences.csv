start	end	sentNum	speaker	confidence	text
650	2240	1	A	0.60708	Hello, cohort one.
2610	4074	2	A	0.76031	It's meeting 18.
4202	8880	3	A	0.82834	We're in our second discussion on chapter seven.
9650	21822	4	A	0.97731	Let's go to the questions and see what we can explore today or see where else to go and or look towards chapter eight.
21876	24622	5	A	0.99956	But there's any number of ways we can do it.
24676	29400	6	A	0.58586	So go first, just on chapter seven.
30650	42300	7	A	0.99669	Does anyone want to turn to any of these questions, add another question, add another reflection or a thought that arose in the last week?
60330	81610	8	A	1.0	One question that I had was, are there any ways or suites for, for example, taking in an analytical expression and then providing equivalent phrasings that might have other advantages?
86360	93130	9	B	0.93808	Ali you mean by translating it into natural language or.
96060	117120	10	A	0.82879	Like what we saw in equation 2.5 and 2.6, like, to take in an expression and then output isomorphic or I guess not isomorphic, but equivalent expressions, expressions that have the exact same value as calculated.
121380	125928	11	A	0.71	And when we look through the derivation, sometimes it's possible to trace the trail.
126044	138680	12	A	0.99999	But to know which representations of perhaps even the same functional or same term, it seems quite relevant.
140620	141128	13	B	0.99998	Actually.
141214	154988	14	B	0.99023	There's an AI assistant for deriving formal proofs for mathematical theorems, but I haven't used it myself, but I'm not sure if that's what you mean.
155154	160910	15	B	0.79589	I'll look at the name of that and just 1 second, I'll post.
162980	163632	16	C	0.99898	Maybe.
163766	182660	17	D	0.95744	There'S a way to reformulate these specific terms from, like, a Bayesian mechanics formalism, like posterior predictive, predicted entropy.
183240	186790	18	D	0.70122	That looks like something that Dalton probably described as well.
189180	191530	19	D	0.99958	Question is whether you can just interchange it.
195380	195792	20	A	0.98311	Yeah.
195846	209220	21	A	0.99989	Does working with the particular partition enable equations to be operated with more composably?
213660	225100	22	A	1.0	Because we can know that there are certain operations that are always, sometimes never going to be valid.
227360	229084	23	A	0.99308	I'm just kind of asking.
229202	231890	24	A	1.0	I don't even know if that's the right way to have it.
232260	264970	25	A	0.53154	Um, so last time, we talked primarily about the mouse in the maze, and we are going through the way that the chapter is layering on features of the model.
265660	277980	26	A	0.78044	So first we saw the mouse just go for it, and now we're going to be where it gets the queue.
284240	290560	27	A	1.0	And this comes to our earlier points about the resolution of the Explore exploit tradeoff.
307150	308334	28	A	0.80798	I'm going to add a question.
308372	310318	29	A	0.99969	Anyone can give a thought while I'm adding it.
310484	315150	30	A	0.99997	What are posterior predictive entropy and expected ambiguity?
341070	348830	31	A	0.74615	They're part of the decomposition of epistemic value posterior.
352400	369410	32	A	0.99338	Let me just make sure to say the posterior predictive entropy is the expected surprise or the entropy of the distribution of observations conditioned on a policy.
369880	373510	33	A	0.85431	So we're in expected free energy world.
374360	384616	34	A	0.9996	We're talking about evaluating policies with respect to now, putting aside pragmatic value.
384718	394940	35	A	0.63339	We're talking about decompositions of how the informational or the epistemic value of a policy is evaluated.
395680	397550	36	A	0.99991	So there's two terms here.
398000	406830	37	A	1.0	First, how dispersed is your distribution of outcomes for that policy?
408900	416880	38	A	1.0	One can imagine that, all things being equal, you would want to select policies that have a tighter distribution of outcomes.
419080	422304	39	A	0.99987	Here we have an expectation.
422432	428230	40	A	0.99963	Interestingly, this is italics e, but it's not a fancy e.
429560	433480	41	A	0.68178	Do people think there's a difference that matters or do they think that's a slight error?
438300	440068	42	B	1.0	I think that should be a typo.
440164	457590	43	A	0.99962	Yeah, I'm going to add it to erata e is TALIX should be fancy e.
469980	485112	44	A	0.99845	It's the expectation over our hidden state estimates condition on a policy and it's the expectation of the expected ambiguity.
485256	496530	45	A	0.99273	So entropy of the A matrix, functionally how outcomes depend on states.
498020	524756	46	A	0.9755	So this is saying I want to be I want to be more is it the case that it says, I want to be more certain about observations and how they map to policy and I want to have a tighter A matrix?
524948	525800	47	A	0.99405	Eric.
528140	530428	48	E	0.9958	So, nice question.
530514	533390	49	E	0.99999	Why is there an expectation on the right and not on the left?
534560	542560	50	A	1.0	I was also going to ask this isn't entropy already the expectation of surprise?
547860	552610	51	E	0.59417	Just another point of I'm sorry.
554840	567160	52	E	0.75571	So another point of clarification for myself that want to make sure this is right is the O tilde is observations.
568060	573690	53	E	1.0	And what we're looking at here is a functional Q.
574460	591272	54	E	0.97907	So that's a distribution of observations that will get over policy and that's what's being adjusted in the free energy.
591346	602860	55	E	0.61105	So we're going to explore the distributions Q and the O Tilde's are going to be past observations.
602940	604532	56	E	0.97608	We're not looking forward there.
604586	607712	57	E	0.83922	We're looking past because those are actual observations.
607776	612950	58	E	0.93569	They're not predicted through the hidden states s.
613880	616150	59	E	0.56754	So straighten that out.
617340	620356	60	A	0.99877	Are we sure that O are only past observations?
620468	629400	61	A	1.0	I believe that past observations are accounted for with respect to how they influence the prior.
632640	639420	62	A	0.99999	But does O tilde also include future observations?
641620	645730	63	E	0.81484	Well, if so, then it has to be an expectation over something, mustn't it?
646900	649730	64	A	0.98926	But this is conditioned upon a given policy.
651140	659350	65	A	1.0	Then this is the time series of observations expected under that policy.
663160	674840	66	A	0.99987	Is the left term the Q expected distribution of observations conditioned on policy.
674990	675960	67	A	0.50574	Jacob.
678460	693280	68	D	0.55	I was just going to say that I think I've seen this in other parts of the book as well, and I think they use the h and then just the expectation notation for entropy kind of interchangeably.
697570	701710	69	A	1.0	Okay, if they do, but then isn't this like a double expectation?
704050	713090	70	A	0.99985	This is the expectation of surprise on the queue, but this is the expectation of the expectation of surprise.
717830	718610	71	C	0.49685	That'S.
721130	725800	72	D	0.99819	Yeah, that I don't know.
731760	743110	73	A	0.9761	Eric okay, but the left term just Ali, please.
746040	762270	74	B	0.98835	Well, shouldn't that first line, I mean, shouldn't both of them be at the argument of the expectation according to equation 7.4?
763680	772860	75	B	0.99998	Because in equation 7.4 we have the negative Epistemic value, but it's somehow different from this equation.
772940	774000	76	B	0.99794	Yeah, that's.
785060	785810	77	A	0.17	Okay.
786340	794900	78	A	0.86319	So in 7.4 we see a restatement of expected free energy.
795050	796390	79	A	0.92957	Equation two, six.
798600	802710	80	A	0.99986	Let's leave pragmatic value to the side for now.
804860	809140	81	A	0.9932	Here we see fancy E-Q-S tilde pi.
809220	811544	82	A	0.99981	So this is looking like this probably could.
811582	826700	83	A	0.99482	Or should we also see Hposhpos minus H-Q-O pi?
832720	836510	84	A	1.0	Okay, so let's take the negative of the first line.
837680	839710	85	A	0.78	The negative of the first line.
841360	843710	86	A	0.99	You can kind of just flip the terms.
844160	845836	87	A	0.7508	It's the expected ambiguity.
845868	850400	88	A	1.0	Now minus the posterior predictive entropy.
855120	858908	89	A	1.0	Then an expectation is being taken over.
858994	859630	90	A	1.0	That.
861620	862912	91	B	0.61805	Sorry, my bad.
862966	870316	92	B	1.0	I thought the square bracket was encompassing the whole expectation term.
870348	870930	93	B	0.99958	Sorry.
872500	874470	94	A	0.99995	No, you're right, actually.
875560	882310	95	A	0.76178	Well, there's a lot of layers of it, actually.
882380	883990	96	B	0.50133	It's right as it's written.
891040	892270	97	A	0.58	Okay, wait.
892640	896172	98	A	1.0	I always color my brackets when I'm trying to do this.
896306	896990	99	A	0.98	Okay.
897520	900776	100	A	1.0	The outermost bracket there actually may be a bracket.
900808	901516	101	A	0.71	I don't even know.
901538	902656	102	A	0.99998	Let's try to figure it out.
902758	907228	103	A	1.0	The outer bracket starts here on the left, and then it closes.
907324	907728	104	A	1.0	Okay.
907814	908672	105	A	0.99703	Pragmatic value.
908726	920740	106	A	0.99946	If we take that as a standalone if we take that as standalone, is anyone else seeing a bracket sort of scenario?
921160	927688	107	A	0.99985	If we look at the brackets as a whole, this first bracket closes this, right?
927774	929512	108	B	0.73155	No, I don't think so.
929566	938484	109	B	1.0	The first bracket closes at the end of the S tilde parentheses bracket.
938532	941228	110	D	0.99224	Yeah, that's why I was confused, too.
941394	942508	111	A	0.96	I agree.
942674	946060	112	A	1.0	I was thinking, should we evaluate the brackets locally?
946720	948590	113	A	0.99996	But yes, they are.
949680	950830	114	A	1.0	Okay, good.
953380	958064	115	B	0.98655	So it's exactly the negative of equation 7.8.
958182	966900	116	A	0.99699	Left side here is the negative of the first term.
968840	972260	117	A	0.84215	Let's just say it in terms of picking good policies.
973320	980196	118	A	0.90742	We want policies are better, with more clarity about observation outcomes.
980308	981720	119	A	0.87555	That's the first term.
982380	984680	120	A	1.0	And a tighter a matrix.
986860	988440	121	A	0.99935	That's the second term.
989520	990270	122	A	0.87	Okay.
991040	992110	123	A	0.99963	Middle line.
997360	1006320	124	A	0.99578	This is a KL divergence between the product two products of distributions.
1007700	1014470	125	A	0.99999	Both of them are dealing with O in the first and S in the second.
1015400	1019350	126	A	1.0	The Q distribution in the second part is the same for both.
1021080	1046540	127	A	0.99972	So maybe there's some interactions in how distributions of different kinds multiply, but it may be fair to interpret this as the POS is the A matrix and the Q O.
1046610	1053810	128	A	0.97764	Pi is how observations are shaped by policy.
1058790	1072600	129	A	0.99999	Or another way to say it might be POS is outcome expectations conditioned upon hidden states of the world, whereas this is outcome distributions conditioned on what we do.
1073930	1096130	130	E	1.0	To me, that just looks like base law there because you've got at least in the left side a little bit, because you've got conditioned on S and then S conditioned on pi.
1096950	1107250	131	E	0.99908	So you're just basically saying you're trying to figure out what O is from pi through this intermediary S delta.
1117430	1119666	132	A	0.99977	Another angle would be where is this zero?
1119848	1126070	133	A	0.62992	Well, this is zero when these two are identical.
1128490	1145900	134	A	0.9945	When the epistemic value of a policy is low, when the resulting O distribution is being unconditioned by the policy that we choose.
1147550	1157040	135	A	0.99997	Whereas for policies that change how we think about the A matrix, those policies have higher informational value.
1161020	1175308	136	A	0.99998	Because POS is kind of like a policy agnostic mapping, but we're like drilling down to kind of subvariance partitions of informational value.
1175394	1183570	137	A	0.99999	These are different ways to look at what makes a policy have epistemic value.
1184100	1193060	138	A	0.51	The first one line was coming down to clarity around the tightness of outcome distributions.
1194840	1206120	139	A	0.63	The second one is related to how policy deviates our understanding of the A matrix.
1225450	1228940	140	A	0.59	Now let's turn to here's another.
1229950	1230874	141	A	0.50546	Italic e.
1230912	1232346	142	A	0.99998	Not a fancy e.
1232528	1234480	143	A	0.5697	Thank you, Ollie, for this link.
1234930	1240960	144	A	0.99862	I've heard of this one, the Cock proof assistant, but I've never seen it used or anything.
1244980	1248820	145	A	0.99875	Yeah, this looks pretty interesting.
1248890	1254660	146	A	1.0	I wonder if anyone has done that kind of formal work on proving certain parts of FEP.
1255400	1281020	147	A	1.0	And then a more distal question would be if we have formal ontology representations, for example with Sumo or otherwise, could we make proofs around the ontology and the Sumo suggested upper merged ontology?
1281520	1285470	148	A	0.99979	We explored some of those things but didn't go super, super far.
1286560	1293090	149	A	0.57863	It was just take somebody who knows that area.
1294260	1302740	150	A	0.74	Okay, the bottom term bottom line an expectation over Q.
1302810	1311450	151	A	0.52208	But now our Q is on outcomes not focusing on the hidden states.
1314780	1331470	152	A	1.0	Then it's an expectation about outcomes of a KL divergence between this is about hidden states.
1333040	1349810	153	A	0.99998	This KL divergence is zero when outcomes are not when in addition to policy, outcomes don't change how we think about hidden states.
1351620	1358370	154	A	0.99975	So when outcomes influence how we think about hidden states, this is going to be higher.
1373060	1378470	155	A	0.83	And what does on the right?
1378920	1391210	156	A	0.99997	This is a triangle, meaning defined as so QS pi, O is defined as and this looks like a Bayes theorem situation.
1393900	1396730	157	A	0.76081	Like kind of that chaining that Eric just mentioned.
1399980	1405480	158	A	0.78475	Like you have P of O conditioned on S and then you have S conditioned on pi.
1405640	1413580	159	A	0.99654	So then those kind of like it's like a way to get from O through S to pi.
1413940	1421200	160	A	0.99896	Whereas this one on the bottom is just like going directly about O conditioned on pi.
1424420	1430070	161	A	1.0	Which interestingly is I don't know if this is like in one of these or in some subsets of them.
1431240	1443672	162	A	1.0	One can think about that kind of structural model like the minimum hidden Markov model, not even in its full framing, but just like what are the three pillars of the hidden Markov model?
1443806	1449480	163	A	0.99683	Observations, hidden states, policy, yes, transitions and so on.
1449630	1456270	164	A	1.0	And so what is gained by going through an intermediary hidden state?
1457920	1464690	165	A	0.84165	There's going to be situations where the observable contains 100% of the information for policy.
1465780	1481670	166	A	0.99999	Or we can imagine situations where the ability to choose policies based upon S, which is where the actual POMDP is intervening is better.
1482200	1494010	167	A	0.59719	Whereas in a fully observable Markov decision process, you don't have S, you're just making decisions on O.
1500730	1506554	168	E	0.99999	Kind of just revisiting the point I raised earlier about what Otilda is.
1506752	1513530	169	E	0.8682	It seems to me that this only works when you have the observations O tilde.
1514430	1521054	170	E	0.65783	So I don't see how it could be anything like looking toward the future.
1521252	1534226	171	E	0.99999	Because if you actually want to carry out that expectation, you want to work out what Q is or use it, you have to compute it and you have to have the observations to compute it.
1534408	1543158	172	E	0.58	And the S tilde you have to have actually pipe through and use them.
1543244	1546120	173	E	0.96641	So help me understand if I'm wrong with that.
1556640	1560304	174	A	0.96	Okay, here would be one thought.
1560422	1565116	175	A	1.0	The A matrix captures how hidden states mapped observations.
1565308	1583600	176	A	0.99999	So whether we have a really tight A matrix or whether we have a super dispersed A matrix, using the Gaussian form for the A matrix ensures that we can always say what the most likely observation is for a given hidden state.
1583770	1589210	177	A	1.0	If we had a bimodal A matrix, then there are some issues.
1589820	1603420	178	A	0.52712	But if we use a Gaussian form, which is the Laplacian approximation, it's more tractable, it's monotonic, it's more optimizable.
1603760	1613520	179	A	0.65057	So for a given S sequences of S, we can always derive the expected observations.
1614100	1621910	180	A	0.99996	Again, those might have a lot of uncertainty around them, but we can always make a sequence of O's corresponding to any sequence of S.
1623640	1627684	181	E	0.93283	But this equation makes no expectation at all.
1627722	1633784	182	E	0.79428	Nothing says nothing about what the form of the distribution is that you use, whether it's parametric or not parametric or whatever.
1633902	1636330	183	E	0.96751	It's just saying you have a distribution Q.
1639020	1642010	184	E	0.84637	So this is how we evaluate it?
1649190	1649940	185	A	0.52174	Yes.
1650310	1658260	186	A	0.99987	Is this true for all families of distributions q Ali, what do you think about that or anything?
1660390	1670310	187	B	0.99989	Actually, it's interesting that in the step by step active inference paper, this epistemic value is defined somewhat differently.
1670890	1690750	188	B	0.99939	I'm not sure if they're exactly equivalent with each other or there are some minor differences between them, but they've just defined the epistemic value only in terms of the expectation over the surprisals.
1692450	1692814	189	A	0.66	The.
1692852	1694922	190	B	0.99999	Difference between the Q and P surprisals.
1694986	1706020	191	B	0.99541	So we don't have this kind of expression as conditioned on the S tilde here.
1710070	1713320	192	B	1.0	I don't know if they're the same or not.
1716410	1724060	193	B	0.78	I don't know how to copy the image into this chat, but chat of the gatherer or just put it in here.
1724430	1727100	194	A	0.99992	Put it in the question.
1728110	1728666	195	B	1.0	Okay.
1728768	1741870	196	A	0.9377	Yeah, well, one thought is policy is not an open ended policy in its specification is always going to be of a finite horizon.
1742450	1755490	197	A	0.55946	So if we're considering policies of length two so interestingly, this is a policy that includes leading up to the Presence.
1757030	1764182	198	A	0.99999	If this is the presence, I think we had one meme that was like what if my T minus one was T or something like that.
1764236	1774780	199	A	0.99732	But this is a policy that is influencing and this is just the variant of 43.
1775790	1793598	200	A	0.99758	But let's just say that we're considering policy of true future looking length two, then we're exploring different ways in which S is going to unfold with respect to basically B one and B two.
1793764	1807374	201	A	1.0	Not using that in the SuperScript way to mean like two parallel B's, but just like there's our current, there's our estimate now and then there's the way that we have, let's just say four affordances.
1807502	1815846	202	A	0.99981	So then there's four B ones, and then now there's going to be four options for S.
1816028	1819640	203	A	1.0	And then each of those four we could apply the second affordance again.
1820010	1823058	204	A	0.99997	So there's 16 policies to evaluate.
1823234	1840090	205	A	0.99637	Each one is defined by either taking, like, B one, B three, B one, B one, B four, B four, and for each of those S, in the next two time steps, we can emit observations.
1841550	1844490	206	A	0.25124	Ali and then rohan.
1848020	1849544	207	B	0.9996	Sorry, my hand.
1849622	1850984	208	B	0.99691	Just stay away.
1851102	1851770	209	A	0.97361	Sorry.
1853100	1854120	210	A	0.65829	Rohan.
1863160	1863572	211	B	0.99997	Hello.
1863626	1864820	212	A	0.81591	Yes, greetings.
1865900	1866552	213	C	0.9635	Yeah.
1866686	1872090	214	C	0.99432	So coming back to what the posterior distribution is, right?
1875980	1892540	215	C	0.99926	Isn't it just that we assume there's a posterior distribution of states, like an ideal posterior distribution that we want to that the free energy principle moves the current empirical distribution towards?
1892610	1893240	216	C	0.63905	Isn't that?
1893330	1898064	217	C	0.89	I thought that's what we were doing from context in chapter one.
1898182	1902770	218	A	0.99999	Yes, go ahead.
1905080	1909184	219	C	0.97854	Yeah, sorry, cohort one, I think that's what we discussed.
1909232	1909444	220	C	0.7841	Right?
1909482	1913904	221	C	0.98926	So it's essentially trying to maintain some form of homeostasis.
1913952	1923844	222	C	0.98941	So whatever policies you have performed should eventually bring the system to that ideal homeostatic perspective.
1923892	1924104	223	C	0.63134	Right.
1924142	1928164	224	C	0.55575	So why couldn't we just assume, like, a normal distribution?
1928212	1946850	225	C	0.66063	So even if it is fat tailed, for example, the empirical distribution is like the P distribution in this formula has a heavy left tail, you can essentially bring it back towards that by essentially modifying the policy.
1947220	1948864	226	C	0.99993	That was the context of this.
1948902	1949490	227	C	0.54293	Right?
1950980	1953232	228	A	0.89526	Yes, broadly, you're right.
1953366	1971716	229	A	0.99951	So when we look at the full free energy, expected free energy formalization so here's equation 7.4 or equation 2.6, you're absolutely right that there's a pragmatic value that's based around reducing divergence between observations and preferences.
1971908	1980990	230	A	0.99998	But in equation one that we've been focusing on and trying to really unpack, because, again, also like O tilde is used in pragmatic value.
1981600	1983550	231	A	0.66714	We're only talking here about.
1988400	1990092	232	C	0.88479	But that would make sense, right?
1990146	2003532	233	C	0.99998	Because you're conditioning on your actions, having some effect on the state, on your own state and your state in the world or maybe some state in the world.
2003686	2009936	234	C	0.99996	So you have to continuously monitor until it comes down to the expected observation.
2009968	2012532	235	C	0.99967	That's what the generative model would spit out.
2012586	2019530	236	C	0.99511	Right, so it makes perfect sense and it's completely coherent in that way, right?
2020620	2021320	237	A	0.58963	Yes.
2021470	2025864	238	A	1.0	I hope and expect and prefer that to be true.
2025982	2033144	239	A	0.988	What you're describing about bringing the homeostatic variable closer to preferences expectations, that is pragmatic.
2033192	2033790	240	C	0.86144	Yeah.
2036640	2041500	241	C	0.99976	It wouldn't be surprising that Hotel does that in that P distribution as well as what I'm saying.
2041570	2043552	242	C	0.98202	No, it would need it to be there.
2043686	2050160	243	A	0.99535	You're right, it is needed to be there because these are surprises and entropies and so on about observations.
2051380	2058710	244	A	1.0	And then this part is really digging into decompositions of epistemic value.
2059080	2070840	245	A	0.85497	So there might be two policies that are both expected to bring your temperature down or whatever, but they have different epistemic values.
2073180	2076490	246	C	0.99706	Does it matter if it just brings your temperature down?
2077020	2078570	247	C	0.99999	It would look the same.
2079520	2081228	248	C	0.99946	Essentially, it would have the same effect.
2081314	2083150	249	C	0.99287	So why does it matter?
2085360	2093952	250	C	0.67718	Why did we have to have this kind of yeah, so there would be multiple policies that would bring it down, bring the temperature down.
2094086	2098364	251	C	0.9899	If the preference is to bring the temperature down, it does not matter which policy is chosen.
2098412	2098576	252	A	0.90945	Right.
2098598	2102050	253	C	0.99882	So that's essentially what I think it's being said here.
2102740	2105292	254	C	1.0	Okay, so as long as you minimize.
2105356	2115590	255	A	1.0	The expected here's two investment opportunities from $100 150 plus or minus one or 150 plus or -200 which one do you want.
2117480	2122824	256	C	0.98114	That'S not what you were discussing it is the payoff space is very different.
2122942	2124440	257	C	0.9604	No, it's not the same thing.
2124510	2135260	258	C	0.59185	They are the payoff space expected ambiguity in the $200 plus or minus $200 is very different and your mutual information will be very different because the payoff space is different.
2135330	2135660	259	A	0.99999	Exactly.
2135730	2148752	260	C	0.96	You are essentially taking yeah, but if something is going to bring down temperature plus or minus two, plus or minus four, it's not a huge difference as long as it brings it down.
2148806	2149024	261	C	0.89039	Right.
2149062	2153404	262	C	0.99265	So that's essentially what I'm getting at, why does it matter?
2153542	2168040	263	A	1.0	Okay, so in this modeling, the body's expected temperature is like the expected return on investment and the variance around the expected temperature is like the variance around the investments.
2169740	2172440	264	C	0.93983	Right, but they wouldn't be the same policy is what I'm saying.
2172510	2172840	265	C	0.96726	Sorry.
2172910	2176184	266	C	0.99917	They wouldn't be the same payoff space in the second case.
2176302	2199810	267	C	0.99988	So if something generates $100 but has a low probability of going bust so that there's a high variance, that's not the same payoff as something that produces the same $100, but that has a lower variance, the payoff spaces are different and that will be captured in that callback diversions that you have there.
2200420	2220660	268	C	0.27176	It would prefer the one with the lower that's right, because the payoffs yeah, but I think the discussion was around if multiple things lead to the same epistemic value and they have the same payoff, why does it matter which one is taken?
2220730	2221350	269	C	0.77229	Right.
2222200	2223336	270	A	0.3408	There's that last piece.
2223438	2236780	271	A	0.99999	There may be multiple policies with the same pragmatic value and then epistemic value is the difference maker that favors policies that have more clarity about observation outcomes.
2238640	2239390	272	C	0.99961	Right.
2240800	2243950	273	A	0.51952	So pragmatic value would say.
2246100	2247392	274	C	0.99961	That'S not necessarily true.
2247446	2248450	275	C	0.99922	Let's say that.
2248820	2249376	276	C	0.83	Okay.
2249478	2263268	277	C	0.99982	When the payoff space is completely unknown that the steps that you're taking at T equals one, two, three will bring down the will have the same pragmatic value.
2263434	2272650	278	C	0.71607	But one of these actions will lead to some very bad consequences at step T equals 15 because you're giving up something else.
2274220	2275976	279	C	0.95695	It will be like paying space because.
2275998	2278004	280	A	0.99953	If we're talking about temperature, we're in the space of temperature.
2278052	2283160	281	C	0.99	Okay, all right, so let's just bring it back to the finance.
2283320	2284764	282	A	0.99	The space is dollars.
2284962	2286590	283	A	0.99964	That's the dollars space.
2289600	2290012	284	C	0.80146	Yeah.
2290066	2292804	285	C	1.0	Okay, so we could use dollar as numerator.
2292952	2294640	286	C	0.92574	So we benchmarked the dollars.
2294710	2296476	287	C	1.0	How many dollars we're making profit.
2296588	2308212	288	C	0.99952	So I have two investments, both of them generate $100, but one of them requires me to borrow $100 in order to generate $100.
2308266	2312544	289	C	1.0	The other one would generate $100 over, say, 50 times steps.
2312592	2313860	290	C	0.99997	This would be quicker.
2314440	2316808	291	C	0.65	The second one where you borrow $100.
2316894	2319720	292	C	0.989	So it would be quicker because you're borrowing $100.
2319790	2324250	293	C	0.99999	But there's an interest rate that you have to pay out.
2325820	2336044	294	C	0.73481	So you will have to generate much more than $100 in order for this to be viable because you have an interest rate to pay off.
2336242	2338780	295	A	0.99	Okay, try to follow up here.
2338930	2344130	296	A	0.78016	Policies are defined in terms of sequences of affordances to take.
2344740	2348850	297	A	0.99085	So the affordance here that you're highlighting is borrowing or not?
2350980	2364740	298	C	0.99999	Yes, but there are actions that would be like if you have a very high fever, in order to bring down that high fever, sometimes it makes sense to borrow that $100.
2364890	2371780	299	C	0.99998	Whatever metaphorical variation of that borrowing that you have to bring it down to something more manageable.
2371860	2372490	300	C	0.99447	Right.
2372860	2378360	301	C	0.99788	So it depends on how close you are to some sort of threshold where it's intolerable.
2380640	2388136	302	C	0.98	I mean, putting it in more mundane terms that, hey, I have a bad toothache.
2388168	2391576	303	C	0.97118	It doesn't make sense for me to borrow $100 to fix my tooth.
2391608	2398080	304	C	1.0	I could just brush my teeth and hope that it goes away, but I have a knife in my back, I need to get to the hospital.
2398580	2403196	305	C	0.99994	It makes sense to pay $100 because you're going to die if you don't.
2403308	2420456	306	A	0.99	Okay, let me try to try to see this is how I'm seeing that is when we're near homeostasis, then we take actions that basically keep us there, and we try not to add risk to the situation.
2420638	2429876	307	A	0.99968	Whereas near the limits of our homeostatic tolerance, we may engage in high risk behavior because there's already a non negligible.
2430068	2432760	308	C	0.67774	Risk because the payoff space is much larger.
2432840	2437070	309	C	0.9757	Yeah, the payoff space, the positive payoff from your own life.
2438480	2439550	310	C	0.97991	No, it's not.
2439920	2441288	311	A	0.6	The space is no.
2441394	2450464	312	C	0.93671	So when you have the knife in your back, the payoff from borrowing $100 will save your life.
2450662	2455580	313	C	0.99839	When you have a toothache, borrowing $100 might cause more risk that you go bankrupt.
2455740	2456076	314	C	0.99441	Right.
2456118	2458240	315	C	0.77207	But you will still survive.
2458320	2463940	316	C	0.99999	If you have a toothache, you're just short $100, which you could have used elsewhere.
2464280	2468132	317	C	0.99875	There is no way you can use that $100 if you're dead.
2468196	2476376	318	C	0.99162	Yeah, there's a much higher payoff borrowing at that point than it is with the toothache example.
2476558	2479288	319	C	0.50604	It depends on where you take these actions.
2479464	2480220	320	A	1.0	I got you.
2480290	2483224	321	A	0.71839	I'm seeing the space are the axes.
2483272	2485790	322	A	0.99984	So here we have dollars and life.
2486640	2488748	323	A	1.0	And so you're saying that yeah, but.
2488834	2491004	324	C	0.99985	It is a multidimensional, this one.
2491042	2497836	325	C	0.51589	So if you think about it as, let's say yeah, so let's make it more concrete.
2497868	2498112	326	C	0.99477	Right?
2498166	2519528	327	C	0.99855	So if you have a drone and you have a trade off between battery life and staying in the air, and let's say one of your propellers goes out, just redistributing power to the other motors so that you stay in the air even though it reduces your hover time.
2519614	2530220	328	C	0.99998	Makes more sense because you could land versus trying to fly around and then crash eventually because you lost one motor.
2531280	2548924	329	C	0.7202	But whereas let's say that the other option would be let's just go faster to a certain destination when it doesn't really make sense to do so, you're just reducing your battery life to get to a destination faster.
2549052	2563540	330	C	0.79567	But if something happens on the wave which causes you to lose your motor, then you don't have enough battery in order to enough battery power to distribute to the automotive so that you can hover and come down safely.
2564440	2565750	331	C	0.99988	Does that make sense?
2567180	2568040	332	A	0.99934	Eric.
2570220	2572516	333	E	1.0	I wonder if we could table this particular discussion.
2572548	2577880	334	E	0.95789	It seems to be going and get back to some of the other questions in the chapter.
2580060	2589900	335	E	0.94555	We had a question that was pending from last week that I think is kind of critical to the understanding of the chapter, and I wonder if we could just want to make sure we have time for it, because we're running out of time here.
2589970	2609830	336	E	0.91793	Yeah, and that's the question about what the relative role is of how it actually operates, that the rat learns some information prior to deciding which of the two branches of the maze to take.
2610520	2621544	337	E	1.0	And the way they seem to frame the chapter is that, well, they set it up so that the rat has its preference for epistemic value.
2621662	2628132	338	E	0.99999	Because of epistemic value of learning which of the two branches to take is a bigger term.
2628196	2636940	339	E	0.71067	It'll first go to the bottom branch and learn which of the two top branches has the payoff.
2637280	2644716	340	E	1.0	Then once that's resolved, it knows that, then it decides which of the top branches to go to and it gets its reward.
2644908	2663860	341	E	0.99858	So my understanding of that is that and this is my claim in this question, is that the rat has a preference to resolve epistemic uncertainty regardless of whether it's useful or not.
2664010	2673848	342	E	0.99992	So if you added more uncertain questions like, what color is my apple today?
2674014	2683976	343	E	0.99015	But there's no apple to be gotten, it'll still want to resolve that question and any other questions because those are uncertainties.
2684088	2692880	344	E	0.99711	So epistemic value, it'll collect that epistemic value, and then finally it gets around to collecting its reward.
2694500	2704916	345	E	0.9983	That's in contrast to a model where the purpose of resolving epistemic uncertainty is to gain the reward, which requires look ahead.
2705098	2714336	346	E	0.99792	So in other words, the planner or the policy would be, I try to learn what's needed to collect the reward.
2714448	2730792	347	E	0.92	I do that, and that's posted as epistemic value with a purpose, then the order of operation is, okay, I figure out what I need to know, I learn what I need to know, then I go and do the exploitation.
2730936	2738124	348	E	0.97443	So that's what I would expect that this kind of framing would give us, but I don't see that that's what you're actually giving us.
2738322	2742480	349	A	0.96739	All right, here's how I see that.
2742630	2754164	350	A	0.85	The example that we would want to see would have a bunch of uncertainty resolving this one picks a number between one and ten and tells you what it is.
2754202	2755668	351	A	0.99997	This one's a number between one and a million.
2755754	2759910	352	A	0.99976	So it's an incredible information resolving button.
2761480	2769050	353	A	1.0	Then the question is can the relevant sources of information be sought after?
2775670	2780740	354	A	0.92	I think the examples is prepared in several ways.
2781370	2810480	355	A	0.99973	For example, the rat already knows the semantics that this is related to here that's been just the structure of the model is already preparing that any uncertainty reduction about this basically it's already encoded in the model that this bottom queue is information about this.
2812770	2844778	356	A	0.99238	So this is kind of like saying and this relates earlier, not only are those implicit structurally encoded aspects of knowledge but also there's parameterization questions like we discussed if it has a dire urgency for food to the earlier questions about like the urgency of the imperative it may just go for it.
2844944	2850890	357	A	0.94	And so I'd rather have a 50 50 shot now than a 98% in two timesteps.
2851310	2881794	358	A	0.61649	So it depends on how it's parameterized but even more deeply it depends on the construction of the model, what kinds of relationships are implicitly and explicitly being linked and it's like a deep level of modeling to understand what would be like a more neutral way to frame this question.
2881832	2884278	359	A	0.53	And then it's like well, how many layers back do you want to pull?
2884444	2903526	360	A	0.98122	Like you could have a preplay where a rat has an association matrix, there's three information sources and it's allowed to freely explore without any shock or food and determine which one of these information sources has like a causal relationship to these edges.
2903718	2928640	361	A	1.0	And then there's a learnt queue that now we're in game time and now it's going to rely on its past learning about which information resource is relevant for the Pragmatic value and then it's I mean there's just how many layers back does one need to pull before the rabbit is not in the hat?
2929220	2940390	362	A	1.0	I think is going to be a serious question because even toy demonstrations have been seen as slam dunk and they're not.
2943510	2949954	363	E	0.99923	Well since they're bringing up Palm DPS here palmdps are really good for planning but they don't have this rat do any planning.
2950002	2958360	364	E	0.99998	It seems like they're really not aspiring very strongly to build a smart rat here and they're not achieving it either.
2960830	2976830	365	A	0.53891	Yeah, it's true because here the first movement again presuming that the parameterization is such that epistemic value is salient.
2977890	2988450	366	A	1.0	The first move is dominated by epistemic value and they say well, now that that value has been tapped, now it can step into pursuing Pragmatic value with even increased confidence.
2991510	3007590	367	A	1.0	But it's not considering the set of time horizon two policies that is not being explicitly encoded or.
3012460	3013256	368	E	0.9776	That means that.
3013278	3016700	369	A	0.99233	Even implicit yeah, these are transition matrices.
3017680	3021550	370	A	0.99982	These are just the four options depending on where you are.
3023680	3026690	371	A	1.0	You can stay, you can go down.
3034650	3035254	372	A	0.98674	Yeah.
3035372	3036322	373	A	0.96179	Does section.
3036386	3039270	374	A	0.91	73 address planning.
3051990	3054740	375	A	0.99	And if it doesn't, why do we need g?
3057510	3063430	376	A	1.0	Why can't we just use a one step variational free energy approximation?
3072520	3075750	377	A	0.88971	It's saying, what's the most likely thing for me to do right now?
3077580	3079850	378	A	0.64267	Is a question about now in the past.
3082780	3099112	379	E	0.67875	Well, I think the answer to the question is exactly as you said it before, which is they baked in this particular hardwired rat, for which in this case, the best thing to do is to first explore, learn, and then exploit.
3099256	3110896	380	E	0.91115	So they designed it to do only this by rote to do the right thing, as opposed to the rat actually doing any look ahead or any intelligence to do the right thing.
3110998	3131050	381	A	0.99494	Yeah, it's structurally implicitly hardwired, and then it's fine tuned because again, there's structures where it wouldn't do this and there's tunings where it wouldn't do this, but in the tens place and in the decimal point.
3132060	3143660	382	A	0.99992	This example exists in a very limited manifold of models and parameterizations where one step optimal policies emulate two step planning.
3144160	3152444	383	A	0.99233	Like, I just happen to love sacrificing pawns and taking castles, so I'm willing to sacrifice a pawn so that later I can do a castle.
3152492	3156720	384	A	0.99997	But of course, you can't walk around with the belief that you love sacrificing pawns.
3158020	3164560	385	A	0.89045	Yeah, I actually wondered why there wasn't enumeration of policies in the planning section.
3170300	3190820	386	A	0.79	Okay, learning hyperpriers hidden states on hidden states, seeing hidden states as outcomes of other hidden states so that they can be learned or fixed.
3195100	3219510	387	A	0.98917	Little bit of technical details on multi parameter minimization theta being just the vector of parameters for the generative model and deer Schlee distributions, which I think could be gotten into, but that's kind of distribution specific creatures select the most appropriate data to improve their generative models.
3226850	3236690	388	A	0.96	I think there's more that we can dig into and explore on similar wavelengths like risk plus ambiguity, but then now there's an information gain.
3237830	3240530	389	A	1.0	I thought we just had risk plus ambiguity.
3245480	3259840	390	A	0.5	Then we didn't really yeah, people point to structure learning, but don't they always as a way to get around these questions.
3262150	3270580	391	A	0.82984	But then it's the structure of the structure learner, and then people end up with these turing machines that don't plan.
3274690	3281540	392	A	0.99668	People hope that Bayesian model reduction will be attractable and provide heuristics for structure learning.
3283430	3297190	393	A	1.0	But I haven't seen any empirical examples that come to mind where Bayesian model reduction was used to identify actionable lower dimensional useful models.
3299210	3316720	394	A	0.99989	But we know that structure learning on the state space of hierarchical models is going to be essential active inference or beyond, because people thought that the explosions, the computational complexity class of just branching time, active inference or anything within a model.
3317490	3322590	395	A	0.99989	This is going to be like exploding upon that by several exponents.
3326180	3330880	396	A	0.93	And we have these fundamental questions about the continuous time interpolations.
3334820	3345700	397	A	0.63214	Why are there continuous interpolations in chapter seven when it's the discrete time chapter?
3346920	3351956	398	A	0.99	Okay, next time we come to chapter eight, I think this will be quite interesting.
3352058	3356960	399	A	0.6801	We're going to talk about dynamical systems, motor control, lock of Ultera.
3357040	3367420	400	A	0.56144	We'll have some justified continuous lines, LaPlace assumption, Lorenz stream number 32, stochastic chaos and Markov blankets.
3369200	3377260	401	A	0.97908	Hybrid models with discrete continuous fusion and some advances in continuous time modeling.
3378560	3379630	402	A	0.99977	Thanks everybody.
3380080	3381150	403	A	0.99978	See you soon.
3382480	3383050	404	D	0.56248	Thanks everyone.

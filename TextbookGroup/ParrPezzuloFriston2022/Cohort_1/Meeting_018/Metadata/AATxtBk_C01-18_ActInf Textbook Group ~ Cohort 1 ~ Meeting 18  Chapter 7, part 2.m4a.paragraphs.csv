start	end	paragNum	speaker	confidence	startTime	wordCount	text
650	24622	1	A	0.60708	00:00	48	Hello, cohort one. It's meeting 18. We're in our second discussion on chapter seven. Let's go to the questions and see what we can explore today or see where else to go and or look towards chapter eight. But there's any number of ways we can do it.
24676	42300	2	A	0.58586	00:24	32	So go first, just on chapter seven. Does anyone want to turn to any of these questions, add another question, add another reflection or a thought that arose in the last week?
60330	81610	3	A	1.0	01:00	30	One question that I had was, are there any ways or suites for, for example, taking in an analytical expression and then providing equivalent phrasings that might have other advantages?
86360	93130	4	B	0.93808	01:26	10	Ali you mean by translating it into natural language or.
96060	117120	5	A	0.82879	01:36	36	Like what we saw in equation 2.5 and 2.6, like, to take in an expression and then output isomorphic or I guess not isomorphic, but equivalent expressions, expressions that have the exact same value as calculated.
121380	160910	6	A	0.71	02:01	72	And when we look through the derivation, sometimes it's possible to trace the trail. But to know which representations of perhaps even the same functional or same term, it seems quite relevant. Actually. There's an AI assistant for deriving formal proofs for mathematical theorems, but I haven't used it myself, but I'm not sure if that's what you mean. I'll look at the name of that and just 1 second, I'll post.
162980	186790	7	C	0.99898	02:42	30	Maybe. There'S a way to reformulate these specific terms from, like, a Bayesian mechanics formalism, like posterior predictive, predicted entropy. That looks like something that Dalton probably described as well.
189180	191530	8	D	0.99958	03:09	8	Question is whether you can just interchange it.
195380	209220	9	A	0.98311	03:15	15	Yeah. Does working with the particular partition enable equations to be operated with more composably?
213660	225100	10	A	1.0	03:33	18	Because we can know that there are certain operations that are always, sometimes never going to be valid.
227360	277980	11	A	0.99308	03:47	68	I'm just kind of asking. I don't even know if that's the right way to have it. Um, so last time, we talked primarily about the mouse in the maze, and we are going through the way that the chapter is layering on features of the model. So first we saw the mouse just go for it, and now we're going to be where it gets the queue.
284240	290560	12	A	1.0	04:44	15	And this comes to our earlier points about the resolution of the Explore exploit tradeoff.
307150	315150	13	A	0.80798	05:07	23	I'm going to add a question. Anyone can give a thought while I'm adding it. What are posterior predictive entropy and expected ambiguity?
341070	348830	14	A	0.74615	05:41	9	They're part of the decomposition of epistemic value posterior.
352400	397550	15	A	0.99338	05:52	69	Let me just make sure to say the posterior predictive entropy is the expected surprise or the entropy of the distribution of observations conditioned on a policy. So we're in expected free energy world. We're talking about evaluating policies with respect to now, putting aside pragmatic value. We're talking about decompositions of how the informational or the epistemic value of a policy is evaluated. So there's two terms here.
398000	406830	16	A	1.0	06:38	11	First, how dispersed is your distribution of outcomes for that policy?
408900	416880	17	A	1.0	06:48	21	One can imagine that, all things being equal, you would want to select policies that have a tighter distribution of outcomes.
419080	433480	18	A	0.99987	06:59	32	Here we have an expectation. Interestingly, this is italics e, but it's not a fancy e. Do people think there's a difference that matters or do they think that's a slight error?
438300	457590	19	B	1.0	07:18	22	I think that should be a typo. Yeah, I'm going to add it to erata e is TALIX should be fancy e.
469980	525800	20	A	0.99845	07:49	76	It's the expectation over our hidden state estimates condition on a policy and it's the expectation of the expected ambiguity. So entropy of the A matrix, functionally how outcomes depend on states. So this is saying I want to be I want to be more is it the case that it says, I want to be more certain about observations and how they map to policy and I want to have a tighter A matrix? Eric.
528140	542560	21	E	0.9958	08:48	30	So, nice question. Why is there an expectation on the right and not on the left? I was also going to ask this isn't entropy already the expectation of surprise?
547860	552610	22	E	0.59417	09:07	6	Just another point of I'm sorry.
554840	604532	23	E	0.75571	09:14	74	So another point of clarification for myself that want to make sure this is right is the O tilde is observations. And what we're looking at here is a functional Q. So that's a distribution of observations that will get over policy and that's what's being adjusted in the free energy. So we're going to explore the distributions Q and the O Tilde's are going to be past observations. We're not looking forward there.
604586	629400	24	E	0.83922	10:04	45	We're looking past because those are actual observations. They're not predicted through the hidden states s. So straighten that out. Are we sure that O are only past observations? I believe that past observations are accounted for with respect to how they influence the prior.
632640	639420	25	A	0.99999	10:32	8	But does O tilde also include future observations?
641620	659350	26	E	0.81484	10:41	34	Well, if so, then it has to be an expectation over something, mustn't it? But this is conditioned upon a given policy. Then this is the time series of observations expected under that policy.
663160	675960	27	A	0.99987	11:03	14	Is the left term the Q expected distribution of observations conditioned on policy. Jacob.
678460	693280	28	D	0.55	11:18	38	I was just going to say that I think I've seen this in other parts of the book as well, and I think they use the h and then just the expectation notation for entropy kind of interchangeably.
697570	701710	29	A	1.0	11:37	12	Okay, if they do, but then isn't this like a double expectation?
704050	713090	30	A	0.99985	11:44	19	This is the expectation of surprise on the queue, but this is the expectation of the expectation of surprise.
717830	718610	31	C	0.49685	11:57	1	That'S.
721130	725800	32	D	0.99819	12:01	5	Yeah, that I don't know.
731760	743110	33	A	0.9761	12:11	9	Eric okay, but the left term just Ali, please.
746040	774000	34	B	0.98835	12:26	41	Well, shouldn't that first line, I mean, shouldn't both of them be at the argument of the expectation according to equation 7.4? Because in equation 7.4 we have the negative Epistemic value, but it's somehow different from this equation. Yeah, that's.
785060	796390	35	A	0.17	13:05	15	Okay. So in 7.4 we see a restatement of expected free energy. Equation two, six.
798600	802710	36	A	0.99986	13:18	9	Let's leave pragmatic value to the side for now.
804860	826700	37	A	0.9932	13:24	24	Here we see fancy E-Q-S tilde pi. So this is looking like this probably could. Or should we also see Hposhpos minus H-Q-O pi?
832720	850400	38	A	1.0	13:52	34	Okay, so let's take the negative of the first line. The negative of the first line. You can kind of just flip the terms. It's the expected ambiguity. Now minus the posterior predictive entropy.
855120	870930	39	A	1.0	14:15	23	Then an expectation is being taken over. That. Sorry, my bad. I thought the square bracket was encompassing the whole expectation term. Sorry.
872500	883990	40	A	0.99995	14:32	18	No, you're right, actually. Well, there's a lot of layers of it, actually. It's right as it's written.
891040	901516	41	A	0.58	14:51	27	Okay, wait. I always color my brackets when I'm trying to do this. Okay. The outermost bracket there actually may be a bracket. I don't even know.
901538	920740	42	A	0.99998	15:01	43	Let's try to figure it out. The outer bracket starts here on the left, and then it closes. Okay. Pragmatic value. If we take that as a standalone if we take that as standalone, is anyone else seeing a bracket sort of scenario?
921160	942508	43	A	0.99985	15:21	42	If we look at the brackets as a whole, this first bracket closes this, right? No, I don't think so. The first bracket closes at the end of the S tilde parentheses bracket. Yeah, that's why I was confused, too. I agree.
942674	950830	44	A	1.0	15:42	15	I was thinking, should we evaluate the brackets locally? But yes, they are. Okay, good.
953380	981720	45	B	0.98655	15:53	43	So it's exactly the negative of equation 7.8. Left side here is the negative of the first term. Let's just say it in terms of picking good policies. We want policies are better, with more clarity about observation outcomes. That's the first term.
982380	984680	46	A	1.0	16:22	5	And a tighter a matrix.
986860	992110	47	A	0.99935	16:26	7	That's the second term. Okay. Middle line.
997360	1053810	48	A	0.99578	16:37	78	This is a KL divergence between the product two products of distributions. Both of them are dealing with O in the first and S in the second. The Q distribution in the second part is the same for both. So maybe there's some interactions in how distributions of different kinds multiply, but it may be fair to interpret this as the POS is the A matrix and the Q O. Pi is how observations are shaped by policy.
1058790	1107250	49	A	0.99999	17:38	82	Or another way to say it might be POS is outcome expectations conditioned upon hidden states of the world, whereas this is outcome distributions conditioned on what we do. To me, that just looks like base law there because you've got at least in the left side a little bit, because you've got conditioned on S and then S conditioned on pi. So you're just basically saying you're trying to figure out what O is from pi through this intermediary S delta.
1117430	1126070	50	A	0.99977	18:37	17	Another angle would be where is this zero? Well, this is zero when these two are identical.
1128490	1157040	51	A	0.9945	18:48	41	When the epistemic value of a policy is low, when the resulting O distribution is being unconditioned by the policy that we choose. Whereas for policies that change how we think about the A matrix, those policies have higher informational value.
1161020	1206120	52	A	0.99998	19:21	67	Because POS is kind of like a policy agnostic mapping, but we're like drilling down to kind of subvariance partitions of informational value. These are different ways to look at what makes a policy have epistemic value. The first one line was coming down to clarity around the tightness of outcome distributions. The second one is related to how policy deviates our understanding of the A matrix.
1225450	1240960	53	A	0.59	20:25	35	Now let's turn to here's another. Italic e. Not a fancy e. Thank you, Ollie, for this link. I've heard of this one, the Cock proof assistant, but I've never seen it used or anything.
1244980	1293090	54	A	0.99875	20:44	77	Yeah, this looks pretty interesting. I wonder if anyone has done that kind of formal work on proving certain parts of FEP. And then a more distal question would be if we have formal ontology representations, for example with Sumo or otherwise, could we make proofs around the ontology and the Sumo suggested upper merged ontology? We explored some of those things but didn't go super, super far. It was just take somebody who knows that area.
1294260	1311450	55	A	0.74	21:34	23	Okay, the bottom term bottom line an expectation over Q. But now our Q is on outcomes not focusing on the hidden states.
1314780	1358370	56	A	1.0	21:54	55	Then it's an expectation about outcomes of a KL divergence between this is about hidden states. This KL divergence is zero when outcomes are not when in addition to policy, outcomes don't change how we think about hidden states. So when outcomes influence how we think about hidden states, this is going to be higher.
1373060	1391210	57	A	0.83	22:53	28	And what does on the right? This is a triangle, meaning defined as so QS pi, O is defined as and this looks like a Bayes theorem situation.
1393900	1396730	58	A	0.76081	23:13	9	Like kind of that chaining that Eric just mentioned.
1399980	1421200	59	A	0.78475	23:19	51	Like you have P of O conditioned on S and then you have S conditioned on pi. So then those kind of like it's like a way to get from O through S to pi. Whereas this one on the bottom is just like going directly about O conditioned on pi.
1424420	1464690	60	A	1.0	23:44	90	Which interestingly is I don't know if this is like in one of these or in some subsets of them. One can think about that kind of structural model like the minimum hidden Markov model, not even in its full framing, but just like what are the three pillars of the hidden Markov model? Observations, hidden states, policy, yes, transitions and so on. And so what is gained by going through an intermediary hidden state? There's going to be situations where the observable contains 100% of the information for policy.
1465780	1494010	61	A	0.99999	24:25	42	Or we can imagine situations where the ability to choose policies based upon S, which is where the actual POMDP is intervening is better. Whereas in a fully observable Markov decision process, you don't have S, you're just making decisions on O.
1500730	1543158	62	E	0.99999	25:00	92	Kind of just revisiting the point I raised earlier about what Otilda is. It seems to me that this only works when you have the observations O tilde. So I don't see how it could be anything like looking toward the future. Because if you actually want to carry out that expectation, you want to work out what Q is or use it, you have to compute it and you have to have the observations to compute it. And the S tilde you have to have actually pipe through and use them.
1543244	1546120	63	E	0.96641	25:43	9	So help me understand if I'm wrong with that.
1556640	1603420	64	A	0.96	25:56	90	Okay, here would be one thought. The A matrix captures how hidden states mapped observations. So whether we have a really tight A matrix or whether we have a super dispersed A matrix, using the Gaussian form for the A matrix ensures that we can always say what the most likely observation is for a given hidden state. If we had a bimodal A matrix, then there are some issues. But if we use a Gaussian form, which is the Laplacian approximation, it's more tractable, it's monotonic, it's more optimizable.
1603760	1636330	65	A	0.65057	26:43	78	So for a given S sequences of S, we can always derive the expected observations. Again, those might have a lot of uncertainty around them, but we can always make a sequence of O's corresponding to any sequence of S. But this equation makes no expectation at all. Nothing says nothing about what the form of the distribution is that you use, whether it's parametric or not parametric or whatever. It's just saying you have a distribution Q.
1639020	1642010	66	E	0.84637	27:19	7	So this is how we evaluate it?
1649190	1658260	67	A	0.52174	27:29	19	Yes. Is this true for all families of distributions q Ali, what do you think about that or anything?
1660390	1706020	68	B	0.99989	27:40	76	Actually, it's interesting that in the step by step active inference paper, this epistemic value is defined somewhat differently. I'm not sure if they're exactly equivalent with each other or there are some minor differences between them, but they've just defined the epistemic value only in terms of the expectation over the surprisals. The. Difference between the Q and P surprisals. So we don't have this kind of expression as conditioned on the S tilde here.
1710070	1713320	69	B	1.0	28:30	9	I don't know if they're the same or not.
1716410	1755490	70	B	0.78	28:36	73	I don't know how to copy the image into this chat, but chat of the gatherer or just put it in here. Put it in the question. Okay. Yeah, well, one thought is policy is not an open ended policy in its specification is always going to be of a finite horizon. So if we're considering policies of length two so interestingly, this is a policy that includes leading up to the Presence.
1757030	1815846	71	A	0.99999	29:17	130	If this is the presence, I think we had one meme that was like what if my T minus one was T or something like that. But this is a policy that is influencing and this is just the variant of 43. But let's just say that we're considering policy of true future looking length two, then we're exploring different ways in which S is going to unfold with respect to basically B one and B two. Not using that in the SuperScript way to mean like two parallel B's, but just like there's our current, there's our estimate now and then there's the way that we have, let's just say four affordances. So then there's four B ones, and then now there's going to be four options for S.
1816028	1844490	72	A	1.0	30:16	59	And then each of those four we could apply the second affordance again. So there's 16 policies to evaluate. Each one is defined by either taking, like, B one, B three, B one, B one, B four, B four, and for each of those S, in the next two time steps, we can emit observations. Ali and then rohan.
1848020	1854120	73	B	0.9996	30:48	8	Sorry, my hand. Just stay away. Sorry. Rohan.
1863160	1872090	74	B	0.99997	31:03	14	Hello. Yes, greetings. Yeah. So coming back to what the posterior distribution is, right?
1875980	1902770	75	C	0.99926	31:15	49	Isn't it just that we assume there's a posterior distribution of states, like an ideal posterior distribution that we want to that the free energy principle moves the current empirical distribution towards? Isn't that? I thought that's what we were doing from context in chapter one. Yes, go ahead.
1905080	1924104	76	C	0.97854	31:45	38	Yeah, sorry, cohort one, I think that's what we discussed. Right? So it's essentially trying to maintain some form of homeostasis. So whatever policies you have performed should eventually bring the system to that ideal homeostatic perspective. Right.
1924142	1953232	77	C	0.55575	32:04	59	So why couldn't we just assume, like, a normal distribution? So even if it is fat tailed, for example, the empirical distribution is like the P distribution in this formula has a heavy left tail, you can essentially bring it back towards that by essentially modifying the policy. That was the context of this. Right? Yes, broadly, you're right.
1953366	1983550	78	A	0.99951	32:33	67	So when we look at the full free energy, expected free energy formalization so here's equation 7.4 or equation 2.6, you're absolutely right that there's a pragmatic value that's based around reducing divergence between observations and preferences. But in equation one that we've been focusing on and trying to really unpack, because, again, also like O tilde is used in pragmatic value. We're only talking here about.
1988400	2019530	79	C	0.88479	33:08	71	But that would make sense, right? Because you're conditioning on your actions, having some effect on the state, on your own state and your state in the world or maybe some state in the world. So you have to continuously monitor until it comes down to the expected observation. That's what the generative model would spit out. Right, so it makes perfect sense and it's completely coherent in that way, right?
2020620	2033790	80	A	0.58963	33:40	27	Yes. I hope and expect and prefer that to be true. What you're describing about bringing the homeostatic variable closer to preferences expectations, that is pragmatic. Yeah.
2036640	2070840	81	C	0.99976	33:56	80	It wouldn't be surprising that Hotel does that in that P distribution as well as what I'm saying. No, it would need it to be there. You're right, it is needed to be there because these are surprises and entropies and so on about observations. And then this part is really digging into decompositions of epistemic value. So there might be two policies that are both expected to bring your temperature down or whatever, but they have different epistemic values.
2073180	2083150	82	C	0.99706	34:33	27	Does it matter if it just brings your temperature down? It would look the same. Essentially, it would have the same effect. So why does it matter?
2085360	2105292	83	C	0.67718	34:45	60	Why did we have to have this kind of yeah, so there would be multiple policies that would bring it down, bring the temperature down. If the preference is to bring the temperature down, it does not matter which policy is chosen. Right. So that's essentially what I think it's being said here. Okay, so as long as you minimize.
2105356	2135660	84	A	1.0	35:05	75	The expected here's two investment opportunities from $100 150 plus or minus one or 150 plus or -200 which one do you want. That'S not what you were discussing it is the payoff space is very different. No, it's not the same thing. They are the payoff space expected ambiguity in the $200 plus or minus $200 is very different and your mutual information will be very different because the payoff space is different. Exactly.
2135730	2172440	85	C	0.96	35:35	88	You are essentially taking yeah, but if something is going to bring down temperature plus or minus two, plus or minus four, it's not a huge difference as long as it brings it down. Right. So that's essentially what I'm getting at, why does it matter? Okay, so in this modeling, the body's expected temperature is like the expected return on investment and the variance around the expected temperature is like the variance around the investments. Right, but they wouldn't be the same policy is what I'm saying.
2172510	2221350	86	C	0.96726	36:12	112	Sorry. They wouldn't be the same payoff space in the second case. So if something generates $100 but has a low probability of going bust so that there's a high variance, that's not the same payoff as something that produces the same $100, but that has a lower variance, the payoff spaces are different and that will be captured in that callback diversions that you have there. It would prefer the one with the lower that's right, because the payoffs yeah, but I think the discussion was around if multiple things lead to the same epistemic value and they have the same payoff, why does it matter which one is taken? Right.
2222200	2243950	87	A	0.3408	37:02	38	There's that last piece. There may be multiple policies with the same pragmatic value and then epistemic value is the difference maker that favors policies that have more clarity about observation outcomes. Right. So pragmatic value would say.
2246100	2272650	88	C	0.99961	37:26	60	That'S not necessarily true. Let's say that. Okay. When the payoff space is completely unknown that the steps that you're taking at T equals one, two, three will bring down the will have the same pragmatic value. But one of these actions will lead to some very bad consequences at step T equals 15 because you're giving up something else.
2274220	2286590	89	C	0.95695	37:54	38	It will be like paying space because. If we're talking about temperature, we're in the space of temperature. Okay, all right, so let's just bring it back to the finance. The space is dollars. That's the dollars space.
2289600	2308212	90	C	0.80146	38:09	44	Yeah. Okay, so we could use dollar as numerator. So we benchmarked the dollars. How many dollars we're making profit. So I have two investments, both of them generate $100, but one of them requires me to borrow $100 in order to generate $100.
2308266	2324250	91	C	1.0	38:28	42	The other one would generate $100 over, say, 50 times steps. This would be quicker. The second one where you borrow $100. So it would be quicker because you're borrowing $100. But there's an interest rate that you have to pay out.
2325820	2348850	92	C	0.73481	38:45	54	So you will have to generate much more than $100 in order for this to be viable because you have an interest rate to pay off. Okay, try to follow up here. Policies are defined in terms of sequences of affordances to take. So the affordance here that you're highlighting is borrowing or not?
2350980	2378360	93	C	0.99999	39:10	66	Yes, but there are actions that would be like if you have a very high fever, in order to bring down that high fever, sometimes it makes sense to borrow that $100. Whatever metaphorical variation of that borrowing that you have to bring it down to something more manageable. Right. So it depends on how close you are to some sort of threshold where it's intolerable.
2380640	2420456	94	C	0.98	39:40	107	I mean, putting it in more mundane terms that, hey, I have a bad toothache. It doesn't make sense for me to borrow $100 to fix my tooth. I could just brush my teeth and hope that it goes away, but I have a knife in my back, I need to get to the hospital. It makes sense to pay $100 because you're going to die if you don't. Okay, let me try to try to see this is how I'm seeing that is when we're near homeostasis, then we take actions that basically keep us there, and we try not to add risk to the situation.
2420638	2441288	95	A	0.99968	40:20	47	Whereas near the limits of our homeostatic tolerance, we may engage in high risk behavior because there's already a non negligible. Risk because the payoff space is much larger. Yeah, the payoff space, the positive payoff from your own life. No, it's not. The space is no.
2441394	2463940	96	C	0.93671	40:41	54	So when you have the knife in your back, the payoff from borrowing $100 will save your life. When you have a toothache, borrowing $100 might cause more risk that you go bankrupt. Right. But you will still survive. If you have a toothache, you're just short $100, which you could have used elsewhere.
2464280	2483224	97	C	0.99875	41:04	47	There is no way you can use that $100 if you're dead. Yeah, there's a much higher payoff borrowing at that point than it is with the toothache example. It depends on where you take these actions. I got you. I'm seeing the space are the axes.
2483272	2498112	98	A	0.99984	41:23	37	So here we have dollars and life. And so you're saying that yeah, but. It is a multidimensional, this one. So if you think about it as, let's say yeah, so let's make it more concrete. Right?
2498166	2565750	99	C	0.99855	41:38	154	So if you have a drone and you have a trade off between battery life and staying in the air, and let's say one of your propellers goes out, just redistributing power to the other motors so that you stay in the air even though it reduces your hover time. Makes more sense because you could land versus trying to fly around and then crash eventually because you lost one motor. But whereas let's say that the other option would be let's just go faster to a certain destination when it doesn't really make sense to do so, you're just reducing your battery life to get to a destination faster. But if something happens on the wave which causes you to lose your motor, then you don't have enough battery in order to enough battery power to distribute to the automotive so that you can hover and come down safely. Does that make sense?
2567180	2568040	100	A	0.99934	42:47	1	Eric.
2570220	2577880	101	E	1.0	42:50	26	I wonder if we could table this particular discussion. It seems to be going and get back to some of the other questions in the chapter.
2580060	2636940	102	E	0.94555	43:00	142	We had a question that was pending from last week that I think is kind of critical to the understanding of the chapter, and I wonder if we could just want to make sure we have time for it, because we're running out of time here. Yeah, and that's the question about what the relative role is of how it actually operates, that the rat learns some information prior to deciding which of the two branches of the maze to take. And the way they seem to frame the chapter is that, well, they set it up so that the rat has its preference for epistemic value. Because of epistemic value of learning which of the two branches to take is a bigger term. It'll first go to the bottom branch and learn which of the two top branches has the payoff.
2637280	2692880	103	E	1.0	43:57	110	Then once that's resolved, it knows that, then it decides which of the top branches to go to and it gets its reward. So my understanding of that is that and this is my claim in this question, is that the rat has a preference to resolve epistemic uncertainty regardless of whether it's useful or not. So if you added more uncertain questions like, what color is my apple today? But there's no apple to be gotten, it'll still want to resolve that question and any other questions because those are uncertainties. So epistemic value, it'll collect that epistemic value, and then finally it gets around to collecting its reward.
2694500	2742480	104	E	0.9983	44:54	116	That's in contrast to a model where the purpose of resolving epistemic uncertainty is to gain the reward, which requires look ahead. So in other words, the planner or the policy would be, I try to learn what's needed to collect the reward. I do that, and that's posted as epistemic value with a purpose, then the order of operation is, okay, I figure out what I need to know, I learn what I need to know, then I go and do the exploitation. So that's what I would expect that this kind of framing would give us, but I don't see that that's what you're actually giving us. All right, here's how I see that.
2742630	2769050	105	A	0.85	45:42	59	The example that we would want to see would have a bunch of uncertainty resolving this one picks a number between one and ten and tells you what it is. This one's a number between one and a million. So it's an incredible information resolving button. Then the question is can the relevant sources of information be sought after?
2775670	2810480	106	A	0.92	46:15	55	I think the examples is prepared in several ways. For example, the rat already knows the semantics that this is related to here that's been just the structure of the model is already preparing that any uncertainty reduction about this basically it's already encoded in the model that this bottom queue is information about this.
2812770	2903526	107	A	0.99238	46:52	179	So this is kind of like saying and this relates earlier, not only are those implicit structurally encoded aspects of knowledge but also there's parameterization questions like we discussed if it has a dire urgency for food to the earlier questions about like the urgency of the imperative it may just go for it. And so I'd rather have a 50 50 shot now than a 98% in two timesteps. So it depends on how it's parameterized but even more deeply it depends on the construction of the model, what kinds of relationships are implicitly and explicitly being linked and it's like a deep level of modeling to understand what would be like a more neutral way to frame this question. And then it's like well, how many layers back do you want to pull? Like you could have a preplay where a rat has an association matrix, there's three information sources and it's allowed to freely explore without any shock or food and determine which one of these information sources has like a causal relationship to these edges.
2903718	2940390	108	A	1.0	48:23	78	And then there's a learnt queue that now we're in game time and now it's going to rely on its past learning about which information resource is relevant for the Pragmatic value and then it's I mean there's just how many layers back does one need to pull before the rabbit is not in the hat? I think is going to be a serious question because even toy demonstrations have been seen as slam dunk and they're not.
2943510	2958360	109	E	0.99923	49:03	44	Well since they're bringing up Palm DPS here palmdps are really good for planning but they don't have this rat do any planning. It seems like they're really not aspiring very strongly to build a smart rat here and they're not achieving it either.
2960830	2988450	110	A	0.53891	49:20	51	Yeah, it's true because here the first movement again presuming that the parameterization is such that epistemic value is salient. The first move is dominated by epistemic value and they say well, now that that value has been tapped, now it can step into pursuing Pragmatic value with even increased confidence.
2991510	3007590	111	A	1.0	49:51	18	But it's not considering the set of time horizon two policies that is not being explicitly encoded or.
3012460	3021550	112	E	0.9776	50:12	21	That means that. Even implicit yeah, these are transition matrices. These are just the four options depending on where you are.
3023680	3026690	113	A	1.0	50:23	7	You can stay, you can go down.
3034650	3039270	114	A	0.98674	50:34	6	Yeah. Does section. 73 address planning.
3051990	3054740	115	A	0.99	50:51	9	And if it doesn't, why do we need g?
3057510	3063430	116	A	1.0	50:57	12	Why can't we just use a one step variational free energy approximation?
3072520	3079850	117	A	0.88971	51:12	21	It's saying, what's the most likely thing for me to do right now? Is a question about now in the past.
3082780	3152444	118	E	0.67875	51:22	159	Well, I think the answer to the question is exactly as you said it before, which is they baked in this particular hardwired rat, for which in this case, the best thing to do is to first explore, learn, and then exploit. So they designed it to do only this by rote to do the right thing, as opposed to the rat actually doing any look ahead or any intelligence to do the right thing. Yeah, it's structurally implicitly hardwired, and then it's fine tuned because again, there's structures where it wouldn't do this and there's tunings where it wouldn't do this, but in the tens place and in the decimal point. This example exists in a very limited manifold of models and parameterizations where one step optimal policies emulate two step planning. Like, I just happen to love sacrificing pawns and taking castles, so I'm willing to sacrifice a pawn so that later I can do a castle.
3152492	3164560	119	A	0.99997	52:32	29	But of course, you can't walk around with the belief that you love sacrificing pawns. Yeah, I actually wondered why there wasn't enumeration of policies in the planning section.
3170300	3190820	120	A	0.79	52:50	25	Okay, learning hyperpriers hidden states on hidden states, seeing hidden states as outcomes of other hidden states so that they can be learned or fixed.
3195100	3219510	121	A	0.98917	53:15	48	Little bit of technical details on multi parameter minimization theta being just the vector of parameters for the generative model and deer Schlee distributions, which I think could be gotten into, but that's kind of distribution specific creatures select the most appropriate data to improve their generative models.
3226850	3240530	122	A	0.96	53:46	33	I think there's more that we can dig into and explore on similar wavelengths like risk plus ambiguity, but then now there's an information gain. I thought we just had risk plus ambiguity.
3245480	3259840	123	A	0.5	54:05	22	Then we didn't really yeah, people point to structure learning, but don't they always as a way to get around these questions.
3262150	3270580	124	A	0.82984	54:22	21	But then it's the structure of the structure learner, and then people end up with these turing machines that don't plan.
3274690	3297190	125	A	0.99668	54:34	39	People hope that Bayesian model reduction will be attractable and provide heuristics for structure learning. But I haven't seen any empirical examples that come to mind where Bayesian model reduction was used to identify actionable lower dimensional useful models.
3299210	3322590	126	A	0.99989	54:59	55	But we know that structure learning on the state space of hierarchical models is going to be essential active inference or beyond, because people thought that the explosions, the computational complexity class of just branching time, active inference or anything within a model. This is going to be like exploding upon that by several exponents.
3326180	3330880	127	A	0.93	55:26	11	And we have these fundamental questions about the continuous time interpolations.
3334820	3377260	128	A	0.63214	55:34	71	Why are there continuous interpolations in chapter seven when it's the discrete time chapter? Okay, next time we come to chapter eight, I think this will be quite interesting. We're going to talk about dynamical systems, motor control, lock of Ultera. We'll have some justified continuous lines, LaPlace assumption, Lorenz stream number 32, stochastic chaos and Markov blankets. Hybrid models with discrete continuous fusion and some advances in continuous time modeling.
3378560	3383050	129	A	0.99977	56:18	7	Thanks everybody. See you soon. Thanks everyone.

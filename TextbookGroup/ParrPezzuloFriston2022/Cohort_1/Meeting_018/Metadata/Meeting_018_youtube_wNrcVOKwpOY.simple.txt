SPEAKER_04:
Hello, cohort one.

It's meeting 18.

We're in our second discussion on chapter seven.

Let's go to the questions and see what we can explore today or see where else to go and or look towards chapter eight, but there's any number of ways we can do it.

So first, just on chapter seven,

Does anyone want to turn to any of these questions, add another question, add another reflection or a thought that arose in the last week?

One question that I had was, are there any ways or suites for, for example, taking in an analytical expression and then providing equivalent phrasings that might have other advantages?

Ali?


SPEAKER_03:
You mean by translating it into natural language?


SPEAKER_04:
Or... Like what we saw in equation 2.5 and 2.6.

Like to take in an expression and then output isomorphic, or I guess not isomorphic, but equivalent expressions.

Expressions that have the exact same value as calculated expressions.

Because some of, and when we look through the derivation, sometimes it's possible to trace the trail, but to know which representations of perhaps even the same functional or same term, it seems quite relevant.


SPEAKER_03:
Actually, there's an AI assistant for

deriving formal proofs for mathematical theorems, but I haven't used it myself, but I'm not sure if that's what you mean.

I'll look at the name of that and just one second.


SPEAKER_00:
Maybe there's a way to reformulate

these specific terms from a Bayesian mechanics formalism, like posterior predictive entropy, that looks like something that Dalton probably described as well.

The question is whether you can just interchange it.


SPEAKER_04:
Yeah, does working with the particular partition

enable equations to be operated with more composably because we can know that there are certain operations that are um like always sometimes never going to be valid

I'm just kind of asking.

I don't even know if that's the right way to have it.

So, last time we talked primarily about the mouse in the maze.

And we were going through...

uh the way that the this the chapter is layering on features of the model so first we saw um the mouse just go for it and now we're going to be uh it gets where it gets the cue

this comes to our earlier points about the resolution of the explore exploit trade-off i'm going to add a question anyone can give a thought while i'm adding it

What are posterior predictive entropy and expected ambiguity?

part of the decomposition of epistemic value posterior let me just make sure to say the posterior predictive entropy is the expected surprise or the entropy of the distribution of observations conditioned on a policy

So we're in expected free energy world.

We're talking about evaluating policies with respect to now putting aside pragmatic value, we're talking about decompositions of how the informational or the epistemic value of a policy is evaluated.

So there's two terms here.

First,

How dispersed is your distribution of outcomes for that policy?

One can imagine that all things being equal, you would want to select policies that have a tighter distribution of outcomes.

Here, we have an expectation.

Interestingly, this is a italics E, but it's not a fancy E.

Do people think there's a difference that matters or do they think that's a slight error?


SPEAKER_03:
I think that should be a typo.


SPEAKER_04:
Yeah, I'm going to add it to errata.

E is italics.

Should be fancy E.


UNKNOWN:
Okay.


SPEAKER_04:
It's the expectation over our hidden state estimates condition on a policy.

And it's the expectation of expected ambiguity.

So entropy of the A matrix.

functionally how outcomes depend on states so this is saying I want to be I want to be more um is it the case that it says I want to be more certain about observations and how they uh map to policy and

want to have a tighter a matrix eric so um knife question why is there an expectation on the right and not on the left i was also going to ask this isn't entropy already the expectation of surprise so just another point of um


SPEAKER_02:
Oh, I'm sorry.

So another point of clarification for myself that I want to make sure this is right is the O tilde is observations.

And what we're looking at here is a functional Q. So that's a distribution.

of observations that we'll get over policy, and that's what's being adjusted in the free energy.

So we're going to explore the distributions, Q, and the O-tildes are going to be past observations.

We're not looking forward there.

We're looking past, because those are actual observations.

They're not predicted.


SPEAKER_04:
through um the the hidden states s so straighten that out are we sure that o are only past observations I believe that past observations are accounted for with respect to how they influence the prior um but does o tilde also include future observations

if so then it has to be an expectation over something that mustn't it like but this is conditioned upon a given policy then these are the this is the time series of observations expected under that policy is the left term the q expected

distribution of observations conditioned on policy.

Jakob?


SPEAKER_00:
I was just going to say that I think I've seen this in other parts of the book as well, and I think they use the H and then just the expectation notation for entropy kind of interchangeably.


SPEAKER_04:
um okay if they do but then isn't this like a double expectation this is the this is the expectation of surprise on the queue but this is the expectation of the expectation of surprise yeah uh that i don't know

Eric?

Okay, but the left term, just, Ali, please.


SPEAKER_03:
Well, shouldn't that first line, I mean, the H, the both H's, shouldn't both of them be as the argument of the expectation according to equation 7.4?

Because in equation 7.4, we have the negative epistemic value, but it's somehow different from this equation.

Yeah.

Okay.


SPEAKER_04:
So in 7.4, we see a restatement of...

expected free energy equation two six let's leave pragmatic value to the side for now here we see fancy e qs tilde pi so this is looking like this probably could or should we also see hpos hpos minus

h q o pi okay so let's take the negative of the first line the negative of the first line you can kind of just flip the terms it's the expected ambiguity now minus the the posterior predictive entropy

then an expectation is being taken over that.


SPEAKER_03:
Oh, sorry, sorry, my bad.

I thought the square brackets, I mean, was encompassing the whole expectation term.

Sorry, sorry.


SPEAKER_04:
No, no, no, you're right, actually.

Oh, well, there's a lot of layers of it.


SPEAKER_03:
Actually, it's right as it's written.


SPEAKER_04:
Okay, okay.

Yeah, yeah.

That's great.

Okay, wait.

I always color my brackets when I'm trying to do this.

Okay, the outermost bracket, there actually may be a bracket.

I don't even know.

Let's try to figure it out.

The outer bracket starts here on the left and then it closes.

Okay, pragmatic value, if we take that as a standalone,

If we take that as standalone, is anyone else seeing a bracket sort of scenario?

If we look at the brackets as a whole, this first bracket closes this rightmost... No, I don't think so.


SPEAKER_03:
The first bracket closes at the end of the S tilde parenthesis bracket.

That's why I was confused.


SPEAKER_04:
I agree.

I was thinking, should we evaluate the brackets locally?

But yes, they are.


SPEAKER_03:
Okay, good.


SPEAKER_04:
okay so it's exactly the negative of equation 7.8 left side here is the negative of okay so the first term um let's just say it in terms of picking good policies we want

Policies are better with more clarity about observation outcomes.

That's the first term and a tighter A matrix.

That's the second term.

Okay.

Middle line.

This is a KL divergence.

between the two products of distributions.

Both of them are dealing with O in the first and S in the second.

The Q distribution in the second part is the same for both.

So maybe there's some interactions in how distributions of different kinds multiply, but

It may be fair to interpret this as the POS is the A matrix and the QO pi is how observations are shaped by policy.

Or another way to say it might be POS is outcome expectations conditioned upon hidden states of the world.

Whereas this is outcome distributions conditioned on what we do.


SPEAKER_02:
I mean, to me, that just looks like base law there because you've got at least in the

the left side a little bit because you've got condition on s and then s conditioned on pi so you're just basically saying you're trying to figure out what o is from pi through this intermediary s delta oh one yeah so


SPEAKER_04:
another angle would be where is this zero well this is zero when these two are identical when the epistemic value of a policy is low when it when the resulting o distribution is being unconditioned by the policy that we choose

Whereas for policies that change how we think about the A matrix, those policies have higher informational value.

Because POS is kind of like a policy agnostic mapping.

But we're like drilling down to kind of sub-variance partitions of informational value.

These are different ways to look at

what makes a policy have epistemic value the first one line was coming down to uh clarity around the tightness of outcome distributions the second one is related to how policy deviates our understanding of the a matrix

okay now let's turn to okay here's another um italic e not a fancy e thank you ali for this link i've i've heard of this one the the cock proof assistant but i've never seen it used or anything yeah

this looks pretty pretty interesting I wonder if anyone has done that kind of formal work on proving certain parts of FEP and then a more distal um question would be if we have formal ontology representations for example with sumo or otherwise could we make proofs around the ontology

And the sumo suggested upper merged ontology.

We explored some of those things, but didn't go super, super far.

It was just, it's, you know, take somebody who knows that area.

Okay.

The bottom term, the bottom line.

An expectation over Q, but now our Q is,

is on outcomes not focusing on the hidden states then it's an expectation about outcomes of a kl divergence between it's about this is about hidden states this kl divergence

is zero when outcomes are not when in addition to policy outcomes don't change how we think about hidden states so when outcomes influence how we think about hidden states this is going to be higher

and what what does on the right this is a triangle meaning defined as so q s pi o is defined as and this looks like a bayes theorem situation like kind of that chaining that eric just mentioned

Like you have P of O conditioned on S and then you have S conditioned on pi.

So then those kind of like, it's like a way to get from O through S to pi.

Whereas this one on the bottom is just like going directly about O conditioned on pi.

Which interestingly is, I don't know if this is reflected in one of these or in some subsets of them,

one can think about that kind of structural model.

Like the minimum hidden Markov model, not even in its full framing, but just like what are the three pillars of the hidden Markov model?

Observations, hidden states, policy.

Yes, transitions and so on.

And so what is gained by going through an intermediary hidden state

There's going to be situations where the observable contains 100% of the information for policy.

Or we can imagine situations where the ability to choose policies based upon S, which is where the actual POMDP is intervening, is better.

Whereas in a fully observable Markov decision process,


SPEAKER_02:
you don't have s you're just making decisions on o um kind of just revisiting the point i raised earlier about what o tilde is it seems to me that this only works when you can when you have the observations o tilde so

I don't see how it could be anything like looking toward the future.

Because if you actually want to carry out that, that you want to work out what Q is or use it, you have to compute it and you have to have the observations to compute it.

And, you know, the S-tildes, you have to have, you know, actually pipe through and use them.

So help me understand if I'm wrong with that.


SPEAKER_04:
Okay, here would be one thought.

The A matrix captures how hidden states map to observations.

So whether we have a really tight A matrix or whether we have a super dispersed A matrix, using the Gaussian form for the A matrix ensures that we can always say what the most likely observation is for a given hidden state.

If we had a bimodal A matrix,

then there are some issues but if we um use a gaussian form for the um like which is the laplacian approximation it's more tractable it's monotonic it's more optimizable so if we're given s sequences of s we can always derive the expected observations

Again, those might have a lot of uncertainty around them, but we can't always make a sequence of O's corresponding to any sequence of S. But this equation makes no expectation at all.


SPEAKER_02:
It says nothing about what the form of the distribution is that you use, whether it's parametric or not parametric or whatever.

It's just saying you have a distribution, Q. So this is how we evaluate it.


SPEAKER_03:
yes is this true for all families of distributions q ali what do you think about that or anything uh actually it's interesting that uh in the step-by-step active inference paper this epistemic value is defined uh somewhat differently uh i'm not sure if they're exactly equivalent with each other or there are some minor

differences between them, but they've just defined the epistemic value only in terms of the expectation over, I mean, the surprisals of, I mean, the difference between the Q and P surprisals.

So we don't have this kind of expression as conditioned on the S tilde here.

So

Yeah, that's, I don't know if they're the same or not.

I don't know how to copy the image into this chat of the gather, but I'm not sure if it's possible or not.


SPEAKER_04:
Put it in the question.


SPEAKER_03:
Okay.


SPEAKER_04:
Yeah.

Well, one thought is policy is not an open-ended thing.

policy in its specification is always going to be of a finite horizon so if we're considering policies of length two um well so so interestingly this is a policy that includes leading up to the presence if this is the presence

I think we had one meme that was like, what if my T minus one was T or something like that?

But this is a policy that, right, is influencing, and this is just the variant of 4.3.

Okay, but let's just say that we're considering policy of true future looking length two.

Then,

we're exploring different ways in which S is going to unfold with respect to basically B1 and B2.

Not using that in the superscript way to mean like two parallel Bs, but just like there's our current, there's our estimate now.

And then there's the way that we have, let's just say four affordances.

So then there's four B1s.

And then you have, now there's going to be four options for S and then each of those four, we could apply the second affordance again.

So there's 16 policies to evaluate.

Each one is defined by either taking like B1, B3, B1, B1, B4, B4.

And for each of those S in the next two time steps, we can emit observations.

Ali, and then Rohan.


SPEAKER_03:
Sorry, my hand just stayed raised.

I'm sorry.


SPEAKER_04:
Rohan?

Hello.

Yes, greetings.


SPEAKER_01:
Yeah.

So coming back to what the

posterior distribution is, right?

So how far, so is it just that, isn't it just that we assume there's a posterior distribution of states, like an ideal posterior distribution that we want to, that free energy, that the free energy principle moves, moves the current empirical distribution towards, isn't that, I thought that's what we were doing from the context in chapter one.


SPEAKER_04:
Yes.

So just just to quickly.


SPEAKER_01:
Yeah, go ahead.

Yeah.

Yeah, sorry.

So sorry, cohort one, I think that's what we discussed, right.

So it's essentially trying to maintain some form of homeostasis.

So whatever policies you have performed should eventually bring the system to that ideal homeostatic perspective, right.

So

Why couldn't we just assume like a normal distribution?

So even if it is fat-tailed, for example, the empirical distribution is like the P distribution in this formula is, you know, has a heavy left tail.

You can essentially bring it back towards that by essentially modifying the policies.

That was the context of this, right?


SPEAKER_04:
yes broadly you're right so when we look at the full free energy expected free energy formalization so here's equation 7.4 or equation 2.6 you're absolutely right that there's a pragmatic value that's based around reducing divergence between observations and preferences but in equation what we've been focusing on and trying to really unpack because again also like o tilde is used in pragmatic value


SPEAKER_01:
We're only talking here about... I mean, it would still be... Yeah, yeah, but that would make sense, right?

Because your conditioning on your actions having some effect on the state, on your own state and your state in the world, or maybe some state in the world,

So you have to continuously monitor until it comes down to the expected observation.

That's what the generative model would spit out, right?

So it makes perfect sense and it's completely coherent in that way, right?


SPEAKER_04:
Yes, I hope and slash expect and prefer that to be true.

What you're describing about like bringing the homeostatic variable closer to preferences expectations.

that is pragmatic yeah but that would also yeah so that wouldn't it wouldn't be surprising that hotel does that in like p distribution as well as what i'm saying no you wouldn't need it to be there yeah you're right it is needed to be there because these are surprises and entropies and so on about observations um and then these this part is really digging into um decompositions of epistemic value so these are

There might be two policies that are both expected to bring your temperature down or whatever.

But they have different epistemic values.


SPEAKER_01:
Does it matter if it just brings your temperature down?

It would look the same.

Essentially, it would have the same effect.

So why does it matter?

Why did we have to have this kind of...

Yeah, so there would be multiple policies that would bring it down, bring the temperature down.

If the preference is to bring the temperature down, it does not matter which policy is chosen, right?

So that's essentially what I think is being said here.


SPEAKER_04:
Okay.

So as long as you minimize the expected... Here's two investment opportunities from $100.

$150 plus or minus $100 or $150 plus or minus $200?

Which one do you want?


SPEAKER_01:
That's not what you were discussing.

So the payoff space is very different.

No, it's not the same thing.

They are the payoff space expected ambiguity in the $200 plus or minus $200 is very, very different.

And your mutual information will be very different because the payoff space is different.


SPEAKER_04:
Exactly.


SPEAKER_01:
You are essentially taking, yeah, but if something is going to bring down temperature plus or minus two, plus or minus four,

I mean, it's not a huge difference as long as it brings it down, right?

So that's essentially what I'm getting at.

Why does it matter?


SPEAKER_04:
Okay.

So in this modeling, the body's expected temperature is like the expected return on investment.

And the variance around the expected temperature is like the variance around the investment.

And so.


SPEAKER_01:
Right.

But they wouldn't be the same policy is what I'm saying.

Sorry, they wouldn't be the same payoff space in the second case.

So if something generates one hundred dollars but has a low probability of going bust, there's a high variance.

That's not the same payoff as something that reduces the same hundred dollars, but is that has a lower variance.

the payoff spaces are different, and that will be captured in that Kullback-Lindblur divergence that you have there.

That's right.

So you would prefer the one with the lower variance.

That's right.

Because the payoffs, yeah.

But I think the discussion was around if multiple things lead to the same epistemic value, and they have the same payoff,

Why does it matter which one is taken, right?

They're all the same.


SPEAKER_04:
Just that last piece.

There might be multiple policies with the same pragmatic value.

And then epistemic value is the difference maker that favors policies that have more clarity about observation outcomes.


SPEAKER_01:
Right, right.


SPEAKER_04:
So pragmatic value would say... That's not necessarily true, right?


SPEAKER_01:
That's not necessarily true.

Let's say that, okay, when the payoff space is completely unknown, that the steps that you're taking at t equals 1, 2, 3 will bring down the, will have the same pragmatic value.

But one of these actions will lead to some very bad consequences at step t equals 15, because you're giving up something else.


SPEAKER_04:
I'm just not sure if you want to pay off space because if we're talking about temperature, we're in the space of temperature.


SPEAKER_01:
Okay, so let's just bring it back to the finance.


SPEAKER_04:
Okay, the space is dollars.

That's the dollars space.


SPEAKER_01:
Yeah, okay, so we could use dollar as numeric.

So we benchmarked the dollars, how many dollars are making profit.

So I have two investments.

One, both of them generate

hundred dollars but one of them requires me to borrow a hundred dollars in order to generate a hundred dollars the other one will generate a hundred dollars over say 50 time steps this would be quicker the second one where you borrow a hundred dollars so it would be quicker because you're borrowing a hundred dollars but there's an interest rate that you have to uh pay out so there's a much uh so you will have to generate much more than a hundred dollars in order to

for this to be viable, right?

Because you have an interest rate to pay off.


SPEAKER_04:
Okay.

Try to follow up here.

Policies are defined in terms of sequences of affordances to take.

So the affordance here that you're highlighting is borrowing or not.


SPEAKER_01:
Yes, but there are actions that would be like, if you have a very high fever,

in order to bring down that high fever, sometimes it makes sense to borrow that $100 or whatever metaphorical variation of that borrowing that you have to bring it down to something more manageable, right?

So it depends on how close you are to some sort of threshold where it's intolerable.

I mean, putting it in more mundane terms,

that, hey, I have a bad toothache.

It doesn't make sense for me to borrow $100 to fix my tooth.

I could just brush my teeth and hope that it goes away.

But I have a knife in my back.

I need to get to the hospital.

It makes sense to pay $100 because you're going to die if you don't.


SPEAKER_04:
Okay, let me try to see.

This is how I'm seeing that is when we're near homeostasis,

then yeah we take actions that basically keep us there and we try not to like add risk to the situation whereas near the limits of our homeostatic tolerance we may engage in high risk behavior because there's like already a non-negligible like risk because the payoff space is much larger yeah the payoff space the positive payoff from saving your own life


SPEAKER_01:
space no it's not it's not the space is no it's not so when you are when you're closer to when you're closer when you have the knife in your back the payoff from borrowing a hundred dollars will save your life when you have a toothache borrowing a hundred dollars might cause more risk that you go bankrupt right but you won't you will still survive if you have the tooth toothache you're just short a hundred dollars which you could have used elsewhere

There's no way you can use that hundred dollars if you're dead.


SPEAKER_04:
Yeah.


SPEAKER_01:
So it does, there's a much higher payoff borrowing at that point than it is with the toothache example, right?

It depends on where you take these actions.


SPEAKER_04:
I got you.

I'm seeing the space are the axes.

So here we have dollars and life.


SPEAKER_01:
And so you're saying that, yeah, but it is a multi-dimensional, uh, this one.

So if you think about it as, let's say, yeah.

So let's make it more concrete, right?

So if you have a drone and you have a trade-off between battery life and staying in the air, and let's say one of your propellers goes out, you know, just redistributing power to the other motor so that you stay in the air, even though it reduces your hover time makes more sense because you could land versus, you know,

trying to fly around and then crash eventually because you lost one motor.

But whereas, let's say that the other option would be, let's just go faster to a certain destination when it doesn't really make sense to do so.

You're just reducing your battery life to get to a destination faster.

If something happens on the wave which causes you to lose your motor, then you don't have enough battery in order to, enough battery power to distribute to the automotor so that you can hover and come down safely.

Does it make sense?

Eric?


SPEAKER_02:
You know, I wonder if we could table this particular discussion.

It seems to be going, you know, and get back to some of the other questions in the chapter.

I mean, I actually...

we had a question that was pending from last week that I think is kind of critical to the understanding of the chapter.

And I wonder if we could, so I want to make sure we have time for it.

We're running out of time here.

Yeah.

And that's the, the question about what the relative role is of how it actually operates, that you learn some, that the rat learns some information prior to deciding which of the two branches of the maze to take.

And the way they seem to frame the chapter is that, well, they set it up so that the rat has its preference for epistemic value because the epistemic value of learning which of the two branches to take is a bigger term.

It'll first go to the bottom branch and learn which of the two top branches has the payoff.

Then once that's resolved, it knows that, then it decides which of the top branches to go to and it gets its reward.

So my understanding of that is that, and this is my claim in this question, is that the rat has a preference to resolve epistemic uncertainty regardless of whether it's useful or not.

So if you added more uncertain questions like,

you know, what color is, you know, my apple today, but there's no apple to be gotten, it'll still want to resolve that question and any other questions because those are uncertainties, so there's epistemic value.

It'll collect that epistemic value, and then finally it gets around to collecting its reward.

that's in contrast to a model where the purpose of resolving epistemic uncertainty is to gain the reward, which requires look ahead.

So in other words, the planner or the policy would be, I try to learn what's needed to collect the reward.

I do that.

And that's posted as epistemic value with a purpose.

Then the,

the order of operation is, okay, I figure out what I need to know, I learn what I need to know, then I go and do the exploitation.

So that's what I would expect that this kind of framing would give us, but I don't see that that's what you're actually giving us.


SPEAKER_04:
All right.

Here's how I see that.

The example that we would want to see

would have a bunch of uncertainty resolving.

This one picks a number between one and 10 and tells you what it is.

This one's a number between one and a million.

So it's an incredible information resolving button.

Then the question is, can the relevant sources of information be sought after?

I think the example is prepared in several ways.

For example, the rat already knows the semantics that this is related to here.

That's been just the structure of the model is already preparing that any information, any uncertainty reduction

about this like basically it's already encoded in the model that this bottom queue is information about this so this is kind of like saying um and this relates earlier like not only are the those those those like implicit structurally encoded aspects of knowledge but also there's parameterization questions like we discussed

if it has a dire urgency for food to the earlier questions about like the urgency of the imperative, it may just go for it.

And so I'd rather have a 50-50 shot now than a 98% in two time steps.

So it depends on how it's parameterized, but even more deeply, it depends on the construction of the model

what kinds of relationships are implicitly and explicitly being linked.

And it's like a deep level of modeling to understand what would be a more neutral way

to frame this question.

And then it's like, well, how many layers back do you want to pull?

Like you could have a pre-play where a rat has an association matrix.

There's three information sources.

And it's allowed to freely explore without any shock or food and determine which one of these information sources has like a causal relationship to these edges.

And then there's a learned cue that now we're in game time.

And now it's going to rely on its past learning about which information resource is relevant for the pragmatic value.

And then it's, I mean, there's just, how many layers back does one need to pull before the rabbit is not in the hat?

I think is going to be a serious question.

Because even toy demonstrations have been seen as slam dunk and they're not.


SPEAKER_02:
Well, since they're bringing up POMDPs here, POMDPs are really good for planning, but they don't have this rat do any planning.

It seems like they're really not aspiring very strongly to build a smart rat here, and they're not achieving it either.


SPEAKER_04:
Yeah, it's true because here, the first movement, again,

presuming that the parameterization is such that epistemic value is salient.

The first move is dominated by epistemic value.

And they say, well, now that that value has been tapped, now it can step into pursuing pragmatic value with even increased confidence.

But it's not considering the set of Time Horizon 2 policies.

That is not being explicitly encoded or.


SPEAKER_02:
That means that even implicit.


SPEAKER_04:
Yeah.

These are, these are transition matrices.

These are just the four options.

This is depending on where you are.

You can, if you're, you know, you can stay, you can go down.

Yeah, does Section 7 address planning?

And if it doesn't, why do we need G?

Why can't we just use a one-step variational free energy approximation?


SPEAKER_02:
what's it's it's saying what's the most likely thing for me to do right now is a question about now in the past well I think the answer to the question is exactly as you said it before which is they baked in this particular hardwired rat for which in this case it it it the the best thing to do is to first explore learn and then exploit

So they designed it to do only this by rote to do the right thing, as opposed to the rat actually doing any look ahead or any intelligence to do the right thing.


SPEAKER_04:
Yeah.

It's structurally implicitly hardwired.

And then it's fine-tuned.

Because, again, there's structures where it wouldn't do this, and there's tunings where it wouldn't do this.

But there's...

kind of in the tens place and in the decimal point, this example exists in a very limited manifold of models and parameterizations where one-step optimal policies emulate two-step planning.

Like, oh, I just happen to love sacrificing pawns and taking castles, so I'm willing to sacrifice a pawn so that later I can do a castle.

But of course, you can't walk around with the belief that you love sacrificing pawns

Yeah.

I actually wondered why there wasn't enumeration of policies in the planning section.

Okay.

Learning, hyper priors, hidden states on hidden states.

Seeing hidden states as outcomes.

of other hidden states so that they can be learnt or fixed a little bit of technical details on um on multi-parameter minimization theta being just the vector of parameters for the generative model and deersley distributions which i think could be gotten into but that's kind of distribution specific

Creatures select the most appropriate data to improve their generative models.

I think there's more that we can dig into and explore on similar wavelengths, like risk plus ambiguity, but then now there's an information gain.

I thought we just had risk plus ambiguity.

then we didn't really yeah people point to structure learning but don't they always as a way to get around these questions but then it's the structure of the structure learner and then people end up with these turing machines that don't plan

people hope that Bayesian model reduction will be attractable and provide heuristics for structure learning but I haven't seen any empirical examples that come to mind where Bayesian model reduction was used to identify actionable lower dimensional useful models

we know that structure learning on the state space of hierarchical models is going to be essential active inference or beyond because if people thought that the explosions the the computational complexity class of just branching time active inference or anything within a model this is going to be like exploding upon that by several exponents

and we have these fundamental questions about the continuous time interpolations why are there continuous interpolations in chapter seven when it's the discrete time chapter

okay next time we come to chapter eight i think this will be quite interesting we're going to talk about dynamical systems motor control um lockable terra we'll have some justified continuous lines um laplace assumption lorenz live stream number 32 stochastic chaos and markov blankets hybrid models with discrete continuous fusion and some advances in continuous time modeling

Thanks everybody.

See you soon.

Thanks everyone.
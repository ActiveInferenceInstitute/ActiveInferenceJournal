start	end	speaker	sentiment	confidence	text
650	2240	A	0.6372881531715393	Hello, cohort one.
2610	4074	A	0.8686521649360657	It's meeting 18.
4202	8880	A	0.8956182599067688	We're in our second discussion on chapter seven.
9650	21822	A	0.8148256540298462	Let's go to the questions and see what we can explore today or see where else to go and or look towards chapter eight.
21876	24622	A	0.5992566347122192	But there's any number of ways we can do it.
24676	29400	A	0.8837008476257324	So go first, just on chapter seven.
30650	42300	A	0.8010255694389343	Does anyone want to turn to any of these questions, add another question, add another reflection or a thought that arose in the last week?
60330	81610	A	0.821099042892456	One question that I had was, are there any ways or suites for, for example, taking in an analytical expression and then providing equivalent phrasings that might have other advantages?
86360	93130	B	0.8167642951011658	Ali you mean by translating it into natural language or.
96060	117120	A	0.8002219796180725	Like what we saw in equation 2.5 and 2.6, like, to take in an expression and then output isomorphic or I guess not isomorphic, but equivalent expressions, expressions that have the exact same value as calculated.
121380	125928	A	0.6349740624427795	And when we look through the derivation, sometimes it's possible to trace the trail.
126044	138680	A	0.6424018740653992	But to know which representations of perhaps even the same functional or same term, it seems quite relevant.
140620	141128	B	0.559049129486084	Actually.
141214	154988	B	0.6895913481712341	There's an AI assistant for deriving formal proofs for mathematical theorems, but I haven't used it myself, but I'm not sure if that's what you mean.
155154	160910	B	0.8299058675765991	I'll look at the name of that and just 1 second, I'll post.
162980	163632	C	0.6050544381141663	Maybe.
163766	182660	D	0.7893549799919128	There'S a way to reformulate these specific terms from, like, a Bayesian mechanics formalism, like posterior predictive, predicted entropy.
183240	186790	D	0.7633892893791199	That looks like something that Dalton probably described as well.
189180	191530	D	0.8003897666931152	Question is whether you can just interchange it.
195380	195792	A	0.5491447448730469	Yeah.
195846	209220	A	0.8316348791122437	Does working with the particular partition enable equations to be operated with more composably?
213660	225100	A	0.6100841164588928	Because we can know that there are certain operations that are always, sometimes never going to be valid.
227360	229084	A	0.7789665460586548	I'm just kind of asking.
229202	231890	A	0.703547477722168	I don't even know if that's the right way to have it.
232260	264970	A	0.9076024293899536	Um, so last time, we talked primarily about the mouse in the maze, and we are going through the way that the chapter is layering on features of the model.
265660	277980	A	0.8021857738494873	So first we saw the mouse just go for it, and now we're going to be where it gets the queue.
284240	290560	A	0.89670729637146	And this comes to our earlier points about the resolution of the Explore exploit tradeoff.
307150	308334	A	0.8297858238220215	I'm going to add a question.
308372	310318	A	0.8399542570114136	Anyone can give a thought while I'm adding it.
310484	315150	A	0.8575580716133118	What are posterior predictive entropy and expected ambiguity?
341070	348830	A	0.7028114199638367	They're part of the decomposition of epistemic value posterior.
352400	369410	A	0.8384544253349304	Let me just make sure to say the posterior predictive entropy is the expected surprise or the entropy of the distribution of observations conditioned on a policy.
369880	373510	A	0.7377621531486511	So we're in expected free energy world.
374360	384616	A	0.8654150366783142	We're talking about evaluating policies with respect to now, putting aside pragmatic value.
384718	394940	A	0.8728364706039429	We're talking about decompositions of how the informational or the epistemic value of a policy is evaluated.
395680	397550	A	0.8240792751312256	So there's two terms here.
398000	406830	A	0.8622544407844543	First, how dispersed is your distribution of outcomes for that policy?
408900	416880	A	0.7892988324165344	One can imagine that, all things being equal, you would want to select policies that have a tighter distribution of outcomes.
419080	422304	A	0.6333428025245667	Here we have an expectation.
422432	428230	A	0.7306002378463745	Interestingly, this is italics e, but it's not a fancy e.
429560	433480	A	0.5624662041664124	Do people think there's a difference that matters or do they think that's a slight error?
438300	440068	B	0.6476942896842957	I think that should be a typo.
440164	457590	A	0.7275886535644531	Yeah, I'm going to add it to erata e is TALIX should be fancy e.
469980	485112	A	0.8129168748855591	It's the expectation over our hidden state estimates condition on a policy and it's the expectation of the expected ambiguity.
485256	496530	A	0.7898008823394775	So entropy of the A matrix, functionally how outcomes depend on states.
498020	524756	A	0.8200061321258545	So this is saying I want to be I want to be more is it the case that it says, I want to be more certain about observations and how they map to policy and I want to have a tighter A matrix?
524948	525800	A	0.6499149203300476	Eric.
528140	530428	E	0.7950792908668518	So, nice question.
530514	533390	E	0.5359663963317871	Why is there an expectation on the right and not on the left?
534560	542560	A	0.793080747127533	I was also going to ask this isn't entropy already the expectation of surprise?
547860	552610	E	0.8199589252471924	Just another point of I'm sorry.
554840	567160	E	0.8538562059402466	So another point of clarification for myself that want to make sure this is right is the O tilde is observations.
568060	573690	E	0.8443050384521484	And what we're looking at here is a functional Q.
574460	591272	E	0.8872400522232056	So that's a distribution of observations that will get over policy and that's what's being adjusted in the free energy.
591346	602860	E	0.9064704179763794	So we're going to explore the distributions Q and the O Tilde's are going to be past observations.
602940	604532	E	0.7552258968353271	We're not looking forward there.
604586	607712	E	0.7869530320167542	We're looking past because those are actual observations.
607776	612950	E	0.5844922661781311	They're not predicted through the hidden states s.
613880	616150	E	0.7266128659248352	So straighten that out.
617340	620356	A	0.8287554383277893	Are we sure that O are only past observations?
620468	629400	A	0.8118396997451782	I believe that past observations are accounted for with respect to how they influence the prior.
632640	639420	A	0.9078232049942017	But does O tilde also include future observations?
641620	645730	E	0.5936345458030701	Well, if so, then it has to be an expectation over something, mustn't it?
646900	649730	A	0.7421735525131226	But this is conditioned upon a given policy.
651140	659350	A	0.8116780519485474	Then this is the time series of observations expected under that policy.
663160	674840	A	0.8595778346061707	Is the left term the Q expected distribution of observations conditioned on policy.
674990	675960	A	0.627000093460083	Jacob.
678460	693280	D	0.8453892469406128	I was just going to say that I think I've seen this in other parts of the book as well, and I think they use the h and then just the expectation notation for entropy kind of interchangeably.
697570	701710	A	0.6115425229072571	Okay, if they do, but then isn't this like a double expectation?
704050	713090	A	0.7060583233833313	This is the expectation of surprise on the queue, but this is the expectation of the expectation of surprise.
717830	718610	C	0.6331021189689636	That'S.
721130	725800	D	0.631454348564148	Yeah, that I don't know.
731760	743110	A	0.7252163290977478	Eric okay, but the left term just Ali, please.
746040	762270	B	0.8015019297599792	Well, shouldn't that first line, I mean, shouldn't both of them be at the argument of the expectation according to equation 7.4?
763680	772860	B	0.64737468957901	Because in equation 7.4 we have the negative Epistemic value, but it's somehow different from this equation.
772940	774000	B	0.6530584692955017	Yeah, that's.
785060	785810	A	0.584351658821106	Okay.
786340	794900	A	0.8111434578895569	So in 7.4 we see a restatement of expected free energy.
795050	796390	A	0.7453086376190186	Equation two, six.
798600	802710	A	0.8155229091644287	Let's leave pragmatic value to the side for now.
804860	809140	A	0.5931736826896667	Here we see fancy E-Q-S tilde pi.
809220	811544	A	0.711269199848175	So this is looking like this probably could.
811582	826700	A	0.8796533346176147	Or should we also see Hposhpos minus H-Q-O pi?
832720	836510	A	0.626422107219696	Okay, so let's take the negative of the first line.
837680	839710	A	0.5442472696304321	The negative of the first line.
841360	843710	A	0.7721485495567322	You can kind of just flip the terms.
844160	845836	A	0.6724110245704651	It's the expected ambiguity.
845868	850400	A	0.7302055954933167	Now minus the posterior predictive entropy.
855120	858908	A	0.5278410911560059	Then an expectation is being taken over.
858994	859630	A	0.5773851871490479	That.
861620	862912	B	0.7604926824569702	Sorry, my bad.
862966	870316	B	0.7407252192497253	I thought the square bracket was encompassing the whole expectation term.
870348	870930	B	0.5092127919197083	Sorry.
872500	874470	A	0.604986310005188	No, you're right, actually.
875560	882310	A	0.6751487851142883	Well, there's a lot of layers of it, actually.
882380	883990	B	0.6824164986610413	It's right as it's written.
891040	892270	A	0.6239427924156189	Okay, wait.
892640	896172	A	0.7476474642753601	I always color my brackets when I'm trying to do this.
896306	896990	A	0.584351658821106	Okay.
897520	900776	A	0.9113884568214417	The outermost bracket there actually may be a bracket.
900808	901516	A	0.5432154536247253	I don't even know.
901538	902656	A	0.7882980108261108	Let's try to figure it out.
902758	907228	A	0.9069432020187378	The outer bracket starts here on the left, and then it closes.
907324	907728	A	0.584351658821106	Okay.
907814	908672	A	0.6454819440841675	Pragmatic value.
908726	920740	A	0.9015370011329651	If we take that as a standalone if we take that as standalone, is anyone else seeing a bracket sort of scenario?
921160	927688	A	0.8858222961425781	If we look at the brackets as a whole, this first bracket closes this, right?
927774	929512	B	0.6271827220916748	No, I don't think so.
929566	938484	B	0.9257416725158691	The first bracket closes at the end of the S tilde parentheses bracket.
938532	941228	D	0.7124656438827515	Yeah, that's why I was confused, too.
941394	942508	A	0.6408694982528687	I agree.
942674	946060	A	0.8906784057617188	I was thinking, should we evaluate the brackets locally?
946720	948590	A	0.5816994309425354	But yes, they are.
949680	950830	A	0.7295194268226624	Okay, good.
953380	958064	B	0.6162962317466736	So it's exactly the negative of equation 7.8.
958182	966900	A	0.6738911867141724	Left side here is the negative of the first term.
968840	972260	A	0.6422020792961121	Let's just say it in terms of picking good policies.
973320	980196	A	0.5033755898475647	We want policies are better, with more clarity about observation outcomes.
980308	981720	A	0.7329141497612	That's the first term.
982380	984680	A	0.7566422820091248	And a tighter a matrix.
986860	988440	A	0.7712099552154541	That's the second term.
989520	990270	A	0.584351658821106	Okay.
991040	992110	A	0.5259913206100464	Middle line.
997360	1006320	A	0.6816501617431641	This is a KL divergence between the product two products of distributions.
1007700	1014470	A	0.8699598908424377	Both of them are dealing with O in the first and S in the second.
1015400	1019350	A	0.8888322114944458	The Q distribution in the second part is the same for both.
1021080	1046540	A	0.8599615097045898	So maybe there's some interactions in how distributions of different kinds multiply, but it may be fair to interpret this as the POS is the A matrix and the Q O.
1046610	1053810	A	0.8461676239967346	Pi is how observations are shaped by policy.
1058790	1072600	A	0.6030614972114563	Or another way to say it might be POS is outcome expectations conditioned upon hidden states of the world, whereas this is outcome distributions conditioned on what we do.
1073930	1096130	E	0.8338102698326111	To me, that just looks like base law there because you've got at least in the left side a little bit, because you've got conditioned on S and then S conditioned on pi.
1096950	1107250	E	0.7441784143447876	So you're just basically saying you're trying to figure out what O is from pi through this intermediary S delta.
1117430	1119666	A	0.7357338666915894	Another angle would be where is this zero?
1119848	1126070	A	0.5030433535575867	Well, this is zero when these two are identical.
1128490	1145900	A	0.5374706983566284	When the epistemic value of a policy is low, when the resulting O distribution is being unconditioned by the policy that we choose.
1147550	1157040	A	0.6125807762145996	Whereas for policies that change how we think about the A matrix, those policies have higher informational value.
1161020	1175308	A	0.825992226600647	Because POS is kind of like a policy agnostic mapping, but we're like drilling down to kind of subvariance partitions of informational value.
1175394	1183570	A	0.844241738319397	These are different ways to look at what makes a policy have epistemic value.
1184100	1193060	A	0.8723823428153992	The first one line was coming down to clarity around the tightness of outcome distributions.
1194840	1206120	A	0.8317998051643372	The second one is related to how policy deviates our understanding of the A matrix.
1225450	1228940	A	0.7808934450149536	Now let's turn to here's another.
1229950	1230874	A	0.7086993455886841	Italic e.
1230912	1232346	A	0.5959346294403076	Not a fancy e.
1232528	1234480	A	0.9715050458908081	Thank you, Ollie, for this link.
1234930	1240960	A	0.6273118853569031	I've heard of this one, the Cock proof assistant, but I've never seen it used or anything.
1244980	1248820	A	0.9824355244636536	Yeah, this looks pretty interesting.
1248890	1254660	A	0.8408377766609192	I wonder if anyone has done that kind of formal work on proving certain parts of FEP.
1255400	1281020	A	0.8761082887649536	And then a more distal question would be if we have formal ontology representations, for example with Sumo or otherwise, could we make proofs around the ontology and the Sumo suggested upper merged ontology?
1281520	1285470	A	0.5306292176246643	We explored some of those things but didn't go super, super far.
1286560	1293090	A	0.8238628506660461	It was just take somebody who knows that area.
1294260	1302740	A	0.5773379802703857	Okay, the bottom term bottom line an expectation over Q.
1302810	1311450	A	0.6596157550811768	But now our Q is on outcomes not focusing on the hidden states.
1314780	1331470	A	0.7981565594673157	Then it's an expectation about outcomes of a KL divergence between this is about hidden states.
1333040	1349810	A	0.5475364923477173	This KL divergence is zero when outcomes are not when in addition to policy, outcomes don't change how we think about hidden states.
1351620	1358370	A	0.7323058843612671	So when outcomes influence how we think about hidden states, this is going to be higher.
1373060	1378470	A	0.857039213180542	And what does on the right?
1378920	1391210	A	0.8499248623847961	This is a triangle, meaning defined as so QS pi, O is defined as and this looks like a Bayes theorem situation.
1393900	1396730	A	0.7959577441215515	Like kind of that chaining that Eric just mentioned.
1399980	1405480	A	0.8086419105529785	Like you have P of O conditioned on S and then you have S conditioned on pi.
1405640	1413580	A	0.8735813498497009	So then those kind of like it's like a way to get from O through S to pi.
1413940	1421200	A	0.8300669193267822	Whereas this one on the bottom is just like going directly about O conditioned on pi.
1424420	1430070	A	0.7138996720314026	Which interestingly is I don't know if this is like in one of these or in some subsets of them.
1431240	1443672	A	0.8367657661437988	One can think about that kind of structural model like the minimum hidden Markov model, not even in its full framing, but just like what are the three pillars of the hidden Markov model?
1443806	1449480	A	0.7472147345542908	Observations, hidden states, policy, yes, transitions and so on.
1449630	1456270	A	0.7837856411933899	And so what is gained by going through an intermediary hidden state?
1457920	1464690	A	0.8555426001548767	There's going to be situations where the observable contains 100% of the information for policy.
1465780	1481670	A	0.7733609676361084	Or we can imagine situations where the ability to choose policies based upon S, which is where the actual POMDP is intervening is better.
1482200	1494010	A	0.7604689002037048	Whereas in a fully observable Markov decision process, you don't have S, you're just making decisions on O.
1500730	1506554	E	0.9169211387634277	Kind of just revisiting the point I raised earlier about what Otilda is.
1506752	1513530	E	0.5626250505447388	It seems to me that this only works when you have the observations O tilde.
1514430	1521054	E	0.4979633688926697	So I don't see how it could be anything like looking toward the future.
1521252	1534226	E	0.8499215841293335	Because if you actually want to carry out that expectation, you want to work out what Q is or use it, you have to compute it and you have to have the observations to compute it.
1534408	1543158	E	0.816606879234314	And the S tilde you have to have actually pipe through and use them.
1543244	1546120	E	0.7454037070274353	So help me understand if I'm wrong with that.
1556640	1560304	A	0.8291850686073303	Okay, here would be one thought.
1560422	1565116	A	0.8846701979637146	The A matrix captures how hidden states mapped observations.
1565308	1583600	A	0.8372153639793396	So whether we have a really tight A matrix or whether we have a super dispersed A matrix, using the Gaussian form for the A matrix ensures that we can always say what the most likely observation is for a given hidden state.
1583770	1589210	A	0.6117851138114929	If we had a bimodal A matrix, then there are some issues.
1589820	1603420	A	0.6814709901809692	But if we use a Gaussian form, which is the Laplacian approximation, it's more tractable, it's monotonic, it's more optimizable.
1603760	1613520	A	0.8495318293571472	So for a given S sequences of S, we can always derive the expected observations.
1614100	1621910	A	0.8464205265045166	Again, those might have a lot of uncertainty around them, but we can always make a sequence of O's corresponding to any sequence of S.
1623640	1627684	E	0.6198928356170654	But this equation makes no expectation at all.
1627722	1633784	E	0.6978215575218201	Nothing says nothing about what the form of the distribution is that you use, whether it's parametric or not parametric or whatever.
1633902	1636330	E	0.8047275543212891	It's just saying you have a distribution Q.
1639020	1642010	E	0.7589848637580872	So this is how we evaluate it?
1649190	1649940	A	0.46103888750076294	Yes.
1650310	1658260	A	0.9093210101127625	Is this true for all families of distributions q Ali, what do you think about that or anything?
1660390	1670310	B	0.7559388279914856	Actually, it's interesting that in the step by step active inference paper, this epistemic value is defined somewhat differently.
1670890	1690750	B	0.6689662337303162	I'm not sure if they're exactly equivalent with each other or there are some minor differences between them, but they've just defined the epistemic value only in terms of the expectation over the surprisals.
1692450	1692814	A	0.5665541291236877	The.
1692852	1694922	B	0.8557802438735962	Difference between the Q and P surprisals.
1694986	1706020	B	0.6564193367958069	So we don't have this kind of expression as conditioned on the S tilde here.
1710070	1713320	B	0.5073081254959106	I don't know if they're the same or not.
1716410	1724060	B	0.7112654447555542	I don't know how to copy the image into this chat, but chat of the gatherer or just put it in here.
1724430	1727100	A	0.7190808653831482	Put it in the question.
1728110	1728666	B	0.584351658821106	Okay.
1728768	1741870	A	0.6472472548484802	Yeah, well, one thought is policy is not an open ended policy in its specification is always going to be of a finite horizon.
1742450	1755490	A	0.7956814169883728	So if we're considering policies of length two so interestingly, this is a policy that includes leading up to the Presence.
1757030	1764182	A	0.8198755979537964	If this is the presence, I think we had one meme that was like what if my T minus one was T or something like that.
1764236	1774780	A	0.5660178065299988	But this is a policy that is influencing and this is just the variant of 43.
1775790	1793598	A	0.8950799703598022	But let's just say that we're considering policy of true future looking length two, then we're exploring different ways in which S is going to unfold with respect to basically B one and B two.
1793764	1807374	A	0.7940186262130737	Not using that in the SuperScript way to mean like two parallel B's, but just like there's our current, there's our estimate now and then there's the way that we have, let's just say four affordances.
1807502	1815846	A	0.8734846711158752	So then there's four B ones, and then now there's going to be four options for S.
1816028	1819640	A	0.876594603061676	And then each of those four we could apply the second affordance again.
1820010	1823058	A	0.8271760940551758	So there's 16 policies to evaluate.
1823234	1840090	A	0.8876654505729675	Each one is defined by either taking, like, B one, B three, B one, B one, B four, B four, and for each of those S, in the next two time steps, we can emit observations.
1841550	1844490	A	0.77362459897995	Ali and then rohan.
1848020	1849544	B	0.6520562171936035	Sorry, my hand.
1849622	1850984	B	0.5745919346809387	Just stay away.
1851102	1851770	A	0.5092127919197083	Sorry.
1853100	1854120	A	0.7415806651115417	Rohan.
1863160	1863572	B	0.5208580493927002	Hello.
1863626	1864820	A	0.7769964933395386	Yes, greetings.
1865900	1866552	C	0.5491447448730469	Yeah.
1866686	1872090	C	0.9128526449203491	So coming back to what the posterior distribution is, right?
1875980	1892540	C	0.799255907535553	Isn't it just that we assume there's a posterior distribution of states, like an ideal posterior distribution that we want to that the free energy principle moves the current empirical distribution towards?
1892610	1893240	C	0.7665123343467712	Isn't that?
1893330	1898064	C	0.8884915113449097	I thought that's what we were doing from context in chapter one.
1898182	1902770	A	0.5011704564094543	Yes, go ahead.
1905080	1909184	C	0.6335853338241577	Yeah, sorry, cohort one, I think that's what we discussed.
1909232	1909444	C	0.7360090017318726	Right?
1909482	1913904	C	0.858701765537262	So it's essentially trying to maintain some form of homeostasis.
1913952	1923844	C	0.8383924961090088	So whatever policies you have performed should eventually bring the system to that ideal homeostatic perspective.
1923892	1924104	C	0.5664746165275574	Right.
1924142	1928164	C	0.7209480404853821	So why couldn't we just assume, like, a normal distribution?
1928212	1946850	C	0.8617910742759705	So even if it is fat tailed, for example, the empirical distribution is like the P distribution in this formula has a heavy left tail, you can essentially bring it back towards that by essentially modifying the policy.
1947220	1948864	C	0.8189731240272522	That was the context of this.
1948902	1949490	C	0.7360090017318726	Right?
1950980	1953232	A	0.6700548529624939	Yes, broadly, you're right.
1953366	1971716	A	0.5326160788536072	So when we look at the full free energy, expected free energy formalization so here's equation 7.4 or equation 2.6, you're absolutely right that there's a pragmatic value that's based around reducing divergence between observations and preferences.
1971908	1980990	A	0.8644393682479858	But in equation one that we've been focusing on and trying to really unpack, because, again, also like O tilde is used in pragmatic value.
1981600	1983550	A	0.7787700891494751	We're only talking here about.
1988400	1990092	C	0.7181395292282104	But that would make sense, right?
1990146	2003532	C	0.7543702721595764	Because you're conditioning on your actions, having some effect on the state, on your own state and your state in the world or maybe some state in the world.
2003686	2009936	C	0.8226407170295715	So you have to continuously monitor until it comes down to the expected observation.
2009968	2012532	C	0.6341745853424072	That's what the generative model would spit out.
2012586	2019530	C	0.8462790846824646	Right, so it makes perfect sense and it's completely coherent in that way, right?
2020620	2021320	A	0.46103888750076294	Yes.
2021470	2025864	A	0.639806866645813	I hope and expect and prefer that to be true.
2025982	2033144	A	0.8709010481834412	What you're describing about bringing the homeostatic variable closer to preferences expectations, that is pragmatic.
2033192	2033790	C	0.5491447448730469	Yeah.
2036640	2041500	C	0.8023149371147156	It wouldn't be surprising that Hotel does that in that P distribution as well as what I'm saying.
2041570	2043552	C	0.683661699295044	No, it would need it to be there.
2043686	2050160	A	0.619809091091156	You're right, it is needed to be there because these are surprises and entropies and so on about observations.
2051380	2058710	A	0.7794093489646912	And then this part is really digging into decompositions of epistemic value.
2059080	2070840	A	0.5745672583580017	So there might be two policies that are both expected to bring your temperature down or whatever, but they have different epistemic values.
2073180	2076490	C	0.5356802344322205	Does it matter if it just brings your temperature down?
2077020	2078570	C	0.690502941608429	It would look the same.
2079520	2081228	C	0.8432930707931519	Essentially, it would have the same effect.
2081314	2083150	C	0.675072968006134	So why does it matter?
2085360	2093952	C	0.8510085940361023	Why did we have to have this kind of yeah, so there would be multiple policies that would bring it down, bring the temperature down.
2094086	2098364	C	0.5900068283081055	If the preference is to bring the temperature down, it does not matter which policy is chosen.
2098412	2098576	A	0.5664746165275574	Right.
2098598	2102050	C	0.7964041233062744	So that's essentially what I think it's being said here.
2102740	2105292	C	0.740520715713501	Okay, so as long as you minimize.
2105356	2115590	A	0.8903646469116211	The expected here's two investment opportunities from $100 150 plus or minus one or 150 plus or -200 which one do you want.
2117480	2122824	C	0.5445398688316345	That'S not what you were discussing it is the payoff space is very different.
2122942	2124440	C	0.548575222492218	No, it's not the same thing.
2124510	2135260	C	0.7649382948875427	They are the payoff space expected ambiguity in the $200 plus or minus $200 is very different and your mutual information will be very different because the payoff space is different.
2135330	2135660	A	0.5662814974784851	Exactly.
2135730	2148752	C	0.5954124927520752	You are essentially taking yeah, but if something is going to bring down temperature plus or minus two, plus or minus four, it's not a huge difference as long as it brings it down.
2148806	2149024	C	0.5664746165275574	Right.
2149062	2153404	C	0.6722292900085449	So that's essentially what I'm getting at, why does it matter?
2153542	2168040	A	0.8269611597061157	Okay, so in this modeling, the body's expected temperature is like the expected return on investment and the variance around the expected temperature is like the variance around the investments.
2169740	2172440	C	0.6722694635391235	Right, but they wouldn't be the same policy is what I'm saying.
2172510	2172840	C	0.5092127919197083	Sorry.
2172910	2176184	C	0.5885531902313232	They wouldn't be the same payoff space in the second case.
2176302	2199810	C	0.6368124485015869	So if something generates $100 but has a low probability of going bust so that there's a high variance, that's not the same payoff as something that produces the same $100, but that has a lower variance, the payoff spaces are different and that will be captured in that callback diversions that you have there.
2200420	2220660	C	0.7581586241722107	It would prefer the one with the lower that's right, because the payoffs yeah, but I think the discussion was around if multiple things lead to the same epistemic value and they have the same payoff, why does it matter which one is taken?
2220730	2221350	C	0.5664746165275574	Right.
2222200	2223336	A	0.7628438472747803	There's that last piece.
2223438	2236780	A	0.584818959236145	There may be multiple policies with the same pragmatic value and then epistemic value is the difference maker that favors policies that have more clarity about observation outcomes.
2238640	2239390	C	0.5664746165275574	Right.
2240800	2243950	A	0.7737436294555664	So pragmatic value would say.
2246100	2247392	C	0.5198335647583008	That'S not necessarily true.
2247446	2248450	C	0.6871539354324341	Let's say that.
2248820	2249376	C	0.584351658821106	Okay.
2249478	2263268	C	0.7042409777641296	When the payoff space is completely unknown that the steps that you're taking at T equals one, two, three will bring down the will have the same pragmatic value.
2263434	2272650	C	0.9499112963676453	But one of these actions will lead to some very bad consequences at step T equals 15 because you're giving up something else.
2274220	2275976	C	0.7860333323478699	It will be like paying space because.
2275998	2278004	A	0.8654914498329163	If we're talking about temperature, we're in the space of temperature.
2278052	2283160	C	0.7746670246124268	Okay, all right, so let's just bring it back to the finance.
2283320	2284764	A	0.6619258522987366	The space is dollars.
2284962	2286590	A	0.6958611011505127	That's the dollars space.
2289600	2290012	C	0.5491447448730469	Yeah.
2290066	2292804	C	0.8533404469490051	Okay, so we could use dollar as numerator.
2292952	2294640	C	0.7859129905700684	So we benchmarked the dollars.
2294710	2296476	C	0.8366549611091614	How many dollars we're making profit.
2296588	2308212	C	0.708997905254364	So I have two investments, both of them generate $100, but one of them requires me to borrow $100 in order to generate $100.
2308266	2312544	C	0.8509750366210938	The other one would generate $100 over, say, 50 times steps.
2312592	2313860	C	0.493772029876709	This would be quicker.
2314440	2316808	C	0.8042544722557068	The second one where you borrow $100.
2316894	2319720	C	0.6818532347679138	So it would be quicker because you're borrowing $100.
2319790	2324250	C	0.5943187475204468	But there's an interest rate that you have to pay out.
2325820	2336044	C	0.7096329927444458	So you will have to generate much more than $100 in order for this to be viable because you have an interest rate to pay off.
2336242	2338780	A	0.8089375495910645	Okay, try to follow up here.
2338930	2344130	A	0.8120297789573669	Policies are defined in terms of sequences of affordances to take.
2344740	2348850	A	0.822909414768219	So the affordance here that you're highlighting is borrowing or not?
2350980	2364740	C	0.6460129022598267	Yes, but there are actions that would be like if you have a very high fever, in order to bring down that high fever, sometimes it makes sense to borrow that $100.
2364890	2371780	C	0.8450536727905273	Whatever metaphorical variation of that borrowing that you have to bring it down to something more manageable.
2371860	2372490	C	0.5664746165275574	Right.
2372860	2378360	C	0.664646327495575	So it depends on how close you are to some sort of threshold where it's intolerable.
2380640	2388136	C	0.8299678564071655	I mean, putting it in more mundane terms that, hey, I have a bad toothache.
2388168	2391576	C	0.9485898017883301	It doesn't make sense for me to borrow $100 to fix my tooth.
2391608	2398080	C	0.8709922432899475	I could just brush my teeth and hope that it goes away, but I have a knife in my back, I need to get to the hospital.
2398580	2403196	C	0.4815084934234619	It makes sense to pay $100 because you're going to die if you don't.
2403308	2420456	A	0.8175633549690247	Okay, let me try to try to see this is how I'm seeing that is when we're near homeostasis, then we take actions that basically keep us there, and we try not to add risk to the situation.
2420638	2429876	A	0.525458037853241	Whereas near the limits of our homeostatic tolerance, we may engage in high risk behavior because there's already a non negligible.
2430068	2432760	C	0.8014265894889832	Risk because the payoff space is much larger.
2432840	2437070	C	0.7328010201454163	Yeah, the payoff space, the positive payoff from your own life.
2438480	2439550	C	0.5608491897583008	No, it's not.
2439920	2441288	A	0.5903409123420715	The space is no.
2441394	2450464	C	0.6479761004447937	So when you have the knife in your back, the payoff from borrowing $100 will save your life.
2450662	2455580	C	0.9053844809532166	When you have a toothache, borrowing $100 might cause more risk that you go bankrupt.
2455740	2456076	C	0.5664746165275574	Right.
2456118	2458240	C	0.610102117061615	But you will still survive.
2458320	2463940	C	0.680580198764801	If you have a toothache, you're just short $100, which you could have used elsewhere.
2464280	2468132	C	0.9195315837860107	There is no way you can use that $100 if you're dead.
2468196	2476376	C	0.7048448324203491	Yeah, there's a much higher payoff borrowing at that point than it is with the toothache example.
2476558	2479288	C	0.8547633290290833	It depends on where you take these actions.
2479464	2480220	A	0.4730842113494873	I got you.
2480290	2483224	A	0.7783032059669495	I'm seeing the space are the axes.
2483272	2485790	A	0.644646167755127	So here we have dollars and life.
2486640	2488748	A	0.5859366655349731	And so you're saying that yeah, but.
2488834	2491004	C	0.765097439289093	It is a multidimensional, this one.
2491042	2497836	C	0.7959146499633789	So if you think about it as, let's say yeah, so let's make it more concrete.
2497868	2498112	C	0.7360090017318726	Right?
2498166	2519528	C	0.7211907505989075	So if you have a drone and you have a trade off between battery life and staying in the air, and let's say one of your propellers goes out, just redistributing power to the other motors so that you stay in the air even though it reduces your hover time.
2519614	2530220	C	0.6069122552871704	Makes more sense because you could land versus trying to fly around and then crash eventually because you lost one motor.
2531280	2548924	C	0.467045396566391	But whereas let's say that the other option would be let's just go faster to a certain destination when it doesn't really make sense to do so, you're just reducing your battery life to get to a destination faster.
2549052	2563540	C	0.5467332601547241	But if something happens on the wave which causes you to lose your motor, then you don't have enough battery in order to enough battery power to distribute to the automotive so that you can hover and come down safely.
2564440	2565750	C	0.7228710055351257	Does that make sense?
2567180	2568040	A	0.6499149203300476	Eric.
2570220	2572516	E	0.9028005599975586	I wonder if we could table this particular discussion.
2572548	2577880	E	0.8903491497039795	It seems to be going and get back to some of the other questions in the chapter.
2580060	2589900	E	0.7580854296684265	We had a question that was pending from last week that I think is kind of critical to the understanding of the chapter, and I wonder if we could just want to make sure we have time for it, because we're running out of time here.
2589970	2609830	E	0.8386314511299133	Yeah, and that's the question about what the relative role is of how it actually operates, that the rat learns some information prior to deciding which of the two branches of the maze to take.
2610520	2621544	E	0.6366210579872131	And the way they seem to frame the chapter is that, well, they set it up so that the rat has its preference for epistemic value.
2621662	2628132	E	0.7951346635818481	Because of epistemic value of learning which of the two branches to take is a bigger term.
2628196	2636940	E	0.8755918145179749	It'll first go to the bottom branch and learn which of the two top branches has the payoff.
2637280	2644716	E	0.8387459516525269	Then once that's resolved, it knows that, then it decides which of the top branches to go to and it gets its reward.
2644908	2663860	E	0.85104900598526	So my understanding of that is that and this is my claim in this question, is that the rat has a preference to resolve epistemic uncertainty regardless of whether it's useful or not.
2664010	2673848	E	0.7367164492607117	So if you added more uncertain questions like, what color is my apple today?
2674014	2683976	E	0.6774871349334717	But there's no apple to be gotten, it'll still want to resolve that question and any other questions because those are uncertainties.
2684088	2692880	E	0.8464803099632263	So epistemic value, it'll collect that epistemic value, and then finally it gets around to collecting its reward.
2694500	2704916	E	0.6609402894973755	That's in contrast to a model where the purpose of resolving epistemic uncertainty is to gain the reward, which requires look ahead.
2705098	2714336	E	0.878166913986206	So in other words, the planner or the policy would be, I try to learn what's needed to collect the reward.
2714448	2730792	E	0.8492511510848999	I do that, and that's posted as epistemic value with a purpose, then the order of operation is, okay, I figure out what I need to know, I learn what I need to know, then I go and do the exploitation.
2730936	2738124	E	0.6019111275672913	So that's what I would expect that this kind of framing would give us, but I don't see that that's what you're actually giving us.
2738322	2742480	A	0.7785913944244385	All right, here's how I see that.
2742630	2754164	A	0.7247804999351501	The example that we would want to see would have a bunch of uncertainty resolving this one picks a number between one and ten and tells you what it is.
2754202	2755668	A	0.8548091650009155	This one's a number between one and a million.
2755754	2759910	A	0.8802663087844849	So it's an incredible information resolving button.
2761480	2769050	A	0.8127711415290833	Then the question is can the relevant sources of information be sought after?
2775670	2780740	A	0.8440057039260864	I think the examples is prepared in several ways.
2781370	2810480	A	0.8547800779342651	For example, the rat already knows the semantics that this is related to here that's been just the structure of the model is already preparing that any uncertainty reduction about this basically it's already encoded in the model that this bottom queue is information about this.
2812770	2844778	A	0.8734076619148254	So this is kind of like saying and this relates earlier, not only are those implicit structurally encoded aspects of knowledge but also there's parameterization questions like we discussed if it has a dire urgency for food to the earlier questions about like the urgency of the imperative it may just go for it.
2844944	2850890	A	0.624770998954773	And so I'd rather have a 50 50 shot now than a 98% in two timesteps.
2851310	2881794	A	0.821569561958313	So it depends on how it's parameterized but even more deeply it depends on the construction of the model, what kinds of relationships are implicitly and explicitly being linked and it's like a deep level of modeling to understand what would be like a more neutral way to frame this question.
2881832	2884278	A	0.7737411856651306	And then it's like well, how many layers back do you want to pull?
2884444	2903526	A	0.890117883682251	Like you could have a preplay where a rat has an association matrix, there's three information sources and it's allowed to freely explore without any shock or food and determine which one of these information sources has like a causal relationship to these edges.
2903718	2928640	A	0.6484150886535645	And then there's a learnt queue that now we're in game time and now it's going to rely on its past learning about which information resource is relevant for the Pragmatic value and then it's I mean there's just how many layers back does one need to pull before the rabbit is not in the hat?
2929220	2940390	A	0.5561123490333557	I think is going to be a serious question because even toy demonstrations have been seen as slam dunk and they're not.
2943510	2949954	E	0.4995049238204956	Well since they're bringing up Palm DPS here palmdps are really good for planning but they don't have this rat do any planning.
2950002	2958360	E	0.8826956152915955	It seems like they're really not aspiring very strongly to build a smart rat here and they're not achieving it either.
2960830	2976830	A	0.7404511570930481	Yeah, it's true because here the first movement again presuming that the parameterization is such that epistemic value is salient.
2977890	2988450	A	0.5041642189025879	The first move is dominated by epistemic value and they say well, now that that value has been tapped, now it can step into pursuing Pragmatic value with even increased confidence.
2991510	3007590	A	0.5587401986122131	But it's not considering the set of time horizon two policies that is not being explicitly encoded or.
3012460	3013256	E	0.598052442073822	That means that.
3013278	3016700	A	0.7924312949180603	Even implicit yeah, these are transition matrices.
3017680	3021550	A	0.8499195575714111	These are just the four options depending on where you are.
3023680	3026690	A	0.7219871878623962	You can stay, you can go down.
3034650	3035254	A	0.5491447448730469	Yeah.
3035372	3036322	A	0.7202738523483276	Does section.
3036386	3039270	A	0.7634440064430237	73 address planning.
3051990	3054740	A	0.5998111963272095	And if it doesn't, why do we need g?
3057510	3063430	A	0.6877697110176086	Why can't we just use a one step variational free energy approximation?
3072520	3075750	A	0.822607159614563	It's saying, what's the most likely thing for me to do right now?
3077580	3079850	A	0.8359586596488953	Is a question about now in the past.
3082780	3099112	E	0.6139109134674072	Well, I think the answer to the question is exactly as you said it before, which is they baked in this particular hardwired rat, for which in this case, the best thing to do is to first explore, learn, and then exploit.
3099256	3110896	E	0.5478578805923462	So they designed it to do only this by rote to do the right thing, as opposed to the rat actually doing any look ahead or any intelligence to do the right thing.
3110998	3131050	A	0.7476528286933899	Yeah, it's structurally implicitly hardwired, and then it's fine tuned because again, there's structures where it wouldn't do this and there's tunings where it wouldn't do this, but in the tens place and in the decimal point.
3132060	3143660	A	0.7907143831253052	This example exists in a very limited manifold of models and parameterizations where one step optimal policies emulate two step planning.
3144160	3152444	A	0.4982683062553406	Like, I just happen to love sacrificing pawns and taking castles, so I'm willing to sacrifice a pawn so that later I can do a castle.
3152492	3156720	A	0.6015357375144958	But of course, you can't walk around with the belief that you love sacrificing pawns.
3158020	3164560	A	0.5605866312980652	Yeah, I actually wondered why there wasn't enumeration of policies in the planning section.
3170300	3190820	A	0.8549992442131042	Okay, learning hyperpriers hidden states on hidden states, seeing hidden states as outcomes of other hidden states so that they can be learned or fixed.
3195100	3219510	A	0.6928519606590271	Little bit of technical details on multi parameter minimization theta being just the vector of parameters for the generative model and deer Schlee distributions, which I think could be gotten into, but that's kind of distribution specific creatures select the most appropriate data to improve their generative models.
3226850	3236690	A	0.6186246871948242	I think there's more that we can dig into and explore on similar wavelengths like risk plus ambiguity, but then now there's an information gain.
3237830	3240530	A	0.6925926804542542	I thought we just had risk plus ambiguity.
3245480	3259840	A	0.6810002326965332	Then we didn't really yeah, people point to structure learning, but don't they always as a way to get around these questions.
3262150	3270580	A	0.7000338435173035	But then it's the structure of the structure learner, and then people end up with these turing machines that don't plan.
3274690	3281540	A	0.7284494042396545	People hope that Bayesian model reduction will be attractable and provide heuristics for structure learning.
3283430	3297190	A	0.598655641078949	But I haven't seen any empirical examples that come to mind where Bayesian model reduction was used to identify actionable lower dimensional useful models.
3299210	3316720	A	0.7915059328079224	But we know that structure learning on the state space of hierarchical models is going to be essential active inference or beyond, because people thought that the explosions, the computational complexity class of just branching time, active inference or anything within a model.
3317490	3322590	A	0.6744088530540466	This is going to be like exploding upon that by several exponents.
3326180	3330880	A	0.7651938796043396	And we have these fundamental questions about the continuous time interpolations.
3334820	3345700	A	0.6045120358467102	Why are there continuous interpolations in chapter seven when it's the discrete time chapter?
3346920	3351956	A	0.9795613884925842	Okay, next time we come to chapter eight, I think this will be quite interesting.
3352058	3356960	A	0.8958686590194702	We're going to talk about dynamical systems, motor control, lock of Ultera.
3357040	3367420	A	0.8142459988594055	We'll have some justified continuous lines, LaPlace assumption, Lorenz stream number 32, stochastic chaos and Markov blankets.
3369200	3377260	A	0.6925826072692871	Hybrid models with discrete continuous fusion and some advances in continuous time modeling.
3378560	3379630	A	0.7698175311088562	Thanks everybody.
3380080	3381150	A	0.5413564443588257	See you soon.
3382480	3383050	D	0.8613404035568237	Thanks everyone.

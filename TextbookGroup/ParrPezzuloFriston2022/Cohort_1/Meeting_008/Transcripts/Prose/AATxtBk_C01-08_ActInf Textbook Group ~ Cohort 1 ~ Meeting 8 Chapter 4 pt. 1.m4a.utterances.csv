start	end	speaker	confidence	text
1450	154710	A	0.9322042962962955	Hello everyone. It's June 23, 2022 and we are in week eight of the textbook group cohort one. We're starting chapter four and we'll be continuing with chapter four next week. We're more than halfway done with the time and with the chapters that we're going through in the first half of the book. So let's go to chapter four and just raise your hand and gather or write in the chat if you have anything that you want to address. So I'm going first to the math overviews page where I've written some overviews for the previous chapters of varying levels of completeness. But this is very important as we set off into chapter four. So on page 64, this chapter is more technical than chapters one through three, appealing to linear algebra, differentiation and a Taylor series expansion. Those readers interested in the details may turn to the appendices dot, dot, dot. Those who do not want to delve into the theoretical underpinnings may skip this chapter. So keep that in mind as we continue on. And it can be an area of discussion and learning and such, but let's approach these themes and formalisms with the authors forewarning that this is something we can look for more detail in the appendixes as well as skip this chapter, even if we might disagree, of course. Okay, any other overall comments on chapter four just for whomever read it, even a part of it? What was their overall perspective on chapter four? What was it trying to do? What approach did they take? Yeah, Ali and then anyone else?
157880	214090	B	0.9250305617977528	I think if we take the materials in chapters one through three as the foundational materials on which the active inference theory is supposed to build, well, this chapter four is one of the first steps toward building the actual theory and I mean going beyond just the basics and foundational materials. So using the tools and foundations established in previous chapters, we are now perhaps ready to tackle the problem of actually constructing the generative models in two different situations as discrete time models and as continuous one.
215820	586610	A	0.9241004585798829	Awesome, thanks. Yes. In the chapters page we can recall back to chapter one that just laid out the structure of the book. Chapter two provided the low road to active inference, which began with Bayesian inference and talked about a few other prerequisite or preliminary themes including introducing variational and expected free energy as imperatives in the sense that they're able to be bounding surprise. Chapter three introduced the high road to active inference, which was starting not from the mechanistic kind of nucleus of the Bayes equation, but rather from the imperative for survival and persistence. Also introducing in a first pass the Markov blanket concept in partitioning. Chapter four is indeed when we start to get into many details that were not covered in earlier sections. It's going to first begin by bringing us closer to this connection between Bayesian inference and the free energy evaluation and then the central idea of a generative model is discussed that is going to be described in discrete time, specifically using the POMDP formalism, partially observable Markov decision process. And then, as well as in continuous time, then there's going to be some very interesting figures and formalisms and discussions on what generative models underlie predictive coding and motor reflexes which is moving us towards chapter five which is going to have some empirical work mainly cited out and some discussion on the plausible Neurobiologies that can be implementing or modeled as implementing or modeled with the kinds of generative models of which have been prepared for in chapters two and three. Motivated in chapters two and three. Really? And then described in their essence in chapter four. Also just a reminder that in the math group activities but we're all in the math group, we're all in this learning journey together, we've been striving to make the natural language descriptions for equations. So that would be extremely, extremely helpful. Every annotation that people can make is helpful. People shouldn't feel abashed or ashamed to make any kind of contribution. It can always be reordered or edited by others. But this is how we're going to create those natural language descriptions of equations and ask questions about them, even just copying and pasting. What does this mean? What does this mean? They're all helpful contributions because this is very technical and it's not immediately apparent how, for example, these copious equations in chapter four relate to some of the broader discussions from the earlier chapters. But we'll get that, okay, if anyone has more questions to add, they can do that. Or if they want to upvote questions, feel free to do that. It's motivating for other people when they see that their questions are like being improved or other people are paying attention to their questions. So that's always something free and helpful that people can engage in are beliefs, policy or state. If the person who asked it is here, they can feel free to address it because it's a very short, partially formed question. I also wasn't sure whether this was using the CAD CAD ontology, which uses policy and state in a slightly different way. But beliefs in a general sense coming from the Bayesian ontology are referring to any distributional expression Bayesian beliefs. Those are states policies in the active ontology, which you can just mouse over and find out it's a sequence of actions. And policies are constructed or enumerated by the affordances, the e vector that the generative model has over a given time horizon. So if it's able to go left or right over two time steps, all the policies are left, left, right, left. Does anyone else just want to add anything else to this short question? Otherwise, it's a fine clarifying question. Okay. In the discussion of active inference in POMDP, the authors write to update beliefs about policies, we find the posterior that minimizes the free energy does a posterior at time T become a prior at time T plus one. What does anyone think?
601560	626680	C	0.9075735185185182	I guess it depends on the specific posterior and prior because that can mean multiple things. But in the context of Figure 4.3, I'd say it does because we are updating our prior, which becomes at the end of a time step, becomes the posterior probability, which then is fed back as the prior.
628080	1020600	A	0.9282437589670024	Thanks. Totally agree with that. There needs to be an initiating prior D, which is shown with a three here in the discrete time. And that is like the initial prior. And then prior and posterior are referring to what? Incoming data. So after the data at zero one comes in, the updated posterior is now playing the functional role of the prior for the next data point coming in. Okay, then here's sort of a related question. Well, we've addressed this question in the narrow sense, but it would be very helpful to look through the order in which different topics are addressed in chapter four because these are some questions that are invoking things that are much later. And we don't assume that people have high or low comprehension of this chapter yet. But many of these questions, it's really important that we understand why we're bringing them up. So let's just walk through very quickly the chapter to see why they're bringing up things in a different order in terms of looking at the figures. And formalism. 4.1 is returning to Bayes theorem. And I think this is a really nice representation here's the joint distribution of x and Y, the hidden states and the observables. And it's like it gets split up into Y condition x and X. So this is like the probability, the joint distribution of the coin flip and the die is the distribution of the coin flip conditioned upon the die and the die itself. So it's just separating out a joint distribution. And that's the heart of Bayes theorem. This is a key move to move an integration problem, a sum in the discrete case or an integration problem in the continuous case into an optimization problem which permits incremental solutions of improving quality. Unlike an integration problem, which you might just be tallying up numbers and not necessarily moving closer to knowing when you're done with that integral. They introduced Jensen's inequality, which is the log of the average is always greater or equal to the average of a log. And that's true for any shape that has this kind of a curve, anything where there's a decreasing curve. And here's where that gets applied above we had this taking this joint distribution, multiplying it by an arbitrary distribution divided by itself so that's one, whatever Q is, later Q will play a more specific role. And then here the expectation is inside the log. Here the expectation is over the log itself. So the one that we'd really want would be like the joint distribution would be like the expectation of the joint distribution divided by some function. Q Jensen's inequality allows us to make this instead of an equal sign, make a greater than an equal sign, and then pull the expectation out and utilize that as the negative free energy which is going to be bounding us on this one that we would truly want. But going about it by using this nice feature of the natural log base theorem in the logarithmic form and anyone can totally raise their hand and add details allows rearrangement into this form which is already recalling the free energy that we saw from variational, free energy from from equation 2.5 more details to be filled in and that's why we want to be annotating these equations and so on. Generative Models is the name of this chapter. To calculate free energy we need three things data variational, distribution, family, and a generative model which at minimum is composing of the prior and the likelihood they're going to be describing two different kinds of generative models. This is a really nice and intuitive graphic for those who might have different familiarity with statistical representations. The circles are random variables and the squares are probability distributions describing relationships. These are some Bayes graphs that are simpler and less action involved than this canonical active representation where Pi is policy. And so yeah, yakub, you wrote three is the B matrix. Yes, it is. These threes are the B matrix, but this is the D letter. So I agree though.
1022590	1024742	C	0.8916957142857144	Sorry, which one is the D letter?
1024886	1025930	B	1.0	The first three.
1026080	1033660	A	0.9616011111111111	Yeah, because they're playing similar roles but they're a little bit or. What do you think about that?
1034370	1044910	C	0.9200714999999999	Yeah, I guess I was thinking about it as this being just one snapshot of like an infinite factor graph.
1047110	1047474	A	0.9996	Just.
1047512	1071480	C	0.9336726829268293	The fact that it's assigned the P of S tau plus one given S tau and pi. I understand what you mean by saying that. It's kind of like it plays the role of the yeah, I'm not entirely sure though.
1073950	1315030	A	0.9285446174142487	We'Ll come back to this because there's actually a few other interesting notes. So this is kind of the simplest of the graphs that they describe. This is like one variable influencing another variable. This is like two factors, z and X influencing Y. So two is like a little factory that outputs Y conditioned upon the state of X and Z. So knowing this notation and how to read these directed graphs is relatively essential for interpreting more complex graphs that involve actions and so on. Here's X as like an upstream hidden cause that influences two downstream variables that don't influence each other. And here is like a hierarchical model where V influences X and X influences Y. Does anyone have like thoughts or questions about this? This is a graphical probabilistic model. The variables are stochastic or they're statistical random variables. And it's a graph, it's consisting of nodes and edges. 4.3 is going to introduce these two basic forms of the generative model dynamic through time used in activ in the factor graph form. Does anyone want to describe what they see in this figure? The top is in the discrete time case. S is the hidden state temperature of the room. O are the observations, the thermometer readings. The two is the relationship between the temperature in the room and the readings of the thermometer three is how the temperature is changing through time. And then it is just like Yakub said, it's kind of like here. We can think of this as being like a little bit of a snippet from this infinite sequence of temperatures through time. But in practice there has to be an initiating prior pi are policies, policies are sequences of actions, sequences of affordances that are concatenated over some time horizon. That's what we're evaluating. Like expected free energy on and policies represent causal impact in the world. And we can look at this graph, which is why this is an important prerequisite to understand. Because the way in which policies influence the world isn't like by taking the temperature and just changing it to a different value, it's changing three, which is how the temperature changes through time. Any thoughts on the discrete time formalism? Yes Mike.
1319700	1325330	D	0.8689650000000002	So as represented in this figure, the policy is not changing over time, is that correct?
1327220	1347028	A	0.8943148148148147	The set of policies in this figure is not changing. It's not that it can't. This is just only showing two steps of some policy being applied.
1347124	1363550	D	0.91996175	Yeah, just the way it is in this figure. So if we were to maybe extend this figure with two more of the bottom section, we could also extend policy and have that changing over time and feeding into that.
1365600	1805800	A	0.9260520527522949	Yes, and that's actually one of the amazing and interesting things about this graphical representation is we can say, okay, well let's carry out this S to five more time steps and then let's have pi one implemented here, and then let's have it do pi two here. And it's kind of like if you can draw it, you can do it. And that's very important. Work from DeVries and Friston and Par from around five years ago was demonstrating that if the Bayes graph can be drawn, that there's a 40 factor graph representation and where there's the 40 factor graph representation, there's attractable variational, message passing, approximation. I don't know exactly what the guide rails are for, like are there graphs that can't be amenable to message passing and et cetera, but just at a first pass for graphs that look like this, they can be drawn and more variables can be added and so on. So it's kind of like a visual code for probabilistic graphical models. So maybe that's a fun exercise is like to think about causal inference in our day. Like the temperature in my room is being influenced by whether the air conditioner is running in the other room and the temperature outside. Which one of these scenarios does it model onto? And just kind of taking scenarios that are familiar to us and then separating them in terms of how are the hidden states changing through time, what are the observations? What policies influence how hidden states change their time? But this is going to be an interesting different view on the bottom here with the continuous time model. Does anyone want to describe the continuous time variance? Also one interesting piece here is in the live streams that we just did over the last few weeks 46. They make a clear distinguishing between Dai decision active inference and Mai motor active inference. And beyond just modeling cognitive decision making versus motor reflex arcs, they also highlight how the Dai is a discrete time model that's using the POMDP, while the motor models tended to use continuous time. So they're laying out and also kind of generalizing to show the similarities between these two different variants. And we can use the notation concordance table to highlight some of those parallels. This is where the Taylor series approximation comes in and the generalized coordinates of motion, though they can also be applied to the discrete time. Here we have x, x prime and x double prime. The prime notation indicates temporal derivatives and second derivatives. If anyone has a thought on that, feel free because this is not doing time series prediction in the same way that the POMDP is. The POMDP is in the memory of the program. There's a value at the previous state, the current state and the next state and however many other states in the time horizon. The way that a continuous time Taylor series approximation is dealing with reduction of uncertainty about future data is quite different than the way that these discrete time models are doing. So what's happening here is the value of the function now is being estimated or provided, then the first derivative is calculated. So here three has the same structure. But whereas this is the hidden state at the next time point conditioned upon the hidden state at this time point. And the policy here's the derivative of x conditioned upon x and the policy V also has a slightly different interpretation because it's not sequence of events either. And analogously for the second derivative and the higher derivatives. So they're still emitting observations, but these are not observations at future time points in the same way. And that is visualized in this box 4.2. So here is x of t at time t. So this is like the way that the time series is going to go. X of t is just this value. Here five, the first term being added in the Taylor series expansion is the first derivative rate of change at that x zero. That's giving you a better approximation through time. Then the second derivative adds this quadratic feature. And so Taylor series converge closer and closer, moving further and further away from the target point as they include higher and higher derivatives. But there isn't an explicit calculation in the Taylor series of like, let's just say this is like one, two, three timesteps. It's not like, well, what is going to happen at three timesteps from now? The Taylor series could then be plugged in with three to calculate that. However, this is not calculating x x prime x double prime at t equals three, it's calculating it for x sub zero and then using this expansion to achieve reduction of uncertainty of more and more distal points. So that's quite interesting. And again, it's an extremely different way of doing time series prediction in the continuous time framework than this kind of explicit consideration of past, present and future state values. Okay, any other thoughts on 4.3? Because this is one of the most key figures, but the things we can explore and ask Eli.
1808560	1837140	B	0.9382085185185185	About this figure on page 69 it says the relationship between a state and its temporal derivative here depends on slowly varying causes. New, I just wanted to make sure, does this mean this slowly varying causes here is used to account for the fluctuations or it has some different meaning, slowly varying causes.
1843240	1880960	A	0.8726091111111111	So policies are influencing the causal unfolding of states in this discrete formalization here in the continuous time setting, the slowly varying causes more slowly than these changes play a similar role to policies above. So these causes intervene in how the derivatives are calculated?
1887670	1934990	B	0.8871782894736844	No, I mean here in the discrete time. Well, obviously we have, we can say stable policies that doesn't, that don't change through time, but in the continuous time. Well, I don't know, at least that was my understanding that the policies can slightly change but the change can be somehow negligible. And well, that's because of the additional term for fluctuations in the equations and they don't necessarily affect the main components of the equations.
1938630	2010470	A	0.9211477037037038	One could set it up so that the policies are ineffectual, so that the state changes through time are with their own endogenous dynamics, not influenced by policy. Or one could imagine a situation where the states don't change at all through time and their changes are entirely driven by policy. And I think analogously there could be a setting in which the derivative of x is hardly influenced by these v slowly varying causes or V might entirely describe the derivatives of x. But I'm not sure if just at this level of generality we can allocate importance to kind of the endogenous dynamics or like sort of the policy independent changes just the way that those states are changing or analogously derivatives are calculated. But that's a great question Lyle. And then Mike?
2011790	2099970	E	0.9314163316582919	Yeah, this might be just slightly off topic, but the tools, if you're using a multimode simulation tool, then it can handle both in some sense it's going to try and solve both continuous formulations and discrete time formulations. So one they would typically call agent based and the other would be Ode based or something like that. The way that they do that is a little bit of sleight of hand because of course they're not solving the equations per se, they're estimating the solutions. And if you have that, then if you have a slowly changing set of parameters like you might model with the discrete case, then you've got an event queue and those events are however far apart they need to be. Then to estimate the continuous form, they simply set that delta t delt. Delta T goes to zero in some sense. It never goes in the software exactly to zero. But then you compress that delta t down to very small time slices and then you have estimated your continuous form. And so by doing that, you can really have a pretty comprehensive way to represent both cases in the same modeling environment.
2101130	2129550	A	0.947537115384615	Yes, great point. Numerical approximations to continuous processes can be done through breaking them down into discrete processes and then making sure that as your delta T is getting smaller and smaller, that you're getting a convergent estimator. The approach that's taken analytically here is to use a Taylor series approximation, Mike.
2131250	2154950	D	0.8943799999999998	And so in thinking about how we should build a mental model of the Taylor series approximation or incorporate that in our model, that's one example of how you might capture the time series structure. And you could potentially substitute in other approaches to maybe capture finer detail or discrete events that might occur on the series.
2158010	2159000	A	0.892215	Like what?
2161210	2172250	D	0.8533709090909092	Like Loaz decomposition or something like that, where you're taking apart components of the time series. Things like trend seasonality discrete events.
2173630	2196370	A	0.8537971428571428	Yes, I think the analytical comparisons would be tight or loose, but Coleman Filter, Generalized, Bayesian filtering, Splines, time series decomposition, these are all in the category or like in the genre of this. Ollie.
2199690	2235120	B	0.9207196226415095	By the way, Lionel, thanks for the clarification, that was really helpful. Well, to ask my question more explicitly, on page 78, equation 4.15, we have some additional omega terms which are defined as stochastic fluctuations. My question was that is the term slightly varying somehow related to these stochastic fluctuations or not?
2236870	2546350	A	0.9266489902912632	Yes, I believe that the slowly varying is in relationship to it being included with the flow component rather than with the at that time scale stochastic more thermal like vibration. And that was explored in like live stream 45 with free energy principle made simpler but not too simple with this idea of the length of N and how physics and mechanics are predicated upon the separation into flow like actions at a given scale. And then stochastic non flow aligned changes. And the path of least action is when the limit of the stochastic term going to zero. But let's come there as people can see, even though it's like we could totally read this like 20 times and it still is like why is the next word there? Why is the next equation there? But okay, they're introducing these two types of generative models that have kind of tantalizing isomorphisms but also very interesting differences. They're going to first focus on the discrete time, the partially observable Markov decision process formalism is used. Someone asked like why is the categorical notation used? It just allows it makes it easy to look at a matrix and to interpret the matrix as like a confusion matrix, not just a matrix that is confusing, like they can be, but one that has to do with like a coin flip. So categorical outcomes. The three nodes are the transition function here in the categorical context between different states that's this B. And here's where we see the D, the prior over the initial state and B. And together these account for the three nodes in Figure 4.3. They play a functionally similar role here's, where they get distinguished selecting between models of behavior. And that was another question which maybe we could get to requires selection amongst these categorically discrete plans and then the soft max normalizes and ensures that the probability over those policies is normalized so that they can be understood as a probability distribution, like a categorical probability distribution. Here's the expected free energy that we've seen before. More on expected free energy, specifically on how this KL divergence term, which as we talked about last week was about how the preferred states are realized. Active inference uses F and G. They're related, but they play different roles. VFE F is the primary quantity minimized over time. That was interesting to read because EFE is what is minimized prospectively through time. Whereas here the claim is that variational free energy is what is minimized over time in relationship to generative model. So again, more could be explored there. They are going to focus on a rearrangement of equation 2.6 here with the ambiguity and risk being juxtaposed with the informational value, information gain, infomax and Pragmatic value. They then move into the linear algebraic form where the bolds okay, I don't know if bold is used in the same way in this appendix as it is in these equations. Does anyone know about what bold means here? It could mean the vector Ali.
2549170	2555150	B	0.851158181818182	Yes, I also think bold most probably means vector or matrix.
2556150	2599720	A	0.9542759420289852	Yes, it gets pretty subtle though sometimes because there's times where there's italics bold and neutral being used very closely. When they're used closely, it can be confusing and then when they're used not closely, it's hard to juxtapose. More expected free energy rewriting within the linear algebraic form. Softmax bringing back the logs. Variational inference rests upon factorization that's related to the sparsity of the Bayes graph. Moritz.
2602220	2631830	F	0.9031349295774648	Hey, so just going to 410 again, I was wondering what's the use of the categorical distribution here because maybe I miss it, but it's not really explained in the text what they use it for. There's one sentence saying like oh yeah. The fifth line shows that the prior belief about observations is a categorical distribution. Like, what is it for? What does it do? I've never seen it before.
2638940	2699244	A	0.9185877659574465	I don't know if it has to be categorical, a preference distribution could be a continuous function. But here it's just simpler perhaps to show it as a categorical. Like, there's two outcomes having the food and not having the food. So then that's a categorical distribution of preferences and of observations. So they're just modeling a situation where there's categorical differences that are being preferred as opposed to, like, a preference over some continuous distribution. Does that address it? They're just modeling a situation where there's a categorical difference with observations and with.
2699282	2705630	F	0.9661090909090909	Preferences just because mathematically it's easier now to start with that.
2706640	2750630	A	0.9421840624999996	Yeah, I think didactically it's a lot clearer because you can see like, a two by two matrix. Did I observe the food or not? Did I get the food or not? Instead of like a distribution of temperature preference and a distribution of observations. I'd expect that the formalism will work out the same in the sense that you're still minimizing your surprisal and you're still performing distribution matching. But also this is like a distribution matching in the categorical context that has an interpretation of almost like false positive and false negative. All right.
2750700	2753618	F	0.9660881818181818	Okay. I will go with it. Thank you for the explanation.
2753794	3406090	A	0.9236730303030315	Yeah, it's a good question. The matrices help connect to the linear algebra and the MATLAB representations. And also just to sort of match or no match, like in 46, the example had to do with wanting ice cream and then they observed ice cream or not. Okay, POMDP. And also, where does this ita? We'll come back to that another time, but just to kind of get through it all. The first pass more details on a POMDP, S, and V auxiliary variable. I don't think this is the V here because we're in the POMDP setting. So it's just an auxiliary variable used as sort of an analytical convenience. That concludes their discussion of the discrete time model into continuous time. But first they describe again Markov blankets and take kind of a second pass. The blankets are the causes of X upstream, which are parents and the children and the parents of the children. The kind of co influencers. Of course, there's more to say, but that's what the Pearl definition is, how that gets mapped to sense and action states and what influences what's in the blanket, on the blanket, et cetera. Those are model specific. And then here are two common message passing schemes that are used for approximate inference. Unless the Bayes graph is fully connected. There is a Markov partitioning here. This is pretty funny. NB notepen a good note. Good note to note the slightly nonstandard use of the expectation operator, perhaps by overloading it with a massive subscript. But it's unclear what exactly they meant. There is showing alignment between these two different schemes, but there's definitely citations to go into, more into. Detail on like belief propagation and message passing inactive. And then we see several of these figures. Personally, I think it's slightly challenging to understand whether this is being used illustratively or whether these specific topologies are as directly interpretable as these topologies are. But here we see kind of if you blur your eyes here's, policy at the top. Here's the observations. Here's time minus one. Here's s minus t minus one, t and T plus one. So we can see some resonances with 4.3. But now we're in the continuous time motor. Active inference from 46 is justifying the use of continuous time based upon the continuous unrolling aspect of sensory input and motor output. They're going to start in a different place than they did with the POMDP model. In this case, we have much more of a physics grounding. Again, check out live stream number 45 to see about the lengthen and how this is connected. But this is a much more physics based grounding in terms of like a flow operator and a stochastic term. At a given timescale, the stochastic term is assumed to have a normal distribution. And that kind of relates to what Mike said about detrending. One can think of this stochastic term as being what happens when you've detrended all of the signal that's non Gaussian out. You're left with the residuals which have a gaussian nature. Precision is capital pi and that's the inverse of the fluctuation. And so that's they're going to connect that to the common filtering. Here we return again to the generalized coordinates of motion coming from this Taylor Series approximation route. So if somebody said I have ten numbers and we need to predict ten years in the future, the discrete time way would be like, well, let's try to predict it at each of those ten years and those will be your ten numbers. The generalized coordinates of motion's approach which we explored the most in live stream number 26 with the Costa et al. In Bayesian mechanics, the ten numbers could be like the value today and the derivative today, the second derivative and the third and the fourth and the fifth up to however many that's the generalized coordinates of motion. So it's like a snapshot of the process and all of its higher derivatives. And so the generalized coordinates of motion are very similar to a Taylor Series approximations in how they represent centered at a certain point x naught how one expects as movement happens away from that x knot, reducing surprise. And so here is like x with a dot on top is the change in x. And then this is like the derivative of the change in x and so on. Here the Tilde notation is used for the generalized coordinates of motion. So we see this equation which was just for one value, like where you are on the freeway. And then this is the generalized coordinates of motion. The free energy is written down for this generalized coordinates of motion with precision. Here. The closest that we came to exploring it was in 43 on predictive coding. There are some more details on Gaussian, the relationship between like, Gaussian processes and also, I think Lyle brought up this sort of mode and multimode simulations. This is modeling a single mode. Not that it can't model a bimodal distribution, but this is where the LaPlace approximation comes into play. This is potentially even unnecessarily complex, but it's there. And the LaPlace approximation is fitting a quadratic distribution that tracks the mode. So like the highest point on the distribution. And then it fits a parabola around that. Here is more details on the hierarchical nature and the kind of multi timescale nature that's implied ultimately by the lengthen. And then there's it'd be good to juxtapose figure four, six, and four to see how they're similar and different. But here we see message passing happening on the generalized predictive coding architecture. And that was like, explored a lot more, not with this exact figure, but this was explored more from like an analytical and empirical perspective in 43. Then there's an abrupt ending, but the following chapters are going to appeal to the formalisms and apply them. So, any thoughts or ideas on chapter four as we close out, and hopefully next week, we can have a lot of questions and things like that, even basic questions. It's like if you read it and you understood it, then it'd be awesome to contribute a question that would explore someone else's understanding or prompt towards how you thought about it in a way that made sense for you. And if you don't understand it, then just ask the question Ali and then anyone else?
3408460	3470860	B	0.9422441592920352	I just wanted to briefly mention an additional point related to the question about the reason behind using the categorical distribution. On page 74, it says the fifth line shows that the prior belief about the observations is a categorical distribution whose sufficient statistics are given in the C vector. I think here, sufficient statistics is the key term to understand the reason behind using categorical distributions, because for insufficient statistics, we don't necessarily use the exact modeling or distributions. Instead, we use a substituted statistics that is simple enough to calculate and also close enough to the actual data. So I think that can help in understanding the reason behind that decision.
3475010	3488690	A	0.9623460714285714	Good point, and this is definitely a technical note, but is it fair to say that the mean and the variance are sufficient statistics for a Gaussian distribution?
3491610	3493240	B	0.7791075000000001	Yes, I think so.
3496170	3642506	A	0.9255807172995774	That's one of the perhaps proximate mechanisms by which the LaPlace approximation and variational inference more generally are able to deal with arbitrary true distributions by fitting a family of distributions that have tractable optimization structure and a vastly reduced set of sufficient statistics. If you had even just a simple bimodal distribution, well, there'd be like the location of the two peaks, the relative heights of the two peaks, the skewedness and all the you could have many, many parameters needed to describe it. In contrast, the LaPlace approximation only requires the location of the mode and the variance estimate. And so where that's adequate, it's simple and fast. Where it's inadequate, you'll at least know because you'll continue to be surprised at new observations coming in. Okay, any final comments on this? Chapter four, part one. Definitely a challenging, though interesting chapter, and it really is at the heart of active. And as we hopefully explored a little bit today, it includes, like, basic Intuition pumps, as well as some Rosetta Stone like representations and many, many equations which I hope we can unpack. So could someone explain it in simple words? It's what we've been asking for every single equation, and it's something that everyone can contribute to. All right, well, looking forward to people's edits over the coming week and additions, and we'll come back in one week for part two on four.

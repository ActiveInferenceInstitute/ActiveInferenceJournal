start	end	speaker	sentiment	confidence	text
1450	2302	A	0.5579421520233154	Hello everyone.
2436	11920	A	0.9079873561859131	It's June 23, 2022 and we are in week eight of the textbook group cohort one.
12690	19120	A	0.7310384511947632	We're starting chapter four and we'll be continuing with chapter four next week.
20370	29480	A	0.6327273845672607	We're more than halfway done with the time and with the chapters that we're going through in the first half of the book.
30490	38220	A	0.8233101963996887	So let's go to chapter four and just raise your hand and gather or write in the chat if you have anything that you want to address.
39550	50618	A	0.9119395017623901	So I'm going first to the math overviews page where I've written some overviews for the previous chapters of varying levels of completeness.
50714	56160	A	0.7286850214004517	But this is very important as we set off into chapter four.
56530	66530	A	0.68831467628479	So on page 64, this chapter is more technical than chapters one through three, appealing to linear algebra, differentiation and a Taylor series expansion.
68310	72610	A	0.7827250361442566	Those readers interested in the details may turn to the appendices dot, dot, dot.
72950	77670	A	0.644982635974884	Those who do not want to delve into the theoretical underpinnings may skip this chapter.
79130	82840	A	0.80678790807724	So keep that in mind as we continue on.
83690	116260	A	0.7723140716552734	And it can be an area of discussion and learning and such, but let's approach these themes and formalisms with the authors forewarning that this is something we can look for more detail in the appendixes as well as skip this chapter, even if we might disagree, of course.
117190	126230	A	0.9200322031974792	Okay, any other overall comments on chapter four just for whomever read it, even a part of it?
126300	130294	A	0.9294053316116333	What was their overall perspective on chapter four?
130412	132120	A	0.8205086588859558	What was it trying to do?
132890	134760	A	0.8823460936546326	What approach did they take?
152680	154710	A	0.8361570835113525	Yeah, Ali and then anyone else?
157880	187548	B	0.6782646179199219	I think if we take the materials in chapters one through three as the foundational materials on which the active inference theory is supposed to build, well, this chapter four is one of the first steps toward building the actual theory and I mean going beyond just the basics and foundational materials.
187644	214090	B	0.6638693809509277	So using the tools and foundations established in previous chapters, we are now perhaps ready to tackle the problem of actually constructing the generative models in two different situations as discrete time models and as continuous one.
215820	217336	A	0.9686442017555237	Awesome, thanks.
217518	218248	A	0.46103888750076294	Yes.
218414	230030	A	0.9043483734130859	In the chapters page we can recall back to chapter one that just laid out the structure of the book.
230480	255220	A	0.8395524024963379	Chapter two provided the low road to active inference, which began with Bayesian inference and talked about a few other prerequisite or preliminary themes including introducing variational and expected free energy as imperatives in the sense that they're able to be bounding surprise.
256040	270360	A	0.8125708699226379	Chapter three introduced the high road to active inference, which was starting not from the mechanistic kind of nucleus of the Bayes equation, but rather from the imperative for survival and persistence.
270700	277340	A	0.922258198261261	Also introducing in a first pass the Markov blanket concept in partitioning.
281040	290560	A	0.7584545612335205	Chapter four is indeed when we start to get into many details that were not covered in earlier sections.
291140	318570	A	0.7687017321586609	It's going to first begin by bringing us closer to this connection between Bayesian inference and the free energy evaluation and then the central idea of a generative model is discussed that is going to be described in discrete time, specifically using the POMDP formalism, partially observable Markov decision process.
319100	359156	A	0.8054494857788086	And then, as well as in continuous time, then there's going to be some very interesting figures and formalisms and discussions on what generative models underlie predictive coding and motor reflexes which is moving us towards chapter five which is going to have some empirical work mainly cited out and some discussion on the plausible Neurobiologies that can be implementing or modeled as implementing or modeled with the kinds of generative models of which have been prepared for in chapters two and three.
359258	360904	A	0.6763067841529846	Motivated in chapters two and three.
360942	361528	A	0.6008133888244629	Really?
361694	365690	A	0.9153581261634827	And then described in their essence in chapter four.
368460	383100	A	0.5737828612327576	Also just a reminder that in the math group activities but we're all in the math group, we're all in this learning journey together, we've been striving to make the natural language descriptions for equations.
383600	387868	A	0.936782717704773	So that would be extremely, extremely helpful.
388044	391404	A	0.8986237645149231	Every annotation that people can make is helpful.
391532	395644	A	0.5268390774726868	People shouldn't feel abashed or ashamed to make any kind of contribution.
395772	400500	A	0.8344694972038269	It can always be reordered or edited by others.
400570	410644	A	0.7925505638122559	But this is how we're going to create those natural language descriptions of equations and ask questions about them, even just copying and pasting.
410692	411464	A	0.7871558666229248	What does this mean?
411502	412248	A	0.7871558666229248	What does this mean?
412334	430700	A	0.6966584920883179	They're all helpful contributions because this is very technical and it's not immediately apparent how, for example, these copious equations in chapter four relate to some of the broader discussions from the earlier chapters.
431600	443856	A	0.7552199363708496	But we'll get that, okay, if anyone has more questions to add, they can do that.
443878	447730	A	0.8507617712020874	Or if they want to upvote questions, feel free to do that.
448180	454964	A	0.8101547360420227	It's motivating for other people when they see that their questions are like being improved or other people are paying attention to their questions.
455162	466490	A	0.8212918043136597	So that's always something free and helpful that people can engage in are beliefs, policy or state.
467820	479550	A	0.7414243221282959	If the person who asked it is here, they can feel free to address it because it's a very short, partially formed question.
480880	488690	A	0.659157931804657	I also wasn't sure whether this was using the CAD CAD ontology, which uses policy and state in a slightly different way.
489460	502020	A	0.8947939872741699	But beliefs in a general sense coming from the Bayesian ontology are referring to any distributional expression Bayesian beliefs.
502760	516120	A	0.8586407899856567	Those are states policies in the active ontology, which you can just mouse over and find out it's a sequence of actions.
517180	531272	A	0.8678581118583679	And policies are constructed or enumerated by the affordances, the e vector that the generative model has over a given time horizon.
531416	539250	A	0.87086021900177	So if it's able to go left or right over two time steps, all the policies are left, left, right, left.
542900	546770	A	0.7945581674575806	Does anyone else just want to add anything else to this short question?
551680	554430	A	0.648137092590332	Otherwise, it's a fine clarifying question.
555920	556670	A	0.584351658821106	Okay.
565200	583410	A	0.8768475651741028	In the discussion of active inference in POMDP, the authors write to update beliefs about policies, we find the posterior that minimizes the free energy does a posterior at time T become a prior at time T plus one.
585060	586610	A	0.8254761099815369	What does anyone think?
601560	610260	C	0.8460147976875305	I guess it depends on the specific posterior and prior because that can mean multiple things.
610330	626680	C	0.8802765607833862	But in the context of Figure 4.3, I'd say it does because we are updating our prior, which becomes at the end of a time step, becomes the posterior probability, which then is fed back as the prior.
628080	628684	A	0.6283750534057617	Thanks.
628802	630430	A	0.6458945870399475	Totally agree with that.
630800	638510	A	0.8960086107254028	There needs to be an initiating prior D, which is shown with a three here in the discrete time.
638820	641840	A	0.8079177141189575	And that is like the initial prior.
642340	646784	A	0.742036759853363	And then prior and posterior are referring to what?
646902	648370	A	0.8468883633613586	Incoming data.
648900	660870	A	0.8978700637817383	So after the data at zero one comes in, the updated posterior is now playing the functional role of the prior for the next data point coming in.
666900	672310	A	0.841888427734375	Okay, then here's sort of a related question.
672840	695870	A	0.5735619068145752	Well, we've addressed this question in the narrow sense, but it would be very helpful to look through the order in which different topics are addressed in chapter four because these are some questions that are invoking things that are much later.
696480	704364	A	0.5738163590431213	And we don't assume that people have high or low comprehension of this chapter yet.
704562	712064	A	0.7392266392707825	But many of these questions, it's really important that we understand why we're bringing them up.
712182	722528	A	0.8460757732391357	So let's just walk through very quickly the chapter to see why they're bringing up things in a different order in terms of looking at the figures.
722544	723540	A	0.5459345579147339	And formalism.
725000	729380	A	0.8712610006332397	4.1 is returning to Bayes theorem.
729900	738840	A	0.9553430080413818	And I think this is a really nice representation here's the joint distribution of x and Y, the hidden states and the observables.
739340	747870	A	0.6601951122283936	And it's like it gets split up into Y condition x and X.
748320	761696	A	0.8604376316070557	So this is like the probability, the joint distribution of the coin flip and the die is the distribution of the coin flip conditioned upon the die and the die itself.
761878	764720	A	0.8396626710891724	So it's just separating out a joint distribution.
765060	767600	A	0.8226656317710876	And that's the heart of Bayes theorem.
770520	785690	A	0.7287569046020508	This is a key move to move an integration problem, a sum in the discrete case or an integration problem in the continuous case into an optimization problem which permits incremental solutions of improving quality.
786140	797020	A	0.6386404037475586	Unlike an integration problem, which you might just be tallying up numbers and not necessarily moving closer to knowing when you're done with that integral.
799040	807760	A	0.795653223991394	They introduced Jensen's inequality, which is the log of the average is always greater or equal to the average of a log.
807910	816880	A	0.8343546986579895	And that's true for any shape that has this kind of a curve, anything where there's a decreasing curve.
818200	839880	A	0.8880144357681274	And here's where that gets applied above we had this taking this joint distribution, multiplying it by an arbitrary distribution divided by itself so that's one, whatever Q is, later Q will play a more specific role.
840540	845404	A	0.6976748108863831	And then here the expectation is inside the log.
845602	849070	A	0.6606258153915405	Here the expectation is over the log itself.
850480	866720	A	0.8671941161155701	So the one that we'd really want would be like the joint distribution would be like the expectation of the joint distribution divided by some function.
866790	893950	A	0.7139863967895508	Q Jensen's inequality allows us to make this instead of an equal sign, make a greater than an equal sign, and then pull the expectation out and utilize that as the negative free energy which is going to be bounding us on this one that we would truly want.
894560	941280	A	0.6437132358551025	But going about it by using this nice feature of the natural log base theorem in the logarithmic form and anyone can totally raise their hand and add details allows rearrangement into this form which is already recalling the free energy that we saw from variational, free energy from from equation 2.5 more details to be filled in and that's why we want to be annotating these equations and so on.
944610	947410	A	0.8681541681289673	Generative Models is the name of this chapter.
948310	962310	A	0.8656253218650818	To calculate free energy we need three things data variational, distribution, family, and a generative model which at minimum is composing of the prior and the likelihood they're going to be describing two different kinds of generative models.
963290	972970	A	0.9572955965995789	This is a really nice and intuitive graphic for those who might have different familiarity with statistical representations.
976270	982970	A	0.8397186398506165	The circles are random variables and the squares are probability distributions describing relationships.
984610	1001074	A	0.6282323002815247	These are some Bayes graphs that are simpler and less action involved than this canonical active representation where Pi is policy.
1001272	1005918	A	0.7892757654190063	And so yeah, yakub, you wrote three is the B matrix.
1006094	1007780	A	0.5537595152854919	Yes, it is.
1009370	1015990	A	0.8482333421707153	These threes are the B matrix, but this is the D letter.
1017930	1020600	A	0.6242495179176331	So I agree though.
1022590	1024742	C	0.707956850528717	Sorry, which one is the D letter?
1024886	1025930	B	0.720083475112915	The first three.
1026080	1032314	A	0.7895267009735107	Yeah, because they're playing similar roles but they're a little bit or.
1032352	1033660	A	0.8296127915382385	What do you think about that?
1034370	1044910	C	0.8134232759475708	Yeah, I guess I was thinking about it as this being just one snapshot of like an infinite factor graph.
1047110	1047474	A	0.47801169753074646	Just.
1047512	1055250	C	0.885347843170166	The fact that it's assigned the P of S tau plus one given S tau and pi.
1057910	1061494	C	0.7756518721580505	I understand what you mean by saying that.
1061532	1071480	C	0.8397897481918335	It's kind of like it plays the role of the yeah, I'm not entirely sure though.
1073950	1077450	A	0.8596270680427551	We'Ll come back to this because there's actually a few other interesting notes.
1078750	1087310	A	0.7848500609397888	So this is kind of the simplest of the graphs that they describe.
1089090	1092270	A	0.7571389079093933	This is like one variable influencing another variable.
1094050	1099140	A	0.8753066658973694	This is like two factors, z and X influencing Y.
1100230	1110840	A	0.8082613945007324	So two is like a little factory that outputs Y conditioned upon the state of X and Z.
1112090	1128220	A	0.7471929788589478	So knowing this notation and how to read these directed graphs is relatively essential for interpreting more complex graphs that involve actions and so on.
1129870	1139120	A	0.7339048981666565	Here's X as like an upstream hidden cause that influences two downstream variables that don't influence each other.
1139810	1147460	A	0.8394789099693298	And here is like a hierarchical model where V influences X and X influences Y.
1148150	1151140	A	0.8577839136123657	Does anyone have like thoughts or questions about this?
1161620	1164016	A	0.801468014717102	This is a graphical probabilistic model.
1164118	1169360	A	0.7520620822906494	The variables are stochastic or they're statistical random variables.
1169520	1173220	A	0.8418218493461609	And it's a graph, it's consisting of nodes and edges.
1178580	1190150	A	0.90608811378479	4.3 is going to introduce these two basic forms of the generative model dynamic through time used in activ in the factor graph form.
1194140	1198010	A	0.8216385245323181	Does anyone want to describe what they see in this figure?
1208870	1212280	A	0.7959870100021362	The top is in the discrete time case.
1213130	1217800	A	0.8102774024009705	S is the hidden state temperature of the room.
1218330	1222230	A	0.814873993396759	O are the observations, the thermometer readings.
1223150	1237580	A	0.880630612373352	The two is the relationship between the temperature in the room and the readings of the thermometer three is how the temperature is changing through time.
1239230	1242542	A	0.7227340340614319	And then it is just like Yakub said, it's kind of like here.
1242596	1250340	A	0.8378630876541138	We can think of this as being like a little bit of a snippet from this infinite sequence of temperatures through time.
1251190	1270082	A	0.871929407119751	But in practice there has to be an initiating prior pi are policies, policies are sequences of actions, sequences of affordances that are concatenated over some time horizon.
1270226	1271650	A	0.8264997005462646	That's what we're evaluating.
1271730	1278940	A	0.7896271347999573	Like expected free energy on and policies represent causal impact in the world.
1279630	1285600	A	0.5060592293739319	And we can look at this graph, which is why this is an important prerequisite to understand.
1286130	1300180	A	0.7734360694885254	Because the way in which policies influence the world isn't like by taking the temperature and just changing it to a different value, it's changing three, which is how the temperature changes through time.
1305190	1307970	A	0.8794536590576172	Any thoughts on the discrete time formalism?
1313850	1315030	A	0.5244571566581726	Yes Mike.
1319700	1325330	D	0.7642541527748108	So as represented in this figure, the policy is not changing over time, is that correct?
1327220	1335140	A	0.7363665103912354	The set of policies in this figure is not changing.
1339640	1341152	A	0.713499903678894	It's not that it can't.
1341216	1347028	A	0.7643724083900452	This is just only showing two steps of some policy being applied.
1347124	1349144	D	0.7740265727043152	Yeah, just the way it is in this figure.
1349262	1363550	D	0.8650227189064026	So if we were to maybe extend this figure with two more of the bottom section, we could also extend policy and have that changing over time and feeding into that.
1365600	1384628	A	0.9494030475616455	Yes, and that's actually one of the amazing and interesting things about this graphical representation is we can say, okay, well let's carry out this S to five more time steps and then let's have pi one implemented here, and then let's have it do pi two here.
1384794	1388070	A	0.49295005202293396	And it's kind of like if you can draw it, you can do it.
1388600	1390788	A	0.7819547653198242	And that's very important.
1390874	1415660	A	0.7325879335403442	Work from DeVries and Friston and Par from around five years ago was demonstrating that if the Bayes graph can be drawn, that there's a 40 factor graph representation and where there's the 40 factor graph representation, there's attractable variational, message passing, approximation.
1418220	1439968	A	0.6098672151565552	I don't know exactly what the guide rails are for, like are there graphs that can't be amenable to message passing and et cetera, but just at a first pass for graphs that look like this, they can be drawn and more variables can be added and so on.
1440054	1446260	A	0.8633526563644409	So it's kind of like a visual code for probabilistic graphical models.
1449220	1455650	A	0.796463668346405	So maybe that's a fun exercise is like to think about causal inference in our day.
1456020	1462470	A	0.8266376256942749	Like the temperature in my room is being influenced by whether the air conditioner is running in the other room and the temperature outside.
1463240	1466660	A	0.881916880607605	Which one of these scenarios does it model onto?
1467480	1478228	A	0.8864760994911194	And just kind of taking scenarios that are familiar to us and then separating them in terms of how are the hidden states changing through time, what are the observations?
1478404	1481530	A	0.7450602054595947	What policies influence how hidden states change their time?
1481840	1490990	A	0.70535808801651	But this is going to be an interesting different view on the bottom here with the continuous time model.
1492320	1496060	A	0.8399824500083923	Does anyone want to describe the continuous time variance?
1505040	1511110	A	0.877007007598877	Also one interesting piece here is in the live streams that we just did over the last few weeks 46.
1511640	1518828	A	0.8578593134880066	They make a clear distinguishing between Dai decision active inference and Mai motor active inference.
1519024	1534108	A	0.7859811186790466	And beyond just modeling cognitive decision making versus motor reflex arcs, they also highlight how the Dai is a discrete time model that's using the POMDP, while the motor models tended to use continuous time.
1534274	1546316	A	0.8564638495445251	So they're laying out and also kind of generalizing to show the similarities between these two different variants.
1546508	1552320	A	0.7748159766197205	And we can use the notation concordance table to highlight some of those parallels.
1553940	1561190	A	0.8528077006340027	This is where the Taylor series approximation comes in and the generalized coordinates of motion, though they can also be applied to the discrete time.
1561880	1569460	A	0.8021112084388733	Here we have x, x prime and x double prime.
1570060	1574200	A	0.8020103573799133	The prime notation indicates temporal derivatives and second derivatives.
1576220	1585244	A	0.5452017784118652	If anyone has a thought on that, feel free because this is not doing time series prediction in the same way that the POMDP is.
1585442	1590030	A	0.8408082723617554	The POMDP is in the memory of the program.
1590800	1596720	A	0.8049828410148621	There's a value at the previous state, the current state and the next state and however many other states in the time horizon.
1597620	1610644	A	0.6806008815765381	The way that a continuous time Taylor series approximation is dealing with reduction of uncertainty about future data is quite different than the way that these discrete time models are doing.
1610682	1628308	A	0.8947148323059082	So what's happening here is the value of the function now is being estimated or provided, then the first derivative is calculated.
1628484	1631456	A	0.7895612120628357	So here three has the same structure.
1631588	1638060	A	0.8470377922058105	But whereas this is the hidden state at the next time point conditioned upon the hidden state at this time point.
1638130	1655170	A	0.8188053369522095	And the policy here's the derivative of x conditioned upon x and the policy V also has a slightly different interpretation because it's not sequence of events either.
1655940	1660100	A	0.8301922082901001	And analogously for the second derivative and the higher derivatives.
1661080	1668676	A	0.6922681927680969	So they're still emitting observations, but these are not observations at future time points in the same way.
1668858	1674970	A	0.848000705242157	And that is visualized in this box 4.2.
1675500	1679592	A	0.8990297317504883	So here is x of t at time t.
1679646	1682350	A	0.8761858940124512	So this is like the way that the time series is going to go.
1682800	1685372	A	0.7830837965011597	X of t is just this value.
1685426	1699090	A	0.8762098550796509	Here five, the first term being added in the Taylor series expansion is the first derivative rate of change at that x zero.
1700420	1704050	A	0.7121620774269104	That's giving you a better approximation through time.
1704500	1707620	A	0.8711197376251221	Then the second derivative adds this quadratic feature.
1708120	1723716	A	0.7310901880264282	And so Taylor series converge closer and closer, moving further and further away from the target point as they include higher and higher derivatives.
1723908	1736232	A	0.764060914516449	But there isn't an explicit calculation in the Taylor series of like, let's just say this is like one, two, three timesteps.
1736376	1741710	A	0.5571196675300598	It's not like, well, what is going to happen at three timesteps from now?
1742080	1747410	A	0.9239770174026489	The Taylor series could then be plugged in with three to calculate that.
1748660	1770490	A	0.8277686834335327	However, this is not calculating x x prime x double prime at t equals three, it's calculating it for x sub zero and then using this expansion to achieve reduction of uncertainty of more and more distal points.
1774420	1776130	A	0.9631375074386597	So that's quite interesting.
1776500	1792980	A	0.5480777025222778	And again, it's an extremely different way of doing time series prediction in the continuous time framework than this kind of explicit consideration of past, present and future state values.
1795980	1798280	A	0.889076292514801	Okay, any other thoughts on 4.3?
1798430	1805800	A	0.773812472820282	Because this is one of the most key figures, but the things we can explore and ask Eli.
1808560	1818540	B	0.8503980040550232	About this figure on page 69 it says the relationship between a state and its temporal derivative here depends on slowly varying causes.
1819040	1837140	B	0.774196445941925	New, I just wanted to make sure, does this mean this slowly varying causes here is used to account for the fluctuations or it has some different meaning, slowly varying causes.
1843240	1870620	A	0.7618334889411926	So policies are influencing the causal unfolding of states in this discrete formalization here in the continuous time setting, the slowly varying causes more slowly than these changes play a similar role to policies above.
1875460	1880960	A	0.8342581987380981	So these causes intervene in how the derivatives are calculated?
1887670	1891442	B	0.7415190935134888	No, I mean here in the discrete time.
1891576	1904840	B	0.7304672598838806	Well, obviously we have, we can say stable policies that doesn't, that don't change through time, but in the continuous time.
1905690	1919190	B	0.7219235897064209	Well, I don't know, at least that was my understanding that the policies can slightly change but the change can be somehow negligible.
1919350	1934990	B	0.8337164521217346	And well, that's because of the additional term for fluctuations in the equations and they don't necessarily affect the main components of the equations.
1938630	1952520	A	0.8260178565979004	One could set it up so that the policies are ineffectual, so that the state changes through time are with their own endogenous dynamics, not influenced by policy.
1953290	1959718	A	0.6668558716773987	Or one could imagine a situation where the states don't change at all through time and their changes are entirely driven by policy.
1959884	1980400	A	0.8345937132835388	And I think analogously there could be a setting in which the derivative of x is hardly influenced by these v slowly varying causes or V might entirely describe the derivatives of x.
1983330	2004230	A	0.7544010877609253	But I'm not sure if just at this level of generality we can allocate importance to kind of the endogenous dynamics or like sort of the policy independent changes just the way that those states are changing or analogously derivatives are calculated.
2006890	2009234	A	0.7644376158714294	But that's a great question Lyle.
2009282	2010470	A	0.7913030982017517	And then Mike?
2011790	2031530	E	0.6496439576148987	Yeah, this might be just slightly off topic, but the tools, if you're using a multimode simulation tool, then it can handle both in some sense it's going to try and solve both continuous formulations and discrete time formulations.
2031610	2038980	E	0.9220653772354126	So one they would typically call agent based and the other would be Ode based or something like that.
2039910	2049466	E	0.6996025443077087	The way that they do that is a little bit of sleight of hand because of course they're not solving the equations per se, they're estimating the solutions.
2049598	2067450	E	0.6868312358856201	And if you have that, then if you have a slowly changing set of parameters like you might model with the discrete case, then you've got an event queue and those events are however far apart they need to be.
2067600	2072806	E	0.8762204051017761	Then to estimate the continuous form, they simply set that delta t delt.
2072838	2075446	E	0.5250259637832642	Delta T goes to zero in some sense.
2075488	2077870	E	0.509019136428833	It never goes in the software exactly to zero.
2077940	2086720	E	0.8576928377151489	But then you compress that delta t down to very small time slices and then you have estimated your continuous form.
2087110	2099970	E	0.7080790400505066	And so by doing that, you can really have a pretty comprehensive way to represent both cases in the same modeling environment.
2101130	2103878	A	0.8734541535377502	Yes, great point.
2104044	2120090	A	0.813433051109314	Numerical approximations to continuous processes can be done through breaking them down into discrete processes and then making sure that as your delta T is getting smaller and smaller, that you're getting a convergent estimator.
2121470	2129550	A	0.8906074166297913	The approach that's taken analytically here is to use a Taylor series approximation, Mike.
2131250	2144610	D	0.8747950792312622	And so in thinking about how we should build a mental model of the Taylor series approximation or incorporate that in our model, that's one example of how you might capture the time series structure.
2145190	2154950	D	0.842039942741394	And you could potentially substitute in other approaches to maybe capture finer detail or discrete events that might occur on the series.
2158010	2159000	A	0.6021074056625366	Like what?
2161210	2168310	D	0.8133569359779358	Like Loaz decomposition or something like that, where you're taking apart components of the time series.
2168390	2172250	D	0.8397900462150574	Things like trend seasonality discrete events.
2173630	2194020	A	0.8338493704795837	Yes, I think the analytical comparisons would be tight or loose, but Coleman Filter, Generalized, Bayesian filtering, Splines, time series decomposition, these are all in the category or like in the genre of this.
2195430	2196370	A	0.6561701893806458	Ollie.
2199690	2204390	B	0.9717777371406555	By the way, Lionel, thanks for the clarification, that was really helpful.
2205530	2225658	B	0.9161825776100159	Well, to ask my question more explicitly, on page 78, equation 4.15, we have some additional omega terms which are defined as stochastic fluctuations.
2225754	2235120	B	0.8434528708457947	My question was that is the term slightly varying somehow related to these stochastic fluctuations or not?
2236870	2254550	A	0.8276655673980713	Yes, I believe that the slowly varying is in relationship to it being included with the flow component rather than with the at that time scale stochastic more thermal like vibration.
2254970	2276750	A	0.7368373870849609	And that was explored in like live stream 45 with free energy principle made simpler but not too simple with this idea of the length of N and how physics and mechanics are predicated upon the separation into flow like actions at a given scale.
2277090	2282894	A	0.7811500430107117	And then stochastic non flow aligned changes.
2283092	2291140	A	0.6585481762886047	And the path of least action is when the limit of the stochastic term going to zero.
2292630	2304454	A	0.6523109674453735	But let's come there as people can see, even though it's like we could totally read this like 20 times and it still is like why is the next word there?
2304492	2305734	A	0.7830754518508911	Why is the next equation there?
2305772	2316570	A	0.8965483903884888	But okay, they're introducing these two types of generative models that have kind of tantalizing isomorphisms but also very interesting differences.
2317310	2328538	A	0.8973329663276672	They're going to first focus on the discrete time, the partially observable Markov decision process formalism is used.
2328624	2332094	A	0.7055256962776184	Someone asked like why is the categorical notation used?
2332212	2346206	A	0.6154416799545288	It just allows it makes it easy to look at a matrix and to interpret the matrix as like a confusion matrix, not just a matrix that is confusing, like they can be, but one that has to do with like a coin flip.
2346398	2348450	A	0.7549107074737549	So categorical outcomes.
2351930	2364538	A	0.9013994336128235	The three nodes are the transition function here in the categorical context between different states that's this B.
2364704	2370540	A	0.893118679523468	And here's where we see the D, the prior over the initial state and B.
2371070	2374700	A	0.91066974401474	And together these account for the three nodes in Figure 4.3.
2375650	2386590	A	0.8711974024772644	They play a functionally similar role here's, where they get distinguished selecting between models of behavior.
2387030	2416090	A	0.8549824357032776	And that was another question which maybe we could get to requires selection amongst these categorically discrete plans and then the soft max normalizes and ensures that the probability over those policies is normalized so that they can be understood as a probability distribution, like a categorical probability distribution.
2418030	2421660	A	0.7979702353477478	Here's the expected free energy that we've seen before.
2428940	2447020	A	0.8619126677513123	More on expected free energy, specifically on how this KL divergence term, which as we talked about last week was about how the preferred states are realized.
2453420	2455832	A	0.8578202724456787	Active inference uses F and G.
2455966	2458200	A	0.8330265879631042	They're related, but they play different roles.
2458860	2463130	A	0.8500891327857971	VFE F is the primary quantity minimized over time.
2463820	2472430	A	0.8116919994354248	That was interesting to read because EFE is what is minimized prospectively through time.
2474080	2483810	A	0.8094861507415771	Whereas here the claim is that variational free energy is what is minimized over time in relationship to generative model.
2485140	2488770	A	0.5903740525245667	So again, more could be explored there.
2490260	2504010	A	0.8986179232597351	They are going to focus on a rearrangement of equation 2.6 here with the ambiguity and risk being juxtaposed with the informational value, information gain, infomax and Pragmatic value.
2509910	2537450	A	0.6616356372833252	They then move into the linear algebraic form where the bolds okay, I don't know if bold is used in the same way in this appendix as it is in these equations.
2538190	2543214	A	0.855537474155426	Does anyone know about what bold means here?
2543252	2546350	A	0.84769207239151	It could mean the vector Ali.
2549170	2555150	B	0.8493096828460693	Yes, I also think bold most probably means vector or matrix.
2556150	2566050	A	0.6808544993400574	Yes, it gets pretty subtle though sometimes because there's times where there's italics bold and neutral being used very closely.
2567850	2573590	A	0.6432226896286011	When they're used closely, it can be confusing and then when they're used not closely, it's hard to juxtapose.
2576890	2585100	A	0.8824051022529602	More expected free energy rewriting within the linear algebraic form.
2585710	2590170	A	0.8741517066955566	Softmax bringing back the logs.
2592920	2598724	A	0.8711352348327637	Variational inference rests upon factorization that's related to the sparsity of the Bayes graph.
2598772	2599720	A	0.6278713941574097	Moritz.
2602220	2617276	F	0.4895395040512085	Hey, so just going to 410 again, I was wondering what's the use of the categorical distribution here because maybe I miss it, but it's not really explained in the text what they use it for.
2617378	2619890	F	0.6877409815788269	There's one sentence saying like oh yeah.
2621460	2626908	F	0.8878155946731567	The fifth line shows that the prior belief about observations is a categorical distribution.
2627084	2628624	F	0.7581675052642822	Like, what is it for?
2628662	2629810	F	0.8034924864768982	What does it do?
2630180	2631830	F	0.572772204875946	I've never seen it before.
2638940	2648010	A	0.8521849513053894	I don't know if it has to be categorical, a preference distribution could be a continuous function.
2649360	2659820	A	0.7673562169075012	But here it's just simpler perhaps to show it as a categorical.
2660880	2665010	A	0.5973435640335083	Like, there's two outcomes having the food and not having the food.
2666180	2672400	A	0.8407303094863892	So then that's a categorical distribution of preferences and of observations.
2673720	2687880	A	0.732395589351654	So they're just modeling a situation where there's categorical differences that are being preferred as opposed to, like, a preference over some continuous distribution.
2690380	2692184	A	0.7647771239280701	Does that address it?
2692222	2699244	A	0.6116997599601746	They're just modeling a situation where there's a categorical difference with observations and with.
2699282	2705630	F	0.5900542736053467	Preferences just because mathematically it's easier now to start with that.
2706640	2712492	A	0.6884461641311646	Yeah, I think didactically it's a lot clearer because you can see like, a two by two matrix.
2712636	2714352	A	0.7961463332176208	Did I observe the food or not?
2714486	2716256	A	0.7264622449874878	Did I get the food or not?
2716438	2723920	A	0.8052880167961121	Instead of like a distribution of temperature preference and a distribution of observations.
2726840	2736748	A	0.7265317440032959	I'd expect that the formalism will work out the same in the sense that you're still minimizing your surprisal and you're still performing distribution matching.
2736944	2745880	A	0.6874841451644897	But also this is like a distribution matching in the categorical context that has an interpretation of almost like false positive and false negative.
2750090	2750630	A	0.4896697998046875	All right.
2750700	2751126	F	0.584351658821106	Okay.
2751228	2752166	F	0.5326089262962341	I will go with it.
2752188	2753618	F	0.8950693607330322	Thank you for the explanation.
2753794	2756040	A	0.821924090385437	Yeah, it's a good question.
2761450	2768810	A	0.76518315076828	The matrices help connect to the linear algebra and the MATLAB representations.
2769390	2779760	A	0.8370794653892517	And also just to sort of match or no match, like in 46, the example had to do with wanting ice cream and then they observed ice cream or not.
2783550	2786470	A	0.7360846996307373	Okay, POMDP.
2786550	2789050	A	0.8091235160827637	And also, where does this ita?
2789810	2792766	A	0.7504250407218933	We'll come back to that another time, but just to kind of get through it all.
2792788	2805166	A	0.9155084490776062	The first pass more details on a POMDP, S, and V auxiliary variable.
2805358	2814930	A	0.5791919827461243	I don't think this is the V here because we're in the POMDP setting.
2817130	2823590	A	0.8536101579666138	So it's just an auxiliary variable used as sort of an analytical convenience.
2827630	2835338	A	0.885988175868988	That concludes their discussion of the discrete time model into continuous time.
2835424	2840380	A	0.9062542915344238	But first they describe again Markov blankets and take kind of a second pass.
2841970	2852634	A	0.5835346579551697	The blankets are the causes of X upstream, which are parents and the children and the parents of the children.
2852772	2855010	A	0.7413492202758789	The kind of co influencers.
2858110	2873114	A	0.8663728833198547	Of course, there's more to say, but that's what the Pearl definition is, how that gets mapped to sense and action states and what influences what's in the blanket, on the blanket, et cetera.
2873242	2874686	A	0.7822639346122742	Those are model specific.
2874868	2882610	A	0.874182939529419	And then here are two common message passing schemes that are used for approximate inference.
2883990	2886094	A	0.8012195229530334	Unless the Bayes graph is fully connected.
2886142	2890194	A	0.8446864485740662	There is a Markov partitioning here.
2890312	2891402	A	0.964823305606842	This is pretty funny.
2891486	2894146	A	0.57359778881073	NB notepen a good note.
2894258	2905530	A	0.49556195735931396	Good note to note the slightly nonstandard use of the expectation operator, perhaps by overloading it with a massive subscript.
2905950	2908854	A	0.6447956562042236	But it's unclear what exactly they meant.
2908902	2918954	A	0.6809982657432556	There is showing alignment between these two different schemes, but there's definitely citations to go into, more into.
2918992	2922990	A	0.8306784629821777	Detail on like belief propagation and message passing inactive.
2923410	2931010	A	0.8471501469612122	And then we see several of these figures.
2938870	2957880	A	0.6160032749176025	Personally, I think it's slightly challenging to understand whether this is being used illustratively or whether these specific topologies are as directly interpretable as these topologies are.
2961960	2968630	A	0.5384519100189209	But here we see kind of if you blur your eyes here's, policy at the top.
2969400	2971300	A	0.7920060157775879	Here's the observations.
2972360	2974150	A	0.761621356010437	Here's time minus one.
2974860	2979050	A	0.8774391412734985	Here's s minus t minus one, t and T plus one.
2979500	2987790	A	0.5282655358314514	So we can see some resonances with 4.3.
2988640	2993336	A	0.8001886606216431	But now we're in the continuous time motor.
2993368	3004480	A	0.8894377946853638	Active inference from 46 is justifying the use of continuous time based upon the continuous unrolling aspect of sensory input and motor output.
3007940	3012210	A	0.7484032511711121	They're going to start in a different place than they did with the POMDP model.
3013320	3018020	A	0.7912567257881165	In this case, we have much more of a physics grounding.
3018600	3024492	A	0.930609405040741	Again, check out live stream number 45 to see about the lengthen and how this is connected.
3024656	3033092	A	0.8005843162536621	But this is a much more physics based grounding in terms of like a flow operator and a stochastic term.
3033156	3045944	A	0.8675202131271362	At a given timescale, the stochastic term is assumed to have a normal distribution.
3046072	3049260	A	0.8635309934616089	And that kind of relates to what Mike said about detrending.
3049760	3057696	A	0.7439533472061157	One can think of this stochastic term as being what happens when you've detrended all of the signal that's non Gaussian out.
3057878	3067220	A	0.78138667345047	You're left with the residuals which have a gaussian nature.
3068920	3074980	A	0.8354657888412476	Precision is capital pi and that's the inverse of the fluctuation.
3075420	3078840	A	0.7624323964118958	And so that's they're going to connect that to the common filtering.
3081020	3093976	A	0.9301937222480774	Here we return again to the generalized coordinates of motion coming from this Taylor Series approximation route.
3094168	3105890	A	0.8233152031898499	So if somebody said I have ten numbers and we need to predict ten years in the future, the discrete time way would be like, well, let's try to predict it at each of those ten years and those will be your ten numbers.
3106260	3117316	A	0.8132734298706055	The generalized coordinates of motion's approach which we explored the most in live stream number 26 with the Costa et al.
3117498	3132020	A	0.884454607963562	In Bayesian mechanics, the ten numbers could be like the value today and the derivative today, the second derivative and the third and the fourth and the fifth up to however many that's the generalized coordinates of motion.
3132180	3137480	A	0.8678618669509888	So it's like a snapshot of the process and all of its higher derivatives.
3137920	3156124	A	0.8451977372169495	And so the generalized coordinates of motion are very similar to a Taylor Series approximations in how they represent centered at a certain point x naught how one expects as movement happens away from that x knot, reducing surprise.
3156172	3162188	A	0.8843744397163391	And so here is like x with a dot on top is the change in x.
3162294	3165670	A	0.8600077629089355	And then this is like the derivative of the change in x and so on.
3166920	3171700	A	0.9143227934837341	Here the Tilde notation is used for the generalized coordinates of motion.
3173400	3185960	A	0.7819687128067017	So we see this equation which was just for one value, like where you are on the freeway.
3186880	3189820	A	0.8305404186248779	And then this is the generalized coordinates of motion.
3192880	3204640	A	0.8916350603103638	The free energy is written down for this generalized coordinates of motion with precision.
3205060	3205810	A	0.5710901021957397	Here.
3206260	3212320	A	0.8619657158851624	The closest that we came to exploring it was in 43 on predictive coding.
3218130	3235590	A	0.8918143510818481	There are some more details on Gaussian, the relationship between like, Gaussian processes and also, I think Lyle brought up this sort of mode and multimode simulations.
3239630	3242694	A	0.8484393358230591	This is modeling a single mode.
3242822	3251020	A	0.8129968643188477	Not that it can't model a bimodal distribution, but this is where the LaPlace approximation comes into play.
3252530	3259520	A	0.7721591591835022	This is potentially even unnecessarily complex, but it's there.
3259890	3274850	A	0.7950518727302551	And the LaPlace approximation is fitting a quadratic distribution that tracks the mode.
3276310	3280338	A	0.7944281697273254	So like the highest point on the distribution.
3280514	3284120	A	0.783470094203949	And then it fits a parabola around that.
3290590	3306410	A	0.9213758111000061	Here is more details on the hierarchical nature and the kind of multi timescale nature that's implied ultimately by the lengthen.
3309150	3322420	A	0.6044403314590454	And then there's it'd be good to juxtapose figure four, six, and four to see how they're similar and different.
3323590	3328542	A	0.7762821316719055	But here we see message passing happening on the generalized predictive coding architecture.
3328686	3336600	A	0.7792560458183289	And that was like, explored a lot more, not with this exact figure, but this was explored more from like an analytical and empirical perspective in 43.
3341410	3354340	A	0.8288354277610779	Then there's an abrupt ending, but the following chapters are going to appeal to the formalisms and apply them.
3360330	3375130	A	0.5719529986381531	So, any thoughts or ideas on chapter four as we close out, and hopefully next week, we can have a lot of questions and things like that, even basic questions.
3375200	3388478	A	0.7482861280441284	It's like if you read it and you understood it, then it'd be awesome to contribute a question that would explore someone else's understanding or prompt towards how you thought about it in a way that made sense for you.
3388644	3406090	A	0.58392333984375	And if you don't understand it, then just ask the question Ali and then anyone else?
3408460	3419208	B	0.9212232232093811	I just wanted to briefly mention an additional point related to the question about the reason behind using the categorical distribution.
3419384	3432620	B	0.8823232650756836	On page 74, it says the fifth line shows that the prior belief about the observations is a categorical distribution whose sufficient statistics are given in the C vector.
3432700	3451640	B	0.6361325979232788	I think here, sufficient statistics is the key term to understand the reason behind using categorical distributions, because for insufficient statistics, we don't necessarily use the exact modeling or distributions.
3452140	3463548	B	0.7262088060379028	Instead, we use a substituted statistics that is simple enough to calculate and also close enough to the actual data.
3463714	3470860	B	0.711050808429718	So I think that can help in understanding the reason behind that decision.
3475010	3488690	A	0.5223214626312256	Good point, and this is definitely a technical note, but is it fair to say that the mean and the variance are sufficient statistics for a Gaussian distribution?
3491610	3493240	B	0.5154740810394287	Yes, I think so.
3496170	3526110	A	0.66897052526474	That's one of the perhaps proximate mechanisms by which the LaPlace approximation and variational inference more generally are able to deal with arbitrary true distributions by fitting a family of distributions that have tractable optimization structure and a vastly reduced set of sufficient statistics.
3526450	3539480	A	0.7896977663040161	If you had even just a simple bimodal distribution, well, there'd be like the location of the two peaks, the relative heights of the two peaks, the skewedness and all the you could have many, many parameters needed to describe it.
3540410	3547906	A	0.8430655002593994	In contrast, the LaPlace approximation only requires the location of the mode and the variance estimate.
3548098	3554700	A	0.5254491567611694	And so where that's adequate, it's simple and fast.
3555630	3565040	A	0.6788391470909119	Where it's inadequate, you'll at least know because you'll continue to be surprised at new observations coming in.
3569570	3572462	A	0.8322556614875793	Okay, any final comments on this?
3572596	3574240	A	0.7449131011962891	Chapter four, part one.
3580500	3593690	A	0.9279760122299194	Definitely a challenging, though interesting chapter, and it really is at the heart of active.
3595570	3616530	A	0.533626914024353	And as we hopefully explored a little bit today, it includes, like, basic Intuition pumps, as well as some Rosetta Stone like representations and many, many equations which I hope we can unpack.
3617030	3619540	A	0.8299441337585449	So could someone explain it in simple words?
3619910	3626980	A	0.6268423795700073	It's what we've been asking for every single equation, and it's something that everyone can contribute to.
3627510	3642506	A	0.8515164852142334	All right, well, looking forward to people's edits over the coming week and additions, and we'll come back in one week for part two on four.

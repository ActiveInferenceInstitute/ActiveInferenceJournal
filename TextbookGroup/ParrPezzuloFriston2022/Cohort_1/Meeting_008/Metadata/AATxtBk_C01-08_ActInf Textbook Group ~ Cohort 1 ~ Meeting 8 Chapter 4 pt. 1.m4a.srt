1
00:00:01,450 --> 00:00:03,950
Hello everyone. It's June 23,

2
00:00:04,020 --> 00:00:08,160
2022 and we are in week eight

3
00:00:08,530 --> 00:00:11,920
of the textbook group cohort one.

4
00:00:12,690 --> 00:00:16,094
We're starting chapter four and

5
00:00:16,132 --> 00:00:17,950
we'll be continuing with chapter four

6
00:00:18,100 --> 00:00:22,106
next week. We're more than halfway

7
00:00:22,298 --> 00:00:25,494
done with the time and

8
00:00:25,532 --> 00:00:27,702
with the chapters that we're going

9
00:00:27,756 --> 00:00:29,480
through in the first half of the book.

10
00:00:30,490 --> 00:00:33,574
So let's go to chapter four and just

11
00:00:33,612 --> 00:00:35,766
raise your hand and gather or write in

12
00:00:35,788 --> 00:00:37,126
the chat if you have anything that you

13
00:00:37,148 --> 00:00:41,638
want to address. So I'm

14
00:00:41,654 --> 00:00:44,186
going first to the math overviews page

15
00:00:44,288 --> 00:00:46,218
where I've written some overviews for

16
00:00:46,224 --> 00:00:49,686
the previous chapters of varying levels

17
00:00:49,718 --> 00:00:53,022
of completeness. But this is very

18
00:00:53,076 --> 00:00:55,514
important as we set off into chapter

19
00:00:55,562 --> 00:00:59,546
four. So on page 64, this chapter

20
00:00:59,578 --> 00:01:01,166
is more technical than chapters one

21
00:01:01,188 --> 00:01:03,558
through three, appealing to linear

22
00:01:03,594 --> 00:01:05,326
algebra, differentiation and a Taylor

23
00:01:05,358 --> 00:01:09,134
series expansion. Those readers

24
00:01:09,182 --> 00:01:10,818
interested in the details may turn to

25
00:01:10,824 --> 00:01:13,586
the appendices dot, dot, dot. Those who

26
00:01:13,608 --> 00:01:14,958
do not want to delve into the

27
00:01:14,984 --> 00:01:16,886
theoretical underpinnings may skip this

28
00:01:16,908 --> 00:01:20,774
chapter. So keep that

29
00:01:20,812 --> 00:01:22,840
in mind as we continue on.

30
00:01:23,690 --> 00:01:27,234
And it can be an area of

31
00:01:27,372 --> 00:01:29,740
discussion and learning and such,

32
00:01:31,070 --> 00:01:35,034
but let's approach these themes and

33
00:01:35,072 --> 00:01:41,230
formalisms with the authors forewarning

34
00:01:41,730 --> 00:01:45,854
that this is something we can look for

35
00:01:45,892 --> 00:01:49,390
more detail in the appendixes as well as

36
00:01:49,540 --> 00:01:53,220
skip this chapter, even if we

37
00:01:54,310 --> 00:01:56,260
might disagree, of course.

38
00:01:57,190 --> 00:02:01,202
Okay, any other overall comments on

39
00:02:01,256 --> 00:02:04,834
chapter four just for whomever read

40
00:02:04,872 --> 00:02:07,414
it, even a part of it? What was their

41
00:02:07,612 --> 00:02:10,294
overall perspective on chapter four?

42
00:02:10,412 --> 00:02:13,794
What was it trying to do? What approach

43
00:02:13,842 --> 00:02:14,760
did they take?

44
00:02:32,680 --> 00:02:34,710
Yeah, Ali and then anyone else?

45
00:02:37,880 --> 00:02:40,456
I think if we take the materials in

46
00:02:40,478 --> 00:02:44,504
chapters one through three as the

47
00:02:44,542 --> 00:02:48,056
foundational materials on which the

48
00:02:48,158 --> 00:02:51,864
active inference theory is supposed to

49
00:02:51,902 --> 00:02:55,756
build, well, this chapter four is one

50
00:02:55,778 --> 00:02:59,096
of the first steps toward

51
00:02:59,288 --> 00:03:02,670
building the actual theory and

52
00:03:03,600 --> 00:03:06,096
I mean going beyond just the basics and

53
00:03:06,118 --> 00:03:09,344
foundational materials. So using the

54
00:03:09,542 --> 00:03:12,496
tools and foundations established in

55
00:03:12,518 --> 00:03:17,292
previous chapters, we are now perhaps

56
00:03:17,356 --> 00:03:20,708
ready to tackle the problem of actually

57
00:03:20,794 --> 00:03:27,716
constructing the generative models in

58
00:03:27,738 --> 00:03:30,028
two different situations as discrete

59
00:03:30,064 --> 00:03:33,444
time models and as continuous

60
00:03:33,492 --> 00:03:36,472
one. Awesome,

61
00:03:36,606 --> 00:03:41,210
thanks. Yes. In the chapters page

62
00:03:42,640 --> 00:03:46,670
we can recall back to chapter one

63
00:03:47,360 --> 00:03:49,436
that just laid out the structure of the

64
00:03:49,458 --> 00:03:52,252
book. Chapter two provided the low road

65
00:03:52,306 --> 00:03:54,984
to active inference, which began with

66
00:03:55,042 --> 00:03:57,856
Bayesian inference and talked about a

67
00:03:57,878 --> 00:04:01,772
few other prerequisite or preliminary

68
00:04:01,836 --> 00:04:06,236
themes including introducing variational

69
00:04:06,268 --> 00:04:10,128
and expected free energy as imperatives

70
00:04:10,304 --> 00:04:13,364
in the sense that they're able to be

71
00:04:13,402 --> 00:04:16,884
bounding surprise. Chapter three

72
00:04:16,922 --> 00:04:18,688
introduced the high road to active

73
00:04:18,704 --> 00:04:20,664
inference, which was starting not from

74
00:04:20,702 --> 00:04:24,296
the mechanistic kind of nucleus of

75
00:04:24,318 --> 00:04:26,936
the Bayes equation, but rather from the

76
00:04:26,958 --> 00:04:30,360
imperative for survival and persistence.

77
00:04:30,700 --> 00:04:33,836
Also introducing in a first pass the

78
00:04:33,858 --> 00:04:37,340
Markov blanket concept in partitioning.

79
00:04:41,040 --> 00:04:43,708
Chapter four is indeed when we start to

80
00:04:43,794 --> 00:04:46,960
get into many details that were not

81
00:04:47,110 --> 00:04:50,560
covered in earlier sections.

82
00:04:51,140 --> 00:04:55,196
It's going to first begin by bringing

83
00:04:55,228 --> 00:04:58,192
us closer to this connection between

84
00:04:58,246 --> 00:05:00,048
Bayesian inference and the free energy

85
00:05:00,134 --> 00:05:03,196
evaluation and then the

86
00:05:03,238 --> 00:05:05,972
central idea of a generative model is

87
00:05:06,106 --> 00:05:09,364
discussed that is going to be

88
00:05:09,402 --> 00:05:11,850
described in discrete time,

89
00:05:12,220 --> 00:05:15,892
specifically using the POMDP formalism,

90
00:05:16,036 --> 00:05:17,924
partially observable Markov decision

91
00:05:17,972 --> 00:05:21,272
process. And then, as well as in

92
00:05:21,326 --> 00:05:25,036
continuous time, then there's going

93
00:05:25,058 --> 00:05:28,044
to be some very interesting figures and

94
00:05:28,082 --> 00:05:31,340
formalisms and discussions on what

95
00:05:31,410 --> 00:05:33,096
generative models underlie predictive

96
00:05:33,128 --> 00:05:35,952
coding and motor reflexes which is

97
00:05:36,006 --> 00:05:39,376
moving us towards chapter five which is

98
00:05:39,398 --> 00:05:42,396
going to have some empirical work mainly

99
00:05:42,428 --> 00:05:45,280
cited out and some discussion on the

100
00:05:45,350 --> 00:05:48,884
plausible Neurobiologies that can be

101
00:05:48,922 --> 00:05:51,600
implementing or modeled as implementing

102
00:05:51,680 --> 00:05:54,432
or modeled with the kinds of generative

103
00:05:54,496 --> 00:05:57,652
models of which have been prepared for

104
00:05:57,706 --> 00:06:00,088
in chapters two and three. Motivated in

105
00:06:00,094 --> 00:06:02,312
chapters two and three. Really? And then

106
00:06:02,366 --> 00:06:05,044
described in their essence in chapter

107
00:06:05,092 --> 00:06:05,690
four.

108
00:06:08,460 --> 00:06:11,716
Also just a reminder that in the math

109
00:06:11,748 --> 00:06:14,396
group activities but we're all in the

110
00:06:14,418 --> 00:06:16,572
math group, we're all in this learning

111
00:06:16,626 --> 00:06:19,436
journey together, we've been striving to

112
00:06:19,458 --> 00:06:21,512
make the natural language descriptions

113
00:06:21,656 --> 00:06:25,680
for equations. So that would be

114
00:06:25,750 --> 00:06:28,384
extremely, extremely helpful. Every

115
00:06:28,422 --> 00:06:30,704
annotation that people can make is

116
00:06:30,742 --> 00:06:33,516
helpful. People shouldn't feel abashed

117
00:06:33,548 --> 00:06:34,944
or ashamed to make any kind of

118
00:06:34,982 --> 00:06:39,104
contribution. It can always be reordered

119
00:06:39,152 --> 00:06:42,404
or edited by others. But this is how

120
00:06:42,442 --> 00:06:44,704
we're going to create those natural

121
00:06:44,752 --> 00:06:47,124
language descriptions of equations and

122
00:06:47,162 --> 00:06:49,332
ask questions about them, even just

123
00:06:49,466 --> 00:06:51,224
copying and pasting. What does this

124
00:06:51,262 --> 00:06:53,112
mean? What does this mean? They're all

125
00:06:53,246 --> 00:06:56,744
helpful contributions because this is

126
00:06:56,862 --> 00:07:01,284
very technical and it's

127
00:07:01,332 --> 00:07:03,836
not immediately apparent how, for

128
00:07:03,858 --> 00:07:06,316
example, these copious equations in

129
00:07:06,338 --> 00:07:08,316
chapter four relate to some of the

130
00:07:08,338 --> 00:07:09,896
broader discussions from the earlier

131
00:07:09,928 --> 00:07:12,990
chapters. But we'll get that,

132
00:07:16,580 --> 00:07:20,290
okay, if anyone has

133
00:07:20,820 --> 00:07:23,856
more questions to add, they can do that.

134
00:07:23,878 --> 00:07:25,760
Or if they want to upvote questions,

135
00:07:25,910 --> 00:07:29,340
feel free to do that. It's motivating

136
00:07:29,420 --> 00:07:31,044
for other people when they see that

137
00:07:31,082 --> 00:07:32,656
their questions are like being improved

138
00:07:32,688 --> 00:07:34,068
or other people are paying attention to

139
00:07:34,074 --> 00:07:36,452
their questions. So that's always

140
00:07:36,586 --> 00:07:38,004
something free and helpful that people

141
00:07:38,042 --> 00:07:44,304
can engage in are

142
00:07:44,362 --> 00:07:46,490
beliefs, policy or state.

143
00:07:47,820 --> 00:07:49,912
If the person who asked it is here,

144
00:07:49,966 --> 00:07:54,572
they can feel free to address it because

145
00:07:54,626 --> 00:07:58,712
it's a very short, partially formed

146
00:07:58,856 --> 00:08:02,204
question. I also wasn't sure whether

147
00:08:02,242 --> 00:08:04,860
this was using the CAD CAD ontology,

148
00:08:05,540 --> 00:08:07,456
which uses policy and state in a

149
00:08:07,478 --> 00:08:11,216
slightly different way. But beliefs in

150
00:08:11,238 --> 00:08:13,964
a general sense coming from the Bayesian

151
00:08:14,012 --> 00:08:17,090
ontology are referring to any

152
00:08:17,800 --> 00:08:21,072
distributional expression Bayesian

153
00:08:21,136 --> 00:08:25,590
beliefs. Those are states

154
00:08:27,400 --> 00:08:30,340
policies in the active ontology,

155
00:08:30,860 --> 00:08:33,272
which you can just mouse over and find

156
00:08:33,326 --> 00:08:36,120
out it's a sequence of actions.

157
00:08:37,180 --> 00:08:39,784
And policies are constructed or

158
00:08:39,822 --> 00:08:43,384
enumerated by the affordances,

159
00:08:43,432 --> 00:08:47,084
the e vector that

160
00:08:47,122 --> 00:08:50,092
the generative model has over a given

161
00:08:50,146 --> 00:08:53,084
time horizon. So if it's able to go left

162
00:08:53,122 --> 00:08:55,696
or right over two time steps, all the

163
00:08:55,718 --> 00:08:59,250
policies are left, left, right, left.

164
00:09:02,900 --> 00:09:04,464
Does anyone else just want to add

165
00:09:04,502 --> 00:09:06,770
anything else to this short question?

166
00:09:11,680 --> 00:09:13,720
Otherwise, it's a fine clarifying

167
00:09:13,800 --> 00:09:16,670
question. Okay.

168
00:09:25,200 --> 00:09:27,404
In the discussion of active inference in

169
00:09:27,442 --> 00:09:31,032
POMDP, the authors write to update

170
00:09:31,096 --> 00:09:32,940
beliefs about policies,

171
00:09:33,920 --> 00:09:37,056
we find the posterior that minimizes the

172
00:09:37,078 --> 00:09:40,592
free energy does a posterior at time T

173
00:09:40,726 --> 00:09:43,410
become a prior at time T plus one.

174
00:09:45,060 --> 00:09:46,610
What does anyone think?

175
00:10:01,560 --> 00:10:05,844
I guess it depends on the specific

176
00:10:06,042 --> 00:10:08,390
posterior and prior because that can

177
00:10:08,760 --> 00:10:11,584
mean multiple things. But in the context

178
00:10:11,632 --> 00:10:15,800
of Figure 4.3, I'd say

179
00:10:15,870 --> 00:10:19,460
it does because we are updating

180
00:10:19,540 --> 00:10:21,688
our prior, which becomes at the end of a

181
00:10:21,694 --> 00:10:23,124
time step, becomes the posterior

182
00:10:23,172 --> 00:10:25,816
probability, which then is fed back as

183
00:10:25,838 --> 00:10:29,564
the prior. Thanks. Totally agree

184
00:10:29,602 --> 00:10:31,964
with that. There needs to be an

185
00:10:32,002 --> 00:10:35,576
initiating prior D, which is shown

186
00:10:35,608 --> 00:10:38,510
with a three here in the discrete time.

187
00:10:38,820 --> 00:10:41,840
And that is like the initial prior.

188
00:10:42,340 --> 00:10:45,344
And then prior and posterior are

189
00:10:45,382 --> 00:10:48,370
referring to what? Incoming data.

190
00:10:48,900 --> 00:10:52,950
So after the data at zero one comes in,

191
00:10:53,320 --> 00:10:56,864
the updated posterior is now playing

192
00:10:56,912 --> 00:10:59,076
the functional role of the prior for the

193
00:10:59,098 --> 00:11:00,870
next data point coming in.

194
00:11:06,900 --> 00:11:09,140
Okay, then here's sort of a related

195
00:11:11,560 --> 00:11:13,590
question. Well,

196
00:11:17,960 --> 00:11:21,016
we've addressed this question in the

197
00:11:21,038 --> 00:11:24,072
narrow sense, but it would be very

198
00:11:24,206 --> 00:11:27,416
helpful to look through the order in

199
00:11:27,438 --> 00:11:29,176
which different topics are addressed in

200
00:11:29,198 --> 00:11:33,292
chapter four because these are

201
00:11:33,346 --> 00:11:34,796
some questions that are invoking things

202
00:11:34,818 --> 00:11:38,104
that are much later. And we don't

203
00:11:38,152 --> 00:11:41,692
assume that people have high or low

204
00:11:41,746 --> 00:11:44,364
comprehension of this chapter yet.

205
00:11:44,562 --> 00:11:46,370
But many of these questions,

206
00:11:48,260 --> 00:11:50,432
it's really important that we understand

207
00:11:50,566 --> 00:11:52,796
why we're bringing them up. So let's

208
00:11:52,828 --> 00:11:54,944
just walk through very quickly the

209
00:11:54,982 --> 00:11:57,956
chapter to see why they're bringing up

210
00:11:57,978 --> 00:12:01,652
things in a different order in terms of

211
00:12:01,706 --> 00:12:03,540
looking at the figures. And formalism.

212
00:12:05,000 --> 00:12:09,380
4.1 is returning to Bayes theorem.

213
00:12:09,900 --> 00:12:13,080
And I think this is a really nice

214
00:12:13,150 --> 00:12:15,396
representation here's the joint

215
00:12:15,428 --> 00:12:17,284
distribution of x and Y, the hidden

216
00:12:17,332 --> 00:12:20,068
states and the observables. And it's

217
00:12:20,084 --> 00:12:24,190
like it gets split up into

218
00:12:25,520 --> 00:12:29,164
Y condition x and X. So this is like

219
00:12:29,202 --> 00:12:30,744
the probability, the joint distribution

220
00:12:30,792 --> 00:12:35,824
of the coin flip and the die is the

221
00:12:35,862 --> 00:12:37,600
distribution of the coin flip

222
00:12:38,580 --> 00:12:41,036
conditioned upon the die and the die

223
00:12:41,068 --> 00:12:43,616
itself. So it's just separating out a

224
00:12:43,638 --> 00:12:46,224
joint distribution. And that's the heart

225
00:12:46,262 --> 00:12:47,600
of Bayes theorem.

226
00:12:50,520 --> 00:12:52,996
This is a key move to move an

227
00:12:53,018 --> 00:12:55,796
integration problem, a sum in the

228
00:12:55,818 --> 00:12:57,492
discrete case or an integration problem

229
00:12:57,546 --> 00:12:59,236
in the continuous case into an

230
00:12:59,258 --> 00:13:01,940
optimization problem which permits

231
00:13:02,020 --> 00:13:05,012
incremental solutions of improving

232
00:13:05,076 --> 00:13:08,040
quality. Unlike an integration problem,

233
00:13:08,190 --> 00:13:10,584
which you might just be tallying up

234
00:13:10,622 --> 00:13:12,200
numbers and not necessarily moving

235
00:13:12,270 --> 00:13:15,516
closer to knowing when you're done with

236
00:13:15,538 --> 00:13:17,020
that integral.

237
00:13:19,040 --> 00:13:21,980
They introduced Jensen's inequality,

238
00:13:22,880 --> 00:13:25,004
which is the log of the average is

239
00:13:25,042 --> 00:13:26,764
always greater or equal to the average

240
00:13:26,812 --> 00:13:31,244
of a log. And that's true for any shape

241
00:13:31,292 --> 00:13:33,840
that has this kind of a curve,

242
00:13:34,260 --> 00:13:36,044
anything where there's a decreasing

243
00:13:36,092 --> 00:13:39,652
curve. And here's where that gets

244
00:13:39,706 --> 00:13:43,364
applied above we

245
00:13:43,402 --> 00:13:47,220
had this taking this joint distribution,

246
00:13:48,280 --> 00:13:50,316
multiplying it by an arbitrary

247
00:13:50,368 --> 00:13:53,524
distribution divided by itself so that's

248
00:13:53,572 --> 00:13:56,090
one, whatever Q is,

249
00:13:56,780 --> 00:13:59,880
later Q will play a more specific role.

250
00:14:00,540 --> 00:14:04,092
And then here the expectation is

251
00:14:04,146 --> 00:14:07,500
inside the log. Here the expectation is

252
00:14:07,570 --> 00:14:09,070
over the log itself.

253
00:14:10,480 --> 00:14:13,836
So the one that we'd really want

254
00:14:14,018 --> 00:14:17,280
would be like the joint distribution

255
00:14:18,980 --> 00:14:21,376
would be like the expectation of the

256
00:14:21,398 --> 00:14:25,264
joint distribution divided by

257
00:14:25,302 --> 00:14:29,536
some function. Q Jensen's

258
00:14:29,568 --> 00:14:33,716
inequality allows us to make

259
00:14:33,818 --> 00:14:36,980
this instead of

260
00:14:37,130 --> 00:14:39,428
an equal sign, make a greater than an

261
00:14:39,434 --> 00:14:41,464
equal sign, and then pull the

262
00:14:41,502 --> 00:14:45,076
expectation out and utilize

263
00:14:45,108 --> 00:14:48,250
that as the negative free energy

264
00:14:48,700 --> 00:14:52,056
which is going to be bounding us on this

265
00:14:52,078 --> 00:14:55,532
one that we would truly want. But going

266
00:14:55,586 --> 00:14:58,750
about it by using this

267
00:15:00,400 --> 00:15:06,944
nice feature of the natural log base

268
00:15:06,982 --> 00:15:10,576
theorem in the logarithmic form and

269
00:15:10,598 --> 00:15:12,240
anyone can totally raise their hand and

270
00:15:12,310 --> 00:15:22,166
add details allows

271
00:15:22,198 --> 00:15:26,826
rearrangement into this form

272
00:15:27,008 --> 00:15:30,074
which is already recalling the free

273
00:15:30,112 --> 00:15:32,278
energy that we saw from variational,

274
00:15:32,294 --> 00:15:36,142
free energy from from equation 2.5 more

275
00:15:36,196 --> 00:15:38,254
details to be filled in and that's why

276
00:15:38,292 --> 00:15:40,314
we want to be annotating these equations

277
00:15:40,362 --> 00:15:41,280
and so on.

278
00:15:44,610 --> 00:15:46,594
Generative Models is the name of this

279
00:15:46,632 --> 00:15:49,698
chapter. To calculate free energy we

280
00:15:49,704 --> 00:15:51,406
need three things data variational,

281
00:15:51,438 --> 00:15:53,054
distribution, family, and a generative

282
00:15:53,102 --> 00:15:56,242
model which at minimum is composing of

283
00:15:56,296 --> 00:15:59,378
the prior and the likelihood they're

284
00:15:59,394 --> 00:16:00,614
going to be describing two different

285
00:16:00,652 --> 00:16:03,814
kinds of generative models. This is

286
00:16:03,852 --> 00:16:07,782
a really nice and intuitive graphic for

287
00:16:07,836 --> 00:16:09,734
those who might have different

288
00:16:09,852 --> 00:16:11,630
familiarity with statistical

289
00:16:11,810 --> 00:16:12,970
representations.

290
00:16:16,270 --> 00:16:19,354
The circles are random variables and

291
00:16:19,392 --> 00:16:20,598
the squares are probability

292
00:16:20,694 --> 00:16:22,970
distributions describing relationships.

293
00:16:24,610 --> 00:16:28,190
These are some Bayes graphs

294
00:16:28,690 --> 00:16:32,042
that are simpler and less action

295
00:16:32,106 --> 00:16:36,080
involved than this

296
00:16:37,090 --> 00:16:40,206
canonical active representation where Pi

297
00:16:40,238 --> 00:16:44,066
is policy. And so yeah, yakub, you wrote

298
00:16:44,248 --> 00:16:47,780
three is the B matrix. Yes, it is.

299
00:16:49,370 --> 00:16:51,670
These threes are the B matrix,

300
00:16:52,490 --> 00:16:55,990
but this is the D letter.

301
00:16:57,930 --> 00:17:00,600
So I agree though.

302
00:17:02,590 --> 00:17:05,146
Sorry, which one is the D letter? The

303
00:17:05,168 --> 00:17:06,780
first three. Yeah,

304
00:17:08,190 --> 00:17:10,582
because they're playing similar roles

305
00:17:10,646 --> 00:17:12,666
but they're a little bit or. What do you

306
00:17:12,688 --> 00:17:16,766
think about that? Yeah, I guess I

307
00:17:16,788 --> 00:17:19,182
was thinking about it as this being just

308
00:17:19,236 --> 00:17:22,910
one snapshot of like an infinite

309
00:17:23,650 --> 00:17:24,910
factor graph.

310
00:17:27,110 --> 00:17:30,898
Just. The fact that it's assigned the P

311
00:17:30,984 --> 00:17:34,466
of S tau plus one given S tau and

312
00:17:34,488 --> 00:17:35,250
pi.

313
00:17:37,910 --> 00:17:41,254
I understand what you mean by saying

314
00:17:41,292 --> 00:17:42,998
that. It's kind of like it plays the

315
00:17:43,004 --> 00:17:49,814
role of the yeah,

316
00:17:49,932 --> 00:17:51,480
I'm not entirely sure though.

317
00:17:53,950 --> 00:17:55,206
We'Ll come back to this because there's

318
00:17:55,238 --> 00:17:57,450
actually a few other interesting notes.

319
00:17:58,750 --> 00:18:04,014
So this is kind of the simplest of

320
00:18:04,052 --> 00:18:07,310
the graphs that they describe.

321
00:18:09,090 --> 00:18:11,114
This is like one variable influencing

322
00:18:11,162 --> 00:18:14,574
another variable. This is

323
00:18:14,612 --> 00:18:18,430
like two factors, z and X influencing

324
00:18:18,510 --> 00:18:22,660
Y. So two

325
00:18:23,350 --> 00:18:26,882
is like a little factory that

326
00:18:27,016 --> 00:18:29,734
outputs Y conditioned upon the state of

327
00:18:29,772 --> 00:18:34,162
X and Z. So knowing

328
00:18:34,306 --> 00:18:37,702
this notation and how to read these

329
00:18:37,756 --> 00:18:42,060
directed graphs is

330
00:18:42,510 --> 00:18:44,582
relatively essential for interpreting

331
00:18:44,646 --> 00:18:47,110
more complex graphs that involve actions

332
00:18:47,190 --> 00:18:51,194
and so on. Here's X

333
00:18:51,392 --> 00:18:55,022
as like an upstream hidden cause that

334
00:18:55,076 --> 00:18:57,566
influences two downstream variables that

335
00:18:57,588 --> 00:19:00,734
don't influence each other. And here is

336
00:19:00,772 --> 00:19:04,142
like a hierarchical model where V

337
00:19:04,196 --> 00:19:07,460
influences X and X influences Y.

338
00:19:08,150 --> 00:19:09,874
Does anyone have like thoughts or

339
00:19:09,912 --> 00:19:11,140
questions about this?

340
00:19:21,620 --> 00:19:24,016
This is a graphical probabilistic model.

341
00:19:24,118 --> 00:19:27,312
The variables are stochastic or they're

342
00:19:27,456 --> 00:19:30,564
statistical random variables. And it's a

343
00:19:30,602 --> 00:19:32,596
graph, it's consisting of nodes and

344
00:19:32,618 --> 00:19:33,220
edges.

345
00:19:38,580 --> 00:19:42,420
4.3 is going to introduce these two

346
00:19:42,490 --> 00:19:44,468
basic forms of the generative model

347
00:19:44,554 --> 00:19:48,404
dynamic through time used in activ in

348
00:19:48,442 --> 00:19:50,150
the factor graph form.

349
00:19:54,140 --> 00:19:56,376
Does anyone want to describe what they

350
00:19:56,558 --> 00:19:58,010
see in this figure?

351
00:20:08,870 --> 00:20:12,280
The top is in the discrete time case.

352
00:20:13,130 --> 00:20:17,078
S is the hidden state temperature of

353
00:20:17,084 --> 00:20:20,466
the room. O are the observations,

354
00:20:20,578 --> 00:20:25,386
the thermometer readings. The two is

355
00:20:25,488 --> 00:20:28,054
the relationship between the temperature

356
00:20:28,102 --> 00:20:30,186
in the room and the readings of the

357
00:20:30,208 --> 00:20:33,802
thermometer three is how

358
00:20:33,856 --> 00:20:36,954
the temperature is changing through

359
00:20:36,992 --> 00:20:40,426
time. And then it

360
00:20:40,448 --> 00:20:42,078
is just like Yakub said, it's kind of

361
00:20:42,084 --> 00:20:43,742
like here. We can think of this as being

362
00:20:43,796 --> 00:20:46,814
like a little bit of a snippet from this

363
00:20:46,852 --> 00:20:49,486
infinite sequence of temperatures

364
00:20:49,518 --> 00:20:53,394
through time. But in

365
00:20:53,432 --> 00:20:56,158
practice there has to be an initiating

366
00:20:56,254 --> 00:21:00,042
prior pi

367
00:21:00,206 --> 00:21:02,150
are policies,

368
00:21:02,890 --> 00:21:06,210
policies are sequences of actions,

369
00:21:06,290 --> 00:21:07,894
sequences of affordances that are

370
00:21:07,932 --> 00:21:10,082
concatenated over some time horizon.

371
00:21:10,226 --> 00:21:11,974
That's what we're evaluating. Like

372
00:21:12,012 --> 00:21:15,770
expected free energy on and policies

373
00:21:16,350 --> 00:21:18,940
represent causal impact in the world.

374
00:21:19,630 --> 00:21:22,266
And we can look at this graph, which is

375
00:21:22,288 --> 00:21:25,006
why this is an important prerequisite to

376
00:21:25,028 --> 00:21:27,422
understand. Because the way in which

377
00:21:27,476 --> 00:21:30,814
policies influence the world isn't like

378
00:21:30,852 --> 00:21:32,974
by taking the temperature and just

379
00:21:33,012 --> 00:21:35,402
changing it to a different value, it's

380
00:21:35,466 --> 00:21:38,482
changing three, which is how the

381
00:21:38,536 --> 00:21:40,180
temperature changes through time.

382
00:21:45,190 --> 00:21:46,898
Any thoughts on the discrete time

383
00:21:46,984 --> 00:21:47,970
formalism?

384
00:21:53,850 --> 00:21:55,030
Yes Mike.

385
00:21:59,700 --> 00:22:02,544
So as represented in this figure, the

386
00:22:02,582 --> 00:22:04,496
policy is not changing over time, is

387
00:22:04,518 --> 00:22:08,064
that correct? The set of

388
00:22:08,102 --> 00:22:12,260
policies in this

389
00:22:12,330 --> 00:22:15,140
figure is not changing.

390
00:22:19,640 --> 00:22:22,408
It's not that it can't. This is just

391
00:22:22,574 --> 00:22:26,056
only showing two steps of some policy

392
00:22:26,158 --> 00:22:28,264
being applied. Yeah, just the way it is

393
00:22:28,302 --> 00:22:31,672
in this figure. So if we were to maybe

394
00:22:31,726 --> 00:22:36,012
extend this figure with two more

395
00:22:36,066 --> 00:22:38,812
of the bottom section, we could also

396
00:22:38,866 --> 00:22:41,144
extend policy and have that changing

397
00:22:41,192 --> 00:22:43,550
over time and feeding into that.

398
00:22:45,600 --> 00:22:47,970
Yes, and that's actually one of the

399
00:22:48,580 --> 00:22:50,480
amazing and interesting things about

400
00:22:50,550 --> 00:22:53,776
this graphical representation is we can

401
00:22:53,798 --> 00:22:55,632
say, okay, well let's carry out this S

402
00:22:55,686 --> 00:22:59,104
to five more time steps and

403
00:22:59,142 --> 00:23:02,196
then let's have pi one implemented here,

404
00:23:02,298 --> 00:23:04,628
and then let's have it do pi two here.

405
00:23:04,794 --> 00:23:06,532
And it's kind of like if you can draw

406
00:23:06,586 --> 00:23:10,180
it, you can do it. And that's very

407
00:23:10,330 --> 00:23:13,236
important. Work from DeVries and Friston

408
00:23:13,268 --> 00:23:16,792
and Par from around five years ago was

409
00:23:16,846 --> 00:23:19,960
demonstrating that if the Bayes graph

410
00:23:20,780 --> 00:23:24,212
can be drawn, that there's a 40

411
00:23:24,286 --> 00:23:27,532
factor graph representation and where

412
00:23:27,586 --> 00:23:28,904
there's the 40 factor graph

413
00:23:28,952 --> 00:23:32,776
representation, there's attractable

414
00:23:32,888 --> 00:23:34,472
variational, message passing,

415
00:23:34,536 --> 00:23:35,660
approximation.

416
00:23:38,220 --> 00:23:40,184
I don't know exactly what the guide

417
00:23:40,232 --> 00:23:42,536
rails are for, like are there graphs

418
00:23:42,568 --> 00:23:45,372
that can't be amenable to message

419
00:23:45,426 --> 00:23:48,028
passing and et cetera, but just at a

420
00:23:48,034 --> 00:23:51,970
first pass for graphs that look

421
00:23:52,820 --> 00:23:57,756
like this, they can be drawn

422
00:23:57,788 --> 00:23:59,616
and more variables can be added and so

423
00:23:59,638 --> 00:24:01,970
on. So it's kind of like a visual code

424
00:24:02,760 --> 00:24:06,260
for probabilistic graphical models.

425
00:24:09,220 --> 00:24:11,696
So maybe that's a fun exercise is like

426
00:24:11,718 --> 00:24:14,736
to think about causal inference in our

427
00:24:14,918 --> 00:24:17,628
day. Like the temperature in my room is

428
00:24:17,654 --> 00:24:19,716
being influenced by whether the air

429
00:24:19,738 --> 00:24:21,044
conditioner is running in the other room

430
00:24:21,082 --> 00:24:23,796
and the temperature outside. Which one

431
00:24:23,818 --> 00:24:26,660
of these scenarios does it model onto?

432
00:24:27,480 --> 00:24:30,596
And just kind of taking scenarios that

433
00:24:30,618 --> 00:24:32,996
are familiar to us and then separating

434
00:24:33,028 --> 00:24:35,796
them in terms of how are the hidden

435
00:24:35,828 --> 00:24:37,128
states changing through time, what are

436
00:24:37,134 --> 00:24:39,380
the observations? What policies

437
00:24:39,460 --> 00:24:40,904
influence how hidden states change their

438
00:24:40,942 --> 00:24:45,164
time? But this is going to be an

439
00:24:45,202 --> 00:24:48,332
interesting different view on the bottom

440
00:24:48,386 --> 00:24:50,990
here with the continuous time model.

441
00:24:52,320 --> 00:24:54,076
Does anyone want to describe the

442
00:24:54,098 --> 00:24:56,060
continuous time variance?

443
00:25:05,040 --> 00:25:07,908
Also one interesting piece here is in

444
00:25:07,914 --> 00:25:09,236
the live streams that we just did over

445
00:25:09,258 --> 00:25:12,404
the last few weeks 46. They make

446
00:25:12,442 --> 00:25:14,800
a clear distinguishing between Dai

447
00:25:14,880 --> 00:25:17,536
decision active inference and Mai motor

448
00:25:17,568 --> 00:25:20,184
active inference. And beyond just

449
00:25:20,222 --> 00:25:21,928
modeling cognitive decision making

450
00:25:22,014 --> 00:25:25,352
versus motor reflex arcs, they also

451
00:25:25,406 --> 00:25:27,672
highlight how the Dai is a discrete time

452
00:25:27,726 --> 00:25:30,376
model that's using the POMDP, while the

453
00:25:30,398 --> 00:25:33,464
motor models tended to use continuous

454
00:25:33,512 --> 00:25:38,812
time. So they're laying out and

455
00:25:38,946 --> 00:25:42,252
also kind of generalizing to show

456
00:25:42,306 --> 00:25:44,944
the similarities between these two

457
00:25:44,982 --> 00:25:47,696
different variants. And we can use the

458
00:25:47,718 --> 00:25:51,004
notation concordance table to highlight

459
00:25:51,052 --> 00:25:52,320
some of those parallels.

460
00:25:53,940 --> 00:25:55,436
This is where the Taylor series

461
00:25:55,468 --> 00:25:57,236
approximation comes in and the

462
00:25:57,258 --> 00:25:58,704
generalized coordinates of motion,

463
00:25:58,752 --> 00:26:00,276
though they can also be applied to the

464
00:26:00,298 --> 00:26:04,036
discrete time. Here we

465
00:26:04,058 --> 00:26:07,984
have x, x prime

466
00:26:08,032 --> 00:26:11,092
and x double prime. The prime notation

467
00:26:11,156 --> 00:26:12,936
indicates temporal derivatives and

468
00:26:12,958 --> 00:26:16,936
second derivatives. If anyone has

469
00:26:16,958 --> 00:26:18,744
a thought on that, feel free because

470
00:26:18,862 --> 00:26:22,324
this is not doing time series prediction

471
00:26:22,452 --> 00:26:25,756
in the same way that the POMDP is. The

472
00:26:25,778 --> 00:26:29,404
POMDP is in the memory of the

473
00:26:29,442 --> 00:26:32,456
program. There's a value at the previous

474
00:26:32,488 --> 00:26:33,804
state, the current state and the next

475
00:26:33,842 --> 00:26:35,536
state and however many other states in

476
00:26:35,558 --> 00:26:38,576
the time horizon. The way that a

477
00:26:38,598 --> 00:26:40,876
continuous time Taylor series

478
00:26:40,908 --> 00:26:43,724
approximation is dealing with reduction

479
00:26:43,772 --> 00:26:46,932
of uncertainty about future data is

480
00:26:46,986 --> 00:26:48,964
quite different than the way that these

481
00:26:49,002 --> 00:26:51,270
discrete time models are doing. So

482
00:26:52,760 --> 00:26:56,596
what's happening here is the

483
00:26:56,618 --> 00:27:00,312
value of the function now is

484
00:27:00,366 --> 00:27:03,720
being estimated or provided,

485
00:27:04,460 --> 00:27:08,308
then the first derivative is calculated.

486
00:27:08,484 --> 00:27:11,456
So here three has the same structure.

487
00:27:11,588 --> 00:27:13,708
But whereas this is the hidden state at

488
00:27:13,714 --> 00:27:16,716
the next time point conditioned upon the

489
00:27:16,738 --> 00:27:19,084
hidden state at this time point. And the

490
00:27:19,122 --> 00:27:23,000
policy here's the derivative

491
00:27:23,080 --> 00:27:26,576
of x conditioned upon x and

492
00:27:26,598 --> 00:27:30,272
the policy V also

493
00:27:30,326 --> 00:27:31,644
has a slightly different interpretation

494
00:27:31,692 --> 00:27:34,556
because it's not sequence of events

495
00:27:34,588 --> 00:27:37,860
either. And analogously for the second

496
00:27:37,930 --> 00:27:40,100
derivative and the higher derivatives.

497
00:27:41,080 --> 00:27:43,460
So they're still emitting observations,

498
00:27:43,880 --> 00:27:46,820
but these are not observations at future

499
00:27:46,890 --> 00:27:50,472
time points in the same way. And that is

500
00:27:50,526 --> 00:27:54,970
visualized in this box 4.2.

501
00:27:55,500 --> 00:27:59,032
So here is x of t at

502
00:27:59,086 --> 00:28:00,728
time t. So this is like the way that the

503
00:28:00,734 --> 00:28:04,604
time series is going to go. X of t is

504
00:28:04,642 --> 00:28:06,830
just this value. Here five,

505
00:28:08,960 --> 00:28:12,556
the first term being added in

506
00:28:12,578 --> 00:28:15,344
the Taylor series expansion is the first

507
00:28:15,382 --> 00:28:18,400
derivative rate of change at that x

508
00:28:18,470 --> 00:28:21,712
zero. That's giving you a better

509
00:28:21,766 --> 00:28:25,136
approximation through time. Then the

510
00:28:25,158 --> 00:28:26,944
second derivative adds this quadratic

511
00:28:26,992 --> 00:28:31,940
feature. And so Taylor series converge

512
00:28:33,000 --> 00:28:34,580
closer and closer,

513
00:28:35,880 --> 00:28:38,644
moving further and further away from the

514
00:28:38,682 --> 00:28:42,184
target point as they include higher and

515
00:28:42,222 --> 00:28:47,736
higher derivatives. But there

516
00:28:47,758 --> 00:28:50,376
isn't an explicit calculation in the

517
00:28:50,398 --> 00:28:52,876
Taylor series of like, let's just say

518
00:28:52,898 --> 00:28:56,232
this is like one, two, three timesteps.

519
00:28:56,376 --> 00:28:59,548
It's not like, well, what is going to

520
00:28:59,554 --> 00:29:02,396
happen at three timesteps from now? The

521
00:29:02,418 --> 00:29:04,544
Taylor series could then be plugged in

522
00:29:04,582 --> 00:29:07,410
with three to calculate that.

523
00:29:08,660 --> 00:29:14,320
However, this is not calculating

524
00:29:14,820 --> 00:29:17,584
x x prime x double prime at t equals

525
00:29:17,632 --> 00:29:20,772
three, it's calculating it for x sub

526
00:29:20,826 --> 00:29:24,820
zero and then using this expansion

527
00:29:25,720 --> 00:29:28,740
to achieve reduction of uncertainty of

528
00:29:28,810 --> 00:29:30,490
more and more distal points.

529
00:29:34,420 --> 00:29:37,504
So that's quite interesting. And again,

530
00:29:37,542 --> 00:29:40,032
it's an extremely different way of doing

531
00:29:40,086 --> 00:29:43,324
time series prediction in the continuous

532
00:29:43,372 --> 00:29:46,596
time framework than this kind

533
00:29:46,618 --> 00:29:49,700
of explicit consideration of past,

534
00:29:49,770 --> 00:29:52,980
present and future state values.

535
00:29:55,980 --> 00:29:58,280
Okay, any other thoughts on 4.3?

536
00:29:58,430 --> 00:30:00,312
Because this is one of the most key

537
00:30:00,366 --> 00:30:03,624
figures, but the

538
00:30:03,662 --> 00:30:05,800
things we can explore and ask Eli.

539
00:30:08,560 --> 00:30:12,012
About this figure on page 69 it says

540
00:30:12,146 --> 00:30:14,124
the relationship between a state and its

541
00:30:14,162 --> 00:30:16,572
temporal derivative here depends on

542
00:30:16,626 --> 00:30:19,790
slowly varying causes. New,

543
00:30:21,060 --> 00:30:23,536
I just wanted to make sure, does this

544
00:30:23,558 --> 00:30:26,880
mean this slowly varying causes

545
00:30:27,620 --> 00:30:30,576
here is used to account for the

546
00:30:30,598 --> 00:30:33,952
fluctuations or it has some different

547
00:30:34,006 --> 00:30:37,140
meaning, slowly varying causes.

548
00:30:43,240 --> 00:30:47,544
So policies are

549
00:30:47,662 --> 00:30:50,872
influencing the causal unfolding of

550
00:30:50,926 --> 00:30:54,680
states in this discrete formalization

551
00:30:56,940 --> 00:31:00,540
here in the continuous time setting,

552
00:31:01,520 --> 00:31:05,144
the slowly varying causes more slowly

553
00:31:05,272 --> 00:31:08,476
than these changes play

554
00:31:08,498 --> 00:31:10,620
a similar role to policies above.

555
00:31:15,460 --> 00:31:19,104
So these causes intervene in how

556
00:31:19,142 --> 00:31:20,960
the derivatives are calculated?

557
00:31:27,670 --> 00:31:31,442
No, I mean here in the discrete time.

558
00:31:31,576 --> 00:31:34,260
Well, obviously we have,

559
00:31:35,510 --> 00:31:39,090
we can say stable policies that doesn't,

560
00:31:39,750 --> 00:31:43,334
that don't change through time, but in

561
00:31:43,372 --> 00:31:46,978
the continuous time. Well, I don't

562
00:31:46,994 --> 00:31:48,994
know, at least that was my understanding

563
00:31:49,042 --> 00:31:53,260
that the policies can slightly change

564
00:31:54,190 --> 00:31:58,214
but the change can be somehow

565
00:31:58,262 --> 00:32:02,058
negligible. And well, that's because of

566
00:32:02,224 --> 00:32:05,966
the additional term for fluctuations in

567
00:32:05,988 --> 00:32:09,630
the equations and they don't necessarily

568
00:32:10,210 --> 00:32:13,866
affect the main components

569
00:32:13,898 --> 00:32:14,990
of the equations.

570
00:32:18,630 --> 00:32:22,494
One could set it up so that the policies

571
00:32:22,542 --> 00:32:25,714
are ineffectual, so that the state

572
00:32:25,832 --> 00:32:29,346
changes through time are with

573
00:32:29,368 --> 00:32:31,166
their own endogenous dynamics, not

574
00:32:31,208 --> 00:32:33,926
influenced by policy. Or one could

575
00:32:33,948 --> 00:32:35,478
imagine a situation where the states

576
00:32:35,564 --> 00:32:37,382
don't change at all through time and

577
00:32:37,436 --> 00:32:39,126
their changes are entirely driven by

578
00:32:39,148 --> 00:32:42,026
policy. And I think analogously there

579
00:32:42,048 --> 00:32:44,314
could be a setting in which the

580
00:32:44,352 --> 00:32:48,374
derivative of x is hardly influenced

581
00:32:48,422 --> 00:32:53,050
by these v slowly varying causes or

582
00:32:53,120 --> 00:32:58,334
V might entirely describe the

583
00:32:58,372 --> 00:33:00,400
derivatives of x.

584
00:33:03,330 --> 00:33:05,294
But I'm not sure if just at this level

585
00:33:05,332 --> 00:33:09,854
of generality we can allocate

586
00:33:09,902 --> 00:33:13,790
importance to kind of the endogenous

587
00:33:13,870 --> 00:33:15,666
dynamics or like sort of the policy

588
00:33:15,768 --> 00:33:19,606
independent changes just the way

589
00:33:19,628 --> 00:33:21,766
that those states are changing or

590
00:33:21,788 --> 00:33:24,230
analogously derivatives are calculated.

591
00:33:26,890 --> 00:33:29,446
But that's a great question Lyle. And

592
00:33:29,468 --> 00:33:33,146
then Mike? Yeah, this might be

593
00:33:33,248 --> 00:33:37,290
just slightly off topic, but the tools,

594
00:33:38,190 --> 00:33:40,854
if you're using a multimode simulation

595
00:33:40,902 --> 00:33:44,234
tool, then it can

596
00:33:44,272 --> 00:33:47,278
handle both in some sense it's going to

597
00:33:47,284 --> 00:33:48,842
try and solve both continuous

598
00:33:48,906 --> 00:33:50,894
formulations and discrete time

599
00:33:50,932 --> 00:33:52,926
formulations. So one they would

600
00:33:52,948 --> 00:33:55,614
typically call agent based and the other

601
00:33:55,652 --> 00:33:58,354
would be Ode based or something like

602
00:33:58,392 --> 00:34:02,594
that. The way that they do that is

603
00:34:02,712 --> 00:34:04,338
a little bit of sleight of hand because

604
00:34:04,424 --> 00:34:06,178
of course they're not solving the

605
00:34:06,264 --> 00:34:08,626
equations per se, they're estimating the

606
00:34:08,648 --> 00:34:11,574
solutions. And if you have that,

607
00:34:11,612 --> 00:34:14,694
then if you have a slowly changing set

608
00:34:14,732 --> 00:34:16,646
of parameters like you might model with

609
00:34:16,668 --> 00:34:21,666
the discrete case, then you've

610
00:34:21,698 --> 00:34:25,046
got an event queue and those events

611
00:34:25,078 --> 00:34:27,450
are however far apart they need to be.

612
00:34:27,600 --> 00:34:29,978
Then to estimate the continuous form,

613
00:34:30,144 --> 00:34:32,806
they simply set that delta t delt.

614
00:34:32,838 --> 00:34:35,598
Delta T goes to zero in some sense. It

615
00:34:35,604 --> 00:34:37,598
never goes in the software exactly to

616
00:34:37,604 --> 00:34:40,574
zero. But then you compress that delta t

617
00:34:40,612 --> 00:34:43,694
down to very small time slices and then

618
00:34:43,732 --> 00:34:46,720
you have estimated your continuous form.

619
00:34:47,110 --> 00:34:50,482
And so by doing that, you can really

620
00:34:50,536 --> 00:34:54,242
have a pretty comprehensive way

621
00:34:54,296 --> 00:34:57,540
to represent both cases in the same

622
00:34:58,070 --> 00:35:01,880
modeling environment. Yes,

623
00:35:02,730 --> 00:35:06,262
great point. Numerical approximations to

624
00:35:06,316 --> 00:35:09,560
continuous processes can be done through

625
00:35:10,010 --> 00:35:12,246
breaking them down into discrete

626
00:35:12,278 --> 00:35:14,874
processes and then making sure that as

627
00:35:14,912 --> 00:35:16,906
your delta T is getting smaller and

628
00:35:16,928 --> 00:35:18,618
smaller, that you're getting a

629
00:35:18,624 --> 00:35:22,166
convergent estimator. The approach

630
00:35:22,198 --> 00:35:25,166
that's taken analytically here is to use

631
00:35:25,188 --> 00:35:26,990
a Taylor series approximation,

632
00:35:28,690 --> 00:35:32,510
Mike. And so in

633
00:35:32,580 --> 00:35:35,294
thinking about how we should build a

634
00:35:35,332 --> 00:35:37,262
mental model of the Taylor series

635
00:35:37,326 --> 00:35:38,946
approximation or incorporate that in our

636
00:35:38,968 --> 00:35:41,986
model, that's one example of how you

637
00:35:42,008 --> 00:35:44,610
might capture the time series structure.

638
00:35:45,190 --> 00:35:47,650
And you could potentially substitute in

639
00:35:47,800 --> 00:35:50,706
other approaches to maybe capture finer

640
00:35:50,738 --> 00:35:53,414
detail or discrete events that might

641
00:35:53,452 --> 00:35:54,950
occur on the series.

642
00:35:58,010 --> 00:35:59,000
Like what?

643
00:36:01,210 --> 00:36:04,922
Like Loaz decomposition or something

644
00:36:04,976 --> 00:36:06,694
like that, where you're taking apart

645
00:36:06,742 --> 00:36:08,634
components of the time series. Things

646
00:36:08,672 --> 00:36:12,250
like trend seasonality discrete events.

647
00:36:13,630 --> 00:36:16,234
Yes, I think the analytical comparisons

648
00:36:16,282 --> 00:36:20,266
would be tight or loose, but Coleman

649
00:36:20,298 --> 00:36:23,082
Filter, Generalized, Bayesian filtering,

650
00:36:23,226 --> 00:36:27,550
Splines, time series decomposition,

651
00:36:27,990 --> 00:36:30,866
these are all in the category or like in

652
00:36:30,888 --> 00:36:34,020
the genre of this.

653
00:36:35,430 --> 00:36:36,370
Ollie.

654
00:36:39,690 --> 00:36:41,894
By the way, Lionel, thanks for the

655
00:36:41,932 --> 00:36:44,390
clarification, that was really helpful.

656
00:36:45,530 --> 00:36:48,614
Well, to ask my question more

657
00:36:48,652 --> 00:36:51,800
explicitly, on page 78,

658
00:36:52,510 --> 00:36:56,460
equation 4.15, we have

659
00:36:58,270 --> 00:37:02,618
some additional omega terms which are

660
00:37:02,704 --> 00:37:06,014
defined as stochastic fluctuations. My

661
00:37:06,052 --> 00:37:08,762
question was that is the term slightly

662
00:37:08,826 --> 00:37:12,318
varying somehow related to these

663
00:37:12,404 --> 00:37:15,120
stochastic fluctuations or not?

664
00:37:16,870 --> 00:37:19,630
Yes, I believe that the slowly varying

665
00:37:19,790 --> 00:37:22,974
is in relationship to it being included

666
00:37:23,022 --> 00:37:26,898
with the flow component rather

667
00:37:26,984 --> 00:37:30,386
than with the at that time scale

668
00:37:30,578 --> 00:37:34,550
stochastic more thermal like vibration.

669
00:37:34,970 --> 00:37:37,334
And that was explored in like live

670
00:37:37,372 --> 00:37:40,274
stream 45 with free energy principle

671
00:37:40,322 --> 00:37:43,882
made simpler but not too simple with

672
00:37:43,936 --> 00:37:47,114
this idea of the length of

673
00:37:47,152 --> 00:37:50,426
N and how physics and mechanics are

674
00:37:50,448 --> 00:37:52,486
predicated upon the separation into flow

675
00:37:52,518 --> 00:37:56,014
like actions at a given

676
00:37:56,052 --> 00:37:59,994
scale. And then stochastic non

677
00:38:00,042 --> 00:38:03,818
flow aligned changes. And the path

678
00:38:03,834 --> 00:38:07,780
of least action is when the limit of

679
00:38:08,550 --> 00:38:11,140
the stochastic term going to zero.

680
00:38:12,630 --> 00:38:16,402
But let's come there as people

681
00:38:16,456 --> 00:38:18,146
can see, even though it's like we could

682
00:38:18,168 --> 00:38:20,774
totally read this like 20 times and it

683
00:38:20,812 --> 00:38:24,182
still is like why is the next word

684
00:38:24,236 --> 00:38:25,734
there? Why is the next equation there?

685
00:38:25,772 --> 00:38:28,406
But okay, they're introducing these two

686
00:38:28,588 --> 00:38:31,618
types of generative models that have

687
00:38:31,804 --> 00:38:34,026
kind of tantalizing isomorphisms but

688
00:38:34,048 --> 00:38:36,570
also very interesting differences.

689
00:38:37,310 --> 00:38:38,778
They're going to first focus on the

690
00:38:38,784 --> 00:38:39,740
discrete time,

691
00:38:43,390 --> 00:38:45,894
the partially observable Markov decision

692
00:38:45,942 --> 00:38:49,402
process formalism is used. Someone asked

693
00:38:49,456 --> 00:38:51,498
like why is the categorical notation

694
00:38:51,594 --> 00:38:55,022
used? It just allows it makes it easy

695
00:38:55,076 --> 00:38:57,406
to look at a matrix and to interpret the

696
00:38:57,428 --> 00:39:00,194
matrix as like a confusion matrix, not

697
00:39:00,232 --> 00:39:01,906
just a matrix that is confusing, like

698
00:39:01,928 --> 00:39:04,786
they can be, but one that has to do with

699
00:39:04,808 --> 00:39:07,502
like a coin flip. So categorical

700
00:39:07,566 --> 00:39:08,450
outcomes.

701
00:39:11,930 --> 00:39:15,970
The three nodes are the transition

702
00:39:16,050 --> 00:39:19,590
function here in the categorical context

703
00:39:20,570 --> 00:39:23,882
between different states that's this

704
00:39:23,936 --> 00:39:27,146
B. And here's where we see the D, the

705
00:39:27,168 --> 00:39:30,540
prior over the initial state and B.

706
00:39:31,070 --> 00:39:32,954
And together these account for the three

707
00:39:32,992 --> 00:39:36,286
nodes in Figure 4.3. They play a

708
00:39:36,308 --> 00:39:38,606
functionally similar role here's, where

709
00:39:38,628 --> 00:39:44,170
they get distinguished selecting

710
00:39:44,250 --> 00:39:47,618
between models of behavior. And that was

711
00:39:47,624 --> 00:39:48,786
another question which maybe we could

712
00:39:48,808 --> 00:39:52,318
get to requires

713
00:39:52,494 --> 00:39:54,830
selection amongst these categorically

714
00:39:54,910 --> 00:39:58,660
discrete plans and then

715
00:39:59,030 --> 00:40:03,174
the soft max normalizes and

716
00:40:03,212 --> 00:40:05,510
ensures that the probability over those

717
00:40:05,580 --> 00:40:08,806
policies is normalized so that they can

718
00:40:08,828 --> 00:40:11,678
be understood as a probability

719
00:40:11,794 --> 00:40:14,582
distribution, like a categorical

720
00:40:14,646 --> 00:40:16,090
probability distribution.

721
00:40:18,030 --> 00:40:20,586
Here's the expected free energy that

722
00:40:20,608 --> 00:40:21,660
we've seen before.

723
00:40:28,940 --> 00:40:32,916
More on expected

724
00:40:32,948 --> 00:40:33,770
free energy,

725
00:40:35,420 --> 00:40:38,692
specifically on how this KL divergence

726
00:40:38,756 --> 00:40:41,692
term, which as we talked about last week

727
00:40:41,746 --> 00:40:45,996
was about how the preferred states are

728
00:40:46,098 --> 00:40:47,020
realized.

729
00:40:53,420 --> 00:40:56,276
Active inference uses F and G. They're

730
00:40:56,308 --> 00:40:58,200
related, but they play different roles.

731
00:40:58,860 --> 00:41:02,292
VFE F is the primary quantity minimized

732
00:41:02,356 --> 00:41:05,540
over time. That was interesting to read

733
00:41:05,630 --> 00:41:09,148
because EFE is

734
00:41:09,234 --> 00:41:11,772
what is minimized prospectively through

735
00:41:11,826 --> 00:41:15,228
time. Whereas here

736
00:41:15,314 --> 00:41:17,452
the claim is that variational free

737
00:41:17,506 --> 00:41:22,112
energy is what is minimized over time in

738
00:41:22,166 --> 00:41:23,810
relationship to generative model.

739
00:41:25,140 --> 00:41:28,770
So again, more could be explored there.

740
00:41:30,260 --> 00:41:31,668
They are going to focus on a

741
00:41:31,674 --> 00:41:35,076
rearrangement of equation 2.6 here with

742
00:41:35,098 --> 00:41:38,432
the ambiguity and risk being juxtaposed

743
00:41:38,496 --> 00:41:40,004
with the informational value,

744
00:41:40,122 --> 00:41:43,356
information gain, infomax and Pragmatic

745
00:41:43,408 --> 00:41:44,010
value.

746
00:41:49,910 --> 00:41:53,714
They then move into

747
00:41:53,832 --> 00:41:57,462
the linear algebraic form where

748
00:41:57,516 --> 00:42:05,080
the bolds okay,

749
00:42:07,050 --> 00:42:09,800
I don't know if bold is used in the same

750
00:42:10,510 --> 00:42:14,682
way in this appendix as

751
00:42:14,736 --> 00:42:17,450
it is in these equations.

752
00:42:18,190 --> 00:42:21,834
Does anyone know

753
00:42:21,872 --> 00:42:23,566
about what bold means here? It could

754
00:42:23,588 --> 00:42:26,350
mean the vector Ali.

755
00:42:29,170 --> 00:42:33,022
Yes, I also think bold most probably

756
00:42:33,156 --> 00:42:36,900
means vector or matrix. Yes,

757
00:42:38,150 --> 00:42:40,466
it gets pretty subtle though sometimes

758
00:42:40,568 --> 00:42:41,726
because there's times where there's

759
00:42:41,758 --> 00:42:45,090
italics bold and neutral being used very

760
00:42:45,160 --> 00:42:49,122
closely. When they're used closely,

761
00:42:49,186 --> 00:42:50,566
it can be confusing and then when

762
00:42:50,588 --> 00:42:52,566
they're used not closely, it's hard to

763
00:42:52,588 --> 00:42:53,590
juxtapose.

764
00:42:56,890 --> 00:43:00,282
More expected free energy

765
00:43:00,336 --> 00:43:04,230
rewriting within the linear algebraic

766
00:43:04,390 --> 00:43:08,886
form. Softmax bringing

767
00:43:08,918 --> 00:43:10,170
back the logs.

768
00:43:12,920 --> 00:43:14,724
Variational inference rests upon

769
00:43:14,772 --> 00:43:16,808
factorization that's related to the

770
00:43:16,814 --> 00:43:19,720
sparsity of the Bayes graph. Moritz.

771
00:43:22,220 --> 00:43:25,496
Hey, so just going to 410

772
00:43:25,598 --> 00:43:28,428
again, I was wondering what's the use of

773
00:43:28,434 --> 00:43:30,380
the categorical distribution here

774
00:43:30,450 --> 00:43:33,212
because maybe I miss it, but it's not

775
00:43:33,266 --> 00:43:36,444
really explained in the text what they

776
00:43:36,482 --> 00:43:38,832
use it for. There's one sentence saying

777
00:43:38,886 --> 00:43:42,124
like oh yeah. The fifth

778
00:43:42,172 --> 00:43:44,112
line shows that the prior belief about

779
00:43:44,166 --> 00:43:45,900
observations is a categorical

780
00:43:45,980 --> 00:43:48,624
distribution. Like, what is it for?

781
00:43:48,662 --> 00:43:51,260
What does it do? I've never seen it

782
00:43:51,270 --> 00:43:51,830
before.

783
00:43:58,940 --> 00:44:01,400
I don't know if it has to be

784
00:44:01,470 --> 00:44:05,428
categorical, a preference distribution

785
00:44:05,524 --> 00:44:08,010
could be a continuous function.

786
00:44:09,360 --> 00:44:14,248
But here it's

787
00:44:14,424 --> 00:44:18,364
just simpler perhaps to show

788
00:44:18,402 --> 00:44:22,160
it as a categorical. Like, there's two

789
00:44:22,230 --> 00:44:24,272
outcomes having the food and not having

790
00:44:24,326 --> 00:44:28,460
the food. So then that's a categorical

791
00:44:28,620 --> 00:44:31,424
distribution of preferences and of

792
00:44:31,462 --> 00:44:35,076
observations. So they're just modeling a

793
00:44:35,098 --> 00:44:37,360
situation where there's categorical

794
00:44:37,520 --> 00:44:42,516
differences that are being preferred as

795
00:44:42,538 --> 00:44:46,296
opposed to, like, a preference over

796
00:44:46,318 --> 00:44:47,880
some continuous distribution.

797
00:44:50,380 --> 00:44:52,616
Does that address it? They're just

798
00:44:52,638 --> 00:44:55,688
modeling a situation where there's a

799
00:44:55,694 --> 00:44:58,784
categorical difference with observations

800
00:44:58,852 --> 00:45:01,470
and with. Preferences just because

801
00:45:02,160 --> 00:45:04,764
mathematically it's easier now to start

802
00:45:04,802 --> 00:45:08,072
with that. Yeah, I think didactically

803
00:45:08,136 --> 00:45:10,604
it's a lot clearer because you can see

804
00:45:10,642 --> 00:45:13,388
like, a two by two matrix. Did I observe

805
00:45:13,404 --> 00:45:15,616
the food or not? Did I get the food or

806
00:45:15,638 --> 00:45:18,704
not? Instead of like a distribution of

807
00:45:18,742 --> 00:45:21,984
temperature preference and a

808
00:45:22,022 --> 00:45:23,920
distribution of observations.

809
00:45:26,840 --> 00:45:29,316
I'd expect that the formalism will work

810
00:45:29,338 --> 00:45:30,656
out the same in the sense that you're

811
00:45:30,688 --> 00:45:34,084
still minimizing your surprisal and

812
00:45:34,122 --> 00:45:35,712
you're still performing distribution

813
00:45:35,776 --> 00:45:39,240
matching. But also this is like a

814
00:45:39,310 --> 00:45:41,332
distribution matching in the categorical

815
00:45:41,396 --> 00:45:43,592
context that has an interpretation of

816
00:45:43,646 --> 00:45:45,156
almost like false positive and false

817
00:45:45,188 --> 00:45:45,880
negative.

818
00:45:50,090 --> 00:45:52,166
All right. Okay. I will go with it.

819
00:45:52,188 --> 00:45:54,422
Thank you for the explanation. Yeah,

820
00:45:54,556 --> 00:45:56,040
it's a good question.

821
00:46:01,450 --> 00:46:04,534
The matrices help connect to the linear

822
00:46:04,582 --> 00:46:08,810
algebra and the MATLAB representations.

823
00:46:09,390 --> 00:46:13,034
And also just to sort of match or

824
00:46:13,072 --> 00:46:15,678
no match, like in 46, the example had to

825
00:46:15,684 --> 00:46:18,014
do with wanting ice cream and then they

826
00:46:18,052 --> 00:46:19,760
observed ice cream or not.

827
00:46:23,550 --> 00:46:27,338
Okay, POMDP. And also,

828
00:46:27,504 --> 00:46:30,606
where does this ita? We'll come back

829
00:46:30,628 --> 00:46:31,966
to that another time, but just to kind

830
00:46:31,988 --> 00:46:33,920
of get through it all. The first pass

831
00:46:35,090 --> 00:46:37,870
more details on a POMDP,

832
00:46:40,610 --> 00:46:45,166
S, and V auxiliary variable.

833
00:46:45,358 --> 00:46:50,340
I don't think this is the V here

834
00:46:52,550 --> 00:46:54,930
because we're in the POMDP setting.

835
00:46:57,130 --> 00:47:00,182
So it's just an auxiliary variable used

836
00:47:00,236 --> 00:47:03,590
as sort of an analytical convenience.

837
00:47:07,630 --> 00:47:09,386
That concludes their discussion of the

838
00:47:09,408 --> 00:47:12,700
discrete time model

839
00:47:13,710 --> 00:47:16,282
into continuous time. But first they

840
00:47:16,336 --> 00:47:19,114
describe again Markov blankets and take

841
00:47:19,152 --> 00:47:22,970
kind of a second pass. The blankets

842
00:47:23,050 --> 00:47:26,686
are the causes of X upstream, which are

843
00:47:26,708 --> 00:47:31,214
parents and the children and

844
00:47:31,252 --> 00:47:33,474
the parents of the children. The kind of

845
00:47:33,512 --> 00:47:35,010
co influencers.

846
00:47:38,110 --> 00:47:40,570
Of course, there's more to say, but

847
00:47:40,640 --> 00:47:44,460
that's what the Pearl definition is,

848
00:47:46,350 --> 00:47:48,886
how that gets mapped to sense and action

849
00:47:48,918 --> 00:47:51,038
states and what influences what's in the

850
00:47:51,044 --> 00:47:53,114
blanket, on the blanket, et cetera.

851
00:47:53,242 --> 00:47:55,614
Those are model specific. And then here

852
00:47:55,652 --> 00:47:59,998
are two common message passing schemes

853
00:48:00,094 --> 00:48:02,610
that are used for approximate inference.

854
00:48:03,990 --> 00:48:05,598
Unless the Bayes graph is fully

855
00:48:05,614 --> 00:48:07,454
connected. There is a Markov

856
00:48:07,502 --> 00:48:10,706
partitioning here. This is

857
00:48:10,728 --> 00:48:14,146
pretty funny. NB notepen a good note.

858
00:48:14,258 --> 00:48:17,426
Good note to note the slightly

859
00:48:17,458 --> 00:48:19,202
nonstandard use of the expectation

860
00:48:19,266 --> 00:48:23,462
operator, perhaps by overloading it with

861
00:48:23,516 --> 00:48:27,174
a massive subscript. But it's unclear

862
00:48:27,222 --> 00:48:32,026
what exactly they meant. There is

863
00:48:32,208 --> 00:48:34,874
showing alignment between these two

864
00:48:34,912 --> 00:48:36,934
different schemes, but there's

865
00:48:36,982 --> 00:48:38,746
definitely citations to go into, more

866
00:48:38,768 --> 00:48:41,146
into. Detail on like belief propagation

867
00:48:41,178 --> 00:48:43,934
and message passing inactive. And then

868
00:48:43,972 --> 00:48:51,010
we see several of these figures.

869
00:48:58,870 --> 00:49:01,806
Personally, I think it's slightly

870
00:49:01,918 --> 00:49:03,746
challenging to understand whether this

871
00:49:03,768 --> 00:49:07,654
is being used illustratively or

872
00:49:07,692 --> 00:49:12,550
whether these specific topologies are

873
00:49:12,700 --> 00:49:16,422
as directly interpretable as these

874
00:49:16,476 --> 00:49:17,880
topologies are.

875
00:49:21,960 --> 00:49:25,856
But here we see kind of if you blur

876
00:49:25,888 --> 00:49:28,630
your eyes here's, policy at the top.

877
00:49:29,400 --> 00:49:33,092
Here's the observations. Here's time

878
00:49:33,146 --> 00:49:37,160
minus one. Here's s minus t minus one,

879
00:49:37,310 --> 00:49:40,890
t and T plus one. So we can see

880
00:49:41,580 --> 00:49:46,424
some resonances with

881
00:49:46,542 --> 00:49:50,152
4.3. But now we're in the continuous

882
00:49:50,216 --> 00:49:53,336
time motor.

883
00:49:53,368 --> 00:49:56,796
Active inference from 46 is

884
00:49:56,818 --> 00:49:58,492
justifying the use of continuous time

885
00:49:58,546 --> 00:50:01,596
based upon the continuous unrolling

886
00:50:01,628 --> 00:50:03,676
aspect of sensory input and motor

887
00:50:03,708 --> 00:50:04,480
output.

888
00:50:07,940 --> 00:50:09,824
They're going to start in a different

889
00:50:09,862 --> 00:50:11,564
place than they did with the POMDP

890
00:50:11,612 --> 00:50:15,364
model. In this case, we have much

891
00:50:15,402 --> 00:50:18,020
more of a physics grounding.

892
00:50:18,600 --> 00:50:21,028
Again, check out live stream number 45

893
00:50:21,194 --> 00:50:23,316
to see about the lengthen and how this

894
00:50:23,338 --> 00:50:26,584
is connected. But this is a much more

895
00:50:26,622 --> 00:50:29,416
physics based grounding in terms of like

896
00:50:29,438 --> 00:50:33,092
a flow operator and a stochastic term.

897
00:50:33,156 --> 00:50:34,840
At a given timescale,

898
00:50:36,960 --> 00:50:41,900
the stochastic term

899
00:50:42,880 --> 00:50:45,112
is assumed to have a normal

900
00:50:45,176 --> 00:50:47,016
distribution. And that kind of relates

901
00:50:47,048 --> 00:50:50,088
to what Mike said about detrending. One

902
00:50:50,114 --> 00:50:53,024
can think of this stochastic term as

903
00:50:53,062 --> 00:50:54,860
being what happens when you've detrended

904
00:50:54,940 --> 00:50:56,940
all of the signal that's non Gaussian

905
00:50:57,020 --> 00:51:00,240
out. You're left with the residuals

906
00:51:00,660 --> 00:51:07,220
which have a gaussian nature.

907
00:51:08,920 --> 00:51:12,836
Precision is capital pi and that's the

908
00:51:12,858 --> 00:51:15,896
inverse of the fluctuation. And so

909
00:51:15,918 --> 00:51:17,528
that's they're going to connect that to

910
00:51:17,534 --> 00:51:18,840
the common filtering.

911
00:51:21,020 --> 00:51:23,476
Here we return again to the generalized

912
00:51:23,508 --> 00:51:29,900
coordinates of motion coming

913
00:51:29,970 --> 00:51:33,000
from this Taylor Series approximation

914
00:51:33,160 --> 00:51:35,804
route. So if somebody said I have ten

915
00:51:35,842 --> 00:51:37,404
numbers and we need to predict ten years

916
00:51:37,442 --> 00:51:40,076
in the future, the discrete time way

917
00:51:40,098 --> 00:51:41,376
would be like, well, let's try to

918
00:51:41,398 --> 00:51:43,536
predict it at each of those ten years

919
00:51:43,718 --> 00:51:46,624
and those will be your ten numbers. The

920
00:51:46,662 --> 00:51:48,524
generalized coordinates of motion's

921
00:51:48,572 --> 00:51:52,788
approach which we explored the most in

922
00:51:52,954 --> 00:51:56,224
live stream number 26 with the Costa

923
00:51:56,352 --> 00:51:59,140
et al. In Bayesian mechanics,

924
00:52:00,440 --> 00:52:03,988
the ten numbers could be like the value

925
00:52:04,074 --> 00:52:06,756
today and the derivative today, the

926
00:52:06,778 --> 00:52:08,088
second derivative and the third and the

927
00:52:08,094 --> 00:52:09,768
fourth and the fifth up to however many

928
00:52:09,854 --> 00:52:11,336
that's the generalized coordinates of

929
00:52:11,358 --> 00:52:14,744
motion. So it's like a snapshot of the

930
00:52:14,782 --> 00:52:16,392
process and all of its higher

931
00:52:16,446 --> 00:52:19,064
derivatives. And so the generalized

932
00:52:19,112 --> 00:52:20,620
coordinates of motion are very similar

933
00:52:20,690 --> 00:52:23,356
to a Taylor Series approximations in how

934
00:52:23,378 --> 00:52:25,724
they represent centered at a certain

935
00:52:25,762 --> 00:52:30,680
point x naught how one expects

936
00:52:30,840 --> 00:52:33,532
as movement happens away from that x

937
00:52:33,586 --> 00:52:36,944
knot, reducing surprise. And so here is

938
00:52:36,982 --> 00:52:40,784
like x with a dot on top

939
00:52:40,902 --> 00:52:42,916
is the change in x. And then this is

940
00:52:42,938 --> 00:52:44,644
like the derivative of the change in x

941
00:52:44,682 --> 00:52:48,704
and so on. Here the Tilde notation

942
00:52:48,752 --> 00:52:50,816
is used for the generalized coordinates

943
00:52:50,848 --> 00:52:54,544
of motion. So we see

944
00:52:54,682 --> 00:52:59,050
this equation which was just for one

945
00:53:00,140 --> 00:53:04,184
value, like where

946
00:53:04,222 --> 00:53:07,916
you are on the freeway. And then this is

947
00:53:07,938 --> 00:53:09,820
the generalized coordinates of motion.

948
00:53:12,880 --> 00:53:17,070
The free energy is written down for

949
00:53:18,560 --> 00:53:21,836
this generalized coordinates

950
00:53:21,868 --> 00:53:24,640
of motion with precision.

951
00:53:25,060 --> 00:53:27,616
Here. The closest that we came to

952
00:53:27,638 --> 00:53:31,516
exploring it was in 43 on predictive

953
00:53:31,548 --> 00:53:32,320
coding.

954
00:53:38,130 --> 00:53:40,530
There are some more details on Gaussian,

955
00:53:40,870 --> 00:53:43,090
the relationship between like, Gaussian

956
00:53:44,150 --> 00:53:48,774
processes and also,

957
00:53:48,892 --> 00:53:52,374
I think Lyle brought up this sort of

958
00:53:52,492 --> 00:53:55,590
mode and multimode simulations.

959
00:53:59,630 --> 00:54:02,694
This is modeling a single mode.

960
00:54:02,822 --> 00:54:05,254
Not that it can't model a bimodal

961
00:54:05,382 --> 00:54:08,666
distribution, but this is where the

962
00:54:08,688 --> 00:54:11,020
LaPlace approximation comes into play.

963
00:54:12,530 --> 00:54:15,470
This is potentially even unnecessarily

964
00:54:15,970 --> 00:54:19,520
complex, but it's there.

965
00:54:19,890 --> 00:54:24,194
And the LaPlace approximation is

966
00:54:24,312 --> 00:54:28,450
fitting a quadratic

967
00:54:30,230 --> 00:54:34,034
distribution that tracks the

968
00:54:34,072 --> 00:54:37,874
mode. So like the highest

969
00:54:37,922 --> 00:54:41,174
point on the distribution. And then it

970
00:54:41,212 --> 00:54:44,120
fits a parabola around that.

971
00:54:50,590 --> 00:54:54,870
Here is more details on the hierarchical

972
00:54:54,950 --> 00:54:58,166
nature and the kind of multi

973
00:54:58,198 --> 00:55:01,210
timescale nature that's implied

974
00:55:04,190 --> 00:55:06,410
ultimately by the lengthen.

975
00:55:09,150 --> 00:55:14,638
And then there's it'd

976
00:55:14,654 --> 00:55:16,818
be good to juxtapose figure four, six,

977
00:55:16,904 --> 00:55:20,706
and four to see

978
00:55:20,728 --> 00:55:22,420
how they're similar and different.

979
00:55:23,590 --> 00:55:25,454
But here we see message passing

980
00:55:25,502 --> 00:55:27,326
happening on the generalized predictive

981
00:55:27,358 --> 00:55:29,566
coding architecture. And that was like,

982
00:55:29,608 --> 00:55:31,462
explored a lot more, not with this exact

983
00:55:31,516 --> 00:55:33,606
figure, but this was explored more from

984
00:55:33,628 --> 00:55:35,042
like an analytical and empirical

985
00:55:35,106 --> 00:55:36,600
perspective in 43.

986
00:55:41,410 --> 00:55:44,450
Then there's an abrupt ending,

987
00:55:48,710 --> 00:55:50,802
but the following chapters are going to

988
00:55:50,856 --> 00:55:54,340
appeal to the formalisms and apply them.

989
00:56:00,330 --> 00:56:03,954
So, any thoughts or ideas

990
00:56:04,002 --> 00:56:07,000
on chapter four as we close out,

991
00:56:08,010 --> 00:56:11,706
and hopefully next week, we can have a

992
00:56:11,728 --> 00:56:13,690
lot of questions and things like that,

993
00:56:13,840 --> 00:56:16,906
even basic questions. It's like if you

994
00:56:16,928 --> 00:56:18,918
read it and you understood it, then it'd

995
00:56:18,934 --> 00:56:21,274
be awesome to contribute a question that

996
00:56:21,312 --> 00:56:22,954
would explore someone else's

997
00:56:23,002 --> 00:56:26,174
understanding or prompt towards how you

998
00:56:26,212 --> 00:56:27,486
thought about it in a way that made

999
00:56:27,508 --> 00:56:29,386
sense for you. And if you don't

1000
00:56:29,418 --> 00:56:31,486
understand it, then just ask the

1001
00:56:31,508 --> 00:56:44,756
question Ali

1002
00:56:44,788 --> 00:56:46,090
and then anyone else?

1003
00:56:48,460 --> 00:56:51,276
I just wanted to briefly mention an

1004
00:56:51,298 --> 00:56:54,124
additional point related to the question

1005
00:56:54,242 --> 00:56:57,340
about the reason behind using

1006
00:56:57,410 --> 00:56:59,980
the categorical distribution. On page

1007
00:57:00,050 --> 00:57:03,470
74, it says

1008
00:57:05,040 --> 00:57:06,636
the fifth line shows that the prior

1009
00:57:06,668 --> 00:57:08,256
belief about the observations is a

1010
00:57:08,278 --> 00:57:09,804
categorical distribution whose

1011
00:57:09,852 --> 00:57:12,224
sufficient statistics are given in the C

1012
00:57:12,262 --> 00:57:13,852
vector. I think here, sufficient

1013
00:57:13,916 --> 00:57:16,580
statistics is the key term to understand

1014
00:57:16,650 --> 00:57:18,624
the reason behind using categorical

1015
00:57:18,672 --> 00:57:22,672
distributions, because for insufficient

1016
00:57:22,736 --> 00:57:27,030
statistics, we don't necessarily use

1017
00:57:28,780 --> 00:57:31,640
the exact modeling or distributions.

1018
00:57:32,140 --> 00:57:35,764
Instead, we use a substituted statistics

1019
00:57:35,892 --> 00:57:38,744
that is simple enough to calculate and

1020
00:57:38,782 --> 00:57:42,668
also close enough to the actual

1021
00:57:42,834 --> 00:57:47,150
data. So I think that can help

1022
00:57:47,760 --> 00:57:50,044
in understanding the reason behind that

1023
00:57:50,082 --> 00:57:50,860
decision.

1024
00:57:55,010 --> 00:57:58,786
Good point, and this

1025
00:57:58,808 --> 00:58:01,694
is definitely a technical note,

1026
00:58:01,742 --> 00:58:05,186
but is it fair to say that the mean and

1027
00:58:05,208 --> 00:58:06,974
the variance are sufficient statistics

1028
00:58:07,022 --> 00:58:08,690
for a Gaussian distribution?

1029
00:58:11,610 --> 00:58:13,240
Yes, I think so.

1030
00:58:16,170 --> 00:58:20,082
That's one of the perhaps

1031
00:58:20,146 --> 00:58:23,002
proximate mechanisms by which the

1032
00:58:23,136 --> 00:58:25,878
LaPlace approximation and variational

1033
00:58:25,894 --> 00:58:29,370
inference more generally are

1034
00:58:29,440 --> 00:58:33,870
able to deal with arbitrary true

1035
00:58:33,940 --> 00:58:37,470
distributions by fitting a family

1036
00:58:37,540 --> 00:58:41,194
of distributions that have tractable

1037
00:58:41,242 --> 00:58:43,834
optimization structure and a vastly

1038
00:58:43,882 --> 00:58:46,110
reduced set of sufficient statistics.

1039
00:58:46,450 --> 00:58:48,826
If you had even just a simple bimodal

1040
00:58:48,858 --> 00:58:50,674
distribution, well, there'd be like the

1041
00:58:50,712 --> 00:58:53,182
location of the two peaks, the relative

1042
00:58:53,246 --> 00:58:55,614
heights of the two peaks, the skewedness

1043
00:58:55,662 --> 00:58:57,378
and all the you could have many, many

1044
00:58:57,464 --> 00:59:00,774
parameters needed to describe it. In

1045
00:59:00,812 --> 00:59:03,222
contrast, the LaPlace approximation only

1046
00:59:03,276 --> 00:59:06,214
requires the location of the mode and

1047
00:59:06,252 --> 00:59:11,154
the variance estimate. And so where

1048
00:59:11,212 --> 00:59:14,700
that's adequate, it's simple and fast.

1049
00:59:15,630 --> 00:59:18,250
Where it's inadequate, you'll at least

1050
00:59:18,320 --> 00:59:21,194
know because you'll continue to be

1051
00:59:21,232 --> 00:59:25,040
surprised at new observations coming in.

1052
00:59:29,570 --> 00:59:32,462
Okay, any final comments on this?

1053
00:59:32,596 --> 00:59:34,240
Chapter four, part one.

1054
00:59:40,500 --> 00:59:43,312
Definitely a challenging, though

1055
00:59:43,366 --> 00:59:45,680
interesting chapter,

1056
00:59:49,630 --> 00:59:53,690
and it really is at the heart of active.

1057
00:59:55,570 --> 00:59:57,406
And as we hopefully explored a little

1058
00:59:57,428 --> 00:59:59,818
bit today, it includes, like, basic

1059
00:59:59,994 --> 01:00:01,630
Intuition pumps,

1060
01:00:04,050 --> 01:00:07,642
as well as some Rosetta Stone

1061
01:00:07,706 --> 01:00:11,362
like representations and many,

1062
01:00:11,416 --> 01:00:14,722
many equations which

1063
01:00:14,776 --> 01:00:18,082
I hope we can unpack. So could someone

1064
01:00:18,136 --> 01:00:20,690
explain it in simple words? It's what

1065
01:00:20,760 --> 01:00:22,722
we've been asking for every single

1066
01:00:22,856 --> 01:00:25,314
equation, and it's something that

1067
01:00:25,352 --> 01:00:27,954
everyone can contribute to. All right,

1068
01:00:27,992 --> 01:00:31,678
well, looking forward to people's edits

1069
01:00:31,774 --> 01:00:35,130
over the coming week and additions,

1070
01:00:35,710 --> 01:00:40,154
and we'll come back in one

1071
01:00:40,192 --> 01:00:42,506
week for part two on four.



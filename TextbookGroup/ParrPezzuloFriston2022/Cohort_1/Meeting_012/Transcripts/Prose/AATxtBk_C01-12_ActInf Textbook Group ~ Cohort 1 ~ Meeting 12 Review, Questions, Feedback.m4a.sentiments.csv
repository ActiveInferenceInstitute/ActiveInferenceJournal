start	end	speaker	sentiment	confidence	text
1370	2286	A	0.5911835432052612	Hey, everyone.
2468	10320	A	0.7087887525558472	Welcome to week twelve of active textbook group cohort one.
11410	15840	A	0.9078975915908813	We are in the Review and Synthesis weeks.
16930	26358	A	0.9715850353240967	It's been a great three monthish journey and it doesn't stop here but we'll will go over that.
26444	37694	A	0.8563459515571594	So two points of process for those who are here live and of course re watching and then we'll move to the more conceptual parts.
37842	43530	A	0.9257903099060059	So first in the onboarding page you'll see these two columns.
43950	54400	A	0.776467502117157	The two columns are Rcping yes to join Cohort Two, part one going through chapters one through five if you would like to for a second time.
54850	60622	A	0.541056215763092	Many people will be just doing it for fun or for learning or however they want.
60756	79720	A	0.923957884311676	And also there's the opportunity to continue on with the second half of the textbook which is going to get into a lot of the hands on modeling and we'll be especially looking forward to developing a lot of the notebooks and interactive formats that will help us get a lot of understanding there.
80650	95980	A	0.9081225991249084	So please check yes for whichever row your name is in for whether you would like to continue on with part two as well as retake part one as well.
96350	110990	A	0.8274005055427551	You'd be welcome to be just a participant and also feel free to get in touch if you are interested in taking some other role such as like facilitating or scaffolding some aspect but we can talk more about that for those who are interested.
111140	118740	A	0.8753398656845093	So point one go to the onboarding list and continue on however you want.
120070	131160	A	0.9113137125968933	Second point if you go to Future Textbook Groups page you'll see an embedded form as well as a link to the form if you want it in a new window like that.
132010	144570	A	0.7841678261756897	This would be exceptionally requested and helpful for you to provide some evaluations numerically and as short or as long as you want on providing feedback.
145630	149462	A	0.6338858008384705	Those are one way that you can provide feedback anonymously.
149606	155760	A	0.730182409286499	The second way is building on some of the ideas that people are already adding here.
156290	166430	A	0.8135751485824585	This is an editable page, so people can add thoughts as they see fit about what would be awesome for future textbook groups or any other feedback.
166510	173780	A	0.8226329684257507	Or, as always, they can email Activeinference@gmail.com if they want, like a response to any specific points.
174710	179986	A	0.7453851699829102	And then also it's provided here at the link to Share for people who want to join future cohorts.
180098	189382	A	0.8433099389076233	So that link will be like getting people onboarded into the next cohort which is September 2022 in this case.
189436	204160	A	0.526036262512207	But this is going to be like an evergreen form that will just continue to have a list of people who expressed interest and be onboarding them into subsequent cohorts of which will begin several per year going forward.
208370	219060	A	0.7386430501937866	And next week we'll be talking more about the feedback specifically and about projects and about carrying on.
219590	231174	A	0.8280433416366577	Today is like a little bit of a more conceptual synthesis and review and again next week will be more like logistical and project oriented review.
231292	244330	A	0.7966376543045044	So just on these points or anything else, does anyone want to just raise their hand or unmute and share anything that they liked?
259840	278800	A	0.9005770087242126	Okay, let us conceptually review, then we read the first five chapters of the active inference textbook.
282350	288538	A	0.9224703907966614	Does anyone have overall thoughts that they'd like to provide at the five chapter scale?
288714	291130	A	0.892845630645752	Then we're going to go into a chapter scale.
291290	297258	A	0.8611676096916199	Then we'll be continuing to dive in as Granularly as required.
297434	307380	A	0.9137734174728394	But at the scale of the first five chapters, you can see them here on this preface page as well as here in the chapters list.
309510	315000	A	0.8430867791175842	As a unit of five chapters, what were people's sense?
323270	324478	A	0.8270262479782104	How did it update?
324574	325726	A	0.6016400456428528	Yes, please, Ben.
325838	327086	A	0.7610263228416443	And then Ali.
327278	341814	B	0.6381675601005554	Well, I was just going to say, I think, as I've said to people before, I think, coming from a non mathematics background, I thought that these chapters were really accessible in terms of the kind of conceptual things that they were putting on the table.
341862	361118	B	0.8567330837249756	But I just wondered what other people's opinions were in terms of understanding the conceptual toolkit of active inference before you've built up a kind of mathematical understanding, because I'm about to start kind of learning some of the mathematics behind it.
361204	369346	B	0.8341427445411682	And I wondered what people thought of the potential to fully grasp these concepts without the mathematics and what the relationship between the two might be.
369368	371860	B	0.8466570973396301	I'm just curious what other people think about that, I guess.
373510	373970	A	0.6283750534057617	Thanks.
374040	379400	A	0.6386758089065552	Great question, Ali and then Mike and then anyone else who wants to chime in.
383820	399660	C	0.970596432685852	Yeah, it was a pretty exciting journey for me, but I'll definitely need to read over all those five chapters once again, at least once again in order to grasp more fully the contents of those chapters.
400560	425316	C	0.518761157989502	My own personal opinion about the way Materials is organized in the first part of the textbook is I couldn't see a kind of hierarchical or, I don't know, structural organization for these chapters as much as I like to.
425338	438968	C	0.5701587200164795	See them because sometimes they delve into theoretical aspect of all the things much more deeply than the mathematical side.
439054	441332	C	0.8086736798286438	And sometimes it's vice versa.
441476	473204	C	0.7474037408828735	But I think that if it was organized in a way that, as Ben mentioned, we can go from a firm conceptual foundation into the granular formalism and mathematics, at least for me, that would be much more accessible and much more I would be able to organize the materials in a much more coherent way in my mind.
473322	508770	C	0.9716123938560486	So one thing that I really enjoyed about these chapters is the kind of you see a kind of the vision it tries to put forward regarding the future possible research and also things especially in chapter five and the path that can be taken from this point on.
510260	511452	A	0.8810703158378601	Thank you, Ali.
511596	513410	A	0.8453531861305237	Mike and then anyone else?
514900	525540	D	0.7705792784690857	Yeah, I'll pick up from where Ali left off, I think it's somewhat remarkable that they were able to fit everything they did into those first five chapters.
527080	528624	D	0.5415648818016052	And understanding.
528752	537716	D	0.8589383959770203	They probably had some idea of how much space they wanted in that part of the book before getting into the practical or applied components.
537748	549500	D	0.6097476482391357	In part two, the high road, low road contrast and sort of coming at the problem from two different angles I think was useful.
552560	566576	D	0.5737605690956116	Like Ben, I found the math to be generally accessible, although I confess I think I still go through this sort of pattern where I feel like I get it and I have this intuitive understanding of it.
566598	585064	D	0.5451002717018127	But then there are things where I sort of bogged down and looking at some of the mathematical details or some of the graphical representations related to things like message passing have to slow down and sort of take those apart more.
585182	594908	D	0.8469095826148987	And I guess related to that, I should add, I think this group has been so good at taking apart the ideas of the book.
594994	603436	D	0.9614554047584534	This is perhaps one of the most effective settings I've seen for really wringing concepts out of a text.
603538	613968	D	0.9661307334899902	And the work that's going on with the coda to build up the ontology and take apart the equations and things like that I think is just tremendous awesome.
614054	614850	A	0.8529649972915649	Thank you.
619560	624820	A	0.6464803814888	The British call them maths, plural instead of math as an area.
624970	627460	A	0.6635907292366028	And it really does engage.
628440	638584	A	0.8896418213844299	Like there's visual formalisms which could be represented with a sparsity matrix or it could be represented with other ways.
638782	665280	A	0.8509673476219177	But there are almost like a multiscale diversity of formalisms ranging from more traditional equal sign in the middle mathematics to fusion, schematic, equation graphics, different types of notations even within the equation.
666840	673840	A	0.5105790495872498	And the difficulty is not signaled.
673920	697470	A	0.6778732538223267	Not that it has to be, but the difficulty does provide a little whiplash sometimes because it moves variously in the main thread of the text, the boxes and the appendices from like, this is how linear algebra works and this is what Bayes theorem is all the way to.
698480	713120	A	0.8841190338134766	Topics that are approaching physics flows on partitioned states, various kinds of fundamental, or like in principle, relationships about mathematics generally.
715300	733240	A	0.8727359771728516	All the way to postulated architectures where message passing is implemented to achieve some computational function and or thread the needle with resembling neuro computational architectures.
734220	736490	A	0.7565041780471802	The math is doing a lot.
737260	756348	A	0.5909866690635681	Many things are happening and so it's only to be expected that the perception is challenging and there's probably a lot more to even say and unpack there.
756514	763196	A	0.9155354499816895	Anyone else want to raise their hand and give a thought on this section?
763228	764130	A	0.7045059204101562	One Area.
774760	776630	A	0.7465029358863831	JF and Then anyone Else.
778440	779028	C	0.5358616709709167	Have kind.
779034	799420	E	0.785062849521637	Of A general Question about The Limits Of Mathematics, the Notion Of Computational irreducibility that You Have To Go Through Every Step Of A Process to Get At State n plus.
799490	804990	E	0.8029202222824097	One from state n from state n minus whatever.
805440	817552	E	0.4883642792701721	That certain problems cannot be solved by plugging values into a formula and generating a future state.
817606	819970	E	0.7527374029159546	You have to compute all the intermediate states.
821620	825700	E	0.8558633923530579	I wonder where that fits in in this picture.
826040	835200	E	0.8133968710899353	Are there hard limits to the application of mathematics to the problem of active inference?
835280	836810	E	0.8162118196487427	It's just a general question.
854650	880750	A	0.6026697754859924	Returning to some earlier comments on the firm conceptual ground, what is an area, no matter how narrow or distal, from active, where people believe that there is a firm conceptual ground, they felt like that provided a foundation for them to learn further.
881970	889950	A	0.892276406288147	Are we seeking an analogy to some other field of theory or practice or domain?
890470	897860	A	0.7039602398872375	Or are we kind of seeking for an epistemic territory that we haven't quite seen?
910090	930778	F	0.5713722705841064	I'll just add I think that what inactive inference, what you're talking about there, Jeff, is literally blankets and uncertainty, like the corollary there of, like, computation and irreducibility is that there's some large amount of uncertainty in hidden states in the world that you're just never going to get at.
930944	948100	F	0.7867384552955627	The blanket is in some sense not just how you're relating to it, but some hard limit on your surprisal and things like that.
951160	951572	A	0.6283750534057617	Thanks.
951626	956230	A	0.6303179860115051	Brock Blue had written yes, Mike, please.
958920	972032	D	0.8057858943939209	So responding to the question about maybe adjacent SpaceX or spaces or topic areas, I think there's certainly overlap with concepts from system dynamics.
972116	980910	D	0.7791405916213989	And certainly a lot of agent based modeling has been applied to model systems that have feedback loops and adaptation, things like that.
981760	987136	D	0.875276505947113	We discussed during the past weeks relationships with reinforcement learning.
987238	1010090	D	0.7290783524513245	And I think there's a belief that active inference kind of cleans up some of the challenging, or maybe not challenging, but less formalized or less defined aspects related to reinforcement learning, such as how do you engineer a utility function that makes sense in the context of what you're trying to model?
1020170	1027240	A	0.6493790745735168	Thanks agreed about creating an integrated utility function.
1028350	1062414	A	0.5277900099754333	There's so many unprincipled, not to say ineffective or not even to say not elegant but unprincipled methods of creating a chimera utility model like delay, discounting, novelty, bonus, curiosity, bonuses, alternating phases or hyperparameters, as well as purely implementational strategies like discarding burn in parallel chains.
1062542	1084540	A	0.6290371417999268	There's like a whole toolkit and indeed fields on creating effectively integrated utility models and putting that work into the construction of the generative model.
1085630	1101070	A	0.865696907043457	And how the generative model is partitioned from the generative process allows a generic free energy functional or set of related free energy formulations to play that role.
1103110	1125400	A	0.8006736040115356	So that's one very interesting angle, as well as the way that that process of specifying the generative model and generative processes and so on are world models, whether in the Yon Lacoun sense or in the Adam Safron sense.
1126330	1134780	A	0.6232970952987671	These generative models encompass world models and deep learning approaches and various topics like that.
1137550	1139340	A	0.6172739863395691	So let's see.
1139790	1154366	A	0.7126575708389282	Blue wrote, I wish there was practice problems in the book to develop our understanding yes, to a large extent.
1154478	1162050	A	0.8038697838783264	And also whether people want to reflect on any specific questions or even just on this scheme.
1162790	1165300	A	0.7909231185913086	These are many of these questions.
1166410	1175910	A	0.8161258101463318	These questions can be open, infinite game type questions about the material.
1176410	1179770	A	0.7226802110671997	They can be clearly addressable questions about the material.
1180190	1194430	A	0.5151346325874329	They can be checks for understanding, but we'll be continuing to develop and improve and curate questions around the material.
1195010	1226374	A	0.9115681052207947	So I think it's a great comment and this is why we have this future textbook groups and all of these affordances for people to stay involved and be improving this, as well as experimenting in their own spaces and ways to develop the kinds of material that are helping them learn and understand any more.
1226492	1255780	A	0.8463214635848999	Section one overview thoughts, chapters one through five and the framing of the book being that the first five chapters are more focused on the conceptual background and the second five chapters are going to be starting with a recipe for designing the active models and getting more into the modeling itself.
1258150	1291580	A	0.9115342497825623	So any section one through five commentary the book comprises two parts, page three.
1291650	1301170	A	0.8217864632606506	These are aimed at readers who want to understand active inference first part, and those who seek to use it for their own research second part.
1301780	1310080	A	0.8370829820632935	The first part of the book introduces active inference, both conceptually and formally contextualizing it within current theories of cognition.
1314600	1318884	A	0.8528507947921753	How did they succeed and where?
1318922	1339960	A	0.8670644760131836	Was there a divergence between what you preferred a priori or now comprehensive, formal and self contained introduction to active inference, its main constructs and implications for the study of brain and cognition?
1353560	1366650	B	0.625063955783844	I'm interested in the distinction between understanding active inference and using it for our own research and the way that that's set up as those being two completely separate things.
1370220	1379564	B	0.554948627948761	It seems to suggest that one could fully understand active inference and yet not be in a position to use it in their own research, not having worked through the second half of the book.
1379602	1380930	B	0.9643256664276123	And I think that's quite interesting.
1385220	1386160	A	0.6054749488830566	It is.
1386310	1387292	A	0.7387918829917908	Good comment.
1387356	1388210	A	0.8529649972915649	Thank you.
1390340	1399750	A	0.5840855836868286	Not to spoil the ending, but the last sentence is ultimately we are confident that you will continue to pursue active inference in some form.
1403720	1417690	A	0.8583440184593201	There may be a multi year incubation for different people in contexts many people now or even in some amazing decentralized science future.
1418000	1428300	A	0.6172245740890503	They may not think of themselves as researchers or of doing research, though they may be even included on research projects.
1429700	1454340	A	0.888595700263977	So it's a very interesting comment about how they distinguish theory and learning from part two, research and practice, however they discuss applications.
1457430	1464850	A	0.5974555611610413	But this isn't the playbook, it's not the toolkit, it's not the modeling tool ecosystem.
1465670	1466530	A	0.5794492959976196	Mike.
1468870	1481386	D	0.5354386568069458	Yeah, that's an interesting comment at the end, and I don't know if this is my perceptual bias, but there seems to be this implicit assumption that active inference is the way to go.
1481488	1486700	D	0.8698784112930298	Basically, this is the tool to use within the text.
1488590	1503280	D	0.5817446708679199	As a result, what's not there is a discussion of maybe these are cases where you might not want to use active inference, or these are cases where applying active inference could create challenges in what you're solving for.
1511370	1513500	A	0.7131232619285583	Ali and anyone else.
1515950	1527120	C	0.5786136388778687	And in fact, one of the most popular criticisms put toward the active inference or FEP in general is that it tries to explain everything.
1527810	1529840	C	0.7906370759010315	It ends up explaining nothing.
1533970	1551000	D	0.5723398327827454	Yeah, I think there's a risk of maybe not sufficiently constraining the problem space or leaving things undefined in a way that allow for active insurance to be the solution without a critical point of view.
1576090	1582134	A	0.801384687423706	They highlight behavior and cognition.
1582262	1608542	A	0.7838183045387268	I mean, one could almost see even the name active inference as being a synthesis of action behavior and cognition inference in ways that as we're discovering and unpacking, they're integrated in ways that other formalisms of perception as a type of cognition.
1608606	1621906	A	0.5689792633056641	So we could just lump it there other frameworks of behavior and cognition have not integrated or have approached from a non first principles.
1622018	1627570	A	0.5007504224777222	Or maybe they do follow first principles, like the higher the impact factor, the better the paper, something like that.
1627660	1628890	A	0.7242389917373657	That's a first principle.
1629870	1631290	A	0.7223573923110962	No, just kidding.
1633070	1656710	A	0.7856739163398743	But placing it within the framework of cognition while also surfing on a wave of something like Pan cognitivism or like pan computationalism does expand the scope.
1658170	1684720	A	0.7935330271720886	And that's even before one starts to enter into the more recent research, especially where there is a highlight on system persistence, not simply in terms of the resistance to dissipation entropically, but in a relational context as repeated measurements, like in the quantum work.
1686930	1689610	A	0.7671478390693665	So it's like cognition.
1689690	1693780	A	0.7248213291168213	Oh, like brains, like that schema with the brain or with like a person.
1695110	1704790	A	0.8032596707344055	Well, oh, by cognition, it's any system with a blanket that we're interacting with through repeated measurement.
1705850	1707186	A	0.8050835728645325	Is that cognition?
1707298	1709174	A	0.6566193699836731	Where did cognition go?
1709372	1710230	A	0.6659750938415527	Brock.
1714090	1723610	F	0.8219224810600281	I kind of want to, I guess, echo all those things that were just said about a common criticism and this where does it apply?
1723680	1724570	F	0.5212488174438477	Where does it not apply?
1724640	1730510	F	0.6812347769737244	But also, I guess all those things that you were just listing.
1731890	1766150	F	0.7486667633056641	I always hear that from people that have not engaged with the material and from my engagement with it, it seems like all of the examples in which it is used are complex, dynamical systems that don't have great, that have, at best, mathematical approximations and have almost no or literally no kind of analytical approaches.
1767770	1775270	F	0.8605340123176575	It just seems like a good way to model a system of things, of entities that are interacting.
1776810	1793778	F	0.7298882007598877	And I guess what I'm to put it into one question is that kind of some anthropomorphizing or whatever projecting of like, well, it's applied to this, it's applied to that, it must apply to everything.
1793864	1796562	F	0.6908389925956726	And how can it apply to all those things?
1796616	1797934	F	0.7986425757408142	How can it apply everywhere?
1798062	1799246	F	0.5946890115737915	It's not applying to everywhere.
1799278	1809480	F	0.6640523672103882	It's just applying to these complex systems which are everywhere but also don't have and we're not trying to model the apple falling from the tree or.
1811290	1811654	A	0.6959936022758484	These.
1811692	1813174	F	0.8214014172554016	Sorts of simple cases or something.
1813212	1816422	F	0.552643358707428	It's not being applied there, is it?
1816556	1839406	F	0.6234981417655945	I haven't seen actually, now that I think about it, any of those sorts of situations where it's applied to things that we already understand, maybe in some back checking way, but not in a way that's suggested that we should use this much more complex approach to something that's kind of check.
1839508	1841040	F	0.571941614151001	Already got that.
1845510	1846478	A	0.766993522644043	Thanks, Brock.
1846574	1847410	A	0.6077884435653687	Ali.
1849910	1866002	C	0.7972257137298584	I kind of feel that some of these criticisms stems from the fact from a serious lack of deep understanding of concepts related to FEP and active inference.
1866146	1881962	C	0.7877819538116455	For example, I came across a PhD thesis just today in which it tries to critique everything related to Markup Blanket.
1882106	1895662	C	0.4971597194671631	But as I was glancing over it, I observed some serious gaps in our argument throughout all the thesis.
1895726	1911160	C	0.7735488414764404	And I think materials such as this book can definitely help in filling up these gaps of understanding which even the serious researchers are suffering from.
1914180	1915330	A	0.8315922617912292	Yes, thanks.
1916100	1924310	A	0.636430025100708	So to this thread, I've never heard it about the linear model.
1925320	1928500	A	0.5016050934791565	How could the linear model be used in so many settings?
1930300	1931690	A	0.7035629153251648	It explains nothing.
1932300	1940920	A	0.7603899836540222	Well, is the linear model an explanation or is it an investigative tool in the investigator's toolbox?
1941900	1949580	A	0.787902295589447	So at least for me personally, doesn't need to have broader reach than those who resonate with it.
1949650	1957090	A	0.7738701105117798	But again, I always try to ask, would somebody say this or could they say that about a linear model?
1957700	1973910	A	0.8932592272758484	And then if the active formalism with f equals dot dot dot dot dot or g equals dot dot dot dot dot, and all of the predicates that it's associated with and the partition and all of that, could someone say that about y equals MX plus b?
1974600	1982068	A	0.7006747722625732	And then if not, and there may be cases where it is not the same, what is it that's different about active inference?
1982244	2002440	A	0.7952749133110046	Or is it a valid and important and even reasonable question, but one that might be bumping up like what JF raised about computational irreducibility or about map territory even disguised or camouflaged questions about relationality?
2002600	2005920	A	0.6165561676025391	Well, how can complexity science apply to so many topics?
2007300	2011090	A	0.7236040234565735	We're applying complexity science to whatever topic we want to.
2011780	2013090	A	0.634149968624115	Where's the issue?
2015940	2019296	A	0.7992550134658813	Mike and then ron yeah, a lot.
2019318	2026020	D	0.8506989479064941	Of it hinges on how the model is to be applied and what the intent of model application is.
2026090	2033304	D	0.8907898664474487	So taking the linear model as an example, we might use something like that to be predictive about a linear system.
2033422	2036090	D	0.6109933853149414	And so there's potential value there.
2036780	2051304	D	0.7418707013130188	When we do system dynamics models, a lot of times the value in doing a system dynamics model is aligning the stakeholders around what are the model components and how do we think about those model components.
2051352	2063708	D	0.8790015578269958	So working through a process similar to what we've done in this textbook group, although few do it as rigorously to identify the system elements and how they interact and interrelate and so forth.
2063804	2068820	D	0.7302393913269043	And that can lead to more advanced implementation like agent based models.
2070360	2083096	D	0.7826870679855347	But a lot of times, at least in my experience, those agent based models are used for deeper systems understanding more than, say, predictive power for what the system is going to do at some point in the future.
2083198	2093950	D	0.8750881552696228	So again, it comes back to the reason for applying the models and thinking about how the model fits with that reason and what sort of results you would expect from that.
2095120	2095596	A	0.8529649972915649	Thank you.
2095618	2095944	A	0.6604843139648438	Mike?
2095992	2096780	A	0.689151406288147	Ron?
2100000	2105180	G	0.8393545150756836	Yeah, I generally agree with what both you and Mike said.
2105330	2112480	G	0.6587885022163391	So from a practitioner's perspective, we do hear criticisms of linear models, right?
2112550	2117890	G	0.7563557028770447	It's not that we don't, but it's usually about which domain is applied in.
2121640	2143284	G	0.6245560050010681	But generally speaking, well, let me put it this way because active inference is probably more complex and it takes a lot more time to crop than Y equals MX plus b, which could be one reason for these kind of criticisms.
2143412	2147332	G	0.6847639679908752	And it's probably like they're falling back into old habits.
2147396	2155440	G	0.6170576810836792	Okay, why should I jump my old model as opposed to adopting this newer, more the?
2155550	2161680	G	0.6020531058311462	How much more does it explain than what I think that probably is driving a lot of the criticism.
2166100	2167090	A	0.8529649972915649	Thank you.
2169320	2172310	A	0.5938143134117126	I can't help but add a domain specific example.
2173640	2189172	A	0.8755303025245667	This is a 2016 paper that is speaking to like a multi decade multi career brouhaha about different approaches to modeling evolution and selection.
2189316	2193470	A	0.886069118976593	For example, in the eusocial insects and in social animals as well.
2194000	2212080	A	0.9227591753005981	And this article is very fascinating because they use a Bayesian causal graph to identify situations where two different formulations can and the multilevel selection models have identical predictions.
2212740	2216320	A	0.8062124252319336	Where the causal graphs have identical predictions.
2218100	2227348	A	0.5105193853378296	Any measurement you can think of as being like on a Y equals x manifold where it doesn't resolve your uncertainty about which framework is correct or not.
2227514	2235370	A	0.5726829171180725	Yet the literature is littered with we measured these ovaries, so kin selection is or isn't happening.
2235820	2246940	A	0.6586687564849854	However, a vast set of those empirical cases might be essentially falling on this manifold where those two models are not distinguishable.
2247840	2262210	A	0.6547255516052246	And so this is a theory driven approach, like a first principles approach to identify situations that are providing unique informative value.
2262580	2277300	A	0.7049925923347473	In this case about canon multilevel selection and knowing what territories measurements will not have explanatory value, which relates to active inference.
2278440	2297228	A	0.7689682245254517	Just like Rohan and like many people have brought up, especially when we kind of take like a meta science or like a communicating science or onboarding people into active concern, people are asking explicitly or implicitly why should I update my cognitive model?
2297394	2299580	A	0.8849770426750183	What is the value of active inference?
2300160	2306450	A	0.7691280841827393	Can you appetize me with a two minute video so I can understand it?
2307300	2312560	A	0.826259434223175	Or with a two minute video so it's enough to want to continue going down that path?
2316270	2337970	A	0.7830603718757629	There's a lot to say and there's a lot of work to identify the situations and the ways of truly addressing and perhaps even resolving long standing scientific divergences.
2338630	2358250	A	0.613095223903656	Like, if it is the case that the explore exploit dialectic is addressed in a novel way through the free energy functional balancing pragmatic and epistemic reward, that is quite a vast scope.
2361100	2370250	A	0.5954726934432983	People continue to use, explore and exploit today and it is the evident that those ontology terms wouldn't be useful in the future.
2370640	2386284	A	0.8651683926582336	In fact, some of the discussions on like folk psychology with the belief, desires, intentions can we say what an active inference entity wants or intends based upon its beliefs and its desires?
2386332	2404580	A	0.7674710154533386	For example, can we still talk about exploratory and exploitative behavior as a phenomena but have a different way of modeling how that behavior arises or is regulated?
2406920	2411640	A	0.8881311416625977	What are the settings in behavioral cognitive science?
2413900	2425500	A	0.9143171310424805	What are the settings where systems dynamics has been applied where we can now use active inference to identify?
2427120	2438480	A	0.5911272764205933	Where have we been just throwing a thousand darts at the same grain of sand?
2439380	2446470	A	0.5255590677261353	Where are the vast fields where we don't even know?
2451030	2460790	A	0.8592137694358826	And as people have highlighted, the book is written with a specific rhetorical bent, both explicitly and implicitly.
2462970	2474970	A	0.5249363780021667	For the regime of attention of the book to be about active inference, one can read as an implicit endorsements that this is something that is valid or valuable to pursue.
2476350	2498900	A	0.5370399355888367	And especially at this stage in the field, it is almost like an exhortation to persist amidst uncertainty, to learn and apply few disjointed thoughts there but hope it makes sense.
2500550	2504100	A	0.8506247997283936	Let's look back to the sections of the book.
2506790	2517640	A	0.8486316800117493	Anyone can raise their hand or give a thought like how could the book you write begin?
2521370	2523030	A	0.7539043426513672	What reviewer comments?
2523550	2526460	A	0.8168304562568665	Whether you're reviewer one, reviewer two, reviewer three.
2526910	2531290	A	0.8924254179000854	What comments would you have provided if this were, like, a draft?
2532510	2535450	A	0.8399843573570251	And you could even make structural suggestions.
2545010	2547118	H	0.770494818687439	So I have a lot of comments there.
2547284	2547614	A	0.46103888750076294	Yes.
2547652	2547946	A	0.6215739846229553	Blue.
2547978	2548990	A	0.7181885838508606	And then Ron.
2549410	2550400	H	0.6695564985275269	Oh, sorry.
2551570	2553120	H	0.5541780591011047	I didn't mean to cut you off.
2555990	2567510	H	0.8083739280700684	Having taught lots of classes before, I think a list of key terms and definitions would be critical.
2568010	2576770	H	0.8546477556228638	And learning objectives for each chapter, like I said, like a practice quiz.
2576930	2579000	H	0.7504008412361145	Test your knowledge of this.
2579630	2581174	H	0.8183241486549377	What is surprise?
2581222	2583580	H	0.7450360655784607	Like, how many different forms of surprise are there?
2584510	2599920	H	0.9076476097106934	Just some practice questions to see if you grasped the concepts prevented in each chapter and especially what I was mentioning in the chat like math problems can you set up your own generative model?
2601890	2603658	H	0.8511644005775452	Or even just the Bayesian graph?
2603754	2605290	H	0.8769161701202393	Can you make a Bayesian graph?
2605370	2617780	H	0.8997222185134888	What would that look like for give a question like a verbal description and then give the Bayesian graph answer or make the answers available online so you could test your own knowledge of the subject first.
2618710	2625480	H	0.5422883629798889	I think that that would be totally instrumental in using this functionally like a textbook anyway, so sorry.
2626810	2627634	A	0.7538392543792725	Thanks Blue.
2627682	2629670	A	0.7784972190856934	Nice predictive programming.
2630110	2632010	A	0.8757218718528748	Rohan and then Brock.
2634350	2635100	G	0.5491447448730469	Yeah.
2637790	2641274	G	0.8884736895561218	As a practitioner, I work in the engineering field.
2641312	2643070	G	0.8224114179611206	I'm a control systems engineer.
2643970	2658050	G	0.5008233189582825	My preference would be to immediately see some application not necessarily to control systems, but something that it begins with like a project and explains how active inference is better than its own baseline.
2658470	2660980	G	0.7949900031089783	That would be interesting to see.
2661910	2670610	G	0.6614053249359131	Like a perfect example of this would be I don't know if you've heard of Fast AI by Rachel Thomas and Jeremy Howard.
2670770	2674310	G	0.6971808671951294	They have a very unique way of teaching.
2676810	2679398	G	0.8628430962562561	So they have a computational linear algebra course.
2679484	2691430	G	0.8746512532234192	So the way they teach the linear algebra part is by basically implementing Gaussian loader, for example, and then connecting it to so you start with the code and then you do the math, then you connect it to the math.
2691590	2701870	G	0.8582208156585693	So something like that might be far more useful, maybe not from a point, but it would definitely be useful for practitioners around the world.
2701940	2709266	G	0.8313128352165222	Okay, so if Controls engineer gets this okay, this looks like a better form model predictive control.
2709368	2714690	G	0.7168281078338623	I can replace my model predictive controller with something that resembles active inference.
2717610	2722440	G	0.6446247696876526	We might actually make more progress that it actually gets out of the way.
2725050	2726950	A	0.8925948143005371	Thank you, Ron Brock.
2730190	2734570	F	0.7964129447937012	Yeah, I've had similar comments to that.
2734640	2735260	A	0.5345407128334045	Of.
2738670	2742114	F	0.7288805842399597	Fast AI and Benchmarks.
2742182	2746062	F	0.6540349125862122	That's kind of the ultimate way.
2746196	2752320	F	0.6179307103157043	That's just the way that practitioners change their attention now.
2752770	2754074	F	0.8841734528541565	And that would be really useful.
2754122	2761060	F	0.8356654644012451	But in the context of the book, I think I agree with everything else that was said too.
2761990	2770054	F	0.514686644077301	I'm not sure how realistic it is to do that from the point where the whole field is at right now.
2770092	2771080	A	0.6856347322463989	Sorry about that.
2771690	2784178	F	0.6975274085998535	But I think just having some things in context, there's always this comment about it's a nonlinear thing.
2784284	2787498	F	0.49336615204811096	Text is linear, so of course it will be out of out of order.
2787664	2809214	F	0.593136727809906	But often the definitions of things are kind of scattered throughout the explanation of them and kind of italicized instead of kind of brought up to the top in some way.
2809252	2830570	F	0.7630048990249634	You might expect, at least from my pedagogical experience, everything like all math is taught to define your knowns and unknowns and variables at the top and this sort of thing right in the context of the appendices.
2832510	2859058	F	0.763104259967804	Again, I understand that it's a lot, you can't add that all in or whatever, but just if there was like either every whenever the next part of the appendices became relevant, again, just like having it at the top at each section you're like, okay, so we're going to use part now, building on what we've done already.
2859224	2862830	F	0.9035148620605469	Now we're going to use this part of this section in the appendices.
2862910	2865860	F	0.8888950347900391	In this section our equation, one, two, three, whatever.
2867290	2893840	F	0.6661043763160706	In this next section, like just stating it up front again, just like a big box at the top or whatever, it just would help in way finding and disentangling the unfortunate linear format of books that is mapped to this complex thing.
2897810	2900554	F	0.6518357396125793	I don't know, those general things would be helpful.
2900682	2908450	F	0.8884437084197998	One other thing that is maybe beyond the scope of this book, but spoke to Ali about very briefly at one point about the notation.
2909110	2923894	F	0.5193808078765869	Like even in the basic example of the frog here, like Bayesian notation, it's incredibly easy to manipulate variables when they're one letter and subscripts, but.
2923932	2947200	F	0.7075515389442444	When the whole variable is like ten or 20 letters, like p of x equals frog jump condition on a y, it becomes a larger I don't know if this is just me and Dyslexic, I don't know, but it just seems harder to cognitively kind of work with that.
2951920	2952524	A	0.566673219203949	I don't know.
2952562	2953580	A	0.9182145595550537	Thank you, Brock.
2954000	2964610	A	0.9263859391212463	Yes, the chunking is multilevel and it will be great to explore ways to learn around that.
2966260	2972980	A	0.8528313636779785	Here are a few forays where we can copy quotes and have in line definitions.
2973560	2997064	A	0.6299092769622803	So for those who are interested in this kind of work with an open source textbook, it is not an infinite task or even a non automatable task to copy out the plain text and integrate it with a versioning ontology so that the definition can be even rendered differently.
2997112	3001116	A	0.8080651164054871	It could be in a different language, it could be every single time that a term comes up.
3001298	3017120	A	0.8076265454292297	There's like so much enrichment that this seed can lead to of the textbook and a lot of it is for us to pick up.
3017270	3018240	A	0.6419747471809387	Honestly.
3019560	3021590	A	0.883098304271698	Rohan and then anyone else?
3025480	3027460	G	0.5097953677177429	Oh, I forgot to put down.
3027610	3028308	A	0.6883722543716431	Oh, no worries.
3028394	3029270	A	0.8529649972915649	Thank you.
3031960	3050140	A	0.8510780334472656	So in our last seven or so minutes for this session next week, we will continue with any conceptual points and questions that people want to raise.
3050800	3074870	A	0.778186023235321	We'll also be turning our attention a bit more to making sure first off that please, please everyone complete the feedback form in the future textbook groups and or add information here or contact us because it's just one of the most important feedback mechanisms we have.
3076120	3102540	A	0.6618863344192505	We'll also be exploring a little bit different project ideas, several of these threads involving different system models we've already found a home for in the Active Blockprints project where we're meeting twice on Wednesdays weekly.
3104640	3142120	A	0.550997257232666	Other projects we're going to have new vistas on during and after going through the second half of the textbook, which is still black and white static PDFs, but we'll be able to augment this heavily with some simulation tools as well as other work like the Plain Text Enrichment, or even anyone who wants to collaborate on an audiobook version.
3143420	3169344	A	0.9221959710121155	There's no lack of active tasks today and these are like high leverage point moments where staying in the game and improving the game will do tremendous service towards reducing research debt and improving the rigor in the applications that we see.
3169542	3170560	A	0.7809367179870605	Rohan?
3174830	3175482	G	0.7381101250648499	Yeah?
3175616	3178166	G	0.8896424174308777	Can I make a project suggestion?
3178358	3179420	G	0.6264714598655701	That's fine.
3179870	3180940	A	0.6842275261878967	Yes please.
3181390	3182410	A	0.7568896412849426	Everyone can?
3182480	3183100	G	0.5491447448730469	Yeah.
3183970	3187306	G	0.8353625535964966	Okay, so there is put this in the chat.
3187338	3197102	G	0.9017581343650818	There's this online lab by Georgia Tech's Intelligence Systems unit called Robotario that's basically a bunch of swarm.
3197166	3199570	G	0.8735151290893555	It's to test swarm controllers.
3200070	3203346	G	0.8655241131782532	So there's a good baseline over here.
3203448	3214310	G	0.6343995332717896	It would be a good idea to try like an active inference version, like an active inference inspired swarm controller to finish the tasks.
3215690	3221510	G	0.8498477935791016	So this runs on actual robots in that lab and you can see the results.
3223710	3225020	A	0.9831928014755249	This is awesome.
3226270	3232460	A	0.8110577464103699	I had some colleagues who worked with some of the ant groups here.
3233070	3242990	A	0.5117006301879883	It's right at the intersection of JF's work and many's interest in the embodied and robotic angle.
3243330	3258434	A	0.5126633048057556	And we have somewhat extensive multi scale generative models of ants which would apply to other swarm settings as well.
3258552	3265314	A	0.6791679263114929	So absolutely we want to scaffold those projects.
3265362	3292874	A	0.5731782913208008	And one way to start, but not the only way to start, is to copy a version of these fields and add and remove as relevant for you and give some handle for others to be involved or just stay in the game so that more people can join.
3292922	3297250	A	0.8124076724052429	There will be a flow of people entering active inference forever.
3299270	3300770	A	0.8183833956718445	How will we greet them?
3300920	3304420	G	0.8722323179244995	So what would I need to put in over there?
3309110	3312070	A	0.8560820817947388	It depends on how you want to carry forward the project.
3312140	3321800	A	0.7841498851776123	But you can feel in this row, add any notes you'd like and more details on what you're thinking of doing.
3322110	3331820	A	0.8742278814315796	And then however you want to be more specific about directions and just signal it in the idea.
3332190	3340320	A	0.8941927552223206	Or you can in the notes or draft catechism, which is what these templates are based on.
3342130	3354370	A	0.788699209690094	And if there is ever something you're not sure about, then ask around and different projects are going to proceed differently.
3356550	3356962	G	0.584351658821106	Okay.
3357016	3357474	G	0.5491447448730469	Yeah.
3357592	3361090	G	0.8794934153556824	So I'll fill that out with my talks.
3361770	3362520	A	0.918424665927887	Awesome.
3365210	3370230	A	0.8988028764724731	Well, interesting meeting.
3370970	3375370	A	0.8971479535102844	It's our penultimate discussion in this section.
3376590	3393358	A	0.8052135109901428	Then we hope that everybody who is motivated and ready to do so continues for part two and or rejoins for part one, cohort two.
3393524	3396590	A	0.93077152967453	Both of these will be starting in September.
3396950	3401250	A	0.906407356262207	So we'll take August off of the textbook.
3401830	3424700	A	0.9188527464866638	And at this point, the tentative plan is that we'll have two sequential hours, like section one and then followed by section two so that those who are on section one can see how section two is.
3426030	3428554	A	0.6901401281356812	We can explore different architectures and stuff.
3428592	3443722	A	0.8006197810173035	And again, this is like why people's active participation and their feedback and the oversharing on topic and just writing all the questions that came in their mind and all the things that they would want to ask somebody to check.
3443776	3446482	A	0.5498192310333252	Like every contribution is important.
3446576	3452850	A	0.8340517282485962	It does anyone else have anything they'd like to add before we stop the recording?
3463550	3464970	A	0.7434884309768677	Stopping the recording?
3466270	3466794	A	0.5585339665412903	Oh, yeah.
3466832	3468060	A	0.7583539485931396	Someone go for it.
3471990	3472530	A	0.4896697998046875	All right.
3472600	3473310	A	0.7053525447845459	Stopping the recording.

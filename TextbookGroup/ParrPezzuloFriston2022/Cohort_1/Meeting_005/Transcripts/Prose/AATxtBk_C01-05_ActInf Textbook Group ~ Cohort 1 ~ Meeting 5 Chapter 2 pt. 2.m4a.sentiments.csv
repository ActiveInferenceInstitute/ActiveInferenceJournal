start	end	speaker	sentiment	confidence	text
650	1246	A	0.546256959438324	You.
1428	2478	A	0.5579421520233154	Hello everyone.
2644	9760	A	0.9133391380310059	It is June 2, 2022 and we're in week five of textbook group cohort one.
10370	19280	A	0.8074266910552979	We're having our second discussion on chapter two and we will get right to the questions.
19810	28550	A	0.7084136605262756	First, just wanted to highlight the Math learning group, especially for those who are not regularly attending the meetings yet.
28620	31154	A	0.9345784783363342	The meetings are on 19 UTC on Wednesdays.
31202	36486	A	0.7390505075454712	But just with everything, don't let the synchronous time dissuade you.
36588	40102	A	0.7571274042129517	We can set up other synchronous times if people want to cowork.
40246	44502	A	0.8190799355506897	And also the resources and the modifications are all asynchronous.
44646	48860	A	0.8706101179122925	So we've been doing several things in the Math learning group.
49550	53590	A	0.5263001918792725	First, we've been sharing important resources and categorizing them.
53680	61886	A	0.5044668912887573	So if somebody wants to look at a video on something or find a textbook on something, or if you come across something useful, just add it here.
62068	72100	A	0.8319928646087646	And then if you feel like leveraging the resources that you or other people have added, then you can categorize it in the table and just add a few pieces of information on it.
72630	78550	A	0.6490206718444824	We have a notation table that we'll be building, especially as we get into the continuous time active inference.
79050	89850	A	0.6830840110778809	We have some questions, less so questions about the textbook and more so, just cool math questions that people are raising as they're discussing the material.
90350	93878	A	0.8081516027450562	And then we're building a math oriented overview.
94054	102142	A	0.915467381477356	So we've gotten up to equation 2.5 in chapter two and this is just sort of like point by point, like what are the formalisms doing?
102196	103662	A	0.8784353733062744	Where are the figures coming into play?
103716	105630	A	0.8578122854232788	How are some things related?
105970	109898	A	0.8174171447753906	And any and all note taking can be added.
109994	115822	A	0.8745596408843994	You can always add it to like unsorted notes or erota and then we can categorize it later.
115876	121058	A	0.7900698781013489	But this is how we develop a shared epistemic resource that's useful for everyone.
121224	123010	A	0.8819147348403931	And we're going to come to this in the questions.
123080	129606	A	0.8888512253761292	So we won't go into detail right now, but equation 2.5 is the variational free energy.
129708	136162	A	0.8836131691932678	2.6 is about expected free energy, the G, and this is a key equation and set of equations.
136306	157258	A	0.8920945525169373	So yesterday with Blue, Brock, Jessica Jakub, we went through and line by line we went with examples, natural language readings, questions, interpretations, and then Yaakob added some derivations.
157354	160654	A	0.8366831541061401	Like how are those three lines in 2.5 related to each other?
160772	167150	A	0.8594365119934082	How do we go from the energy and entropy formulation to the complexity accuracy formulation?
167310	171730	A	0.8412449955940247	So this is the essence of understanding the equations.
172550	174658	A	0.8205243945121765	It's an infinite learning journey for everyone.
174744	177446	A	0.8275884389877319	So whatever level, whether you want to.
177468	178040	B	0.5836908221244812	Be.
179850	192106	A	0.7561367154121399	Catching up to what has been written here, or whether you see ways to reorganize it and go beyond and add more connections, every single background and level of math familiarity can contribute in this way.
192208	195260	A	0.9044866561889648	But we'll come to the more details on 2.5 in the questions.
197230	202206	A	0.8871676325798035	Any just preliminary thoughts that people would like to share?
202388	207200	A	0.8820129036903381	They can raise their hand or write in the chat at any time.
208210	218210	A	0.7618975639343262	Is there anything that anybody wants to share before we start going through the questions, starting with the most upvoted ones that we haven't addressed, but we'll just pause for a few seconds.
218790	225274	A	0.9015518426895142	Tune into the chapter two regime of attention upvote any questions that you'd like to see addressed?
225422	227030	A	0.8920138478279114	Post anything in the chat?
232640	233820	A	0.8519666194915771	Yes, Jose?
234960	240750	C	0.9080328345298767	Yeah, I'd like to ask if there's a recording of the math sessions which you had.
241920	251200	A	0.8757165670394897	No, we record Stick mergically with the traces in the notes and we had this conversation like about whether it should be recorded or et cetera.
251800	257172	A	0.6527867317199707	If there's enough desire, we could do something like have some unrecorded and some recorded.
257306	266720	A	0.5692991614341736	This session is obviously recorded and rewatchable here and we do many live streams and there is a lot of recorded material.
266880	275732	A	0.7498819231987	And so we want to also hold a space for people who don't want to speak on a recorded version, even though everything's recorded in universe and in each other's minds.
275876	282510	A	0.8409718871116638	But yes, if there's enough interest in people who want to record sessions, we absolutely will do it.
283600	291600	A	0.5691462755203247	We also just wanted to make sure that there was a total open math learning space where people wouldn't even have that cross their horizon.
292500	294400	A	0.8787832260131836	But we did discuss that yesterday.
297220	298112	A	0.8529649972915649	Thank you.
298246	298688	A	0.5491447448730469	Yeah.
298774	300400	A	0.7836205363273621	Any other comments?
302740	307024	A	0.8205257654190063	People can just raise their hand with six in gather or right in the chat.
307072	321608	A	0.8501936197280884	So we're going to just go to the questions and there's still time to upvote, like during the discussions and we're just going to upvote the ones once we've discussed them.
321694	326730	A	0.7509258389472961	So we'll start with the first question that we haven't discussed yet.
329840	332830	A	0.8906823992729187	Okay, this is from page 32.
333760	341152	A	0.870632529258728	Policy dependent outcomes are not immediately available, but they can be predicted by chaining together two components of the generative model.
341286	345970	A	0.8461407423019409	The first is our beliefs about how hidden states change their time.
350340	355590	A	0.8456448912620544	Just want to find what the second part was.
360520	363300	A	0.7892439365386963	The second component is the likelihood distribution.
370250	377110	A	0.7947463989257812	What really changes as a result of policy selection, the hidden states or the outcomes?
377930	385194	A	0.8000520467758179	Bringing a jacket doesn't necessarily change the temperature hidden state but instead changes my observation perception of whether it's cold or not.
385392	387958	A	0.7370343208312988	Can we actually change hidden states with policy selection?
387974	389660	A	0.8271970152854919	And what would be an example of this?
391970	396926	A	0.8684317469596863	Does anyone want to raise their hand and give a thought on this question?
397108	400154	A	0.8208529353141785	What really changes as a result of policy selection?
400202	407090	A	0.7399845123291016	The hidden states are the outcomes while people are raising their hand.
407160	409380	A	0.818787157535553	I'll give one possible thought.
409990	411614	A	0.7489765286445618	The answer is both slash.
411662	412430	A	0.7104793787002563	It depends.
412510	415678	A	0.8472896218299866	So bringing a jacket does change the temperature around you.
415784	418514	A	0.8017906546592712	It's not just changing your perception of temperature.
418642	438650	A	0.8379657864570618	So one example of changing a hidden state with temperature, hidden state of temperature with policy selection would be like changing the setting on the thermostat that is intervening into the causal process, the generative process that's giving rise to temperature, which is giving rise to your perceptions of temperature.
439230	448170	A	0.7935627102851868	And so assuming that your temperature perception is accurate, you are able to change both the temperature in the room or the temperature around you with the jacket.
448330	457170	A	0.8315179944038391	And thus because temperatures are giving rise to observations of temperature, also you end up changing both blue.
460550	467362	D	0.9071662425994873	So this is my question and I think Karl kind of possibly started to answer it on the live stream yesterday.
467426	470934	D	0.7664363980293274	I need to think about it more and unpack it a little bit more.
470972	479654	D	0.699172854423523	But we change the temperature around ourselves, but we're not going to change the thermometer reading by bringing a jacket.
479702	496942	D	0.7094854712486267	Right, but anyway, what Karl said and what might be beneficial for others to hear is that when you think about what's changing, it's like really, when you think about a hidden state, there's a causal chain underlying that hidden state.
496996	503210	D	0.8501786589622498	And the same thing, like when you think about an observation, there's a causal chain underlying that observation.
503370	509106	D	0.7656983137130737	So like, no, I'm not going to change what my eyes tell me the temperature is, but I'm going to change what my body tells me the temperature is.
509128	514834	D	0.8520320057868958	And so it depends because it's a multisensory observation, right?
514872	520790	D	0.7815819978713989	So which sense is going to change not my visual reading on the thermometer, but my physical perception of cold.
521690	530394	A	0.9895048141479492	Also yesterday on the live stream, thomas Parr and Karl Friston joined So that was really awesome and a great one to watch.
530432	534890	A	0.9052425026893616	45.1 and we can continue on this Ali and then Mike.
538270	555860	E	0.7493737936019897	Yeah, I think that's what meant by the word hidden in the statement because if it was transparency observation to any kind of perception or observation, I don't think it would be called a hidden state.
556310	565250	E	0.7777224779129028	I believe that the reason it's called this, it's called as such is because it's opaque to any kind of perception or observation.
565690	578470	E	0.6079201102256775	So it's kind of inherent to the universe and probably not amenable to any change or action.
579790	582090	A	0.7407853007316589	Thanks Mike.
586440	590896	B	0.6484659910202026	Yeah, I'm not sure if I'm thinking about it the right way, but intuitively.
590928	602650	C	0.8748068809509277	I was thinking that policy selection would update the expected conditions or the expected state, so maybe adjusting the priors in evasion sense.
603820	604580	A	0.6283750534057617	Thanks.
604750	620610	A	0.8956320285797119	So we'll come to this later, but the answer based upon the partially observable Markov decision process, discrete and continuous time versions, policies pi intervene in between hidden states and how they change their time.
621220	628508	A	0.8621290326118469	Different hidden states, should they be giving rise to different observations, will result in different observations.
628684	636708	A	0.9052469730377197	So changing the thermometer policy selection is intervening in the causal process of temperature changing through time in the room hidden state.
636874	645172	A	0.8619343638420105	And if the temperature in the room is associated through the a matrix with your observations of temperature, then you will be changing your observations.
645236	654304	A	0.8779075145721436	And then just like Mike said, that is reflected based upon the expectations that the individual has about the observations of temperature.
654452	660140	A	0.8369240164756775	So they're both changing because the hidden states and the observations are linked.
660480	671324	A	0.8623946905136108	That's what it means to have a generative model or for there to be a partially observable model with hidden states and observables hierarchical models.
671452	677104	A	0.7386730909347534	A given state could be in emission from a higher state and emitting a lower state.
677222	682772	A	0.7749277353286743	And that's kind of like how the current moment is like a consequence of the past, but it's a cause of the future.
682906	692020	A	0.8541103601455688	So this is related to Bayesian modeling, but using this architecture, we can say that policy intervenes only in between hidden states.
692170	695860	A	0.7256790399551392	We don't directly intervene in how observations appear.
700360	703670	A	0.8571946620941162	Any other thoughts or questions on this question?
712070	712820	A	0.584351658821106	Okay.
719530	720440	A	0.4896698594093323	All right.
723790	737864	A	0.9117011427879333	In Figure 2.2 figure 2.2 they write x, Star, and X.
737982	741736	A	0.7616773247718811	So that's going to be generative process and generative model.
741838	742756	A	0.7171269059181213	Latent variables.
742788	747300	A	0.6952592730522156	Hidden states do not necessarily live in the same space of measurement.
747460	755340	A	0.6930913925170898	It might be the case that hidden states in the external world take on values that lie outside the space of explanations available to the brain.
755680	761230	A	0.6415045261383057	Conversely, it might be that the brain's explanations include variables that do not exist in the outside world.
762080	765596	A	0.8667746782302856	For example, the former could be five dimensional and the latter two dimensional.
765628	768800	A	0.786846399307251	Or one could be continuous and the other could be categorical.
769140	786170	A	0.8888682723045349	So what are examples people can think of where the dimensionality of the generative model, the cognitive entity, is the same as the generative process, the niche process giving rise to observations, passing them to the generative model.
787820	789130	A	0.718879759311676	That's one question.
790780	802860	A	0.8371792435646057	Second question how does this approach or framing speak up to the map territory debate hashtag, instrumentalism and realism?
804000	811388	A	0.8994665741920471	What papers or live streams best characterize how active inference models the action, cognition perception loop?
811564	818236	A	0.8961222171783447	And what other possibilities or models are there for the action, cognition perception loop?
818428	823536	A	0.8943257331848145	What would be the different strengths and limitations of these different framings and partitionings?
823728	825184	A	0.7981656789779663	Thanks, Joe.
825232	827910	A	0.8276007175445557	And then Blue and then anyone else who raises their hand.
829880	836644	F	0.47482919692993164	I was just thinking a nice case where you could say dimensionality is pretty obvious is board games.
836692	851288	F	0.7634543180465698	If I was trying to anticipate what my opponent is going to play in a board game, they have a limited number of possible moves and I'm assuming I'm playing the same game, I'm expecting them to make some move on the same board.
851374	858652	F	0.7533040046691895	So me, that sounds like the dimensionality is the same because you're in this limited world, I don't have to know what they're thinking.
858706	860516	F	0.7574149370193481	I don't have to know what they had for breakfast.
860648	862624	F	0.6887978315353394	I don't have to know a lot of stuff.
862662	864528	F	0.6158381700515747	I'm just thinking, and I might be wrong.
864694	865840	F	0.8484843969345093	What movement?
868740	869248	A	0.7671424746513367	Great.
869334	870450	A	0.840377688407898	Thanks for that.
873800	883990	A	0.7887158393859863	So with the same dimensionality, it's like if we're playing the same game, then we're playing a game where we're tracking a little cursor on the screen.
884600	890084	A	0.870966911315918	The movement space within that is just the x and the y value or just two dimensions.
890212	904490	A	0.8551969528198242	And so the observation is like the location in two dimensions, and then that is the same dimensionality that inference is occurring on.
905180	918508	F	0.7231737971305847	But as an I'm thinking just to add a little bit to more of my comment, if I wanted to enrich it and say oh, but I'm starting to think strategically and anticipating their next move or anticipating maybe they're bluffing or maybe I'm bluffing and I want to see if they know that I'm bluffing.
918604	920044	F	0.6700286865234375	We add more dimensionality.
920092	930100	F	0.8778290152549744	And I guess in principle, again, you could still add that dimensionality in somehow in parity with parts of the hidden states.
930250	930916	F	0.681637167930603	You see what I mean?
930938	936448	F	0.6287977695465088	That I'm thinking, oh, they think they know I'm bluffing or I think they're bluffing.
936544	938564	F	0.49023059010505676	You're talking again about the same thing.
938682	940292	F	0.7725448608398438	You don't have to know what they had for breakfast.
940356	945720	F	0.47534072399139404	You're just talking about this extra information which isn't in the board itself, but that gets a bit more complicated.
946620	948910	A	0.831592321395874	Yes, thanks.
950240	956860	A	0.8758905529975891	So that is a good answer.
956930	967360	A	0.8309170007705688	Other ones that people have written thermostat is having the same dimensionality, like it's a temperature value, it's a dimensionality of one.
967510	975248	A	0.8968797922134399	And so then the person's generative model is also on this one dimensional temperature axis.
975424	980512	A	0.8523958325386047	And then they mentioned that it could be like categorical versus continuous.
980656	987108	A	0.8283372521400452	So without going too many rabbit holes deep, we could just say temperature in the room is a continuous variable.
987204	988584	A	0.5625863671302795	It's like taking on a number.
988622	993560	A	0.8389524817466736	It could be a decimal point number between any number, zero to infinity.
994780	1000190	A	0.826906144618988	The person's model might be the same dimensionality like one.
1000560	1004008	A	0.7730957269668579	But then they might be trying to make a categorical estimate.
1004104	1006296	A	0.8387243151664734	Is this livable or unlivable?
1006408	1008824	A	0.8885562419891357	Is it hot, neutral or cold?
1008872	1011200	A	0.8400461673736572	That'd be like a three state categorical model.
1011350	1023090	A	0.7599229216575623	So the generative process doesn't have to have the same dimensionality or continuous versus discreteness of the generative model.
1027480	1029536	A	0.8509153127670288	Second answer for same dimensionality.
1029648	1041560	A	0.8768596053123474	Controls for a car have two degrees physical degrees of freedom the acceleration, the speed, whether you can hit the gas or the brake and the direction of the front wheels controlled by the steering wheel.
1042780	1048040	A	0.8701425194740295	So there's like a speed dimension and then there's a turning angle dimension.
1048560	1054552	A	0.8572928309440613	This is the model that people use when controlling a car at some level about abstraction.
1054696	1064444	A	0.8514291644096375	And then the acceleration dimension can be acted upon through policy selections like stepping on the gas and the brakes.
1064492	1072384	A	0.7846493721008301	And then it's like, well, those are two categorically different affordances and then there's continuous variables inside of that categorical difference.
1072502	1080310	A	0.8767328262329102	And we'll talk about like hybrid control models with both categorical and continuous aspects in play.
1081480	1086360	A	0.7723870277404785	And in this case, the speed of the car in the speed axis is the hidden state.
1086510	1089480	A	0.8388134837150574	The observable would be the speedometer.
1090780	1096516	A	0.8122898936271667	So again, the gas and the brake are policy selections that modify.
1096628	1097832	A	0.8789571523666382	Let's say we're thinking about a train.
1097886	1105390	A	0.7534540891647339	No turning policy selection is influencing how speed changes through time.
1105920	1113276	A	0.7075172662734985	And if you have a speedometer that's accurate, even if it's noisy, it's giving rise to observations reflecting those changes in speed.
1113468	1117852	A	0.6883097290992737	But gas and brake do not change the speedometer directly.
1117916	1126092	A	0.7580050230026245	They're changing a hidden cause, which is the speed different dimensionality rain comes from clouds.
1126156	1130310	A	0.534198522567749	Simple mental model is that the darker the clouds, the more there will be.
1130840	1133510	A	0.805966854095459	That's a two dimensional generative model.
1135580	1141240	A	0.7850558757781982	So how much rain, how dark the clouds?
1141580	1144840	A	0.9116613268852234	And this also is going to speak to this map territory.
1145740	1149640	A	0.6559669971466064	Someone could say, well, there's the size of the cloud and there's also the humidity.
1149720	1154220	A	0.7051076292991638	So that's like maps with increasing amounts of variables.
1154960	1159308	A	0.7857667207717896	So no one's denying the territory of the actual chaos of the cloud.
1159474	1164988	A	0.8134122490882874	It's just a question of how much detail and what data we're actually treating as observables.
1165164	1183344	A	0.7635673880577087	And what sophistication is being taken with unobserved variables with hidden states which are modeled in the computer or on paper, but they're not directly observed, then the mental model can be decomposed even more and then add infinitem.
1183472	1195880	A	0.8042306303977966	On the reality side, the actual generative process for weather is including all the butterflies.
1196780	1202584	A	0.6046091318130493	So that's a cognitive model that is two dimensional.
1202632	1210468	A	0.8342304229736328	In this case, how much rain is expected and how dark is the cloud, that one could make increasingly nuanced cognitive models.
1210664	1218096	A	0.8144055604934692	But whatever cognitive model is being proposed, the generative process is something that's like totally different.
1218198	1219964	A	0.7704501152038574	Cognitive model generative.
1220012	1224740	A	0.8365132212638855	Model map generative process niche territory.
1226200	1229008	A	0.5974410176277161	Plants do better when they have water and fertilizer.
1229184	1234820	A	0.8188239932060242	Generative mental Model leaf droopiness is a sign of moisture, but leaf size and color are signs for nutrients.
1235720	1244984	A	0.8009073734283447	So one could imagine like a causal graph where there's like moisture level is unobserved, but it is emitting leaf droopiness as a state.
1245102	1254360	A	0.8741977214813232	But we're observing leaf droopiness and we're using that to infer moisture, whereas nutrient is an unobserved state to the visual gardener.
1254520	1257340	A	0.7199558019638062	And then they're using two dimensions, leaf size and color.
1257410	1263404	A	0.8765115141868591	So this is a three dimensional cognitive model leaf droopiness, leaf size and color are observables.
1263532	1270320	A	0.7578766942024231	And then there's two unobserved cognitive states, nutrients and moisture.
1271700	1275984	A	0.6298382878303528	But whatever map gets constructed, that's not the plant.
1276112	1294808	A	0.8198579549789429	Now, the gardener might then want to take an informative experiment being an optimal Bayesian gardener, and then measure something, take something that was an unobserved in the initial model and then measure using LiDAR, like using some sort of sensor to understand.
1294974	1306296	A	0.9004528522491455	And then that would make that data point an outcome related through some a matrix, which we're going to get to later of the hidden state being like the true levels of copper.
1306408	1313552	A	0.8234420418739319	And then if we have an accurate test, we can start using the outcomes of the observed levels of copper and start flushing out our model that way.
1313606	1314320	A	0.5845615863800049	Joe.
1317380	1319680	F	0.6231330037117004	I chucked in a comment below.
1319750	1322304	F	0.5205993056297302	I hope it's showing up just that.
1322422	1327504	F	0.8615979552268982	The other part of the question was asking about the map territory debate.
1327552	1334148	F	0.73135906457901	And here's from the person who kind of came up with that dichotomy in the way we usually talk about it.
1334154	1336748	F	0.8777549862861633	He says that the map and territory should have a similar structure.
1336784	1338792	F	0.9681311845779419	And I think that's really interesting to think about.
1338926	1347096	F	0.8409726619720459	If I'm making again moves in a board game, I might make a reduction of their mental state.
1347278	1349192	F	0.6628120541572571	Maybe I'm playing against a computer for example.
1349246	1356136	F	0.556656002998352	So I don't know that they have a mental state or maybe I'm playing against a person I happen to know as a beginner.
1356168	1359980	F	0.4976015090942383	So I know they're likely to make naive mistakes or whatever, something like that.
1360130	1369570	F	0.6262637972831726	But I was just thinking like with regard to the leaf droopiness thing, you could have all kinds of really badly structured models of plant health.
1371380	1374464	F	0.900857150554657	Maybe it relates to how recently you watered it or something.
1374502	1380260	F	0.4743764102458954	Well, that sounds good in general, but if it's a cactus, maybe that's like a defeater to your system.
1380330	1393050	F	0.8197920918464661	So the point being it seemed interesting that your map should maybe if it doesn't have exactly the same dimensionality, should still be a nice reduction of the thing you're trying to model, whatever that means.
1393820	1394570	A	0.6283750534057617	Thanks.
1396460	1409260	A	0.5938524007797241	One aspect on this is evolution, natural selection, dissipation in our world, sweeping off the table entities that are failing to at least be adequate.
1409760	1419756	A	0.5392590761184692	So in a dissipative situation that we're in, the map has to be at least good enough, otherwise the entity will fail to exist.
1419868	1429140	A	0.6416775584220886	So that sort of closes the loop and is like we're seeing the persistent entities that are acting good enough to navigate.
1430120	1445128	A	0.64532870054245	And it's just interesting like how many the allegory of the cave to the blind people and the elephant and realism instrumentalism and we've had many live streams on this especially.
1445214	1448840	A	0.9411469101905823	Check out number 14 on Mel Andrew's paper.
1448990	1451000	A	0.535492479801178	The math is not the territory.
1451160	1459116	A	0.9182787537574768	So this one sort of explicitly jumps in right there and tackles this issue with the FEP front and center.
1459218	1462136	A	0.9800944924354553	So this is a great series of live streams.
1462328	1473552	A	0.8984695672988892	The dot zeros are background and context where we just go over the paper in a small group, like one person or two or three people and then the dot one and the dot two.
1473686	1479584	A	0.5300235152244568	We usually have more of an open participatory discussion and the authors sometimes to usually join.
1479632	1486100	A	0.9632439017295837	So everybody's welcome to participate in contributing to dot zeros as well as enjoying the dot one and the dot two.
1486170	1496280	A	0.7587084174156189	But just so you know, when you're looking in the live stream table, the dot zero is a good one to watch first because it has the background and overview on the paper.
1496350	1506670	A	0.6101633906364441	But then like yesterday in 45.1, we had Friston and Par join to discuss just so that people can navigate the live streams a little better.
1507120	1518716	A	0.7432865500450134	So we can continue asynchronously adding more thoughts on like map territory and what papers and live streams and what other possibilities for the action perception loop.
1518748	1521692	A	0.8958273530006409	But these are big fun areas.
1521756	1528980	A	0.8864020705223083	But we're going to continue with the questions on the notion of surprise.
1529880	1538516	A	0.7642839550971985	Is the agent's perception not only affected by the influence of not only its environment but also an agent's peers?
1538708	1546680	A	0.8721229434013367	So perhaps to restate, how is an agent's perception affected by its direct perception and its assessment of peers?
1547180	1551420	A	0.8145890831947327	So the agent's peers are updating their beliefs in a collaborative fashion.
1552960	1561064	A	0.6748345494270325	What would be the extent of the update to an agent's generative model of perception if the agent witnesses the annihilation of one of its peers.
1561112	1567632	A	0.7850923538208008	For example, something like this, okay, will to be from bird box.
1567686	1578912	A	0.4933568239212036	Not going to open any video links, but perhaps we'll observe some annihilation like some animal attacks one of the wild beast on the right, but then the other one learns.
1579056	1581028	A	0.980963945388794	So this is a great video.
1581114	1582950	A	0.9695908427238464	Definitely good to watch.
1585080	1598120	A	0.8349370360374451	So far in the textbook and indeed for the textbook it's focusing on just like we saw in figure two two, it's like one entity in the niche.
1598940	1602636	A	0.7112079858779907	The niche can consist of others like me.
1602818	1612156	A	0.9013382196426392	Other entities that I expect are similar to myself in terms of like the coarse grained cognitive architecture they have or their preferences, their affordances.
1612188	1614640	A	0.6196198463439941	They're able to do similar things, they want similar things.
1614710	1618320	A	0.8818685412406921	They have a similar history to some extent.
1619460	1624120	A	0.851991593837738	So that is sometimes called thinking through other minds.
1624220	1628950	A	0.7953844666481018	Ttom and just to give one thought and then anyone can raise their hand.
1629800	1657260	A	0.48524415493011475	If we were to accurately observe another peer taking some action and then failing to exist, we're driving on the highway, and then we observe somebody taking a policy to go off the highway, and then we see them fail to exist, we could use that information to update our beliefs about the consequences of the action to drive off the highway.
1659600	1677060	A	0.6079870462417603	So I believe it would be possible to implement every variation of direct perception, direct perception of peers thinking through other minds, theory of mind learning by example, imitation, contrarianism.
1677640	1686792	A	0.8715395331382751	Every phenomena could be modeled as just has to be whatever the specific model is actually about.
1686846	1687640	A	0.6077883839607239	Ali.
1692050	1696960	E	0.855922520160675	I have a related question about 2.5 and 2.6.
1697490	1713320	E	0.8318237662315369	Well, correct me if I'm wrong, but I understand that these equations describe the behavior of a kind of rational behavior or the kind of behavior of a rational agent.
1713930	1717640	E	0.830470860004425	Where does irrationality comes into this?
1718570	1726810	E	0.6602526903152466	Or let me put it this way can irrationality be a parameter?
1727790	1746510	E	0.9196583032608032	In order to qualify these two equations, 2.5 and 2.6, or seen from the other side, can we use these two equations to measure the extent of rationality as a measurement of rationality?
1747890	1749200	A	0.6243525743484497	Good question.
1749750	1751182	A	0.7413427829742432	Here's one quote.
1751246	1754980	A	0.501736044883728	And we'll come to equation 2.5 and six as well because those are important.
1757750	1761620	A	0.6150596737861633	Distinguishing the generative model and the generative process is really important.
1762310	1765826	A	0.8246870040893555	That's the difference between the entity and the niche.
1766018	1772070	A	0.8359361886978149	It's important to distinguish them to contextualize psychological claims about the optimality of inference.
1772410	1777782	A	0.8735783696174622	To the extent these claims are valid on a Bayesian view is always contingent on the organism's resource.
1777926	1781658	A	0.5080929398536682	So don't want to step out too far.
1781744	1784854	A	0.6577216386795044	But rationality and irrationality.
1784982	1786554	A	0.5654114484786987	It's rational to believe in X.
1786592	1789274	A	0.5630977153778076	It is irrational to believe in X is always subjective.
1789322	1797034	A	0.8515708446502686	From that speaker's perspective, given the priors and the update rules, all there is is rationality.
1797082	1799754	A	0.7691606879234314	Which is to say this is just the Bayesian updating process.
1799892	1806638	A	0.7444330453872681	So there are maladaptive priors ones that trend towards dissolution of the entity.
1806734	1808702	A	0.7195155620574951	But there aren't irrational priors.
1808766	1810942	A	0.7085300087928772	There just is what it is and how it updates.
1811086	1826330	A	0.6206651926040649	And then that could be inadequate, it could be any number of other things and these are like getting into relatively complex cognitive phenomena where there could be like multiple layered models.
1826910	1837840	A	0.8530967831611633	But I would say rationality in the Bayes process is like the cognition is Bayes optimal or modeled with a Bayesian process.
1838770	1849520	A	0.8763412833213806	And then sometimes the interaction of the generative model and the generative process as appropriately defined specifically is going to result in what it results in.
1852550	1855650	A	0.7682167887687683	But we will come to 2.5 and 2.6.
1855800	1856402	A	0.6493546962738037	Okay here.
1856456	1865058	A	0.8343045115470886	Suffice to say, yes, social learning and collective behavior and emergent outcomes at the group level is something that's important.
1865144	1883660	A	0.5330947637557983	But we haven't addressed it so far in the textbook, and multi agent models are not really addressed in the textbook that much because there's so much to understand about the kernel of the perception cognition action loop that even though it's so important for real systems, it's not brought up.
1893970	1905890	A	0.7275456786155701	Okay, wanting to go to the equation, the dark room problem for many years, many Papes firing back and forth.
1909130	1915142	A	0.8848708271980286	But let's go to the equations for our let me check the chat here.
1915196	1933914	A	0.6884298920631409	Yes, variational free energy is minimized through two different possible approaches minimizing divergence of Q and P, changing mind and maximizing evidence, taking action, changing perception.
1934042	1937760	A	0.9180284738540649	This is done based upon prior and present information.
1942130	1958130	A	0.7025079727172852	Equation 2.5 variational free energy to plan best actions, generative models and internal policies produce simulated outcomes that can be used to estimate expected free energy as the average of the log probability of outcomes.
1958970	1971590	A	0.8805320858955383	Equation 2.6 expected free energy and there's other variants in the context of planning, planning as inference.
1971930	1979770	A	0.8484921455383301	Equation 2.6 provides a view of EFE that establishes consistency in measurement units of exploration, exploitation.
1980670	1986750	A	0.7871195077896118	The relative balance between these terms determines whether behavior is predominantly explorative or exploitative.
1987410	1997340	A	0.8470807671546936	34 in other words, dissolving the classic explore exploit dilemma in behavioral psychology.
1998820	2001344	A	0.778049886226654	What causes this balance to change?
2001462	2002032	A	0.5757660269737244	I e.
2002086	2003760	A	0.7954420447349548	What controls this balance?
2004180	2007250	A	0.8804922103881836	Very important and interesting question.
2009720	2015510	A	0.8445777297019958	Perhaps we can come to equation 2.5 first and then we can come to equation 2.6.
2015960	2020550	A	0.6534594297409058	So where we're going is the future.
2020920	2027480	A	0.8004795908927917	The future has several sources of, in principle and in practice uncertainty.
2027900	2030490	A	0.6575059294700623	Observations in the future have not happened.
2030860	2043528	A	0.5679287910461426	Actions in the future have not happened and the causes of future outcomes and hidden states actions as well as the generative processes endogenous changes haven't occurred.
2043624	2052530	A	0.7482006549835205	So there are several fundamental and in practice limitations of the kind of precision that you can get on the future.
2053620	2065430	A	0.7981374263763428	That's planning as inference, it's prospective in contrast and that's expected free energy because it's about the free energy of an expected future.
2067000	2076824	A	0.888817310333252	Variational free energy f expected free energy is g variational free energy F is in equation 2.5.
2077022	2082970	A	0.8920415639877319	Variational free energy as the question says, is based upon present and prior information.
2083420	2094984	A	0.8872852921485901	So variational free energy is like now casting and verging into something kind of like memory as inference.
2095112	2102848	A	0.8728036284446716	We're integrating the past in the sense of our priors and the present incoming data point why?
2103014	2110310	A	0.8535552024841309	And it's like a snapshot evaluation of optimal perception in the purely sensory case.
2110920	2118580	A	0.5134246349334717	And if we include action within variational free energy, it's taking one step optimal actions.
2119480	2122788	A	0.5654009580612183	But those are not necessarily like the ones that's not planning.
2122884	2125464	A	0.6273104548454285	That's just like snapshot decision making.
2125662	2132484	A	0.8181213140487671	In contrast, 2.6 will explicitly consider policies.
2132532	2138888	A	0.8714895248413086	It is a function of policy pi and so it is including planning as inference.
2138984	2141630	A	0.8373881578445435	But first let's look at 2.5.
2148580	2150370	A	0.6986006498336792	Yaakov or anyone else.
2150740	2154770	A	0.8861823081970215	Would you like to just describe one pass?
2155140	2157840	A	0.8658774495124817	What is equation 2.5 showing?
2171220	2193192	B	0.852070689201355	I guess if you mean like describing, say, the first line or the lines in a bit more detail, essentially the energy term is just the expectation of the log of the joint probability of the data and the observation.
2193256	2207810	B	0.7190207242965698	So I don't understand quite why they changed the notation from appendix B, but Y would essentially be the S, I think, and X would be the O.
2212140	2228716	A	0.858218252658844	Yes, anyone could be taking more notes on this, but Y is the observable data and X is the hidden state in the POMDP that we're going to be getting to later x out there, the hidden state is S and O are observations.
2228828	2235660	A	0.8134045600891113	So this is using more of a regression familiar framework and variable notation.
2235820	2244832	A	0.9147000312805176	But this could also be written LNP of O comma S observations and states that's the joint distribution.
2244976	2249380	A	0.7743452191352844	Joint distribution with a comma conditional distributions with a vertical pipe.
2253610	2253926	B	0.5491447448730469	Yeah.
2253948	2261110	B	0.6517807245254517	And so we are also taking the entropy term is only based on the beliefs of our hidden states.
2261180	2267986	B	0.6625821590423584	So people were saying before that the hidden states are unobservable.
2268178	2274360	B	0.7866283059120178	The only thing that we can say about the hidden states are the beliefs about the hidden states.
2275050	2286286	B	0.6425036191940308	I don't think that the hidden states X can ever really appear on their own because we always need to have some kind of belief about them.
2286388	2324380	B	0.4938964545726776	And this was also related to a discussion we were having yesterday that's interesting that they kind of switched between the notation of entropy with H as the functional and just the pure expectation with the E, because the actual definition of entropy is the expectation of the negative log of whatever is in is the variable, which in this case is Q of X.
2325390	2332538	B	0.8142581582069397	And that's actually how we get to the other two expressions with complexity and accuracy and divergence and evidence.
2332634	2355830	B	0.5585091710090637	And there's also a link I think might be wrong with to theoretical physics where free energy is essentially the kind of cognitive statistical equivalent of the Lagrangian, which is defined as kinetic energy minus potential energy.
2355900	2374346	B	0.8613364100456238	So we were discussing yesterday what that means in terms of the energy and entropy in this case, whether the energy is perhaps kind of the immediate cognitive kinetic energy and then the entropy is like.
2374368	2381360	B	0.8671009540557861	The potential energy or vice versa might be going off on a tangent at this point.
2382290	2382990	A	0.9184247851371765	Awesome.
2383140	2386030	A	0.8728502988815308	Let me just add in a few other notes.
2386850	2390020	A	0.6796025037765503	The variables f is a function.
2390470	2393074	A	0.8303203582763672	It's the variational free energy function.
2393272	2396850	A	0.7251215577125549	The arguments that it is taking in are Q.
2397000	2399618	A	0.8219361305236816	That's the variational distribution that we control.
2399704	2402302	A	0.5073800683021545	That's what makes it a variational free energy.
2402456	2404854	A	0.7800534963607788	Q is the one that we control.
2405052	2408726	A	0.8170210719108582	P is the actual probability distribution out there.
2408908	2410600	A	0.739969789981842	Y is the data point.
2410970	2418300	A	0.9138391017913818	So we have on the left side of the equation, variational free energy is a function of Q and Y.
2418910	2427840	A	0.6905907988548279	On the right side, we have three equal signs and these are three values, three ways to phrase this one function.
2429090	2436878	A	0.8011986017227173	In the bottom here, Yaakov has shown how this energy minus entropy can result.
2436964	2443262	A	0.735652506351471	In the second line, complexity minus accuracy, and in the third line, divergence minus evidence.
2443406	2451890	A	0.5928279757499695	So that was like one really interesting thing, is it's not like this is how it is and this is a transformation and then this is the transformation.
2452390	2466780	A	0.5789411664009094	Even though in a sense that was also true, it is also probably helpful to think about this energy minus entropy formalism for what it is, which is the closest to a physics framing of free energy.
2467150	2482400	A	0.8397204875946045	And then think about these two more statistical ontology ways of framing it, complexity minus accuracy, which is like very commonly brought up in the context of model fitting and divergence minus evidence also.
2484050	2485054	A	0.8524129986763	And yeah, great.
2485092	2493970	A	0.5146587491035461	So Blue, if that sounds like a very helpful norm in the description, we'll try to have like a description of the equation.
2494470	2500114	A	0.7963369488716125	The first row says this, this people can type it however they want.
2500152	2510920	A	0.8413286805152893	And then of course, we can replace terms that are in the ontology with the special at sign.
2511370	2525290	A	0.7213791608810425	And that will facilitate finding equations that mention those terms and it will facilitate translating these equations and also the descriptions into different languages, computer languages and human languages.
2525870	2532922	A	0.5722734928131104	So then we can work on it in this ontological space and then make sure that it's accessible to different languages.
2532986	2542990	A	0.9045369625091553	For example, then we thought about this example of like a ball and its location in a bowl.
2543150	2548930	A	0.9163455367088318	So there's a true location of where it is in the bowl X.
2549080	2553800	A	0.8865547776222229	And then there's an observation of where we're observing the ball to be.
2555530	2558882	A	0.884772539138794	And this also speaks to the quantum to classical handoff.
2559026	2568220	A	0.6794226765632629	If the bowl is a swimming pool and the ball is a bowling ball, after enough time, it's going to be at the bottom and not moving.
2568910	2582334	A	0.8042038083076477	If the bowl is like a molecular well and the ball is like an electron or like one atom, then repeated measurement is going to be entirely different.
2582452	2589070	A	0.8282248377799988	And as the ball in the bowl become more massive, it approaches like a classical limit.
2590870	2598770	A	0.871028482913971	Little hints at the continuity between classical statistical and quantum and thermophysics.
2599190	2601938	A	0.8327370285987854	But we thought about this ball being in the bowl.
2602114	2627646	A	0.8269064426422119	So we looked at two different cases, which is when the ball is being observed in the bottom of the bowl, it's like, given the generative model of the shape of the bowl and gravity, which is like a potential energy function, observing the ball at the center of the bowl is, like, strictly the most likely thing that could have happened.
2627828	2631710	A	0.6886249780654907	And then observing it up on a side is like a less likely observation.
2632130	2634030	A	0.6886582374572754	Eric and then blue.
2639570	2640320	G	0.5491447448730469	Yeah.
2640930	2643698	G	0.7290918231010437	So you mentioned that F is a function.
2643784	2649214	G	0.8001460433006287	And as they put it, it's a function of a function, which is a functional.
2649342	2655490	G	0.8699951171875	So the way I think about this is that the function that it's a function of is the Q.
2655560	2658594	G	0.7474299669265747	So the Q is a distribution over your beliefs.
2658722	2661474	G	0.8421711921691895	So I think of that as being a function that you're trying to optimize.
2661522	2667820	G	0.8038899302482605	You're trying to say, well, how do I adjust my internal belief state the Q.
2668510	2671702	G	0.6437409520149231	And that's what I'm optimizing in this equation.
2671766	2675846	G	0.803043782711029	And the functional says that's what I'm trying to optimize is that function and distribution.
2675958	2689406	G	0.8653519153594971	And then it breaks down into the two parts, which is your generative Model P, which says well, P says for any there's the joint probability of X.
2689588	2697134	G	0.8550503253936768	X is the internal state variable that you have a distribution over that's Q.
2697332	2702182	G	0.8616136312484741	And so how do your Observables and your internal states go together?
2702316	2720140	G	0.7964235544204712	And then you have the entropy term, which is well, you want to have maximum uncertainty so that if you're not constrained by your observations, you want to be as agnostic as possible about what the possible values are of your internal states or make that queue as flat as possible.
2721390	2727742	G	0.8529726266860962	As flat as possible under the constraints of the observations going through your generative Model P.
2727876	2731200	G	0.864902675151825	So I guess that's the way I talk through this first line.
2732130	2732830	A	0.9184247851371765	Awesome.
2732980	2735870	A	0.8583195805549622	Thank you, eric blue and then Brock.
2736610	2738694	D	0.9735485315322876	So, Eric, that was super helpful.
2738842	2743634	D	0.6921703219413757	I've been since yesterday trying to figure out what exactly is meant by Q.
2743752	2744420	D	0.571090042591095	Here.
2744790	2749470	D	0.9376890063285828	I get later on we're going to talk about Q as representing beliefs.
2749550	2757640	D	0.8036617040634155	But as it's given, as it's laid out in chapter two, they don't actually define Q at all.
2759290	2762726	D	0.8835229873657227	So I am having a hard time up here.
2762828	2767420	D	0.7540553212165833	They describe it right before they get into that box.
2767870	2773050	D	0.9045073390007019	They describe Q in Section 2.6 as an approximate posterior.
2773390	2776234	D	0.8393617272377014	So approximate posterior of what?
2776272	2781280	D	0.8782656192779541	And should we denote Q as the function of beliefs here?
2781650	2786782	D	0.7511524558067322	Even though it's like they're skipping ahead and looping back at the same time?
2786836	2788286	D	0.7770189642906189	It seems like, or feels like to.
2788308	2797466	A	0.8494004011154175	Me, q is the variational distribution where the form of the posterior has been chosen.
2797658	2814682	A	0.9004045128822327	So exact bays using matrix multiplication, basically, or sampling based approaches to approximate Bayesian computation, like Monte Carlo, Markov chain, those try to recapitulate the actual form of P.
2814816	2823318	A	0.9116749167442322	So if P is truly bimodal, then exact Bayes or sampling based approaches would try to reconstruct that distribution.
2823494	2841194	A	0.5325679183006287	In contrast, the variational approach gains tractability because you're choosing a posterior, approximate posterior, a Q of a form that you know is going to have a tractable optimization scheme and there's message passing algorithms that can do it computationally.
2841322	2860230	A	0.8673978447914124	So if you choose Q to be a gaussian, then depending on how you parameterize it, it's either going to be seeking the mode and go to the hump that's bigger and center around that with a variance, or it'll just spread out a lot and go over and try to cover as much probability density as it can by covering both humps.
2861050	2880346	D	0.8000874519348145	Okay, so the variation of free energy function F as a function of the function representing the approximate posterior Q and the data Y is equal to the negative expectation of Q given the observations x times the natural log of the probability of the joint distribution of the data and observations.
2880458	2882234	D	0.8785041570663452	Is that the English translation of that equation?
2882282	2883742	D	0.7148637175559998	I think I got it finally, after.
2883796	2888046	F	0.7699106335639954	Like two days minus close, I would.
2888068	2890770	G	0.7844353914260864	Say one thing I would say slightly different.
2890920	2895700	G	0.6987379193305969	It's the expectation over Q, not expectation of Q.
2897830	2900370	A	0.7709676623344421	We actually talked about that a little bit.
2900440	2906120	A	0.5215622186660767	Like here we had negative expectation over Q and yes.
2908330	2910294	A	0.7064933180809021	Okay, blue then.
2910332	2915034	D	0.6622910499572754	Brock yeah, sorry, the ontology terms don't match the terms that are given in the book.
2915072	2917146	D	0.8539060950279236	And I put in a request in the questions here.
2917168	2925450	D	0.8601640462875366	If we can append the ontology because they use data is the same as observations.
2927410	2931040	A	0.6767892837524414	Those are different words, but they're similar.
2932930	2938910	D	0.6758114099502563	Yeah, there's a lot of synonyms or synonymous use, and so I would hate to step on them.
2938980	2943026	D	0.5269541144371033	But also the terms aren't in the ontology at all.
2943048	2946180	D	0.8696889281272888	So maybe we should update the ontology to reflect the terms.
2946630	2953074	A	0.6576922535896301	We will absolutely add terms into the supplemental or eventually into the core terms if that's required.
2953122	2963470	A	0.7866379022598267	But just leave a comment or email activeinference at gmail to add terms to the ontology if there's a term that you want to tag that's not being tagged.
2963650	2964410	A	0.5254635214805603	Frock.
2968190	2983360	C	0.9079226851463318	I feel like you may have indirectly answered this just now, but in the math discussion yesterday, we were talking about specifically about the Q, the distribution of beliefs over there.
2986290	3001958	C	0.7082192897796631	If X is this uncertainty term, this hidden state, like, what what value could it possibly have?
3002044	3003640	C	0.8247736096382141	Where is it coming from?
3004250	3006614	C	0.7090803384780884	Or is that so Q?
3006652	3008946	C	0.8660624623298645	Is this property of distribution of beliefs?
3008978	3016780	C	0.9088360071182251	Is X the belief value and that's being generated in the previous step?
3019230	3023420	C	0.7799952626228333	I think we in the Bowl Marble thing.
3024190	3035870	C	0.8428829312324524	Like my question was, or I was just pointing out, I guess that the hidden state is changing however it wants, perhaps below our threshold of observation.
3037570	3045234	C	0.5474404692649841	That's not reflected in the equation for obvious reasons, but then maybe it crosses a threshold and then we update or whatever.
3045352	3049058	C	0.736918568611145	But where is that X coming from if it's a hidden state?
3049224	3049986	A	0.7328410148620605	Do you see what I'm saying?
3050008	3051986	C	0.4980185329914093	Is kind of an ontological problem there.
3052008	3053734	C	0.6337534785270691	I'm trying to solve it's like it's a hidden state.
3053772	3056790	C	0.585024893283844	How can have a value or be a variable if it's hidden?
3058250	3062502	G	0.7034580111503601	Yes, Eric, I'll take a stab at that.
3062636	3065560	G	0.8205004334449768	So, as shown in the pictures, there's two hidden states.
3066190	3072140	G	0.7685697674751282	There's a hidden state of the real world, and then there's a hidden state of your generative model.
3073470	3075706	G	0.813517153263092	And the generative model is whatever the model is.
3075728	3077358	G	0.8019970059394836	And that's however the model was built.
3077444	3083200	G	0.9106853604316711	So once you have the parameters X, then that's what you're working with until you change your model.
3084370	3087470	A	0.8236367702484131	That was my yes.
3087620	3094900	A	0.5449948906898499	And over evolutionary or developmental times, you get swept off the table if there isn't adequacy, at least.
3095350	3101220	A	0.6368629932403564	But in a simulation environment, you could make the X and the X star to be anything but.
3102170	3106870	A	0.852921724319458	The hidden state is as modeled by the generative model of the cognitive entity.
3108090	3121450	A	0.8713546395301819	So in the cognitive model of the entity, in the bowl example, they're modeling an unobservable, which is the location of the ball, and they're modeling an observable, which is the observation of location.
3121870	3124410	A	0.8790472149848938	So both of those are in the generative model.
3124560	3129840	A	0.8754913806915283	And then the generative process would be like there's somebody with a magnet who's moving the ball around or something, or it could be anything.
3131810	3136778	A	0.8871158361434937	We're going to have five more minutes before we close this session.
3136954	3141106	A	0.9018250703811646	So what does anyone see in 2.4?
3141288	3145266	A	0.8668153285980225	What is free energy minimization doing?
3145448	3166666	A	0.7036579251289368	And what does it mean that variational free energy is an upper bound on the negative log evidence, just one quick qualitative read is that this is what we would actually like to be minimizing.
3166778	3173170	A	0.5668994784355164	This is the floor we truly want to reach would be unsurprised about observations.
3174550	3183526	A	0.7454386949539185	However, that is intractable because we don't have access to the form or the parameterization of the true generative process.
3183708	3185666	A	0.8545514941215515	What is attractable approximation?
3185778	3190818	A	0.8397653102874756	Approximate Bayesian computation is variational Bayesian inference.
3190914	3197530	A	0.8660823106765747	So here's f q of y q and y and then this is a KL divergence.
3198910	3205306	A	0.7780762314796448	One vertical pipe means conditioned on like X conditioned on Y x given Y.
3205408	3215742	A	0.8820315003395081	Here probability of a hidden state being the case given a data or the other flipping, which would be the probability of given data point given a hidden state.
3215876	3218890	A	0.8279393911361694	Two vertical lines is in divergence notation.
3219050	3227140	A	0.8023984432220459	And that's the divergence between this and that between Q of X and between P of X given Y.
3227590	3245098	A	0.7969937324523926	So that reducing this divergence, which has very nice properties in terms of its implementation, is bringing us to lower our upper bound on what we really want to be minimizing in a tractable way.
3245184	3248090	A	0.8255482316017151	So that's one aspect on free energy minimization.
3248590	3259514	A	0.8219665884971619	And then here is reframing the two ways that free energy, variational free energy can be minimized through perception, the updating of beliefs.
3259642	3263406	A	0.7915229797363281	And we'll talk more about the continuity between perception and learning.
3263588	3272558	A	0.8922929763793945	Like if one observes the ball moving across the visual field, is that perception or is it parameter learning updating the location of the ball?
3272654	3281250	A	0.9085675477981567	That depends on the timescale and such or action can be taken so that the expected observations are changed to maximize evidence.
3282170	3298150	A	0.8170831799507141	And then in the last minutes, Ali asked, can a hidden state be an unquantifiable quality, like a qualia quality or like an unquantifiable quantity?
3303980	3324992	E	0.6844830513000488	Well, of course most qualities can be parameterized, but well, perhaps we can think of a hidden state as a pure quality that cannot be fundamentally unquantifiable and we cannot parameterize it.
3325046	3327570	E	0.6367700695991516	I mean, can we think of such a hidden state?
3332340	3344404	C	0.8014750480651855	Brock, I was just going to say probably the quantifiable way to state that question is are there intractable hidden states?
3344522	3347210	C	0.5339560508728027	Which the answer to that is like, obviously yes.
3348460	3351192	C	0.9026692509651184	Can we get some estimate on it?
3351246	3351848	A	0.46103888750076294	Yes.
3352014	3356712	C	0.7775686383247375	But then is that commensurate with the idea?
3356846	3359892	C	0.6485759019851685	And then does that mean it's a quality and not a quantity?
3359956	3360830	C	0.5666733384132385	I don't know.
3361280	3366780	A	0.8887113928794861	And also to the earlier discussion about the dimensionality of the generative model and the generative process.
3366930	3385872	A	0.6750764846801758	The unquantifiable part could be like the baby's happiness, but then we're able to still have an inference on the happiness as a parameter x, the underlying state that's giving rise to the different sounds without necessarily knowing the distributional form of happiness.
3386016	3390656	A	0.7254101634025574	But we have to have a distributional form for the variational approximation of happiness.
3390768	3397380	F	0.9105697274208069	Joe, I was just going to say, I think that this qualitative versus quantitative distinction is interesting.
3397450	3401156	F	0.8064844608306885	If you're thinking about like a light source, it's a source of electrons.
3401188	3412632	F	0.8258800506591797	Well, you could count the number of electrons, but whether or not there is a light source, there a source of electrons in this framework I'm proposing, you're not counting that, you're just saying there is a light source or some other dimensions.
3412696	3414376	F	0.8382855653762817	That's the thing we've been calling dimensions.
3414488	3419950	F	0.6118849515914917	Yes, you can count the number of dimensions, but given that they're all different, they're not really relatable to each other.
3421140	3421744	A	0.6283750534057617	Thanks.
3421862	3423170	A	0.9295148253440857	Yes, very interesting.
3424100	3446500	A	0.8467684984207153	So in our final seconds, we'll just point towards 2.6, which is expected free energy g and its prospective, which is going to enable inference conditioned on action x hidden states, the world conditioned on policy selection.
3446860	3454644	A	0.8557725548744202	And that is what is going to allow comparison of different policies, which are sequences of actions over a given time horizon.
3454772	3462332	A	0.8071156740188599	So minimizing the divergence between two things minus some things, reframing it as a few things.
3462466	3479248	A	0.6842702031135559	So we didn't go as much into 2.6 as we did into 2.5 because 2.5 is an important precursor and simpler cousin of 2.6, but it's like super important and especially to those who are here and those who are not here.
3479334	3502644	A	0.7320919632911682	We would really love over multiple cohorts when everybody will hopefully be able to join and rejoin as participant, as facilitator, to be able to annotate all the equations with what they mean, because what is there to be said about expected free energy without some understanding of what this is actually discussing?
3502692	3505956	A	0.8229004144668579	What variables go in, what is it doing, how are some of them being related?
3506068	3510404	A	0.7152868509292603	So we want to learn this and we can do it collaboratively.
3510452	3520124	A	0.7263426184654236	If people just pick and choose affordances, they see annotate some things and then they can highlight it and say I'm not sure about this, but I just wanted to add it.
3520162	3521036	A	0.6971480250358582	Can somebody improve it?
3521058	3521564	A	0.6936384439468384	And so on.
3521602	3525276	A	0.6396999359130859	Like we're all there to help each other learn and improve our epistemic.
3525308	3527676	A	0.8370084762573242	Niche 2.6 is expected.
3527708	3528530	A	0.5280101895332336	Free energy.
3528900	3542950	A	0.550414502620697	There's further discussion on Expected free energy, showing how it's composed of these five sections, and that by leaving some out you get different special cases that are quite important.
3543720	3550488	A	0.8188897371292114	For example, leaving out this, you get this leaving out just one, you get 2345 and so on.
3550574	3551896	A	0.7182033658027649	Figure two six.
3552078	3555876	A	0.6935576796531677	And then there's a summary of the low road to active inference.
3555908	3557032	A	0.8315856456756592	That's chapter two.
3557166	3562796	A	0.9064425230026245	So this concludes chapter two and our discussion in cohort one.
3562978	3568376	A	0.7994471788406372	In the coming two weeks we're going to be discussing chapter three, the High Road to Active Inference.
3568568	3576080	A	0.8798187375068665	It's going to take another tack and we will go through it in the coming two weeks and be addressing people's questions.
3576150	3581392	A	0.5844920873641968	But hopefully we can continue answering and addressing just wherever we see fit.
3581526	3586420	A	0.6867255568504333	No one's going to do it all, but we do need everybody to do some hopefully.
3588040	3590656	A	0.8915601968765259	We're going to now close recording.
3590768	3596960	A	0.8993889689445496	We'll then take a 1 minute break and then in this room we're going to continue with Dot tools.
3597120	3604788	A	0.9144595265388489	If you want to continue discussing the textbook or anything else, then head up to a room up and to the left.
3604874	3608856	A	0.7969083786010742	But in this room in 1 minute we're going to continue with Dot tools.
3608928	3609980	A	0.9182262420654297	So thanks everybody.
3610050	3610990	A	0.5413564443588257	See you soon.

start	end	paragNum	speaker	confidence	startTime	wordCount	text
1450	35026	1	A	0.81	00:01	63	Okay, it is May 19, 2022 and it's week three of the textbook group first cohort. We're in week three discussing appendix A and B and there are some notes in the sections of the book. In the chapters. There are also some ideas and questions that people have raised. So we'll go to the questions and then start with the most upvoted.
35058	65680	2	A	0.99991	00:35	74	So feel free to add more upvotes if you want to discuss it, like even in this discussion. And then hopefully if people are available to take notes in this section, that will help add their thoughts in and also capture what the speakers are saying. And then we'll look at the question and then try to come to different answers and just add more information that people can add more structure to later.
67970	83670	3	A	0.65	01:07	38	Okay, the first question says appendix A is described as the mathematical background. So maybe question one for the authors or for you. What is the process of determining what is figuring ground for the formalisms of active?
86840	101880	4	A	0.99966	01:26	36	What other math concepts and formalisms are important for learning and applying active inference? And then three, what are some resources and approaches for learning math that help us learn what is useful for active inference?
127200	152900	5	A	0.88853	02:07	65	Anyone can raise their hand or we can just start to add some annotations here, like what should be included in the primary regime of attention with a reading of a book, either linearly, like some books are, or in a maybe moving around the sections. So what should be in the chapters? What should be in the appendix? What is not in the appendix?
156690	161790	6	A	0.99992	02:36	14	What did people expect would be in the chapter in the appendix? Not covered.
186930	189280	7	A	0.9	03:06	6	Yeah, Jessica and then anyone else?
191650	210420	8	B	0.99	03:11	34	Yeah, I was wondering about the multiplication of the matrices that we covered yesterday. Which equations have the multiplication of the matrices? A couple of examples just so I can play around with them.
213530	216790	9	B	0.97	03:33	6	In the actual active inference equations.
221270	248410	10	A	0.99777	03:41	57	Does anyone know? One, this is referencing the way that the appendix A is starting with linear algebra and then introducing this operation of multiplying two matrices. Or to get a product if someone can find like an equation that we already seen or some other equation while I'm typing up that question, that would be helpful.
315250	329780	11	A	0.98488	05:15	34	Could anyone just describe what they thought the intention was of starting with linear algebra and using 8.1 as the first equation of the appendix? Or we'll return to just the more general questions.
339560	345780	12	C	0.4	05:39	12	Yeah, I mean, learning our algebra is kind of the most discreet.
348220	386570	13	A	0.97	05:48	50	I don't. Want to say fundamental, but like practical and comprehensive way of kind of working with a large space of data, I guess, together and computing on it. So it's kind of the basis for the discrete parts and maybe easier than the nondiscrete parts. Good reason it started.
397850	445846	14	D	0.78	06:37	98	I guess if the question was where is linear algebra used in the active inference math? When you go to appendix b. Then you've got the equations of active inference and these are all expressed in terms of large vector spaces of variables, so probability of distributions. And so for example, on page 245, you've got this dot notation which is the expectation of a value. So that goes directly back to that first section of appendix A is what does that mean in terms of how do you take an expectation of a large vector of things?
445948	447990	15	D	0.58092	07:25	6	How do you express that compactly?
450490	485970	16	A	0.54043	07:30	69	Thanks. Could you unpack we looked at the dot notation a little bit yesterday and they were mentioning how well, I think it was actually one of the questions too. Let's just see if someone asks this, okay? They say the dot operator in a three, the dot operator is equivalent to standard matrix multiplication where the first matrix has been transposed. So what is the relationship with expectation?
487910	493750	17	A	0.62952	08:07	12	How does expectation for anyone, how does expectation relate to linear algebra?
506160	549400	18	D	0.89	08:26	120	Yeah, expectation is the probability of a value, the average probability of some value. So you're going to take all the possible values and you're going to multiply them by the probability of that value and then divide by the sum of the probability of normal is to one. So that will be the expectation. So if you have a whole array or a vector in your distribution, you can express that all in compact notation by saying with this simple way of saying we're going to take every one of these terms and we're going to multiply it by the probability of that term and the probability is some to one. So then that will be your average overall.
551340	553550	19	A	0.99947	09:11	5	Awesome, thanks for that answer.
559200	577152	20	A	1.0	09:19	39	Okay, thanks for that awesome answer. Does anyone have any other thoughts on just this first question and then we'll continue. So what is figuring ground? We're learning active inference in the chapters. That's why the chapters are there.
577286	612380	21	A	1.0	09:37	71	The appendix is there to somehow lightly supplement that. Like they describe it as an introduction or a refresher to the basic mathematical techniques. And linear algebra is the first section, a two that is discussing a lot of important things like derivatives and Probabilities taylor series variational calculus and Stochastic dynamics are going to come in in the coming chapters, but the linear algebra is important for the upcoming chapters.
616580	621730	22	A	0.85	10:16	8	Okay, so this question asked in a three.
627460	667760	23	A	0.99864	10:27	111	They say the dot operator is equivalent to standard matrix multiplication where the first matrix has been transposed, which is flipped like it's the operation in Google Sheets or Excel right click paste transpose. They say that in the case of column matrices, this is equivalent to the dot product. So the dot operator here is like a generalization of the vector dot operator, the sum of the products of the corresponding entries of the two sequences of numbers. So the questions here were what is interpretation, implication or use of a dot product of vectors? And then what is an interpretation, implication or use of a more generalized dot product.
667830	670290	24	A	1.0	11:07	8	And there looks like there's an awesome answer.
711750	719350	25	A	0.96147	11:51	19	What would be a situation where if anyone can think of one, like the Dot product would be applied.
721610	734780	26	A	0.9999	12:01	30	We're trying to do this in this situation. This is the operation that we needed. We wanted to compute matrix A on something of something. We used B dot C.
742990	780670	27	C	0.99809	12:22	47	Continue eric's description from the probability like you have expectation probability and you wanted to kind of see the error or whatever the difference, I guess between observation and the expectation, then a Dot product would give you that answer or it's a probability density of it.
787930	815730	28	A	0.99999	13:07	51	What are some ways to compute differences? Like if there are scalars you can subtract like five minus three or something, then it was mentioned of cosine similarity. Does anyone think of or imagine another way that you could compute the difference between two different kinds of features of different dimensions.
823970	841190	29	A	0.93	13:43	40	Like another example is divergence because that's like measuring the differences between these two distributions and there's different divergences. The KL is one of them. What other things are in this space that might be just tractable possible distance measures?
843850	867280	30	D	0.99869	14:03	44	Well, these are the same dimensionality in all these cases. But another very popular one is the sum squared difference, sum squared error. And then you can have absolute different value difference. So these are called different norms. I think that's the right word.
867730	878530	31	A	0.99981	14:27	16	Yes. There's also the Ginny index, the Jacard index, euclidean distances like sample to sample distances.
907910	918150	32	A	0.9798	15:07	20	So could anyone talk about L zero, l one and L two, et cetera norms? While I'm adding some links.
933520	955760	33	D	0.75781	15:33	55	If I have the right numbers here l two norm would be the square root of the sum of squares. L one norm would be the average of the absolute absolute value differences and L zero. I don't know what that is. I don't think it's a difference, is it? It's like a presence absence.
956680	957430	34	D	1.0	15:56	1	Okay.
960520	985630	35	A	0.99934	16:00	51	That kind of encoding is used in a lot of different algorithms and then the L two norm is the sum of squares. So the L two norm is what is used like in at least squares regression, classical statistics, ttest inova that's a lot driven by the L two norm.
988880	996400	36	A	1.0	16:28	17	And all of that is like in the genre of what the A three notation is describing.
998820	1008310	37	A	0.9498	16:38	33	But depending on what the B and the C are and what the A is, et cetera, there's just the dimensions of everything and other operations that are happening before or after it.
1013190	1025510	38	D	0.99998	16:53	35	Later on they mentioned the quadratic, which is the L two norm. So that's where you have the same term as multiplied by itself. Or if you have cross terms, then you get the covariance.
1028780	1036440	39	A	1.0	17:08	15	And that's going to be used later with the LaPlace approximation and other approximation techniques.
1039760	1048780	40	A	0.99962	17:19	18	Any other comments on a three or dot product? Or just like this kind of linear algebra topic?
1061240	1083820	41	A	0.99973	17:41	42	Linear algebra the basics, the trace and the determinant, which are probably less important than the dot product but still come into play, the derivatives, how things change with respect to each other, possibly time, possibly some other surface, and then probabilities.
1088320	1097170	42	A	0.97572	18:08	28	So does anyone have any other comments or thoughts on that whole section? Like anything that they read or like had uncertainty around up to equation A 22?
1109860	1115010	43	A	0.99983	18:29	15	Great. That no one has uncertainty around any of the equations up to A 22?
1118600	1125590	44	A	1.0	18:38	16	Yeah, obviously there is a lot to learn and to understand and we're not going to.
1128200	1135210	45	A	0.6464	18:48	25	It's just I don't know people who want to think about that. Like, what are we learning in Appendix A? Why is appendix A there?
1137420	1178820	46	A	0.99868	18:57	89	Hopefully we can start to understand some of this in Math Learning Group. The people who want to be really engaged with the math questioning and process and also connecting it to computer science into the applications has been happening and then hopefully everyone can benefit from this, like finding the resources and stuff. Because the relationships between the terms, relationships amongst the terms are driven by formalisms. Like the way that preferences and expectations are framed in active inference is related to equations, not only a conceptual linkage.
1190120	1195800	47	A	0.99967	19:50	15	Maybe here we can just write Math Learning Group and copy this to the resources.
1199100	1222640	48	A	0.98	19:59	51	Okay, so what kinds of examples do you think would be useful for learning the equations and concepts in Appendix A? If the equations felt too general and not applying to anything in particular, what example or scenario or would have made sense or you felt would have made that clearer?
1227020	1261140	49	A	0.99	20:27	85	Like, we're running a business and first this person wants to do this and then this person wants to do that? Or what example or what would somebody have expected to have found or wanted to have found that would have made it clear? Examples or other exercises or a format? This is kind of like a math lecture. It's kind of directional versus like, what other things could happen where you would feel like you were understanding and learning this and engaging with it.
1261210	1321620	50	A	0.66534	21:01	93	Yes. Thanks, Brock. I was kind of reminded of a video that Karl A presentation where he in real time demonstrates and helps you work through a pattern of red. Like it's like three dots sort of that are moving around red, green and pattern. And so understanding just a simple visual example of how active inference would work that doesn't require the formalism to just surface level kind of pattern recognized, that would be like ideal maybe of then bringing in the math for this part and that part and some scaffolding.
1322460	1371740	51	C	0.96893	22:02	98	But yeah, I'm not sure the appendix, I'm not sure it was meant to be bred in. I'm not sure there's a great way to read one order or another there because you kind of need it, but you kind of it's a lot too, right? Yeah, great point. And the organizing team for the textbook group which was open to everybody who wanted to join the and the comms weekly meetings. So for future cohorts, people could totally be involved with planning it, communicating it, doing anything for future cohorts as a co organizer or other role.
1372240	1418124	52	A	0.93661	22:52	101	But we talked a lot about the reading order. Would it make sense to read them in without appendix or putting the appendix last or somewhere else? One was short and hopefully the one week of regime of attention on appendix A and B is like we're just skimming it in a week. We're not getting a PhD in math, we're not generating these equations on a blank piece of paper. We're just like seeing the forms that will be used and then some of the key areas in the order that the authors thought that those background topics were important.
1418242	1465740	53	A	0.9972	23:38	117	So like for appendix A, linear algebra, which is what we talked the most about, because it's going to come into play most quickly, especially with these notions of expectation and differences which is going to come into play with everything that's going to happen. Taylor series variational calculus and Stochastic dynamics we talked less about. They happened in a later order within the appendix, but hopefully there will still be many questions on them because they're probably also areas with a lot to learn and to clarify. And also appendix A is like not the final resource of it diffusing into us. Like something that was really important that came up in the math learning group.
1465890	1512860	54	A	0.99994	24:25	122	Is there's not like a glossary of variables or table of variables so that's something we can work on with notation is connecting the variables to natural language ontology terms and then also things like this what do we even look up or a symbol or what does it mean when there's a double arrow? There's a lot of things that like a lot of dots that could be connected if people ask them and then we can probably get a response like what was the double arrow in a three? Does it mean anything that they're offset? Or what are the parts of this shape that are mattering for the background of learning? Active inference, anyone can ask like any question.
1512930	1539060	55	A	0.99999	25:12	65	If it's coming to mind as an uncertainty, then writing it down just anywhere is really helpful. Because then on the first pass through learning or a primary branch of learning, we can just go, okay, here's what a one shows, and then have that in a way that reduces uncertainty more rapidly than how any of us would just give an ad hoc explanation.
1545160	1576990	56	D	0.70304	25:45	86	Moritz wrote one thing that I always find useful in thinking about matrix, linear algebra matrices is to write out pictures of the matrices in their dimensions because often the notation, you've got these I's and J's and K's and stuff going around, but you don't know how that maps out into the actual elements of the matrix. So those illustrations are always helpful that could be added into this book. They're in a few places, but not that much. There you go. Things like that.
1579120	1626416	57	A	0.82	26:19	136	Yeah. And also connecting the programming experience that people might have even in no code tools like Excel Google Sheet, you have rows and columns, ones have numbers and ones have letters, but they can be other things. And then anyone who's used something like Python or R or even just doing some statistical calculations, like if you put two lists of numbers into the t test, that would be vector versus vector something, and then you would address the fifth element of the list. And then if it was in Excel, you would need to address two numbers to say where you were in the matrix. And then the tensor is just any number of more than three dimensions, which is sometimes like it's like, oh, but how spatially are we going to see it?
1626438	1644580	58	A	0.55319	27:06	43	But then we're familiar with working with spreadsheets and data sets that are larger than two by two or larger than two dimensions, and that will come into play a lot. And the numbers can be representing probabilities or be used for probabilities.
1646840	1677680	59	E	0.71	27:26	74	I also Daniel in the chat shared a link to a Python notebook that is a link, and it shows kind of the operations on matrices. And you can see both the mathematical yeah, there you go. So you can see the actual matrix and you can actually run code too, there. So that particular awesome. So I pasted the link in here and then we can fill out the rest of the row.
1679060	1699760	60	A	0.99989	27:59	40	But then someone could be looking for a guide on matrix or programming and matrix or something before this state. So then each person, if they find it and add it here, will have a lot to share and learn.
1705340	1730980	61	A	1.0	28:25	47	Okay, does anybody want to like, if they were the one who asked it this question, I i think it's probably referring to long equations that that probably none of us are ready to give answers to at this point. Unless somebody wants to contextualize this question.
1738320	1745890	62	A	0.99954	28:58	20	I'm hoping a read of chapter four will elucidate when the time comes. Just exploring the formal foundations here. Okay.
1751470	1752220	63	A	0.75	29:11	1	Okay.
1756370	1765630	64	A	0.99084	29:16	20	Does anyone have a favorite or recommended textbook that it contains the KL divergence and Dearsley distribution in the index.
1768230	1780094	65	A	0.91382	29:28	35	This is for the math learning group. Okay. Maybe we could tag some questions too, and so then we can know which ones they can look at. Wikipedia is good. The Kale divergence is approachable.
1780142	1786280	66	A	1.0	29:40	19	The deerish lay is tougher. The links take you to definitions that enable you to put the pieces together.
1788490	1801500	67	A	0.99	29:48	28	Okay, any comments on KL and Dear schley from someone who is familiar? Otherwise it sounds very technical and we don't need to go into it right now.
1816590	1820700	68	A	0.97989	30:16	10	What is up with dividing by sigma in a 31?
1824390	1867150	69	A	0.999	30:24	80	Also detail, but we'll just see if there's any questions that are not going to be specific details because math is very hard to understand sometimes on the fly and there's definitely a space for the interactive discussions around math that are unrecorded which are important. However, this is a little bit of a different format, which is how I'm hopefully interpreting people's activity rather than their disengagements because they are here and hopefully have at least scanned these chapters.
1875910	1914560	70	A	0.9904	31:15	51	Anyone can raise their hand at any time or add questions or upload things. Mean field mode, equation nine, equation 16 probability resource, statistical question, appendix B just to see if there was any. Also seem like all details. So we have 23 minutes. What does anybody want to ask now?
1915010	1928580	71	A	1.0	31:55	30	Or is there some question here like the most upvoted one or some other one that's far less challenging that they would like to address for the next 20 minutes?
1970350	1974410	72	A	0.99991	32:50	8	Did anyone read Appendix A or B visually?
1982990	1999278	73	E	0.99984	33:02	39	Did you just ask if anyone read it? Yes. What would anyone like to share about reading it? I totally read it with a highlighter and made several notes in the margin in my physical book. It was challenging.
1999374	2033660	74	E	1.0	33:19	55	A lot of the notation I feel like is crazy, even just independence b was it the very first thing? Are they using O for observations or are they using O for outcomes? I was totally unclear on that. Completely. So like state inference, Markov decision processes like the states that influence outcomes O, right?
2034030	2043454	75	E	0.99963	33:54	31	Literally. That's what it says on the top of page 244. So it says that the variable O is an outcome. Is an outcome and observation. Are those the same thing?
2043492	2059374	76	E	0.99999	34:03	34	Are they interchangeable? I've always thought O was observations. I put that question in there in the past. It's already there and then the question right before it. This is a really tricky section.
2059422	2091918	77	E	0.80228	34:19	89	Right? So the two questions that are in Appendix B that are together, it says the likelihood of observations given a policy is not straightforward to compute. This is because POMDP problem is structured so that policies pi influence trajectories indicated by tilde of states S that influence outcomes o without a direct influence of policies on outcomes. The problem then involves the sum over trajectories of states to marginalize these out and find a marginal likelihood of observations given policies. Like what does that these even refer to?
2092004	2115270	78	E	1.0	34:52	68	If you look in the questions in Appendix B, they're there in the questions already listed. I felt like it was very obscure and not super clear. And also the notation gets crazy and I'm really a stickler for defining every single variable and symbol in an equation to understand the math so that I can actually read it in English. So I was a little bit lost.
2117690	2147280	79	A	0.98444	35:17	73	Totally agree. Thanks for sharing it, Eric. Just with respect to that particular question, I think that these refers to the trajectories of states and you can see that because the sum is over the tilde s. So the tilde s is the trajectories of states. So you sum over all the trajectories of states to try to figure out, well, what are we going to get if we apply a given policy?
2148390	2163510	80	D	0.98588	35:48	34	That's my intuition about that. And I suspect also that observations are outcomes. They're sensory outcomes. They're the outcome of a generative model or a generative process, but they're the outcome that is observed.
2168090	2187280	81	A	0.99977	36:08	52	How many dots we need to actually unpack and connect for unsighted grammatically vague sentences. Right. So thank you. My uncertainty has been reduced substantially just by figuring that out. But I felt like that the whole time I was reading the appendix, I was like, what is even going on here?
2188690	2189440	82	A	0.99	36:28	1	Yeah.
2192290	2223442	83	D	0.489	36:32	100	Sir, the way I view it is like exercising. You're not going to run a marathon the first time you set out, but the more you stretch and warm up and I think that's how I kind of view this. Going through the appendix is kind of refreshing on what you used to know about math, at least for me. And so that's like stretching and doing a little bit of exercise last week. And then every time you go through it, every time you see the notation, you've slept on it some more, it becomes more and more familiar.
2223506	2245994	84	D	1.0	37:03	79	And so you can start to put together bigger, bigger chunks. Some vaguely you can map to the notation and the math. Other ones it's still going to be dangling, but that's okay because there's fewer dangling things that you're groping with at any given moment. So I think it's great to go through these dependencies and see it all through one pass and all right, so I didn't run a marathon, but that's okay. I ran 2 miles.
2246042	2273274	85	D	0.99813	37:26	44	That's pretty good for first start. Thanks, Eric. Another metaphor is just like, oh, yes, please. Jakub. Yeah, I was just going to comment on what Blue said with some confusion about the notation because I asked the question about the marginalizing of state.
2273312	2303038	86	G	0.99999	37:53	77	But then I was also confused about the O notation because in equation B one, it seems that it's a trajectory over outcomes. But then there's also an O without a tilde on it. So does that mean that it's an outcome at a particular point in time? But then there's also a bold o, which implies that it's kind of a vector. But if it's a trajectory of outcomes, that also seems to be a vector.
2303134	2331660	87	G	0.58	38:23	60	Like a trajectory for me implies some kind of vector form because it's a sequence of outcomes given a policy. But then what does that bold O mean if not trajectories over outcomes? So just to kind of add to the confusion, I guess. Thanks, Jacob. I'm glad we're groking we're on the same page, you and I for sure.
2334690	2367640	88	A	0.99951	38:54	79	It would be awesome for description or any notes. What is the reading or a reading in language? The this of this conditioned on that is this over that of these two things. The first thing is this. The second one is that an example of that is in this situation, here's the script, here's the graphical abstract, here's the paper, here's the person you could ask, which one of these do you think is going to be interesting?
2373790	2382970	89	A	0.71	39:33	30	Okay, so Mike asked, and I'll copy this in for those who have read the entire book. Did you find the appendices useful as you went along through the chapters?
2398940	2399880	90	A	0.91861	39:58	1	Ali.
2404300	2425330	91	F	0.99	40:04	37	Yeah. I also think that if the materials in these two appendices were to be integrated in the linear narrative of the whole book, it would be much more useful than separating out as separate appendices. Because.
2427700	2451320	92	F	0.99321	40:27	49	As we see, that's my own experience. Well, reading the chapters independently and in isolation of the narrative doesn't give me a sense of their concrete applications and I don't know how to use them and how to use all of these equations, and everything becomes much more fuzzy.
2461800	2493040	93	A	1.0	41:01	72	I hope this is not controversial. That linear reading of the appendix as something that you look through is extremely confusing or extremely imprecise because there's many symbols introduced that might have been introduced earlier, like O is probably discussed earlier, but then it's briefly just sort of mentioned. And then there's a lot of symbology that's not mentioned. What it is. So it's hard to read the appendix without prior knowledge.
2494260	2500720	94	A	0.72175	41:34	8	Yet, ostensibly, Appendix A is the introduction refresher.
2505020	2524780	95	A	0.96964	41:45	33	So is this the introduction offering? They hope so. They expect and prefer perhaps the appendix to go some way towards remedying it being the maths required to understand this not complicated basis.
2527550	2556770	96	A	1.0	42:07	70	The multidisciplinary basis means it is often difficult to find resources that bring together the necessary prerequisites they're going some way. So this is like one kind of plank out there from their point of view, from the chapters. This is stuff. Ali I also kind of agree on what ordering these are really interesting questions. So then what is the next connector that picks up here with this artifact?
2558550	2592750	97	A	0.55109	42:38	81	Because someone mentioned how Moritz mentioned you could basically do a whole course on deep learning to apply most of the matrix operations and whole courses on the math of them so they can't go into all this detail. They can't spend 1 hour or like multiple coda pages on just what this means. So there has to be some kind of compromise, but then there's not going to be one specific perfect compromise, especially with like, length and audience considered.
2596390	2639466	98	E	0.69	43:16	117	I undertook a very detailed linear reading of the book because that's just kind of how I am, of the appendixes at least. But I think for many people that I've been there before, where you're staring at math equations like this is gibberish. Just to read the text and to read it and go through it, it's helpful to know that it's there. So if nothing else, at least you can be reading the book and then be like, oh, I remember kind of reading about Taylor series expansion in the appendix. And so then you can just go back and just having access to it or the refresh recall access, even though it is confusing.
2639498	2667510	99	E	1.0	43:59	66	I agree to try to a linear read of the appendix. I think skimming over it or deeply reading it and then being able to refer back to it is useful. Thanks, Blue. So in next week, we're going next two weeks. So the pace of one section per week might be like whatever it was for you, however much time you put in, et cetera.
2668090	2689162	100	A	1.0	44:28	68	The coming chapters are going to be very different because one didn't have any formalisms or figures really, just the kind of overview figures. But we're going to have two weeks for two. So no need to rush it. Read a couple of pages and then just go back to the beginning and just restart the same pages. That's like the multiple coats of paint in a mural.
2689306	2709062	101	A	1.0	44:49	47	Like the low road to active inference. That's where they're going to pick up in that high road, low road dialectic that we talked about last time. Let's just see what figures, what equations might happen. Okay. Lot of terms that hopefully are in the ontology already.
2709196	2732590	102	A	0.86	45:09	44	Like you can use the at symbol hopefully to call most of them, but if something's not there, we can add it to supplemental or entailed. Okay, so what figures and figures and equations will we might see? Here's a box 2.1 about probability.
2734770	2768330	103	A	0.65547	45:34	65	Here's an equation that's the Bayesian kernel and it's going to be talking about this example of a frog and an apple and jumping or not jumping. And that example is going to play a current role. But then some likelihoods are shown in a specific example of like frogs and jumping or not and apples. There's a work through example of exact Bayesian inference.
2770750	2773770	104	A	0.93433	46:10	6	Here's a table on statistical distributions.
2776350	2788346	105	A	0.87722	46:16	37	It would be really interesting, like to hear what support and surprise mean and what are these district? Are these all the distributions? Are there other ones? Or like why these ones? Or why are they useful?
2788538	2791280	106	A	0.99995	46:28	10	Where have they been used? What does it even mean?
2794550	2816310	107	A	0.99452	46:34	49	Here's where the KL divergence is introduced and then some more analysis from a surprise perspective. Bayesian surprise with the KL divergence. There's the box on expectations, which is also what we talked about a little bit today. And that was really interesting to connect it to the matrices.
2819930	2845390	108	A	0.98	46:59	27	A figure of the generative process and the generative model is the caption a figure both perception and action minimize discrepancy between model and world free energy.
2850200	2881870	109	A	0.91398	47:30	67	Variational free energy as an upper bound on negative log evidence the figure but with equations in it. Very common format figure complementary roles of perception and action in the minimization of variational free energy. Big act, imp theme and like something kind of a common fristinism like perception and. Action, being in the same game or being in the same service of the same objective function.
2886180	2892400	110	A	0.99985	48:06	8	Planning but no specific equations but probably citations.
2894820	2909430	111	A	0.99996	48:14	29	Just introducing expected free energy, which Yaakob and others can probably go into a lot more detail on. Expected free energy about a future where the outcomes haven't happened.
2911740	2937090	112	A	0.99937	48:31	44	Expected free energy figure with equations, the end of the low road. Introducing the two key terms variational free energy, expected free energy summary does anybody, like, want to, like, add or what was something cool or whether they read it already or not?
2951280	2953390	113	A	0.82048	49:11	6	What does anyone Ali? Yes, please.
2958180	2965600	114	F	0.99839	49:18	16	Well, I've noticed the dispersion of some sidebar boxes like box 2.1 or in this chapter.
2967700	3008480	115	F	0.52	49:27	80	I don't know the distinction between the purpose of these boxes and I mean, what's the function of these boxes as compared to what we see in the appendices? Because presumably these boxes cover the concepts that could be skipped over if one is familiar with these mathematical concepts. So do you have any idea about the reason behind this decision? I mean, covering some basic concepts, basic mathematical concepts in these sidebar boxes and some others in the appendices?
3018910	3028310	116	A	0.97395	50:18	21	That's a great question. A box could reflect like, okay, expectations. Got it. Or like, okay, some product rule. All right.
3028480	3044900	117	A	0.99997	50:28	48	But it's kind of like you might want to read that. Or is this introductory highlighting it? This is the 101 on probability. Or is it saying this is a total skippable unit? If you want to learn about how the lizard does it, here's where you look.
3045510	3056790	118	A	0.89882	50:45	20	It can sometimes mean both. And the format of the book is also relatively austere, though in a concise tone.
3059770	3101582	119	A	0.95865	50:59	96	It's in black and white, which especially in some later sections make some visualizations where it's like it's hard to understand what curves are doing what it's a black and white image, so what is there to see with the color? But it also might predispose towards more simple or visually accessible material. So what is going to be accessible and the order. So those are all really important questions. So we can just take notes on it in the weeks that we're going to continue to do this because we only have limited live time.
3101736	3109750	120	A	0.99944	51:41	20	How about in the last like 3 minutes? What does anybody think about the opening quotations or specifically this quotation?
3111850	3117750	121	A	0.99998	51:51	16	My thinking is first and last and always for the sake of my doing. William James.
3123060	3127250	122	D	1.0	52:03	17	I thought the guy was a philosopher, so I guess you just abused me of that one.
3129490	3130240	123	B	0.38	52:09	1	I.
3137210	3166830	124	A	0.84	52:17	51	Yeah, these would be really nice areas to look into for people who like the history part. Blue and I and others are working on some different kinds of ways to reference papers and just up cool paper. Sensation perception. We have those terms. We could just link the paper somewhere.
3167490	3188070	125	A	0.54565	52:47	43	So if anyone's like, interested in that kind of architectures or that kind of philosophy question the literature right now. Is small enough to know what has and hasn't been done in a lot of areas that are philosophical and applied and technical.
3193830	3197720	126	A	1.0	53:13	13	But just from, like, the first principles, what does this seem to mean.
3200330	3249240	127	C	0.98849	53:20	91	That shared that other group that we're kind of both a part of Daniel Liminal dow thing. That's not really a dow. I don't know what it is, but there's this debate for some reason that's going on there about different ways of knowing and being and doing and a bunch of stuff that is kind of a mix of philosophy and linguistic fallacies. But if thinking is actually a physical process, which I think anyone here is going to argue against, then it is something necessarily that is being done.
3252410	3268700	128	C	0.8131	54:12	30	However, I guess, small, you want to draw that Markov blanket. And then however large, whether that's your physical body actions or some extended thinking, it's literally what you're doing.
3272930	3287314	129	C	0.9952	54:32	31	It's not just for the sake of your actions, but they are your actions. They're just another set of your actions. All right. I'll offer a different take on it. Yeah.
3287352	3317500	130	D	0.99847	54:47	62	Awesome. William James was a great visionary, and he foresaw Mark's Mark Zuckerberg's metaverse 120 years ago or whatever, and he said, no, I don't want to live just inside my mind, inside the metaverse. I want to be out in the world, interact with real people and real things. So he was a meta or Facebook skeptic way before his time.
3321010	3322750	131	A	1.0	55:21	4	Okay. Thank you, Ali.
3325970	3386580	132	A	0.95	55:25	89	Yeah. As Brock mentioned, I think it relates to an epistemological distinction between propositional knowledge and how to knowledge. And I think this statement by this statement here tries to blur this distinction, blurs the line between propositional knowledge and know how knowledge. Because especially in analytical school of philosophy, there's always been a very, let's say not a heated debate, but there's a long standing debate about the distinction between these two kinds of knowledge and whether in fact they can be distinguished from each other or not.
3388790	3419370	133	A	0.99997	56:28	52	Thank you. Could you just unpack what are the two kinds of knowledge again, and what are just they referring to? Sure. Well, about propositional knowledge well, an example of propositional knowledge is to know the exact mechanisms of walking. I mean, which muscles contract and which I mean, in what angles.
3422930	3466010	134	F	1.0	57:02	85	The whole thing about the biomechanics of walking, the whole knowledge about the biomechanics of knowledge can constitute this kind of propositional knowledge. And it's totally different from knowing how to walk. I mean, a three or four year old year old child has a know how knowledge of walking, but not necessarily where he or she doesn't know anything about the biomechanics of walking. So the biomechanics of walking is the propositional knowledge. Actually, knowing how to walk constitutes the know how of walking.
3474340	3526544	135	A	0.62514	57:54	112	So when people say things like active inference is integrating perception, cognition, and action, maybe it is rethinking some of these long held mental frameworks for distinguishing or operating differently on action and perception. Like, one fascinating recent example from me and working with Eric was in the area of ant pheromone modeling. Without going into too many details, people often modeled preference as a function of the absolute amount of pheromone on trails because that's what the exponential, like, decay is on. That's what can be manipulated rather than the perceived intensity, which might have a different scaling relationship. So it's like a dim room, you can detect a small change.
3526582	3557960	136	A	0.93666	58:46	58	Bright room, you can't detect a small change. So that kind of psychophysics of perception gets ignored implicitly because of calls for measurability, because the cognitive can't be measured directly. Even if you potentially had an electrical measurement or something like that happening, then these are awesome questions. Can you think without acting or is thinking in action?
3564880	3616460	137	A	0.93667	59:24	97	This active paper, which was on a live stream, so we can provide the link to it. Models like perception and action in the sort of like kernel level, just the autonomous sensing sentient bot level. And then attention and metacognition are both related as actions to the lower level, which is like why the paper is relating computational phenomenology with mental action. Again, just to have a look at the kinds of models that can happen. And then now does making any model, fitting any data well enough and saying, well, we modeled it as action.
3616800	3632050	138	A	0.99992	1:00:16	39	So is thinking in action? Will that model that says, yeah, it's consistent with that, will that ever constitute positive evidence for saying thinking is action? Or why even say that? Or what does it mean to say it?
3634580	3654070	139	A	0.99	1:00:34	27	And then here's a funny meme. Here's the generative model and the generative the partially observable Markov decision process. Here's Karl, Friston, Jessica and then anyone else.
3656600	3699744	140	B	0.99753	1:00:56	94	Yes. I guess it's like a beginning understanding of this. I tend to think that there's like a bias towards action in active inference. I think the first chapter is saying even if you want to sense or perceive, you have to do some kind of action in order to gain the information. And this quote is basically saying like, okay, even though the whole field is trying to understand cognition, what we decide to do and the thinking that we go through in order to determine our policy is to determine our actions.
3699792	3731404	141	B	1.0	1:01:39	77	And then it sort of feed into the thinking itself. So maybe that's why there's like the bias to action because our actions is what it's going to be allowing the thinking. But when we update our models or trying to come up with the policies that we're going to be doing, like, we have to be processing things. So the thinking is in as a service to action. That's sort of like what I was thinking.
3731442	3751590	142	B	0.88	1:02:11	57	I don't know, it's a little bit like the chicken and the egg, but I kind of tend to think of it. It's like, yes, we have to act in order to understand, but the understanding which will be coming from the thinking is what determines our next action. Yeah, that's kind of what I was thinking.
3756260	3758320	143	A	0.71791	1:02:36	3	Matthew. Thanks, Jessica.
3760740	3827510	144	H	0.8	1:02:40	163	Yeah, it kind of seems to me like we're stuck in a little bit of a linguistic Godellian loop of some sort because thinking is essentially a verb and verbs tend to imply, in a linguistic emergent term, actions. And so we have to then pop outside of that and try to consider with our internal mental processes how to disentangle these concepts. But I see no reason to think, ironically, that internal mental states aren't also conscious parameterizations of processes in the brain and our ability to grasp control of that to some extent. And so it seems very strange to me to try to separate those linguistically from our current vantage point, even though I understand why, historically, working only with language, it might have made sense. So those kind of quotes like James strike me as somewhat antiquated or just operating on dichotomies that don't seem to make sense given that we've unpacked these processes to the degree we have.
3830520	3849900	145	A	0.99983	1:03:50	41	Thank you, Matthew. Very deep points. Like you mentioned, thinking as a verb implies action. And then just to kind of complete that thought is like the word for the noun form of this one. So then what does that imply?
3856900	3892890	146	A	0.55	1:04:16	79	And then also, like Jessica mentioned, to. Some extent that we can only express processes through their discretization via symbols. Right. The idea that there's this process, there's a flow, there's something that can't necessarily be separated, obviously, from the rest of the flows around it. But to reference it, to point to it, to describe it, communicate it to another agent, we have to nounize it kind of like which is the opposite of verbalizing, so to speak.
3895440	3897340	147	A	0.99363	1:04:55	4	Wow. Thank you, Matthew.
3899760	3901740	148	A	0.99981	1:04:59	5	Again. These are yes. Eric.
3910030	3927550	149	D	0.99997	1:05:10	30	Sorry, I didn't have anything. I didn't have the hand was yeah. So Karl Friston had an influential 2019 paper. Long paper. The free energy principle for a particular physics.
3927710	3948630	150	A	0.65051	1:05:27	47	That was in 2019. So it's been several years since then. And like, particular allegedly accidentally or intentionally is a pun. So it can mean specific, like a physics for the specific systems that we're modeling. Or it could be specific, like this is a specific approach.
3949870	3987570	151	A	0.99954	1:05:49	73	Another interpretation is like it's for particulate entities when we define thing and the partition, which is going to coinstantiate, like the generative model and the generative process and then the Markov blanket or the interface that's separating them, that is separating the figure from ground. It's separating, like, the entity from the niche. It's separating what from what? What can it represent? The separation of something from all possible separations, some separations.
3988890	4004970	152	A	0.99	1:06:28	26	And then, Matthew, you mentioned, like yeah, verbalizing speech, but it feels like we're speaking nouns sometimes. Yet verbalization is a process, especially dialogue. Yes. Matthew.
4007630	4045990	153	H	0.69786	1:06:47	108	Could I ask a question that brings in a little bit of content from the other chapters of the book is that yeah, sure. I'm just kind of curious because when you're talking about those, you're talking about the Markov blankets and integrating it into this idea of how we draw boundaries. I'm kind of curious. Is it fair to say that to the extent we've drawn our boundaries and do see a reduction of free energy in this system, it reinforces the idea that there's something to that boundary structure as an entity in the world, that there's a reality to that? Is that a fair interpretation?
4050440	4105784	154	A	0.54	1:07:30	117	I think many people would have a lot to say and add. This touches on the question of reification of scientific models and the ability for identified statistical model to be like transcending itself and used in a realist way, like ontologically real about true joints of the world. And I'm going to go to live stream table and find several of those who explained their research on this topic. But even short of trying to claim it's real, does that ability like once we've drawn a boundary using these Markov blankets and we do see that it seems to be, let's say, using information to self evidence, do we? Think that that's I mean, it seems.
4105822	4106410	155	A	1.0	1:08:25	1	Like.
4108700	4120380	156	H	0.99986	1:08:28	35	This paradigm of thought is predicated on the idea that to the extent that that occurs and reduces free energy, it should attract, or further attract attention, at the very least, for examination or investigation.
4122720	4134640	157	A	0.71344	1:08:42	31	Yes. So, great questions, Jacob. Do you have something to add on that or is it a slightly different area? I guess it's kind of related to the Markov blanket discussion.
4137540	4198550	158	G	0.99992	1:08:57	134	This may be completely wrong, but it seems to me like with that, this kind of discretization of the Markov blankets forces us to a specific kind of discrete thinking. But I feel like that just this model of a Markov blanket. I think of it as a kind of classical model, but I presume that the reality is more something like a schrodinger Markov blanket where we can't really draw boundaries between thinking and observing a generative process. In the same way that we can think of, say, electrons as balls bouncing off each other, but we can think of them as classical particles and that will help us to an extent, but then we can also observe the process of thinking. So thinking is also kind of entangled with the generative process.
4200520	4243590	159	G	0.71557	1:10:00	71	So even though the Markov blanket is a useful formalism, I think that there are definitely certain scenarios that in which it's more like a probabilistic. Well, it's already probabilistic, but in the sense that the entity that's performing inference also in its own inference loop performs inference on itself and therefore thinking is at the same time an action, but at the same time it's part of the generative process.
4250850	4290870	160	A	0.99981	1:10:50	95	Another angle on this thanks Jacob, is what Dean T often talks about with active inference as a framework or a filter. These are just sort of like different distinctions that might be transiently useful in some axis, like fitting something to a framework. It's kind of the procrusty's bed. Stretch it out so that this perception and then active as a filter, was reminding me of what Matthew was saying, like going out and discovering divergences from expectation of there not being metabolic activity on a planet. And then there is some statistical deviation.
4291030	4316130	161	A	0.82	1:11:31	60	And so, like, in the classical statistical framework, that might be like it's a one sigma difference in the Bayesian framework, well, there's a base factor of two. And here for this evidence, and there's other ways to talk about detection of novel entities that might be part of, like, a reification process of the extended cognition of the modelers.
4319420	4388332	162	A	0.78	1:11:59	109	Then, like, one philosophical angle on that. And then also anyone who wants to read, then Ali is this book by Helen Longineau which talks about methodological pluralism and about how there can be, like, disciplinary rigor and the ways in which often unstated, like social priors can be the substance of what becomes understood to be science as, like, a complex phenomena. Ali and then Brock. Adding to what Yakov said, I think the concept of Markov blanket emanates from the Plutonian way of thinking, specifically hylomorphism school of thought as opposed to hylozoism. The distinction between hylomorphism and hylozoism is that, well, in hylomorphism, everything is disconnected.
4388396	4438988	163	F	0.99993	1:13:08	78	Every concept is disconnected and with every other concept. And so there's a discontinuity, metaphysical discontinuity between the concepts. But in hylozoism, the philosophers advocating this school of thought, well, they claim that hylomorphism fails at answering the question of what it is adequately, and they even rephrase the question. And they claim that the right question to ask is not what it is, it's what it can do. So that's basically the distinction between hylomorphism and hylozoism.
4439164	4452980	164	F	0.99998	1:13:59	22	But I think the mark of blanket is a way of formalizing this hylomorphic way of thinking. At least that's my opinion.
4455760	4458060	165	A	0.9999	1:14:15	7	Thanks a lot for that, Ollie. Eric?
4468500	4471840	166	A	0.5037	1:14:28	9	Sorry, I'm not hearing Eric. Can others hear Eric?
4474910	4480830	167	A	0.9997	1:14:34	14	Maybe I need to reload. Can you hear me? Okay, I'm going to reload.
4500250	4506060	168	E	0.99624	1:15:00	18	Can you hear me? Yeah, I can hear blue. Okay. You're still not hearing Eric, though? No, sorry.
4506750	4509370	169	A	0.96	1:15:06	8	Yeah, I just did, like, a small disrupt.
4517700	4524740	170	A	0.74	1:15:17	19	Okay, wait, eric, try again. Okay. How about now? Yeah, now I can see you in here again. Okay.
4524890	4535050	171	D	0.99995	1:15:24	18	How about everyone else who's the audience? Okay. Wow. Disrupt on the high low field. It's happened before.
4537420	4581850	172	D	0.55	1:15:37	80	I just throw in my two cent about my interpretation of a markup blanket. It's about trying to perform simplifications that make computation tractable. If you have everything interacting with everything else, then we can't do computing on that. So we apply compartmentalization and build objects and interfaces for how the objects interact with one another, and those become tractable. That's what a Bay's net does, is it says what are independent and what are dependent variables on each other.
4582220	4632856	173	D	1.0	1:16:22	134	And that works well when those abstractions, the objects and relations, nouns and verbs are a good fit to how the world actually operates. So that is a driver for learning or building representations that will use these Markov blankets and put the compartments where they actually are faithful to the compartmentalization that we can abstract over the way the world parts in the world operate. So things that are distant, that separate, that interact only weekly, we try to maybe factor those out, pretend they don't interact at all, or use some other intermediate variables to represent the interaction in order to simplify things so we can do computation. So that's how I think Marco blankets and we'll see if the math tells us that later on when we get to it. Awesome.
4633038	4635560	174	A	0.58861	1:17:13	2	Brock ethanoli.
4647630	4654766	175	A	1.0	1:17:27	20	Okay. I'm like not hearing Brock, unfortunately. Do you? Yes, we all hear Brock. Brock is reloading, I think now.
4654788	4666946	176	E	0.95109	1:17:34	27	Again. Okay. Yeah. Gather does this sometimes, but I don't know if it's because we have 15 people. Yeah, but like Matthew said, I don't hear him.
4666968	4669250	177	A	0.54204	1:17:46	7	It's just like there's different drop offs.
4671990	4678438	178	E	1.0	1:17:51	24	I can hear you, but I still think Daniel can't hear you. Daniel, you can't wait. Now you're back. Go for it. Okay, continue.
4678604	4684518	179	C	0.95077	1:17:58	9	Interesting. Can you hear me? Yeah. Oh, okay. Yeah.
4684684	4726690	180	C	0.91933	1:18:04	73	This is just this ongoing thing that I don't know, we just keeps coming up about. Yeah, well, okay, maybe they exist, maybe they don't. Markov blankets, really? But how would they form in any how would they form an evolving collapse? Presumably there is some point in that process, this evolution part where the Markov blanket is extremely poorly defined relative to the system and that's basically these sort of underexplored areas.
4726770	4761630	181	C	0.99249	1:18:46	50	It was also in relation to Matthew's question also about the free energy minimization thing where there's systems like non equilibrium systems basically, where still something interesting worthy of study is happening, but perhaps is not free energy minimizing in that state. Thanks for sharing it. Just a few thoughts.
4765170	4816762	182	A	0.77106	1:19:25	113	Let's just say some sort of meso scale or global as that's called, but that doesn't mean about the whole world, but some sort of global free energy minimization in the joint model is achieved like in a conversation that isn't the same thing as the local disconnected free energy minimization. Otherwise fitting high parameter models optimally would be simply reducing to fitting single dimensional models alone. If we could just do the linear optimization on the standalone variables, then why would we ever need the larger dimensional methods? And then think of this textbook maybe with where things could be in the coming years. The Markov membrane is like the linear algebra.
4816906	4868770	183	A	1.0	1:20:16	107	And then Jacob mentioned that this is classical model. It is kind of classical in a sense, in the timeless classic sense. So the one layer Markov blanket when thinking about the rise and fall of civilization is not the end all that's. Like I think maybe analogous to a linear regression y equals MX plus B and then you have all these hundred years of development on the linear regression model and all these techniques and applications and pipelines. So the Markov blanket one layer potentially over interpreting it as already presenting with strengths or weaknesses in certain situations when not being just like empirically demonstrated.
4869190	4876850	184	A	1.0	1:21:09	13	In the general case, we'll see what could be said and when. Matthew.
4879130	4903840	185	H	1.0	1:21:19	68	Yeah, along those lines, I was kind of curious. I've seen that there's been a decent amount of work on the sort of hierarchical or fractal composition of these models. I'm curious if that implies that there's a fractality to the boundary by default or if there needs there any specific work on sort of hierarchical composition of the boundaries of that so called membrane. As you said.
4907570	4954990	186	A	0.99996	1:21:47	118	With better annotation, we could have better answers because there's many papers that we've discussed in a guest stream or a paper and there's also like many other papers obviously, that we don't have that kind of annotations of because it's quite common to see nested models in the context of nesting of cognitive processes. Like in that San Fed Smith paper, which was I think, number 25. That was about cognition as mental action. So that was nested and the nesting was interpreted as cognitive actions and counterfactuals, basically. And then sometimes nesting is used to refer to actual, like, well, the state is inside of the country and then the region is inside of the state.
4955140	4973140	187	A	0.99954	1:22:35	35	It's implied that the map is mapping onto the territory, maybe even spatially, the cell is inside of the tissue. And that's like answering Schrodinger's question. Everything happening with the planetary scale analyses, cellular level.
4976870	5039750	188	A	1.0	1:22:56	131	And then this is hopefully what is spoken to with the composability of active inference. And sometimes that's framed in terms of the lateral composability of you could have three ants interacting or 300 ants and then the nested composability, you could have the fifth and the 7th and all those layers with computational trade offs, maybe with no extra information to be gleaned. And then learning the structure of the generative model or just the structure of the partition more broadly and like what the variables are and everything that's the structure. Learning challenge and cognition as structured learning. Hashtag synergetics geometry of thought is what Friston has and others have raised as like a total open area because there's the parameter fine tuning once you have the Bayes graph.
5040510	5070580	189	A	0.9994	1:24:00	76	So then you're now inside of the ability to reify or not that model. You're just doing parameter optimization. We used two factors in this linear regression and then we optimized it with the L two norm. Two is the best number of factors, but it couldn't be set at that scale. You could say in a two parameter model, this is the best parameters with this norm, but then you couldn't pull back another level.
5071270	5121780	190	A	1.0	1:24:31	132	And so that's like meta scientific and metabasian analyses, which they're going to talk about later. Okay, I guess I'm also kind of wondering if there are any examples that come to mind of something like a system that is structured. Such as the clearest or simplest example that comes to mind is like, let's say that you have a nation and a state within that nation and there is a port and there is an overlap of sort of that port interface to an outside structure that is shared by both the national and overlapping national and state interests and decision processes. And so I'm kind of curious if there are examples of demonstrating that kind of composability of generative models or if that's something that's still just new and open.
5123830	5157694	191	A	0.99953	1:25:23	59	Those are awesome questions. If people know any some areas to investigate, like in the phenomena that you're overlapping interests informally, there would be like shared regimes of attention. There's the question of synchrony and that doesn't mean like synchronization identically but generalized synchrony. Then there's like coordination of affordances. We're not going to do this because of that.
5157812	5189350	192	A	0.82809	1:25:57	83	So that's like in an area that Yaako and others have been working on is just describing that situation as an affordance on an affordance or like E sub E. It's not in the textbook. But if people are staying this long then this is just kind of some ways that some people are thinking about it. And these are the areas where people can also do research and learning with us. But affordances on affordances, the safety could be on or off.
5189420	5209360	193	A	0.97378	1:26:29	38	So then there's like an affordance that modifies another affordance and could those have compositionality just like some of these other generalization affordances for generalization basically, what are those? And then isn't it like discovering what those are?
5217150	5253446	194	C	0.83249	1:26:57	61	Just I think it's kind of also related to that question of this formation evolution, collapse of markup blankets is when does the observation and actions of one system map to the preferences of another and vice versa? How does that come into being existence? How does that start to happen and how does it unhappy? I don't know. Awesome question.
5253548	5280590	195	A	0.99568	1:27:33	72	Thank you. And another angle on that, let me just check and then Ali is like this is a graph that we're going to look at a ton and we will interpret what all of it means. What is the OASB, pi, g, et cetera. But this is a basing graph like Eric mentioned. And the edges are like dependencies, not causal influences in the world, but apparent potentially or different interpretations.
5281190	5302230	196	A	0.92716	1:28:01	47	Is this the only skeleton? No, this is the Y equals MX plus b of skeletons. And there's the linear regression in the first equation in the textbook of the stats textbook. And then there's all these accessory tests. Then there's like creation and destruction as applied.
5302970	5328590	197	A	0.63487	1:28:22	62	Maybe there can just be a loop that's not active inference, just checking every day if something's within an arbitrary threshold. So there could be some liminal or gray area like an interface. I think steven Solette mentioned it as like a nail bed or something, like an interface between more particulate and then less particulate, more field versus the particle. Ali.
5331830	5400118	198	F	0.99043	1:28:51	104	Well, as a little side note, there's a popular science book coming out, I think in June, namely The Romance of Reality by Bobby Azerian, which touches on the emergence of Markup blankets, the dynamics of emergence of Markup blankets. And it attacks it from many different angles, evolutionary angle, I mean, from a cosmic, even perspective. And it even goes as far as well, considering the whole cosmos, the whole universe, as, let's say, a kind of Metamark of blanket, so to speak. And well, I think that could be an interesting thought put forward by Bob Yadarian in this book. All right.
5400124	5427790	199	H	0.99984	1:30:00	52	Thanks, Daniel. Thanks. In fact, he gave part one of what was intended and may still be like a multiple part discussion, but this was him giving that presentation. So we were in contact is definitely an interesting view. And it's going to take all kinds of research and education and communication.
5428210	5429390	200	A	0.99449	1:30:28	1	Jessica.
5432930	5497300	201	B	0.99954	1:30:32	135	Yes. I guess first, join the lab. Some of the ideas that helped me a little bit to start even conceptualizing this, in addition to the filter that you mentioned before, Daniel, and how it could be more like porous or more hard and allow things in and out, but it was also about it could be like just grouping of relationships and interactions. So, like, the closer like a relationship, like different things are, maybe that's more like on biological systems, I don't know if they're closely related or they have more interactions, even though they might be different, but grouping it in those sense and maybe that relates to what Eric said from the calculational part. But that was sort of like some of the things where I started connecting Marco blankets to.
5498330	5536980	202	B	0.9	1:31:38	78	Okay, if items or different objects and things like they're closely related, you can start putting a blanket around and maybe go through with the idea of wrapping things from some of the definitions. But, yeah, those are kind of like the kind of visual ideas that started helping me a little bit and I still don't understand properly, but those are the things I was like, okay, so relationships, filters, kind of like connecting interactions between things.
5539990	5554290	203	A	0.99995	1:32:19	28	Awesome. Thanks a lot for that. Like interacting entities. So you could have the edges representing some interaction, like when people are fitting a linear model. Interacting variables.
5554450	5588206	204	A	0.53902	1:32:34	85	So there's this whole discussion topic of whether the interactions are like the two ants bumping into each other, whatever that means in the, quote, real world. And then there's like the statistical interaction. So then defining certain variables in statistical models. So leaving that debate behind us and just talking about the statistical models that we have, Bayesian graphs, some variables have edges between them that could be either designed to be there or not. Or you could do some sort of like thresholding.
5588238	5650210	205	A	0.99867	1:33:08	148	Approach and explore that your thresholding parameter was acceptable. But the more of a loose interaction you allow, the more challenge there is to fit that statistical model. And you may not have enough data to fit like the ten variable by ten variable with all the interaction terms which is like why when doing linear modeling in a health population example they would like do model selection on what their statistical power is with that data set to resolve certain kinds of effects and correlations and non sphericities. And that is addressed a lot in the SPM textbook and Kristen's earlier pre active work. But then you mentioned how to be engaged with that process of the wrapping or the seeing the clusters and then knowing where is there going to be maybe generative models arising that are consisting of other generative models or other generative processes?
5650370	5699394	206	A	1.0	1:34:10	115	And then some things interact more with others statistically in a model as by design or just as an outcome of whatever. So there's sparse connectivity amongst the variables. They're not like all by all connected. And then that sparse connectivity simplifies a lot of things that's also related to like a Lasso regression and also to the below l two norms and then a sparse model can be factorized. And that is what allows for the variational Bayesian inference, which is like doing Bayesian model fitting on a factorized because it's sparsely connected graph and so a lot of these discussions are quite downstream of a lot of the philosophy of map and territory.
5699522	5728080	207	A	0.99995	1:34:59	50	But it's still a super important conversation. However, within the model inference, with a model inference using a model, a lot of these questions are very technical and downstream of importance qualitative things that are also important to keep in mind but of a different type, jessica or anyone else.
5731270	5733060	208	A	0.6	1:35:31	4	Yeah. Wow, very interesting.
5738550	5753510	209	A	0.997	1:35:38	40	Here's one other question I guess we can discuss. Like as the as it currently stands, there's a 1 hour meeting and then at the end of the hour, the very next hour begins the Tools regular organizational unit meeting.
5756650	5791700	210	A	0.69664	1:35:56	78	It is the people who are here now but hopefully others would be listening to it if they're not able to make this time. But what will help people with the synchronous and Asynchronous make the most of the next few months? Do people appreciate having a longer discussion for this? Do people think that the main versus the math group? Is there another subgroup, like a philosophy discussion, that people want to kind of really do this?
5792950	5839140	211	A	0.99978	1:36:32	92	Because we're experimenting with open endedness on our first cohort and in these early phases of the book and people can probably imagine various of the things we want to balance, like respecting everyone's time and different backgrounds, respecting their preferences for how much they want to learn about different topics. Being realistic about how much asynchronous and synchronous, direct and peripheral time makes sense. But also being realistic like there isn't a two minute video for a process. So how do people think about that, those who have stayed this far?
5862720	5864590	212	A	0.50391	1:37:42	5	Mike and then anyone else?
5868000	5884732	213	A	0.99998	1:37:48	44	Can you hear me? Yes. Okay. Yeah. I think that as noted earlier in the discussion, there are just so many themes that run through this that there are opportunities to pull on any of a number of threads in the course of discussion.
5884796	5937350	214	I	0.59	1:38:04	106	And so today's discussion was interesting and I stayed for this part of the discussion simply because of the interesting threads that were being pulled. Certainly a contrast with the first part of the discussion around math and I think a lot of uncertainty amongst the attendees about how to engage with the mathematical aspects. Maybe to try and put a point on it, from my perspective, learning about what is active inference and how can active inference be applied in real world situations is a motivator for me participating. Obviously there are also sort of philosophical and maybe more scientific discussions that can take place.
5941880	5975148	215	I	0.99991	1:39:01	82	Any meeting could touch on any of those aspects as well as some of the asynchronous interaction could pick those up as well. Thanks a lot. Okay. Anyone can raise their hand here's just a few other options. Like if somebody here's something that's interesting to them, we can in the discord make a channel that is relate or people can participate in the regular channels just like questions that's going to potentially be seen and interacted with by people more broadly.
5975244	6023872	216	A	0.99	1:39:35	100	Like what if we posted questions that we're having we're in the cohort one of the text and we had this question do you know that's one option? Another, and this is like to Mike's expression that applied active inference is a motivator epistemic and pragmatic value, expected epistemic value. We expect to learn a lot by like staying in these sometimes challenging or oblique or whatever discussions. But then there's also expected pragmatic value with applying active inference, learning and applying active inference. So then maybe that is a group we can partition, like an applied active inference group.
6024006	6092596	217	A	0.99989	1:40:24	147	So then we can be clear about what the focal artifact is because also we want to not all groups at all times will be able to have nor is it useful to have open ended discussions of any discursion length. So like knowing how far and in what ways and how many minutes of people's linear time. Ali yes, the meetings, just one note on the organizational unit meetings. The organization units in the active lab like comms and tools for education, communication and tools are like directory 1 hour lab meetings or group meetings because we're not doing the education work necessarily in that 1 hour. But some groups sometimes it's possible to do some stuff, but it's like increasingly moving towards sharing updates from people who want to commit in Asynchronous work or smaller groups that want to commit to doing something like that.
6092778	6156120	218	A	0.99304	1:41:32	125	So then educational related projects, that's their opportunity to ask for help, share updates and so on. But earlier on there was more topical material, but then through particularization and operations and other approaches, it becomes more like Pragmatic and less discovery and less mixed media role specification, all these processes. So people sticking around is sort of the and being engaged and like seeing an affordance and then just making that contribution, wanting to be a facilitator for a certain project or wanting to contribute actively or just even connect with another participant. You can email them if they've provided their information, just some random things. But we want to have the applied angle, the philosophical loop and how to even partition that discussion.
6157820	6179120	219	A	0.87	1:42:37	45	Like we couldn't just overgo every math, every philosophy, every applied question and have the judge and the jury and all this apparatus. So how can we scaffold that conversation around applied active inference for the people who are like super excited and motivated? Mike?
6181140	6230530	220	A	0.94	1:43:01	121	Yeah. I just want to add there's an interesting duality related to what you just said in that we can't go into fine detail in all of the content and at the same time, I've found this group to be remarkable at unpacking things and sort of really getting into what do we mean when we say that? Types of discussions and what does this term mean not taking things for granted as going through the text. There's a balance to be struck in taking that approach of asking what do we mean when we're talking about this and putting nouns on things and relating it to language and not going too far over into getting into fine detail about things.
6237130	6286210	221	A	0.81507	1:43:57	115	Thank you, Lyle. And then I'll leave with your question. Yeah, this firstly great session, really great conversation, I really enjoy it. Most of this is new material for me and so I'm really enjoying the breadth of the conversation and I'm cognizant of this aspect you're digging into is how much do we sort of separate out the pieces and go deep dive in different places versus more of touching on multiple threads and their interactions? For me, while I understand there is a balance to be struck there, I'm really enjoying the challenge of relating as an example, the math to the philosophy, to all these different threads together because they are linked.
6286290	6316270	222	A	0.94998	1:44:46	63	Right. I do understand that some of those areas need a separate group that you can drill down into for me personally. And so this may be not the same as other people in the group. I'm really enjoying understanding the connections, the philosophy, how these different thought process came to be through history and get that depth of understanding. Thank you, Lyle.
6316430	6337526	223	A	1.0	1:45:16	51	And for those who want to listen or view, these live streams have many, many themes. So check if there's papers that you're interested in here. The live streams is about papers, so there's 46 papers. Guest streams are not driven by a paper. Sometimes the person is sharing a paper.
6337628	6390504	224	A	0.99999	1:45:37	123	But these are like presentations, hearing from different perspectives. And if anybody wants to help organize these, that's what we do in comms. If they want to facilitate and participate in these discussions, if they want to contact authors, recommend papers, these are all distributed tasks that are leverage points for people who care to just do a ton of amazing things. Like if somebody's interested to connect it to a given community and make an artifact or a live stream co organize that, we can catalyze it at the lab scale for individuals who know about the affordance but then want to take that affordance to just have really leveraged impacts in the active ecosystem. So, ali's question and then jessica sorry.
6390542	6398010	225	F	0.78	1:46:30	21	I don't have any questions you asked. About the timetable for the meetings, like what specific times are they occurring at?
6400140	6418620	226	A	0.99995	1:46:40	35	Sure. Yeah. Okay. They are on Mondays at 13 and 23 UTC. There's two meetings to reflect, like education being the primary mission of ActInf lab, and then that spreads them out by time zones.
6418780	6447556	227	A	0.99995	1:46:58	80	But if somebody really wants to contribute to an area, the organizational unit meeting is not the rate limiting step. Anyone who has attention to contribute will be able to find a regime of attention that is connected to a task that's meaningful. The rate limiting step is not people's availability for a 1 hour meeting. It's however much time people want to contribute. Whatever practices and things will find something that works for people who want to be engaged.
6447668	6465680	228	A	0.79084	1:47:27	38	So don't take these meeting times as being like when we're doing it, when we're deciding. Even anybody who wants to can email or contact just the lab email address and be started on figuring that out. Jessica.
6468340	6522028	229	B	0.99944	1:47:48	112	Yes. This was related to the question about applied projects. I was thinking that maybe one thing that we could do is on the project ideas to have a table where people can briefly share what they want to do or just say, okay, I'm interested in machine learning and apply active inference and maybe this specific topic. Who will be interested in discussing this or exploring what to do and then adding the names of the people who will be interested in that. So the person who started the project, then they can start maybe contacting those people and see what time they can meet and start creating their own subgroups.
6522124	6548490	230	B	0.93021	1:48:42	60	So maybe that's something that we could do to facilitate that in a simple way. So just start saying, this is what I would like to do. Apply active inference while also studying the course. And we like to connect with people here who might share the same interest and see what kind of feedback that person can get. Thanks.
6548940	6604510	231	A	1.0	1:49:08	105	And just connecting and building trust and having like a buddy system or small groups or just people who are on the relatively long path to apply active inference on teams. We don't have the speed dating hot swap activation protocol but connecting with people. And then however it's authentic in that relationship. Seeing what you're interested in in three months will be in a pretty different situation, but we still won't have even gone through the second half of the book on the application of the textbook. So there's a lot of time for us to develop ideas and to connect with each other.
6607440	6650040	232	A	0.99988	1:50:07	101	So thanks for sharing that a lot. Jessica and I tagged you so that we can create a table with the right way or do it however is the right way because it helps enable connections for people who want to connect around applying. And also it helps us remember these are the specific reference points in the text that are like our attractor regime of attention for this textbook group. Like the textbook group isn't all of active flap. If people want to apply active inference, it doesn't have to be from the ground up, but it totally could be.
6650190	6666930	233	A	0.99989	1:50:50	38	But there's many ways to apply that are just in different codas that people can get involved in immediately. So if somebody feels like doing things, I hope they feel like they have the agency to do that.
6670180	6730172	234	A	1.0	1:51:10	142	Okay, any final thoughts on this interesting semi pattern breaking? And also yes, one final thought for me is we would have had tools organizational unit meeting at this time, which is why near the end I asked if you want to do different scheduling because in general it's probably not good practice or ways of working to go overtime to respect everyone's time and all of these types of things. But also in the future we're in gather so people who want to keep discussing the textbook could go there into a different room. People who want to do Tools can go into another place. So we need to figure out how to do that through the people who want to be there, like these people and the feedback that everybody has in the ways they want to co create it.
6730246	6738470	235	A	0.99992	1:52:10	30	Tim and then anyone else who has like last thoughts? Hey. Yeah, I was just going to say this is generally way more interesting than the Tools meeting generally is.
6741260	6790856	236	A	0.95651	1:52:21	125	Cross pollination shows great potential already, I would say. Yeah, sounds good. I just wanted to add to that conversation, though, about someone even mentioned Plato, Socrates and all that and that whole sort of recursive involution of that raification thing you were talking about and sort of the thought processes maybe being almost like a recapitulation of the structure learning and the Kreptasian factor graphs and all that kind of thing you were talking about there. Plato had like a concept of knowledge which he referred to as recollection and this idea way, way back where all learning and knowledge is actually an act of recollecting what we already know. And I just wanted to point out there's kind of maybe just more of.
6790878	6792650	237	I	0.93863	1:53:10	2	An interesting.
6796860	6807530	238	A	0.92453	1:53:16	24	Illusion or connection, I guess between those two where there seems like what's old is new again. Or what have you covered? Thanks, Tim.
6809740	6813690	239	A	0.61652	1:53:29	12	Anyone else who hasn't spoken or who would like to add anything?
6831220	6831470	240	A	0.62	1:53:51	1	Okay.

1
00:00:00,810 --> 00:00:03,438
All right, greetings everyone. It is

2
00:00:03,604 --> 00:00:07,322
September 9, 2022. It's the 15th meeting

3
00:00:07,386 --> 00:00:10,830
for Cohort one and we are

4
00:00:10,980 --> 00:00:13,054
discussing chapter six. We're in the

5
00:00:13,092 --> 00:00:16,240
first of two discussions on chapter six

6
00:00:16,610 --> 00:00:20,078
of the textbook. We're going to

7
00:00:20,084 --> 00:00:23,630
go to the questions page and start there

8
00:00:23,780 --> 00:00:26,134
where we have one question that we

9
00:00:26,172 --> 00:00:29,798
explored a bit last week in our welcome

10
00:00:29,884 --> 00:00:32,342
back. And then there's at least one

11
00:00:32,396 --> 00:00:34,582
question prepared, but anyone is welcome

12
00:00:34,636 --> 00:00:37,850
to add more questions here while we turn

13
00:00:37,920 --> 00:00:39,946
first to this question. What are the

14
00:00:39,968 --> 00:00:42,666
four steps in the recipe to construct an

15
00:00:42,688 --> 00:00:44,060
active inference model?

16
00:00:53,740 --> 00:00:55,932
And can we make templates that

17
00:00:55,986 --> 00:00:59,036
facilitate people walking through some

18
00:00:59,058 --> 00:01:00,060
of these stages?

19
00:01:10,940 --> 00:01:13,444
How much model building do people prefer

20
00:01:13,572 --> 00:01:17,288
to be engaged in in this section of the

21
00:01:17,294 --> 00:01:20,520
textbook group? Do people want to have

22
00:01:20,590 --> 00:01:23,416
one or more model that they're

23
00:01:23,448 --> 00:01:24,940
personally developing?

24
00:01:26,480 --> 00:01:32,264
Do they want those to be scaffolded

25
00:01:32,312 --> 00:01:34,552
in a shared page to help increment

26
00:01:34,616 --> 00:01:36,176
models along and see models at different

27
00:01:36,198 --> 00:01:38,210
stages? Yeah, Brock and anyone else?

28
00:01:39,940 --> 00:01:42,784
It's the same problem as, like, the

29
00:01:42,822 --> 00:01:45,196
first half of the textbook without math.

30
00:01:45,388 --> 00:01:47,892
It'S not going to go well. This half

31
00:01:47,946 --> 00:01:49,220
without modeling,

32
00:01:51,080 --> 00:01:52,756
it's going to be quite difficult, I

33
00:01:52,778 --> 00:01:56,630
think. So I think we should just get

34
00:01:57,320 --> 00:02:00,004
that out of the way that it's not about

35
00:02:00,042 --> 00:02:01,670
whether you want to or not or whatever.

36
00:02:02,440 --> 00:02:04,068
If you want to just understand it at a

37
00:02:04,074 --> 00:02:06,024
cursory, philosophical, perhaps level,

38
00:02:06,062 --> 00:02:07,896
then maybe not. But if you actually want

39
00:02:07,918 --> 00:02:09,256
to understand it, then, yeah, we are

40
00:02:09,278 --> 00:02:11,096
going to. Have to model something some

41
00:02:11,118 --> 00:02:13,644
way. It doesn't have to be the most

42
00:02:13,682 --> 00:02:16,190
complex, most precise one, but yeah,

43
00:02:21,260 --> 00:02:24,712
I wonder if we could develop like maybe

44
00:02:24,766 --> 00:02:26,890
a couple examples or simple.

45
00:02:29,100 --> 00:02:31,070
Ones that. People might like to model.

46
00:02:32,080 --> 00:02:34,924
I don't know. Those are all going to be

47
00:02:34,962 --> 00:02:36,270
purely agent based.

48
00:02:39,220 --> 00:02:42,864
Thanks. I agree. I wanted to frame it as

49
00:02:42,902 --> 00:02:44,800
a question, but I agree with that

50
00:02:44,950 --> 00:02:48,316
perspective. And following

51
00:02:48,348 --> 00:02:54,208
along as we structure

52
00:02:54,384 --> 00:02:58,036
a few specific cases, the rat and the

53
00:02:58,058 --> 00:03:02,244
team maze, the icicating which

54
00:03:02,282 --> 00:03:05,864
are used in the textbook, then some

55
00:03:05,902 --> 00:03:07,960
of the examples that are used

56
00:03:08,110 --> 00:03:11,032
recurrently in the literature, like

57
00:03:11,086 --> 00:03:14,664
birdsong and a few others. And for

58
00:03:14,702 --> 00:03:18,988
these, many scripts exist and

59
00:03:19,074 --> 00:03:23,244
helping people who might

60
00:03:23,282 --> 00:03:26,604
not have the setup either by with some

61
00:03:26,642 --> 00:03:28,252
walkthroughs. Okay, here's how you get

62
00:03:28,306 --> 00:03:29,856
octave running. So you can do this

63
00:03:29,878 --> 00:03:31,564
MATLAB script or here's the standalone

64
00:03:31,612 --> 00:03:34,864
dem demo because this textbook really

65
00:03:34,902 --> 00:03:39,148
only gives the pointer to MATLAB scripts

66
00:03:39,244 --> 00:03:42,790
and methods. Like when they say

67
00:03:43,320 --> 00:03:45,124
that the standard schemes can be

68
00:03:45,162 --> 00:03:49,540
applied, they mean a MATLAB script.

69
00:03:49,880 --> 00:03:52,084
And that's exactly what the step by step

70
00:03:52,122 --> 00:03:55,930
guide model stream one is built around.

71
00:03:57,340 --> 00:04:00,776
And chapter seven on modeling in

72
00:04:00,798 --> 00:04:04,664
discrete time is going to be

73
00:04:04,862 --> 00:04:07,928
akin to the step by step guide. But it's

74
00:04:07,944 --> 00:04:10,990
not step by step by step by step.

75
00:04:11,360 --> 00:04:13,656
It's more like it takes two bigger

76
00:04:13,688 --> 00:04:17,230
steps. Okay,

77
00:04:20,500 --> 00:04:24,560
how is this four step recipe for

78
00:04:24,630 --> 00:04:26,624
active inference modeling similar or

79
00:04:26,662 --> 00:04:29,472
different than approaches that people

80
00:04:29,526 --> 00:04:32,644
have seen for systems modeling from

81
00:04:32,682 --> 00:04:33,940
other frameworks,

82
00:04:36,200 --> 00:04:39,030
for example, in a reinforcement learning

83
00:04:39,880 --> 00:04:42,932
or cybernetic modeling. So something

84
00:04:42,986 --> 00:04:45,592
more recent and computational or

85
00:04:45,646 --> 00:04:50,644
something more pre formal

86
00:04:50,692 --> 00:04:51,800
or computational.

87
00:04:54,220 --> 00:04:56,844
Should this recipe be surprising to

88
00:04:56,882 --> 00:04:58,780
people coming from a certain background?

89
00:05:02,200 --> 00:05:04,036
Are there substeps that are relevant to

90
00:05:04,058 --> 00:05:05,750
consider? Brock yes.

91
00:05:09,160 --> 00:05:14,084
I don't have extensive formal

92
00:05:14,132 --> 00:05:18,824
modeling experience, but most

93
00:05:18,862 --> 00:05:20,088
of the things that I've done for the

94
00:05:20,094 --> 00:05:23,564
last half decade entail some form

95
00:05:23,602 --> 00:05:27,484
of this. And I don't see how you

96
00:05:27,522 --> 00:05:30,364
could model anything without going

97
00:05:30,402 --> 00:05:34,428
through these basic questions

98
00:05:34,514 --> 00:05:35,150
here.

99
00:05:39,380 --> 00:05:42,210
I don't see how this is specific or

100
00:05:45,140 --> 00:05:47,596
exclusive or whatever to active

101
00:05:47,628 --> 00:05:53,242
inference. It's a bit ambiguous

102
00:05:53,306 --> 00:05:55,994
to me. How that besides specifically

103
00:05:56,042 --> 00:05:57,860
degenerative model, I guess. But.

104
00:05:59,670 --> 00:06:01,442
Which system are we modeling? Another

105
00:06:01,496 --> 00:06:03,334
way to ask that is what questions are we

106
00:06:03,372 --> 00:06:07,062
trying to answer or

107
00:06:07,116 --> 00:06:11,206
who? I think Lyle brought this up like,

108
00:06:11,388 --> 00:06:13,094
who are you making the model for? Sort

109
00:06:13,132 --> 00:06:13,880
of thing.

110
00:06:18,030 --> 00:06:20,700
Because the same like you were saying

111
00:06:21,950 --> 00:06:27,534
just earlier about the

112
00:06:27,572 --> 00:06:29,594
particular choice of how you're modeling

113
00:06:29,642 --> 00:06:33,578
it is just a choice

114
00:06:33,674 --> 00:06:38,026
on that system, and that entails

115
00:06:38,218 --> 00:06:41,520
particular kind of

116
00:06:41,970 --> 00:06:45,038
hidden states, mark on blanket, et

117
00:06:45,054 --> 00:06:51,266
cetera. That may

118
00:06:51,288 --> 00:06:53,894
be relevant or may not be, depending on

119
00:06:53,932 --> 00:06:55,606
the questions, what you're trying to

120
00:06:55,628 --> 00:06:58,774
model, but that arises from those

121
00:06:58,812 --> 00:07:00,520
questions, I guess, is what I'm saying.

122
00:07:01,850 --> 00:07:03,430
Thank you. Ali.

123
00:07:06,990 --> 00:07:09,930
Well, yeah, I just wanted to mention

124
00:07:10,000 --> 00:07:13,270
that, referring back to your previous

125
00:07:13,350 --> 00:07:17,234
question, I also am currently developing

126
00:07:17,382 --> 00:07:21,482
a model, actually an emotion

127
00:07:21,546 --> 00:07:25,454
perceiving agent by pretty much

128
00:07:25,492 --> 00:07:28,350
closely following those four steps.

129
00:07:29,670 --> 00:07:31,874
But I'm getting some helps in that

130
00:07:31,912 --> 00:07:34,174
regard in how to actually implement

131
00:07:34,302 --> 00:07:37,474
those four steps in my particular

132
00:07:37,592 --> 00:07:41,802
situation. But on the topic of modeling,

133
00:07:41,966 --> 00:07:45,430
well, at a very basic level,

134
00:07:45,580 --> 00:07:49,542
we can categorize different

135
00:07:49,596 --> 00:07:52,518
approaches to modelings in terms of

136
00:07:52,604 --> 00:07:55,500
answering to three different questions

137
00:07:56,350 --> 00:07:59,546
the what models, the how models, and the

138
00:07:59,568 --> 00:08:02,442
why models. At least that's the kind of

139
00:08:02,496 --> 00:08:06,070
approach mostly done in neuroscientific

140
00:08:06,150 --> 00:08:09,390
modeling. So in my opinion,

141
00:08:10,530 --> 00:08:16,480
these four steps and this recipe can

142
00:08:16,930 --> 00:08:20,338
be applied mostly to

143
00:08:20,504 --> 00:08:22,260
the how and the why.

144
00:08:36,730 --> 00:08:40,470
What models are mostly mostly

145
00:08:41,530 --> 00:08:45,514
descriptive models. But when

146
00:08:45,552 --> 00:08:49,482
we want to model an agent and

147
00:08:49,536 --> 00:08:51,180
self evidencing agent,

148
00:08:54,590 --> 00:08:58,160
which behaves as if it

149
00:08:58,770 --> 00:09:01,550
infers something about its environment,

150
00:09:02,050 --> 00:09:04,014
we have moved beyond just the

151
00:09:04,052 --> 00:09:07,262
descriptive stage and we're dealing with

152
00:09:07,396 --> 00:09:10,450
the how and probably even the why

153
00:09:10,520 --> 00:09:11,780
questions there.

154
00:09:15,430 --> 00:09:18,226
Thanks. When I hear why, I always think

155
00:09:18,248 --> 00:09:20,834
of Tin Bergen's four questions, the four

156
00:09:20,872 --> 00:09:23,190
whys, aristotle's four whys.

157
00:09:23,610 --> 00:09:28,710
Why is a subjective modeler's

158
00:09:29,210 --> 00:09:32,790
pluralistic playground, and it

159
00:09:32,860 --> 00:09:36,250
doesn't lend itself to one specific even

160
00:09:36,320 --> 00:09:39,930
type of answer, let alone a framing or a

161
00:09:40,000 --> 00:09:42,698
specific parameter combination. And I

162
00:09:42,704 --> 00:09:44,810
think that's a huge aspect of the move

163
00:09:44,880 --> 00:09:47,946
from like what are systems to how am I

164
00:09:47,968 --> 00:09:49,726
going to model the system? Which is a

165
00:09:49,748 --> 00:09:51,550
part of the turn that happens when we

166
00:09:51,620 --> 00:09:55,774
start to model is we dispense with

167
00:09:55,812 --> 00:09:58,302
absolutism. All of a sudden, these

168
00:09:58,356 --> 00:10:01,042
questions about what we're trying to do,

169
00:10:01,176 --> 00:10:04,034
our preferences and affordances and

170
00:10:04,072 --> 00:10:06,546
constraints and limitations and

171
00:10:06,568 --> 00:10:08,914
everything come into play and we might

172
00:10:08,952 --> 00:10:12,710
make a portfolio of models that are

173
00:10:12,780 --> 00:10:15,014
shining light on different aspects of

174
00:10:15,052 --> 00:10:17,750
even the very same phenomena.

175
00:10:18,330 --> 00:10:20,450
So it's kind of like an empirically

176
00:10:20,530 --> 00:10:24,054
grounded pluralism and pragmatism when

177
00:10:24,092 --> 00:10:26,614
we start to be engaged in real useful

178
00:10:26,662 --> 00:10:30,620
modeling. And that is something that

179
00:10:31,470 --> 00:10:34,700
I feel that one learns on the field,

180
00:10:35,230 --> 00:10:39,626
not necessarily bystanding.

181
00:10:39,738 --> 00:10:42,586
So that's just to say that the modeling

182
00:10:42,618 --> 00:10:45,454
is really important, even for

183
00:10:45,492 --> 00:10:48,286
philosophical reasons, to say nothing of

184
00:10:48,308 --> 00:10:49,562
some of the things that are coming to

185
00:10:49,636 --> 00:10:51,986
play in chapter seven and beyond, where

186
00:10:52,008 --> 00:10:55,922
it's like, yeah, the C vector has this

187
00:10:55,976 --> 00:10:59,540
many entries because why

188
00:11:00,470 --> 00:11:02,854
and how many rows and columns does this

189
00:11:02,892 --> 00:11:05,718
have? But once we understand why the

190
00:11:05,724 --> 00:11:07,606
rows and the columns and the shapes of

191
00:11:07,628 --> 00:11:10,086
these different entities are the way

192
00:11:10,108 --> 00:11:13,862
they are, then we just have

193
00:11:13,916 --> 00:11:16,890
to use this scaffold to think about

194
00:11:16,960 --> 00:11:19,050
certain systems,

195
00:11:21,070 --> 00:11:25,014
consider their form at a very structural

196
00:11:25,142 --> 00:11:28,926
level, like is it in discrete time or in

197
00:11:28,948 --> 00:11:32,334
continuous time and so on. And then

198
00:11:32,532 --> 00:11:35,274
we might set up not just one generative

199
00:11:35,322 --> 00:11:38,400
model, but again, families of them.

200
00:11:39,090 --> 00:11:42,050
In the future, we hope and expect that

201
00:11:42,120 --> 00:11:43,314
active block, for instance, will

202
00:11:43,352 --> 00:11:45,422
facilitate us doing parameter sweeps

203
00:11:45,486 --> 00:11:47,666
across families of cognitive models and

204
00:11:47,688 --> 00:11:50,642
generative models. But even in this

205
00:11:50,696 --> 00:11:54,146
boutique phase of model construction

206
00:11:54,178 --> 00:11:57,240
that we're in in current year,

207
00:11:57,690 --> 00:11:59,798
one would still say, okay, I want to

208
00:11:59,804 --> 00:12:01,734
have one model where it's either

209
00:12:01,852 --> 00:12:04,438
acceptable temperature or too hot, and I

210
00:12:04,444 --> 00:12:05,722
want to have one with a three state

211
00:12:05,776 --> 00:12:08,522
model. Too cold, just right, too hot.

212
00:12:08,656 --> 00:12:10,538
Here's going to be a four stage model.

213
00:12:10,704 --> 00:12:13,820
And just being able to specify that,

214
00:12:14,670 --> 00:12:18,270
again, instills pluralism through action

215
00:12:18,690 --> 00:12:21,006
about how we're modeling a system,

216
00:12:21,188 --> 00:12:23,166
which is a philosophical point that it

217
00:12:23,188 --> 00:12:29,050
seems like sometimes isn't grasped

218
00:12:29,210 --> 00:12:32,674
by those who don't engage in the

219
00:12:32,712 --> 00:12:35,346
specific discussions around modeling a

220
00:12:35,368 --> 00:12:35,940
system.

221
00:12:38,950 --> 00:12:42,694
Okay, so where does Figure 6.1

222
00:12:42,732 --> 00:12:43,800
come into play?

223
00:12:51,800 --> 00:12:56,096
It's a rehearsal of some of the figures

224
00:12:56,128 --> 00:12:59,112
we've seen earlier in the book and many,

225
00:12:59,166 --> 00:13:01,960
many other places. This is the

226
00:13:02,030 --> 00:13:06,920
particular partition. The particle is

227
00:13:07,070 --> 00:13:09,752
the internal and the active sensory

228
00:13:09,816 --> 00:13:12,268
states. So internal plus blanket states

229
00:13:12,354 --> 00:13:15,644
equals particular states. Those are the

230
00:13:15,682 --> 00:13:17,672
particles. If we were looking at dust

231
00:13:17,736 --> 00:13:19,992
under the microscope, like Brownian

232
00:13:20,056 --> 00:13:22,888
Motion was describing, initially the

233
00:13:22,914 --> 00:13:25,264
particles were the dust particle, and

234
00:13:25,302 --> 00:13:27,504
then the external states were like that,

235
00:13:27,542 --> 00:13:29,772
which was not the dust particle

236
00:13:29,916 --> 00:13:32,336
partitioning the generative process and

237
00:13:32,358 --> 00:13:34,028
the generative model off from each

238
00:13:34,054 --> 00:13:36,704
other. We're dealing in Bayesian

239
00:13:36,752 --> 00:13:39,892
mechanics with particular states,

240
00:13:40,026 --> 00:13:43,572
with particles, with things that are

241
00:13:43,626 --> 00:13:47,320
able to engage in sensory motor type

242
00:13:47,470 --> 00:13:51,384
loops. This is just bringing in

243
00:13:51,422 --> 00:13:55,796
a few more pieces

244
00:13:55,828 --> 00:13:58,104
than that, which is just to call

245
00:13:58,142 --> 00:14:00,076
attention to the direction and the

246
00:14:00,098 --> 00:14:04,140
existence of the arrows in the work.

247
00:14:04,210 --> 00:14:05,852
How particular is the free energy

248
00:14:05,906 --> 00:14:09,164
principle of aguilera at all? And in

249
00:14:09,202 --> 00:14:11,652
follow up work, there's discussion

250
00:14:11,736 --> 00:14:16,112
around which topologies on

251
00:14:16,166 --> 00:14:19,852
this particular partition facilitate

252
00:14:19,916 --> 00:14:23,276
or enable which kinds of formal claims

253
00:14:23,308 --> 00:14:26,596
to be made. For example, one might

254
00:14:26,618 --> 00:14:28,704
find it kind of interesting that sensory

255
00:14:28,752 --> 00:14:31,092
states have a bi directional arrow with

256
00:14:31,146 --> 00:14:34,324
external states or might

257
00:14:34,362 --> 00:14:36,576
find it equally interesting or

258
00:14:36,618 --> 00:14:37,784
differently interesting or less

259
00:14:37,822 --> 00:14:41,928
interesting that also active states have

260
00:14:42,094 --> 00:14:44,356
an arrow pointing back to internal

261
00:14:44,388 --> 00:14:48,424
states. And of course the

262
00:14:48,462 --> 00:14:50,892
questions that we have raised all along

263
00:14:50,946 --> 00:14:53,196
the way of like, okay, let's just say

264
00:14:53,218 --> 00:14:55,640
that we're going to model a person's

265
00:14:55,800 --> 00:14:59,710
arm. What are the active states,

266
00:15:00,960 --> 00:15:03,648
what are the sensory states? What does

267
00:15:03,654 --> 00:15:05,196
it mean that there's a bi directional

268
00:15:05,228 --> 00:15:08,288
arrow connecting them? Is this the only

269
00:15:08,454 --> 00:15:11,360
topology for this particular partition?

270
00:15:11,940 --> 00:15:14,790
Could we have the around the clock model

271
00:15:15,960 --> 00:15:18,448
external states influencing sensory,

272
00:15:18,544 --> 00:15:20,324
then internal, then active and just

273
00:15:20,362 --> 00:15:23,024
going around the clock without any cross

274
00:15:23,072 --> 00:15:25,700
talk in the blanket, without any back

275
00:15:25,770 --> 00:15:30,340
arrows? Is that plausible?

276
00:15:30,500 --> 00:15:32,440
Does that change the formalism?

277
00:15:33,820 --> 00:15:36,568
On one hand, how could it not change the

278
00:15:36,574 --> 00:15:39,156
formalism when it's changing the

279
00:15:39,198 --> 00:15:40,776
structure of how variables are related

280
00:15:40,808 --> 00:15:41,710
to each other?

281
00:15:43,760 --> 00:15:45,804
Or maybe there are formalisms that are

282
00:15:45,842 --> 00:15:50,190
actually abstracted away from any

283
00:15:50,720 --> 00:15:55,024
topology choice because

284
00:15:55,142 --> 00:15:58,800
what is being described are the flows

285
00:15:59,460 --> 00:16:03,360
and the gradients on these variables.

286
00:16:04,280 --> 00:16:06,820
But then what are those variables.

287
00:16:20,380 --> 00:16:22,696
That. Just highlights this need for

288
00:16:22,798 --> 00:16:25,732
modeling? But if we just really briefly

289
00:16:25,796 --> 00:16:30,350
go. Back to that, it's like

290
00:16:31,120 --> 00:16:32,476
the fact that there's this bi

291
00:16:32,498 --> 00:16:34,296
directional arrow between external

292
00:16:34,328 --> 00:16:38,190
states and sensory states is

293
00:16:40,080 --> 00:16:44,144
bit confusing in

294
00:16:44,182 --> 00:16:48,144
that. Again, the language like well

295
00:16:48,262 --> 00:16:51,388
if it is acting on the environment,

296
00:16:51,484 --> 00:16:54,304
isn't that an action state or something

297
00:16:54,342 --> 00:16:55,190
like this?

298
00:16:57,640 --> 00:17:00,564
In what way is it bi directional? So

299
00:17:00,602 --> 00:17:03,056
could you not draw that as one arrow

300
00:17:03,088 --> 00:17:05,296
going to the sensory state and one arrow

301
00:17:05,328 --> 00:17:07,176
from the sensory state back to the

302
00:17:07,198 --> 00:17:09,032
environment and then say what

303
00:17:09,086 --> 00:17:13,144
functionally does that entail? And then

304
00:17:13,262 --> 00:17:15,588
you're doing this particular selection.

305
00:17:15,764 --> 00:17:19,076
Of how you're modeling. Right. But I

306
00:17:19,118 --> 00:17:20,830
think you have to do that.

307
00:17:22,880 --> 00:17:25,544
To understand why that is bi directional

308
00:17:25,592 --> 00:17:28,824
there right. In that particular framing

309
00:17:28,872 --> 00:17:31,624
of the generative process generate model

310
00:17:31,682 --> 00:17:32,850
there right.

311
00:17:38,500 --> 00:17:42,844
Which is still to me also slightly

312
00:17:42,892 --> 00:17:47,364
nebulous. But I think yeah,

313
00:17:47,482 --> 00:17:50,132
just the bi directionality of it between

314
00:17:50,186 --> 00:17:52,244
the action and sense states that kind of

315
00:17:52,282 --> 00:17:55,796
makes sense almost that

316
00:17:55,818 --> 00:17:57,604
you sense your actions and your actions

317
00:17:57,652 --> 00:17:59,636
or response to your sense. But that's

318
00:17:59,668 --> 00:18:01,432
not really what's being done there

319
00:18:01,486 --> 00:18:03,130
precisely either. Right.

320
00:18:07,660 --> 00:18:10,396
Just explicating. That is what we need

321
00:18:10,418 --> 00:18:14,830
to separate and explicate what

322
00:18:16,240 --> 00:18:20,652
yeah. Another really important question

323
00:18:20,786 --> 00:18:24,096
is like this action perception loop is

324
00:18:24,118 --> 00:18:27,410
appealed to early and often.

325
00:18:27,860 --> 00:18:30,748
It's the first picture in the textbook

326
00:18:30,924 --> 00:18:32,944
before the high road and the low road in

327
00:18:32,982 --> 00:18:36,000
1.2 is eleven.

328
00:18:36,520 --> 00:18:39,812
We're talking about agents and their

329
00:18:39,866 --> 00:18:42,688
interfacing with the niche and we're

330
00:18:42,704 --> 00:18:44,292
going to end up calling how we model

331
00:18:44,346 --> 00:18:46,496
agents a generative model and we're

332
00:18:46,528 --> 00:18:48,436
going to end up calling the niche and

333
00:18:48,458 --> 00:18:50,068
the dynamics of the niche and how it's

334
00:18:50,084 --> 00:18:51,930
influenced as the generative process.

335
00:18:52,620 --> 00:18:54,612
And we're going to call the interface

336
00:18:54,676 --> 00:18:55,928
between the generative model and the

337
00:18:55,934 --> 00:18:58,056
generative process a Markov blanket and

338
00:18:58,078 --> 00:18:59,444
it's going to inherit this technical

339
00:18:59,492 --> 00:19:02,088
definition from Bayes graphs and we're

340
00:19:02,104 --> 00:19:04,248
going to add these variables and we're

341
00:19:04,264 --> 00:19:05,964
going to connect them a certain way.

342
00:19:06,162 --> 00:19:08,184
But we're talking about the feedback

343
00:19:08,232 --> 00:19:10,380
loop with the agent in the niche.

344
00:19:13,700 --> 00:19:16,930
Oh, so what is this then?

345
00:19:19,380 --> 00:19:23,490
Are the observations the sensory states?

346
00:19:24,740 --> 00:19:27,008
And this isn't just like a low hanging

347
00:19:27,024 --> 00:19:31,216
fruit like well then why couldn't

348
00:19:31,248 --> 00:19:34,612
we call them? I mean, they are called as

349
00:19:34,746 --> 00:19:38,064
X and Y. Interestingly in the continuous

350
00:19:38,112 --> 00:19:41,016
time formulation and I get it that

351
00:19:41,038 --> 00:19:44,232
they're being distinguished here because

352
00:19:44,286 --> 00:19:46,696
this is the continuous time POMDP and

353
00:19:46,718 --> 00:19:50,040
the discrete time POMDP.

354
00:19:51,840 --> 00:19:56,584
But where do we see active

355
00:19:56,632 --> 00:19:57,870
states here?

356
00:20:00,320 --> 00:20:01,900
We see policies.

357
00:20:05,390 --> 00:20:08,358
So are active states along the branch

358
00:20:08,454 --> 00:20:11,914
influencing the B matrix how

359
00:20:11,952 --> 00:20:13,740
the hidden states change through time?

360
00:20:15,710 --> 00:20:21,070
So there may be a very simple trail

361
00:20:21,490 --> 00:20:24,910
from this action perception loop

362
00:20:26,290 --> 00:20:30,318
framing to the discrete

363
00:20:30,334 --> 00:20:32,370
and continuous time POMDPs.

364
00:20:35,110 --> 00:20:38,738
Yeah, I think just to me, the simple way

365
00:20:38,904 --> 00:20:40,466
instead of thinking of this as a

366
00:20:40,488 --> 00:20:42,190
circular thing where you're literally

367
00:20:42,270 --> 00:20:44,706
going from one state to the next state

368
00:20:44,728 --> 00:20:46,514
to the way you might to in a normal

369
00:20:46,562 --> 00:20:49,320
state machine if you're thinking of no

370
00:20:49,930 --> 00:20:52,280
when you have an active state,

371
00:20:55,310 --> 00:20:58,154
however, it is changing. That is

372
00:20:58,192 --> 00:21:00,794
necessarily entailing change in the.

373
00:21:00,832 --> 00:21:03,126
Sensory external state and the internal

374
00:21:03,158 --> 00:21:06,026
states and vice versa. Here with the

375
00:21:06,048 --> 00:21:09,374
sensory states like that. If you perform

376
00:21:09,492 --> 00:21:11,614
some action of some. Kind, there's some

377
00:21:11,652 --> 00:21:14,554
active change in that state. Like you're

378
00:21:14,602 --> 00:21:18,074
necessarily changing the internal hidden

379
00:21:18,122 --> 00:21:20,420
state, right?

380
00:21:22,870 --> 00:21:26,834
If you grab a

381
00:21:26,952 --> 00:21:29,474
glass of water or something like this,

382
00:21:29,672 --> 00:21:33,910
what you're modeling now has changed

383
00:21:34,330 --> 00:21:38,146
slightly, right? And your internal

384
00:21:38,178 --> 00:21:40,406
states and the sensory states are doing

385
00:21:40,428 --> 00:21:43,430
the same thing where the glass of water

386
00:21:43,500 --> 00:21:50,586
or whatever was this object

387
00:21:50,688 --> 00:21:53,802
just sitting there doing nothing, but

388
00:21:53,856 --> 00:21:57,206
now it's become a thing to drink out

389
00:21:57,248 --> 00:22:01,358
of. And that's changed in some sense the

390
00:22:01,524 --> 00:22:04,894
kind of external states that are being

391
00:22:05,092 --> 00:22:08,314
fed into your sensory states. It's

392
00:22:08,362 --> 00:22:11,726
changed the Markov blanket that you're

393
00:22:11,838 --> 00:22:14,900
kind of selecting for or relating to.

394
00:22:21,990 --> 00:22:25,350
I think that is visibly they're

395
00:22:26,250 --> 00:22:28,886
doing the same sort of thing where if

396
00:22:28,908 --> 00:22:31,938
you have this observation, you're

397
00:22:32,034 --> 00:22:35,834
necessarily creating a new internal or

398
00:22:35,872 --> 00:22:37,994
sense state that is going to change your

399
00:22:38,112 --> 00:22:41,526
policy selection and that's

400
00:22:41,558 --> 00:22:43,754
going to change your sense states.

401
00:22:43,952 --> 00:22:44,700
So.

402
00:22:57,670 --> 00:23:00,402
Returning to the recipe, one question

403
00:23:00,456 --> 00:23:03,838
that I think will reduce our uncertainty

404
00:23:03,854 --> 00:23:07,300
about as we really do the modeling is

405
00:23:12,810 --> 00:23:16,422
how do we account for residuals and

406
00:23:16,476 --> 00:23:19,350
understand which modeling residuals,

407
00:23:19,770 --> 00:23:23,290
which is to say variability in data

408
00:23:23,360 --> 00:23:26,140
that are not explained by the model,

409
00:23:27,550 --> 00:23:31,210
which and what amounts of residuals are

410
00:23:31,280 --> 00:23:33,974
acceptable? So if we were doing a linear

411
00:23:34,022 --> 00:23:36,926
regression and the only data we had on

412
00:23:36,948 --> 00:23:39,438
hand was height, and the only data that

413
00:23:39,444 --> 00:23:42,510
we wanted to predict were weight, then

414
00:23:42,660 --> 00:23:44,366
we could just say we've used all the

415
00:23:44,388 --> 00:23:47,726
data and whether the model fits

416
00:23:47,838 --> 00:23:50,802
with an R value of zero one or zero

417
00:23:50,856 --> 00:23:55,122
nine, we know what the residual is of

418
00:23:55,256 --> 00:23:58,446
height on weight regression. And here's

419
00:23:58,478 --> 00:24:00,338
a number that describes what fraction of

420
00:24:00,344 --> 00:24:02,886
the variability was described with this

421
00:24:02,908 --> 00:24:05,622
regression? R value of one being like

422
00:24:05,676 --> 00:24:08,406
it's all perfectly on a line. R value of

423
00:24:08,428 --> 00:24:09,942
zero being like it's just a total

424
00:24:09,996 --> 00:24:13,994
scatter plot. We can say how much

425
00:24:14,112 --> 00:24:17,606
variability is explained and then we're

426
00:24:17,638 --> 00:24:19,146
done with the empirical data that we

427
00:24:19,168 --> 00:24:21,514
had. So we can't explain any more

428
00:24:21,632 --> 00:24:25,706
variability when we're

429
00:24:25,738 --> 00:24:28,430
looking at empirical traces of behavior

430
00:24:29,970 --> 00:24:34,042
to explain even a small fraction

431
00:24:34,186 --> 00:24:37,394
of behavior. For some systems, it might

432
00:24:37,432 --> 00:24:39,214
be the case that the generative models

433
00:24:39,262 --> 00:24:41,090
already must be quite complex.

434
00:24:42,710 --> 00:24:44,946
In another scenario, it might be the

435
00:24:44,968 --> 00:24:48,740
case that a very simple model like

436
00:24:50,550 --> 00:24:54,038
looks to the left are 90% of

437
00:24:54,044 --> 00:24:55,366
the time followed by looking to the

438
00:24:55,388 --> 00:24:58,578
right. I mean on average,

439
00:24:58,674 --> 00:25:02,442
isn't that true? So then when we are

440
00:25:02,496 --> 00:25:04,806
evaluating models, whether in block,

441
00:25:04,838 --> 00:25:07,098
for instance, sweeping across models or

442
00:25:07,184 --> 00:25:10,380
just qualitatively or in a boutique way,

443
00:25:12,350 --> 00:25:16,302
are we looking to reduce the residual to

444
00:25:16,356 --> 00:25:20,014
zero? What traces of

445
00:25:20,052 --> 00:25:21,710
data are we looking to explain

446
00:25:21,780 --> 00:25:25,214
variability in? And then

447
00:25:25,252 --> 00:25:28,254
if we're not going to be with empirical

448
00:25:28,302 --> 00:25:32,146
traces of behavior looking to reduce our

449
00:25:32,328 --> 00:25:34,130
unexplained variance,

450
00:25:35,270 --> 00:25:39,494
what is going to be the criteria by

451
00:25:39,532 --> 00:25:44,246
which we do model selection on the

452
00:25:44,268 --> 00:25:47,398
AIC and the BIC, which are information

453
00:25:47,484 --> 00:25:52,230
criteria, balance variability

454
00:25:52,310 --> 00:25:55,590
explained across families of models

455
00:25:55,750 --> 00:25:58,358
with their degree of parameterization

456
00:25:58,534 --> 00:26:00,646
penalizing having more parameters,

457
00:26:00,758 --> 00:26:02,954
rewarding better explanations. So

458
00:26:03,072 --> 00:26:06,698
seeking to find a balance with models

459
00:26:06,714 --> 00:26:09,070
that are on the kind of pareto frontier

460
00:26:09,570 --> 00:26:11,920
of explanatory and simple,

461
00:26:12,930 --> 00:26:14,862
that's great. That's like one extra

462
00:26:14,916 --> 00:26:18,482
nuance. On top of merely explaining more

463
00:26:18,536 --> 00:26:21,266
variability with a smaller residual, it

464
00:26:21,288 --> 00:26:23,074
also adds in the penalty for having more

465
00:26:23,112 --> 00:26:26,462
parameters. So that in theory

466
00:26:26,526 --> 00:26:30,610
and in practice stops the modeler

467
00:26:30,950 --> 00:26:32,990
from just, okay, well now we're going to

468
00:26:33,000 --> 00:26:34,694
add in the temperature in this other

469
00:26:34,732 --> 00:26:36,518
part of the world because 1% of the

470
00:26:36,524 --> 00:26:38,374
variability just happened to be

471
00:26:38,412 --> 00:26:40,466
explained. So now we're just developing

472
00:26:40,498 --> 00:26:42,874
these like totally spiraling looking for

473
00:26:42,912 --> 00:26:45,754
more and more data sources to explain a

474
00:26:45,792 --> 00:26:47,846
decreasing amount of variability that's

475
00:26:47,878 --> 00:26:50,474
left. So it's good to pull back from

476
00:26:50,512 --> 00:26:53,450
that edge through the use of information

477
00:26:53,520 --> 00:26:56,158
criterion. But at the core of the

478
00:26:56,164 --> 00:26:59,454
criterion is still the imperative to

479
00:26:59,492 --> 00:27:02,702
reduce the residual, to have a model

480
00:27:02,756 --> 00:27:05,950
that fits within

481
00:27:06,020 --> 00:27:08,786
a model the way that we fine tune it so

482
00:27:08,808 --> 00:27:10,402
that it fits data well, but not

483
00:27:10,456 --> 00:27:13,058
overfitting. And then across models we

484
00:27:13,064 --> 00:27:14,766
can see an analogous process of wanting

485
00:27:14,798 --> 00:27:18,210
the model structure that fits without,

486
00:27:18,360 --> 00:27:20,054
for example, overfitting or over

487
00:27:20,092 --> 00:27:21,350
including parameters.

488
00:27:23,930 --> 00:27:26,726
So I think one question which might be

489
00:27:26,748 --> 00:27:30,520
added here is like what data do we have

490
00:27:35,070 --> 00:27:38,346
if we're doing a didactic model and we

491
00:27:38,368 --> 00:27:41,146
just want to sketch it out and run a

492
00:27:41,168 --> 00:27:43,660
simulation and just purely generate data

493
00:27:44,350 --> 00:27:45,902
that might be useful in certain

494
00:27:45,956 --> 00:27:49,262
settings. But the question

495
00:27:49,316 --> 00:27:51,854
of what behavioral data we have is very

496
00:27:51,892 --> 00:27:53,598
nontrivial if we're going to be

497
00:27:53,604 --> 00:27:54,874
approaching this as an empirical

498
00:27:54,922 --> 00:27:58,658
analysis problem. Brock just another

499
00:27:58,744 --> 00:27:59,780
question there.

500
00:28:04,150 --> 00:28:06,466
What observations must we make or

501
00:28:06,488 --> 00:28:09,718
something is related to what data do

502
00:28:09,724 --> 00:28:12,760
we have? If you don't have the data that

503
00:28:13,290 --> 00:28:15,154
if there's not sufficient affordance

504
00:28:15,282 --> 00:28:17,046
information contained in the data to

505
00:28:17,068 --> 00:28:18,520
model the problem, then.

506
00:28:28,720 --> 00:28:31,500
Do we have to model our action selection

507
00:28:33,440 --> 00:28:37,790
at a second or third order?

508
00:28:39,060 --> 00:28:42,176
Yeah, exactly. To realize the right

509
00:28:42,198 --> 00:28:43,410
epistemic value.

510
00:28:47,220 --> 00:28:50,530
Okay, so that was figure 61,

511
00:28:55,640 --> 00:28:57,220
discrete and continuous,

512
00:28:58,680 --> 00:29:01,460
shallow, deep and hierarchical.

513
00:29:02,040 --> 00:29:05,640
So just to address

514
00:29:05,710 --> 00:29:08,724
this, which might come up shallow

515
00:29:08,772 --> 00:29:13,080
versus deep is describing how iterated

516
00:29:13,740 --> 00:29:17,310
the model is through time,

517
00:29:18,000 --> 00:29:21,070
within one type of time.

518
00:29:21,440 --> 00:29:23,704
So you could have the 1 hour shallow

519
00:29:23,752 --> 00:29:27,276
model where hours are our units, or you

520
00:29:27,298 --> 00:29:28,812
could have the 10 hours where it goes

521
00:29:28,866 --> 00:29:32,448
1234-5678, 910, or you could have 100

522
00:29:32,534 --> 00:29:35,136
depth of 100 with a time click of one.

523
00:29:35,318 --> 00:29:38,240
Or one could have a hierarchical model

524
00:29:38,390 --> 00:29:40,932
where there's 10 hours that make like a

525
00:29:40,986 --> 00:29:44,996
deca hour and then there's ten clicks of

526
00:29:45,018 --> 00:29:48,656
the deca hour to reach a planning

527
00:29:48,688 --> 00:29:51,940
horizon of 100 time steps.

528
00:29:52,380 --> 00:29:56,136
So deep is describing within

529
00:29:56,318 --> 00:30:00,010
one unit or counter of time

530
00:30:00,780 --> 00:30:06,008
how iterated that counter is depth

531
00:30:06,104 --> 00:30:09,032
is describing strict hierarchical

532
00:30:09,096 --> 00:30:12,684
nesting and so they can both

533
00:30:12,722 --> 00:30:16,844
be used separately or together to

534
00:30:16,882 --> 00:30:20,112
describe events over longer and

535
00:30:20,166 --> 00:30:21,920
longer time horizons.

536
00:30:23,540 --> 00:30:25,856
Crucially, this is in the context of

537
00:30:25,878 --> 00:30:28,128
language processing. The duration of the

538
00:30:28,134 --> 00:30:31,636
word transcends that of any phoneme and

539
00:30:31,658 --> 00:30:34,068
the sentence transcends that of any word

540
00:30:34,154 --> 00:30:37,136
in the sequence. And if we assume

541
00:30:37,168 --> 00:30:38,384
paragraphs are made of multiple

542
00:30:38,432 --> 00:30:41,664
sentences, paragraphs always transcend

543
00:30:41,712 --> 00:30:44,964
and encompass sentences. So anyone else

544
00:30:45,002 --> 00:30:47,272
want to just add a point? I'm sure that

545
00:30:47,326 --> 00:30:49,144
you fellows are familiar with this

546
00:30:49,182 --> 00:30:51,128
distinction, but just wanted to point it

547
00:30:51,134 --> 00:30:53,956
out since temporality in these models

548
00:30:54,068 --> 00:30:57,372
and doing model selection on different

549
00:30:57,506 --> 00:31:00,924
forms of temporality is going to be an

550
00:31:00,962 --> 00:31:02,700
essential piece of the puzzle.

551
00:31:09,600 --> 00:31:13,296
Yeah. And it also reminds me of the

552
00:31:13,318 --> 00:31:16,496
Shankarian analysis theory that, if you

553
00:31:16,518 --> 00:31:20,000
remember, we talked about earlier,

554
00:31:20,580 --> 00:31:22,720
and it's hierarchical.

555
00:31:23,960 --> 00:31:30,576
It's temporal hierarchy that tries

556
00:31:30,608 --> 00:31:34,644
to describe the whole musical piece as

557
00:31:34,762 --> 00:31:38,036
a kind of hierarchical layers, which I

558
00:31:38,058 --> 00:31:41,576
think maps perfectly into this specific

559
00:31:41,678 --> 00:31:45,960
example of language parsing because

560
00:31:46,110 --> 00:31:50,030
that also is a kind of thing that

561
00:31:50,800 --> 00:31:54,524
should ideally be modeled in

562
00:31:54,562 --> 00:31:58,236
a very hierarchical way. But the

563
00:31:58,258 --> 00:32:02,384
caveat here is unlike language, which is

564
00:32:02,582 --> 00:32:05,970
at least in some languages, are

565
00:32:06,580 --> 00:32:10,112
necessarily hierarchical, music is

566
00:32:10,246 --> 00:32:13,836
at least tonal. Music or even nontonal

567
00:32:13,868 --> 00:32:18,756
music are not necessarily they

568
00:32:18,778 --> 00:32:21,120
don't necessarily have this hierarchical

569
00:32:21,200 --> 00:32:23,876
structure. So depending on the kind of

570
00:32:23,898 --> 00:32:27,032
music or the genre of music, we might

571
00:32:27,086 --> 00:32:30,692
have either a very shallow

572
00:32:30,756 --> 00:32:33,464
and non hierarchical structure or a very

573
00:32:33,502 --> 00:32:38,010
deep one. So that's something that

574
00:32:39,280 --> 00:32:43,244
I think again refers back to the kind of

575
00:32:43,362 --> 00:32:47,036
system we're trying to model. But even

576
00:32:47,218 --> 00:32:50,750
that kind of system needs to be more

577
00:32:52,500 --> 00:32:56,656
specified in a more granular level in

578
00:32:56,678 --> 00:33:00,332
order to have an effective modeling

579
00:33:00,396 --> 00:33:03,936
strategy there. Thanks for bringing it

580
00:33:03,958 --> 00:33:07,430
in that domain. It makes me think of

581
00:33:09,560 --> 00:33:12,436
the intro, the chorus, the bridge, the

582
00:33:12,458 --> 00:33:15,140
verse. Perhaps these are only in certain

583
00:33:15,210 --> 00:33:17,784
genre of music, but analogously and then

584
00:33:17,822 --> 00:33:20,776
the measure transcends any beat in the

585
00:33:20,798 --> 00:33:23,176
measure with a special case of like a

586
00:33:23,198 --> 00:33:25,976
one one time signature or something and

587
00:33:25,998 --> 00:33:29,484
so on. And then, although language is

588
00:33:29,522 --> 00:33:31,624
described in a strictly hierarchical

589
00:33:31,672 --> 00:33:35,004
sense, I'm wondering how syntax and

590
00:33:35,042 --> 00:33:37,868
grammar actually create kind of a

591
00:33:37,874 --> 00:33:40,872
strange loop. For example, it's trivial

592
00:33:40,936 --> 00:33:42,752
to say that phonemes are nested within

593
00:33:42,806 --> 00:33:45,504
words and sentences and so on, but what

594
00:33:45,542 --> 00:33:48,704
about the semantics of a sentence that

595
00:33:48,742 --> 00:33:51,680
contains commas and exceptions?

596
00:33:52,020 --> 00:33:53,936
And then at the end the speaker says the

597
00:33:53,958 --> 00:33:55,844
previous sentence is not how you think

598
00:33:55,882 --> 00:33:59,572
it is. That is also

599
00:33:59,706 --> 00:34:02,016
causing kind of like a deep linguistic

600
00:34:02,048 --> 00:34:06,116
recall that can

601
00:34:06,138 --> 00:34:08,564
be trivially modeled as containing

602
00:34:08,612 --> 00:34:12,200
nested units. But will

603
00:34:12,270 --> 00:34:16,180
a model that narrowly considers

604
00:34:16,340 --> 00:34:18,456
the semantics of words, then the

605
00:34:18,478 --> 00:34:21,596
semantics of clauses, how will it be

606
00:34:21,618 --> 00:34:23,470
able to address some of these questions?

607
00:34:24,800 --> 00:34:27,052
Here I think we can point to a really

608
00:34:27,106 --> 00:34:30,216
excellent attribute of the active

609
00:34:30,248 --> 00:34:34,784
inference modeling framework. So in

610
00:34:34,822 --> 00:34:38,336
this discussion where the LW paper was

611
00:34:38,358 --> 00:34:42,930
cited, we could just

612
00:34:43,540 --> 00:34:45,824
marvel at nested systems and draw

613
00:34:45,862 --> 00:34:48,436
blankets on blankets and so on. They

614
00:34:48,458 --> 00:34:50,176
could be drawn around every organelle

615
00:34:50,208 --> 00:34:52,148
and every little lipid granule in the

616
00:34:52,154 --> 00:34:55,844
neurons or the brain regions. And so we

617
00:34:55,882 --> 00:34:57,784
discussed a little earlier like doing

618
00:34:57,902 --> 00:34:59,848
structural model selection on which one

619
00:34:59,854 --> 00:35:03,176
of those are relevant. However, this

620
00:35:03,198 --> 00:35:05,016
does not mean we need to attempt to

621
00:35:05,038 --> 00:35:06,520
model the entire brain to develop

622
00:35:06,590 --> 00:35:10,476
meaningful pragmatic simulations of a

623
00:35:10,498 --> 00:35:13,644
single level. For example, if we wanted

624
00:35:13,682 --> 00:35:15,276
to focus on word processing, we could

625
00:35:15,298 --> 00:35:16,988
address some aspects without having to

626
00:35:16,994 --> 00:35:18,700
deal with phoneme processes.

627
00:35:20,720 --> 00:35:22,408
This means we can treat inputs from

628
00:35:22,434 --> 00:35:23,836
parts of the brain drawing inference

629
00:35:23,868 --> 00:35:26,108
about phonemes as providing observations

630
00:35:26,204 --> 00:35:28,124
from the perspective of word processing

631
00:35:28,172 --> 00:35:31,468
areas. So one could say I'm

632
00:35:31,484 --> 00:35:35,140
going to make a figure 4.3 where

633
00:35:35,210 --> 00:35:39,424
S is the true word and phonemes

634
00:35:39,472 --> 00:35:42,932
are being passed as observations to my

635
00:35:42,986 --> 00:35:46,308
word inference engine. And someone can

636
00:35:46,314 --> 00:35:48,776
say but aren't phonemes the result of

637
00:35:48,798 --> 00:35:50,650
inference? And someone can say great,

638
00:35:51,260 --> 00:35:54,856
make that module in blockprints, make

639
00:35:54,878 --> 00:35:58,010
that module and pass me that data.

640
00:35:58,940 --> 00:36:01,436
And then we can do a nested model and we

641
00:36:01,458 --> 00:36:04,830
can graft those models together, but

642
00:36:05,680 --> 00:36:07,896
through the Markov blanket formalism,

643
00:36:08,088 --> 00:36:10,104
not even the whole Markov blanket

644
00:36:10,152 --> 00:36:12,748
bounding a specific particular entity,

645
00:36:12,924 --> 00:36:15,264
but just the trivial Markov blanket that

646
00:36:15,302 --> 00:36:18,076
exists insulating any two unconnected

647
00:36:18,108 --> 00:36:21,970
nodes in a base graph. We can use that

648
00:36:22,420 --> 00:36:25,376
broader, still technical concept of a

649
00:36:25,398 --> 00:36:28,016
Markov blanket and just say all right,

650
00:36:28,198 --> 00:36:31,584
phoneium observations come in and surely

651
00:36:31,632 --> 00:36:34,484
we could make a model where another type

652
00:36:34,522 --> 00:36:36,196
of information comes in and the

653
00:36:36,218 --> 00:36:37,716
inference is on phoneme and that's

654
00:36:37,748 --> 00:36:40,808
what's passed out. So I

655
00:36:40,974 --> 00:36:43,124
think again to point to this advantage

656
00:36:43,172 --> 00:36:46,756
of active inference modeling. It allows

657
00:36:46,788 --> 00:36:51,752
us to situate

658
00:36:51,816 --> 00:36:54,584
phenomena as complex multiscale nested

659
00:36:54,632 --> 00:37:00,296
systems and also bite

660
00:37:00,328 --> 00:37:04,016
off what we can chew and makes it an

661
00:37:04,038 --> 00:37:07,164
empirical question of how grafting

662
00:37:07,212 --> 00:37:09,068
together which different structures

663
00:37:09,164 --> 00:37:12,556
might be useful. But we don't

664
00:37:12,588 --> 00:37:16,624
need to go

665
00:37:16,742 --> 00:37:18,324
all the way like turtles all the way

666
00:37:18,362 --> 00:37:21,604
down. We can just say, even if it were

667
00:37:21,642 --> 00:37:23,936
turtles all the way down. We're modeling

668
00:37:23,968 --> 00:37:26,868
the third through the fifth turtle. And

669
00:37:26,954 --> 00:37:29,348
the fifth turtle gets a top down prior,

670
00:37:29,524 --> 00:37:32,004
and the third turtle gets sensory input

671
00:37:32,052 --> 00:37:34,904
from one below it. And that's the part

672
00:37:34,942 --> 00:37:37,396
of the stack. And this is the lateral

673
00:37:37,428 --> 00:37:39,640
width that we're modeling.

674
00:37:44,720 --> 00:37:47,016
Another advantage of active inference,

675
00:37:47,048 --> 00:37:48,632
which not even saying this is a unique

676
00:37:48,696 --> 00:37:51,668
advantage, is that this goal

677
00:37:51,704 --> 00:37:55,708
directedness and corollary capacities

678
00:37:55,804 --> 00:37:59,296
like counterfactuals on goals and the

679
00:37:59,318 --> 00:38:01,200
way that counterfactuals on world states

680
00:38:01,270 --> 00:38:04,312
influence our goal selection, nested

681
00:38:04,396 --> 00:38:07,780
goals and transient goals or

682
00:38:07,850 --> 00:38:11,460
conditional goals can be resolved

683
00:38:12,200 --> 00:38:15,220
in an unfolding action perception loop.

684
00:38:16,780 --> 00:38:20,792
That's something that as this example of

685
00:38:20,846 --> 00:38:24,520
like wanting to enter an apartment,

686
00:38:25,420 --> 00:38:28,840
of being goal driven over

687
00:38:28,910 --> 00:38:31,592
multiple timescales and then being able

688
00:38:31,646 --> 00:38:33,772
to again zoom in. So it's like, oh,

689
00:38:33,826 --> 00:38:35,884
grab the keys. Okay, well here's the key

690
00:38:35,922 --> 00:38:37,196
grabbing module. And someone says,

691
00:38:37,218 --> 00:38:39,020
well, the key grabbing is actually

692
00:38:39,170 --> 00:38:40,636
accomplished through this kind of

693
00:38:40,658 --> 00:38:44,928
grasping behavior, then that

694
00:38:45,014 --> 00:38:47,328
could be modeled. Or you can just say

695
00:38:47,414 --> 00:38:50,844
we're subsuming finger grasping

696
00:38:50,972 --> 00:38:54,060
kinetics in the grasping module

697
00:38:54,220 --> 00:38:55,716
because the phenomena that we're trying

698
00:38:55,738 --> 00:38:58,436
to explain and the data we have is just

699
00:38:58,458 --> 00:39:01,524
about when things were grasped or maybe

700
00:39:01,562 --> 00:39:03,236
the only observations we have is when

701
00:39:03,258 --> 00:39:06,564
apartments were entered. So we can

702
00:39:06,602 --> 00:39:08,550
actually, I expect,

703
00:39:10,520 --> 00:39:14,660
frame systems extremely expansively

704
00:39:15,480 --> 00:39:18,144
and clarify where we're doing formal

705
00:39:18,192 --> 00:39:19,060
modeling.

706
00:39:32,760 --> 00:39:36,056
I think maybe this

707
00:39:36,078 --> 00:39:39,044
is just a philosophical or ontological

708
00:39:39,092 --> 00:39:42,088
framing of it, but I don't see how you

709
00:39:42,094 --> 00:39:45,930
can model anything well

710
00:39:46,300 --> 00:39:50,270
without admitting that it is either

711
00:39:51,520 --> 00:39:56,248
incomplete or partially

712
00:39:56,344 --> 00:39:59,896
consistent or that

713
00:39:59,938 --> 00:40:02,704
there is some hidden state which is

714
00:40:02,742 --> 00:40:05,776
probably not included in the model. And

715
00:40:05,878 --> 00:40:08,784
to me, active inference kind of.

716
00:40:08,822 --> 00:40:12,160
Natively has an affordance,

717
00:40:12,520 --> 00:40:15,750
at least at the very least a bookmark to

718
00:40:16,360 --> 00:40:17,350
do that.

719
00:40:19,400 --> 00:40:23,120
And so whatever depth

720
00:40:23,200 --> 00:40:25,690
or hierarchy you model the system.

721
00:40:28,540 --> 00:40:31,096
It'S okay. It's okay that there's still

722
00:40:31,118 --> 00:40:34,584
hidden state and it's incomplete. If it

723
00:40:34,622 --> 00:40:36,712
sufficiently models answers the question

724
00:40:36,766 --> 00:40:40,284
at hand, then you can always add

725
00:40:40,322 --> 00:40:43,324
another layer or make the model more

726
00:40:43,362 --> 00:40:46,076
complex. And that's just what we were

727
00:40:46,098 --> 00:40:47,420
going to do anyways.

728
00:40:49,540 --> 00:40:52,608
Nice comment. I totally agree with that.

729
00:40:52,774 --> 00:40:55,136
Again, thinking back to the linear model

730
00:40:55,238 --> 00:40:59,152
example, where is that

731
00:40:59,206 --> 00:41:03,312
extra information about adjacent

732
00:41:03,376 --> 00:41:04,950
possibles for the model?

733
00:41:06,920 --> 00:41:10,470
It's unstructured. It's kind of like

734
00:41:11,000 --> 00:41:13,156
either the structured information is

735
00:41:13,178 --> 00:41:15,320
going to enter your linear regression

736
00:41:15,660 --> 00:41:19,048
just so and everything that's outside of

737
00:41:19,054 --> 00:41:22,056
your linear regression is we're going to

738
00:41:22,078 --> 00:41:25,208
need to go back to the square one. If we

739
00:41:25,214 --> 00:41:26,968
include a second observable, we're going

740
00:41:26,974 --> 00:41:28,648
to have to go to square one and do the

741
00:41:28,654 --> 00:41:30,492
model selection all over again and test

742
00:41:30,546 --> 00:41:32,510
for all the repeat interactions again.

743
00:41:33,600 --> 00:41:35,596
But in these multiscale models that

744
00:41:35,618 --> 00:41:37,308
we're discussing, like you said, it was

745
00:41:37,314 --> 00:41:39,230
a bookmark. It's kind of a nice way to

746
00:41:40,180 --> 00:41:42,720
frame it, which is like the phonemes

747
00:41:43,140 --> 00:41:46,752
you're bookmarking or leaving an open

748
00:41:46,806 --> 00:41:48,370
USB port or something.

749
00:41:50,180 --> 00:41:54,340
Anything that outputs a phoneme

750
00:41:54,760 --> 00:41:57,350
will plug into that part of the model

751
00:41:58,760 --> 00:42:02,740
anything that is able to

752
00:42:02,810 --> 00:42:06,632
hear can be plugged in to

753
00:42:06,686 --> 00:42:10,296
the frog riveting. Anything that can see

754
00:42:10,478 --> 00:42:13,556
can be plugged into the visual component

755
00:42:13,588 --> 00:42:17,136
of the frog. And so it's

756
00:42:17,188 --> 00:42:20,444
almost like consistent with all of this

757
00:42:20,562 --> 00:42:23,852
blanket talk, we're being clear about

758
00:42:23,906 --> 00:42:26,716
what is modeled formally, and the

759
00:42:26,738 --> 00:42:30,296
borders of the formal model are

760
00:42:30,338 --> 00:42:33,456
degrees of freedom as

761
00:42:33,558 --> 00:42:37,120
formulated to be expanded upon

762
00:42:37,540 --> 00:42:38,800
and composed.

763
00:42:43,560 --> 00:42:47,076
One other, I'm not

764
00:42:47,098 --> 00:42:49,844
sure the context is something I heard

765
00:42:49,962 --> 00:42:55,376
Ben Gertzel say attributed to Pink

766
00:42:55,408 --> 00:43:00,372
Schweight, but Pei Wang that in circular

767
00:43:00,436 --> 00:43:03,016
reasoning, when your circle. Gets big

768
00:43:03,038 --> 00:43:05,000
enough, it becomes coherent.

769
00:43:08,640 --> 00:43:10,524
Maybe another way of saying like, that

770
00:43:10,562 --> 00:43:12,380
if your model is insufficient,

771
00:43:15,280 --> 00:43:16,510
that if. You.

772
00:43:19,200 --> 00:43:22,432
Add enough states enough, if you

773
00:43:22,486 --> 00:43:24,480
expand your blanket enough, eventually

774
00:43:25,140 --> 00:43:26,764
if you're modeling something correctly,

775
00:43:26,812 --> 00:43:30,236
I guess the generative

776
00:43:30,268 --> 00:43:33,430
process can match

777
00:43:39,860 --> 00:43:42,048
your blanket states can can come into

778
00:43:42,214 --> 00:43:43,280
alignment.

779
00:43:48,440 --> 00:43:52,176
Nice. Some more active inference

780
00:43:52,208 --> 00:43:56,096
isms so exteriorceptive proprioceptive,

781
00:43:56,128 --> 00:44:00,196
interceptive. We're going to use active

782
00:44:00,228 --> 00:44:03,556
inference modeling memory, attention,

783
00:44:03,668 --> 00:44:06,500
anticipation, planning as inference.

784
00:44:06,660 --> 00:44:08,680
We're going to use active inference.

785
00:44:09,820 --> 00:44:12,380
That is what it looks like. To integrate

786
00:44:12,880 --> 00:44:16,430
disparate fields using one model,

787
00:44:17,280 --> 00:44:19,100
one family of models.

788
00:44:23,700 --> 00:44:28,912
Others can work out some fun

789
00:44:28,966 --> 00:44:32,130
ways to communicate and frame it.

790
00:44:32,900 --> 00:44:36,324
But I think when people are like, how is

791
00:44:36,362 --> 00:44:38,996
this simplifying when it might feel like

792
00:44:39,018 --> 00:44:42,976
it's bringing in a lot or even stepping

793
00:44:43,008 --> 00:44:44,884
on namespaces that people already have

794
00:44:44,922 --> 00:44:46,736
familiarity in? Like we discussed

795
00:44:46,768 --> 00:44:47,540
earlier,

796
00:44:50,340 --> 00:44:52,576
you could ask, how is memory related to

797
00:44:52,598 --> 00:44:56,684
attention? Or perhaps more saliently,

798
00:44:56,812 --> 00:44:59,196
how will you model how memory is related

799
00:44:59,228 --> 00:45:02,308
to attention? Are you

800
00:45:02,314 --> 00:45:03,716
going to go on archive and just look for

801
00:45:03,738 --> 00:45:06,516
neural network architectures? Are you

802
00:45:06,538 --> 00:45:09,776
going to read a book written

803
00:45:09,808 --> 00:45:13,124
before computers were invented and look

804
00:45:13,162 --> 00:45:16,600
at prose? Are you going to

805
00:45:16,670 --> 00:45:19,240
dive into the psychoanalytic tradition

806
00:45:20,700 --> 00:45:22,392
or something that's maybe even

807
00:45:22,446 --> 00:45:27,688
clinically, biomedical what's

808
00:45:27,704 --> 00:45:30,776
the move? And active

809
00:45:30,808 --> 00:45:37,500
inference addresses that's

810
00:45:43,840 --> 00:45:48,056
in these theme

811
00:45:48,088 --> 00:45:50,524
of words that people use that mean

812
00:45:50,562 --> 00:45:54,510
different things. For 400, we have

813
00:45:54,960 --> 00:45:58,816
Bay's optimal behavior. Well,

814
00:45:58,838 --> 00:46:00,770
then why do things go wrong?

815
00:46:02,500 --> 00:46:04,880
And there's multiple layers to unpack.

816
00:46:05,220 --> 00:46:08,852
First is going wrong is about

817
00:46:08,986 --> 00:46:11,732
your perspective on how the system,

818
00:46:11,866 --> 00:46:14,436
perhaps how you prefer it to be or how

819
00:46:14,458 --> 00:46:18,710
you expected it to be. But given where

820
00:46:19,980 --> 00:46:23,256
you put the dish when it

821
00:46:23,278 --> 00:46:25,528
fell off the table, that was just

822
00:46:25,614 --> 00:46:28,568
compatible with gravity. So it was

823
00:46:28,654 --> 00:46:30,504
finding its optimal position and

824
00:46:30,542 --> 00:46:33,550
fragmentation where you placed it.

825
00:46:34,720 --> 00:46:37,564
The ball rolled downhill as a

826
00:46:37,602 --> 00:46:39,644
consequence of where it was placed and

827
00:46:39,682 --> 00:46:41,310
the slope it was on,

828
00:46:42,080 --> 00:46:45,360
and framing behavior

829
00:46:46,020 --> 00:46:49,804
in that light, we're modeling

830
00:46:49,852 --> 00:46:52,048
behavior as rolling to the bottom of a

831
00:46:52,054 --> 00:46:55,916
bowl. What's the bowl? What's the ball?

832
00:46:55,948 --> 00:46:59,108
Those are the questions. But it's like

833
00:46:59,194 --> 00:47:02,436
chemical reactions proceeding under

834
00:47:02,538 --> 00:47:05,956
Gibbs free energy minimization when we

835
00:47:05,978 --> 00:47:09,464
have policy selection driven by

836
00:47:09,662 --> 00:47:11,864
variational and expected free energy

837
00:47:11,982 --> 00:47:12,920
minimization.

838
00:47:15,180 --> 00:47:17,784
But how can things that go wrong be

839
00:47:17,822 --> 00:47:18,680
optimal?

840
00:47:20,620 --> 00:47:23,532
Well, it has to do with different

841
00:47:23,586 --> 00:47:26,364
parameters in the model, given how they

842
00:47:26,402 --> 00:47:29,900
are. The model performs optimally.

843
00:47:30,880 --> 00:47:32,456
That doesn't mean computationally

844
00:47:32,488 --> 00:47:35,200
efficiently. It doesn't mean adequately,

845
00:47:36,980 --> 00:47:41,104
doesn't mean it satisfies, but it

846
00:47:41,142 --> 00:47:49,904
may be framed as Bayes optimal fixed

847
00:47:50,032 --> 00:47:53,716
and learned behaviors. This is

848
00:47:53,738 --> 00:47:55,984
going to come into play in chapter

849
00:47:56,032 --> 00:48:01,592
seven. There's an amazing continuity in

850
00:48:01,646 --> 00:48:05,144
theory and in simple examples with

851
00:48:05,182 --> 00:48:07,784
perception and learning, which is to say

852
00:48:07,822 --> 00:48:09,524
that perception happens over faster

853
00:48:09,572 --> 00:48:11,456
timescales or perceptive like processes

854
00:48:11,508 --> 00:48:13,340
happen over faster timescales while

855
00:48:13,410 --> 00:48:15,740
learning like processes happen over

856
00:48:15,810 --> 00:48:19,420
slower timescales. Or even just more

857
00:48:19,490 --> 00:48:22,812
generally, perception and learning refer

858
00:48:22,866 --> 00:48:25,020
to parametric update processes,

859
00:48:25,520 --> 00:48:27,516
and you might even be able to situate

860
00:48:27,548 --> 00:48:29,904
the same example both ways. Like if we

861
00:48:29,942 --> 00:48:32,210
see a ball move across our visual field,

862
00:48:33,060 --> 00:48:35,008
are we perceiving the location of the

863
00:48:35,014 --> 00:48:37,268
ball? Are we inferring the location of

864
00:48:37,274 --> 00:48:39,590
the ball or are we updating and learning

865
00:48:40,200 --> 00:48:42,916
our position of the ball? For those with

866
00:48:42,938 --> 00:48:45,270
a computational background, learning

867
00:48:46,040 --> 00:48:48,820
often equates to parameter updating.

868
00:48:49,340 --> 00:48:51,704
And so in that sense any kind of

869
00:48:51,742 --> 00:48:54,410
dynamical perception is learning.

870
00:48:55,660 --> 00:48:59,640
So for some backgrounds the difference

871
00:48:59,710 --> 00:49:03,224
between perception and learning will

872
00:49:03,262 --> 00:49:06,168
be seamless. For other backgrounds,

873
00:49:06,344 --> 00:49:07,816
those are going to sound like totally

874
00:49:07,848 --> 00:49:09,016
different processes. I mean, isn't

875
00:49:09,048 --> 00:49:10,316
perception when you see the book? But

876
00:49:10,338 --> 00:49:12,956
then learning is like when you

877
00:49:12,978 --> 00:49:18,896
understand something about the book and

878
00:49:18,918 --> 00:49:21,628
then speaking from a more modeling

879
00:49:21,724 --> 00:49:26,364
perspective, here a fixed model or

880
00:49:26,422 --> 00:49:30,176
fixing a parameter of a model reduces

881
00:49:30,288 --> 00:49:32,708
the model's complexity immensely. Or

882
00:49:32,794 --> 00:49:35,664
another way to say it taking a fixed

883
00:49:35,712 --> 00:49:38,500
parameter and making it learnable

884
00:49:39,260 --> 00:49:42,760
introduces multiple hyperparameters.

885
00:49:44,380 --> 00:49:47,464
And there's no single way to address

886
00:49:47,582 --> 00:49:50,424
parameter learning. You could say,

887
00:49:50,462 --> 00:49:52,324
well, we're doing moving average

888
00:49:52,372 --> 00:49:55,276
learning like a column filter or just a

889
00:49:55,298 --> 00:49:57,356
simple sliding window. We're taking the

890
00:49:57,378 --> 00:49:59,756
average of the last three. But now you

891
00:49:59,778 --> 00:50:02,030
have to parameter sweep across how many.

892
00:50:02,400 --> 00:50:05,170
And with a single trial of data,

893
00:50:09,220 --> 00:50:12,368
it may not be clear which learning

894
00:50:12,454 --> 00:50:14,832
strategy is implemented because

895
00:50:14,886 --> 00:50:16,536
especially if we want to differentiate

896
00:50:16,588 --> 00:50:20,004
learning strategies empirically, we

897
00:50:20,042 --> 00:50:22,340
would want to see like multiple

898
00:50:22,920 --> 00:50:26,836
comparable trajectories within and

899
00:50:26,858 --> 00:50:30,664
amongst individuals in similar and

900
00:50:30,702 --> 00:50:34,936
different contexts. But now our

901
00:50:34,958 --> 00:50:38,536
lab has gotten quite big and we're doing

902
00:50:38,638 --> 00:50:43,180
quite large model selections even

903
00:50:43,250 --> 00:50:47,256
for learning and updating

904
00:50:47,448 --> 00:50:48,892
parameters that might just

905
00:50:48,946 --> 00:50:51,772
conversationally seem really simple like

906
00:50:51,826 --> 00:50:53,580
preferences being updated.

907
00:50:58,510 --> 00:51:02,186
So here's this description of when

908
00:51:02,208 --> 00:51:05,594
it comes to inference and perception as

909
00:51:05,632 --> 00:51:07,514
being fast changes and learning as

910
00:51:07,552 --> 00:51:10,094
slower changes, though they're being

911
00:51:10,132 --> 00:51:12,880
modeled in a really analogous way.

912
00:51:14,850 --> 00:51:16,734
It is worth noting in this book we

913
00:51:16,772 --> 00:51:18,506
exemplify rather simple generative

914
00:51:18,538 --> 00:51:20,262
models that are defined using tabular

915
00:51:20,346 --> 00:51:23,106
methods, e. G with explicit matrices or

916
00:51:23,128 --> 00:51:26,082
tensors for priors and likelihoods in

917
00:51:26,136 --> 00:51:30,398
small state spaces. In comparison,

918
00:51:30,494 --> 00:51:33,574
more sophisticated GMs are being

919
00:51:33,612 --> 00:51:35,506
developed in machine learning, deep

920
00:51:35,538 --> 00:51:37,830
learning, robotics, et cetera.

921
00:51:40,950 --> 00:51:43,220
So is active inference deep learning

922
00:51:49,490 --> 00:51:52,474
okay, good fellow generative,

923
00:51:52,522 --> 00:51:54,190
adversarial networks,

924
00:51:56,130 --> 00:52:00,126
but with Bengio and

925
00:52:00,148 --> 00:52:04,274
Mirza. But are

926
00:52:04,312 --> 00:52:06,446
we saying those are formally active

927
00:52:06,478 --> 00:52:10,514
inference systems or

928
00:52:10,632 --> 00:52:15,686
are we just saying that we can think

929
00:52:15,708 --> 00:52:17,990
of it like birdsong conceptually?

930
00:52:21,770 --> 00:52:25,014
And then are we just adding a wrapper or

931
00:52:25,052 --> 00:52:28,486
a descriptor to the gans that our non

932
00:52:28,518 --> 00:52:31,946
active colleagues are building is it

933
00:52:31,968 --> 00:52:34,182
shining any new epistemic or Pragmatic

934
00:52:34,246 --> 00:52:35,450
light on Gans?

935
00:52:40,550 --> 00:52:43,090
I mean, here's a whole paragraph on Gans

936
00:52:56,410 --> 00:52:59,080
behind. Oh, that's chapter ten.

937
00:53:00,030 --> 00:53:03,066
What happened? But there's still a whole

938
00:53:03,088 --> 00:53:04,570
paragraph on Gantz.

939
00:53:09,420 --> 00:53:10,730
Did you see that?

940
00:53:25,680 --> 00:53:27,070
How did that happen?

941
00:53:31,190 --> 00:53:32,050
Whoa.

942
00:53:37,620 --> 00:53:40,096
I'm not sure what the optimal way to

943
00:53:40,118 --> 00:53:43,504
view a PDF is or that one exists,

944
00:53:43,632 --> 00:53:47,008
but I don't think it's the browser.

945
00:53:47,184 --> 00:53:50,820
It's not bad, but it's not optimal.

946
00:53:54,200 --> 00:53:58,264
I'm interested in, again the

947
00:53:58,302 --> 00:54:00,904
generative model. Oh, I know what those

948
00:54:00,942 --> 00:54:04,010
are. So, generative model and how

949
00:54:04,460 --> 00:54:07,316
specifically Gans here an active

950
00:54:07,348 --> 00:54:11,230
inference. Is the

951
00:54:11,600 --> 00:54:14,652
rectangle a square? Sorry, square

952
00:54:14,706 --> 00:54:16,236
rectangle rectangle not a square here.

953
00:54:16,258 --> 00:54:19,570
Is that what is being said or is there

954
00:54:25,060 --> 00:54:28,636
an indexed blanket here of varying

955
00:54:28,668 --> 00:54:31,170
degree? Yes,

956
00:54:37,160 --> 00:54:39,780
as far as I know. And and also you

957
00:54:39,930 --> 00:54:43,616
fellows might know like this

958
00:54:43,658 --> 00:54:46,436
is being pointed to variational auto

959
00:54:46,468 --> 00:54:49,604
encoders and variational Bayesian

960
00:54:49,652 --> 00:54:55,080
inference are directly implicated as

961
00:54:55,150 --> 00:54:57,016
potentially recursive cortical networks

962
00:54:57,048 --> 00:54:59,340
are and world models.

963
00:55:00,720 --> 00:55:05,100
But if these bridges can be made solid,

964
00:55:06,480 --> 00:55:07,756
like you said, with squares and

965
00:55:07,778 --> 00:55:10,640
rectangles so that we can say all some

966
00:55:10,710 --> 00:55:14,290
or none of X-R-Y

967
00:55:14,820 --> 00:55:17,696
or here's how you project this model to

968
00:55:17,718 --> 00:55:23,300
that model. It's going to just break

969
00:55:23,370 --> 00:55:28,768
a dam with possibly

970
00:55:28,864 --> 00:55:30,980
certain applications of active.

971
00:55:31,880 --> 00:55:36,136
Any final comments on six, and I

972
00:55:36,158 --> 00:55:39,450
think certainly by two weeks from now,

973
00:55:41,340 --> 00:55:46,920
let's look at the SPM implementation

974
00:55:47,820 --> 00:55:49,884
and look at some of those that are

975
00:55:49,922 --> 00:55:51,564
pointed towards in the book, as well as

976
00:55:51,602 --> 00:55:54,670
some of like the epistemic chaining and

977
00:55:55,120 --> 00:55:58,012
Pi MDP type models, as well as anything

978
00:55:58,066 --> 00:56:00,512
that anyone else builds. So, any final

979
00:56:00,566 --> 00:56:04,032
comments? Yeah,

980
00:56:04,166 --> 00:56:07,452
I just wanted to point out the recent

981
00:56:07,596 --> 00:56:12,656
work by Fields at all, especially the

982
00:56:12,678 --> 00:56:16,704
two papers that first and was also

983
00:56:16,902 --> 00:56:20,804
the co author of. One of their

984
00:56:20,842 --> 00:56:23,956
main objectives is to cross that bridge

985
00:56:23,988 --> 00:56:27,336
between active inference and VAE via the

986
00:56:27,358 --> 00:56:30,964
CCCD system. So that bridge

987
00:56:31,012 --> 00:56:33,592
they're talking about in this book is

988
00:56:33,646 --> 00:56:37,512
already being investigated to be filled.

989
00:56:37,576 --> 00:56:40,844
So yeah, they're pretty interesting

990
00:56:40,962 --> 00:56:43,950
developments going in that regard.

991
00:56:46,480 --> 00:56:48,556
Okay, any last notes before I stop the

992
00:56:48,578 --> 00:56:51,250
recording? What is that paper?

993
00:56:53,380 --> 00:56:55,564
I'll send you the link from the discord.

994
00:56:55,612 --> 00:56:59,056
I forgot. Maybe just one.

995
00:56:59,238 --> 00:57:03,396
I don't know. We haven't trying

996
00:57:03,418 --> 00:57:07,684
to make an effort on the code to lay

997
00:57:07,722 --> 00:57:11,536
down some basic examples

998
00:57:11,568 --> 00:57:15,064
from the first half, but should

999
00:57:15,102 --> 00:57:16,330
there be some?

1000
00:57:19,180 --> 00:57:21,770
If we're going to model stuff, should we

1001
00:57:22,220 --> 00:57:25,272
have a sub page for that or do put

1002
00:57:25,326 --> 00:57:27,196
something somewhere in the code of for

1003
00:57:27,218 --> 00:57:29,212
that? I don't know. We've had this code

1004
00:57:29,266 --> 00:57:30,700
section from the beginning.

1005
00:57:32,800 --> 00:57:35,144
However, it should be totally

1006
00:57:35,192 --> 00:57:38,236
modifiable, full control.

1007
00:57:38,338 --> 00:57:41,624
So however this and anything sub pages

1008
00:57:41,672 --> 00:57:44,990
need to be modified and the GitHub link

1009
00:57:45,760 --> 00:57:48,140
to a textbook,

1010
00:57:49,520 --> 00:57:52,196
anything that people suggests, just let

1011
00:57:52,218 --> 00:57:53,030
us know.

1012
00:58:00,870 --> 00:58:02,020
Anything else.



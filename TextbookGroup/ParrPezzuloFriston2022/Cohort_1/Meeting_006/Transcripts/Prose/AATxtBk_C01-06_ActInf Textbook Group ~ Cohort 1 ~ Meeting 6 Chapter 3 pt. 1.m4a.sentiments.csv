start	end	speaker	sentiment	confidence	text
1770	2510	A	0.5148600339889526	Hello, everyone.
2660	13920	A	0.9008375406265259	It is June 9, 2022, and it's chapter three, week one, and it's the first cohort of the textbook group.
14770	16720	A	0.8358489274978638	We're going to go to the questions.
17490	26486	A	0.808504581451416	There weren't any ideas added for three, but if people have any key themes or any ideas that they picked up on with three, they can add them here.
26668	29080	A	0.6911749839782715	Otherwise we'll go to the questions.
30090	41322	A	0.7638766765594482	And also if people have questions in the chat, or it would be also great just to take notes or add questions and continue to upload them.
41456	44394	A	0.8735839128494263	And then next week we'll continue the discussion on three.
44512	53822	A	0.8868803381919861	So this is just the opening set of questions, maybe just the first ideas of the book.
53956	67250	A	0.9166600108146667	Turning to questions, now that we've at least gone through chapter two once and part of chapter three, or the whole thing of chapter three, what is the high road to active inference?
78740	90950	A	0.8756009340286255	What does anyone think about the high road to active inference or about this low road high road construct that is used in chapters two and three?
94220	100990	A	0.8731379508972168	Just broadly, how did the approach of chapter three feel different than the approach or the topics of chapter two?
112160	113900	A	0.7424266338348389	JF and then Ali.
115620	116416	B	0.5112842917442322	I just want.
116438	119296	A	0.8065935969352722	To make the Joe comment that the.
119318	121916	B	0.8407534956932068	High road approach is what first grabbed.
121948	124572	A	0.8006997108459473	Me when I encountered the free energy principle.
124716	127236	B	0.8911364674568176	That what really fascinated me, that there.
127258	131556	A	0.7991873621940613	Was an attempt at a unifying answer.
131658	136820	B	0.8215555548667908	To self organization, cognition.
140380	141224	A	0.6577287912368774	That this could.
141262	144760	B	0.49917298555374146	All be brought together around some relatively.
145740	148004	A	0.663157045841217	Sparse number of concepts.
148132	151130	A	0.6204549670219421	That's what grabbed me much more than the lower road.
153280	156300	A	0.7691379189491272	Thank you, Ali and then Ben?
159040	176716	C	0.6393427848815918	Well, yeah, reading chapter three actually clarified my previously held, in fact wrong belief that previously I thought that a low road was a kind of bottom off approach and the high road is top down approach.
176748	191192	C	0.6923407316207886	But now I see that low road was basically tackling the problem of active inference from the inferential point of view and from the statistical and inferential point of view.
191246	215790	C	0.7469953298568726	But in the high road we have a much more generalized and much more, let's say, all encompassing view to tackle the problem of active inference, namely by way of defining markup blankets and all the properties of living organisms and so on.
216800	217308	A	0.9184247851371765	Awesome.
217394	219132	A	0.8840214014053345	Thank you, Ben.
219196	221312	A	0.7509777545928955	And then anyone else who raises their hand.
221366	225388	A	0.8376691341400146	And also you can take notes on this idea of the book, like high road generally.
225484	227280	A	0.8094848990440369	Ben and then Mike.
227780	235044	D	0.7972701191902161	Yeah, I think for me as well, this chapter three and the high road has been an easier road in.
235082	242756	D	0.827575147151947	And the thing that I noticed about it was, it seems to me, so they say in the introduction, that active inference is a normative theory.
242868	248264	D	0.6575442552566528	I feel like the high road, to me, feels there's much more normativity in the high road.
248382	251480	D	0.8368295431137085	It's an account of what organisms have to do.
251550	259132	D	0.8947097659111023	And I think compared to the previous chapter, I think chapter two felt a lot more descriptive for me.
259186	260732	D	0.5954681038856506	I'm not sure other people would.
260786	269200	D	0.9696826338768005	Maybe that's just the way that I was reading it, but it felt like chapter three was a lot more kind of it had a more normative force to it that I really enjoyed.
270420	271840	A	0.8904486894607544	Thank you, Mike.
281430	282434	E	0.5760965347290039	There we go.
282552	285970	E	0.9190576672554016	Sorry, my computer was not responding.
286630	295910	E	0.8528866171836853	So I went through a similar experience as Ollie described in terms of going into it, thinking about bottoms up and top down and coming out of it, thinking about it differently.
297710	301862	E	0.6921812295913696	I feel like in chapter three there's still a lot to unpack around the Markov blanket.
301926	307500	E	0.8608044385910034	So that is still capturing this inferential nature of things.
308190	312640	E	0.7064105272293091	It still has a Bayesian quality like what we saw in chapter two.
313890	324930	E	0.5680950880050659	But it also, for me at least, it still feels a bit opaque at this point in terms of this sort of indirect influence that's taking place across the Markov blanket.
326950	327700	A	0.9184247851371765	Awesome.
328070	331262	A	0.8131527900695801	Okay, any other comments?
331326	335430	A	0.8972923159599304	Otherwise we're going to go to the questions and we'll approach many of those topics.
340340	356378	A	0.8468940854072571	Okay, first question regarding the Markov blanket explanation.
356474	365010	A	0.64053875207901	On page 43 and box 3.1, they write, no additional information about the future is gained by finding out about the past.
365080	367918	A	0.6728379130363464	Assuming we know the present, how can this be true?
368024	382950	A	0.8807223439216614	That's one question they define regarding the Markov blanket as the set of variables that mediate all statistical interactions between a system and its environment.
383110	386570	A	0.6962822675704956	What is a statistical rather than a non statistical interaction?
390110	399738	A	0.8546344637870789	They write that they've supplemented conditional independencies with dynamical constraints so that the flows do not depend upon states on the opposite side of the blanket.
399914	404640	A	0.4989289939403534	Why does independence between the flows on the two sides of the blanket matter so much?
405910	407380	A	0.8581835031509399	All right, great.
407990	412146	A	0.9115357398986816	So who would like to give a thought on this?
412168	413010	A	0.7309772372245789	First question?
413160	421000	A	0.5401113033294678	How can it be true about Marco blankets that no additional information about the future is gained by finding out about the past?
432790	434610	E	0.8339806199073792	So I added a note in there.
434680	447146	E	0.8113123774528503	You can tell me if this is correct or not with regard to Markov chains that as I understand the Markov chain in its present state is effectively capturing all the information about the past.
447248	451770	E	0.8991370797157288	So I assume that translates into the Markov blanket.
452670	453420	A	0.46103888750076294	Yes.
453950	454966	A	0.8622214794158936	Great comment.
454998	460746	A	0.8067954182624817	And a Markov chain is used very broadly.
460938	473266	A	0.483398973941803	And just to give one example, it's like the current state of the chessboard, knowing more about previous moves doesn't tell you more about the future.
473368	479922	A	0.7919195294380188	And so it's like it's a one Markov chain because the dependency is only about that one time step.
480056	499190	A	0.8281277418136597	Now if it were to violate that condition of being a one Markov blanket through time and have like a three Markov blanket, that would mean that the same board position, depending on what move happened three before, was still having some influence on how the system evolved.
499350	505930	A	0.7643827199935913	That wouldn't violate the Markovian property, it would just make it a different kind of Markov chain.
506430	516350	A	0.8766918778419495	And that is commonly used in the temporal modeling, like in time series modeling and in transitions of stochastic processes.
516850	519754	A	0.8092052936553955	The Markov blanket is abstracting.
519802	524002	A	0.8449257612228394	It from this time sequence idea.
524136	527218	A	0.739433228969574	Like the now is a blanket between the past and the future.
527384	540390	A	0.8756126165390015	And we could think about spatial systems like a Newton's cradle or something like that, as well as spatial temporal systems or just abstract causal systems.
540890	542390	A	0.5547290444374084	So how can this be true?
542460	546840	A	0.7132219076156616	It's true by definition because that's what the definition of a Markov blanket is.
547230	561354	A	0.8450337052345276	Whether that description is adequate or maps on to carving, nature of the joints and all these other things are respectively statistical model comparison and ontological philosophical questions.
561472	564486	A	0.8181920647621155	But it's true by definition about Markov blankets.
564598	569438	B	0.7615309953689575	Lyle yeah, I think actually you just pretty much covered it.
569444	577982	B	0.7212182879447937	But I would just say this is the same condition if you're building RL models of different kinds of autonomous systems, this is the same restriction.
578126	588674	B	0.7943337559700012	I think your comment about the nested blankets is interesting because in the RL space we typically don't have that kind of richness.
588802	593990	B	0.8446250557899475	But what we do then would be, for example, we want to know the distance.
595770	609106	B	0.8715932369232178	And in that case we would just create, basically create a state variable, which is the history of the state transition.
609158	610318	B	0.4938085377216339	So we fudge it a little bit.
610404	622320	B	0.7266693711280823	But your explanation that in this conception with nested Markov blankets, then that would be the more comprehensive solution to that issue.
623570	623994	A	0.6283750534057617	Thanks.
624052	639462	A	0.9063212871551514	Just to kind of clarify, there like one hack or workaround would be like you make the list of all of the last possible three moves and then you make a one Markov blanket with all the last combinations of three.
639596	640038	B	0.5689783096313477	That's right.
640044	648362	A	0.8651451468467712	So it still uses the machinery of a one Markov chain, which has the simplest transition matrix, but it could encompass the last states.
648416	652326	A	0.7328301668167114	But it sort of compresses them into a one Markov chain format.
652518	662234	B	0.8190391659736633	Yeah, I mean, very often you want to know the average of the last 20 time steps.
662362	665422	B	0.6967495679855347	You're just trying to smooth something, right.
665556	668894	B	0.8652825355529785	And it's just way too much work to build all that machinery in there.
668932	672646	B	0.7022267580032349	So you make that average.
672698	675380	B	0.8666431307792664	Actually a present variable is what you do.
676310	677554	A	0.8606451749801636	Great example.
677672	693682	A	0.858681321144104	If something really does depend on the last 20 time steps, or you want to do model comparison as to whether it's a better model that is conditioned on the last 15 or 20 or 30, you can just condense it with summary statistics and descriptions.
693826	696494	A	0.8802072405815125	Again into this one Markov framework.
696642	701926	A	0.821498453617096	Otherwise the combinatorics of how the 20 influence each other is vast.
702118	707994	F	0.8674929141998291	Brock I guess that's related to what my comment was just going to be.
708032	710320	F	0.682769238948822	That the way they worded it.
711090	728930	F	0.7445868253707886	If you're not rigidly thinking about a singular Markov blanket, just practically speaking, finding about the past usually does give you information about the future.
729000	732766	F	0.8893713355064392	If you're talking about outside of a singular Markov blanket.
732878	733540	A	0.5664746165275574	Right.
736150	746502	F	0.6775632500648499	Adding the words no additional information about the future of the Markov blanket is gained by finding out about the past.
746556	749578	F	0.8853782415390015	The Markov blanket kind of balance that.
749664	759100	F	0.813951313495636	But in the case of the machine learning example and comparing models, you're necessarily talking about a much larger markup blanket than just a singular one there, right?
760270	762080	F	0.5849931836128235	Yes, same thing.
762930	763438	A	0.46103888750076294	Yes.
763524	775018	A	0.8045293092727661	And just like many concepts have like a broader conversational, informal sense in a narrow, more technical sense, assuming we know the present means fully knowing the present, fully knowing the blanket.
775114	777954	A	0.7250133752822876	So it's like but I could still learn things about the past.
778072	783010	A	0.7752624750137329	So that's not to say that there isn't novel information that you could discover about the past.
783160	786578	A	0.7943210601806641	Like you could still go back in that chess game and learn information.
786744	789446	A	0.7384368181228638	So it's not that there isn't information in the past.
789628	791974	A	0.5330665111541748	It's not that it wouldn't even be interesting.
792172	808170	A	0.8926664590835571	It's that for the purposes of how the present goes into the next time step the present blanket in this temporal Markov chain, which is like a blanket through time, it contains the information you need for the transition frequencies.
810290	811040	A	0.584351658821106	Okay.
811650	815310	A	0.842260479927063	They define that it's the set of variables that mediate.
815890	831854	A	0.8075218200683594	And also the Markov blanket using last names, personal opinion is not helpful because not only are there multiple Markovs, but the Markovian property is not something that's technical or defined.
831982	840950	A	0.6424863338470459	So maybe someday we'll have better ways to describe and be more specific that don't involve invoking ambiguity around names.
841690	855030	A	0.861719012260437	That being said, it was worked out analytically by Markov and sun and others, but most recently it was Pearl 1988 and causal inference with Bayesian perspective.
855110	858214	A	0.6946030259132385	So this is not like an active inference, ism it's?
858262	868458	A	0.8112536072731018	Drawing upon a total Bayesian statistical framework, any Bayes graph that's not fully connected is going to have some nodes that intermediate.
868554	870462	A	0.8419959545135498	So that's going to relate to this question.
870596	874610	A	0.7199302315711975	So what is a statistical rather than a non statistical interaction?
877190	880500	A	0.8847116827964783	Ali and then anyone else who would like to address that?
882710	889234	C	0.8583908081054688	Well, I think it has something to do with the way these interactions are parameterized and modeled.
889282	903590	C	0.7833236455917358	Because, you see, every phenomena can be parameterized and modeled from many various perspectives and frameworks within various frameworks.
904510	918426	C	0.8139160871505737	And the reason I think they put the term statistical in parentheses is that they mean that these variables mediate all interactions that have been parameterized statistically.
918618	937030	C	0.8633017539978027	So I think it refers to the nature of parameterizing and modeling these interactions as opposed to distinguishing between statistical and non statistical interactions.
942610	947920	A	0.838474452495575	All right, so anyone else can raise their hand, but just here's a few thoughts on that.
948850	954450	A	0.8674103021621704	A statistical interaction is the edge in a causal Bayes graph.
955030	965910	A	0.8540405035018921	And people might be more familiar or have seen like structural equation modeling, where nodes are variables and edges are the correlation between or among variables.
966970	970070	A	0.7206780910491943	Conditioning on the structure of the graph.
971130	977750	A	0.8527659177780151	In the causal Bayes graph, the nodes are variables and the edges are statistical causal influences.
977910	985130	A	0.650809645652771	So causality and statistics is not always the same as what people mean by cause in the real world.
985280	991120	A	0.7959820628166199	But like granger causality and just this notion of like coarse graining and cause.
992850	1003138	A	0.8909527659416199	It may line up with the sort of narrative natural language description that somebody has between when they're speaking.
1003224	1005586	A	0.7668353319168091	It might be narrower than that, it might be a different thing than that.
1005608	1007380	A	0.852415919303894	It might be more general than that.
1009830	1012980	A	0.85603266954422	What would a non statistical interaction be?
1015290	1017638	A	0.8404644131660461	There's various ways to approach that.
1017724	1022994	A	0.8187767267227173	It's kind of pointing towards some sort of like a touch could be an interaction.
1023122	1029530	A	0.7729867100715637	But what if things are touching but those variables don't change as a function of their touching?
1030270	1033020	A	0.8013522028923035	Then do those things interact or not?
1033630	1034938	A	0.718879759311676	That's one question.
1035104	1043710	A	0.7356500625610352	And then cause there's like just tremendous real world and philosophy questions like what causes what, what's the difference making cause?
1043780	1045520	A	0.8477197885513306	What's the cause that makes a difference?
1046450	1048346	A	0.6552892923355103	Necessity and sufficiency.
1048538	1055054	A	0.7692239284515381	There's so many questions in this like causal philosophy and this is highlighting.
1055182	1057026	A	0.8382012248039246	We're talking about the model.
1057208	1064158	A	0.9071125984191895	We made a Bayes graph with height, weight and shirt color and here are the statistical effectors.
1064334	1068278	A	0.6816504001617432	And so we're not saying that there's no interaction between this and that in the real world.
1068364	1072450	A	0.879045307636261	Now we're within the model and we're talking about the causal graph.
1072610	1088498	A	0.6136768460273743	And again, unless the Bayes graph is fully connected, there's going to be some partitioning scheme that will result in some set of nodes being conditionally independent from another set of nodes when conditioned upon a blanket.
1088614	1096686	A	0.5559490323066711	So it's not like features of the real world or even variables in a statistical equation can just be unilaterally tagged as blanket states.
1096868	1104030	A	0.7227654457092285	It's always a partition that coinstantiates two sides, internal and external.
1104110	1106958	A	0.8029478192329407	But there's a symmetry and the blanket.
1107134	1114310	A	0.5682387948036194	So it's not like features of the world or even features of the model are intrinsically, internal, external or blanket states.
1114460	1124866	A	0.8726868629455566	But that's a partitioning scheme that can be applied to Bayes graphs of like a vast various structures of Bayes graphs.
1124978	1130010	A	0.7870437502861023	And Bayes graphs can be applied to a vast number of real world situations.
1131070	1141194	A	0.8686593770980835	And many, many papers and live streams discuss sort of like all encompassing Markovian monism Markov blankets all the way down nested Markov blankets.
1141242	1156740	A	0.5333189368247986	Are the world ranging to the critical perspectives, not necessarily detractors, but just those who believe that the usage of the concept is out of alignment with ways that people have used it.
1157430	1159220	A	0.717284083366394	Mike and then anyone else.
1161910	1169714	E	0.8598147630691528	Does it make sense to think of a Markov blanket in the context of an internal agent state and an external environment?
1169842	1183370	E	0.9162368178367615	Is the Markov blanket serving as this kind of a decoupling component because of the way that things are conditionally independent based on the variables inside the blanket?
1191090	1192400	A	0.7630743980407715	What do people think?
1196210	1196960	A	0.46103888750076294	Yes.
1200130	1225190	F	0.7716568112373352	I mean, for the internal state to be different than the external state or have some other equilibrium other non equilibrium state that the external environment is in, you would have to necessarily have I'm not sure if decoupling is the right word, but some sort of conditional, dependent and independent sort of zone that separates those two regions.
1226330	1227894	A	0.8672176003456116	So here's figure 31.
1227932	1228950	A	0.9182145595550537	Thank you, Brock.
1229290	1251034	A	0.8944528102874756	The active in the sensory states compose the blanket now in various live streams and papers, the history of the concept again from the analytical pre computational phrasing of insulation of fluctuation of random variables to Pearl's Bayesian and computational framing.
1251162	1257986	A	0.6873438954353333	However, Pearl's Markov blanket concept doesn't have a delineation of active and sensory states.
1258168	1262846	A	0.6622655987739563	It's only like there's one kind of fabric in that blanket.
1262958	1264740	A	0.5351047515869141	It's just blanket states.
1265290	1272182	A	0.9218819737434387	Friston maybe this also will be shown to be traced to other places and ways.
1272236	1291520	A	0.8842829465866089	But one of the core things that Friston at all have brought into the picture was interpreting within the blanket states the states that have incoming dependencies to the system of interest as sensory and then the states that have outgoing dependencies from the system of interest as active states.
1292050	1300266	A	0.8341226577758789	So people may already see challenges and complexities that arise with the arrows and the directions.
1300458	1306820	A	0.8921559453010559	And a lot of that is addressed in the how particular is the free energy principle of Aguilera et al.
1309190	1315250	A	0.9436790943145752	We're going to continue talking about this entity partitioning Ali and then Lyle.
1317770	1341610	C	0.5086190700531006	Well, I think we should point to a very important typographical error in this picture here because in this figure, active states are expressed in terms of external states and markup blankets and sensory states are expressed in terms of internal states and markup blankets.
1342030	1343670	C	0.7881929278373718	But it should be vice versa.
1343750	1355906	C	0.8625344634056091	I mean, the flows of internal and active states are independent of external states and the flows of external and sensory states do not depend on internal states.
1356088	1362370	C	0.8972618579864502	So we should change x and mu in those two equations.
1363750	1374454	A	0.759590208530426	Yes, sometimes with a possible typo like that, it's so Egregious blatant that it's challenging to even know.
1374492	1377794	A	0.8440893292427063	But we can absolutely ask the authors.
1377842	1381980	A	0.8521183133125305	But just to read the equation and think about why it is that way.
1382590	1385366	A	0.8062548041343689	The dot is the rate of change of a variable.
1385478	1396414	A	0.8994482755661011	So the internal states are mu and so this is the rate of change of internal states, rate of change of external states x and the rate of change of u, the active states and y the sensory states.
1396532	1402894	A	0.9170460104942322	So we're looking at how these partitioned states in a Bayes graph change through time.
1403092	1408340	A	0.7777180075645447	The states that are the blanket and the internal are the particular states.
1408710	1414862	A	0.9038393497467041	Those are referring to the particle that's like figure versus ground, the particle that's moving around the curious particle.
1414926	1424550	A	0.749897837638855	The active entity is the particular states that is partitioning the thing, literally the thing away from the niche.
1424970	1428626	A	0.6962860822677612	Then there's the blanket states that are intermediating that interface.
1428738	1434346	A	0.8802008628845215	And then we'll also talk about autonomous states which are just the internal and the active states.
1434528	1445040	A	0.8478259444236755	So the equation is saying the rate of change in, for example, active states is a flow function F and a noise function.
1445490	1450862	A	0.8148477077484131	Both the flow and the noise function the subscript is referring to like that one.
1450916	1471750	A	0.8408368229866028	It's kind of like IPSO in Latin like, it's the flow on Mu and the noise on Mu, the flow, which is what the principle of least action converges us towards in the limit, when the statistical fluctuations from the Omega are low, is a function of certain variables.
1472330	1484620	A	0.8433895707130432	And so this is saying the flow of active states is a function of external sensory and active states.
1485310	1499582	A	0.8540114760398865	However, as Ali has pointed to, the arguments that should be guiding the flow of active states are actually the blanket states and internal states, not external states.
1499636	1510500	A	0.8266459703445435	And then analogously sensory states should have a flow that's defined in terms of blanket states and external states, not internal states.
1512390	1516834	A	0.8516884446144104	Lyle and then Ali yeah.
1516872	1526118	B	0.6899336576461792	So actually I think you're getting right right at the question I had because I was confused about the bi directional arrows, particularly between internal states and active states.
1526204	1526840	A	0.7360090017318726	Right?
1528410	1531814	B	0.6014065742492676	And I think you're drilling right into that.
1531852	1535640	B	0.8552851676940918	I still don't completely understand what that picture should look like.
1536410	1544362	B	0.565066933631897	But I have trouble conceptually I sort of get a conceptually, I feel like I get the Markov blanket, but that representation.
1544426	1546880	B	0.7668852210044861	I have trouble working through the details on that.
1548130	1548590	A	0.84200119972229	Cool.
1548660	1551134	A	0.9036953449249268	So, guest stream number seven.
1551332	1554690	A	0.8958229422569275	How particular is the physics of the free energy principle?
1555270	1562770	A	0.8406808972358704	It explores different entity niche causal relationships.
1563430	1569174	A	0.831314206123352	Like, for example, we could imagine a simple around the clock entity model.
1569372	1571702	A	0.7596905827522278	External states induce sensory states.
1571756	1585210	A	0.8476636409759521	One directional arrow, sensory goes into internal, internal to active, active to external, like an UDA loop or something like that, but just an around the clock, no bi directionality, no connection between active and sensory.
1586030	1604594	A	0.5825662612915039	Given this partitioning, one could imagine that topology of cause, one could imagine all kinds of topologies of cause, some that violate the Markov blanket condition, like internal states and external states that are no longer internal and external because they would have a causal link.
1604712	1614020	A	0.8665793538093567	But even preserving the Markov blanket condition, we could imagine different topologies, different sparsities of couplings between these different states.
1614870	1619494	A	0.8112329840660095	A bi directional arrow is kind of like there would be a non zero cause.
1619692	1631660	A	0.8378077745437622	And in the other side of the matrix, it would also be a non zero cause versus a unidirectional relationship where like variable two has a causal influence on three, but three doesn't have it on two.
1632830	1654058	A	0.8574378490447998	In this paper in Guest Room 7.1 and that paper and further line of research, they explore what sparse coupling structures, causal architectures of the entity niche relationship and interface would grant different statistical capacities.
1654234	1666290	A	0.6264762282371521	So I could be wrong on this, but active inference is not a commitment to this specific coupling architecture.
1666450	1686860	A	0.5288329124450684	It may be possible to design or describe or imagine systems that have different causal architectures, maybe even true at this basal most kernel loop, but certainly true when we start thinking about nested structures and cognitive structures and so on.
1687470	1690800	A	0.860560417175293	So yes, it is a little unfortunate about this.
1691490	1716802	A	0.8895591497421265	However, this is just thinking about causal relationships of the particular partition and we're describing statistical relationships here causal influence, as inferred from like, time series data with Granger causality or other methods on specific random variables.
1716946	1730106	A	0.7231267094612122	So this is not the conversational notion of cause, but what I'm thinking is eventually resulting in things happening in the outside world, or aren't things in the outside world eventually changing things?
1730208	1737718	A	0.7899199724197388	Yes, but just like the chessboard through time, the blanket is statistically intermediating.
1737894	1745920	A	0.7178327441215515	And again, that's why it gets so messy with the application of like well, the retina must be the sense states.
1749090	1756926	A	0.7234621047973633	If one can read that example and not fall into the quicksand, it could be didactic.
1757118	1771446	A	0.5065957307815552	But to tag retina as sense state is like the tip of the iceberg of a partially specified model, often one where the measurements have not actually occurred and so on.
1771548	1792160	A	0.5969389081001282	So then that can be extremely difficult for those with less familiarity with a formalism to generalize accurately from, because shouldn't it be like retina sense states and then the arm is the active state, but it's a partitioning that coinstantiates and it's model dependent, it's not describing features of the real world.
1793090	1794030	A	0.7020851373672485	Ollie?
1797830	1811058	C	0.788030743598938	Yeah, I just wanted to .1 of Ramset at All's recent fascinating paper on Bayesian mechanics, which I put the link in the chat.
1811154	1824646	C	0.8022732734680176	And in that paper, especially in section three, they have a very illuminating discussion about these whole business of partitioning and markup blankets.
1824758	1835486	C	0.5221701264381409	And one sentence that's pretty relevant to our discussion here is that the key point to note is that the flow of internal and active components I e.
1835508	1840522	C	0.8294490575790405	Their trajectory through state space does not depend upon external components.
1840586	1848606	C	0.8551889061927795	And reciprocally, the flow of external and sensory states or paths does not depend upon internal states or paths.
1848638	1850098	C	0.8636291027069092	From page 15.
1850264	1859430	C	0.8338959217071533	So yeah, if you refer to that article, it has been elaborated much explicitly.
1860410	1866502	A	0.9089581966400146	Also, in four days we will have Dalton and Maxwell et al.
1866556	1867850	A	0.8435953855514526	For a guest stream.
1868910	1870202	A	0.7931357026100159	The paper just came out.
1870256	1876410	A	0.6889949440956116	Now we'll be able to have a discussion with them and learn and ask them questions.
1876480	1882010	A	0.6036410927772522	So if people have suggestions for guest streams, that can be accomplished.
1882090	1887710	A	0.8737143278121948	If people want to facilitate or participate in different streams, if they have questions for different streams.
1888050	1911510	A	0.9234928488731384	Being there live, but especially getting involved eagerly and actively and early is the biggest leverage point because that allows us to design the material so that it's, like permanently useful rather than potentially ad hoc questions that arise during which can be super important and hopefully people can see some affordances for participation and development.
1912330	1922570	A	0.7664560675621033	Let's just get to this last question and we have other questions, but these are essential and it's one of the opening formalisms of the chapter.
1924590	1930640	A	0.6623672246932983	Just to conclude here though, why does independence between the flows on the two sides of the blanket matter so much?
1942210	1944894	A	0.9619607925415039	These are good things to think about.
1945092	1952420	A	0.7006428837776184	Just one short thought would be without independence between the flows, there is no distinguishing one thing from another.
1953510	1954450	A	0.6077883839607239	Ali.
1957380	1970452	C	0.8614063858985901	And also I think it relates to what we read in chapter two about hidden states because that's what the word hidden probably refers to.
1970586	1976592	C	0.5849989056587219	I mean, internal state does not have direct access to external states and vice versa.
1976656	1986312	C	0.9360095858573914	So that Markup Blanket pretty much formalizes this hiddenness, yes, awesome.
1986446	2002472	A	0.8751673102378845	And partially observable models of the Bayesian type which are used in partially observable Markov decision processes, expectation maximization, any kind of Bayesian priors and hyper priors and so on.
2002626	2009324	A	0.873755156993866	All of these have Bayes graphs that reflect conditionally independent variables.
2009452	2023760	A	0.7558457255363464	So it's not that they're independent in a conversational way, like they don't have any way of communicating, it's that they are specifically involved in a causal network that has conditional independence.
2023920	2027184	A	0.7169685363769531	But we'll hopefully develop more answers and notes here.
2027322	2028410	A	0.6775549054145813	We'll move on.
2029900	2039204	A	0.8744220733642578	What is the differences between and among the terms Bayesian brain hypothesis, predictive processing, predictive coding and active inference?
2039332	2044104	A	0.9195540547370911	Might you be able to suggest references or resources for where these distinctions are delineated?
2044232	2047084	A	0.5233243703842163	Sure, so anyone can add more.
2047122	2050460	A	0.8537313342094421	But here's, I think two key resources here.
2050610	2052888	A	0.891056478023529	The first is live stream 43.
2053074	2063084	A	0.9533300995826721	In 430, Maria gave a really excellent overview of predictive processing and predictive coding from a historical perspective.
2063212	2086490	A	0.8258349299430847	And briefly, she proposed that predictive coding can refer to a unidirectional data encoding scheme of the kind that we see in signal processing, video compression, so on, whereas predictive processing is referring to a bi directional architecture where predictive coding is implemented in this ongoing top down, bottom up way.
2087660	2093016	A	0.8809525370597839	Bayesian brain hypothesis was also touched on and we connected this to active inference.
2093048	2109164	A	0.8747265338897705	But 43.0 is a long technical review paper, but we have three discussions on it predictive coding, theoretical, experimental review, recent paper, very good source for the predictive processing encoding.
2109292	2115192	A	0.8707307577133179	And predictive processing encoding initially was more of about a sensory framework.
2115356	2121808	A	0.7495557069778442	However, at the end of that paper they show, okay, now we're going to do predictive processing on action.
2121904	2123380	A	0.8334850072860718	It's active inference.
2123880	2142744	A	0.8317509889602661	So predictive processing about action is six of one and half dozen of another within a margin of reasonable approximation, similar, but there's also some key differences like reliance on different formalisms.
2142792	2147870	A	0.8406603932380676	But these are all things for us to explore and unpack as we work on the ontology and so on.
2150240	2154400	A	0.8684985637664795	Specifically, predictive coding active inference in the Bayesian brain.
2154820	2170244	A	0.8460954427719116	This is an interview that a departed colleague and I did with Karl Friston in 2018 and we specifically asked the Bayesian brain hypothesis, predictive coding and the free energy principle are often equated with one another.
2170442	2175440	A	0.8394846320152283	You yourself have suggested that the three frameworks are variations of the same basic mechanisms.
2175600	2177990	A	0.6954303979873657	So 2018 was a very different time.
2178620	2185944	A	0.652998685836792	Active inference had not been delineated from FEP in the same way that it is coming to be now.
2186062	2190396	A	0.789115309715271	And there was a lot of other differences between then and now.
2190498	2192670	A	0.8025088310241699	But he gives an extended answer.
2193040	2206210	A	0.7690137624740601	So for people who want to learn about that, his extended answer and Martin's Restatement of the Bayesian brain hypothesis are still some of the best places to find clarity on that issue.
2208100	2211250	A	0.8679495453834534	Does anyone have any other thoughts or questions on this?
2215000	2226532	A	0.8033524751663208	Just one other note, while anyone can raise their hand, is like the Bayesian brain hypothesis sometimes could be used in a very instrumentalist way.
2226666	2230570	A	0.8215706944465637	We're using Bayesian statistics to do neurobehavioral research.
2231100	2240824	A	0.8284624218940735	It might be a realist implementational claim, like the brain is doing Bayesian statistics or something like Bayesian statistics.
2240952	2249820	A	0.8873193264007568	So this hypothesis floats kind of among the layers of Mars analysis, like implementational algorithmic, computational.
2251600	2254210	A	0.5327282547950745	And people do use it to mean different things.
2254580	2262364	A	0.8618239164352417	And in Bayesian graphs, in Bayesian statistics, it can be about perception or it could be about action.
2262492	2277220	A	0.8499906659126282	So Bayesian brain hypothesis, depending on how somebody frames it and what specific models they're talking about, might include, for example, all of these, because these might be applied in a Bayesian framework to talk about the brain.
2278060	2278840	A	0.6653041243553162	Ben.
2281580	2290248	D	0.5321252942085266	Yeah, I think you've covered it really well, but I wish I'd have had access or knowledge of the result, the talk that you just mentioned.
2290334	2297628	D	0.9154107570648193	Because trying to differentiate predictive processing from active inference has been a kind of source of pain and confusion for me over the last few months.
2297714	2314748	D	0.7635258436203003	And I thought another way of putting it, perhaps because I think I know the difference now is essentially some presentations of predictive processing are active inference in the sense that they're highly embodied, inactive, embedded.
2314924	2322112	D	0.8663897514343262	And so if you have a predictive processing that draws on these Fourier accounts of cognition, it tends to just that just is active inference.
2322176	2326208	D	0.7585799694061279	Active inference is kind of necessarily inactive and embodied.
2326304	2333300	D	0.8953282833099365	And there's a paper I should say there's a paper by I believe it's called Wilding the Predictive Brain.
2333460	2355292	D	0.8051028251647949	It's by Andy Clark, Kate Nave, Mark Miller and George Dean, I think, which basically takes predictive processing as a theory of neural processing and weaves in these kind of cognitive, these four E narratives really, really well and kind of fleshes out a much more active inferencey picture.
2355436	2359490	D	0.9631534218788147	The paper brings predictive process and active inference together really nicely, I think.
2360420	2361024	A	0.6283750534057617	Thanks.
2361142	2362412	A	0.8918519616127014	Great suggestion.
2362556	2378712	A	0.8139557838439941	And just without going into too much detail, if this is the causal graph architecture of a predictive processing system, one can imagine that, like, Mu is being a Markov blanket with respect to these two epsilons or so on.
2378846	2384036	A	0.5147108435630798	So predictive processing doesn't highlight as much the Markov blanket.
2384068	2394300	A	0.7631798982620239	Formalism predictive processing is not a physics for particular systems like FEP is, but it's not incompatible.
2394720	2400300	A	0.8583972454071045	So maybe they're just different fingers pointing at similar or different parts of the moon.
2401040	2425860	A	0.7434363961219788	So there isn't an incompatibility, especially as over the recent years, active inference implemented predictive architectures and approaches, and predictive processing started to undergo that Pragmatic, inactive, four E, five E, whatever, infusion which led to them modeling action as a variable.
2426520	2428744	A	0.6191580891609192	And so all they did was they just made it.
2428782	2432404	A	0.7555896043777466	So instead of the free energy being only about sensory observations.
2432452	2433540	A	0.5261340141296387	And expectations.
2433700	2441144	A	0.5872454643249512	They just tucked action in and it doesn't radically restructure the architecture.
2441272	2444780	A	0.7273972034454346	It just means that action is a variable in these equations.
2445120	2451308	A	0.7896198034286499	So 43.1 or zero, one and two are all good places to look.
2451474	2456288	A	0.8003928065299988	Okay, this was just a short question.
2456374	2459020	A	0.8839489221572876	Internal state and external state were added to the ontology.
2459100	2464210	A	0.6997421979904175	But sometimes, like, if you have a quotation mark and then an at, it won't bring it up.
2465080	2468390	A	0.7981193661689758	So sometimes you have to type it with an at first.
2470440	2480960	A	0.9771794676780701	Okay, in our last 15 minutes, okay, these two are good.
2481030	2486850	A	0.8764387965202332	Maybe we could do a first pass on them and then in the coming week, like add notes and more questions to discuss.
2488420	2498020	A	0.8259682059288025	They wrote, if one defines preferred states as expected states, then one can say that living organisms must minimize the surprise of their sensory observations.
2498600	2501236	A	0.5768768191337585	I'm not clear on the ontology here.
2501418	2509210	A	0.8309351205825806	Is it that preference is the same as expectation or that formally we can use them in the same equation or something else?
2510460	2520344	A	0.7957387566566467	Does active inference make ontological claims like preference is actually expectation, just as heat is actually the excitation of molecules more generally?
2520392	2524540	A	0.8697463870048523	Is the free energy and physics just an analogy or is it an ontological assertion?
2525520	2528670	A	0.9205678105354309	Who would like to approach one of the aspects of this question?
2541720	2545450	G	0.5400729179382324	I would like to take a stop, but again, I guess guess yes, please.
2546540	2557310	G	0.6862025856971741	I guess it's not saying that they're the same, but that if we're minimizing free energy, we're trying to make it the same.
2558160	2561390	G	0.5821336507797241	But if they're surprised, they won't be the same.
2561760	2565230	G	0.8200727105140686	I guess the goal is to get them as close as possible.
2567280	2569630	G	0.8601999282836914	That will be my guess on this.
2570660	2571600	A	0.8529649972915649	Thank you.
2571750	2572892	A	0.7810095548629761	Good insight.
2573036	2578370	A	0.8851024508476257	I'll also point to two ways that expectation is conversationally used.
2579300	2585728	A	0.829028844833374	The expectation with the fancy e is the expected mean of a distribution.
2585824	2593232	A	0.8531215786933899	Like, expected returns might be about the future, but it's actually referring to the mean estimate of a distribution.
2593376	2596824	A	0.7928275465965271	In the Gaussian case, the mean and the mode are the same.
2597022	2599604	A	0.5432218909263611	Mode seeking and mean seeking are equivalent.
2599732	2602616	A	0.8715959191322327	There can also be a variance estimator and so on.
2602718	2607660	A	0.6294000148773193	So the expectation of a Gaussian is like tremendously informative.
2608320	2613950	A	0.760120689868927	Expectation can also mean things that are predicted about the future.
2617040	2618072	A	0.7638475298881531	That's a conversational.
2618136	2620400	A	0.9074767231941223	What do you expect it to be tomorrow?
2621140	2622796	A	0.764032781124115	Both can be conjoined.
2622908	2626240	A	0.9113667607307434	Like what do you expect the temperature to be tomorrow?
2626820	2633460	A	0.8862674236297607	Is the expectation of a distribution of the uncertainty of the temperature distribution at the time point tomorrow.
2634040	2636704	A	0.7763591408729553	So they're not exclusive definitions.
2636832	2650410	A	0.591755747795105	It just has to be understood what this means because it'd be like, well, if our predictions about the future are our preferences, then there's all kinds of tangles that one might find himself in.
2651660	2682722	A	0.8872696757316589	Also the I'm not remembering which number it was, but the dual usage of the same term to reflect the organism's preferred states and also the centering of the distribution at that time point and at other time points.
2682776	2704522	A	0.8371709585189819	And then just like jessica mentioned, this operation of free energy minimization is in service of reducing the divergence between the preferred state and the expectation of the preferred state, which could be set or learned, and then the observations that are coming in.
2704656	2711162	A	0.8575112223625183	So in one equation, it leans more towards the English description of a preference.
2711226	2714400	A	0.8063750863075256	Like the organism prefers to be at 72.
2715170	2719550	A	0.795242965221405	And in the other sense, that is like the expectation of a distribution.
2720130	2735750	A	0.85337233543396	And there's some observation distribution with a minimization of a divergence such that if the observations were realizing preferences, then those distributions would be aligned.
2737850	2746710	A	0.8241174817085266	So, yes, these terms are more like phenomena or natural language tags.
2747150	2748378	A	0.9716407656669617	Also, it'll be awesome.
2748464	2757398	A	0.7725874185562134	In the last two weeks of June, in 46, active inference models do not contradict folk psychology.
2757574	2763630	A	0.875899076461792	This will be extended, extended discussions on, like, wanting beliefs, intentions.
2764050	2769886	A	0.8661231398582458	Which variables in active are appropriate for being described that way?
2770068	2772082	A	0.7860719561576843	Does it contradict one or the other?
2772136	2774530	A	0.690754234790802	Those are like things that we'll explore.
2775350	2778900	A	0.6780436038970947	So the equations are just what they are.
2779270	2789894	A	0.8372846841812134	And then in some usages, a variable is like the organism's preferred states, what it's trying to reduce its divergence between.
2790092	2793446	A	0.7893004417419434	And also that is like, where it expects to be.
2793548	2798694	A	0.7128344774246216	And that's what licenses the use of surprise and minimizing.
2798742	2807846	A	0.7357361912727356	Surprise because an observation right at the center of the Gaussian is minimally surprising.
2807958	2809694	A	0.8670724034309387	We talked about that in chapter two.
2809812	2813790	A	0.6497445106506348	An observation that's outside of the Gaussian is highly surprising.
2814210	2831250	A	0.8940268158912659	So if we have some sort of distribution of outcomes, we could ask how closely are they aligned with our preferences in terms of surprise due to this dual use of the same variables?
2832870	2835314	A	0.8441039323806763	Does active inference make ontological claims?
2835362	2836950	A	0.7954883575439453	Do theories make claims?
2838090	2839746	A	0.6527324318885803	Do people make claims?
2839938	2840294	A	0.547865092754364	E.
2840332	2844870	A	0.7859208583831787	G preference is actually expectation, just as heat is actually excitation of molecules.
2847070	2849660	A	0.6574842929840088	Anyone can raise their hand at any time.
2852510	2855830	A	0.8936049342155457	We'll talk about that in the folk psychology.
2855910	2857578	A	0.5161387920379639	But it's an interesting question.
2857664	2881614	D	0.850614607334137	Ben yeah, I wonder if it seems to me like these questions are taking us maybe into the map territory debate that I think we spoke about last time, about how realist or concrete should we think about the kind of ontological claims of active inference?
2881662	2887430	D	0.8499980568885803	It's certainly the last part of that question, I think, about whether we should take it as an analogy.
2888890	2891800	D	0.6014426350593567	So, yeah, I think it's quite a big question, but I don't know.
2895770	2901142	A	0.7404536008834839	Okay, I won't find the slide here, but is the free energy in physics an analogy?
2901286	2922798	A	0.6034381985664368	Not sure if this is saying, like, the Gibbs free energy or if that's actually talking about the variational free energy or the expected free energy, which are statistical so I don't know if it's an analogy or an ontological assertion other than just to connect it to a linear model with a Gaussian Error distribution.
2922974	2927780	A	0.7834222316741943	Is that an ontological assertion that there is a Gaussian Error distribution in the world?
2928390	2929140	A	0.49361860752105713	No.
2929750	2934760	A	0.8578051924705505	So are statistical quantities ontological assertions about the world?
2936570	2939720	A	0.723456084728241	Maybe there's some nuance, but broadly speaking, no.
2943690	2952598	A	0.8402805924415588	And also, just like on the Math learning group, we've expanded our operation due to many active participants.
2952774	2959554	A	0.9189455509185791	So just note that the meetings are at 19 UTC on Monday, Tuesday, Wednesday and Thursday.
2959702	2963310	A	0.634365975856781	And they're in the discord voice chat, not in this gather.
2963730	2984018	A	0.7500429153442383	But we've been working on the resources on the notation, basic and background questions, as well as overviews of the math for different chapters, like basically just summarizing and then several people, and this is an example of the broken link.
2984104	2986210	A	0.5190842747688293	So, like variational, free energy is deleted.
2986290	2991046	A	0.8264390826225281	So just one time somebody has to come through and just add it back in.
2991228	3004070	A	0.8088448643684387	But these are natural language descriptions that will be very easy to transmute into computer code and translate amongst human languages and provides a lot of legibility and comprehensibility.
3004230	3017002	A	0.726654052734375	So a lot of the math group has been modifying the equations and everybody is welcome to join those discord sessions and contribute with questions or with knowledge and expertise.
3017066	3023326	A	0.7998495101928711	Like, no matter where somebody is in learning or discussing these equations, they are the essence of active inference.
3023438	3030930	A	0.6736868023872375	So questions, random notes, related thoughts, expertise are all essential for us improving our shared understanding.
3031510	3051254	A	0.5565389394760132	And then just in the last five minutes, page 42, they wrote in Advanced organisms, preferred states can also extend to learned cognitive goals and go on to say that advanced organisms like humans, can achieve preferred states by increasingly abstract social and cultural strategies.
3051302	3057610	A	0.8685598373413086	Like they talked about thermoregulation and then all the way up to, like, air conditioning distribution systems.
3058030	3069280	A	0.9021584987640381	My question is, if they are cast in terms of active inference, must all of these preferred states and strategies be ultimately linked to survival in some way?
3070450	3075490	A	0.8353086709976196	Or in the case of advanced organisms, can free energy be related to something other than survival?
3079440	3081470	A	0.8453531861305237	Mike and then anyone else?
3083520	3091650	E	0.5685479640960693	Yeah, it feels like this notion of survival and preferred states gets conflated in a sort of odd way.
3092260	3093964	E	0.7314532399177551	Not everything is about survival.
3094012	3094272	E	0.7360090017318726	Right?
3094326	3110996	E	0.6507693529129028	And so you might have some measurement of surprise if you're in a temperature that's 120 degrees, but it's survivable for a period of time, whereas if you're in a temperature of 300 degrees Fahrenheit, not survivable.
3111108	3111770	E	0.5664746165275574	Right.
3113020	3125256	E	0.7352626323699951	But if you contrast that with other things that could be equally surprising or outside the distribution, as you described it earlier, the notion of that all swans are white, but then you find a black swan.
3125288	3136780	E	0.6900036931037903	And so you'd never conceived that there might be a black swan before that lives outside your distribution and creates a surprise, but it's not necessarily survival rated.
3138560	3139550	A	0.8529649972915649	Thank you.
3139940	3143040	E	0.6189594268798828	There's sort of a missing severity in there somewhere.
3144580	3144992	A	0.584351658821106	Okay.
3145046	3145996	A	0.8926437497138977	Thank you, Ollie.
3146028	3147120	A	0.8017311096191406	And then Lyle.
3149140	3160656	C	0.7583867907524109	I think Live here is used somehow in a more direct way, meaning system that resists dispersion and energy dissipation.
3160848	3164740	C	0.7837637662887573	So it's not necessarily biological survival.
3166620	3167880	A	0.8734541535377502	Yes, great point.
3167950	3173530	A	0.8985861539840698	Systems that fail to persist, survive, will not continue to be that thing.
3173900	3182248	A	0.8131130337715149	And that's just in like a physical sense, repeated measurements are only enabled by systems that are persistent at that timescale, not eternally.
3182424	3192720	A	0.7047184705734253	And so when we want to have a theory or a framework or a physics for things, then they have to look as if they are minimizing free energy.
3192870	3195570	A	0.6384572386741638	Otherwise they will simply not be that thing.
3196100	3197840	A	0.877938449382782	Ben and then lyle.
3201720	3202612	B	0.7916223406791687	This was my question.
3202666	3205510	D	0.8425283432006836	I suppose just to kind of clarify it quickly.
3207080	3209792	D	0.6754570603370667	One of the things I wonder most about active inferences.
3209856	3222904	D	0.49183130264282227	It seems to me, at least in my life, I'm quite often making decisions and having preferences that seem so abstracted from my attunement with my environment or my continued viability as a system.
3223022	3227550	D	0.8861601948738098	Like my preference for which mug I'm going to drink coffee out of in the morning.
3227920	3236240	D	0.5237487554550171	It seems in some sense removed from my free energy the way that free energy is described in the literature.
3237140	3238800	D	0.8221766948699951	That's kind of what I meant.
3240340	3240896	A	0.7671424746513367	Great.
3240998	3252052	A	0.6894801259040833	In yesterday's 45.2 we asked specifically, like, how can all behavior be optimal when mistakes are made?
3252106	3256960	A	0.8539238572120667	Or I'll be asked the question, like, what about when there's like sacrificial or altruistic behavior?
3257120	3268036	A	0.6947983503341675	And there's a great answer in 45.2, but suffice to say that with different priors, all behavior can be cast as optimal behavior.
3268148	3275880	A	0.6428306698799133	So the normativity isn't referring to what should be done or what is best, but it's rather like a process normativity.
3275960	3279420	B	0.7552212476730347	Lyle right, yeah.
3279570	3300244	B	0.7691351771354675	This for me tees up the question I continue to have about the approach, which is how does the concept of novelty, which some organisms would pursue, fit into this concept of I mean, how would it even be represented here?
3300362	3313380	B	0.5177397131919861	And novelty is not just a human trait, but you would see it across all kinds of creatures that would seek out novel and surprising experience, not expected.
3313540	3318708	B	0.8640558123588562	So for me, it's kind of an open question about how that's captured in this framework.
3318884	3329116	A	0.600887656211853	Yes, the beginning and ending of 45.2 carl's focus was on curiosity and there's various ways to approach it.
3329218	3335416	A	0.7628384828567505	And it's an excellent question about novelty of different types and scales.
3335608	3341884	A	0.7007446885108948	And within the hierarchical modeling, one can understand novelty of different kinds.
3342012	3344140	A	0.737737774848938	And this again is just like the kernel.
3344220	3346492	A	0.6121605038642883	This is like the single linear regression.
3346636	3354624	A	0.8413642644882202	And then there's like model selection across hierarchically nested linear models, much development beyond the kernel.
3354752	3356260	A	0.4903316795825958	But this is like a kernel.
3356600	3369864	A	0.4987892806529999	And especially when thinking about expected free energy, a path can move to a novel area because it's part of an expected trajectory that does reduce risk and ambiguity and so on.
3369982	3374600	A	0.7558313608169556	So that's the deflationary approach to novelty pursuit.
3374760	3378060	A	0.8471428751945496	Jessica and then we'll end the discussion.
3383650	3384640	G	0.5980674028396606	At the end.
3385010	3386174	G	0.6934666633605957	You mentioned it.
3386292	3391680	G	0.8429073691368103	I guess I see novelty as sort of like the epistemic origin.
3392390	3404790	G	0.7712891697883606	And so it's like you are seeking new information, new things, in order to minimize that uncertainty.
3405610	3424922	G	0.6332541704177856	Not so much that novelty is uncertainty, because if we look at Nova's thinking that, oh, it's new and therefore it's going to be surprising and so we want to minimize those things then yes.
3425056	3438650	G	0.8308145403862	Or also if we look at uncertainty equal to risk, we can say like novelty is because of the unknown represented risk and if we're minimizing uncertainty, we're trying to minimize risk.
3438810	3450130	G	0.8331143260002136	But from what I saw a little bit on chapter two, it's sort of like we're not necessarily trying to minimize risk or the unknown.
3451210	3455202	G	0.8031790256500244	Depending on our preferences, we might tolerate high risk.
3455346	3459862	G	0.7092295289039612	And what we're trying to minimize is the surprise of what we expect.
3459996	3472140	G	0.5610665678977966	If I'm in finance, and I expect very high risk in an investment, but I already know that and high risk comes back.
3473870	3482574	G	0.7210127115249634	My uncertainty was minimized, even though there was a lot of risk or there were a lot of unknowns or novelty and things like that.
3482612	3486158	G	0.7430124878883362	But it was already in my expectation of what was going to happen.
3486324	3500690	G	0.6393387913703918	So I was already minimizing that, even though in reality it may look like that there was a lot of uncertainty in actuality, but in my model there isn't.
3501050	3519740	G	0.6914305090904236	So that's kind of like how I explained that to me a little bit and in terms of novelty for adventure and curiosity and things like that, I equated more to epistemic foraging as a weight of learning and updating the model and that's why we're doing it.
3520510	3521686	A	0.7834749221801758	Thanks, Jessica.
3521798	3524154	A	0.8537740111351013	I'll just add one closing note on that.
3524272	3545426	A	0.5220023393630981	I would argue that it's the pragmatic absolutism, the reinforcement and reward learning centrism that has curtailed an effective theory of novelty or curiosity because radically different kinds of actions have to be coerced into a value framework, whether it's deep Q learning or reinforcement or anything like that.
3545528	3550718	A	0.7263286113739014	There has to be like a novelty bonus or all these other ad hoc unprincipled approaches.
3550814	3561206	A	0.6053244471549988	They might be super technical, they might be super effective, but they're not driven from first principles in variational free energy f and expected free energy g.
3561388	3578282	A	0.765950620174408	There's this minimum of two with pragmatic value which is to bring alignment between the preferences and the observations and the epistemic value associated with what is casually called novelty or curiosity.
3578426	3598190	A	0.631229043006897	At that scale of potentially a hierarchical model, So active inference does provide partitioning and an imperative that helps us address the phenomena of curiosity and of novelty in a way that is quite disparate from trying to coerce it into a value of curiosity.
3598270	3605940	A	0.785111129283905	Is expected pragmatic value or the value of basic research is the expected utility that it could bring.
3606550	3610610	A	0.9691905975341797	But these are all really awesome and open and ongoing questions.
3610760	3615802	A	0.903638482093811	So we will close the recording and then have just a 1 minute break.
3615936	3625690	A	0.9055956602096558	Then in this room we'll continue with tools, organizational unit and if anybody wants to talk about something else, they can go to a different gather room.
3625760	3628380	A	0.9581927061080933	So thanks everybody and see you next week.
3629630	3629880	C	0.6283750534057617	Thanks.

[
  {
    "start": 1.766,
    "end": 30.84,
    "text": " hello everyone it is june 9th 2022 and it's chapter three week one and it's the first cohort of the textbook group we're gonna go to the questions uh there weren't any ideas added for three but if people have any key themes or any ideas that they picked up on with three they can add them here otherwise we'll go to the questions and um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 32.853,
    "end": 58.252,
    "text": " also if people have questions in the chat or it would be also great just to take notes or add questions and continue to upload them and then next week we'll continue the discussion on three so this is just the opening set of questions maybe just the first ideas of the book turning two questions now that we've at least",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 59.44,
    "end": 85.601,
    "text": " gone through chapter two once and part of chapter three or the whole thing of chapter three what is the high road to active inference what does anyone think about the high road to active inference or about this low road high road",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 87.335,
    "end": 113.327,
    "text": " construct that is used in chapters two and three just broadly how did the approach of chapter three feel different than the approach or the topics of chapter two um jf and then ali",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 115.589,
    "end": 124.415,
    "text": " I just want to make the comment that the high road approach is what first grabbed me when I encountered the free energy principle.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 124.495,
    "end": 134.641,
    "text": "That's what really fascinated me, that there was an attempt at a unifying answer to self-organization, cognition,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 140.666,
    "end": 147.75,
    "text": " that this could all be brought together around some relatively sparse number of concepts.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 148.35,
    "end": 150.691,
    "text": "That's what grabbed me, much more than the low road.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 153.453,
    "end": 153.773,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 154.894,
    "end": 155.714,
    "text": "Ali, and then Ben.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 159.281,
    "end": 176.689,
    "text": " Well, yeah, reading Chapter 3 actually clarified my previously held, in fact, wrong belief that previously I thought that low road was a kind of bottom-up approach and the high road is a top-down approach.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 176.729,
    "end": 183.632,
    "text": "But now I see that low road was basically tackling the problem of",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 184.572,
    "end": 191.142,
    "text": " active inference from the inferential point of view and from the statistical and inferential point of view.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 191.242,
    "end": 199.114,
    "text": "But in High Road, we have a much more generalized and much more",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 200.336,
    "end": 215.274,
    "text": " Let's say all-encompassing view to tackle the problem of active inference, namely by way of defining markup blankets and all the properties of living organisms and so on.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 217.036,
    "end": 217.376,
    "text": "Awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 217.456,
    "end": 217.757,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 218.766,
    "end": 221.228,
    "text": " Ben, and then anyone else who raises their hand.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 221.388,
    "end": 225.191,
    "text": "And also, you can take notes on this idea as the book, like High Road generally.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 225.591,
    "end": 226.812,
    "text": "Ben, and then Mike.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 227.993,
    "end": 234.999,
    "text": "Yeah, I think for me as well, this chapter three in the High Road has been an easier road in.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 235.199,
    "end": 241.584,
    "text": "And the thing that I noticed about it was it seems to me, so they say in the introduction that active inference is a normative thing.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 242.233,
    "end": 271.323,
    "text": " theory but i feel like the high road to me feels there's much more normativity in the high road it's an account of what organisms have to do and i think compared to the previous compared to the previous chapter i think chapter two felt a lot more descriptive for me i'm not sure other people would maybe that's just the way that i was reading it but it it felt like chapter three was a lot more kind of it had a more normative force to it that i i really enjoyed thank you mike",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 281.755,
    "end": 282.135,
    "text": " There we go.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 282.535,
    "end": 285.477,
    "text": "Sorry, my computer is not responding.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 286.858,
    "end": 295.502,
    "text": "So I went through a similar experience as Ali described in terms of going into it, thinking about bottoms up and top down and coming out of it, thinking about it differently.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 297.822,
    "end": 324.411,
    "text": " i feel like in chapter three there's still a lot to unpack around the markov blanket so um that is still capturing this inferential nature of things um it still has a bayesian quality like what we saw in chapter two um but it also for me at least it still feels a bit opaque at this point in terms of um this sort of indirect influence that's taking place across the markov blanket",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 327.17,
    "end": 327.45,
    "text": " Awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 328.31,
    "end": 328.571,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 330.311,
    "end": 331.312,
    "text": "Any other comments?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 331.392,
    "end": 334.993,
    "text": "Otherwise, we're going to go to the questions and we'll approach many of those topics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 340.555,
    "end": 340.836,
    "text": "Okay.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 351.16,
    "end": 351.7,
    "text": "First question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 353.956,
    "end": 371.265,
    "text": " regarding the markov blanket explanation on page 43 and box 3.1 they write no additional information about the future is gained by finding out about the past assuming we know the present how can this be true that's one question they write",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 373.166,
    "end": 382.628,
    "text": " They define regarding the Markov blanket as the set of variables that mediate all statistical interactions between a system and its environment.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 383.288,
    "end": 386.149,
    "text": "What is a statistical rather than a non-statistical interaction?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 390.37,
    "end": 399.212,
    "text": "They write that they have supplemented conditional independencies with dynamical constraints so that the flows do not depend upon states on the opposite side of the blanket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 400.12,
    "end": 404.222,
    "text": " Why does independence between the flows on the two sides of the blanket matter so much?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 406.083,
    "end": 407.024,
    "text": "All right, great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 408.224,
    "end": 412.767,
    "text": "So who would like to give a thought on this first question?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 413.327,
    "end": 420.571,
    "text": "How can it be true about Markov blankets that no additional information about the future is gained by finding out about the past?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 433.051,
    "end": 451.236,
    "text": " so I added a note in there you can tell me if this is correct or not with regard to Markov chains that as I understand the Markov chain in its present state is effectively capturing all the information about the past so I assume that translates into the Markov blanket",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 452.793,
    "end": 469.967,
    "text": " yes um great comment and a markov chain is is a uh used very broadly and just to give one example it's like the current state of the chessboard knowing more about previous moves",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 471.201,
    "end": 473.181,
    "text": " doesn't tell you more about the future.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 473.541,
    "end": 479.623,
    "text": "And so it's like, it's a one Markov chain because the dependency is only about that one time step.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 480.203,
    "end": 498.726,
    "text": "Now, if it were to violate that condition of being a one Markov blanket through time and have like a three Markov blanket, that would mean that the same board position, depending on what move happened three before, was still having some influence on how the system evolved.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 499.508,
    "end": 501.831,
    "text": " That wouldn't violate the Markovian property.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 502.491,
    "end": 505.455,
    "text": "It would just make it a different kind of Markov chain.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 506.696,
    "end": 515.947,
    "text": "And that is commonly used in the temporal modeling, like in time series modeling, and in transitions of stochastic processes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 517.128,
    "end": 523.85,
    "text": " The Markov blanket is abstracting it from this like time sequence idea.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 524.311,
    "end": 526.872,
    "text": "Like the now is a blanket between the past and the future.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 527.632,
    "end": 539.876,
    "text": "And we could think about spatial systems, like a Newton's cradle or something like that, as well as like spatial temporal systems or just abstract causal systems.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 541.197,
    "end": 542.357,
    "text": "So how can this be true?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 542.597,
    "end": 546.359,
    "text": "It's true by definition, because that's what the definition of a Markov blanket is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 547.369,
    "end": 561.257,
    "text": " whether that description is adequate or maps on to carving nature of the joints and all these other things are respectively statistical model comparison and ontological philosophical questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 561.658,
    "end": 564.159,
    "text": "But it's true by definition about Markov blankets.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 564.739,
    "end": 565.0,
    "text": "Lyle?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 566.841,
    "end": 575.526,
    "text": "Yeah, I mean, I think you actually, you just pretty much covered it, but I would just say this is the same condition if you're building RL models of different kinds of autonomous systems.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 576.089,
    "end": 577.609,
    "text": " This is the same restriction.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 577.629,
    "end": 605.016,
    "text": "I think your comment about the nested, the nested blankets is interesting because in the RL space, you know, we, we typically don't have that kind of richness, but what we do then would be, for example, we want to know that just, and in that case, we would just create a, you basically create a state variable, which is the, the pro you know, the,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 605.956,
    "end": 609.159,
    "text": " the history of the state transition.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 609.179,
    "end": 610.1,
    "text": "So we fudge it a little bit.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 610.501,
    "end": 621.913,
    "text": "But your explanation that in this conception with nested Markov blankets, then that would be the more comprehensive solution to that issue.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 623.732,
    "end": 639.179,
    "text": " Thanks, just to kind of clarify there, like one hack or workaround would be like, you make the list of all of the last possible three moves, and then you make a one Markov blanket with all the last combinations of three.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 639.199,
    "end": 651.765,
    "text": "So it still uses the machinery of a one Markov chain, which has the simplest transition matrix, but it could encompass the last states, but it sort of compresses them into a one Markov chain format.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 652.734,
    "end": 674.851,
    "text": " yeah i mean very often you want to know the you know across you want to know the la the average of the last you know 20 time steps yeah you're just trying to smooth something right and it's just way too much work to build all that machinery in there so you you make you make that average actually a present variable that's what you do",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 676.457,
    "end": 702.511,
    "text": " great example if something really does depend on the last 20 time steps or you want to do model comparison as to whether it's a better model that is conditioned on the last 15 or 20 or 30 um you can just condense it with summary statistics and descriptions again into this one markov framework otherwise the combinatorics of how the 20 influence each other is vast brock",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 704.662,
    "end": 706.122,
    "text": " I guess that's related to it.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 706.842,
    "end": 733.107,
    "text": "My comment was just going to be that the way they worded it, if you're not rigidly thinking about a singular Markov blanket, that, you know, I mean, just practically speaking, finding out about the past usually does give you information about the future if you're talking about outside of a singular Markov blanket, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 733.655,
    "end": 749.401,
    "text": " So adding the words, no additional information about the Markov, about the future of the Markov blanket is gained by finding out about the past Markov blanket, kind of balance that.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 749.661,
    "end": 758.685,
    "text": "But in the case of the machine learning example and comparing models, you're necessarily talking about a much larger Markov blanket than just a singular one there, right?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 758.705,
    "end": 761.626,
    "text": "So it's the same thing.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 763.083,
    "end": 774.692,
    "text": " Yes, and just like many concepts have like a broader conversational, informal sense and a narrow, more technical sense, assuming we know the present means fully knowing the present, fully knowing the blanket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 775.273,
    "end": 777.795,
    "text": "So it's like, but I could still learn things about the past.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 778.275,
    "end": 782.819,
    "text": "So that's not to say that there isn't novel information that you could discover about the past.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 783.339,
    "end": 786.602,
    "text": "Like you could still go back in that chess game and learn information.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 786.962,
    "end": 789.043,
    "text": "So it's not that there isn't information in the past,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 789.844,
    "end": 791.905,
    "text": " It's not that it wouldn't even be interesting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 792.445,
    "end": 807.752,
    "text": "It's that for the purposes of how the present goes into the next time step, the present blanket in this temporal Markov chain, which is like a blanket through time, it contains the information you need for the transition frequencies.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 810.193,
    "end": 810.714,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 811.874,
    "end": 817.337,
    "text": "They define that it's the set of variables that mediate all, and also the Markov blanket",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 819.432,
    "end": 838.215,
    "text": " using last names you know personal opinion is not helpful because not only are there multiple markovs but the markovian property is not something that's technical or defined so maybe someday we'll have better ways to describe and be more specific that don't involve invoking",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 839.076,
    "end": 866.755,
    "text": " ambiguity around names um that being said it was worked out analytically by markov and sun and others but most recently it was pearl 1988 and causal inference with bayesian perspective so this is not like an active inference ism it's drawing upon a total bayesian statistical framework any base graph that's not fully connected is going to have some nodes",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 867.475,
    "end": 880.016,
    "text": " that intermediate so that's going to relate to this question so what is a statistical rather than a non-statistical interaction Ali and then anyone else who would like to address that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 882.964,
    "end": 903.04,
    "text": " Well, I think it has something to do with the way these interactions are parameterized and modeled, because you see, every phenomenon can be parameterized and modeled from many different, many various perspectives and frameworks within various frameworks.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 903.82,
    "end": 918.172,
    "text": " But here, and the reason I think they put the term statistical in parentheses is that they mean that these variables mediate all interactions that have been parameterized statistically.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 918.812,
    "end": 927.419,
    "text": "So I think it refers to the nature of parameterizing and modeling these interactions.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 928.259,
    "end": 944.171,
    "text": " as opposed to distinguishing between statistical and non-statistical interactions.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 944.391,
    "end": 947.393,
    "text": "Anyone else can raise their hand, but just here's a few thoughts on that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 949.214,
    "end": 954.058,
    "text": "A statistical interaction is the edge in a causal Bayes graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 955.359,
    "end": 969.631,
    "text": " And people might be more familiar or have seen like structural equation modeling where nodes are variables and edges are the correlation between or among variables conditioning on the structure of the graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 971.374,
    "end": 977.456,
    "text": " In the causal Bayes graph, the nodes are variables and the edges are statistical causal influences.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 978.056,
    "end": 995.442,
    "text": "So causality and statistics is not always the same as what people mean by cause in the real world, but like Granger causality and just this notion of like coarse graining and cause, it may line up with causality.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 996.627,
    "end": 1022.6,
    "text": " the sort of narrative natural language description that somebody has between when they're speaking it might be narrower than that it might be a different thing than that it might be more general than that um what would a non-statistical interaction be i mean there's various ways to approach that it's kind of pointing towards some sort of like a touch could be an interaction",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1023.339,
    "end": 1029.024,
    "text": " But what if things are touching, but those variables don't change as a function of their touching?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1030.485,
    "end": 1032.626,
    "text": "Then do those things interact or not?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1033.847,
    "end": 1034.628,
    "text": "That's one question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1035.289,
    "end": 1041.073,
    "text": "And then cause, there's like just tremendous real world and philosophy questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1041.093,
    "end": 1042.214,
    "text": "Like what causes what?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1042.274,
    "end": 1043.775,
    "text": "What's the difference making cause?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1043.836,
    "end": 1045.297,
    "text": "What's the cause that makes a difference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1046.71,
    "end": 1048.071,
    "text": " necessity and sufficiency.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1048.732,
    "end": 1052.755,
    "text": "There's so many questions in this like causal philosophy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1053.735,
    "end": 1056.737,
    "text": "And this is highlighting, we're talking about the model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1057.458,
    "end": 1060.88,
    "text": "We made a base graph with height, weight, and shirt color.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1061.758,
    "end": 1063.699,
    "text": " And here are the statistical effectors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1064.599,
    "end": 1068.181,
    "text": "And so we're not saying that there's no interaction between this and that in the real world.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1068.421,
    "end": 1072.043,
    "text": "Now we're within the model and we're talking about the causal graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1072.843,
    "end": 1085.489,
    "text": "And again, unless the Bayes graph is fully connected, there's going to be some partitioning scheme that will result in some set of nodes being conditionally independent from another set of nodes",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1086.4,
    "end": 1088.121,
    "text": " when conditioned upon a blanket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1088.862,
    "end": 1096.287,
    "text": "So it's not like features of the real world or even variables in a statistical equation can just be unilaterally tagged as blanket states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1097.068,
    "end": 1106.375,
    "text": "It's always a partition that co-instantiates two sides, internal and external, but there's a symmetry, and the blanket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1107.321,
    "end": 1124.512,
    "text": " So it's not like features of the world or even features of the model are intrinsically internal, external, or blanket states, but that's a partitioning scheme that can be applied to Bayes graphs of like a vast, various structures of Bayes graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1125.192,
    "end": 1129.555,
    "text": "And Bayes graphs can be applied to a vast number of real world situations",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1131.405,
    "end": 1156.206,
    "text": " And many, many papers and live streams discuss sort of like all-encompassing Markovian monism, Markov blankets all the way down, nested Markov blankets are the world, ranging to the critical perspectives, not necessarily detractors, but just those who believe that the usage of the concept is out of alignment with ways that people have used it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1157.667,
    "end": 1158.708,
    "text": "Mike, and then anyone else.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1162.097,
    "end": 1169.505,
    "text": " Does it make sense to think of a Markov blanket in the context of an internal agent state and an external environment?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1170.065,
    "end": 1182.86,
    "text": "Is the Markov blanket serving as this kind of a decoupling component because of the way that things are conditionally independent based on the variables inside the blanket?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1191.291,
    "end": 1191.952,
    "text": " What do people think?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1196.355,
    "end": 1196.615,
    "text": "Yes.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1200.398,
    "end": 1212.647,
    "text": "I mean, for the internal state to be different than the external state or have some other equilibrium, other non-equilibrium state that the external environment is in, you would have to necessarily have",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1215.143,
    "end": 1233.796,
    "text": " i'm not sure if decoupling is the right word but some sort of conditional dependent independent sort of zone that separates those two regions so here's figure three one thank you brock the active and the sensory states compose the blanket now",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 1235.08,
    "end": 1264.352,
    "text": " in various like live streams and papers the history of the concept again from the analytical pre-computational phrasing of insulation of fluctuation of random variables to pearls bayesian and computational framing however pearls markov blanket concept doesn't have a delineation of active and sensory states it's only like there's one kind of fabric in that blanket it's just blanket states",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1265.438,
    "end": 1292.353,
    "text": " friston maybe this is also will be shown to be traced to other places and ways but one of the core things that friston at all have brought into the picture was interpreting within the blanket states the states that have incoming dependencies to the system of interest as sensory and then the states that have outgoing dependencies from the system of interest as active states so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1293.68,
    "end": 1299.886,
    "text": " People may already see challenges and complexities that arise with the arrows and the directions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1300.727,
    "end": 1306.293,
    "text": "And a lot of that is addressed in the how particular is the free energy principle of Aguilera et al.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1307.073,
    "end": 1312.739,
    "text": "And so we're going to continue talking about this entity partitioning.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1313.86,
    "end": 1314.741,
    "text": "Ali and then Lyle.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1318.002,
    "end": 1341.261,
    "text": " Well, I think we should point to a very important typographical error in this picture here, because in this figure active states are expressed in terms of external states and markup blankets, and sensory states are expressed in terms of internal states and markup blankets.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1342.242,
    "end": 1343.463,
    "text": " But it should be vice versa.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1343.784,
    "end": 1355.635,
    "text": "I mean, the flows of internal active states are independent of external states, and the flows of external and sensory states do not depend on internal states.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1356.295,
    "end": 1362.141,
    "text": "So we should change x and mu in those two equations.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1363.963,
    "end": 1366.545,
    "text": "Yes, sometimes with a possible...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1367.748,
    "end": 1381.557,
    "text": " typo like that it's like it's it's so egregious slash blatant that it's challenging to even know but we can absolutely ask the authors but just to like read the equation and think about why it is that way",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1382.796,
    "end": 1385.178,
    "text": " The dot is the rate of change of a variable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1385.679,
    "end": 1396.149,
    "text": "So the internal states are mu, and so this is the rate of change of internal states, rate of change of external states, x, and the rate of change of u, the active states, and y, the sensory states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1396.71,
    "end": 1402.296,
    "text": "So we're looking at how these partitioned states in a Bayes graph change through time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1403.359,
    "end": 1408.082,
    "text": " The states that are the blanket and the internal are the particular states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1408.883,
    "end": 1410.464,
    "text": "Those are referring to the particle.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1410.624,
    "end": 1412.045,
    "text": "That's like figure versus ground.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1412.365,
    "end": 1417.809,
    "text": "The particle that's moving around, the curious particle, the active entity, is the particular states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1418.289,
    "end": 1424.033,
    "text": "That is partitioning the thing, literally the thing, away from the niche.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1425.12,
    "end": 1428.362,
    "text": " Then there's the blanket states that are intermediating that interface.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1428.902,
    "end": 1433.925,
    "text": "And then we'll also talk about autonomous states, which are just the internal and the active states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1434.846,
    "end": 1444.691,
    "text": "So the equation is saying the rate of change in, for example, active states is a flow function f and a noise function.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1445.732,
    "end": 1471.25,
    "text": " both the flow and the noise function the subscript is referring to like that one it's kind of like ipso in latin like it's the flow on mu and the noise on mu the flow which is what the principle of least action converges us towards in the limit when the statistical fluctuations from the omega are low is a function of certain variables",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1472.654,
    "end": 1493.524,
    "text": " and so this is saying the flow of active states is a function of external sensory and active states however as ali has pointed to the arguments that should be guiding the flow of active states",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1494.454,
    "end": 1499.599,
    "text": " are actually the blanket states and internal states, not external states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1499.679,
    "end": 1510.17,
    "text": "And then analogously, sensory states should have a flow that's defined in terms of blanket states and external states, not internal states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1512.593,
    "end": 1513.754,
    "text": "Lyle and then Alik.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1516.633,
    "end": 1546.3,
    "text": " yeah so actually i think you're getting right right at the question at hand because i was confused about the the bi-directional arrows particularly between internal states and active states right and and i think you're drilling right into that i still don't completely understand what that picture should look like uh but i i have trouble uh conceptually i sort of get a conception i feel like i get the markov blanket but that representation i have trouble working through the details on that",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1548.319,
    "end": 1548.599,
    "text": " Cool.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1548.659,
    "end": 1554.283,
    "text": "So guest stream number seven, how particular is the physics of the free energy principle?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1555.524,
    "end": 1562.329,
    "text": "It explores different entity niche causal relationships.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1563.709,
    "end": 1568.813,
    "text": "Like for example, we could imagine a simple around the clock entity model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1569.633,
    "end": 1572.676,
    "text": "External states induce sensory states, one directional arrow.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1573.536,
    "end": 1584.759,
    "text": " Sensory goes into internal, internal to active, active to external, like an OODA loop or something like that, but just in around the clock, no bidirectionality, no connection between active and sensory.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1585.419,
    "end": 1590.521,
    "text": "But given this partitioning, one could imagine that topology of cause.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1591.701,
    "end": 1596.883,
    "text": "One could imagine all kinds of topologies of cause, some that violate the Markov blanket condition,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1597.628,
    "end": 1604.317,
    "text": " like internal states and external states that are no longer internal and external because they would have a causal link.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1604.838,
    "end": 1613.73,
    "text": "But even preserving the Markov blanket condition, we can imagine different topologies, different sparsities of couplings between these different states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1615.154,
    "end": 1631.226,
    "text": " a bidirectional arrow is kind of like there would be a non-zero cause and in the other side of the matrix, it would also be a non-zero cause versus a unidirectional relationship where like variable two has a causal influence on three, but three doesn't have it on two.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1633.088,
    "end": 1638.052,
    "text": "In this paper, in GuestStream 7.1 and that paper and further line of research,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1639.11,
    "end": 1653.744,
    "text": " they explore what sparse coupling structures, causal architectures of the entity niche relationship and interface would grant different statistical capacities.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1654.445,
    "end": 1663.814,
    "text": "So I could be wrong on this, but active inference is not a commitment to this specific entity.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1664.659,
    "end": 1693.454,
    "text": " coupling architecture it may be possible to design or describe or imagine systems that have different causal architectures maybe even true at this basal most kernel loop but certainly true when we start thinking about nested structures and cognitive structures and so on um so yes it is a little unfortunate about this however um like",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1694.81,
    "end": 1699.459,
    "text": " This is just thinking about causal relationships of the particular partition.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1705.347,
    "end": 1707.828,
    "text": " And we're describing statistical relationships here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1708.168,
    "end": 1716.311,
    "text": "Causal influence as inferred from time series data with Granger causality or other methods on specific random variables.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1717.152,
    "end": 1720.173,
    "text": "So this is not the conversational notion of cause.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1720.513,
    "end": 1725.275,
    "text": "But what I'm thinking is eventually resulting in things happening in the outside world.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1725.995,
    "end": 1729.977,
    "text": "Or aren't things in the outside world eventually changing things?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1730.217,
    "end": 1730.437,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1731.377,
    "end": 1732.998,
    "text": "But just like the chessboard through time,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1733.953,
    "end": 1737.334,
    "text": " the blanket is statistically intermediating.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1738.135,
    "end": 1745.677,
    "text": "And again, that's why it gets so messy with the application of like, well, the retina must be the sense states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1749.339,
    "end": 1756.361,
    "text": "If one can read that example and not fall into the quicksand, it could be didactic.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1757.302,
    "end": 1761.003,
    "text": "But to tag retina as sense state",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1762.483,
    "end": 1766.484,
    "text": " is like the tip of the iceberg of a partially specified model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1768.265,
    "end": 1771.166,
    "text": "Often one where the measurements have not actually occurred and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1771.666,
    "end": 1779.009,
    "text": "So then that can be extremely difficult for those with less familiarity with the formalism to generalize accurately from.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1779.829,
    "end": 1783.99,
    "text": "Because shouldn't it be like, you know, retina has sense states and then like the arm is the active state?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1784.53,
    "end": 1788.952,
    "text": "But it's a partitioning that co-instantiates and it's model dependent.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1789.532,
    "end": 1791.793,
    "text": "It's not describing features of the real world.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1793.519,
    "end": 1810.867,
    "text": " Yeah, I just wanted to point to one of Ramstad et al's recent fascinating paper on Bayesian mechanics, which I put the link in the chat.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1811.348,
    "end": 1815.469,
    "text": "And in that paper, especially in section three, they have a very",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1817.01,
    "end": 1834.186,
    "text": " illuminating discussion about these whole business of partitioning and markup blankets and one sentence that's pretty relevant to our discussion here is that the key point to note is that the flow of internal and active components",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1835.227,
    "end": 1835.448,
    "text": " i.e.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1835.508,
    "end": 1849.458,
    "text": "their trajectory through state space does not depend upon external components and reciprocally the flow of external and sensory states or paths does not depend upon internal states or paths from page 15.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1850.419,
    "end": 1859.006,
    "text": "So, yeah, in that, if you refer to that article, it has been elaborated much explicitly.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1860.563,
    "end": 1867.329,
    "text": " Also, in four days, we will have Dalton and Maxwell et al for a guest stream.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1869.21,
    "end": 1870.091,
    "text": "The paper just came out.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1870.291,
    "end": 1876.456,
    "text": "Now we'll be able to have a discussion with them and learn and ask them questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1876.496,
    "end": 1881.841,
    "text": "So if people have suggestions for guest streams, that can be accomplished.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1882.281,
    "end": 1887.285,
    "text": "If people want to facilitate or participate in different streams, if they have questions for different streams,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1888.271,
    "end": 1911.08,
    "text": " being there live but especially getting involved eagerly and actively and early is the biggest leverage point because that allows us to design the material so that it's like permanently useful rather than potentially ad hoc questions that arise during which can be super important and hopefully people can see some affordances for participation and development",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1912.08,
    "end": 1930.161,
    "text": " um let's just get to this last question and it's it's like we have other questions but these are essential and it's one of the opening formalisms of the chapter um just to conclude here though why does independence between the flows on the two sides of the blanket matter so much",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1942.469,
    "end": 1944.453,
    "text": " These are good things to think about.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1945.335,
    "end": 1952.149,
    "text": "Just one short thought would be, without independence between the flows, there is no distinguishing one thing from another.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1953.772,
    "end": 1953.953,
    "text": "Ali?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1957.677,
    "end": 1970.247,
    "text": " And also, I think it relates to what we read in chapter two about hidden states, because that's what the word hidden probably refers to.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1970.268,
    "end": 1976.573,
    "text": "I mean, internal state does not have direct access to external states and vice versa.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1976.633,
    "end": 1982.618,
    "text": "So that Markov blanket pretty much formalizes this hiddenness.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 1984.5,
    "end": 1986.201,
    "text": " Yes, awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1986.701,
    "end": 2008.927,
    "text": "And partially observable models of the Bayesian type, which are used in like partially observable Markov decision processes, expectation maximization, any kind of Bayesian priors and hyper priors and so on, all of these have Bayes graphs that reflect conditionally independent variables.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2009.667,
    "end": 2011.648,
    "text": "So it's not that they're independent",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2013.058,
    "end": 2023.331,
    "text": " in a conversational way, like they don't have any way of communicating, it's that they are specifically involved in a causal network that has conditional independence.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2024.192,
    "end": 2026.775,
    "text": "But we'll hopefully develop more answers and notes here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2027.396,
    "end": 2027.837,
    "text": "We'll move on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2030.134,
    "end": 2038.782,
    "text": " What is the differences between and among the terms Bayesian brain hypothesis, predictive processing, predictive coding, and active inference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2039.463,
    "end": 2043.847,
    "text": "Might you be able to suggest references or resources for where these distinctions are delineated?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2044.368,
    "end": 2044.548,
    "text": "Sure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2045.889,
    "end": 2049.953,
    "text": "So anyone can add more, but here's, I think, two key resources here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2050.919,
    "end": 2072.98,
    "text": " first is live stream 43 in 43.0 maria gave a really excellent overview of predictive processing and predictive coding from a historical perspective and briefly she proposed that predictive coding can refer to a unidirectional data encoding scheme of the kind that we see in signal processing video compression so on",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2074.181,
    "end": 2086.065,
    "text": " Whereas predictive processing is referring to a bidirectional architecture where predictive coding is implemented in this ongoing top-down, bottom-up way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2087.926,
    "end": 2092.948,
    "text": "Bayesian brain hypothesis was also touched on, and we connected this to active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2092.988,
    "end": 2097.99,
    "text": "But 43.0 is a long technical review paper, but we have three discussions on it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2098.39,
    "end": 2100.711,
    "text": "Predictive coding, theoretical, experimental review.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2102.0,
    "end": 2108.762,
    "text": " Recent paper, very good source for the predictive processing and coding.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2109.562,
    "end": 2114.904,
    "text": "And predictive processing and coding initially was more of about a sensory framework.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2115.484,
    "end": 2121.506,
    "text": "However, at the end of that paper, they show, okay, now we're gonna do predictive processing on action.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2122.106,
    "end": 2122.906,
    "text": "It's active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2124.107,
    "end": 2131.189,
    "text": "So predictive processing about action is six of one and half dozen of another.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2132.236,
    "end": 2136.639,
    "text": " within a margin of reasonable approximation, similar.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2136.999,
    "end": 2142.562,
    "text": "But there's also some key differences, like reliance on difference formalisms.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2142.903,
    "end": 2147.345,
    "text": "But these are all things for us to explore and unpack as we work on the ontology and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2150.447,
    "end": 2153.869,
    "text": "Specifically, predictive coding active inference in the Bayesian brain",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2155.011,
    "end": 2169.821,
    "text": " This is an interview that a departed colleague and I did with Carl Fristen in 2018, and we specifically asked, the Bayesian brain hypothesis, predictive coding, and the free energy principle are often equated with one another.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2170.741,
    "end": 2174.964,
    "text": "You yourself have suggested that the three frameworks are variations of the same basic mechanisms.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2175.885,
    "end": 2177.646,
    "text": "So 2018 was a very different time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2178.866,
    "end": 2185.789,
    "text": " active inference had not been delineated from FEP in the same way that it is coming to be now.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2186.269,
    "end": 2192.332,
    "text": "And there was a lot of other differences between then and now, but he gives an extended answer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2193.252,
    "end": 2205.758,
    "text": "So for people who want to learn about that, his extended answer and Martin's restatement of the Bayesian brain hypothesis are still some of the best places to find clarity on that issue.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2208.339,
    "end": 2210.76,
    "text": " Does anyone have any other thoughts or questions on this?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2215.182,
    "end": 2226.207,
    "text": "Just one other note while anyone can raise their hand is like the Bayesian brain hypothesis sometimes could be used in a very instrumentalist way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2226.887,
    "end": 2230.269,
    "text": "We're using Bayesian statistics to do neurobehavioral research.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2231.309,
    "end": 2234.191,
    "text": "It might be a realist, implementational,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2235.602,
    "end": 2240.524,
    "text": " claim, like the brain is doing Bayesian statistics or something like Bayesian statistics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2241.165,
    "end": 2249.629,
    "text": "So this hypothesis floats kind of among the layers of Mars analysis, like implementational, algorithmic, computational.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2251.89,
    "end": 2253.77,
    "text": "And people do use it to mean different things.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2254.891,
    "end": 2254.991,
    "text": "And",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2256.576,
    "end": 2262.121,
    "text": " In Bayesian graphs, in Bayesian statistics, it can be about perception or it could be about action.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2262.722,
    "end": 2276.694,
    "text": "So Bayesian brain hypothesis, depending on how somebody frames it and what specific models they're talking about, might include, for example, all of these because these might be applied in a Bayesian framework to talk about the brain.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2278.216,
    "end": 2278.356,
    "text": "Ben?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2278.376,
    "end": 2278.436,
    "text": "Ben?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2281.777,
    "end": 2284.279,
    "text": " Yeah, I mean, I think you've covered it really, really well.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2284.359,
    "end": 2297.489,
    "text": "But I wish I'd have had access or knowledge of the result, the talk that you just mentioned, because trying to differentiate predictive processing from active inference has been a kind of source of pain and confusion for me over the last few months.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2297.789,
    "end": 2304.274,
    "text": "And I just I thought another way of putting it, perhaps, because I think I know the difference now is essentially some",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2306.115,
    "end": 2327.669,
    "text": " presentations of predictive processing are active inference in the sense that they're highly embodied inactive embedded and so if you have a predictive processing that draws on these four accounts of cognition it tends to just that just is active inference active inference is kind of necessarily inactive and embodied and there's a paper um",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2328.61,
    "end": 2347.616,
    "text": " i should say there's a paper by i i believe it's called wilding the predictive brain it's by andy clark kate knave mark miller and george dean i think which basically takes predictive processing as a theory of neural processing and weaves in these kind of",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2348.581,
    "end": 2354.962,
    "text": " cognitive, these four E narratives really, really well and kind of fleshes out a much more active inference-y picture.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2355.102,
    "end": 2359.003,
    "text": "It's kind of the paper brings predictive process and active inference together really nicely, I think.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2360.584,
    "end": 2360.864,
    "text": "Thanks.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2361.244,
    "end": 2362.064,
    "text": "Great suggestion.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2362.764,
    "end": 2369.746,
    "text": "And just without going into too much detail, if this is the causal graph architecture of a predictive processing system,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2370.857,
    "end": 2378.343,
    "text": " one can imagine that like mu is being a Markov blanket with respect to these two epsilons or so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2378.983,
    "end": 2384.548,
    "text": "So predictive processing doesn't highlight as much the Markov blanket formalism.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2385.128,
    "end": 2393.895,
    "text": "Predictive processing is not a physics for particular systems like FEP is, but it's not incompatible",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2394.935,
    "end": 2399.88,
    "text": " So maybe they're just different fingers pointing at similar or different parts of the moon.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2401.261,
    "end": 2419.859,
    "text": "So there isn't an incompatibility, especially as over the recent years, Active Inference implemented predictive architectures and approaches and predictive processing started to undergo that pragmatic inactive 4E, 5E, whatever infusion",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2421.553,
    "end": 2425.456,
    "text": " which led to them modeling action as a variable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2426.817,
    "end": 2428.718,
    "text": "And so all they did was they just made it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2428.798,
    "end": 2440.826,
    "text": "So instead of the free energy being only about sensory observations and expectations, they just tucked action in and it doesn't radically restructure the architecture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2441.506,
    "end": 2444.468,
    "text": "It just means that action is a variable in these equations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2445.369,
    "end": 2445.529,
    "text": "So 43.1,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2448.712,
    "end": 2474.432,
    "text": " or zero one and two are all good places to look okay um this was a short question internal state and external state were added to the ontology but sometimes um like if you have a quotation mark and then an at it won't bring it up so sometimes you have to type it with an at first okay in our um last 15 minutes",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2479.107,
    "end": 2480.968,
    "text": " Okay, these two are good.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2481.068,
    "end": 2486.53,
    "text": "Maybe we could do a first pass on them and then in the coming week, like add notes and more questions to discuss.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2488.611,
    "end": 2497.714,
    "text": "Bay wrote, if one defines preferred states as expected states, then one can say that living organisms must minimize the surprise of their sensory observations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2498.895,
    "end": 2500.656,
    "text": "I'm not clear on the ontology here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2501.676,
    "end": 2508.719,
    "text": "Is it that preference is the same as expectation or that formally we can use them in the same equation or something else?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2510.742,
    "end": 2518.604,
    "text": " Does active inference make ontological claims like preference is actually expectation just as heat is actually the excitation of molecules?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2519.825,
    "end": 2524.046,
    "text": "More generally, is the free energy in physics just an analogy or is it an ontological assertion?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2525.806,
    "end": 2528.327,
    "text": "Who would like to approach one of the aspects of this question?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2540.615,
    "end": 2569.188,
    "text": " i can i i would like to take a stab but again i guess guess yes please i guess it's not saying that they're the same but that we want like if we're minimizing free energy we're trying to make him the same and but if they're surprised they won't be the same i guess the goal is to get them as close as possible that's that will be my guess on this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2570.794,
    "end": 2571.134,
    "text": " Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2571.855,
    "end": 2572.636,
    "text": "Good insight.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2572.936,
    "end": 2577.961,
    "text": "I'll also point to two ways that expectation is conversationally used.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2579.523,
    "end": 2581.665,
    "text": "The expectation with a fancy E,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2583.459,
    "end": 2585.7,
    "text": " is the expected mean of a distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2585.86,
    "end": 2593.023,
    "text": "Like expected returns might be about the future, but it's actually referring to the mean estimate of a distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2593.603,
    "end": 2596.404,
    "text": "In the Gaussian case, the mean and the mode are the same.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2597.244,
    "end": 2599.285,
    "text": "Mode seeking and mean seeking are equivalent.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2599.905,
    "end": 2602.306,
    "text": "There can also be a variance estimator and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2602.786,
    "end": 2607.308,
    "text": "So the expectation of a Gaussian is like tremendously informative.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2608.456,
    "end": 2613.64,
    "text": " Expectation can also mean things that are predicted about the future.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2617.262,
    "end": 2618.143,
    "text": "That's a conversational.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2618.183,
    "end": 2620.104,
    "text": "What do you expect it to be tomorrow?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2621.265,
    "end": 2622.546,
    "text": "Both can be conjoined.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2623.006,
    "end": 2633.073,
    "text": "Like, what do you expect the temperature to be tomorrow is the expectation of a distribution of the uncertainty of the temperature distribution at the time point tomorrow.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2634.281,
    "end": 2636.382,
    "text": " So they're not exclusive definitions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2637.042,
    "end": 2649.906,
    "text": "It just has to be understood what this means because it'd be like, well, if our predictions about the future are our preferences, you know, then there's all kinds of tangles that, you know, one might find themselves in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2651.887,
    "end": 2652.247,
    "text": "Also, the...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2660.678,
    "end": 2682.739,
    "text": " I'm not remembering which number it was, but the dual usage of the same term to reflect the organism's preferred states and also the centering of the distribution at that time point and other time points.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2682.819,
    "end": 2684.1,
    "text": "And then just like Jessica mentioned,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2684.878,
    "end": 2704.068,
    "text": " This operation of free energy minimization is in service of reducing the divergence between the preferred state and the expectation of the preferred state, which could be set or learned, and then the observations that are coming in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2704.769,
    "end": 2713.293,
    "text": "So in one equation, it leans more towards the English description of a preference, like the organism prefers to be at 72,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2715.43,
    "end": 2719.333,
    "text": " And in the other sense, that is like the expectation of a distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2720.393,
    "end": 2735.263,
    "text": "And there's some observation distribution with a minimization of a divergence such that if the observations were realizing preferences, then those distributions would be aligned.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2738.065,
    "end": 2746.249,
    "text": " So yes, these terms are more like phenomena or natural language tags.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2747.329,
    "end": 2751.551,
    "text": "Also, it'll be awesome in the last two weeks of June.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2753.456,
    "end": 2781.885,
    "text": " um in 46 active inference models do not contradict folk psychology this will be extended extended discussions on like wanting beliefs intentions which variables in active are appropriate for being described that way does it contradict one or the other those are like things that we'll explore um so the equations are just what they are and then in some usages",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2783.297,
    "end": 2799.117,
    "text": " a variable is like the organism's preferred states what it's trying to reduce its diversions between and also that is like where it expects to be and that's what licenses the use of surprise and minimizing surprise",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2802.762,
    "end": 2807.524,
    "text": " because an observation right at the center of the Gaussian is minimally surprising.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2808.144,
    "end": 2809.305,
    "text": "We talked about that in chapter two.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2810.025,
    "end": 2813.327,
    "text": "An observation that's outside of the Gaussian is highly surprising.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2814.427,
    "end": 2830.735,
    "text": "So if we have some sort of distribution of outcomes, we could ask how closely are they aligned with our preferences in terms of surprise due to this dual use of the same variables.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2833.083,
    "end": 2835.144,
    "text": " Does active inference make ontological claims?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2835.485,
    "end": 2836.445,
    "text": "Do theories make claims?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2838.387,
    "end": 2839.167,
    "text": "Do people make claims?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2840.248,
    "end": 2840.508,
    "text": "E.g.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2840.588,
    "end": 2844.331,
    "text": "preference is actually expectation just as heat is actually excitation of molecules.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2847.473,
    "end": 2849.294,
    "text": "Anyone can raise their hand at any time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2852.676,
    "end": 2857.82,
    "text": "We'll talk about that in the folk psychology, but it's an interesting question, Ben.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2860.154,
    "end": 2881.85,
    "text": " Yeah, I wonder if it seems to me like these questions are taking us maybe into the map territory debate that I think we spoke about last time, about how realist or concrete should we think about the ontological claims of active inference.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2881.95,
    "end": 2886.893,
    "text": "Certainly the last part of that question, I think, about whether we should take it as an analogy.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2889.295,
    "end": 2916.183,
    "text": " yeah i think it's quite a big question but i don't i don't know um okay i won't find the slide here but um is the free energy in physics an analogy not sure if this is saying like the gibbs free energy or if that's actually talking about like the variational free energy or the expected free energy which are statistical so i don't know if it's an analogy or an ontological assertion other than",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2918.618,
    "end": 2946.783,
    "text": " just to connect it to like a linear model with a gaussian error distribution is that an ontological assertion that there is a gaussian error distribution in the world no so are statistical quantities ontological assertions about the world maybe there's some nuance but broadly speaking no um and also just like on the math learning group we've",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2947.682,
    "end": 2952.206,
    "text": " expanded our operation due to many active participants.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2952.987,
    "end": 2963.097,
    "text": "So just note that the meetings are at 19 UTC on Monday, Tuesday, Wednesday, and Thursday, and they're in the Discord voice chat, not in this Gather.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2963.898,
    "end": 2969.884,
    "text": "But we've been working on the resources, on the notation,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2971.226,
    "end": 2990.432,
    "text": " basic and background questions as well as overviews of the math for different chapters like basically just summarizing and then um several um people and this is an example of like the broken link it's like variational free energy is deleted so just one time somebody has to come through and just add it back in",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2991.599,
    "end": 3008.162,
    "text": " these are natural language descriptions that will be very easy to um transmute into computer code and translate amongst human languages and provides a lot of legibility and comprehensibility so a lot of the math group has been modifying like the equations",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3009.625,
    "end": 3035.133,
    "text": " and everybody is welcome to join those like discord sessions and contribute with questions or with knowledge and expertise like no matter where somebody is in learning or discussing these equations they are the essence of active inference so questions random notes related thoughts expertise are all essential for us improving our shared understanding and then just in the last five minutes um page 42 they wrote",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3036.88,
    "end": 3051.246,
    "text": " In advanced organisms, preferred states can also extend to learned cognitive goals and go on to say that advanced organisms like humans can achieve preferred states by increasingly abstract social cultural strategies.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3051.346,
    "end": 3057.068,
    "text": "Like they talked about thermoregulation and then all the way up to like air conditioning distribution systems.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3058.229,
    "end": 3062.811,
    "text": "My question is if they are cast in terms of active inference,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3064.565,
    "end": 3080.935,
    "text": " must all of these preferred states and strategies be ultimately linked to survival in some way or in the case of advanced organisms can free energy be related to something other than survival mike and then anyone else",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3083.769,
    "end": 3091.29,
    "text": " Yeah, it feels like this notion of survival in preferred states gets conflated in a sort of odd way.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3092.471,
    "end": 3094.191,
    "text": "Not everything is about survival, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3094.331,
    "end": 3111.314,
    "text": "And so you might have some measurement of surprise if you're in a temperature that's 120 degrees, but it's survivable for a period of time, whereas if you're in a temperature of 300 degrees Fahrenheit, not survivable, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3113.342,
    "end": 3125.324,
    "text": " But if you contrast that with other things that could be equally surprising or outside the distribution, as you described it earlier, the notion of that all swans are white, but then you find a black swan.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3125.464,
    "end": 3136.226,
    "text": "And so you'd never conceived that there might be a black swan before that lives outside your distribution and creates a surprise, but it's not necessarily survival rated.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3138.746,
    "end": 3139.086,
    "text": "Thank you.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3139.546,
    "end": 3142.607,
    "text": "So there's sort of a missing severity in there somewhere.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3144.83,
    "end": 3145.47,
    "text": " Okay, thank you.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3145.69,
    "end": 3146.651,
    "text": "Ali and then Lyle.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3149.112,
    "end": 3160.417,
    "text": "Well, I think work-to-live here is used somehow in a more abstract way, meaning a system that resists dispersion and energy dissipation.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 3160.957,
    "end": 3164.239,
    "text": "So it's not necessarily biological survival.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 3166.803,
    "end": 3195.176,
    "text": " yes great point systems that fail to persist survive will not continue to be that thing and that's just in like a physical sense repeated measurements are only enabled by systems that are persistent at that time scale not eternally and so when we want to have a theory or a framework or a physics for things then they have to look as if they are minimizing free energy otherwise they will simply not be that thing",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3196.227,
    "end": 3197.305,
    "text": " Ben and then Lyle.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3201.551,
    "end": 3205.292,
    "text": " So this was my question, I suppose, just to kind of clarify it quickly.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 3207.313,
    "end": 3227.298,
    "text": "One of the things I wonder most about active influence is it seems to me, at least in my life, I'm quite often making decisions and having preferences that seem so abstracted from my attunement with my environment or my continued viability as a system, like my preference for which mug I'm going to drink coffee out of in the morning.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 3228.163,
    "end": 3235.978,
    "text": " it seems in some sense removed from my free energy, the way that free energy is described in the literature.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 3237.22,
    "end": 3238.282,
    "text": "So that's kind of what I meant.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 3240.441,
    "end": 3258.2,
    "text": " great in yesterday's 45.2 we asked specifically like how can all behavior be optimal when like mistakes are made or Ali asked the question like what about when there's like sacrificial or altruistic behavior and there's",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3259.588,
    "end": 3288.824,
    "text": " great answer in 45.2 but suffice to say that with different priors all behavior can be cast as optimal behavior so the normativity isn't referring to what should be done or what is best but it's rather like a process normativity lyle right so yeah this just for me tees up the question i continue to have about the approach which is how does how does the concept of novelty which",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3289.449,
    "end": 3299.913,
    "text": " some organisms would pursue fit into this concept of, I mean, how would it even be represented here?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3300.493,
    "end": 3312.957,
    "text": "And novelty is not just a human trait, but you would see it across all kinds of creatures that would seek out novel and surprising experience, not expected.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3313.697,
    "end": 3318.239,
    "text": "So for me, it's kind of an open question about how that's captured in this framework.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3319.025,
    "end": 3328.756,
    "text": " Yes, the beginning and ending of 45.2, Carl's focus was on curiosity and there's various ways to approach it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3329.417,
    "end": 3334.723,
    "text": "And it's an excellent question about novelty of different types and scales.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3335.859,
    "end": 3341.542,
    "text": " And within the hierarchical modeling, one can understand novelty of different kinds.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3342.243,
    "end": 3344.104,
    "text": "And this, again, is just like the kernel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3344.224,
    "end": 3346.165,
    "text": "This is like the single linear regression.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3346.846,
    "end": 3354.35,
    "text": "And then there's like model selection across hierarchically nested linear models, much development beyond the kernel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3354.93,
    "end": 3356.051,
    "text": "But this is like a kernel.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3356.911,
    "end": 3362.715,
    "text": "And especially when thinking about expected free energy, a path can move to a novel area",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3364.4,
    "end": 3369.543,
    "text": " because it's part of an expected trajectory that does reduce risk and ambiguity and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3370.083,
    "end": 3374.046,
    "text": "So that's the deflationary approach to novelty pursuit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3374.966,
    "end": 3377.588,
    "text": "Jessica, and then we'll end the discussion.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3382.891,
    "end": 3386.293,
    "text": "On that, at the end, Danny, you mentioned it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3386.433,
    "end": 3391.536,
    "text": "I guess I see novelty as sort of like the systemic origin.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3392.626,
    "end": 3405.616,
    "text": " And so it's like you are seeking like new information, new things in order to minimize that uncertainty.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3405.816,
    "end": 3409.099,
    "text": "Not so much that novelty is uncertainty.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3409.419,
    "end": 3417.165,
    "text": "Because like if you look at novelty and thinking like, oh, it's new and therefore it's,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3418.134,
    "end": 3419.275,
    "text": " going to be surprising.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3420.737,
    "end": 3424.761,
    "text": "And so we want to minimize those things, then yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3425.261,
    "end": 3434.65,
    "text": "Or also if we look at uncertainty equal to risk, we can say like novelty is because of the unknown represented risk.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3434.891,
    "end": 3438.274,
    "text": "And if we minimize that uncertainty, we're trying to minimize risk.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3438.975,
    "end": 3441.357,
    "text": "But from what I saw a little bit on chapter two,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3442.677,
    "end": 3449.679,
    "text": " And it's sort of like, we're not necessarily trying to minimize risk or the unknown.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3450.519,
    "end": 3454.82,
    "text": "And depending on our preferences, we might tolerate high risk.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3455.601,
    "end": 3459.682,
    "text": "And what we're trying to minimize is the surprise of what we expect.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3459.822,
    "end": 3468.244,
    "text": "Like if I'm in finance and I expect very high risk in an investment, but I already know that, and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3469.906,
    "end": 3482.596,
    "text": " you know, and high risk comes back and like my uncertainty was minimized, even though there was a lot of risk or there were a lot of like unknowns or like novelty and things like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3482.656,
    "end": 3485.899,
    "text": "But it was already in my expectation of what was going to happen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3486.519,
    "end": 3492.024,
    "text": "So I was already minimizing that, even though in reality, it may look like, um,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3493.641,
    "end": 3496.723,
    "text": " there was a lot of uncertainty in actuality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3497.824,
    "end": 3501.106,
    "text": "But in my model, there isn't.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3501.266,
    "end": 3504.508,
    "text": "So that's kind of how I explained that to me a little bit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3505.168,
    "end": 3517.816,
    "text": "And in terms of novelty for adventure and curiosity and things like that, I equated more to epistemic foraging as a way of learning and updating the model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3518.036,
    "end": 3519.237,
    "text": "And that's why we're doing it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3520.759,
    "end": 3521.42,
    "text": " Thanks, Jessica.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3521.64,
    "end": 3523.821,
    "text": "I'll just add one closing note on that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3524.442,
    "end": 3537.252,
    "text": "I would argue that it's the pragmatic absolutism, the reinforcement and reward learning centrism that has curtailed an effective theory of novelty or curiosity because",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3538.012,
    "end": 3555.873,
    "text": " radically different kinds of actions have to be coerced into a value framework whether it's deep q learning or reinforcement or anything like that there has to be like a novelty bonus or all these other ad hoc unprincipled approaches they might be super technical they might be super effective but they're not driven from first principles",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3557.313,
    "end": 3586.433,
    "text": " in variational free energy f and expected free energy g there's this like minimum of two with pragmatic value which is to um bring alignment between the preferences and the observations and the epistemic value associated with what is casually called novelty or curiosity at that scale of potentially a hierarchical model so active inference does provide partitioning and an imperative",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3587.173,
    "end": 3605.469,
    "text": " that helps us address the phenomena of curiosity and of novelty in a way that is quite disparate from trying to coerce it into a value of curiosity is expected pragmatic value, or the value of basic research is the expected utility that it could bring.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3606.169,
    "end": 3630.019,
    "text": " so but these are all really awesome and open and ongoing questions so we will um close the recording and then have just a one minute break then in this room we'll continue with dot tools organizational unit and if anybody wants to like talk about something else they can go to a different um gather room so thanks everybody and see you next week thanks",
    "speaker": "SPEAKER_02"
  }
]
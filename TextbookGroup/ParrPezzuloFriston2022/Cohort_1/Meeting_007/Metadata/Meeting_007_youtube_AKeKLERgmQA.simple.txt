SPEAKER_06:
hello everyone it is june 16 2022 and it's meeting seven of the active textbook textbook group cohort one we're in our second discussion of chapter three today we'll be looking at the questions that haven't been addressed so the ones without thumbs

and also it's not too late to add more questions or to upvote questions and then additionally since the math learning group has been quite active in providing synchronous affordances

for a conversation around some of the background on the math, as well as some aspects of the equations that we'll go through today.

And also even to like applications of the math and developing notebooks and such.

We'll take a look at the math in our move through chapter three.

Before we go to the questions, does anyone

Have anything that they just want to like remark on chapter three?

Ali, and then anyone else.


SPEAKER_01:
Yeah, well, last week I talked about how I changed my mind about these bottom-up and top-down characterizations of low-road and high-road, respectively, after reading chapter 3.

But it turns out that they are, in fact, bottom-up and top-down approaches after all.

You see, I came across this, in my opinion, undeservedly lesser-known piece of beautiful, beautiful writing by Carl Friston,

which explicitly argues for such characterizations and the philosophical importance of this distinction.

It's called Beyond the Desert Landscape and it's inconspicuously hidden in this book Andy Clark and his critics.

So I'd like to quote a couple of sentences from this writing.

Well,

He writes, quote, the high road stands in for a top-down approach that starts by asking fundamental questions about the necessary properties things must possess if they exist.

Using mathematical variational principles, one can then show that existence is an embodied exchange of a creature,

with its environment that necessarily entails predictive processing as one aspect of a self-evidencing mechanics.

The low road is the purpose the agenda established by is to pursue the agenda established by Kant and Hellholz and generalize in a bottom-up way the capacity for inference and prediction

to see how far it takes us in understanding embodied exchange with the environment.

So if anyone likes to gain a deeper understanding about these conceptual characterizations, I highly, highly recommend this chapter of this book.


SPEAKER_06:
quite unusually for a first in writing it also lacks any mathematical formalism it doesn't include a single mathematical equation there thank you yeah in a recent stream we also talked about the desert landscape and Clark's concern was that we might do away with what populates our

implicitly assumed to be rich ecosystems of psychology philosophy and so on with important ecosystem players like goals and so the fear or concern was that we might be in a quinian desert landscape if we replace some of these again implicitly or explicitly stated to be rich constructs like goals and we're like well it's just distribution matching

then we've made some sort of like a desert monoculture and that would be um depauperate of what what does what does clark say to his desert living critics um all right if anyone else has a thought they can raise their hand otherwise we can look at the questions okay so this first question

If one defines preferred states as expected states, one can say living organisms must minimize the surprise of their sensory observations.

I'm not clear on the ontology here.

Is it that preference is the same as expectation or that formally we can use them in the same equation?

Is that an ontological claim?

And is the free energy in physics an analogy or an ontological assertion about how things really are?

so one starting place was uh i i found the slide and so wanted to highlight it this was in um free energy a user's guide which was live stream number 37. um the slides will be probably a little bigger um it's worth restating just how unusual it is to interpret p as a measure of both probabilities and preferences they both start with p though

There's nothing wrong with treating a distribution as a measure of preferences because distributions can have different interpretations.

But what's unorthodox and in need of justification is giving the same mathematical term two different interpretations within the same equation.

They then add some more details that unpack a little bit about how it is the case that the same

term p can be utilized as both a probability and a preference and then they even extended it into a third p the ph fitness where by allowing your probabilities and your preferences to converge to reduce the divergence between those two distributions

then fitness is achieved.

And so they provided a restatement of the way that that is implemented and then highlighted an additional grounding or interpretive line.

So that's at least one part of that question.

And also to look towards the equations, it's something that we'll be able to have very clear answers for.

And thanks to the work of Ali and Blue and others who have been annotating equations.

So for example, in equation 3.1, or we can look at like box one, equation one, which is like a Markov blanket definition.

In the description field,

we have a natural language description.

And then the terms that are used in the description are those that are referenced using variables.

So that will help us translate across natural language of different human languages.

The discrete and continuous time formalisms, which we're going to explore in this textbook, also

identical or isomorphic or related formalisms in other papers, which sometimes are using the same notation letters, but not always.

But we could have another column for Fristin 2019.

And here's Fristin 2013.

And then there's a conversation to be had about where they align or don't.

And that's the kind of annotation work that somebody can engage in from publications.

And that also helps us connect it to the programmatic implementations like in active block inference or in other scripts

so that even that we can promote norms and practices around variable naming in math and in programming, but also where there is divergences or differences which are important for innovating and evolving the norms, we can also map them pretty coherently.

So all the equations for three can be read naturally.

So we discussed this a lot in the math group.

And again, a lot of people contributed to this with one of the goals or functions being that whenever we are gonna bring up an equation, like if there's a question about equation 3.1, we can just briefly read the current state of the description.

Because whether you're super familiar with the letters and variables and how they're concatenated,

or whether it's just like a first pass on your seeing this equation everybody will benefit when we align our regime of attention to actually what it says and then that will help us do many many other things but to return back to this question um does anyone want to raise their hand and give a thought on

relationship between preference and expectation ontological claims about preference and its relationship with expectation and more generally is the free energy something that is an analogy or mathematical isomorphism or an equation that rhymes or is it also an assertion


SPEAKER_04:
It's an equation that rhymes.


SPEAKER_06:
What makes you say that?


SPEAKER_04:
No, just because it's cute, that's all.

I just joined the conversation, but preference and expectation, I wouldn't think they were at all referring to the same thing.

I mean, expectation is a mathematical operator.


SPEAKER_06:
preference is a statement yes expectation is used in multiple ways um it can refer to predictions about the mean or the mode or other attributes of a future time point like i'm expecting uh you know what are you expecting to happen on next monday

Also, expectation can refer, and that's how it's used with the fancy E, to refer to the expected value of a distribution, which for a Gaussian is the mode in the center of a symmetric peak.

And that doesn't need to be about a future time point.

That can just be the expectation of the height of the classroom right now.

It's not saying that's what we're expecting it to be in a year, as it might be used conversationally, but it's referring to just the average of a distribution.


SPEAKER_04:
Right.

Mathematically, it's an operator, and verbally, it can be a prediction or a desire or something.


SPEAKER_06:
Yes, yes.

And then, yeah, again, 37 and elsewhere...

It'll be worth returning to this again and again because it's one of the essential architectural decisions in Active Inference that enable... It's part of the necessity and sufficiency complex.

I don't know whether this is the whole meat of it, but it's part of what allows the dispensal of an explicit reward term.

Like a homeostatic reinforcement reward-driven learner would be like, I'm more rewarded by being in my homeostatic range.

And then there needs to be this architecture around, well, how are we going to pursue reward?

Alternatively, an organism trying to maintain homeostasis, realism, or us modeling an organism that is maintaining homeostasis, instrumentalism, we can just say,

It expects to be in the range that conversationally we're going to describe as a preference.

How do we know?

We put the animal in the room and it's spending a lot of its time on this warmer side.

It prefers to be there.

We also expect it to be there.

We're less surprised when we see it there.

So then...

it's possible to, as they write here, interpret that p location distribution in the room for the animal that we're observing or the temperature distribution for the body in the homeostatic context as being a probability density over temperatures, over locations, as well as reflecting the preferences.

not the qualia or the phenomenology or anything like too far out about what it means to prefer where preferences arise from whether they're fixed or learned in the model all these like many many trails that can come out of that but just at this like essential layer we don't need to interpolate a reward term between what is expected and what is preferred

Any other comments from someone?

Yes, Eric, and then anyone else?


SPEAKER_02:
Yeah, just to remark on that, it seems you're still not removing the free variable of what the preference of the agent organism is.

You're just folding it and putting it in a different place so that you can use it in this unified equation.

But you still have this

It's a pre-variable.

What do you want to do?

Okay, I'm going to call that an expectation, and that's what I'm going to try to drive for when I optimize my variational free energy.

But in that sense, it's kind of a convenience or it's a trick.

It's a little bit of an unnatural act.

I don't know the way I think of it for a purpose.

But there's nothing...

you haven't solved any other problem, the deeper problem of what should the organism attempt to achieve, the goals.


SPEAKER_06:
Yes.

Thanks, Ali.


SPEAKER_01:
Well, I think it also might be quite helpful to refer back to Chapter 2 on page 36, I think.

Yeah, page 36.

I think, well, here they have actually...

distinguish between the term prediction and exactly yes here the term prediction and expectation quite explicitly in terms of equation 2.6 so okay so let's look at equation 2.6 okay a few of the we're all um we don't have the third line


SPEAKER_06:
fully described, but just here's the third line.

What does this speak to with respect to preferences and expectations?


SPEAKER_01:
but here the preference actually is in the second term the callback divergence here and so it's actually inherent in the second term and the expected ambiguity as it speaks for itself is the first term so we have here

uh the risk is defined as the divergence between the pre the preference and the states so i think that we can see the preference in this equation as well one note here is like that this g is equal to the first the second and um this line


SPEAKER_06:
but one way that we can understand the relationship of the second and the third line is like, they're very similar.

The first term is identical.

Here, the only difference is that instead of a Y, we see an X. And so this is like X being the hidden state and Y being the observation.

In the case where it's a fully observable Markov process,

then the outcomes and the hidden states are going to be like equal uncertainty about them because in that case it's functionally that the hidden states are observables however if there's any amount of ambiguity like the a matrix between the hidden state and the observable then there's always going to be more uncertainty about the hidden state relative to the observable and then importantly on both sides

there's a conditioning vertical line on policy selection.

And so that is what prevents us from getting into like, well, what if it's 95% correct or 96 or 97% correct or any of these sort of like model diminishing returns debates.

It's a functional that's calculated for each policy that have been,

um possible of composing given the affordances a policy is just a sequence of actions so if there's four movement affordances there's four policies of time depth one there's 16 policies of time depth two and so on and for whatever in this discrete formalism finite set of policies there are they're evaluated here and then there's a policy selection that could choose the lowest

expected free energy or could proportionally choose from them with different kinds of weighting and so on but this is comparing the relative expected ambiguity and risk conditioned upon different specific policies and there's a process by which those specific policies can be enumerated and then this is a process by which they're going to be evaluated and then there's a policy selection by which they're selected so

organisms that fail to do this adequately enough will be dissipated or systems whether it's just a molecular system or some other kind of statistical system that fail to do this well enough to remain being that kind of thing are not amenable to repeated measurements they are not that thing but things that remain being that thing for the time horizon of observations

will be selecting policies that are consistent with this.


SPEAKER_02:
Can I ask a question about this?

Yeah.

About this equation here?

Say, like, the second line.

So I would have expected... So first of all, C, the preferences, is not really spelled out very much in this chapter, and I don't know where it will be.

But I would have expected something more like preferences given...

observations, like, you know, the observations of what's happened in the world that we can measure.

That's observation Y. And how does that fit with my preferences?

So what I want to optimize is my preference, the achievement of my preferences

as delivered by the observations.

Now, I understand that in that second equation, you're gonna do a divergence between observations, the y-tildes.

So you can't flip around P of C given y-tilde there.

So can somebody explain to me how this all fits together then?

How do you achieve preferences under that equation?


SPEAKER_06:
all give one um thought and i absolutely agree to the first point which is like c is just like i don't even know if it's described in this chapter like it just 34 i think 34.


SPEAKER_01:
Let me check it again.


SPEAKER_02:
I mean, there is a... He said this parameter includes preferences, but that's all.


SPEAKER_01:
It includes parameter?

Yeah.

And also, if you go to section 7.4, I think, after 7.

Yes.

In that section, it is untangled why these two parameters are related and how these two parameters are related to each other.


SPEAKER_06:
yeah um section yeah yeah I mean like there's there's that's like multiple scales of where the information is not provided before when it needs to be provided which is like this I have made as just a helpful diagram of the partially observable Markov decision process

which is C is the preferences and it influences how G evaluates policies, which are generated from affordances over a given time horizon.

Policies intervene upon how hidden states change in the world.

And there's more in different readings, but to return to Eric's question, and then again, anyone with the raised hand.

So the question was, whether we're comparing outcomes or states, how does this second term compare

help us realize our preference well when is the kl we're we're trying to minimize g of pi better policies have a lower score it's like um golf i guess

Or you could add negatives and make it higher.

And also just, it's an interesting note, like climbing Mount Improbable, biologists often think about the fitness peak and climbing to fitness peaks.

Whereas for a physicist, it's often framed and also like computer science, gradient descending and local minima, global minima, but they're just actually like the same landscape with some negative science.

So the KL divergence, and here we're trying to minimize.

If it's zero, that would mean like we are perfectly realizing our preference.

So let's just say that there's two policies that we're comparing between.

And one of them is going to perfectly realize our preferences, the other one isn't.

So we have like pi 1 and pi 2.

And we're going to be calculating the KL divergence of the outcomes that we're expecting under the variational distribution queue under policy 1 and under policy 2.

So if policy one is going to perfectly realize our, if the outcomes of policy one are expected to perfectly realize our preferences, this term will be zero.

Whereas if policy two is going to be expected to result in an outcome distribution that's not aligned with our preferences, it doesn't get penalized here.

this is just evaluated as a as a value nor does it get penalized here because in this case of fixed preferences that's also not changing as we iterate through policies however there would be a divergence that would be non-zero like all divergences are between the inferior policy and the superior policy so by iterating over the finite set of policies that we have

taking this term and just leaving it to the side for now, only the KL divergence, this will lead us to select policies that can be expected to bring alignment between the observations conditioned on policy and our preferences, preferred outcome distribution.


SPEAKER_02:
Thank you, that's very helpful.

So just to kind of say that slightly differently is, given your preferences, that's going to imply some outcomes.

And what you're trying to do with that KL divergence is you're trying to get the policy that will achieve those outcomes.


SPEAKER_06:
So if we expect and prefer to be within the homeostatic thermal range, and then the two policies in the snowstorm are add the jacket or remove the jacket, which one of those are expected to

have the minimum divergence between us being in the homeostatic range and then the policy conditioned expected outcomes i'm going to have less divergence it doesn't even mean you're going to live it just means i'm going to have less divergence by having the jacket on in the snowstorm than by taking it off which is going to move the expected outcome distribution further from the preference distribution which here is fixed but doesn't have to be fixed

Okay.

So we have several.

Is anyone here the person who has written any of these questions?

And then we can just go to that one.

Lyle?


SPEAKER_03:
Yes.

There we go.

I think I'm

Okay, so yeah, I wrote the one that is referring to the living organisms and the question about to what extent are these techniques valuable in what I would call social systems of wide-ranging sorts.

And I know that this book at the outset limits itself to

biological systems at least that's my recollection from the introduction but uh it just seems obvious to me that this would have much broader applicability i'm curious what sort of what the state of that is well um do i hesitate to put anyone on the spot john would you give a first response there while i add some resources


SPEAKER_04:
Yeah, my first thought on that is, obviously, our civilization is struggling.

And civilizations have failed in the past, collapsed.

So it's not necessarily the case that every social structure or societal structure is actually performing functional active inference.

But if we were a...

a highly functional organism, if societies were a highly functional organism, we would be doing something like active inference and learning from our mistakes and making sure that we put ourselves in an environment that is sustainable and all that kind of stuff.

So my first thought is, I think that all of this is applicable to societal systems, but it doesn't necessarily mean that


SPEAKER_06:
any existing societal systems actually perform functional active inference yeah thanks it's almost like saying uh okay we're studying the penguins and their thermostats and homeostats

this model could be parameterized to fit a thriving or a failing society along various different dimensions and so in the sense that this is like a linear regression that can capture failing as well as succeeding societies yes the nested markov blanket formalism

has qualitatively but increasingly quantitatively been applied.

And there's multiple startups and applications that are explicitly pursuing this angle.

It's been brought up for several years now.

And in 2020, several of us wrote about it being applied to remote teams and organizations and communities.

I can add these links to decentralized science as a system and ecosystem.


SPEAKER_03:
yes people have started to sketch that continent out and there's a lot more to understand what do you think wow yeah well i think that's that makes a ton of sense i think uh obviously i guess what i was thinking about the question is not so much whether to what extent social systems do this because social systems so you could take two points of view one is you could say

social systems are behaving cognitively in the ways that we're discussing.

In other words, there are preferred states that are driving behavior.

Now, we could say those preferred states are the wrong states for whatever reasons.

So there's a question between what we actually, as in any particular social system,

what's actually happening, which I would argue could be described through this technique, or what they should be doing, which we could also describe through this technique, right?

So I think it's important to have that distinction, and there's a really long history of using dynamic systems models to explore those questions, especially in the business context.

So that's mostly how I've made my living through it.

through the decades.

So that sort of as is to be or would prefer to be sort of question, it just seems like this technique would potentially be a significant advance over the tools in that space that currently exists, which are primarily things like system dynamics and agent-based modeling, stuff like that.

But this would seem to have the potential


SPEAKER_06:
to look at things as they are why is it broken what we might do yes so i have added several other uh references it's important and the book again is like if it's describing the linear model then there's the applications of the linear model and as people and as john brought up like many times in this uh series of live streams that we did together with four with just the two of us with john leading through

basically the entire thread of this discussion of three papers and then two more group discussions there's been recent work on like scripts by Mao and colleagues with a strong and a weak script formalization like a strong script being when the norms are practiced like in a ritualistic way like reading from a script versus the looser script concept which could include just norms and social praxis

And then more recently, they're modeling work using PyMDP to talk about epistemic communities.

And then some of the very earliest live streams that we did were on narrative, human communication, cultural affordances.

So these are absolutely topics that have been addressed, although many of the earlier works are like, here's people in the world.

We're going to use active inference to model it.

then there's so much more beyond that it's almost like a 1950s cybernetics book well we can just imagine that the person is going to be involved in this feedback loop yeah but then that's not the the end of the discussion on that um ali and then john


SPEAKER_01:
I just wanted to mention that FEP can potentially be applied to any dynamical system with ergodicity.

Because you see, any system with ergodic behavior have some points of attraction, some attractors that all the other states are converging to.

So it's pretty general and, well, almost all dynamical systems we know of on different time scales can be potentially modeled with FEP and as it's corollary with active inference.


SPEAKER_03:
That's the key point, right?

Because that's what we look at, the ergodicity.

Tough word for me.

And you can actually look at that even in the context of businesses, business strategy, and so forth, with some understanding of the absorbing barriers, right?

When does a business dissipate?

When it's not close enough to its preferred state, like adequate cash flow.

So, yeah, so I think that that just seems like a really interesting progression from where the state of the art of dynamic, nonlinear dynamic systems

is the way we model businesses now.

So, well, that's the way some of us do it.

A lot of people use a spreadsheet, but this would just seem like a potentially tremendous advancement.


SPEAKER_06:
Yeah.

And then just a few notes then, John, this Stephen Fox, who's also joined on and talked about industrial engineering.

he's recently had this paper come out just a couple days ago and ironically a spreadsheet is provided in this paper but this is also in practice in several places in the world in startup ecosystems and it's very active influenced and then one other more conceptual note um cad cad which is a complex systems modeling software and i know some of you are involved in cad cad learning as well

They describe it as a language for encoding generalized dynamical systems.

And that is what it is.

Active block inference is our project where Jakob has done much of the initial work that is implementing active inference in CAD-CAD.

And we have a whole litany of questions about the relationships between Bayes graphs, generalized dynamical systems,

variational inference, CAD-CAD, which takes more of like a sampling-based approach versus the variational approaches that we're discussing here in Act-Inf.

And it's going to be very interesting as this systems engineering, complex systems simulation, as well as tokenomic and other things becomes integrated with this analytical framework, especially if we can demonstrate a true concordance between active inference and GDS.

John?


SPEAKER_04:
Lyle, I just wanted to say your comments were really spot on, your earlier comments.

You had, I think if I can paraphrase, you had essentially said maybe even a dysfunctional society is performing active inference and then that would lead us to the

really to the key components then of that society or that social system is what is its set of preferred states and where is it placing its attention?

So if an elite group of very wealthy people or powerful people are running the world and they're running it for their preferences, which is to maintain their wealth and to ensure that they stay on top, well, then the world, they could be

performing functional active inference, and the world could suffer the problems that that kind of system would offer.

On the other hand, in a highly democratic system, if people understand that they want clean air and clean water and peace and all that kind of stuff, and then that's their set of preferred states and their attention is on that, and they're performing active inference,


SPEAKER_06:
Thank you, John.

Lyle, then Mike.


SPEAKER_03:
I would just throw, so John, I'm completely on board with that.

I would just amplify a little bit on that.

I would say to a certain extent, preferences and societal decisions are emergent.

So you aren't going to find too many people in the world who are going to say they want to be starving or have war or whatever, right?

And yet we do.

So the the interesting thing is how to me is how does the individual you know, I mean, this is the whole build up, right?

How does the individual behavior somehow create an emergent system that has many bad outcomes?

even though potentially none of those are bad actors some of them are bad actors but let's just take the the simple case there are no bad actors yet we get bad outcome why is that and we can observe that in existing systems and then we can ask the question well you know why is that and then maybe if we understand why that is maybe we can do something to improve yes


SPEAKER_06:
Great point.

And then just a short note there was in our remote teams, we've framed communication and information architectures as organizational affordances.

So that could assist with decision-making find alignment where people do have similar preferences, like a zone of proximate agreement in a negotiation context.

It might be broader, but no matter how broad it is, if you can't forage the path to it, it's going to be a failing system.

Mike.


SPEAKER_00:
Yeah, I just want to add, I think scaling or the scale factor is hugely important here.

So if we're talking about interactions amongst a handful of individuals, we're going to see different behaviors and outcomes than we might see on a global scale of cultural interaction.


SPEAKER_06:
Okay, and then a last comment on that, John, go for it.

If your hands up, I don't hear you actually.

Okay.

Your microphone may have changed or can other people hear John?

No.

Okay.

Eric, go for it.


SPEAKER_02:
Yeah.

How does this all apply to a highly distributed system?

I don't see that.


SPEAKER_06:
Yes.


SPEAKER_02:
Like a society is.


SPEAKER_06:
Good question.

The multi agent case.

is something that hasn't been studied across the board but in 2021 with colleagues we made this is like one of the early it's not the first multi-agent simulation because there were multi-agent simulations however it was the first active inference model involving stigmergy which is the feedback process with agents modifying their environments and so on

We can think about even a nested, distributed cognitive system as inheriting some sparsity from a causal perspective.

Like there's more connections in the Bayes graph within the nestmate's head as we're modeling it.

And then interactions induce like a sparsity between two brains.

And then they're always coupled with the environment and so on.

And that sparsity is a statistical convenience, but also it allows factorizability of models.

And the models may be factorized in a way where free energy calculations could be, for example, summed within a level of collective behavior.

I don't know if that's going... That's not the solution in and of itself to the modeling.

But when we have systems of nested Markov blankets, enclosing systems, enclosing systems...

and then also lateral interactions and interfaces like collective behavior those systems are at least amenable to this kind of principled composition via their sparsity which does map onto a causal architecture or at least a useful one of the real world so also remains to be seen but i think that's a very important question um lyle yeah i would throw eric i would throw in on that that question because


SPEAKER_03:
Society, of course, broadly speaking, is loosely coupled.

However, of course, there are cliques and densely coupled areas that are, you know, to create the well-documented complex connection graphs, right?

So that connection graph is emergent, right?

So the question is, I mean, I think this would be actually a really fascinating research project.

Can we show that FEP would generate connection graphs or patterns of, you know,

societal interactions similar to what has been observed, right?

So there's lots and lots of research on what those graphs look like.

So is it possible for us to, or someone, to use FEP to show how those kinds of connection patterns would emerge?

I think that's a really interesting question.


SPEAKER_06:
Yes, how we threshold and parameterize those causal graphs, how we evaluate counterfactuals on those graphs, and that's related to the problem of structured learning, but Bayesian model selection and other sort of hierarchical Bayesian methods

have the tools there.

Also, just a quick pointer to the project ideas table.

There's some very interesting projects that people have suggested.

Jakob, no big deal.

Sounds like a good one.

But we can absolutely do it, and that's

One of the advantages of being in an activity-driven participatory research and education space is we're hopefully going to be able to help each other identify those projects and where there has or hasn't been overlap, and then scaffold those projects, support them, and then provide them a place to communicate about the project.

playing into the ontology, active block prints, all these pieces that one researcher working by themselves would have just not those affordances.

Let's just try to quickly look at the last several questions.

So, must all preferred states and strategies be ultimately linked to survival in some way?

Or in the case of some organisms, can free energy be related to something other than survival?

So just a quick thought on that.

In the context of repeated measurements, yes, survival is the capacity for repeated measurements.

free energies for domain-specific models can be made that have nothing to do with survival.

And I don't think that there's any contradiction there.

John?

I still don't hear you, actually.

Rohan?

And then anyone else who raises their hand.


SPEAKER_05:
Yeah, hi.

So I was actually wondering, because there's no treatment of time in any of these methods, right?

So is there any accommodation made?

So if you're sampling something, let's say you sample at a minute frequency, you're getting a certain set of events.

Whereas if you're sampling at a much higher frequency, you would get a very different, much more noisy

data set, but that might be essential to capture certain things that might actually enable survival.

Maybe there's certain events that act only at certain times.

So from the perspective of repeated measurement, right?

So I think free energy principle as it relates to survival, could it just be related to some sort of creating an internal clock for repeated measurement?


SPEAKER_06:
great question yes so far it's been almost i'm independent as you suggest but we're going to be dealing with two ways to deal with time in chapter four and beyond and that's like a discrete time formalization where future and past states are explicitly modeled partially observable markov decision process

and then also a continuous time variance where future and past time points are not explicitly modeled.

However, a Taylor series approximation, higher and higher derivatives of a function are utilized to glean an approximation to the future behavior of a function.

Those are two different ways of dealing with time.

And then you talked about like sampling rate and

think uh you'll see many connections and ways to apply that for example you could imagine like if the sensor is every second but you're able to think 10 times per second or what about when the sensor is every 10 seconds but you're only able to think over this time scale and it's those kinds of situations where actually the frequencies are happening differently that we also might be able to address yes rohan


SPEAKER_05:
Yeah, yeah.

Also, like if there are multiple sensors, some of them react quickly, some of them react slowly, or if there's just, or if we look at active inferences, some sort of model averaging phenomenon, right?

So you have a model that runs really fast, that reacts really quickly and produces predictions really quickly, and the other one acts more slowly.

Would those be valid targets in any sense?


SPEAKER_06:
Yes, I think it's a very interesting question.

I'll just add it here just because it was where we were discussing it.

Like about different time scales and the way that cognitive clock rates, for example, and sensory clock rates, and then the time scale events, the statistical regularities in the world happen.

In Model Stream 4.1 with De Vries and colleagues, they talked about the message passing in a factor graph.

So anytime that there's a Bayes graph, there's a way to accomplish variational inference on that Bayes graph using message passing techniques.

That's what their package is about, ForneyLab.

And then what they hinted at towards the end is actually this reactive message passing,

development, which uses reactive programming techniques.

So whereas in message passing, at least in the Forney factor graph formalization, it's like you crank the model and a message is passed amongst every node, or there's like a given execution order of messages that are passed for the entire graph.

However, the reactive message passing paradigm may allow like uncoupling of the frequency of message passing.

So you might have one prior that gets one message per hour, and then there's another distribution that gets one message per minute, and another one is getting one per second.

And even that one could be doing model averaging of something much faster.

So there's many ways that I think your questions will be starting to be explored, Rohan, and then John.

Okay, John, if your mic is back.


SPEAKER_04:
Can you hear me now?


SPEAKER_06:
Oh, good.


SPEAKER_04:
On the question of survival, I think you would have to identify, to answer it, you'd have to identify first, what is the organism or what is the agent that you're talking about?

Because obviously, if it's an individual person, a person might sacrifice their life for others.

So then, you know, survival is not the main, necessarily the main driver.

But if it's a society, then, you know, that's a

than any individual might not survive, but society as a whole might.

I suspect in the bigger picture that it's not really survival of a species.

It's probably the ability to, the propagation of information somehow.

That's what I would guess in the biggest picture.

That's probably what everything is doing is propagating information.


SPEAKER_06:
and mike levin has also explored like how changes in the scope of the self can lead to like an organism with reduced fitness as a function of a cancer with increased fitness and vision so to speak and so like just because there's an imperative or normative way that we model behavior doesn't mean that everything works or anything like that um yes um ali


SPEAKER_01:
Yes, adding to what John said, well, yes, again, I believe here survival doesn't necessarily mean biological survival.

And actually, it has a much broader sense.

And it means the existence of things.

Actually, it's a metaphysical concept here, because

As I said, Carl Friston has defined the existence or the prerequisites of existence as having two parameters or two features namely being an ergodic system and having markup blankets

so any system which has these any system which has these two properties can basically be considered as an existing system or we can call it a surviving system here just to in our last minutes before we go to tools touch on some of these points can markup blanket be considered a decoupling mechanism so can anything mathematical be considered a mechanism


SPEAKER_06:
People use terms in different ways.

So just to raise the point that like a linear regression on height and weight isn't a mechanism of height and weight, it's just a descriptor.

So Markov blanket doesn't need to be any kind of mechanism at all.

It doesn't need to cut nature at the joints or anything like that.

That being said, there are ways in which one could think of the blanket, again, variously, depending on how the real generative model and generative process separated as part of a Markov partition

Sometimes there's ways to think of it as how they're coupled.

Other times it is a statistical decoupling of a type.

It's a partial information encapsulation, but it's not a total encapsulation.

And that way it is like an interface.

And yes, thanks to whomever has added this quote.

This is one reason why the transcripts of all of the live streams and discussions augmented with the ontology will be so helpful and searchable because we could find sentences that have raised similar points and address them.

So good question.

And then just the last ones.

um maybe we can come back to this and then um this one is a a nice next step question thinking about friction what is the analogy to friction in a cognitive system what is what is the stochastic fluctuation on our cognitive path of least action

What is that in the real world, realism, ontological claims?

And then from the behavioral modeling perspective, where are we using these equations and

partitioning variance into a path of or a flow of least action and a variable that is analogous to stochastic fluctuation or friction but doesn't take on any of the baggage of describing some feature or component or mechanism of the physical system so also a great question though and we'll leave this one in case people want to continue to address it so

that brings us to the end of chapter three live meetings next week we'll head into two weeks of chapter four then two weeks of chapter five and then we'll have two weeks to sort of ground ourselves think about what we've learned talk about project ideas and then we'll be heading into the second cohort of the first half of the book

And for those who are here who want to continue into the back nine, as it were, the second half of the textbook.

And also people are welcomed and encouraged to be facilitators or return participants in the second cohort first part, because it's many coats of paint for everyone.

And so we want to provide that affordance to return again and again and again, because the basics are the classics and the essence and the seed of innovation, all this other important things that happen when we return and clarify.

Also, we've had to sort of do all of these entering of figures and equations for the first time.

but now that we have the architecture people can focus on annotating them or they can focus on adding new questions or improving the questions that others have written if anyone else has any comments that they want to make while the recording is still occurring I'll just give a few seconds for anyone to raise their hand

And then we'll end this session and then take a one minute break.

And then in this room, we'll be continuing on with tools, organizational unit.

And if you want to continue speaking with people here about just any topic at all, you can head up to a room up top.

And then of course, always you can join into the Discord voice channels and just be working on things like during the math reading group or any other area.

Rohan, and then anyone else who raises their hand.


SPEAKER_05:
Yeah, so I actually, sorry for this late thought.

So I'm just curious.

So when you say the free energy principle, it's basically figuring out some set point of states, internal states that the agent wants to be with, right?

It's not necessary that it actually couples to the real world.

So why wouldn't it be that

Okay, it just becomes a brain in the vat type of situation.

So it just needs to find a set point that helps it survive.

So it doesn't have to actually model the real world.

It just has to find a place and no matter what happens, just stay there.

So it's not really dynamic.

That could be...

Is that a possibility?


SPEAKER_06:
It is dynamic.

It is explicitly modeling time.

And one can make a model just like a linear regression could be static.

Or you could use the linear regression technique and use it in a time-series analysis.


SPEAKER_05:
I was more referring it.

So if you look at some of the reinforcement, deep reinforcement learning models, they usually find some trick within the game.

Like if you're looking at Atari games and you say maximize the score, for example.

So it tries to find, you know, okay, I can maximize the score by doing the simplest thing that doesn't require me to progress in any way, but it would increase the score.


SPEAKER_06:
Okay.

So yes, these are important things.

So write them down so that we can have infinite discourse on them.

um the brain the vat does still have an interface with the environment whether it's receive only and then you brought up like it doesn't have to be like modeling the quote real world this is the architecture that helps us bypass that and say right the particulate states the particular states which are the blanket states and the internal states are doing inference acting as if they're doing inference on external states

So it's perfectly compatible with scientific realism, scientific surrealism, aesthetic surrealism.

So I mean, the states could be hallucinating.


SPEAKER_05:
It could be hallucinating the world and it's actually just doing inference on the hallucination rather than actual reality as such.


SPEAKER_06:
anil seth and many others who speak of that reality as a controlled hallucination they're they're two feet in that territory without any qualms and feeling like it addresses questions that have been unaddressed by other perspectives okay okay yeah okay so i'll just put this down yeah cool thank you everybody
start	end	speaker	sentiment	confidence	text
170	1550	A	0.7096458673477173	You're okay.
1700	2398	A	0.5208580493927002	Hello.
2564	4430	A	0.9051958918571472	It's September 16.
4850	9760	A	0.7578098773956299	We're in Cohort, one of the active textbook group.
10130	20522	A	0.8687282204627991	It's meeting 15, or I guess yes, no, 16 on let me just update the title meeting 16.
20586	23102	A	0.8994444012641907	We're in our second discussion on chapter six.
23236	40140	A	0.5999124646186829	So are there any any overall comments that people would like to address about chapter six or any other questions or points to add or just areas of chapter six that they found to be interesting.
64610	76740	B	0.8864731192588806	Distinction between the interceptive extraceptive modalities which have been mentioned in box 6.1.
78230	106170	B	0.7975782155990601	And I think Kristen with Michael Allen, if I'm not mistaken, has published a recent paper just today, exactly on that topic, in which they've somehow tried to provide a computational model for the first time for the unification of the interceptive and extraceptive modalities.
106590	135190	B	0.5010948777198792	So since this chapter is also the modeling of those agents, I think this paper, from in the Body's Eye the computational anatomy of interceptive inference might be relevant for anyone who wants to model these modalities with the aid of active inference framework.
136890	137590	A	0.8529649972915649	Thank you.
137660	138326	A	0.9649319648742676	Very nice.
138428	139960	A	0.9237832427024841	Thanks for sharing it.
141370	144360	A	0.906543493270874	Let's look at box 61.
169330	175970	A	0.8682616949081421	As people see it, what distinguishes exteriorceptive proprioceptive and interoceptive modalities?
179260	183480	A	0.6474708914756775	For example, why are proprioceptive and interceptive separated?
204760	207924	B	0.5742834806442261	Because I couldn't hear you.
207962	212292	B	0.6003870964050293	I don't know if it's on my side or connection was unstable from your side.
212346	212950	B	0.5092127919197083	Sorry.
213960	214916	A	0.7213068008422852	Let me try something.
214938	216250	A	0.7990482449531555	I'll be on for a second.
228480	229630	A	0.8754059076309204	Am I back?
230880	231388	A	0.584351658821106	Okay.
231474	231964	A	0.6129430532455444	So, yeah.
232002	232396	A	0.5652865171432495	What?
232498	234620	A	0.7491868138313293	Yeah, brock, I do need a starlink.
236180	239760	A	0.5787791609764099	Why are proprioception and terraception separated?
241060	245408	A	0.6223816275596619	And why are these three categories modeled as different?
245574	247490	A	0.5039522051811218	Why have they been framed as different.
271300	285312	B	0.8495202660560608	Biological and neurological mechanisms, neural mechanisms that somehow control those different kind of perceptions?
285376	318844	B	0.7939172983169556	Because for the extraceptive perception, the organism would necessarily need a kind of sensory I mean, the sensation it gets from the environment is just on the level of the imminent stimuli that it gets from the external environment.
318972	329556	B	0.8398718237876892	But for the interceptive modality, the sensation is just the internal sensations and so on.
329658	337552	B	0.8809322714805603	And for the preceptive, it's related to motor control feedback loop.
337696	352104	B	0.6877245306968689	So I guess because of their different mechanisms it might be helpful to talk in different terms about each of them.
352222	355470	B	0.7114132642745972	But I'm not totally sure about that.
361280	361932	A	0.6408694982528687	I agree.
361986	370560	A	0.8230091333389282	I think one aspect, even in a pre blanket or non blanket modality is like they're about external things versus about internal things.
370710	390720	A	0.7872491478919983	And then between proprioception and interoception, you're absolutely right that they have different neural mechanisms and physiological mechanisms with some blurry space, perhaps.
390880	402730	A	0.8225685358047485	For example, like interoception about blood pressure is somewhere in between, like a muscular reflex and an organ reflex, but the tissues being addressed are different.
403500	407348	A	0.716492772102356	Let's look at this paper again.
407454	424530	A	0.8742468953132629	We'll return to the book, but we're going to be thinking today and then especially going forward with like, how do you identify the let's go to this question, which we just look at it.
425300	427200	A	0.8361095786094666	Which system are we modeling?
428420	430930	A	0.8845044374465942	What is the appropriate form for the generative model.
431300	433428	A	0.8810838460922241	How do we set up a generative model?
433594	435590	A	0.8808903694152832	How do we set up a generative process?
436040	440340	A	0.8790189623832703	So let's see if we can determine some of those aspects.
440760	452200	A	0.8547346591949463	In this paper, hashtag figure 43, it is the very same.
454810	469286	A	0.8652945160865784	Okay, this is just a general pointing to a partially observable Markov decision process with a slight plus computational biology graphical veneer to match the color scheme.
469398	471920	A	0.6435592770576477	Probably not, but it kind of looks like it.
475730	484820	A	0.8654066324234009	Here is where we see the schematic of the form of the generative model.
488070	497490	A	0.8818619847297668	Variables are used to reflect that those variables can be fit from empirical data or just a priori set and simulated.
497650	500040	A	0.7568633556365967	But this is the form of the model.
500410	512860	A	0.8800011873245239	These are the shapes of the matrices and tensors that go from that are going to address what their model is about.
515230	520060	A	0.7021430134773254	Does anybody want to describe what they see in this?
522190	529870	A	0.816191554069519	What do you think they were trying to explain and what do you think they were trying to model?
530020	533280	A	0.8807748556137085	What is their system of interest based upon what you see here?
538790	550626	B	0.9196950197219849	Actually, one interesting term that I came up with used in this paper is cardiac active inference.
550818	565990	B	0.8832988739013672	The term that they've used in this paper to describe the kind of interrelationship between the heart rhythm and the interception.
566150	579530	B	0.8682717680931091	So this generative model, I believe, is just about modeling that relationship between the heart rhythm and the interception.
579610	599750	B	0.8577905297279358	So we can see, as usual, the B matrices C and A matrices that we've seen before, but in this case, they've been applied to the variation in the heart rhythm and how it relates to the interception.
601210	601960	A	0.46103888750076294	Yes.
602330	622400	A	0.8561417460441589	So again, using the variables that we know from figure four three in the book and their figure one here, the hidden states that are not directly observed are on the interceptive side relating to the state of the heart.
623410	629470	A	0.866363525390625	On the exteriorceptive side, whether there is a flower or a spider.
632130	645300	A	0.7527018785476685	The outcomes that are observed are the visual exterocception of a flower or a spider and the heart rate probably a low and a high heart rate.
647610	665174	A	0.8413087725639343	The piece that links these two is the sensory attenuation between the high heart rate state and the mapping of exterocception in the B matrix.
665302	670140	A	0.6863943934440613	You can just see that even the allocation of the symbols is different.
671410	679470	A	0.8232709169387817	Like when you're aroused, when you're in, this is like heart open and then heart contract.
680450	686980	A	0.7638015747070312	So when you're in the aroused situation, b is how the hidden states change their time.
687670	704360	A	0.7670994997024536	You're always going to go from here to here, whereas in the relaxed situation, sometimes you go from this state to this one.
706650	711340	A	0.5924035906791687	Other times you're relaxed and so you dwell within that state.
711870	717180	A	0.7891061305999756	So here it's like pump rest, pump rest, pump rest.
717630	720630	A	0.6634013652801514	Here it's 50% slower.
720710	732750	A	0.5847874283790588	Just it depends on the parameters, but it's 50% slower because sometimes you're going to go from one, but it looks like as it's here that it's actually like or it's twice as slow.
733430	735086	A	0.7445229887962341	Rest, rest, beat.
735198	736770	A	0.7445229887962341	Rest, rest, beat.
737110	740020	A	0.8401898145675659	So these are the two interoceptive states.
742950	753640	A	0.6947372555732727	One has a preference for having relaxed hearts when flowers are around and having contracted heart when spiders are around.
755290	769610	A	0.888171374797821	The A matrix maps on these two sensory modalities between hidden states and outcome states, just like any A matrix would do in any POMDP.
770430	776080	A	0.8618431687355042	And then there's also a transition matrix of the generative process.
777170	788274	A	0.7264878153800964	So it's a very simple generative process where either flowers are flowers or spiders are spiders, as they would say.
788312	791646	A	0.8750733733177185	You can use standard routines to simulate these techniques.
791678	793540	A	0.7685145735740662	I hope that the code are available.
795030	802050	A	0.9046949148178101	You can plot parameter sweeps as you explore the beta and alpha.
803370	807030	A	0.8668131828308105	So here's, like the beta parameter and alpha parameter.
808170	816726	A	0.9125460386276245	So you can do parameter sweeps with this grid search or other approaches and then do statistics on those outputs.
816918	819174	A	0.8689584732055664	Is this quote active inference?
819302	823600	A	0.8379480242729187	Well, it's the output of a parameter sweep on an active inference model.
824850	826586	A	0.8828332424163818	There's various other kinds of methods.
826618	841780	A	0.8593213558197021	So, not to spend too much time on the paper, but does anyone have any thoughts on that case or how they see it applying to chapter six as content?
850980	857570	A	0.7527859807014465	I think it speaks to a fulfillment cycle in active inference research.
859540	860290	A	0.5443294644355774	Oh.
886900	889890	B	0.9369065761566162	This worked example helps a lot.
891860	916344	B	0.8642295002937317	I mean, if anyone wants to, especially because describing this kind of cardiac active inference as a kind of system that has a definite Markov blanket, it was really interesting question at the first place.
916462	943330	B	0.9507425427436829	And I think they've done really great job, obviously, in that regard and thinking really creatively and out of the box about how we can describe that whole system with the aid of using markup blankets to describe its internal and external environment and so on.
945140	949750	A	0.8733962774276733	It also relates to this earlier question.
950200	961348	A	0.8750373125076294	So how does the figure 43 paradigm discrete and continuous state spaces with all the attending complexities?
961524	962920	A	0.8167968988418579	Do you hear me, Ollie?
964860	973050	A	0.8294661641120911	Okay, how does this representation connect to.
974460	976512	B	0.8582078218460083	Can you hear, Daniel?
976676	978110	A	0.7155882716178894	Yeah, I can.
980640	982492	A	0.7981547713279724	It's a story for another day.
982626	986960	A	0.9474614858627319	Some of the incredible, the truly incredible Internet.
987380	988544	A	0.7349585890769958	I'm going to try one thing.
988582	990928	A	0.7908497452735901	So I think that or Holly, can.
990934	991376	B	0.7293274998664856	You hear me now?
991398	992130	B	0.5889445543289185	Or no.
995680	996910	A	0.6965872645378113	I'll try something.
1020430	1024422	A	0.8844387531280518	It here's what's interesting with the Wi Fi.
1024486	1039200	A	0.508727490901947	And then we'll continue, because it's not a Wi Fi learning group, but with the Hotspot, there are a set of people who I cannot interact with, but with WiFi, I am able to interact with those people.
1041190	1042450	A	0.8055286407470703	Just a note.
1042790	1047970	A	0.8714190125465393	So we see the POMDP representation.
1049030	1063280	A	0.49220946431159973	Now, what is not in this paper here is a graphical layout of the POMDP.
1065220	1065970	A	0.7123860716819763	Okay?
1066340	1067856	A	0.5105802416801453	What is not in this paper?
1068038	1073780	A	0.8878575563430786	A figure six one, or indeed a figure eleven representation.
1077370	1083750	A	0.7224785089492798	So why is that's?
1089710	1099902	B	0.6878361105918884	I'm not sure I've understood your question quite clearly, but could you please elaborate on that a little bit more?
1100036	1100814	A	0.7282710671424866	Yes, sure.
1100932	1111838	A	0.8346038460731506	So when we look at papers like Vassal at all, communication is active inference social, inactive, active inference, loops, loops, loops, viotkin at all remote teams.
1111934	1119140	A	0.6649090647697449	Loops, loops, loops, Markov blanket interface loops, entities, things.
1121610	1130600	A	0.7197640538215637	Yet in the application, what is specified is not the loopy situation.
1131450	1137260	A	0.6371768712997437	What needs to be specified is chapter six, not chapter one.
1138590	1160450	A	0.8922404050827026	So I find that very interesting because so much of the focus attention, discourse has been around abstractions, around Markov blankets and their interpretations from various stances and about boundaries, physical boundaries and conceptual boundaries.
1162710	1169540	A	0.7585949301719666	But it's a non controversy when it's written this way.
1170710	1177426	A	0.5661102533340454	And there are Markov blankets between many of these variables because it's a Bayesian graph that's not fully connected.
1177458	1180754	A	0.7084366083145142	So every Bayesian graph that's not fully connected has Markov blankets.
1180882	1181750	A	0.7604264616966248	Trivially.
1183310	1193260	A	0.6300987601280212	But this paper doesn't need to describe what is.
1193630	1197078	A	0.8442739248275757	This is enough to say what's inside and outside the organismal.
1197094	1212290	A	0.8382208943367004	Markov blanket, if we want to even think about it, is like here in this line, kind of not on the Bayes graph, but it's between the spider and the flower outside and the vision of the spider and the flower.
1213270	1240342	A	0.8372280597686768	But that's like taken care of with the a matrix of the exterocceptive modality, which is, I think, why this recipe does not focus on which thing are we modeling, but rather which system are we modeling.
1240486	1247680	A	0.5032327771186829	And yes, we're going to pursue the generative model before the generative process, even though they may have symmetry or they may be very similar.
1250050	1279670	A	0.7942046523094177	But I just find it very interesting, open to hearing what people think, like why the Cybernetic loop action perception loop is used so much in contrast with the form of the model that's actually implemented, which is a different topology in a different discussion.
1281930	1284310	A	0.8353034257888794	I know that they're integrated and compatible.
1295060	1327340	B	0.863325297832489	Just one comment on that is, you see, at least to my understanding, markup blanket is a kind of modeling tool or a way of thinking about basically the ontology of I mean ontology in the sense of its philosophical use, the ontology of the system that we're about to model.
1327490	1346736	B	0.8261991143226624	So it's not necessarily the relationships among the different components of the markup blanket is something not necessarily physical or tangible relationships in that system.
1346838	1365848	B	0.8909915685653687	So it might be kind of just informational, as we saw in, I think, chapter five, if I'm not mistaken, the information passing among the different components and so on.
1365934	1396240	B	0.648689329624176	So at least I think the reason for not using the Schematic model of markup I mean the Cybernetic model of Markup blanket instead of using just the Schematic form, probably it's because of this kind of, let's say nonphysical or nontangible relationship among the different components.
1396660	1409780	B	0.6368350386619568	By nontangible and nonphysical I mean something that can't be explained in terms of its ontological existence.
1412120	1418650	B	0.5528445839881897	I don't know if I could express myself in that regard or not.
1421100	1424090	A	0.8563863039016724	Thank you, Elite, Brock and anyone else.
1427680	1450130	C	0.7634298205375671	I was just going to I don't know, I feel like personally, I'm just kind of at the affordances that I have to get pragmatic and epistemic value here without doing some modeling of some kind.
1454200	1455670	C	0.5627530217170715	Yeah, I don't know.
1456680	1457430	A	0.7732179760932922	Good.
1458920	1459572	A	0.6283750534057617	Thanks.
1459706	1467656	A	0.8118482828140259	I mean, this is the second discussion on chapter six, and then this is setting us up to actually pursue a model.
1467758	1470090	A	0.8277165293693542	So we should be preparing these.
1470940	1471592	C	0.5491447448730469	Yeah.
1471726	1475370	C	0.5709861516952515	I'm just saying it's definitely ripe for that.
1477740	1478536	A	0.46103888750076294	Yes.
1478718	1479800	A	0.7415806651115417	Rohan.
1481740	1495710	D	0.8670713305473328	Yeah, actually, just to get some clarification, you're asking why the form of the model is as mentioned in chapter six of the project given in chapter six, which is chapter one.
1499360	1500084	A	0.46103888750076294	Yes.
1500242	1502432	A	0.6320534348487854	I mean, they're compatible.
1502496	1508260	A	0.8478652238845825	Topologies that actually are describing an overlapping set of notes.
1510040	1530540	A	0.586537778377533	I was just calling attention to the amount of discourse and focus on the potentially more familiar, more schematic action perception loop and the way that that draws so much attention to the organism environment interface.
1531440	1543090	A	0.7956584095954895	Whereas when it comes to the empirical modeling, that interface never had to be specified as like a first class type.
1544580	1546850	A	0.8287190198898315	It just was included by the buy.
1547300	1556740	A	0.7835357189178467	We just said there are spiders and flowers out there, hidden states, s, and then there's observations of and there's no error.
1558920	1559824	A	0.7392863035202026	There's no error.
1559872	1560180	D	0.5664746165275574	Right.
1560250	1565128	D	0.7967228293418884	But there would be no error in the generative process itself.
1565214	1575772	D	0.6864951848983765	The model might have error that comes from some measurement issue.
1575826	1576524	D	0.7360090017318726	Right?
1576722	1585736	D	0.5625013709068298	So when you measure it and you confuse, say, spider for flower or flower for spider, then you would get some error.
1585768	1594530	D	0.5438117980957031	And there may be that's where I guess you can talk about the markup blanket misspecifying or misobserving something.
1595460	1599664	D	0.6761085987091064	So I'm not sure what exactly is.
1599702	1600290	B	0.5665541291236877	The.
1602420	1604530	D	0.5457839965820312	I mean, they're compatible, right?
1605800	1607600	D	0.7150520086288452	In this case, there's no error.
1607760	1612790	D	0.7827141880989075	But if there was error, obviously there would be something wrong with the.
1614700	1617112	A	0.5249459743499756	Let me actually just correct this.
1617166	1622132	A	0.6322051286697388	I believe be I didn't mean to say this is the visual error.
1622196	1626940	A	0.7947235703468323	This is saying that they're unchanging flowers, so they're either one or the other.
1627090	1627788	A	0.571090042591095	Here.
1627954	1632220	A	0.6037189364433289	Parametrically it's possible to introduce visual error in the A matrix.
1632640	1641730	A	0.7579017281532288	But I'm just pointing to the architecture of the model doesn't require even engaging with this.
1643460	1645212	A	0.8554955720901489	It's totally compatible.
1645356	1664010	A	0.7441965341567993	And the observations are the sense states and the actions are the active states and so on, but as presented and as constructed, if we're going to use a POMDP architecture, we don't necessarily even need a representation like this.
1668780	1669530	D	0.5491447448730469	Yeah.
1671260	1674010	D	0.9349290728569031	Jessica okay, that's interesting.
1678480	1678940	E	0.5325649380683899	Hi.
1679010	1679630	E	0.46103888750076294	Yes.
1680960	1694096	E	0.5090866088867188	For me, the reason why it's like two parts, the reason why the craft is so popular, and I think maybe it's also the reason why it was used.
1694198	1700944	E	0.7425718903541565	And this is the simplicity to show the concept and the relationship like the new one.
1701142	1708852	E	0.5027030110359192	It's great for, I guess, for the paper that it was created, but it doesn't show the relationship as clear as the other ones.
1708906	1710084	E	0.5111649036407471	It's very simple.
1710282	1713140	E	0.84170001745224	So I think that's why it was used.
1713210	1720730	E	0.716288685798645	Maybe like, remove a lot of extra information that the second paper illustrates in it.
1721180	1726740	E	0.7040404677391052	But the simplicity of that graph is why it really encapsulates the concept.
1726900	1730524	E	0.8791518211364746	And it's also the reason why it's so popular, because it's so simple.
1730722	1736750	E	0.8550646305084229	I think that's the relationship, I think, from how I see it.
1738880	1740412	A	0.853069543838501	Yes, totally makes sense.
1740466	1741330	A	0.8529649972915649	Thank you.
1742260	1753288	A	0.8141720294952393	And once we see that there is a mapping from the Schematic cybernetic view, then we can probably fill in a lot of the parts of the PMDP.
1753324	1763830	A	0.7577020525932312	But just to look one more level here, I posted the code below, so it's in MATLAB which people may or may not have access to.
1764200	1764612	A	0.5092127919197083	Sorry.
1764666	1765076	A	0.584351658821106	Okay.
1765178	1766840	A	0.7627984881401062	Ali and then Ron.
1770140	1775290	B	0.67839115858078	No, I just wanted to point out one thing that I forgot to mention before.
1777100	1791548	B	0.539915919303894	One thing that might be helpful to keep in mind is that actually, in this generative model, there is a kind of unification, or kind of integration of interceptive and extraceptive modalities.
1791644	1812544	B	0.8425203561782837	Because, as they pointed out in the paper, this agent that they're modeling adopts two modes of engagement with the world, one of which is the relaxed and the other one is the aroused mode of engagement.
1812672	1819784	B	0.8295274376869202	And because of that, there are two sorts of hidden states related to each interceptive and extraceptive one.
1819982	1841732	B	0.6466810703277588	So I guess if they wanted to put that kind of interrelationships between these kind of modalities in that simplified schematic model, it wouldn't necessarily clarify these kind of integration between the modalities.
1841816	1852290	B	0.8785752058029175	So one reason might be related to that peculiar situation of this particular case here.
1853300	1854096	A	0.6283750534057617	Thanks.
1854278	1859668	A	0.8617591857910156	It also speaks to the step in the model of the form of a generative model.
1859834	1868372	A	0.8147329092025757	And then I think when people get married to the cybernetic scheme, you can imagine, well, then there's two nodes inside of the internal states.
1868426	1870040	A	0.7927144169807434	There's two modes you can go between.
1870190	1880750	A	0.8031913638114929	But now you're like, mixing and matching what arrows mean, where some of the arrows are reflecting causal influence and some of them are reflecting transitions between mutually exclusive states.
1881120	1898188	A	0.5187785029411316	And so then we end up back into the wireframe nightmare, where it's like a schematic, where some edges are tactile connections and some are causal links and some are mutually exclusive transfers of information and all these other ways that people visualize models.
1898364	1905472	A	0.8239826560020447	And this representation reminds us that we're dealing with variables and states and their connections.
1905616	1906580	A	0.6628338694572449	Jessica.
1911020	1912648	E	0.8384183049201965	I think that was from before.
1912814	1913530	E	0.584351658821106	Okay.
1916780	1928124	E	0.8894481658935547	I think the visual has to do with what are you trying to convey in that moment and how much depth of information do you need?
1928322	1933616	E	0.572715699672699	I think, like, at least for this one, you need an extra information so that all the model wouldn't work.
1933798	1938816	E	0.8543875217437744	Well, just share with the idea of what the model is.
1938918	1945072	E	0.5699707865715027	Maybe this version with the extra information might be too complex for the other one.
1945126	1946560	E	0.4702882766723633	There's a better purpose.
1947000	1952960	E	0.8965954780578613	What is the purpose you're trying to accomplish, and what do you want to communicate?
1953120	1959720	E	0.5210464596748352	And depending on that, then models with more information or less information are more successful.
1961260	1961912	A	0.46103888750076294	Yes.
1962046	1962728	A	0.8529649972915649	Thank you.
1962814	1963512	A	0.570740818977356	Totally agree.
1963566	1966792	A	0.5886136293411255	I mean, they didn't do any of the loop at all.
1966846	1970040	A	0.7979550957679749	They just went straight to the PMDP representation.
1970200	1976510	A	0.8114762902259827	And also, this isn't the only form of an interoceptive exterocceptive integration model.
1976880	1979432	A	0.7331603765487671	One could brainstorm a whole host.
1979496	1981632	A	0.7399218678474426	And again, that's what the recipe is for.
1981766	1994070	A	0.8979417681694031	Well, we could have a mutually exclusive transfer between these two states defined by these two B matrices, which entails a policy variable switching between the two.
1994520	1998160	A	0.8667365312576294	Or we could have a continuous variable called arousal.
1998320	2002756	A	0.7440398931503296	And arousal is engaged in some sort of attenuation relationship here.
2002938	2007450	A	0.829395055770874	So this isn't even the only form, but it shows us a form.
2010700	2018910	A	0.7956942319869995	Okay, soon, in 20 minutes, we're going to be heading into chapter seven.
2019680	2034130	A	0.8424002528190613	So let's preview it so that when we're working on this in the next week, preparing our questions, starting to think about what we want to be modeling, we'll kind of know what's going to happen.
2034820	2038624	A	0.8527727723121643	All right, chapter seven is active inference in discrete time.
2038662	2039904	A	0.6502225399017334	People can still hear me okay.
2039942	2040560	A	0.5470303893089294	I think so.
2040630	2049220	A	0.8096694350242615	All right, so here is where we cross the rubicon with modeling.
2052250	2054520	A	0.8452441096305847	This is abstract up till now.
2056590	2058170	C	0.6547338962554932	You cut out there for a second.
2058320	2059020	A	0.584351658821106	Okay.
2059630	2060940	A	0.8648590445518494	It's all good now.
2062110	2062522	C	0.5491447448730469	Yeah.
2062576	2064442	C	0.7941016554832458	Just start from the review.
2064496	2065002	A	0.8529649972915649	Thank you.
2065056	2073280	A	0.7726808190345764	Chapter seven, we're crossing the rubicon together, and it's been abstract up till now.
2074850	2079178	A	0.7914193272590637	First, what is addressed is perceptual processing.
2079354	2092980	A	0.7556607127189636	You'll find a lot of resonance between this chapter seven and Step by Step, which is model stream one.
2093350	2095090	A	0.7864255905151367	Maybe it's Stephen.
2098730	2112118	A	0.8544615507125854	This paper step by step tutorial is going to do a certain walkthrough of adding incremental complexity into the model in terms of model parameter depth.
2112214	2112810	A	0.584351658821106	Okay.
2112960	2114106	A	0.7967024445533752	It's model stream one.
2114128	2117100	A	0.9122053980827332	There's four streams that Ryan and Chris White did.
2117790	2126030	A	0.8219223022460938	Static perception, dynamic perception, dynamic perception with preferences and free energy minimization.
2126930	2133730	A	0.8337780833244324	Dynamic perception with free energy minimization and flexible policy selection through uncertainty modulation.
2134790	2137780	A	0.7368272542953491	We're going to do something similar in chapter seven.
2138470	2142962	A	0.6892967224121094	They don't do that upper left corner of step by step.
2143016	2148710	A	0.8849035501480103	They go straight to hidden Markov models for dynamic perception.
2149130	2155314	A	0.9019635319709778	This is analogous to a Coleman filter or any other number of General Bayesian filtering schemes.
2155442	2163690	A	0.9043431282043457	We have a prior D hidden states S and a sequence of observables through time T that are mapped to hidden states.
2163760	2167034	A	0.8466426134109497	A B describes how hidden states change through time.
2167232	2170486	A	0.8558269739151001	So we have the temperature is a hidden variable.
2170678	2176782	A	0.8929683566093445	And then we're getting observations from the thermometer that are mapped to the real temperature by A.
2176836	2179518	A	0.5976707339286804	Maybe it's a good thermometer, maybe it's biased or whatever.
2179684	2182000	A	0.7432458400726318	And then there's the temperature changing through time.
2184370	2189940	A	0.7572214007377625	Then there's a very Ollie Centric discussion of listening to music.
2190870	2194870	A	0.9568701386451721	This has to be one of the most glaring errors in the textbook.
2196810	2202440	A	0.706082284450531	It's very warhol like, though, so maybe it has a certain aesthetic to it.
2203530	2207900	A	0.8176719546318054	But the example that's going to be provided is like a musical listening example.
2208510	2220960	A	0.9008167386054993	So the A matrix is emitting notes based upon the hidden state, which is the score of the music.
2222770	2227466	A	0.6803900599479675	The B matrix is and they have 70% accuracy.
2227578	2230590	A	0.8858398795127869	So this one 10th distributes through the matrix.
2231010	2233566	A	0.8693544268608093	So it's just to use, like, integers here.
2233748	2238946	A	0.6686221361160278	But really this is like zero 7.1.1 .1, because it has to add up to one.
2239128	2247154	A	0.8355598449707031	But when this person, when the hidden state of the score is in the first note, they play it 70% of the time.
2247192	2250054	A	0.7612869739532471	And then the other three notes, they make an error 10% of the time.
2250092	2254402	A	0.6579984426498413	Each time, 70% of the time, the musician hits their note.
2254546	2257654	A	0.8208524584770203	The B matrix is how things change through time, the hidden state.
2257692	2260250	A	0.8196365237236023	So this is the transitions in the score.
2260670	2263990	A	0.7143992781639099	And here there's a 97% probability.
2264070	2269370	A	0.8926621079444885	Again, it's distributed throughout with this one over 100 just to represent it with integers.
2269810	2275840	A	0.833034336566925	And there's a prior with no uncertainty or ambiguity of just it begins in the first position.
2277650	2289646	A	0.5728416442871094	And so this we can even see, like do do you know, I don't want to use ABCD, but it would be like BCDA in the musical ontology.
2289838	2293538	A	0.819837749004364	So here's BCCDA like that.
2293704	2298822	A	0.7157703638076782	But then there is an error in this one simulation run.
2298876	2309130	A	0.6828117966651917	It'd be probabilistic, but they're showing for didactic reasons, a run where the third point in the third observation, there is an error.
2309790	2312940	A	0.7550944089889526	However, it's heard as the correct value.
2317230	2319402	A	0.669253408908844	That's a pure inference example.
2319536	2321930	A	0.5083297491073608	This is not active inference.
2322290	2325790	A	0.586516797542572	This is just Bayesian models of perception.
2331010	2334910	A	0.9751867651939392	This kind of sessile creature is rather uninteresting.
2335990	2347700	A	0.8700889945030212	Okay, now we move from a hidden Markov model crotching tiger, hidden Markov, into a partially observable Markov decision process.
2348230	2355446	A	0.8008137345314026	A hidden Markov model is a partially observable Markov model because some parts of it are hidden, some are not.
2355548	2358710	A	0.8471001982688904	But the key piece here is the introduction of the decision.
2359390	2369340	A	0.8822416067123413	We know that we model that with policy that influences how states change their time, and that policy selection is governed by g.
2370350	2376640	A	0.7940416932106018	Figure four three people can still hear because I see all these, like, frozen for me.
2378530	2379838	A	0.5435476899147034	Okay, all right.
2379924	2380606	A	0.7883909344673157	Okay, thank you.
2380628	2400280	A	0.4806499183177948	Yeah, it's just strange stuff here's four three again, here when we're talking about policy selection, we're talking about states that haven't happened yet, observations, and hidden states that haven't happened yet.
2400650	2409480	A	0.8767328262329102	We see a restatement of equation 2.6 with expected free energy here, broken into epistemic and pragmatic value.
2413180	2426040	A	0.879621148109436	Now, to flesh out this POMDP, we turn to a classic par and Friston example, which is the teammase.
2428800	2438000	A	0.7372742891311646	We're not going to go through every single part of this teammase right now, but it'll be a key area for us to unpack.
2439060	2442530	A	0.6546008586883545	Why do the matrices have the dimensions that they have?
2445220	2454116	A	0.9166492819786072	Where are the four locations that the mouse can be here, down at the bottom, to the left or to the right?
2454298	2456900	A	0.6877977252006531	So this is not like a mouse moving in continuous space.
2456970	2459204	A	0.8266285061836243	It's like there are four locations it can be in.
2459242	2460580	A	0.7862787842750549	Those are the columns.
2461580	2480750	A	0.8291276693344116	There's five hidden states of location which we can see are like, based upon whether when it goes down to the bottom to get a queue, whether the hidden state is going to reveal the queue or not.
2482160	2486828	A	0.8512299656867981	And then figure seven, four and 75 you'll see differ.
2486914	2490480	A	0.7876097559928894	First off, one of them is bolded, this one is bolded, this one's unbolded.
2490900	2496848	A	0.7254257202148438	But here we see a one in the L column because the black is on the L.
2497014	2501716	A	0.8174175024032593	And here we see a one in the R column because the black is on the R.
2501898	2519210	A	0.8411170244216919	So understanding why that is is going to be the crux of understanding how to construct an epistemic informative q as a hidden environmental variable in a generative model.
2520220	2524620	A	0.8361005187034607	Here are the B matrices transition probabilities.
2525600	2527740	A	0.8214110732078552	Here's the preference vector.
2528160	2531084	A	0.9066493511199951	We'll talk about what C one and C two are.
2531282	2535790	A	0.9383034110069275	Here are the priors we'll talk about what D one and D two are.
2536820	2543010	A	0.6670114398002625	Now there's more unpacking of the epistemic value itself.
2543780	2555030	A	0.8473047018051147	So in the equation above there was a restatement of equation 2.6 which partitions the expected free energy into the negative epistemic value and the pragmatic value.
2555400	2575880	A	0.8995270729064941	Now we're going to follow up on the epistemic value I of pi the epistemic value of a policy and break that down into these representations.
2580330	2587430	A	0.8676892518997192	Here are simulations, simulated epistemic and pragmatic behavior of a rat foraging in a teammates.
2588490	2591590	A	0.881837010383606	So we'll try to model what these runs are.
2591740	2607010	A	0.8062148094177246	I think it might be one small note about the book that especially if they knew that it was going to be black and white, which it seems like that was known, this is not ultra helpful.
2608630	2610370	A	0.57435142993927	Perhaps people disagree.
2611270	2612914	A	0.6843836307525635	But another war.
2613112	2616950	C	0.5573300719261169	What said another warhol?
2618490	2622550	A	0.6008081436157227	Yeah, this one you can even see the Campbell soup.
2624090	2637420	A	0.844330370426178	But I think if someone were familiar with reading these traces, they would still want to understand what these different lines were and like what the dashed one is.
2638350	2654820	A	0.5409443974494934	But if people are unfamiliar reading these kinds of error residual traces, I don't know what is being shown here figure 7.7.
2655270	2660898	A	0.630211353302002	That was the simulation of the rat getting the epistemic queue and going to the left.
2661064	2670754	A	0.6547542214393616	So instead of just taking a 50 50 shot on where the food was, the rat navigates this explore exploit dilemma.
2670882	2682460	A	0.8880719542503357	It engages in an epistemically oriented action or an action driven by epistemic value to reduce uncertainty, get the queue and then it secures the food.
2684990	2696270	A	0.9048066139221191	In this box 71 we see a discussion about parameterization of uncertainty and how precision is the inverse of variance.
2696770	2707540	A	0.8802531361579895	And this omega is the precision variable here, just like Mark Solms uses, but others use different ones.
2710710	2716520	A	0.6540493369102478	The lower this value is, the more even things are.
2717370	2724390	A	0.8256293535232544	And so this has come up in the ice cream example with the folk psychology live stream.
2724810	2743930	A	0.8092464804649353	Like if your c vector is zero six negative six versus zero 500 negative 500, that's like a sharper preference distribution.
2744750	2754114	A	0.8702440857887268	So it's the quantitative values in the C, for example, that specified not just the direction but also the intensity of preference.
2754262	2760320	A	0.8761411309242249	And then analogously precision can be specified in a continuous form.
2761410	2772878	A	0.9118366241455078	Here is a reference to an earlier par and Friston work on vision that has to do with ocular motor foraging, which is heavily epistemically driven.
2773054	2788994	A	0.6717023253440857	But then they are using this example to explore how precision the viewer shows a version to the upper left square when it is specified with a less precise, more ambiguous likelihood.
2789042	2797210	A	0.4964233934879303	Mapping the lower left square is epistemically attractive when the transition probabilities are specified as more uncertain.
2801770	2804310	A	0.8300347924232483	We'll try to understand these examples.
2805130	2816970	A	0.8985387682914734	Learning a Novelty 7.5 here we take that POMDP and we push it back in the genealogy.
2817470	2825418	A	0.6916002035140991	Now we have priors on the hidden states which are themselves equipped with prior beliefs.
2825594	2831050	A	0.8797720670700073	These are dear Schlee conjugate priors, but they can be of other forms.
2831130	2842020	A	0.7628644704818726	But they're just shown as dear Schlee here because it's like a convenient statistical distribution family for variational inference which we explored in several chapters ago.
2843510	2850898	A	0.8574344515800476	These can be considered as hyper priors, but in the end just variables on the graph.
2851074	2857000	A	0.5266655683517456	And it's not like they're tagged with being a hyper prior and you can imagine priors that nest on them.
2858190	2875546	A	0.6510306596755981	Exploring these potentially open ended Bayesian graph structures is why it's so important to have structure learning, like Karl has pointed out for years, and why we want to have parameter sweeps across cognitive model structures in active block prints.
2875658	2876878	A	0.5618176460266113	Because why stop?
2877044	2880080	A	0.8042007684707642	Why not have another prior on this one?
2880930	2884500	A	0.815073549747467	So we should have the notation and the capacity to sweep that.
2886550	2889970	A	0.9103271961212158	Here is a discussion on conjugate priors.
2891430	2898578	A	0.8849056959152222	A conjugate prior means that when used to perform Bayesian inference, the posterior belief will be the same type of distribution.
2898754	2906470	A	0.9058679342269897	So previously we talked about how like the hidden state could be a continuous variable and then the internal state could be like a discrete variable.
2906970	2911814	A	0.8845483660697937	The temperature is continuous outside and then inside is it hot or is it cold?
2911862	2913434	A	0.6407209038734436	Is like what I'm caring about.
2913632	2916780	A	0.7045981884002686	So the families don't have to match.
2917310	2927902	A	0.6087365746498108	But when we use conjugate priors it makes it so that where you can design it to play nice.
2928036	2940820	A	0.6199108958244324	It does play nice, but we'll work through that also with anybody who wants to go into more detail on the math because these are just like Bayesian statistics points.
2943510	2946498	A	0.7543323040008545	Here theta is used to just describe parameters.
2946594	2957814	A	0.7818381190299988	It's a very common notation, but not 100% pervasive to use theta for parameters, just generally here there's more unpacking of the update rules.
2957862	2962060	A	0.8377699255943298	This is probably of secondary consideration for just constructing a model.
2966470	2976520	A	0.8697465658187866	Here we return to expected free energy and several more representations beyond what was seen above and beyond what was seen in equation 2.6.
2978730	2989130	A	0.9211421012878418	Here the learning model that was presented above the POMDP with another generation of genealogy is described which is drawn from these two papers.
2991150	2993930	A	0.8734531998634338	Then structured learning is introduced.
2994270	2996134	A	0.8108239769935608	We'll see how deep this one goes.
2996192	3001310	A	0.940459668636322	But these papers are quite interesting and connected to Bayesian model reduction.
3003330	3009600	A	0.9169590473175049	Here a hierarchical model is presented here's figure 43 in the dashed box.
3010450	3013762	A	0.7850093245506287	Now there's a figure 43 yo dog.
3013816	3015620	A	0.7557834386825562	I heard you liked figure four, three.
3018230	3028070	A	0.9096887111663818	Then there's another hierarchical model simulation from the above showing belief updating again with these trace diagrams.
3030330	3033430	A	0.703903079032898	And then there's a summary.
3034670	3040170	A	0.8340371251106262	So it's a somewhat long chapter.
3040910	3050000	A	0.860558032989502	It contains like, the music example, the rat example, the maze example, the eye example.
3051170	3061794	A	0.8888435959815979	So we're seeing like, data points in model space, um, in the last three minutes.
3061912	3063810	A	0.8573772311210632	Does anyone have any thoughts or questions?
3063880	3066100	A	0.9237990379333496	How are they going to tackle chapter seven?
3067110	3071380	A	0.854659378528595	And as they start to construct their model, if they want to?
3079940	3108072	C	0.6572141647338867	I am just every direction that I can because it is quite dense still and kind of unapproachable, I think, for me anyways, it just is quite difficult to get a grasp on it from the textbook specifically at this point, like the generative model, like, what does that mean in code?
3108126	3113550	C	0.8555924296379089	And where does that correlate to this particular function or that particular time step or whatever?
3115520	3116076	C	0.5666733384132385	I don't know.
3116098	3117310	C	0.641782283782959	It's just a lot.
3117760	3119950	C	0.7290142178535461	And so I'm just trying to.
3122900	3123650	B	0.639332115650177	From.
3124100	3148296	C	0.8200864195823669	Yeah, I don't know, from just generally building up simulations, which are just things that you can put together in a browser, like with simple JavaScript to Cadcad and active block prints and these sorts of things and trying to grasp at some real world situation.
3148398	3166344	C	0.6107439398765564	So maybe that rat Tmaze is a really good place to focus on for me, like a good intersection between the book and something that isn't like the frog jumping or is it not jumping is so contrived.
3166392	3177328	C	0.7997759580612183	It's really difficult to think about modeling that and in a way that would help explain what's going it's like, yes, I knew the frog was jumping or not jumping, so that doesn't really help.
3177414	3178096	C	0.5664746165275574	Right.
3178278	3192150	C	0.8149034380912781	Whereas the rat maybe coming up with some left or right hidden state based on the queue that's partially there's some explanatory power there that may be realized, I think.
3194200	3195190	C	0.5666733384132385	I don't know.
3196440	3204680	C	0.718001127243042	I'm trying to bridge jump over the Grand Canyon of the philosophical conceptual okay, got it.
3204750	3206708	C	0.758579432964325	But the math there's.
3206804	3207160	C	0.584351658821106	Okay.
3207230	3211630	C	0.7152630090713501	But there's a gap there for me, so I'm trying to pull them together.
3214160	3214572	D	0.5491447448730469	Yeah.
3214626	3229120	D	0.8611769676208496	I have a question related to calibration of those priors and among other things, is that ever explained as to how to go about doing that for any of these models?
3229460	3230210	A	0.46103888750076294	Yes.
3230660	3243376	A	0.7575451731681824	So model based data analysis is going to describe all right, chapter seven describes just in our last minute.
3243408	3246064	A	0.7943945527076721	Chapter seven describes the discrete state models.
3246192	3248692	A	0.8119080662727356	Chapter eight describes continuous time models.
3248836	3254004	A	0.8863871693611145	Chapter nine describes using empirical data for model driven data analysis.
3254132	3255720	A	0.8434261083602905	Chapter ten is a summary.
3256060	3257610	A	0.6824788451194763	The textbook is.
3259740	3261832	D	0.9609568119049072	Yeah, we're excited towards.
3261886	3265960	A	0.7960213422775269	The Grand Canyon, but I think, again, we have several months to unpack this together.
3266030	3268424	A	0.9451910853385925	So I hope that we can go quite far with this.
3268462	3274924	A	0.6144151091575623	So please stay engaged and add your questions and start building from both sides.
3274972	3282930	A	0.7731416821479797	Play with the code examples provided, break them and then start from the other side with a recipe on a situation that you care about, and then they're going to meet in the middle.
3284740	3286850	A	0.9268144965171814	Okay, thank you, everyone.
3288180	3289040	A	0.5942321419715881	Farewell.
3292500	3293344	D	0.5990492701530457	Bye, everyone.
3293462	3293740	A	0.5137447118759155	Bye.

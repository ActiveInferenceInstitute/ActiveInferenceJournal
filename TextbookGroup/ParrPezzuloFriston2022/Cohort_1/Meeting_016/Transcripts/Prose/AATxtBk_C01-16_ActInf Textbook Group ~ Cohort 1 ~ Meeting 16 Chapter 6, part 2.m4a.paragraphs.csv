start	end	paragNum	speaker	confidence	startTime	wordCount	text
170	20522	1	A	0.15427	00:00	33	You're okay. Hello. It's September 16. We're in Cohort, one of the active textbook group. It's meeting 15, or I guess yes, no, 16 on let me just update the title meeting 16.
20586	40140	2	A	0.99301	00:20	44	We're in our second discussion on chapter six. So are there any any overall comments that people would like to address about chapter six or any other questions or points to add or just areas of chapter six that they found to be interesting.
64610	138326	3	B	0.93177	01:04	106	Distinction between the interceptive extraceptive modalities which have been mentioned in box 6.1. And I think Kristen with Michael Allen, if I'm not mistaken, has published a recent paper just today, exactly on that topic, in which they've somehow tried to provide a computational model for the first time for the unification of the interceptive and extraceptive modalities. So since this chapter is also the modeling of those agents, I think this paper, from in the Body's Eye the computational anatomy of interceptive inference might be relevant for anyone who wants to model these modalities with the aid of active inference framework. Thank you. Very nice.
138428	144360	4	A	0.75989	02:18	9	Thanks for sharing it. Let's look at box 61.
169330	175970	5	A	0.8405	02:49	11	As people see it, what distinguishes exteriorceptive proprioceptive and interoceptive modalities?
179260	183480	6	A	0.9999	02:59	8	For example, why are proprioceptive and interceptive separated?
204760	216250	7	B	0.99826	03:24	31	Because I couldn't hear you. I don't know if it's on my side or connection was unstable from your side. Sorry. Let me try something. I'll be on for a second.
228480	234620	8	A	0.85	03:48	14	Am I back? Okay. So, yeah. What? Yeah, brock, I do need a starlink.
236180	247490	9	A	0.9823	03:56	22	Why are proprioception and terraception separated? And why are these three categories modeled as different? Why have they been framed as different.
271300	352104	10	B	0.88428	04:31	100	Biological and neurological mechanisms, neural mechanisms that somehow control those different kind of perceptions? Because for the extraceptive perception, the organism would necessarily need a kind of sensory I mean, the sensation it gets from the environment is just on the level of the imminent stimuli that it gets from the external environment. But for the interceptive modality, the sensation is just the internal sensations and so on. And for the preceptive, it's related to motor control feedback loop. So I guess because of their different mechanisms it might be helpful to talk in different terms about each of them.
352222	355470	11	B	0.99999	05:52	7	But I'm not totally sure about that.
361280	407348	12	A	1.0	06:01	80	I agree. I think one aspect, even in a pre blanket or non blanket modality is like they're about external things versus about internal things. And then between proprioception and interoception, you're absolutely right that they have different neural mechanisms and physiological mechanisms with some blurry space, perhaps. For example, like interoception about blood pressure is somewhere in between, like a muscular reflex and an organ reflex, but the tissues being addressed are different. Let's look at this paper again.
407454	435590	13	A	0.90535	06:47	65	We'll return to the book, but we're going to be thinking today and then especially going forward with like, how do you identify the let's go to this question, which we just look at it. Which system are we modeling? What is the appropriate form for the generative model. How do we set up a generative model? How do we set up a generative process?
436040	452200	14	A	0.9998	07:16	22	So let's see if we can determine some of those aspects. In this paper, hashtag figure 43, it is the very same.
454810	471920	15	A	0.99	07:34	36	Okay, this is just a general pointing to a partially observable Markov decision process with a slight plus computational biology graphical veneer to match the color scheme. Probably not, but it kind of looks like it.
475730	484820	16	A	1.0	07:55	14	Here is where we see the schematic of the form of the generative model.
488070	512860	17	A	0.79075	08:08	51	Variables are used to reflect that those variables can be fit from empirical data or just a priori set and simulated. But this is the form of the model. These are the shapes of the matrices and tensors that go from that are going to address what their model is about.
515230	520060	18	A	0.99997	08:35	10	Does anybody want to describe what they see in this?
522190	533280	19	A	0.5694	08:42	31	What do you think they were trying to explain and what do you think they were trying to model? What is their system of interest based upon what you see here?
538790	601960	20	B	0.99915	08:58	96	Actually, one interesting term that I came up with used in this paper is cardiac active inference. The term that they've used in this paper to describe the kind of interrelationship between the heart rhythm and the interception. So this generative model, I believe, is just about modeling that relationship between the heart rhythm and the interception. So we can see, as usual, the B matrices C and A matrices that we've seen before, but in this case, they've been applied to the variation in the heart rhythm and how it relates to the interception. Yes.
602330	629470	21	A	0.66995	10:02	52	So again, using the variables that we know from figure four three in the book and their figure one here, the hidden states that are not directly observed are on the interceptive side relating to the state of the heart. On the exteriorceptive side, whether there is a flower or a spider.
632130	645300	22	A	1.0	10:32	27	The outcomes that are observed are the visual exterocception of a flower or a spider and the heart rate probably a low and a high heart rate.
647610	704360	23	A	0.76	10:47	93	The piece that links these two is the sensory attenuation between the high heart rate state and the mapping of exterocception in the B matrix. You can just see that even the allocation of the symbols is different. Like when you're aroused, when you're in, this is like heart open and then heart contract. So when you're in the aroused situation, b is how the hidden states change their time. You're always going to go from here to here, whereas in the relaxed situation, sometimes you go from this state to this one.
706650	735086	24	A	0.99997	11:46	62	Other times you're relaxed and so you dwell within that state. So here it's like pump rest, pump rest, pump rest. Here it's 50% slower. Just it depends on the parameters, but it's 50% slower because sometimes you're going to go from one, but it looks like as it's here that it's actually like or it's twice as slow. Rest, rest, beat.
735198	740020	25	A	0.99644	12:15	10	Rest, rest, beat. So these are the two interoceptive states.
742950	791646	26	A	1.0	12:22	85	One has a preference for having relaxed hearts when flowers are around and having contracted heart when spiders are around. The A matrix maps on these two sensory modalities between hidden states and outcome states, just like any A matrix would do in any POMDP. And then there's also a transition matrix of the generative process. So it's a very simple generative process where either flowers are flowers or spiders are spiders, as they would say. You can use standard routines to simulate these techniques.
791678	819174	27	A	0.92	13:11	53	I hope that the code are available. You can plot parameter sweeps as you explore the beta and alpha. So here's, like the beta parameter and alpha parameter. So you can do parameter sweeps with this grid search or other approaches and then do statistics on those outputs. Is this quote active inference?
819302	841780	28	A	0.9929	13:39	49	Well, it's the output of a parameter sweep on an active inference model. There's various other kinds of methods. So, not to spend too much time on the paper, but does anyone have any thoughts on that case or how they see it applying to chapter six as content?
850980	860290	29	A	0.97	14:10	13	I think it speaks to a fulfillment cycle in active inference research. Oh.
886900	961348	30	B	1.0	14:46	111	This worked example helps a lot. I mean, if anyone wants to, especially because describing this kind of cardiac active inference as a kind of system that has a definite Markov blanket, it was really interesting question at the first place. And I think they've done really great job, obviously, in that regard and thinking really creatively and out of the box about how we can describe that whole system with the aid of using markup blankets to describe its internal and external environment and so on. It also relates to this earlier question. So how does the figure 43 paradigm discrete and continuous state spaces with all the attending complexities?
961524	978110	31	A	0.95928	16:01	19	Do you hear me, Ollie? Okay, how does this representation connect to. Can you hear, Daniel? Yeah, I can.
980640	991376	32	A	0.99978	16:20	31	It's a story for another day. Some of the incredible, the truly incredible Internet. I'm going to try one thing. So I think that or Holly, can. You hear me now?
991398	992130	33	B	0.95	16:31	2	Or no.
995680	996910	34	A	0.88596	16:35	3	I'll try something.
1020430	1063280	35	A	0.73288	17:00	70	It here's what's interesting with the Wi Fi. And then we'll continue, because it's not a Wi Fi learning group, but with the Hotspot, there are a set of people who I cannot interact with, but with WiFi, I am able to interact with those people. Just a note. So we see the POMDP representation. Now, what is not in this paper here is a graphical layout of the POMDP.
1065220	1073780	36	A	0.99	17:45	17	Okay? What is not in this paper? A figure six one, or indeed a figure eleven representation.
1077370	1083750	37	A	0.99787	17:57	4	So why is that's?
1089710	1119140	38	B	0.66851	18:09	57	I'm not sure I've understood your question quite clearly, but could you please elaborate on that a little bit more? Yes, sure. So when we look at papers like Vassal at all, communication is active inference social, inactive, active inference, loops, loops, loops, viotkin at all remote teams. Loops, loops, loops, Markov blanket interface loops, entities, things.
1121610	1160450	39	A	0.89007	18:41	58	Yet in the application, what is specified is not the loopy situation. What needs to be specified is chapter six, not chapter one. So I find that very interesting because so much of the focus attention, discourse has been around abstractions, around Markov blankets and their interpretations from various stances and about boundaries, physical boundaries and conceptual boundaries.
1162710	1193260	40	A	0.99523	19:22	50	But it's a non controversy when it's written this way. And there are Markov blankets between many of these variables because it's a Bayesian graph that's not fully connected. So every Bayesian graph that's not fully connected has Markov blankets. Trivially. But this paper doesn't need to describe what is.
1193630	1247680	41	A	1.0	19:53	114	This is enough to say what's inside and outside the organismal. Markov blanket, if we want to even think about it, is like here in this line, kind of not on the Bayes graph, but it's between the spider and the flower outside and the vision of the spider and the flower. But that's like taken care of with the a matrix of the exterocceptive modality, which is, I think, why this recipe does not focus on which thing are we modeling, but rather which system are we modeling. And yes, we're going to pursue the generative model before the generative process, even though they may have symmetry or they may be very similar.
1250050	1279670	42	A	1.0	20:50	45	But I just find it very interesting, open to hearing what people think, like why the Cybernetic loop action perception loop is used so much in contrast with the form of the model that's actually implemented, which is a different topology in a different discussion.
1281930	1284310	43	A	1.0	21:21	7	I know that they're integrated and compatible.
1295060	1409780	44	B	0.99738	21:35	170	Just one comment on that is, you see, at least to my understanding, markup blanket is a kind of modeling tool or a way of thinking about basically the ontology of I mean ontology in the sense of its philosophical use, the ontology of the system that we're about to model. So it's not necessarily the relationships among the different components of the markup blanket is something not necessarily physical or tangible relationships in that system. So it might be kind of just informational, as we saw in, I think, chapter five, if I'm not mistaken, the information passing among the different components and so on. So at least I think the reason for not using the Schematic model of markup I mean the Cybernetic model of Markup blanket instead of using just the Schematic form, probably it's because of this kind of, let's say nonphysical or nontangible relationship among the different components. By nontangible and nonphysical I mean something that can't be explained in terms of its ontological existence.
1412120	1418650	45	B	1.0	23:32	13	I don't know if I could express myself in that regard or not.
1421100	1424090	46	A	0.99969	23:41	7	Thank you, Elite, Brock and anyone else.
1427680	1450130	47	C	0.98	23:47	36	I was just going to I don't know, I feel like personally, I'm just kind of at the affordances that I have to get pragmatic and epistemic value here without doing some modeling of some kind.
1454200	1470090	48	C	0.98	24:14	34	Yeah, I don't know. Good. Thanks. I mean, this is the second discussion on chapter six, and then this is setting us up to actually pursue a model. So we should be preparing these.
1470940	1475370	49	C	0.71	24:30	9	Yeah. I'm just saying it's definitely ripe for that.
1477740	1495710	50	A	0.95	24:37	34	Yes. Rohan. Yeah, actually, just to get some clarification, you're asking why the form of the model is as mentioned in chapter six of the project given in chapter six, which is chapter one.
1499360	1543090	51	A	1.0	24:59	72	Yes. I mean, they're compatible. Topologies that actually are describing an overlapping set of notes. I was just calling attention to the amount of discourse and focus on the potentially more familiar, more schematic action perception loop and the way that that draws so much attention to the organism environment interface. Whereas when it comes to the empirical modeling, that interface never had to be specified as like a first class type.
1544580	1556740	52	A	0.74911	25:44	29	It just was included by the buy. We just said there are spiders and flowers out there, hidden states, s, and then there's observations of and there's no error.
1558920	1576524	53	A	0.86689	25:58	27	There's no error. Right. But there would be no error in the generative process itself. The model might have error that comes from some measurement issue. Right?
1576722	1600290	54	D	0.93918	26:16	49	So when you measure it and you confuse, say, spider for flower or flower for spider, then you would get some error. And there may be that's where I guess you can talk about the markup blanket misspecifying or misobserving something. So I'm not sure what exactly is. The.
1602420	1622132	55	D	0.58	26:42	43	I mean, they're compatible, right? In this case, there's no error. But if there was error, obviously there would be something wrong with the. Let me actually just correct this. I believe be I didn't mean to say this is the visual error.
1622196	1645212	56	A	1.0	27:02	45	This is saying that they're unchanging flowers, so they're either one or the other. Here. Parametrically it's possible to introduce visual error in the A matrix. But I'm just pointing to the architecture of the model doesn't require even engaging with this. It's totally compatible.
1645356	1664010	57	A	0.52	27:25	40	And the observations are the sense states and the actions are the active states and so on, but as presented and as constructed, if we're going to use a POMDP architecture, we don't necessarily even need a representation like this.
1668780	1674010	58	D	1.0	27:48	5	Yeah. Jessica okay, that's interesting.
1678480	1708852	59	E	0.57396	27:58	71	Hi. Yes. For me, the reason why it's like two parts, the reason why the craft is so popular, and I think maybe it's also the reason why it was used. And this is the simplicity to show the concept and the relationship like the new one. It's great for, I guess, for the paper that it was created, but it doesn't show the relationship as clear as the other ones.
1708906	1730524	60	E	0.53938	28:28	52	It's very simple. So I think that's why it was used. Maybe like, remove a lot of extra information that the second paper illustrates in it. But the simplicity of that graph is why it really encapsulates the concept. And it's also the reason why it's so popular, because it's so simple.
1730722	1736750	61	E	0.59	28:50	12	I think that's the relationship, I think, from how I see it.
1738880	1764612	62	A	0.92	28:58	61	Yes, totally makes sense. Thank you. And once we see that there is a mapping from the Schematic cybernetic view, then we can probably fill in a lot of the parts of the PMDP. But just to look one more level here, I posted the code below, so it's in MATLAB which people may or may not have access to. Sorry.
1764666	1766840	63	A	1.0	29:24	5	Okay. Ali and then Ron.
1770140	1841732	64	B	0.86	29:30	135	No, I just wanted to point out one thing that I forgot to mention before. One thing that might be helpful to keep in mind is that actually, in this generative model, there is a kind of unification, or kind of integration of interceptive and extraceptive modalities. Because, as they pointed out in the paper, this agent that they're modeling adopts two modes of engagement with the world, one of which is the relaxed and the other one is the aroused mode of engagement. And because of that, there are two sorts of hidden states related to each interceptive and extraceptive one. So I guess if they wanted to put that kind of interrelationships between these kind of modalities in that simplified schematic model, it wouldn't necessarily clarify these kind of integration between the modalities.
1841816	1870040	65	B	0.99991	30:41	64	So one reason might be related to that peculiar situation of this particular case here. Thanks. It also speaks to the step in the model of the form of a generative model. And then I think when people get married to the cybernetic scheme, you can imagine, well, then there's two nodes inside of the internal states. There's two modes you can go between.
1870190	1906580	66	A	0.99999	31:10	90	But now you're like, mixing and matching what arrows mean, where some of the arrows are reflecting causal influence and some of them are reflecting transitions between mutually exclusive states. And so then we end up back into the wireframe nightmare, where it's like a schematic, where some edges are tactile connections and some are causal links and some are mutually exclusive transfers of information and all these other ways that people visualize models. And this representation reminds us that we're dealing with variables and states and their connections. Jessica.
1911020	1913530	67	E	1.0	31:51	7	I think that was from before. Okay.
1916780	1946560	68	E	0.74	31:56	76	I think the visual has to do with what are you trying to convey in that moment and how much depth of information do you need? I think, like, at least for this one, you need an extra information so that all the model wouldn't work. Well, just share with the idea of what the model is. Maybe this version with the extra information might be too complex for the other one. There's a better purpose.
1947000	1963512	69	E	0.99998	32:27	35	What is the purpose you're trying to accomplish, and what do you want to communicate? And depending on that, then models with more information or less information are more successful. Yes. Thank you. Totally agree.
1963566	1981632	70	A	0.98	32:43	46	I mean, they didn't do any of the loop at all. They just went straight to the PMDP representation. And also, this isn't the only form of an interoceptive exterocceptive integration model. One could brainstorm a whole host. And again, that's what the recipe is for.
1981766	2007450	71	A	0.99647	33:01	60	Well, we could have a mutually exclusive transfer between these two states defined by these two B matrices, which entails a policy variable switching between the two. Or we could have a continuous variable called arousal. And arousal is engaged in some sort of attenuation relationship here. So this isn't even the only form, but it shows us a form.
2010700	2040560	72	A	0.99	33:30	68	Okay, soon, in 20 minutes, we're going to be heading into chapter seven. So let's preview it so that when we're working on this in the next week, preparing our questions, starting to think about what we want to be modeling, we'll kind of know what's going to happen. All right, chapter seven is active inference in discrete time. People can still hear me okay. I think so.
2040630	2049220	73	A	0.53	34:00	12	All right, so here is where we cross the rubicon with modeling.
2052250	2054520	74	A	1.0	34:12	6	This is abstract up till now.
2056590	2064442	75	C	0.52	34:16	18	You cut out there for a second. Okay. It's all good now. Yeah. Just start from the review.
2064496	2095090	76	A	0.56954	34:24	45	Thank you. Chapter seven, we're crossing the rubicon together, and it's been abstract up till now. First, what is addressed is perceptual processing. You'll find a lot of resonance between this chapter seven and Step by Step, which is model stream one. Maybe it's Stephen.
2098730	2126030	77	A	1.0	34:58	52	This paper step by step tutorial is going to do a certain walkthrough of adding incremental complexity into the model in terms of model parameter depth. Okay. It's model stream one. There's four streams that Ryan and Chris White did. Static perception, dynamic perception, dynamic perception with preferences and free energy minimization.
2126930	2155314	78	A	0.99795	35:26	59	Dynamic perception with free energy minimization and flexible policy selection through uncertainty modulation. We're going to do something similar in chapter seven. They don't do that upper left corner of step by step. They go straight to hidden Markov models for dynamic perception. This is analogous to a Coleman filter or any other number of General Bayesian filtering schemes.
2155442	2179518	79	A	0.99736	35:55	67	We have a prior D hidden states S and a sequence of observables through time T that are mapped to hidden states. A B describes how hidden states change through time. So we have the temperature is a hidden variable. And then we're getting observations from the thermometer that are mapped to the real temperature by A. Maybe it's a good thermometer, maybe it's biased or whatever.
2179684	2182000	80	A	1.0	36:19	8	And then there's the temperature changing through time.
2184370	2220960	81	A	0.99997	36:24	71	Then there's a very Ollie Centric discussion of listening to music. This has to be one of the most glaring errors in the textbook. It's very warhol like, though, so maybe it has a certain aesthetic to it. But the example that's going to be provided is like a musical listening example. So the A matrix is emitting notes based upon the hidden state, which is the score of the music.
2222770	2247154	82	A	1.0	37:02	64	The B matrix is and they have 70% accuracy. So this one 10th distributes through the matrix. So it's just to use, like, integers here. But really this is like zero 7.1.1 .1, because it has to add up to one. But when this person, when the hidden state of the score is in the first note, they play it 70% of the time.
2247192	2263990	83	A	1.0	37:27	51	And then the other three notes, they make an error 10% of the time. Each time, 70% of the time, the musician hits their note. The B matrix is how things change through time, the hidden state. So this is the transitions in the score. And here there's a 97% probability.
2264070	2298822	84	A	0.85871	37:44	76	Again, it's distributed throughout with this one over 100 just to represent it with integers. And there's a prior with no uncertainty or ambiguity of just it begins in the first position. And so this we can even see, like do do you know, I don't want to use ABCD, but it would be like BCDA in the musical ontology. So here's BCCDA like that. But then there is an error in this one simulation run.
2298876	2312940	85	A	0.64598	38:18	30	It'd be probabilistic, but they're showing for didactic reasons, a run where the third point in the third observation, there is an error. However, it's heard as the correct value.
2317230	2325790	86	A	0.97354	38:37	17	That's a pure inference example. This is not active inference. This is just Bayesian models of perception.
2331010	2369340	87	A	0.85	38:51	82	This kind of sessile creature is rather uninteresting. Okay, now we move from a hidden Markov model crotching tiger, hidden Markov, into a partially observable Markov decision process. A hidden Markov model is a partially observable Markov model because some parts of it are hidden, some are not. But the key piece here is the introduction of the decision. We know that we model that with policy that influences how states change their time, and that policy selection is governed by g.
2370350	2409480	88	A	0.99799	39:30	72	Figure four three people can still hear because I see all these, like, frozen for me. Okay, all right. Okay, thank you. Yeah, it's just strange stuff here's four three again, here when we're talking about policy selection, we're talking about states that haven't happened yet, observations, and hidden states that haven't happened yet. We see a restatement of equation 2.6 with expected free energy here, broken into epistemic and pragmatic value.
2413180	2426040	89	A	1.0	40:13	19	Now, to flesh out this POMDP, we turn to a classic par and Friston example, which is the teammase.
2428800	2442530	90	A	0.9998	40:28	34	We're not going to go through every single part of this teammase right now, but it'll be a key area for us to unpack. Why do the matrices have the dimensions that they have?
2445220	2480750	91	A	0.99993	40:45	85	Where are the four locations that the mouse can be here, down at the bottom, to the left or to the right? So this is not like a mouse moving in continuous space. It's like there are four locations it can be in. Those are the columns. There's five hidden states of location which we can see are like, based upon whether when it goes down to the bottom to get a queue, whether the hidden state is going to reveal the queue or not.
2482160	2519210	92	A	1.0	41:22	87	And then figure seven, four and 75 you'll see differ. First off, one of them is bolded, this one is bolded, this one's unbolded. But here we see a one in the L column because the black is on the L. And here we see a one in the R column because the black is on the R. So understanding why that is is going to be the crux of understanding how to construct an epistemic informative q as a hidden environmental variable in a generative model.
2520220	2543010	93	A	1.0	42:00	44	Here are the B matrices transition probabilities. Here's the preference vector. We'll talk about what C one and C two are. Here are the priors we'll talk about what D one and D two are. Now there's more unpacking of the epistemic value itself.
2543780	2575880	94	A	0.99994	42:23	53	So in the equation above there was a restatement of equation 2.6 which partitions the expected free energy into the negative epistemic value and the pragmatic value. Now we're going to follow up on the epistemic value I of pi the epistemic value of a policy and break that down into these representations.
2580330	2612914	95	A	0.58	43:00	67	Here are simulations, simulated epistemic and pragmatic behavior of a rat foraging in a teammates. So we'll try to model what these runs are. I think it might be one small note about the book that especially if they knew that it was going to be black and white, which it seems like that was known, this is not ultra helpful. Perhaps people disagree. But another war.
2613112	2660898	96	C	0.75293	43:33	81	What said another warhol? Yeah, this one you can even see the Campbell soup. But I think if someone were familiar with reading these traces, they would still want to understand what these different lines were and like what the dashed one is. But if people are unfamiliar reading these kinds of error residual traces, I don't know what is being shown here figure 7.7. That was the simulation of the rat getting the epistemic queue and going to the left.
2661064	2682460	97	A	0.5457	44:21	47	So instead of just taking a 50 50 shot on where the food was, the rat navigates this explore exploit dilemma. It engages in an epistemically oriented action or an action driven by epistemic value to reduce uncertainty, get the queue and then it secures the food.
2684990	2707540	98	A	1.0	44:44	38	In this box 71 we see a discussion about parameterization of uncertainty and how precision is the inverse of variance. And this omega is the precision variable here, just like Mark Solms uses, but others use different ones.
2710710	2760320	99	A	1.0	45:10	81	The lower this value is, the more even things are. And so this has come up in the ice cream example with the folk psychology live stream. Like if your c vector is zero six negative six versus zero 500 negative 500, that's like a sharper preference distribution. So it's the quantitative values in the C, for example, that specified not just the direction but also the intensity of preference. And then analogously precision can be specified in a continuous form.
2761410	2797210	100	A	1.0	46:01	75	Here is a reference to an earlier par and Friston work on vision that has to do with ocular motor foraging, which is heavily epistemically driven. But then they are using this example to explore how precision the viewer shows a version to the upper left square when it is specified with a less precise, more ambiguous likelihood. Mapping the lower left square is epistemically attractive when the transition probabilities are specified as more uncertain.
2801770	2842020	101	A	0.96418	46:41	77	We'll try to understand these examples. Learning a Novelty 7.5 here we take that POMDP and we push it back in the genealogy. Now we have priors on the hidden states which are themselves equipped with prior beliefs. These are dear Schlee conjugate priors, but they can be of other forms. But they're just shown as dear Schlee here because it's like a convenient statistical distribution family for variational inference which we explored in several chapters ago.
2843510	2880080	102	A	0.99754	47:23	87	These can be considered as hyper priors, but in the end just variables on the graph. And it's not like they're tagged with being a hyper prior and you can imagine priors that nest on them. Exploring these potentially open ended Bayesian graph structures is why it's so important to have structure learning, like Karl has pointed out for years, and why we want to have parameter sweeps across cognitive model structures in active block prints. Because why stop? Why not have another prior on this one?
2880930	2884500	103	A	0.98811	48:00	12	So we should have the notation and the capacity to sweep that.
2886550	2913434	104	A	0.65	48:06	75	Here is a discussion on conjugate priors. A conjugate prior means that when used to perform Bayesian inference, the posterior belief will be the same type of distribution. So previously we talked about how like the hidden state could be a continuous variable and then the internal state could be like a discrete variable. The temperature is continuous outside and then inside is it hot or is it cold? Is like what I'm caring about.
2913632	2940820	105	A	0.99976	48:33	56	So the families don't have to match. But when we use conjugate priors it makes it so that where you can design it to play nice. It does play nice, but we'll work through that also with anybody who wants to go into more detail on the math because these are just like Bayesian statistics points.
2943510	2962060	106	A	1.0	49:03	43	Here theta is used to just describe parameters. It's a very common notation, but not 100% pervasive to use theta for parameters, just generally here there's more unpacking of the update rules. This is probably of secondary consideration for just constructing a model.
2966470	2976520	107	A	0.77	49:26	24	Here we return to expected free energy and several more representations beyond what was seen above and beyond what was seen in equation 2.6.
2978730	2989130	108	A	1.0	49:38	24	Here the learning model that was presented above the POMDP with another generation of genealogy is described which is drawn from these two papers.
2991150	3001310	109	A	0.65905	49:51	24	Then structured learning is introduced. We'll see how deep this one goes. But these papers are quite interesting and connected to Bayesian model reduction.
3003330	3015620	110	A	1.0	50:03	27	Here a hierarchical model is presented here's figure 43 in the dashed box. Now there's a figure 43 yo dog. I heard you liked figure four, three.
3018230	3028070	111	A	1.0	50:18	17	Then there's another hierarchical model simulation from the above showing belief updating again with these trace diagrams.
3030330	3063810	112	A	0.55	50:30	48	And then there's a summary. So it's a somewhat long chapter. It contains like, the music example, the rat example, the maze example, the eye example. So we're seeing like, data points in model space, um, in the last three minutes. Does anyone have any thoughts or questions?
3063880	3071380	113	A	0.99992	51:03	20	How are they going to tackle chapter seven? And as they start to construct their model, if they want to?
3079940	3119950	114	C	0.91	51:19	81	I am just every direction that I can because it is quite dense still and kind of unapproachable, I think, for me anyways, it just is quite difficult to get a grasp on it from the textbook specifically at this point, like the generative model, like, what does that mean in code? And where does that correlate to this particular function or that particular time step or whatever? I don't know. It's just a lot. And so I'm just trying to.
3122900	3178096	115	B	0.9981	52:02	121	From. Yeah, I don't know, from just generally building up simulations, which are just things that you can put together in a browser, like with simple JavaScript to Cadcad and active block prints and these sorts of things and trying to grasp at some real world situation. So maybe that rat Tmaze is a really good place to focus on for me, like a good intersection between the book and something that isn't like the frog jumping or is it not jumping is so contrived. It's really difficult to think about modeling that and in a way that would help explain what's going it's like, yes, I knew the frog was jumping or not jumping, so that doesn't really help. Right.
3178278	3192150	116	C	0.89659	52:58	30	Whereas the rat maybe coming up with some left or right hidden state based on the queue that's partially there's some explanatory power there that may be realized, I think.
3194200	3211630	117	C	0.99	53:14	38	I don't know. I'm trying to bridge jump over the Grand Canyon of the philosophical conceptual okay, got it. But the math there's. Okay. But there's a gap there for me, so I'm trying to pull them together.
3214160	3246064	118	D	0.99	53:34	59	Yeah. I have a question related to calibration of those priors and among other things, is that ever explained as to how to go about doing that for any of these models? Yes. So model based data analysis is going to describe all right, chapter seven describes just in our last minute. Chapter seven describes the discrete state models.
3246192	3257610	119	A	0.99998	54:06	25	Chapter eight describes continuous time models. Chapter nine describes using empirical data for model driven data analysis. Chapter ten is a summary. The textbook is.
3259740	3282930	120	D	0.77	54:19	78	Yeah, we're excited towards. The Grand Canyon, but I think, again, we have several months to unpack this together. So I hope that we can go quite far with this. So please stay engaged and add your questions and start building from both sides. Play with the code examples provided, break them and then start from the other side with a recipe on a situation that you care about, and then they're going to meet in the middle.
3284740	3289040	121	A	1.0	54:44	5	Okay, thank you, everyone. Farewell.
3292500	3293740	122	D	0.78985	54:52	3	Bye, everyone. Bye.

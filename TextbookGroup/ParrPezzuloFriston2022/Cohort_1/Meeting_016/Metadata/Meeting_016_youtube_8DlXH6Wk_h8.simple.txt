SPEAKER_00:
okay hello it's september 16th we're in cohort one of the active textbook group it's uh meeting 15 or i guess uh yes no 16 on um let me just update the title meeting 16 we're in our second discussion on chapter six so are there any overall comments that people would like to

address about chapter six or any other questions or points to add or just areas of chapter six that they found to be interesting


SPEAKER_01:
distinction between the interceptive-extraceptive modalities, which have been mentioned in box 6.1.

And first, I think, Princeton, with Michael Allen, if I'm not mistaken, has published a recent paper just today, exactly on that topic, in which they've somehow tried to provide

a computational model for the first time for the unification of the interoceptive and extraoceptive modalities.

So, since this chapter is also about the modeling of those agents, I think this paper in the body's eye, the Computational Anatomy of Interoceptive Inference

might be relevant for anyone who wants to model these modalities with the aid of Active Inference framework.


SPEAKER_00:
Thank you.

Very nice.

Thanks for sharing it.

Let's look at box 6-1.

as people see it what distinguishes exteroceptive proprioceptive and interoceptive modalities for example why are proprioceptive and interoceptive separated


SPEAKER_01:
because uh i couldn't hear you i don't know if it's on my side or oh connection was unstable from your side sorry let me try something i'll be gone for a second am i back okay so yeah what uh yeah brock i do need the star link


SPEAKER_00:
Why are proprioception and terraception separated?

And why are these three categories modeled as different?

Why have they been framed as different?


SPEAKER_01:
biological and neurological mechanisms, neural mechanisms that somehow can control those different kind of perceptions.

Because for the extra receptive perception, the organism would necessarily need a kind of

I mean, the sensation it gets from the environment is just on the level of the imminent stimuli that it gets from the external environment.

But for the interoceptive modality,

uh the sensation is just uh the internal uh sensations and so on and for the proprioceptive it's related to motor control feedback loop so i guess because of their different mechanisms it

might be helpful to talk in different terms about each of them, but I'm not totally sure about that.


SPEAKER_00:
i agree i think one aspect even in a pre-blanket or non-blanket modality is like they're about external things versus about internal things and then between proprioception and interoception you're absolutely right that they have different neural mechanisms and physiological mechanisms um

with some blurry space, perhaps.

For example, like interoception about blood pressure is somewhere in between like a muscular reflex and an organ reflex, but the tissues being addressed are different.

Let's look at this paper.

Again, we'll return to the book, but we're going to be thinking today, and then especially going forward with like, how do you

identify the um let's go to this question which we just just to look at it what system are we modeling what is the appropriate form for the generative model how do we set up a generative model how do we set up a generative process so let's see if we can determine some of those aspects in this paper

Hashtag figure four three.

It is the very same.

Okay, this is just a general pointing to a partially observable Markov decision process with a slight PLOS computational biology graphical veneer to match the color scheme.

Probably not, but it kind of looks like it.

Here is where we see the schematic of the form of the generative model.

Variables are used to reflect that those variables can be fit from empirical data or just a priori set and simulated.

But this is the form of the model.

These are the shapes of the matrices and tensors

go from that are going to address what their model is about um does anybody want to like describe what they see in this what do you think they were trying to explain and what do you think they were trying to model what is their system of interest based upon what you see here


SPEAKER_01:
Actually, one interesting term that I used in this paper is cardiac active inference, the term that they've used in this paper to describe the kind of interrelationship between the heart rhythm

and the interception.

So this generative model, I believe, is just about modeling that relationship between the heart rhythm and the interception.

So we can see, as usual, the B matrices, C and A matrices that we've

seen before and but in this case they've been applied to the variation in the heart rhythm and how it relates to the interception and so yes so again using the variables that we know from figure four three in the book and their figure one here the hidden states that are not directly


SPEAKER_00:
observed are on the interoceptive side relating to the state of the heart on the exteroceptive side whether there is a flower or a spider the outcomes that are observed are the visual exteroception of a flower or a spider

and the heart rate, probably a low and a high heart rate.

The piece that links these two is the sensory attenuation between the high heart rate state and the mapping of exteroception.

In the B matrix, you can just see that even the allocation of the symbols is different.

Like when you're aroused, when you're in this, this is like heart open and then heart contract.

So when you're in the aroused situation, B is how the hidden states change their time.

You're always going to go from here to here.

Whereas in the relaxed situation,

Sometimes you go to the, from this state to this one.

Other times you're relaxed.

And so you actually, you dwell within that state.

So here it's like pump, rest, pump, rest, pump, rest.

Here it's 50% slower.

Just, it depends on the parameters, but it's 50% slower because sometimes

you're going to go from one but it looks like as it's here that it's actually like or it's twice as slow rest rest beat rest rest beat so these are the two interoceptive states one has a preference for having relaxed hearts when flowers are around and having contracted heart when spiders are around

a matrix maps on these two sensory modalities between hidden states and outcome states just like any a matrix would do in any pomdp and then there's also a transition matrix of the generative process so it's a very simple generative process where either flowers are flowers or spiders are spiders

As they would say, you can use standard routines to simulate these techniques.

I hope that the code are available.

You can plot parameter sweeps as you explore the beta and the alpha.

Yeah.

So here's like the beta parameter and the alpha parameter.

So you can do parameter sweeps with this grid search or other approaches and then do statistics on those outputs.

Is this quote active inference?

Well, it's the output of a parameter sweep on an active inference model.

There's various other kinds of methods.

So not to spend too, too much time on the paper, but does anyone have any thoughts on like that case or how they see it applying to chapter six's content?

I think it speaks to a fulfillment cycle in active inference research.


SPEAKER_01:
this work example helps a lot I mean if anyone wants to especially because describing this kind of cardiac active inference as

a kind of system that has a definite mark of blanket, it was a really interesting question at the first place.

And I think they've done a really

Great job, obviously, in that regard and thinking really creatively and out of the box about how we can describe that whole system with the aid of using markup blankets to describe its internal and external environment and so on.


SPEAKER_00:
It also relates to this earlier question.

So how does the figure four, three paradigm, discrete and continuous state spaces with all the attending complexities?

Do you hear me, Ellie?

Okay.

Okay.

How does this representation connect to?


SPEAKER_01:
Can you hear Daniel?


SPEAKER_00:
Yeah, I can hear you.

it's a story for another day some of the incredible the truly incredible internet i'm going to try one thing so i think that or holly can you hear me now or no i'll try something

here's what's interesting with the Wi-Fi and then we'll continue because it's not a Wi-Fi learning group but with the hotspot there are a set of people who I cannot interact with but with Wi-Fi I am able to interact with those people

Just to note, so we see the POMDP representation.

Now, what is not in this paper?

Here is a graphical layout of the POMDP.

What is not in this paper?

A figure 6-1 or indeed a figure 1-1 representation.

So why is that?


SPEAKER_01:
I'm not sure I've understood your question quite clearly, but could you please elaborate on that a little bit more?


SPEAKER_00:
Yes, sure.

So when we look at papers like Vassal et al., Communication as Active Inference, Social and Active, Active Inference, Loops, Loops, Loops, Vyotkin et al., Remote Teams, Loops, Loops, Loops, Markov Blanket, Interface, Loops, Entities, Things,

yet in the application what is specified is not the loopy situation what needs to be specified is chapter six not chapter one so i find that very interesting because so much of the focus attention discourse has been around abstractions around markov blankets

their interpretations from various stances and about boundaries physical boundaries and conceptual boundaries but it's a non-controversy when it's written this way and there are markov blankets between many of these variables because it's a bayesian graph that's not fully connected so every bayesian graph that's not fully connected has markov blankets trivially

But this paper doesn't need to describe what is... This is enough to say what's inside and outside.

The organismal Markov blanket, if we want to even think about it, is like here in this line, kind of.

Not on the Bayes graph, but it's between the spider and the flower outside and the vision of the spider and the flower.

but that's like taken care of with the a matrix of the exteroceptive modality which is i think why this recipe does not focus on which thing are we modeling but rather which system are we modeling

yes we're going to pursue the generative model before the generative process even though they may have symmetry or they may be very similar um but i just find it very interesting open to hearing what people think like why the cybernetic loop action perception loop is used so much

in contrast with the form of the model that's actually implemented which is a it's a different topology in a different discussion i know that they're integrated and compatible


SPEAKER_01:
Just one comment on that is, you see, at least to my understanding, Markov Blanket is a kind of modeling tool or a kind of a way of thinking about basically the ontology of, I mean, ontology in the sense of its philosophical use,

the ontology of the system that we're about to model.

So it's not necessarily the relationships among the different components of the Markov blanket is something not necessarily

physical or tangible relationships in that system.

So it might be kind of just informational as we've just

we saw in I think chapter 5 if I'm not mistaken the information passing among the different components and so on so at least I think the reason for not using the

schematic model of Markov, I mean, the cybernetic model of Markov blanket instead of using just the schematic form, probably it's because of these kind of non, let's say, non-physical or non-tangible relationship among the different components.

By non-tangible and non-physical, I mean

something that can't be explained in terms of its ontological existence.

I don't know if I could express myself in that regard or not.


SPEAKER_00:
Thank you, Ali.

Brock, and anyone else?


SPEAKER_02:
I was just going to...

I don't know.

I feel like personally, I'm just kind of at my affordances that I have to get pragmatic and epistemic value here without like doing some modeling of some kind.

Yeah, I don't know.


SPEAKER_00:
That's just good.

Thanks.

I mean, this is the second discussion on Chapter 6, and then this is setting us up to actually pursue a model.

So we should be providing these things.


SPEAKER_02:
Yeah, I'm just saying it.

It's definitely ripe for that.

I mean, yes.


SPEAKER_00:
Rohan?


SPEAKER_03:
Yeah, so actually just to get some clarification, you're asking why...


SPEAKER_00:
form of the model is as mentioned in chapter six the topology given in chapter six which is chapter one yes i mean they're they're compatible topologies that actually are describing an overlapping set of notes i was just calling attention to the um amount of discourse and focus

on the potentially more familiar, more schematic action perception loop and the way that that draws so much attention to the organism environment interface.

Whereas when it comes to the empirical modeling, that interface never had to be specified as like a first class type.

It just was included by the by

We just said there are spiders and flowers out there, hidden states, S. And then there's observations of, and there's no error.

There's no error in this.


SPEAKER_03:
Right, but there would be no error in the generative process itself.

The model might have error, right, so.

I mean, that comes from some measurement issue, right?

So when you measure it and you confuse, say, spider for flower or flower for spider, then you would get some error.

And that's where I guess you can talk about the Markov blanket mis-specifying or mis-observing something.

So I'm not sure what exactly is the... I mean, they're compatible, right?

So in this case, there's no error.

But if there was error, obviously, there would be something wrong with the...


SPEAKER_00:
Um, let me actually just correct this.

I believe be, I may, I, I didn't mean to say this is the visual error.

This is saying that they're unchanging flowers.

So it's either they're either one or the other here, parametrically, it's possible to introduce visual error in the a matrix, but I'm, I'm just pointing to the, the architecture of the model doesn't require even engaging with this.

It's totally compatible and the observations are the sense states and the actions are the active states and so on.

But as presented and as constructed, if we're going to use a POMDP architecture, we don't necessarily even need a representation like this.


SPEAKER_03:
Yeah.

Jessica?

Okay, that's interesting.


SPEAKER_05:
Hi, yes.

For me, the reason why... It's like two parts.

The reason why the Asian craft is so popular, and I think maybe it's also the reason why it was used, is the simplicity to show the concept and the relationship.

So like the new one,

It's great, I guess, for the paper that it was created, but it doesn't show the relationship as clear as the other ones.

It's very simple.

So I think that's why it was used.

Maybe like remove a lot of extra information that the second paper illustrates complexity in it.

But the simplicity of that graph is why it really encapsulates the concept.

And it's also the reason why it's so popular, because it's so simple.

So I think that's the relationship with it from how I see it.


SPEAKER_00:
Yep, totally makes sense.

Thank you.

And once we see that there is a mapping from the schematic cybernetic view,

then we can probably fill in a lot of the parts of the pmdp but just to look one more level here i mean the code i posted the code below so it's in matlab which people may or may not have access to oh sorry okay um ali and then rowan


SPEAKER_01:
No, I just wanted to point out one thing that I forgot to mention before.

One thing that might be helpful to keep in mind is that actually in this generative model, there is a kind of unification or a kind of integration of interoceptive and extraoceptive modalities.

Because as they pointed out in the paper, this agent that they're modeling

adopt two modes of engagement with the world, one of which is the relaxed and the other one is the aroused mode of engagement.

And because of that, there are two sorts of hidden states related to each, interoceptive and extraoceptive one.

So I guess if they wanted to put that kind of interrelationships between these kind of modalities in that simplified schematic model, it wouldn't necessarily clarify these kind of integration between the modalities.

One reason might be related to that peculiar situation of this particular case here.


SPEAKER_00:
Thanks.

It also speaks to the step in the model of the form of a generative model.

And then I think when people get married to the cybernetic scheme, you can imagine, well, then there's two nodes inside of the internal states.

There's two modes you can go between.

But now you're like mixing and matching what arrows mean, where some of the arrows are reflecting causal influence and some of them are reflecting transitions between mutually exclusive states.

And so then we end up back into the wireframe nightmare where it's like a schematic of

where some edges are tactile connections and some are causal links and some are mutually exclusive transfers of information and all these other ways that people visualize models.

And this representation reminds us that we're dealing with variables and states and their connections.

Jessica?


SPEAKER_04:
I think that was from before.

But yeah, but I guess I can add a little, I think it's like,

The visual, it has to do with what are you trying to convey in that moment?

And, you know, how much depth of information do you need?

I think like Ali said, like for this one, you need an extra information so that all the model wouldn't work.

While just share like with the idea of what the model is, maybe this version with the extra information might be too complex.

So the other one serves a better purpose.

So it's like, what is the purpose you're trying to accomplish and what do you want to communicate?

And depending on that, then models with more information or less information are more successful.


SPEAKER_00:
Yep.

Thank you.

Totally agree.

I mean, they didn't do any of the loop at all.

They just went straight to the POMDP representation.

And also this isn't the only form of an interoceptive, exteroceptive integration model.

One could brainstorm a whole host.

And again, that's what the recipe is for.

Well, we could have a mutually exclusive transfer between these two states defined by these two B matrices, right?

which entails a policy variable switching between the two.

Or we could have a continuous variable called arousal, and arousal is engaged in some sort of attenuation relationship here.

So this isn't even the only form, but it shows us a form.

Okay.

Soon, in 20 minutes, we're going to be heading into chapter seven.

So let's preview it so that when we're working on this in the next week, preparing our questions, starting to think about what we want to be modeling, we'll kind of know what's going to happen.

All right.

Chapter seven is active inference in discrete time.

People can still hear me?

Okay.

I think so.

All right.

So here is where we cross the Rubicon with modeling.

this is abstract up till now you cut out there for a second oh okay okay i'm it's all good now uh yeah just okay okay thank you chapter seven we're crossing the rubicon together and um it's been abstract up till now first what is addressed is perceptual processing

you'll find a lot of resonance between this chapter seven and step-by-step, which is model stream one.

Maybe it's step hyphen.

Yeah.

This paper, step-by-step tutorial, is going to do a certain walkthrough

of adding incremental complexity into the model in terms of model parameter depth, okay?

It's model stream one.

There's four streams that Ryan and Chris White did.

Static perception, dynamic perception, dynamic perception with preferences and free energy minimization, dynamic perception with free energy minimization and flexible policy selection through uncertainty modulation.

We're going to do something similar in chapter seven.

They don't do that upper left corner of step-by-step.

They go straight to hidden Markov models for dynamic perception.

This is analogous to a Kalman filter or any other number of general Bayesian filtering schemes.

We have a prior D, hidden states S,

and a sequence of observables through time, T, that are mapped to hidden states, A. B describes how hidden states change through time.

So we have the temperature is a hidden variable, and then we're getting observations from the thermometer that are mapped to the real temperature by A. Maybe it's a good thermometer, maybe it's biased or whatever.

And then there's the temperature changing through time.

Then there's a very Ali-centric discussion of listening to music.

This has to be one of the most glaring errors in the textbook.

It's very Warhol-like though, so maybe it has a certain aesthetic to it.

But the example that's going to be provided is like a musical listening example.

So the A matrix is emitting notes based upon the hidden state, which is the score of the music.

The B matrix is, and they have 70% accuracy.

So this one-tenth distributes through the matrix.

So it's just to use like integers here, but really this is like 0.7, 0.1, 0.1, 0.1, because it has to add up to one.

But when this person, when the hidden state of the score is in the first note, they play it 70% of the time.

And then the other three notes, they make an error 10% of the time each time.

70% of the time, the musician hits their note.

The B matrix is how things change through time, the hidden state.

So this is the transitions in the score.

And here, there's a 97% probability.

Again, it's distributed throughout with this one over 100, just to represent it with integers.

And there's a prior with no uncertainty or ambiguity of just it begins in the first position.

And so this, we can even see like, doot, doot, doot, doot.

You know, I don't want to use ABCD, but it would be like B, C, D, A.

in the musical ontology so here's b c d a like that but then there is an error in this one simulation run it'd be probabilistic but they're showing for didactic reasons a run where the third point in the third observation there is an error however it's heard as the correct value

That's a pure inference example.

This is not active inference.

This is just Bayesian models of perception.

This kind of sessile creature is rather uninteresting.

Okay.

Now we move from a hidden Markov model, Crouching Tiger, hidden Markov,

into a partially observable Markov decision process.

A hidden Markov model is a partially observable Markov model because some parts of it are hidden, some are not.

But the key piece here is the introduction of the decision.

We know that we model that with policy that influences how states change through time and that policy selection is governed by G. Figure 4.3.

people can still hear because I see all these like frozen for me okay all right okay thank you yeah it's just strange stuff um here's four three again here when we're talking about policy selection we're talking about states that haven't happened yet observations and hidden states that haven't happened yet

we see a restatement of equation 2.6 with expected free energy, here broken into epistemic and pragmatic value.

Now, to flesh out this POMDP, we turn to a classic Parr and Friston example, which is the T-MACE.

we're not going to go through every single part of this teamaze right now but it'll be a key area for us to unpack why do the matrices have the dimensions that they have where are the where are the four locations that the mouse can be you know here down at the bottom to the left or to the right

this is not like a mouse moving in continuous space it's like there are four locations it can be in those are the columns there's five hidden states of location which we can see are like based upon whether when it goes down to the bottom to get a cue whether the hidden state is going to reveal the cue or not

And then figure seven, four and seven, five, you'll see differ.

First off, one of them is bolded.

This one is bolded.

This one's unbolded.

But here we see a one in the L column because the black is on the L. And here we see a one in the R column because the black is on the R. So understanding why that is, is going to be the crux of understanding how to construct

an epistemic informative Q as a hidden environmental variable in a generative model.

Here are the B matrices, transition probabilities.

Here's the preference vector.

We'll talk about what C1 and C2 are.

Here are the priors.

We'll talk about what D1 and D2 are.

Now there's more unpacking of the epistemic value itself.

So in the equation above, there was a restatement of equation 2.6, which partitions the expected free energy into the negative epistemic value and the pragmatic value.

Now we're going to follow up on the epistemic value, I of pi, the epistemic value of a policy.

and break that down into these representations.

Here are simulations, simulated epistemic and pragmatic behavior of a rat foraging in a T-Maze.

So we'll try to model what these runs are.

I think it might be one small note about the book that especially if they knew that it was going to be black and white, which it seems like that was known,

This is not ultra helpful.

Perhaps people disagree.

Another Warhol.

What?

I said another Warhol.

Yeah, this one.

You can even see the Campbell soup.

But I think if someone were familiar with reading these traces, they would still want to understand what these different lines were.

and like what the dashed one is.

But if people are unfamiliar reading these kinds of error residual traces, I don't know what is being shown here.

Figure 7.7.

That was the simulation of the rat getting the epistemic cue and going to the left.

So instead of just taking a 50-50 shot,

on where the food was, the rat navigates this explore-exploit dilemma.

It engages in an epistemically oriented action or an action driven by epistemic value to reduce uncertainty, get the cue, and then it secures the food.

In this box 7-1, we see a discussion about parameterization of uncertainty

and how precision is the inverse of variance.

And this omega is the precision variable here, just like Mark Solms uses, but others use different ones.

The lower this value is, the more even things are.

And so this has come up in the ice cream example with the folk psychology live stream.

Like if your C vector is zero, six, negative six versus zero, 500, negative 500, that's like a sharper preference distribution.

So it's the quantitative values in the sea, for example, that specified not just the direction, but also the intensity of preference.

And then analogously, precision can be specified in a continuous form.

Here is a reference to an earlier Parr and Friston work on vision that has to do with ocular motor foraging, which is heavily epistemically driven, but then,

they um are using this example to explore how precision the viewer shows a version to the upper left square when it is specified with a less precise more ambiguous likelihood mapping the lower left square is epistemically attractive when the transition probabilities are specified as more uncertain we'll try to understand these examples

Learning and novelty 7.5.

Here we take that POMDP and like we push it back in the genealogy.

Now we have a priors on the hidden states, which are themselves equipped with prior beliefs.

These are Dirichlet conjugate priors.

but they can be of other forms, but they're just shown as Dirichlet here because it's like a convenient statistical distribution family for variational inference, which we explored in several chapters ago.

These can be considered as hyperpriors because they, but in the end, just variables on a graph.

And it's not like they're tagged with being a hyperprior.

And you can imagine priors that nest on them,

Exploring these potentially open-ended Bayesian graph structures is why it's so important to have structure learning, like Carl has pointed out for years, and why we want to have parameter sweeps across cognitive model structures in active block friends.

Because why stop?

Why not have another prior on this one?

So we should have the notation and the capacity to sweep that.

Here is a discussion on conjugate priors.

A conjugate prior means that when used to perform Bayesian inference, the posterior belief will be the same type of distribution.

So previously we talked about how like the hidden state could be a continuous variable, and then the internal state could be like a discrete variable.

The temperature is continuous outside, and then inside is it hot or is it cold, is like what I'm caring about.

so the families don't have to match but when we use conjugate priors it makes it so that where you can design it to play nice it does play nice but we'll work through that also with anybody who wants to go into more detail on the math and because these are just like Bayesian statistics points

Here, theta is used to just describe parameters.

It's a very common notation, but not 100% pervasive to use theta for parameters just generally.

Here, there's more unpacking of the update rules.

This is probably of secondary consideration for just constructing a model.

Here, we return to expected free energy

and several more representations beyond what was seen above and beyond what was seen in equation 2.6.

Here, the learning model that was presented above the POMDP with another generation of genealogy is described, which is drawn from these two papers.

Then, structured learning is introduced.

We'll see how deep this one goes, but these papers are quite interesting and connected to Bayesian model reduction.

Here, a hierarchical model is presented.

Here's figure 4-3 in the dashed box.

Now there's a figure 4-3, you know, yo dog, I heard you liked figure 4-3.

Then there's another hierarchical model simulation from the above showing belief updating again with these trace diagrams.

And then there's a summary.

So it's a somewhat long chapter.

It contains the music example, the rat example, the maze example, the eye example.

So we're seeing data points in model space.

in the last three minutes does anyone have any thoughts or questions how are they going to tackle chapter seven and as they start to construct their model if they want to i am just um every direction quick that i can because it is


SPEAKER_02:
quite dense still and kind of unapproachable, I think, for me anyways.

It just is quite difficult to get a grasp on it from the textbook specifically at this point, like the generative model, like what does that mean in code and where does that correlate to this particular function or that particular time step or whatever?

I don't know.

It's just a lot.

And so I'm just trying to

From, yeah, I don't know, from just generally building up simulations, which are just things that you can, you know, put together in a browser, like with simple JavaScript to like CAD CAD and active block rents and, you know, these sorts of things and trying to grasp at some real world situation.

So maybe that, that rat, you know, teammates is a,

really good place to focus on for me like a good intersection between the book and something that isn't like the frog jumping or is it not jumping is

you know, so contrived, it's really difficult to think about modeling that in a way that would help explain what's going, it's like, yes, I knew the frog was jumping or not jumping.

So that doesn't really help, right?

Whereas the rat may be coming up with some left or right hidden state based on the cue.

That's partially, there's some explanatory power there that may be realized, I think so.

I'm just, yeah, I don't know, it's, I'm trying to bridge, jump over the Grand Canyon of the philosophical, conceptual, okay, got it, but the math, there's a, okay, but there's a gap there for me, so I'm trying to pull them together, and it's...


SPEAKER_03:
Yeah, I have a question related to calibration of those priors and things.

Is that ever explained as to how to go about doing that for any of these models?


SPEAKER_00:
Yes.

So model based data analysis.


SPEAKER_03:
is going to describe... Okay, so it's in the future.


SPEAKER_00:
I'll be for that.

Chapter 7 describes, just in our last minute, Chapter 7 describes the discrete state models.

Chapter 8 describes continuous time models.

Chapter 9 describes using empirical data for model-driven data analysis.

Chapter 10 is a summary.

The textbook is...


SPEAKER_06:
Yeah.


SPEAKER_00:
We're accelerating towards the Grand Canyon.

But I think, again, we have several months to unpack this together.

So I hope that we can go quite far with this.

So please stay engaged and add your questions and start building from both sides.

Play with the code examples provided.

Break them.

And then start from the other side with a recipe on a situation that you care about.

And then they're going to meet in the middle.

Okay.

Thank you, everyone.

Farewell.


SPEAKER_03:
Bye everyone.


SPEAKER_00:
Bye.
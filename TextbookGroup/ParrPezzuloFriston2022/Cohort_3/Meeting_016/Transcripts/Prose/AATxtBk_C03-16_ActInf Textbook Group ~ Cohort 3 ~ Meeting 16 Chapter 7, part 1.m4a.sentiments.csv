start	end	speaker	sentiment	confidence	text
560	1110	A	0.546256959438324	You.
3000	4070	A	0.5579421520233154	Hello everyone.
4760	11444	A	0.8984872698783875	We're in our first discussion of chapter seven in cohort three.
11642	20790	A	0.8932943940162659	So let's head over to chapter seven and we will come to the questions.
21960	37550	A	0.9065021872520447	But let's just start with anyone who wants to do you want to just give a general comment or any reflection or thought on seven or anything up to this point?
43020	45610	A	0.8478419184684753	Ali and then anyone else?
49020	49432	B	0.46103888750076294	Yes.
49486	62140	B	0.9059278964996338	So chapter seven is basically the application of the first half of chapter four in terms of modeling discrete time situations.
62800	96330	B	0.8777127265930176	And it goes through a number of case studies in order to show how the matrices A-B-C and D can be constructed in various situations and the challenges or rather the concerns we may have when constructing such matrices in order to have a viable discrete time model for each situation.
97180	104110	B	0.8597369194030762	So, yeah, that's basically the premise of chapter seven.
106560	108220	A	0.9811747074127197	Awesome, thank you.
108370	109644	A	0.570740818977356	Totally agree.
109842	114776	A	0.8243458271026611	Just like chapters two and three with low road and the high road, we're kind of like a pair.
114968	121740	A	0.9154086709022522	Chapters seven and eight are kind of like a pair because seven is going to go into discrete time modeling.
121900	125600	A	0.8864356875419617	Chapter eight is on continuous time and hybrid modeling.
126180	132710	A	0.9170852303504944	Does anyone else want to make any just overall reflection or comment on chapter seven?
154400	161560	A	0.7566260099411011	All right, I'm checking if there's any okay.
162430	164234	A	0.8280325531959534	Does anyone have any question at all?
164272	174694	A	0.8966473937034607	Otherwise we'll turn first to these questions, start with the written questions we have and then just approach it from there.
174832	178560	A	0.8969634771347046	Does anyone have anything else that they want to consider first though?
185940	203684	A	0.8271192908287048	Okay, I'm putting in the chat, that's the social sciences link, but here's the chapter seven link in the chat and so you can follow along there.
203882	214810	A	0.7133975028991699	Also please upvote the questions that are interesting to you because the questions that have a bunch of upvoted points we're going to make the short videos for.
217420	229310	A	0.7619198560714722	Okay, so we'll just go to the I'm going to unvote all the questions from my own you can still vote for them, but I'm just going to unvote them so that once we get to them, I'll vote for them.
231960	232710	A	0.584351658821106	Okay.
234540	241530	A	0.8716776967048645	All right, first question here's, chapter seven.
241900	244180	A	0.8457105755805969	Chapter seven begins with a quotation.
244260	246184	A	0.9221367835998535	What I cannot create, I do not understand.
246302	250696	A	0.8959704041481018	Richard Feigman, what do you think this means in the context of active inference?
250728	278300	A	0.7230881452560425	Specifically, I can't remember exactly who added this in a previous cohort, but they gave a mild and a stronger answer.
278990	282586	A	0.7339805364608765	The mild answer is you have to learn by doing.
282688	288090	A	0.8310170769691467	You actually have to build the generative model to understand active inference.
290430	293662	A	0.625892698764801	And then the stronger answer goes even beyond that.
293796	305780	A	0.568023681640625	And it's not just that you have to build the generative model, you actually have to have something that escapes your hands and does something interesting to really test the method and understand it.
308230	311700	A	0.8893074989318848	Anyone else want to add a comment on this?
317750	323302	A	0.9008574485778809	Okay, kind of a warm up question, but anyone else want to add anything on this?
323356	325480	A	0.9221367835998535	What I cannot create, I do not understand.
326330	330380	A	0.9260345697402954	Otherwise we'll come into the more specifics of the chapter seven now.
336350	343980	A	0.7582747936248779	Okay, so in that case, I'm just going to go with the most upvoted questions.
348050	348800	A	0.584351658821106	Okay.
353340	353960	A	0.4896697998046875	All right.
354030	356596	A	0.7995691895484924	This is on page 130, the textbook they wrote.
356708	368968	A	0.8563452363014221	This choice, which we're going to look to the textbook in a second, speaks to the classical exploration exploitation dilemma and psychology, a dilemma that is resolved under active inference.
369144	370750	A	0.8433384895324707	What do you think about this?
371280	386124	A	0.7525153756141663	Okay, so the setting that's being considered is decision making under uncertain payoff or reward structure.
386252	391876	A	0.5917840003967285	So you think there's two slot machines, one of them pays out 50% of the time.
392058	394470	A	0.6697199940681458	You don't know how much the other one pays out.
394840	406664	A	0.4654337167739868	So you could continue to get 50% that would be exploit, or you could explore, and it might be better than 50%, it might be worse than 50%.
406782	411900	A	0.6743339896202087	And so that is sometimes called the exploration exploitation dilemma.
412480	420140	A	0.924517035484314	Does anyone want to give a comment or a related question on how is active inference going to be approaching Explore exploit?
428990	442730	C	0.8980744481086731	Michael yeah, I just wanted to both affirm that I found the mathematical representation of these two ideas, that it's a trade off very interesting.
442900	457060	C	0.542826235294342	But to describe them as resolved is something that I find might be a little more overdescribed, if you will.
460410	470294	C	0.8040491938591003	Maybe the word is this formula describes the trade off or as a way of representing it, but resolution is a much more open topic.
470342	476780	C	0.8280563950538635	So just thought I would give a response, give feedback and say, well.
478590	479962	A	0.5567741394042969	Yeah, that's it.
480016	488240	A	0.6950609087944031	Yes, maybe resolve in the sense of like a high resolution image, like we can see it.
488690	496946	A	0.7273507118225098	We've resolved the dilemma by perceiving it not we have explained it away or said that there is no trade off.
497128	505570	A	0.7731239199638367	I believe this was Eric Sounds previously, who kind of contested the idea that Explore exploit is a dilemma.
507290	508898	A	0.6719316244125366	It's not a dilemma.
508994	511474	A	0.5702438354492188	There's not two options that are intrinsically opposed.
511522	514790	A	0.8026025891304016	There's just one integrated behavioral choice.
515850	525450	A	0.7621304988861084	All of this being said, let's look at how Explore exploit our approach differently in active inference.
526910	555394	C	0.7496470808982849	One more remark might be that in a different sense making context where the explore versus exploit issue arose continuously, the word sharpening visibility was a way of describing the space that sometimes just your resolution comment helps remind me of that.
555432	566950	C	0.6292147636413574	That it was like boosting the visibility of the space does not resolve the problems that you're trying to address, but it does improve your ability to act on the space.
567020	574406	C	0.8053721189498901	And I guess that's maybe a suggestion for language boosting visibility versus resolving that's.
574438	578074	A	0.6549745202064514	All yes.
578192	578860	A	0.918424665927887	Awesome.
581230	581930	A	0.5491447448730469	Yeah.
582080	594206	A	0.9015546441078186	Just to briefly catch us up to how we get to this claim, chapter seven, like Ollie mentioned, is going to be concerned with a series of examples of building discrete time generative models.
594238	597940	A	0.8015637397766113	So these are models where time clicks like one step at a time.
600470	608710	A	0.9090360999107361	Figure 7.1 is showing just the perceptual or the downstairs component of a generative model.
608780	614486	A	0.8602449297904968	So D is the prior, S are the hidden states, b are how the hidden states change through time.
614668	618646	A	0.8392898440361023	A is the matrix that maps hidden states to observations.
618838	623206	A	0.8156726956367493	So this is just the temperature in the room changing through time outputting.
623238	625498	A	0.837455153465271	A thermometer reading at every time point.
625664	628300	A	0.7812398076057434	This is hidden Markov model.
628990	633118	A	0.6290366649627686	Not a Markov decision process, just a hidden Markov model.
633204	634030	A	0.6434743404388428	Hmm.
636610	644670	A	0.9244529604911804	They then write the A and the B matrix as well as the D for a musical setting.
644830	646350	A	0.7980709671974182	So there's four notes.
646510	657618	A	0.8082192540168762	The A matrix is basically saying you hear the correct note 70% of the time, and then the B is the transition matrix.
657794	664760	A	0.8571411967277527	So this is just playing four notes that transition to each other 97% of the time in this order.
666170	668730	A	0.7289428114891052	And it always starts on the first note.
669470	678154	A	0.7969644069671631	So with just those three matrices defined A, B, and D, you have fully defined the generative model for this.
678192	678998	A	0.6434743404388428	Hmm.
679174	683134	A	0.7972355484962463	There's no decision making yet, but that's all you need to define this.
683172	683950	A	0.6434743404388428	Hmm.
686770	701300	A	0.7678651213645935	And then in Figure 7.2, there's an example where actually there turns out in this simulation that the third observation, it should be here, but it's actually here.
702310	708360	A	0.7755703926086426	So that's what's heard because there's such a prior that that's what should be heard.
710570	718090	A	0.8808507919311523	Okay, now we get into action, which is where the explore and exploit are going to come back into play.
718160	736030	A	0.5730387568473816	So if it's just a purely passive inference, then the kind of dilemma or unavoidable trade off is like over and under fitting perceptually, making something that's over compressed or under compressed with your beliefs.
736610	749666	A	0.7600988745689392	But action has a different set of challenges because it's related to selection of causal intervention in the world and it changes sequences of future observations and all of this.
749848	765510	A	0.7586823105812073	And as we know from the expected free energy, the imperative for action is to have both epistemic value, like to gain information, and to have pragmatic value, to have alignment between your observations, your preferences.
767130	769100	A	0.8775968551635742	Here's figure 7.3.
769470	774780	A	0.8420102596282959	This is kind of the classic generative model in discrete time.
775150	779180	A	0.7252031564712524	So we see that same downstairs component, the exact same.
779950	800100	A	0.8630830645561218	Plus there's the upstairs decision making component, pi policy intervening into how states change their time, expected free energy g updating policy decisions, updating them based upon their expected free energy.
809420	809876	A	0.6745842099189758	Michael?
809908	810408	C	0.6354975700378418	Yeah, sorry.
810494	811530	A	0.5299542546272278	Yeah, please.
811980	824616	C	0.6245821118354797	I'm jumping back 60 seconds, but right before you described that diagram, you said how you compress the I can't remember, but you use the word compression.
824728	830476	C	0.5163899064064026	And I wondered, why is he using compression as the verb to describe what's happening?
830578	848724	C	0.7385257482528687	And are you suggesting that in the same way that let's say when you compress data onto to save more storage space, you do an abstraction, reduce it, and it's a compression of what the data is.
848762	850772	C	0.7409191727638245	That is what's happening.
850826	854196	C	0.5351066589355469	There is a I'm sorry to be wordsmithing here.
854218	859290	C	0.811188817024231	I'm just trying to make sure I'm tracking what you were saying when you said okay.
860380	862490	A	0.8618331551551819	It is very closely related to this.
864380	866344	A	0.7778973579406738	There's two ways to think about it.
866382	873032	A	0.8121829628944397	One is like we're, we're fitting some gaussian, let's just say some bell curve over some uncertainty.
873096	878968	A	0.8836682438850403	So we could think about like, do we want over compressing it in terms of tightening the variance?
879144	884032	A	0.7560454607009888	That would be like overfitting versus like if it's loose, then it's like under.
884166	887184	A	0.49255016446113586	So that's a little bit like a loose sense of compression, really.
887222	888988	A	0.7312396764755249	It's just a variance estimation.
889164	897700	A	0.7904701232910156	Or you can think about complexity minus accuracy as being like optimal zip compression.
898360	907990	A	0.5963828563690186	Like obviously if you wanted a lossless compression you wanted perfect accuracy, then you're opening the door to an infinitely complex model.
909100	926270	A	0.7746340036392212	On the other hand, if you're willing to find a trade off between complexity and accuracy, which is summarized by variational free energy in the moment, then you would end up with at the point of diminishing returns, like the kind of the Pareto optimal compression point.
930800	931740	C	0.8529649972915649	Thank you.
931810	933984	A	0.6888453960418701	Sorry to no, it's all good.
934102	950790	A	0.8686132431030273	So now to study this action setting, which is where explore exploit comes into play, we're going to switch from the music listening example, which was just kind of familiarizing us with the A, B and the D.
951160	961320	A	0.8761884570121765	And now we're going to bring in action, which is going to require talking about Pi policy and C our preferences.
963100	967160	A	0.798515796661377	Okay, so it's a T maze task.
968540	980990	A	0.8932580947875977	There's an aversive stimulus in one arm, an attractive stimulus in the other arm, and a Q that indicates the location of the two stimuli in the bottom of the T.
981920	994012	A	0.8976384401321411	So the organism can either pick one of the arms left or right, maybe quickly get to the reward, but maybe get the aversive stimulus.
994076	1008868	A	0.7940342426300049	Or it could choose to seek out this informative cue and then having reduced its uncertainty about the location of the reward, then just walk over to the reward.
1009044	1022860	A	0.683698296546936	So that's going to be one, two, three steps instead of just going straight to one of the arms, but you then have higher accuracy with getting to one of the arms.
1024320	1039664	A	0.8557108640670776	So how do you get flexible and rapid and adaptive switching between pragmatic oriented behavior and epistemic oriented behavior especially?
1039782	1048550	A	0.6871078014373779	How can that happen as new information are rolling in without necessarily retraining the model?
1049880	1054230	A	0.8662629723548889	That is what is going to be explored here.
1056600	1064970	A	0.9001248478889465	Anyone else want to just add a comment or thought or how they see it at this point?
1068700	1093970	D	0.8976331353187561	Yeah, have a general question in terms of if you do an experiment with different animals, for example, or different observers, and you want to model different balance between this epistemic and pragmatic behavior, where in the ABCD matrices would that be?
1095160	1098150	A	0.849902331829071	Yeah, very good question.
1098680	1104630	A	0.8751194477081299	One place that this plays out is in the amplitude of C.
1105400	1125080	A	0.6184563636779785	So let's just say that left and right are the two directions on the teammates and we don't have a habit for either one of them if we prefer the sugar over the aversive.
1125160	1147140	A	0.8097765445709229	So now we're going to talk about the C matrix, if it's one and zero so aversive we don't really care about, then we give a value of one for the reward versus if the C was a million and negative a million, then the pragmatic value term would basically just be like scaled larger.
1147800	1160010	A	0.9143648147583008	So the amplitude on C is commonly used to weight the Pragmatic value relative to the epistemic value.
1160780	1175020	A	0.5387212038040161	Because just framing things like this does not guarantee that your parameterization is going to be at this adaptive point that actually does flexibly trade off between epistemic and Pragmatic behavior.
1176080	1188848	A	0.49987903237342834	Because if C is very small and washed out so utility seeking has shapes behavior a little bit, then you're going to end up with a mainly epistemic agent.
1189014	1211640	A	0.5904626846313477	And if C is vast relative to the epistemic value, then you're going to end up with mainly reward seeking, just like we see in the in figure Two six, where if we totally discard the pragmatic term we end up with purely epistemic behavior.
1212220	1217880	A	0.5193104147911072	And if we totally discard the epistemic term we end up with purely pragmatic behavior.
1219100	1228860	A	0.5667057037353516	But also if this term is just really small, you can imagine it doesn't play that large of a role and vice versa.
1230240	1264890	A	0.8433902859687805	So in an empirical setting, you have to parameterize the value, for example, of C, so that you do get goal reward oriented behavior, but not just like one track mind, um, Ali and then anyone else.
1268620	1299460	B	0.7898819446563721	Yeah, well, actually this C parameter C matrix here in the formulation of expected free energy is a relatively recent addition to active inference formulation because in some earlier literature or even some recent literature, this parameter c is not explicitly expressed in the formulation, they just express it as P of observation.
1299800	1323884	B	0.7446232438087463	Because you see, this is a crucial step actually in active inference because if we want to derive this expected free energy as a parallel to variation of free energy, then here instead of C, we should have only Pi, right?
1324002	1335964	B	0.6374630331993103	But replacing Pi with C denotes the kind of inaccuracy in terms of our policy selection.
1336092	1359960	B	0.8447798490524292	In other words, it doesn't necessarily entail, I mean, undertaking the policy we've selected, we have a preference matrix for undertaking some actions, but it doesn't necessarily imply that we only take those specific actions per se.
1360110	1375484	B	0.8477011322975159	So we somehow approximate our preferences without restricting ourselves to the actual policies that's been undertaken in the situation.
1375682	1392800	B	0.818608820438385	So that allows us to even formulate or at least describe some counterfactual examples as well as well as the actual behavior we observe for the agent.
1392950	1420170	B	0.6124467253684998	So that's, I think, one of the crucial steps in terms of allowing for a broader behavior of the agents and somehow broadening the possibility spaces of the agents, not just the actualized trajectories of the internal states versus the external states.
1424500	1447520	A	0.7322238683700562	Yeah, it allows us to articulate what is the action possibility space and separate that from the outcome preference so we don't have our finger on the scale directly, can't control observations directly.
1447600	1451404	A	0.7812390923500061	That's the kind of perceptual control theory insight.
1451552	1456760	A	0.5095943212509155	You want your preferences to be about observations, but you don't want to be able to directly control observations.
1457100	1464030	A	0.815906286239624	So what are the two channels that we have to reduce expected free energy, change our mind, change the world?
1465360	1467276	A	0.8230698704719543	How much to balance those two?
1467378	1469310	A	0.7777982950210571	That was Olivier's question.
1469680	1473152	A	0.7624476552009583	And that is the question is how do you balance those two?
1473206	1485368	A	0.8622545599937439	But those are the two things that you want to have from a first principles being balanced off in terms of your imperative for action selection.
1485484	1493760	A	0.5772531032562256	You want your action selection to be an epistemically informative path and a pragmatically useful path.
1493920	1505720	A	0.6819419860839844	Epistemic value, expected information gain under a policy pragmatic value how well it aligns with your preferences expectations.
1511120	1517656	A	0.7026872038841248	So this is how exploitative and explorative behavior are again going to be resolved.
1517688	1521020	A	0.7564859390258789	Not to say addressed away, but how it's approached.
1521600	1529116	A	0.5460559129714966	If we were in a reward setting, then exploitative behavior needs no secondary explanation.
1529228	1531596	A	0.7192304134368896	It's like there's money on the ground.
1531708	1534892	A	0.7420169711112976	If it's about getting money, then just pick it up.
1535046	1541220	A	0.5912361741065979	So exploitative behavior under reward centric paradigm does not need a secondary explanation.
1541800	1553524	A	0.854276716709137	Exploratory behavior on the other hand, eventually needs to navigate into the currency of utilitarian value in a reward centric path.
1553652	1563160	A	0.8629145622253418	And so an exploratory path might be highly valued because for example, there might be a reward.
1563320	1572452	A	0.8465753793716431	So if there's a 10% chance of a $1,000, then that policy would have that expected reward.
1572616	1583360	A	0.500519335269928	And so that might be more preferable than for sure one dollars or 10% of $1,000 in the reward centric approach.
1583780	1587616	A	0.8623202443122864	So then that's how you get explore and exploit balancing.
1587808	1602730	A	0.8433657884597778	You have exploratory trajectories being evaluated in terms of their expected returns in reward and then you can have everything compared on the common currency of expected pragmatic value in reward learning.
1603500	1607000	A	0.9046086668968201	Okay, so how are we going to approach it differently in active inference?
1607580	1614860	A	0.8786882162094116	Different trajectories of action are going to be compared again in a common currency.
1615280	1623730	A	0.6639043688774109	But rather than that common currency being expected reward, the common currency is going to be expected free energy.
1624420	1633410	A	0.6369154453277588	Expected free energy is going to have two components expected information gain and expected pragmatic value.
1635160	1643670	A	0.8779367208480835	As stated before, balancing those two terms into an adaptive zone is a fine tuning question.
1644040	1654520	A	0.7909753322601318	But this is how we approach the question in a first principles way by making a unified imperative that one includes perception and action.
1655260	1658500	A	0.8512787222862244	That's the integration of perception action loop.
1658580	1660904	A	0.8417036533355713	We see like observations and we see actions.
1660952	1667230	A	0.773257315158844	They're just all there in the same equation and it unifies epistemic and pragmatic value.
1667920	1687780	A	0.765084981918335	Instead of coercing epistemic actions into their expected reward and then comparing everything on the reward meter stick, we just have a single unified imperative, expected free energy that contains an epistemic and a pragmatic loading.
1689320	1690340	A	0.6781592965126038	Michael.
1692600	1693252	C	0.9301145076751709	Great stuff.
1693306	1694230	C	0.5908355712890625	That's all.
1696280	1707560	A	0.7727729678153992	Yeah, equation 2.6 gets revisited again and again and just different ways of seeing expected free energy so now we're going to continue seeing the teammates.
1708460	1713556	A	0.9210904240608215	So here the matrices are shown for this teammate.
1713588	1719420	A	0.671547532081604	So, yeah, there's the mouse aversive and beneficial outcome.
1720480	1722864	A	0.5278633832931519	Olivier yeah, sorry.
1722902	1731410	D	0.7564880847930908	No, it was just a bit like it's behind on the last equation 7.4 for you.
1731780	1736480	D	0.982537567615509	I find your last explanation really great and very enlightening.
1737460	1746050	D	0.887575626373291	And in a basic context of reinforcement learning, you will have somehow the first term within the second, is that what you're saying?
1749600	1758332	A	0.7535186409950256	It wouldn't be nested within the second strictly, but yes.
1758386	1768288	A	0.6487168669700623	Basically policies, one slot machine is giving you the 50% reward and then you don't know the other one.
1768454	1775344	A	0.5836067199707031	But a policy might be entertained and even selected because it has a high expected value.
1775542	1795860	A	0.5319268107414246	That is expected value based decision making because you say, well, I think slot machine, this slot machine might have a high value, so I'm going to take an exploratory action to kind of explore that, but not because of the information gain it gives me, but because of the expected reward.
1796020	1804220	A	0.7981345653533936	So it's kind of like converting everything to a pragmatic currency and then comparing the expected utility or the expected reward.
1804800	1814930	A	0.7707826495170593	And so you get utility discounting, you get time preference, all this stuff, but those are all correction factors within pragmatic value.
1815700	1816160	D	0.5491447448730469	Yeah.
1816230	1816864	D	0.6095885634422302	Okay, yeah.
1816902	1817216	D	0.8529649972915649	Thank you.
1817238	1818560	D	0.8168513178825378	Sorry for the interruptions.
1819700	1828260	A	0.7678607702255249	This is one of the key topics and this is how policies are selected.
1829240	1847112	A	0.6855485439300537	And that is the empirical challenge, is being able to parameterize everything so that it doesn't just behave trivially like a novelty seeking agent over here, or like a utility seeking agent over here.
1847246	1862216	A	0.9084184169769287	We want to have some training in the middle here's where the matrices are shown for these two settings.
1862248	1866450	A	0.8687939047813416	Here the block is on the right and here the white is on the right.
1867620	1870576	A	0.6945021748542786	And it just shows what the matrices look like.
1870758	1880868	A	0.5853668451309204	So it's helpful to see what the actual matrices look like in the generative model, because remember, this is the generative model.
1881034	1890516	A	0.8522337079048157	So if you define A, B and D, just like we saw in the music example, they're just specific matrices of whatever dimensionality it is that you're studying.
1890548	1903070	A	0.9036746025085449	If there was four notes, then D had four in a vector, a was a four by four, b was a four x four matrix in the music example.
1903520	1911544	A	0.847631573677063	Now when you add in policy, policy has a dimensionality of however many policies you can take.
1911682	1925760	A	0.8998355269432068	So if there's four options policy has, there's four options in the pi vector and then there's going to be a slice of B for each of the dimensionality of pi.
1926840	1933750	A	0.861719012260437	So if there were four options for what you could do, then there would be four slices in B.
1934120	1946970	A	0.8702170848846436	And it's kind of like, okay, if you choose action three, then B three third slice in b is going to be the one that we use to update S.
1948140	1955180	A	0.8289266228675842	So once you define this very limited, very interpretable set, of parameters.
1956160	1966928	A	0.8323715329170227	You will have stated the generative model and then you're ready to engage with standard methods for training and updating this generative model.
1967094	1991640	A	0.7878808975219727	And so that's why in active inference we spend a lot of time think about chapter six setting up our understanding of the problem because once you can describe the dimensionality and the parameters of the generative model that was the work, then you just update this as a statistical model using standard methods.
1993100	2017116	A	0.6844849586486816	So there's a lot of work in understanding the problem and in framing it and understanding what is the system of interest and all these other things, what form of degenerative model is appropriate but then you don't need to engage hopefully but actually in secondary engineering concerns for reasonable projects.
2017148	2020816	A	0.808615505695343	I'm sure at some very large scale these things do come back into into play.
2020998	2024392	A	0.9248021245002747	But that's kind of the cool thing about active inference.
2024476	2035990	A	0.8656535744667053	We understand what all these variables are and then when we want to model our own setting we just specify the generative model and kind of hit play.
2040250	2042310	A	0.8572782874107361	More matrices being shown.
2044170	2052014	A	0.8547143936157227	Here's the C vector and so plus six for the attractive stimulus and negative six for the aversive stimulus.
2052162	2068570	A	0.7985627055168152	Now if this were zero zero six and negative six so the amplitude of C were just smaller so the relative preferences would stay the same but the pragmatic term would just be smaller.
2068730	2071466	A	0.8374339938163757	So then the agent would behave more epistemically.
2071578	2079250	A	0.7819169759750366	Whereas if this were 6 trillion and negative 6 trillion then the pragmatic term would come to dominate.
2088540	2093000	A	0.8536084294319153	So again, what is the approach of the resolution?
2094160	2109900	A	0.7892768383026123	It's putting different trajectories of behavior, action selection on a common footing, expected free energy which unifies the epistemic and the pragmatic value of different actions.
2110060	2113504	A	0.7185161113739014	We don't say epistemic and pragmatic value are the same thing.
2113702	2116960	A	0.6761780977249146	In fact they're different but they're complementary.
2117400	2136090	A	0.6422175168991089	And so having a unified imperative helps us adaptively approach this question of the actual trade offs between courses of action that have known consequence and those that are providing new information.
2144280	2145492	A	0.8768413662910461	Anyone want to have a thought?
2145546	2146790	A	0.5779587030410767	Yeah, go for it.
2148440	2151796	C	0.5193254947662354	I'm a babe out of water in so many levels.
2151828	2168540	C	0.8167102336883545	But in my original field of training in economics we had this phrase we used often called Satiris paribus which was a way of kind of saying that's an extradinality to our models and we're just going to keep going with the way we do things, which is close enough.
2168690	2185004	C	0.8929035067558289	And the consequences of this kind of poor treatment of dilemmas was that things like pollution and other externalities were they might not fit in the representational model and therefore were kind of underweighted and trivialized.
2185052	2209972	C	0.7739376425743103	And what I'm hearing you make a case for, in a different discipline, in a different way is changing the math to handle what is otherwise trivialized as external, too inconsequential to be attended to, is embracing the tension and thus making it a part of what then can be the sense making model of the discipline overall.
2210116	2215390	C	0.49109166860580444	Which seems something we can learn from you all.
2218640	2222964	A	0.7553950548171997	Building not even one layer and connecting that to kind of good hearts.
2223032	2226080	A	0.6483669281005859	Paradox like a metric becomes Gamified.
2226580	2237460	A	0.5476862788200378	Do we care about the levels of lead in the measurement or do we care about the hidden unobserved levels of lead in the soil?
2237960	2247780	A	0.6769176721572876	And so in our systems model that hopefully everyone will be able to come to the table on, we could say, well really there's both.
2247930	2249140	A	0.6287274360656738	They're just different things.
2249210	2259320	A	0.6375014185905457	There's the true underlying distribution which we're never going to measure at every little grain of sand and then there's the sensor fusion that we have to do with observations.
2259980	2285680	A	0.8070012927055359	So let's not try to game the observations, but rather come to the table with a more holistic understanding of the relationship between hidden states and observations and how hidden states are changed and how our actions influence the hidden state distribution, but how that emits observations, just articulate the problem, break it down into the components.
2286120	2313630	A	0.5337005257606506	And then the amazing thing is these natural components of the situation again prior beliefs about hidden state underlying hidden states that are unobserved emission of observations, how the world changes through time, actions that influence how the world changes through time, our strategy of deciding on action, those are our naturally separable aspects of thinking about a situation.
2314400	2325650	A	0.8707966804504395	And in a way it's kind of amazing that all you have to do is just like smash the matrices together and the math does kind of work out again in a way that we would want it to.
2329300	2340100	A	0.7502299547195435	Yeah, it's Chill 7.4 is going to go into a little bit more detail on this epistemic value of policies.
2341000	2344180	A	0.6890877485275269	So there's a few different breakdowns here.
2344250	2357210	A	0.8585178256034851	They're all equal but this is a few different representations of just how we can think about the informational gain associated with a given policy.
2358300	2372380	A	0.7817429304122925	So, whereas the pragmatic value for a policy is fairly straightforwardly identified with how close the expected observations are to the preferences.
2373600	2381836	A	0.5975204110145569	So the most pragmatically valuable trajectory is the one where the body temperature is right in the middle of the temperature distribution.
2382028	2384690	A	0.6161414384841919	The least pragmatic one would be the further away.
2385060	2387904	A	0.8569716811180115	But here's where we see this I of Pi.
2388032	2391824	A	0.9092375040054321	So there's a few more descriptions on I of Pi.
2391952	2422076	A	0.6177604794502258	And so it turns out in this case with this teamase, initially the action that the mouse selects is to it takes an epistemic action to reduce uncertainty because it's like, okay, there might be some pragmatic value, but also it's a risky decision up here, but going down is going to be a policy selection that gives me a lot of information.
2422258	2428464	A	0.6875814199447632	Now this isn't the whole iguana as they brought up in the beginning of the book.
2428502	2436768	A	0.7769867181777954	Like how does the mouse come to know that the aboutness of the queue is this setting?
2436944	2453624	A	0.5156037211418152	So this isn't like from the big bang to the mouse making the decision, the total model, there's always stuff that's structurally encoded in the model how does it mouse come to know that the aversive stimulus is negative and so on.
2453742	2466424	A	0.8399876356124878	So these are levels that you can add in but this is just showing basically again in this simple example it's just showing what it looks like when the mouse undertakes an epistemic action.
2466472	2482252	A	0.8645035028457642	And in the Pymdp model stream 7.2, where there's a very similar example to this it's not in a Tmaze but it is with a mouse that gets some information and then makes a decision.
2482396	2485170	A	0.8372878432273865	So it's very similar to this.
2486520	2494390	A	0.8626678586006165	You could play around in the script and you can see what happens when you change the preference vector to like a super high or super low value.
2500360	2509640	A	0.8658190369606018	Okay, little bit of boxed discussion on precision and entropy.
2510800	2519950	A	0.5049318075180054	The h is associated with entropy but let's just continue on.
2521120	2529040	A	0.5010039806365967	That just relates to the over underfitting of the expected the expected sharpness or blurriness of a distribution.
2531380	2545750	A	0.6227360367774963	They move on to another of the settings that's been very well repeatedly studied in active inference which is the eye cicade setting the motion of the eyes.
2546440	2561100	A	0.6564347743988037	And so one reason why this has been studied a lot is that the fovea of the vision, the area that we have high resolution color vision is actually very, very small relative to the visual field.
2561250	2567790	A	0.8551471829414368	And that's why multiple times per second the eye is making these policy selections to move around.
2568240	2583276	A	0.5640515685081482	And those policy selections, although we might look at something that's beautiful or rewarding and have that fix our gaze, to a first approximation, the imperative of the eye circade is informational.
2583468	2599880	A	0.8718104362487793	It's gaining information about the visual field and so that is why the epistemic visual search is a really useful intangible setting.
2600300	2602120	A	0.6329861879348755	Again, it's not like gambling.
2605020	2615320	A	0.82853102684021	You're kind of setting up a situation towards pragmatic value but rather this is one where there's going to be an emphasis on the epistemic.
2615480	2627468	A	0.8321394324302673	And so this is like based upon the ambiguity of stimuli and about how epistemic attraction occurs.
2627644	2629650	A	0.7798609137535095	When there's information to learn.
2630020	2635570	A	0.5068307518959045	Where something where there's no information to learn doesn't that moving there does not have epistemic value.
2640520	2648080	A	0.8165768384933472	Kind of this is a little bit of a formalization of the streetlight story from every culture.
2648240	2650756	A	0.7594316601753235	Where did, where, you know, where are you looking for your keys?
2650788	2651764	A	0.7494862079620361	Under the streetlight.
2651892	2653320	A	0.6612832546234131	Oh, did you lose them there?
2653390	2656836	A	0.6293861865997314	No, this is just where I could look clearly.
2657028	2675810	A	0.5041271448135376	So that is about looking where there's light and searching where you can not necessarily because you have a high prior that the information is there but just because it's easily resolvable under the streetlight best place to find high quality unambiguous uncertainty resolving information.
2678340	2684736	A	0.729659378528595	So the ambiguous poorly lit square is ignored reproducing in silico streetlight effect.
2684918	2705224	A	0.6354122161865234	If you're not going to get resolvable information it's not of epistemic value to go there but if it is going to be high quality uncertainty reducing information that is like epistemically valuable 75 Learning and Novelty so in.
2705262	2715390	A	0.9222431778907776	Figure four, three in seven, where we, where we revisit it in 73.
2716320	2727200	A	0.774005651473999	There's no learning in this model as written, there's updating of the hidden states by moving it through the B matrix.
2728740	2733570	A	0.7005680799484253	But the A, B and D themselves do not change.
2734740	2739830	A	0.8459218740463257	So now they're going to talk about how learning can come into play.
2740520	2745060	A	0.7575162053108215	So in other words, like the A matrix is fixed in this setting.
2746040	2751528	A	0.816366970539093	But you might be interested in a situation where the A matrix can change through time.
2751694	2757530	A	0.8742568492889404	Like maybe the mapping between hidden states and observations changes.
2759420	2775970	A	0.8646400570869446	The way that learning is generally approached in Bayesian inference settings is you make a prior distribution on the variable of interest.
2776660	2785570	A	0.8771232962608337	So for example, lowercase A is going to be a hyper prior or just a prior distribution on A.
2786600	2804250	A	0.626533031463623	So instead of just saying A is seven one one like we saw in the Music example, you can have a distribution over what A looks like that is going to help with learning.
2804780	2819420	A	0.6475476622581482	In the extreme case where learning rate converges to zero, that's like a super sharp distribution around what you think A is going to be like so that you have very strong prior on A.
2819570	2823810	A	0.7520745396614075	So new information is only going to move that sharp distribution a tiny amount.
2824340	2840790	A	0.7819322943687439	Conversely, if you were having a lot of learning on A, you'd have a very flat prior on what A could be so that new observations would update your beliefs about A a lot.
2842040	2858120	A	0.6694310307502747	But this is just an example of the composability of this framework in that the base model doesn't have learning attention, all these different relatively sophisticated cognitive phenomena.
2859100	2875704	A	0.8234403133392334	But this is like the little firmware nugget or kernel which by understanding the compositionality relationships of then we can start to bring in learning on different parameters.
2875832	2879408	A	0.9865887761116028	And of course learning is awesome and it sounds awesome, it is.
2879574	2905490	A	0.5370115041732788	But if you also think about it from a statistical perspective, that doesn't mean that every parameter should be learnable because sometimes you're just increasing the computational challenge of your problem vastly more information on learning.
2909800	2921080	A	0.7054599523544312	Here we see another difference with learning between active inference and many other machine learning approaches.
2921900	2928110	A	0.871330976486206	And this is the embedding of action in the inference process.
2929040	2934190	A	0.8506038784980774	We actively think about how much information are we going to get.
2935040	2941360	A	0.6553410887718201	Like when people are training an image recognition, it just here's a thousand images in the training set just plug and chug.
2941780	2948800	A	0.8111931085586548	But what if you had an agent that was like which image should I look in of these thousands?
2949240	2962970	A	0.8912172317504883	Then it would become of primary importance which sequence of learning observations the agents took within those thousand labeled data points.
2964380	2977210	A	0.5720672011375427	So that's what G helps us understand some new terms salience, novelty, different decompositions of G.
2981450	2990250	A	0.6674872636795044	One final example, again, more just like referenced because it's in other papers but not really explained with the matrices.
2990670	3005840	A	0.8996955156326294	Here we have a maze exploration agent which is like another kind of classic setting so it has gray in the likelihood, which is like 50 50 between white and black.
3006290	3012874	A	0.7743165493011475	But it starts off, it takes these three steps and it's like, okay, it's updating, now it moves.
3012922	3014306	A	0.6971744298934937	Here updates that.
3014328	3018340	A	0.812982439994812	Okay, that's a white square, that's a white square, that's a block square, that's a block square.
3018950	3029970	A	0.6451252698898315	So this is like somebody looking over a maze visually seeking a path to solve the maze.
3034250	3056400	A	0.6632525324821472	7.3 is a little bit of a summary of a pretty advanced topic which is Bayesian model reduction and structure learning which is treating the question of what structure should the generative model have?
3056930	3062000	A	0.5388379693031311	Treating that question as a parametric inference problem.
3063190	3080920	A	0.8546866774559021	So should the model be one, two or three time steps deep and then say, okay, well we're going to have some time horizon parameter and then we're going to test whether one, two or three is the better model.
3081610	3105630	A	0.8910398483276367	So like we're going to learn the structure of a model as part of reducing down from a portfolio of models but they just sort of briefly note it and there's more citations and then as and then any other just thoughts or questions.
3105700	3110190	A	0.8093590140342712	Seven kind of has a lot of little short vignettes.
3116340	3125750	A	0.7967783808708191	Okay, just briefly, just so that we touch it all, then next time we can have voted and added a lot of questions and everything.
3126120	3128852	A	0.6613659858703613	712 is a two level model.
3128906	3131690	A	0.7323347926139832	It's a nested or a hierarchical model.
3132540	3138200	A	0.8607029318809509	Both the higher and the lower level, one and two are discrete time.
3138350	3142828	A	0.8264598250389099	You can tell because they all have the s t minus one t t plus one.
3142994	3151150	A	0.8648076057434082	So later we'll see a hierarchical model where the lower level is continuous time and the top level is discrete time.
3151680	3156210	A	0.8370628952980042	But in this setting both nested levels are discrete time.
3156740	3163890	A	0.8175087571144104	So this could be every hour and this is 60 minutes within an hour, every day, 24 hours within a day.
3167380	3188916	A	0.7926406264305115	Here's a multi scale updating generative model that's related to sentence reading and they've done a lot of simulation studies of multilevel inference in reading, like where the eyes are moving to resolve uncertainty about the letter.
3189108	3193884	A	0.843694806098938	Order of letters is sought out to reduce uncertainty about words.
3194082	3199100	A	0.830624520778656	Order of words in a sentence is sought out to reduce uncertainty about semantics.
3199520	3203070	A	0.8215245008468628	Semantics are operated on for pragmatic value.
3206100	3207984	A	0.8289855718612671	So that's chapter seven.
3208182	3219744	A	0.861909806728363	It's a fairly loosely connected sequence of discrete time models.
3219872	3239732	A	0.8900412917137146	Several examples, starting from the musical example, which is just a passive inference hidden Markov model bringing us back to the discrete time generative model that we saw in Figure 4.3 that's shown again in 7.3 and then several decompositions.
3239796	3264000	A	0.7611407041549683	Of expected free energy focusing on the Epistemic value, especially in terms of how getting Epistemic and Pragmatic value unified under this G functional allows us to approach the adaptive finessing of exploratory and exploitative behavior.
3268590	3273470	A	0.6516234874725342	And again, the work is in understanding and designing the generative models.
3276670	3278330	A	0.6891095638275146	That's not work about work.
3278400	3295600	A	0.8119325637817383	That's really the task itself is the systems modeling and systems understanding, because essentially, by saying what these variables are, what shape these variables are, you will have prepared your model to run.
3296370	3301840	A	0.5055676102638245	And until these models are defined in terms of their dimensionality, it's not ready to run.
3304840	3309688	A	0.8816298246383667	Any kind of closing thoughts, Michael?
3309744	3314430	C	0.7681916356086731	It looks like Martin might have had to do the math class after all.
3318960	3319900	A	0.5810921788215637	There's so many.
3319970	3320380	A	0.5801408290863037	I mean.
3320450	3321550	C	0.7476216554641724	As will I.
3323700	3324784	A	0.6419163942337036	There'd be so many.
3324822	3338090	A	0.8889791965484619	And and again, like, let's try to let's explore how the let's see how the the AI explanations did for chapter seven math explanations.
3338170	3339840	A	0.517223060131073	Why would a high schooler be interested?
3343900	3346810	A	0.8822912573814392	Let's look at it for one of the epistemic ones.
3351420	3379708	A	0.6911923885345459	People can evaluate and add comments or change the text, but for those who are learning, and ultimately we all are, I doubt there are few, if any, people for whom this equation doesn't require contextualization and clarification and deeper breathing.
3379884	3388644	A	0.5508427023887634	Yeah, these lines are not it's not like, divide by two and then do this.
3388682	3391700	A	0.6820697784423828	It's like these are multi step derivations.
3393980	3403050	A	0.6487262845039368	And so going back to the accuracy minus compression, that's the challenge of writing a textbook accuracy minus compression of what they had to share.
3406640	3413180	A	0.8126981854438782	So our work is to kind of refine that balance for ourselves.
3414160	3415464	A	0.9184439778327942	Some of these look helpful.
3415512	3417020	A	0.9428170323371887	Some of them don't look helpful.
3418260	3421104	A	0.8432465195655823	In terms of the AI answers, I.
3421142	3422800	C	0.5706192851066589	Would use the word scary.
3429310	3430506	C	0.7581890821456909	I got to run, by the way.
3430528	3430762	A	0.7671424746513367	Great.
3430816	3431290	A	0.8529649972915649	Thank you.
3431360	3431546	A	0.5491447448730469	Yeah.
3431568	3432810	A	0.7836206555366516	Any other comments?
3437470	3438202	A	0.8581835031509399	All right, great.
3438256	3442442	A	0.8460819125175476	Well, next time, second discussion on chapter seven.
3442496	3446914	A	0.9112497568130493	So we'll return and look at what questions have been upvoted or asked.
3447032	3448658	A	0.9631696939468384	So thank you all.
3448824	3449650	A	0.5942320823669434	Farewell.
3450470	3451330	A	0.5137446522712708	Bye.
3453190	3453680	D	0.8529649972915649	Thank you.

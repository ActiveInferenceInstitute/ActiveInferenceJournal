1
00:00:00,560 --> 00:00:04,070
You. Hello everyone.

2
00:00:04,760 --> 00:00:08,192
We're in our first discussion of chapter

3
00:00:08,256 --> 00:00:11,444
seven in cohort three.

4
00:00:11,642 --> 00:00:15,744
So let's head over to chapter

5
00:00:15,792 --> 00:00:19,684
seven and we will come to

6
00:00:19,722 --> 00:00:23,252
the questions. But let's just

7
00:00:23,306 --> 00:00:27,150
start with anyone who wants to

8
00:00:27,680 --> 00:00:29,710
do you want to just give a general

9
00:00:30,480 --> 00:00:33,772
comment or any reflection or thought on

10
00:00:33,826 --> 00:00:37,550
seven or anything up to this point?

11
00:00:43,020 --> 00:00:45,610
Ali and then anyone else?

12
00:00:49,020 --> 00:00:52,744
Yes. So chapter seven is basically the

13
00:00:52,782 --> 00:00:56,312
application of the first half of chapter

14
00:00:56,376 --> 00:01:00,808
four in terms of modeling discrete

15
00:01:00,824 --> 00:01:06,056
time situations. And it

16
00:01:06,098 --> 00:01:09,840
goes through a number of case studies

17
00:01:10,340 --> 00:01:14,224
in order to show how the matrices A-B-C

18
00:01:14,262 --> 00:01:16,924
and D can be constructed in various

19
00:01:17,052 --> 00:01:21,524
situations and the

20
00:01:21,562 --> 00:01:25,856
challenges or rather the concerns

21
00:01:26,048 --> 00:01:28,532
we may have when constructing such

22
00:01:28,586 --> 00:01:32,292
matrices in order to have a viable

23
00:01:32,436 --> 00:01:36,330
discrete time model for each situation.

24
00:01:37,180 --> 00:01:41,464
So, yeah, that's basically the

25
00:01:41,502 --> 00:01:44,110
premise of chapter seven.

26
00:01:46,560 --> 00:01:50,204
Awesome, thank you. Totally agree. Just

27
00:01:50,242 --> 00:01:51,916
like chapters two and three with low

28
00:01:51,938 --> 00:01:53,628
road and the high road, we're kind of

29
00:01:53,634 --> 00:01:56,540
like a pair. Chapters seven and eight

30
00:01:56,610 --> 00:01:59,056
are kind of like a pair because seven is

31
00:01:59,078 --> 00:02:01,740
going to go into discrete time modeling.

32
00:02:01,900 --> 00:02:04,144
Chapter eight is on continuous time and

33
00:02:04,182 --> 00:02:07,216
hybrid modeling. Does anyone else want

34
00:02:07,238 --> 00:02:10,772
to make any just overall reflection or

35
00:02:10,826 --> 00:02:12,710
comment on chapter seven?

36
00:02:34,400 --> 00:02:35,310
All right,

37
00:02:38,970 --> 00:02:41,560
I'm checking if there's any okay.

38
00:02:42,430 --> 00:02:44,234
Does anyone have any question at all?

39
00:02:44,272 --> 00:02:46,490
Otherwise we'll turn first to these

40
00:02:46,560 --> 00:02:49,866
questions, start with

41
00:02:49,888 --> 00:02:53,034
the written questions we have and

42
00:02:53,072 --> 00:02:55,086
then just approach it from there. Does

43
00:02:55,108 --> 00:02:56,638
anyone have anything else that they want

44
00:02:56,724 --> 00:02:58,560
to consider first though?

45
00:03:05,940 --> 00:03:06,690
Okay,

46
00:03:10,820 --> 00:03:14,464
I'm putting in the chat, that's the

47
00:03:14,582 --> 00:03:18,416
social sciences link, but here's

48
00:03:18,448 --> 00:03:21,284
the chapter seven link in the chat and

49
00:03:21,322 --> 00:03:24,340
so you can follow along there. Also

50
00:03:24,410 --> 00:03:26,644
please upvote the questions that are

51
00:03:26,682 --> 00:03:30,280
interesting to you because the questions

52
00:03:30,350 --> 00:03:32,408
that have a bunch of upvoted points

53
00:03:32,494 --> 00:03:34,056
we're going to make the short videos

54
00:03:34,158 --> 00:03:34,810
for.

55
00:03:37,420 --> 00:03:41,048
Okay, so we'll just go to the

56
00:03:41,134 --> 00:03:43,244
I'm going to unvote all the questions

57
00:03:43,362 --> 00:03:45,084
from my own you can still vote for them,

58
00:03:45,122 --> 00:03:46,476
but I'm just going to unvote them so

59
00:03:46,498 --> 00:03:48,716
that once we get to them, I'll vote for

60
00:03:48,738 --> 00:03:49,310
them.

61
00:03:51,960 --> 00:03:56,010
Okay. All right, first question

62
00:04:00,060 --> 00:04:02,760
here's, chapter seven. Chapter seven

63
00:04:02,830 --> 00:04:04,836
begins with a quotation. What I cannot

64
00:04:04,868 --> 00:04:06,804
create, I do not understand. Richard

65
00:04:06,852 --> 00:04:09,036
Feigman, what do you think this means in

66
00:04:09,058 --> 00:04:10,696
the context of active inference?

67
00:04:10,728 --> 00:04:11,660
Specifically,

68
00:04:28,970 --> 00:04:31,414
I can't remember exactly who added this

69
00:04:31,532 --> 00:04:33,590
in a previous cohort,

70
00:04:34,350 --> 00:04:37,622
but they gave a mild and a stronger

71
00:04:37,686 --> 00:04:40,810
answer. The mild answer is

72
00:04:40,960 --> 00:04:43,242
you have to learn by doing. You actually

73
00:04:43,296 --> 00:04:45,274
have to build the generative model to

74
00:04:45,312 --> 00:04:48,090
understand active inference.

75
00:04:50,430 --> 00:04:52,734
And then the stronger answer goes even

76
00:04:52,772 --> 00:04:55,006
beyond that. And it's not just that you

77
00:04:55,028 --> 00:04:56,894
have to build the generative model, you

78
00:04:56,932 --> 00:04:58,382
actually have to have something that

79
00:04:58,436 --> 00:05:01,010
escapes your hands and does something

80
00:05:01,080 --> 00:05:04,414
interesting to really test the method

81
00:05:04,462 --> 00:05:05,780
and understand it.

82
00:05:08,230 --> 00:05:11,074
Anyone else want to add a comment on

83
00:05:11,112 --> 00:05:11,700
this?

84
00:05:17,750 --> 00:05:20,758
Okay, kind of a warm up question,

85
00:05:20,844 --> 00:05:22,982
but anyone else want to add anything on

86
00:05:23,036 --> 00:05:24,854
this? What I cannot create, I do not

87
00:05:24,892 --> 00:05:27,734
understand. Otherwise we'll come into

88
00:05:27,772 --> 00:05:29,722
the more specifics of the chapter seven

89
00:05:29,776 --> 00:05:30,380
now.

90
00:05:36,350 --> 00:05:39,580
Okay, so in that case, I'm just going to

91
00:05:40,110 --> 00:05:43,980
go with the most upvoted questions.

92
00:05:48,050 --> 00:05:48,800
Okay.

93
00:05:53,340 --> 00:05:55,528
All right. This is on page 130, the

94
00:05:55,534 --> 00:05:57,624
textbook they wrote. This choice, which

95
00:05:57,662 --> 00:05:59,416
we're going to look to the textbook in a

96
00:05:59,438 --> 00:06:01,576
second, speaks to the classical

97
00:06:01,688 --> 00:06:04,476
exploration exploitation dilemma and

98
00:06:04,498 --> 00:06:07,224
psychology, a dilemma that is resolved

99
00:06:07,272 --> 00:06:09,596
under active inference. What do you

100
00:06:09,618 --> 00:06:12,030
think about this? Okay,

101
00:06:12,980 --> 00:06:17,180
so the setting that's

102
00:06:17,340 --> 00:06:21,330
being considered is decision making

103
00:06:21,860 --> 00:06:25,196
under uncertain payoff or reward

104
00:06:25,308 --> 00:06:27,856
structure. So you think there's two slot

105
00:06:27,888 --> 00:06:31,076
machines, one of them pays out 50% of

106
00:06:31,098 --> 00:06:33,156
the time. You don't know how much the

107
00:06:33,178 --> 00:06:35,764
other one pays out. So you could

108
00:06:35,802 --> 00:06:38,904
continue to get 50% that would be

109
00:06:38,942 --> 00:06:41,640
exploit, or you could explore,

110
00:06:42,300 --> 00:06:45,016
and it might be better than 50%, it

111
00:06:45,038 --> 00:06:47,576
might be worse than 50%. And so that is

112
00:06:47,598 --> 00:06:49,380
sometimes called the exploration

113
00:06:49,460 --> 00:06:53,052
exploitation dilemma. Does anyone

114
00:06:53,106 --> 00:06:55,432
want to give a comment or a related

115
00:06:55,496 --> 00:06:57,736
question on how is active inference

116
00:06:57,768 --> 00:07:00,140
going to be approaching Explore exploit?

117
00:07:08,990 --> 00:07:12,640
Michael yeah, I just

118
00:07:13,410 --> 00:07:17,582
wanted to both affirm that I found the

119
00:07:17,716 --> 00:07:20,334
mathematical representation of these two

120
00:07:20,372 --> 00:07:22,094
ideas, that it's a trade off very

121
00:07:22,132 --> 00:07:24,354
interesting. But to describe them as

122
00:07:24,392 --> 00:07:28,082
resolved is something

123
00:07:28,136 --> 00:07:30,580
that I find might be a little more

124
00:07:35,110 --> 00:07:37,060
overdescribed, if you will.

125
00:07:40,410 --> 00:07:43,682
Maybe the word is this formula

126
00:07:43,746 --> 00:07:45,526
describes the trade off or as a way of

127
00:07:45,548 --> 00:07:48,186
representing it, but resolution is a

128
00:07:48,208 --> 00:07:51,418
much more open topic. So just thought I

129
00:07:51,424 --> 00:07:55,194
would give a response, give feedback and

130
00:07:55,232 --> 00:07:56,780
say, well.

131
00:07:58,590 --> 00:08:00,620
Yeah, that's it. Yes,

132
00:08:02,850 --> 00:08:05,342
maybe resolve in the sense of like a

133
00:08:05,396 --> 00:08:07,614
high resolution image, like we can see

134
00:08:07,652 --> 00:08:10,382
it. We've resolved the dilemma by

135
00:08:10,436 --> 00:08:13,906
perceiving it not we have explained it

136
00:08:13,928 --> 00:08:16,946
away or said that there is no trade off.

137
00:08:17,128 --> 00:08:19,186
I believe this was Eric Sounds

138
00:08:19,288 --> 00:08:22,798
previously, who kind of contested

139
00:08:22,894 --> 00:08:24,658
the idea that Explore exploit is a

140
00:08:24,664 --> 00:08:28,898
dilemma. It's not a dilemma.

141
00:08:28,994 --> 00:08:30,534
There's not two options that are

142
00:08:30,572 --> 00:08:32,262
intrinsically opposed. There's just one

143
00:08:32,316 --> 00:08:34,790
integrated behavioral choice.

144
00:08:35,850 --> 00:08:39,146
All of this being said, let's look at

145
00:08:39,168 --> 00:08:41,094
how Explore exploit our approach

146
00:08:41,142 --> 00:08:44,646
differently in active

147
00:08:44,678 --> 00:08:47,862
inference. One more remark

148
00:08:47,926 --> 00:08:51,822
might be that in a different sense

149
00:08:51,876 --> 00:08:56,046
making context where the

150
00:08:56,148 --> 00:08:59,946
explore versus exploit issue arose

151
00:09:00,138 --> 00:09:03,614
continuously, the word sharpening

152
00:09:03,662 --> 00:09:08,994
visibility was a way of describing the

153
00:09:09,032 --> 00:09:13,186
space that sometimes just your

154
00:09:13,208 --> 00:09:15,154
resolution comment helps remind me of

155
00:09:15,192 --> 00:09:17,078
that. That it was like boosting the

156
00:09:17,084 --> 00:09:19,826
visibility of the space does not resolve

157
00:09:19,858 --> 00:09:21,574
the problems that you're trying to

158
00:09:21,612 --> 00:09:24,902
address, but it does improve your

159
00:09:24,956 --> 00:09:27,574
ability to act on the space. And I guess

160
00:09:27,612 --> 00:09:31,034
that's maybe a suggestion for

161
00:09:31,072 --> 00:09:33,462
language boosting visibility versus

162
00:09:33,526 --> 00:09:38,074
resolving that's. All yes.

163
00:09:38,192 --> 00:09:38,860
Awesome.

164
00:09:41,230 --> 00:09:44,094
Yeah. Just to briefly catch us up to how

165
00:09:44,132 --> 00:09:47,870
we get to this claim, chapter seven,

166
00:09:47,940 --> 00:09:49,854
like Ollie mentioned, is going to be

167
00:09:49,892 --> 00:09:51,934
concerned with a series of examples of

168
00:09:51,972 --> 00:09:53,742
building discrete time generative

169
00:09:53,806 --> 00:09:55,522
models. So these are models where time

170
00:09:55,576 --> 00:09:57,940
clicks like one step at a time.

171
00:10:00,470 --> 00:10:03,954
Figure 7.1 is showing just the

172
00:10:03,992 --> 00:10:07,646
perceptual or the downstairs component

173
00:10:07,678 --> 00:10:09,686
of a generative model. So D is the

174
00:10:09,708 --> 00:10:12,486
prior, S are the hidden states, b are

175
00:10:12,508 --> 00:10:13,814
how the hidden states change through

176
00:10:13,852 --> 00:10:17,106
time. A is the matrix that maps hidden

177
00:10:17,138 --> 00:10:19,626
states to observations. So this is just

178
00:10:19,648 --> 00:10:21,366
the temperature in the room changing

179
00:10:21,398 --> 00:10:23,894
through time outputting. A thermometer

180
00:10:23,942 --> 00:10:26,538
reading at every time point. This is

181
00:10:26,704 --> 00:10:30,226
hidden Markov model. Not a Markov

182
00:10:30,278 --> 00:10:32,746
decision process, just a hidden Markov

183
00:10:32,778 --> 00:10:34,030
model. Hmm.

184
00:10:36,610 --> 00:10:40,270
They then write the A and the B matrix

185
00:10:40,770 --> 00:10:44,670
as well as the D for a musical setting.

186
00:10:44,830 --> 00:10:47,954
So there's four notes. The A matrix is

187
00:10:47,992 --> 00:10:49,570
basically saying you hear the correct

188
00:10:49,640 --> 00:10:53,300
note 70% of the time,

189
00:10:54,870 --> 00:10:57,618
and then the B is the transition matrix.

190
00:10:57,794 --> 00:11:00,966
So this is just playing four notes that

191
00:11:01,068 --> 00:11:03,814
transition to each other 97% of the time

192
00:11:03,852 --> 00:11:07,062
in this order. And it always

193
00:11:07,116 --> 00:11:10,474
starts on the first note. So with just

194
00:11:10,512 --> 00:11:13,466
those three matrices defined A,

195
00:11:13,568 --> 00:11:16,986
B, and D, you have fully defined the

196
00:11:17,008 --> 00:11:19,526
generative model for this. Hmm. There's

197
00:11:19,558 --> 00:11:22,046
no decision making yet, but that's all

198
00:11:22,068 --> 00:11:23,950
you need to define this. Hmm.

199
00:11:26,770 --> 00:11:29,006
And then in Figure 7.2, there's an

200
00:11:29,028 --> 00:11:32,846
example where actually there turns

201
00:11:32,878 --> 00:11:36,626
out in this simulation that

202
00:11:36,808 --> 00:11:39,698
the third observation, it should be

203
00:11:39,704 --> 00:11:42,926
here, but it's actually here. So that's

204
00:11:42,958 --> 00:11:45,766
what's heard because there's such a

205
00:11:45,788 --> 00:11:48,360
prior that that's what should be heard.

206
00:11:50,570 --> 00:11:53,906
Okay, now we get into action,

207
00:11:54,098 --> 00:11:56,866
which is where the explore and exploit

208
00:11:56,898 --> 00:11:58,538
are going to come back into play. So if

209
00:11:58,544 --> 00:12:00,646
it's just a purely passive inference,

210
00:12:00,758 --> 00:12:06,806
then the kind of dilemma or unavoidable

211
00:12:06,838 --> 00:12:09,206
trade off is like over and under fitting

212
00:12:09,318 --> 00:12:11,514
perceptually, making something that's

213
00:12:11,562 --> 00:12:14,894
over compressed or under compressed with

214
00:12:14,932 --> 00:12:18,190
your beliefs. But action

215
00:12:18,690 --> 00:12:21,498
has a different set of challenges

216
00:12:21,674 --> 00:12:23,890
because it's related to selection of

217
00:12:24,040 --> 00:12:26,466
causal intervention in the world and it

218
00:12:26,488 --> 00:12:28,494
changes sequences of future observations

219
00:12:28,542 --> 00:12:31,266
and all of this. And as we know from the

220
00:12:31,288 --> 00:12:34,210
expected free energy, the imperative for

221
00:12:34,280 --> 00:12:37,634
action is to have both epistemic

222
00:12:37,682 --> 00:12:40,518
value, like to gain information, and to

223
00:12:40,524 --> 00:12:43,154
have pragmatic value, to have alignment

224
00:12:43,202 --> 00:12:44,806
between your observations, your

225
00:12:44,828 --> 00:12:49,100
preferences. Here's figure 7.3.

226
00:12:49,470 --> 00:12:52,294
This is kind of the classic generative

227
00:12:52,342 --> 00:12:55,658
model in discrete time. So we

228
00:12:55,664 --> 00:12:58,202
see that same downstairs component, the

229
00:12:58,256 --> 00:13:01,814
exact same. Plus there's the upstairs

230
00:13:01,862 --> 00:13:05,680
decision making component, pi policy

231
00:13:06,290 --> 00:13:09,246
intervening into how states change their

232
00:13:09,268 --> 00:13:14,862
time, expected free energy g updating

233
00:13:15,006 --> 00:13:18,274
policy decisions, updating them based

234
00:13:18,312 --> 00:13:20,100
upon their expected free energy.

235
00:13:29,420 --> 00:13:32,356
Michael? Yeah, sorry. Yeah, please. I'm

236
00:13:32,388 --> 00:13:36,056
jumping back 60 seconds, but right

237
00:13:36,078 --> 00:13:38,824
before you described that diagram, you

238
00:13:38,942 --> 00:13:42,564
said how you compress the I can't

239
00:13:42,612 --> 00:13:43,884
remember, but you use the word

240
00:13:43,922 --> 00:13:45,964
compression. And I wondered, why is he

241
00:13:46,002 --> 00:13:49,436
using compression as the verb to

242
00:13:49,458 --> 00:13:51,164
describe what's happening? And are you

243
00:13:51,202 --> 00:13:54,000
suggesting that in the same way that

244
00:13:54,070 --> 00:13:57,852
let's say when you compress

245
00:13:57,916 --> 00:14:01,344
data onto to

246
00:14:01,382 --> 00:14:03,136
save more storage space, you do an

247
00:14:03,158 --> 00:14:04,976
abstraction, reduce it, and it's a

248
00:14:04,998 --> 00:14:08,724
compression of what the data is.

249
00:14:08,762 --> 00:14:12,768
That is what's happening. There is a I'm

250
00:14:12,784 --> 00:14:14,516
sorry to be wordsmithing here. I'm just

251
00:14:14,538 --> 00:14:15,716
trying to make sure I'm tracking what

252
00:14:15,738 --> 00:14:19,290
you were saying when you said okay.

253
00:14:20,380 --> 00:14:22,490
It is very closely related to this.

254
00:14:24,380 --> 00:14:26,584
There's two ways to think about it. One

255
00:14:26,622 --> 00:14:28,536
is like we're, we're fitting some

256
00:14:28,558 --> 00:14:31,156
gaussian, let's just say some bell curve

257
00:14:31,268 --> 00:14:33,756
over some uncertainty. So we could think

258
00:14:33,778 --> 00:14:35,496
about like, do we want over compressing

259
00:14:35,528 --> 00:14:38,968
it in terms of tightening the variance?

260
00:14:39,144 --> 00:14:41,784
That would be like overfitting versus

261
00:14:41,832 --> 00:14:43,472
like if it's loose, then it's like

262
00:14:43,526 --> 00:14:45,168
under. So that's a little bit like a

263
00:14:45,174 --> 00:14:47,184
loose sense of compression, really.

264
00:14:47,222 --> 00:14:50,656
It's just a variance estimation. Or you

265
00:14:50,678 --> 00:14:52,572
can think about complexity minus

266
00:14:52,636 --> 00:14:56,704
accuracy as being like optimal zip

267
00:14:56,752 --> 00:14:59,636
compression. Like obviously if you

268
00:14:59,658 --> 00:15:03,028
wanted a lossless compression you wanted

269
00:15:03,114 --> 00:15:05,088
perfect accuracy, then you're opening

270
00:15:05,104 --> 00:15:07,990
the door to an infinitely complex model.

271
00:15:09,100 --> 00:15:10,984
On the other hand, if you're willing to

272
00:15:11,022 --> 00:15:13,304
find a trade off between complexity and

273
00:15:13,342 --> 00:15:15,624
accuracy, which is summarized by

274
00:15:15,662 --> 00:15:17,930
variational free energy in the moment,

275
00:15:18,780 --> 00:15:21,756
then you would end up with at the point

276
00:15:21,778 --> 00:15:23,708
of diminishing returns, like the kind of

277
00:15:23,714 --> 00:15:26,270
the Pareto optimal compression point.

278
00:15:30,800 --> 00:15:33,984
Thank you. Sorry to no, it's all good.

279
00:15:34,102 --> 00:15:37,308
So now to study this action setting,

280
00:15:37,484 --> 00:15:39,264
which is where explore exploit comes

281
00:15:39,302 --> 00:15:42,144
into play, we're going to switch from

282
00:15:42,182 --> 00:15:45,650
the music listening example,

283
00:15:46,440 --> 00:15:48,804
which was just kind of familiarizing us

284
00:15:48,842 --> 00:15:51,968
with the A, B and the D. And now we're

285
00:15:51,984 --> 00:15:54,468
going to bring in action, which is going

286
00:15:54,474 --> 00:15:58,250
to require talking about Pi policy

287
00:15:58,700 --> 00:16:01,320
and C our preferences.

288
00:16:03,100 --> 00:16:07,160
Okay, so it's a T maze task.

289
00:16:08,540 --> 00:16:11,340
There's an aversive stimulus in one arm,

290
00:16:12,160 --> 00:16:14,860
an attractive stimulus in the other arm,

291
00:16:15,200 --> 00:16:17,916
and a Q that indicates the location of

292
00:16:17,938 --> 00:16:20,990
the two stimuli in the bottom of the T.

293
00:16:21,920 --> 00:16:26,480
So the organism can either pick

294
00:16:26,550 --> 00:16:29,170
one of the arms left or right,

295
00:16:30,260 --> 00:16:32,496
maybe quickly get to the reward, but

296
00:16:32,518 --> 00:16:35,536
maybe get the aversive stimulus. Or it

297
00:16:35,558 --> 00:16:37,024
could choose to seek out this

298
00:16:37,062 --> 00:16:40,016
informative cue and then having reduced

299
00:16:40,048 --> 00:16:42,772
its uncertainty about the location of

300
00:16:42,826 --> 00:16:47,220
the reward, then just

301
00:16:47,290 --> 00:16:50,728
walk over to the reward. So that's going

302
00:16:50,734 --> 00:16:54,232
to be one, two, three steps instead

303
00:16:54,286 --> 00:16:55,688
of just going straight to one of the

304
00:16:55,694 --> 00:16:59,704
arms, but you

305
00:16:59,742 --> 00:17:01,452
then have higher accuracy with getting

306
00:17:01,506 --> 00:17:06,156
to one of the arms. So how

307
00:17:06,258 --> 00:17:09,992
do you get flexible and rapid

308
00:17:10,056 --> 00:17:14,620
and adaptive switching between pragmatic

309
00:17:14,700 --> 00:17:17,644
oriented behavior and epistemic oriented

310
00:17:17,692 --> 00:17:21,090
behavior especially? How can that happen

311
00:17:21,940 --> 00:17:24,768
as new information are rolling in

312
00:17:24,934 --> 00:17:27,956
without necessarily retraining the

313
00:17:27,978 --> 00:17:31,652
model? That is

314
00:17:31,786 --> 00:17:34,230
what is going to be explored here.

315
00:17:36,600 --> 00:17:38,596
Anyone else want to just add a comment

316
00:17:38,628 --> 00:17:43,304
or thought or how

317
00:17:43,342 --> 00:17:44,970
they see it at this point?

318
00:17:48,700 --> 00:17:51,756
Yeah, have a general question in terms

319
00:17:51,778 --> 00:17:53,676
of if you do an experiment with

320
00:17:53,858 --> 00:17:56,956
different animals, for example, or

321
00:17:56,978 --> 00:17:59,724
different observers, and you want to

322
00:17:59,762 --> 00:18:02,976
model different balance between this

323
00:18:02,998 --> 00:18:06,800
epistemic and pragmatic behavior,

324
00:18:09,140 --> 00:18:12,860
where in the ABCD matrices

325
00:18:12,940 --> 00:18:15,910
would that be? Yeah,

326
00:18:16,280 --> 00:18:19,604
very good question. One place that

327
00:18:19,642 --> 00:18:23,568
this plays out is in the amplitude

328
00:18:23,664 --> 00:18:28,708
of C. So let's

329
00:18:28,724 --> 00:18:32,168
just say that left and right are the two

330
00:18:32,254 --> 00:18:35,850
directions on the teammates and

331
00:18:37,100 --> 00:18:39,016
we don't have a habit for either one of

332
00:18:39,038 --> 00:18:43,340
them if we prefer the

333
00:18:43,490 --> 00:18:45,768
sugar over the aversive. So now we're

334
00:18:45,784 --> 00:18:48,044
going to talk about the C matrix, if

335
00:18:48,082 --> 00:18:51,228
it's one and zero so aversive we

336
00:18:51,234 --> 00:18:53,456
don't really care about, then we give a

337
00:18:53,478 --> 00:18:57,340
value of one for the reward versus

338
00:18:57,420 --> 00:19:00,048
if the C was a million and negative a

339
00:19:00,054 --> 00:19:04,236
million, then the pragmatic value term

340
00:19:04,348 --> 00:19:06,224
would basically just be like scaled

341
00:19:06,272 --> 00:19:09,696
larger. So the amplitude

342
00:19:09,728 --> 00:19:15,524
on C is commonly used to

343
00:19:15,642 --> 00:19:18,728
weight the Pragmatic value relative to

344
00:19:18,734 --> 00:19:21,608
the epistemic value. Because just

345
00:19:21,694 --> 00:19:24,040
framing things like this does not

346
00:19:24,110 --> 00:19:27,384
guarantee that your parameterization is

347
00:19:27,422 --> 00:19:30,324
going to be at this adaptive point that

348
00:19:30,382 --> 00:19:32,748
actually does flexibly trade off between

349
00:19:32,834 --> 00:19:35,020
epistemic and Pragmatic behavior.

350
00:19:36,080 --> 00:19:39,784
Because if C is very small and washed

351
00:19:39,832 --> 00:19:43,660
out so utility seeking has

352
00:19:43,810 --> 00:19:46,304
shapes behavior a little bit, then

353
00:19:46,342 --> 00:19:47,596
you're going to end up with a mainly

354
00:19:47,628 --> 00:19:50,720
epistemic agent. And if C is vast

355
00:19:51,060 --> 00:19:53,936
relative to the epistemic value, then

356
00:19:53,958 --> 00:19:55,224
you're going to end up with mainly

357
00:19:55,292 --> 00:19:58,868
reward seeking, just like

358
00:19:59,034 --> 00:20:02,630
we see in the in figure Two six,

359
00:20:03,720 --> 00:20:06,852
where if we totally discard the

360
00:20:06,906 --> 00:20:10,276
pragmatic term we end up with purely

361
00:20:10,308 --> 00:20:13,332
epistemic behavior. And if we totally

362
00:20:13,396 --> 00:20:15,768
discard the epistemic term we end up

363
00:20:15,774 --> 00:20:20,090
with purely pragmatic behavior. But also

364
00:20:20,560 --> 00:20:23,150
if this term is just really small,

365
00:20:23,760 --> 00:20:25,484
you can imagine it doesn't play that

366
00:20:25,522 --> 00:20:28,860
large of a role and vice versa.

367
00:20:30,240 --> 00:20:33,664
So in an empirical setting, you have

368
00:20:33,702 --> 00:20:38,160
to parameterize the

369
00:20:38,230 --> 00:20:42,400
value, for example, of C, so that you do

370
00:20:42,470 --> 00:20:46,384
get goal reward oriented

371
00:20:46,432 --> 00:20:53,510
behavior, but not just

372
00:20:53,880 --> 00:20:55,590
like one track mind,

373
00:21:00,360 --> 00:21:04,232
um, Ali and then anyone

374
00:21:04,286 --> 00:21:04,890
else.

375
00:21:08,620 --> 00:21:12,284
Yeah, well, actually this C parameter C

376
00:21:12,322 --> 00:21:14,444
matrix here in the formulation of

377
00:21:14,482 --> 00:21:16,856
expected free energy is a relatively

378
00:21:16,888 --> 00:21:19,096
recent addition to active inference

379
00:21:19,128 --> 00:21:22,644
formulation because in some earlier

380
00:21:22,712 --> 00:21:26,572
literature or even some recent

381
00:21:26,636 --> 00:21:29,904
literature, this parameter c is

382
00:21:29,942 --> 00:21:32,288
not explicitly expressed in the

383
00:21:32,294 --> 00:21:35,984
formulation, they just express

384
00:21:36,032 --> 00:21:39,460
it as P of observation.

385
00:21:39,800 --> 00:21:41,590
Because you see,

386
00:21:44,840 --> 00:21:47,204
this is a crucial step actually in

387
00:21:47,322 --> 00:21:50,376
active inference because if

388
00:21:50,398 --> 00:21:53,764
we want to derive this expected

389
00:21:53,812 --> 00:21:56,724
free energy as a parallel to variation

390
00:21:56,772 --> 00:22:00,148
of free energy, then here

391
00:22:00,254 --> 00:22:03,288
instead of C, we should have only Pi,

392
00:22:03,384 --> 00:22:06,984
right? But replacing Pi

393
00:22:07,032 --> 00:22:11,820
with C denotes the kind of inaccuracy

394
00:22:12,980 --> 00:22:16,336
in terms of our policy selection. In

395
00:22:16,358 --> 00:22:18,800
other words, it doesn't necessarily

396
00:22:19,220 --> 00:22:22,576
entail, I mean,

397
00:22:22,758 --> 00:22:25,432
undertaking the policy we've selected,

398
00:22:25,596 --> 00:22:30,230
we have a preference matrix for

399
00:22:31,160 --> 00:22:34,144
undertaking some actions, but it doesn't

400
00:22:34,192 --> 00:22:37,472
necessarily imply that we only take

401
00:22:37,626 --> 00:22:41,640
those specific actions per se. So we

402
00:22:41,710 --> 00:22:44,600
somehow approximate our preferences

403
00:22:45,340 --> 00:22:48,970
without restricting ourselves to

404
00:22:49,900 --> 00:22:52,044
the actual policies that's been

405
00:22:52,082 --> 00:22:55,484
undertaken in the situation.

406
00:22:55,682 --> 00:22:59,870
So that allows us to even

407
00:23:00,480 --> 00:23:03,664
formulate or at least describe some

408
00:23:03,702 --> 00:23:07,184
counterfactual examples as well as well

409
00:23:07,222 --> 00:23:10,464
as the actual behavior we

410
00:23:10,502 --> 00:23:13,884
observe for the agent. So that's,

411
00:23:13,932 --> 00:23:17,744
I think, one of the crucial

412
00:23:17,792 --> 00:23:21,712
steps in terms of allowing

413
00:23:21,776 --> 00:23:25,110
for a broader behavior of the agents

414
00:23:27,080 --> 00:23:30,868
and somehow broadening the possibility

415
00:23:30,964 --> 00:23:34,360
spaces of the agents, not just the

416
00:23:34,430 --> 00:23:37,556
actualized trajectories of the internal

417
00:23:37,588 --> 00:23:40,170
states versus the external states.

418
00:23:44,500 --> 00:23:45,250
Yeah,

419
00:23:48,660 --> 00:23:51,264
it allows us to articulate what is the

420
00:23:51,302 --> 00:23:55,004
action possibility space and

421
00:23:55,062 --> 00:23:58,256
separate that from the outcome

422
00:23:58,448 --> 00:24:02,208
preference so we don't

423
00:24:02,224 --> 00:24:04,960
have our finger on the scale directly,

424
00:24:05,040 --> 00:24:07,520
can't control observations directly.

425
00:24:07,600 --> 00:24:09,300
That's the kind of perceptual control

426
00:24:09,370 --> 00:24:12,136
theory insight. You want your

427
00:24:12,158 --> 00:24:13,764
preferences to be about observations,

428
00:24:13,812 --> 00:24:14,856
but you don't want to be able to

429
00:24:14,878 --> 00:24:17,656
directly control observations. So what

430
00:24:17,678 --> 00:24:19,864
are the two channels that we have to

431
00:24:19,902 --> 00:24:22,444
reduce expected free energy, change our

432
00:24:22,482 --> 00:24:26,204
mind, change the world? How much to

433
00:24:26,242 --> 00:24:28,664
balance those two? That was Olivier's

434
00:24:28,712 --> 00:24:31,916
question. And that is the question is

435
00:24:31,938 --> 00:24:33,904
how do you balance those two? But those

436
00:24:33,942 --> 00:24:35,970
are the two things that you want to have

437
00:24:37,460 --> 00:24:40,684
from a first principles being balanced

438
00:24:40,732 --> 00:24:44,224
off in terms of your imperative for

439
00:24:44,262 --> 00:24:46,512
action selection. You want your action

440
00:24:46,576 --> 00:24:48,944
selection to be an epistemically

441
00:24:48,992 --> 00:24:52,624
informative path and a pragmatically

442
00:24:52,672 --> 00:24:55,270
useful path. Epistemic value,

443
00:24:55,640 --> 00:24:58,970
expected information gain under a policy

444
00:25:00,300 --> 00:25:03,864
pragmatic value how well it aligns with

445
00:25:03,902 --> 00:25:05,720
your preferences expectations.

446
00:25:11,120 --> 00:25:13,676
So this is how exploitative and

447
00:25:13,698 --> 00:25:17,148
explorative behavior are again going to

448
00:25:17,154 --> 00:25:19,052
be resolved. Not to say addressed away,

449
00:25:19,186 --> 00:25:22,648
but how it's approached. If we were in a

450
00:25:22,674 --> 00:25:26,268
reward setting, then exploitative

451
00:25:26,364 --> 00:25:29,116
behavior needs no secondary explanation.

452
00:25:29,228 --> 00:25:31,596
It's like there's money on the ground.

453
00:25:31,708 --> 00:25:33,776
If it's about getting money, then just

454
00:25:33,878 --> 00:25:37,152
pick it up. So exploitative behavior

455
00:25:37,296 --> 00:25:39,556
under reward centric paradigm does not

456
00:25:39,578 --> 00:25:41,220
need a secondary explanation.

457
00:25:41,800 --> 00:25:45,190
Exploratory behavior on the other hand,

458
00:25:45,720 --> 00:25:48,136
eventually needs to navigate into the

459
00:25:48,158 --> 00:25:52,024
currency of utilitarian value in

460
00:25:52,062 --> 00:25:54,504
a reward centric path. And so an

461
00:25:54,542 --> 00:25:58,708
exploratory path might be highly valued

462
00:25:58,804 --> 00:26:02,444
because for example, there might be a

463
00:26:02,482 --> 00:26:05,676
reward. So if there's a 10% chance of a

464
00:26:05,778 --> 00:26:09,804
$1,000, then that policy

465
00:26:09,922 --> 00:26:13,056
would have that expected reward. And so

466
00:26:13,078 --> 00:26:16,976
that might be more preferable than for

467
00:26:16,998 --> 00:26:20,896
sure one dollars or 10% of $1,000

468
00:26:21,078 --> 00:26:24,528
in the reward centric approach. So then

469
00:26:24,614 --> 00:26:26,608
that's how you get explore and exploit

470
00:26:26,704 --> 00:26:29,552
balancing. You have exploratory

471
00:26:29,696 --> 00:26:32,116
trajectories being evaluated in terms of

472
00:26:32,138 --> 00:26:35,236
their expected returns in reward and

473
00:26:35,258 --> 00:26:37,236
then you can have everything compared on

474
00:26:37,258 --> 00:26:39,668
the common currency of expected

475
00:26:39,764 --> 00:26:42,730
pragmatic value in reward learning.

476
00:26:43,500 --> 00:26:45,176
Okay, so how are we going to approach it

477
00:26:45,198 --> 00:26:47,000
differently in active inference?

478
00:26:47,580 --> 00:26:51,512
Different trajectories of action

479
00:26:51,656 --> 00:26:53,836
are going to be compared again in a

480
00:26:53,858 --> 00:26:56,444
common currency. But rather than that

481
00:26:56,482 --> 00:26:59,820
common currency being expected reward,

482
00:27:00,320 --> 00:27:02,392
the common currency is going to be

483
00:27:02,466 --> 00:27:05,312
expected free energy. Expected free

484
00:27:05,366 --> 00:27:07,920
energy is going to have two components

485
00:27:08,500 --> 00:27:12,060
expected information gain and expected

486
00:27:12,140 --> 00:27:13,410
pragmatic value.

487
00:27:15,160 --> 00:27:18,244
As stated before, balancing those two

488
00:27:18,282 --> 00:27:22,004
terms into an adaptive zone is

489
00:27:22,042 --> 00:27:24,756
a fine tuning question. But this is how

490
00:27:24,778 --> 00:27:26,404
we approach the question in a first

491
00:27:26,442 --> 00:27:29,604
principles way by making a unified

492
00:27:29,652 --> 00:27:33,556
imperative that one includes perception

493
00:27:33,588 --> 00:27:37,000
and action. That's the integration of

494
00:27:37,070 --> 00:27:39,224
perception action loop. We see like

495
00:27:39,262 --> 00:27:40,904
observations and we see actions.

496
00:27:40,952 --> 00:27:41,916
They're just all there in the same

497
00:27:41,938 --> 00:27:45,656
equation and it unifies epistemic

498
00:27:45,688 --> 00:27:50,168
and pragmatic value. Instead of coercing

499
00:27:50,264 --> 00:27:53,132
epistemic actions into their expected

500
00:27:53,196 --> 00:27:55,856
reward and then comparing everything on

501
00:27:55,878 --> 00:27:59,696
the reward meter stick, we just

502
00:27:59,718 --> 00:28:01,680
have a single unified imperative,

503
00:28:02,180 --> 00:28:05,556
expected free energy that contains an

504
00:28:05,578 --> 00:28:07,780
epistemic and a pragmatic loading.

505
00:28:09,320 --> 00:28:13,252
Michael. Great stuff.

506
00:28:13,306 --> 00:28:14,230
That's all.

507
00:28:16,280 --> 00:28:19,416
Yeah, equation 2.6 gets revisited again

508
00:28:19,438 --> 00:28:21,224
and again and just different ways of

509
00:28:21,262 --> 00:28:24,308
seeing expected free energy so now we're

510
00:28:24,324 --> 00:28:27,560
going to continue seeing the teammates.

511
00:28:28,460 --> 00:28:32,532
So here the matrices are shown

512
00:28:32,676 --> 00:28:34,316
for this teammate. So, yeah, there's the

513
00:28:34,338 --> 00:28:38,360
mouse aversive and beneficial

514
00:28:38,520 --> 00:28:41,790
outcome. Olivier yeah,

515
00:28:42,500 --> 00:28:45,324
sorry. No, it was just a bit like it's

516
00:28:45,372 --> 00:28:48,930
behind on the last equation 7.4

517
00:28:50,500 --> 00:28:53,292
for you. I find your last explanation

518
00:28:53,356 --> 00:28:56,480
really great and very enlightening.

519
00:28:57,460 --> 00:29:00,604
And in a basic context of reinforcement

520
00:29:00,652 --> 00:29:03,344
learning, you will have somehow the

521
00:29:03,382 --> 00:29:05,136
first term within the second, is that

522
00:29:05,158 --> 00:29:06,050
what you're saying?

523
00:29:09,600 --> 00:29:13,416
It wouldn't

524
00:29:13,448 --> 00:29:17,720
be nested within the second strictly,

525
00:29:17,800 --> 00:29:20,300
but yes. Basically policies,

526
00:29:22,160 --> 00:29:25,472
one slot machine is giving you the 50%

527
00:29:25,526 --> 00:29:27,696
reward and then you don't know the other

528
00:29:27,718 --> 00:29:30,572
one. But a policy might be entertained

529
00:29:30,636 --> 00:29:33,776
and even selected because it has a

530
00:29:33,798 --> 00:29:36,604
high expected value. That is expected

531
00:29:36,652 --> 00:29:39,056
value based decision making because you

532
00:29:39,078 --> 00:29:40,916
say, well, I think slot machine, this

533
00:29:40,938 --> 00:29:42,564
slot machine might have a high value,

534
00:29:42,682 --> 00:29:44,224
so I'm going to take an exploratory

535
00:29:44,272 --> 00:29:47,770
action to kind of explore that,

536
00:29:48,620 --> 00:29:50,884
but not because of the information gain

537
00:29:50,932 --> 00:29:54,344
it gives me, but because

538
00:29:54,382 --> 00:29:56,648
of the expected reward. So it's kind of

539
00:29:56,654 --> 00:29:58,008
like converting everything to a

540
00:29:58,014 --> 00:30:01,336
pragmatic currency and then comparing

541
00:30:01,448 --> 00:30:03,384
the expected utility or the expected

542
00:30:03,432 --> 00:30:05,960
reward. And so you get utility

543
00:30:06,040 --> 00:30:07,912
discounting, you get time preference,

544
00:30:07,976 --> 00:30:11,084
all this stuff, but those are all

545
00:30:11,122 --> 00:30:14,252
correction factors within pragmatic

546
00:30:14,316 --> 00:30:17,216
value. Yeah. Okay, yeah. Thank you.

547
00:30:17,238 --> 00:30:20,336
Sorry for the interruptions. This is one

548
00:30:20,358 --> 00:30:24,096
of the key topics and this

549
00:30:24,118 --> 00:30:28,260
is how policies are selected.

550
00:30:29,240 --> 00:30:31,780
And that is the empirical challenge,

551
00:30:32,120 --> 00:30:34,710
is being able to parameterize everything

552
00:30:35,640 --> 00:30:39,092
so that it doesn't just behave trivially

553
00:30:39,236 --> 00:30:42,090
like a novelty seeking agent over here,

554
00:30:42,940 --> 00:30:46,584
or like a utility seeking agent over

555
00:30:46,622 --> 00:30:50,556
here. We want to have some training in

556
00:30:50,578 --> 00:30:57,340
the middle here's

557
00:30:57,840 --> 00:31:01,052
where the matrices are shown for these

558
00:31:01,106 --> 00:31:03,948
two settings. Here the block is on the

559
00:31:03,954 --> 00:31:05,856
right and here the white is on the

560
00:31:05,878 --> 00:31:09,056
right. And it just shows what the

561
00:31:09,078 --> 00:31:14,736
matrices look like. So it's helpful to

562
00:31:14,758 --> 00:31:16,756
see what the actual matrices look like

563
00:31:16,778 --> 00:31:18,452
in the generative model, because

564
00:31:18,506 --> 00:31:20,868
remember, this is the generative model.

565
00:31:21,034 --> 00:31:23,956
So if you define A, B and D, just like

566
00:31:23,978 --> 00:31:26,496
we saw in the music example, they're

567
00:31:26,528 --> 00:31:28,328
just specific matrices of whatever

568
00:31:28,414 --> 00:31:30,068
dimensionality it is that you're

569
00:31:30,084 --> 00:31:33,112
studying. If there was four notes, then

570
00:31:33,246 --> 00:31:36,664
D had four in a vector, a was a four

571
00:31:36,702 --> 00:31:40,220
by four, b was a four x four matrix

572
00:31:41,680 --> 00:31:44,524
in the music example. Now when you add

573
00:31:44,562 --> 00:31:46,110
in policy,

574
00:31:47,280 --> 00:31:49,772
policy has a dimensionality of however

575
00:31:49,826 --> 00:31:52,048
many policies you can take. So if

576
00:31:52,054 --> 00:31:53,984
there's four options policy has,

577
00:31:54,022 --> 00:31:56,720
there's four options in the pi vector

578
00:31:58,020 --> 00:32:00,592
and then there's going to be a slice of

579
00:32:00,646 --> 00:32:04,604
B for each of the dimensionality

580
00:32:04,652 --> 00:32:08,852
of pi. So if

581
00:32:08,906 --> 00:32:10,548
there were four options for what you

582
00:32:10,554 --> 00:32:12,292
could do, then there would be four

583
00:32:12,346 --> 00:32:15,124
slices in B. And it's kind of like,

584
00:32:15,162 --> 00:32:18,836
okay, if you choose action three, then B

585
00:32:18,938 --> 00:32:21,608
three third slice in b is going to be

586
00:32:21,614 --> 00:32:25,656
the one that we use to

587
00:32:25,758 --> 00:32:29,960
update S. So once you define

588
00:32:30,040 --> 00:32:33,576
this very limited, very interpretable

589
00:32:33,768 --> 00:32:37,288
set, of parameters. You will have stated

590
00:32:37,304 --> 00:32:40,456
the generative model and then you're

591
00:32:40,488 --> 00:32:43,176
ready to engage with standard methods

592
00:32:43,368 --> 00:32:45,744
for training and updating this

593
00:32:45,782 --> 00:32:48,016
generative model. And so that's why in

594
00:32:48,038 --> 00:32:49,872
active inference we spend a lot of time

595
00:32:49,926 --> 00:32:52,624
think about chapter six setting up our

596
00:32:52,662 --> 00:32:55,332
understanding of the problem because

597
00:32:55,386 --> 00:32:58,820
once you can describe the dimensionality

598
00:32:59,720 --> 00:33:02,672
and the parameters of the generative

599
00:33:02,736 --> 00:33:05,350
model that was the work,

600
00:33:06,200 --> 00:33:08,168
then you just update this as a

601
00:33:08,174 --> 00:33:10,696
statistical model using standard

602
00:33:10,798 --> 00:33:14,424
methods. So there's a lot of work in

603
00:33:14,462 --> 00:33:16,916
understanding the problem and in framing

604
00:33:16,948 --> 00:33:18,472
it and understanding what is the system

605
00:33:18,526 --> 00:33:20,556
of interest and all these other things,

606
00:33:20,738 --> 00:33:22,476
what form of degenerative model is

607
00:33:22,498 --> 00:33:25,836
appropriate but then you don't need

608
00:33:25,858 --> 00:33:30,590
to engage hopefully but actually in

609
00:33:31,040 --> 00:33:34,960
secondary engineering concerns

610
00:33:35,780 --> 00:33:37,616
for reasonable projects. I'm sure at

611
00:33:37,638 --> 00:33:39,216
some very large scale these things do

612
00:33:39,238 --> 00:33:41,836
come back into into play. But that's

613
00:33:41,868 --> 00:33:43,788
kind of the cool thing about active

614
00:33:43,804 --> 00:33:45,604
inference. We understand what all these

615
00:33:45,642 --> 00:33:48,996
variables are and then when we want

616
00:33:49,018 --> 00:33:52,352
to model our own setting we just specify

617
00:33:52,416 --> 00:33:55,332
the generative model and kind of hit

618
00:33:55,386 --> 00:33:55,990
play.

619
00:34:00,250 --> 00:34:02,310
More matrices being shown.

620
00:34:04,170 --> 00:34:08,422
Here's the C vector and so plus

621
00:34:08,476 --> 00:34:09,926
six for the attractive stimulus and

622
00:34:09,948 --> 00:34:12,014
negative six for the aversive stimulus.

623
00:34:12,162 --> 00:34:15,530
Now if this were zero zero six

624
00:34:15,600 --> 00:34:19,098
and negative six so the amplitude of

625
00:34:19,104 --> 00:34:22,518
C were just smaller so the relative

626
00:34:22,614 --> 00:34:25,838
preferences would stay the same but

627
00:34:25,924 --> 00:34:27,694
the pragmatic term would just be

628
00:34:27,732 --> 00:34:30,234
smaller. So then the agent would behave

629
00:34:30,282 --> 00:34:32,702
more epistemically. Whereas if this were

630
00:34:32,756 --> 00:34:36,080
6 trillion and negative 6 trillion then

631
00:34:36,710 --> 00:34:38,386
the pragmatic term would come to

632
00:34:38,408 --> 00:34:39,250
dominate.

633
00:34:48,540 --> 00:34:52,136
So again, what is the approach of the

634
00:34:52,158 --> 00:34:56,028
resolution? It's putting different

635
00:34:56,114 --> 00:34:58,616
trajectories of behavior, action

636
00:34:58,648 --> 00:35:02,136
selection on a common footing, expected

637
00:35:02,168 --> 00:35:06,060
free energy which unifies

638
00:35:06,140 --> 00:35:08,864
the epistemic and the pragmatic value of

639
00:35:08,902 --> 00:35:10,656
different actions. We don't say

640
00:35:10,678 --> 00:35:12,576
epistemic and pragmatic value are the

641
00:35:12,598 --> 00:35:15,456
same thing. In fact they're different

642
00:35:15,558 --> 00:35:18,244
but they're complementary. And so having

643
00:35:18,282 --> 00:35:23,200
a unified imperative helps us adaptively

644
00:35:23,280 --> 00:35:26,228
approach this question of the actual

645
00:35:26,314 --> 00:35:29,740
trade offs between courses of action

646
00:35:29,920 --> 00:35:33,112
that have known consequence and

647
00:35:33,166 --> 00:35:35,496
those that are providing new

648
00:35:35,518 --> 00:35:36,090
information.

649
00:35:44,280 --> 00:35:46,036
Anyone want to have a thought? Yeah, go

650
00:35:46,058 --> 00:35:50,008
for it. I'm a

651
00:35:50,014 --> 00:35:51,796
babe out of water in so many levels.

652
00:35:51,828 --> 00:35:54,584
But in my original field of training in

653
00:35:54,622 --> 00:35:57,944
economics we had this phrase we used

654
00:35:57,982 --> 00:36:00,696
often called Satiris paribus which was a

655
00:36:00,718 --> 00:36:03,096
way of kind of saying that's an

656
00:36:03,118 --> 00:36:04,808
extradinality to our models and we're

657
00:36:04,824 --> 00:36:06,172
just going to keep going with the way we

658
00:36:06,226 --> 00:36:09,004
do things, which is close enough. And

659
00:36:09,042 --> 00:36:12,216
the consequences of this kind of poor

660
00:36:12,248 --> 00:36:15,584
treatment of dilemmas was that things

661
00:36:15,622 --> 00:36:18,476
like pollution and other externalities

662
00:36:18,588 --> 00:36:21,104
were they might not fit in the

663
00:36:21,142 --> 00:36:22,844
representational model and therefore

664
00:36:22,892 --> 00:36:24,496
were kind of underweighted and

665
00:36:24,518 --> 00:36:26,740
trivialized. And what I'm hearing you

666
00:36:26,810 --> 00:36:28,372
make a case for, in a different

667
00:36:28,426 --> 00:36:31,076
discipline, in a different way is

668
00:36:31,258 --> 00:36:34,724
changing the math to handle what is

669
00:36:34,762 --> 00:36:37,940
otherwise trivialized as external,

670
00:36:38,540 --> 00:36:41,016
too inconsequential to be attended to,

671
00:36:41,118 --> 00:36:45,016
is embracing the tension and

672
00:36:45,118 --> 00:36:47,416
thus making it a part of what then can

673
00:36:47,438 --> 00:36:48,808
be the sense making model of the

674
00:36:48,814 --> 00:36:51,484
discipline overall. Which seems

675
00:36:51,682 --> 00:36:55,390
something we can learn from you all.

676
00:36:58,640 --> 00:37:01,164
Building not even one layer and

677
00:37:01,202 --> 00:37:02,964
connecting that to kind of good hearts.

678
00:37:03,032 --> 00:37:06,080
Paradox like a metric becomes Gamified.

679
00:37:06,580 --> 00:37:10,928
Do we care about the levels of lead

680
00:37:11,094 --> 00:37:14,240
in the measurement or do we care about

681
00:37:14,310 --> 00:37:16,596
the hidden unobserved levels of lead in

682
00:37:16,618 --> 00:37:20,420
the soil? And so in our systems model

683
00:37:20,490 --> 00:37:22,116
that hopefully everyone will be able to

684
00:37:22,138 --> 00:37:24,596
come to the table on, we could say,

685
00:37:24,618 --> 00:37:27,780
well really there's both.

686
00:37:27,930 --> 00:37:29,536
They're just different things. There's

687
00:37:29,568 --> 00:37:31,656
the true underlying distribution which

688
00:37:31,678 --> 00:37:32,904
we're never going to measure at every

689
00:37:32,942 --> 00:37:35,396
little grain of sand and then there's

690
00:37:35,428 --> 00:37:37,928
the sensor fusion that we have to do

691
00:37:38,014 --> 00:37:41,036
with observations. So let's not try to

692
00:37:41,058 --> 00:37:46,140
game the observations, but rather

693
00:37:46,210 --> 00:37:48,568
come to the table with a more holistic

694
00:37:48,744 --> 00:37:50,284
understanding of the relationship

695
00:37:50,402 --> 00:37:52,232
between hidden states and observations

696
00:37:52,296 --> 00:37:54,176
and how hidden states are changed and

697
00:37:54,198 --> 00:37:55,996
how our actions influence the hidden

698
00:37:56,028 --> 00:37:58,492
state distribution, but how that emits

699
00:37:58,556 --> 00:38:02,256
observations, just articulate the

700
00:38:02,278 --> 00:38:04,624
problem, break it down into the

701
00:38:04,662 --> 00:38:07,556
components. And then the amazing thing

702
00:38:07,578 --> 00:38:10,004
is these natural components of the

703
00:38:10,042 --> 00:38:12,756
situation again prior beliefs about

704
00:38:12,778 --> 00:38:14,884
hidden state underlying hidden states

705
00:38:14,922 --> 00:38:16,964
that are unobserved emission of

706
00:38:17,002 --> 00:38:19,352
observations, how the world changes

707
00:38:19,406 --> 00:38:21,864
through time, actions that influence how

708
00:38:21,902 --> 00:38:24,248
the world changes through time, our

709
00:38:24,334 --> 00:38:26,520
strategy of deciding on action,

710
00:38:27,020 --> 00:38:30,840
those are our naturally separable

711
00:38:31,340 --> 00:38:33,630
aspects of thinking about a situation.

712
00:38:34,400 --> 00:38:36,476
And in a way it's kind of amazing that

713
00:38:36,498 --> 00:38:38,184
all you have to do is just like smash

714
00:38:38,232 --> 00:38:41,372
the matrices together and the math does

715
00:38:41,426 --> 00:38:44,576
kind of work out again in a way that we

716
00:38:44,598 --> 00:38:45,650
would want it to.

717
00:38:49,300 --> 00:38:54,624
Yeah, it's Chill 7.4

718
00:38:54,742 --> 00:38:56,244
is going to go into a little bit more

719
00:38:56,282 --> 00:38:59,108
detail on this epistemic value of

720
00:38:59,194 --> 00:39:02,484
policies. So there's a few different

721
00:39:02,602 --> 00:39:06,548
breakdowns here. They're all equal but

722
00:39:06,634 --> 00:39:09,400
this is a few different representations

723
00:39:10,300 --> 00:39:13,432
of just how we can think about the

724
00:39:13,486 --> 00:39:16,248
informational gain associated with a

725
00:39:16,254 --> 00:39:19,924
given policy. So, whereas the pragmatic

726
00:39:19,972 --> 00:39:23,640
value for a policy is fairly

727
00:39:23,720 --> 00:39:26,940
straightforwardly identified with how

728
00:39:27,010 --> 00:39:30,648
close the expected observations

729
00:39:30,744 --> 00:39:34,624
are to the preferences. So the most

730
00:39:34,662 --> 00:39:37,136
pragmatically valuable trajectory is the

731
00:39:37,158 --> 00:39:39,344
one where the body temperature is right

732
00:39:39,382 --> 00:39:40,844
in the middle of the temperature

733
00:39:40,892 --> 00:39:43,376
distribution. The least pragmatic one

734
00:39:43,398 --> 00:39:45,776
would be the further away. But here's

735
00:39:45,808 --> 00:39:49,028
where we see this I of Pi. So there's a

736
00:39:49,034 --> 00:39:52,356
few more descriptions on I of Pi. And so

737
00:39:52,378 --> 00:39:55,252
it turns out in this case with this

738
00:39:55,306 --> 00:39:58,756
teamase, initially the action

739
00:39:58,788 --> 00:40:02,792
that the mouse selects is to it

740
00:40:02,846 --> 00:40:06,884
takes an epistemic action to reduce

741
00:40:06,932 --> 00:40:09,096
uncertainty because it's like, okay,

742
00:40:09,278 --> 00:40:10,812
there might be some pragmatic value,

743
00:40:10,866 --> 00:40:12,990
but also it's a risky decision up here,

744
00:40:14,240 --> 00:40:18,188
but going down is going to

745
00:40:18,274 --> 00:40:21,196
be a policy selection that gives me a

746
00:40:21,218 --> 00:40:24,588
lot of information. Now this

747
00:40:24,594 --> 00:40:27,228
isn't the whole iguana as they brought

748
00:40:27,244 --> 00:40:28,896
up in the beginning of the book. Like

749
00:40:28,998 --> 00:40:32,064
how does the mouse come to know that the

750
00:40:32,102 --> 00:40:36,768
aboutness of the queue is this setting?

751
00:40:36,944 --> 00:40:40,484
So this isn't like from the

752
00:40:40,522 --> 00:40:42,564
big bang to the mouse making the

753
00:40:42,602 --> 00:40:45,104
decision, the total model, there's

754
00:40:45,152 --> 00:40:47,824
always stuff that's structurally encoded

755
00:40:47,872 --> 00:40:50,436
in the model how does it mouse come to

756
00:40:50,458 --> 00:40:52,376
know that the aversive stimulus is

757
00:40:52,398 --> 00:40:54,948
negative and so on. So these are levels

758
00:40:54,964 --> 00:40:57,176
that you can add in but this is just

759
00:40:57,198 --> 00:40:58,792
showing basically again in this simple

760
00:40:58,846 --> 00:41:01,724
example it's just showing what it looks

761
00:41:01,762 --> 00:41:05,272
like when the mouse undertakes

762
00:41:05,336 --> 00:41:09,340
an epistemic action. And in the Pymdp

763
00:41:09,680 --> 00:41:13,064
model stream 7.2, where there's

764
00:41:13,192 --> 00:41:15,548
a very similar example to this it's not

765
00:41:15,554 --> 00:41:18,850
in a Tmaze but it is with a mouse that

766
00:41:19,380 --> 00:41:21,616
gets some information and then makes a

767
00:41:21,638 --> 00:41:25,170
decision. So it's very similar to this.

768
00:41:26,520 --> 00:41:29,316
You could play around in the script and

769
00:41:29,338 --> 00:41:31,252
you can see what happens when you change

770
00:41:31,306 --> 00:41:32,804
the preference vector to like a super

771
00:41:32,842 --> 00:41:34,390
high or super low value.

772
00:41:40,360 --> 00:41:44,884
Okay, little bit of boxed

773
00:41:45,012 --> 00:41:48,436
discussion on precision

774
00:41:48,468 --> 00:41:52,856
and entropy. The h is associated

775
00:41:52,888 --> 00:41:58,284
with entropy but

776
00:41:58,322 --> 00:42:01,676
let's just continue on. That just

777
00:42:01,698 --> 00:42:03,676
relates to the over underfitting of the

778
00:42:03,698 --> 00:42:07,344
expected the expected sharpness or

779
00:42:07,382 --> 00:42:09,040
blurriness of a distribution.

780
00:42:11,380 --> 00:42:14,780
They move on to another of the settings

781
00:42:14,940 --> 00:42:18,544
that's been very well repeatedly

782
00:42:18,592 --> 00:42:21,364
studied in active inference which is the

783
00:42:21,402 --> 00:42:24,516
eye cicade setting the

784
00:42:24,538 --> 00:42:27,812
motion of the eyes. And so one reason

785
00:42:27,866 --> 00:42:31,076
why this has been studied a lot is that

786
00:42:31,258 --> 00:42:34,536
the fovea of the vision, the area that

787
00:42:34,558 --> 00:42:37,224
we have high resolution color vision is

788
00:42:37,262 --> 00:42:39,976
actually very, very small relative to

789
00:42:39,998 --> 00:42:42,012
the visual field. And that's why

790
00:42:42,066 --> 00:42:44,940
multiple times per second the eye is

791
00:42:45,010 --> 00:42:47,164
making these policy selections to move

792
00:42:47,202 --> 00:42:50,536
around. And those policy selections,

793
00:42:50,648 --> 00:42:51,932
although we might look at something

794
00:42:51,986 --> 00:42:54,496
that's beautiful or rewarding and have

795
00:42:54,518 --> 00:42:56,784
that fix our gaze, to a first

796
00:42:56,822 --> 00:43:00,428
approximation, the imperative of the eye

797
00:43:00,444 --> 00:43:04,284
circade is informational. It's gaining

798
00:43:04,332 --> 00:43:08,390
information about the visual field

799
00:43:08,760 --> 00:43:12,224
and so that is why the epistemic

800
00:43:12,272 --> 00:43:15,670
visual search is a really

801
00:43:16,600 --> 00:43:19,880
useful intangible setting.

802
00:43:20,300 --> 00:43:22,120
Again, it's not like gambling.

803
00:43:25,020 --> 00:43:28,456
You're kind of setting up a situation

804
00:43:28,638 --> 00:43:31,852
towards pragmatic value but rather

805
00:43:31,906 --> 00:43:33,516
this is one where there's going to be an

806
00:43:33,538 --> 00:43:36,156
emphasis on the epistemic. And so this

807
00:43:36,178 --> 00:43:38,076
is like based upon the ambiguity of

808
00:43:38,098 --> 00:43:42,750
stimuli and about how

809
00:43:44,820 --> 00:43:47,984
epistemic attraction occurs. When

810
00:43:48,022 --> 00:43:50,432
there's information to learn. Where

811
00:43:50,486 --> 00:43:51,472
something where there's no information

812
00:43:51,526 --> 00:43:54,096
to learn doesn't that moving there does

813
00:43:54,118 --> 00:43:55,570
not have epistemic value.

814
00:44:00,520 --> 00:44:02,276
Kind of this is a little bit of a

815
00:44:02,298 --> 00:44:05,510
formalization of the streetlight story

816
00:44:06,120 --> 00:44:08,996
from every culture. Where did, where,

817
00:44:09,018 --> 00:44:10,376
you know, where are you looking for your

818
00:44:10,398 --> 00:44:12,696
keys? Under the streetlight. Oh, did you

819
00:44:12,718 --> 00:44:15,096
lose them there? No, this is just where

820
00:44:15,118 --> 00:44:18,264
I could look clearly. So that is

821
00:44:18,302 --> 00:44:21,720
about looking where there's light and

822
00:44:21,790 --> 00:44:25,404
searching where you can not

823
00:44:25,442 --> 00:44:26,636
necessarily because you have a high

824
00:44:26,658 --> 00:44:28,476
prior that the information is there but

825
00:44:28,498 --> 00:44:30,460
just because it's easily resolvable

826
00:44:30,880 --> 00:44:32,684
under the streetlight best place to find

827
00:44:32,722 --> 00:44:34,684
high quality unambiguous uncertainty

828
00:44:34,732 --> 00:44:35,810
resolving information.

829
00:44:38,340 --> 00:44:41,296
So the ambiguous poorly lit square is

830
00:44:41,318 --> 00:44:43,548
ignored reproducing in silico

831
00:44:43,644 --> 00:44:45,628
streetlight effect. If you're not going

832
00:44:45,654 --> 00:44:48,436
to get resolvable information it's not

833
00:44:48,458 --> 00:44:51,316
of epistemic value to go there but if it

834
00:44:51,338 --> 00:44:53,232
is going to be high quality uncertainty

835
00:44:53,296 --> 00:44:55,476
reducing information that is like

836
00:44:55,498 --> 00:45:00,570
epistemically valuable 75

837
00:45:01,100 --> 00:45:05,224
Learning and Novelty so in.

838
00:45:05,262 --> 00:45:10,104
Figure four, three in

839
00:45:10,142 --> 00:45:14,140
seven, where we, where we revisit it in

840
00:45:14,290 --> 00:45:18,252
73. There's no learning in

841
00:45:18,306 --> 00:45:21,848
this model as written,

842
00:45:22,024 --> 00:45:25,264
there's updating of the hidden states by

843
00:45:25,382 --> 00:45:27,200
moving it through the B matrix.

844
00:45:28,740 --> 00:45:32,544
But the A, B and D themselves do

845
00:45:32,582 --> 00:45:35,904
not change. So now

846
00:45:36,102 --> 00:45:38,548
they're going to talk about how learning

847
00:45:38,634 --> 00:45:41,572
can come into play. So in other words,

848
00:45:41,626 --> 00:45:44,148
like the A matrix is fixed in this

849
00:45:44,234 --> 00:45:47,748
setting. But you might be interested in

850
00:45:47,754 --> 00:45:50,404
a situation where the A matrix can

851
00:45:50,442 --> 00:45:54,936
change through time. Like maybe the

852
00:45:54,958 --> 00:45:56,328
mapping between hidden states and

853
00:45:56,334 --> 00:45:59,944
observations changes. The way

854
00:45:59,982 --> 00:46:04,088
that learning is generally approached

855
00:46:04,264 --> 00:46:08,444
in Bayesian inference settings is

856
00:46:08,642 --> 00:46:11,740
you make a prior distribution

857
00:46:12,900 --> 00:46:15,970
on the variable of interest.

858
00:46:16,660 --> 00:46:18,832
So for example,

859
00:46:18,966 --> 00:46:22,684
lowercase A is going to be a hyper prior

860
00:46:22,812 --> 00:46:25,570
or just a prior distribution on A.

861
00:46:26,600 --> 00:46:29,572
So instead of just saying A is seven one

862
00:46:29,626 --> 00:46:32,916
one like we saw in the

863
00:46:32,938 --> 00:46:35,476
Music example, you can have a

864
00:46:35,498 --> 00:46:39,352
distribution over what

865
00:46:39,406 --> 00:46:43,432
A looks like that is going to help

866
00:46:43,486 --> 00:46:47,128
with learning. In the extreme case

867
00:46:47,294 --> 00:46:50,136
where learning rate converges to zero,

868
00:46:50,318 --> 00:46:52,840
that's like a super sharp distribution

869
00:46:53,340 --> 00:46:55,276
around what you think A is going to be

870
00:46:55,298 --> 00:46:58,552
like so that you have very strong prior

871
00:46:58,616 --> 00:47:01,308
on A. So new information is only going

872
00:47:01,314 --> 00:47:03,188
to move that sharp distribution a tiny

873
00:47:03,224 --> 00:47:07,250
amount. Conversely, if you were having

874
00:47:08,180 --> 00:47:11,568
a lot of learning on A, you'd have a

875
00:47:11,574 --> 00:47:15,170
very flat prior on what A could be

876
00:47:15,720 --> 00:47:18,592
so that new observations would update

877
00:47:18,656 --> 00:47:20,790
your beliefs about A a lot.

878
00:47:22,040 --> 00:47:23,796
But this is just an example of the

879
00:47:23,818 --> 00:47:27,172
composability of this framework in that

880
00:47:27,226 --> 00:47:32,360
the base model doesn't have learning

881
00:47:32,510 --> 00:47:34,840
attention, all these different

882
00:47:34,990 --> 00:47:37,092
relatively sophisticated cognitive

883
00:47:37,156 --> 00:47:40,716
phenomena. But this is

884
00:47:40,738 --> 00:47:43,292
like the little firmware nugget or

885
00:47:43,346 --> 00:47:46,910
kernel which by understanding the

886
00:47:47,680 --> 00:47:51,324
compositionality relationships of then

887
00:47:51,442 --> 00:47:54,684
we can start to bring in learning on

888
00:47:54,722 --> 00:47:56,512
different parameters. And of course

889
00:47:56,566 --> 00:47:58,192
learning is awesome and it sounds

890
00:47:58,246 --> 00:48:00,576
awesome, it is. But if you also think

891
00:48:00,598 --> 00:48:03,116
about it from a statistical perspective,

892
00:48:03,308 --> 00:48:05,132
that doesn't mean that every parameter

893
00:48:05,196 --> 00:48:07,220
should be learnable because sometimes

894
00:48:07,290 --> 00:48:09,168
you're just increasing the computational

895
00:48:09,264 --> 00:48:24,064
challenge of your problem vastly more

896
00:48:24,102 --> 00:48:25,490
information on learning.

897
00:48:29,800 --> 00:48:33,450
Here we see another difference

898
00:48:34,380 --> 00:48:38,596
with learning between active inference

899
00:48:38,628 --> 00:48:40,040
and many other machine learning

900
00:48:40,110 --> 00:48:43,390
approaches. And this is the

901
00:48:44,160 --> 00:48:47,464
embedding of action in the inference

902
00:48:47,512 --> 00:48:50,910
process. We actively think about

903
00:48:51,280 --> 00:48:53,564
how much information are we going to

904
00:48:53,602 --> 00:48:56,636
get. Like when people are training an

905
00:48:56,658 --> 00:48:58,416
image recognition, it just here's a

906
00:48:58,438 --> 00:49:00,144
thousand images in the training set just

907
00:49:00,182 --> 00:49:03,024
plug and chug. But what if you had an

908
00:49:03,062 --> 00:49:06,144
agent that was like which image should I

909
00:49:06,182 --> 00:49:08,800
look in of these thousands?

910
00:49:09,240 --> 00:49:11,424
Then it would become of primary

911
00:49:11,472 --> 00:49:15,792
importance which sequence

912
00:49:15,856 --> 00:49:19,012
of learning observations the agents

913
00:49:19,146 --> 00:49:22,088
took within those thousand labeled data

914
00:49:22,254 --> 00:49:26,116
points. So that's what G helps

915
00:49:26,148 --> 00:49:29,656
us understand some

916
00:49:29,678 --> 00:49:32,132
new terms salience,

917
00:49:32,276 --> 00:49:36,116
novelty, different decompositions

918
00:49:36,148 --> 00:49:37,210
of G.

919
00:49:41,450 --> 00:49:44,758
One final example, again, more just like

920
00:49:44,844 --> 00:49:48,006
referenced because it's in other papers

921
00:49:48,038 --> 00:49:49,338
but not really explained with the

922
00:49:49,344 --> 00:49:52,950
matrices. Here we have a maze

923
00:49:53,030 --> 00:49:56,170
exploration agent which is like another

924
00:49:56,240 --> 00:50:00,046
kind of classic setting so it

925
00:50:00,068 --> 00:50:03,566
has gray in the likelihood, which is

926
00:50:03,588 --> 00:50:06,606
like 50 50 between white and black. But

927
00:50:06,628 --> 00:50:08,574
it starts off, it takes these three

928
00:50:08,612 --> 00:50:10,586
steps and it's like, okay, it's

929
00:50:10,618 --> 00:50:14,094
updating, now it moves. Here updates

930
00:50:14,142 --> 00:50:15,218
that. Okay, that's a white square,

931
00:50:15,304 --> 00:50:16,766
that's a white square, that's a block

932
00:50:16,798 --> 00:50:19,426
square, that's a block square. So this

933
00:50:19,448 --> 00:50:21,582
is like somebody looking over a maze

934
00:50:21,646 --> 00:50:25,330
visually seeking a path

935
00:50:28,470 --> 00:50:29,970
to solve the maze.

936
00:50:34,250 --> 00:50:40,646
7.3 is a little bit of a summary

937
00:50:40,678 --> 00:50:44,586
of a pretty advanced topic which

938
00:50:44,608 --> 00:50:46,730
is Bayesian model reduction and

939
00:50:46,800 --> 00:50:50,206
structure learning which is treating the

940
00:50:50,228 --> 00:50:54,234
question of what structure

941
00:50:54,362 --> 00:50:56,400
should the generative model have?

942
00:50:56,930 --> 00:51:00,682
Treating that question as a parametric

943
00:51:00,746 --> 00:51:07,698
inference problem. So should

944
00:51:07,784 --> 00:51:09,954
the model be one, two or three time

945
00:51:09,992 --> 00:51:13,106
steps deep and then say, okay, well

946
00:51:13,128 --> 00:51:15,402
we're going to have some time horizon

947
00:51:15,486 --> 00:51:18,118
parameter and then we're going to test

948
00:51:18,204 --> 00:51:20,262
whether one, two or three is the better

949
00:51:20,316 --> 00:51:23,126
model. So like we're going to learn the

950
00:51:23,148 --> 00:51:26,760
structure of a model as part of

951
00:51:27,630 --> 00:51:31,370
reducing down from a portfolio of models

952
00:51:31,870 --> 00:51:34,490
but they just sort of briefly note it

953
00:51:34,640 --> 00:51:38,250
and there's more citations

954
00:51:39,490 --> 00:51:44,174
and then as and then any

955
00:51:44,212 --> 00:51:46,030
other just thoughts or questions. Seven

956
00:51:46,100 --> 00:51:49,102
kind of has a lot of little short

957
00:51:49,156 --> 00:51:50,190
vignettes.

958
00:51:56,340 --> 00:51:58,372
Okay, just briefly, just so that we

959
00:51:58,426 --> 00:52:03,428
touch it all, then next time we can have

960
00:52:03,514 --> 00:52:05,188
voted and added a lot of questions and

961
00:52:05,194 --> 00:52:08,852
everything. 712 is a two level model.

962
00:52:08,906 --> 00:52:11,690
It's a nested or a hierarchical model.

963
00:52:12,540 --> 00:52:15,096
Both the higher and the lower level,

964
00:52:15,198 --> 00:52:18,200
one and two are discrete time.

965
00:52:18,350 --> 00:52:20,296
You can tell because they all have the s

966
00:52:20,398 --> 00:52:23,976
t minus one t t plus one. So later we'll

967
00:52:24,008 --> 00:52:26,892
see a hierarchical model where the lower

968
00:52:26,946 --> 00:52:29,612
level is continuous time and the top

969
00:52:29,666 --> 00:52:32,524
level is discrete time. But in this

970
00:52:32,562 --> 00:52:35,596
setting both nested levels are discrete

971
00:52:35,628 --> 00:52:39,024
time. So this could be every hour and

972
00:52:39,062 --> 00:52:41,010
this is 60 minutes within an hour,

973
00:52:41,380 --> 00:52:43,890
every day, 24 hours within a day.

974
00:52:47,380 --> 00:52:52,304
Here's a multi scale updating generative

975
00:52:52,352 --> 00:52:55,744
model that's related

976
00:52:55,792 --> 00:52:59,864
to sentence reading and

977
00:52:59,902 --> 00:53:01,972
they've done a lot of simulation studies

978
00:53:02,036 --> 00:53:04,692
of multilevel inference in reading,

979
00:53:04,836 --> 00:53:06,536
like where the eyes are moving to

980
00:53:06,558 --> 00:53:08,916
resolve uncertainty about the letter.

981
00:53:09,108 --> 00:53:12,296
Order of letters is sought out to reduce

982
00:53:12,328 --> 00:53:15,100
uncertainty about words. Order of words

983
00:53:15,170 --> 00:53:17,176
in a sentence is sought out to reduce

984
00:53:17,208 --> 00:53:20,216
uncertainty about semantics. Semantics

985
00:53:20,248 --> 00:53:23,070
are operated on for pragmatic value.

986
00:53:26,100 --> 00:53:29,280
So that's chapter seven. It's a

987
00:53:29,350 --> 00:53:34,540
fairly loosely

988
00:53:34,620 --> 00:53:38,656
connected sequence of discrete

989
00:53:38,688 --> 00:53:41,412
time models. Several examples, starting

990
00:53:41,466 --> 00:53:43,476
from the musical example, which is just

991
00:53:43,498 --> 00:53:46,630
a passive inference hidden Markov model

992
00:53:47,480 --> 00:53:50,916
bringing us back to the discrete

993
00:53:50,948 --> 00:53:52,936
time generative model that we saw in

994
00:53:52,958 --> 00:53:57,704
Figure 4.3 that's shown again in 7.3 and

995
00:53:57,742 --> 00:54:01,044
then several decompositions. Of expected

996
00:54:01,092 --> 00:54:04,940
free energy focusing on the Epistemic

997
00:54:05,280 --> 00:54:09,276
value, especially in terms of how

998
00:54:09,458 --> 00:54:11,884
getting Epistemic and Pragmatic value

999
00:54:12,002 --> 00:54:15,840
unified under this G functional

1000
00:54:16,340 --> 00:54:19,852
allows us to approach the adaptive

1001
00:54:19,916 --> 00:54:22,432
finessing of exploratory and

1002
00:54:22,486 --> 00:54:24,000
exploitative behavior.

1003
00:54:28,590 --> 00:54:30,874
And again, the work is in understanding

1004
00:54:30,922 --> 00:54:33,470
and designing the generative models.

1005
00:54:36,670 --> 00:54:38,726
That's not work about work. That's

1006
00:54:38,758 --> 00:54:41,894
really the task itself is the systems

1007
00:54:41,942 --> 00:54:44,178
modeling and systems understanding,

1008
00:54:44,374 --> 00:54:47,214
because essentially, by saying what

1009
00:54:47,252 --> 00:54:49,454
these variables are, what shape these

1010
00:54:49,492 --> 00:54:53,470
variables are, you will have

1011
00:54:53,620 --> 00:54:57,230
prepared your model to run. And until

1012
00:54:57,300 --> 00:54:59,646
these models are defined in terms of

1013
00:54:59,668 --> 00:55:01,246
their dimensionality, it's not ready to

1014
00:55:01,268 --> 00:55:01,840
run.

1015
00:55:04,840 --> 00:55:07,904
Any kind of closing

1016
00:55:07,952 --> 00:55:11,836
thoughts, Michael? It looks like

1017
00:55:11,858 --> 00:55:13,176
Martin might have had to do the math

1018
00:55:13,208 --> 00:55:14,430
class after all.

1019
00:55:18,960 --> 00:55:21,550
There's so many. I mean. As will I.

1020
00:55:23,700 --> 00:55:25,520
There'd be so many. And and again,

1021
00:55:25,590 --> 00:55:28,850
like, let's try to let's explore how the

1022
00:55:29,540 --> 00:55:32,172
let's see how the the AI explanations

1023
00:55:32,236 --> 00:55:37,434
did for chapter seven math

1024
00:55:37,482 --> 00:55:39,066
explanations. Why would a high schooler

1025
00:55:39,098 --> 00:55:39,840
be interested?

1026
00:55:43,900 --> 00:55:45,448
Let's look at it for one of the

1027
00:55:45,534 --> 00:55:46,810
epistemic ones.

1028
00:55:51,420 --> 00:55:54,412
People can evaluate and add comments or

1029
00:55:54,466 --> 00:55:58,190
change the text,

1030
00:56:00,960 --> 00:56:03,692
but for those who are learning, and

1031
00:56:03,746 --> 00:56:06,908
ultimately we all are, I doubt

1032
00:56:06,924 --> 00:56:10,464
there are few, if any, people for

1033
00:56:10,502 --> 00:56:13,600
whom this equation doesn't require

1034
00:56:15,060 --> 00:56:18,512
contextualization and clarification and

1035
00:56:18,566 --> 00:56:22,164
deeper breathing. Yeah, these lines are

1036
00:56:22,202 --> 00:56:25,236
not it's not like, divide by two and

1037
00:56:25,258 --> 00:56:28,644
then do this.

1038
00:56:28,682 --> 00:56:30,612
It's like these are multi step

1039
00:56:30,666 --> 00:56:34,728
derivations. And so

1040
00:56:34,814 --> 00:56:36,356
going back to the accuracy minus

1041
00:56:36,388 --> 00:56:38,856
compression, that's the challenge of

1042
00:56:38,878 --> 00:56:41,236
writing a textbook accuracy minus

1043
00:56:41,268 --> 00:56:43,050
compression of what they had to share.

1044
00:56:46,640 --> 00:56:51,224
So our work is to kind of refine

1045
00:56:51,352 --> 00:56:53,180
that balance for ourselves.

1046
00:56:54,160 --> 00:56:55,836
Some of these look helpful. Some of them

1047
00:56:55,858 --> 00:56:59,164
don't look helpful. In terms of the AI

1048
00:56:59,212 --> 00:57:02,800
answers, I. Would use the word scary.

1049
00:57:09,310 --> 00:57:10,986
I got to run, by the way. Great. Thank

1050
00:57:11,008 --> 00:57:12,810
you. Yeah. Any other comments?

1051
00:57:17,470 --> 00:57:19,660
All right, great. Well, next time,

1052
00:57:20,750 --> 00:57:22,666
second discussion on chapter seven. So

1053
00:57:22,688 --> 00:57:25,330
we'll return and look at what questions

1054
00:57:25,400 --> 00:57:28,034
have been upvoted or asked. So thank you

1055
00:57:28,072 --> 00:57:31,330
all. Farewell. Bye.

1056
00:57:33,190 --> 00:57:33,680
Thank you.



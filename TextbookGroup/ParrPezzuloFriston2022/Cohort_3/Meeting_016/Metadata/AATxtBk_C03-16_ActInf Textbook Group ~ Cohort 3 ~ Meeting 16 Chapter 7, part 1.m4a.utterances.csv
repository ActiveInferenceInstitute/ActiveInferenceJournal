start	end	speaker	confidence	text
560	45610	A	0.9255565625000002	You. Hello everyone. We're in our first discussion of chapter seven in cohort three. So let's head over to chapter seven and we will come to the questions. But let's just start with anyone who wants to do you want to just give a general comment or any reflection or thought on seven or anything up to this point? Ali and then anyone else?
49020	104110	B	0.9160884146341463	Yes. So chapter seven is basically the application of the first half of chapter four in terms of modeling discrete time situations. And it goes through a number of case studies in order to show how the matrices A-B-C and D can be constructed in various situations and the challenges or rather the concerns we may have when constructing such matrices in order to have a viable discrete time model for each situation. So, yeah, that's basically the premise of chapter seven.
106560	420140	A	0.9396071507352948	Awesome, thank you. Totally agree. Just like chapters two and three with low road and the high road, we're kind of like a pair. Chapters seven and eight are kind of like a pair because seven is going to go into discrete time modeling. Chapter eight is on continuous time and hybrid modeling. Does anyone else want to make any just overall reflection or comment on chapter seven? All right, I'm checking if there's any okay. Does anyone have any question at all? Otherwise we'll turn first to these questions, start with the written questions we have and then just approach it from there. Does anyone have anything else that they want to consider first though? Okay, I'm putting in the chat, that's the social sciences link, but here's the chapter seven link in the chat and so you can follow along there. Also please upvote the questions that are interesting to you because the questions that have a bunch of upvoted points we're going to make the short videos for. Okay, so we'll just go to the I'm going to unvote all the questions from my own you can still vote for them, but I'm just going to unvote them so that once we get to them, I'll vote for them. Okay. All right, first question here's, chapter seven. Chapter seven begins with a quotation. What I cannot create, I do not understand. Richard Feigman, what do you think this means in the context of active inference? Specifically, I can't remember exactly who added this in a previous cohort, but they gave a mild and a stronger answer. The mild answer is you have to learn by doing. You actually have to build the generative model to understand active inference. And then the stronger answer goes even beyond that. And it's not just that you have to build the generative model, you actually have to have something that escapes your hands and does something interesting to really test the method and understand it. Anyone else want to add a comment on this? Okay, kind of a warm up question, but anyone else want to add anything on this? What I cannot create, I do not understand. Otherwise we'll come into the more specifics of the chapter seven now. Okay, so in that case, I'm just going to go with the most upvoted questions. Okay. All right. This is on page 130, the textbook they wrote. This choice, which we're going to look to the textbook in a second, speaks to the classical exploration exploitation dilemma and psychology, a dilemma that is resolved under active inference. What do you think about this? Okay, so the setting that's being considered is decision making under uncertain payoff or reward structure. So you think there's two slot machines, one of them pays out 50% of the time. You don't know how much the other one pays out. So you could continue to get 50% that would be exploit, or you could explore, and it might be better than 50%, it might be worse than 50%. And so that is sometimes called the exploration exploitation dilemma. Does anyone want to give a comment or a related question on how is active inference going to be approaching Explore exploit?
428990	476780	C	0.9006783132530124	Michael yeah, I just wanted to both affirm that I found the mathematical representation of these two ideas, that it's a trade off very interesting. But to describe them as resolved is something that I find might be a little more overdescribed, if you will. Maybe the word is this formula describes the trade off or as a way of representing it, but resolution is a much more open topic. So just thought I would give a response, give feedback and say, well.
478590	525450	A	0.9344923157894736	Yeah, that's it. Yes, maybe resolve in the sense of like a high resolution image, like we can see it. We've resolved the dilemma by perceiving it not we have explained it away or said that there is no trade off. I believe this was Eric Sounds previously, who kind of contested the idea that Explore exploit is a dilemma. It's not a dilemma. There's not two options that are intrinsically opposed. There's just one integrated behavioral choice. All of this being said, let's look at how Explore exploit our approach differently in active inference.
526910	574406	C	0.890281264367816	One more remark might be that in a different sense making context where the explore versus exploit issue arose continuously, the word sharpening visibility was a way of describing the space that sometimes just your resolution comment helps remind me of that. That it was like boosting the visibility of the space does not resolve the problems that you're trying to address, but it does improve your ability to act on the space. And I guess that's maybe a suggestion for language boosting visibility versus resolving that's.
574438	809876	A	0.9378308951965071	All yes. Awesome. Yeah. Just to briefly catch us up to how we get to this claim, chapter seven, like Ollie mentioned, is going to be concerned with a series of examples of building discrete time generative models. So these are models where time clicks like one step at a time. Figure 7.1 is showing just the perceptual or the downstairs component of a generative model. So D is the prior, S are the hidden states, b are how the hidden states change through time. A is the matrix that maps hidden states to observations. So this is just the temperature in the room changing through time outputting. A thermometer reading at every time point. This is hidden Markov model. Not a Markov decision process, just a hidden Markov model. Hmm. They then write the A and the B matrix as well as the D for a musical setting. So there's four notes. The A matrix is basically saying you hear the correct note 70% of the time, and then the B is the transition matrix. So this is just playing four notes that transition to each other 97% of the time in this order. And it always starts on the first note. So with just those three matrices defined A, B, and D, you have fully defined the generative model for this. Hmm. There's no decision making yet, but that's all you need to define this. Hmm. And then in Figure 7.2, there's an example where actually there turns out in this simulation that the third observation, it should be here, but it's actually here. So that's what's heard because there's such a prior that that's what should be heard. Okay, now we get into action, which is where the explore and exploit are going to come back into play. So if it's just a purely passive inference, then the kind of dilemma or unavoidable trade off is like over and under fitting perceptually, making something that's over compressed or under compressed with your beliefs. But action has a different set of challenges because it's related to selection of causal intervention in the world and it changes sequences of future observations and all of this. And as we know from the expected free energy, the imperative for action is to have both epistemic value, like to gain information, and to have pragmatic value, to have alignment between your observations, your preferences. Here's figure 7.3. This is kind of the classic generative model in discrete time. So we see that same downstairs component, the exact same. Plus there's the upstairs decision making component, pi policy intervening into how states change their time, expected free energy g updating policy decisions, updating them based upon their expected free energy. Michael?
809908	810408	C	0.8171550000000001	Yeah, sorry.
810494	811530	A	0.774215	Yeah, please.
811980	859290	C	0.9410755555555556	I'm jumping back 60 seconds, but right before you described that diagram, you said how you compress the I can't remember, but you use the word compression. And I wondered, why is he using compression as the verb to describe what's happening? And are you suggesting that in the same way that let's say when you compress data onto to save more storage space, you do an abstraction, reduce it, and it's a compression of what the data is. That is what's happening. There is a I'm sorry to be wordsmithing here. I'm just trying to make sure I'm tracking what you were saying when you said okay.
860380	926270	A	0.9217171779141101	It is very closely related to this. There's two ways to think about it. One is like we're, we're fitting some gaussian, let's just say some bell curve over some uncertainty. So we could think about like, do we want over compressing it in terms of tightening the variance? That would be like overfitting versus like if it's loose, then it's like under. So that's a little bit like a loose sense of compression, really. It's just a variance estimation. Or you can think about complexity minus accuracy as being like optimal zip compression. Like obviously if you wanted a lossless compression you wanted perfect accuracy, then you're opening the door to an infinitely complex model. On the other hand, if you're willing to find a trade off between complexity and accuracy, which is summarized by variational free energy in the moment, then you would end up with at the point of diminishing returns, like the kind of the Pareto optimal compression point.
930800	931740	C	0.9405600000000001	Thank you.
931810	1064970	A	0.9151314741035853	Sorry to no, it's all good. So now to study this action setting, which is where explore exploit comes into play, we're going to switch from the music listening example, which was just kind of familiarizing us with the A, B and the D. And now we're going to bring in action, which is going to require talking about Pi policy and C our preferences. Okay, so it's a T maze task. There's an aversive stimulus in one arm, an attractive stimulus in the other arm, and a Q that indicates the location of the two stimuli in the bottom of the T. So the organism can either pick one of the arms left or right, maybe quickly get to the reward, but maybe get the aversive stimulus. Or it could choose to seek out this informative cue and then having reduced its uncertainty about the location of the reward, then just walk over to the reward. So that's going to be one, two, three steps instead of just going straight to one of the arms, but you then have higher accuracy with getting to one of the arms. So how do you get flexible and rapid and adaptive switching between pragmatic oriented behavior and epistemic oriented behavior especially? How can that happen as new information are rolling in without necessarily retraining the model? That is what is going to be explored here. Anyone else want to just add a comment or thought or how they see it at this point?
1068700	1093970	D	0.8865521428571429	Yeah, have a general question in terms of if you do an experiment with different animals, for example, or different observers, and you want to model different balance between this epistemic and pragmatic behavior, where in the ABCD matrices would that be?
1095160	1264890	A	0.9369315181518155	Yeah, very good question. One place that this plays out is in the amplitude of C. So let's just say that left and right are the two directions on the teammates and we don't have a habit for either one of them if we prefer the sugar over the aversive. So now we're going to talk about the C matrix, if it's one and zero so aversive we don't really care about, then we give a value of one for the reward versus if the C was a million and negative a million, then the pragmatic value term would basically just be like scaled larger. So the amplitude on C is commonly used to weight the Pragmatic value relative to the epistemic value. Because just framing things like this does not guarantee that your parameterization is going to be at this adaptive point that actually does flexibly trade off between epistemic and Pragmatic behavior. Because if C is very small and washed out so utility seeking has shapes behavior a little bit, then you're going to end up with a mainly epistemic agent. And if C is vast relative to the epistemic value, then you're going to end up with mainly reward seeking, just like we see in the in figure Two six, where if we totally discard the pragmatic term we end up with purely epistemic behavior. And if we totally discard the epistemic term we end up with purely pragmatic behavior. But also if this term is just really small, you can imagine it doesn't play that large of a role and vice versa. So in an empirical setting, you have to parameterize the value, for example, of C, so that you do get goal reward oriented behavior, but not just like one track mind, um, Ali and then anyone else.
1268620	1420170	B	0.9175232765957444	Yeah, well, actually this C parameter C matrix here in the formulation of expected free energy is a relatively recent addition to active inference formulation because in some earlier literature or even some recent literature, this parameter c is not explicitly expressed in the formulation, they just express it as P of observation. Because you see, this is a crucial step actually in active inference because if we want to derive this expected free energy as a parallel to variation of free energy, then here instead of C, we should have only Pi, right? But replacing Pi with C denotes the kind of inaccuracy in terms of our policy selection. In other words, it doesn't necessarily entail, I mean, undertaking the policy we've selected, we have a preference matrix for undertaking some actions, but it doesn't necessarily imply that we only take those specific actions per se. So we somehow approximate our preferences without restricting ourselves to the actual policies that's been undertaken in the situation. So that allows us to even formulate or at least describe some counterfactual examples as well as well as the actual behavior we observe for the agent. So that's, I think, one of the crucial steps in terms of allowing for a broader behavior of the agents and somehow broadening the possibility spaces of the agents, not just the actualized trajectories of the internal states versus the external states.
1424500	1690340	A	0.9265088372093038	Yeah, it allows us to articulate what is the action possibility space and separate that from the outcome preference so we don't have our finger on the scale directly, can't control observations directly. That's the kind of perceptual control theory insight. You want your preferences to be about observations, but you don't want to be able to directly control observations. So what are the two channels that we have to reduce expected free energy, change our mind, change the world? How much to balance those two? That was Olivier's question. And that is the question is how do you balance those two? But those are the two things that you want to have from a first principles being balanced off in terms of your imperative for action selection. You want your action selection to be an epistemically informative path and a pragmatically useful path. Epistemic value, expected information gain under a policy pragmatic value how well it aligns with your preferences expectations. So this is how exploitative and explorative behavior are again going to be resolved. Not to say addressed away, but how it's approached. If we were in a reward setting, then exploitative behavior needs no secondary explanation. It's like there's money on the ground. If it's about getting money, then just pick it up. So exploitative behavior under reward centric paradigm does not need a secondary explanation. Exploratory behavior on the other hand, eventually needs to navigate into the currency of utilitarian value in a reward centric path. And so an exploratory path might be highly valued because for example, there might be a reward. So if there's a 10% chance of a $1,000, then that policy would have that expected reward. And so that might be more preferable than for sure one dollars or 10% of $1,000 in the reward centric approach. So then that's how you get explore and exploit balancing. You have exploratory trajectories being evaluated in terms of their expected returns in reward and then you can have everything compared on the common currency of expected pragmatic value in reward learning. Okay, so how are we going to approach it differently in active inference? Different trajectories of action are going to be compared again in a common currency. But rather than that common currency being expected reward, the common currency is going to be expected free energy. Expected free energy is going to have two components expected information gain and expected pragmatic value. As stated before, balancing those two terms into an adaptive zone is a fine tuning question. But this is how we approach the question in a first principles way by making a unified imperative that one includes perception and action. That's the integration of perception action loop. We see like observations and we see actions. They're just all there in the same equation and it unifies epistemic and pragmatic value. Instead of coercing epistemic actions into their expected reward and then comparing everything on the reward meter stick, we just have a single unified imperative, expected free energy that contains an epistemic and a pragmatic loading. Michael.
1692600	1694230	C	0.913535	Great stuff. That's all.
1696280	1722864	A	0.8814178723404253	Yeah, equation 2.6 gets revisited again and again and just different ways of seeing expected free energy so now we're going to continue seeing the teammates. So here the matrices are shown for this teammate. So, yeah, there's the mouse aversive and beneficial outcome. Olivier yeah, sorry.
1722902	1746050	D	0.8920087755102041	No, it was just a bit like it's behind on the last equation 7.4 for you. I find your last explanation really great and very enlightening. And in a basic context of reinforcement learning, you will have somehow the first term within the second, is that what you're saying?
1749600	1814930	A	0.947324928571428	It wouldn't be nested within the second strictly, but yes. Basically policies, one slot machine is giving you the 50% reward and then you don't know the other one. But a policy might be entertained and even selected because it has a high expected value. That is expected value based decision making because you say, well, I think slot machine, this slot machine might have a high value, so I'm going to take an exploratory action to kind of explore that, but not because of the information gain it gives me, but because of the expected reward. So it's kind of like converting everything to a pragmatic currency and then comparing the expected utility or the expected reward. And so you get utility discounting, you get time preference, all this stuff, but those are all correction factors within pragmatic value.
1815700	1818560	D	0.8995177777777776	Yeah. Okay, yeah. Thank you. Sorry for the interruptions.
1819700	2146790	A	0.9320719749216297	This is one of the key topics and this is how policies are selected. And that is the empirical challenge, is being able to parameterize everything so that it doesn't just behave trivially like a novelty seeking agent over here, or like a utility seeking agent over here. We want to have some training in the middle here's where the matrices are shown for these two settings. Here the block is on the right and here the white is on the right. And it just shows what the matrices look like. So it's helpful to see what the actual matrices look like in the generative model, because remember, this is the generative model. So if you define A, B and D, just like we saw in the music example, they're just specific matrices of whatever dimensionality it is that you're studying. If there was four notes, then D had four in a vector, a was a four by four, b was a four x four matrix in the music example. Now when you add in policy, policy has a dimensionality of however many policies you can take. So if there's four options policy has, there's four options in the pi vector and then there's going to be a slice of B for each of the dimensionality of pi. So if there were four options for what you could do, then there would be four slices in B. And it's kind of like, okay, if you choose action three, then B three third slice in b is going to be the one that we use to update S. So once you define this very limited, very interpretable set, of parameters. You will have stated the generative model and then you're ready to engage with standard methods for training and updating this generative model. And so that's why in active inference we spend a lot of time think about chapter six setting up our understanding of the problem because once you can describe the dimensionality and the parameters of the generative model that was the work, then you just update this as a statistical model using standard methods. So there's a lot of work in understanding the problem and in framing it and understanding what is the system of interest and all these other things, what form of degenerative model is appropriate but then you don't need to engage hopefully but actually in secondary engineering concerns for reasonable projects. I'm sure at some very large scale these things do come back into into play. But that's kind of the cool thing about active inference. We understand what all these variables are and then when we want to model our own setting we just specify the generative model and kind of hit play. More matrices being shown. Here's the C vector and so plus six for the attractive stimulus and negative six for the aversive stimulus. Now if this were zero zero six and negative six so the amplitude of C were just smaller so the relative preferences would stay the same but the pragmatic term would just be smaller. So then the agent would behave more epistemically. Whereas if this were 6 trillion and negative 6 trillion then the pragmatic term would come to dominate. So again, what is the approach of the resolution? It's putting different trajectories of behavior, action selection on a common footing, expected free energy which unifies the epistemic and the pragmatic value of different actions. We don't say epistemic and pragmatic value are the same thing. In fact they're different but they're complementary. And so having a unified imperative helps us adaptively approach this question of the actual trade offs between courses of action that have known consequence and those that are providing new information. Anyone want to have a thought? Yeah, go for it.
2148440	2215390	C	0.9144363190184049	I'm a babe out of water in so many levels. But in my original field of training in economics we had this phrase we used often called Satiris paribus which was a way of kind of saying that's an extradinality to our models and we're just going to keep going with the way we do things, which is close enough. And the consequences of this kind of poor treatment of dilemmas was that things like pollution and other externalities were they might not fit in the representational model and therefore were kind of underweighted and trivialized. And what I'm hearing you make a case for, in a different discipline, in a different way is changing the math to handle what is otherwise trivialized as external, too inconsequential to be attended to, is embracing the tension and thus making it a part of what then can be the sense making model of the discipline overall. Which seems something we can learn from you all.
2218640	3309688	A	0.9232162656400426	Building not even one layer and connecting that to kind of good hearts. Paradox like a metric becomes Gamified. Do we care about the levels of lead in the measurement or do we care about the hidden unobserved levels of lead in the soil? And so in our systems model that hopefully everyone will be able to come to the table on, we could say, well really there's both. They're just different things. There's the true underlying distribution which we're never going to measure at every little grain of sand and then there's the sensor fusion that we have to do with observations. So let's not try to game the observations, but rather come to the table with a more holistic understanding of the relationship between hidden states and observations and how hidden states are changed and how our actions influence the hidden state distribution, but how that emits observations, just articulate the problem, break it down into the components. And then the amazing thing is these natural components of the situation again prior beliefs about hidden state underlying hidden states that are unobserved emission of observations, how the world changes through time, actions that influence how the world changes through time, our strategy of deciding on action, those are our naturally separable aspects of thinking about a situation. And in a way it's kind of amazing that all you have to do is just like smash the matrices together and the math does kind of work out again in a way that we would want it to. Yeah, it's Chill 7.4 is going to go into a little bit more detail on this epistemic value of policies. So there's a few different breakdowns here. They're all equal but this is a few different representations of just how we can think about the informational gain associated with a given policy. So, whereas the pragmatic value for a policy is fairly straightforwardly identified with how close the expected observations are to the preferences. So the most pragmatically valuable trajectory is the one where the body temperature is right in the middle of the temperature distribution. The least pragmatic one would be the further away. But here's where we see this I of Pi. So there's a few more descriptions on I of Pi. And so it turns out in this case with this teamase, initially the action that the mouse selects is to it takes an epistemic action to reduce uncertainty because it's like, okay, there might be some pragmatic value, but also it's a risky decision up here, but going down is going to be a policy selection that gives me a lot of information. Now this isn't the whole iguana as they brought up in the beginning of the book. Like how does the mouse come to know that the aboutness of the queue is this setting? So this isn't like from the big bang to the mouse making the decision, the total model, there's always stuff that's structurally encoded in the model how does it mouse come to know that the aversive stimulus is negative and so on. So these are levels that you can add in but this is just showing basically again in this simple example it's just showing what it looks like when the mouse undertakes an epistemic action. And in the Pymdp model stream 7.2, where there's a very similar example to this it's not in a Tmaze but it is with a mouse that gets some information and then makes a decision. So it's very similar to this. You could play around in the script and you can see what happens when you change the preference vector to like a super high or super low value. Okay, little bit of boxed discussion on precision and entropy. The h is associated with entropy but let's just continue on. That just relates to the over underfitting of the expected the expected sharpness or blurriness of a distribution. They move on to another of the settings that's been very well repeatedly studied in active inference which is the eye cicade setting the motion of the eyes. And so one reason why this has been studied a lot is that the fovea of the vision, the area that we have high resolution color vision is actually very, very small relative to the visual field. And that's why multiple times per second the eye is making these policy selections to move around. And those policy selections, although we might look at something that's beautiful or rewarding and have that fix our gaze, to a first approximation, the imperative of the eye circade is informational. It's gaining information about the visual field and so that is why the epistemic visual search is a really useful intangible setting. Again, it's not like gambling. You're kind of setting up a situation towards pragmatic value but rather this is one where there's going to be an emphasis on the epistemic. And so this is like based upon the ambiguity of stimuli and about how epistemic attraction occurs. When there's information to learn. Where something where there's no information to learn doesn't that moving there does not have epistemic value. Kind of this is a little bit of a formalization of the streetlight story from every culture. Where did, where, you know, where are you looking for your keys? Under the streetlight. Oh, did you lose them there? No, this is just where I could look clearly. So that is about looking where there's light and searching where you can not necessarily because you have a high prior that the information is there but just because it's easily resolvable under the streetlight best place to find high quality unambiguous uncertainty resolving information. So the ambiguous poorly lit square is ignored reproducing in silico streetlight effect. If you're not going to get resolvable information it's not of epistemic value to go there but if it is going to be high quality uncertainty reducing information that is like epistemically valuable 75 Learning and Novelty so in. Figure four, three in seven, where we, where we revisit it in 73. There's no learning in this model as written, there's updating of the hidden states by moving it through the B matrix. But the A, B and D themselves do not change. So now they're going to talk about how learning can come into play. So in other words, like the A matrix is fixed in this setting. But you might be interested in a situation where the A matrix can change through time. Like maybe the mapping between hidden states and observations changes. The way that learning is generally approached in Bayesian inference settings is you make a prior distribution on the variable of interest. So for example, lowercase A is going to be a hyper prior or just a prior distribution on A. So instead of just saying A is seven one one like we saw in the Music example, you can have a distribution over what A looks like that is going to help with learning. In the extreme case where learning rate converges to zero, that's like a super sharp distribution around what you think A is going to be like so that you have very strong prior on A. So new information is only going to move that sharp distribution a tiny amount. Conversely, if you were having a lot of learning on A, you'd have a very flat prior on what A could be so that new observations would update your beliefs about A a lot. But this is just an example of the composability of this framework in that the base model doesn't have learning attention, all these different relatively sophisticated cognitive phenomena. But this is like the little firmware nugget or kernel which by understanding the compositionality relationships of then we can start to bring in learning on different parameters. And of course learning is awesome and it sounds awesome, it is. But if you also think about it from a statistical perspective, that doesn't mean that every parameter should be learnable because sometimes you're just increasing the computational challenge of your problem vastly more information on learning. Here we see another difference with learning between active inference and many other machine learning approaches. And this is the embedding of action in the inference process. We actively think about how much information are we going to get. Like when people are training an image recognition, it just here's a thousand images in the training set just plug and chug. But what if you had an agent that was like which image should I look in of these thousands? Then it would become of primary importance which sequence of learning observations the agents took within those thousand labeled data points. So that's what G helps us understand some new terms salience, novelty, different decompositions of G. One final example, again, more just like referenced because it's in other papers but not really explained with the matrices. Here we have a maze exploration agent which is like another kind of classic setting so it has gray in the likelihood, which is like 50 50 between white and black. But it starts off, it takes these three steps and it's like, okay, it's updating, now it moves. Here updates that. Okay, that's a white square, that's a white square, that's a block square, that's a block square. So this is like somebody looking over a maze visually seeking a path to solve the maze. 7.3 is a little bit of a summary of a pretty advanced topic which is Bayesian model reduction and structure learning which is treating the question of what structure should the generative model have? Treating that question as a parametric inference problem. So should the model be one, two or three time steps deep and then say, okay, well we're going to have some time horizon parameter and then we're going to test whether one, two or three is the better model. So like we're going to learn the structure of a model as part of reducing down from a portfolio of models but they just sort of briefly note it and there's more citations and then as and then any other just thoughts or questions. Seven kind of has a lot of little short vignettes. Okay, just briefly, just so that we touch it all, then next time we can have voted and added a lot of questions and everything. 712 is a two level model. It's a nested or a hierarchical model. Both the higher and the lower level, one and two are discrete time. You can tell because they all have the s t minus one t t plus one. So later we'll see a hierarchical model where the lower level is continuous time and the top level is discrete time. But in this setting both nested levels are discrete time. So this could be every hour and this is 60 minutes within an hour, every day, 24 hours within a day. Here's a multi scale updating generative model that's related to sentence reading and they've done a lot of simulation studies of multilevel inference in reading, like where the eyes are moving to resolve uncertainty about the letter. Order of letters is sought out to reduce uncertainty about words. Order of words in a sentence is sought out to reduce uncertainty about semantics. Semantics are operated on for pragmatic value. So that's chapter seven. It's a fairly loosely connected sequence of discrete time models. Several examples, starting from the musical example, which is just a passive inference hidden Markov model bringing us back to the discrete time generative model that we saw in Figure 4.3 that's shown again in 7.3 and then several decompositions. Of expected free energy focusing on the Epistemic value, especially in terms of how getting Epistemic and Pragmatic value unified under this G functional allows us to approach the adaptive finessing of exploratory and exploitative behavior. And again, the work is in understanding and designing the generative models. That's not work about work. That's really the task itself is the systems modeling and systems understanding, because essentially, by saying what these variables are, what shape these variables are, you will have prepared your model to run. And until these models are defined in terms of their dimensionality, it's not ready to run. Any kind of closing thoughts, Michael?
3309744	3314430	C	0.9097021428571429	It looks like Martin might have had to do the math class after all.
3318960	3320380	A	0.8525959999999999	There's so many. I mean.
3320450	3321550	C	0.9655533333333333	As will I.
3323700	3421104	A	0.93244717791411	There'd be so many. And and again, like, let's try to let's explore how the let's see how the the AI explanations did for chapter seven math explanations. Why would a high schooler be interested? Let's look at it for one of the epistemic ones. People can evaluate and add comments or change the text, but for those who are learning, and ultimately we all are, I doubt there are few, if any, people for whom this equation doesn't require contextualization and clarification and deeper breathing. Yeah, these lines are not it's not like, divide by two and then do this. It's like these are multi step derivations. And so going back to the accuracy minus compression, that's the challenge of writing a textbook accuracy minus compression of what they had to share. So our work is to kind of refine that balance for ourselves. Some of these look helpful. Some of them don't look helpful. In terms of the AI answers, I.
3421142	3430506	C	0.9307683333333333	Would use the word scary. I got to run, by the way.
3430528	3451330	A	0.9156164864864864	Great. Thank you. Yeah. Any other comments? All right, great. Well, next time, second discussion on chapter seven. So we'll return and look at what questions have been upvoted or asked. So thank you all. Farewell. Bye.
3453190	3453680	D	0.998405	Thank you.

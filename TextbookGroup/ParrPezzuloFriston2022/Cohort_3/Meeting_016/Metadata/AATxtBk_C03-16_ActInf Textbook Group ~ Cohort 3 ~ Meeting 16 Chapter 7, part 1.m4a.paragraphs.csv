start	end	paragNum	speaker	confidence	startTime	wordCount	text
560	37550	1	A	0.47	00:00	59	You. Hello everyone. We're in our first discussion of chapter seven in cohort three. So let's head over to chapter seven and we will come to the questions. But let's just start with anyone who wants to do you want to just give a general comment or any reflection or thought on seven or anything up to this point?
43020	45610	2	A	0.72443	00:43	5	Ali and then anyone else?
49020	104110	3	B	0.92	00:49	82	Yes. So chapter seven is basically the application of the first half of chapter four in terms of modeling discrete time situations. And it goes through a number of case studies in order to show how the matrices A-B-C and D can be constructed in various situations and the challenges or rather the concerns we may have when constructing such matrices in order to have a viable discrete time model for each situation. So, yeah, that's basically the premise of chapter seven.
106560	125600	4	A	0.99895	01:46	53	Awesome, thank you. Totally agree. Just like chapters two and three with low road and the high road, we're kind of like a pair. Chapters seven and eight are kind of like a pair because seven is going to go into discrete time modeling. Chapter eight is on continuous time and hybrid modeling.
126180	132710	5	A	0.99964	02:06	15	Does anyone else want to make any just overall reflection or comment on chapter seven?
154400	178560	6	A	0.82	02:34	48	All right, I'm checking if there's any okay. Does anyone have any question at all? Otherwise we'll turn first to these questions, start with the written questions we have and then just approach it from there. Does anyone have anything else that they want to consider first though?
185940	214810	7	A	0.98857	03:05	55	Okay, I'm putting in the chat, that's the social sciences link, but here's the chapter seven link in the chat and so you can follow along there. Also please upvote the questions that are interesting to you because the questions that have a bunch of upvoted points we're going to make the short videos for.
217420	229310	8	A	0.99449	03:37	41	Okay, so we'll just go to the I'm going to unvote all the questions from my own you can still vote for them, but I'm just going to unvote them so that once we get to them, I'll vote for them.
231960	250696	9	A	0.99534	03:51	36	Okay. All right, first question here's, chapter seven. Chapter seven begins with a quotation. What I cannot create, I do not understand. Richard Feigman, what do you think this means in the context of active inference?
250728	288090	10	A	0.66918	04:10	43	Specifically, I can't remember exactly who added this in a previous cohort, but they gave a mild and a stronger answer. The mild answer is you have to learn by doing. You actually have to build the generative model to understand active inference.
290430	305780	11	A	1.0	04:50	43	And then the stronger answer goes even beyond that. And it's not just that you have to build the generative model, you actually have to have something that escapes your hands and does something interesting to really test the method and understand it.
308230	311700	12	A	0.97735	05:08	9	Anyone else want to add a comment on this?
317750	330380	13	A	0.95684	05:17	36	Okay, kind of a warm up question, but anyone else want to add anything on this? What I cannot create, I do not understand. Otherwise we'll come into the more specifics of the chapter seven now.
336350	343980	14	A	0.9883	05:36	15	Okay, so in that case, I'm just going to go with the most upvoted questions.
348050	348800	15	A	0.98009	05:48	1	Okay.
353340	386124	16	A	0.95	05:53	63	All right. This is on page 130, the textbook they wrote. This choice, which we're going to look to the textbook in a second, speaks to the classical exploration exploitation dilemma and psychology, a dilemma that is resolved under active inference. What do you think about this? Okay, so the setting that's being considered is decision making under uncertain payoff or reward structure.
386252	420140	17	A	1.0	06:26	86	So you think there's two slot machines, one of them pays out 50% of the time. You don't know how much the other one pays out. So you could continue to get 50% that would be exploit, or you could explore, and it might be better than 50%, it might be worse than 50%. And so that is sometimes called the exploration exploitation dilemma. Does anyone want to give a comment or a related question on how is active inference going to be approaching Explore exploit?
428990	457060	18	C	0.71126	07:08	45	Michael yeah, I just wanted to both affirm that I found the mathematical representation of these two ideas, that it's a trade off very interesting. But to describe them as resolved is something that I find might be a little more overdescribed, if you will.
460410	496946	19	C	0.99892	07:40	79	Maybe the word is this formula describes the trade off or as a way of representing it, but resolution is a much more open topic. So just thought I would give a response, give feedback and say, well. Yeah, that's it. Yes, maybe resolve in the sense of like a high resolution image, like we can see it. We've resolved the dilemma by perceiving it not we have explained it away or said that there is no trade off.
497128	525450	20	A	1.0	08:17	54	I believe this was Eric Sounds previously, who kind of contested the idea that Explore exploit is a dilemma. It's not a dilemma. There's not two options that are intrinsically opposed. There's just one integrated behavioral choice. All of this being said, let's look at how Explore exploit our approach differently in active inference.
526910	578860	21	C	1.0	08:46	90	One more remark might be that in a different sense making context where the explore versus exploit issue arose continuously, the word sharpening visibility was a way of describing the space that sometimes just your resolution comment helps remind me of that. That it was like boosting the visibility of the space does not resolve the problems that you're trying to address, but it does improve your ability to act on the space. And I guess that's maybe a suggestion for language boosting visibility versus resolving that's. All yes. Awesome.
581230	597940	22	A	0.64753	09:41	48	Yeah. Just to briefly catch us up to how we get to this claim, chapter seven, like Ollie mentioned, is going to be concerned with a series of examples of building discrete time generative models. So these are models where time clicks like one step at a time.
600470	625498	23	A	0.99989	10:00	64	Figure 7.1 is showing just the perceptual or the downstairs component of a generative model. So D is the prior, S are the hidden states, b are how the hidden states change through time. A is the matrix that maps hidden states to observations. So this is just the temperature in the room changing through time outputting. A thermometer reading at every time point.
625664	634030	24	A	0.92773	10:25	16	This is hidden Markov model. Not a Markov decision process, just a hidden Markov model. Hmm.
636610	668730	25	A	0.99999	10:36	72	They then write the A and the B matrix as well as the D for a musical setting. So there's four notes. The A matrix is basically saying you hear the correct note 70% of the time, and then the B is the transition matrix. So this is just playing four notes that transition to each other 97% of the time in this order. And it always starts on the first note.
669470	683950	26	A	0.74	11:09	35	So with just those three matrices defined A, B, and D, you have fully defined the generative model for this. Hmm. There's no decision making yet, but that's all you need to define this. Hmm.
686770	708360	27	A	1.0	11:26	43	And then in Figure 7.2, there's an example where actually there turns out in this simulation that the third observation, it should be here, but it's actually here. So that's what's heard because there's such a prior that that's what should be heard.
710570	769100	28	A	0.99591	11:50	124	Okay, now we get into action, which is where the explore and exploit are going to come back into play. So if it's just a purely passive inference, then the kind of dilemma or unavoidable trade off is like over and under fitting perceptually, making something that's over compressed or under compressed with your beliefs. But action has a different set of challenges because it's related to selection of causal intervention in the world and it changes sequences of future observations and all of this. And as we know from the expected free energy, the imperative for action is to have both epistemic value, like to gain information, and to have pragmatic value, to have alignment between your observations, your preferences. Here's figure 7.3.
769470	800100	29	A	0.99996	12:49	52	This is kind of the classic generative model in discrete time. So we see that same downstairs component, the exact same. Plus there's the upstairs decision making component, pi policy intervening into how states change their time, expected free energy g updating policy decisions, updating them based upon their expected free energy.
809420	830476	30	A	0.99905	13:29	47	Michael? Yeah, sorry. Yeah, please. I'm jumping back 60 seconds, but right before you described that diagram, you said how you compress the I can't remember, but you use the word compression. And I wondered, why is he using compression as the verb to describe what's happening?
830578	862490	31	C	1.0	13:50	73	And are you suggesting that in the same way that let's say when you compress data onto to save more storage space, you do an abstraction, reduce it, and it's a compression of what the data is. That is what's happening. There is a I'm sorry to be wordsmithing here. I'm just trying to make sure I'm tracking what you were saying when you said okay. It is very closely related to this.
864380	887184	32	A	0.86051	14:24	68	There's two ways to think about it. One is like we're, we're fitting some gaussian, let's just say some bell curve over some uncertainty. So we could think about like, do we want over compressing it in terms of tightening the variance? That would be like overfitting versus like if it's loose, then it's like under. So that's a little bit like a loose sense of compression, really.
887222	926270	33	A	0.82963	14:47	88	It's just a variance estimation. Or you can think about complexity minus accuracy as being like optimal zip compression. Like obviously if you wanted a lossless compression you wanted perfect accuracy, then you're opening the door to an infinitely complex model. On the other hand, if you're willing to find a trade off between complexity and accuracy, which is summarized by variational free energy in the moment, then you would end up with at the point of diminishing returns, like the kind of the Pareto optimal compression point.
930800	967160	34	C	0.89112	15:30	74	Thank you. Sorry to no, it's all good. So now to study this action setting, which is where explore exploit comes into play, we're going to switch from the music listening example, which was just kind of familiarizing us with the A, B and the D. And now we're going to bring in action, which is going to require talking about Pi policy and C our preferences. Okay, so it's a T maze task.
968540	1039664	35	A	0.98353	16:08	137	There's an aversive stimulus in one arm, an attractive stimulus in the other arm, and a Q that indicates the location of the two stimuli in the bottom of the T. So the organism can either pick one of the arms left or right, maybe quickly get to the reward, but maybe get the aversive stimulus. Or it could choose to seek out this informative cue and then having reduced its uncertainty about the location of the reward, then just walk over to the reward. So that's going to be one, two, three steps instead of just going straight to one of the arms, but you then have higher accuracy with getting to one of the arms. So how do you get flexible and rapid and adaptive switching between pragmatic oriented behavior and epistemic oriented behavior especially?
1039782	1054230	36	A	1.0	17:19	24	How can that happen as new information are rolling in without necessarily retraining the model? That is what is going to be explored here.
1056600	1064970	37	A	0.99988	17:36	18	Anyone else want to just add a comment or thought or how they see it at this point?
1068700	1147140	38	D	0.78467	17:48	147	Yeah, have a general question in terms of if you do an experiment with different animals, for example, or different observers, and you want to model different balance between this epistemic and pragmatic behavior, where in the ABCD matrices would that be? Yeah, very good question. One place that this plays out is in the amplitude of C. So let's just say that left and right are the two directions on the teammates and we don't have a habit for either one of them if we prefer the sugar over the aversive. So now we're going to talk about the C matrix, if it's one and zero so aversive we don't really care about, then we give a value of one for the reward versus if the C was a million and negative a million, then the pragmatic value term would basically just be like scaled larger.
1147800	1217880	39	A	1.0	19:07	138	So the amplitude on C is commonly used to weight the Pragmatic value relative to the epistemic value. Because just framing things like this does not guarantee that your parameterization is going to be at this adaptive point that actually does flexibly trade off between epistemic and Pragmatic behavior. Because if C is very small and washed out so utility seeking has shapes behavior a little bit, then you're going to end up with a mainly epistemic agent. And if C is vast relative to the epistemic value, then you're going to end up with mainly reward seeking, just like we see in the in figure Two six, where if we totally discard the pragmatic term we end up with purely epistemic behavior. And if we totally discard the epistemic term we end up with purely pragmatic behavior.
1219100	1264890	40	A	0.99999	20:19	60	But also if this term is just really small, you can imagine it doesn't play that large of a role and vice versa. So in an empirical setting, you have to parameterize the value, for example, of C, so that you do get goal reward oriented behavior, but not just like one track mind, um, Ali and then anyone else.
1268620	1375484	41	B	0.73353	21:08	166	Yeah, well, actually this C parameter C matrix here in the formulation of expected free energy is a relatively recent addition to active inference formulation because in some earlier literature or even some recent literature, this parameter c is not explicitly expressed in the formulation, they just express it as P of observation. Because you see, this is a crucial step actually in active inference because if we want to derive this expected free energy as a parallel to variation of free energy, then here instead of C, we should have only Pi, right? But replacing Pi with C denotes the kind of inaccuracy in terms of our policy selection. In other words, it doesn't necessarily entail, I mean, undertaking the policy we've selected, we have a preference matrix for undertaking some actions, but it doesn't necessarily imply that we only take those specific actions per se. So we somehow approximate our preferences without restricting ourselves to the actual policies that's been undertaken in the situation.
1375682	1420170	42	B	1.0	22:55	69	So that allows us to even formulate or at least describe some counterfactual examples as well as well as the actual behavior we observe for the agent. So that's, I think, one of the crucial steps in terms of allowing for a broader behavior of the agents and somehow broadening the possibility spaces of the agents, not just the actualized trajectories of the internal states versus the external states.
1424500	1467276	43	A	0.54393	23:44	86	Yeah, it allows us to articulate what is the action possibility space and separate that from the outcome preference so we don't have our finger on the scale directly, can't control observations directly. That's the kind of perceptual control theory insight. You want your preferences to be about observations, but you don't want to be able to directly control observations. So what are the two channels that we have to reduce expected free energy, change our mind, change the world? How much to balance those two?
1467378	1505720	44	A	1.0	24:27	76	That was Olivier's question. And that is the question is how do you balance those two? But those are the two things that you want to have from a first principles being balanced off in terms of your imperative for action selection. You want your action selection to be an epistemically informative path and a pragmatically useful path. Epistemic value, expected information gain under a policy pragmatic value how well it aligns with your preferences expectations.
1511120	1534892	45	A	1.0	25:11	54	So this is how exploitative and explorative behavior are again going to be resolved. Not to say addressed away, but how it's approached. If we were in a reward setting, then exploitative behavior needs no secondary explanation. It's like there's money on the ground. If it's about getting money, then just pick it up.
1535046	1583360	46	A	1.0	25:35	89	So exploitative behavior under reward centric paradigm does not need a secondary explanation. Exploratory behavior on the other hand, eventually needs to navigate into the currency of utilitarian value in a reward centric path. And so an exploratory path might be highly valued because for example, there might be a reward. So if there's a 10% chance of a $1,000, then that policy would have that expected reward. And so that might be more preferable than for sure one dollars or 10% of $1,000 in the reward centric approach.
1583780	1623730	47	A	1.0	26:23	88	So then that's how you get explore and exploit balancing. You have exploratory trajectories being evaluated in terms of their expected returns in reward and then you can have everything compared on the common currency of expected pragmatic value in reward learning. Okay, so how are we going to approach it differently in active inference? Different trajectories of action are going to be compared again in a common currency. But rather than that common currency being expected reward, the common currency is going to be expected free energy.
1624420	1660904	48	A	0.99814	27:04	71	Expected free energy is going to have two components expected information gain and expected pragmatic value. As stated before, balancing those two terms into an adaptive zone is a fine tuning question. But this is how we approach the question in a first principles way by making a unified imperative that one includes perception and action. That's the integration of perception action loop. We see like observations and we see actions.
1660952	1690340	49	A	0.99372	27:40	52	They're just all there in the same equation and it unifies epistemic and pragmatic value. Instead of coercing epistemic actions into their expected reward and then comparing everything on the reward meter stick, we just have a single unified imperative, expected free energy that contains an epistemic and a pragmatic loading. Michael.
1692600	1694230	50	C	0.9928	28:12	4	Great stuff. That's all.
1696280	1731410	51	A	0.73784	28:16	63	Yeah, equation 2.6 gets revisited again and again and just different ways of seeing expected free energy so now we're going to continue seeing the teammates. So here the matrices are shown for this teammate. So, yeah, there's the mouse aversive and beneficial outcome. Olivier yeah, sorry. No, it was just a bit like it's behind on the last equation 7.4 for you.
1731780	1746050	52	D	0.51	28:51	33	I find your last explanation really great and very enlightening. And in a basic context of reinforcement learning, you will have somehow the first term within the second, is that what you're saying?
1749600	1804220	53	A	0.36	29:09	118	It wouldn't be nested within the second strictly, but yes. Basically policies, one slot machine is giving you the 50% reward and then you don't know the other one. But a policy might be entertained and even selected because it has a high expected value. That is expected value based decision making because you say, well, I think slot machine, this slot machine might have a high value, so I'm going to take an exploratory action to kind of explore that, but not because of the information gain it gives me, but because of the expected reward. So it's kind of like converting everything to a pragmatic currency and then comparing the expected utility or the expected reward.
1804800	1818560	54	A	0.98	30:04	31	And so you get utility discounting, you get time preference, all this stuff, but those are all correction factors within pragmatic value. Yeah. Okay, yeah. Thank you. Sorry for the interruptions.
1819700	1870576	55	A	0.99978	30:19	91	This is one of the key topics and this is how policies are selected. And that is the empirical challenge, is being able to parameterize everything so that it doesn't just behave trivially like a novelty seeking agent over here, or like a utility seeking agent over here. We want to have some training in the middle here's where the matrices are shown for these two settings. Here the block is on the right and here the white is on the right. And it just shows what the matrices look like.
1870758	1925760	56	A	0.97	31:10	127	So it's helpful to see what the actual matrices look like in the generative model, because remember, this is the generative model. So if you define A, B and D, just like we saw in the music example, they're just specific matrices of whatever dimensionality it is that you're studying. If there was four notes, then D had four in a vector, a was a four by four, b was a four x four matrix in the music example. Now when you add in policy, policy has a dimensionality of however many policies you can take. So if there's four options policy has, there's four options in the pi vector and then there's going to be a slice of B for each of the dimensionality of pi.
1926840	1991640	57	A	0.59	32:06	138	So if there were four options for what you could do, then there would be four slices in B. And it's kind of like, okay, if you choose action three, then B three third slice in b is going to be the one that we use to update S. So once you define this very limited, very interpretable set, of parameters. You will have stated the generative model and then you're ready to engage with standard methods for training and updating this generative model. And so that's why in active inference we spend a lot of time think about chapter six setting up our understanding of the problem because once you can describe the dimensionality and the parameters of the generative model that was the work, then you just update this as a statistical model using standard methods.
1993100	2035990	58	A	0.68	33:13	104	So there's a lot of work in understanding the problem and in framing it and understanding what is the system of interest and all these other things, what form of degenerative model is appropriate but then you don't need to engage hopefully but actually in secondary engineering concerns for reasonable projects. I'm sure at some very large scale these things do come back into into play. But that's kind of the cool thing about active inference. We understand what all these variables are and then when we want to model our own setting we just specify the generative model and kind of hit play.
2040250	2079250	59	A	0.65	34:00	83	More matrices being shown. Here's the C vector and so plus six for the attractive stimulus and negative six for the aversive stimulus. Now if this were zero zero six and negative six so the amplitude of C were just smaller so the relative preferences would stay the same but the pragmatic term would just be smaller. So then the agent would behave more epistemically. Whereas if this were 6 trillion and negative 6 trillion then the pragmatic term would come to dominate.
2088540	2136090	60	A	0.67	34:48	85	So again, what is the approach of the resolution? It's putting different trajectories of behavior, action selection on a common footing, expected free energy which unifies the epistemic and the pragmatic value of different actions. We don't say epistemic and pragmatic value are the same thing. In fact they're different but they're complementary. And so having a unified imperative helps us adaptively approach this question of the actual trade offs between courses of action that have known consequence and those that are providing new information.
2144280	2185004	61	A	0.90641	35:44	106	Anyone want to have a thought? Yeah, go for it. I'm a babe out of water in so many levels. But in my original field of training in economics we had this phrase we used often called Satiris paribus which was a way of kind of saying that's an extradinality to our models and we're just going to keep going with the way we do things, which is close enough. And the consequences of this kind of poor treatment of dilemmas was that things like pollution and other externalities were they might not fit in the representational model and therefore were kind of underweighted and trivialized.
2185052	2215390	62	C	1.0	36:25	67	And what I'm hearing you make a case for, in a different discipline, in a different way is changing the math to handle what is otherwise trivialized as external, too inconsequential to be attended to, is embracing the tension and thus making it a part of what then can be the sense making model of the discipline overall. Which seems something we can learn from you all.
2218640	2249140	63	A	0.98526	36:58	73	Building not even one layer and connecting that to kind of good hearts. Paradox like a metric becomes Gamified. Do we care about the levels of lead in the measurement or do we care about the hidden unobserved levels of lead in the soil? And so in our systems model that hopefully everyone will be able to come to the table on, we could say, well really there's both. They're just different things.
2249210	2325650	64	A	0.61423	37:29	185	There's the true underlying distribution which we're never going to measure at every little grain of sand and then there's the sensor fusion that we have to do with observations. So let's not try to game the observations, but rather come to the table with a more holistic understanding of the relationship between hidden states and observations and how hidden states are changed and how our actions influence the hidden state distribution, but how that emits observations, just articulate the problem, break it down into the components. And then the amazing thing is these natural components of the situation again prior beliefs about hidden state underlying hidden states that are unobserved emission of observations, how the world changes through time, actions that influence how the world changes through time, our strategy of deciding on action, those are our naturally separable aspects of thinking about a situation. And in a way it's kind of amazing that all you have to do is just like smash the matrices together and the math does kind of work out again in a way that we would want it to.
2329300	2381836	65	A	0.99629	38:49	96	Yeah, it's Chill 7.4 is going to go into a little bit more detail on this epistemic value of policies. So there's a few different breakdowns here. They're all equal but this is a few different representations of just how we can think about the informational gain associated with a given policy. So, whereas the pragmatic value for a policy is fairly straightforwardly identified with how close the expected observations are to the preferences. So the most pragmatically valuable trajectory is the one where the body temperature is right in the middle of the temperature distribution.
2382028	2428464	66	A	1.0	39:42	107	The least pragmatic one would be the further away. But here's where we see this I of Pi. So there's a few more descriptions on I of Pi. And so it turns out in this case with this teamase, initially the action that the mouse selects is to it takes an epistemic action to reduce uncertainty because it's like, okay, there might be some pragmatic value, but also it's a risky decision up here, but going down is going to be a policy selection that gives me a lot of information. Now this isn't the whole iguana as they brought up in the beginning of the book.
2428502	2485170	67	A	0.87617	40:28	134	Like how does the mouse come to know that the aboutness of the queue is this setting? So this isn't like from the big bang to the mouse making the decision, the total model, there's always stuff that's structurally encoded in the model how does it mouse come to know that the aversive stimulus is negative and so on. So these are levels that you can add in but this is just showing basically again in this simple example it's just showing what it looks like when the mouse undertakes an epistemic action. And in the Pymdp model stream 7.2, where there's a very similar example to this it's not in a Tmaze but it is with a mouse that gets some information and then makes a decision. So it's very similar to this.
2486520	2494390	68	A	1.0	41:26	28	You could play around in the script and you can see what happens when you change the preference vector to like a super high or super low value.
2500360	2529040	69	A	0.83875	41:40	39	Okay, little bit of boxed discussion on precision and entropy. The h is associated with entropy but let's just continue on. That just relates to the over underfitting of the expected the expected sharpness or blurriness of a distribution.
2531380	2599880	70	A	0.99998	42:11	136	They move on to another of the settings that's been very well repeatedly studied in active inference which is the eye cicade setting the motion of the eyes. And so one reason why this has been studied a lot is that the fovea of the vision, the area that we have high resolution color vision is actually very, very small relative to the visual field. And that's why multiple times per second the eye is making these policy selections to move around. And those policy selections, although we might look at something that's beautiful or rewarding and have that fix our gaze, to a first approximation, the imperative of the eye circade is informational. It's gaining information about the visual field and so that is why the epistemic visual search is a really useful intangible setting.
2600300	2602120	71	A	0.99963	43:20	5	Again, it's not like gambling.
2605020	2635570	72	A	0.94754	43:25	64	You're kind of setting up a situation towards pragmatic value but rather this is one where there's going to be an emphasis on the epistemic. And so this is like based upon the ambiguity of stimuli and about how epistemic attraction occurs. When there's information to learn. Where something where there's no information to learn doesn't that moving there does not have epistemic value.
2640520	2656836	73	A	0.95566	44:00	47	Kind of this is a little bit of a formalization of the streetlight story from every culture. Where did, where, you know, where are you looking for your keys? Under the streetlight. Oh, did you lose them there? No, this is just where I could look clearly.
2657028	2675810	74	A	1.0	44:17	45	So that is about looking where there's light and searching where you can not necessarily because you have a high prior that the information is there but just because it's easily resolvable under the streetlight best place to find high quality unambiguous uncertainty resolving information.
2678340	2733570	75	A	1.0	44:38	96	So the ambiguous poorly lit square is ignored reproducing in silico streetlight effect. If you're not going to get resolvable information it's not of epistemic value to go there but if it is going to be high quality uncertainty reducing information that is like epistemically valuable 75 Learning and Novelty so in. Figure four, three in seven, where we, where we revisit it in 73. There's no learning in this model as written, there's updating of the hidden states by moving it through the B matrix. But the A, B and D themselves do not change.
2734740	2775970	76	A	1.0	45:34	74	So now they're going to talk about how learning can come into play. So in other words, like the A matrix is fixed in this setting. But you might be interested in a situation where the A matrix can change through time. Like maybe the mapping between hidden states and observations changes. The way that learning is generally approached in Bayesian inference settings is you make a prior distribution on the variable of interest.
2776660	2840790	77	A	0.93	46:16	136	So for example, lowercase A is going to be a hyper prior or just a prior distribution on A. So instead of just saying A is seven one one like we saw in the Music example, you can have a distribution over what A looks like that is going to help with learning. In the extreme case where learning rate converges to zero, that's like a super sharp distribution around what you think A is going to be like so that you have very strong prior on A. So new information is only going to move that sharp distribution a tiny amount. Conversely, if you were having a lot of learning on A, you'd have a very flat prior on what A could be so that new observations would update your beliefs about A a lot.
2842040	2905490	78	A	0.99981	47:22	104	But this is just an example of the composability of this framework in that the base model doesn't have learning attention, all these different relatively sophisticated cognitive phenomena. But this is like the little firmware nugget or kernel which by understanding the compositionality relationships of then we can start to bring in learning on different parameters. And of course learning is awesome and it sounds awesome, it is. But if you also think about it from a statistical perspective, that doesn't mean that every parameter should be learnable because sometimes you're just increasing the computational challenge of your problem vastly more information on learning.
2909800	2948800	79	A	0.99997	48:29	80	Here we see another difference with learning between active inference and many other machine learning approaches. And this is the embedding of action in the inference process. We actively think about how much information are we going to get. Like when people are training an image recognition, it just here's a thousand images in the training set just plug and chug. But what if you had an agent that was like which image should I look in of these thousands?
2949240	2977210	80	A	0.55	49:09	37	Then it would become of primary importance which sequence of learning observations the agents took within those thousand labeled data points. So that's what G helps us understand some new terms salience, novelty, different decompositions of G.
2981450	3018340	81	A	0.79	49:41	89	One final example, again, more just like referenced because it's in other papers but not really explained with the matrices. Here we have a maze exploration agent which is like another kind of classic setting so it has gray in the likelihood, which is like 50 50 between white and black. But it starts off, it takes these three steps and it's like, okay, it's updating, now it moves. Here updates that. Okay, that's a white square, that's a white square, that's a block square, that's a block square.
3018950	3029970	82	A	0.77	50:18	17	So this is like somebody looking over a maze visually seeking a path to solve the maze.
3034250	3110190	83	A	0.98	50:34	136	7.3 is a little bit of a summary of a pretty advanced topic which is Bayesian model reduction and structure learning which is treating the question of what structure should the generative model have? Treating that question as a parametric inference problem. So should the model be one, two or three time steps deep and then say, okay, well we're going to have some time horizon parameter and then we're going to test whether one, two or three is the better model. So like we're going to learn the structure of a model as part of reducing down from a portfolio of models but they just sort of briefly note it and there's more citations and then as and then any other just thoughts or questions. Seven kind of has a lot of little short vignettes.
3116340	3142828	84	A	0.99415	51:56	67	Okay, just briefly, just so that we touch it all, then next time we can have voted and added a lot of questions and everything. 712 is a two level model. It's a nested or a hierarchical model. Both the higher and the lower level, one and two are discrete time. You can tell because they all have the s t minus one t t plus one.
3142994	3163890	85	A	1.0	52:22	52	So later we'll see a hierarchical model where the lower level is continuous time and the top level is discrete time. But in this setting both nested levels are discrete time. So this could be every hour and this is 60 minutes within an hour, every day, 24 hours within a day.
3167380	3203070	86	A	0.82543	52:47	69	Here's a multi scale updating generative model that's related to sentence reading and they've done a lot of simulation studies of multilevel inference in reading, like where the eyes are moving to resolve uncertainty about the letter. Order of letters is sought out to reduce uncertainty about words. Order of words in a sentence is sought out to reduce uncertainty about semantics. Semantics are operated on for pragmatic value.
3206100	3264000	87	A	0.98	53:26	90	So that's chapter seven. It's a fairly loosely connected sequence of discrete time models. Several examples, starting from the musical example, which is just a passive inference hidden Markov model bringing us back to the discrete time generative model that we saw in Figure 4.3 that's shown again in 7.3 and then several decompositions. Of expected free energy focusing on the Epistemic value, especially in terms of how getting Epistemic and Pragmatic value unified under this G functional allows us to approach the adaptive finessing of exploratory and exploitative behavior.
3268590	3273470	88	A	0.6	54:28	12	And again, the work is in understanding and designing the generative models.
3276670	3301840	89	A	0.99955	54:36	54	That's not work about work. That's really the task itself is the systems modeling and systems understanding, because essentially, by saying what these variables are, what shape these variables are, you will have prepared your model to run. And until these models are defined in terms of their dimensionality, it's not ready to run.
3304840	3314430	90	A	0.99994	55:04	20	Any kind of closing thoughts, Michael? It looks like Martin might have had to do the math class after all.
3318960	3321550	91	A	0.86339	55:18	8	There's so many. I mean. As will I.
3323700	3339840	92	A	0.68569	55:23	35	There'd be so many. And and again, like, let's try to let's explore how the let's see how the the AI explanations did for chapter seven math explanations. Why would a high schooler be interested?
3343900	3346810	93	A	0.97416	55:43	10	Let's look at it for one of the epistemic ones.
3351420	3391700	94	A	0.99998	55:51	63	People can evaluate and add comments or change the text, but for those who are learning, and ultimately we all are, I doubt there are few, if any, people for whom this equation doesn't require contextualization and clarification and deeper breathing. Yeah, these lines are not it's not like, divide by two and then do this. It's like these are multi step derivations.
3393980	3403050	95	A	0.98	56:33	25	And so going back to the accuracy minus compression, that's the challenge of writing a textbook accuracy minus compression of what they had to share.
3406640	3422800	96	A	0.54	56:46	35	So our work is to kind of refine that balance for ourselves. Some of these look helpful. Some of them don't look helpful. In terms of the AI answers, I. Would use the word scary.
3429310	3432810	97	C	1.0	57:09	14	I got to run, by the way. Great. Thank you. Yeah. Any other comments?
3437470	3449650	98	A	0.92	57:17	29	All right, great. Well, next time, second discussion on chapter seven. So we'll return and look at what questions have been upvoted or asked. So thank you all. Farewell.
3450470	3453680	99	A	0.81407	57:30	3	Bye. Thank you.

start	end	sentNum	speaker	confidence	text
3000	4070	2	A	0.98682	Hello everyone.
4760	11444	3	A	0.62501	We're in our first discussion of chapter seven in cohort three.
11642	20790	4	A	1	So let's head over to chapter seven and we will come to the questions.
21960	37550	5	A	0.99866	But let's just start with anyone who wants to do you want to just give a general comment or any reflection or thought on seven or anything up to this point?
43020	45610	6	A	0.72443	Ali and then anyone else?
49020	49432	7	B	0.92	Yes.
49486	62140	8	B	0.96	So chapter seven is basically the application of the first half of chapter four in terms of modeling discrete time situations.
62800	96330	9	B	1	And it goes through a number of case studies in order to show how the matrices A-B-C and D can be constructed in various situations and the challenges or rather the concerns we may have when constructing such matrices in order to have a viable discrete time model for each situation.
97180	104110	10	B	0.56	So, yeah, that's basically the premise of chapter seven.
106560	108220	11	A	0.99895	Awesome, thank you.
108370	109644	12	A	0.78202	Totally agree.
109842	114776	13	A	0.99995	Just like chapters two and three with low road and the high road, we're kind of like a pair.
114968	121740	14	A	0.98613	Chapters seven and eight are kind of like a pair because seven is going to go into discrete time modeling.
121900	125600	15	A	0.99996	Chapter eight is on continuous time and hybrid modeling.
126180	132710	16	A	0.99964	Does anyone else want to make any just overall reflection or comment on chapter seven?
154400	161560	17	A	0.82	All right, I'm checking if there's any okay.
162430	164234	18	A	0.96503	Does anyone have any question at all?
164272	174694	19	A	0.7989	Otherwise we'll turn first to these questions, start with the written questions we have and then just approach it from there.
174832	178560	20	A	0.99936	Does anyone have anything else that they want to consider first though?
185940	203684	21	A	0.98857	Okay, I'm putting in the chat, that's the social sciences link, but here's the chapter seven link in the chat and so you can follow along there.
203882	214810	22	A	0.99996	Also please upvote the questions that are interesting to you because the questions that have a bunch of upvoted points we're going to make the short videos for.
217420	229310	23	A	0.99449	Okay, so we'll just go to the I'm going to unvote all the questions from my own you can still vote for them, but I'm just going to unvote them so that once we get to them, I'll vote for them.
231960	232710	24	A	0.99534	Okay.
234540	241530	25	A	0.74	All right, first question here's, chapter seven.
241900	244180	26	A	0.72537	Chapter seven begins with a quotation.
244260	246184	27	A	0.99994	What I cannot create, I do not understand.
246302	250696	28	A	0.9953	Richard Feigman, what do you think this means in the context of active inference?
250728	278300	29	A	0.66918	Specifically, I can't remember exactly who added this in a previous cohort, but they gave a mild and a stronger answer.
278990	282586	30	A	1	The mild answer is you have to learn by doing.
282688	288090	31	A	1	You actually have to build the generative model to understand active inference.
290430	293662	32	A	1	And then the stronger answer goes even beyond that.
293796	305780	33	A	1	And it's not just that you have to build the generative model, you actually have to have something that escapes your hands and does something interesting to really test the method and understand it.
308230	311700	34	A	0.97735	Anyone else want to add a comment on this?
317750	323302	35	A	0.95684	Okay, kind of a warm up question, but anyone else want to add anything on this?
323356	325480	36	A	0.78376	What I cannot create, I do not understand.
326330	330380	37	A	0.90123	Otherwise we'll come into the more specifics of the chapter seven now.
336350	343980	38	A	0.9883	Okay, so in that case, I'm just going to go with the most upvoted questions.
348050	348800	39	A	0.98009	Okay.
353340	353960	40	A	0.95	All right.
354030	356596	41	A	0.99982	This is on page 130, the textbook they wrote.
356708	368968	42	A	0.99997	This choice, which we're going to look to the textbook in a second, speaks to the classical exploration exploitation dilemma and psychology, a dilemma that is resolved under active inference.
369144	370750	43	A	0.99997	What do you think about this?
371280	386124	44	A	0.99709	Okay, so the setting that's being considered is decision making under uncertain payoff or reward structure.
386252	391876	45	A	1	So you think there's two slot machines, one of them pays out 50% of the time.
392058	394470	46	A	1	You don't know how much the other one pays out.
394840	406664	47	A	0.62	So you could continue to get 50% that would be exploit, or you could explore, and it might be better than 50%, it might be worse than 50%.
406782	411900	48	A	0.57	And so that is sometimes called the exploration exploitation dilemma.
412480	420140	49	A	0.99995	Does anyone want to give a comment or a related question on how is active inference going to be approaching Explore exploit?
428990	442730	50	C	0.71126	Michael yeah, I just wanted to both affirm that I found the mathematical representation of these two ideas, that it's a trade off very interesting.
442900	457060	51	C	0.9992	But to describe them as resolved is something that I find might be a little more overdescribed, if you will.
460410	470294	52	C	0.99892	Maybe the word is this formula describes the trade off or as a way of representing it, but resolution is a much more open topic.
470342	476780	53	C	0.99	So just thought I would give a response, give feedback and say, well.
478590	479962	54	A	0.80898	Yeah, that's it.
480016	488240	55	A	1	Yes, maybe resolve in the sense of like a high resolution image, like we can see it.
488690	496946	56	A	0.95846	We've resolved the dilemma by perceiving it not we have explained it away or said that there is no trade off.
497128	505570	57	A	1	I believe this was Eric Sounds previously, who kind of contested the idea that Explore exploit is a dilemma.
507290	508898	58	A	0.92642	It's not a dilemma.
508994	511474	59	A	0.54953	There's not two options that are intrinsically opposed.
511522	514790	60	A	0.99961	There's just one integrated behavioral choice.
515850	525450	61	A	1	All of this being said, let's look at how Explore exploit our approach differently in active inference.
526910	555394	62	C	1	One more remark might be that in a different sense making context where the explore versus exploit issue arose continuously, the word sharpening visibility was a way of describing the space that sometimes just your resolution comment helps remind me of that.
555432	566950	63	C	0.53	That it was like boosting the visibility of the space does not resolve the problems that you're trying to address, but it does improve your ability to act on the space.
567020	574406	64	C	0.52	And I guess that's maybe a suggestion for language boosting visibility versus resolving that's.
574438	578074	65	A	0.95	All yes.
578192	578860	66	A	0.77535	Awesome.
581230	581930	67	A	0.64753	Yeah.
582080	594206	68	A	1	Just to briefly catch us up to how we get to this claim, chapter seven, like Ollie mentioned, is going to be concerned with a series of examples of building discrete time generative models.
594238	597940	69	A	1	So these are models where time clicks like one step at a time.
600470	608710	70	A	0.99989	Figure 7.1 is showing just the perceptual or the downstairs component of a generative model.
608780	614486	71	A	0.8	So D is the prior, S are the hidden states, b are how the hidden states change through time.
614668	618646	72	A	0.6	A is the matrix that maps hidden states to observations.
618838	623206	73	A	1	So this is just the temperature in the room changing through time outputting.
623238	625498	74	A	1	A thermometer reading at every time point.
625664	628300	75	A	0.92773	This is hidden Markov model.
628990	633118	76	A	0.99947	Not a Markov decision process, just a hidden Markov model.
633204	634030	77	A	0.99261	Hmm.
636610	644670	78	A	0.99999	They then write the A and the B matrix as well as the D for a musical setting.
644830	646350	79	A	1	So there's four notes.
646510	657618	80	A	1	The A matrix is basically saying you hear the correct note 70% of the time, and then the B is the transition matrix.
657794	664760	81	A	1	So this is just playing four notes that transition to each other 97% of the time in this order.
666170	668730	82	A	0.71	And it always starts on the first note.
669470	678154	83	A	0.74	So with just those three matrices defined A, B, and D, you have fully defined the generative model for this.
678192	678998	84	A	0.99891	Hmm.
679174	683134	85	A	0.99998	There's no decision making yet, but that's all you need to define this.
683172	683950	86	A	0.62809	Hmm.
686770	701300	87	A	1	And then in Figure 7.2, there's an example where actually there turns out in this simulation that the third observation, it should be here, but it's actually here.
702310	708360	88	A	1	So that's what's heard because there's such a prior that that's what should be heard.
710570	718090	89	A	0.99591	Okay, now we get into action, which is where the explore and exploit are going to come back into play.
718160	736030	90	A	1	So if it's just a purely passive inference, then the kind of dilemma or unavoidable trade off is like over and under fitting perceptually, making something that's over compressed or under compressed with your beliefs.
736610	749666	91	A	0.99996	But action has a different set of challenges because it's related to selection of causal intervention in the world and it changes sequences of future observations and all of this.
749848	765510	92	A	0.99	And as we know from the expected free energy, the imperative for action is to have both epistemic value, like to gain information, and to have pragmatic value, to have alignment between your observations, your preferences.
767130	769100	93	A	0.99659	Here's figure 7.3.
769470	774780	94	A	0.99996	This is kind of the classic generative model in discrete time.
775150	779180	95	A	1	So we see that same downstairs component, the exact same.
779950	800100	96	A	0.99999	Plus there's the upstairs decision making component, pi policy intervening into how states change their time, expected free energy g updating policy decisions, updating them based upon their expected free energy.
809420	809876	97	A	0.99905	Michael?
809908	810408	98	C	0.76082	Yeah, sorry.
810494	811530	99	A	0.55301	Yeah, please.
811980	824616	100	C	0.99849	I'm jumping back 60 seconds, but right before you described that diagram, you said how you compress the I can't remember, but you use the word compression.
824728	830476	101	C	1	And I wondered, why is he using compression as the verb to describe what's happening?
830578	848724	102	C	1	And are you suggesting that in the same way that let's say when you compress data onto to save more storage space, you do an abstraction, reduce it, and it's a compression of what the data is.
848762	850772	103	C	0.87	That is what's happening.
850826	854196	104	C	0.86493	There is a I'm sorry to be wordsmithing here.
854218	859290	105	C	0.99699	I'm just trying to make sure I'm tracking what you were saying when you said okay.
860380	862490	106	A	1	It is very closely related to this.
864380	866344	107	A	0.86051	There's two ways to think about it.
866382	873032	108	A	1	One is like we're, we're fitting some gaussian, let's just say some bell curve over some uncertainty.
873096	878968	109	A	1	So we could think about like, do we want over compressing it in terms of tightening the variance?
879144	884032	110	A	1	That would be like overfitting versus like if it's loose, then it's like under.
884166	887184	111	A	1	So that's a little bit like a loose sense of compression, really.
887222	888988	112	A	0.82963	It's just a variance estimation.
889164	897700	113	A	1	Or you can think about complexity minus accuracy as being like optimal zip compression.
898360	907990	114	A	0.98087	Like obviously if you wanted a lossless compression you wanted perfect accuracy, then you're opening the door to an infinitely complex model.
909100	926270	115	A	0.99999	On the other hand, if you're willing to find a trade off between complexity and accuracy, which is summarized by variational free energy in the moment, then you would end up with at the point of diminishing returns, like the kind of the Pareto optimal compression point.
930800	931740	116	C	0.89112	Thank you.
931810	933984	117	A	0.95466	Sorry to no, it's all good.
934102	950790	118	A	1	So now to study this action setting, which is where explore exploit comes into play, we're going to switch from the music listening example, which was just kind of familiarizing us with the A, B and the D.
951160	961320	119	A	1	And now we're going to bring in action, which is going to require talking about Pi policy and C our preferences.
963100	967160	120	A	0.99252	Okay, so it's a T maze task.
968540	980990	121	A	0.98353	There's an aversive stimulus in one arm, an attractive stimulus in the other arm, and a Q that indicates the location of the two stimuli in the bottom of the T.
981920	994012	122	A	1	So the organism can either pick one of the arms left or right, maybe quickly get to the reward, but maybe get the aversive stimulus.
994076	1008868	123	A	1	Or it could choose to seek out this informative cue and then having reduced its uncertainty about the location of the reward, then just walk over to the reward.
1009044	1022860	124	A	0.98	So that's going to be one, two, three steps instead of just going straight to one of the arms, but you then have higher accuracy with getting to one of the arms.
1024320	1039664	125	A	1	So how do you get flexible and rapid and adaptive switching between pragmatic oriented behavior and epistemic oriented behavior especially?
1039782	1048550	126	A	1	How can that happen as new information are rolling in without necessarily retraining the model?
1049880	1054230	127	A	1	That is what is going to be explored here.
1056600	1064970	128	A	0.99988	Anyone else want to just add a comment or thought or how they see it at this point?
1068700	1093970	129	D	0.78467	Yeah, have a general question in terms of if you do an experiment with different animals, for example, or different observers, and you want to model different balance between this epistemic and pragmatic behavior, where in the ABCD matrices would that be?
1095160	1098150	130	A	0.99731	Yeah, very good question.
1098680	1104630	131	A	1	One place that this plays out is in the amplitude of C.
1105400	1125080	132	A	0.92	So let's just say that left and right are the two directions on the teammates and we don't have a habit for either one of them if we prefer the sugar over the aversive.
1125160	1147140	133	A	0.75	So now we're going to talk about the C matrix, if it's one and zero so aversive we don't really care about, then we give a value of one for the reward versus if the C was a million and negative a million, then the pragmatic value term would basically just be like scaled larger.
1147800	1160010	134	A	1	So the amplitude on C is commonly used to weight the Pragmatic value relative to the epistemic value.
1160780	1175020	135	A	0.99999	Because just framing things like this does not guarantee that your parameterization is going to be at this adaptive point that actually does flexibly trade off between epistemic and Pragmatic behavior.
1176080	1188848	136	A	0.98445	Because if C is very small and washed out so utility seeking has shapes behavior a little bit, then you're going to end up with a mainly epistemic agent.
1189014	1211640	137	A	1	And if C is vast relative to the epistemic value, then you're going to end up with mainly reward seeking, just like we see in the in figure Two six, where if we totally discard the pragmatic term we end up with purely epistemic behavior.
1212220	1217880	138	A	1	And if we totally discard the epistemic term we end up with purely pragmatic behavior.
1219100	1228860	139	A	0.99999	But also if this term is just really small, you can imagine it doesn't play that large of a role and vice versa.
1230240	1264890	140	A	1	So in an empirical setting, you have to parameterize the value, for example, of C, so that you do get goal reward oriented behavior, but not just like one track mind, um, Ali and then anyone else.
1268620	1299460	141	B	0.73353	Yeah, well, actually this C parameter C matrix here in the formulation of expected free energy is a relatively recent addition to active inference formulation because in some earlier literature or even some recent literature, this parameter c is not explicitly expressed in the formulation, they just express it as P of observation.
1299800	1323884	142	B	0.99977	Because you see, this is a crucial step actually in active inference because if we want to derive this expected free energy as a parallel to variation of free energy, then here instead of C, we should have only Pi, right?
1324002	1335964	143	B	0.99999	But replacing Pi with C denotes the kind of inaccuracy in terms of our policy selection.
1336092	1359960	144	B	0.99998	In other words, it doesn't necessarily entail, I mean, undertaking the policy we've selected, we have a preference matrix for undertaking some actions, but it doesn't necessarily imply that we only take those specific actions per se.
1360110	1375484	145	B	0.97	So we somehow approximate our preferences without restricting ourselves to the actual policies that's been undertaken in the situation.
1375682	1392800	146	B	1	So that allows us to even formulate or at least describe some counterfactual examples as well as well as the actual behavior we observe for the agent.
1392950	1420170	147	B	1	So that's, I think, one of the crucial steps in terms of allowing for a broader behavior of the agents and somehow broadening the possibility spaces of the agents, not just the actualized trajectories of the internal states versus the external states.
1424500	1447520	148	A	0.54393	Yeah, it allows us to articulate what is the action possibility space and separate that from the outcome preference so we don't have our finger on the scale directly, can't control observations directly.
1447600	1451404	149	A	0.54794	That's the kind of perceptual control theory insight.
1451552	1456760	150	A	0.81	You want your preferences to be about observations, but you don't want to be able to directly control observations.
1457100	1464030	151	A	0.85	So what are the two channels that we have to reduce expected free energy, change our mind, change the world?
1465360	1467276	152	A	0.75787	How much to balance those two?
1467378	1469310	153	A	1	That was Olivier's question.
1469680	1473152	154	A	1	And that is the question is how do you balance those two?
1473206	1485368	155	A	0.99995	But those are the two things that you want to have from a first principles being balanced off in terms of your imperative for action selection.
1485484	1493760	156	A	1	You want your action selection to be an epistemically informative path and a pragmatically useful path.
1493920	1505720	157	A	0.99642	Epistemic value, expected information gain under a policy pragmatic value how well it aligns with your preferences expectations.
1511120	1517656	158	A	1	So this is how exploitative and explorative behavior are again going to be resolved.
1517688	1521020	159	A	0.9999	Not to say addressed away, but how it's approached.
1521600	1529116	160	A	1	If we were in a reward setting, then exploitative behavior needs no secondary explanation.
1529228	1531596	161	A	0.65078	It's like there's money on the ground.
1531708	1534892	162	A	1	If it's about getting money, then just pick it up.
1535046	1541220	163	A	1	So exploitative behavior under reward centric paradigm does not need a secondary explanation.
1541800	1553524	164	A	0.99764	Exploratory behavior on the other hand, eventually needs to navigate into the currency of utilitarian value in a reward centric path.
1553652	1563160	165	A	1	And so an exploratory path might be highly valued because for example, there might be a reward.
1563320	1572452	166	A	0.97	So if there's a 10% chance of a $1,000, then that policy would have that expected reward.
1572616	1583360	167	A	1	And so that might be more preferable than for sure one dollars or 10% of $1,000 in the reward centric approach.
1583780	1587616	168	A	1	So then that's how you get explore and exploit balancing.
1587808	1602730	169	A	1	You have exploratory trajectories being evaluated in terms of their expected returns in reward and then you can have everything compared on the common currency of expected pragmatic value in reward learning.
1603500	1607000	170	A	0.99798	Okay, so how are we going to approach it differently in active inference?
1607580	1614860	171	A	0.99995	Different trajectories of action are going to be compared again in a common currency.
1615280	1623730	172	A	0.99994	But rather than that common currency being expected reward, the common currency is going to be expected free energy.
1624420	1633410	173	A	0.99814	Expected free energy is going to have two components expected information gain and expected pragmatic value.
1635160	1643670	174	A	0.98365	As stated before, balancing those two terms into an adaptive zone is a fine tuning question.
1644040	1654520	175	A	0.99994	But this is how we approach the question in a first principles way by making a unified imperative that one includes perception and action.
1655260	1658500	176	A	0.9997	That's the integration of perception action loop.
1658580	1660904	177	A	0.99999	We see like observations and we see actions.
1660952	1667230	178	A	0.99372	They're just all there in the same equation and it unifies epistemic and pragmatic value.
1667920	1687780	179	A	1	Instead of coercing epistemic actions into their expected reward and then comparing everything on the reward meter stick, we just have a single unified imperative, expected free energy that contains an epistemic and a pragmatic loading.
1689320	1690340	180	A	0.9316	Michael.
1692600	1693252	181	C	0.9928	Great stuff.
1693306	1694230	182	C	0.70204	That's all.
1696280	1707560	183	A	0.73784	Yeah, equation 2.6 gets revisited again and again and just different ways of seeing expected free energy so now we're going to continue seeing the teammates.
1708460	1713556	184	A	1	So here the matrices are shown for this teammate.
1713588	1719420	185	A	0.81	So, yeah, there's the mouse aversive and beneficial outcome.
1720480	1722864	186	A	0.64006	Olivier yeah, sorry.
1722902	1731410	187	D	0.61	No, it was just a bit like it's behind on the last equation 7.4 for you.
1731780	1736480	188	D	0.51	I find your last explanation really great and very enlightening.
1737460	1746050	189	D	0.9	And in a basic context of reinforcement learning, you will have somehow the first term within the second, is that what you're saying?
1749600	1758332	190	A	0.36	It wouldn't be nested within the second strictly, but yes.
1758386	1768288	191	A	0.99996	Basically policies, one slot machine is giving you the 50% reward and then you don't know the other one.
1768454	1775344	192	A	0.99996	But a policy might be entertained and even selected because it has a high expected value.
1775542	1795860	193	A	1	That is expected value based decision making because you say, well, I think slot machine, this slot machine might have a high value, so I'm going to take an exploratory action to kind of explore that, but not because of the information gain it gives me, but because of the expected reward.
1796020	1804220	194	A	1	So it's kind of like converting everything to a pragmatic currency and then comparing the expected utility or the expected reward.
1804800	1814930	195	A	0.98	And so you get utility discounting, you get time preference, all this stuff, but those are all correction factors within pragmatic value.
1815700	1816160	196	D	0.8464	Yeah.
1816230	1816864	197	D	0.99753	Okay, yeah.
1816902	1817216	198	D	0.99965	Thank you.
1817238	1818560	199	D	0.99832	Sorry for the interruptions.
1819700	1828260	200	A	0.99978	This is one of the key topics and this is how policies are selected.
1829240	1847112	201	A	0.99	And that is the empirical challenge, is being able to parameterize everything so that it doesn't just behave trivially like a novelty seeking agent over here, or like a utility seeking agent over here.
1847246	1862216	202	A	0.99999	We want to have some training in the middle here's where the matrices are shown for these two settings.
1862248	1866450	203	A	0.67462	Here the block is on the right and here the white is on the right.
1867620	1870576	204	A	1	And it just shows what the matrices look like.
1870758	1880868	205	A	0.97	So it's helpful to see what the actual matrices look like in the generative model, because remember, this is the generative model.
1881034	1890516	206	A	1	So if you define A, B and D, just like we saw in the music example, they're just specific matrices of whatever dimensionality it is that you're studying.
1890548	1903070	207	A	1	If there was four notes, then D had four in a vector, a was a four by four, b was a four x four matrix in the music example.
1903520	1911544	208	A	1	Now when you add in policy, policy has a dimensionality of however many policies you can take.
1911682	1925760	209	A	0.92	So if there's four options policy has, there's four options in the pi vector and then there's going to be a slice of B for each of the dimensionality of pi.
1926840	1933750	210	A	0.59	So if there were four options for what you could do, then there would be four slices in B.
1934120	1946970	211	A	1	And it's kind of like, okay, if you choose action three, then B three third slice in b is going to be the one that we use to update S.
1948140	1955180	212	A	1	So once you define this very limited, very interpretable set, of parameters.
1956160	1966928	213	A	1	You will have stated the generative model and then you're ready to engage with standard methods for training and updating this generative model.
1967094	1991640	214	A	0.37	And so that's why in active inference we spend a lot of time think about chapter six setting up our understanding of the problem because once you can describe the dimensionality and the parameters of the generative model that was the work, then you just update this as a statistical model using standard methods.
1993100	2017116	215	A	0.68	So there's a lot of work in understanding the problem and in framing it and understanding what is the system of interest and all these other things, what form of degenerative model is appropriate but then you don't need to engage hopefully but actually in secondary engineering concerns for reasonable projects.
2017148	2020816	216	A	0.99976	I'm sure at some very large scale these things do come back into into play.
2020998	2024392	217	A	0.99999	But that's kind of the cool thing about active inference.
2024476	2035990	218	A	0.99845	We understand what all these variables are and then when we want to model our own setting we just specify the generative model and kind of hit play.
2040250	2042310	219	A	0.65	More matrices being shown.
2044170	2052014	220	A	0.99837	Here's the C vector and so plus six for the attractive stimulus and negative six for the aversive stimulus.
2052162	2068570	221	A	1	Now if this were zero zero six and negative six so the amplitude of C were just smaller so the relative preferences would stay the same but the pragmatic term would just be smaller.
2068730	2071466	222	A	1	So then the agent would behave more epistemically.
2071578	2079250	223	A	0.93331	Whereas if this were 6 trillion and negative 6 trillion then the pragmatic term would come to dominate.
2088540	2093000	224	A	0.67	So again, what is the approach of the resolution?
2094160	2109900	225	A	0.99133	It's putting different trajectories of behavior, action selection on a common footing, expected free energy which unifies the epistemic and the pragmatic value of different actions.
2110060	2113504	226	A	0.99997	We don't say epistemic and pragmatic value are the same thing.
2113702	2116960	227	A	1	In fact they're different but they're complementary.
2117400	2136090	228	A	1	And so having a unified imperative helps us adaptively approach this question of the actual trade offs between courses of action that have known consequence and those that are providing new information.
2144280	2145492	229	A	0.90641	Anyone want to have a thought?
2145546	2146790	230	A	0.99531	Yeah, go for it.
2148440	2151796	231	C	0.92024	I'm a babe out of water in so many levels.
2151828	2168540	232	C	1	But in my original field of training in economics we had this phrase we used often called Satiris paribus which was a way of kind of saying that's an extradinality to our models and we're just going to keep going with the way we do things, which is close enough.
2168690	2185004	233	C	1	And the consequences of this kind of poor treatment of dilemmas was that things like pollution and other externalities were they might not fit in the representational model and therefore were kind of underweighted and trivialized.
2185052	2209972	234	C	1	And what I'm hearing you make a case for, in a different discipline, in a different way is changing the math to handle what is otherwise trivialized as external, too inconsequential to be attended to, is embracing the tension and thus making it a part of what then can be the sense making model of the discipline overall.
2210116	2215390	235	C	1	Which seems something we can learn from you all.
2218640	2222964	236	A	0.98526	Building not even one layer and connecting that to kind of good hearts.
2223032	2226080	237	A	0.83002	Paradox like a metric becomes Gamified.
2226580	2237460	238	A	1	Do we care about the levels of lead in the measurement or do we care about the hidden unobserved levels of lead in the soil?
2237960	2247780	239	A	0.99	And so in our systems model that hopefully everyone will be able to come to the table on, we could say, well really there's both.
2247930	2249140	240	A	0.65224	They're just different things.
2249210	2259320	241	A	0.61423	There's the true underlying distribution which we're never going to measure at every little grain of sand and then there's the sensor fusion that we have to do with observations.
2259980	2285680	242	A	1	So let's not try to game the observations, but rather come to the table with a more holistic understanding of the relationship between hidden states and observations and how hidden states are changed and how our actions influence the hidden state distribution, but how that emits observations, just articulate the problem, break it down into the components.
2286120	2313630	243	A	0.69	And then the amazing thing is these natural components of the situation again prior beliefs about hidden state underlying hidden states that are unobserved emission of observations, how the world changes through time, actions that influence how the world changes through time, our strategy of deciding on action, those are our naturally separable aspects of thinking about a situation.
2314400	2325650	244	A	0.94	And in a way it's kind of amazing that all you have to do is just like smash the matrices together and the math does kind of work out again in a way that we would want it to.
2329300	2340100	245	A	0.99629	Yeah, it's Chill 7.4 is going to go into a little bit more detail on this epistemic value of policies.
2341000	2344180	246	A	1	So there's a few different breakdowns here.
2344250	2357210	247	A	0.9208	They're all equal but this is a few different representations of just how we can think about the informational gain associated with a given policy.
2358300	2372380	248	A	0.69	So, whereas the pragmatic value for a policy is fairly straightforwardly identified with how close the expected observations are to the preferences.
2373600	2381836	249	A	1	So the most pragmatically valuable trajectory is the one where the body temperature is right in the middle of the temperature distribution.
2382028	2384690	250	A	1	The least pragmatic one would be the further away.
2385060	2387904	251	A	1	But here's where we see this I of Pi.
2388032	2391824	252	A	0.59	So there's a few more descriptions on I of Pi.
2391952	2422076	253	A	0.51	And so it turns out in this case with this teamase, initially the action that the mouse selects is to it takes an epistemic action to reduce uncertainty because it's like, okay, there might be some pragmatic value, but also it's a risky decision up here, but going down is going to be a policy selection that gives me a lot of information.
2422258	2428464	254	A	1	Now this isn't the whole iguana as they brought up in the beginning of the book.
2428502	2436768	255	A	0.87617	Like how does the mouse come to know that the aboutness of the queue is this setting?
2436944	2453624	256	A	0.68	So this isn't like from the big bang to the mouse making the decision, the total model, there's always stuff that's structurally encoded in the model how does it mouse come to know that the aversive stimulus is negative and so on.
2453742	2466424	257	A	1	So these are levels that you can add in but this is just showing basically again in this simple example it's just showing what it looks like when the mouse undertakes an epistemic action.
2466472	2482252	258	A	1	And in the Pymdp model stream 7.2, where there's a very similar example to this it's not in a Tmaze but it is with a mouse that gets some information and then makes a decision.
2482396	2485170	259	A	0.92	So it's very similar to this.
2486520	2494390	260	A	1	You could play around in the script and you can see what happens when you change the preference vector to like a super high or super low value.
2500360	2509640	261	A	0.83875	Okay, little bit of boxed discussion on precision and entropy.
2510800	2519950	262	A	1	The h is associated with entropy but let's just continue on.
2521120	2529040	263	A	1	That just relates to the over underfitting of the expected the expected sharpness or blurriness of a distribution.
2531380	2545750	264	A	0.99998	They move on to another of the settings that's been very well repeatedly studied in active inference which is the eye cicade setting the motion of the eyes.
2546440	2561100	265	A	0.99	And so one reason why this has been studied a lot is that the fovea of the vision, the area that we have high resolution color vision is actually very, very small relative to the visual field.
2561250	2567790	266	A	1	And that's why multiple times per second the eye is making these policy selections to move around.
2568240	2583276	267	A	1	And those policy selections, although we might look at something that's beautiful or rewarding and have that fix our gaze, to a first approximation, the imperative of the eye circade is informational.
2583468	2599880	268	A	0.56239	It's gaining information about the visual field and so that is why the epistemic visual search is a really useful intangible setting.
2600300	2602120	269	A	0.99963	Again, it's not like gambling.
2605020	2615320	270	A	0.94754	You're kind of setting up a situation towards pragmatic value but rather this is one where there's going to be an emphasis on the epistemic.
2615480	2627468	271	A	1	And so this is like based upon the ambiguity of stimuli and about how epistemic attraction occurs.
2627644	2629650	272	A	0.99991	When there's information to learn.
2630020	2635570	273	A	0.99722	Where something where there's no information to learn doesn't that moving there does not have epistemic value.
2640520	2648080	274	A	0.95566	Kind of this is a little bit of a formalization of the streetlight story from every culture.
2648240	2650756	275	A	0.64127	Where did, where, you know, where are you looking for your keys?
2650788	2651764	276	A	0.99998	Under the streetlight.
2651892	2653320	277	A	0.5	Oh, did you lose them there?
2653390	2656836	278	A	0.95	No, this is just where I could look clearly.
2657028	2675810	279	A	1	So that is about looking where there's light and searching where you can not necessarily because you have a high prior that the information is there but just because it's easily resolvable under the streetlight best place to find high quality unambiguous uncertainty resolving information.
2678340	2684736	280	A	1	So the ambiguous poorly lit square is ignored reproducing in silico streetlight effect.
2684918	2705224	281	A	1	If you're not going to get resolvable information it's not of epistemic value to go there but if it is going to be high quality uncertainty reducing information that is like epistemically valuable 75 Learning and Novelty so in.
2705262	2715390	282	A	0.8373	Figure four, three in seven, where we, where we revisit it in 73.
2716320	2727200	283	A	0.99994	There's no learning in this model as written, there's updating of the hidden states by moving it through the B matrix.
2728740	2733570	284	A	0.99906	But the A, B and D themselves do not change.
2734740	2739830	285	A	1	So now they're going to talk about how learning can come into play.
2740520	2745060	286	A	0.96	So in other words, like the A matrix is fixed in this setting.
2746040	2751528	287	A	1	But you might be interested in a situation where the A matrix can change through time.
2751694	2757530	288	A	0.9919	Like maybe the mapping between hidden states and observations changes.
2759420	2775970	289	A	1	The way that learning is generally approached in Bayesian inference settings is you make a prior distribution on the variable of interest.
2776660	2785570	290	A	0.93	So for example, lowercase A is going to be a hyper prior or just a prior distribution on A.
2786600	2804250	291	A	1	So instead of just saying A is seven one one like we saw in the Music example, you can have a distribution over what A looks like that is going to help with learning.
2804780	2819420	292	A	1	In the extreme case where learning rate converges to zero, that's like a super sharp distribution around what you think A is going to be like so that you have very strong prior on A.
2819570	2823810	293	A	1	So new information is only going to move that sharp distribution a tiny amount.
2824340	2840790	294	A	0.99851	Conversely, if you were having a lot of learning on A, you'd have a very flat prior on what A could be so that new observations would update your beliefs about A a lot.
2842040	2858120	295	A	0.99981	But this is just an example of the composability of this framework in that the base model doesn't have learning attention, all these different relatively sophisticated cognitive phenomena.
2859100	2875704	296	A	1	But this is like the little firmware nugget or kernel which by understanding the compositionality relationships of then we can start to bring in learning on different parameters.
2875832	2879408	297	A	1	And of course learning is awesome and it sounds awesome, it is.
2879574	2905490	298	A	0.99999	But if you also think about it from a statistical perspective, that doesn't mean that every parameter should be learnable because sometimes you're just increasing the computational challenge of your problem vastly more information on learning.
2909800	2921080	299	A	0.99997	Here we see another difference with learning between active inference and many other machine learning approaches.
2921900	2928110	300	A	0.98	And this is the embedding of action in the inference process.
2929040	2934190	301	A	0.99999	We actively think about how much information are we going to get.
2935040	2941360	302	A	0.97922	Like when people are training an image recognition, it just here's a thousand images in the training set just plug and chug.
2941780	2948800	303	A	0.9819	But what if you had an agent that was like which image should I look in of these thousands?
2949240	2962970	304	A	0.55	Then it would become of primary importance which sequence of learning observations the agents took within those thousand labeled data points.
2964380	2977210	305	A	1	So that's what G helps us understand some new terms salience, novelty, different decompositions of G.
2981450	2990250	306	A	0.79	One final example, again, more just like referenced because it's in other papers but not really explained with the matrices.
2990670	3005840	307	A	0.99998	Here we have a maze exploration agent which is like another kind of classic setting so it has gray in the likelihood, which is like 50 50 between white and black.
3006290	3012874	308	A	0.94017	But it starts off, it takes these three steps and it's like, okay, it's updating, now it moves.
3012922	3014306	309	A	1	Here updates that.
3014328	3018340	310	A	0.9978	Okay, that's a white square, that's a white square, that's a block square, that's a block square.
3018950	3029970	311	A	0.77	So this is like somebody looking over a maze visually seeking a path to solve the maze.
3034250	3056400	312	A	0.98	7.3 is a little bit of a summary of a pretty advanced topic which is Bayesian model reduction and structure learning which is treating the question of what structure should the generative model have?
3056930	3062000	313	A	0.99935	Treating that question as a parametric inference problem.
3063190	3080920	314	A	0.7	So should the model be one, two or three time steps deep and then say, okay, well we're going to have some time horizon parameter and then we're going to test whether one, two or three is the better model.
3081610	3105630	315	A	1	So like we're going to learn the structure of a model as part of reducing down from a portfolio of models but they just sort of briefly note it and there's more citations and then as and then any other just thoughts or questions.
3105700	3110190	316	A	0.75	Seven kind of has a lot of little short vignettes.
3116340	3125750	317	A	0.99415	Okay, just briefly, just so that we touch it all, then next time we can have voted and added a lot of questions and everything.
3126120	3128852	318	A	0.88	712 is a two level model.
3128906	3131690	319	A	0.99929	It's a nested or a hierarchical model.
3132540	3138200	320	A	0.99999	Both the higher and the lower level, one and two are discrete time.
3138350	3142828	321	A	1	You can tell because they all have the s t minus one t t plus one.
3142994	3151150	322	A	1	So later we'll see a hierarchical model where the lower level is continuous time and the top level is discrete time.
3151680	3156210	323	A	0.99999	But in this setting both nested levels are discrete time.
3156740	3163890	324	A	0.99	So this could be every hour and this is 60 minutes within an hour, every day, 24 hours within a day.
3167380	3188916	325	A	0.82543	Here's a multi scale updating generative model that's related to sentence reading and they've done a lot of simulation studies of multilevel inference in reading, like where the eyes are moving to resolve uncertainty about the letter.
3189108	3193884	326	A	0.99969	Order of letters is sought out to reduce uncertainty about words.
3194082	3199100	327	A	0.9755	Order of words in a sentence is sought out to reduce uncertainty about semantics.
3199520	3203070	328	A	0.99884	Semantics are operated on for pragmatic value.
3206100	3207984	329	A	0.98	So that's chapter seven.
3208182	3219744	330	A	0.31777	It's a fairly loosely connected sequence of discrete time models.
3219872	3239732	331	A	0.99603	Several examples, starting from the musical example, which is just a passive inference hidden Markov model bringing us back to the discrete time generative model that we saw in Figure 4.3 that's shown again in 7.3 and then several decompositions.
3239796	3264000	332	A	0.99	Of expected free energy focusing on the Epistemic value, especially in terms of how getting Epistemic and Pragmatic value unified under this G functional allows us to approach the adaptive finessing of exploratory and exploitative behavior.
3268590	3273470	333	A	0.6	And again, the work is in understanding and designing the generative models.
3276670	3278330	334	A	0.99955	That's not work about work.
3278400	3295600	335	A	0.99979	That's really the task itself is the systems modeling and systems understanding, because essentially, by saying what these variables are, what shape these variables are, you will have prepared your model to run.
3296370	3301840	336	A	0.98	And until these models are defined in terms of their dimensionality, it's not ready to run.
3304840	3309688	337	A	0.99994	Any kind of closing thoughts, Michael?
3309744	3314430	338	C	0.46	It looks like Martin might have had to do the math class after all.
3318960	3319900	339	A	0.86339	There's so many.
3319970	3320380	340	A	0.5	I mean.
3320450	3321550	341	C	0.98378	As will I.
3323700	3324784	342	A	0.68569	There'd be so many.
3324822	3338090	343	A	0.77	And and again, like, let's try to let's explore how the let's see how the the AI explanations did for chapter seven math explanations.
3338170	3339840	344	A	0.99999	Why would a high schooler be interested?
3343900	3346810	345	A	0.97416	Let's look at it for one of the epistemic ones.
3351420	3379708	346	A	0.99998	People can evaluate and add comments or change the text, but for those who are learning, and ultimately we all are, I doubt there are few, if any, people for whom this equation doesn't require contextualization and clarification and deeper breathing.
3379884	3388644	347	A	0.99966	Yeah, these lines are not it's not like, divide by two and then do this.
3388682	3391700	348	A	0.81729	It's like these are multi step derivations.
3393980	3403050	349	A	0.98	And so going back to the accuracy minus compression, that's the challenge of writing a textbook accuracy minus compression of what they had to share.
3406640	3413180	350	A	0.54	So our work is to kind of refine that balance for ourselves.
3414160	3415464	351	A	0.99999	Some of these look helpful.
3415512	3417020	352	A	1	Some of them don't look helpful.
3418260	3421104	353	A	0.99994	In terms of the AI answers, I.
3421142	3422800	354	C	0.99807	Would use the word scary.
3429310	3430506	355	C	1	I got to run, by the way.
3430528	3430762	356	A	0.93467	Great.
3430816	3431290	357	A	0.9916	Thank you.
3431360	3431546	358	A	0.54334	Yeah.
3431568	3432810	359	A	0.99994	Any other comments?
3437470	3438202	360	A	0.92	All right, great.
3438256	3442442	361	A	0.99964	Well, next time, second discussion on chapter seven.
3442496	3446914	362	A	1	So we'll return and look at what questions have been upvoted or asked.
3447032	3448658	363	A	0.99	So thank you all.
3448824	3449650	364	A	0.89744	Farewell.
3450470	3451330	365	A	0.81407	Bye.
3453190	3453680	366	D	0.99681	Thank you.

start	end	paragNum	speaker	confidence	startTime	wordCount	text
800	12230	1	A	0.50338	00:00	17	You. All right. Greetings, everyone. It's June 20, 2023. We're in our second discussion on chapter six.
12760	39730	2	A	0.70092	00:12	60	So we'll first just have any general comments or anything. Then we'll turn to the questions table and look at what questions we didn't get to last time and just kind of revisit, maybe condense some questions or just see what we can do with what is here. So anyone just want to make any general comments on chapter six?
48750	56940	3	B	0.68	00:48	20	I believe this is the second session on chapter six, right? Correct. Or maybe I lost. Okay. Yeah, it is.
58690	67710	4	A	0.78848	00:58	21	Yeah. There was only one onboarding needed for the welcome back here, so we started one week faster into the rhythm.
69910	84440	5	A	0.86957	01:09	32	Okay. So it looks like these were the questions I remember we left off with this one. Let's just develop this into what was into a question, and then we'll continue on.
97370	98120	6	A	0.80974	01:37	1	Okay.
101130	121500	7	A	1.0	01:41	25	Is there a common or good representation or rubric for evaluating Generative Model Generative process, which is change it to Generative Models and Generative Processes?
131210	135000	8	A	0.99992	02:11	13	Anyone want to give a first thought or just some other related question?
146310	148600	9	B	0.99858	02:26	7	Go on, please. Sorry. No, it's okay.
153560	218490	10	B	0.93383	02:33	103	Yeah, I'm not sure about what's the exact criteria for evaluating generative models or generative processes, but one thing is for sure that, well, I mean, one of the main at least when evaluating generative model one of our main. Criteria should be how closely it tracks our situation of interest and specifically how relevant it is for addressing the question we're trying to examine. I don't think it's something clear cut or, I don't know, written in stone. And based on the context and the situation, the Evaluation Criteria rubric should be different, I suppose, both for generative Model and generative process.
224290	278766	11	A	0.89955	03:44	100	Yeah. So again, we'll revisit this Generative Model Generative process question later, following the recent Livestream, but generative process working with generative process as the underlying process that gives rise to the observations. This would seem to be more adequate to the extent that it can better describe the measurements. Generative Model, similarly, is being evaluated based upon its ability to fit to the generative process as well as phenomena of interest. So not just to fit visual data, but maybe to model something like some illusion in visual, then there's a wide range of more general statistical modeling techniques.
278798	302182	12	A	0.99907	04:38	40	So first is like the ultimate grab bag, which is just how relevant is the overall modeling. Then there are statistical evaluations of model adequacy and model selection. Ike information criterion. Bayesian information criterion, base factor. Hierarchical likelihood ratio test.
302316	329410	13	A	0.99999	05:02	45	If you have a parametric Model Bootstrap and non parametric statistics. Just general modeling, and then taking that more out of the statistics into the Engineering Model Lifecycle, then that's where you can think about validation, verification, validity, quality, et cetera. Systems engineering model lifecycle.
334980	337170	14	A	0.99999	05:34	7	Any other thoughts or questions on this?
343010	343760	15	A	0.76508	05:43	1	Okay.
346450	409700	16	C	0.99065	05:46	116	I'm coming from kind of an early career data scientist background here, but as far as what I find attractive about active inference and doing things as generative models is the attempt to describe or explain what's going on in the data. And that being said, in terms of a lot of many applications of data science and machine learning, the emphasis is upon prediction. And so are these listed statistical metrics you have here base factor, BIC, et cetera? Are those basically like your error measurements? Are they your, how to say, just attempt at measuring like accuracy of prediction or or is there some trade off between explainability and prediction, if that makes sense?
411930	441930	17	A	0.98	06:51	71	Yeah, just to kind of summarize these. So when you have nested parametric models, then you can evaluate whether two models are better in terms of including or not some parametric factor and then testing for their likelihood ratios. So this is a pretty straightforward test, but it's pretty limited to just strictly nested parametric models. So just getting that out of the way. In general, we're in the Bayesian setting.
442090	476700	18	A	1.0	07:22	76	And so one of the advantages of the Bayesian setting is that you can compare two different models that have totally different architectures on the same data set. So one way that that's done is with this Ike information criterion or the Bayesian information criterion. Both of them do something very similar, which is they reward model fit and they penalize the number of parameters. So actually in that way it's a lot like equation 2.5.
479650	515100	19	A	0.99956	07:59	89	You want to reward the fit but penalize the complexity of the model. Base factor is kind of like the Hlrt, but it doesn't have to be ratios of nested models. You can just compare the relative evidence in favor of one model or another. So this is used in structure modeling and structure learning, but this is all kind of like testing amongst a portfolio of models, which ones are better? Then how do you really get into the space of like, well, how effective is this model?
516270	569290	20	A	0.99993	08:36	114	You could test on a new data set, you could do all the regular techniques like cross validation, leave one out, et cetera, or test on a new data set. But then the very interesting question is how do you establish the efficacy of an action model? Because it's not just a passive inference model. So there you would need to get into like benchmarks like the OpenAI gym and other other settings where you can actually test the efficacy of an action model against some test input, rather than just a recognition model in some input. But that gets so situational that there's not like general there's not as many general considerations there.
569440	600850	21	A	0.9196	09:29	82	Another strategy is actually to even if it seems like a passive inference problem, like the MNIST digit data set, you could frame that in terms of the labeling is an action. So sometimes you can then take what seems to be a passive inference problem and then frame it as an active inference problem. So then you don't need to worry about these kind of like out of sample novel context action models, you can just kind of use standard benchmarks.
605680	656830	22	B	0.99941	10:05	90	Also, as far as I know, probably the only serious work on benchmarking the performance of active inference models is the work of Theofi Champion and his colleagues branching time active inference, which is they have proposed several different variants of branching time active inference, each of which with increasing performance in terms of their benchmarks. And aside from that, I'm not aware of any other work that takes this benchmarking study seriously enough to be reliable. I guess, yeah, surely there's a lot of proprietary work in this area.
659040	688180	23	A	0.98	10:59	49	The paper discussed in live stream eight scaling active inference. So here they used like the kind of several standard testing environments I forget the pendulum, the hopper, and then I think like the mountain car or maybe that's not this one, but they test the standard AI tests.
694810	718006	24	A	0.83	11:34	38	All right, just looking just going through the ones. All right. Are the transitions be independent from the emissions? So for context, we're talking about Figure 7.3, discrete time active inference. Are the transitions independent from the emissions?
718038	720560	25	A	1.0	11:58	8	And what does the next observation depend on?
724140	749090	26	A	0.99998	12:04	56	Short answer yes, they're independent. That's the sparsity of the Bayes graph. So we can clarify and give it a cleaner textual answer. But yes, the conditional independencies, which is the sparsity of the base graph which allows us to do factorized variational inference and get all of these advantages. That's exactly what we're looking at.
750180	759920	27	A	0.99253	12:30	13	So this visual graphical model is the sparsity architecture of the conditional independencies.
762360	768900	28	A	0.91736	12:42	8	So yes, A and B are conditionally independent.
772570	782490	29	A	0.80833	12:52	22	Conditionally independent based on what? Hidden state. That's how Markov blanket works. Conditionally dependent on the blanket. A and B are independent.
783630	808770	30	A	0.99857	13:03	65	That's true of all Bayesian graphs. So without worrying about the Markov blanket and the interface and the cybernetic agent and all of that, just any node that intervenes between two other nodes is the Markov blanket in that setting. And then some other node, something else is blanketing it from something else. But yes, they're conditionally independent. What does the next observation depend on?
808920	844350	31	A	0.81708	13:28	76	Well, a matrix is the emission matrix tail of two densities. It can emit from a hidden state or it can recognize from an observation. So any given observation is only dependent upon the hidden state at that time and the hidden state at the next time is only dependent upon the transition matrix. The transition matrix has a slice for every action that can be taken. Every policy makes a slice in the B tensor.
844690	867640	32	A	0.99976	14:04	59	So it's like temperature in the room, hidden state, thermometer observation turn on the heater or not? And there's some B matrix for the heater is on. And there's some B matrix for the heater is not on. Those are two slices in the same object. And then policy selection means which submatrix of B, which slice of B?
868250	875260	33	A	0.99639	14:28	19	Should we multiply this to get to the next time step. And then what observation would I expect there?
890780	916850	34	A	0.66831	14:50	56	Good kind of standard question. Okay, what is the relevance of thinking about good regulator theorem for thinking about the generative model? So, from Cybernetics, good regulator theorem originally stated every good regulator of a system must be a model of that system. Or more accurately, every good regulator must contain a model of that system.
927550	929530	35	A	0.97264	15:27	7	Here's a few quotes from the textbook.
936470	958362	36	A	1.0	15:36	57	One way to approach this. Again, these are all kind of vague questions, but we can just see them many times. What happens to bad regulators? Well, information processing, sense making, decision making has a non zero informational cost land hour limit. There's a certain amount of actual jewels it takes to write and erase a bit.
958496	988100	37	A	0.90725	15:58	52	It from bit Chris Fields, everything that is in that area. So information processing is never free. So in a dissipative and even adversarial universe, to fail to regulate is to fail to exist. How do we operationalize regulation in this setting? That's going to be revisiting some non equilibrium steady state.
988570	1052490	38	A	0.99852	16:28	126	So if we're an observer looking at a system, in order for us to measure it as signal relative to noise, it has to be persistently remeasurable over the background. So we have to repeatedly measure it or an organism, let's just say, can remeasure itself. So minimizing surprise about its homeostatic temperature is being adaptive. Now, if temperature were just flat and so you could be unsurprised at homeostasis by doing nothing, you'd get lazy agents. But if temperature was really variable and contextual and there was kind of these nonlinear cues in the environment and all of that, then in order to reduce surprise about temperature, you'd end up coming to effectively have a model of the causal nexus that gives rise to observations.
1053630	1078020	39	A	0.99992	17:33	56	So the structure of the generative model doesn't have to be the structure of the generative process. However, they may come to have certain isomorphisms with each other or at least statistical regularities. If there's actually a 24 hours cycle in temperature, then you're going to see some kind of oscillatory model in the generative model.
1080150	1120800	40	A	0.92194	18:00	91	So there's more to say there. But this is one good note. Perhaps rather than good or bad regulator, the language of morality or preference, the language should be accurate and inaccurate, effective, ineffective, viable inviable skillful, unskillful. So there's always many ways to see it. But basically, if the generative model in the limit case, it just totally knows the causal architecture of the world, that's the easiest way to be absolutely unsurprised and to have things as you expect prefer is literally know how they're going to play out.
1121170	1152570	41	A	0.99993	18:41	71	That's not plausible. We use course grading and approximations and heuristics like variational inference. So the best we can do is just iteratively optimize towards bounding surprisal. So it's kind of like an empirical optimizable heuristic for being a good regulator without getting too bogged down into the philosophy and the exactitude the point is the ones that do well enough to live, live ones that don't do well enough, don't.
1156750	1170670	42	A	0.85956	19:16	33	But active inference is in the lineage of cybernetics. So it's unsurprising that good. Regulator theorem requisite diversity. Viable systems models. A lot of things in the Cybernetics ontology have a natural home.
1170740	1181860	43	A	0.99946	19:30	26	In the active ontology, they're talking about the same territory adaptive agents. So it's not surprising that they don't invalidate each other or anything like that.
1187500	1200110	44	B	0.73	19:47	25	And actually, I think it's one of the reasons cybernetics has experienced kind of revitalization in recent years, especially in the past couple of years.
1204320	1208000	45	A	0.58	20:04	12	All right, how can organs other than the brain be making inferences?
1211480	1218390	46	A	0.99983	20:11	20	So there's a few angles on this. The first angle or Ali or anyone else want to give a thought?
1223340	1289196	47	B	0.99	20:23	102	One thing to point is this sense of kind of semi PANC computationalism that if not pan computationalism, but something that bestows inference not only to complex self organizing systems such as the brain, but even to very simple linear systems, even a system as simple as an inert rock. So basically, it covers a spectrum from the point of view of FEP. There isn't anything specifically unique about the brain. In other words, the same mathematical technology can be applied both to inert rocks as well as the brain. So in my opinion, this question should be turned on its head.
1289378	1406400	48	B	1.0	21:29	187	And it's not that how does inference can be seen in other systems, other simpler systems, but rather, the relevant question should be how does active inference or the notion of inference, inactive inference literature applies to all of those situations. So one way to do that is to define a kind of sparse coupling between the environment and the agent and define variational density as the internal states of the system. And also, obviously, the external states of the environment would be the states that needs to be tracked by those variational densities. And in this case, there isn't anything inherently different between the way the brains or, I don't know, even sentient agents somehow undertake inference from the way that the inert rocks can partition their states into internal and external states. But the main difference would be the way the Markov blanket in those simpler systems can act as a kind of statistical boundary that allows for non causal relationships I'm sorry, nonlinear causal relationships as opposed to linear causal relationships as opposed to nonlinear causal relationships as observed in complex agents or systems.
1409720	1453130	49	B	0.93993	23:29	53	That's some of the main distinctions between those. Awesome. Yeah. And then the classic paper that we often return to, like Ollie mentioned, the inert rock. So this paper has the kind of visual taxonomy from simpler to more sophisticated agents that's by sometimes the papers are in white, sometimes they're in black.
1455840	1495588	50	A	0.98665	24:15	77	Here inert systems, active, classical conservative systems, strange systems with world models of their own and all of this. But this is a great response, which is in a pan computationalist or pan cognitive pan inferentialist world, then how does active inference apply? One other angle is like, let's just think about the liver or the pancreas and blood sugar. That's our system of interest. Realism is like, is the pancreas actually doing inference on blood sugar?
1495764	1523868	51	A	0.84	24:55	67	And people can have a range of opinions, but in a pan cognitiveist world, the answer is yes. Or one can kind of pull back and just say, we're going to model the pancreas as doing inference on blood sugar. So it is no different. It's just an interpretation of the same statistical apparatus, basically. But there might be a setting where scientific realism is more justified.
1523964	1585620	52	A	0.99988	25:23	138	There's multiple lines of converging evidence. The model is comprehensive and being used to generate unique explanations and predictions versus, like, we're just going to do a linear relationship between these two things in the public health. And so then we're not going to confuse the linear model with those with the actual causal architecture. But there are situations that you can design or analyze where the sparsity structure of the system, which as early points to is what grants it all these interesting properties. It is becoming known to an extent that within two or three or four sigma, it's like we're starting to talk about how it is not mistaking the map for the territory, that's the map territory fallacy, but also not doing some kind of map denialism, which is the map territory fallacy, fallacy.
1588390	1593460	53	A	1.0	26:28	12	And that's the paper which I'll add into by Maxwell et al.
1596620	1635830	54	B	0.96	26:36	57	And also, if I may add another point with regard to the distinction between realism and instrumentalism. I believe, at least in the realism school, probably the most relevant or well situated stance to reframe active inference. I mean, reframe its ontological status is structural realism, as argued by Majid. Benny and others in several papers. Nice.
1636520	1679410	55	A	0.71796	27:16	79	Yes. He's joined several discussions and it's great to okay, so there's some quotes from the book, but yes, basically, forget organs other than the brain, how could anything be doing inference? And then another angle to take is just like the way that a baseball computes a parabola. You don't need to think that it's doing it like a calculator, but that's kind of like naturalizing the computation. That's a path of least action in a spatial space.
1680020	1686260	56	A	0.99999	28:00	17	But also you could have a path of least action in a cognitive space. That's Bayesian Mechanics.
1689080	1732480	57	A	0.69	28:09	75	And then you could have more of a realist or more of an instrumentalist angle. Yeah, please. Sorry. Just one thing that somehow I mean, gets confused is some sometimes people think FEP's claim is that, okay, so if a self organizing if FEP formalism applies to a self organizing system, then it should persist through time by preserving its markup blankets intact. But actually the claim of the FEP is the other way around.
1732630	1800680	58	B	0.99977	28:52	122	So its main assertion is that if anything persists through time, how does FEP applies to it? So in other sense, FEP doesn't provide any justification per se for describing the way that the system persists through time. But we take the persistence of the system through time as the premise and then apply FEP to somehow investigate the implications of that premise through Bayesian mechanics and FEP formalism. Yeah, without the implicit or explicit acknowledgment of the persistent existence of what you're measuring or yourself, you don't even have something to discuss. And a lot of times when people just start to specify what they mean by the nouns and the verbs and the adjectives they use, that's a deflationary approach.
1801420	1808910	59	A	0.99996	30:01	33	What if there's something that comes into being so fast in between moments and it's not being measured and you can't detect it? It's like, well, then how do you know it's there?
1811580	1825940	60	A	0.50673	30:11	24	That's not within that statistical sensing apparatus. So you have to take the existence of the thing to be primary in modeling some thing.
1828150	1831560	61	A	0.52	30:28	11	And Axle Constant has a lot of work in that area.
1835810	1844800	62	B	0.93567	30:35	16	Daniel, could you please pull up the path integral paper on page four, if I may?
1849830	1858100	63	B	0.99577	30:49	16	Sorry. Yep, here we nice. Thank you. Exactly right there. The FTP addresses the following question.
1860250	1886490	64	A	1.0	31:00	66	And it's like, well, if we're talking about something that exists even in a mental geometry, that's what we want. We want to start with if it exists, even hypothetically. So people say, can it really describe all things? It's like, well, all the things that exist or could exist. We could use this modeling approach to model and talk about what properties it must possess.
1887390	1927914	65	A	0.99995	31:27	99	But if you're willing to open the door to things that don't or can't exist, then of course the sky's the limit on what properties they do or don't possess, because they might. It's like the non elephant animals. Things might not exist for a huge variety of reasons. However, things that exist, either from an external measure measurement or from self measurement, they must be acting as if they're self evidencing, reducing surprise relative to a generative model path of least action through an information geometric space. If it isn't doing that, you're not going to observe it.
1928112	1980860	66	A	1.0	32:08	95	And that's where we get the particular partition and the particular partition in the particular physics. Bayesian mechanics since Karl Friston's 2019 monograph, free Energy Principle for a Particular Physics, that was like a big inflection point in this line of research. Whereas in the 2016 to 2018, there's a lot of qualitative work and the philosophical work applying to multiscale and nested systems and self organization amidst other empirical simulations happening continually in all of this reading works of different times, like different pieces are kind of going to be made clear or not.
1988950	1992690	67	A	0.99972	33:08	9	What is the role of precision in nested models?
1999960	2033250	68	A	0.57371	33:19	66	Well, here and also by way of demonstrating the transferability of active inference generative models, these three figures are discussed in live stream 28 from the original paper. Smith et al. Computational phenomenology. So this is a static perceptual Bayesian model. We have hidden state observations and we hear as a modulator, literally a neuromodulator on the A matrix Taylor two densities recognition matrix generative model.
2033860	2065640	69	A	0.99998	33:53	63	You have the precision. So precision is one over the temperature. So whether you think of it as like higher temperature is lower precision or lower or vice versa, they're the same. When your precision is low, your temperature is high, a gets blurred out. The high temperature limit is like A becomes flattened, whereas in the low temperature limit, A becomes sharpened.
2066060	2082530	70	A	0.83	34:26	31	And so this is a common method in recognition modeling. Okay? Now they move into adding action in. So here there's the policy. Figure 4.3, figure 7.3, everything we've looked at.
2083220	2124076	71	A	1.0	34:43	75	And then they move to this nested model. Now, in the Sandbed Smith paper, it was being mobilized in terms of basically phenomenology meditative experiences, direct sensory perception, visual perception and isocades attention, unreportable attention and then awareness at a third level, observing that. And so you can add an uncertainty, like a precision on any variable. You could have an uncertainty on your prior d. You could have uncertainty on all kinds of things.
2124098	2176940	72	A	0.95439	35:24	84	But sometimes uncertainties on specific variables become associated with certain cognitive phenomena. In this case, they're using it to describe introspective and explainable AI systems, taking the exact same figures, exact same formalisms, just moving them into the AI setting. So precision on A and here's that live stream 28 and the slides and everything. Precision on A is like sensory ambiguity, precision on G. So how precise are you on the free energy that is associated with the affect in this sophisticated affect?
2178240	2208516	73	A	0.95818	36:18	60	Here's some discussion on Embodiment and the Alexander technique. So, suffice to say, precision and nested modeling is basically like the general precision concept, which is basically used everywhere. This is how we fit Bayesian models. We have some hyper prior distribution on a parameter. And if that prior distribution is too sharp, then it's a pathology of the prior.
2208548	2227404	74	A	0.98993	36:48	38	It's too precise. If it was too imprecise, you have an underfit model. So it's like that. And especially in nested models, precision plays a role in kind of gating. Just in this case, it's a general question.
2227442	2259610	75	A	0.99999	37:07	72	So you could make it do anything, basically. But here the precision on A is like gating, let's just say the first and the third level. So if there's no attention being paid to sensory input, don't be surprised that they don't come up to attention. But if this gating can be like super direct and forceful, then you'll have propagation of informational causality on this network up to the higher level.
2272740	2274800	76	A	0.99653	37:52	5	Page 113 in the textbook.
2284360	2325760	77	A	0.96	38:04	80	All right, the decision this is about planning as inference. Which one of these questions was also about the decision whether to model alternative futures, counterfactual futures conditioned upon policy selection. How would the world change if I did this or that? Is largely tied up with the choice between discrete and continuous models because the idea of selecting between alternative futures defined by sequences of actions is more simply articulated using discrete time models. All right, so figure 4.3.
2330610	2341140	78	A	0.49885	38:50	28	Figure 4.3 is like the rosetta stone. It juxtaposes the discrete time continuous discrete time generative model and the continuous time generative model. So I'll just copy this.
2343270	2371190	79	A	0.99978	39:03	72	In the discrete time model, you have st minus one s of t, s of t plus one. So futures and pasts, if it's sophisticated, active inference, are being explicitly modeled. So it's like what would happen at 07:00 if I did this? That is explicitly addressable with a discrete time model of the correct time horizon. So you have to explicitly say, I'm talking about an hourly model and seven depth.
2371350	2431200	80	A	0.95	39:31	141	And so then in branching time active inference, just like a chess algorithm, it's dealing with the branching structure because if you have a lot of affordances and you have a lot of time depth, you can imagine this is a combinatorial explosion. In contrast, though these two generative models are shown here to emphasize their structural similarity, the continuous time generative model is actually not explicitly modeling past, present, future time steps. Rather, it's modeling the generalized coordinates of motion x hidden state x prime rate of change, x double prime second derivative. So in that way, it's a lot more like a Taylor series expansion. So a Taylor series expansion, technically even a Taylor series expansion of depth one, it has an answer for every single point in the number line, but that doesn't mean it's a good one.
2432370	2473326	81	A	1.0	40:32	82	And so continuous time models kind of trivially have something to say about all possible time steps just by analytic continuation of the Taylor series expansion in the generalized coordinates of motion way. However, you don't necessarily know how accurate it is for any given point. It also doesn't have explicit counterfactuals. So if you do some Taylor series expansion, it says yeah, at x equals three over y is five. But then if you say, well, what if it were different?
2473508	2487780	82	A	0.84	41:13	39	The only way you could do that would be change the parameters of the Taylor series and recalculate it at that point. So the model itself is not exactly doing these. Like what would happen if I did that?
2491610	2515120	83	A	0.9377	41:31	40	So futures are explicit in discrete time models and interpretable explicitly, whereas in continuous time models, pasts and futures are kind of trivially predicted such that it doesn't really make sense to talk about counterfactuals in the same exact way.
2519090	2527780	84	A	0.75	41:59	13	And these models can be hybridized and fused, which is in chapter eight.
2536400	2545330	85	A	0.92793	42:16	21	Can we generate a code of template? Looks like yes, but there's of course more information. But it's an important question.
2553510	2571670	86	A	0.9999	42:33	42	How is temporal depth specific to planning? Does temporal depth in perception make any sense? What is temporal depth? So temporal depth is how many timesteps the model is considered in the discrete time case. Does temporal depth make sense in perception?
2572090	2604020	87	A	0.99998	42:52	55	Well, perception is always modeled as instantaneous. However, depending on the temporal scale of a model, that perception may be instantaneous over a certain temporal thickness power. Why is temporal depth specific to planning? So in planning as inference, policy selection as inference, you're selecting or sampling from policies based upon their expected free energy.
2607230	2618000	88	A	0.99998	43:27	24	In order to plan over a given time horizon and actually evaluate the playouts, you need to have a temporal depth of that amount.
2620930	2661500	89	A	0.99792	43:40	72	Thomas Parr in his bookstream so early implementations were coming from the generalized filtering approach. Maybe it wasn't generalized at that point, but particle filtering approach. And then quipped these models with action. Then people started thinking about how to get sequential dynamics. Predator prey lock of voltera models winnerless competition neural Darwinism in these situations, there's an emergence of sequential recurrent continuous dynamics, like a limit cycle of more than two.
2662990	2689650	90	A	1.0	44:22	50	Then people wanted to model explicit long term planning. And so from around 2014 or something, there was a lot more development in the discrete state space model, which enabled the well understood, partially observed Markov decision process and a lot of the characterization of planning as inference explore exploit.
2692010	2715766	91	A	0.87929	44:52	48	Then later developments supported nested models. Here there's a discrete time on top and a discrete time on the bottom. But also it could be continuous time on the bottom. Oh yes. Then continuous state models were reintroduced as the lower levels of higher level discrete time models.
2715958	2736370	92	A	1.0	45:15	43	And that is kind of like the folk psychology Livestream 46 active inference does not contradict folk psychology. Discrete time decision making up top, discrete time and discrete decisions and then more in the sensory motor. It's more like continuous time, continuous action.
2749580	2762720	93	A	0.99766	45:49	5	Okay, new question, figure 6.1.
2769870	2774410	94	A	0.86	46:09	4	All right, particular partition.
2779200	2806404	95	A	0.99991	46:19	61	How is the mutual interaction between active states and sensory states meant in six one? So the bi directional line here, can they mutually change their states without impacting neither internal nor external states? Yes, they could. You could have nodes that these nodes are in communication with each other. In general, this image, it's more important what it doesn't show.
2806522	2827372	96	A	0.99989	46:46	55	So there's no backwards arrow from external to active or from internal to sensory. You can't have your thumb on the scale and the symmetry of whatever that is from the outside. And the only other constraint is no telepathy. No telekinesis. The only way to receive information about external states is through sensory states.
2827426	2867080	97	A	1.0	47:07	97	The only way to act on costly intervene on external states is through active states. So no thumb on the scale and the mirror, no telepathy. No telekinesis. That doesn't mean that for any given model, these arrows are like all important or all relative or relatively of the same importance. So if you want to design a computer system where the sensory states comes in one computer, and this is a second computer and this is the third computer, so like there's no direct edge between sensory and active states, you can create that causal architecture.
2867500	2890690	98	A	0.99999	47:47	57	What is it supposed. To model nothing. Modeling of supposed things is done in chapter six and beyond. But in general, this is not trying to model any specific situation despite the brain in the world. Could they circle mutually modifying their states and then eventually arrive at a state where they change the external internal states?
2893540	2917876	99	A	0.97399	48:13	62	Yeah, maybe they have a faster timescale or something. Or maybe they're in communication with each other and then active states, like sensory states ends up influencing internal states via active influencing it. And so yes, it could happen. Would that mean that we can model with this trick, any arbitrary behavior? It's not necessarily a trick, it's just a particular partition.
2918068	2944850	100	A	0.90181	48:38	67	Would that allow us to model Turing equivalents? I'm not sure if there's an understanding of what formally demonstrates Turing equivalents then, but I believe it's possible. I think as a Turing architecture, as far as I understand it, abstracted from the von Neumann architecture is basically just saying, like, you can do a Turing tape. So I don't see why not. Why would we need that?
2946820	2949890	101	A	1.0	49:06	8	512k ought to be enough for anyone, right?
2952440	2955990	102	A	0.63	49:12	10	I don't know why we need it. We expect it.
2959160	2989090	103	A	0.96606	49:19	83	But yeah, these graphs in general are more like the space of the possible with the important caveats that were mentioned with the no thumb on the scale and the mirror and the no telepathy and no telkinesis. But how relevant these edges are or what systems have what behavior or what cognitive phenomena are granted by what dynamics on graphs. There's just no general answer to those things because if you do a graph for one setting, it's going to be different.
2994230	3032770	104	A	1.0	49:54	76	What did you mean when you said that any entree could be ordered with any side dish like that in this recipe theme that we're in models that include continuous or discrete variables or both, what is meant by variable states or observations? How can states be continuous, as those here are probably familiar with? States or observations could be like discrete, like zero or one or any integer. Or it could be a continuous number.
3037510	3059050	105	A	0.54901	50:37	37	Computers discretize continuous functions to approximate them. But there's so many exciting directions with unconventional computing, analog computing, mem computing, et cetera, that there's more to it. But just simply it could be any type of variable.
3065130	3095170	106	A	0.97034	51:05	55	This new question, let's maybe look at this one in closing, if the external state is unknowable, how can we set up a generative process so that's what's generating the observations when our experience is based solely on the process of producing variational free energy based on predictions and sensory inputs across the markup blanket?
3097190	3129630	107	A	1.0	51:37	81	This person has just described the challenge of life. I don't think that there is a specific answer to that. I think this is literally a restatement of the particular partition. If this was just said, the challenge is to given the unknowability, direct unknowability of internal states. The challenge and the opportunity is to set up a generative model based solely on the process of reducing free energy, based on predictions and sensory inputs across the blanket and adaptive action.
3131810	3145540	108	A	0.99781	52:11	37	I'm not sure if they meant process or model here because setting up a generative process is something that the human modeler does when they're designing a simulation, but not like what the animal has to do.
3149080	3178030	109	A	0.99999	52:29	88	What I'm trying to get at here is we are making an assumption about the generator of sensory input when making the model. Yes, but that model can never be accurate. Well, it absolutely can be accurate. It's never going to be a one to one map is the territory, but of course it can be accurate. We don't need to understand have an atomic simulation of the sun to have a generative process of how many photons are going to hit my window tomorrow at 07:00 a.m..
3179440	3205210	110	A	0.68151	52:59	80	So there's a core invalidity in the notion that we set up a generative process model. Not sure what ontology mixing is happening, but almost by definition we're introducing an error in the model by setting up the parameters for the generative process. The generative model, we remove the fundamental aspect of actin, which is the dynamical system has only one thing it can do and that's to reduce free energy. So there's some good points and some mixed things.
3207740	3244308	111	A	0.99997	53:27	108	What it does is the action it takes in the world variational. Free energy is just a tractable computational heuristic. Yes, if you set up the generative process to be a number continuous number between one and ten and then the generative model to do the exact same, to kind of have pre structurally learnt the problem, then it's going to be a simple simulation. Whereas you could have a more sophisticated simulation that involves structured learning as part of the agent's generative model. Are we not introducing information to the model that would not be available in the real world if we actually defined the generative process?
3244474	3287090	112	A	0.99984	54:04	94	Yeah, you can give any model too much information, essentially. So if the model actually knows what is really happening, then if you were doing a video game and you had an agent with supervisory access to the other players inferences or even control their actions, yeah, that's not going to work in a tournament. But here with a particular partition, we can actually know the information encapsulation. And Majid Benny explores that in terms of the partial information encapsulation. So are we not introducing information to the model that would not be available?
3288100	3317740	113	A	0.9309	54:48	70	It's your restaurant, do whatever you got to do with the recipe. If you want a pedagogical example that just makes some clean graphs and is very straightforward and doesn't engage the complexities of like sophisticated cognitive structure learning, that model is not going to complain. If you want to do strange loop reflexive structure modeling for ambiguous inputs of unstructured multimodal data, et cetera, then that is your challenge.
3320460	3347760	114	A	0.72949	55:20	70	So for many people it's just one closing comment. In many people. This is the first time they've been exposed to statistical modeling, and iterative modeling formally. So that's why there are often questions that are, like, less about active inference, but sometimes crop up about basically using statistical modeling overall, which is a great thing because these are challenging and areas with a lot of implicit and tacit knowledge.
3350740	3374630	115	A	0.99964	55:50	42	Thank you, fellows. Looking forward to next discussions and into heading into chapter seven next week. Ali, maybe we'll do a maybe we'll do our zero for chapter seven and eight, but not in a hurry. But we'll figure it out. Sure.
3375240	3379528	116	B	0.9983	56:15	9	Thank you so much. Thank you. Thank you. Bye.

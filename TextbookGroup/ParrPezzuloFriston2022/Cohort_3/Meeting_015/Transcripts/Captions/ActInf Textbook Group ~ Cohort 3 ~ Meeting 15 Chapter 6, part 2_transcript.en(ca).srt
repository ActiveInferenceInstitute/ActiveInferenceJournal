1
00:00:02,918 --> 00:00:03,441
Daniel Friedman: All right.

2
00:00:03,479 --> 00:00:04,546
Greetings, everyone.

3
00:00:06,114 --> 00:00:08,716
It's June 20, 2023.

4
00:00:08,834 --> 00:00:11,285
We're in our second discussion on chapter

5
00:00:11,285 --> 00:00:12,321
six.

6
00:00:12,374 --> 00:00:15,654
So we'll first just have any general

7
00:00:15,654 --> 00:00:17,825
comments or anything.

8
00:00:17,870 --> 00:00:20,171
Then we'll turn to the questions table

9
00:00:20,171 --> 00:00:20,171
and look at what questions we didn't get

10
00:00:20,171 --> 00:00:20,171
to last time and just kind of revisit,

11
00:00:20,171 --> 00:00:20,171
maybe condense some questions or just see

12
00:00:20,171 --> 00:00:33,406
what we can do with what is here.

13
00:00:33,413 --> 00:00:37,813
So anyone just want to make any general

14
00:00:37,813 --> 00:00:39,069
comments on chapter six?

15
00:00:48,970 --> 00:00:50,168
Andrew Pashea: I believe this is the second

16
00:00:50,168 --> 00:00:52,339
session on chapter six, right?

17
00:00:52,352 --> 00:00:53,425
Daniel: Correct.

18
00:00:53,443 --> 00:00:55,616
Andrew: Or maybe I lost.

19
00:00:55,623 --> 00:00:55,674
Daniel: Okay.

20
00:00:55,688 --> 00:00:56,788
Yeah, it is.

21
00:00:58,963 --> 00:00:59,038
Yeah.

22
00:01:00,561 --> 00:01:03,818
There was only one onboarding needed for

23
00:01:03,818 --> 00:01:03,818
the welcome back here, so we started one

24
00:01:03,818 --> 00:01:07,270
week faster into the rhythm.

25
00:01:09,490 --> 00:01:10,560
Okay.

26
00:01:10,575 --> 00:01:14,972
So it looks like these were the questions

27
00:01:14,972 --> 00:01:18,341
I remember we left off with this one.

28
00:01:18,353 --> 00:01:21,618
Let's just develop this into what was

29
00:01:21,618 --> 00:01:21,618
into a question, and then we'll continue

30
00:01:21,618 --> 00:01:24,941
on.

31
00:01:37,233 --> 00:01:38,308
Okay.

32
00:01:41,608 --> 00:01:45,009
Is there a common or good representation

33
00:01:45,009 --> 00:01:45,009
or rubric for evaluating Generative Model

34
00:01:45,009 --> 00:01:45,009
Generative process, which is change it to

35
00:01:45,009 --> 00:01:45,009
Generative Models and Generative

36
00:01:45,009 --> 00:02:01,104
Processes?

37
00:02:11,201 --> 00:02:13,221
Anyone want to give a first thought or

38
00:02:13,221 --> 00:02:15,239
just some other related question?

39
00:02:26,352 --> 00:02:27,361
Andrew: Go on, please.

40
00:02:27,362 --> 00:02:27,365
Sorry.

41
00:02:27,366 --> 00:02:28,375
Daniel: No, it's okay.

42
00:02:33,425 --> 00:02:37,463
Andrew: Yeah, I'm not sure about what's

43
00:02:37,463 --> 00:02:37,463
the exact criteria for evaluating

44
00:02:37,463 --> 00:02:37,463
generative models or generative processes,

45
00:02:37,463 --> 00:02:37,463
, but one thing is for sure that, well, I

46
00:02:37,463 --> 00:02:37,463
mean, one of the main at least when

47
00:02:37,463 --> 00:02:37,463
evaluating generative model one of our

48
00:02:37,463 --> 00:02:56,655
main.

49
00:02:56,655 --> 00:03:01,649
Criteria should be how closely it tracks

50
00:03:01,649 --> 00:03:01,649
our situation of interest and

51
00:03:01,649 --> 00:03:01,649
specifically how relevant it is for

52
00:03:01,649 --> 00:03:01,649
addressing the question we're trying to

53
00:03:01,649 --> 00:03:17,804
examine.

54
00:03:19,823 --> 00:03:22,853
I don't think it's something clear cut or,

55
00:03:22,853 --> 00:03:24,877
, I don't know, written in stone.

56
00:03:24,879 --> 00:03:27,906
And based on the context and the

57
00:03:27,906 --> 00:03:27,906
situation, the Evaluation Criteria rubric

58
00:03:27,906 --> 00:03:27,906
should be different, I suppose, both for

59
00:03:27,906 --> 00:03:38,014
generative Model and generative process.

60
00:03:44,072 --> 00:03:44,078
Daniel: Yeah.

61
00:03:45,079 --> 00:03:47,108
So again, we'll revisit this Generative

62
00:03:47,108 --> 00:03:47,108
Model Generative process question later,

63
00:03:47,108 --> 00:03:47,108
following the recent Livestream, but

64
00:03:47,108 --> 00:03:47,108
generative process working with

65
00:03:47,108 --> 00:03:47,108
generative process as the underlying

66
00:03:47,108 --> 00:03:47,108
process that gives rise to the

67
00:03:47,108 --> 00:04:01,188
observations.

68
00:04:02,192 --> 00:04:04,215
This would seem to be more adequate to

69
00:04:04,215 --> 00:04:04,215
the extent that it can better describe

70
00:04:04,215 --> 00:04:08,250
the measurements.

71
00:04:08,255 --> 00:04:12,294
Generative Model, similarly, is being

72
00:04:12,294 --> 00:04:12,294
evaluated based upon its ability to fit

73
00:04:12,294 --> 00:04:12,294
to the generative process as well as

74
00:04:12,294 --> 00:04:23,399
phenomena of interest.

75
00:04:24,418 --> 00:04:28,453
So not just to fit visual data, but maybe

76
00:04:28,453 --> 00:04:28,453
to model something like some illusion in

77
00:04:28,453 --> 00:04:28,453
visual, then there's a wide range of more

78
00:04:28,453 --> 00:04:38,557
general statistical modeling techniques.

79
00:04:38,557 --> 00:04:41,581
So first is like the ultimate grab bag,

80
00:04:41,581 --> 00:04:41,581
which is just how relevant is the overall

81
00:04:41,581 --> 00:04:44,614
modeling.

82
00:04:45,627 --> 00:04:49,667
Then there are statistical evaluations of

83
00:04:49,667 --> 00:04:53,702
model adequacy and model selection.

84
00:04:53,704 --> 00:04:55,722
Ike information criterion.

85
00:04:55,723 --> 00:04:57,749
Bayesian information criterion, base

86
00:04:57,749 --> 00:04:58,754
factor.

87
00:04:59,764 --> 00:05:02,731
Hierarchical likelihood ratio test.

88
00:05:02,733 --> 00:05:05,761
If you have a parametric Model Bootstrap

89
00:05:05,761 --> 00:05:07,782
and non parametric statistics.

90
00:05:07,786 --> 00:05:11,822
Just general modeling, and then taking

91
00:05:11,822 --> 00:05:11,822
that more out of the statistics into the

92
00:05:11,822 --> 00:05:11,822
Engineering Model Lifecycle, then that's

93
00:05:11,822 --> 00:05:11,822
where you can think about validation,

94
00:05:11,822 --> 00:05:11,822
verification, validity, quality, et

95
00:05:11,822 --> 00:05:27,981
cetera.

96
00:05:27,982 --> 00:05:29,003
Systems engineering model lifecycle.

97
00:05:34,059 --> 00:05:37,081
Any other thoughts or questions on this?

98
00:05:43,139 --> 00:05:43,147
Okay.

99
00:05:46,174 --> 00:05:49,203
Speaker C: I'm coming from kind of an

100
00:05:49,203 --> 00:05:49,203
early career data scientist background

101
00:05:49,203 --> 00:05:49,203
here, but as far as what I find

102
00:05:49,203 --> 00:05:49,203
attractive about active inference and

103
00:05:49,203 --> 00:05:49,203
doing things as generative models is the

104
00:05:49,203 --> 00:05:49,203
attempt to describe or explain what's

105
00:05:49,203 --> 00:06:05,308
going on in the data.

106
00:06:05,309 --> 00:06:09,349
And that being said, in terms of a lot of

107
00:06:09,349 --> 00:06:09,349
many applications of data science and

108
00:06:09,349 --> 00:06:09,349
machine learning, the emphasis is upon

109
00:06:09,349 --> 00:06:18,436
prediction.

110
00:06:18,439 --> 00:06:22,470
And so are these listed statistical

111
00:06:22,470 --> 00:06:22,470
metrics you have here base factor, BIC,

112
00:06:22,470 --> 00:06:26,514
et cetera?

113
00:06:26,516 --> 00:06:30,553
Are those basically like your error

114
00:06:30,553 --> 00:06:31,568
measurements?

115
00:06:31,569 --> 00:06:36,609
Are they your, how to say, just attempt

116
00:06:36,609 --> 00:06:36,609
at measuring like accuracy of prediction

117
00:06:36,609 --> 00:06:36,609
or or is there some trade off between

118
00:06:36,609 --> 00:06:36,609
explainability and prediction, if that

119
00:06:36,609 --> 00:06:49,746
makes sense?

120
00:06:51,768 --> 00:06:55,808
Daniel: Yeah, just to kind of summarize

121
00:06:55,808 --> 00:06:56,815
these.

122
00:06:57,823 --> 00:07:00,794
So when you have nested parametric models,

123
00:07:00,794 --> 00:07:00,794
, then you can evaluate whether two

124
00:07:00,794 --> 00:07:00,794
models are better in terms of including

125
00:07:00,794 --> 00:07:00,794
or not some parametric factor and then

126
00:07:00,794 --> 00:07:11,902
testing for their likelihood ratios.

127
00:07:11,904 --> 00:07:13,925
So this is a pretty straightforward test,

128
00:07:13,925 --> 00:07:13,925
but it's pretty limited to just strictly

129
00:07:13,925 --> 00:07:17,961
nested parametric models.

130
00:07:17,962 --> 00:07:19,982
So just getting that out of the way.

131
00:07:19,984 --> 00:07:21,009
In general, we're in the Bayesian setting.

132
00:07:21,009 --> 00:07:21,009
.

133
00:07:22,010 --> 00:07:24,036
And so one of the advantages of the

134
00:07:24,036 --> 00:07:24,036
Bayesian setting is that you can compare

135
00:07:24,036 --> 00:07:24,036
two different models that have totally

136
00:07:24,036 --> 00:07:24,036
different architectures on the same data

137
00:07:24,036 --> 00:07:34,130
set.

138
00:07:34,131 --> 00:07:36,157
So one way that that's done is with this

139
00:07:36,157 --> 00:07:36,157
Ike information criterion or the Bayesian

140
00:07:36,157 --> 00:07:41,200
information criterion.

141
00:07:41,204 --> 00:07:44,229
Both of them do something very similar,

142
00:07:44,229 --> 00:07:44,229
which is they reward model fit and they

143
00:07:44,229 --> 00:07:48,278
penalize the number of parameters.

144
00:07:49,280 --> 00:07:54,336
So actually in that way it's a lot like

145
00:07:54,336 --> 00:07:56,356
equation 2.5.

146
00:07:59,385 --> 00:08:02,359
You want to reward the fit but penalize

147
00:08:02,359 --> 00:08:05,383
the complexity of the model.

148
00:08:06,391 --> 00:08:09,420
Base factor is kind of like the Hlrt, but

149
00:08:09,420 --> 00:08:09,420
it doesn't have to be ratios of nested

150
00:08:09,420 --> 00:08:12,454
models.

151
00:08:12,455 --> 00:08:14,477
You can just compare the relative

152
00:08:14,477 --> 00:08:14,477
evidence in favor of one model or another.

153
00:08:14,477 --> 00:08:17,506
.

154
00:08:17,507 --> 00:08:21,541
So this is used in structure modeling and

155
00:08:21,541 --> 00:08:21,541
structure learning, but this is all kind

156
00:08:21,541 --> 00:08:21,541
of like testing amongst a portfolio of

157
00:08:21,541 --> 00:08:30,631
models, which ones are better?

158
00:08:30,636 --> 00:08:32,657
Then how do you really get into the space

159
00:08:32,657 --> 00:08:32,657
of like, well, how effective is this

160
00:08:32,657 --> 00:08:35,680
model?

161
00:08:36,692 --> 00:08:38,717
You could test on a new data set, you

162
00:08:38,717 --> 00:08:38,717
could do all the regular techniques like

163
00:08:38,717 --> 00:08:38,717
cross validation, leave one out, et

164
00:08:38,717 --> 00:08:46,793
cetera, or test on a new data set.

165
00:08:46,795 --> 00:08:49,827
But then the very interesting question is

166
00:08:49,827 --> 00:08:49,827
how do you establish the efficacy of an

167
00:08:49,827 --> 00:08:54,871
action model?

168
00:08:55,885 --> 00:08:59,928
Because it's not just a passive inference

169
00:08:59,928 --> 00:09:00,875
model.

170
00:09:00,876 --> 00:09:03,906
So there you would need to get into like

171
00:09:03,906 --> 00:09:03,906
benchmarks like the OpenAI gym and other

172
00:09:03,906 --> 00:09:03,906
other settings where you can actually

173
00:09:03,906 --> 00:09:03,906
test the efficacy of an action model

174
00:09:03,906 --> 00:09:03,906
against some test input, rather than just

175
00:09:03,906 --> 00:09:18,051
a recognition model in some input.

176
00:09:18,055 --> 00:09:22,096
But that gets so situational that there's

177
00:09:22,096 --> 00:09:22,096
not like general there's not as many

178
00:09:22,096 --> 00:09:29,162
general considerations there.

179
00:09:29,164 --> 00:09:32,193
Another strategy is actually to even if

180
00:09:32,193 --> 00:09:32,193
it seems like a passive inference problem,

181
00:09:32,193 --> 00:09:32,193
, like the MNIST digit data set, you

182
00:09:32,193 --> 00:09:32,193
could frame that in terms of the labeling

183
00:09:32,193 --> 00:09:42,292
is an action.

184
00:09:44,311 --> 00:09:46,332
So sometimes you can then take what seems

185
00:09:46,332 --> 00:09:46,332
to be a passive inference problem and

186
00:09:46,332 --> 00:09:46,332
then frame it as an active inference

187
00:09:46,332 --> 00:09:50,374
problem.

188
00:09:51,386 --> 00:09:54,409
So then you don't need to worry about

189
00:09:54,409 --> 00:09:54,409
these kind of like out of sample novel

190
00:09:54,409 --> 00:09:54,409
context action models, you can just kind

191
00:09:54,409 --> 00:10:00,418
of use standard benchmarks.

192
00:10:05,466 --> 00:10:09,504
Andrew: Also, as far as I know, probably

193
00:10:09,504 --> 00:10:09,504
the only serious work on benchmarking the

194
00:10:09,504 --> 00:10:09,504
performance of active inference models is

195
00:10:09,504 --> 00:10:09,504
the work of Theofi Champion and his

196
00:10:09,504 --> 00:10:09,504
colleagues branching time active

197
00:10:09,504 --> 00:10:09,504
inference, which is they have proposed

198
00:10:09,504 --> 00:10:09,504
several different variants of branching

199
00:10:09,504 --> 00:10:09,504
time active inference, each of which with

200
00:10:09,504 --> 00:10:09,504
increasing performance in terms of their

201
00:10:09,504 --> 00:10:39,808
benchmarks.

202
00:10:40,810 --> 00:10:43,847
And aside from that, I'm not aware of any

203
00:10:43,847 --> 00:10:43,847
other work that takes this benchmarking

204
00:10:43,847 --> 00:10:51,920
study seriously enough to be reliable.

205
00:10:51,921 --> 00:10:54,949
Daniel: I guess, yeah, surely there's a

206
00:10:54,949 --> 00:10:56,977
lot of proprietary work in this area.

207
00:10:59,999 --> 00:11:02,975
The paper discussed in live stream eight

208
00:11:02,975 --> 00:11:04,997
scaling active inference.

209
00:11:05,004 --> 00:11:09,040
So here they used like the kind of

210
00:11:09,040 --> 00:11:09,040
several standard testing environments I

211
00:11:09,040 --> 00:11:09,040
forget the pendulum, the hopper, and then

212
00:11:09,040 --> 00:11:09,040
I think like the mountain car or maybe

213
00:11:09,040 --> 00:11:09,040
that's not this one, but they test the

214
00:11:09,040 --> 00:11:28,231
standard AI tests.

215
00:11:34,297 --> 00:11:38,334
All right, just looking just going

216
00:11:38,334 --> 00:11:40,354
through the ones.

217
00:11:40,358 --> 00:11:41,363
All right.

218
00:11:41,363 --> 00:11:43,386
Are the transitions be independent from

219
00:11:43,386 --> 00:11:44,395
the emissions?

220
00:11:45,402 --> 00:11:49,446
So for context, we're talking about

221
00:11:49,446 --> 00:11:49,446
Figure 7.3, discrete time active

222
00:11:49,446 --> 00:11:55,500
inference.

223
00:11:55,504 --> 00:11:57,524
Are the transitions independent from the

224
00:11:57,524 --> 00:11:58,529
emissions?

225
00:11:58,529 --> 00:12:00,493
And what does the next observation depend

226
00:12:00,493 --> 00:12:00,495
on?

227
00:12:04,531 --> 00:12:08,570
Short answer yes, they're independent.

228
00:12:08,572 --> 00:12:10,597
That's the sparsity of the Bayes graph.

229
00:12:10,598 --> 00:12:13,620
So we can clarify and give it a cleaner

230
00:12:13,620 --> 00:12:13,629
textual answer.

231
00:12:14,630 --> 00:12:16,659
But yes, the conditional independencies,

232
00:12:16,659 --> 00:12:16,659
which is the sparsity of the base graph

233
00:12:16,659 --> 00:12:16,659
which allows us to do factorized

234
00:12:16,659 --> 00:12:16,659
variational inference and get all of

235
00:12:16,659 --> 00:12:26,753
these advantages.

236
00:12:26,759 --> 00:12:29,780
That's exactly what we're looking at.

237
00:12:30,791 --> 00:12:34,829
So this visual graphical model is the

238
00:12:34,829 --> 00:12:34,829
sparsity architecture of the conditional

239
00:12:34,829 --> 00:12:39,888
independencies.

240
00:12:42,913 --> 00:12:47,960
So yes, A and B are conditionally

241
00:12:47,960 --> 00:12:48,978
independent.

242
00:12:52,015 --> 00:12:54,038
Conditionally independent based on what?

243
00:12:55,039 --> 00:12:56,050
Hidden state.

244
00:12:56,052 --> 00:12:58,072
That's how Markov blanket works.

245
00:12:58,076 --> 00:13:00,037
Conditionally dependent on the blanket.

246
00:13:00,038 --> 00:13:02,054
A and B are independent.

247
00:13:03,066 --> 00:13:06,097
That's true of all Bayesian graphs.

248
00:13:06,099 --> 00:13:09,122
So without worrying about the Markov

249
00:13:09,122 --> 00:13:09,122
blanket and the interface and the

250
00:13:09,122 --> 00:13:09,122
cybernetic agent and all of that, just

251
00:13:09,122 --> 00:13:09,122
any node that intervenes between two

252
00:13:09,122 --> 00:13:09,122
other nodes is the Markov blanket in that

253
00:13:09,122 --> 00:13:19,223
setting.

254
00:13:19,226 --> 00:13:21,249
And then some other node, something else

255
00:13:21,249 --> 00:13:24,271
is blanketing it from something else.

256
00:13:24,273 --> 00:13:25,287
But yes, they're conditionally

257
00:13:25,287 --> 00:13:26,294
independent.

258
00:13:26,295 --> 00:13:28,315
What does the next observation depend

259
00:13:28,315 --> 00:13:28,317
on?

260
00:13:28,318 --> 00:13:32,350
Well, a matrix is the emission matrix

261
00:13:32,350 --> 00:13:33,369
tail of two densities.

262
00:13:34,370 --> 00:13:36,393
It can emit from a hidden state or it can

263
00:13:36,393 --> 00:13:38,411
recognize from an observation.

264
00:13:38,418 --> 00:13:41,445
So any given observation is only

265
00:13:41,445 --> 00:13:41,445
dependent upon the hidden state at that

266
00:13:41,445 --> 00:13:41,445
time and the hidden state at the next

267
00:13:41,445 --> 00:13:41,445
time is only dependent upon the

268
00:13:41,445 --> 00:13:52,553
transition matrix.

269
00:13:52,558 --> 00:13:56,595
The transition matrix has a slice for

270
00:13:56,595 --> 00:13:59,627
every action that can be taken.

271
00:14:00,574 --> 00:14:03,606
Every policy makes a slice in the B

272
00:14:03,606 --> 00:14:04,613
tensor.

273
00:14:04,616 --> 00:14:07,645
So it's like temperature in the room,

274
00:14:07,645 --> 00:14:07,645
hidden state, thermometer observation

275
00:14:07,645 --> 00:14:12,694
turn on the heater or not?

276
00:14:12,696 --> 00:14:14,714
And there's some B matrix for the heater

277
00:14:14,714 --> 00:14:14,718
is on.

278
00:14:14,718 --> 00:14:16,736
And there's some B matrix for the heater

279
00:14:16,736 --> 00:14:17,741
is not on.

280
00:14:17,742 --> 00:14:19,763
Those are two slices in the same object.

281
00:14:20,772 --> 00:14:24,810
And then policy selection means which

282
00:14:24,810 --> 00:14:27,846
submatrix of B, which slice of B?

283
00:14:28,852 --> 00:14:30,874
Should we multiply this to get to the

284
00:14:30,874 --> 00:14:31,884
next time step.

285
00:14:32,891 --> 00:14:34,917
And then what observation would I expect

286
00:14:34,917 --> 00:14:35,922
there?

287
00:14:50,077 --> 00:14:54,115
Good kind of standard question.

288
00:14:55,128 --> 00:14:58,155
Okay, what is the relevance of thinking

289
00:14:58,155 --> 00:14:58,155
about good regulator theorem for thinking

290
00:14:58,155 --> 00:15:03,144
about the generative model?

291
00:15:03,145 --> 00:15:05,167
So, from Cybernetics, good regulator

292
00:15:05,167 --> 00:15:05,167
theorem originally stated every good

293
00:15:05,167 --> 00:15:05,167
regulator of a system must be a model of

294
00:15:05,167 --> 00:15:11,222
that system.

295
00:15:12,230 --> 00:15:14,255
Or more accurately, every good regulator

296
00:15:14,255 --> 00:15:16,278
must contain a model of that system.

297
00:15:27,385 --> 00:15:29,405
Here's a few quotes from the textbook.

298
00:15:36,474 --> 00:15:37,487
One way to approach this.

299
00:15:37,487 --> 00:15:39,504
Again, these are all kind of vague

300
00:15:39,504 --> 00:15:39,504
questions, but we can just see them many

301
00:15:39,504 --> 00:15:41,527
times.

302
00:15:41,528 --> 00:15:43,547
What happens to bad regulators?

303
00:15:43,548 --> 00:15:46,575
Well, information processing, sense

304
00:15:46,575 --> 00:15:46,575
making, decision making has a non zero

305
00:15:46,575 --> 00:15:52,632
informational cost land hour limit.

306
00:15:52,633 --> 00:15:55,665
There's a certain amount of actual jewels

307
00:15:55,665 --> 00:15:58,693
it takes to write and erase a bit.

308
00:15:58,694 --> 00:16:01,660
It from bit Chris Fields, everything that

309
00:16:01,660 --> 00:16:02,671
is in that area.

310
00:16:02,673 --> 00:16:06,710
So information processing is never free.

311
00:16:06,714 --> 00:16:10,759
So in a dissipative and even adversarial

312
00:16:10,759 --> 00:16:10,759
universe, to fail to regulate is to fail

313
00:16:10,759 --> 00:16:16,817
to exist.

314
00:16:17,826 --> 00:16:20,851
How do we operationalize regulation in

315
00:16:20,851 --> 00:16:21,860
this setting?

316
00:16:21,864 --> 00:16:25,903
That's going to be revisiting some non

317
00:16:25,903 --> 00:16:28,930
equilibrium steady state.

318
00:16:28,935 --> 00:16:31,968
So if we're an observer looking at a

319
00:16:31,968 --> 00:16:31,968
system, in order for us to measure it as

320
00:16:31,968 --> 00:16:31,968
signal relative to noise, it has to be

321
00:16:31,968 --> 00:16:31,968
persistently remeasurable over the

322
00:16:31,968 --> 00:16:43,086
background.

323
00:16:43,086 --> 00:16:47,125
So we have to repeatedly measure it or an

324
00:16:47,125 --> 00:16:47,125
organism, let's just say, can remeasure

325
00:16:47,125 --> 00:16:52,170
itself.

326
00:16:52,175 --> 00:16:55,200
So minimizing surprise about its

327
00:16:55,200 --> 00:16:55,200
homeostatic temperature is being adaptive.

328
00:16:55,200 --> 00:16:58,233
.

329
00:16:58,237 --> 00:17:02,213
Now, if temperature were just flat and so

330
00:17:02,213 --> 00:17:02,213
you could be unsurprised at homeostasis

331
00:17:02,213 --> 00:17:09,282
by doing nothing, you'd get lazy agents.

332
00:17:09,286 --> 00:17:12,317
But if temperature was really variable

333
00:17:12,317 --> 00:17:12,317
and contextual and there was kind of

334
00:17:12,317 --> 00:17:12,317
these nonlinear cues in the environment

335
00:17:12,317 --> 00:17:12,317
and all of that, then in order to reduce

336
00:17:12,317 --> 00:17:12,317
surprise about temperature, you'd end up

337
00:17:12,317 --> 00:17:12,317
coming to effectively have a model of the

338
00:17:12,317 --> 00:17:12,317
causal nexus that gives rise to

339
00:17:12,317 --> 00:17:32,514
observations.

340
00:17:33,525 --> 00:17:35,543
So the structure of the generative model

341
00:17:35,543 --> 00:17:35,543
doesn't have to be the structure of the

342
00:17:35,543 --> 00:17:37,568
generative process.

343
00:17:39,581 --> 00:17:42,610
However, they may come to have certain

344
00:17:42,610 --> 00:17:42,610
isomorphisms with each other or at least

345
00:17:42,610 --> 00:17:47,661
statistical regularities.

346
00:17:47,665 --> 00:17:50,695
If there's actually a 24 hours cycle in

347
00:17:50,695 --> 00:17:50,695
temperature, then you're going to see

348
00:17:50,695 --> 00:17:50,695
some kind of oscillatory model in the

349
00:17:50,695 --> 00:17:58,769
generative model.

350
00:18:00,731 --> 00:18:04,778
So there's more to say there.

351
00:18:04,779 --> 00:18:06,794
But this is one good note.

352
00:18:06,795 --> 00:18:09,821
Perhaps rather than good or bad regulator,

353
00:18:09,821 --> 00:18:09,821
, the language of morality or preference,

354
00:18:09,821 --> 00:18:09,821
the language should be accurate and

355
00:18:09,821 --> 00:18:09,821
inaccurate, effective, ineffective,

356
00:18:09,821 --> 00:18:18,917
viable inviable skillful, unskillful.

357
00:18:18,917 --> 00:18:21,947
So there's always many ways to see it.

358
00:18:21,948 --> 00:18:24,978
But basically, if the generative model in

359
00:18:24,978 --> 00:18:24,978
the limit case, it just totally knows the

360
00:18:24,978 --> 00:18:24,978
causal architecture of the world, that's

361
00:18:24,978 --> 00:18:24,978
the easiest way to be absolutely

362
00:18:24,978 --> 00:18:24,978
unsurprised and to have things as you

363
00:18:24,978 --> 00:18:24,978
expect prefer is literally know how they'

364
00:18:24,978 --> 00:18:40,013
're going to play out.

365
00:18:41,014 --> 00:18:42,015
That's not plausible.

366
00:18:42,015 --> 00:18:45,018
We use course grading and approximations

367
00:18:45,018 --> 00:18:45,018
and heuristics like variational inference.

368
00:18:45,018 --> 00:18:48,021
.

369
00:18:48,021 --> 00:18:51,024
So the best we can do is just iteratively

370
00:18:51,024 --> 00:18:55,028
optimize towards bounding surprisal.

371
00:18:55,028 --> 00:18:57,030
So it's kind of like an empirical

372
00:18:57,030 --> 00:18:57,030
optimizable heuristic for being a good

373
00:18:57,030 --> 00:18:57,030
regulator without getting too bogged down

374
00:18:57,030 --> 00:18:57,030
into the philosophy and the exactitude

375
00:18:57,030 --> 00:18:57,030
the point is the ones that do well enough

376
00:18:57,030 --> 00:18:57,030
to live, live ones that don't do well

377
00:18:57,030 --> 00:19:12,039
enough, don't.

378
00:19:16,043 --> 00:19:19,046
But active inference is in the lineage of

379
00:19:19,046 --> 00:19:20,047
cybernetics.

380
00:19:20,047 --> 00:19:22,049
So it's unsurprising that good.

381
00:19:22,049 --> 00:19:24,051
Regulator theorem requisite diversity.

382
00:19:24,051 --> 00:19:26,053
Viable systems models.

383
00:19:27,053 --> 00:19:28,055
A lot of things in the Cybernetics

384
00:19:28,055 --> 00:19:30,057
ontology have a natural home.

385
00:19:30,057 --> 00:19:33,060
In the active ontology, they're talking

386
00:19:33,060 --> 00:19:33,060
about the same territory adaptive agents.

387
00:19:33,060 --> 00:19:37,064
.

388
00:19:37,064 --> 00:19:39,066
So it's not surprising that they don't

389
00:19:39,066 --> 00:19:39,066
invalidate each other or anything like

390
00:19:39,066 --> 00:19:41,068
that.

391
00:19:47,074 --> 00:19:50,077
Andrew: And actually, I think it's one of

392
00:19:50,077 --> 00:19:50,077
the reasons cybernetics has experienced

393
00:19:50,077 --> 00:19:50,077
kind of revitalization in recent years,

394
00:19:50,077 --> 00:20:00,081
especially in the past couple of years.

395
00:20:04,085 --> 00:20:06,087
Daniel: All right, how can organs other

396
00:20:06,087 --> 00:20:08,088
than the brain be making inferences?

397
00:20:11,092 --> 00:20:13,094
So there's a few angles on this.

398
00:20:14,095 --> 00:20:16,097
The first angle or Ali or anyone else

399
00:20:16,097 --> 00:20:18,099
want to give a thought?

400
00:20:23,104 --> 00:20:28,109
Andrew: One thing to point is this sense

401
00:20:28,109 --> 00:20:28,109
of kind of semi PANC computationalism

402
00:20:28,109 --> 00:20:28,109
that if not pan computationalism, but

403
00:20:28,109 --> 00:20:28,109
something that bestows inference not only

404
00:20:28,109 --> 00:20:28,109
to complex self organizing systems such

405
00:20:28,109 --> 00:20:28,109
as the brain, but even to very simple

406
00:20:28,109 --> 00:20:28,109
linear systems, even a system as simple

407
00:20:28,109 --> 00:20:58,139
as an inert rock.

408
00:20:59,140 --> 00:21:04,139
So basically, it covers a spectrum from

409
00:21:04,139 --> 00:21:08,143
the point of view of FEP.

410
00:21:08,143 --> 00:21:12,146
There isn't anything specifically unique

411
00:21:12,146 --> 00:21:13,148
about the brain.

412
00:21:13,148 --> 00:21:16,151
In other words, the same mathematical

413
00:21:16,151 --> 00:21:16,151
technology can be applied both to inert

414
00:21:16,151 --> 00:21:22,157
rocks as well as the brain.

415
00:21:22,157 --> 00:21:27,162
So in my opinion, this question should be

416
00:21:27,162 --> 00:21:29,164
turned on its head.

417
00:21:29,164 --> 00:21:33,168
And it's not that how does inference can

418
00:21:33,168 --> 00:21:33,168
be seen in other systems, other simpler

419
00:21:33,168 --> 00:21:33,168
systems, but rather, the relevant

420
00:21:33,168 --> 00:21:33,168
question should be how does active

421
00:21:33,168 --> 00:21:33,168
inference or the notion of inference,

422
00:21:33,168 --> 00:21:33,168
inactive inference literature applies to

423
00:21:33,168 --> 00:21:54,189
all of those situations.

424
00:21:55,190 --> 00:21:59,194
So one way to do that is to define a kind

425
00:21:59,194 --> 00:21:59,194
of sparse coupling between the

426
00:21:59,194 --> 00:21:59,194
environment and the agent and define

427
00:21:59,194 --> 00:21:59,194
variational density as the internal

428
00:21:59,194 --> 00:22:13,202
states of the system.

429
00:22:13,202 --> 00:22:17,206
And also, obviously, the external states

430
00:22:17,206 --> 00:22:17,206
of the environment would be the states

431
00:22:17,206 --> 00:22:17,206
that needs to be tracked by those

432
00:22:17,206 --> 00:22:24,213
variational densities.

433
00:22:25,214 --> 00:22:29,218
And in this case, there isn't anything

434
00:22:29,218 --> 00:22:29,218
inherently different between the way the

435
00:22:29,218 --> 00:22:29,218
brains or, I don't know, even sentient

436
00:22:29,218 --> 00:22:29,218
agents somehow undertake inference from

437
00:22:29,218 --> 00:22:29,218
the way that the inert rocks can

438
00:22:29,218 --> 00:22:29,218
partition their states into internal and

439
00:22:29,218 --> 00:22:53,242
external states.

440
00:22:53,242 --> 00:22:57,246
But the main difference would be the way

441
00:22:57,246 --> 00:22:57,246
the Markov blanket in those simpler

442
00:22:57,246 --> 00:22:57,246
systems can act as a kind of statistical

443
00:22:57,246 --> 00:22:57,246
boundary that allows for non causal

444
00:22:57,246 --> 00:22:57,246
relationships I'm sorry, nonlinear causal

445
00:22:57,246 --> 00:22:57,246
relationships as opposed to linear causal

446
00:22:57,246 --> 00:22:57,246
relationships as opposed to nonlinear

447
00:22:57,246 --> 00:22:57,246
causal relationships as observed in

448
00:22:57,246 --> 00:23:26,269
complex agents or systems.

449
00:23:29,272 --> 00:23:32,275
That's some of the main distinctions

450
00:23:32,275 --> 00:23:33,276
between those.

451
00:23:35,278 --> 00:23:35,278
Daniel: Awesome.

452
00:23:36,279 --> 00:23:37,280
Yeah.

453
00:23:37,280 --> 00:23:40,283
And then the classic paper that we often

454
00:23:40,283 --> 00:23:40,283
return to, like Ollie mentioned, the

455
00:23:40,283 --> 00:23:45,288
inert rock.

456
00:23:45,288 --> 00:23:51,294
So this paper has the kind of visual

457
00:23:51,294 --> 00:23:51,294
taxonomy from simpler to more

458
00:23:51,294 --> 00:23:51,294
sophisticated agents that's by sometimes

459
00:23:51,294 --> 00:23:51,294
the papers are in white, sometimes they'

460
00:23:51,294 --> 00:24:13,310
're in black.

461
00:24:15,312 --> 00:24:19,316
Here inert systems, active, classical

462
00:24:19,316 --> 00:24:19,316
conservative systems, strange systems

463
00:24:19,316 --> 00:24:19,316
with world models of their own and all of

464
00:24:19,316 --> 00:24:27,324
this.

465
00:24:29,326 --> 00:24:32,329
But this is a great response, which is in

466
00:24:32,329 --> 00:24:32,329
a pan computationalist or pan cognitive

467
00:24:32,329 --> 00:24:32,329
pan inferentialist world, then how does

468
00:24:32,329 --> 00:24:40,337
active inference apply?

469
00:24:40,337 --> 00:24:43,340
One other angle is like, let's just think

470
00:24:43,340 --> 00:24:43,340
about the liver or the pancreas and blood

471
00:24:43,340 --> 00:24:47,344
sugar.

472
00:24:47,344 --> 00:24:48,345
That's our system of interest.

473
00:24:49,346 --> 00:24:53,350
Realism is like, is the pancreas actually

474
00:24:53,350 --> 00:24:55,352
doing inference on blood sugar?

475
00:24:55,352 --> 00:24:57,354
And people can have a range of opinions,

476
00:24:57,354 --> 00:24:57,354
but in a pan cognitiveist world, the

477
00:24:57,354 --> 00:25:00,351
answer is yes.

478
00:25:01,352 --> 00:25:04,355
Or one can kind of pull back and just say,

479
00:25:04,355 --> 00:25:04,355
, we're going to model the pancreas as

480
00:25:04,355 --> 00:25:09,360
doing inference on blood sugar.

481
00:25:10,360 --> 00:25:12,363
So it is no different.

482
00:25:12,363 --> 00:25:15,366
It's just an interpretation of the same

483
00:25:15,366 --> 00:25:17,368
statistical apparatus, basically.

484
00:25:18,369 --> 00:25:20,371
But there might be a setting where

485
00:25:20,371 --> 00:25:23,374
scientific realism is more justified.

486
00:25:23,374 --> 00:25:26,377
There's multiple lines of converging

487
00:25:26,377 --> 00:25:26,377
evidence.

488
00:25:27,378 --> 00:25:30,381
The model is comprehensive and being used

489
00:25:30,381 --> 00:25:30,381
to generate unique explanations and

490
00:25:30,381 --> 00:25:30,381
predictions versus, like, we're just

491
00:25:30,381 --> 00:25:30,381
going to do a linear relationship between

492
00:25:30,381 --> 00:25:42,393
these two things in the public health.

493
00:25:42,393 --> 00:25:45,396
And so then we're not going to confuse

494
00:25:45,396 --> 00:25:45,396
the linear model with those with the

495
00:25:45,396 --> 00:25:49,400
actual causal architecture.

496
00:25:49,400 --> 00:25:51,402
But there are situations that you can

497
00:25:51,402 --> 00:25:51,402
design or analyze where the sparsity

498
00:25:51,402 --> 00:25:51,402
structure of the system, which as early

499
00:25:51,402 --> 00:25:51,402
points to is what grants it all these

500
00:25:51,402 --> 00:26:01,406
interesting properties.

501
00:26:01,406 --> 00:26:04,409
It is becoming known to an extent that

502
00:26:04,409 --> 00:26:04,409
within two or three or four sigma, it's

503
00:26:04,409 --> 00:26:04,409
like we're starting to talk about how it

504
00:26:04,409 --> 00:26:04,409
is not mistaking the map for the

505
00:26:04,409 --> 00:26:04,409
territory, that's the map territory

506
00:26:04,409 --> 00:26:04,409
fallacy, but also not doing some kind of

507
00:26:04,409 --> 00:26:04,409
map denialism, which is the map territory

508
00:26:04,409 --> 00:26:25,430
fallacy, fallacy.

509
00:26:28,433 --> 00:26:31,436
And that's the paper which I'll add into

510
00:26:31,436 --> 00:26:33,438
by Maxwell et al.

511
00:26:36,441 --> 00:26:39,444
Andrew: And also, if I may add another

512
00:26:39,444 --> 00:26:39,444
point with regard to the distinction

513
00:26:39,444 --> 00:26:44,449
between realism and instrumentalism.

514
00:26:46,451 --> 00:26:51,456
I believe, at least in the realism school,

515
00:26:51,456 --> 00:26:51,456
, probably the most relevant or well

516
00:26:51,456 --> 00:26:51,456
situated stance to reframe active

517
00:26:51,456 --> 00:27:00,459
inference.

518
00:27:02,461 --> 00:27:06,465
I mean, reframe its ontological status is

519
00:27:06,465 --> 00:27:10,469
structural realism, as argued by Majid.

520
00:27:10,469 --> 00:27:13,472
Benny and others in several papers.

521
00:27:15,474 --> 00:27:15,474
Daniel: Nice.

522
00:27:16,475 --> 00:27:17,476
Yes.

523
00:27:18,477 --> 00:27:22,481
He's joined several discussions and it's

524
00:27:22,481 --> 00:27:22,481
great to okay, so there's some quotes

525
00:27:22,481 --> 00:27:22,481
from the book, but yes, basically, forget

526
00:27:22,481 --> 00:27:22,481
organs other than the brain, how could

527
00:27:22,481 --> 00:27:37,496
anything be doing inference?

528
00:27:37,496 --> 00:27:40,499
And then another angle to take is just

529
00:27:40,499 --> 00:27:40,499
like the way that a baseball computes a

530
00:27:40,499 --> 00:27:44,503
parabola.

531
00:27:45,504 --> 00:27:49,507
You don't need to think that it's doing

532
00:27:49,507 --> 00:27:49,507
it like a calculator, but that's kind of

533
00:27:49,507 --> 00:27:55,514
like naturalizing the computation.

534
00:27:56,515 --> 00:27:58,517
That's a path of least action in a

535
00:27:58,517 --> 00:27:59,518
spatial space.

536
00:28:00,513 --> 00:28:02,515
But also you could have a path of least

537
00:28:02,515 --> 00:28:04,517
action in a cognitive space.

538
00:28:04,517 --> 00:28:06,519
That's Bayesian Mechanics.

539
00:28:09,522 --> 00:28:10,523
And then you could have more of a realist

540
00:28:10,523 --> 00:28:12,525
or more of an instrumentalist angle.

541
00:28:12,525 --> 00:28:13,526
Yeah, please.

542
00:28:14,527 --> 00:28:15,528
Andrew: Sorry.

543
00:28:15,528 --> 00:28:20,533
Just one thing that somehow I mean, gets

544
00:28:20,533 --> 00:28:20,533
confused is some sometimes people think

545
00:28:20,533 --> 00:28:20,533
FEP's claim is that, okay, so if a self

546
00:28:20,533 --> 00:28:20,533
organizing if FEP formalism applies to a

547
00:28:20,533 --> 00:28:20,533
self organizing system, then it should

548
00:28:20,533 --> 00:28:20,533
persist through time by preserving its

549
00:28:20,533 --> 00:28:47,559
markup blankets intact.

550
00:28:47,560 --> 00:28:50,563
But actually the claim of the FEP is the

551
00:28:50,563 --> 00:28:52,565
other way around.

552
00:28:52,565 --> 00:28:56,569
So its main assertion is that if anything

553
00:28:56,569 --> 00:28:56,569
persists through time, how does FEP

554
00:28:56,569 --> 00:29:01,568
applies to it?

555
00:29:01,568 --> 00:29:04,571
So in other sense, FEP doesn't provide

556
00:29:04,571 --> 00:29:04,571
any justification per se for describing

557
00:29:04,571 --> 00:29:04,571
the way that the system persists through

558
00:29:04,571 --> 00:29:12,579
time.

559
00:29:12,579 --> 00:29:16,583
But we take the persistence of the system

560
00:29:16,583 --> 00:29:16,583
through time as the premise and then

561
00:29:16,583 --> 00:29:16,583
apply FEP to somehow investigate the

562
00:29:16,583 --> 00:29:16,583
implications of that premise through

563
00:29:16,583 --> 00:29:31,598
Bayesian mechanics and FEP formalism.

564
00:29:33,600 --> 00:29:36,603
Daniel: Yeah, without the implicit or

565
00:29:36,603 --> 00:29:36,603
explicit acknowledgment of the persistent

566
00:29:36,603 --> 00:29:36,603
existence of what you're measuring or

567
00:29:36,603 --> 00:29:36,603
yourself, you don't even have something

568
00:29:36,603 --> 00:29:48,615
to discuss.

569
00:29:49,616 --> 00:29:52,619
And a lot of times when people just start

570
00:29:52,619 --> 00:29:52,619
to specify what they mean by the nouns

571
00:29:52,619 --> 00:29:52,619
and the verbs and the adjectives they use,

572
00:29:52,619 --> 00:30:00,621
, that's a deflationary approach.

573
00:30:01,622 --> 00:30:03,624
What if there's something that comes into

574
00:30:03,624 --> 00:30:03,624
being so fast in between moments and it's

575
00:30:03,624 --> 00:30:03,624
not being measured and you can't detect

576
00:30:03,624 --> 00:30:06,627
it?

577
00:30:06,627 --> 00:30:08,629
It's like, well, then how do you know it'

578
00:30:08,629 --> 00:30:08,629
's there?

579
00:30:11,632 --> 00:30:13,634
That's not within that statistical

580
00:30:13,634 --> 00:30:15,636
sensing apparatus.

581
00:30:15,636 --> 00:30:20,641
So you have to take the existence of the

582
00:30:20,641 --> 00:30:20,641
thing to be primary in modeling some

583
00:30:20,641 --> 00:30:25,646
thing.

584
00:30:28,649 --> 00:30:30,651
And Axle Constant has a lot of work in

585
00:30:30,651 --> 00:30:31,652
that area.

586
00:30:35,656 --> 00:30:39,660
Andrew: Daniel, could you please pull up

587
00:30:39,660 --> 00:30:39,660
the path integral paper on page four, if

588
00:30:39,660 --> 00:30:44,665
I may?

589
00:30:49,670 --> 00:30:50,671
Sorry.

590
00:30:51,671 --> 00:30:53,674
Daniel: Yep, here we nice.

591
00:30:53,674 --> 00:30:54,675
Andrew: Thank you.

592
00:30:54,675 --> 00:30:55,676
Exactly right there.

593
00:30:55,676 --> 00:30:58,679
The FTP addresses the following question.

594
00:30:58,679 --> 00:30:58,679
.

595
00:31:00,675 --> 00:31:02,677
Daniel: And it's like, well, if we're

596
00:31:02,677 --> 00:31:02,677
talking about something that exists even

597
00:31:02,677 --> 00:31:02,677
in a mental geometry, that's what we want.

598
00:31:02,677 --> 00:31:07,682
.

599
00:31:08,683 --> 00:31:11,686
We want to start with if it exists, even

600
00:31:11,686 --> 00:31:12,687
hypothetically.

601
00:31:14,689 --> 00:31:16,691
So people say, can it really describe all

602
00:31:16,691 --> 00:31:16,691
things?

603
00:31:16,691 --> 00:31:18,693
It's like, well, all the things that

604
00:31:18,693 --> 00:31:20,694
exist or could exist.

605
00:31:20,695 --> 00:31:22,697
We could use this modeling approach to

606
00:31:22,697 --> 00:31:22,697
model and talk about what properties it

607
00:31:22,697 --> 00:31:26,701
must possess.

608
00:31:27,702 --> 00:31:29,704
But if you're willing to open the door to

609
00:31:29,704 --> 00:31:29,704
things that don't or can't exist, then of

610
00:31:29,704 --> 00:31:29,704
course the sky's the limit on what

611
00:31:29,704 --> 00:31:29,704
properties they do or don't possess,

612
00:31:29,704 --> 00:31:37,712
because they might.

613
00:31:37,712 --> 00:31:40,715
It's like the non elephant animals.

614
00:31:40,715 --> 00:31:43,718
Things might not exist for a huge variety

615
00:31:43,718 --> 00:31:44,719
of reasons.

616
00:31:44,719 --> 00:31:47,722
However, things that exist, either from

617
00:31:47,722 --> 00:31:47,722
an external measure measurement or from

618
00:31:47,722 --> 00:31:47,722
self measurement, they must be acting as

619
00:31:47,722 --> 00:31:47,722
if they're self evidencing, reducing

620
00:31:47,722 --> 00:31:47,722
surprise relative to a generative model

621
00:31:47,722 --> 00:31:47,722
path of least action through an

622
00:31:47,722 --> 00:32:02,731
information geometric space.

623
00:32:04,733 --> 00:32:06,735
If it isn't doing that, you're not going

624
00:32:06,735 --> 00:32:07,736
to observe it.

625
00:32:08,737 --> 00:32:10,739
And that's where we get the particular

626
00:32:10,739 --> 00:32:10,739
partition and the particular partition in

627
00:32:10,739 --> 00:32:15,744
the particular physics.

628
00:32:15,744 --> 00:32:19,748
Bayesian mechanics since Karl Friston's

629
00:32:19,748 --> 00:32:19,748
2019 monograph, free Energy Principle for

630
00:32:19,748 --> 00:32:19,748
a Particular Physics, that was like a big

631
00:32:19,748 --> 00:32:19,748
inflection point in this line of research.

632
00:32:19,748 --> 00:32:31,760
.

633
00:32:33,761 --> 00:32:36,765
Whereas in the 2016 to 2018, there's a

634
00:32:36,765 --> 00:32:36,765
lot of qualitative work and the

635
00:32:36,765 --> 00:32:36,765
philosophical work applying to multiscale

636
00:32:36,765 --> 00:32:36,765
and nested systems and self organization

637
00:32:36,765 --> 00:32:36,765
amidst other empirical simulations

638
00:32:36,765 --> 00:32:36,765
happening continually in all of this

639
00:32:36,765 --> 00:32:36,765
reading works of different times, like

640
00:32:36,765 --> 00:32:36,765
different pieces are kind of going to be

641
00:32:36,765 --> 00:33:00,783
made clear or not.

642
00:33:08,791 --> 00:33:12,795
What is the role of precision in nested

643
00:33:12,795 --> 00:33:12,795
models?

644
00:33:19,802 --> 00:33:22,805
Well, here and also by way of

645
00:33:22,805 --> 00:33:22,805
demonstrating the transferability of

646
00:33:22,805 --> 00:33:22,805
active inference generative models, these

647
00:33:22,805 --> 00:33:22,805
three figures are discussed in live

648
00:33:22,805 --> 00:33:34,817
stream 28 from the original paper.

649
00:33:34,817 --> 00:33:35,817
Smith et al.

650
00:33:35,818 --> 00:33:36,819
Computational phenomenology.

651
00:33:37,820 --> 00:33:40,823
So this is a static perceptual Bayesian

652
00:33:40,823 --> 00:33:41,824
model.

653
00:33:41,824 --> 00:33:44,827
We have hidden state observations and we

654
00:33:44,827 --> 00:33:44,827
hear as a modulator, literally a

655
00:33:44,827 --> 00:33:44,827
neuromodulator on the A matrix Taylor two

656
00:33:44,827 --> 00:33:44,827
densities recognition matrix generative

657
00:33:44,827 --> 00:33:53,836
model.

658
00:33:53,836 --> 00:33:55,838
You have the precision.

659
00:33:55,838 --> 00:33:58,841
So precision is one over the temperature.

660
00:33:58,841 --> 00:33:58,841
.

661
00:33:58,841 --> 00:34:02,839
So whether you think of it as like higher

662
00:34:02,839 --> 00:34:02,839
temperature is lower precision or lower

663
00:34:02,839 --> 00:34:09,846
or vice versa, they're the same.

664
00:34:10,847 --> 00:34:12,849
When your precision is low, your

665
00:34:12,849 --> 00:34:15,852
temperature is high, a gets blurred out.

666
00:34:16,853 --> 00:34:19,856
The high temperature limit is like A

667
00:34:19,856 --> 00:34:19,856
becomes flattened, whereas in the low

668
00:34:19,856 --> 00:34:25,862
temperature limit, A becomes sharpened.

669
00:34:26,863 --> 00:34:29,866
And so this is a common method in

670
00:34:29,866 --> 00:34:32,869
recognition modeling.

671
00:34:32,869 --> 00:34:33,870
Okay?

672
00:34:33,870 --> 00:34:37,873
Now they move into adding action in.

673
00:34:37,874 --> 00:34:38,875
So here there's the policy.

674
00:34:38,875 --> 00:34:41,878
Figure 4.3, figure 7.3, everything we've

675
00:34:41,878 --> 00:34:42,879
looked at.

676
00:34:43,880 --> 00:34:48,885
And then they move to this nested model.

677
00:34:48,885 --> 00:34:52,889
Now, in the Sandbed Smith paper, it was

678
00:34:52,889 --> 00:34:52,889
being mobilized in terms of basically

679
00:34:52,889 --> 00:34:52,889
phenomenology meditative experiences,

680
00:34:52,889 --> 00:34:52,889
direct sensory perception, visual

681
00:34:52,889 --> 00:34:52,889
perception and isocades attention,

682
00:34:52,889 --> 00:34:52,889
unreportable attention and then awareness

683
00:34:52,889 --> 00:35:12,903
at a third level, observing that.

684
00:35:12,903 --> 00:35:16,907
And so you can add an uncertainty, like a

685
00:35:16,907 --> 00:35:18,909
precision on any variable.

686
00:35:19,910 --> 00:35:21,912
You could have an uncertainty on your

687
00:35:21,912 --> 00:35:21,912
prior d.

688
00:35:21,912 --> 00:35:23,914
You could have uncertainty on all kinds

689
00:35:23,914 --> 00:35:24,915
of things.

690
00:35:24,915 --> 00:35:26,917
But sometimes uncertainties on specific

691
00:35:26,917 --> 00:35:26,917
variables become associated with certain

692
00:35:26,917 --> 00:35:30,921
cognitive phenomena.

693
00:35:31,922 --> 00:35:33,924
In this case, they're using it to

694
00:35:33,924 --> 00:35:33,924
describe introspective and explainable AI

695
00:35:33,924 --> 00:35:33,924
systems, taking the exact same figures,

696
00:35:33,924 --> 00:35:33,924
exact same formalisms, just moving them

697
00:35:33,924 --> 00:35:45,936
into the AI setting.

698
00:35:46,937 --> 00:35:48,939
So precision on A and here's that live

699
00:35:48,939 --> 00:35:51,942
stream 28 and the slides and everything.

700
00:35:51,942 --> 00:35:58,949
Precision on A is like sensory ambiguity,

701
00:35:58,949 --> 00:36:01,946
precision on G.

702
00:36:01,946 --> 00:36:07,952
So how precise are you on the free energy

703
00:36:07,952 --> 00:36:07,952
that is associated with the affect in

704
00:36:07,952 --> 00:36:16,961
this sophisticated affect?

705
00:36:18,963 --> 00:36:20,965
Here's some discussion on Embodiment and

706
00:36:20,965 --> 00:36:22,967
the Alexander technique.

707
00:36:24,968 --> 00:36:26,971
So, suffice to say, precision and nested

708
00:36:26,971 --> 00:36:26,971
modeling is basically like the general

709
00:36:26,971 --> 00:36:26,971
precision concept, which is basically

710
00:36:26,971 --> 00:36:33,978
used everywhere.

711
00:36:33,978 --> 00:36:35,980
This is how we fit Bayesian models.

712
00:36:36,980 --> 00:36:38,983
We have some hyper prior distribution on

713
00:36:38,983 --> 00:36:39,984
a parameter.

714
00:36:40,985 --> 00:36:44,989
And if that prior distribution is too

715
00:36:44,989 --> 00:36:44,989
sharp, then it's a pathology of the prior.

716
00:36:44,989 --> 00:36:48,993
.

717
00:36:48,993 --> 00:36:49,994
It's too precise.

718
00:36:50,995 --> 00:36:54,999
If it was too imprecise, you have an

719
00:36:54,999 --> 00:36:56,000
underfit model.

720
00:36:56,001 --> 00:36:57,002
So it's like that.

721
00:36:57,002 --> 00:37:01,000
And especially in nested models,

722
00:37:01,000 --> 00:37:01,000
precision plays a role in kind of gating.

723
00:37:01,000 --> 00:37:05,004
.

724
00:37:05,004 --> 00:37:06,005
Just in this case, it's a general

725
00:37:06,005 --> 00:37:07,006
question.

726
00:37:07,006 --> 00:37:08,007
So you could make it do anything,

727
00:37:08,007 --> 00:37:09,008
basically.

728
00:37:09,008 --> 00:37:12,011
But here the precision on A is like

729
00:37:12,011 --> 00:37:12,011
gating, let's just say the first and the

730
00:37:12,011 --> 00:37:16,015
third level.

731
00:37:17,016 --> 00:37:20,019
So if there's no attention being paid to

732
00:37:20,019 --> 00:37:20,019
sensory input, don't be surprised that

733
00:37:20,019 --> 00:37:25,024
they don't come up to attention.

734
00:37:26,025 --> 00:37:29,028
But if this gating can be like super

735
00:37:29,028 --> 00:37:29,028
direct and forceful, then you'll have

736
00:37:29,028 --> 00:37:29,028
propagation of informational causality on

737
00:37:29,028 --> 00:37:39,038
this network up to the higher level.

738
00:37:52,051 --> 00:37:54,053
Page 113 in the textbook.

739
00:38:04,057 --> 00:38:06,059
All right, the decision this is about

740
00:38:06,059 --> 00:38:07,060
planning as inference.

741
00:38:07,060 --> 00:38:12,065
Which one of these questions was also

742
00:38:12,065 --> 00:38:12,065
about the decision whether to model

743
00:38:12,065 --> 00:38:12,065
alternative futures, counterfactual

744
00:38:12,065 --> 00:38:12,065
futures conditioned upon policy selection.

745
00:38:12,065 --> 00:38:25,078
.

746
00:38:25,078 --> 00:38:27,080
How would the world change if I did this

747
00:38:27,080 --> 00:38:27,080
or that?

748
00:38:27,080 --> 00:38:29,082
Is largely tied up with the choice

749
00:38:29,082 --> 00:38:29,082
between discrete and continuous models

750
00:38:29,082 --> 00:38:29,082
because the idea of selecting between

751
00:38:29,082 --> 00:38:29,082
alternative futures defined by sequences

752
00:38:29,082 --> 00:38:29,082
of actions is more simply articulated

753
00:38:29,082 --> 00:38:39,092
using discrete time models.

754
00:38:39,092 --> 00:38:45,098
All right, so figure 4.3.

755
00:38:50,103 --> 00:38:53,105
Figure 4.3 is like the rosetta stone.

756
00:38:53,106 --> 00:38:54,107
It juxtaposes the discrete time

757
00:38:54,107 --> 00:38:54,107
continuous discrete time generative model

758
00:38:54,107 --> 00:38:54,107
and the continuous time generative model.

759
00:38:54,107 --> 00:38:59,112
.

760
00:38:59,112 --> 00:39:01,108
So I'll just copy this.

761
00:39:03,110 --> 00:39:06,113
In the discrete time model, you have st

762
00:39:06,113 --> 00:39:08,115
minus one s of t, s of t plus one.

763
00:39:09,116 --> 00:39:11,118
So futures and pasts, if it's

764
00:39:11,118 --> 00:39:11,118
sophisticated, active inference, are

765
00:39:11,118 --> 00:39:15,122
being explicitly modeled.

766
00:39:16,122 --> 00:39:19,126
So it's like what would happen at 07:00

767
00:39:19,126 --> 00:39:20,127
if I did this?

768
00:39:21,128 --> 00:39:23,130
That is explicitly addressable with a

769
00:39:23,130 --> 00:39:23,130
discrete time model of the correct time

770
00:39:23,130 --> 00:39:26,133
horizon.

771
00:39:26,133 --> 00:39:28,135
So you have to explicitly say, I'm

772
00:39:28,135 --> 00:39:28,135
talking about an hourly model and seven

773
00:39:28,135 --> 00:39:31,138
depth.

774
00:39:31,138 --> 00:39:33,140
And so then in branching time active

775
00:39:33,140 --> 00:39:33,140
inference, just like a chess algorithm,

776
00:39:33,140 --> 00:39:33,140
it's dealing with the branching structure

777
00:39:33,140 --> 00:39:33,140
because if you have a lot of affordances

778
00:39:33,140 --> 00:39:33,140
and you have a lot of time depth, you can

779
00:39:33,140 --> 00:39:33,140
imagine this is a combinatorial explosion.

780
00:39:33,140 --> 00:39:46,153
.

781
00:39:48,155 --> 00:39:51,158
In contrast, though these two generative

782
00:39:51,158 --> 00:39:51,158
models are shown here to emphasize their

783
00:39:51,158 --> 00:39:51,158
structural similarity, the continuous

784
00:39:51,158 --> 00:39:51,158
time generative model is actually not

785
00:39:51,158 --> 00:39:51,158
explicitly modeling past, present, future

786
00:39:51,158 --> 00:40:04,165
time steps.

787
00:40:04,165 --> 00:40:07,168
Rather, it's modeling the generalized

788
00:40:07,168 --> 00:40:07,168
coordinates of motion x hidden state x

789
00:40:07,168 --> 00:40:07,168
prime rate of change, x double prime

790
00:40:07,168 --> 00:40:14,175
second derivative.

791
00:40:14,175 --> 00:40:17,178
So in that way, it's a lot more like a

792
00:40:17,178 --> 00:40:18,179
Taylor series expansion.

793
00:40:18,179 --> 00:40:21,182
So a Taylor series expansion, technically

794
00:40:21,182 --> 00:40:21,182
even a Taylor series expansion of depth

795
00:40:21,182 --> 00:40:21,182
one, it has an answer for every single

796
00:40:21,182 --> 00:40:21,182
point in the number line, but that doesn'

797
00:40:21,182 --> 00:40:31,192
't mean it's a good one.

798
00:40:32,193 --> 00:40:35,196
And so continuous time models kind of

799
00:40:35,196 --> 00:40:35,196
trivially have something to say about all

800
00:40:35,196 --> 00:40:35,196
possible time steps just by analytic

801
00:40:35,196 --> 00:40:35,196
continuation of the Taylor series

802
00:40:35,196 --> 00:40:35,196
expansion in the generalized coordinates

803
00:40:35,196 --> 00:40:49,210
of motion way.

804
00:40:50,211 --> 00:40:53,214
However, you don't necessarily know how

805
00:40:53,214 --> 00:40:56,217
accurate it is for any given point.

806
00:40:58,219 --> 00:41:01,216
It also doesn't have explicit

807
00:41:01,216 --> 00:41:02,217
counterfactuals.

808
00:41:03,218 --> 00:41:06,221
So if you do some Taylor series expansion,

809
00:41:06,221 --> 00:41:06,221
, it says yeah, at x equals three over y

810
00:41:06,221 --> 00:41:09,224
is five.

811
00:41:10,225 --> 00:41:12,227
But then if you say, well, what if it

812
00:41:12,227 --> 00:41:13,228
were different?

813
00:41:13,228 --> 00:41:15,230
The only way you could do that would be

814
00:41:15,230 --> 00:41:15,230
change the parameters of the Taylor

815
00:41:15,230 --> 00:41:20,235
series and recalculate it at that point.

816
00:41:21,236 --> 00:41:25,240
So the model itself is not exactly doing

817
00:41:25,240 --> 00:41:25,240
these.

818
00:41:25,240 --> 00:41:27,242
Like what would happen if I did that?

819
00:41:31,246 --> 00:41:35,250
So futures are explicit in discrete time

820
00:41:35,250 --> 00:41:35,250
models and interpretable explicitly,

821
00:41:35,250 --> 00:41:35,250
whereas in continuous time models, pasts

822
00:41:35,250 --> 00:41:35,250
and futures are kind of trivially

823
00:41:35,250 --> 00:41:35,250
predicted such that it doesn't really

824
00:41:35,250 --> 00:41:35,250
make sense to talk about counterfactuals

825
00:41:35,250 --> 00:41:55,270
in the same exact way.

826
00:41:59,274 --> 00:42:03,272
And these models can be hybridized and

827
00:42:03,272 --> 00:42:07,276
fused, which is in chapter eight.

828
00:42:16,285 --> 00:42:18,287
Can we generate a code of template?

829
00:42:19,288 --> 00:42:20,289
Looks like yes, but there's of course

830
00:42:20,289 --> 00:42:21,290
more information.

831
00:42:22,291 --> 00:42:25,294
But it's an important question.

832
00:42:33,302 --> 00:42:35,304
How is temporal depth specific to

833
00:42:35,304 --> 00:42:36,305
planning?

834
00:42:36,305 --> 00:42:38,307
Does temporal depth in perception make

835
00:42:38,307 --> 00:42:39,308
any sense?

836
00:42:40,309 --> 00:42:41,310
What is temporal depth?

837
00:42:42,311 --> 00:42:44,313
So temporal depth is how many timesteps

838
00:42:44,313 --> 00:42:44,313
the model is considered in the discrete

839
00:42:44,313 --> 00:42:47,316
time case.

840
00:42:48,317 --> 00:42:50,319
Does temporal depth make sense in

841
00:42:50,319 --> 00:42:51,320
perception?

842
00:42:52,321 --> 00:42:55,323
Well, perception is always modeled as

843
00:42:55,323 --> 00:42:56,325
instantaneous.

844
00:42:56,325 --> 00:43:01,324
However, depending on the temporal scale

845
00:43:01,324 --> 00:43:01,324
of a model, that perception may be

846
00:43:01,324 --> 00:43:01,324
instantaneous over a certain temporal

847
00:43:01,324 --> 00:43:11,334
thickness power.

848
00:43:11,334 --> 00:43:13,336
Why is temporal depth specific to

849
00:43:13,336 --> 00:43:14,337
planning?

850
00:43:14,337 --> 00:43:17,340
So in planning as inference, policy

851
00:43:17,340 --> 00:43:17,340
selection as inference, you're selecting

852
00:43:17,340 --> 00:43:17,340
or sampling from policies based upon

853
00:43:17,340 --> 00:43:24,346
their expected free energy.

854
00:43:27,350 --> 00:43:30,353
In order to plan over a given time

855
00:43:30,353 --> 00:43:30,353
horizon and actually evaluate the

856
00:43:30,353 --> 00:43:30,353
playouts, you need to have a temporal

857
00:43:30,353 --> 00:43:38,360
depth of that amount.

858
00:43:40,363 --> 00:43:47,370
Thomas Parr in his bookstream so early

859
00:43:47,370 --> 00:43:47,370
implementations were coming from the

860
00:43:47,370 --> 00:43:59,381
generalized filtering approach.

861
00:43:59,382 --> 00:44:00,377
Maybe it wasn't generalized at that point,

862
00:44:00,377 --> 00:44:01,378
, but particle filtering approach.

863
00:44:01,378 --> 00:44:04,381
And then quipped these models with action.

864
00:44:04,381 --> 00:44:04,381
.

865
00:44:04,381 --> 00:44:06,383
Then people started thinking about how to

866
00:44:06,383 --> 00:44:07,384
get sequential dynamics.

867
00:44:07,384 --> 00:44:10,387
Predator prey lock of voltera models

868
00:44:10,387 --> 00:44:10,387
winnerless competition neural Darwinism

869
00:44:10,387 --> 00:44:10,387
in these situations, there's an emergence

870
00:44:10,387 --> 00:44:10,387
of sequential recurrent continuous

871
00:44:10,387 --> 00:44:10,387
dynamics, like a limit cycle of more than

872
00:44:10,387 --> 00:44:21,398
two.

873
00:44:22,399 --> 00:44:26,403
Then people wanted to model explicit long

874
00:44:26,403 --> 00:44:28,405
term planning.

875
00:44:28,405 --> 00:44:31,408
And so from around 2014 or something,

876
00:44:31,408 --> 00:44:31,408
there was a lot more development in the

877
00:44:31,408 --> 00:44:31,408
discrete state space model, which enabled

878
00:44:31,408 --> 00:44:31,408
the well understood, partially observed

879
00:44:31,408 --> 00:44:31,408
Markov decision process and a lot of the

880
00:44:31,408 --> 00:44:31,408
characterization of planning as inference

881
00:44:31,408 --> 00:44:49,426
explore exploit.

882
00:44:52,428 --> 00:44:56,433
Then later developments supported nested

883
00:44:56,433 --> 00:44:57,434
models.

884
00:44:57,434 --> 00:44:59,436
Here there's a discrete time on top and a

885
00:44:59,436 --> 00:45:01,432
discrete time on the bottom.

886
00:45:01,432 --> 00:45:04,435
But also it could be continuous time on

887
00:45:04,435 --> 00:45:04,435
the bottom.

888
00:45:05,436 --> 00:45:06,437
Oh yes.

889
00:45:06,437 --> 00:45:09,440
Then continuous state models were

890
00:45:09,440 --> 00:45:09,440
reintroduced as the lower levels of

891
00:45:09,440 --> 00:45:15,446
higher level discrete time models.

892
00:45:15,446 --> 00:45:18,449
And that is kind of like the folk

893
00:45:18,449 --> 00:45:18,449
psychology Livestream 46 active inference

894
00:45:18,449 --> 00:45:24,455
does not contradict folk psychology.

895
00:45:25,456 --> 00:45:28,459
Discrete time decision making up top,

896
00:45:28,459 --> 00:45:28,459
discrete time and discrete decisions and

897
00:45:28,459 --> 00:45:33,464
then more in the sensory motor.

898
00:45:33,464 --> 00:45:35,466
It's more like continuous time,

899
00:45:35,466 --> 00:45:36,467
continuous action.

900
00:45:49,480 --> 00:46:02,487
Okay, new question, figure 6.1.

901
00:46:09,494 --> 00:46:14,499
All right, particular partition.

902
00:46:19,504 --> 00:46:20,505
How is the mutual interaction between

903
00:46:20,505 --> 00:46:20,505
active states and sensory states meant in

904
00:46:20,505 --> 00:46:23,508
six one?

905
00:46:23,508 --> 00:46:26,511
So the bi directional line here, can they

906
00:46:26,511 --> 00:46:26,511
mutually change their states without

907
00:46:26,511 --> 00:46:26,511
impacting neither internal nor external

908
00:46:26,511 --> 00:46:33,517
states?

909
00:46:33,518 --> 00:46:34,519
Yes, they could.

910
00:46:35,520 --> 00:46:38,523
You could have nodes that these nodes are

911
00:46:38,523 --> 00:46:41,526
in communication with each other.

912
00:46:42,527 --> 00:46:44,529
In general, this image, it's more

913
00:46:44,529 --> 00:46:46,531
important what it doesn't show.

914
00:46:46,531 --> 00:46:48,533
So there's no backwards arrow from

915
00:46:48,533 --> 00:46:48,533
external to active or from internal to

916
00:46:48,533 --> 00:46:50,535
sensory.

917
00:46:50,535 --> 00:46:53,538
You can't have your thumb on the scale

918
00:46:53,538 --> 00:46:53,538
and the symmetry of whatever that is from

919
00:46:53,538 --> 00:46:56,541
the outside.

920
00:46:57,542 --> 00:47:00,539
And the only other constraint is no

921
00:47:00,539 --> 00:47:01,540
telepathy.

922
00:47:01,540 --> 00:47:03,542
No telekinesis.

923
00:47:03,542 --> 00:47:05,544
The only way to receive information about

924
00:47:05,544 --> 00:47:05,544
external states is through sensory states.

925
00:47:05,544 --> 00:47:07,546
.

926
00:47:07,546 --> 00:47:09,548
The only way to act on costly intervene

927
00:47:09,548 --> 00:47:09,548
on external states is through active

928
00:47:09,548 --> 00:47:12,551
states.

929
00:47:12,551 --> 00:47:17,556
So no thumb on the scale and the mirror,

930
00:47:17,556 --> 00:47:19,558
no telepathy.

931
00:47:19,558 --> 00:47:20,559
No telekinesis.

932
00:47:21,560 --> 00:47:23,562
That doesn't mean that for any given

933
00:47:23,562 --> 00:47:23,562
model, these arrows are like all

934
00:47:23,562 --> 00:47:23,562
important or all relative or relatively

935
00:47:23,562 --> 00:47:31,570
of the same importance.

936
00:47:32,570 --> 00:47:34,573
So if you want to design a computer

937
00:47:34,573 --> 00:47:34,573
system where the sensory states comes in

938
00:47:34,573 --> 00:47:34,573
one computer, and this is a second

939
00:47:34,573 --> 00:47:34,573
computer and this is the third computer,

940
00:47:34,573 --> 00:47:34,573
so like there's no direct edge between

941
00:47:34,573 --> 00:47:34,573
sensory and active states, you can create

942
00:47:34,573 --> 00:47:47,586
that causal architecture.

943
00:47:47,586 --> 00:47:48,587
What is it supposed.

944
00:47:48,587 --> 00:47:49,588
To model nothing.

945
00:47:50,589 --> 00:47:54,593
Modeling of supposed things is done in

946
00:47:54,593 --> 00:47:56,595
chapter six and beyond.

947
00:47:56,595 --> 00:47:59,598
But in general, this is not trying to

948
00:47:59,598 --> 00:47:59,598
model any specific situation despite the

949
00:47:59,598 --> 00:48:02,595
brain in the world.

950
00:48:03,596 --> 00:48:05,598
Could they circle mutually modifying

951
00:48:05,598 --> 00:48:05,598
their states and then eventually arrive

952
00:48:05,598 --> 00:48:05,598
at a state where they change the external

953
00:48:05,598 --> 00:48:10,603
internal states?

954
00:48:13,606 --> 00:48:15,608
Yeah, maybe they have a faster timescale

955
00:48:15,608 --> 00:48:15,608
or something.

956
00:48:15,608 --> 00:48:18,611
Or maybe they're in communication with

957
00:48:18,611 --> 00:48:18,611
each other and then active states, like

958
00:48:18,611 --> 00:48:18,611
sensory states ends up influencing

959
00:48:18,611 --> 00:48:18,611
internal states via active influencing it.

960
00:48:18,611 --> 00:48:27,620
.

961
00:48:27,620 --> 00:48:29,622
And so yes, it could happen.

962
00:48:29,622 --> 00:48:31,624
Would that mean that we can model with

963
00:48:31,624 --> 00:48:33,626
this trick, any arbitrary behavior?

964
00:48:34,627 --> 00:48:36,629
It's not necessarily a trick, it's just a

965
00:48:36,629 --> 00:48:37,630
particular partition.

966
00:48:38,631 --> 00:48:40,633
Would that allow us to model Turing

967
00:48:40,633 --> 00:48:41,634
equivalents?

968
00:48:41,634 --> 00:48:44,637
I'm not sure if there's an understanding

969
00:48:44,637 --> 00:48:44,637
of what formally demonstrates Turing

970
00:48:44,637 --> 00:48:44,637
equivalents then, but I believe it's

971
00:48:44,637 --> 00:48:50,643
possible.

972
00:48:50,643 --> 00:48:52,645
I think as a Turing architecture, as far

973
00:48:52,645 --> 00:48:52,645
as I understand it, abstracted from the

974
00:48:52,645 --> 00:48:52,645
von Neumann architecture is basically

975
00:48:52,645 --> 00:48:52,645
just saying, like, you can do a Turing

976
00:48:52,645 --> 00:49:00,647
tape.

977
00:49:01,648 --> 00:49:02,649
So I don't see why not.

978
00:49:03,650 --> 00:49:04,651
Why would we need that?

979
00:49:06,653 --> 00:49:09,656
512k ought to be enough for anyone,

980
00:49:09,656 --> 00:49:09,656
right?

981
00:49:12,659 --> 00:49:14,661
I don't know why we need it.

982
00:49:14,661 --> 00:49:15,662
We expect it.

983
00:49:19,666 --> 00:49:21,668
But yeah, these graphs in general are

984
00:49:21,668 --> 00:49:21,668
more like the space of the possible with

985
00:49:21,668 --> 00:49:21,668
the important caveats that were mentioned

986
00:49:21,668 --> 00:49:21,668
with the no thumb on the scale and the

987
00:49:21,668 --> 00:49:21,668
mirror and the no telepathy and no

988
00:49:21,668 --> 00:49:32,679
telkinesis.

989
00:49:32,679 --> 00:49:35,682
But how relevant these edges are or what

990
00:49:35,682 --> 00:49:35,682
systems have what behavior or what

991
00:49:35,682 --> 00:49:35,682
cognitive phenomena are granted by what

992
00:49:35,682 --> 00:49:41,687
dynamics on graphs.

993
00:49:41,688 --> 00:49:44,691
There's just no general answer to those

994
00:49:44,691 --> 00:49:44,691
things because if you do a graph for one

995
00:49:44,691 --> 00:49:49,696
setting, it's going to be different.

996
00:49:54,701 --> 00:49:58,705
What did you mean when you said that any

997
00:49:58,705 --> 00:49:58,705
entree could be ordered with any side

998
00:49:58,705 --> 00:49:58,705
dish like that in this recipe theme that

999
00:49:58,705 --> 00:49:58,705
we're in models that include continuous

1000
00:49:58,705 --> 00:49:58,705
or discrete variables or both, what is

1001
00:49:58,705 --> 00:49:58,705
meant by variable states or

1002
00:49:58,705 --> 00:50:17,718
observations?

1003
00:50:17,718 --> 00:50:20,721
How can states be continuous, as those

1004
00:50:20,721 --> 00:50:22,723
here are probably familiar with?

1005
00:50:23,724 --> 00:50:26,727
States or observations could be like

1006
00:50:26,727 --> 00:50:26,727
discrete, like zero or one or any integer.

1007
00:50:26,727 --> 00:50:30,731
.

1008
00:50:30,731 --> 00:50:32,733
Or it could be a continuous number.

1009
00:50:37,738 --> 00:50:40,741
Computers discretize continuous functions

1010
00:50:40,741 --> 00:50:41,742
to approximate them.

1011
00:50:43,744 --> 00:50:46,747
But there's so many exciting directions

1012
00:50:46,747 --> 00:50:46,747
with unconventional computing, analog

1013
00:50:46,747 --> 00:50:46,747
computing, mem computing, et cetera, that

1014
00:50:46,747 --> 00:50:55,756
there's more to it.

1015
00:50:55,756 --> 00:50:58,759
But just simply it could be any type of

1016
00:50:58,759 --> 00:50:59,759
variable.

1017
00:51:05,760 --> 00:51:08,763
This new question, let's maybe look at

1018
00:51:08,763 --> 00:51:08,763
this one in closing, if the external

1019
00:51:08,763 --> 00:51:08,763
state is unknowable, how can we set up a

1020
00:51:08,763 --> 00:51:08,763
generative process so that's what's

1021
00:51:08,763 --> 00:51:08,763
generating the observations when our

1022
00:51:08,763 --> 00:51:08,763
experience is based solely on the process

1023
00:51:08,763 --> 00:51:08,763
of producing variational free energy

1024
00:51:08,763 --> 00:51:08,763
based on predictions and sensory inputs

1025
00:51:08,763 --> 00:51:35,790
across the markup blanket?

1026
00:51:37,792 --> 00:51:38,793
This person has just described the

1027
00:51:38,793 --> 00:51:39,794
challenge of life.

1028
00:51:41,796 --> 00:51:42,797
I don't think that there is a specific

1029
00:51:42,797 --> 00:51:43,798
answer to that.

1030
00:51:43,798 --> 00:51:45,800
I think this is literally a restatement

1031
00:51:45,800 --> 00:51:46,801
of the particular partition.

1032
00:51:47,802 --> 00:51:51,806
If this was just said, the challenge is

1033
00:51:51,806 --> 00:51:51,806
to given the unknowability, direct

1034
00:51:51,806 --> 00:51:57,812
unknowability of internal states.

1035
00:51:57,812 --> 00:51:59,814
The challenge and the opportunity is to

1036
00:51:59,814 --> 00:51:59,814
set up a generative model based solely on

1037
00:51:59,814 --> 00:51:59,814
the process of reducing free energy,

1038
00:51:59,814 --> 00:51:59,814
based on predictions and sensory inputs

1039
00:51:59,814 --> 00:52:09,818
across the blanket and adaptive action.

1040
00:52:11,820 --> 00:52:14,823
I'm not sure if they meant process or

1041
00:52:14,823 --> 00:52:14,823
model here because setting up a

1042
00:52:14,823 --> 00:52:14,823
generative process is something that the

1043
00:52:14,823 --> 00:52:14,823
human modeler does when they're designing

1044
00:52:14,823 --> 00:52:14,823
a simulation, but not like what the

1045
00:52:14,823 --> 00:52:25,834
animal has to do.

1046
00:52:29,838 --> 00:52:30,839
What I'm trying to get at here is we are

1047
00:52:30,839 --> 00:52:30,839
making an assumption about the generator

1048
00:52:30,839 --> 00:52:34,843
of sensory input when making the model.

1049
00:52:35,843 --> 00:52:38,847
Yes, but that model can never be accurate.

1050
00:52:38,847 --> 00:52:38,847
.

1051
00:52:39,848 --> 00:52:42,851
Well, it absolutely can be accurate.

1052
00:52:42,851 --> 00:52:44,853
It's never going to be a one to one map

1053
00:52:44,853 --> 00:52:44,853
is the territory, but of course it can be

1054
00:52:44,853 --> 00:52:48,857
accurate.

1055
00:52:48,857 --> 00:52:50,859
We don't need to understand have an

1056
00:52:50,859 --> 00:52:50,859
atomic simulation of the sun to have a

1057
00:52:50,859 --> 00:52:50,859
generative process of how many photons

1058
00:52:50,859 --> 00:52:50,859
are going to hit my window tomorrow at

1059
00:52:50,859 --> 00:52:58,866
07:00 a.m..

1060
00:52:59,868 --> 00:53:01,864
So there's a core invalidity in the

1061
00:53:01,864 --> 00:53:01,864
notion that we set up a generative

1062
00:53:01,864 --> 00:53:03,866
process model.

1063
00:53:03,866 --> 00:53:05,868
Not sure what ontology mixing is

1064
00:53:05,868 --> 00:53:05,868
happening, but almost by definition we're

1065
00:53:05,868 --> 00:53:05,868
introducing an error in the model by

1066
00:53:05,868 --> 00:53:05,868
setting up the parameters for the

1067
00:53:05,868 --> 00:53:13,876
generative process.

1068
00:53:13,876 --> 00:53:15,878
The generative model, we remove the

1069
00:53:15,878 --> 00:53:15,878
fundamental aspect of actin, which is the

1070
00:53:15,878 --> 00:53:15,878
dynamical system has only one thing it

1071
00:53:15,878 --> 00:53:21,884
can do and that's to reduce free energy.

1072
00:53:21,884 --> 00:53:24,887
So there's some good points and some

1073
00:53:24,887 --> 00:53:25,888
mixed things.

1074
00:53:27,890 --> 00:53:29,892
What it does is the action it takes in

1075
00:53:29,892 --> 00:53:30,893
the world variational.

1076
00:53:30,893 --> 00:53:33,896
Free energy is just a tractable

1077
00:53:33,896 --> 00:53:35,897
computational heuristic.

1078
00:53:35,898 --> 00:53:37,900
Yes, if you set up the generative process

1079
00:53:37,900 --> 00:53:37,900
to be a number continuous number between

1080
00:53:37,900 --> 00:53:37,900
one and ten and then the generative model

1081
00:53:37,900 --> 00:53:37,900
to do the exact same, to kind of have pre

1082
00:53:37,900 --> 00:53:37,900
structurally learnt the problem, then it'

1083
00:53:37,900 --> 00:53:49,912
's going to be a simple simulation.

1084
00:53:50,913 --> 00:53:51,914
Whereas you could have a more

1085
00:53:51,914 --> 00:53:51,914
sophisticated simulation that involves

1086
00:53:51,914 --> 00:53:51,914
structured learning as part of the agent'

1087
00:53:51,914 --> 00:53:57,920
's generative model.

1088
00:53:58,921 --> 00:54:00,917
Are we not introducing information to the

1089
00:54:00,917 --> 00:54:00,917
model that would not be available in the

1090
00:54:00,917 --> 00:54:00,917
real world if we actually defined the

1091
00:54:00,917 --> 00:54:04,921
generative process?

1092
00:54:04,921 --> 00:54:07,924
Yeah, you can give any model too much

1093
00:54:07,924 --> 00:54:09,926
information, essentially.

1094
00:54:11,928 --> 00:54:14,931
So if the model actually knows what is

1095
00:54:14,931 --> 00:54:14,931
really happening, then if you were doing

1096
00:54:14,931 --> 00:54:14,931
a video game and you had an agent with

1097
00:54:14,931 --> 00:54:14,931
supervisory access to the other players

1098
00:54:14,931 --> 00:54:14,931
inferences or even control their actions,

1099
00:54:14,931 --> 00:54:14,931
yeah, that's not going to work in a

1100
00:54:14,931 --> 00:54:32,949
tournament.

1101
00:54:32,949 --> 00:54:35,952
But here with a particular partition, we

1102
00:54:35,952 --> 00:54:35,952
can actually know the information

1103
00:54:35,952 --> 00:54:38,954
encapsulation.

1104
00:54:38,955 --> 00:54:40,957
And Majid Benny explores that in terms of

1105
00:54:40,957 --> 00:54:42,959
the partial information encapsulation.

1106
00:54:43,960 --> 00:54:45,962
So are we not introducing information to

1107
00:54:45,962 --> 00:54:47,964
the model that would not be available?

1108
00:54:48,965 --> 00:54:50,967
It's your restaurant, do whatever you got

1109
00:54:50,967 --> 00:54:51,968
to do with the recipe.

1110
00:54:52,968 --> 00:54:54,971
If you want a pedagogical example that

1111
00:54:54,971 --> 00:54:54,971
just makes some clean graphs and is very

1112
00:54:54,971 --> 00:54:54,971
straightforward and doesn't engage the

1113
00:54:54,971 --> 00:54:54,971
complexities of like sophisticated

1114
00:54:54,971 --> 00:54:54,971
cognitive structure learning, that model

1115
00:54:54,971 --> 00:55:05,976
is not going to complain.

1116
00:55:05,976 --> 00:55:09,979
If you want to do strange loop reflexive

1117
00:55:09,979 --> 00:55:09,979
structure modeling for ambiguous inputs

1118
00:55:09,979 --> 00:55:09,979
of unstructured multimodal data, et

1119
00:55:09,979 --> 00:55:17,988
cetera, then that is your challenge.

1120
00:55:20,991 --> 00:55:23,994
So for many people it's just one closing

1121
00:55:23,994 --> 00:55:24,995
comment.

1122
00:55:24,995 --> 00:55:25,996
In many people.

1123
00:55:25,996 --> 00:55:27,998
This is the first time they've been

1124
00:55:27,998 --> 00:55:27,998
exposed to statistical modeling, and

1125
00:55:27,998 --> 00:55:30,001
iterative modeling formally.

1126
00:55:30,001 --> 00:55:33,004
So that's why there are often questions

1127
00:55:33,004 --> 00:55:33,004
that are, like, less about active

1128
00:55:33,004 --> 00:55:33,004
inference, but sometimes crop up about

1129
00:55:33,004 --> 00:55:33,004
basically using statistical modeling

1130
00:55:33,004 --> 00:55:33,004
overall, which is a great thing because

1131
00:55:33,004 --> 00:55:33,004
these are challenging and areas with a

1132
00:55:33,004 --> 00:55:47,018
lot of implicit and tacit knowledge.

1133
00:55:50,021 --> 00:55:52,023
Thank you, fellows.

1134
00:55:53,024 --> 00:55:58,029
Looking forward to next discussions and

1135
00:55:58,029 --> 00:55:58,029
into heading into chapter seven next week.

1136
00:55:58,029 --> 00:56:04,029
.

1137
00:56:04,029 --> 00:56:07,032
Ali, maybe we'll do a maybe we'll do our

1138
00:56:07,032 --> 00:56:07,032
zero for chapter seven and eight, but not

1139
00:56:07,032 --> 00:56:11,036
in a hurry.

1140
00:56:11,036 --> 00:56:12,037
But we'll figure it out.

1141
00:56:13,038 --> 00:56:14,039
Andrew: Sure.

1142
00:56:15,040 --> 00:56:16,041
Thank you so much.

1143
00:56:16,041 --> 00:56:17,042
Daniel: Thank you.

1144
00:56:17,042 --> 00:56:18,043
Speaker C: Thank you.

1145
00:56:18,043 --> 00:56:19,044
Daniel: Bye.


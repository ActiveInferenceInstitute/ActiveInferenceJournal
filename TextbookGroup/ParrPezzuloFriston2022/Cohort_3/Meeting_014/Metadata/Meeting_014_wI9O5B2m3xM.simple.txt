SPEAKER_00:
Hello, everyone.

It's June 13th, 23, and we're in cohort three discussing chapter six.

So we'll head over to the chapter six page.

And before we look at some of the specific questions and like pick ones that are interesting and so on, does anyone just want to give any overall thoughts or questions?

Okay.

Well, of course, just right in the chat, raise your hand if you want.

We can look to some of the previously addressed or at least asked questions and get some more perspectives and improve some of the discourse.

Does anybody have an alternative preference?


SPEAKER_01:
Nope.


SPEAKER_00:
All right.

So also definitely please vote with an interesting button because once we get above like 10 or some number of votes, then we'll make a short video.

So we'll kind of be upvoting and then seeking to improve those sections most.

All right.

Why is modeling of the generative process a necessary step for building an active inference model?

Who wants to give a first thought on this?

That we can review, but first just yeah, Ali and then anyone else just why?

Why are we talking about modeling the generative process?


SPEAKER_02:
Yeah, I think as we talked about the distinction between generative model and generative process before, well, at least according to the previous active inference framework, generative process is basically what's, I mean, it refers to the latent state of the

observary data we get from the environment.

In other words, what causes those data, in this case, is put into a kind of probabilistic model.

So on the other hand, generative model

I mean, the aim of generative model is to provide a kind of mechanism to track those generative process as close as it can, or at least based on the situation of interest.

So that's one distinction between generative model and generative process.

But on the other hand,

Well, recently I got to the understanding that it's not so straightforward to separate generative model from generative process because in some situations we might never know what generative process

is, I mean, what actually constitutes generative process.

And the only thing we can concentrate or we can at least confidently focus on is to model the generative model based on some of the assumptions about how the environment will behave in the future or at least

based on some of our knowledge about the overall mechanism of the environment.

But in some simple situations, it might be possible to differentiate between the two.


SPEAKER_00:
Awesome.

Thank you.

Anyone else have any related thought or question?

about the role of building a generative process and a generative model.

And then we'll look at some of the previous responses.

And then we'll move on to another question.

I'll bring in one paper, path integrals, particular kinds.

And of course, remember that you can always just click on the icon of the institute on the right, and you'll jump to where we are.

So in this paper, which we often look back towards,

we see this taxonomy of different kinds of things ranging from the simplest inert systems through active systems, but perhaps simple active systems like a metronome on through sophisticated or strange systems

which have hidden internal states that are doing like counterfactual kind of world modeling, self in world modeling.

And so in chapter six, we're talking about whether you're trying to set up a simple inert system or a simple active system, or you're on the path towards making more complex cognitive models,

there's gonna be certain steps that are outlined in the recipe.

So that is like the continuity of active inference type modeling of the particular partition, splitting things into internal, external, and blanket states, and then further differentiating the blanket into the incoming sensory states and the outgoing action states.

Well, calling them sense and action is of course,

conditioned upon choosing one side of that blanket to be internal.

And that's kind of like the cognitive system of interest.

And so we call the cognitive system of interest in terms of its particular states, the internal and the blanket states, that is like the thing.

And that is the generative model.

Ali?


SPEAKER_02:
yeah and one other related thing is i mean this typology is actually based on a three-level classification namely at one hand we have the distinction between active and inert particles which do and do not have active states and then on the other level we have distinction with between dissipative and conservative particles which are and are not

subject to random fluctuations.

And finally, the ordinary and strange particles which have active states, which do have and do not have active states.

And one other point is it's interesting that in some recent literature, especially the one

the white paper published about four months ago, Designing Ecosystems of Intelligence from First Principles.

And this recent paper

The inner screen model of consciousness, the term generative process and never appear even once in the entire paper so basically they they began began to use a generative model interchangeably with generative process in some sense.


SPEAKER_00:
yes um this is this is something that we're going to be asking about in guest stream 45 in six days because we noticed that in this presentation that Maxwell had generative model covering the entire particular partition rather than the way that it was shown

in um the figures of the textbook where we had the generative model with internal blanket states essentially being partitioned from the generative process which is the process which actually gives rise to the observations so we can think of it as kind of like some uh

uh dialectical usage in the dialect sense of the active ontology so evolution and in use of what people are referring to um that's how it goes when things are changing fast but we'll find out in a few days some more takes on this um

Active Inference Agent employs a generative model as its model of the external world and uses the generative model to perform inferences about perceptual input to make decisions about actions.

The external world or the generative process is complex and unknowable.

Generative models may or may not be able to learn.

but learning and also action selection puts the generative model in better alignment with the world's generative process, whatever it might be.

Two ways to minimize free energy.

Change your mind through perception and learning.

Change the world through action.

And there's some more discussion.

Any other, from anyone, a comment or a question about the generative process and the generative model?

And kind of where we're at in chapter six, or we'll move on to the next question.

All right.

Awesome.

And also please use the upvote so that we can just figure out which questions we want to make improvements and short videos around.

Okay.

Next question.

Please just raise your hand if you have a different question or put it in the chat.

So this is kind of a core question about chapter six.

What are the four steps in the recipe to construct an active inference model?

So from the very first sentence of chapter six,

what the chapter is aiming to provide is this four-step recipe to construct an active inference model now it doesn't have to be strictly linearly followed they're more like four considerations or four areas of questioning more so than like you must take on one two three four in this order um but

Let's look at the active inference model recipe.

We made just a sub page because this is such a central topic.

I mean, all of our expectations and preferences slash hopes and dreams about applying active inference, using active inference modeling for some system or phenomena of interest.

This is kind of what it comes down to.

Again, this isn't the only or the total

protocol for modeling but chapter six is the transition point in the book where the first five chapters set us up to understand epistemically what active inference models were about

the low road and the high road in chapters two and three, how we approach active inference.

Chapter four, the generative model itself and some of the mathematics and the formalisms of it.

And then chapter five, exploring some applications in mammalian neurophysiology.

And now chapter six is like,

our system of interest whether it's another aspect of mammalian neurophysiology or whether it's any other of the many many situations that we're interested in modeling this is the model recipe as laid out by the authors here so does anyone want to give a thought or a question or just overview on the four considerations that that are in chapter six for the model recipe


SPEAKER_02:
yeah Ali and then anyone else yeah just one thing uh that I think we've also talked about before is to keep in mind that it's not absolutely necessary uh to follow those steps uh strictly in a sequential order so it would be possible to uh to address them in at least uh

in an asynchronous way or even in some cases even change the order of the steps so it's just a general framework to follow the recipe it's not I think needs to be strictly observed yeah maybe it's more like making four dishes


SPEAKER_00:
But let's look at what they are.

So first question, which system are we modeling?

And this is analogous to the identification of the system of interest in systems engineering.

And there are situations where this is trivial.

or where the teacher tells you which system is being modeled but there's also situations where it's non-trivial because of course everything is attached to everything in the world and so making these distinctions about the spatial and the temporal and the causal boundaries

of the system that we're actually going to choose to model such that it's at some base optimal trade-off point between being tractable and understandable implementable all of these different features that's kind of an iterated process and that's not one that's just simply written on the world as it is so we just get to show up and find out like this is where

We have to take an active approach to modeling to even determine some of these processes through inquiry, but what which system are we modeling.

The second question is what is the most appropriate form for the generative model so let's look at where they describe this in a bit more detail.

This is the decision, whether it is appropriate to think of a process in terms of categorical or discrete inferences.

or continuous inferences motivating the choice between discrete or continuous time implementations or hybrid of active inference so.

just to recall figure 4.3 is where we see a juxtaposition of the discrete time active inference generative model on the top and the continuous time active inference generative model on the bottom they're being laid out this way to highlight their structural similarities like they have this kind of downward facing e and then the upstairs with action in both cases

but there are some differences between the discrete time and the continuous time formulation we're not going to go into too much more uh detail on their differences right now but we can discuss if people want and this is going to get explored in a lot more detail in chapters seven and eight the coming two chapters which are explicitly about discrete and continuous time generative models

And you can also make hybrid implementations.

So the most common hybrid implementation is in the folk psychological setting where decision making is discrete at a higher level.

Like, should I reach for the orange or the apple?

And then at the sensory motor or lower, smaller or faster level, you have continuous time model, like a motor reflex model.

And then the motor reflex model, the attractor of where it's going to be expecting to find itself, which is where it reaches towards, is being imposed from the top down by that categorical decision-making model.

So hybrid models are absolutely possible.

And one setting that they're used in is systems that have discrete decision-making processes that interface with continuous sensory motor activities.

Then we need to select the most appropriate hierarchical depth, motivating the choice between shallow versus deep models.

So how much spatial temporal or causal depth is the model biting off?

Are you trying to model weather for one season or for one year or for 20 years or for a million years?

And then how is that scope going to be compartmentalized into the model?

So let's just say we wanted to do an 100-year predictive model.

One approach would be one tick of the model, one time step in the discrete time model is a hundred years.

And we fit all the parameters so that they are like what's accurate after a hundred years go by.

But then we wouldn't be able to necessarily make a prediction at 50 years.

So, okay, what else could we do?

We could have the tick be one year and have this model go a hundred into the future.

Or we could have a nested model where we have decades where the tick goes by one year at a time and then 10 decades.

And so at the higher nested level, it's like the transition between decades.

And so these are all structural variants of a model that can do prediction of 100 years.

Or maybe you want every single day.

Maybe you want a transition matrix amongst seconds.

But each of these are going to have trade-offs in terms of the computational requirements of the model, whether you're going to be able to calibrate it with data, the amount of parameters that you're going to be creating.

And so this is the modeler's quest and art and science to find the right models for a given situation because all models are wrong, some are useful.

And finally, we need to consider whether it's necessary to endow the generative model with temporal depth, kind of previously mentioned, and the ability to predict action contingent observations to support planning.

That's called planning as inference, and it is using the expected free energy, where if you have different actions that can be taken, different policies, which is kind of like an action over the given time horizon, right?

then planning as inference means predicting the consequences of those actions and then doing a type of inference to determine which path of action to be taken after determining the form for the generative model there are the questions of setting up the generative model and setting up the generative process so

the kernel of active inference if we see it in figure 4.3 again the discrete time or the continuous time this doesn't have attention metacognition um learning of preferences counterfactual world modeling what would happen if they did that

There's a lot of sophisticated cognitive phenomena that are not found in the base model.

In fact, the base model doesn't even have learning at all.

We're going to see in the coming chapters how this can be straightforwardly elaborated to include learning on any parameter shown here.

But this by itself does not include learning.

And so they're highlighting that there's a lot of decisions to be made

in terms of setting the priors and which aspects are going to be fixed and what aspects are going to be learnable and remember that the models are maps not territories so just because something is learnable in principle or in practice in the real world doesn't necessarily mean that the model is going to be simply mimicking that and vice versa so again these are just some steps to consider

essentially setting up the generative model and the generative process the the type of thing that we're modeling the cognitive entity and their environment and they say that these four steps in most cases suffice to design active inference model once completed the behavior of the system is determined by the standard schemes of active inference

this is a huge advantage of working with active inference cognitive modeling is once you get the model set up and entered into whatever implementation approach you're using spm or pi mdp or rx infer etc then the methods of parameterizing it fine-tuning it fitting that model

are very generic and straightforward.

So it's not like you kind of slap together how you're thinking about the system, and then you're gonna spend all of this engineering work on making the optimization tractable.

It's the opposite.

A lot of the work goes into the structural understanding of the system

Because once you have your model entered in to PyMDP, for example, then standard routines are used to fit the model.

Ali, then anyone else?


SPEAKER_02:
I yeah i'm wondering how I mean to what extent this recipe is applicable when it comes to modeling the collective intelligence, which is.

I mean the current interest of active inference research largely and, in particular, I believe, based on the stages of development for active inference outlined in designing ecosystems white paper currently.

we're on the systemic intelligence level of modeling, and we're aiming toward modeling or toward developing an efficient way to model sentient intelligence, but we're not there yet on the problem of modeling sophisticated intelligence and sympathetic and

ultimately shared intelligence.

So it's interesting to see how those four steps can be applied when we finally get into those stages of development of active inference agents.


SPEAKER_00:
Yes, this is from the paper from 2022.

Olivier?


SPEAKER_01:
Yeah, so I suppose that's, you know, the best way to understand these four different points is to look at how, you know, current examples of papers have done them.

So take, you know, already implemented models and identify, you know, step one, two, three and four.

And do we have in the Active Entrance textbook groups some kind of list of example models to do that?

Yes.


SPEAKER_00:
Yes, absolutely.

So it's a great suggestion, though.

I agree that showing these stages being carried out

We can hope becomes a practice in the active inference ecosystem, because it would connect a lot of the dots for people replicating the analyses and for learners.

But here's what we can look at now in the code page.

We have several dozen different languages, different implementations of active inference drawn from a variety of papers, pretty much the GitHub pulled out where we can.

Now, not all of these are heavily documented or pedagogically oriented.

So sometimes there's some forensic work to be done with like understanding because and also a lot of this happens outside of the context of shared epistemic niches and shared education.

So the variables are going to have different names and the implementations are going to be quite different.

These are not necessarily created.

after reading chapter six, especially if they, yeah.

But it certainly is a retroactive approach that can describe.

And then I think one of the most clearly unpacked and didactic examples that we can look at is Model Stream 7.2.

And in Model Stream 7.2, in the video description is the notebook of this presentation.

And so it does in a much more pedagogical way and in an executable way, it runs perfectly as is.

You can see how is B getting set up.

So again, this isn't exactly aligned with the four stages of the recipe, but like we said, those don't have to happen in that order.

But this is like, these are the variables that you need to set up.

A, B, C, D, E. And then at the bottom, you see the routines, the run active inference loop.

So if seeing it in a pseudocode way helps you understand the processual nature of the active inference loop better than the analytical equations, that's definitely a good way to learn and study this.

Okay, yeah, great.

Any more that we can say on this?

One other resource I'll link in there.

This is to the Active Blockference CODA.

This is just another project at the Institute.

So here we've kind of expanded on the recipe to include more up front and along the way.

So more of like the structural understanding of the system and why the model is being made.

Then we kind of include the steps of chapter six, but also take more of a life cycle perspective on the operations of these models.

And then we have all these different settings where people have started to explore those kinds of modeling approaches.

So I'll just link to this page also.

in in the recipe okay let's go back to the questions so what are the four steps we just looked through them essentially they're they're a set of considerations that build the generative model the generative process so the cognitive thing and the environment the niche that's giving observations

four considerations that help us develop that from its kind of sketchiest outlines on through the determination of which variables are going to be like learnable or not which ones are discrete state space which ones are continuous state space Etc and then once we have that set up and entered into our implementation approach then all the routines are standard

And there's always exciting developments with making those routines better at dealing with sophisticated planning and so on.

So again, this is a big strength and a growing one for active inference that the work in understanding the system gets us to the point where it can be run generically.

as opposed to just taking an ad hoc understanding of a system, which is like kind of a mistake, but one that flies by in many situations, and then confusing the engineering work with improving the computational aspects of the model with an understanding of the system.

Okay, so that was this question.

Again, upvote the question so they rank higher, but I'm just gonna continue down from the top.

In figure 6.1, what do the unidirectional and bidirectional arrows mean?

So we see there are some connections between nodes.

Nodes are random variables and edges represent causal influences, possible causal influences.

Just because they're here doesn't mean that they're relevant in a given situation.

But this is kind of like the set of the possible connectivities

of the action perception loop.

So the blanket states are connected to each other possibly because they form that insulating set around the internal states.

So again, you can design a system where the API call comes in and then a second computer computes it and then a third computer emits it.

So there doesn't have to be an edge between these two, but in principle it's allowable.

we see this symmetry where again, an internal and external, that just depends which system we're focusing on, but there's a symmetry across here.

And so you can kind of see it's like a rotated 180 degrees symmetry where sensory states don't have a backwards arrow from internal states and external states don't have a backwards arrow to active states.

However, external and sensory have bidirectionality and internal action have bidirectionality.

So

The particle, that which is being partitioned off from the world, can have different sensory and active blanket states, blanket being the joint set of U and Y, and internal states.

And these arrows are just shaping out the space of the possible interactions amongst those variables.

And

um in let's see guest stream number seven how particular is the physics of the free energy principle if people want to go into more detail on this question uh miguel aguilera and colleagues

provided some technical analyses and basically asked under what topologies of the action perception loop does the free energy principle apply for example if we have like a simple around the clock model none of these backwards arrows do things still apply or if we have more or less connectivity in in principle what applies

And the short answer to this is, basically, it's always going to hold in some fashion, but it may or may not be interesting.

And the important constraint that we're really adhering to, the most important constraint, is across the interface, there's no telepathy and there's no telekinesis.

That doesn't mean internal states

can't track or phase lock onto external states with very sparse sensory input.

The point is they do so only through the blanket.

So you can't get information about hidden states except by sensory states.

That's no telepathy clause.

And the no telekinesis clause is you can only influence external states through the active states of the blanket.

So the fact that there's no connection between internal and external states is really the most important constraint on the topology of perception and action and active inference.

And other than that, there's some technical discussions to be had about exactly which situations have bidirectional arrows and so on.

And some of it is explored in 7.1.

Ali, and then anyone else.


SPEAKER_02:
I just wanted to mention a couple of technical terms that have been used in the literature repeatedly.

One is the autonomous states, which refers to the joint set of internal states and the activist states instead of the whole blanket state.

So particle is the joint set of internal states and the whole blanket states.

If we consider only the activist states among the blanket states, then that's been referred to autonomous states.

And the other term is a sparse coupling, which exactly refers to that unidirectional arrow in this figure.

So very briefly, sparse coupling refers to the fact that things are defined by what they are not connected to.

So if the whole

diagram here would be connected, the whole states here would be connected through bidirectional arrows, then that wouldn't be a sparse coupling.

So the reason this term is used is because of that unidirectionality in this figure.


SPEAKER_00:
Awesome.

And one important piece on the autonomous states, and this is kind of like a cool way to think about ACT-IN formally, but also informally,

Autonomous states, again, are just the internal and active states.

So these are the two ways that you can reduce your free energy.

Change your mind, change the world through action.

Learning and perception, update internal states.

Action, updating active states.

Our preferences, the C vector, are over sensory states.

So we say, I prefer to find myself at 37C, homeostatic body temperature.

And so because I prefer that, I'm going to seek policies that meander my way, even if it's one step backwards, two step forwards, to reduce my surprise, bound my surprise, reduce my expected free energy about finding myself at homeostatic body temperature.

So our preferences are over sensory states, but the two things that we can control, the autonomous states...

like all-knowing and all-powerful are the internal and active states.

And another way of saying that is you don't directly have your thumb on the scale.

We want to see a certain number on the scale, but all we want to be able to do, even in our maximal empowerment, would be perfect updating of learning and perception and perfect selection of action.

If you enable the model to also take control over its sensory incoming data, not the downstream processing of it, but the primary sensory data, then the model may, in pursuit of seeking its preferences, just simply modify the sensory data.

So if we'd like to see a sunset,

You know, we want to either learn, but we're not seeing one.

You want to either learn that we actually don't prefer that, then all is good with the world, or make a plan to see it.

But if you can just turn on the VR in your mind and see it right there, you've kind of made a hollow satisficing of your preferences.

And in the ecological evolutionary context, you're going to get swept off the table.

So the autonomous states are like what we would want to control under maximal empowerment.

And they're the two avenues of reducing free energy, changing mind and changing the world.

Olivier?


SPEAKER_01:
Yeah, I mean, I'm not sure I got it.

But you said the preference is put on the sensory state.


SPEAKER_00:
Correct.

Correct.


SPEAKER_01:
Yeah.

So...


SPEAKER_00:
is that um reflected in the mathematical uh expression of free energy oh absolutely it is not shown here this on the particular partition we don't see the c vector or preferences but um let's look at it in the equations

so here equation 2.6 on expected free energy and so in the top decomposition expected free energy g is a function that's evaluating policies so it's loading in our policy prior that's like our habits

and then it's sharpening those policy priors into a policy posterior that then you can sample from or just select the best option from so let's just say that um two-thirds of the time I turn left that's my habit and then if we just passed the prior on policy through then action would be selected according to that ratio two-thirds of the time going to the left

However, what expected free energy does is it sharpens the policy prior habit into policy posterior.

In the top decomposition, we see one way of thinking about what contributes to the expected free energy for a given policy concern or option.

And that's thinking about it in terms of the information gain and the pragmatic value.

So first, the information gain.

Here, we see that there's a KL divergence

between hidden states through time, X tilde, pi is on both sides, and here there's a Y. So if the Y, the expected sensory information under that policy, if it didn't give us any information, it's not expected to give us information, then the KL divergence is zero.

no information is expected to be gained from that course of action however if the policy does have a sensory observations that would lead us to learn then epistemic gain is non-zero here we have pragmatic value and this is basically how surprised are you about the sensory data given your preference

And so if your preference is 37 and it's like a bowl around 37, then the highest pragmatic value policy would be the least surprising, meaning that the ball would be right at the bottom of the bowl at 37.

So just to contrast with reward learning, reinforcement learning,

in reward and reinforcement learning, you have this reward function that's like 37 is the most rewarding body temperature.

So I'm going to try to find my way up to the most rewarding point.

We take an opposite approach.

We say, I expect and prefer to be at 37 C. That's what I'd be least surprised by if my thermoreceptors were at 37.

That's why expectations and preferences are kind of used together in that way.

And then policies are selected that are able to contribute to pragmatic value and information gain.

And during the modeling process, you see that there's some fine tuning

needed to balance these two drives and that's actually explored in this notebook like if you set the preference for getting sugar to 1 million to one then it'll never seek information it's ready to risk it for the chance of winning a million but if the pragmatic value is um very small then the information gain term dominates and another way that that is shown in a figure

is in figure 2.6 where we have expected free energy just as shown now here the pragmatic value term is on the left but it's the same equation yeah and so if only pragmatic value matters so it's a fully observable there's nothing epistemic to gains like a chess board then you get expected utility Theory

In contrast, when there's no pragmatic value on the table, so all outcomes are equally pragmatically valuable, then you only have two, three, four, five, in which case you're having information driving decision-making.

Yeah.

But both can be on the table.

And that is how active inference in a first principles way is negotiating the explore-exploit dilemma.

Yeah.

Thank you.

Ali, thanks.


SPEAKER_02:
And also this kind of information seeking or preference seeking is what referred to as

curiosity or rather artificial curiosity in the literature, but it's probably worth mentioning another recent trend in active inference literature, which is the development of sophisticated active inference, which I don't think is explicitly stated in the textbook, but basically sophisticated inference deals with the situations in which rather than

trying to answer what will happen if I do this.

They try to address what will I believe if I do this.

So basically, it's based on the consequences of actions for beliefs rather than just the states of the world.


SPEAKER_00:
Thank you.

yeah especially with all of the developments and elaborations happening like around the time of the writing of this textbook and since then um it's kind of it kind of contextualizes the bigger picture but like figure 4.3 is like the Lego block this is like the the neutral minimal cognitive kernel

shown in its discrete time and continuous time setting and a lot of the work right now on the analytical and on the formal side for example with category theory is to understand the compositionality of these models like can we really attach this to that in principle and then a lot of the computational work which you'll see um in the code section

are using techniques from computer science broadly, like search on large trees and branching and pruning on trees, using these computer science and engineering approaches to make running large active inference models more tractable.

So the textbook helps us parachute in and get a first overview

But for those who want to see more, there's certainly more on the computer science implementation side and on the analytical and on the formal side.

But all things considered, the textbook walks the line between that.

And again, chapter six is like the moment in the book where it transitions from the first five chapters, which are like a background and statistics class,

on through chapter six and beyond, which through our work, we can even develop into something like a playbook or a set of code.

And in cohort three, which we're in right now with chapter six, one of our goals for this activity interval is to leave an executable trail.

So we want to develop the answers to questions, but also especially where we can to leave a code or executable or just pseudocode way to approach this.

And that's something that can be collaboratively worked on and versioned through the cohorts.

Okay.

Few more questions.

All right, how does the particular partition, so figure 6.1, particular partition, the particle, blanket, and internal states.

Autonomous states, just internal and active.

But the particle, imagine if you were looking at a dust particle under the microscope.

That would be an inert particle.

But a cognitive particle is like the agent in the agent-based modeling.

So we see sometimes a simple action perception loop

And other times we see the more expanded particular physics.

But then this question asks, how does the particular partition relate to the partially observable Markov decision process in figure 4.3?

So where did the Markov blanket go in figure 4.3?

Okay, one simple thought, then anyone can add more.

The observations here, O, are the sensory states.

And so in the continuous time, they're even shown with Y. So that's actually the same variable as used for sensory states in the particular partition.

But the O are the observables across the Markov blanket.

And then here, pi is policy selection, policy inference.

which is influencing the B matrix, which is how the agent thinks things change through time.

But we could also imagine that pi also reaches out to influence the generative process, which just isn't shown here.

Here, just the agent's generative model is shown.

There's no generative process.

But if there were a generative process, it would be receiving an input from pi,

and sending an output to observations.

So the POMDP can be seen as like a way of unrolling the particular partition, because this gives a sequential linear executable format to modeling the particular partition.

Any other thoughts on this, Ali?


SPEAKER_02:
uh yeah one other way uh to like look at it is to uh interpret the uh I mean because in sorry in uh pomdp uh the a the agents preferred observations uh somehow strives toward keeping it within the high probability States uh which would be consistent uh with its continued existence in other words uh this uh striving toward uh

keeping these high probability states can be interpreted as keeping its markup blanket intact.

So in other words, markup blanket is kind of implicitly implied in POMDP, but not obviously explicitly stated, but it can be interpreted from various perspectives.


SPEAKER_00:
yes thank you okay gibsonian affordance concept does anyone want to give a little information on this question and then i think we'll have a response well it's a controversial issue indeed


SPEAKER_01:
If you can just briefly remind Gibson's affordance concept.


SPEAKER_00:
Yes.

Okay.

Yes.

Let me get the right page here.

Okay.

Here I'm going to share some work by Paolo.

an intern, and he's analyzing the affordance concept and the representation concept in the English and in the Portuguese literature using the active inference ontology to connect between the two of them.

And there's a table.

So recently there was a paper written by Ramstead about the affordance 3.0 concept.

and basically says the way that we talk about affordances in Act-Inf, that is pretty much the Gibsonian affordance concept.

So as Ali tactfully noted, this is like an area of discussion slash controversy.

So I look forward to Paolo developing the work more and bringing this out.

However, affordance 1.0 concept by Gibson

was that the affordance was about the perception of a capacity for action in the environment.

Like a handle is perceived as for grasping.

And people focus a lot on this direct perception and there's the whole Markov blanket trick, which we've talked about elsewhere.

And people focus on like whether the Markov blanket is direct or indirect perception, but that is actually a little bit of a red herring.

the key piece of the gibsonian affordance concept is that the affordance is doing something explanatory about perception it's explaining a perception of a certain type which is the perception for action in a niche however in active inference as it's been used affordance is used to describe action capacities that are not necessarily perceived

more like something that the system can do and so when we look at the pi variable from like figure 4.3 then like we call those what we call e affordances and then pi is the policies of affordances so if it's just one time step then pi is the same as e but if it's uh 12 time steps then it's all the 12-step combinations of the affordances

e what can be done however active inference does not highlight the perceptual aspects of the affordance because it's kind of a view from the inside view from the outside which we're going to come back to in chapter nine but just to show what this looks like the experimenter might say well that mouse can go up down left right

So that's how the active inference affordance concept is used.

Up, down, left, right are the four capacities for action.

However, the traditional ecological psychological Gibsonian affordance concept highlighted the view from the inside, which is the perception of an action capacity.

Now that's not a perfect concept in and of itself because it just begets the next question.

Well, what raises salient affordances to awareness?

So it's not like one is better than the other and surely they can be understood of as compatible.

However, there is some significant work needed to understand the continuity of these concepts.

And whether or not it's valid to say that the affordance 3.0 concept is actually in the direct lineage of the affordance 1.0 and 2.0 concept.

And Paolo S is the best intern to speak to about this topic.

Cool.

Well, that brings us to the...

end here i'll unlike these ones for uh next week we'll be at the other time and um yeah everyone's welcome to add or upload questions or anything else any anyone else want to add anything before we close

Awesome.

Thank you.

Yes, chapter six is when it really starts getting into the modeling and the doing.

So also next week, I'll hope that we can develop out some of the code, maybe even annotate some of the PyMDP examples or some of the other examples in terms of the recipe.

Okay.

Thank you all.

Farewell.

Thank you.

Thanks.

Bye.
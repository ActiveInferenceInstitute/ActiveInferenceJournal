[
  {
    "start": 3.541,
    "end": 30.088,
    "text": " all right hi everyone it's cohort 3 7 25 23 and we're in our first discussion on chapter nine here so let us head over to chapter nine and begin before we go into any questions with just any general thoughts or comments that people want to bring up about chapter nine",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 33.764,
    "end": 62.404,
    "text": " model-based data analysis even just what what however in-depth people have read this like what what would they expect such a chapter to cover what would somebody want this type of a chapter to cover what does this title bring up for people or the quote from President Obama",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 85.351,
    "end": 97.104,
    "text": " Well, I'll just give thoughts, but please write in the chat or raise your hand or, you know, feel free to add in more.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 97.605,
    "end": 97.865,
    "text": "Great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 97.885,
    "end": 98.105,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 98.186,
    "end": 98.346,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 98.426,
    "end": 98.806,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 99.087,
    "end": 101.269,
    "text": "It's totally fine across different cohorts.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 101.329,
    "end": 103.391,
    "text": "There's like not really a difference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 103.632,
    "end": 110.92,
    "text": "And just it's so many coats of paint that skipping around, it's not out of line.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 112.52,
    "end": 128.823,
    "text": " Um, and also in a sense, chapter nine in specific, um, I mean, let's look at the textbook chapter one and chapter 10 are very similar that they're both overview chapters.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 128.843,
    "end": 129.744,
    "text": "There's no equations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 130.245,
    "end": 131.727,
    "text": "They get big picture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 132.736,
    "end": 134.458,
    "text": " Chapter 2 and 3, we know the low road.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 134.518,
    "end": 137.142,
    "text": "Chapter 4, technical details with generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 137.402,
    "end": 140.426,
    "text": "Chapter 5, a lot of neurobiology and examples.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 141.167,
    "end": 149.637,
    "text": "Chapter 6, with a recipe for designing active inference models, probably makes more sense after knowing what the generative model is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 149.697,
    "end": 154.664,
    "text": "However, you don't have to know it in technical detail.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 155.625,
    "end": 162.253,
    "text": "Chapter 7 and 8, on the discrete and continuous time generative models, again, probably helps to understand the generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 164.309,
    "end": 170.58,
    "text": " So seven and eight might rely on four, but nine in a sense stands alone.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 170.6,
    "end": 186.469,
    "text": "And so I hope that we can like dive into some of the intuitive and immediately accessible components of chapter nine, because it's gonna be a very different tack than the approaches from the previous chapters.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 188.744,
    "end": 190.71,
    "text": " So let's get into it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 191.232,
    "end": 197.05,
    "text": "In this chapter, we focus on the ways in which active inference can be applied in understanding empirical data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 197.333,
    "end": 218.879,
    "text": " Now, in all the previous sections, the examples, whether it's the frog jumping out of the hand or the musical notes or the rat in the maze or bird song, all of these examples follow kind of a similar pattern, which is like a generative model is specified and then it's played out.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 219.44,
    "end": 220.421,
    "text": "It generates data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 220.921,
    "end": 225.687,
    "text": "That's one reason why these are called generative models, because these are models that generate data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 226.831,
    "end": 230.416,
    "text": " However, in the real world, we want to go both ways.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 230.877,
    "end": 245.058,
    "text": "We want to be able to specify a model a priori and then generate data, but also we want to have structured data and then do inference about hidden states, for example, given the empirical data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 245.859,
    "end": 251.127,
    "text": "So we don't just want to spin out mountains of data just to show that we can.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 251.187,
    "end": 255.994,
    "text": "We want to take the data that we have, which is called empirical data,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 256.666,
    "end": 259.832,
    "text": " and then do some kind of inference with it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 262.157,
    "end": 270.754,
    "text": "Our general goal is to recover the parameters of the generative model that a subject's brain or mind uses to produce behavior, the subjective model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 271.756,
    "end": 276.345,
    "text": "So if all we were interested in was descriptions of behavior,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 276.764,
    "end": 279.847,
    "text": " We wouldn't need sophisticated modeling at all.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 280.388,
    "end": 285.414,
    "text": "We would just need summary statistics or descriptive statistics on behavior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 286.235,
    "end": 291.461,
    "text": "Like if all we wanted to do was just how often does this person make a sound?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 292.242,
    "end": 293.223,
    "text": "Again, you could just count.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 294.204,
    "end": 301.612,
    "text": "But if you wanted to say, how frequently does the baby transition between happy and unhappy?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 302.453,
    "end": 305.877,
    "text": "So in that case, you need to parameterize",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 307.443,
    "end": 325.519,
    "text": " aspects that are not directly observable but are based upon observables and that's subjective because it's about a subject so we'll talk about subjective and objective models and this is going to be introduced in terms of meta bayesian methods",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 325.634,
    "end": 337.792,
    "text": " which sounds very meta, but once we see the figure, it'll actually be revealed to be, in fact, the structure of behavioral observation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 339.154,
    "end": 345.463,
    "text": "And this relates to many different topics we've discussed, like Bayes' optimality of behavior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 346.325,
    "end": 352.654,
    "text": "So looking at the diversity of neural systems or cognitive systems and asking,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 353.123,
    "end": 372.297,
    "text": " what is the generative model such that this system's behavior is base optimal that doesn't mean it's the most profitable business or you know the best free throw shooter or any of these things but still given the generative model its path of least action is base optimal in some way",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 374.032,
    "end": 396.968,
    "text": " and this relates to computational phenotyping for example in the human um domain with psychiatry but also in other domains like ants research okay just raise your hand or write some some thoughts if you'd like so nine two goes into the metabasian methods",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 398.686,
    "end": 403.612,
    "text": " The chapter deals with the utility of active inference formulations in analyzing data from behavioral experiments.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 404.093,
    "end": 412.143,
    "text": "This goes beyond proof of principle simulations we have seen in previous chapter and exploits active inference in answering scientific questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 414.286,
    "end": 416.349,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 416.369,
    "end": 422.196,
    "text": "There are two related reasons for fitting a computational model to observe behavior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 423.307,
    "end": 433.497,
    "text": " The first is to estimate parameters of interest from the model that best explain the behavior of a specific subject, so one individual or a group of subjects.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 434.819,
    "end": 437.061,
    "text": "This is computational phenotyping.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 438.863,
    "end": 451.035,
    "text": "So computational phenotyping helps us go beyond the summary statistics of behavior and try to understand underlying differences",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 451.538,
    "end": 464.772,
    "text": " that are potentially only accessible or are manifested through multiple types of data, like FMRI data, EEG data, behavioral data, self-report data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 466.743,
    "end": 477.162,
    "text": " And those kinds of computational phenotypes might be really important for forecasting behaviors or for deciding interventions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 478.164,
    "end": 485.136,
    "text": "So let's just imagine a situation where a system is engaging in a behavior that we want to change.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 485.677,
    "end": 489.865,
    "text": "So ignoring why we want to change it and all this kind of stuff.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 489.98,
    "end": 493.406,
    "text": " We want it to do this, and it always does that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 493.906,
    "end": 507.829,
    "text": "Well, there might be a few different reasons that that happens, and we can look at this in the context of figure 4.3, our standard generative model here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 510.173,
    "end": 516.443,
    "text": "So one option for why the system engages in the behavior is that it's a habit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 516.558,
    "end": 521.764,
    "text": " So it's policy prior is that it goes a certain way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 522.825,
    "end": 523.566,
    "text": "That's option one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 524.727,
    "end": 529.532,
    "text": "Option two is that it prefers to go that direction.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 530.113,
    "end": 540.464,
    "text": "So whether or not it's habitual for it, it's expected free energy calculation sharpens that policy prior to the policy posterior and selects this action.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 541.565,
    "end": 544.168,
    "text": "And the third option is that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 544.334,
    "end": 561.972,
    "text": " it has an understanding of how that action changes the world such that whether it's habitual, option one, and whether it's preferred, option two, it still believes that that course of action is the one that brings the world into alignment with preferences.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 562.458,
    "end": 569.106,
    "text": " And those are three very different situations, for example, in the context of treating addiction.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 569.947,
    "end": 576.215,
    "text": "So in the case of habitual behavior, maybe one course of action is appropriate.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 577.217,
    "end": 583.224,
    "text": "In the case where somebody prefers something, maybe you speak directly to the preferences.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 583.925,
    "end": 589.512,
    "text": "And then in the case where somebody's B matrix or their causal model of the world",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 589.964,
    "end": 618.958,
    "text": " is maladaptive then you might engage in education around how hidden states change their time so that's how you can use this type of a cognitive model to go a little bit deeper than just the phenotype manifested and and get unique explanations and predictions about systems that's the first reason and the second reason that you might want to do this kind of computational modeling",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 619.478,
    "end": 627.106,
    "text": " is to compare alternative hypotheses that represent different explanations for a behavioral phenomenon.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 629.188,
    "end": 632.452,
    "text": "So we're modeling ants coming in and out of the colony.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 633.153,
    "end": 637.277,
    "text": "And we have one model, it's a single parameter model, there's a drive for foraging.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 638.218,
    "end": 649.35,
    "text": "And then there's, now we're gonna contrast that with a two parameter model, a drive for foraging and fear of predation, and a three parameter model with some third proposed cause.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 649.668,
    "end": 659.099,
    "text": " And we want to know which one of those models are helping us develop scientific explanations or effective accounts.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 660.42,
    "end": 663.964,
    "text": "And so we can use Bayesian modeling to do that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 673.034,
    "end": 676.198,
    "text": "Bigger 9.1 is a really",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 677.68,
    "end": 681.611,
    "text": " interesting and clever diagram.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 681.791,
    "end": 688.129,
    "text": "Does anyone want to, up to this point or about figure 9.1, like give any thought or questions?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 696.513,
    "end": 713.053,
    "text": " Daniel, I've got a question, although it's not quite formulated just yet, but it had particular resonance to addiction, because I'm in the process of treating such a condition in somebody.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 713.033,
    "end": 729.692,
    "text": " i haven't quite formulated my question but i'll pitch it to you as soon as i've got it in mind we might if you don't mind go back uh if that's okay uh oh yeah a page or two but but i'm just still formulating it so yeah you can just hang ten i'll i'll get one to you soon",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 741.89,
    "end": 745.656,
    "text": " Let's look at Figure 9-1 and then feel free to ask it with this setting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 745.677,
    "end": 750.024,
    "text": "This is kind of like just a graphical overview of the setting that was described above.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 750.946,
    "end": 756.475,
    "text": "And this is going to help us clarify subjective and objective modeling.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 758.519,
    "end": 765.13,
    "text": "So this is the relationship between subjective and objective models in the context of this Bayesian inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 765.582,
    "end": 767.486,
    "text": " The inner dashed box.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 768.849,
    "end": 771.294,
    "text": "So inner dashed box should look familiar.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 771.935,
    "end": 772.596,
    "text": "It's figure 4.3.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 772.817,
    "end": 783.679,
    "text": "The inner dashed box is the subjective model used by an experimental subject.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 784.06,
    "end": 785.683,
    "text": "So that's why it's subjective.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 786.237,
    "end": 816.075,
    "text": " is this is the the rat in the maze this is the ant in the colony this is the person in the armchair this is who is subject to being modeling this is the subject of modeling this is the generative model that is being proposed for the subject's cognitive processes and so the same exact model that we've been talking about generative model of an agent we're just putting that into this dashed box",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 818.519,
    "end": 827.291,
    "text": " The outer dashed box is the experimenter's objective model that includes prior beliefs and so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 828.713,
    "end": 835.041,
    "text": "So now let's take a step outside of just the agent of interest, outside of just the rat in the maze.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 836.724,
    "end": 839.247,
    "text": "Now let's think about the whole laboratory.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 840.949,
    "end": 848.239,
    "text": "We have the scientist observing the rat in the maze, which is parameterized a certain way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 848.472,
    "end": 854.118,
    "text": " It could have been 50-50 with sugar versus bitter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 854.378,
    "end": 855.359,
    "text": "It could have been 80-20.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 855.439,
    "end": 856.921,
    "text": "It could have been three flavors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 856.961,
    "end": 858.763,
    "text": "It could have been 22 flavors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 860.525,
    "end": 865.93,
    "text": "That's the parameterization of the laboratory, theta.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 867.712,
    "end": 877.382,
    "text": "Then the experimental stimuli, that could be the visual stimuli that's presented, or it could be the decision-making stimuli that are presented.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 878.24,
    "end": 903.673,
    "text": " those stimuli which become observations of the subject are actions by the experimenter and analogously the actions of the subject whether the rat goes left or right whether the person goes for the gamble or not the actions of the subject",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 904.177,
    "end": 908.703,
    "text": " become the empirical objective observed behavior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 910.605,
    "end": 922.28,
    "text": "So this is kind of like wrapping a behavioral analysis layer or an ethological layer around the generative model of an agent.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 923.702,
    "end": 927.046,
    "text": "And so it speaks to the composability of these models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 929.83,
    "end": 933.154,
    "text": "This is also a little bit more of a holistic picture",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 934.315,
    "end": 958.753,
    "text": " because it describes a setting that I believe is is non-controversial here's a person watching a bird or a scientist watching a mouse but it helps us get a bigger picture it's not just a free floating generative model it's actually the generative model especially in a laboratory or a controlled setting",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 960.404,
    "end": 975.907,
    "text": " where the scientist is presenting stimuli that hopefully are maximally informative with respect to differentiating this model, and the outputs of that model are the inputs to our empirical data analysis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 977.93,
    "end": 981.636,
    "text": "So anyone have some thoughts or ideas on this?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 987.556,
    "end": 993.682,
    "text": " Daniel, if I may, I haven't raised my hand, but I hope I can just speak.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 996.384,
    "end": 1012.499,
    "text": "As you're speaking, I'm mapping myself as an objective observer in my consulting room to a particular patient who has just brought new observable evidence in the form of a relapse.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1013.439,
    "end": 1017.343,
    "text": "And I'm just wondering how",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1017.323,
    "end": 1019.885,
    "text": " So I hear what you said.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1019.925,
    "end": 1047.33,
    "text": "If you went back to the, if you focused on simply on the subjective model that he is using substances because it's his preferred, I can't remember what those three things were exactly in order, but it's because in a consulting room is precisely not a controlled environment like a mouse might be in terms of what they are exposed to and what they are able to perceive.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1047.31,
    "end": 1053.9,
    "text": " He is being influenced by other generative models outside of the consulting room.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1054.761,
    "end": 1070.864,
    "text": "And I think it's just lends kind of credence to the idea of how hard it is to help or to help people with addictions because it's not in a controlled environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1071.565,
    "end": 1075.731,
    "text": "And there are so many generative models immersed or overlapping",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1076.133,
    "end": 1083.13,
    "text": " that I am not privy to and therefore only come into the picture at a particular moment when relapse has already occurred.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1083.952,
    "end": 1090.107,
    "text": "And I can't get to his brain dynamics and inner workings of his brain directly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1090.187,
    "end": 1091.931,
    "text": "I can only",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1092.603,
    "end": 1100.501,
    "text": " You know it's only me and him as experiencing humans at the interface, rather than speaking directly to his and.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1101.944,
    "end": 1107.076,
    "text": "yeah so it's just so tricky it's a very ineffective way of trying to help people I realized.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1108.519,
    "end": 1109.802,
    "text": "But hopefully let's try it now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1111.655,
    "end": 1127.217,
    "text": " absolutely a lot to to um say about that um of course only from my experience as a bug doctor this is a map not a territory so so um this solid black line",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1127.652,
    "end": 1148.968,
    "text": " represents like the veil of unknowing it's a markup blanket ultimately so here's you and in the context of of a conversational setting you're right it's it's not a total behavioral control setting your stimuli that you are using to um appreciate influence and control or",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1149.674,
    "end": 1162.05,
    "text": " Another way to think about this is the actions that you're taking, which are declarative or question expressions, are selected based upon expected free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1162.691,
    "end": 1164.953,
    "text": "So what is expected free energy based upon?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1165.254,
    "end": 1168.178,
    "text": "Well, it has two components, pragmatic and epistemic value.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1170.18,
    "end": 1176.328,
    "text": "A given expression you might select through your expertise based upon the epistemic value of that path.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1176.668,
    "end": 1178.01,
    "text": "So asking a question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1178.227,
    "end": 1187.541,
    "text": " Or conversely, to yield pragmatic value by being more declarative or explicit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1188.503,
    "end": 1190.025,
    "text": "So let's think about, again, those three.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1191.528,
    "end": 1197.677,
    "text": "Let's think about a generative model for that type of usage in this setting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1198.699,
    "end": 1202.164,
    "text": "And just like there's not one map for New York City.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1203.206,
    "end": 1203.306,
    "text": "Sure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1203.64,
    "end": 1212.621,
    "text": " there's there's not just one map but so yes definitely once this individual is out of the context now there's there's the whole ecosystem of shared intelligence",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1212.888,
    "end": 1221.222,
    "text": " that it's engaging with in an embodied way that there's so much complexity, but this you're gonna, um, you're, you're deciding which question to ask.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1221.402,
    "end": 1233.523,
    "text": "You only get one question and you're, you're wanting to, um, again, super coarsely differentiate which of three categories of patients you're dealing with.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1234.424,
    "end": 1234.544,
    "text": "Right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1234.564,
    "end": 1237.95,
    "text": "The three categories are gonna be habitual,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1238.42,
    "end": 1245.589,
    "text": " but understands the world and what they prefer is healthy, but their habit is unhealthy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1246.23,
    "end": 1247.672,
    "text": "Again, let's just speak normatively this way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1247.992,
    "end": 1255.501,
    "text": "So category one is going to be action prior is habitually addicted.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1255.521,
    "end": 1256.943,
    "text": "Yes, yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1256.963,
    "end": 1261.509,
    "text": "Option two is there's going to be in the C vector.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1261.889,
    "end": 1263.812,
    "text": "So the action prior is going to be the pie.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1264.853,
    "end": 1267.937,
    "text": "Option two is C, preference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1269.2,
    "end": 1274.928,
    "text": " which is over observations, is such that the person prefers the outcomes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1274.988,
    "end": 1278.353,
    "text": "Like I prefer to see wild colors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1278.873,
    "end": 1287.125,
    "text": "So I'm using this thing that makes me see colors because I actually prefer the consequences of that action, whether it's habitual or not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1287.325,
    "end": 1288.967,
    "text": "So habit, preference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1289.428,
    "end": 1295.376,
    "text": "And the third option we can just coarsely say is B, the world model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1295.356,
    "end": 1321.363,
    "text": " which is some learning about such that even if the person has healthy habits and healthy preferences you know it's like they don't understand the consequences of their action right now which question should you ask well without going into you know counter deception and second level um cognitive modeling and things you could ask",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1322.491,
    "end": 1323.092,
    "text": " What do you prefer?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1323.152,
    "end": 1326.455,
    "text": "Or category one, what do you usually do?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1327.195,
    "end": 1328.917,
    "text": "Category two, what do you actually prefer?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1328.937,
    "end": 1332.42,
    "text": "And category three, what do you think would happen if you did this?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1335.163,
    "end": 1350.017,
    "text": "So those are three distinct paths of questioning that are gonna be stimuli for them, which reveal an outcome action selection from them, which we hope is honest.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1350.858,
    "end": 1370.976,
    "text": " which then feeds back into your model and here the parameters might might just be the general um the context of the room and now what is the the flexibility and extensibility of this model yes what if the experimental stimuli was not just you asking a question",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1371.344,
    "end": 1388.62,
    "text": " But what if the stimuli was just you asking a question and the observation was what they said and EEG and fMRI and a biomarker and skin galvanic response and pupil and arousal and saccades.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1388.64,
    "end": 1399.731,
    "text": "So that is why the sensor fusion capacity for active inference and the composability is really a positive feature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1399.947,
    "end": 1407.774,
    "text": " because we can deal with the experimental stimuli is just verbal, and the observed behavior could be this whole vector of outputs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1408.775,
    "end": 1418.363,
    "text": "Or maybe the stimuli is a whole vector of inputs with transcranial and with this and with this and with that, and the observed behavior is just self-report.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1419.705,
    "end": 1429.313,
    "text": "So whether we have single input, single output, multi-input, multi-output, we're able to make maps, cognitive maps,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1430.795,
    "end": 1459.671,
    "text": " that reflect the setting right do those always have to be discrete or distinct um and effects or can they can they be a combination some sort of a uh kind of an amalgamation of um the first and the second for example of their generative model that I am choosing to do this because and you know are",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1460.545,
    "end": 1464.711,
    "text": " Gareth J. How can they they can't be some sort of a mixture and.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1464.731,
    "end": 1468.698,
    "text": "Gareth J. Great great question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1470.0,
    "end": 1472.123,
    "text": "Gareth J. So.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1472.143,
    "end": 1472.403,
    "text": "Gareth J. um.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1472.423,
    "end": 1485.203,
    "text": "Gareth J. let's recall, one of the early equivalence is that we brought up, which is like the maximum model evidence maximizing model evidence is equivalent to minimizing surprise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1485.74,
    "end": 1493.64,
    "text": " If you knew exactly how to be minimally surprised, you would have maximum evidence, and we can balance surprise with variational free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1493.66,
    "end": 1494.702,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1494.722,
    "end": 1501.078,
    "text": "Now, model comparison rests on finding the marginal likelihoods, i.e.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1501.098,
    "end": 1502.702,
    "text": "evidence, for each model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1503.931,
    "end": 1519.858,
    "text": " So we might have three, this is, again, a really simplified model, but let's just say those three different cohorts or categories of patients, the habitual, the preference, and the world model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1520.8,
    "end": 1523.304,
    "text": "So we have a portfolio of three of these models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1524.486,
    "end": 1527.171,
    "text": "And then we can ask empirically,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1528.636,
    "end": 1543.405,
    "text": " given this and their behavior, given the input in their behavior, which one of those three models, one, two, and three, have the highest posterior evidence?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1544.971,
    "end": 1549.858,
    "text": " Which one of the, and so now, how do you approach this as a full Bayesian consultant?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1549.878,
    "end": 1553.704,
    "text": "Well, the first thing you ask is, well, what's the prior on these models being true?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1554.004,
    "end": 1556.588,
    "text": "Let's say, well, that's actually 33, 33, 33.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1556.928,
    "end": 1558.911,
    "text": "Those are all equally likely in society.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1559.772,
    "end": 1568.124,
    "text": "Or it might be known from demographic analyses that actually the habit is 90%, the other two are 5%.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1568.145,
    "end": 1570.969,
    "text": "So it's the prior on that model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1571.95,
    "end": 1574.033,
    "text": "Then we get new data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1574.958,
    "end": 1594.605,
    "text": " And then we use that new data, just like the frog jumping out of our hands, we use that new data to update and reevaluate after every question, which one of that portfolio of models has what likelihood.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1595.245,
    "end": 1598.85,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1598.87,
    "end": 1603.817,
    "text": "I'm almost thinking in this case that it makes sense that this person",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1604.371,
    "end": 1613.2,
    "text": " almost schedules a time to use drugs rather than try to give them up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1615.222,
    "end": 1630.598,
    "text": "It seems possible that the best, the way of minimizing expected and variational free energy is to actually schedule into his generative model a time to use",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1634.797,
    "end": 1639.564,
    "text": " Certainly, that is one way to reduce uncertainty.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1640.345,
    "end": 1645.533,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1645.654,
    "end": 1652.083,
    "text": "Under what context and what cognitive diversity is that healthy and compliant?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1652.103,
    "end": 1653.846,
    "text": "I don't know, but that's the question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1654.447,
    "end": 1656.33,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1656.35,
    "end": 1656.45,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1660.16,
    "end": 1660.56,
    "text": " Cool.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1660.661,
    "end": 1660.941,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1661.101,
    "end": 1666.306,
    "text": "And again, this is the black line here is the markup blanket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1666.547,
    "end": 1667.568,
    "text": "So map territory.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1668.649,
    "end": 1687.348,
    "text": "This allows us to project and entertain multiple competing models and then continually re-update that portfolio as new data come in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1688.695,
    "end": 1717.42,
    "text": " what is the imperative for our action selection as experimenters well we want to learn that's the epistemic imperative that's the good scientist and we want to control that's the pragmatic imperative and that's the good engineer and so this is this kind of blend of like good scientist good engineer that constitutes expected free energy in a pure",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1717.518,
    "end": 1740.994,
    "text": " um epistemic setting our imperative is maximally to reduce uncertainty that's the bayesian information maximizing principle and so we should be asking questions that are in our zone of proximal uncertainty conversely if all we cared about was pragmatic value",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1741.531,
    "end": 1752.09,
    "text": " we didn't want to learn anything, didn't care to learn anything, had a very fixed mindset, then we would only use declarative expressions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1756.318,
    "end": 1758.021,
    "text": "So this...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1759.03,
    "end": 1786.945,
    "text": " really nicely wraps figure 4.3 again here it's just showing a single level discrete time model but this could be a hierarchical model this could be a hybrid model this could be continuous time the this subjective model the researcher proposes um if the experimental stimuli don't differentiate hypotheses well",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1787.785,
    "end": 1789.827,
    "text": " they're not informative stimuli.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1790.408,
    "end": 1808.53,
    "text": "For example, if we're trying to test the efficacy of a cancer drug and we're testing it at dose 1,000, 2,000, and 3,000, and maybe that's all way, way, way too high or too low, those are not informative experiments.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1808.61,
    "end": 1811.093,
    "text": "They're not using our funding appropriately.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1812.575,
    "end": 1813.536,
    "text": "So how do we approach that?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1813.736,
    "end": 1815.058,
    "text": "Well, what do we do?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1815.342,
    "end": 1832.288,
    "text": " We create multiple orders of magnitude in a first experiment and then reduce our uncertainty further by pursuing data points that would give us informative values of measurement and continually update our empirical analyses.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1850.678,
    "end": 1879.639,
    "text": " understanding the technical details is not required to use these methods effectively readers uninterested in these details are invited to skip sections nine three and nine four funny very funny um okay now we take a little detour into what's called variational laplace so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1880.851,
    "end": 1905.812,
    "text": " the exact technical details of nine three are are not super important just at the type the title level variational that means we're going to be doing the variational inference so we're going to be using energy-based methods or the evidence lower bound in the Bayesian setting or the surprisal bounding so just everything we talked about variational inferences in play",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1905.792,
    "end": 1918.085,
    "text": " And the Laplace approximation broadly just means we're going to be using a certain kind of approximation that helps us fit a model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1918.526,
    "end": 1920.588,
    "text": "So just a little technical subsection.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1921.589,
    "end": 1922.73,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1922.75,
    "end": 1925.734,
    "text": "Now, parametric empirical Bayes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1925.754,
    "end": 1933.622,
    "text": "This is really a quite applicable and important concept.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1934.007,
    "end": 1953.66,
    "text": " and it relates to theoretical questions that people have, like, well, if in Bayesian analysis we need to specify a prior, then how do we know that our prior isn't just throwing us way off and actually we're just getting our assumptions back in the analysis because we have to be explicit about our assumptions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1954.561,
    "end": 1958.007,
    "text": "So again, how do we know that we're not just getting our assumptions back",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1958.29,
    "end": 1963.221,
    "text": " after it's been laundered through the data, that could be very disconcerting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1964.644,
    "end": 1973.143,
    "text": "So there's several approaches to this, and the two major approaches to dealing with this question of prior specification are as follows.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1974.706,
    "end": 1976.029,
    "text": "The theoretical",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1976.532,
    "end": 1997.562,
    "text": " approach here is to select priors from a diversity of values and even different families of priors so using different families of statistical distributions Poisson Gaussian Gamma and so on",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1997.542,
    "end": 2027.295,
    "text": " And starting from multiple different positions, if you find that across families and across fine tunings of priors, including weak priors and strong priors, you find that you converge to a single value or region, that gives good evidence that like your starting point isn't that important because you converged to an empirically derived outcome from multiple starting points.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2027.393,
    "end": 2035.346,
    "text": " So there's enough signal in your data to recover the true value or at least the true region from a range of starting points.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2035.747,
    "end": 2036.688,
    "text": "That's one approach.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2039.152,
    "end": 2048.087,
    "text": "The empirical, like ultra empiricist approach is called parametric empirical base or PEB.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2049.009,
    "end": 2052.895,
    "text": "So we're in the classroom, we're measuring children.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2053.313,
    "end": 2055.195,
    "text": " And we want to know their height.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2055.215,
    "end": 2057.697,
    "text": "We want the posterior distribution on the children's height.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2058.478,
    "end": 2069.507,
    "text": "So again, that first approach would say, let's try a Gaussian of different widths centered around 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2069.527,
    "end": 2072.95,
    "text": "And let's try a Poisson of different distributions, et cetera, et cetera.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2072.99,
    "end": 2075.673,
    "text": "And we converge to five feet plus or minus one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2077.474,
    "end": 2079.716,
    "text": "Parametric empirical Bayes takes a really different approach.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2080.897,
    "end": 2081.498,
    "text": "That says,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2082.693,
    "end": 2094.766,
    "text": " If you have empirical data, start with those empirical data and use the first measurements or all of the measurements that you get to bootstrap the prior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2094.786,
    "end": 2102.114,
    "text": "So in that case, we go into the classroom, we measure 10 children, their height is five feet plus or minus one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2102.154,
    "end": 2110.303,
    "text": "And then we take that empirically derived descriptive statistic and utilize it as our prior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2110.975,
    "end": 2114.039,
    "text": " So the prior is empirically generated.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2114.78,
    "end": 2117.424,
    "text": "And then we engage in standard Bayesian analysis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2124.974,
    "end": 2135.127,
    "text": "Here is a generalized linear model, GLM-like account of the experimental setting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2136.119,
    "end": 2156.158,
    "text": " For people who are interested in generalized linear modeling, SPM, the earlier work of, and ongoing, of Friston et al., statistical parametric modeling, statistical parametric mapping, goes into a lot more detail on this kind of notation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2163.044,
    "end": 2165.046,
    "text": "So this section is a little bit sparse.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2168.232,
    "end": 2196.332,
    "text": " essentially the point is we can model empirical data by articulating the design matrix x so that the experimental design is represented in this matrix",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2196.869,
    "end": 2221.543,
    "text": " if there was um you know hot and cold and uh seven days of the week you have one column for hot and cold and another column for day of the week one through seven and then each row in the experimental matrix would be like okay um sample 12 it was hot and it was a tuesday sample 13 it was hot and it was a wednesday sample 14 it was cold and it was a thursday",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2222.131,
    "end": 2230.781,
    "text": " And so that matrix, the design matrix, gets separated out from the measurements themselves.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2231.582,
    "end": 2235.226,
    "text": "And that allows us to do this generalized linear regression approach.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2236.187,
    "end": 2244.116,
    "text": "And that can be done within this parametric empirical Bayes framework.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2244.136,
    "end": 2246.198,
    "text": "These are very short sections.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2248.237,
    "end": 2252.062,
    "text": " And again, they were the ones that were suggested that could be skipped.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2252.883,
    "end": 2259.792,
    "text": "So there's a lot more to say, but what we get to is this is kind of like a chapter six moment.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2260.373,
    "end": 2262.796,
    "text": "Chapter six, where we had the recipe for generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2263.797,
    "end": 2267.382,
    "text": "Here we get the instructions for model-based data analysis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2267.983,
    "end": 2271.207,
    "text": "And again, this is also very closely related to SPM.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2273.684,
    "end": 2283.65,
    "text": " So here, instead of starting like chapter six with the conceptualization of the system of interest, we're going to start with a behavioral data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2284.573,
    "end": 2285.415,
    "text": "Ideally,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2286.509,
    "end": 2296.119,
    "text": " chapter six has already occurred before the grant was submitted or before the experiment was preregistered or before the data were collected so that it can be done in a principled way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2297.06,
    "end": 2305.528,
    "text": "As they always say, like, I don't know the exact quote, but like the statistician is called in like a forensic doctor to like consult on why the experiment went wrong.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2305.588,
    "end": 2312.695,
    "text": "But no one ever talks to the statistician before the experiment to make sure that the statistical power is is valid.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2313.316,
    "end": 2315.638,
    "text": "There's a much pithier quote, though.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2315.618,
    "end": 2322.867,
    "text": " But we start with this empirical step, which is all of the data that we actually collected.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2324.789,
    "end": 2328.314,
    "text": "Step two, formulate a generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2328.895,
    "end": 2337.465,
    "text": "It says here a POMDP partially observable Markov decision process as in chapter seven, discrete time, but it doesn't have to be in discrete time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2338.066,
    "end": 2342.892,
    "text": "Perhaps more generically, we could say formulate a generative model as in chapter six.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2344.627,
    "end": 2351.095,
    "text": " That model is going to be constrained to the data that were collected.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2352.096,
    "end": 2365.032,
    "text": "Not only, but if you collected temperature and humidity from a sensor, you know, you'd want a generative model that had the outputs of temperature and humidity.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2369.517,
    "end": 2374.826,
    "text": " Several components that are required for Bayesian analysis are specified.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2378.393,
    "end": 2381.979,
    "text": "And then group level analysis occurs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2385.405,
    "end": 2389.913,
    "text": "And figure 9.2 is going to summarize this in the context of that rat in the maze.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2390.835,
    "end": 2391.576,
    "text": "So here we go.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2392.552,
    "end": 2395.236,
    "text": " We have the data collection of the rat and the mace.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2395.757,
    "end": 2401.667,
    "text": "Again, step zero was built the laboratory and design the experiment and carry it out.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2401.687,
    "end": 2403.99,
    "text": "But let's just assume that the data are just presented to us.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2404.812,
    "end": 2414.307,
    "text": "We have several thousand runs from several hundred mice of where they were at what time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2416.328,
    "end": 2422.277,
    "text": " Maybe it's video data and we coarse grain it with this location detection or so on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2422.297,
    "end": 2426.624,
    "text": "But where we get to is like location through time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2426.644,
    "end": 2434.236,
    "text": "We generate a POMDP with location and time as the key components.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2434.637,
    "end": 2435.618,
    "text": "That was in chapter seven.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2437.04,
    "end": 2444.412,
    "text": "And we specify a likelihood function, which is the generative model message passing architecture",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2444.83,
    "end": 2472.525,
    "text": " for that generative model we as experimenters then this was steps three through six above specified likelihood here's specification likelihood function four specify the prior beliefs we had two conditions red light blue light and our prior was that uh",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2473.72,
    "end": 2482.67,
    "text": " They might have an effect of up to three standard deviations or up to such and such value, but we're centering it around zero.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2483.591,
    "end": 2490.979,
    "text": "Or from literature, we believe that red light has a 20% increase on turning right, but we're not too sure about that or something.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2493.141,
    "end": 2496.225,
    "text": "And then U tilde through time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2497.226,
    "end": 2503.693,
    "text": "U tilde, that's the empirical behavior of the rack.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2504.551,
    "end": 2532.837,
    "text": " our observations but its actions so here's the empirical data coming in this part two three and four are kind of like the skeleton or like the the aqueduct system of our intelligence infrastructure and here comes the data flowing through this is the actual empirical results",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2534.251,
    "end": 2563.397,
    "text": " that all comes together in this model inversion step so observations and you you are our observations the mouse's actions o are our actions its observations so we're going and theta are all the parameters very commonly theta is used just to mean generalized parameters capital theta so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2565.537,
    "end": 2572.526,
    "text": " David Price- feta all the parameters conditioned upon the generative model and the experiments input output.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2574.008,
    "end": 2579.415,
    "text": "David Price- is proportional to so that's what allows us to fit these models relatively.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2580.897,
    "end": 2582.68,
    "text": "David Price- parameterization given the model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2584.562,
    "end": 2588.467,
    "text": "David Price- multiplied by factorized by separated by.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2588.507,
    "end": 2593.654,
    "text": "David Price- The observed behavior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2595.136,
    "end": 2599.241,
    "text": " conditioned upon theta, our stimuli and the model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2600.863,
    "end": 2603.126,
    "text": "So here is the behavioral modeling.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2603.306,
    "end": 2607.711,
    "text": "This component is optimizing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2608.612,
    "end": 2618.064,
    "text": "We want our parameters ultimately and our model and our experiment itself, but maybe this is already fixed and so we can't really do anything about it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2618.845,
    "end": 2625.092,
    "text": "But we want these components conditioning upon them",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2625.157,
    "end": 2626.619,
    "text": " we want to explain the behavior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2627.36,
    "end": 2635.573,
    "text": "I mean, if we have one model that explains the mouse's behavior 90% of the time and another model that explains it 60% of the time, we want that better model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2636.875,
    "end": 2640.64,
    "text": "So that helps us parameterize the model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2642.603,
    "end": 2645.848,
    "text": "And then that parameterization of the model",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2646.908,
    "end": 2675.059,
    "text": " we can use in a generalized linear regression framework with the design matrix and the effect sizes and a noise term so now here we have on the y-axis is um you know preference for red light or something and here are the five mice that we have and that was represented in our design matrix we had the red and blue light and then we had the five different mice and then we're going to plot",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2676.322,
    "end": 2701.772,
    "text": " just standard statistics once we've gotten to this step so here's the empirical data step in one step two is where we make the cognitive model steps three and four are kind of like further specification of the parameterization and the cognitive model",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2703.338,
    "end": 2727.49,
    "text": " step five we it's like a particle Collider or a particle inverter or something we bring all the pieces together and then what comes out is a super standard group level parametric analysis and these are the kinds of figures that you would see in a scientific paper",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2729.528,
    "end": 2734.713,
    "text": " We looked at five different age cohorts, and we looked at some cognitive variable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2735.774,
    "end": 2747.564,
    "text": "Again, if all you wanted was the behavioral, if you want it on the y-axis, just probability of going left or right, well, then you could just go straight from one to six.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2748.585,
    "end": 2751.128,
    "text": "You would just plot, okay, mouse one through five.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2752.109,
    "end": 2753.15,
    "text": "Here's the 50-50 line.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2753.19,
    "end": 2754.391,
    "text": "This one went 40%.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2754.691,
    "end": 2756.953,
    "text": "This is 75%, plus or minus.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2758.334,
    "end": 2763.962,
    "text": " So if you didn't want to do cognitive modeling, you don't need steps two, three, five, or four.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2764.924,
    "end": 2766.947,
    "text": "You just do descriptive data analysis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2768.248,
    "end": 2780.967,
    "text": "But if you want to engage this cognitive modeling loop with an explicitly specified cognitive model in a Bayesian inference architecture, here's what it looks like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2792.27,
    "end": 2798.518,
    "text": " 9.6 is going to give brief examples touching upon continuous time and discrete generative models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2799.219,
    "end": 2800.862,
    "text": "And the papers are referenced here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2800.922,
    "end": 2815.541,
    "text": "Essentially, these papers model the same system of interest, which is where metabasian inference has been successfully exploited in terms of different kinds of behavioral data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2816.062,
    "end": 2821.129,
    "text": "And in both cases, these are eye-saccade, eye-movement tasks.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2822.78,
    "end": 2826.865,
    "text": " on the left is a continuous time, continuous action setting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2829.328,
    "end": 2832.532,
    "text": "On the right is a discrete action case.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2833.733,
    "end": 2837.037,
    "text": "So continuous time model on the left, discrete time model on the right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2843.566,
    "end": 2848.872,
    "text": "And then again, once you get to step six above, after three, four, five,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2849.459,
    "end": 2869.257,
    "text": " then you're in a total standard statistical inference setting because you're looking at linearized relationships or canonical covariates and parametric estimation at the subject level",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2874.417,
    "end": 2882.104,
    "text": " 97 is ending the chapter with an overview on so-called false inference, but there's so many aspects to this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2883.206,
    "end": 2889.712,
    "text": "A benefit of using active inference is that it addresses multiple dimensions of psychiatric disorders.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2890.873,
    "end": 2900.222,
    "text": "For example, behavior, behavioral level, psychological level, and biological level.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2900.827,
    "end": 2907.276,
    "text": " I would say biological level includes behavioral and psychological and molecular, but this is how they've laid it out.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2913.545,
    "end": 2918.672,
    "text": "They speak to this topic of Bayes' optimality and disorder.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2918.732,
    "end": 2929.146,
    "text": "And there's a ton to say about the socialization and pathologization of cognitive diversity and different people's contexts and everything like this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2929.312,
    "end": 2951.725,
    "text": " um but just broadly working within the pathological um way of thinking here are different computational pathologies that people have done mainly friston at all have done this roadmap on so they've done the chapter six and they've done the chapter nine",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2952.008,
    "end": 2962.562,
    "text": " And now here are the empirical papers that bring it together, provided unique explanations and predictions about these pathological settings.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2962.903,
    "end": 2980.547,
    "text": "Addiction, impulsivity, compulsivity, delusions, hallucination, interpersonal and personality disorders, oculomotor syndromes, pharmacotherapy,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2982.012,
    "end": 2987.702,
    "text": " prefrontal syndromes, visual neglect, disorders of interoception.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2994.374,
    "end": 3006.636,
    "text": "Here's a little bit speaking to the early categorization that we discussed with like, is somebody's behavior due to their habits, policy prior, E,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3008.725,
    "end": 3010.95,
    "text": " Are they due to their preferences, C?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3011.512,
    "end": 3014.018,
    "text": "Are they due to their world model of how things change, B?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3014.619,
    "end": 3018.248,
    "text": "Are they due to their sensory perception, A?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3021.255,
    "end": 3025.485,
    "text": "All of our, all the variables that we know and love",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3027.997,
    "end": 3055.219,
    "text": " these are the knobs in the model this is like you make the circuit board with the nodes and the edges these are simple ones but they could be more sophisticated though that's the circuit board that's the skeleton that's the aqueduct system and then fine-tuning the model is about the the actual values quantitatively that these different nodes and edges take upon",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3059.924,
    "end": 3078.611,
    "text": " they're going to discuss a few specific interpretations and connect the computational components of these different pathologies to, for example, specific neurotransmitter systems.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3079.492,
    "end": 3089.526,
    "text": "Neurotransmitter systems were brought up in chapter five where acetylcholine, dopamine, serotonin, and so on were associated with certain computational roles.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3090.62,
    "end": 3095.345,
    "text": " And so this is how we get model reification for better and worse.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3096.327,
    "end": 3105.397,
    "text": "If we find that empirically impulsive decision-making is associated with increased dopamine, that may or may not mean anything, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3105.537,
    "end": 3106.618,
    "text": "Correlation is not causation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3107.66,
    "end": 3120.154,
    "text": "Then if we find that increases in dopamine increase impulsive decision-making, decreases in dopamine decrease impulsive decision-making, all of a sudden we're getting into a more multifaceted account",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3120.269,
    "end": 3121.971,
    "text": " that's actually adding a lot of value.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3123.273,
    "end": 3125.597,
    "text": "And so that's how the chapter basically ends.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3126.678,
    "end": 3128.181,
    "text": "They lay out the six steps.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3129.462,
    "end": 3139.377,
    "text": "And so chapter nine is kind of like chapter six, not just because the number could be rotated, but because chapter six helps us build that generative model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3140.398,
    "end": 3145.045,
    "text": "And then chapter nine helps us actually imbue it with data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3151.167,
    "end": 3171.724,
    "text": " So it's an interesting chapter and it connects really, really closely to the kind of empirical behavioral work that in some sense we all do, because all the previous chapters, it's like, if you're not a theoretical Bayesian modeler, those chapters seem like they're very removed from settings.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3172.852,
    "end": 3177.579,
    "text": " Whereas here, we're in the field watching the ants.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3177.779,
    "end": 3179.341,
    "text": "We're in the room with the patient.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3179.461,
    "end": 3181.003,
    "text": "We're in the lab with the mouse.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3182.285,
    "end": 3185.249,
    "text": "And I think that makes the analysis that we do a lot more tangible.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3190.096,
    "end": 3196.345,
    "text": "Anyone have any sort of thoughts or questions or what they might want to explore next week in this?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3219.089,
    "end": 3223.736,
    "text": " Cool yeah well we'll see where we get next week, please.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3226.54,
    "end": 3241.101,
    "text": "submit questions, whatever cohort wherever you're at let's add a few more questions in basic or advanced or or related or not questions and next week we'll approach the questions more head on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3242.443,
    "end": 3242.623,
    "text": "So.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3244.506,
    "end": 3245.367,
    "text": "Any last comments.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3256.653,
    "end": 3265.667,
    "text": " cool all right thanks daniel thank you all see you thank you very much bye",
    "speaker": "SPEAKER_02"
  }
]
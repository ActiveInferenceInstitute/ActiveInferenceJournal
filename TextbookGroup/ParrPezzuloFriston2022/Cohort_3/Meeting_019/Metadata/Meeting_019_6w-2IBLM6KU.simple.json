[
  {
    "start": 3.068,
    "end": 12.969,
    "text": " All right, it's July 18th, 2023, and we're in our second discussion on Chapter 8.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 14.152,
    "end": 21.347,
    "text": "So where would anyone like to begin or any general remark on Chapter 8?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 34.557,
    "end": 37.743,
    "text": " Well, I can say something.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 37.943,
    "end": 55.133,
    "text": "I added a couple of questions to the list at the very end on box 8.1, but we can go through some other stuff first, if you want.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 56.916,
    "end": 57.257,
    "text": "Sounds good.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 65.438,
    "end": 70.545,
    "text": " All right, let's look at the first question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 86.646,
    "end": 93.555,
    "text": "Perhaps if I put some background in of my understanding.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 95.122,
    "end": 121.888,
    "text": " So my understanding is we have, well, I think if we go to, there's Raphael Boyach's paper, a tutorial on free energy, which starts off with looking at a single neuron with an input, with an input distribution of a particular mean and variance.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 121.868,
    "end": 122.789,
    "text": " Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 122.809,
    "end": 129.898,
    "text": "And, and he then builds that up to the point where you've got a multi layer system with multiple modes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 132.281,
    "end": 139.651,
    "text": "And the the synapses represent means and variances.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 142.074,
    "end": 145.378,
    "text": "So I think",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 145.865,
    "end": 167.974,
    "text": " There's, we're not seriously suggesting that synapses actually represent particular means and variances or whatever, but I think there's the idea that collectively a population of neurons is going to encode mean values and variances.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 169.49,
    "end": 175.217,
    "text": " Um, and precision is, is inverse variance, of course.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 176.279,
    "end": 198.467,
    "text": "So, uh, I suppose my other frame of reference is Gaussian mixture models where you've, you know, you've got Gaussians characterized by, uh, weights or amplitude mean and, and variance, uh,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 200.422,
    "end": 222.16,
    "text": " So then going on to box 8.1, I'm confused that there seems to be a way of treating precision as a magnitude, as an amplitude, or changing the Gaussian, perhaps by...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 222.14,
    "end": 234.693,
    "text": " Reducing the variance, increasing the precision is going to increase the peak to make it more prominent, which seems a strange way of doing things.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 239.158,
    "end": 240.299,
    "text": "Yeah, exactly.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 240.319,
    "end": 249.008,
    "text": "So for me, precision and attention are not synonyms.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 250.54,
    "end": 273.949,
    "text": " And attention seems to be something at the neuromodulation timescale, as opposed to synaptic weights at a much slower timescale or general influence with usual synaptic neurotransmitters.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 275.482,
    "end": 284.935,
    "text": " So, yeah, it doesn't make sense how, well, a lot of that box",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 303.089,
    "end": 305.953,
    "text": " You're on mute, Daniel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 305.973,
    "end": 307.535,
    "text": "Yeah, Ali or anyone, do you want to give a thought?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 307.816,
    "end": 307.976,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 318.57,
    "end": 327.763,
    "text": "So here, the precision being described is the precision to the sensory input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 329.498,
    "end": 340.051,
    "text": " and so equivalent to attention to sensory input in the sense that if the, and reflected by the A matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 341.513,
    "end": 355.25,
    "text": "So if the incoming sensory data are inferred to be accurate, then they will update the hidden state more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 356.492,
    "end": 359.235,
    "text": "If the sensory data incoming were not,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 360.059,
    "end": 375.911,
    "text": " believe to be accurate, low precision, low attention, then observations wouldn't update hidden states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 375.931,
    "end": 381.983,
    "text": "I don't think that that necessarily refers to the psychological concept of attention.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 389.371,
    "end": 412.037,
    "text": " But is your question about... Well, in terms of significance, you could have something that was more important",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 415.24,
    "end": 425.856,
    "text": " or more worthy of attention at a particular time, even though it had less... A belief was held with less precision.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 432.666,
    "end": 434.088,
    "text": "What would you mean more important?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 434.348,
    "end": 436.412,
    "text": "Like more relevant for survival importance?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 437.012,
    "end": 443.442,
    "text": "More... More relevant...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 444.468,
    "end": 456.034,
    "text": " I see it as from a Gaussian mixture model point of view, I suppose.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 456.596,
    "end": 461.667,
    "text": "So you could have a",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 462.912,
    "end": 490.692,
    "text": " Well, let's say, I think I put in the question, sort of, if you have two Gaussians, let's say you've got a single sense input of measuring the wavelength of light, and you're presented with apples and pears, and so pears cover a fairly narrow green, a spread of the green, so you've got a very sharp...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 490.672,
    "end": 515.734,
    "text": " a low variance distribution there and your apples spread right across red to green but but your input your environment might actually have uh a lot more apples than than pairs so even if you're in the in a uh",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 517.334,
    "end": 527.049,
    "text": " if you sent something that was within the pairs area of the spectrum, it still might be more likely that you had an apple.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 528.812,
    "end": 530.294,
    "text": "That's exactly the prior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 535.802,
    "end": 545.417,
    "text": "And that's factorized away from the attention to the sensory input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 546.612,
    "end": 552.178,
    "text": " the D is coming into the S from the left side, and the attention to sensory input is coming up from the bottom.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 554.301,
    "end": 572.181,
    "text": "So you're able to attend to sensory input or not, but it is very, I believe, useful to consider what happens here in terms of the variational estimation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 574.675,
    "end": 577.838,
    "text": " where there's no action, we're not choosing which thing to look at yet.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 583.764,
    "end": 603.805,
    "text": "But still in this mapping, the fruit identification region of the brain, the rate code would represent the posterior on apple to pear, zero if it's an apple, one if it's a pear, and then the variance would represent",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 605.118,
    "end": 625.55,
    "text": " the precision on the sensory input mapping to it, such that a high precision belief, which is to say low precision on sensory input, would have high inertia as a belief.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 628.855,
    "end": 630.197,
    "text": "I think that's accurate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 634.75,
    "end": 640.937,
    "text": " I think part of the issue is that precision can be on any variable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 642.519,
    "end": 644.361,
    "text": "Uncertainty could be on any variable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 646.784,
    "end": 648.306,
    "text": "So it has to be clarified.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 649.908,
    "end": 651.189,
    "text": "Precision on what?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 651.229,
    "end": 656.235,
    "text": "And this is on precision on sensory input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 656.873,
    "end": 679.928,
    "text": " And I think a question that Sanjeev answered in perhaps the most recent meeting, attention weighting rescales the attention to different variables is what dictates the influence that they have on the overall free energy gradient descent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 682.591,
    "end": 696.689,
    "text": " If you have two variables and they both are equally attended to, then the free energy is a 50-50 compromise such that there's going to be a flat manifold where one of them is getting better by one point and one's getting worse by one point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 697.49,
    "end": 711.147,
    "text": "Whereas if you're attending to one of them 10 times more than the other, so it's 10 times more precision on that belief, then each increment of free energy minimization, you'd be willing to accept 10 decrements on the other one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 712.797,
    "end": 726.486,
    "text": " So the attention blends the role of each parameter being optimized on the overall scalar gradient descent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 734.161,
    "end": 739.088,
    "text": " Yeah, I've still got a bit of a block on that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 739.268,
    "end": 741.131,
    "text": "I'll go away and I'll think about that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 741.351,
    "end": 745.717,
    "text": "But I think what you've said just leads naturally on to my other point.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 746.338,
    "end": 756.152,
    "text": "So as I see it, as you say, that rebalancing of your tension,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 756.132,
    "end": 770.907,
    "text": " So if we have attention given to extraceptive input, then you're sensing the outside world, you're treating that as important.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 770.927,
    "end": 774.495,
    "text": "If you then shift to proprioceptive input,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 775.724,
    "end": 798.255,
    "text": " attention then that's that's more likely to to lead what that would lead to action or there's there's there's the whole thing about sensory attenuation why why does sensory attenuation happen is it a random thing",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 799.99,
    "end": 808.26,
    "text": " There's the splitting across and there's a neuroscientific model framework.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 808.28,
    "end": 813.466,
    "text": "And then there's also actual, we're trying to do some engineering stuff here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 814.467,
    "end": 829.305,
    "text": "But on the neuroscience side, well, generally, are we saying that there is some oscillation, flip-flopping between",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 830.534,
    "end": 833.978,
    "text": " sense and effectively action?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 835.099,
    "end": 846.192,
    "text": "Or is it just something that's happening randomly that things will drift into an action phase or a sensory balance?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 850.597,
    "end": 852.319,
    "text": "Yeah, I think there's a lot of pieces there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 853.941,
    "end": 857.004,
    "text": "So systems don't have to have sensory attenuation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 858.006,
    "end": 860.068,
    "text": "If having a constant",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 860.369,
    "end": 877.734,
    "text": " regime of attention is simply how the system is defined by Fiat or at some level of generative modeling for some variable, there may not be any reason to propose this cognitive phenomena.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 879.537,
    "end": 889.752,
    "text": "However, whether you look at isocating and visual input, where we have like precision, high precision on the sensory input when we're fixed,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 890.204,
    "end": 898.055,
    "text": " And then we pull off the gas when we move so that we suppress input while the visual field is very shifted and then re-engage.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 898.075,
    "end": 919.165,
    "text": "So whether we think about like the eye saccade attention to visual input, or if we think about the motor control example in box 8.1, which is exactly analogous, it's just interoceptive instead of exteroceptive, where there's a suppression of sensory input",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 920.073,
    "end": 925.839,
    "text": " to support action selection instead of belief updating.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 925.859,
    "end": 944.14,
    "text": "In both those cases, a kind of adaptive, emergent cybernetic pattern is this oscillation of attention to enable the benefits of high attention to sensory input",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 946.651,
    "end": 954.28,
    "text": " but then also be able to transiently engage another mode where input to sensory data is lowered.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 956.643,
    "end": 972.142,
    "text": "So I think that just turns out to be a solution or an approach to navigate the trade-offs because you wouldn't want to just have high attention or only low attention.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 974.285,
    "end": 975.426,
    "text": "So then...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 976.722,
    "end": 979.51,
    "text": " the navigation solution oscillates between them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 983.442,
    "end": 989.28,
    "text": "So we're saying that this is just something that happens to be found",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 992.163,
    "end": 1002.52,
    "text": " In real brains, fit into a model of real brains, it's not something that is inherent within active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1003.121,
    "end": 1011.395,
    "text": "It's not something that appears in the discrete time domain, is it?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1012.437,
    "end": 1016.584,
    "text": "I guess you could model sensory attenuation in discrete time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1017.121,
    "end": 1022.828,
    "text": " But correct, sensory attenuation is not a necessary part of active inference agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1023.429,
    "end": 1024.33,
    "text": "So why is this useful?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1024.851,
    "end": 1036.386,
    "text": "Because this pattern helps us understand the cognitive similarities between speaking and listening, where the bird song example is a famous case of sensory attenuation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1036.426,
    "end": 1039.069,
    "text": "Like you expect the song to be playing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1039.089,
    "end": 1041.372,
    "text": "So as long as it's playing, you listen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1041.433,
    "end": 1044.917,
    "text": "And then when it goes away, then now you take action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1045.91,
    "end": 1050.098,
    "text": " And then while acting, your precision decreases.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1050.398,
    "end": 1052.361,
    "text": " until you eventually cease acting again.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1052.441,
    "end": 1061.875,
    "text": "So turn-taking conversation, eye saccades, motor control with proprioceptive, the cognitive patterns can be identified.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1062.577,
    "end": 1071.049,
    "text": "And those are accounts of complex phenomena that are first principles and don't appeal to reward or reinforcement.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1072.391,
    "end": 1078.06,
    "text": "They appeal to some specific computational phenomena",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1078.04,
    "end": 1088.237,
    "text": " that maps between the neurobiology, loosely like you brought up with the mean and the variance, and the statistical parameterization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1089.459,
    "end": 1102.68,
    "text": "And then that model generates unique explanations and predictions and say, oh, well, maybe this dyskinesia is a challenge of initiating movement and this one's a challenge of terminating movements.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1106.626,
    "end": 1111.551,
    "text": " Yeah, yes, and there's a tie-in with various mental illness as well, isn't there?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1111.771,
    "end": 1115.535,
    "text": "Sort of schizophrenia tied in with sensory teneration.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1119.66,
    "end": 1132.954,
    "text": "It kind of reminds me of the work by Adam Safran of Albus, because Friston and Carhart-Harris had this Rebus paper, Relaxed Beliefs Under Psychedelics,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1133.828,
    "end": 1140.177,
    "text": " And then there was like this other proposal was like CBIS, strengthened beliefs under psychedelics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1140.197,
    "end": 1143.982,
    "text": "And then Adam was like, well, it's kind of just altered.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1144.002,
    "end": 1155.598,
    "text": "I know that's really general, but it's just like increasing the precision on this one might then decrease the precision on something else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1155.678,
    "end": 1160.224,
    "text": "So there's not just a precision variable anymore.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1163.444,
    "end": 1169.093,
    "text": " It has to be a little bit more unpacked with precision on what beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1170.455,
    "end": 1181.252,
    "text": "Because we could talk about beliefs on sense states, about beliefs on latent states, beliefs on action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1181.272,
    "end": 1182.795,
    "text": "It's all belief updating.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1182.835,
    "end": 1184.237,
    "text": "It's all Bayesian beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1186.276,
    "end": 1198.107,
    "text": " Can you think, is there any reference that I, perhaps I should read that might get me thinking about precision in a better way?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1198.147,
    "end": 1213.482,
    "text": "Maybe the birdsong, Friston and Frith birdsong example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1213.833,
    "end": 1220.162,
    "text": " And also maybe with Sanjay, if we can ask and see, his simulations seem very amenable to it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1221.323,
    "end": 1230.736,
    "text": "Yeah, I think possibly because I've got it fixed in my mind, the idea of synapses encoding variants.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1233.179,
    "end": 1238.727,
    "text": "Whereas, as you say, there's variants much more broadly.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1240.208,
    "end": 1244.012,
    "text": " in the concept of making inferences posteriors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1249.797,
    "end": 1252.981,
    "text": "That's where I don't really have a good handle.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1253.161,
    "end": 1256.424,
    "text": "I think this is a barrier.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1256.544,
    "end": 1257.285,
    "text": "I have a barrier.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1258.486,
    "end": 1259.787,
    "text": "So how do I get through the barrier?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1261.449,
    "end": 1267.194,
    "text": "Yeah, that's kind of a hard topic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1267.215,
    "end": 1267.735,
    "text": "And",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1269.79,
    "end": 1278.08,
    "text": " It just reminds me of the end of seven here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1279.381,
    "end": 1294.819,
    "text": "Like we call D the prior, but of course, once you escape the most trivial pedagogical generative model, then it makes more sense to be clear, this is a prior on hidden states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1295.795,
    "end": 1300.304,
    "text": " because you're also talking about a prior or hyperprior on other variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1312.45,
    "end": 1316.959,
    "text": "And also that was like kind of the motivation for the generalized notation notation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1317.175,
    "end": 1329.015,
    "text": " So that we could, which was originally called generalized subscript notation, because we wanted to be able to talk about like a preference on preference or a policy on preference or a prior on preference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1330.518,
    "end": 1338.351,
    "text": "Because when you're in the nested models, you have to clarify which term you're talking about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1338.391,
    "end": 1341.276,
    "text": "Hmm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1352.022,
    "end": 1357.05,
    "text": " Okay, yeah, so I'm pretty much happy with the rest of the chapter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1357.07,
    "end": 1360.837,
    "text": "It's that, it's just that one box that I need to get around.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1360.977,
    "end": 1367.848,
    "text": "So yeah, so like you said, the birdsong thing, so I've read that in the past, that makes sense.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1368.65,
    "end": 1376.022,
    "text": "But it's this ambiguity I have around the word precision, and how it",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1377.689,
    "end": 1386.663,
    "text": " Well, that one phrase of precision and attention being synonyms, it's that that I've got the block on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1388.005,
    "end": 1390.308,
    "text": "Let me try one more angle on this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1390.989,
    "end": 1395.917,
    "text": "So the sitting up and sitting down example from the sensory attenuation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1396.578,
    "end": 1405.131,
    "text": "So let's just say that our proprioceptors give us like 90% confident information",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1407.119,
    "end": 1408.961,
    "text": " when we're in that state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1409.161,
    "end": 1412.304,
    "text": "But when we're moving, it gives us 50-50.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1412.705,
    "end": 1413.846,
    "text": "So it's totally ambiguous.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1414.466,
    "end": 1420.372,
    "text": "But then it's 90-10 once we're in the seated or the standing state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1421.353,
    "end": 1428.861,
    "text": "So we have a true hidden state, which is our actual posture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1430.102,
    "end": 1435.067,
    "text": "And then let's just say that we're getting seated input because we're seated.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1435.435,
    "end": 1439.6,
    "text": " But because of a cognitive decision, now we prefer standing input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1441.223,
    "end": 1453.559,
    "text": "Now, if the pragmatic value were scaled super high, then you would take the policy to stand up because the pragmatic value would just overwhelm everything else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1454.6,
    "end": 1464.733,
    "text": "However, if the pragmatic value is not scaled super high, then there's a little bit of like a frustration because",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1466.03,
    "end": 1478.689,
    "text": " it's your preferred state, but not so preferred that it's overwhelming the belief updating to be anchored closer to the sitting down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1478.709,
    "end": 1481.313,
    "text": "So you have kind of these two peaks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1481.774,
    "end": 1487.022,
    "text": "You could be drawing from this peak of observations sitting down, or you could be drawing from the other peak.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1489.306,
    "end": 1494.754,
    "text": "And then even if the other peak is slightly higher,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1496.202,
    "end": 1500.346,
    "text": " the optimization still holds to the left side.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1501.307,
    "end": 1508.213,
    "text": "And then by increasing the variance, reduce your precision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1510.315,
    "end": 1525.348,
    "text": "Then in that ambiguity of the sensory input, then the preferred state, then you can select action to stand up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1526.611,
    "end": 1536.03,
    "text": " It's like temporarily flattening the landscape so that you can make a move without needing to want it more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1539.908,
    "end": 1564.965,
    "text": " Yeah, I think something you said at the beginning that was very significant, I think, because I'm thinking when we're talking about precision, it's precision of beliefs of sensory input, whereas there's also precision in states and between priors and new input.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1567.449,
    "end": 1568.43,
    "text": "It's that balance.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1575.751,
    "end": 1577.454,
    "text": " See what else happens.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1580.939,
    "end": 1582.061,
    "text": "Okay, Lockeville Terra.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1582.101,
    "end": 1590.434,
    "text": "Thomas Pard explains some of this, like the historicity of this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1591.396,
    "end": 1602.433,
    "text": "Lockeville Terra was used to describe structured sequential activities in the continuous time setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1609.703,
    "end": 1610.464,
    "text": " Okay, learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1622.262,
    "end": 1625.707,
    "text": "A functional S that plays the role of a free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1634.299,
    "end": 1638.225,
    "text": "This could definitely be unpacked a little more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1639.741,
    "end": 1657.189,
    "text": " But it seems like this is a continuous time, free energy based method for gradients based optimization of mean and variance of the Laplace approximation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1661.356,
    "end": 1664.601,
    "text": "But it seems like it could use a little bit more context.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1672.968,
    "end": 1681.7,
    "text": " I'm right in thinking so, so if if we visualize a normal distribution and we're saying we increase the precision.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1683.323,
    "end": 1688.33,
    "text": "We can think of that as the shape the change of shape of the distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1689.872,
    "end": 1691.354,
    "text": "So the peak goes up.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1692.756,
    "end": 1694.959,
    "text": "So it's it is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1696.34,
    "end": 1698.923,
    "text": " That that's the right way of, of thinking about it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1698.963,
    "end": 1701.485,
    "text": "We're changing the shape of the normal distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1702.506,
    "end": 1708.512,
    "text": "So, so hence, uh, increasing, increasing the position.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1708.792,
    "end": 1721.284,
    "text": "So reducing the variance is, is the same thing as increasing the, the peak at the, at the mean value, which is the mode as well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1722.546,
    "end": 1722.846,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1723.607,
    "end": 1723.967,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1727.542,
    "end": 1727.882,
    "text": " Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1728.744,
    "end": 1731.427,
    "text": "It sharpens it when it's more tended to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1731.467,
    "end": 1742.763,
    "text": "And this is sort of like the two faces of the KL divergence is mean-seeking versus mode-seeking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1746.327,
    "end": 1754.899,
    "text": "And attention modulation influences which one of those it goes to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1754.98,
    "end": 1760.189,
    "text": " as well as where that optimization starts in the landscape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1763.975,
    "end": 1773.551,
    "text": "Because if it's too attentive to get out of a local optima, then it becomes functionally mode-seeking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1775.614,
    "end": 1780.903,
    "text": "Because whichever mode it gets to, it'll just think, okay, I'm doing as well as I can.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1782.959,
    "end": 1792.094,
    "text": " because also Laplace doesn't, um, the Gaussian has support everywhere, but a Laplace is a downward facing parabola.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1793.076,
    "end": 1795.179,
    "text": "So it doesn't have infinite support.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1796.862,
    "end": 1799.286,
    "text": "Whereas, so it's not exactly a Gaussian mixture model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1803.453,
    "end": 1806.638,
    "text": "And then the mode seeking, um,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1807.732,
    "end": 1824.069,
    "text": " It just tries to find whatever the highest point is and then fit a Gaussian around it versus the mean seeking tries to cover as much of the distribution and match the center of gravity with the central tendency.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1827.613,
    "end": 1831.697,
    "text": "So in the case of the sitting up and standing, standing, um, like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1833.601,
    "end": 1837.927,
    "text": " you're usually wanting to be pretty precise about your body's position.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1838.547,
    "end": 1839.509,
    "text": "So you're mode seeking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1839.689,
    "end": 1841.812,
    "text": "You're mode seeking, you're locally, you're sitting down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1843.013,
    "end": 1849.181,
    "text": "Then it's like the attention relaxes, that moves the KL to be more like mean seeking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1849.201,
    "end": 1851.344,
    "text": "Now it's like 50-50.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1851.364,
    "end": 1857.091,
    "text": "50% you're prior, half the time you're standing, half you're seated, and the sensory data are ambiguous.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1859.034,
    "end": 1860.756,
    "text": "And you prefer to be standing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1861.445,
    "end": 1868.636,
    "text": " So then that preference breaks the symmetry and then you can resharpen your attention back when you're standing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1871.32,
    "end": 1878.23,
    "text": "So then the KL can have kind of this like end to end reduction of divergence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1880.233,
    "end": 1885.14,
    "text": "And the only knob that you had to change was the attention.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1889.879,
    "end": 1895.766,
    "text": " Now how to adaptively tune attention, that's a question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1896.788,
    "end": 1908.262,
    "text": "But that's why it's interesting conversion to evolution that we see the same cyclic attention strategy in the Cicade and in the standing example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1911.145,
    "end": 1914.79,
    "text": "So we see that as a repeated heuristic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1923.038,
    "end": 1937.321,
    "text": " And UMDP chapter seven has largely superseded the use of generalized lockable Terra and active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1937.341,
    "end": 1944.893,
    "text": "That's kind of interesting because that's like saying the discrete time has kind of taken over from the continuous time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1946.536,
    "end": 1946.616,
    "text": "Hmm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1948.3,
    "end": 1972.626,
    "text": " Yeah, and I've made the point before, so a lot of those sort of classic examples, including the bird song in the SPM-12 is based on the SPM-ADM, so Active Dynamic Expectation Maximization model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1973.007,
    "end": 1977.852,
    "text": "So that's all within the continuous, whether this...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1977.832,
    "end": 2001.257,
    "text": " continuous time frame uh concept as opposed to pomdp um so it's it's like that's yeah yeah it has been superseded um because all of the so much of the conversation now is about pomdp yeah",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2002.351,
    "end": 2010.485,
    "text": " Eventually, we'll have every model annotated in the literature so we can look at the trends and which ones were used.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2010.545,
    "end": 2014.612,
    "text": "But suffice to say, Lockeville Terra, I don't see this coming up recently.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2016.976,
    "end": 2021.423,
    "text": "And continuous times are definitely not as frequent as POMTPs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2021.909,
    "end": 2025.093,
    "text": " I mean, PyMDP only describes continuous time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2026.334,
    "end": 2038.249,
    "text": "And it's just so much closer to the setting of discrete time, explicit planning, counterfactuals, reinforcement learning, reward.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2038.309,
    "end": 2044.356,
    "text": "The discrete time formalism is a lot closer to it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2046.53,
    "end": 2056.105,
    "text": " Yeah, it's more useful for sort of high level sort of cognitive functioning as opposed to the basic movement in an environment.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2058.048,
    "end": 2060.872,
    "text": "But that's kind of the cool like Internet of Things question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2060.892,
    "end": 2068.284,
    "text": "Like what if you could have a, you know, a wing or like a water, you know, something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2068.324,
    "end": 2073.432,
    "text": "And then all that it was really like endowed with",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2074.019,
    "end": 2077.586,
    "text": " was heuristics around attention modulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2079.009,
    "end": 2081.474,
    "text": "Maybe that only takes like two lines or something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2082.756,
    "end": 2094.88,
    "text": "And then like with this heuristic around attention, then it can do like in a continuous time sensory capacity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2095.568,
    "end": 2102.603,
    "text": " Whereas you could train a POMDP and do all these big like matrix calculations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2102.623,
    "end": 2104.246,
    "text": "I mean, that's sort of the question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2105.048,
    "end": 2114.488,
    "text": "Where do these different models have different computational advantages or explainability advantages?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2122.197,
    "end": 2123.098,
    "text": " Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2123.699,
    "end": 2123.98,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2124.0,
    "end": 2124.36,
    "text": "Oh yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2124.48,
    "end": 2125.422,
    "text": "These are great questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2126.663,
    "end": 2136.337,
    "text": "Also the, uh, the recent model stream nine, I'll just reloading it with Aspen, Paul.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2139.242,
    "end": 2140.584,
    "text": "Um, here it is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2144.069,
    "end": 2145.911,
    "text": "He presents a computational.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2146.292,
    "end": 2151.9,
    "text": "The only other time we've seen computational complexity estimates was with, uh, branching time, active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2155.694,
    "end": 2157.236,
    "text": " I thought this was super fascinating.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2157.276,
    "end": 2176.896,
    "text": "So here's, so classical ACT-INF, SI, sophisticated inference, timestep 30 horizon or two horizon, DP, dynamic programming, AIF, one is just single timestep ACT-INF.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2178.878,
    "end": 2182.242,
    "text": "So 30 timestep, sophistication,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2185.647,
    "end": 2215.418,
    "text": " 10 to the 68 complexity cognitive complexity and just like how i just found it really interesting how the sophisticated inference blow up is so high and and uh he's using dynamic programming based methods",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2216.377,
    "end": 2221.282,
    "text": " to learn smooth priors on preference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2225.046,
    "end": 2240.982,
    "text": "So I think there's a lot of different, there's a lot of different little motifs that might have very effective performance in a certain setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2241.742,
    "end": 2245.506,
    "text": "Like in the setting of a camera that moves",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2249.131,
    "end": 2254.737,
    "text": " isocating and blinking might converge to being an adaptive strategy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2260.124,
    "end": 2275.982,
    "text": "So finding out which generative models and phenomena parsimoniously describe these different natural systems might be very helpful for",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2276.502,
    "end": 2281.049,
    "text": " them being able to compose them together, then understand, okay, where do we need?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2281.109,
    "end": 2284.814,
    "text": "Thank you, Ali.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2285.916,
    "end": 2286.457,
    "text": "Talk to you later.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2286.517,
    "end": 2294.048,
    "text": "Where do we need different different models?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2297.033,
    "end": 2304.123,
    "text": "I think I asked him, like, can you do like the full, can you do like this approximation and this part of the nested model and that one in this part?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2304.183,
    "end": 2305.345,
    "text": "And he said, yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2307.265,
    "end": 2312.115,
    "text": " So you could even implement two different approximations for different parts of the same generative model, I think.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2321.755,
    "end": 2323.86,
    "text": "Okay, then generalized synchrony.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2324.14,
    "end": 2327.788,
    "text": "Okay, here's where the bird song comes back into play.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2337.792,
    "end": 2340.035,
    "text": " bird hears the song, there's no need to generate it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2340.275,
    "end": 2343.339,
    "text": "If the bird predicts a song that is not heard, it must start singing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2352.65,
    "end": 2356.035,
    "text": "Learning here, it's described more in the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2356.155,
    "end": 2359.699,
    "text": "I assume that it is learning according to this box.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2370.53,
    "end": 2374.995,
    "text": " This leads to turn taking, sometimes phrase to sing from the same hymn sheet.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2378.399,
    "end": 2380.882,
    "text": "Yes, it is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2380.902,
    "end": 2394.158,
    "text": "And therefore, it's not really like a model of improvisational communication or necessarily even communication as information transmission.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2395.589,
    "end": 2402.878,
    "text": " They're literally just, it's like there's something that has to be carried and they just like, one of them carries it until the other one picks up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2404.661,
    "end": 2414.914,
    "text": "So it's like, it's a very basal aspect of communication because the model is really just trying to describe turn-taking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2418.623,
    "end": 2424.491,
    "text": " Acting to generate birdsong requires a reduction in the precision of predictions about the consequences of action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2424.571,
    "end": 2425.973,
    "text": "I'm not sure why that is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2430.399,
    "end": 2437.228,
    "text": "Why couldn't you take action and also increase your precision about the consequences of your actions?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2439.271,
    "end": 2440.773,
    "text": "That'd kind of be like action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2441.714,
    "end": 2448.343,
    "text": "I mean, might even be connected to something like OCD, where action...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2448.931,
    "end": 2468.439,
    "text": " that action that when you take it, blurs your precision is kind of like self-terminated because eventually you'll just wobble out of that behavioral attractor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2469.18,
    "end": 2474.888,
    "text": "Whereas if there's a behavior that is increasing your precision,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2477.247,
    "end": 2485.026,
    "text": " So you're attending more and more to the sensory outcomes that that action is seeking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2485.066,
    "end": 2493.366,
    "text": "Then I feel like that action becomes like tighter and tighter, like an attractor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2497.767,
    "end": 2510.64,
    "text": " But something like, I mean, even though a lot of times different neuropathologies and stuff come up, I think the reality is a lot of these are really sophisticated, multi-level cognitive phenomena.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2510.66,
    "end": 2516.727,
    "text": "And so it's just kind of pointing in that direction, but it's not really, it's not the same thing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2525.676,
    "end": 2526.677,
    "text": "Generalized synchrony.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2528.193,
    "end": 2533.919,
    "text": " Again, sing from the same hymn sheet.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2537.963,
    "end": 2539.664,
    "text": "They have learned to share the same narrative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2539.824,
    "end": 2557.862,
    "text": "I think if you consider the song to be a narrative and also open question is like, how much is it is already like baked in and then it's being fine-tuned versus it's not like they're starting with two different singing concepts and then actually learning the whole thing, but just...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2567.258,
    "end": 2574.528,
    "text": " Okay, then hybrid models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2581.178,
    "end": 2584.643,
    "text": "Single models that contain both continuous and discrete variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2587.103,
    "end": 2592.519,
    "text": " In this case, the lower level is continuous time, x, x prime, x sub prime.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2592.679,
    "end": 2597.333,
    "text": "The higher level is discrete time, past, present, future.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2598.325,
    "end": 2604.954,
    "text": " This is the same family of model that Ryan Smith et al explored in Livestream 46.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2606.356,
    "end": 2615.127,
    "text": "Active inference does not contradict folk psychology, where the lower level is the motor active inference, and then the higher level is the decision-making active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2616.469,
    "end": 2617.851,
    "text": "There's supposed to be an edge here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2619.433,
    "end": 2627.083,
    "text": "Message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2628.127,
    "end": 2633.258,
    "text": " procedures exist for graphs that have discrete and continuous time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2634.48,
    "end": 2639.01,
    "text": "So they were kind of brought up separately just because of pedagogy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2640.132,
    "end": 2646.085,
    "text": "But ultimately, it seems like there's no issue with mixing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2655.835,
    "end": 2672.62,
    "text": " The mapping between discrete and continuous time happens by basically giving discrete state spaces integer values in some continuous state space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2674.603,
    "end": 2680.712,
    "text": "Like here, you could associate the coordinates",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2682.852,
    "end": 2687.578,
    "text": " with the discrete location, but then also have it be a continuous space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2689.079,
    "end": 2707.982,
    "text": "So then you, I guess, can have the discrete time model and the continuous model in the same state space being like kind of described in one from the discrete side as zero one, and then in the continuous one from continuous space from zero to one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2731.25,
    "end": 2733.173,
    "text": " This is a little bit of the.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2736.718,
    "end": 2738.32,
    "text": "This is an ice arcade model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2738.881,
    "end": 2741.705,
    "text": "Probably this is a useful paper to look more into.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2741.725,
    "end": 2755.965,
    "text": "16 neural populations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2758.589,
    "end": 2760.852,
    "text": "Four locations in four time steps.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2772.307,
    "end": 2774.735,
    "text": " One, two, three, and four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2774.755,
    "end": 2779.631,
    "text": "But I don't know why they stay active.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2780.053,
    "end": 2781.658,
    "text": "I don't know why they don't drop back down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2802.766,
    "end": 2806.99,
    "text": " Well, and here's what's happening in this is the person is being told to look at the cross.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2808.471,
    "end": 2813.816,
    "text": "Then a dot is being shown at the top of the screen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2813.856,
    "end": 2822.365,
    "text": "There's some 200 millisecond delay that represents like visual processing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2824.066,
    "end": 2826.769,
    "text": "Then they accelerate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2828.11,
    "end": 2830.292,
    "text": "Here's their point of maximum speed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2831.774,
    "end": 2840.521,
    "text": " They accelerate and then decelerate and fixate on the cube, where the cube was.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2846.146,
    "end": 2848.969,
    "text": "And each of these four traces represents beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2852.992,
    "end": 2861.359,
    "text": "But again, I'm not sure why it's not just like one comes up and then it goes down, the second comes up, it goes down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2868.798,
    "end": 2872.321,
    "text": " Okay, mixture models, Gaussian mixture models like you brought up earlier.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2872.381,
    "end": 2887.836,
    "text": "What do you think the relevance is of the Gaussian mixture here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2887.876,
    "end": 2895.143,
    "text": "Neil?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2900.135,
    "end": 2901.637,
    "text": " Hmm.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2904.802,
    "end": 2908.668,
    "text": "There's reference to Delta functions here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2908.728,
    "end": 2909.529,
    "text": "So precise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2911.031,
    "end": 2913.936,
    "text": "Well, totally precise priors there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2921.948,
    "end": 2923.29,
    "text": "So I'm not sure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2925.987,
    "end": 2932.656,
    "text": " where the Gaussian comes in other than that bottom equation, the normal distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2940.025,
    "end": 2943.53,
    "text": "The priors on the mean and the variance are precise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2943.57,
    "end": 2949.177,
    "text": "So we're basically just looking for a point estimate on mean and a point estimate on variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2951.468,
    "end": 2969.089,
    "text": " not looking to have a secondary uncertainty about mean and a secondary uncertainty about variance that's the delta part and then i guess the i subscript is the number of mixtures",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2978.283,
    "end": 2991.945,
    "text": " and maybe different observations are being allocated or learnt to the different mixtures, categories.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2995.41,
    "end": 2999.797,
    "text": "So the optimal, if you had one mixture, that'd be the same as a Gaussian.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3000.098,
    "end": 3005.927,
    "text": "If you have three mixtures, then you'd be trying to parameterize three different Gaussians with different means and variances.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3006.177,
    "end": 3009.361,
    "text": " and then assigning each data point to one of those mixtures.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3011.503,
    "end": 3026.319,
    "text": "And then the question of which number of mixtures of groups should you have is approached through Bayesian model reduction, Bayesian information criterion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3035.169,
    "end": 3035.349,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3035.37,
    "end": 3035.83,
    "text": "8.1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3037.7,
    "end": 3056.856,
    "text": " Birdsong, oculomotor condition reflex, smooth eye pursuit, psychosis, illusion, saccades, action, observation, attention, hybrid model, self-organization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3086.603,
    "end": 3092.713,
    "text": " Yeah, interesting and kind of very sparse and brief chapter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3096.599,
    "end": 3110.181,
    "text": "I feel like it is very short and just barely touches on a few pieces.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3111.865,
    "end": 3116.856,
    "text": " but I guess provides information where there's other continuous time models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3124.151,
    "end": 3124.653,
    "text": "What do you think?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3129.563,
    "end": 3129.904,
    "text": "Yeah, nice.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3129.924,
    "end": 3131.848,
    "text": "It was a very good chapter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3133.702,
    "end": 3158.562,
    "text": " this this is this is more where my interest lies as you might guess on the continuous time side of things um i just have that one difficulty around the box having accepted what's in the box everything else follows fine it's it all hangs together quite neatly um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3161.123,
    "end": 3163.386,
    "text": " But yeah, there are various things that have been said.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3163.486,
    "end": 3164.647,
    "text": "I need to go away and digest.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3164.947,
    "end": 3177.502,
    "text": "Just like you're saying, a point estimate on variance, whereas I'm thinking of... Perhaps I'm confusing, getting estimates and variance confused.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3178.523,
    "end": 3181.706,
    "text": "There is variance included in the estimate.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3183.128,
    "end": 3184.95,
    "text": "Right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3187.193,
    "end": 3191.097,
    "text": "Variance is like the mean on our variance estimator.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3193.726,
    "end": 3198.153,
    "text": " And usually we don't go two levels deep.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3199.915,
    "end": 3209.61,
    "text": "Like usually the standard deviation, there's not an SD on the SD, usually.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3211.232,
    "end": 3216.239,
    "text": "That's just the point estimate on mean and variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3220.285,
    "end": 3220.746,
    "text": "And then...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3222.717,
    "end": 3226.424,
    "text": " But then it's like, but the variance of the mean, isn't that just the variance?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3232.575,
    "end": 3236.282,
    "text": "How is the variance different than the precision on the mean?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3239.908,
    "end": 3240.089,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3240.97,
    "end": 3245.839,
    "text": "I mean, let's think about the class, you know, the people are 10 feet tall, plus or minus five.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3248.873,
    "end": 3253.68,
    "text": " So variance is five by virtue of the actual variation in height.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3255.502,
    "end": 3260.129,
    "text": "But the precision on the mean, if we sampled a million creatures, might be very low.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3263.934,
    "end": 3267.6,
    "text": "So we might have a very confident estimate it's 10 plus or minus five.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3271.585,
    "end": 3275.691,
    "text": "Or we might have a very imprecise estimate that it's 10 plus or minus one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3280.565,
    "end": 3299.281,
    "text": " And then in the variational gradient descent scheme, attention reweights all the variables so that the gradient descent strikes a balance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3299.301,
    "end": 3310.15,
    "text": "But that just begets the question, how does the attention get trained so that in a multivariate optimization, variables are being traded off appropriately",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3310.45,
    "end": 3317.794,
    "text": " including situations where there's a dynamic reallocation of optimization focus.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3319.563,
    "end": 3347.173,
    "text": " How is the whole system, how are those parameters of the chaotic attractor tuned so that it does follow contained trajectories stable rather than just going off and disappearing down, getting stuck into one attractor rather than another?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3349.752,
    "end": 3350.093,
    "text": " Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3351.936,
    "end": 3363.636,
    "text": "Yeah, well, for a system to be able to descend into an attractor and to escape, it kind of has to be poised on the edge of chaos or have some dynamic or oscillatory component.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3365.359,
    "end": 3369.246,
    "text": "Otherwise, it just gets attracted to where it's attracted to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3369.53,
    "end": 3370.412,
    "text": " But okay, cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3371.254,
    "end": 3371.635,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3371.755,
    "end": 3375.023,
    "text": "Well, tomorrow's the first math learning group or, or so.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3376.046,
    "end": 3382.02,
    "text": "So people are welcome to come to discord if, if they want, but otherwise see you all next time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3383.584,
    "end": 3383.985,
    "text": "That's great.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3384.285,
    "end": 3384.807,
    "text": "Thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3385.468,
    "end": 3385.608,
    "text": "Bye.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3386.21,
    "end": 3386.29,
    "text": "Bye."
  }
]
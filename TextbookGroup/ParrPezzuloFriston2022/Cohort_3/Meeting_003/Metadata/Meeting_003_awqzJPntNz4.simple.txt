SPEAKER_01:
hello it's february 8th 2023 we're in cohort three and we're having our second discussion on chapter one so just a few side notes before we jump to the questions uh first i'll place this in the chat but we can um hey maria are you recording the the meeting

yes i'm recording it with obs like on my local computer okay i'm sorry oh no thank you for checking the icon okay thanks for checking yes it doesn't mention it it's like a yes so last week we talked about neats and scruffies and those terms are due to roger shank and it was actually last week that roger shank had passed away which i have just placed in the chat so

auspicious though tragic and people may find a lot of in this article by harold jarch a lot of interesting um thoughts on learning by doing teaching others to do this is the next step in the education revolution and also there's a student's bill of rights that i had never seen from shank before

so there's a lot of interesting um pieces here so for those who are interested take a look and maybe see how we can bring some of that back to the textbook group and then also awesome thanks yeah

and then um i don't see yana here but yesterday yana and i went to the step-by-step active inference paper which i also put in the chat it's one that we reference all the time because it has a step-by-step building of like static perception dynamic perception these are passive inference and then action comes into play and we used the ontology to describe what was happening

and used LaTeX in Coda to write the equation and then made some tables that allowed us to flip between different ways of talking about what was given and what we're calculating wrote the pseudocode

made tables for every variable and interpreted every column so this was just for the static example but it was a lot of fun and I'll also put that

Thanks, Jonathan.

That sounds cool too.

Yeah, so this was a lot of fun that we had.

And there's definitely some walls that we run up against, like doing vector and matrix multiplication in Coda, but it's a fun direction for making really interactive and multimedia systems.

All right, so let's go to chapter one.

does anyone want to give any thought or comment at all on chapter one any thought or or question at all and then we'll jump into the questions that the cohort has had on chapter one

feels like a long while now since reading it could have been last week could have been last year yeah feel free to raise your hand or write in the chat any comment or question that's presenting itself to you today or we will look into the questions that the cohort wrote

it looks like we may have done this last time what is the relationship between policy and environment maybe anyone can just give a summary or thought on that and then we'll move it up okay yeah very informative comment thread Brock go for it and then anyone else


SPEAKER_03:
Policies are selections on affordances, like latent affordances in the environment.


SPEAKER_01:
Sequences, yeah.

Then let's look at the comment thread.

generative process stomach is empty stomach emits a rumble that's a great example because the generative process helps frame it in terms of what it is which is the process that passes observations to the blanket this doesn't have to be outside quote of the organism so the true status of the stomach is an external state

with respect to the blanket states being interoception and the internal states being cognitive modeling of interoception generative process is just whatever passes observations so it doesn't have to be the niche but in the case of exteroception vision hearing etc packets coming into a server

then it is outside of the agent, but it doesn't have to be spatially outside of the agent.

Jonathan?


SPEAKER_00:
Yeah, we said that the generative process is the thing that passes observations.

Presumably, it doesn't have to pass an observation.

There can be things happening in the outside world that we don't have access to, but may still be part of our generative, what is actually the generative process of the real world.


SPEAKER_01:
Great point.

The generative process does pass observations to the blanket, but that is not all it has to do.

So it could have, the generative process could have 10 variables.

Like the thunderstorm could have temperature, light, sound,

but then we only have a microphone.

So we're only perceiving sound.

So the generative process is passing us sounds, and then that is being used by the agents, but that's not to say that the generative process is only sound.

And that comes back to a slightly more general point, which is that the state space of the generative process doesn't have to be the same as the state space of the cognitive model.

and generative processes can be other generative models in the social setting as jonathan is pointing to that's sometimes called thinking through other minds and let's look at this it's cool and we can see a previous question on the notion of surprise is the agent's perception not only affected by the influence

of its environment but also an agent's peers so the agent's peers are updating their beliefs in a collaborative fashion what would be the extent of the update to an agent's GM of perceptions if an agent witnesses the annihilation of one of its peers for example something like this okay it's a wildebeest people can check out the video

OK.

Michael, please.


SPEAKER_04:
I just have a question with respect.

Previously, you wrote, Don, that a policy is a sequence of affordances.

And I was wondering, is it a sequence or is it one affordance?

Is a policy one affordance, or is a policy a sequence?

So can it be more than one affordance?

I was of the understanding that a policy is one affordance.

But maybe I'm confused there.


SPEAKER_01:
Yeah, good, good.

question and policy and affordance are used slightly differently for example in the reinforcement learning setting but we won't necessarily go there so the affordance are the actions that can be taken in a single moment alternative actions that can be taken

so this could be movement up down left right this could be an eye saccade up down left right policy is going to be a sequence of those for a given time horizon so if the time horizon is one or the temporal depth of the model is one this is the same as affordances but if the the temporal depth

were two, then it would be all combinations of two affordances.

So up-up, up-down, down-down, left-right, right-left.


SPEAKER_04:
But how does it then work with epistemic and pragmatic affordances?

At some points in the book, it says that these are two aspects of one affordance.


SPEAKER_01:
yes great we we are going to get there and here's what it's going to look like just to peek forward in chapter two so um pi our policy pi for policy g is expected free energy expected free energy is going to be calculated for every single policy

So again, if that is temporal depth of one, it's just equivalent to the actions that can be taken in that moment.

If this is a temporal depth of five, then it's the expected free energy of up, up, up, up, up, up, up, up, up, down.

Now, those different policies are going to be evaluated according to this single imperative, expected free energy.

that's going to rank different policies based upon their posterior likelihood.

In reward slash reinforcement learning, you would rank policies by their expected reward.

In FEP, we're going to rank policies by their expected free energy, which is going to describe their posterior likelihood.

different policies might be seen as likely for different reasons and this equation can be partitioned in different ways to understand that so a policy might be seen as really good let's just say it's good but that means the same thing as saying it's a likely policy if it secures a lot of information gain on the other hand a policy could be seen as good or likely if it secures a lot of pragmatic value

those both get projected down into the expected free energy.

Now, conversationally, people will say something like, well, opening the book, that's like an epistemic affordance.

That's compatible with what we're talking about here, but the partition here would be like, the affordance is to open the book or not open the book.

And then the action selection to open the book

is selected because it has a high information gain.

So the idea, and one that we'll continue to revisit, is that you can get behavior that's able to exhibit pragmatic value-seeking components, and when those are unclear on how to pursue, come into an information-seeking mode through a unified imperative G.

so we still do have epistemic afford epistemic affordances in the conversational sense but to be strict about it there are just affordances which are the capacities for action and then different policies are selected or not and we can partition that differently and that partitioning may reveal that a policy was selected because it reduced information had high epistemic value

reduced informational uncertainty, should I say, or it was aligning observations with preferences, which is pragmatic value.

Can we relate policy with goal?

I'm going to just copy in these answers.

page 101 chapter 5 will will come there okay any other comments or questions on this topic okay ali


SPEAKER_05:
Actually, about Miria's question that if we can relate policy with goals, well, maybe, I don't know if I can reiterate that example I pointed out in the last session about picking up a cup.

So yeah, actually, the goal is somehow inherently

consolidated into our actions and our policies, or at least at our goal-directed actions.

So I pointed out to an example from Ritsalati and Senigallia's book, Mirrors in the Brain, which says that

Picking up a cup of coffee can be done in various ways depending on whether we are drinking from it, in that case we would grasp it by the handle, or if we're washing it, in that case we would grasp it by the rim or moving it out of the way.

In that case, we would grasp it by the body.

So yeah, actually, in this scenario, selecting the appropriate policy would necessarily require considering the goal of that policy or action.


SPEAKER_01:
Yes, thank you.

And we can talk about goals.

but the way that it plays out in the generative model, there isn't a variable that's called the goal.

So here, Jonathan wrote, the goal is really about minimizing expected free energy.

So yes, some kind of meta imperative is to fulfill the minimization of free energy.

But let's think about like, the goal is to move the coffee cup to the drying rack

that is actually going to be embedded in the preference we expect and prefer to see the cup on the drying rack oh i observed it in the sink i'm surprised maybe even anxious what are the affordances well without going too deep into a motor model we could like grab or not grab move arm left move arm right

and then some policy like move arm left grab move arm right release the grab that's a four-step policy that has pragmatic value

again looking forward but we're going to come to this in the coming weeks that policy has value and it's like a goal-oriented policy because we reduce the divergence between our preferences and observations so goal-oriented or pragmatic behavior is loaded heavily in this pragmatic value section which at its heart has to do with the

um relationship between preferences and observations that's why we talk about like expect slash prefer like we talk about it as a preference but the way that it plays out is again not by preferred states are the most rewarding but preferred states are the most expected

be least surprised when your body temperature is in its preferred value when your body temperature is out of your preferences you'll be surprised how are we going to bound that surprise free energy just as it's done in bayesian statistics how are you going to balance surprise there's two ways you could change the world or you could change your mind changing your mind depending on the time scale is like perception or learning

Changing the world happens through action.

So let's just say we're again surprised at seeing the coffee cup in the sink.

Changing our mind might look like updating our model of where we expect to see coffee cups so that we're not surprised.

Oh, it's normal that it's in the sink.

So now I'm not surprised anymore.

So we effectively reduce surprise by learning.

or you could reduce your expected free energy by taking sequences of action and whether a generative model learns or how it learns or or whether it acts and how it acts that has to do with the details of how the model are set up

Why exploration and exploitation are automatically balanced through policy selection and active inference?

What does it mean to say both are balanced?

Any thought or related idea here?


SPEAKER_00:
Jonathan and anyone else?

Yeah, I mean, I think to really understand it, you have to understand the equation that you were just showing us, where the information gain is about exploration.

We can think of it about exploration and the pragmatic is about exploitation.

And so to really understand how that's automatically balanced, you have to understand exactly where that equation comes from, I think.


SPEAKER_01:
awesome totally agree yes it's captured in expected free energy calculation so this equation is not necessarily the solution it's not a strategy to balancing explore exploit like depending on what variables are set

you could result in a agent that always goes for the right slot machine or always takes the hint.

So it's not that just by framing it this way, the problem has been solved.

but this as it turns out creates an incredible flexible framework for fitting parameters and testing different models that include the wide diversity of cognitive phenomena like attention and all of these other complex phenomena so it's like linear regression y equals mx plus b

that's not a solution to the regression it's the space in which lines are going to be compared to each other so expected free energy isn't the solution to adaptive policy but it's a space in which policies can be compared with each other how are they going to be compared with each other with expected free energy what are some decompositions of expected free energy

it turns out to have some very useful decompositions that connect us with on one hand in the top line the semantics of value broadly in terms of informational or epistemic value and pragmatic or utility value so that's one huge connection because we can think about special cases like if there's no information to be gained

expected free energy is going to get only pragmatic value if there's no pragmatic value then it is equivalent to a pure information novelty approach and it can do mixes

there's other decompositions that connect to the middle lines here have to do with decision and control theory and then this bottom line points the way towards connections with physics where we see energy and entropy for example in the gibbs free energy so that's kind of a doorway into physics and thermodynamics

and it's this these are all just different decompositions of the same functional so it um it's a an advocate's claim that active inference simply balances explore exploit

but a slightly more measured angle would be it frames the problem so that generative models can be constructed and parameterized that are able to adaptively navigate explore and exploit any

preference for a question to go to next, or anyone, please feel free to just raise any question that's coming to mind.

Michael, and then anyone else.


SPEAKER_04:
Yeah, I was wondering this distinction between exploration and exploitation.

Does it only apply to the expected values or also to the variational free energy?

Good question.


SPEAKER_01:
we will come to explore the similarities and differences between variational free energy, figure 2.5, F, and expected free energy, G, equation 2.6.

In short, variational free energy, F, is taking in as arguments Q, our beliefs, the variational distributions that represent our beliefs, and data.

So in variational free energy, we're actually not engaged in policy selection.

In expected free energy, we are engaged in policy selection.

So variational free energy is a common measure used in machine learning and statistics because of its ability to effectively fit models based on data.

and then to really take the full active turn and talk about not just optimal fitting of data but not overfitting which is like what variational free energy does in the expected free energy case is when we actually get into the consideration of alternate affordances yeah Michael than anyone else


SPEAKER_04:
Yeah, so you're saying that exploration, exploitation relates to the policy, so somehow to the future, right?

But I can also engage right now into an exploration or an exploitation.

Isn't that the case?

So I mean, it does not only relate to the future, I guess, does it?

or what I'm misunderstanding there.

So I can explore right now, not only in a plan for the future.


SPEAKER_00:
OK, Jonathan?

Yeah, I'd say that both exploration and exploitation are sort of active things.

Even if it's in the moment now, it may be some sequence, it may be relatively short, but it's still something about what's happening now and where I want to be.

So I think that even if you're thinking about your exploration in this moment, it is really about what's happening in some future time that you're attempting to do.


SPEAKER_01:
Nice.

Thank you.

And it's great.

Yeah.

And and also like exploration, exploitation are kind of informal terms to describe different behavioral outcomes.

They're not necessarily like formal.

And then also a great comment there in the chat.

Um, I'll just put it in here.

this is a good reminder that like affordances and policies it's like always come back to map and territory there's the affordances that the person can engage in and that's like the territory if you really want to think about it and wonder about it like what can a body do but as modeled there are modeler degrees of freedom in which affordances are going to be considered for the agent to do so

go to the vending machine.

Like that could be, you could have the affordance go to the vending machine or go or not go.

It's just binary.

Or you could have raise your quadricep muscle and flex your calf muscle.

So that's going to be a model that can do a lot of other things, but then you're going to have to have all of this parameterization just to be able to get out of the chair.

So how you coarse grain action is one of the major

modeler degrees of freedom and challenges in action modeling we'll come back to this more times any just offhand comment or question that someone has

Everything following the question mark is just a tracking device.

Okay.

It's a Twitter thread.

We're reading books.

Why you want to go with a computational approach?

Why not just go full on Gibson plus dynamical systems?

Well,

This should be very relevant.

I'll just bring in an update here.


SPEAKER_05:
Actually, Maxwell also had a response on that tweet.


SPEAKER_01:
Okay, let's look at that.

What?

Do you remember, or what was the response?

Oh, he had a response to this first one.

There's active inference.

Wow.

Look at that foundational.

Atlas shrugged.

Do you remember what his response was, Ali?


SPEAKER_05:
Yeah, he somehow...

The gist of it was that those two are not necessarily mutually exclusive, but actually he's writing a paper to show the relevance between the two.

And yeah, they're more compatible than some people might think.


SPEAKER_01:
me find the exact tweet thank you yeah like if it comes to conceptual understanding almost everything is compatible it's like just how flexible are you willing to be but there are things that are incompatible and then the question is like kind of so what or what is the bigger framework that those are

or the context in which those are or aren't compatible um excitingly though this will be tomorrow um i'll put the paper in the chat this is uh so jay benjamin fallen days will join for a guest stream a potential mechanism for gibsonian resonance behavioral entrainment emerges from local homeostasis in an unsupervised reservoir network so i hope to um

see a accessible presentation on modern gibsonian modeling and drawing out some similarities and differences that are real or perceived with active inference everett and then anyone else


SPEAKER_02:
This Gibsonian or computational approach, this also links to the question, is perception inferential or is it more direct perception?

And I don't really find a good argument that discusses the evidence for inferential approach to perception or ecological direct perception.

So I was wondering which way to go.


SPEAKER_01:
Yes.

There's so many angles on this.

One of them is maps, not territories.

So whatever it is that perception really is for ants or humans or computers, we're going to model it with Bayesian inference.

so that's a very strong position because then it leaves the other person debating about how maps really are while we're over here using modern computational tools to do statistical modeling and inference so that kind of sidesteps the discussion about the ontology of what is actual perception and puts it into the instrumentalist camp and then in terms of evidence the first thing that comes to mind is


SPEAKER_02:
we're perceiving colors and shapes in our blind spot so no of course it's not direct perception you don't have photoreceptors there is that conclusive so okay this is this is mainly if it's vision visual perception is mainly uh uh

used as a way to discuss the perception, but it gets more difficult if you talk about, for example, pain or olfactory perception.

And then the question is, is all perception inferential or does it depend on the kind of perception you're talking about?

Is it visual or auditory?

And I'm wondering.


SPEAKER_01:
yeah it's it's um there's definitely a lot of nuance once we step outside of the clear invisible visual setting or the tangible and comprehensible tactile setting those are very immediate

but analogies certainly apply to other sensory modalities.

And again, whatever the modality is, one can say we're using Bayesian inference.

So let's just say whatever direct perception means, how is it being statistically modeled?


SPEAKER_02:
Yeah, the way I understand the difference is that our priors have a big influence on what we have as a perception.

And in the direct perception, our priors don't influence what we perceive.

But maybe I'm too black and white here.


SPEAKER_01:
lay it out and see if it's been argued that way elsewhere or what exactly would reduce your uncertainty about that question and if no one has laid it out so simply then it will be making a contribution jonathan


SPEAKER_00:
I don't know if this answers the question at all, but I think your question there about the priors, it may be that we have very accurate incoming information and therefore, independent of what our priors are, you open a door and you don't know what's behind the door.

The priors will still have some effect, but they may have much less effect than the incoming information.

So I don't think it's a matter of one form of perception using priors and others not.

I think it's just a balance between how strong those priors are.

That's my take on it.


SPEAKER_01:
Awesome.

Yeah, exactly.

In the Bayesian inference modeling setting, we could model a case where accurate information is coming in, which is to say that the generative model has high precision

on the information coming in it's updating priors a lot it has high attention or salience so it's like whatever the new information is it just updates its prior to that so to the posterior becomes the incoming observation that's the special case of like a time series filter with a depth of one it's like whatever the last data point is just go to that so that is functionally direct perception

Whereas if we have ambiguous information coming in, there's low precision on sensory data, priors are not updating a lot, it's low salience, it's low attention, then the belief moving forward is reflecting the a priori stance, the prior stance.

So Bayesian inference is just giving us computational tools and formal methods to describe that possibility

So we can go from something that's being updated time point by time point or not, but I've never seen any kind of useful model presented other than just the appeal to direct perception.

Not saying it's not there.

Brock, and then anyone else.


SPEAKER_03:
I just wanted to bring up something that Ali continues to remind people because I think it's hard

it's easy to forget or not or look over here, is that we're using all this language about vision and in the context of us, a human, and the brain and all of this.

And actually, that sort of entails a belief about active inference being physical.

like that the blanket is physical that the senses and the observations you know actions are always kind of purely physical um or that the epistemic value the pragmatic value well that's epistemic that's information that's you know pragmatic that's always action it's not um so cleanly that is actually an informational state space that we're talking about that the blanket's in right so it doesn't um

If you remember that, I guess, is what I'm saying.

And that it is constrained in a physical substrate.

You are in a space.

You are in a time.

There is a generative process outside of you that is constraining your generative model.

There is a constraint of affordances.

you you can't be a thing with an attracting characteristic like state space um and like ignore that completely sort of um you always have priors so to ask like yeah what what how is it that we have like direct perception or

When are we ignoring our priors?

That's back to this anthropomorphic, purely physical thing.

It's like, when do the beliefs, the statistical beliefs kind of cancel out or something?

When are they purely reflective of the sense of the blanket?

That doesn't mean...

that you're ignoring your priors it just means that like like Daniel was saying is just a you know a well fit you know high resolution high um the expectation and the the blanket States as the Evolver are uh you know low surprise so thanks thanks again just physically constrained that's that's the important part is you can't


SPEAKER_01:
you have to have beliefs you have to have prayers and so thank you yes it is a double-edged sword with the appeals to physical organisms because on one hand it kind of anchors how we think about these informational spaces and relationships on the other hand it naturally leads to a lot of important and interesting questions

that very often it's map not territory but if one doesn't know the territory and is just learning the map then it's it's no bad thing to ask questions that bump up against map territory but to be aware that there is a map territory relationship

that becomes clearer and clearer as people learn about cartography how the maps are built and why and as people explore territory and then um yes some of the key uh citations in this area one of them would be the emperor's new markov blankets which was from we we had some live streams with yella in 2021

And then most recently, The Blanket Trick, which is a whole other story.

We're not going to go into it again, but I'll put the link there.

And it captured a lot of this discourse on the blankets.

okay in our last um oh then just the last thing um so Jonathan I think wrote imagine seeing something in low light or bright light there's a continuum of how strong the priors will will influence I believe the inference and that has been captured like in psychophysics which is from a long time ago small changes in brightness are more detectable in a dim room

So the question is how to bring these kinds of empirical cognitive phenomena into our statistics and inference modeling methods.

And in the eyes of some, it's a done deal.

So here Maxwell is saying that this was never an issue

get information theory in dynamical systems theory naturally so it was never an issue that doesn't mean every single researcher was aware of it but it wasn't an issue and then um the map territory fallacy fallacy which i'll also put in the chat where they i we i think we even brought this up previously but they argue that fep

is the ideal model of these kinds of map territory situations but a lot of the frustrations are related to different understandings and framings of map territory so

anyone please feel free to raise your hand just bring up any comment or thought or question otherwise we'll take a look at one or two more of the chapter one questions that people had ever yes yeah so so as you probably remember i work as a physical therapist and i treat people with maybe chronic pain


SPEAKER_02:
And it is quite essential to make a stand as to perception being inferential or direct, because if pain is the experience that comes about by integrating priors and likelihoods,

And you could make a case that the priors are, let's say, maladaptive and are determining the ongoing pain experience instead of the likelihood in this example.

But yeah.

If that's not the case, then that has quite some implications for the way I treat people.

So that's why it's also very clinically relevant to have a good reason to say, okay, I'm going for this inferential computational explanation of perception.

So just to highlight where this question is coming from.


SPEAKER_01:
thank you yeah that's a great point and so like um we're going to come to this later but with cognitive modeling so let's just say again whether or not it's really direct or inferential in the world we're going to be using bayesian inference as a tool and we find that somebody is engaging in some behavior that we don't prefer it's like a child doing something that we don't prefer or the person has expressed that they don't prefer it or however it is

With cognitive models, it's possible to ask or differentiate, kind of like a differential diagnosis, and Ryan Smith and others have done a lot of work in this area.

Like, is the person repeatedly taking this action because they think that it leads to positive outcomes?

So that it's a learning failure to understand the consequences of action?

Or do they have a preference for that outcome?

those are two different clinical situations does this person prefer x outcome and they are taking the approach that makes sense at which point you'd want to be like hey it's really cool to be x and then figure that they'll take the right path

Or do they know that they want to be somewhere, but they are not clicking on the consequences of their action in taking them to that goal?

So that kind of differential diagnosis, and like you said, is it like that the person has a high prior likelihood on, well, people like me have this kind of pain, at which point maybe any stimuli might get interpreted as that type of pain because it's likely, right?

Why not?

Or is it,

sensory attenuation is it a sensitivity to something it it could be any number of things so we can still use this differential diagnosis paradigm a little bit one step closer to being operationalized it doesn't need to pull back to like direct or inference philosophy

within the maps that we can make you can still ask about the relative contribution of priors likelihood sensory observations and so on but I think that's these are the exact kinds of application avenues where hearing people's like insights and questions on which parts of this framework are relevant for them as they continue to go deeper into it it's like that's what is great to see

okay well let's look at where we're going to be going in the coming weeks and chapters

So we're having our second discussion on chapter one.

Chapter one, there was no equations.

It was giving some big overviews and laying out a lot of the big picture for active inference as the authors see it.

Then we're gonna have two weeks as we go into a lot of the substance of the book, chapter two, three, four, and five.

Okay, what are those chapters gonna be about?

Well, chapter two and three

are going to describe the low road and the high road to active inference and we're going to talk probably a ton about low road and high road um the low road and and we can come up with just tables and tables of concordances it'll be really fun the low road is in a way asking how

How?

With Bayesian statistics.

This is how we're going to do it.

This is how these maps work.

This is how these models work.

Active inferences about cognitive modeling.

How are those models being implemented?

Bayes' theorem.

Not every Bayesian statistics equation has to do with cognition and action and perception.

You could have a Bayesian statistics

model of something that wasn't an active agent but we're interested in the generative models that do perception and action and planning as an inference challenge and then in the case of brains that has been called the bayesian brain approach so how bayes theorem chapter two low road kind of the why

is coming from the imperative for persistence or for survival or for repeated measurement so treat it lightly when we talk about the imperative for persistence and i know we've had some discussions even last week on what about these kinds of um seemingly self-inflicted injuries and all of this but just from a statistical perspective

something has to persist long enough to be measured as being that thing otherwise you won't measure it at that thing like if you're taking measurements every hour and the cloud is dissipating every five seconds you're not going to see that cloud

If the cloud is dissipating on the hourly timescale and you're measuring it every five seconds, there's going to be some equations that you can use to model the cloud moving or growing and changing.

So that's coming from the high road, this imperative for persistence, which becomes operationalized

in a way that's compatible with bayesian statistics through this concept of self-evidencing and the reduction of surprise about one's own perceptions like if you're unsurprised that you're in homeostasis you're going to be persisting as that kind of thing if you find yourself surprised at your homeostatic parameters it won't be long until you are not that kind of thing chapters two and three low road how

High road, Y. Those are importantly the low road to active inference.

High road to active inference.

That's going to bring us to chapter four, generative models of active inference.

That's where we're gonna see the kernel that is situated on one hand using Bayes' theorem as a how

and explaining self-organization and persistence in terms of a why.

Chapter five is then going to talk about the setting where active inference generative models have been studied most extensively to this point, which is the mammalian nervous system.

So that is going to bring up a lot of examples from the central and the peripheral nervous system of mammals and talk about how generative models have been used

to study those kinds of phenomena that people are interested in from motor behavior to decision making that's the first half of the book so we've kind of like gone in the skateboarding ramp or like the ski jump or whatever it is that is like chapter one it's kind of like going over the edge and now we're going to have more

equations and some technical pieces are going to be introduced along these two roads and just like with everything it's many coats of paint so read through it and and ask questions

Every question is important.

So even if it's a very big... What does this letter mean?

Why is this here?

What would this mean if it didn't have to be this way?

How does this apply to this setting?

How does this help us do this or that?

Any structure of question, if each of us can come up with a few, we'll have many.

And...

yeah specifically let's just look at what we'll come to in chapter two and then we'll close william james my thinking first and last and always for the sake of my doing this kind of speaks to this essential embedding of action in inference in active inference we start with perception as inference so that's going to cover some of the topics that we talked about today

Box 2.1 is going to introduce Bayes' theorem and probabilistic reasoning.

There's going to be an example that comes back a few times with a person holding something in their hand.

It could be a frog or an apple, and then it's going to jump or not.

And we're going to model our uncertainty about what that object is based upon what we observe it to do.

There's going to be some discussion of basic statistical concepts like support and expectation.

That is going to be applied in the setting of the generative process and the generative model.

Process like the niche, model like the cognitive agent.

Action as inference.

Previously, we were talking about perception as inference.

Now, action is gonna be treated as a variable that you can do inference on.

And both perception and action can be used to reduce discrepancies, which will also have a technical definition.

variational free energy is going to be introduced as a statistical measure that helps us bounce surprise and it will be shown what that means and then expected free energy is going to be introduced as a way that is as a way that different policies about the future different sequences of actions can be compared with each other

special cases of expected free energy collapse to various other important settings like if there's no information to glean you can pursue pragmatic value if there's no pragmatic value to pursue you can purely pursue informational ends and that's the end of the low road thank you everybody farewell
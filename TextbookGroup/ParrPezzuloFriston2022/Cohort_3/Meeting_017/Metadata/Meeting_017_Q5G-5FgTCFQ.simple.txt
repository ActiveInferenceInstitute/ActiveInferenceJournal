SPEAKER_00:
Hi, thanks everyone.

It's 4th of July, 2023, and we're in our second discussion on Chapter 7.

So we'll head back over there.

Does anyone want to type a question or add any reflection or thought or just question or area they want to look at today?

Okay.


SPEAKER_02:
hey well I could say something either yeah please um so I last time I joined was four weeks ago and I was saying about um well I was advocating having python code for um as examples uh I I went away and just coded up

at the very beginning of chapter seven of the notes, the musical notes, which was sort of transferring, converting the text into Python with his comments.

It didn't really get me.

It it showed the reasons why the matrices contain the numbers they did it didn't really say much more than that, but i've just discovered a pendant see in the book.

And in a way that goes through.

Something.

similar although it doesn't explain the matrices the precise numbers in each matrix in quite so explicit way and then in later on in chapter seven there's the maze example and again that these are all

Billing in with the application, but then it's, it's throwing, it's just formatting the data to the point where it can then be given to an SPM 12 function.

Yeah.

So MDP there without, so I would like to see

what's inside there and then be able to graph, for example, various metrics like equations, things like mutual information, novelty, to help get an intuitive

feel.

Appendix C says about being able to play around with the numbers to get an intuitive understanding, but I'd want to see more than just playing around with the matrices, the inputs.


SPEAKER_00:
so yeah so it's just an update on on where I am really thank you yeah appendix C is very sparse um and it doesn't um I mean they would have done well potentially to have just simply provided the reproducibility code for each figure without interspersing it with plain text hmm

because then it's kind of this reverse engineering process.

But what you said is absolutely right on, which is the entire POMDP, the entire figure 4.3 discrete time model, it's all the letters that we talk about, A, B, C, D, and then E, which is just given the letter V here.

and then the true starting condition and then then you just let it run now here's the spm mdp vbx variational base um x i'm sure exactly what the x is but i i've heard that this is sort of a like um

not inscrutable but it's not simple to to translate this function just straightforwardly to python which is one reason why um like many people do can continue using the um the matlab variant

Because this contains a lot of like accelerated methods that are, and it contains like a lot of extra bells and whistles to generate the figures.

And so that makes it quite easy.

But in general, it speaks to exactly what chapter six highlights, which is that active inference modeling is about representing the agent in the environment and

and then the methods for for letting the agent come to its own solutions are straightforward rather than needing to do a secondary and at some computational scale yes um further techniques are needed but to a first approximation the work of understanding a system of interest is in exactly specifying the dimensionality of the matrices

same as in PI MDP in model stream seven.

But yeah, how was it?

I mean, did you see the matrices any differently or think about the models any differently?


SPEAKER_02:
What I coded up was really just to get an idea of how things might be coded up.

And this wasn't to have something that was certainly not anything that was simulation efficient.

It was something very clumsy.

So, for example, what you're showing there with the B matrix, it would be explaining why the 97s are there.

in those particular places.

So starting off with a, well, an all zeros matrix, or maybe an all ones in this case, and I know it's normalized there, but so that you can, you're then describing the matrix B directly in terms of the,

the application you don't need the separate bits of text just just to explain what there is there so I think I think that was helpful um or would be helpful as part of the wider uh if that was taken to completion and possibly the teammates one could be taken further um and would have been more useful

yeah but but yeah it's it's for me it's it's getting inside those those functions to see what's see what's going on um because i don't have any

any real feel.

So I'd like to be able to see, well, here's free energy changing as various things happen.

And it's comprised of these different factors, or it can be interpreted in this different way so that you can then play around with the numbers, say, to put it into a position where there is novelty and you can see the

the aspect, the term of free energy that's described as novelty and see the effect on that.

So I think that would give me a much better feeling rather than the SPM 12 is just a black box solution.


SPEAKER_00:
Yeah.

yeah cool and if and if you want to we can post on the textbook group um repo so maybe we can then use it and um

You know, chapter seven eventually have every single example, potentially even in multiple languages, but sorted like according to the numbering of the textbook.


SPEAKER_02:
so at the end of probably cohort two I there was I put some comments of you know I'd be interested in setting or be involved in a group that was basically doing what I've I've said uh well to to unpick um pi MDP

And someone made a comment of, well, you could just code up the equations that are in the book.

And maybe that's the right approach.

Because I've looked into SPM12 and the data structures in there are just horrendously complicated, really.

So trying to get into that would be

would be very difficult.

But maybe just building up from the equations in the book would be would be manageable.


SPEAKER_00:
Cool.

Cool.

Thank you.

Um, anyone else want to bring up anything or ask anything?

Or we can look at the questions?

Okay, not I don't remember exactly which ones we may have addressed last time but.

Okay okay that's.

what's happening here.

logged in under a difference.

Those are the thumbs up that I knew had to be there.

Okay.

What do the rows, columns, and numbers mean in equation 7.2?

To check your understanding, could you design another B matrix that could be used in the music example or another example?

So the simple answer is no.

Each cell represents the probability of moving from a row to a column.

And these are the notes 1, 2, 3, 4.

With this 1 over 100, so that summing across, you get 1.

This musician, it has a 97% accuracy.

uh correctness in in moving forward through this song and uh as we look towards what kinds of like questions and variants would help us deepen understanding like what would it look like to be more accurate well there'd be a 98 or what would it look like if the third note to the fourth note was really hard or what would it look like if there was five notes

or write a 12-tone composition using 12 notes.

But anyone else want to have any comments or thoughts on this?

Okay.

Pretty straightforward question.

Okay.

In the caption of Figure 7-2,

It says lower left negative free energy gradients, i.e.

prediction errors through time.

Well, first issue, it's empty.

That may have been fixed in some subsequent, like not a full reprinting, but that may have been fixed, but it's empty there.

What are similarities or differences between prediction error and free energy gradients?

Are they the same thing?

What's the difference between free energy and free energy gradients?

What is a gradients?

here i'm going to look to actually this question which we looked at earlier today so this question from chapter two asked um prediction error surprise and variational free energy they're related to each other but it's sometimes confusing and um

to kind of look at this again, the best fitting model with the most model evidence is gonna have the lowest prediction error, the lowest surprise and the lowest variational free energy, but there are subtle differences.

prediction error is a simple differential between prediction and observation.

So if the prediction is 10 and the observation is 11, the prediction error is positive one.

It's in the units of whatever the observations are about temperature.

Surprises in information, theoretic quantity, every single...

data point observation is on a distribution is associated with some non-zero surprise value but it can be minimized and it's minimized when the prediction error is lowest and variational free energy is described in equation 2.5

So it's not exactly the same as surprise, but it's a bound on surprise.

That's kind of the whole point.

And it's lowest when all of these things are kind of in unison.

then ali also highlighted that surprise is what is calculated in exact bayesian inference where model evidence is maximized via surprise minimization however where exact bayesian inference is intractable then we utilize approximate bayesian inference and there's two ways that you could go about doing that one approach would be a sampling based approach

But that would be enormously computationally costly in certain settings, and you wouldn't really know when you've reached the right time to stop sampling.

But variational inference allows you to approximate Bayesian inference by bounding the surprise with a free energy, and that's the evidence lower bound.

And it gets to the last piece of the question, which is what is a gradient so gradient is a rate of change, it could be on a two dimensional.

surface, the rate of change, like just a ruler at the angle of the rate of change of that local point you could have a gradient in one dimension.

You could have different dimensionalities of gradients, but basically there's a local rate of change around some point.

And so it's like, if you have beliefs about the temperature and the 10, and then the observation comes in and it's 11.

So that's associated with some prediction error, some surprise, and some free energy gradient.

And then the gradient should steer you towards updating your prediction from 10 upwards.

Under the full attention setting, you would go from 10 all the way to 11.

Under a no attention setting, you wouldn't move at all, but you'd want to move from 10 towards 11.

That's the gradient or the flow going towards 11, whereas you wouldn't want to go uphill towards like nine.

Thank you, Ali.

Cool.

Let's replace this in the figure.

I'll do it.

Okay, any other thoughts on this?

A little bit unclear with the dark lines because they're all crossing and everything like that.

But the kind of moral of the story is when something happens, there's a transient...

um adjustment of the model but also as we we've discussed um like we're in discrete time so it's it's not immediately clear why there's interpolated continuous values okay maybe this is speaking to this

yeah so figure 72 and 77 have these curves where belief updating is happening faster and the text alludes to some sort of simulation machine yeah i believe that is the um the stock machinery of the spm package but i'm not exactly sure um

yes it does include like when we just like when we looked at the um the spm vbx it has a section on um dopamine and i think that that is being um displayed in in continuous time interpolated

But i'm not exactly sure why that happens, and I think it would be more.

Clear given our discrete time setting to just show a point.

associated at each time step.

Does anyone have like any other like thought on this.

Okay, what is factorization?

In factor graphs, in the brain, and conceptually?

What is anyone's thought on this?

Okay, let's look at what others have for it.

Okay.

So factorization is when variables operate statistically independently from one another.

So you can have the joint distribution of, let's just say A and B here, but we're not talking just only about the A and the B matrix, but actually it turns out that those are factorizable.

um but here's just being used generically and so it allows you to um break down the statistical joint estimation of a comma b into a times b so for example if you're rolling a die and flipping a coin

You can talk about the joint probability of all 12 options of heads or tails and one through six, but also you can approach that by saying, well, there's a one half probability of heads and a one sixth of getting a three.

So you're able to calculate the joint distribution via separating out the coin flip from the die because they don't influence each other.

Whereas if they did influence each other, then it wouldn't be factorizable.

um neurologically one of the most common places where people discuss factorization is with the what and the where stream in the brain and the idea here is like an object's identity should be factorizable from its for example location in the visual field or size visually because um

You want to be able to separate out what something is and where it is.

And people have identified these two streams of visual information flow that reflect the what and the where.

So I'll bring this in.

But the what and the where are these two.

Oh, funny.

Pozoa.

um these two streams of visual input so people have have kind of like it's it's one of those things where it's like the structure of the inference architecture recapitulates the structure of the problem because again we would expect vision to have um that kind of factorization between what an object is and where it is and um here

someone has added an example about um fruits and berries different properties but they're all kind of related to ripeness ollie


SPEAKER_01:
also in terms of category theory factorization means that I mean every function can be written as a kind of composite function of a surjective function.

and an injective function.

So in other words, the factorization system for a given category consists of two sets of morphisms.

So one of which have the ability to contain all the isomorphisms of the given category.

And also they're closed under the composite function.

So in this sense, factorization system can give rise to the concepts, some other concepts in category theory, such as

And also an important property of factorization system is each factorization is itself a functorial.

So two, I mean, they can be regarded as two morphisms that can be mapped onto each other by commuting arrows or morphisms.

So, yeah, it's one of the important concepts in category theory as it gives rise to some other concepts such as equivalency and orthogonality and so on.


SPEAKER_00:
Cool, thank you.

One other point and the reason why factor graphs and factorizations are even being discussed is the connections in the Bayesian graph reflect which variables have influence on each other.

So for example, O of T minus one is only influenced by hidden state of T minus one.

So it's like through the A matrix and similarly.

And so it's not the case that all variables have an edge to each other.

First off, that would cause connections like through time, which would violate the Markovian property, which says that the only way that the past can influence the future is through the present.

So that's a massively simplifying assumption.

If you think about a time series with 100 time steps, if you enable every time step to be possibly connected to every other time step, you're going to end up with

calculated out, but however many hundreds and hundreds of edges.

Whereas if you abide by the Markovian property, which is at least a defensible statistical approximation, if not even more valid than that, the Markovian property in time simplifies out how many edges you have to fit.

But also there are sparseness about like pi doesn't reach down to influence observations.

And so that first off captures something important and structural about the problem we're trying to solve.

We don't wanna even open the door to an opportunity for policy to just be like changing our observations with a thumb on the scale.

Nor do we want policy to simply

intervene directly in a hidden state but rather with the b matrix we have policy intervening in how hidden states change through time so the sparseness um relative to the all by all edges that could exist the sparseness helps embody the statistical and causal nature of the problem and it's massively computationally simplifying so that's why factorization comes into play

um related factorization and the recipe of chapter six why are there two a matrices so let's just clarify which example so this is probably about the maze yeah okay so this is about the maze on page 131 okay okay didn't see that um

there are two modalities that represent exteroceptive sensory data pertaining to where so that's just the location of the um uh rat in the maze and also there's a what modality that is like am I tasting something delicious or or not

And so just like the what and the where in the brain, there are two matrices and they have different shapes because the location, the where a matrix maps locations to location cues to location hidden states, true locations, which is here a noiseless mapping.

And the second modality maps location to whether there is no tasty queue or aversive queue at these two locations.

So when the mouse is in the center junction or down in the bottom, we're in this dashed line row.

And then in this example, the right side,

has the black circle and the left side has the white circle.

Any other thoughts or comments on this?

Okay.

I think this little test question.

Don't know if we left this for ourselves or someone else's dropping test questions for us.

Walk through the section from 131 to 135.

Explain all the rows, columns, subscripts, and values.

We just described the A1 and A2 matrices.

Columns are locations.

In the A1 matrix, these are queues of locations.

And then in the A2 matrix, these are what is at the location.

So where and what.

This is the same, except now the black is on the left instead of on the right, and so we see that reflected with.

Here we have a one.

And a 98 on the right and a one on the bottom.

Here's the one has moved up a row and the 98 is on the left.

b2 is not influenced by policy one stays at one and one stays at one so it's kind of like a matrix having the diagonal um the identity matrix for a is kind of like the trivial

noiseless perception.

Observations just map one-to-one to hidden states.

And similarly, a B matrix with an identity diagonal is kind of like trivial action.

Because if you're in this state, you stay there 100% of the time.

And if you're in that state, you stay there 100% of the time.

But that's B2.

They're just getting that out of the way.

They're basically saying whatever you do,

you're not changing the structure of the problem in terms of the location of the food.

And that's saying that the rat has no control over the context.

However, they are going to focus in on the four slices of the B matrix

So the B matrix is intermediating hidden states and how they change through time.

So the hidden state S is about the true location that the rat is in.

So there's four locations.

And the B matrix is going to describe the transitions in location under the four different action policies.

And so there are certain states that you can't, well, this is four matrices that the rat can choose.

They allow for a move from any state except for the left and right arm to any other state.

However, the right and the left arms are absorbing states.

The rat must stay.

So here we have the absorbing state.

Here's one.

If you're on the left, you stay on the left.

And if you're on the right, you stay on the right.

And then here, whether you're here or here, you transition to go into the right.

So I guess it can jump from here to there.

And...

c is the preference vector the um c c1 is for for again the location and this just kind of pushes it off the starting location so it does something

and then here in the c2 is the preference for over the context observations like the preferring the tasty food not the aversive and then just note that because these are in like natural log kind of or they're an e to the exponentiation um a six means that the attractive

Is E to the sixth more rewarding or more preferable?

D is the prior.

It has a perfect true belief about its starting location.

And regarding the context, it has a 50-50 uncertainty about the equal probability of which context it's in.


SPEAKER_02:
but this is a good question so there we've we have the two matrices the one and two so going back to what we were saying earlier if you were going to feed that into spm12 would you then um

multiply those A1 and A2 together, a sort of a Kronecker matrix to get something that was something like maybe 16 by 15 there, a big matrix.

And what we're saying is in this application, you're able to, the what and the where are separable.

um but to use use spm12 you would you would have to multiply these together um so a chronicle product or um yeah is that what would be done I think we'd have to look but my understanding is that the s true location


SPEAKER_00:
And the true awareness would be multiplied by this A matrix to give the where observation.

And then the true context would get multiplied by this, which is uninformative about which context it's in if you're in the bottom here.

And so the true context would be emitted through A2 to the context observation.

However, there's many situations, speaking a little more generally, where you can hack it into a huge single matrix.

Like you could imagine there's four locations, or like just speaking of the observations,

Um, you could have five locations with no reward five with the black and five with the white and a 15 across or in a 15 row matrix where every combination of location and, um, context is a unique one.

Yeah.

That defeats the whole point of factorization.


SPEAKER_02:
Mm-hmm yeah.

The factorization makes it usually.


SPEAKER_00:
Gareth J. More simple yeah.

Gareth J. yeah except there may be some interesting settings were very, very large sparse matrices.

Gareth J. may have some special computational representations that actually might make that efficient.

Gareth J. But not in this scale.

How can we read equations such as 7.8?

Okay, this is a little bit of a general comment that might be better fit for the math learning group.

So I'll just throw it over there.

And now to 7.8.

We don't have the natural language description, human written.

However, let us go to the AI.

See how it does.

Okay, 7.8, LaTeX, Coda AI columns.

Okay, this is a pretty...

straightforward reading the epistemic value i of pi is being calculated um based upon the posterior predictive entropy and the expected ambiguity not very helpful explanation this one is why would a high school student be interested in it they may not be interested

Think higher of high school students.

Here's a column asking about where it might be important, and here's defining the variables.

So not super helpful for exactly what we're looking for, but it gives a look.

But to return to the question of reading it,

Anyone want to just pick a line and read how they see it?

Okay, I won't go verbatim, but the epistemic value, so we're just looking at the epistemic aspect of the equation 2.6, expected free energy.

h is an entropy so it's describing how blurry or peaked a distribution is and this is about the observation distribution through time tilde conditioned on action so if our action is associated with very blurry observations then it's going to be high entropy whereas if the observation

um entropy is is very low then those observations have have um lower entropy so if um yeah if if we're getting like a a broad spectrum of observations from policy

that would have higher epistemic value whereas like if there if you know that every time you um open this book it's like this it's going to give you the same 100 time steps the same data then it's not as informative and here is an entropy about essentially the a matrix because it's discussing the relationship between the observations conditioned upon hidden states um

also being conditioned upon the policy but this kind of goes without saying about the conditioning of over policy because basically everything is going to be conditioned upon a policy because the epistemic value is something that's computed on a per policy basis here

Matt Bolian- mutual information which Neil alluded to earlier it's a very interesting quantity, especially because, like it's it's an equivalence.

Matt Bolian- We have Q of s given pie on both sides, so this part is like kind of not changing for a given policy.

Matt Bolian- And here we have P of observations given hidden states it's like the a matrix.

relationship to the variational posterior on observations given policies so again everything's conditioned on policy so it's kind of like p o s difference or divergence with q of o so if um s is um it

If the O's are the same and S doesn't change anything, then the divergence is low.

If you can imagine if S tilde added nothing, conditioning on S added nothing, conditioning on the true temperature, the sequence of thermometer observations was the same, then knowing the true temperature didn't help you at all.

So the mutual information would be low.

Whereas if knowing the hidden states were to provide information, then P of O conditioned on S would differ from Q of O. I'm not 100% confident on any of these readings.

I'm just kind of just saying them.

And this last one is, um, again, after you kind of ignore the pies here, it's Q of S Q of S condition on observations.

So if the observations are meaningless, then this term is zero.

Whereas if the observations do matter, conditioning on O does matter, then there is, um,

a positive information gain and i'm not exactly sure what this little supplement is adding

other than describing the factorization relationship it's saying the variational distribution of hidden states through time condition on policy and observations so kind of like our inference about true temperature depending on what we do and the observations from the thermometer is defined as equal sign with a triangle and here's essentially a factorization

probably derivable from the bayesian theorem and the sparsity of the generative model pos is like the a matrix q of s given pi is basically the b matrix and then this is um the downstairs bit which was seen up here ali


SPEAKER_01:
Dmitriy Kuniskyi, Thanks yeah well, if I may give a kind of more informal reading of this equation so.

Dmitriy Kuniskyi, Here well this equation basically means that both I mean extrinsic and epistemic value.

uh work together to somehow increase uh the um the likelihood of our preferred outcomes uh with by minimizing the uncertainty so for example that

The extrinsic value depends on the posterior predictive entropy, as shown in the first line, which can only be informative if the agent can be certain about a current state or its current state.

So it means that epistemic uncertainty should first be resolved before any kind of action or any kind of utility will come into play.

And also, at the same time,

if the agent is confident enough in its current estate, it shouldn't somehow

undertake or perform in a kind of epistemic actions that would seek to pursue a kind of successful plan.

And especially this is related to the exploitation exploration dilemma.

So briefly, as also we talked about in the previous session,

Well, minimizing free energy corresponds directly to approximate Bayesian inference by choosing the least surprising outcomes.

So if the agents could somehow model their environments, they need to have posterior beliefs about the amount of the control they have

upon the state transitions producing those outcomes.

In other words, we need to somehow take into consideration the posterior beliefs about those controlled states, which by turn relies on prior beliefs about those controlled outcomes.

Yeah, I mean, by using this kind of principled way of resolving the uncertainties, or in other words, resolving the prior uncertainties based on the control states that can minimize the expected free energy, we can somehow

achieve a theory that can provide a formal definition for that epistemic value or the value of the information we can gain from the situation.

Also, it somehow focuses the

the importance of the purposeful behavior that also that also relies on the generative models that generate those or

Rameen Mohammadi, consider those future outcomes, so this is basically the formulation that.

Rameen Mohammadi, includes or unifies some of the perspectives or pre established perspective, such as the kale control theory.

Rameen Mohammadi, and many other.

previously established perspectives and approaches.

So in case anyone wants to further explore the derivation of epistemic value and the implications of it, I highly recommend one of the earlier papers by Friston et al., namely Active Imprints and Epistemic Value from 2015.


SPEAKER_00:
Oh, awesome.

Thanks.

Sten 2015.

Great.

I'll add it in here.

Yeah, a lot of, well, we know pragmatic value.

Unlike reward maximization, our active inference concept of pragmatic value is the alignment between preferences and observations.

So that does the kind of kernel work of preference satisfaction and homeostasis

And then epistemic value, depending on the sophistication of the generative model and what other features it contains, like learning and nested structure, attention, and so on, epistemic value does a ton of work.

And that's exactly what we point to as what is missing in, for example, reward learning.

Two huge issues.

First off, it works well when you know what the reward function is, but discovering it is an issue.

And even when you do, and if you do discover it, or a proxy or a heuristic or however you want to think about it,

All you've really done is just coerced, shoehorned different components of epistemic value into the currency of pragmatic reward.

So with equation 2.6 and expected free energy, we get to delineate epistemic and pragmatic value.

And then the challenge of parameterizing these models is to balance them.

For example, if the preference vector of the maze was six one billionths, then maybe it's not as big of a deal in comparison to the epistemic value.

And in contrast, where the preference is 100 million versus negative 100 million, you see more overall pragmatically oriented behavior.

So expectation pursuing aversion repulsing.

Well, an interesting chapter, learning, more technical details, some equations that we haven't always gone through, but again, I hope for all of these equations that we end up with natural language and pseudocode and code representations, more examples that are just sort of briefly touched upon,

Massive topics like structure learning just being essentially alluded to.

And hierarchically nested models, in this case, a discrete time and a discrete time model.

You can tell because of the minus one t, t plus one.

Little figure 4.3, fractal figure 4.3.

And then another kind of discrete state space discrete time but here nested model it's kind of like a three four song like 123223 with this nested time scale.

that's seven.

Well,

next week we um turn to eight continuous time I think that we've um hopefully surfaced a lot of opportunities for improvements of the um questions and discourse code equations explanations and so on because seven definitely is uh moving lightly through multiple examples

and results and topics and and so I think there could be a lot of good work with us um sweeping our trail and adding questions that we wish would have been asked or would have been added to uh connect the dots and then chapter 8 is going to head into the continuous time setting

and close on hybrid models, which have continuous and discrete time components before chapter nine goes into the model, uh, database modeling and chapter 10 concludes it.

So thanks everyone.

Great times looking forward to, uh, yes.

See you at the, uh, at the alternative or whatever time next week on lucky seven 11.


SPEAKER_02:
Okay.


SPEAKER_00:
Thank you.

Farewell.

Bye.
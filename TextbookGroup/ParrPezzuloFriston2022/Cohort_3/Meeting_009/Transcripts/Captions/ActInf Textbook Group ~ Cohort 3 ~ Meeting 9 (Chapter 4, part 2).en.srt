1
00:00:00,840 --> 00:00:05,460
all right greetings it's March 22nd 2023

2
00:00:05,460 --> 00:00:08,220
we're in cohort three

3
00:00:08,220 --> 00:00:11,400
and meeting nine in our second

4
00:00:11,400 --> 00:00:14,460
discussion on chapter four of the

5
00:00:14,460 --> 00:00:16,619
textbook

6
00:00:16,619 --> 00:00:19,619
so last time we

7
00:00:19,619 --> 00:00:23,220
went through some sections

8
00:00:23,220 --> 00:00:26,460
and we have

9
00:00:26,460 --> 00:00:29,880
um also a lot of questions that have

10
00:00:29,880 --> 00:00:32,098
been addressed

11
00:00:32,098 --> 00:00:35,940
so before we go back to the text or to

12
00:00:35,940 --> 00:00:37,079
the questions

13
00:00:37,079 --> 00:00:39,239
does anyone who's here

14
00:00:39,239 --> 00:00:41,940
want to just like give any thought or

15
00:00:41,940 --> 00:00:44,660
reflection

16
00:01:09,840 --> 00:01:12,479
yeah what what um

17
00:01:12,479 --> 00:01:15,479
what are you still looking to reduce

18
00:01:15,479 --> 00:01:19,100
your uncertainty on

19
00:01:19,380 --> 00:01:21,360
or whatever

20
00:01:21,360 --> 00:01:23,159
yeah I'm just finding a lot of the

21
00:01:23,159 --> 00:01:24,840
mathematics

22
00:01:24,840 --> 00:01:28,860
um it's just not explained in a way that

23
00:01:28,860 --> 00:01:31,979
disambiguates what you know what a

24
00:01:31,979 --> 00:01:34,380
particular expression might be

25
00:01:34,380 --> 00:01:38,340
um I've got certain uncertainties about

26
00:01:38,340 --> 00:01:39,600
um some of the mathematical objects

27
00:01:39,600 --> 00:01:41,100
themselves

28
00:01:41,100 --> 00:01:42,960
um and then particularly on the section

29
00:01:42,960 --> 00:01:45,360
on Taylor series and I feel that I have

30
00:01:45,360 --> 00:01:47,880
a pretty good grasp on Taylor series

31
00:01:47,880 --> 00:01:50,280
um that's feeling particularly opaque to

32
00:01:50,280 --> 00:01:53,820
me which feels surprising

33
00:01:53,820 --> 00:01:54,420
um

34
00:01:54,420 --> 00:01:56,579
yeah I I'm finding it a slightly

35
00:01:56,579 --> 00:01:58,680
frustrating chapter which sort of dives

36
00:01:58,680 --> 00:02:01,740
into the maths but in some sense without

37
00:02:01,740 --> 00:02:03,479
enough detail for me to really deeply

38
00:02:03,479 --> 00:02:06,020
understand it

39
00:02:06,060 --> 00:02:08,639
but maybe that's just a matter of time

40
00:02:08,639 --> 00:02:11,400
thank you well

41
00:02:11,400 --> 00:02:15,920
definitely when building on a sort of

42
00:02:16,020 --> 00:02:18,599
meat and potatoes mathematical concept

43
00:02:18,599 --> 00:02:21,780
someone with your expertise should not

44
00:02:21,780 --> 00:02:24,420
be overly confused it should feel like

45
00:02:24,420 --> 00:02:26,400
you have already been on the on-ramp to

46
00:02:26,400 --> 00:02:29,099
understanding where this is going

47
00:02:29,099 --> 00:02:30,720
because it's hard to imagine it'd be

48
00:02:30,720 --> 00:02:31,739
clearer

49
00:02:31,739 --> 00:02:34,440
for someone who is less familiar with uh

50
00:02:34,440 --> 00:02:37,260
foundational pieces

51
00:02:37,260 --> 00:02:39,780
so let's definitely keep that in mind

52
00:02:39,780 --> 00:02:43,459
when we move through

53
00:02:43,500 --> 00:02:45,780
okay

54
00:02:45,780 --> 00:02:47,640
um

55
00:02:47,640 --> 00:02:51,560
anyone else want to just like give any

56
00:02:51,560 --> 00:02:55,260
overview thought on a section of four or

57
00:02:55,260 --> 00:02:57,599
anything else on how there's

58
00:02:57,599 --> 00:03:00,239
thinking about any aspect of it since

59
00:03:00,239 --> 00:03:02,780
last week

60
00:03:03,599 --> 00:03:08,060
then we will look at the questions

61
00:03:08,580 --> 00:03:11,340
and then continue to move through

62
00:03:11,340 --> 00:03:14,000
the text

63
00:03:22,800 --> 00:03:24,239
I'm just looking at the previous video

64
00:03:24,239 --> 00:03:27,680
to see where we ended

65
00:03:29,159 --> 00:03:32,060
410.

66
00:03:32,940 --> 00:03:35,879
okay any

67
00:03:35,879 --> 00:03:37,440
other comments otherwise we're going to

68
00:03:37,440 --> 00:03:39,959
go to 410

69
00:03:39,959 --> 00:03:41,819
and continue

70
00:03:41,819 --> 00:03:44,899
or a little bit before

71
00:03:49,620 --> 00:03:52,159
okay

72
00:03:53,220 --> 00:03:56,340
all right let's pull back to four four

73
00:03:56,340 --> 00:04:00,060
and just please raise your hand or

74
00:04:00,060 --> 00:04:02,519
um write in the chat with any thoughts

75
00:04:02,519 --> 00:04:04,260
or questions so just looking at the the

76
00:04:04,260 --> 00:04:05,580
questions

77
00:04:05,580 --> 00:04:08,340
that were submitted in this uh cohort

78
00:04:08,340 --> 00:04:12,900
like many of them involve the equations

79
00:04:12,900 --> 00:04:14,040
um

80
00:04:14,040 --> 00:04:15,560
for

81
00:04:15,560 --> 00:04:19,320
seven eight

82
00:04:19,320 --> 00:04:20,699
ten

83
00:04:20,699 --> 00:04:24,060
and so on so these are ones where like

84
00:04:24,060 --> 00:04:25,860
we can

85
00:04:25,860 --> 00:04:27,979
um

86
00:04:28,199 --> 00:04:30,180
look at at what was written for those

87
00:04:30,180 --> 00:04:31,680
where things were written or otherwise

88
00:04:31,680 --> 00:04:33,720
just like kind of

89
00:04:33,720 --> 00:04:35,100
um given

90
00:04:35,100 --> 00:04:39,360
the constraints on what we can do in 50

91
00:04:39,360 --> 00:04:40,979
minutes just recognize these are like

92
00:04:40,979 --> 00:04:42,780
important areas for there to be

93
00:04:42,780 --> 00:04:46,259
asynchronous development on

94
00:04:46,259 --> 00:04:49,259
but this looks like a great

95
00:04:49,259 --> 00:04:52,020
um answer whomever has

96
00:04:52,020 --> 00:04:54,960
written this and it's like a an example

97
00:04:54,960 --> 00:04:55,800
of

98
00:04:55,800 --> 00:04:56,820
um

99
00:04:56,820 --> 00:04:59,820
how to address it asynchronously yes

100
00:04:59,820 --> 00:05:01,560
Jonathan

101
00:05:01,560 --> 00:05:05,580
yes this was my my contribution here

102
00:05:05,580 --> 00:05:08,699
um the the explanation underneath

103
00:05:08,699 --> 00:05:11,040
um I I still have some some questions

104
00:05:11,040 --> 00:05:12,360
about it

105
00:05:12,360 --> 00:05:14,520
um despite the fact that I think I Now

106
00:05:14,520 --> 00:05:16,440
understand the the sort of the the

107
00:05:16,440 --> 00:05:18,600
vector aspect so I I've created a an

108
00:05:18,600 --> 00:05:20,580
overleaf document that sort of explains

109
00:05:20,580 --> 00:05:22,020
all of this

110
00:05:22,020 --> 00:05:25,620
um I'm I'm still slightly confused by

111
00:05:25,620 --> 00:05:28,020
something quite simple here in the

112
00:05:28,020 --> 00:05:30,240
expected free energy because there are

113
00:05:30,240 --> 00:05:31,919
these um Tau

114
00:05:31,919 --> 00:05:34,380
um indices floating around and Tau

115
00:05:34,380 --> 00:05:36,840
indicates I think the the time the

116
00:05:36,840 --> 00:05:39,660
moment in time so for instance s

117
00:05:39,660 --> 00:05:43,740
um of P subscript uh Pi tau is the

118
00:05:43,740 --> 00:05:46,380
probability of being in a given State

119
00:05:46,380 --> 00:05:49,380
um at a particular time given that

120
00:05:49,380 --> 00:05:52,080
you're undertaking some policy pie

121
00:05:52,080 --> 00:05:52,740
um

122
00:05:52,740 --> 00:05:54,360
and so there's a there's a Tau index

123
00:05:54,360 --> 00:05:55,680
floating around but it's still not quite

124
00:05:55,680 --> 00:05:58,199
clear to me in the calculation of the

125
00:05:58,199 --> 00:06:00,120
expected free energy

126
00:06:00,120 --> 00:06:00,720
um

127
00:06:00,720 --> 00:06:02,639
what you do with that Tower index are

128
00:06:02,639 --> 00:06:04,860
you are you summing over all of these

129
00:06:04,860 --> 00:06:06,300
terms

130
00:06:06,300 --> 00:06:07,860
um are you integrating over them it is

131
00:06:07,860 --> 00:06:10,199
an expectation value over all of the

132
00:06:10,199 --> 00:06:12,180
different time steps so that that was

133
00:06:12,180 --> 00:06:14,699
the the one thing that was sort of least

134
00:06:14,699 --> 00:06:16,680
clear to me here

135
00:06:16,680 --> 00:06:18,180
foreign

136
00:06:18,180 --> 00:06:22,520
yes great point

137
00:06:23,639 --> 00:06:26,220
is it expected for energy as an

138
00:06:26,220 --> 00:06:30,120
expectation of snapshot free energies is

139
00:06:30,120 --> 00:06:32,819
it a expected free energy as the sum of

140
00:06:32,819 --> 00:06:35,340
the expected futures

141
00:06:35,340 --> 00:06:37,259
or is it even

142
00:06:37,259 --> 00:06:39,780
the expectation at that last time step

143
00:06:39,780 --> 00:06:41,819
alone

144
00:06:41,819 --> 00:06:45,680
and the other ones are discarded

145
00:06:50,759 --> 00:06:53,900
good points

146
00:06:58,319 --> 00:07:01,199
in figure 4 3

147
00:07:01,199 --> 00:07:02,759
we have

148
00:07:02,759 --> 00:07:04,919
the discrete time

149
00:07:04,919 --> 00:07:06,840
partially observable Markov decision

150
00:07:06,840 --> 00:07:10,440
process pomdp and the continuous time

151
00:07:10,440 --> 00:07:12,120
cousin

152
00:07:12,120 --> 00:07:15,240
in the discrete time case

153
00:07:15,240 --> 00:07:17,220
past present and future time steps

154
00:07:17,220 --> 00:07:20,220
hidden states are explicitly predicted

155
00:07:20,220 --> 00:07:22,919
and action sequences are explicitly

156
00:07:22,919 --> 00:07:24,900
intervening in how hidden States change

157
00:07:24,900 --> 00:07:27,060
their time

158
00:07:27,060 --> 00:07:30,960
in contrast in the continuous time model

159
00:07:30,960 --> 00:07:34,039
rather than predicting or even

160
00:07:34,039 --> 00:07:36,720
explicitly mentioning past present and

161
00:07:36,720 --> 00:07:40,319
future we see like x x Prime X double

162
00:07:40,319 --> 00:07:44,580
Prime so the value of x and then the

163
00:07:44,580 --> 00:07:46,199
higher derivatives

164
00:07:46,199 --> 00:07:47,639
also known as the generalized

165
00:07:47,639 --> 00:07:50,520
coordinates of motion and so this

166
00:07:50,520 --> 00:07:53,699
amounts to making a Taylor series

167
00:07:53,699 --> 00:07:57,599
expansion which is continuous around the

168
00:07:57,599 --> 00:08:00,479
current time step

169
00:08:00,479 --> 00:08:03,479
two different ways of dealing with

170
00:08:03,479 --> 00:08:05,460
temporal depth

171
00:08:05,460 --> 00:08:08,039
and the way that action selection

172
00:08:08,039 --> 00:08:11,220
relates to temporal depth and they have

173
00:08:11,220 --> 00:08:14,060
some in principle and in practices

174
00:08:14,060 --> 00:08:16,199
strengths and weaknesses in different

175
00:08:16,199 --> 00:08:19,039
settings

176
00:08:21,300 --> 00:08:23,160
in 4.4

177
00:08:23,160 --> 00:08:25,379
they're going to focus on the discrete

178
00:08:25,379 --> 00:08:27,419
time model

179
00:08:27,419 --> 00:08:29,220
the discrete time model has been

180
00:08:29,220 --> 00:08:31,940
especially applied to

181
00:08:31,940 --> 00:08:35,880
categorical cognitive decision making

182
00:08:35,880 --> 00:08:38,399
whereas the archetypal case for The

183
00:08:38,399 --> 00:08:41,179
Continuous time generative model is

184
00:08:41,179 --> 00:08:44,339
continuous perception and action of like

185
00:08:44,339 --> 00:08:47,279
a motor unit

186
00:08:47,279 --> 00:08:49,800
and they can play well together as we'll

187
00:08:49,800 --> 00:08:53,899
see in chapter 8 with Hybrid models

188
00:08:59,040 --> 00:09:01,019
we could go

189
00:09:01,019 --> 00:09:04,320
slower fast on equations but we'll just

190
00:09:04,320 --> 00:09:06,060
try to like move through the whole thing

191
00:09:06,060 --> 00:09:09,300
but many many of these

192
00:09:09,300 --> 00:09:11,519
the work to be done

193
00:09:11,519 --> 00:09:16,010
is making sure that we have

194
00:09:16,010 --> 00:09:16,019
[Music]

195
00:09:16,019 --> 00:09:17,279
um

196
00:09:17,279 --> 00:09:21,420
high quality descriptions verbally which

197
00:09:21,420 --> 00:09:23,880
we do for for many

198
00:09:23,880 --> 00:09:27,060
due to many people's great work but

199
00:09:27,060 --> 00:09:28,680
making sure that we have

200
00:09:28,680 --> 00:09:31,620
verbal descriptions in the active

201
00:09:31,620 --> 00:09:33,779
inference ontology

202
00:09:33,779 --> 00:09:36,779
that are faithful

203
00:09:36,779 --> 00:09:39,180
to the symbols

204
00:09:39,180 --> 00:09:42,540
and then where people have questions

205
00:09:42,540 --> 00:09:44,880
about what does this mean why is that

206
00:09:44,880 --> 00:09:46,560
that way what if it were this way what

207
00:09:46,560 --> 00:09:49,860
are the implications here those are all

208
00:09:49,860 --> 00:09:51,959
great questions

209
00:09:51,959 --> 00:09:53,700
and they can all be included in the

210
00:09:53,700 --> 00:09:56,339
notes section for a given equation or in

211
00:09:56,339 --> 00:09:58,260
the more General notes table

212
00:09:58,260 --> 00:10:00,380
but for for many of the equations

213
00:10:00,380 --> 00:10:02,760
including some of the super challenging

214
00:10:02,760 --> 00:10:04,110
ones

215
00:10:04,110 --> 00:10:04,440
[Music]

216
00:10:04,440 --> 00:10:05,339
um

217
00:10:05,339 --> 00:10:08,940
we have things written out which is very

218
00:10:08,940 --> 00:10:11,180
good

219
00:10:12,300 --> 00:10:14,160
um but just kind of

220
00:10:14,160 --> 00:10:15,660
at the

221
00:10:15,660 --> 00:10:16,440
um

222
00:10:16,440 --> 00:10:17,760
skim

223
00:10:17,760 --> 00:10:19,380
level

224
00:10:19,380 --> 00:10:20,880
cat

225
00:10:20,880 --> 00:10:23,640
is a categorical distribution so in this

226
00:10:23,640 --> 00:10:26,820
section we're in a discrete setting so

227
00:10:26,820 --> 00:10:28,140
like

228
00:10:28,140 --> 00:10:31,200
the light is on or off the continuous

229
00:10:31,200 --> 00:10:32,700
setting would be it's a continuous

230
00:10:32,700 --> 00:10:35,399
variable between zero and one but in the

231
00:10:35,399 --> 00:10:38,220
categorical setting we have categorical

232
00:10:38,220 --> 00:10:39,660
distributions

233
00:10:39,660 --> 00:10:43,140
and so we have certain

234
00:10:43,140 --> 00:10:45,660
um ways of summing across them for

235
00:10:45,660 --> 00:10:47,279
example instead of taking an integral

236
00:10:47,279 --> 00:10:50,880
over a continuous distribution

237
00:10:50,880 --> 00:10:53,000
um

238
00:11:00,360 --> 00:11:02,279
action selection

239
00:11:02,279 --> 00:11:03,600
ply

240
00:11:03,600 --> 00:11:06,240
policy selection

241
00:11:06,240 --> 00:11:11,700
is going to be guided by which policies

242
00:11:11,700 --> 00:11:13,920
are most likely

243
00:11:13,920 --> 00:11:17,180
most self-consistent most self-evident

244
00:11:17,180 --> 00:11:19,500
most probable

245
00:11:19,500 --> 00:11:22,800
lowest expected free energy

246
00:11:22,800 --> 00:11:25,260
we're not proposing a reward or utility

247
00:11:25,260 --> 00:11:27,839
function and then evaluating policies

248
00:11:27,839 --> 00:11:31,620
based upon their expected reward

249
00:11:31,620 --> 00:11:34,079
we're evaluating policies based upon

250
00:11:34,079 --> 00:11:36,839
their self-consistency

251
00:11:36,839 --> 00:11:39,420
and in fact the most probable ones are

252
00:11:39,420 --> 00:11:40,920
the ones that lead to the lowest

253
00:11:40,920 --> 00:11:43,880
expected free energy

254
00:11:45,180 --> 00:11:46,320
so

255
00:11:46,320 --> 00:11:51,240
more equations G is expected for energy

256
00:11:51,240 --> 00:11:54,300
and even just looking at which variables

257
00:11:54,300 --> 00:11:56,399
go into this

258
00:11:56,399 --> 00:11:58,740
we see

259
00:11:58,740 --> 00:12:02,880
a probability distribution over policies

260
00:12:02,880 --> 00:12:05,279
that is defined

261
00:12:05,279 --> 00:12:07,380
through and expected free energy

262
00:12:07,380 --> 00:12:09,240
calculation

263
00:12:09,240 --> 00:12:11,220
and this

264
00:12:11,220 --> 00:12:12,959
G of Pi

265
00:12:12,959 --> 00:12:16,019
it's a it's a function of

266
00:12:16,019 --> 00:12:18,120
policy

267
00:12:18,120 --> 00:12:19,920
and it is going to have this expected

268
00:12:19,920 --> 00:12:23,399
free energy Construction

269
00:12:23,519 --> 00:12:25,440
and we've seen expected free energy

270
00:12:25,440 --> 00:12:27,060
before

271
00:12:27,060 --> 00:12:28,800
in

272
00:12:28,800 --> 00:12:32,120
equation 2.6

273
00:12:33,120 --> 00:12:34,500
where

274
00:12:34,500 --> 00:12:37,440
the part that's an expectation

275
00:12:37,440 --> 00:12:39,300
that only involves preferences and

276
00:12:39,300 --> 00:12:43,040
observations is pragmatic value

277
00:12:46,440 --> 00:12:48,060
so

278
00:12:48,060 --> 00:12:49,620
natural log of a distribution

279
00:12:49,620 --> 00:12:52,740
expectation of a log

280
00:12:52,740 --> 00:12:55,980
of something only involving observations

281
00:12:55,980 --> 00:12:58,760
and preferences

282
00:13:00,180 --> 00:13:03,060
here we have y tilde

283
00:13:03,060 --> 00:13:06,600
here we have o tilde Y is sometimes used

284
00:13:06,600 --> 00:13:08,220
for observations more in like a linear

285
00:13:08,220 --> 00:13:09,899
regression setting o is like

286
00:13:09,899 --> 00:13:12,120
observations but they're identical

287
00:13:12,120 --> 00:13:13,380
and then

288
00:13:13,380 --> 00:13:16,980
the other component is a and so it's

289
00:13:16,980 --> 00:13:18,839
pragmatic value because that is the

290
00:13:18,839 --> 00:13:21,300
alignment between the observations and

291
00:13:21,300 --> 00:13:24,360
the preferences or expectations

292
00:13:24,360 --> 00:13:26,579
the other component in this

293
00:13:26,579 --> 00:13:29,519
decomposition of expected free energy is

294
00:13:29,519 --> 00:13:33,300
a KL Divergence expected KL Divergence

295
00:13:33,300 --> 00:13:35,040
between

296
00:13:35,040 --> 00:13:38,399
two variational distributions two

297
00:13:38,399 --> 00:13:41,279
distributions that we control Q

298
00:13:41,279 --> 00:13:43,440
they're both x hidden States Through

299
00:13:43,440 --> 00:13:45,000
Time X tilde

300
00:13:45,000 --> 00:13:46,800
conditioned upon

301
00:13:46,800 --> 00:13:49,019
and then they're very similar

302
00:13:49,019 --> 00:13:52,079
here we only have Pi here we have pi and

303
00:13:52,079 --> 00:13:53,100
Y

304
00:13:53,100 --> 00:13:56,839
so this is the Divergence

305
00:13:57,000 --> 00:14:00,060
with respect to whether adding

306
00:14:00,060 --> 00:14:02,279
observations

307
00:14:02,279 --> 00:14:04,860
updates are prior

308
00:14:04,860 --> 00:14:09,000
if these two cues are the same

309
00:14:09,000 --> 00:14:11,519
that's equivalent to saying there's no

310
00:14:11,519 --> 00:14:14,040
space between the distributions

311
00:14:14,040 --> 00:14:16,920
where we've parameterized x with or

312
00:14:16,920 --> 00:14:19,440
without getting any observations

313
00:14:19,440 --> 00:14:21,660
in other words the observations were

314
00:14:21,660 --> 00:14:23,880
valueless

315
00:14:23,880 --> 00:14:27,240
if the Divergence here is zero

316
00:14:27,240 --> 00:14:30,240
whereas if the observations are really

317
00:14:30,240 --> 00:14:31,620
impactful

318
00:14:31,620 --> 00:14:34,560
on moving the distribution

319
00:14:34,560 --> 00:14:38,700
then this KL Divergence is going to be

320
00:14:38,700 --> 00:14:39,899
big

321
00:14:39,899 --> 00:14:42,779
divergences are always non-zero

322
00:14:42,779 --> 00:14:45,899
and then there's a negative here so the

323
00:14:45,899 --> 00:14:47,940
more information gain the bigger the KL

324
00:14:47,940 --> 00:14:49,560
Divergence the more that the

325
00:14:49,560 --> 00:14:51,660
observations matter

326
00:14:51,660 --> 00:14:54,060
the more this first term is going to go

327
00:14:54,060 --> 00:14:55,560
negative

328
00:14:55,560 --> 00:14:59,100
and so that specific policy

329
00:14:59,100 --> 00:15:03,139
will have a higher epistemic value

330
00:15:03,839 --> 00:15:06,440
Ali

331
00:15:07,639 --> 00:15:11,220
uh yeah I just remembered a

332
00:15:11,220 --> 00:15:13,860
clarification made by Ryan Smith in one

333
00:15:13,860 --> 00:15:17,279
of his lectures about uh two concepts of

334
00:15:17,279 --> 00:15:21,060
or rather two Notions of time used in

335
00:15:21,060 --> 00:15:23,579
active inference literature uh one of

336
00:15:23,579 --> 00:15:26,220
which is denoted by Tau and the other

337
00:15:26,220 --> 00:15:30,779
one is denoted by T So a Tau is used for

338
00:15:30,779 --> 00:15:34,040
the time about which we have a belief

339
00:15:34,040 --> 00:15:38,940
and the examples he gives is for

340
00:15:38,940 --> 00:15:42,360
instance I believe I am now in my car so

341
00:15:42,360 --> 00:15:47,279
that would be S of Tau but I believe I

342
00:15:47,279 --> 00:15:49,620
was in my kitchen 10 minutes ago would

343
00:15:49,620 --> 00:15:52,980
be S Tau minus one where I believe I

344
00:15:52,980 --> 00:15:55,560
will be at work in 20 minutes or

345
00:15:55,560 --> 00:15:58,320
something would be S Tau plus one

346
00:15:58,320 --> 00:16:01,440
but on the other hand we have t which

347
00:16:01,440 --> 00:16:03,420
refers to the time at which a new

348
00:16:03,420 --> 00:16:05,220
observation is given

349
00:16:05,220 --> 00:16:09,060
so for instance as an example and after

350
00:16:09,060 --> 00:16:10,940
turning on a light

351
00:16:10,940 --> 00:16:12,959
I now

352
00:16:12,959 --> 00:16:15,899
in other words essential believe that I

353
00:16:15,899 --> 00:16:17,940
was in the kitchen for the last five

354
00:16:17,940 --> 00:16:19,860
minutes right

355
00:16:19,860 --> 00:16:25,440
so we can update our belief about all uh

356
00:16:25,440 --> 00:16:29,639
all Taos or S of towns with each new

357
00:16:29,639 --> 00:16:33,779
observation at each tease or at each

358
00:16:33,779 --> 00:16:37,980
given observation times so uh briefly

359
00:16:37,980 --> 00:16:41,399
one is the belief time and the other one

360
00:16:41,399 --> 00:16:44,940
is the observation time so tau is used

361
00:16:44,940 --> 00:16:49,320
for leaf times or the time about which

362
00:16:49,320 --> 00:16:52,560
we have leaf and T is used for

363
00:16:52,560 --> 00:16:55,699
observation times

364
00:16:56,519 --> 00:16:59,000
thanks that's very helpful

365
00:16:59,000 --> 00:17:02,820
is it accurate to say that T is like the

366
00:17:02,820 --> 00:17:05,160
Click of our simulation

367
00:17:05,160 --> 00:17:06,959
that's actually handing the new

368
00:17:06,959 --> 00:17:09,780
observations in at a given moment T

369
00:17:09,780 --> 00:17:12,599
after T but then especially in

370
00:17:12,599 --> 00:17:14,939
sophisticated active inference where we

371
00:17:14,939 --> 00:17:16,380
might be considering like the past

372
00:17:16,380 --> 00:17:19,559
present and future at every t

373
00:17:19,559 --> 00:17:24,240
we are reevaluating Towers at each tee

374
00:17:24,240 --> 00:17:28,079
yeah I think we can reframe that in the

375
00:17:28,079 --> 00:17:29,520
in this way as well

376
00:17:29,520 --> 00:17:31,140
awesome

377
00:17:31,140 --> 00:17:32,340
so

378
00:17:32,340 --> 00:17:37,320
here we reach yes potentially with some

379
00:17:37,320 --> 00:17:39,720
um Bumps and Jumps

380
00:17:39,720 --> 00:17:44,280
the expected free energy for a policy in

381
00:17:44,280 --> 00:17:46,140
terms of the pragmatic value

382
00:17:46,140 --> 00:17:48,600
alignment of preferences with

383
00:17:48,600 --> 00:17:51,840
observations and the expected

384
00:17:51,840 --> 00:17:55,559
Information Gain the epistemic value in

385
00:17:55,559 --> 00:18:00,559
terms of how much adding in observations

386
00:18:00,559 --> 00:18:03,120
updates are variational distribution

387
00:18:03,120 --> 00:18:05,940
about hidden States conditioned upon

388
00:18:05,940 --> 00:18:08,720
that policy

389
00:18:15,960 --> 00:18:19,740
here is more Focus

390
00:18:19,740 --> 00:18:24,780
on how we get this KL Divergence

391
00:18:24,780 --> 00:18:29,220
so here we see that KL Divergence

392
00:18:29,220 --> 00:18:31,039
and there's just a few different

393
00:18:31,039 --> 00:18:33,539
expansions and relationships around it

394
00:18:33,539 --> 00:18:35,940
Jonathan

395
00:18:35,940 --> 00:18:38,039
yeah so just zoom zooming into that

396
00:18:38,039 --> 00:18:39,600
equation there

397
00:18:39,600 --> 00:18:41,880
um there's an expectation

398
00:18:41,880 --> 00:18:45,960
um over Q of O given pi

399
00:18:45,960 --> 00:18:48,900
um and I I couldn't figure out how

400
00:18:48,900 --> 00:18:51,179
that's calculated because I I understand

401
00:18:51,179 --> 00:18:53,280
how one can get

402
00:18:53,280 --> 00:18:58,260
um Q of s given o and Pi but it seems

403
00:18:58,260 --> 00:19:00,960
that to calculate Q of O given Pi one

404
00:19:00,960 --> 00:19:04,260
needs to marginalize or sum over the

405
00:19:04,260 --> 00:19:06,240
states and I thought that that was the

406
00:19:06,240 --> 00:19:07,740
whole reason for introducing Q in the

407
00:19:07,740 --> 00:19:11,220
first place was because we we can't

408
00:19:11,220 --> 00:19:13,200
um we can't same overall all external

409
00:19:13,200 --> 00:19:15,440
States

410
00:19:26,820 --> 00:19:31,580
the subscript notation may be

411
00:19:32,460 --> 00:19:35,400
used in an unconventional way but I I

412
00:19:35,400 --> 00:19:36,900
agree with you

413
00:19:36,900 --> 00:19:39,620
on that

414
00:19:40,200 --> 00:19:44,160
here in the inner queue

415
00:19:44,160 --> 00:19:46,860
we have it about s

416
00:19:46,860 --> 00:19:48,660
conditioned upon observations and

417
00:19:48,660 --> 00:19:51,780
policies this doesn't seem contentious

418
00:19:51,780 --> 00:19:54,240
yep that's fine yeah

419
00:19:54,240 --> 00:19:57,600
but the subscript in 4.8 and I think

420
00:19:57,600 --> 00:19:59,039
that was probably one of your questions

421
00:19:59,039 --> 00:20:02,760
yeah yeah

422
00:20:11,520 --> 00:20:13,740
that's the one

423
00:20:13,740 --> 00:20:16,620
sorry with the terrible latent yeah yeah

424
00:20:16,620 --> 00:20:18,059
yeah

425
00:20:18,059 --> 00:20:21,360
um yeah we should look at the the Matlab

426
00:20:21,360 --> 00:20:23,100
code for

427
00:20:23,100 --> 00:20:25,200
wherever that comes into play to see how

428
00:20:25,200 --> 00:20:28,280
it actually is done

429
00:20:28,740 --> 00:20:30,900
or is

430
00:20:30,900 --> 00:20:35,240
a separable variational quality

431
00:20:39,679 --> 00:20:43,080
yeah like this is the just the inner

432
00:20:43,080 --> 00:20:43,919
value

433
00:20:43,919 --> 00:20:46,080
is the KL Divergence of how much

434
00:20:46,080 --> 00:20:49,820
observations update

435
00:20:49,980 --> 00:20:52,980
um uh add in to your changing beliefs

436
00:20:52,980 --> 00:20:54,600
about s

437
00:20:54,600 --> 00:20:57,780
but we're taking an expectation of that

438
00:20:57,780 --> 00:20:59,660
over

439
00:20:59,660 --> 00:21:02,940
expected observation sequences

440
00:21:02,940 --> 00:21:07,620
which then begs the question of

441
00:21:07,620 --> 00:21:11,039
needing to know the distribution of s

442
00:21:11,039 --> 00:21:13,320
that would generate those probabilistic

443
00:21:13,320 --> 00:21:15,299
envelopes of o

444
00:21:15,299 --> 00:21:18,960
exactly yeah

445
00:21:19,559 --> 00:21:21,419
well

446
00:21:21,419 --> 00:21:23,580
one answer is

447
00:21:23,580 --> 00:21:26,520
these are the analytical

448
00:21:26,520 --> 00:21:27,539
um

449
00:21:27,539 --> 00:21:30,360
ideals

450
00:21:30,360 --> 00:21:32,640
for which there are strong analytical

451
00:21:32,640 --> 00:21:34,020
guarantees

452
00:21:34,020 --> 00:21:36,240
and then any given computational

453
00:21:36,240 --> 00:21:39,059
implementation is going to use different

454
00:21:39,059 --> 00:21:41,100
methods

455
00:21:41,100 --> 00:21:45,799
to to implement this just procedurally

456
00:21:49,080 --> 00:21:53,460
but that doesn't make the notation clear

457
00:21:53,460 --> 00:21:55,620
and I I think these are some of the

458
00:21:55,620 --> 00:21:57,360
kinds of artifacts hopefully cohort

459
00:21:57,360 --> 00:21:59,159
after cohort

460
00:21:59,159 --> 00:22:03,780
we can craft like a lookup table

461
00:22:03,780 --> 00:22:06,059
and we've done this to a limited extent

462
00:22:06,059 --> 00:22:08,039
just starting it but having a lookup

463
00:22:08,039 --> 00:22:10,260
tables

464
00:22:10,260 --> 00:22:12,730
for all the different

465
00:22:12,730 --> 00:22:13,020
[Music]

466
00:22:13,020 --> 00:22:13,919
um

467
00:22:13,919 --> 00:22:17,179
shapes and squiggles

468
00:22:19,440 --> 00:22:21,539
But continuing on site I hope that we

469
00:22:21,539 --> 00:22:24,299
can at least see the rest of it

470
00:22:24,299 --> 00:22:25,320
um

471
00:22:25,320 --> 00:22:28,380
variational free energy F and expected

472
00:22:28,380 --> 00:22:30,659
free energy G

473
00:22:30,659 --> 00:22:33,600
variational free energy which we'll

474
00:22:33,600 --> 00:22:37,020
recall from equation two to five

475
00:22:37,020 --> 00:22:39,360
is a functional of Q the distribution we

476
00:22:39,360 --> 00:22:42,720
control and Y the incoming data

477
00:22:42,720 --> 00:22:46,200
so variational free energy is like the

478
00:22:46,200 --> 00:22:51,720
real time unfolding model fit

479
00:22:51,720 --> 00:22:54,059
and this is a quantity that's widely

480
00:22:54,059 --> 00:22:57,379
used in statistics

481
00:22:57,480 --> 00:22:59,880
expected free energy

482
00:22:59,880 --> 00:23:02,940
uses similar

483
00:23:02,940 --> 00:23:04,500
structure

484
00:23:04,500 --> 00:23:07,140
and positioning

485
00:23:07,140 --> 00:23:10,020
but it explicitly brings in two pieces

486
00:23:10,020 --> 00:23:13,260
that aren't present in variational free

487
00:23:13,260 --> 00:23:16,500
energy and those are observations which

488
00:23:16,500 --> 00:23:18,539
haven't happened yet because we need to

489
00:23:18,539 --> 00:23:20,400
talk about how informative we expect

490
00:23:20,400 --> 00:23:22,440
observations that haven't occurred yet

491
00:23:22,440 --> 00:23:23,400
to be

492
00:23:23,400 --> 00:23:25,520
Information Gain epistemic value

493
00:23:25,520 --> 00:23:27,840
obviously variational free energy is

494
00:23:27,840 --> 00:23:29,940
only concerned with the actual data

495
00:23:29,940 --> 00:23:32,760
point being acquired at the moment

496
00:23:32,760 --> 00:23:34,380
and then secondly

497
00:23:34,380 --> 00:23:36,559
the fact that we can take different

498
00:23:36,559 --> 00:23:38,700
policy choices

499
00:23:38,700 --> 00:23:42,720
and policy choices end up conditioning

500
00:23:42,720 --> 00:23:44,340
for example

501
00:23:44,340 --> 00:23:47,340
what pragmatic and epistemic value we

502
00:23:47,340 --> 00:23:49,679
achieve or expect to achieve

503
00:23:49,679 --> 00:23:52,320
so free energy

504
00:23:52,320 --> 00:23:55,799
can have a variational free energy

505
00:23:55,799 --> 00:23:59,280
formulation real time unfolding model

506
00:23:59,280 --> 00:24:02,000
adequacy

507
00:24:02,039 --> 00:24:03,480
or

508
00:24:03,480 --> 00:24:06,299
projecting free energy into the expected

509
00:24:06,299 --> 00:24:08,400
future

510
00:24:08,400 --> 00:24:10,860
we need to invoke observations that

511
00:24:10,860 --> 00:24:12,419
haven't happened yet

512
00:24:12,419 --> 00:24:13,740
and

513
00:24:13,740 --> 00:24:16,320
our agency in selecting alternative

514
00:24:16,320 --> 00:24:17,760
policies

515
00:24:17,760 --> 00:24:20,880
which then of course bear on the

516
00:24:20,880 --> 00:24:22,919
epistemic and pragmatic value we

517
00:24:22,919 --> 00:24:25,340
accomplish

518
00:24:28,880 --> 00:24:31,200
here in 4 9

519
00:24:31,200 --> 00:24:33,360
there are more decompositions of

520
00:24:33,360 --> 00:24:35,760
expected free energy

521
00:24:35,760 --> 00:24:39,360
that help see

522
00:24:39,360 --> 00:24:43,440
previously recognized dialectics or

523
00:24:43,440 --> 00:24:47,179
decompositions or trade-offs as actually

524
00:24:47,179 --> 00:24:50,520
decompositions of the unified imperative

525
00:24:50,520 --> 00:24:53,820
expected free energy so epistemic and

526
00:24:53,820 --> 00:24:56,580
pragmatic value decomposition and an

527
00:24:56,580 --> 00:25:00,439
ambiguity plus risk

528
00:25:00,960 --> 00:25:03,900
no one can kind of walk through and see

529
00:25:03,900 --> 00:25:05,460
what those mean or look at the natural

530
00:25:05,460 --> 00:25:07,760
language

531
00:25:10,919 --> 00:25:14,039
we can rewrite 4 7 in linear algebraic

532
00:25:14,039 --> 00:25:16,220
form

533
00:25:18,539 --> 00:25:21,440
here's four seven

534
00:25:21,659 --> 00:25:22,860
here

535
00:25:22,860 --> 00:25:24,299
is

536
00:25:24,299 --> 00:25:27,799
for the categorical distribution

537
00:25:28,320 --> 00:25:32,820
we can sum over policies

538
00:25:32,820 --> 00:25:35,039
about the policies and the hidden States

539
00:25:35,039 --> 00:25:37,500
Through Time

540
00:25:37,500 --> 00:25:38,460
um

541
00:25:38,460 --> 00:25:42,299
more details and calculations of

542
00:25:42,299 --> 00:25:45,379
variational free energy

543
00:25:47,539 --> 00:25:49,799
factorization and the mean field

544
00:25:49,799 --> 00:25:53,520
approximate mean field approximation

545
00:25:53,520 --> 00:25:55,820
these are some mathematical

546
00:25:55,820 --> 00:25:58,559
technicalities but but

547
00:25:58,559 --> 00:25:59,820
um

548
00:25:59,820 --> 00:26:02,940
they are simplifying assumptions or ways

549
00:26:02,940 --> 00:26:07,340
to approach this in the general case

550
00:26:08,820 --> 00:26:10,559
this factorization is one of many

551
00:26:10,559 --> 00:26:13,020
possibilities in variational inference

552
00:26:13,020 --> 00:26:15,740
and as we talked about last time

553
00:26:15,740 --> 00:26:19,020
variational inference is just one way to

554
00:26:19,020 --> 00:26:21,179
do these calculations

555
00:26:21,179 --> 00:26:23,580
and represents the simplest option and

556
00:26:23,580 --> 00:26:25,320
in practice it's nuanced slightly and

557
00:26:25,320 --> 00:26:26,820
there's like other approximations and

558
00:26:26,820 --> 00:26:30,020
it's a whole thing

559
00:26:35,240 --> 00:26:37,740
more information

560
00:26:37,740 --> 00:26:40,020
on the letters that we know and love

561
00:26:40,020 --> 00:26:43,020
a the tale of two densities

562
00:26:43,020 --> 00:26:46,440
B how hidden States change Through Time

563
00:26:46,440 --> 00:26:49,620
C prior beliefs about observations

564
00:26:49,620 --> 00:26:52,260
which are preferences

565
00:26:52,260 --> 00:26:53,460
and

566
00:26:53,460 --> 00:26:56,100
the simple prior on the initial state to

567
00:26:56,100 --> 00:27:00,020
kick the hidden State chain off

568
00:27:04,799 --> 00:27:06,960
a little bit of

569
00:27:06,960 --> 00:27:09,539
policy and belief updating few more

570
00:27:09,539 --> 00:27:12,440
technical details

571
00:27:13,440 --> 00:27:17,840
into active inference in continuous time

572
00:27:23,279 --> 00:27:28,039
Box 41 on Markov blankets

573
00:27:28,679 --> 00:27:32,600
a markout blanket for a given variable

574
00:27:33,179 --> 00:27:36,419
with just those words the map territory

575
00:27:36,419 --> 00:27:39,539
question is addressed Markov blankets

576
00:27:39,539 --> 00:27:42,679
are about variables

577
00:27:42,840 --> 00:27:44,880
comprises a subset of those that

578
00:27:44,880 --> 00:27:46,559
interact with it if we know everything

579
00:27:46,559 --> 00:27:48,539
about the subset knowledge of anything

580
00:27:48,539 --> 00:27:50,640
outside the subset does not increase our

581
00:27:50,640 --> 00:27:54,200
knowledge of the variable of interest

582
00:27:56,460 --> 00:27:58,799
there are several

583
00:27:58,799 --> 00:28:01,460
Bayesian message passing schemes

584
00:28:01,460 --> 00:28:04,500
expounded on a lot further in other work

585
00:28:04,500 --> 00:28:09,980
with fristin parts of rise at all

586
00:28:10,919 --> 00:28:12,480
probably not

587
00:28:12,480 --> 00:28:15,240
the most relevant to delve into right

588
00:28:15,240 --> 00:28:16,200
now

589
00:28:16,200 --> 00:28:18,539
but the important takeaway

590
00:28:18,539 --> 00:28:22,559
is that by using a base graph

591
00:28:22,559 --> 00:28:26,120
so by making a commitment to the figure

592
00:28:26,120 --> 00:28:29,220
4.3 Lego set

593
00:28:29,220 --> 00:28:31,740
if we choose to have nodes be random

594
00:28:31,740 --> 00:28:33,960
variables and edges reflect

595
00:28:33,960 --> 00:28:36,539
relationships amongst variables in a

596
00:28:36,539 --> 00:28:37,980
certain way

597
00:28:37,980 --> 00:28:40,080
then

598
00:28:40,080 --> 00:28:43,919
we gain access to message passing

599
00:28:43,919 --> 00:28:45,900
techniques

600
00:28:45,900 --> 00:28:50,039
that make even large Bayesian graphs

601
00:28:50,039 --> 00:28:51,360
with different parts of the graph

602
00:28:51,360 --> 00:28:53,940
updating at different speeds and so on

603
00:28:53,940 --> 00:28:56,400
there are software packages

604
00:28:56,400 --> 00:28:59,419
that are able to in a defined

605
00:28:59,419 --> 00:29:02,820
computational complexity setting

606
00:29:02,820 --> 00:29:06,179
run inference on those base graphs

607
00:29:06,179 --> 00:29:08,460
so yes bigger models all things being

608
00:29:08,460 --> 00:29:10,679
equal take more time and more

609
00:29:10,679 --> 00:29:13,200
computational resources to run

610
00:29:13,200 --> 00:29:16,679
however there are some tremendously

611
00:29:16,679 --> 00:29:18,240
scalable

612
00:29:18,240 --> 00:29:20,820
high performance ways

613
00:29:20,820 --> 00:29:23,760
to computationally implement

614
00:29:23,760 --> 00:29:27,779
computation on these graphs

615
00:29:27,779 --> 00:29:29,100
and they have

616
00:29:29,100 --> 00:29:32,220
interesting analytical representations

617
00:29:32,220 --> 00:29:34,740
and

618
00:29:34,740 --> 00:29:36,779
they have

619
00:29:36,779 --> 00:29:38,279
a growing

620
00:29:38,279 --> 00:29:41,039
stat of tool kits

621
00:29:41,039 --> 00:29:44,580
like reactive messagepassing dot JL by

622
00:29:44,580 --> 00:29:48,120
the bias lab with Bert DeVries and

623
00:29:48,120 --> 00:29:50,220
um his lab

624
00:29:50,220 --> 00:29:52,980
this is like really cool

625
00:29:52,980 --> 00:29:55,860
current work that's not even

626
00:29:55,860 --> 00:29:58,559
outside of their lab and colleagues not

627
00:29:58,559 --> 00:30:00,299
heavily used

628
00:30:00,299 --> 00:30:05,100
in even active inference Publications

629
00:30:05,100 --> 00:30:06,480
but

630
00:30:06,480 --> 00:30:11,899
they have incredible recent work

631
00:30:12,120 --> 00:30:13,860
and you can see how they're using

632
00:30:13,860 --> 00:30:17,940
variational free energy in these

633
00:30:17,940 --> 00:30:19,500
um

634
00:30:19,500 --> 00:30:22,820
advanced settings

635
00:30:26,279 --> 00:30:28,820
pulling back to consider message passing

636
00:30:28,820 --> 00:30:32,100
qualitatively in the chapter whereas it

637
00:30:32,100 --> 00:30:34,799
was introduced in this Markov blanket

638
00:30:34,799 --> 00:30:37,460
box

639
00:30:38,039 --> 00:30:40,158
um

640
00:30:40,620 --> 00:30:43,620
message passing they'll be totally

641
00:30:43,620 --> 00:30:48,260
looking oh thank you I will yeah uh

642
00:30:48,960 --> 00:30:51,539
there's the bioslab site and I'll add it

643
00:30:51,539 --> 00:30:54,500
to the figure four

644
00:31:02,100 --> 00:31:04,500
the reactive message passing is like one

645
00:31:04,500 --> 00:31:06,480
of their new pieces

646
00:31:06,480 --> 00:31:07,740
where

647
00:31:07,740 --> 00:31:09,539
um let's say you had two sensors and one

648
00:31:09,539 --> 00:31:10,860
of them was being updated every second

649
00:31:10,860 --> 00:31:13,140
and one was being updated every hour

650
00:31:13,140 --> 00:31:16,620
well if you only could turn the clock

651
00:31:16,620 --> 00:31:18,600
if you you might get into a situation

652
00:31:18,600 --> 00:31:20,100
where you would either say well should I

653
00:31:20,100 --> 00:31:22,140
update my model every second

654
00:31:22,140 --> 00:31:24,899
and then over sample the slow variable

655
00:31:24,899 --> 00:31:27,299
or should I sample my model every hour

656
00:31:27,299 --> 00:31:30,240
and under sample the fast variable

657
00:31:30,240 --> 00:31:33,419
and reactive message passing navigates

658
00:31:33,419 --> 00:31:36,360
that by actually having nodes update on

659
00:31:36,360 --> 00:31:37,740
demand

660
00:31:37,740 --> 00:31:39,899
so you can have one node that's just

661
00:31:39,899 --> 00:31:42,240
getting pinged every second and another

662
00:31:42,240 --> 00:31:44,039
node that's getting updated on a

663
00:31:44,039 --> 00:31:45,360
different time scale

664
00:31:45,360 --> 00:31:48,419
and the computations on demand

665
00:31:48,419 --> 00:31:51,960
so that allows you to have like lifelong

666
00:31:51,960 --> 00:31:56,100
online learning for base graphs flexibly

667
00:31:56,100 --> 00:31:59,700
with different rates of sensors

668
00:31:59,700 --> 00:32:01,380
and and so those are the kinds of

669
00:32:01,380 --> 00:32:03,480
toolkits like like

670
00:32:03,480 --> 00:32:04,559
um

671
00:32:04,559 --> 00:32:07,039
these models are not just being proposed

672
00:32:07,039 --> 00:32:10,320
to promote theoretical unity in the

673
00:32:10,320 --> 00:32:12,240
cybernetic Sciences

674
00:32:12,240 --> 00:32:15,000
also there are powerful software

675
00:32:15,000 --> 00:32:17,279
toolkits

676
00:32:17,279 --> 00:32:20,700
that using these kinds of architectures

677
00:32:20,700 --> 00:32:22,620
enable

678
00:32:22,620 --> 00:32:26,640
so epistemic value pragmatic value

679
00:32:26,640 --> 00:32:29,880
um message passing schemes

680
00:32:29,880 --> 00:32:31,200
um

681
00:32:31,200 --> 00:32:33,299
if anyone knows any takeaways from this

682
00:32:33,299 --> 00:32:35,100
image please feel free because certain

683
00:32:35,100 --> 00:32:37,080
things are like omitted

684
00:32:37,080 --> 00:32:39,419
and also it uses like a lot of squiggles

685
00:32:39,419 --> 00:32:42,480
and ease and different variables

686
00:32:42,480 --> 00:32:44,940
um but

687
00:32:44,940 --> 00:32:47,700
here we have a representation

688
00:32:47,700 --> 00:32:49,140
of

689
00:32:49,140 --> 00:32:50,940
a hierarchical

690
00:32:50,940 --> 00:32:52,620
model so you can tell it's a

691
00:32:52,620 --> 00:32:53,940
hierarchical model because you have like

692
00:32:53,940 --> 00:32:55,740
I and you have this kind of cluster of

693
00:32:55,740 --> 00:32:56,940
four

694
00:32:56,940 --> 00:32:59,399
and you have I plus one

695
00:32:59,399 --> 00:33:03,000
and you have errors getting passed up

696
00:33:03,000 --> 00:33:04,740
and hidden State inferences getting

697
00:33:04,740 --> 00:33:06,779
passed down

698
00:33:06,779 --> 00:33:09,559
and so this would be like the sort of

699
00:33:09,559 --> 00:33:12,779
Timeless representation of that modeling

700
00:33:12,779 --> 00:33:15,059
hierarchy hierarchical base hierarchical

701
00:33:15,059 --> 00:33:18,600
predictive processing architecture

702
00:33:18,600 --> 00:33:20,580
um tilde through time so we're just

703
00:33:20,580 --> 00:33:22,620
describing timelessly

704
00:33:22,620 --> 00:33:26,640
here is the hierarchical Bayesian model

705
00:33:26,640 --> 00:33:30,779
this can be rendered or unpacked through

706
00:33:30,779 --> 00:33:33,600
time now we see that Tau minus one Tau

707
00:33:33,600 --> 00:33:36,559
Tau plus one

708
00:33:36,600 --> 00:33:39,360
as a function of time so instead of just

709
00:33:39,360 --> 00:33:41,220
seeing it as like o tilde like a

710
00:33:41,220 --> 00:33:42,960
sequence and seeing this as like a

711
00:33:42,960 --> 00:33:44,519
skyscraper

712
00:33:44,519 --> 00:33:47,159
this can be unrolled

713
00:33:47,159 --> 00:33:50,399
in a procedural way using message

714
00:33:50,399 --> 00:33:53,580
passing techniques

715
00:33:53,580 --> 00:33:54,960
um okay are the rules for message

716
00:33:54,960 --> 00:33:57,059
passing on a convoluted Bayesian Network

717
00:33:57,059 --> 00:33:58,559
known

718
00:33:58,559 --> 00:34:01,399
yes

719
00:34:09,839 --> 00:34:11,940
this

720
00:34:11,940 --> 00:34:13,560
paper

721
00:34:13,560 --> 00:34:15,899
that I have just put in the chat here we

722
00:34:15,899 --> 00:34:20,000
see figure 4.3 discrete time

723
00:34:20,639 --> 00:34:22,440
again and again

724
00:34:22,440 --> 00:34:26,580
and here is a alternative representation

725
00:34:26,580 --> 00:34:29,520
of this base graph with what is known as

726
00:34:29,520 --> 00:34:31,560
a Forney Factor graph

727
00:34:31,560 --> 00:34:34,320
and so in this paper

728
00:34:34,320 --> 00:34:36,739
they show that

729
00:34:36,739 --> 00:34:37,080
[Music]

730
00:34:37,080 --> 00:34:38,040
um

731
00:34:38,040 --> 00:34:42,119
any base graph can be represented as a

732
00:34:42,119 --> 00:34:44,159
Forney Factor graph

733
00:34:44,159 --> 00:34:47,099
and any phony Factor graph

734
00:34:47,099 --> 00:34:52,080
has a defined procedural message passing

735
00:34:52,080 --> 00:34:54,799
algorithm

736
00:34:56,879 --> 00:35:01,680
so actually a quite incredible

737
00:35:01,680 --> 00:35:05,760
synthesis even in this 2017 paper

738
00:35:05,760 --> 00:35:07,680
because it means if you if you can build

739
00:35:07,680 --> 00:35:09,660
with these Legos

740
00:35:09,660 --> 00:35:14,339
with this semantically interpretable

741
00:35:14,339 --> 00:35:16,940
setting

742
00:35:17,099 --> 00:35:20,040
then there will always be a procedural

743
00:35:20,040 --> 00:35:22,440
technique to calculate it

744
00:35:22,440 --> 00:35:27,060
no matter how convoluted this graph is

745
00:35:27,060 --> 00:35:29,339
which is very important because you

746
00:35:29,339 --> 00:35:30,839
could make um

747
00:35:30,839 --> 00:35:32,400
um you could just add linear model

748
00:35:32,400 --> 00:35:33,599
layers

749
00:35:33,599 --> 00:35:35,640
but then you would start to lose

750
00:35:35,640 --> 00:35:39,500
analytical guarantees very quickly

751
00:35:45,240 --> 00:35:47,940
here now we're in the continuous time so

752
00:35:47,940 --> 00:35:51,180
section 4.5 is a lot like 4.4

753
00:35:51,180 --> 00:35:54,119
except it focuses more on the continuous

754
00:35:54,119 --> 00:35:57,079
time model

755
00:35:57,180 --> 00:36:00,960
just to only describe more on 4 15.

756
00:36:00,960 --> 00:36:04,320
here we have our observations

757
00:36:04,320 --> 00:36:06,119
Through Time

758
00:36:06,119 --> 00:36:08,760
and that's a function G little G not

759
00:36:08,760 --> 00:36:10,260
expected free energy

760
00:36:10,260 --> 00:36:12,359
it's an observation its observations are

761
00:36:12,359 --> 00:36:15,780
being emitted by a function of hidden

762
00:36:15,780 --> 00:36:19,800
States and slowly varying causes V

763
00:36:19,800 --> 00:36:21,599
which kind of plays the role of policy

764
00:36:21,599 --> 00:36:24,540
in continuous time and a noise term so

765
00:36:24,540 --> 00:36:27,240
this is our sense of reading here's EEG

766
00:36:27,240 --> 00:36:30,540
measurements SPM measurements as a

767
00:36:30,540 --> 00:36:31,859
function of

768
00:36:31,859 --> 00:36:35,700
neural activity and sensor noise

769
00:36:35,700 --> 00:36:37,920
and then the rate of change of neural

770
00:36:37,920 --> 00:36:39,060
activity

771
00:36:39,060 --> 00:36:40,980
is a function of

772
00:36:40,980 --> 00:36:44,520
neural activity at that time and slowly

773
00:36:44,520 --> 00:36:46,619
varying causes

774
00:36:46,619 --> 00:36:50,520
so the mapping between uh data and

775
00:36:50,520 --> 00:36:52,800
hidden states which is kind of analogous

776
00:36:52,800 --> 00:36:55,320
to the a matrix is timeless

777
00:36:55,320 --> 00:36:57,300
and then the rate of change of the

778
00:36:57,300 --> 00:36:59,520
Hidden States x dot which is like X

779
00:36:59,520 --> 00:37:00,780
Prime

780
00:37:00,780 --> 00:37:03,180
is defined in a certain way and this is

781
00:37:03,180 --> 00:37:06,480
like familiar structurally to those who

782
00:37:06,480 --> 00:37:09,359
have looked at physics

783
00:37:09,359 --> 00:37:12,180
generalized coordinates of motion

784
00:37:12,180 --> 00:37:13,740
related to the Taylor series

785
00:37:13,740 --> 00:37:17,520
approximation so the the black line is

786
00:37:17,520 --> 00:37:19,920
the function true underlying function

787
00:37:19,920 --> 00:37:22,500
here's our first pass approximation at

788
00:37:22,500 --> 00:37:24,000
this time

789
00:37:24,000 --> 00:37:26,339
evaluate the function and then just

790
00:37:26,339 --> 00:37:30,540
that's you know y equals 4 is this line

791
00:37:30,540 --> 00:37:33,300
then we have the second term

792
00:37:33,300 --> 00:37:35,820
approximation which is

793
00:37:35,820 --> 00:37:38,040
the value for I'm just making that up

794
00:37:38,040 --> 00:37:41,760
and then the slope instantaneously

795
00:37:41,760 --> 00:37:44,760
the third order approximation is going

796
00:37:44,760 --> 00:37:46,020
to have

797
00:37:46,020 --> 00:37:49,079
three coordinates of motion

798
00:37:49,079 --> 00:37:51,420
the position

799
00:37:51,420 --> 00:37:54,480
the velocity slope

800
00:37:54,480 --> 00:37:56,760
and the acceleration

801
00:37:56,760 --> 00:37:58,200
and then if you were to have another

802
00:37:58,200 --> 00:37:59,280
term

803
00:37:59,280 --> 00:38:01,640
taking it into another level of

804
00:38:01,640 --> 00:38:03,960
exponentiation you would end up like

805
00:38:03,960 --> 00:38:07,260
kind of probably coming back out

806
00:38:07,260 --> 00:38:11,880
so that's how smooth approximation of

807
00:38:11,880 --> 00:38:15,079
functions Works using the Taylor series

808
00:38:15,079 --> 00:38:19,140
by evaluating higher derivatives

809
00:38:19,140 --> 00:38:21,960
around the point of focus what is the

810
00:38:21,960 --> 00:38:25,220
difference between the dot and the dash

811
00:38:25,560 --> 00:38:26,940
yeah

812
00:38:26,940 --> 00:38:30,180
the X so in in principle nothing in

813
00:38:30,180 --> 00:38:33,980
principle they both mean rate of change

814
00:38:34,260 --> 00:38:35,940
here

815
00:38:35,940 --> 00:38:39,240
x dot and dash are being used

816
00:38:39,240 --> 00:38:40,619
because

817
00:38:40,619 --> 00:38:42,420
x dot

818
00:38:42,420 --> 00:38:44,760
is

819
00:38:44,760 --> 00:38:47,780
the change

820
00:38:47,880 --> 00:38:50,339
function on hidden States neural

821
00:38:50,339 --> 00:38:53,460
activity changing through time is x dot

822
00:38:53,460 --> 00:38:56,099
so here is our estimate of neural

823
00:38:56,099 --> 00:38:58,800
activity changing Through Time

824
00:38:58,800 --> 00:39:01,800
here's the rate of change

825
00:39:01,800 --> 00:39:04,040
of neural activity changing Through Time

826
00:39:04,040 --> 00:39:06,359
x dot

827
00:39:06,359 --> 00:39:07,980
Prime

828
00:39:07,980 --> 00:39:13,339
but that is not necessarily X double dot

829
00:39:13,500 --> 00:39:17,119
I hope that that's accurate

830
00:39:17,220 --> 00:39:19,879
Jonathan

831
00:39:20,460 --> 00:39:21,780
yes I think one of the things that

832
00:39:21,780 --> 00:39:24,420
confused me most here is that he we've

833
00:39:24,420 --> 00:39:30,180
got an F Prime of X Prime V Prime

834
00:39:30,180 --> 00:39:33,119
um so certainly in figure 4.5 the prime

835
00:39:33,119 --> 00:39:35,160
really does represent a derivative with

836
00:39:35,160 --> 00:39:36,599
respect to Tau

837
00:39:36,599 --> 00:39:39,839
and then within the equation here

838
00:39:39,839 --> 00:39:42,720
it's just not quite clear you know what

839
00:39:42,720 --> 00:39:44,400
what one is taking derivatives with

840
00:39:44,400 --> 00:39:47,160
respect to and how many times like f

841
00:39:47,160 --> 00:39:49,619
double Prime of X double Prime V double

842
00:39:49,619 --> 00:39:51,060
Prime

843
00:39:51,060 --> 00:39:53,040
it's just one it's one does not need

844
00:39:53,040 --> 00:39:55,520
these two

845
00:39:55,800 --> 00:39:57,540
um

846
00:39:57,540 --> 00:40:00,119
well that that's what I'm

847
00:40:00,119 --> 00:40:02,760
so to calculate the Taylor series here

848
00:40:02,760 --> 00:40:04,740
you would want to include derivatives

849
00:40:04,740 --> 00:40:06,900
but it's derivatives with respect to the

850
00:40:06,900 --> 00:40:10,920
variables inside like with respect to X

851
00:40:10,920 --> 00:40:12,300
yeah

852
00:40:12,300 --> 00:40:14,579
yeah yeah and you have to be confused by

853
00:40:14,579 --> 00:40:17,940
this no that that totally makes sense

854
00:40:17,940 --> 00:40:19,920
I um

855
00:40:19,920 --> 00:40:23,040
I agree anywhere where there's a

856
00:40:23,040 --> 00:40:26,900
derivative or an integral

857
00:40:27,119 --> 00:40:29,040
there should be clarification of what

858
00:40:29,040 --> 00:40:31,560
it's with respect to

859
00:40:31,560 --> 00:40:35,299
and that's sometimes done

860
00:40:36,060 --> 00:40:39,000
but here Prime is being used just to

861
00:40:39,000 --> 00:40:41,940
refer to the generalized coordinate of

862
00:40:41,940 --> 00:40:44,359
motion

863
00:40:44,579 --> 00:40:48,079
at that scale

864
00:40:48,240 --> 00:40:50,520
okay then

865
00:40:50,520 --> 00:40:54,180
writing down the variational free energy

866
00:40:54,180 --> 00:40:57,919
with generalized coordinates

867
00:41:02,000 --> 00:41:05,579
LaPlace approximation

868
00:41:05,579 --> 00:41:07,500
this is something where there's a lot of

869
00:41:07,500 --> 00:41:09,000
technical detail

870
00:41:09,000 --> 00:41:11,820
but what the LaPlace approximation does

871
00:41:11,820 --> 00:41:15,359
broadly also please correct me is

872
00:41:15,359 --> 00:41:18,300
finds the single maximum likelihood

873
00:41:18,300 --> 00:41:20,640
estimate for a distribution

874
00:41:20,640 --> 00:41:23,760
and then parameterizes a negative

875
00:41:23,760 --> 00:41:27,720
quadratic with the points being at the

876
00:41:27,720 --> 00:41:29,700
maximum likelihood estimate

877
00:41:29,700 --> 00:41:31,680
so for distributions with a central

878
00:41:31,680 --> 00:41:33,359
tendency

879
00:41:33,359 --> 00:41:36,480
LaPlace approximation can do really well

880
00:41:36,480 --> 00:41:39,240
for distributions that are bimodal like

881
00:41:39,240 --> 00:41:40,680
with two humps

882
00:41:40,680 --> 00:41:42,359
LaPlace approximation is going to

883
00:41:42,359 --> 00:41:45,000
identify the slightly taller hump

884
00:41:45,000 --> 00:41:47,720
in its maximum a posteriori estimator

885
00:41:47,720 --> 00:41:52,260
and then either draw a pathologically

886
00:41:52,260 --> 00:41:55,020
narrow or wide quadratic around the

887
00:41:55,020 --> 00:41:56,220
taller hump

888
00:41:56,220 --> 00:41:58,680
Ollie oh okay

889
00:41:58,680 --> 00:42:01,339
and more details on that

890
00:42:01,339 --> 00:42:03,960
quadratic expansion around the posterior

891
00:42:03,960 --> 00:42:06,359
mode mode most likely point of a

892
00:42:06,359 --> 00:42:09,599
distribution quadratic expansion

893
00:42:09,599 --> 00:42:13,140
drawing an upside down parabola

894
00:42:13,140 --> 00:42:16,040
around that point

895
00:42:16,619 --> 00:42:20,460
expanding some of those discussions from

896
00:42:20,460 --> 00:42:25,920
the above into hierarchical models

897
00:42:28,440 --> 00:42:31,320
re-viewing

898
00:42:31,320 --> 00:42:34,680
the message passing

899
00:42:34,680 --> 00:42:36,960
that we saw previously skyscraper on the

900
00:42:36,960 --> 00:42:40,020
left unrolled Through Time on the right

901
00:42:40,020 --> 00:42:42,300
in the context of generalized predictive

902
00:42:42,300 --> 00:42:45,200
coding schemes

903
00:42:50,880 --> 00:42:54,500
and a short summary

904
00:42:55,560 --> 00:43:00,259
so definitely an interesting chapter

905
00:43:02,700 --> 00:43:05,700
the mathematical details

906
00:43:05,700 --> 00:43:08,419
Beyond

907
00:43:09,119 --> 00:43:12,440
way back when

908
00:43:12,960 --> 00:43:15,960
basically Beyond this point

909
00:43:15,960 --> 00:43:19,380
really escalate fast

910
00:43:19,380 --> 00:43:22,619
and there's also several boxes and

911
00:43:22,619 --> 00:43:23,940
asides

912
00:43:23,940 --> 00:43:27,540
like this message passing

913
00:43:27,540 --> 00:43:29,880
generalized coordinates motion

914
00:43:29,880 --> 00:43:32,819
and LaPlace approximation

915
00:43:32,819 --> 00:43:35,060
all of which are

916
00:43:35,060 --> 00:43:39,779
significant technical Concepts

917
00:43:40,200 --> 00:43:41,400
so

918
00:43:41,400 --> 00:43:43,440
a lot of challenges and trade-offs with

919
00:43:43,440 --> 00:43:45,720
writing such a chapter

920
00:43:45,720 --> 00:43:48,119
especially because this is like active

921
00:43:48,119 --> 00:43:49,920
inference

922
00:43:49,920 --> 00:43:52,079
chapter one overview chapter two low

923
00:43:52,079 --> 00:43:53,400
road this is how we're gonna do it

924
00:43:53,400 --> 00:43:55,440
chapter three High Road this is why

925
00:43:55,440 --> 00:43:58,079
we're gonna do it chapter four

926
00:43:58,079 --> 00:44:00,900
generative models of active inference

927
00:44:00,900 --> 00:44:03,599
figure 4.3 discrete and continuous time

928
00:44:03,599 --> 00:44:05,160
generative models

929
00:44:05,160 --> 00:44:07,440
and then

930
00:44:07,440 --> 00:44:11,660
there's a lot of technical details

931
00:44:12,660 --> 00:44:15,380
so

932
00:44:15,380 --> 00:44:18,660
anyone have any just overall thoughts or

933
00:44:18,660 --> 00:44:21,680
questions on for

934
00:44:23,359 --> 00:44:27,259
Jonathan and then anyone else

935
00:44:27,359 --> 00:44:30,660
yeah I think I'd just to get more of a

936
00:44:30,660 --> 00:44:32,760
sense of it I'd still be interested to

937
00:44:32,760 --> 00:44:36,060
know more about the the Tau in G

938
00:44:36,060 --> 00:44:37,560
um if anyone has any insight into that

939
00:44:37,560 --> 00:44:40,200
when you calculate G

940
00:44:40,200 --> 00:44:42,060
um

941
00:44:42,060 --> 00:44:46,319
yeah does g still have a Tau index or or

942
00:44:46,319 --> 00:44:49,520
is something else happening to it

943
00:44:51,960 --> 00:44:54,780
yes good good that is noted if anyone

944
00:44:54,780 --> 00:44:56,640
has a link or a thought go for it

945
00:44:56,640 --> 00:44:58,800
otherwise that is definitely something

946
00:44:58,800 --> 00:45:01,619
that we will need to revisit and improve

947
00:45:01,619 --> 00:45:05,640
for for future cohorts Kate

948
00:45:05,640 --> 00:45:07,740
um they're just a very general comment

949
00:45:07,740 --> 00:45:10,260
like this was way over my head overall

950
00:45:10,260 --> 00:45:12,839
but I was just wondering if in your

951
00:45:12,839 --> 00:45:14,579
experience um

952
00:45:14,579 --> 00:45:16,200
the

953
00:45:16,200 --> 00:45:18,359
the textbook that you are kind of

954
00:45:18,359 --> 00:45:21,180
working through with um mispronounced

955
00:45:21,180 --> 00:45:23,819
his name name Joshi but the fundamentals

956
00:45:23,819 --> 00:45:25,980
of active inference do you think there

957
00:45:25,980 --> 00:45:30,859
will be some help in any of these areas

958
00:45:31,200 --> 00:45:33,300
Ollie what's your what's your first

959
00:45:33,300 --> 00:45:35,640
thought on that this is in reference to

960
00:45:35,640 --> 00:45:38,640
sanjiv namjoshi's fundamentals textbook

961
00:45:38,640 --> 00:45:41,280
that we are um like working through and

962
00:45:41,280 --> 00:45:44,040
evaluating during 2023. so Ali what

963
00:45:44,040 --> 00:45:46,079
would you say to Kate's question

964
00:45:46,079 --> 00:45:50,220
uh yeah in my opinion I think it is way

965
00:45:50,220 --> 00:45:52,920
way more helpful in understanding the

966
00:45:52,920 --> 00:45:57,359
mathematical details and uh also in fact

967
00:45:57,359 --> 00:46:01,079
uh the conceptual intricacies uh that

968
00:46:01,079 --> 00:46:05,400
have been somehow uh glossed over in

969
00:46:05,400 --> 00:46:09,060
this textbook uh but uh on the other

970
00:46:09,060 --> 00:46:13,619
hand it is estimated to be about 600 to

971
00:46:13,619 --> 00:46:17,700
900 pages long so uh yeah it is much

972
00:46:17,700 --> 00:46:20,460
more expanded than this textbook and it

973
00:46:20,460 --> 00:46:25,740
goes into uh into a very very helpful uh

974
00:46:25,740 --> 00:46:27,240
detail

975
00:46:27,240 --> 00:46:30,000
um expanding upon these

976
00:46:30,000 --> 00:46:34,680
um mathematical derivations but at one

977
00:46:34,680 --> 00:46:39,079
point I wanted to make is that

978
00:46:39,079 --> 00:46:42,599
sanjeev's textbook is not strictly

979
00:46:42,599 --> 00:46:47,460
strictly geared towards uh somehow

980
00:46:47,460 --> 00:46:48,920
somewhat

981
00:46:48,920 --> 00:46:51,300
mathematical derivation of these

982
00:46:51,300 --> 00:46:56,760
equations but its main objective is to

983
00:46:56,760 --> 00:47:00,720
use it as a practical tool in order to

984
00:47:00,720 --> 00:47:01,880
model

985
00:47:01,880 --> 00:47:06,720
these kinds of agents and in fact he

986
00:47:06,720 --> 00:47:09,720
also provides some helpful Jupiter

987
00:47:09,720 --> 00:47:14,480
notebooks in that regard but yes I think

988
00:47:14,480 --> 00:47:19,260
it can be really helpful in Bridging the

989
00:47:19,260 --> 00:47:21,480
Gap between this textbook and the

990
00:47:21,480 --> 00:47:25,079
Practical side of doing things

991
00:47:25,079 --> 00:47:28,380
yeah thanks ali um if anybody so the

992
00:47:28,380 --> 00:47:30,780
book is not available yet it's not even

993
00:47:30,780 --> 00:47:34,319
finished in writing if anybody wants to

994
00:47:34,319 --> 00:47:38,400
be added to view

995
00:47:38,400 --> 00:47:40,800
chapters and the code which are all in

996
00:47:40,800 --> 00:47:42,599
Python which also increases the

997
00:47:42,599 --> 00:47:44,760
accessibility relative to the Matlab of

998
00:47:44,760 --> 00:47:46,859
this part hole

999
00:47:46,859 --> 00:47:48,960
these

1000
00:47:48,960 --> 00:47:50,760
chapters

1001
00:47:50,760 --> 00:47:55,260
in a way that has not appeared in our

1002
00:47:55,260 --> 00:47:58,680
space before are tackling

1003
00:47:58,680 --> 00:48:01,440
the these statistical questions

1004
00:48:01,440 --> 00:48:03,540
so it is still like very much an

1005
00:48:03,540 --> 00:48:07,800
accelerated statistical learning curve

1006
00:48:07,800 --> 00:48:10,680
but as Ali mentioned it's less focused

1007
00:48:10,680 --> 00:48:12,180
on like well here's how you get from

1008
00:48:12,180 --> 00:48:14,460
here to here as just like a as a

1009
00:48:14,460 --> 00:48:17,280
analytical derivation consideration

1010
00:48:17,280 --> 00:48:19,380
and it's a lot more like

1011
00:48:19,380 --> 00:48:21,180
we're going to go step by step with

1012
00:48:21,180 --> 00:48:24,839
building stochastic simulations and um

1013
00:48:24,839 --> 00:48:27,000
so this is gonna it's gonna be a great

1014
00:48:27,000 --> 00:48:30,480
compliment and and Fields have many

1015
00:48:30,480 --> 00:48:32,400
textbooks so there's not just one right

1016
00:48:32,400 --> 00:48:34,560
approach

1017
00:48:34,560 --> 00:48:35,220
um

1018
00:48:35,220 --> 00:48:38,099
but it's been really encouraging to see

1019
00:48:38,099 --> 00:48:40,200
his work so if anybody wants like even

1020
00:48:40,200 --> 00:48:43,440
just access to the coda to review the

1021
00:48:43,440 --> 00:48:45,480
chapters

1022
00:48:45,480 --> 00:48:48,720
um please feel free to email us and

1023
00:48:48,720 --> 00:48:50,640
hopefully

1024
00:48:50,640 --> 00:48:53,940
in the coming year

1025
00:48:53,940 --> 00:48:55,619
we'll have more details on when it'll be

1026
00:48:55,619 --> 00:48:57,660
available

1027
00:48:57,660 --> 00:48:59,660
um

1028
00:48:59,760 --> 00:49:01,980
let us look ahead quickly to chapter

1029
00:49:01,980 --> 00:49:03,900
five

1030
00:49:03,900 --> 00:49:06,359
chapter five is the last chapter in the

1031
00:49:06,359 --> 00:49:08,339
first half of the textbook which is the

1032
00:49:08,339 --> 00:49:10,859
epistemic half of the textbook

1033
00:49:10,859 --> 00:49:13,440
so congratulations to those who have

1034
00:49:13,440 --> 00:49:15,540
continued on especially through that

1035
00:49:15,540 --> 00:49:18,000
second half of chapter four

1036
00:49:18,000 --> 00:49:21,540
which is definitely like

1037
00:49:21,540 --> 00:49:23,040
intense

1038
00:49:23,040 --> 00:49:26,839
to a comical degree

1039
00:49:27,119 --> 00:49:30,540
um and so carefree and Whimsical in its

1040
00:49:30,540 --> 00:49:31,980
writing style

1041
00:49:31,980 --> 00:49:33,420
which which

1042
00:49:33,420 --> 00:49:34,740
um

1043
00:49:34,740 --> 00:49:37,380
has its own kind of Comedy but we're

1044
00:49:37,380 --> 00:49:40,619
gonna have two weeks on chapter five

1045
00:49:40,619 --> 00:49:42,540
which is going to be a great return to

1046
00:49:42,540 --> 00:49:44,099
reality

1047
00:49:44,099 --> 00:49:45,900
and then

1048
00:49:45,900 --> 00:49:46,680
um

1049
00:49:46,680 --> 00:49:49,260
the final meeting will be on feedback

1050
00:49:49,260 --> 00:49:51,240
project ideas Terry I think we need to

1051
00:49:51,240 --> 00:49:52,980
approach to the maths pitched at a

1052
00:49:52,980 --> 00:49:54,960
seven-year-old yeah many angles on that

1053
00:49:54,960 --> 00:49:57,240
one is in the math learning group

1054
00:49:57,240 --> 00:50:00,420
we've explored these kinds of questions

1055
00:50:00,420 --> 00:50:03,720
and another approach is

1056
00:50:03,720 --> 00:50:07,800
a non-mathematics approach which several

1057
00:50:07,800 --> 00:50:10,140
people have investigated

1058
00:50:10,140 --> 00:50:11,640
how important will it be if we didn't

1059
00:50:11,640 --> 00:50:13,260
grasp all the details of chapter four

1060
00:50:13,260 --> 00:50:15,180
when we get to chapter five

1061
00:50:15,180 --> 00:50:17,700
irrelevant

1062
00:50:17,700 --> 00:50:20,160
chapter 5 picks up on a very different

1063
00:50:20,160 --> 00:50:21,839
note

1064
00:50:21,839 --> 00:50:23,700
remember that chapter four they even

1065
00:50:23,700 --> 00:50:25,619
said you could skip

1066
00:50:25,619 --> 00:50:26,700
said if you don't want to know the

1067
00:50:26,700 --> 00:50:28,020
technical details you can skip chapter

1068
00:50:28,020 --> 00:50:30,359
four which was again like it was like

1069
00:50:30,359 --> 00:50:32,040
wait but

1070
00:50:32,040 --> 00:50:33,720
we just took the low road and the high

1071
00:50:33,720 --> 00:50:35,400
road to our city

1072
00:50:35,400 --> 00:50:37,079
and then they're like you don't have to

1073
00:50:37,079 --> 00:50:39,300
go on the tour

1074
00:50:39,300 --> 00:50:42,079
if you don't want to

1075
00:50:42,119 --> 00:50:44,280
chapter 5 message passing and

1076
00:50:44,280 --> 00:50:46,880
neurobiology

1077
00:50:47,099 --> 00:50:49,160
um

1078
00:50:51,059 --> 00:50:53,280
Bayesian message passing

1079
00:50:53,280 --> 00:50:55,200
comes back

1080
00:50:55,200 --> 00:50:56,760
so again that box that we looked at

1081
00:50:56,760 --> 00:51:00,119
earlier the idea that every base graph

1082
00:51:00,119 --> 00:51:01,920
is sparse

1083
00:51:01,920 --> 00:51:05,420
not everything depends on everything

1084
00:51:05,460 --> 00:51:07,260
because of the sparsity of the Bayes

1085
00:51:07,260 --> 00:51:09,480
graph especially

1086
00:51:09,480 --> 00:51:12,599
there are some computational procedures

1087
00:51:12,599 --> 00:51:15,960
that are extremely effective

1088
00:51:15,960 --> 00:51:18,720
and interestingly

1089
00:51:18,720 --> 00:51:22,200
real biological systems also have sparse

1090
00:51:22,200 --> 00:51:24,920
connectivity whether it's the neural

1091
00:51:24,920 --> 00:51:27,540
architecture in the brain or whether

1092
00:51:27,540 --> 00:51:29,520
it's the interaction patterns in an ant

1093
00:51:29,520 --> 00:51:31,200
colony

1094
00:51:31,200 --> 00:51:33,839
in distributed systems

1095
00:51:33,839 --> 00:51:37,160
connections are sparse

1096
00:51:37,920 --> 00:51:42,900
they begin at the microanatomical level

1097
00:51:42,900 --> 00:51:46,020
with looking at neurons and their

1098
00:51:46,020 --> 00:51:47,579
connections

1099
00:51:47,579 --> 00:51:49,020
they focus on one of the most

1100
00:51:49,020 --> 00:51:52,319
well-studied neural tissues which is the

1101
00:51:52,319 --> 00:51:54,180
mammalian cortex

1102
00:51:54,180 --> 00:51:56,160
and the mammalian cortex has what's

1103
00:51:56,160 --> 00:51:58,859
called a columnar architecture

1104
00:51:58,859 --> 00:52:01,800
which is these six histologically

1105
00:52:01,800 --> 00:52:05,099
distinct layers of tissue so they look

1106
00:52:05,099 --> 00:52:07,319
different under a microscope

1107
00:52:07,319 --> 00:52:10,500
and the connectivity patterns in terms

1108
00:52:10,500 --> 00:52:13,140
of what actually sends axons where are

1109
00:52:13,140 --> 00:52:14,880
also essentially completely

1110
00:52:14,880 --> 00:52:16,319
characterized

1111
00:52:16,319 --> 00:52:20,059
and so in here we see the

1112
00:52:20,059 --> 00:52:23,460
tantalizingly realist alignment

1113
00:52:23,460 --> 00:52:25,859
of the histological

1114
00:52:25,859 --> 00:52:29,160
columnar microarchitecture

1115
00:52:29,160 --> 00:52:32,700
with Bayesian graphs

1116
00:52:32,700 --> 00:52:37,460
that have similar structure

1117
00:52:38,579 --> 00:52:41,040
in figure 5.2

1118
00:52:41,040 --> 00:52:42,780
they're going to generalize and kind of

1119
00:52:42,780 --> 00:52:45,720
coarse grain the columnar architecture

1120
00:52:45,720 --> 00:52:49,440
and now look at how different columns

1121
00:52:49,440 --> 00:52:51,599
communicate laterally

1122
00:52:51,599 --> 00:52:53,520
so we have like

1123
00:52:53,520 --> 00:52:55,980
hierarchical communication within a

1124
00:52:55,980 --> 00:52:57,180
columnar

1125
00:52:57,180 --> 00:52:59,460
unit and

1126
00:52:59,460 --> 00:53:01,619
connections across units this is kind of

1127
00:53:01,619 --> 00:53:02,339
like

1128
00:53:02,339 --> 00:53:04,859
um numanta thousand brain hypothesis

1129
00:53:04,859 --> 00:53:08,220
like a lot of work that is using the

1130
00:53:08,220 --> 00:53:11,700
neocortical architecture in search of

1131
00:53:11,700 --> 00:53:14,660
intelligent machines

1132
00:53:14,700 --> 00:53:17,400
so that's the neocortex

1133
00:53:17,400 --> 00:53:19,079
scenario

1134
00:53:19,079 --> 00:53:21,240
we're going to switch systems to the

1135
00:53:21,240 --> 00:53:23,339
mammalian motor chord

1136
00:53:23,339 --> 00:53:26,099
here we have incoming proprioceptive

1137
00:53:26,099 --> 00:53:27,900
data y

1138
00:53:27,900 --> 00:53:30,780
and descending predictions

1139
00:53:30,780 --> 00:53:33,839
those predictions and observations are

1140
00:53:33,839 --> 00:53:37,140
juxtaposed to form an error signal which

1141
00:53:37,140 --> 00:53:38,819
gets passed forward

1142
00:53:38,819 --> 00:53:41,460
so this is a continuous time setting

1143
00:53:41,460 --> 00:53:45,000
and here we can see action as resolving

1144
00:53:45,000 --> 00:53:48,059
discrepancies between Target set points

1145
00:53:48,059 --> 00:53:52,160
and proprioceptive input

1146
00:53:55,260 --> 00:53:57,300
now we switch to another mammalian

1147
00:53:57,300 --> 00:54:00,059
neural anatomical system which is

1148
00:54:00,059 --> 00:54:03,000
subcortical and the basal ganglia in

1149
00:54:03,000 --> 00:54:04,800
particular which is a dopaminergic

1150
00:54:04,800 --> 00:54:07,800
region

1151
00:54:08,040 --> 00:54:10,800
this region plays a role in policy

1152
00:54:10,800 --> 00:54:11,700
selection

1153
00:54:11,700 --> 00:54:14,819
check out the Oliver Sacks book or Robin

1154
00:54:14,819 --> 00:54:17,420
Williams movie Awakenings it's really

1155
00:54:17,420 --> 00:54:19,740
interesting and funny

1156
00:54:19,740 --> 00:54:22,200
here we see two different ways that the

1157
00:54:22,200 --> 00:54:25,520
dopaminergic tone

1158
00:54:25,559 --> 00:54:29,099
tilts the balance towards either action

1159
00:54:29,099 --> 00:54:33,300
selection which passes habits forward

1160
00:54:33,300 --> 00:54:37,920
or that kind of type 1 indirect policy

1161
00:54:37,920 --> 00:54:39,900
selection pathway

1162
00:54:39,900 --> 00:54:42,960
can tip towards a type 2 in economon

1163
00:54:42,960 --> 00:54:45,359
sense deliberative

1164
00:54:45,359 --> 00:54:48,900
free energy updated policy posterior but

1165
00:54:48,900 --> 00:54:51,599
we'll talk more about it

1166
00:54:51,599 --> 00:54:52,740
they talk about different

1167
00:54:52,740 --> 00:54:55,800
neurotransmitters inactive

1168
00:54:55,800 --> 00:54:57,960
they talk about how continuous and

1169
00:54:57,960 --> 00:55:00,000
discrete models can come together

1170
00:55:00,000 --> 00:55:01,980
and then they end the chapter

1171
00:55:01,980 --> 00:55:03,780
with bringing together those three

1172
00:55:03,780 --> 00:55:05,579
systems that were previously described

1173
00:55:05,579 --> 00:55:07,920
the columnar architecture in the

1174
00:55:07,920 --> 00:55:10,079
neocortex

1175
00:55:10,079 --> 00:55:12,839
the basal ganglia dopaminergic

1176
00:55:12,839 --> 00:55:16,559
decision-making architectures and the

1177
00:55:16,559 --> 00:55:19,579
spinal cord reflex proprioceptive

1178
00:55:19,579 --> 00:55:22,020
continuous time model

1179
00:55:22,020 --> 00:55:25,140
and they show how those kinds of models

1180
00:55:25,140 --> 00:55:27,599
can be integrated

1181
00:55:27,599 --> 00:55:32,099
to address some of our favorite topics

1182
00:55:32,099 --> 00:55:34,559
so it's a shorter chapter it doesn't

1183
00:55:34,559 --> 00:55:37,319
have equations like the previous ones do

1184
00:55:37,319 --> 00:55:40,440
so again congratulations to everybody

1185
00:55:40,440 --> 00:55:43,140
who made it through the more

1186
00:55:43,140 --> 00:55:45,300
think of it chapter four

1187
00:55:45,300 --> 00:55:47,160
and chapter five we're going to have a

1188
00:55:47,160 --> 00:55:50,040
lot of space for conversations about

1189
00:55:50,040 --> 00:55:54,180
like neurophysiology and the body

1190
00:55:54,180 --> 00:55:57,119
and it's just a good close to the first

1191
00:55:57,119 --> 00:55:58,740
half of the textbook

1192
00:55:58,740 --> 00:56:00,300
so

1193
00:56:00,300 --> 00:56:02,819
thank you all for joining

1194
00:56:02,819 --> 00:56:05,720
see you next week


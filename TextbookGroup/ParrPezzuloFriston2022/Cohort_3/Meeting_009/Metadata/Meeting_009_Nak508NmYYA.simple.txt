SPEAKER_00:
all right greetings it's march 22nd 2023 we're in cohort three and meeting nine in our second discussion on chapter four of the textbook

so last time we went through some sections and we have um also a lot of questions that have been addressed so before we go back to the text or to the questions does anyone who's here want to just like give any thought or reflection

Yeah, what are you still looking to reduce your uncertainty on?


SPEAKER_01:
I'm just finding a lot of the mathematics, it's just not explained in a way that disambiguates what a particular expression might be.

I've got certain uncertainties about some of the mathematical objects themselves.

And then particularly on the section on Taylor series, and I feel that I have a pretty good grasp on Taylor series,

that's feeling particularly opaque to me, which feels surprising.

Yeah, I'm finding it a slightly frustrating chapter, which sort of dives into the maths, but in some sense without enough detail for me to really deeply understand it.

But maybe that's just a matter of time.


SPEAKER_00:
Thank you.

Well, definitely when building on a sort of

meat and potatoes mathematical concept someone with your expertise should not be overly confused it should feel like you have already been on the on-ramp to understanding where this is going because it's hard to imagine it'd be clearer for someone who is less familiar with the foundational pieces so let's definitely keep that in mind when we move through okay um

anyone else want to just like give any overview thought on a section of four or anything else on how they're thinking about any aspect of it since last week then we will look at the questions and then continue to move through the text

just looking at the previous video to see where we ended 410 okay any other comments otherwise we're going to go to 410 and continue or a little bit before okay

All right, let's pull back to 4.4 and just please raise your hand or write in the chat with any thoughts or questions.

So just looking at the questions that were submitted in this cohort, like many of them involve the equations four, seven, eight,

10 and so on so these are ones where like we can um look at at what was written for those where things were written or otherwise just like kind of um given the constraints on what we can do in 50 minutes just recognize these are like important areas for there to be asynchronous development on but this looks like a great

um answer whomever has written this and it's like a an exemplar of um how to address it asynchronously yes jonathan


SPEAKER_01:
This was my contribution here, the explanation underneath.

I still have some questions about it, despite the fact that I think I now understand the vector aspect, so I've created an Overleaf document that sort of explains all of this.

I'm still slightly confused by something quite simple here in the expected free energy, because there are these tau indices floating around, and tau indicates, I think, the moment in time.

So, for instance, s of p subscript pi tau is the probability of being in a given state at a particular time, given that you're undertaking some policy pi.

And so there's a tau index floating around, but it's still not quite clear to me in the calculation of the expected free energy what you do with that tau index.

Are you summing over all of these terms?

Are you integrating over them?

It is an expectation value over all of the different time steps.

So that was the one thing that was sort of least clear to me here.


SPEAKER_00:
Yes, great point.

is it expected free energy as an expectation of snapshot free energies is it a expected free energy as the sum of the expected futures or is it even the expectation at that last time step alone and the other ones are discarded good points

In figure 4.3, we have the discrete time, partially observable Markov decision process, POMDP, and the continuous time cousin.

In the discrete time case, past, present, and future time steps, hidden states are explicitly predicted, and action sequences are explicitly intervening in how hidden states change their time.

in contrast in the continuous time model rather than predicting or even explicitly mentioning past present and future we see like x x prime x double prime so the value of x and then the higher derivatives also known as the generalized coordinates of motion and so this

amounts to making a taylor series expansion which is continuous around the current time step two different ways of dealing with temporal depth and the way that action selection relates to temporal depth and they have some in principle and in practices strengths and weaknesses in different settings

in 4.4 they're going to focus on the discrete time model the discrete time model has been especially applied to categorical cognitive decision making whereas the archetypal case for the continuous time generative model is continuous perception and action of like a motor unit

they can play well together as we'll see in chapter eight with hybrid models we could go slower fast on equations but we'll just try to like move through the whole thing but many many of these the work to be done is making sure that we have um

high quality descriptions verbally which we do for for many due to many people's great work but making sure that we have verbal descriptions in the active inference ontology that are faithful to the symbols

and then where people have questions about what does this mean why is that that way what if it were this way what are the implications here those are all great questions and they can all be included in the notes section for a given equation or in the more general notes table

but for many of the equations including some of the super challenging ones um we have things written out which is very good um but just kind of at the um skim level cat is a categorical distribution so in this section we're in a discrete setting so like

light is on or off the continuous setting would be it's a continuous variable between zero and one but in the categorical setting we have categorical distributions and so we have certain um ways of summing across them for example instead of taking an integral over a continuous distribution um

action selection pi policy selection is going to be guided by which policies are most likely most self-consistent most self-evident most probable lowest expected free energy we're not proposing a reward or utility function and then evaluating policies based upon their expected reward

evaluating policies based upon their self-consistency and in fact the most probable ones are the ones that lead to the lowest expected free energy so more equations g is expected free energy and even just looking at which variables go into this we see

a probability distribution over policies that is defined through an expected free energy calculation.

And this g of pi, it's a function of policy, and it is going to have this expected free energy construction.

and we've seen expected free energy before in equation 2.6 where the part that's an expectation that only involves preferences and observations is pragmatic value so natural log of a distribution expectation of a log

of something only involving observations and preferences here we have y tilde here we have o tilde y is sometimes used for observations more in like a linear regression setting o is like observations but they're identical and then the other component is a di and so it's pragmatic value because that is the alignment between the observations and the preferences or expectations

other component in this decomposition of expected free energy is a kl divergence expected kl divergence between two variational distributions two distributions that we control q they're both x hidden states through time x tilde conditioned upon and then they're very similar

Here we only have pi.

Here we have pi and y. So this is the divergence with respect to whether adding observations updates our prior.

If these two cues are the same, that's equivalent to saying there's no space between the distributions

where we've parameterized x with or without getting any observations.

In other words, the observations were valueless if the divergence here is zero.

Whereas if the observations are really impactful on moving the distribution, then this KL divergence is going to be big.

Divergences are always not zero.

And then there's a negative here.

So the more information gain, the bigger the KL divergence, the more that the observations matter, the more that this first term is going to go negative.

And so that specific policy will have a higher epistemic value.

Ali?


SPEAKER_03:
Yeah, I just remembered

a clarification made by Ryan Smith in one of his lectures about two concepts or rather two notions of time used in active inference literature, one of which is denoted by tau and the other one is denoted by t. So tau is used for the time about which we have a belief.

And the examples he gives is

For instance, I believe I am now in my car, so that would be S of tau.

But I believe I was in my kitchen 10 minutes ago would be S tau minus 1.

Or I believe I will be at work in 20 minutes or something would be S tau plus 1.

But on the other hand, we have T, which refers to the time at which a new observation is given.

so for instance uh as an example and after turning on a light i now in other words s of tau believe that i was in the kitchen for the last five minutes right

so we can update our belief about all uh all taus or s of taus with each new observation at each tease or at each given observation times so uh briefly one is the belief time and the other one is the observation time so tau is used for


SPEAKER_00:
leaf times or the time about which we have leaf and t is used for observation times thanks that's very helpful is it accurate to say that t is like the click of our simulation that's actually handing the new observations in at a given moment

t after t but then especially in sophisticated active inference where we might be considering like the past present and future at every t we are re-evaluating taus at each t


SPEAKER_03:
Yeah, I think we can reframe that in this way as well.

Awesome.


SPEAKER_00:
So here we reach, yes, potentially with some bumps and jumps, the expected free energy for a policy in terms of the pragmatic value.

alignment of preferences with observations and the expected information gain the epistemic value in terms of how much adding in observations updates our variational distribution about hidden states conditioned upon that policy

Here is more focus on how we get this KL divergence.

So here we see that KL divergence, and there's just a few different expansions and relationships around it.

Jonathan?


SPEAKER_01:
Yeah, so just zooming into that equation there, there's an expectation over Q of O given pi.

And I couldn't figure out how that's calculated because I understand how one can get Q of S given O and pi.

But it seems that to calculate Q of O given pi, one needs to marginalize or sum over the states.

And I thought that that was the whole reason for introducing Q in the first place was because we can't sum over all external states.


SPEAKER_00:
The subscript notation may be used in an unconventional way, but I agree with you on that.

Here, in the inner queue, we have it about S, conditioned upon observations and policies.

This doesn't seem contentious.

Yeah, that's fine.

Yeah.

but the subscript in 4.8 and I think that was probably one of your questions yeah yeah yeah that's the one

Sorry, with the terrible lack of latency.

Yeah, we should look at the MATLAB code for wherever that comes into play to see how it actually is done.

Or is it a separate variational quality?

yeah like this is the just the inner value is the kl divergence of how much observations update um uh add in to your changing beliefs about s but we're taking an expectation of that over expected observation sequences which then begs the question of

needing to know the distribution of s that would generate those probabilistic envelopes of o exactly yeah well one answer is these are the analytical um ideals for which there are strong analytical guarantees

and then any given computational implementation is going to use different methods to to implement this just procedurally but that doesn't make the notation clear and i i think these are some of the kinds of artifacts hopefully cohort after cohort we can craft like a lookup table

we've done this to a limited extent just starting it but having a look up tables for all the different um shapes and squiggles but continuing on so i hope that we can at least see the rest of it um variational free energy f and expected free energy g

variational free energy which we'll recall from equation two two five is a functional of q the distribution we control and y the incoming data so variational free energy is like the real-time unfolding model fit and this is a quantity that's widely used in statistics expected free energy

uses similar structure and and positioning but it explicitly brings in two pieces that aren't present in variational free energy and those are observations which haven't happened yet because we need to talk about how informative we expect observations that haven't occurred yet to be

information gain epistemic value obviously variational free energy is only concerned with the actual data point being acquired at the moment and then secondly the fact that we can take different policy choices and policy choices end up conditioning for example what pragmatic and epistemic value we achieve or expect to achieve so free energy

can have a variational free energy formulation real-time unfolding model adequacy or projecting free energy into the expected future we need to invoke observations that haven't happened yet and our agency in selecting alternative policies

which then of course bear on the epistemic and pragmatic value we accomplish here in four nine there are more decompositions of expected free energy that um help see

previously recognized dialectics or decompositions or trade-offs as actually decompositions of the unified imperative expected free energy so epistemic and pragmatic value decomposition and an ambiguity plus risk and one can kind of walk through and see what those mean or look at the natural language

can rewrite four seven in linear algebraic form here's four seven here is for the categorical distribution we can sum over policies about the policies and the hidden states for time um

more details and calculations of variational free energy factorization and the mean field approximate mean field approximation these are some mathematical technicalities but but um they are simplifying assumptions or ways to approach this in the general case

This factorization is one of many possibilities in variational inference.

And as we talked about last time, variational inference is just one way to do these calculations and represents the simplest option.

And in practice, it's nuanced slightly and there's like other approximations and it's a whole thing.

more information on the letters that we know and love a the tale of two densities b how hidden states change through time c prior beliefs about observations which are preferences and the simple prior on the initial state to kick the hidden state chain off

Little bit of policy and belief updating, few more technical details.

Into active inference in continuous time.

Box 4.1 on Markov blankets.

A Markov blanket for a given variable

With just those words, the map territory question is addressed.

Markov blankets are about variables.

Comprises a subset of those that interact with it.

If we know everything about the subset, knowledge of anything outside the subset does not increase our knowledge of the variable of interest.

There are several

Bayesian message passing schemes expounded on a lot further in other work with Friston, Parr, de Vries et al.

Probably not the most relevant to delve into right now, but the important takeaway is that by using a Bayes graph, so by making a commitment to the figure 4.3 Lego set,

If we choose to have nodes be random variables and edges reflect relationships amongst variables in a certain way, then we gain access to message passing techniques that make even large Bayesian graphs with different parts of the graph updating at different speeds and so on.

There are software packages

that are able to, in a defined computational complexity setting, run inference on those Bayes graphs.

So yes, bigger models, all things being equal, take more time and more computational resources to run.

However, there are some tremendously scalable, high-performance ways to computationally implement computation on these graphs.

they have interesting analytical representations and they have a growing set of toolkits like reactive message passing dot jl by the bias lab with burt de vries and um his lab this is like really cool

current work that's not even outside of their lab and colleagues not heavily used in even active inference publications but they have incredible recent work and you can see how they're using variational free energy in these advanced settings

pulling back to consider message passing qualitatively in the chapter whereas it was introduced in this Markov blanket box um message passing though I'd be totally looking oh thank you I will yeah uh there's the bias lab site and I'll add it to the figure four

the reactive message passing is like one of their new pieces where let's say you had two sensors and one of them was being updated every second and one was being updated every hour.

Well, if you only could turn the clock, you might get into a situation where you were either say, well, should I update my model every second and then oversample the slow variable?

Or should I sample my model every hour and under sample the fast variable?

And reactive message passing

navigates that by actually having nodes update on demand so you can have one node that's just getting pinged every second and another node that's getting updated on a different time scale and the computations on demand so that allows you to have like lifelong online learning for base graphs flexibly with different rates of sensors

And so those are the kinds of toolkits.

These models are not just being proposed to promote theoretical unity in the cybernetic sciences.

Also, there are powerful software toolkits that using these kinds of architectures enable.

So epistemic value, pragmatic value.

Message passing schemes.

If anyone has any takeaways from this image, please feel free because certain things are like omitted.

And also it uses like a lot of squiggles and E's and different variables.

But here we have a representation of a hierarchical model.

So you can tell it's a hierarchical model because you have like I and you have this kind of cluster of four and you have I plus one.

and you have errors getting passed up and hidden state inferences getting passed down.

And so this would be like the sort of timeless representation of that modeling hierarchy, hierarchical Bayes, hierarchical predictive processing architecture, tilde through time.

So we're just describing timelessly, here is the hierarchical Bayesian model.

this can be rendered or unpacked through time now we see that tau minus one tau tau plus one as a function of time so instead of just seeing it as like o tilde like a sequence and seeing this as like a skyscraper this can be unrolled in a procedural way using message passing techniques

Kate, are the rules for message passing on a convoluted Bayesian network known?

Yes.

This paper that I have just put in the chat, here we see figure 4.3, discrete time.

Again and again.

and here is a alternative representation of this Bayes graph with what is known as a Forney factor graph and so in this paper they show that any Bayes graph can be represented as a Forney factor graph and any Forney factor graph has a defined procedural message passing

algorithm so actually a quite incredible synthesis even in this 2017 paper because it means if you if you can build with these Legos with this semantically interpretable setting

then there will always be a procedural technique to calculate it no matter how convoluted this graph is which is very important because you could make um you could just add linear model layers but then you would start to lose analytical guarantees very quickly

here now we're in the continuous time so section 4.5 is a lot like 4.4 except it focuses more on the continuous time model just to only describe more on 4.15 here we have our observations through time and that's a function g little g not expected free energy

Observations are being emitted by a function of hidden states and slowly varying causes, V, which kind of plays the role of policy in continuous time, and a noise term.

So this is our sensor reading.

Here's EEG measurements, SPM measurements, as a function of neural activity and sensor noise.

And then the rate of change of neural activity is a function of neural activity at that time and slowly varying causes.

So the mapping between data and hidden states, which is kind of analogous to the A matrix, is timeless.

And then the rate of change of the hidden states, x dot, which is like x prime,

is defined in a certain way and this is like familiar structurally to those who have looked at physics generalized coordinates of motion related to the taylor series approximation so the the black line is the function true underlying function here's our first pass approximation at this time evaluate the function and then just that's you know y equals four is this line

Then we have the second term approximation, which is the value four, I'm just making that up, and then the slope instantaneously.

The third order approximation is going to have three coordinates of motion, the position, the velocity, slope, and the acceleration.

And then if you were to have another term,

taking it into another level of exponentiation, you would end up like kind of probably coming back out.

So that's how smooth approximation of functions works using the Taylor series by evaluating higher derivatives around the point of focus.

What is the difference between the dot and the dash?

yeah the x so in principle nothing in principle they both mean rate of change here x dot and dash are being used because x dot is the change function on hidden states neural activity changing through time is x dot

so here is our estimate of neural activity changing through time here's the rate of change of neural activity changing through time x dot prime but that is not necessarily x double dot I hope that that's accurate Jonathan


SPEAKER_01:
Yes, I think one of the things that confuse me most here is that we've got an f prime of x prime v prime.

So certainly in figure 4.5, the prime really does represent a derivative with respect to tau.

And then within the equation here, it's just not quite clear what one is taking derivatives with respect to and how many times.

Like f double prime of x double prime v double prime.


SPEAKER_00:
One does not need these two.


SPEAKER_01:
Well, that's what I'm... So to calculate the Taylor series here, you would want to include derivatives, but it's derivatives with respect to the variables inside, like with respect to x. Yeah.

Yeah, I'm just to be confused by this.


SPEAKER_00:
No, that totally makes sense.

I...

agree anywhere where there's a derivative or an integral there should be clarification of what it's with respect to and that's sometimes done but here prime is being used just to refer to the generalized coordinate of motion at that scale okay then

writing down the variational free energy with generalized coordinates laplace approximation this is something where there's a lot of technical detail but what the laplace approximation does broadly also please correct me is finds the single maximum likelihood estimate for a distribution

and then parameterizes a negative quadratic with the point being at the maximum likelihood estimate so for distributions with a central tendency laplace approximation can do really well for distributions that are bimodal like with two humps laplace approximation is going to identify the slightly taller hump

in its maximum a posteriori estimator, and then either draw a pathologically narrow or wide quadratic around the taller hump.

Ali?

Oh, okay.

And more details on that.

Quadratic expansion around the posterior mode.

Mode, most likely point of a distribution.

Quadratic expansion, drawing an upside-down parabola around that point.

expanding some of those discussions from the above into hierarchical models re viewing the message passing that we saw previously skyscraper on the left unrolled through time on the right in the context of generalized predictive coding schemes

and a short summary so definitely an interesting chapter the mathematical details beyond way back when basically beyond this point really escalate fast

and there's also several boxes and asides like this message passing generalized coordinates motion and Laplace approximation all of which are significant technical concepts so a lot of challenges and trade-offs with writing such a chapter especially because this is like active inference

Chapter one, overview.

Chapter two, low road.

This is how we're going to do it.

Chapter three, high road.

This is why we're going to do it.

Chapter four, generative models of active inference.

Figure 4.3, discrete and continuous time, generative models.

And then there's a lot of technical details.

So anyone have any just overall thoughts or questions on four?

Jonathan, and then anyone else.


SPEAKER_01:
Yeah, I think just to get more of a sense of it, I'd still be interested to know more about the tau in G, if anyone has any insight into that.

When you calculate G, yeah, does G still have a tau index, or is something else happening to it?


SPEAKER_02:
yes good good that is noted if anyone has a link or thought go for it otherwise that is definitely something that we will need to revisit and improve for for future cohorts kate um just a very general comment like this was way over my head overall but i was just wondering if in your experience um the the textbook that you are kind of working through with um i mispronounce his name joshi


SPEAKER_00:
about the fundamentals of active inference do you think there will be some help in any of these areas Ali what's your what's your first thought on that this is in reference to Sanjeev nem Joshi's fundamentals textbook that we are um like working through and evaluating during 2023 so Ali what would you say to Kate's question uh yeah in my opinion I think it is way way more helpful in understanding the


SPEAKER_03:
mathematical details and also, in fact, the conceptual intricacies that have been somehow glossed over in this textbook.

But on the other hand, it is estimated to be about 600 to 900 pages long.

So yeah, it is much more expanded than this textbook and it goes into

into a very helpful detail expanding upon these mathematical derivations.

But one point I wanted to make is that

Sanjeev's textbook is not strictly geared toward somewhat mathematical derivation of these equations, but its main objective is

to use it as a practical tool in order to model these kinds of agents.

And in fact, he also provides some helpful Jupyter notebooks in that regard.

But yes, I think it can be really helpful in bridging the gap between this textbook and the practical side of doing things.


SPEAKER_00:
Yeah, thanks, Ali.

So the book is not available yet.

It's not even finished in writing.

If anybody wants to be added to view chapters and the code, which are all in Python, which also increases the accessibility relative to the MATLAB of this part at all, these chapters in a way that...

has not appeared in our space before are tackling that these statistical questions so it is still like very much an accelerated statistical learning curve but as Ali mentioned it's less focused on like well here's how you get from here to here as just like a as a analytical derivation consideration and it's a lot more like

going to go step by step with building stochastic simulations and um so this is gonna it's gonna be a great compliment and and fields have many textbooks so there's not just one right approach um but it's been really encouraging to see his work so if anybody wants like even just access to the coda to review the chapters um please feel free to email us and hopefully

in the coming year we'll have more details on when it'll be available um let us look ahead quickly to chapter five chapter five is the last chapter in the first half of the textbook which is the epistemic half of the textbook so congratulations to those who have continued on especially through that second half of chapter four which is definitely like

intense to a comical degree um and so carefree and whimsical in its writing style which which um has its own kind of comedy but we're gonna have two weeks on chapter five

which is gonna be a great return to reality.

And then the final meeting will be on feedback project ideas.

Terry, I think we need to approach to the maths pitched at a seven-year-old.

Yeah, many angles on that.

One is in the math learning group, we've explored these kinds of questions.

And another approach is a non-mathematics approach, which several people have investigated.

important will it be if we didn't grasp all the details of chapter four when we get to chapter five irrelevant chapter five picks up on a very different note remember that chapter four they even said you could skip said if you don't want to know the technical details you can skip chapter four which was again like it was like wait but we just took the low road and the high road to our city and then they're like you don't have to go on the tour if you don't want to

chapter five message passing and neurobiology Bayesian message passing comes back so again that box that we looked at earlier the idea that every Bayes graph is sparse not everything depends on everything because of the sparsity of the Bayes graph especially

There are some computational procedures that are extremely effective.

And interestingly, real biological systems also have sparse connectivity, whether it's the neural architecture in the brain or whether it's the interaction patterns in an ant colony.

In distributed systems, connections are sparse.

They begin at the micro-anatomical level with looking at neurons and their connections.

They focus on one of the most well-studied neural tissues, which is the mammalian cortex.

And the mammalian cortex has what's called a columnar architecture, which is these six histologically distinct layers of tissue.

So they look different under a microscope.

and the connectivity patterns in terms of what actually sends axons where are also essentially completely characterized and so in here we see the um tantalizingly realist alignment of the histological columnar micro architecture with bayesian graphs that have similar structure

in figure 5.2 they're going to generalize and kind of coarse grain the columnar architecture and now look at how different columns communicate laterally so we have like

hierarchical communication within a columnar unit and connections across units.

This is kind of like Numenta, thousand-brained hypothesis, like a lot of work that is using the neocortical architecture in search of intelligent machines.

So that's the neocortex scenario.

We're going to switch systems to the mammalian motor cord.

Here we have incoming proprioceptive data, Y, and descending predictions.

Those predictions and observations are juxtaposed to form an error signal, which gets passed forward.

And so this is a continuous time setting.

And here we can see action as resolving discrepancies between target set points and proprioceptive input.

now we switch to another mammalian neuro anatomical system which is subcortical and the basal ganglia in particular which is a dopaminergic region this region plays a role in policy selection check out the Oliver Sacks book or Robin Williams movie awakenings it's really interesting and funny here we see two different ways that the dopaminergic tone

tilts the balance towards either action selection, which passes habits forward, or that kind of type one indirect policy selection pathway can tip towards a type two in a Kahneman sense, deliberative, free energy, updated policy posterior, but we'll talk more about it.

They talk about different neurotransmitters in Act-Inf

They talk about how continuous and discrete models can come together.

And then they end the chapter with bringing together those three systems that were previously described, the columnar architecture in the neocortex, the basal ganglia dopaminergic decision-making architectures, and the spinal cord reflex proprioceptive continuous time model.

And they show how those kinds of models can be integrated to address some of our favorite topics.

So it's a shorter chapter.

It doesn't have equations like the previous ones do.

So again, congratulations to everybody who made it through the more

Think of it, chapter four and chapter five, we're going to have a lot of space for conversations about like neurophysiology and the body.

And it's just a good close to the first half of the textbook.

So thank you all for joining.

See you next week.
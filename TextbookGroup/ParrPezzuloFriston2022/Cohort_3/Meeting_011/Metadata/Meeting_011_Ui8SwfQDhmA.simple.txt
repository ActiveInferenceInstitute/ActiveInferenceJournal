SPEAKER_00:
hello it is uh april 5th 2023 and we're in our second discussion of chapter five in cohort three so this is going to be our final recorded session for the first half of the textbook chapters one through five and then

next week we'll have an unrecorded session we will have time to to fill out the feedback form for those who haven't yet we'll discuss project ideas which people can also add ideas to we'll just talk a little bit more broadly and informally about the textbook and about the textbook group and then we will have a a

inter-semester break or it's not a semester but a intermission and we'll pick up when cohort four begins we'll pick up with the second half of cohort three going through just as previous cohorts did chapter six seven eight nine ten and so on so we're in our second discussion of chapter five

head over there now and we'll begin just with anybody in the chat or via raising their hand or just unmuting how does chapter five sit with you what are any thoughts or questions about chapter five that you want to just raise at the outset

Jonathan?


SPEAKER_01:
Yeah, I sort of feel that while in some sense it says that you don't need to have digested chapter four to understand chapter five, it still feels like there's a lot of background there, which is really important.

And the lack of technical clarity from chapter four, I sort of feel there's a bit of a hangover from that into chapter five.


SPEAKER_00:
the technical debt hangover yep yes okay thank you maria i was a bit frustrated with this chapter because i thought it would be more insightful but it seemed to me it was just more the same bunch of equations narrated and plotted

Yes.

I guess the apologist's view would be chapter four was equations and plots in general, and chapter five, in contrast, is applying it to the specific mammalian nervous system examples and to empirical studies.

However, broadly, I totally agree with that.

Any other just

thoughts or primary reflections on chapter five any aspect of it or some part of the discussion or even any other active inference learnings over the last week or two that come to your attention when you're in this setting

so again raise your hand or write in the chat Terry wrote I was struck by the similarity to neural networks you could describe a little more if you want to unmute or we can see how that might come into play

In the previous chapter 10 discussion from cohort three, we were playing with the idea of reading chapter 10 first to provide a lot of the conceptual scope and the historical connections and similarities and dissimilarities of active inference with other topics.

And so it's interesting to note, neural network is described in the citations and twice in chapter 10 in this paragraph.

there's a lot of context that is kind of held out till chapter 10. but the intuitions that are you're having along the way are um very good any other just general thought or a question on five

Otherwise, we will look to the questions and address those which we haven't addressed previously and then continue on through.

Okay.

So continue on this neural network just while we're here.

Let's use the figure 5.5.

So here we have 5.5, the cortical system, the limbic or the midbrain system associated with policy selection and the spinal motor execution arc using this differential between perceptions and set points.

and so then terry you wrote the series of nodes passing information along a chain with inhibitory mechanisms alongside yet neural networks use transformers while active inference uses free energy well great topic to go deeper into the relationship between neural networks

and free energy principle and active inference models in live stream 51 i'm just going to the slides now the um core claim is that here on the right we see the markov blanket and the internal and the external states sense coming in perception

action selection resulting in embodied action and an effect upon the generative process the niche which then provides observations so this is how we've been looking at it in the textbook on the left side we see in a three layer neural network an input layer one or more

compute or intermediate layers which can have a wide range of connectivities and then some output layer so this could be the image comes in and the classifier label comes out or the classifier label comes in and the image generated comes out and then it acts in an environment which then provides another observation and in this paper

they demonstrate that the variational free energy on this partition is equivalent to the loss function on this neural network.

So the training of this neural network is monotonically related to the variational free energy loss on this model learning.

loss function is the function that's being minimized to update this neural network and so it's really quite a stunning conclusion because the topologies so how the pieces are connected of these two models is entirely different so in live stream 51 there's more detail on that um

Neural networks use transformers while active inference uses free energy.

And Jonathan asked, transformers in the post-2017 technical sense?

So in the sort of attention is all you need, pre-trained transformer timeline that we're on?

That's a good question.

And that feels specifically like a reinforcement learning framework, or is it more general?


SPEAKER_01:
So I meant there in terms of the diagram you were giving where there was a neural network and a sort of feedback on an environment.

Yeah, this one here, which, yeah, it looks like a sort of RL framework, but are you saying this is sort of more general than that?


SPEAKER_00:
It is more general as I understood the paper describing that for the observation sequence 1 to t,

and the parameterization all the weights of the neural network and then analogously the variational free energy of those very same observations and the parameterization of the base graph these two are monotonically related within certain classes quite broad classes yes it was i i mean it's

i believe a very major recent finding because it suggests that we can have this like this on one hand it looks like a circuit board and it's it's a lot to look at however there is a semantic interpretability here we see prior

Hidden state B, that's how things change through time.

Hidden state, changing through time, hidden state.

So there's hidden states changing through time.

Here's the A matrix, emitting observations, or you could recognize observations.

Decision-making, preferences guiding decision-making.

So it's like there is an interpretability to this Bayes graph.

And so the idea that a neural network

which can have an arbitrary topology with absolutely no semantic interpretability, kind of like any given neuron in a real neural system.

I mean, Jennifer Aniston neuron aside, no neuron plays an explicit semantic role, let alone a one-to-one semantic role.

So the idea that this kind of a aggregate system

would have a representation where the variables are semantic is quite a claim.

And then the notion that the training function for both of them are monotonically related is really important.

So not to go too far afield from the textbook, but just neural networks were brought in.

And that's an example of some very recent research that makes that connection go beyond intuition.

Okay, here's a question that I don't think that we addressed previously, but if anyone else has any other question, please raise your hand or write in the chat.

So table 5.1.

gives remarkable association between neurotransmitters and precisions should we be suspicious that this seems so clean or is this evidence that the framework is a good match for real brain functioning what what does anyone think or what was their prior suspicion before seeing the table and then what was their posterior suspicion after they saw the table


SPEAKER_01:
Jonathan, and then anyone else.

Yeah, so this was my question.

I think I was just surprised that there was sort of this apparent one-to-one correspondence, or at least near one-to-one correspondence.

So I think I would have expected different neurotransmitters to have a more, I guess, just there not to be this very clean relationship.

That was my prior


SPEAKER_00:
certainly these molecules do um things that are not only listed here like serotonin the word means blood pressure serotonin was discovered in the blood as was dopamine and it has a variety of effects on immune cells

and then later was understood to play a role in the central nervous system.

So I don't think that these evidence provided are suggesting that the exclusive function of these neurotransmitter type molecules is that they're exclusively and exhaustively described by these evidences.

However, these do provide empirical patterns of everything from histology to association with cognitive functions, neuroimaging studies, gene expression analysis, pharmacological manipulation, like the whole molecular neuroscience toolkit.

These are quite disparate phenomena.

And they're disparate phenomena that on one hand have been linked mechanistically to a given molecule and associated receptors and all of this.

But on the other hand, these phenomena are conceptually linked to certain, to the semantics of certain parameters in these statistical models.

So maps are not territories.

It's not saying that it's exhaustively described.

However,

if we use this association as a heuristic if not a hypothesis and we find that we have unique explanatory and predictive powers then by the standards of science it was a valid modeling approach these are explanations and predictions

that would not be achievable without mapping a molecule to the semantics of a parameter like just searching acetylcholine on a literature search engine you'll find a whole variety of different studies different study types different outcomes different species and so on

So that's the scruffy.

The scruffy is what the search engine just gives you.

Each paper is unique, every experiment is unique.

And then the neater than neat would be like all neurotransmitters do one thing.

And so somewhere that is useful enough to help us make explanations and predictions

but not so terminally differentiated that we can't link together novel strands of evidence into explanations and predictions somewhere bridging or respecting the neat and scruffy articulation is active inference and the ways that we can use statistical models to connect to these broad empirical results on different chemicals so

Should we be suspicious that it's so clean?

Well, yes, they're presenting their arguments like a prosecutor.

They're not necessarily going into every caveat or bringing up studies that might disconfirm that.

So for sure, it's connecting dots and not all the dots are being shown and not all the connections are being shown.

But it moves the discussion forward

about how aspects of maps which are to say cognitive models are related to aspects of territory which include the actual materiality of the neural system michael and then anyone else hello daniel um sorry i've been um


SPEAKER_04:
out of pocket for a week or so, but I'm trying to catch up and the way I'm trying to make sense of what I am now landing into, I wanted to kind of validate, does this make sense or not make sense?

If one were to think, and the way I've been trying to, the mental models that I've been trying to map this conversation to are about social conversations and when you could say one level of activity up from, if you will, the biological biochemistry

And I was thinking if these were verbs, you know, sorry, if each of these neurotransmitters were nouns, verbs, adjectives, and that sentences were being created, you know, they could be created many different ways, but they're units of composition, if you will, of message signaling that are creating larger sentences at that level of the system.

Would that metaphor make sense or would it be missing?

How might it be a mismatch for what is happening?


SPEAKER_00:
Great question.

Does anyone have a thought on that?

Potentially referencing figure 5.5 or any other area.

But any thought on this question?

i'll give a first thought then please anyone can build on it so um what is speech really it's this continuous flow of like phonemes and and so it is not subject object verb in how it actually is like the materiality of speech is vibration in the air and all of that and the embodiment and so that's like the territory of speech but let's just say that we're putting on our linguist hat

So now we're gonna think about certain parts of speech as like subject, object, verb, or noun and adverb and so on, even though the speaker may have not thought about it that way.

So we're making maps of sentences as linguists, even if the speaker didn't use that map when they generated the speech.

Let's think about what sentence

does this whole figure represent um and uh beliefs about this and beliefs about that lead to some differential contrasts in light of incoming evidence and the result of that is now we're going to leave that little clause

And the result of that contrast in light of incoming evidence is a decision-making process about what to do given precision on the decision-making process.

And one of the outcomes of that decision-making process is a descending prediction

that contrasts with incoming sensory data and results in an action hopefully it's not too mixed of a metaphor but absolutely these different compositions just like infinite grammar you can keep on adding yes and also in the social space

You can keep on adding sentences to each other and make longer paragraphs.

You can keep on making longer sentences or lists.

Similarly, you can keep on making larger generative models or different generative models.

And so different connections amongst these units are going to be described kind of like by the genre.

Like that was a haiku,

that was a sentence with no noun that was a question that was a directive so loosely it's a great working model to think about this as a graphically arranged sentence that's expressing something where some of the nodes

Some of the tokens are associated with more belief-like states.

Some of the nodes are associated with observations.

Some of the nodes are associated with capacities for action and so on.

And so we can kind of like play with Legos, play with language and compose like, well, what if there were these three sensory modalities and two action possibilities and decision-making was done this way?

That's a generative model.

Cool, yeah.

And then however that sentence is diagrammed, it simply wouldn't be the actual speech itself.

It wouldn't be the vibration of the voice box or the air.

So that's like map territory.

But once we're in the map, then sentences might have this kind of structure.

Or this could be the dependency structure of an organization or the dependency structure of claims in a scientific corpus.

That's kind of what Bayes graphs do.

They describe different variables and their associations.

And that also speaks to their flexibility.

Like you could know all of these and solve for that one, or you could just have these ones and try to reduce your uncertainty about those.

So that'd be like a sentence where you did hear the first part, but not the second part or vice versa.

you dan again every time i during these conversations my mind gets mental models get blown open and uh thank you again good well hopefully they're not um um reduced to rob wool because your your intuition was was um right about taking a linguistic approach i mean who's to say it's right it certainly is compatible and um these this is like a composition

This is a setting where we can compose generative models.

And so that's why sometimes it's like a little challenging.

Like, well, what's the active inference answer on attention?

What's the answer on memory?

It's like, those are complex cognitive phenomena that play out differently in different systems.

So if the framework were rigid enough to give an answer to that in general, it wouldn't have the latitude that we need to actually describe that phenomena.

And so we want to have the latitude to describe different phenomena, including unnamed phenomena, in systems we haven't seen yet, which means that the framework itself can't be so specific about how certain things play out.


SPEAKER_04:
It's mind-blowing.

It's absolutely mind-blowing.

And it's such a privilege.

I know I say it every time.

It's such a privilege to be in this conversation and witnessing you make sense of it through this complex collective lens when I've been thinking in blocks and these blocks are being, you know, it's like, no, these aren't bricks, they're flows and it works, cause and effect works differently.

And it's just, as I say, mind blowing and daunting, but thank you.


SPEAKER_00:
thank you it's definitely team efforts and we've been through it many times and many people's insights have been folded together um does anyone else in chat or raising their hand want to have any reflection on five or a question or we can um

look over some sections and then in the in the last minutes we can like look ahead to the second part of the book or whatever people prefer is there a neural system

or some embodied behaviors that people want to investigate i'll add in this question okay thank you yana are there better or sorry are there alternative visualizations that better show the interdependencies that's really interesting question um

in a base graph the edges reflect um yeah and I'll add this in thank you in the base graph the edges reflect a certain kind of statistical connection amongst variables but this entire structure it's kind of like um a web

or like a tensegrity sculpture or like a guitar string um in that the relate an edge like let's just say from this s3 to pi3 but yes but then if policy selection ends up doing something in the real world then that is going to influence future proprioceptive inputs so

even edges that aren't shown it's not that they're not associated with each other through the reality of the system but the edges are showing the kind of direct connections again not the anatomical direct connections then it would just be an anatomy textbook but it's showing the direct informational or causal connections um

so how can we visualize the interdependencies there's so many ways we might explore but i really like that as something for us to think more about like one more traditional way um in terms of scientific visualization would be we could plot

s3 on the x-axis and mu on the y-axis and then we could ask whether there was any correlation or mutual information between those two variables so the fact that they're not connected it doesn't mean it's going to be just a blur in fact they might be very strongly associated with each other but through some intermediate nodes

So that would be one way of looking at whether any given nodes, two or three or however many, whether they have associations or interdependencies.

And then I guess another little layer is like the space of the counterfactual.

Like some interdependencies might not be revealed unless something changes in the system.

So that is also a layer where...

Two things might have no correlation and then you remove an edge and then all of a sudden two things that weren't in association now are.

Okay, I don't think that's a complete answer because there's a lot of work with visualization to be done, but that is just kind of some aspect of it.

then just continuing with the honest question that and then to jonathan so so it is like a magnetic coil the electric field is not going along the coil it is a field a lot to think about don't have a specific thought if anybody wants to add something but yes i

like what if these are wires and then there's like fields of influence that aren't just the wires but the connections are are there and they're relevant but then there might be other dependencies that aren't simply mapped out on the wire dependencies um okay Jonathan I'm going to copy your question in

has this been looked at in hypergraph language rather than graphs um in active blockference we we looked briefly at hypergraphs we haven't seen any active hypergraphs but

it's very interesting a graph through time so for example what if um this right now there's a static topology so even if there's dynamic information flowing through this graph you could say that it's a static graph the referent is dynamic it's changing through time but the structure is not changing through time but what if um on even and odd time steps

um this connection existed or not then that kind of a of a um system as far as I understand is um a hypergraph is there some other aspect to hypergraph Jonathan or what would enabling this inactive inference help us do


SPEAKER_01:
So hypergraph sort of allow for more general types of edges.

It sort of allows for edges which specifically go through vertices.

Kind of the easiest example is a plane routing system, airplanes.

where if a single aeroplane goes from airport A to B to C, it's very clear that it's the same aeroplane that's gone between those.

And just by drawing an edge between A and B and B and C, that doesn't give the same information that sort of a single piece of information or a single aeroplane has taken a particular route.

So it just allows for more generalizations of edges than in a regular graph.

I just wonder essentially about the way you can encode the types of information that move through a regular graph and a hypergraph, and one might be able to encode more specific information in a hypergraph.


SPEAKER_00:
Yeah, very interesting.

I'm pretty sure that it hasn't been

written on in act inf um a lot of the papers seem very recent like the one you point to is um 2018 and the others are recent so it's um it'd be an interesting area um one note on this to kind of um speak to the other side where the kind of map

uh is very effective with respect to the territory for better and for worse what if these were computers and these were the literal wires connecting them in that case you could design in silico or cyber physical systems where the edges of the base graph very well map to the causal structure of the system physically

so the the kind of one of the hardest systems and one of the richest systems is like trying to make simple statistical models of the brain but when making just in silico systems so just simple pedagogical models or intuition pumps or in certain systems for for um digital and communications systems

the edges can have a much more clear interpretation.

Because as laid out in the beginning of five, this is kind of, this is the tension and the dream.

What if the actual anatomical connections amongst these cell types

what if those were actually reflective of the structure of a base graph and and you'll note that they're not exactly exactly the same like just here's you know two to three and here we see that's two to three but also it goes from here from from um four to two but we don't see a four to two in the tissue

So, but it's close enough, but clearly not close enough to merit calling this simply layer five.

And then I guess to the electric field, these connections are the neural connections.

What if there are glial connections?

What if there are electric fields influences?

Like what the EEG measures

are some of the electrical phenomena of these cells and actually just a specific kind of electrical phenomena of these cells the EEG outside of the head but those electrical signals are not only outputs like epiphenomenal to neural activity they're also inducing different changes in neural activity as well

So there's also physical edges that aren't shown.

Just, you know, complex systems are complex and they engage in multiple mechanistic ways, which is why it's important to have maps of them.

Ali?


SPEAKER_02:
Yeah, well, actually going back to Jana's question and also to touch on what Jonathan just mentioned.

Well, in fact, when we're talking about interaction, obviously there are many different conceptions of this

term interaction so we can talk about physical interaction which is basically i mean for the most part is undertaken the fields so we can also term it as a kind of field interaction but there are some other conceptions of interaction actually there are many others but probably one of

the most important conceptions for our purposes is information theoretic interaction.

So when we're talking about message passing or this conception of interaction through information,

passing or information theoretic conception of it, we're not necessarily deciding on what exactly is the medium of interaction or transmission, because it can be

abstracted away this medium of transmission or interaction, and we can just look at the mathematical framework or the mathematical structure of those information.

So for instance, say IIT framework or integrated information framework,

in which integrated information theory

it's unable to distinguish a highly interdependent set of components dominated by redundant relations from an equally strongly interacting set of components exhibiting a kind of strong synergy between them.

So this is actually kind of a limitation for IIT for some people because

Since redundancy indicates that although strong interaction is needed to ensure that a component is a kind of repetition of the other components, it does not imply that the set of components support collective emergent behavior, which is entirely different from that of the individual components.

In recent years, by recent I mean since 2020, there's some interesting works in that regard trying to formulate the interdependencies and emergence with mathematical formalism.

So Lionel Barnett, if I remember his name correctly, and also Anil Seth and others, I forgot some of their names,

has done some pretty interesting work in that regard in recent years and one of the interesting things they've came across is that they've tried to apply this abstract conception of interaction and interdependencies from information theoretical a theoretic point of view to neuroimaging data and

so they they've been able to describe this interaction of neural interaction between different brain regions without accounting explicitly accounting for synaptic or i don't know junction interaction or so on so it's much more abstract and much more generalized form of interaction


SPEAKER_00:
awesome yes anatomy and physiology are different anatomy is describing the articulations of the system but then the physiological relationships and correlations and even causes are not necessarily following along the anatomical lines and um

If one wants the anatomical answer, you can just slice the tissue and take the picture.

So something different is happening with this modeling.

And it's important, like you mentioned, that this interaction concept amongst variables, by the time we're labeling variables, we're in the map.

We're not labeling the tissue anymore.

so the only possible kind of interaction that these variables can have is like statistical or information theoretic but then there's this tantalizing dream slash fallacy slash actionable framework all of those that parameters do map on to aspects of the territory and that's the map territory

dialectic some maps useful in some contexts doesn't reify the the architecture of the map and it doesn't prove or disprove any architecture of the system and the interesting thing is that's not even active inference specific these are active inference type thank you ollie

These are active inference or predictive processing like models.

However, any statistical or cognitive modeling approach is going to confront this issue.

And then how do we move beyond just simply saying, well, map territory means we can't drive or we can't do route planning.

it's like well if dopamine is associated with precision in this model where should the deep brain stimulator go or what kind of food would make sense to eat or not eat or and so on so we can actually reduce uncertainty about what actions to take using maps and that's what these are and those maps arguably works best when

it's understood what's a map and what's the territory and understanding what the map is can be fully described and held to the standards of reproducibility whereas holding a territory to those same epistemic norms or criteria it doesn't even make sense the territory can't be fully described but when they're overlaid

even symbolically it can become ambiguous well next week we're going to um not record so if you're going to be there again that's awesome

otherwise please fill out the feedback form in the future textbook group section and check out the project ideas but in these last 10 minutes today is there any sections or questions on chapter five that anybody wants to look at or would people like to see into chapter six


SPEAKER_03:
Um, I've just added those additional comments I was making into the coder and I'm making a quick visualization of what I was thinking about.

I'm not sure if this relates directly to the model.

Um, but just, I'll need a minute to get this done.


SPEAKER_00:
Thank you.

I'll leave the tab up.

We'll return to it.


SPEAKER_03:
Yep.


SPEAKER_00:
any preference for a section of five or to to look ahead to chapter six in the recipe even though we'll return to that in like a month well looking at yes go ahead michael

gonna say no objection here i just wanted somebody to respond okay let's revisit the whole first half of the book which you all have now completed one or more passes through chapter one provided an overview chapters two and three describe the low road and the high road the low road how are we going to do it with bayesian statistics uncontentious why

at the very least, we need repeated measurement.

If something can't be repeatedly measured by us, it's not a thing to us.

And then a little bit more from the inside, if something can't take adaptive action or more adaptive action to persist and repeatedly measure itself, it won't be that kind of thing either.

So whether you view from the outside, repeated measurement, or you view from the inside, adaptive action,

we can model both of those in terms of this self-evidencing primary imperative.

How are we going to model that self-evidencing primary imperative?

Bayesian statistics.

Chapter four, with all of its shakes and bumps,

was describing the generative models of active inference and how variational Bayesian inference is used as a heuristic or an approximation to otherwise intractable Bayesian statistical models.

And we just looked at chapter five, which characterized a few subsystems of the mammalian neuroanatomy

and then linked them together and provided some references to other models that go into a lot more detail computationally and molecularly.

Here's what is going to be coming up in the second half of the book.

That is the application oriented part of the book.

Now, it may not go all the way to the guidebook, playbook, handbook that we all expect and prefer, but it is more pragmatically and application oriented than the first half of the book.

In chapter six, there's a recipe for designing active inference models.

With some here, we've even gone through that recipe, but broadly, they're going to lay out some steps

about systems of interest about specification of the generative model and the generative process and there's a lot of good discussions around even broader considerations like why are we making this model who are we making the model for is this model being used descriptively like historically or is it being used proactively that's the recipe so you want to open up a restaurant here's the active inference recipe

chapter 7 and chapter 8 are like a pair chapter 7 is going to cover generative models that have discrete time characteristics chapter 8 is going to look at active inference in continuous time discrete time active inference on the top of figure 4.3

continuous time active inference generative model on the bottom of figure 4.3 so chapters 7 and 8 are going to basically zoom in on figure 4.3 in sequence chapter 9 model-based data analysis is going to describe some of the framings and challenges and opportunities associated with integrating empirical data with these kinds of generative models

So far, up till then, it's kind of been like, here's the generative model we constructed, and now we're going to simulate its behavior.

So generative model specification, simulation, in silico, generation of synthetic data.

But we're not always just interested in generating synthetic data.

Sometimes we want to take in empirical data from measurements, and that's what this chapter is about.

then chapter 10 is a longer chapter there's no equations or figures and it is a capstone that's describing a lot of the conceptual and historical context and as kind of referenced earlier um may make sense to even read first

that is where we're going in the second half of the cohort so the first half as the author's layout is epistemic we're getting like the roads into the city what the city is about or one restaurant in the city and then a few dishes that have been prepared in this city then we're going to go behind the scenes look at the recipe for designing those models

talk about two subtypes of cuisine the discrete time and the continuous time talk about connecting these generative models to measurements so that people can do analyses and write papers and all of that make things that do things in the world and then having done all of that chapter 10 connects a lot of dots

that's the par at all textbook and for being here you all are at least half done with it and um as also mentioned we go at a similar pace we haven't set the dates yet for the second half of cohort three it'll likely uh pick up again in may when we have cohort four begin with chapter one

And if you have any feedback or ideas, it would be extremely appreciated to fill out this feedback form just directly in Coda.

You can just submit it right there.

as well as to share the link for others to join and convey to them just say hey stick around you will learn it's not xyz or it is xyz but feel free to share that so that we can have really engaged and awesome cohorts such as this one and also any other comments you want to write there or any thoughts on how we can improve it and next week um we're gonna focus all about that we're gonna

not record that session we're going to talk about the project ideas table overall reflections on the first half next steps and about the future of the textbook group um Yana I'll look forward to your image and in the last minute or two anyone else please feel free to add other comments

It's not exactly goodbye, as we will meet again, but this is the last recording for this half of the cohort.


SPEAKER_03:
I'll put the image in Coda in a few minutes, so I'll let you know in Discord.

when it's ready.


SPEAKER_00:
Thank you.

And it just makes me think about like, these Bayes graphs are kind of like, this is the necessary and sufficient information for a statistical model.

But we might tell stories or link associations that don't follow this exact topology.

So this is not like the only web we can draw, but it's a really important one for certain kinds of statistical models.

right so next week same time for um feedback and next steps thank you everybody see you then
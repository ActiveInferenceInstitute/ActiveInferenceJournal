SPEAKER_00:
hello and welcome everyone it's march 1st 2023 we're in cohort three and we are having our first discussion today on chapter three chapter three um is the high road to active inference previously in chapter two we talked about the low road and today we're talking about the high road

Those two roads are laid out in figure 1.2.

We took it from Bayes' theorem to active inference in chapter two, and chapter three is gonna be starting from the free energy principle and also taking us to active inference.

So before we go through the chapter or look at any questions, does anyone wanna just give any reflection or thought

What was some experience they had in reading chapter three or a quote or an idea or something that stood out for them?

You can raise your hand or just go for it.

Okay, so feel free to raise your hand or write in the chat if you ever want to add anything.

Chapter three takes a different tack than chapter two.

anyone want anything or we'll we'll start exploring through the chapter and then coming to the questions that people have raised um today or in the next week okay um we'll move through chapter three hopefully

relatively quickly to get to the first of the questions and then we'll have seeded some questions for coming to our discussion next week so in section 3.1 in chapter 2 free energy was motivated as a bound in a way that we can do approximate bayesian inference

coming from the low road to active inference.

And you can do free energy based or energy based methods like a variational autoencoder.

You could do that on any Bayesian statistical problem at all.

And it's used commonly.

It doesn't necessarily mean that it's connected to action selection at all.

In chapter three, they're going to start from the high road

which is this central imperative that organisms or really just things must maintain their existence which is going to be operationalized in a way that's compatible with the surprise minimization that was brought up in chapter two and just like in chapter two where free energy was proposed as a way to bound surprise which is like what we would really want to know

Coming from the high road, we also want to bound surprise.

And it turns out that minimization of free energy is again going to appear as a computationally tractable solution to the problem.

The chapter is going to describe the formal equivalence between the minimization of variational free energy and maximization of model evidence or self-evidencing in Bayesian inference.

then active inference is going to be brought up because we're going to head into chapter four which is going to be all about the generative models at the heart of active inference active inference is about how organisms or adaptive systems maintain their existence by minimizing surprise via more proximally this tractable proxy variational free energy

And the two ways to minimize variational free energy are perception and action.

Change your mind, change the world.

Integrated imperative that supports this unifying perspective on systems.

High Road is going to start from the premise that any living organism or any persistently observed thing

has to maintain itself in a set of preferred states if it doesn't maintain itself in a set of expected slash preferred states it just isn't that kind of thing that is going to apply to everything from an oil droplet that's diffusing or not to more sophisticated cognitive goals

Humans, physiological entities have to stay within their physiological ranges.

In reward learning, it might be proposed that there's this value function where acceptable ranges are more rewarding and then rewarding paths of action would be selected.

In contrast, in active inference, we establish the expectations around those physiological variables in terms of a preference.

And then we reduce our surprise about realizing those preferences slash expectations.

That allows us to take the most likely course of action to maintain being the kind of thing we are, rather than proposing this reward function and then asking what's the most rewarding outcome.

active inference casts the biological problem of or explanation for survival as surprise minimization surely that will spur many conversations but for now this is not like surprise birthday minimization so this isn't to say that surprise doesn't happen or that cognitive surprise is not even sought after actively in certain cases this is referring to the statistical concept of surprise

which is defined for a very specific generative model.

So not just surprise in general, but for a given generative model, at whatever level it's constructed at, surprise minimization is what leads to the maximization of model evidence.

Because if you can minimize surprise or the tractable proxy of variational free energy, your model will basically be as fit as it can be.

Again, any thoughts or just raise your hand and we can take it anyway.

3.2, Markov blankets.

Does anyone want to just give a preemptive thought on Markov blankets or anything else?

Jonathan, go for it.


SPEAKER_02:
Sorry, it's actually with respect to the surprise.

And this is sort of with respect to some preference distribution over the status of the agent.

Presumably, does the agent have to have any awareness of that preference distribution itself?

And can the preference distribution itself be something which is inferred?

You know, we may infer something about our preferred states.

You know, is there a sort of hierarchy to that as well?


SPEAKER_00:
Nice question.

So does anyone want to give a first thought?

Does the agent need awareness of preference?

And can preferences be learned?

Okay, just first passes.

Ali, go for it.


SPEAKER_06:
Actually, I believe it can be modeled in a kind of nested hierarchical way so that the agent can be aware of its own preferences and also preferences is not static at all and they can be dynamic as well and can be learned and updated.


SPEAKER_00:
Great, exactly.

If the model does have a higher nested level that enables the interpretation of awareness on preferences, sure.

In the base model, no.

No awareness, attention, phenomenology, qualia is ever implied at the basal level.

So it doesn't have to be, but you can make a model that has that feature.

Can preferences be learned?

again in the most simple case no the c vector is fixed however there have been several works by sajid at all and others on preference learning will


SPEAKER_05:
Oh, I feel like you said most of what I was just about to say, just that, you know, thinking at the level of a bacterium, you know, awareness is a tricky concept, but clearly there is an embedding of these preferences in, you know, some information at whatever level we're talking about.

And yeah, talking about learning preferences.

know perhaps at the highest level those don't really change but if you're talking about lower level preferences those are kind of adapted to to the individual time span like the bacteria may not kind of change its behavior overall on on the on a lifespan but you know given an individual couple of minutes it's going to say oh I prefer to go left versus right because of the chemical gradient


SPEAKER_00:
great yes so a preference slash expectation for being in the run versus the tumble mode of movement is happening faster and might be learnable or updatable whereas a slower updating process of for example whether sugar is preferred at all might be happening at a different time scale um ever and Francois yes I'm also um


SPEAKER_08:
Thinking about Michael Levin's work, you had him on one of your former episodes where he's talking about basal cognition and that there's maybe even something like an awareness of preferences at the very, very lowest level.


SPEAKER_00:
Cool.

Thank you, Francois.

And then Jonathan.


SPEAKER_07:
The example that

that Will said that lower level preferences could change, like moving left or right for the bacteria.

How do we know if that's a preference or not just like it's reevaluating different policies that would lead to different expected free energies?


SPEAKER_00:
It's a great question.

if we don't define the system like it's not a system that we created then it would be an empirical question whether the data support a model in which you have a fixed preference with um oscillating policy selection based upon expected free energy calculations

or whether there are behavior that seems to go beyond that to actually capture a plastic sea so it's an empirical question from a given data set and from a given generative model

because we can easily imagine and just speak out both of those examples and so it's like a portfolio of models and then we would do model selection or evaluation amongst those alternative strategies um Jonathan and then Everett


SPEAKER_02:
Yeah, I mean, is there some way of thinking about it that there's always going to be a hierarchy, that there is simply a preference for survival within any model like that, that the agent is trying to retain its thingness and from that maybe glucose levels and temperature and whatever else it may be.

But each of those is a preference in sort of


SPEAKER_00:
in order to have this macro preference which is simply survival that's a great way to put it over intra and especially intergenerational time scales you're only going to ever see things that at least act that way or appear that way

That's kind of like the good regulator theorem from cybernetics.

So yeah, at some point, it can be said that there's this implicit pervasive preference for persistence.

Otherwise, you just wouldn't be even observing that system to be any way at all.

Everett?

yeah would you say there's a difference between preferences and goal directed behavior it's a great question in fact goal is not used in the active inference ontology we can definitely talk informally about like

Well, the preference is to be on square three comma three in the grid world.

So just loosely speaking, its goal is to make it to three comma three.

But we don't need to appeal to goals as a formal construct nor reward.

So conversationally, it's totally fair to talk about agentic or goal-directed behavior

the way that that's being operationalized in active inference is through this preference vector.

thanks Scott always might be off there are presumably bad regulators which maintain themselves by luck and bad regulators which will lose their homeostasis imminently yes totally agreed with that luck is only going to go so far and so long and the fact that the imperative is for adaptive regulation doesn't mean there aren't systems that fail ever


SPEAKER_08:
This is probably also related to what we discussed before, the as-if talk, that an agent looks as if it's minimizing this or that, or it looks as if it's pursuing a certain goal.

Would you say that's right?


SPEAKER_00:
Yes, as-if, or perhaps a synonym for as-if, could be modeled as, or I could publish a paper modeling it as.

as a very um uh defined way of talking about as if i can imagine it or it could be written that or one could model that and that puts us more squarely in the instrumentalist perspective or just the imaginative perspective so awesome comments

great so from surprise with respect oh um ali go for it uh yeah i just uh wanted to point out uh perhaps we can


SPEAKER_06:
talk about goal-directed motion as a kind of emergent phenomena that arises from this energy minimization mechanism.

So it's not by itself an objective parameter that we need to describe or model.

It's just it can be seen or described as an emergent phenomena of that mechanism.


SPEAKER_00:
Awesome.

And I just, Everett, go for it.

Sorry, I just had to like reshare my screen.


SPEAKER_08:
Oh, sorry.

I had to hold on my hand.


SPEAKER_00:
All right.

Okay.

Okay.

So continuing on with section 3.2.

Markov blankets.

Big topic in the field.

The way that things are going to be modeled or framed is in terms of their relative insulation from the environment.

The generative process, the environment, and the relative insulation with the generative model or the agent, the particular states.

the absence of this separation there would be no surprise to minimize there must be something to be surprised and something to be surprised about unity is plural at minimum two in other words there are at least two things system and environment and these can be disambiguated from one another these can be modeled as being disambiguated from one another a formal way to express that separation is the statistical construct of a markov blanket

which can be traced to the work of Judea Pearl on Bayesian graphs and causal inference.

Here is box 3.1 with a technical definition of a Markov blanket.

A Markov blanket, in brief, they write, is a set of variables that mediate all statistical interactions between a system and its environment.

So Markov blankets are nodes, one or more nodes, on a Bayesian graph for which two other sets of nodes are conditionally independent.

So if person A says something to person B and B says something to C, it's like person B is the blanket between A and C. So it isn't that internal, external states can't influence each other, but rather it's that their engagements with each other are mediated through the blanket.

And just earlier, it's in chapter nine.

We had a really great discussion about the map territory fallacy and the map territory fallacy fallacy.

And for now, just to sort of see this discussion and continue the movement.

markov blankets exist for generative models because markov blankets are a feature of bayesian graphs so the world the territory does not have markov blankets however maps can have markov blankets and a huge amount of well-intended

and off-key discussion has lost touch with that map territory distinction markov blankets are about maps because we're talking about partition states on a base graph the claim is not that the real world consists of labeled things that have internal external sense and action on them

And then just one more point, and again, feel free to raise your hand or anything.

In the discussions of Perl, the Markov blanket is this insulating set of nodes.

What Friston and collaborators have done in the previous two decades or so is further partition

that undirected markov blanket into two types of nodes nodes with incoming dependencies which are interpreted as sensory states and nodes with outgoing dependencies which are active states that licensed the use of the markov blanket to describe the statistical boundaries of an adaptive or cybernetic agent so the markov blanket concept itself

is undirected it just describes the insulating set of nodes friston at all operationalized it further to bring it into the setting of cognitive modeling so now we revisit that particular partition

that previously was shown as a feedback loop with perception and cognition and action.

And now it's gonna be shown in a way that's gonna be taking us closer to the Bayesian mechanics or the particular partition, which is we have the blanket, which is shown here according to the Friston blanket approach, active states with outgoing statistical dependencies, sensory states with incoming statistical dependencies,

internal states don't have a direct edge to external states so there's no telekinesis and there's no telepathy however internal states can select actions that modify how the external world changes that influences which sensory states are observed which is interpreted through perception and learning by internal states

So consequences percolate throughout the entire partition, but there's no direct connections between external and internal states.

Ali?


SPEAKER_06:
Yeah, I just wanted to quickly point out about two issues here, one of which is, I think, an important error in this figure, because, well, we know that the flow of active states wouldn't

depend on the external states and similarly the flow of the internal sensory states wouldn't will not depend on the internal state so in the equations in that figure x and y should be interchanged yeah that's it and the other one is of course to emphasize my favorite definition of markup blanket which is the

It's a kind of boundary or interface in state space, so it's not necessarily a spatiotemporal boundary, although in some cases it might manifest itself as a spatiotemporal boundary, such as the membranes of cells or organelles or whatever.


SPEAKER_00:
Great way to say it.

The Markov blanket is the boundary in the statistical state space.

Okay, so I'm just gonna add some of these questions.

Jonathan, I'm just adding it to the chapter three questions and we'll come to it.

I'm just having this weird thing with like freezing of my screen.

And then Everett,


SPEAKER_08:
Yeah, I just posted the link in the chat.

I don't know if it's possible to open the page.

Yep.

Because there's a picture a little bit down.

I think that's also a Markov blanket that's shown there, but there are more arrows than in the one that you just showed in the textbook.

A little bit lower.

yeah so if you and i wonder yeah so i wonder no it's not that one okay which page oh i have a look it's on page um let's figure one okay i'm gonna have to reshare again sorry


SPEAKER_00:
I hope that this is a little bit more stable.

We'll just...

a few possible comments any base graph is going to have a multitude of markov blankets markov blankets are not uniquely identifying like agent environment boundaries but those are some boundaries in state space that we're interested in but every base graph

that isn't fully connected.

So if there's any nodes that aren't connected to other nodes, they will have a Markov blanket.

So the concept is actually less philosophically and even semantically loaded than people often expect.

Any base graph that you just draw on a piece of paper, if there's some node that intermediates two other sets of nodes, it's a Markov blanket.

It does not identify cybernetic system boundaries.

with respect to the cybernetic system boundary though which is one kind of base graph that we care about um the work of Aguilera at all how particular is the free energy principle um has explored different topologies of the particular partition

For example, like what if you do or don't have this edge between active and sensory states?

Why is there an arrow going from action back to internal?

Why is there an arrow going from sense back to external?

The first answer on this is that this is a statistical model.

So just because you're enabling that edge doesn't mean that the data are gonna support that edge having a non-zero parameterization.

So just describing the possibilities that are going to get parameterized.

So it's not a claim about the real world, hashtag map territory, it's just saying this is the class of models that we're fitting.

So in practice, it means less than it may seem.

However, it's also the case that what constraints we put on this topology do end up influencing greatly what kinds of models we fit.

Michael?


SPEAKER_01:
Yeah, I have a question on page 43 under this box.

It illustrates the interpretation of Markov blanket in a dynamic setting.

And then the sentence here, the conditional independencies have been supplemented with dynamical constraints

so that the flow do not depend on states on the opposite side of the blanket.

So can you elaborate what that means and what this implies?

What are dynamical constraints in contrast to what else?


SPEAKER_00:
The dynamical constraints are the dynamical equations that are provided for the rates of change of each of these.

and again note what um what ali has brought up in and written in the chat um there there's just a slight error with the the variables as written so where it says that u dot so that's like action states it says that it's a function of x u and y however um it should be mu u and y

because this would suggest that active states do depend on external states, when actually this X should be a mu, which is to say that active states are a function of internal states, sensory states, and their own current position.

That doesn't mean that they're all equal contributors.

It's empirical with your dataset and your generative model,

And you can imagine situations where it's only the internal state that matters, or it's only the sensory state that matters.

This is just like the space in which models are fit from.

This isn't the claim about any specific cybernetic system or any interpretation of the real world.

Octopus?


SPEAKER_03:
So x dot indicates that you're dealing with continuous time dynamics, right?

Correct.

These are flows on the variables.

Okay, so I'm having a tough time mapping this.

Is there any disjunction or any difficulty with things that are inherently discrete?

Like, I love cellular automaton.

And any discrete agent based model can be can be put in a cellular automaton framework, but there's no way to really

I mean, I guess you can interpolate it somehow, but it's not a natural way of thinking about it.

So is continuous time essential to this framework, or is it just a convenience?


SPEAKER_00:
Much of the textbook, thank you, Octopus, is about the differences and similarities between discrete time and continuous time formalizations.

So just to give a quick preview, figure 4.3 shows on the top the partially observable Markov decision process discrete time, discrete state space formalization, and the bottom is the continuous time formalization, which is more like a Taylor series approximation using the generalized coordinates of motion.

And then chapter seven focuses on the discrete time generative models.

Chapter eight focuses on the continuous time models.

And in figure 8.6, there's a nested hierarchical model where the lower level is continuous time and the higher level is discrete time.

So generative models can be either discrete or continuous state spaces, time or other variables.

Jonathan wrote, presumably in reality, all these variables are high dimensional vectors.

For sure, it could be as simple as a scalar, a single number, or it could be an n-dimensional tensor.

Thank you.

Yeah, thanks Octopus.

Everett, is your hand raised or anyone else?


SPEAKER_08:
No, I didn't have my hand raised.


SPEAKER_00:
Okay, it was just my Zoom was lagging.

Okay.

Figure 3.2 is going to give an intuition for what it means when we have this simple system with mu, internal, x, external, and b blanket states.

So we can imagine that we're sampling from this cube, X, Y, and Z, but it's mu, B, and X. And then these three panels are X conditioned on B, mu conditioned on B, and then finally the relationship between X and mu.

And so this shows that you could estimate X

conditioned upon an expectation of mu given b, so internal states given blanket states, that's like cognition, about x. So here's my estimate, my posterior probability estimate of x, which I'm not directly observing, conditioned upon my expectations of internal states conditioned on b.

and so you can have a relationship between x and b like there's some correlation between external blanket states some correlation between the temperature in the room and your thermometer and then there's some correlation between your cognitive states and the blanket your beliefs about temperature given the thermometer and then that results

in there being a an information relationship between external and internal states though they are only mediated through the blanket measurements this is just an inference passive setting there's no action coming into play yet it's just to get a bit of an intuition about how uh markov blanket mediated perceptual inference occurs

3.3 surprise minimization and self-evidencing so an agent with markov blanket appears to model the external environment in the sense that internal states correspond on average to a probabilistic representation of external states of the system figure 3.2 that's what that was showing um

importantly the agent's generative model cannot simply mimic external dynamics otherwise the agent would simply follow external dissipative dynamics so passive inference might lead one to want a model where we simply estimate external states as well as we can we just learn optimally and just sit and watch the show

rather the model must also specify the preferred conditions for the agent's existence or the regions of states that the agent has to visit to maintain its existence or satisfy criteria for its existence in terms of occupying characteristic states so if we want to do more than just watch if we want to act

to bound our surprise and improve our persistence in a dissipative, far from equilibrium environment, we need to have some preferences or expectations that actions can be selected based upon how they will realize those preferences.

Francois?


SPEAKER_07:
Yeah, sorry, I think we kind of skipped over something that I didn't really understand or want to ask about.

Is that

The thing about the symmetry across the Markov blanket and the consequence of this being we can associate pairs of expected internal and external states with each other, I really don't understand how that consequence comes from the symmetry across the Markov blanket.

Could you explain it better in a different way?


SPEAKER_00:
Sure, there's multiple angles.

Let's just say, first starting with a homeostatic angle.

Let's just say that temperature really is relevant for survival.

So the color of the room doesn't matter for survival, but the temperature matters.

And we have a thermometer.

So that's our Markov blanket.

And we're going to be getting some proxied information about temperature through our thermometer.

without going too deep into debates about representationalism there must be something in the generative model the agent the cognitive model of the agent that represents that temperature there has to be some mapping between the temperature in the room unobserved and the agent's perception of temperature such that it's able to engage adaptively

Otherwise, it's just going to be a particle adrift.

So it turns out it doesn't need to have an exact structural relationship.

So temperature could be a continuous variable, and the agent's generative model might just be too hot, just right, or cold.

Three states.

So they don't need to have the same structure, but there does have to be a mapping

between decisions that influence vital variables and the external reality of those vital variables.

So that's the homeostatic angle.

And then just shortly, the other symmetry-based angle is the external states don't have to be an active agent, but they could be.

This could be like a communications interface.

in which case it would be fair to say that the generative model performs inference with an aboutness of the generative process, but also the process as agent is doing inference on the aboutness of the agent on the right side.

So that has been more recently explored with like a symmetry with a niche doing inference on the organism also through the same blanket.

Okay, thank you.

That makes more sense.

Great.

So that baking in of preferences or expectations builds in what they call an implicit optimism bias, which is like steering our perception and action towards realizing preferences about observations.

This optimism bias is necessary for the agent to go beyond the mere duplication of external dynamics or resemblance of external dynamics to prescribe active states that underwrite its preferred or characteristic states.

one can cast optimal behavior with respect to prior preferences which again doesn't mean that it always works doesn't mean everything always lives all the time or that outcomes are the best that they can ever possibly be but given prior preferences the maximization of model evidence by perception is a signal processing perspective that's like the optimal reconstruction of some sound

However, we have a unified imperative that isn't just maximizing model evidence for perception.

We have a joint imperative for the maximization of model evidence by perception and action.

We can reduce our surprise, bound our surprise,

with minimizing variational free energy minimizing expected free energy which is this dual or unified imperative that brings together changing your mind through perception learning and changing the world through action um a good fit indicates the model successfully accounts for sensations at the same time it realizes its preferred sensation given that they are less surprising

Good description, good prescription.

Good scientist, good engineer.

Such a good fit is a guarantee of surprise minimization as maximizing model evidence, P , the probability of the data given the model, is mathematically equivalent to minimizing surprise.

Fancy I, negative natural log of the same.

that is why you'll often hear adaptive systems described as self-evidencing self-evidencing means acting to Garner sensory data consistent with an internal model hence maximizing model evidence okay surprise minimization as a Hamiltonian principle of least action

this is starting to um invoke some more physics based approaches on the low road we were squarely within statistics base theorem base theorem with action active inference coming from the free energy principle on the high road we have a lot more physics based views so expressing this as a physicist might dot dot dot

this is like a ball rolling down a hill from a high gravitational potential energy at the top of the hill to low energy in a basin so just like in classical mechanics it might describe a ball rolling down um an elevation landscape with respect to potential energy we're in an information geometric landscape and the ball rolling down the hill taking the path of least action

is going to be formally equivalent or at least formally analogous to our belief distributions rolling down the hill with negative log evidence or surprise playing the role of gravity

that is what is equivalent to potential energy drawing the ball down the hill and if you want a more technical angle check out live stream 52 which will be happening this week and next week it is going to go a lot deeper into this surprise as potential energy interpretation um

here's a path taken by a two-dimensional random dynamical system it's kind of vibrating but it is also spending most of its time in the center here and the left one another way to view like the landscape that this pen was drawing on or that this ball was rolling on is on the right this is like a topo map

where here is like a Plateau and then it's like a bowl so even though it started here it kind of vibrated around but it spends most of its time in the bottom of the bowl in the middle is a trajectory of a system that has Dynamics bearing no relationship to surprise so here you have uncorrelated just random walk brownian diffusion inner particle

more details and equations that connect minimization of entropy which is kind of like the blurriness of a distribution like how broad or sharp it is has a relationship with the maximization of the probability of observations and also

the expectation of self-information.

So ensuring that a small proportion of sensory states are occupied with high probability, like we want a distribution on temperature states that's very sharp.

that is equivalent to bounding surprise about temperature yes and also maintaining a certain entropy on the temperature distribution Maria does anyone know if the random pattern in the middle is less or more common than the centered one Jonathan wrote it depends on the context totally agreed um you can add more details than that Maria

But there are sets of variables in your imagination or in the world for whom their dynamics look like that.

And there's sets that look like that and various other things in between.

Surprise minimization formalizes the idea of homeostasis.

Ali, go for it.


SPEAKER_06:
I think it might be useful to point out Poincare's taxonomy of singular points, because back in the 19th century, he showed that singular points can have only four possible different types, namely nodes.

saddle points, foci, and centers.

But then he went on claiming that among those different types, nodes, saddle points, and foci are much more generic than centers.

So centers

only arises in exceptional circumstances.

In almost all the physical systems that we model, physical dynamical systems, we will most likely encounter only those three kinds of singular points.

And just very briefly, nodes are the points through which an infinite number of solution curves pass.

Saddle points are the points through which only two solution curves pass.

And foci are the points which the solution curves approach in the manner of a logarithmic spiral.

And the centers are the points around which the solution curves are somehow enclosed and enveloping one another.


SPEAKER_00:
great just really quickly to everett's uh question so this sensory active state notice that the notation is totally different so you got to take a fresh look at the notation every time like external states are q prime and so on um so uh m is a blanket m and phi are a blanket between q and e

also e and omega are a blanket between q and m so markov blankets are everywhere they do no philosophical work they're a purely technical description of nodes on a bayesian graph

Maps have Markov blankets, not territories.

It can be somewhat misleading when we have a picture of a brain as if we were describing it, but it's like actually a map of the brain, not the territory itself.

So brains don't have Markov blankets.

Maps do.

Maps are made by people.

Maps are made amongst portfolios of other maps.

And some of those Markov boundaries in phase space can be identified with different functional phenomena, but

Just drawing a base graph is not a prediction or an explanation by itself, and so it doesn't do too much work.

Yet a huge amount of discourse has focused on those types of topics.

Everett?


SPEAKER_08:
Yes, do you agree that that big description of the Markov blanket captures more maybe things that have...

discussed in previous times about preferences and subjective feelings because i know that mark solmes has talked about precision as being related to the things that we consciously perceive as feelings is that captured here in this picture do you see


SPEAKER_00:
preferences are not shown here unless one uses preference as a subtype of internal state mark solmes indeed has discussed extensively precision affect consciousness awareness and so on but in the end it comes down to just the full specification of the generative model what data and what phenomena are being tried to be explained by that model and data together

and just drawing any kind of um diagram is just the beginning of a graphical abstract or a schema it's not an explanation prediction design imperative or control system it's just a drawing all right in our last um five minutes

Thanks, Ellie.

Yeah, the Poincaré topic's really interesting.

So this is our first discussion on chapter three, and we got about halfway through.

We're just going to peep ahead, and then next week we will pick up with the chapter three questions.

Already there's a bunch of them.

So we've been talking about entropy, self-surprisal, model, evidence, maximization, and their relationships.

3.4, relations between or among, depending on how you prefer your English, inference, cognition, and stochastic dynamics.

Table 3.1 is going to juxtapose statistical physics, Bayesian inference and information theory, and a cognitive interpretation where in rows,

there are going to be sort of three faces so across columns these three fields as the authors are showing are talking about something that's basically like the same or resonant with box 3.2 is talking about free energy in statistical physics and active inference

they want to be clear that variational free energy, which is used commonly in machine learning, is not necessarily referring to, for example, the Gibbs free energy, the actual liberatable energy of the particles constituting that embodied entity.

3.4.1, variational free energy model evidence and surprise.

Here, surprise.

The information on the data given the model is the negative log of model evidence.

And model evidence is always less than the variational free energy, which we'll go into more.

3.42 expected free energy and inference of most likely trajectories so variational free energy is real time it's about the model right now and the data right now expected free energy is going to be about likely courses of actions action sequences are called policies and that's going to entail the description of observations that haven't actually happened yet Jack


SPEAKER_04:
Oh yeah, I just wanted to ask a question regarding the distinction between Gibbs free energy and variational free energy, as you just mentioned in box 3.2.

So is this just a metaphor between the two, like an analogy, or is there a mathematical relationship that we can write down between Gibbs free energy, Helmholtz free energy, and variational free energy?


SPEAKER_00:
yes there are formal relationships among them structurally that that license the description of all of those as energy authors like alex keifer have um argued beyond where many go to talk about the um uh

actual uh free energy minimizing features of gibbs maintaining systems but let's return to that and then 3.5 active inference and novel foundation to understand behavior and cognition plain text section three six models policies and trajectories plain text reconciliation of inactive cybernetic and predictive theories under active inference

Plain text.

Active inference.

Emergence of life to agency.

Plain text.

3.9 summary.

Living organisms visit characteristic states.

Active inference.

Thank you, everybody.

We will return next week or in the office hours and just write your questions in the relevant pages and we'll continue the discussion.

So thanks a lot, everybody, for joining.

Farewell.
SPEAKER_03:
all right hello it's February 15th 2023 we're in cohort three of the textbook group and we're in our first discussion on chapter two of the textbook so there's a lot to discuss chapter two goes hard before we go into the questions that people have raised

Does anyone want to just share a first recollection, something that stuck with them, or a friction, or just a sentiment, or a feeling that they had when they were reading even part of chapter two?

You can just unmute and go for it if you want to share.

it's awesome that so many people added questions there's going to be a lot to discuss anyone just want to give a first thought what did they what did they hope chapter two would do was there any section or like quote or what stood out when people were on the low road to active inference

okay let us skim in a few minutes the whole chapter and then we're going to turn to the questions and just go based upon which questions people have voted as interesting but let's just take a quick uh flyby of the low road so chapter two is going to be coming on the low road

so just to get a visual reminder of what we're talking about with the high road and the low road from figure 1.2 the low road is like the how showing how from bayes theorem which is so simple and tautological and contentless we get a how

that meets with a why which is this imperative for persistence and self-organization and they come together in the generative models used in active inference which we're going to get to in chapter four so the low road is going to talk about a lot of the how and some of the what i guess as well

chapter three is going to come from the high road perspective and chapter four is where we're going to come to the heart of active inference with the generative models thank you maria we'll get to 2.3 and and then feel free to address that section 2.2

is perception as inference.

And this section gives a little bit of an overview on how even before the Bayesian brain was described,

there was concepts of perception as inference with Helmholtz and even further back as Action Maria shared in live stream 43.0 and what active inference is doing is extending that inferential framework beyond perception to also include action that is in the title it's active inference it's about inference on perception and action

This is gonna be operationalized with modern statistics through what is called a generative model.

We're gonna come back through this, so just moving rapidly.

Probabilistic reasoning is described by this equation.

which is it'll be awesome I think as a question asked like to have this in plain language what is being described or what is being done with Bayes equation an example is introduced where a person is holding either a frog or an apple in their hand and then the object is going to either jump or not jump

the person has some prior beliefs about a priori prior beliefs how likely frogs and apples are in the world and they have some beliefs about what is likely to do what action an observation comes in and then one is able to have a posterior belief or a posteriori

of how likely they think the object is to be one thing or the other.

They've updated their prior beliefs through observation.

So like the Bayesian moment is this critical moment where a new observation comes in and that in the context of a likelihood model updates the prior into a posterior.

here's an exact Bayesian inference on that scenario and for simple settings exact Bayesian inference is totally fine it turns out that for larger and more challenging statistical areas one has to use approximate Bayesian inference but what approximate Bayesian inference approximates is exact Bayesian inference

So this is kind of like the core of what you could do if you had infinite computational power.

And you can do it for simple examples.

There's a discussion of probability distributions because there's multiple probability distributions as we're dealing with probabilistic variables.

people may be familiar with the Gaussian or the so-called normal distribution or bell curve however there are other probability distributions that cover a different space on the x-axis that's called support and ultimately just have different shapes

So sometimes you want to be reducing your uncertainty about a bell curve.

Other times you might want to be reducing your uncertainty about other distribution families, like waiting times.

There's a discussion of two kinds of surprise.

There's kind of plain surprise or surprisal, which is just how surprising was that observation?

and it doesn't imply any updating of the generative model or belief it's just like taking a look at that observation alone how surprising was it we call that surprise surprisal and it's measured in information theoretic units a gnat is like a bit

but a bit is like a zero or one coin flip.

A nat is like a bit, but it's like your coin has E sides, 2.7, et cetera sides.

So that's surprise.

Just simply how surprising is an observation.

And then there's also Bayesian surprise.

Bayesian surprise also measured in nats is about how much

the observation updates the prior into the posterior.

So they're both measured in information theoretic units.

Surprisal is describing how surprising an observation is with no reference to the updating.

And the second notion of surprise is the Bayesian surprise, which is how much beliefs update.

expectations are described in box 2.2 although conversationally we might talk about like are you expecting it to rain tomorrow an expectation of a distribution is not a prediction about how it will be in the future necessarily expectation is the just arithmetic mean

So it's just the center of gravity, the center of mass of a probability distribution is its expectation.

Having introduced a more general Bayesian and information theoretic perspective on just inference, it's gonna return to biology.

so maria asked what is um how does uh active inference relate with biology and psychology was that the point of section 2.3 so does anybody want to give a thought like what was section 2.3 doing or how is optimality and bayesian statistics related to biological inference either restating what they added or just adding your own angle on it


SPEAKER_01:
I'll let you go for it.

I think section 2.3 serves as a kind of interlude between 2.2 namely perception as action and 2.4 action sorry, perception as inference and 2.4 action as inference because before dealing with the concept of action as inference we need to be clear what

kind of inference we're dealing with here.

Namely, it's not just

uh any kind of uh inference uh that's for example being discussed in uh other various uh literatures and statistics or in other areas but we're dealing with some specific kinds of bayesian inference uh which uh is one of its main characteristic is being optimal because uh to be optimal

in this framework is a necessary condition for inference because without optimality, we wouldn't have any actionable state of knowledge

So that's one of the main points of 2.3, because I believe the concept of optimality is a necessary requirement for any

biological or even non-biological agent to act as a sentient agent in any kind of environment.

So that optimality is a kind of, at least in this section, it's been described as a kind of requirement for any kind of sentient behavior.


SPEAKER_03:
Thank you.

Awesome.

Great way to put it as like an interlude.

because biological or even the kinds of active systems that aren't necessarily biological we want to talk about inference on perception but also we want to talk about inference on action and part of the unifying capacity of active inference is that perception and action

are going to be in service of a common imperative, which is free energy.

It's a really important note that optimality does not imply that it's adaptive.

It doesn't mean the organism succeeds.

It doesn't mean it even works.

So it's really worth it to see what is meant by optimality.

Optimality is defined in relation to a cost function that's optimized

that's the variational free energy which is going to be related to surprise which is why surprise was brought up as important earlier so it's also unpacked a lot more in the neuropsychiatry context where instead of saying well healthy or neurotypical people have optimal inference and then it's sub-optimal inference for those who are different in this or that way

rather in the precision neuropsychiatry setting we can talk about bayesian inference being optimal however with different generative models the outcomes of optimal inference are going to look different if somebody has an 100 belief that it rains every day then their optimal inference

will lead to them believing that it rains every day so this doesn't mean society accepts doesn't mean the business succeeds doesn't mean that the model works better than any other model it is just a statement about how bayesian inference is going to unfold and as ali pointed to this is helping us um move from a perceptual inference model into a subjective

subject specific generative model of perception and action which can be described as sentient in the sense used by friston at all to mean adaptive and responsive to stimuli

figure 2.2 is going to introduce a really important distinction between the generative process and the generative model so the generative model is the cognitive agent it's there whether you think of it as they are or they have a generative model that's like a realist

angle or we're making a generative model for a cognitive agent that's the instrumentalist more map territory falling more on the map side so generative model is a bayesian statistical model that is being used to describe a cognitive agent the generative process

can be thought of as the niche that is the underlying process passing observations to the generative model.

The niche, the generative process, can be an active inference process itself, but it doesn't have to be.

So the generative process could just be an if-then.

It could be any kind of logic, can have any endogenous dynamics.

However, it passes observations through the Markov blanket as sense states to the generative model, the GM.

Section 2.4, the discussion to this point is common to all Bayesian brain theories.

We now introduce the simple but fundamental advance offered by active inference, alarm bells ringing.

it considers action as inference.

Active inference is like Bayesian brain plus pragmatic turn.

Bayesian inference perspective on action.

That gets unpacked a little bit more.

In the next two sections, we clarify the single quantity that active inference agents minimize through perception and action is variational free energy.

So whereas a reward learning cognitive agents would be maximizing reward, an active inference agent as modeled as the way we're gonna be exploring in this textbook, they have two ways to approach a single imperative.

The single imperative is variational free energy.

The two ways that variational free energy can be minimized is changing your mind and changing the world.

Perception and learning and action.

So first, just minimizing the discrepancy between the model and the world.

At a first approximation, the common objective of perception action can be formulated as a minimization of the discrepancy between the model and the world.

Sometimes this is operationalized in terms of prediction error.

So here's a more classical prediction error minimization.

This is also reminiscent of like TOTE cycles looking for deviations and then acting to resolve deviations or discrepancies between predictions of the generative model, observations coming in from the generative process.

And then here are the two ways that discrepancies can be

reduced perception change belief oh I didn't think it was raining but it's wet discrepancy alarm bells surprise surprisal surprising observations so then what can be done well one can change belief so that upon the next cycle there is no discrepancy

Or there are situations where action can be taken so that future observations are in line with the unchanged predictions.

So those two pathways, perception and action, are going to come back again and again in active inference.

how much should we take the idea of cycles seriously from jonathan presumably it's much more non-linear yeah that's an awesome question does anyone want to give a first thought on that there's there's there's a bunch of ways to explore this

one is although sometimes we see it as a circle this is actually a very informal representation two representations or ways of thinking about it that are a bit closer to formality one

going to come to in figure 4.3 and chapter 7 as well which is when you implement this computationally it's going to be like unrolled as a linear sequence so one answer is computationally you're going to unroll the ways that these variables update each other

and they still may describe nonlinear functions.

So we still can get a nonlinear function being described in various places, even if it's unrolled.

And then the second formal representation that helps us escape this sort of like loops situation is

there's going to be not as much in this textbook but in a lot of the recent work on Bayesian mechanics equations are defined for these variables circles random variables as flows on those partitioned variables so in that setting

you can draw the edges about how different variables flows influence each other but it's actually like an unfolding flow that's just continuous across four different partitions that's the particular partition and so you don't need to draw any kind of like looping but the action perception loop is commonly understood and it's also very deep topic ali go for it


SPEAKER_01:
Sorry, yeah, I think for this question, that figure from Path Integrals' Strange Kinds paper might also help to illuminate this concept of cycles and the relevance of this concept to different types of particles

that have been described in this paper.

So yeah, I just wanted to point it out to that diagram.


SPEAKER_03:
Yes, here's the flows on different states.

And then the diagram.

Looking a little inverted, but continue.

What would you like to say about it?


SPEAKER_01:
So basically, in this paper, they've defined four different types of particles, the simplest one being inert particle.

which doesn't have any active states.

And as we increase in terms of complexity or increasingly more sentience behavior, we're dealing with active particles, which have active states, but as opposed to strange particles, which do possess the

disposition to being actually sentient particles.

In case of active particles and conservative particles, there isn't any hidden active states.

So that basically encompasses the different self-organizing or even non-self-organizing entities

ranging from simple rocks to brains and organisms so i think that's a pretty good way to somehow frame frame all of those behaviors within that bayesian mechanical framework thank you everett


SPEAKER_00:
I have a question of the figure 2.3.

It looks as if there's always discrepancy, but I can also imagine that I have a perception of objects in the world, say, for example, where there isn't any discrepancy between my prediction and my sensory observation.

But that is not shown by this picture, or is it?


SPEAKER_03:
the discrepancy value could be zero.

And especially for in silico, simple examples, like where the generative model is the exact same structure as the generative process, then the discrepancy can be zero.

I predicted a coin flip, heads, it came up heads, no discrepancy.

I don't need to change my belief.

Maybe I'll strengthen my belief and I don't need to take any further action.

So discrepancy value can be zero.

However,

in the context of fitting a generative model that is a coarse grained or partial representation of potentially noisy data coming in there's always going to be a non-zero discrepancy so that's why we're interested in relative discrepancy reduction

but there's also contrived examples where the discrepancy could be down to zero but there are situations where whether the discrepancy is zero or low or already as minimal as it can be that there may be no perception change there may also be no action change or you could imagine observations coming in that induce large changes in belief high bayesian surprise and

are associated with the agent taking action thank you earlier in the case of the object jumping out of the hand or not there was equations showing exact bayesian inference plug and chug you just push the numbers through the bayesian equations

Yet, exact Bayesian inference supporting perception and action is computationally intractable in most cases.

And they're going to describe the cases more?

It can just be understood coarsely as being hard to know the total state space of what could happen?

and it might require taking an integral or sums over functions that are very like large or intractable.

So it can be challenging to do exact Bayesian inference.

And so one of the key moves in active inference as most currently use it today is variational inference.

It will be unpacked in chapter four when we talk more about the generative models

It suffices to say that performing variational Bayesian inference implies substituting the two intractable quantities, posterior probability and log model evidence.

So that's like what we would want to know if we had full access to information, infinite computational resources, doable for simple problems, intractable for higher dimensional state spaces and so on.

We're going to replace those two.

Posterior probability is going to be replaced with an approximate posterior called Q. And then log model evidence is going to be substituted functionally by a free energy calculation.

This is called energy-based learning broadly.

It's been around for decades.

Active inference does not invent the idea of using free energy.

value on a bayesian statistical learning or inference challenge what is different is inference on action but it is not something that was generated within the active inference community it's existed for decades variational free energy variational bays is used broadly and every day active inference

uses inference on action and perception that's what integrates that kind of perceptual signal processing element with the action selection control theoretic element and it uses the particular partition which is the partition of the cognitive agent from the niche to operationalize variational inference in this kind of agent-based

perception cognition action loop or just scenario so it's always important as we're like learning through and again multiple coats of paint but what is actually new and different in active inference

what is describing or condensing or distilling work that is totally non-contentious and has existed for decades and is being used day to day evidence lower bound and energy-based learning people will find copious examples of

So that's one deflationary take to use a Fristianism on active inference is doing variational Bayesian inference isn't novel.

Unified imperatives for perception and action are also not necessarily novel, but using variational inference on the particular partition so that there can be a unified imperative for perception, cognition, action

is when we actually get to active inference equation 2.5 describes variational free energy we're going to come back to that figure 2.4 gives a visual representation of how variational free energy bounds surprise so here's like what we would want to know here's how we can approximate or bound what we want to know


SPEAKER_00:
figure 2.5 revisits that discrepancy yes ever at first yes I I have some difficulty with this bounding uh can you explain to me yes this figure uh what is meant by 3mg bounce what what bound


SPEAKER_03:
awesome so i'll just read what someone else has already added and then anyone else feel free to like raise your hand to add more so um there's something we can't actually calculate but we can calculate something approximately we know that whatever value we get for the approximate thing the true value we're looking for is certainly lower than the thing we can calculate so we want to know the organism's age

don't have any information about them but you do know the organism can never live to be over 10 years old so that's an upper bound on what you believe the age of the organism to be so surprisal so ln is the natural log so natural log and you can undo and do natural logs with exponentiation not going to go into too much detail on that but it's just an operation like whether when you see something ln and then parentheses you can

take it in and out.

We're going to talk more about logs later.

So this is what we would really want to know.

How surprised should we be for a given observation?

Because if we knew how truly surprised to be for any given observation, we would have access to that true underlying distribution that is controlling how surprised we should be.

Like if you don't know the range in temperature,

don't know how surprised to be for different observations if you knew the underlying temperature distribution you would know exactly how surprised to be with different thermometer readings what energy-based learning does again this is not an active inference ism this is energy-based learning or evidence lower bound used broadly in bayesian statistics

is it constructs an approximatable functional, which is a function of a function, an approximatable functional free energy, variational free energy, equation 2.5, that's a function of our approximation, which is a function, that's why F is a functional, and data.

So instead of trying to find the absolute function

underlying surprise how surprised we should be by a data point which would entail knowing the true distribution we're going to construct this functional variational free energy that's a function of q r approximation and y the data and then this is a kl divergence the double line

what is on both of the sides of the double line is what is being divergence minimized.

We're trying to minimize the divergence between our approximation and the true distribution conditioned on data.

This is guaranteed to be an upper bound.

It's always higher than the surprise.

And then by doing divergence minimization with KL divergence,

it's possible to push that upper bound lower so it's like here's how much the bank account actually has okay I know it's less than a billion because that's there's not that many of this token in the world is it less than a thousand is it less than five so it's like as you push the upper bound down

you're able, this can be approximated well.

Again, whether your startup works or whether the organism lives or whether the model's adaptive or whether it's better than any other model anyone else could ever think of, that's not this question.

This is saying you can generate a derivative, not in terms of like a rate of change, but like a sort of corollary value based around data and our approximation

that's always higher than surprise so surprise was brought in early in the chapter as a key imperative to surprise minimize but then that's intractable so we have an approximatable tractable form and that's what energy-based learning is and it's not invented in active inference

but what is a little novel and different is doing energy-based learning Bayesian variational inference on action we revisit in 2.5 figure free energy

And instead of it being discrepancy here, which is kind of like a prediction error minimization, that's like the T-O-T-E, like, is there a discrepancy?

Reduce the discrepancy.

Well, discrepancy between the predictions and the observations, when you think discrepancy, you're thinking like, it's going to be like, I predicted a seven, I got a nine, the difference was two.

That would be prediction error.

free energy is doing energy-based learning.

And so it's like a discrepancy because if the data coming in were exactly what was predicted, this divergence would be zero.

And this term would basically kind of functionally do what you would think a prediction error would do in the situation where the prediction matches the observation.

but it turns out to also extend in a few other ways but the reason why they showed the discrepancy first in 2.3 and then you can change perception or change your action selection and then prediction observation come together calculated discrepancy and then decisions are made cognitive decisions

perception or like embodied decisions in action that change the actual generative process and then in 2.5 that gets built out a little bit beyond just prediction error type discrepancies like there's a four inch discrepancy between these two things into an information theoretic or a statistical quantity that's variational free energy

Section 2.7 talks about expected free energy and planning as inference.

You can't talk about variational free energy on data points that haven't come in yet because they're future observations.

And so expected free energy G is similar and it's explored in 2.8.

It's similar in certain ways.

But importantly, and like we returned to this chapter after chapter, so no worries how much of these equations look fluent to you right now.

One thing is these equations have natural language representations that people can also improve, but especially like 2.5 for F.

you can read it and see the active inference ontology terms being used rather than variables.

And then in 2.6, some part of it is written.

But these are the kinds of augmentations that are always very welcome for people to make because they help us understand.

Expected free energy is like variational free energy, but whereas variational free energy is real time,

and it's crunching our approximation with the incoming data expected free energy is calculated over policies which are sequences of action and it is going to be dealing with observations that haven't yet come in so this is where we get to policy selection in active inference by thinking about expected free energy which is like a prospective

talking about the expected free energy for different possible policies which are sequences of actions there's some more discussion about special cases expected free energy when certain terms are basically zeroed out

expected free energy becomes resembling other equations.

So this is like a common technique in math.

You have some equation and then like you can generalize it.

And then a special case of the general form is this.

And then another special case might be that.

So for example,

We'll come back next week or in the future to what exactly the pieces are representing.

But one can imagine that if pragmatic value was irrelevant, then G would reflect information gain of different policies.

If information gain were irrelevant

like it were a totally discovered situation, then G would reflect purely pragmatic value.

So G is like a generalization, it's a unified imperative under which certain special cases resemble variously information maximizing principles or utility maximizing principles.

Jonathan, is there any equivalent to discounting as in an RL context?

Do you want to unpack what you mean there by discounting?

But then yes, there's definitely some angles on that.


SPEAKER_02:
Yeah, sure.

So in a reinforcement learning context, there's a clear aim, which is to maximize the expected future returns, but generally that's done in a discounted way.

And so the agent is trying to maximize the discounted expected future returns such that rewards that it gets in the future are essentially worth less than rewards that it gets in the near term.

So I know that there isn't the same kind of idea of rewards as there is in reinforcement learning in an active inference context, but I'm interested to know whether there is any sort of idea of discounting that states further into the future are somehow less important than states in the moment.


SPEAKER_03:
Yes, I'll add that question and then add in the one

bristen paper it turns out that yes it's possible to get temporally discounted type behavior so like a simple way to think about that would be if the pragmatic drive were urgent then you would see small pragmatic gain being approached

at the loss of future gain short time Horizon models and urgent desire drive whereas under longer time Horizons longer time preferences then you see the kind of behavior that in utility driven decision making is associated with what is called temporal discounting

So the phenomena, the cognitive behavioral phenomena exists and can be recapitulated, but it is not generated with the same way that it is in other places.

Sure, thank you.

2.9 is the end of the low road.

Variational free energy

real time unfolding f of q and y variational free energy our approximate posterior q q that's the one we control y data just like a regression y equals mx plus b q and y discrepancy except it's a variational free energy it's real time expected free energy g of pi

g is a function of policies and we're going to go into that more but it is regarding observations that haven't come in yet so it has a different structure but some rhyming with variational free energy what do they achieve together where did we get at the end of the low road

So we started with Bayes' theorem.

Generative model could be the thing jumping out of the hand.

Perception is inference.

That goes back to Helmholtz, but as Livestream 43 unpacks, this has been understood broadly across the world for thousands of years.

Predictive coding helps operationalize perception as inference because it's like top-down predictions, bottom-up sensory information.

That's why we don't perceive our blind spot.

To bring in action, we have to talk about perception as inference because we do care about observations, but we have to talk about observations that haven't actually happened yet.

And we have to talk about how our actions influence

which observations are likely to occur.

Perception and action as inference together make up the Bayesian brain, which is like some people, it means the brain does Bayesian computation.

That's a realist take.

The instrumentalist take would be we use Bayesian statistics to study the brain.

And where exact Bayes is intractable, we can use variational Bayes.

energy-based learning that's the end point of the low road into active inference summary active inference is a theory of how living artifacts underwrite their existence by minimizing surprise that's what we would truly want to know surprise if you knew how surprised to be

you would have had to have known the actual underlying distribution.

So we usually can't do that.

So we have a tractable proxy to surprise variational free energy.

So just like we'd want to reduce our surprise because that would reflect having the most optimally trained, but not overfit belief

we want to minimize a proxy that we know is an upper bound.

So by minimizing the upper bounds, we're getting closer and closer to what we really want to approximate.

The two ways to do that are perception and action.

They motivated this from a very first principles perspective

and then extended what is often left only in the realm of perception into the domain of action more discussion surprise is a construct of information theory it's not necessarily equivalent to folk psychology and this comes up again and again we see it in the ontology for like half of the words

These are words that are used broadly.

Perception, action, belief, surprise.

And sometimes they're perfectly overlapping with formal definitions.

Other times there's a little bit of a divergence.

And they close.

Perception, action, minimize, variational free energy in complementary ways.

That's real time.

Active inference also minimizes expected free energy prospectively.

This completes our journey along the low road.

In chapter three, we traveled the high road, which reaches the same conclusion on the basis of first principles and self-organization.

So we started from, we have a Bayesian model.

we didn't start from organisms need to exist but it turns out by connecting surprisal to self-organization or really just persistent repeated measurement we're going to get to the same place all right in our five remaining minutes just next week

we're going to return to the questions.

But I hope that it's useful to also just walk through it because definitely it takes like multiple readings and interrogatings.

And there's things that like you'd want to know in chapter two,

that are going to come up later or come up elsewhere in other papers there's just no single road or path let alone one that would be appropriate for all learners and their preferences so hopefully it was useful to like look over it and see it that way next week we're going to return a lot to the questions

so if anybody wants to do addition of questions it's always great you can jump into the answers and discourse as i see many do and add things you can upvote what is interesting you can enrich what people have written by just replacing where they use a plain text with something that you know could be tagged just like that

So there's all kinds of ways that like wherever chapter two is sitting with you now, either by generating new questions in discourse, you can add notes on chapter two here.

And then also in the math group, which we haven't been super active on, we've done some laying out of just the mathematical skeleton.

of chapter two as an example of how that could be done.

So, but in our closing minutes, just if we could have any closing thoughts.

just all anyone can raise their hand but um so in in relation to the temporal discounting in reinforcement learning ali also wrote reinforcement learning models opposed to act inf are not applicable to perceptual tasks that do not involve rewards or show significantly variable behavior so there's there's a lot there in situations where pragmatic value isn't a key imperative

even if that's just a sub-task, like isocating is driven by information gain.

Yes, our eyes dwell on what might be seen as rewarding, but it's information gain that is the imperative for the isocating.

And so where pragmatic value or reward or utility functions are either non-existent or flat, reinforcement has a hard time

And that leads to all these contrived reinforcement bonuses and novelties.

Oh, maybe we'll pay him for this and we'll do that.

And significantly, and learning that reward function is very difficult.

And learning that reward function, especially in like a partially observable or dynamic or volatile environment, ultimately you need to reward the learning anyway.

So then you end up coercing what we would just call epistemic value into a reward learning framework.

So why not just have a generalization of reward learning where in fully observed situations, pragmatic value can be unabashedly pursued

but in partially observable environments or where the generative model is a simplified or coarse-graining structure relative generative process, why not just authentically pursue the epistemic value, which is what you can do with G, but can only be done in a contrived or an ad hoc way if you have reward as your unified imperative.

that's not to say reinforcement learning models are not effective that again that the agent can't play chess or that the startup wouldn't work it's just to be clear that the the adequacy of a model in the world is an empirical question and especially in um a textbook the points are often being made in principle

great well again thanks for all these questions i hope that we can um rejoin next week with some thoughts and resources and other questions added in there's just going to be a lot of fun ways to take these different questions

we'll come back to them so thank you everyone and if you want um in the following hour you can come to the discord and we're just going to continue just talking and chilling about education so thank you and farewell
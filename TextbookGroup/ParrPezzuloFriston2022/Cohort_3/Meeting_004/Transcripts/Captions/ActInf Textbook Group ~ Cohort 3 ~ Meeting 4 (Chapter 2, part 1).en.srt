1
00:00:01,380 --> 00:00:02,879
all right hello

2
00:00:02,879 --> 00:00:08,280
it's February 15 2023 we're in cohort 3

3
00:00:08,280 --> 00:00:10,320
of The Textbook group

4
00:00:10,320 --> 00:00:12,960
and we're in our first discussion on

5
00:00:12,960 --> 00:00:16,440
chapter two of the textbook

6
00:00:16,440 --> 00:00:18,000
so

7
00:00:18,000 --> 00:00:20,939
there's a lot to discuss chapter two

8
00:00:20,939 --> 00:00:22,680
goes hard

9
00:00:22,680 --> 00:00:24,420
before we

10
00:00:24,420 --> 00:00:28,320
go into the questions that people have

11
00:00:28,320 --> 00:00:30,000
raised

12
00:00:30,000 --> 00:00:33,980
does anyone want to just share a first

13
00:00:33,980 --> 00:00:36,540
recollection something that stuck with

14
00:00:36,540 --> 00:00:39,899
them or a friction or just a sentiment

15
00:00:39,899 --> 00:00:42,420
or a feeling that they had when they

16
00:00:42,420 --> 00:00:43,500
were

17
00:00:43,500 --> 00:00:47,719
reading even part of chapter two

18
00:00:49,620 --> 00:00:51,360
you can just unmute and go for if you

19
00:00:51,360 --> 00:00:53,960
want to share

20
00:01:04,019 --> 00:01:06,720
it's awesome that so many people added

21
00:01:06,720 --> 00:01:09,060
questions there's going to be a lot to

22
00:01:09,060 --> 00:01:10,560
discuss

23
00:01:10,560 --> 00:01:13,200
anyone just want to give a first

24
00:01:13,200 --> 00:01:14,700
thought

25
00:01:14,700 --> 00:01:17,580
what did they what did they hope

26
00:01:17,580 --> 00:01:20,960
chapter two would do

27
00:01:22,500 --> 00:01:26,220
was there any section or like quote or

28
00:01:26,220 --> 00:01:27,720
what

29
00:01:27,720 --> 00:01:30,000
stood out when people were on the low

30
00:01:30,000 --> 00:01:33,920
road to active inference

31
00:01:50,460 --> 00:01:53,040
okay

32
00:01:53,040 --> 00:01:54,899
let us

33
00:01:54,899 --> 00:01:56,280
skim

34
00:01:56,280 --> 00:01:59,399
in a few minutes the whole chapter and

35
00:01:59,399 --> 00:02:02,100
then we're gonna turn to the questions

36
00:02:02,100 --> 00:02:04,619
and just go based upon which questions

37
00:02:04,619 --> 00:02:06,960
people have

38
00:02:06,960 --> 00:02:09,300
voted as interesting

39
00:02:09,300 --> 00:02:12,959
but let's just take a quick uh flyby of

40
00:02:12,959 --> 00:02:15,120
the low road

41
00:02:15,120 --> 00:02:16,680
so chapter two

42
00:02:16,680 --> 00:02:21,060
is going to be coming on the low road

43
00:02:21,060 --> 00:02:23,520
so just to get a visual

44
00:02:23,520 --> 00:02:25,800
reminder of what we're talking about

45
00:02:25,800 --> 00:02:28,319
with the high road and the low road

46
00:02:28,319 --> 00:02:31,379
from figure 1.2

47
00:02:31,379 --> 00:02:34,500
the low road is like the how

48
00:02:34,500 --> 00:02:37,260
showing how from Bayes theorem which is

49
00:02:37,260 --> 00:02:40,140
so simple and tautological and

50
00:02:40,140 --> 00:02:41,879
contentless

51
00:02:41,879 --> 00:02:43,680
we get a how

52
00:02:43,680 --> 00:02:46,019
that meets with a y

53
00:02:46,019 --> 00:02:48,660
which is this imperative for persistence

54
00:02:48,660 --> 00:02:51,000
and self-organization and they come

55
00:02:51,000 --> 00:02:54,420
together in the generative models used

56
00:02:54,420 --> 00:02:56,819
in active inference which we're going to

57
00:02:56,819 --> 00:02:59,099
get to in chapter four so the low road

58
00:02:59,099 --> 00:03:01,319
is going to talk about a lot of the how

59
00:03:01,319 --> 00:03:04,620
and some of the what I guess as well

60
00:03:04,620 --> 00:03:07,200
chapter three is going to come from The

61
00:03:07,200 --> 00:03:09,660
High Road perspective and chapter four

62
00:03:09,660 --> 00:03:11,760
is where we're going to come to the

63
00:03:11,760 --> 00:03:13,319
heart of active inference with the

64
00:03:13,319 --> 00:03:16,159
generative models

65
00:03:17,099 --> 00:03:20,640
thank you Maria we'll get to 2.3 and and

66
00:03:20,640 --> 00:03:24,440
then feel free to address that

67
00:03:25,560 --> 00:03:28,920
section 2.2

68
00:03:28,920 --> 00:03:31,739
is perception as inference

69
00:03:31,739 --> 00:03:33,300
and this section gives a little bit of

70
00:03:33,300 --> 00:03:35,519
an overview on how even before the

71
00:03:35,519 --> 00:03:37,019
Bayesian brain

72
00:03:37,019 --> 00:03:38,879
was described

73
00:03:38,879 --> 00:03:41,879
there was concepts of perception as

74
00:03:41,879 --> 00:03:44,400
inference with helmholtz and even

75
00:03:44,400 --> 00:03:47,519
further back as action Maria shared in

76
00:03:47,519 --> 00:03:49,920
live stream 43.0

77
00:03:49,920 --> 00:03:53,120
and what active inference is doing is

78
00:03:53,120 --> 00:03:55,760
extending that inferential framework

79
00:03:55,760 --> 00:03:59,040
Beyond perception to also include action

80
00:03:59,040 --> 00:04:01,799
that is in the title it's active

81
00:04:01,799 --> 00:04:03,659
inference it's about inference on

82
00:04:03,659 --> 00:04:06,739
perception and action

83
00:04:06,780 --> 00:04:10,439
this is going to be operationalized with

84
00:04:10,439 --> 00:04:12,360
modern statistics

85
00:04:12,360 --> 00:04:15,900
throwba is called a generative model

86
00:04:15,900 --> 00:04:17,459
we're going to come back through this so

87
00:04:17,459 --> 00:04:20,600
just moving rapidly

88
00:04:21,019 --> 00:04:23,520
probabilistic reasoning

89
00:04:23,520 --> 00:04:27,660
is described by this equation

90
00:04:27,660 --> 00:04:29,160
which is

91
00:04:29,160 --> 00:04:30,840
um it'll be awesome I think as a

92
00:04:30,840 --> 00:04:33,060
question asked like to have this in

93
00:04:33,060 --> 00:04:36,180
plain language what is being described

94
00:04:36,180 --> 00:04:38,040
or what is being done with basic

95
00:04:38,040 --> 00:04:40,280
question

96
00:04:40,740 --> 00:04:43,440
an example is introduced

97
00:04:43,440 --> 00:04:46,680
where a person is holding either a frog

98
00:04:46,680 --> 00:04:49,440
or an apple in their hand and then the

99
00:04:49,440 --> 00:04:52,440
object is going to either jump or not

100
00:04:52,440 --> 00:04:53,580
jump

101
00:04:53,580 --> 00:04:56,400
the person has some prior beliefs about

102
00:04:56,400 --> 00:05:00,419
a priori prior beliefs How likely frogs

103
00:05:00,419 --> 00:05:02,160
and apples are in the world

104
00:05:02,160 --> 00:05:05,699
and they have some beliefs about what is

105
00:05:05,699 --> 00:05:09,419
likely to do what action

106
00:05:09,419 --> 00:05:12,540
an observation comes in and then one is

107
00:05:12,540 --> 00:05:15,600
able to have a posterior belief or a

108
00:05:15,600 --> 00:05:18,780
posterior a posteriori

109
00:05:18,780 --> 00:05:22,440
of How likely they think the object is

110
00:05:22,440 --> 00:05:24,120
to be one thing or the other they've

111
00:05:24,120 --> 00:05:25,919
updated their prior beliefs

112
00:05:25,919 --> 00:05:28,979
through observation so like the Bayesian

113
00:05:28,979 --> 00:05:31,860
moment is this critical moment where a

114
00:05:31,860 --> 00:05:34,139
new observation comes in

115
00:05:34,139 --> 00:05:37,380
and that in the context of a likelihood

116
00:05:37,380 --> 00:05:38,639
model

117
00:05:38,639 --> 00:05:43,580
updates the prior into a posterior

118
00:05:44,280 --> 00:05:47,820
here's an exact Bayesian inference on

119
00:05:47,820 --> 00:05:49,860
that

120
00:05:49,860 --> 00:05:53,039
um scenario and for simple settings

121
00:05:53,039 --> 00:05:56,940
exact Bayesian inference is totally fine

122
00:05:56,940 --> 00:06:00,419
it turns out that for larger and more

123
00:06:00,419 --> 00:06:02,340
challenging

124
00:06:02,340 --> 00:06:05,100
statistical areas one has to use

125
00:06:05,100 --> 00:06:07,500
approximate Bayesian inference

126
00:06:07,500 --> 00:06:10,020
but what approximate Bayesian inference

127
00:06:10,020 --> 00:06:12,960
approximates is exact Bayesian inference

128
00:06:12,960 --> 00:06:16,440
so this is kind of like the core of what

129
00:06:16,440 --> 00:06:18,000
you could do if you had infinite

130
00:06:18,000 --> 00:06:19,800
computational power

131
00:06:19,800 --> 00:06:23,780
and you can do it for simple examples

132
00:06:24,300 --> 00:06:26,100
there's a discussion of probability

133
00:06:26,100 --> 00:06:28,380
distributions because there's multiple

134
00:06:28,380 --> 00:06:30,840
probability distributions as we're

135
00:06:30,840 --> 00:06:33,120
dealing with probabilistic variables

136
00:06:33,120 --> 00:06:35,220
people may be familiar with the gaussian

137
00:06:35,220 --> 00:06:38,880
or the so-called normal distribution or

138
00:06:38,880 --> 00:06:40,199
bell curve

139
00:06:40,199 --> 00:06:42,419
however there are other probability

140
00:06:42,419 --> 00:06:43,979
distributions

141
00:06:43,979 --> 00:06:48,360
that cover a different space on the

142
00:06:48,360 --> 00:06:49,620
x-axis

143
00:06:49,620 --> 00:06:51,479
that's called support

144
00:06:51,479 --> 00:06:53,340
and ultimately just have different

145
00:06:53,340 --> 00:06:55,259
shapes

146
00:06:55,259 --> 00:06:58,020
so sometimes you want to be reducing

147
00:06:58,020 --> 00:06:59,580
your uncertainty about a bell curve

148
00:06:59,580 --> 00:07:00,900
other times you might want to be

149
00:07:00,900 --> 00:07:03,360
reducing your uncertainty about other

150
00:07:03,360 --> 00:07:06,720
distribution families

151
00:07:06,720 --> 00:07:09,740
like waiting times

152
00:07:10,620 --> 00:07:12,960
there's a discussion of two kinds of

153
00:07:12,960 --> 00:07:16,860
surprise there's kind of plain surprise

154
00:07:16,860 --> 00:07:20,120
or surprisal which is just

155
00:07:20,120 --> 00:07:24,060
how surprising was that observation

156
00:07:24,060 --> 00:07:27,599
and it doesn't imply any updating of the

157
00:07:27,599 --> 00:07:29,819
generative model or belief it's just

158
00:07:29,819 --> 00:07:32,460
like taking a look at that observation

159
00:07:32,460 --> 00:07:35,280
alone how surprising was it we call that

160
00:07:35,280 --> 00:07:39,720
surprise surprisal and it's measured in

161
00:07:39,720 --> 00:07:43,680
information theoretic units a Nat is

162
00:07:43,680 --> 00:07:44,940
like a bit

163
00:07:44,940 --> 00:07:48,060
but a bit is like a zero or one coin

164
00:07:48,060 --> 00:07:50,759
flip a Nat is like a bit but it's like

165
00:07:50,759 --> 00:07:55,740
your coin has e sides 2.7

166
00:07:55,740 --> 00:08:00,300
Etc sites so that's surprise just simply

167
00:08:00,300 --> 00:08:03,479
how surprising is an observation and

168
00:08:03,479 --> 00:08:05,240
then there's also Bayesian surprise

169
00:08:05,240 --> 00:08:09,000
Bayesian surprise also measured in Nats

170
00:08:09,000 --> 00:08:11,400
is about how much

171
00:08:11,400 --> 00:08:15,419
the observation updates the prior into

172
00:08:15,419 --> 00:08:17,699
the posterior so they're both measured

173
00:08:17,699 --> 00:08:19,580
in information theoretic units

174
00:08:19,580 --> 00:08:23,220
surprisable is describing

175
00:08:23,220 --> 00:08:26,220
um how surprising an observation is with

176
00:08:26,220 --> 00:08:28,440
no reference to the updating and the

177
00:08:28,440 --> 00:08:30,000
second notion of surprise is the

178
00:08:30,000 --> 00:08:32,520
Bayesian surprise which is how much

179
00:08:32,520 --> 00:08:35,360
beliefs update

180
00:08:36,479 --> 00:08:39,479
expectations are described in box 2.2

181
00:08:39,479 --> 00:08:41,760
although conversationally we might talk

182
00:08:41,760 --> 00:08:43,740
about like are you expecting it to rain

183
00:08:43,740 --> 00:08:45,180
tomorrow

184
00:08:45,180 --> 00:08:50,160
and expectation of a distribution is not

185
00:08:50,160 --> 00:08:52,080
a prediction about how it will be in the

186
00:08:52,080 --> 00:08:52,980
future

187
00:08:52,980 --> 00:08:54,660
necessarily

188
00:08:54,660 --> 00:09:01,620
expectation is the just arithmetic mean

189
00:09:01,620 --> 00:09:04,320
so it's just the center of gravity the

190
00:09:04,320 --> 00:09:05,820
center of mass

191
00:09:05,820 --> 00:09:08,760
of a probability distribution is its

192
00:09:08,760 --> 00:09:11,360
expectation

193
00:09:14,279 --> 00:09:17,820
having introduced a more General

194
00:09:17,820 --> 00:09:20,240
Bayesian and information theoretic

195
00:09:20,240 --> 00:09:23,399
perspective on

196
00:09:23,399 --> 00:09:27,180
just inference it's gonna return to

197
00:09:27,180 --> 00:09:28,680
biology

198
00:09:28,680 --> 00:09:31,800
so Maria asked what is

199
00:09:31,800 --> 00:09:34,860
um how does active inference relate with

200
00:09:34,860 --> 00:09:36,839
Biology and psychology was that the

201
00:09:36,839 --> 00:09:39,300
point of section 2.3 so does anybody

202
00:09:39,300 --> 00:09:41,519
want to give a thought like what was

203
00:09:41,519 --> 00:09:45,740
section 2.3 doing or how is

204
00:09:45,740 --> 00:09:48,839
optimality and Bayesian statistics

205
00:09:48,839 --> 00:09:51,300
related to biological inference

206
00:09:51,300 --> 00:09:54,660
either restating what they added or just

207
00:09:54,660 --> 00:09:58,459
adding your own angle on it

208
00:10:04,320 --> 00:10:07,459
Ali go for it

209
00:10:07,940 --> 00:10:13,200
uh I think section 2.3 uh serves as a

210
00:10:13,200 --> 00:10:16,920
kind of interlude between 2.2 namely

211
00:10:16,920 --> 00:10:20,459
perception as action and 2.4 action uh

212
00:10:20,459 --> 00:10:23,100
sorry perception as inference and 2.4

213
00:10:23,100 --> 00:10:26,779
action as inference because uh before

214
00:10:26,779 --> 00:10:30,360
dealing with the concept of action as

215
00:10:30,360 --> 00:10:34,680
inference we need to be clear uh what

216
00:10:34,680 --> 00:10:37,320
kind of inference we're dealing with

217
00:10:37,320 --> 00:10:43,500
here namely uh it's not just uh any kind

218
00:10:43,500 --> 00:10:46,920
of uh inference that's for example been

219
00:10:46,920 --> 00:10:50,880
discussed in other various uh

220
00:10:50,880 --> 00:10:53,700
literatures and statistics or in other

221
00:10:53,700 --> 00:10:56,160
areas but we're dealing with some

222
00:10:56,160 --> 00:10:59,339
specific kinds of Bayesian inference uh

223
00:10:59,339 --> 00:11:03,360
which uh is uh one of its main

224
00:11:03,360 --> 00:11:06,420
characteristic is being optimal because

225
00:11:06,420 --> 00:11:09,000
uh to be optimal

226
00:11:09,000 --> 00:11:12,000
in this framework is a necessary

227
00:11:12,000 --> 00:11:14,940
condition for

228
00:11:14,940 --> 00:11:19,440
um for and for inference because uh

229
00:11:19,440 --> 00:11:24,480
without optimality we wouldn't have any

230
00:11:24,480 --> 00:11:25,320
um

231
00:11:25,320 --> 00:11:27,959
uh actionable

232
00:11:27,959 --> 00:11:31,680
um state of knowledge uh so

233
00:11:31,680 --> 00:11:35,000
um that's one of the main points of 2.3

234
00:11:35,000 --> 00:11:38,820
uh because uh I believe the concept of

235
00:11:38,820 --> 00:11:41,820
optimality is a necessary requirement

236
00:11:41,820 --> 00:11:44,540
for any biological or even

237
00:11:44,540 --> 00:11:48,839
non-biological agent to act as a

238
00:11:48,839 --> 00:11:52,260
sentient agent in any kind of uh

239
00:11:52,260 --> 00:11:55,800
environment so that optimality is a kind

240
00:11:55,800 --> 00:11:56,660
of

241
00:11:56,660 --> 00:11:59,459
at least in this section it's been

242
00:11:59,459 --> 00:12:01,740
described as a kind of requirement for

243
00:12:01,740 --> 00:12:06,000
any kind of sentence sentient Behavior

244
00:12:06,000 --> 00:12:08,940
thank you awesome

245
00:12:08,940 --> 00:12:12,480
great way to put it as like an interlude

246
00:12:12,480 --> 00:12:17,040
because biological or even the kinds of

247
00:12:17,040 --> 00:12:19,019
active systems that aren't necessarily

248
00:12:19,019 --> 00:12:20,820
biological we want to talk about

249
00:12:20,820 --> 00:12:23,220
inference on perception but also we want

250
00:12:23,220 --> 00:12:25,920
to talk about inference on action

251
00:12:25,920 --> 00:12:29,700
and part of the unifying capacity of

252
00:12:29,700 --> 00:12:32,579
active inference is that perception and

253
00:12:32,579 --> 00:12:35,880
action are going to be in service of a

254
00:12:35,880 --> 00:12:38,700
common imperative which is free energy

255
00:12:38,700 --> 00:12:40,680
it's a really important note that

256
00:12:40,680 --> 00:12:42,300
optimality

257
00:12:42,300 --> 00:12:45,660
does not imply that it's adaptive it

258
00:12:45,660 --> 00:12:48,420
doesn't mean the organism succeeds it

259
00:12:48,420 --> 00:12:50,760
doesn't mean it even works so it's

260
00:12:50,760 --> 00:12:53,459
really worth it to see what is meant by

261
00:12:53,459 --> 00:12:55,139
optimality

262
00:12:55,139 --> 00:12:57,540
optimality is defined in relation to a

263
00:12:57,540 --> 00:13:00,540
cost function that's optimized

264
00:13:00,540 --> 00:13:02,820
that's the variational free energy which

265
00:13:02,820 --> 00:13:04,440
is going to be related to surprise which

266
00:13:04,440 --> 00:13:06,600
is why surprise was brought up

267
00:13:06,600 --> 00:13:09,240
as important earlier

268
00:13:09,240 --> 00:13:13,760
so it's also unpacked a lot more in the

269
00:13:13,760 --> 00:13:16,260
neuropsychiatry context

270
00:13:16,260 --> 00:13:18,720
where instead of saying well healthy or

271
00:13:18,720 --> 00:13:20,399
neurotypical people have optimal

272
00:13:20,399 --> 00:13:22,800
inference and then it's sub-optimal

273
00:13:22,800 --> 00:13:24,959
inference for those who are different in

274
00:13:24,959 --> 00:13:26,940
this or that way

275
00:13:26,940 --> 00:13:30,779
rather in the Precision neuropsychiatry

276
00:13:30,779 --> 00:13:32,279
setting

277
00:13:32,279 --> 00:13:34,880
we can talk about Bayesian inference

278
00:13:34,880 --> 00:13:37,620
being optimal

279
00:13:37,620 --> 00:13:40,940
however with different generative models

280
00:13:40,940 --> 00:13:43,800
the outcomes of optimal inference are

281
00:13:43,800 --> 00:13:45,240
going to look different

282
00:13:45,240 --> 00:13:47,639
if somebody has an 100 belief that it

283
00:13:47,639 --> 00:13:48,959
rains every day

284
00:13:48,959 --> 00:13:52,680
then their optimal inference will lead

285
00:13:52,680 --> 00:13:54,120
to them believing that it rains every

286
00:13:54,120 --> 00:13:57,000
day so this doesn't mean Society accepts

287
00:13:57,000 --> 00:13:58,800
doesn't mean the business succeeds

288
00:13:58,800 --> 00:14:01,200
doesn't mean that the model works better

289
00:14:01,200 --> 00:14:03,779
than any other model it is just a

290
00:14:03,779 --> 00:14:07,019
statement about how Bayesian inference

291
00:14:07,019 --> 00:14:10,200
is going to unfold and as Ali pointed to

292
00:14:10,200 --> 00:14:13,079
this is helping us

293
00:14:13,079 --> 00:14:17,279
um move from a perceptual

294
00:14:17,279 --> 00:14:22,700
inference model into a subjective

295
00:14:23,180 --> 00:14:26,579
subject-specific generative model of

296
00:14:26,579 --> 00:14:29,220
perception and action which can be

297
00:14:29,220 --> 00:14:33,120
described as sentient in the sense used

298
00:14:33,120 --> 00:14:35,639
by fristin at all

299
00:14:35,639 --> 00:14:40,139
to mean adaptive and responsive to

300
00:14:40,139 --> 00:14:42,680
stimuli

301
00:14:43,500 --> 00:14:46,139
figure 2.2 is going to introduce a

302
00:14:46,139 --> 00:14:48,660
really important distinction between the

303
00:14:48,660 --> 00:14:50,279
generative process and the generative

304
00:14:50,279 --> 00:14:54,180
model so the generative model is the

305
00:14:54,180 --> 00:14:55,800
cognitive agent

306
00:14:55,800 --> 00:14:58,320
it's there whether you think of it as

307
00:14:58,320 --> 00:15:00,899
they are or they have a generative model

308
00:15:00,899 --> 00:15:05,760
that's like a realist angle or we're

309
00:15:05,760 --> 00:15:08,220
making a generative model for a

310
00:15:08,220 --> 00:15:09,959
cognitive agent that's the

311
00:15:09,959 --> 00:15:12,480
instrumentalist more map territory

312
00:15:12,480 --> 00:15:15,240
falling more on the map side so

313
00:15:15,240 --> 00:15:17,339
generative model is a Bayesian

314
00:15:17,339 --> 00:15:19,800
statistical model

315
00:15:19,800 --> 00:15:23,160
that is being used to describe

316
00:15:23,160 --> 00:15:25,500
a cognitive agent

317
00:15:25,500 --> 00:15:27,899
the generative process

318
00:15:27,899 --> 00:15:31,079
can be thought of as the niche that is

319
00:15:31,079 --> 00:15:33,839
the underlying process passing

320
00:15:33,839 --> 00:15:37,560
observations to the generative model

321
00:15:37,560 --> 00:15:41,040
the niche the generative process can be

322
00:15:41,040 --> 00:15:43,860
an active inference process itself but

323
00:15:43,860 --> 00:15:45,480
it doesn't have to be

324
00:15:45,480 --> 00:15:47,399
so the generative process could just be

325
00:15:47,399 --> 00:15:49,019
an if then

326
00:15:49,019 --> 00:15:51,420
it could be any kind of logic you can

327
00:15:51,420 --> 00:15:54,120
have any endogenous Dynamics

328
00:15:54,120 --> 00:15:56,579
however it passes observations

329
00:15:56,579 --> 00:15:58,500
through the Markov blanket as sent

330
00:15:58,500 --> 00:16:04,339
states to the generative model the GM

331
00:16:04,620 --> 00:16:07,260
section 2.4

332
00:16:07,260 --> 00:16:09,000
the discussion to this point is common

333
00:16:09,000 --> 00:16:12,360
to all Bayesian brain theories

334
00:16:12,360 --> 00:16:14,100
we now introduce the simple but

335
00:16:14,100 --> 00:16:16,560
fundamental Advance offered by active

336
00:16:16,560 --> 00:16:20,839
inference alarm bells ringing

337
00:16:21,120 --> 00:16:24,720
it considers action as inference active

338
00:16:24,720 --> 00:16:26,699
inference is like

339
00:16:26,699 --> 00:16:28,560
Bayesian brain

340
00:16:28,560 --> 00:16:29,839
plus

341
00:16:29,839 --> 00:16:33,079
pragmatic turn

342
00:16:33,660 --> 00:16:35,820
Bayesian inference perspective

343
00:16:35,820 --> 00:16:38,959
on action

344
00:16:40,320 --> 00:16:43,320
that gets unpacked a little bit more

345
00:16:43,320 --> 00:16:45,360
in the next two sections we clarify the

346
00:16:45,360 --> 00:16:46,980
single quantity that active inference

347
00:16:46,980 --> 00:16:48,839
agents minimize through perception and

348
00:16:48,839 --> 00:16:51,779
action is variational free energy

349
00:16:51,779 --> 00:16:55,019
so whereas a reward learning

350
00:16:55,019 --> 00:16:57,120
cognitive agents

351
00:16:57,120 --> 00:17:00,180
would be maximizing reward

352
00:17:00,180 --> 00:17:03,180
and active inference agent as modeled as

353
00:17:03,180 --> 00:17:04,679
the way we're going to be exploring in

354
00:17:04,679 --> 00:17:06,119
this textbook

355
00:17:06,119 --> 00:17:08,900
they have two ways

356
00:17:08,900 --> 00:17:13,079
to approach a single imperative

357
00:17:13,079 --> 00:17:15,839
the single imperative is variational

358
00:17:15,839 --> 00:17:17,579
free energy

359
00:17:17,579 --> 00:17:20,339
the two ways that variational free

360
00:17:20,339 --> 00:17:22,380
energy can be minimized

361
00:17:22,380 --> 00:17:25,140
is changing your mind and changing the

362
00:17:25,140 --> 00:17:26,099
world

363
00:17:26,099 --> 00:17:29,760
perception and learning and action

364
00:17:29,760 --> 00:17:31,679
so first

365
00:17:31,679 --> 00:17:33,780
just minimizing the discrepancy between

366
00:17:33,780 --> 00:17:37,140
the model and the world

367
00:17:37,140 --> 00:17:39,059
at a first approximation the common

368
00:17:39,059 --> 00:17:41,460
objective of perception and action

369
00:17:41,460 --> 00:17:43,679
can be formulated as a minimization of

370
00:17:43,679 --> 00:17:45,840
the discrepancy between the model and

371
00:17:45,840 --> 00:17:46,740
the world

372
00:17:46,740 --> 00:17:49,020
sometimes this is operationalized in

373
00:17:49,020 --> 00:17:51,179
terms of prediction error

374
00:17:51,179 --> 00:17:53,580
so here's a

375
00:17:53,580 --> 00:17:56,880
more classical prediction error

376
00:17:56,880 --> 00:17:59,580
minimization this is also reminiscent of

377
00:17:59,580 --> 00:18:01,740
like t-o-t-e

378
00:18:01,740 --> 00:18:06,419
Cycles looking for deviations and then

379
00:18:06,419 --> 00:18:08,580
acting to resolve deviations or

380
00:18:08,580 --> 00:18:10,260
discrepancies

381
00:18:10,260 --> 00:18:12,900
between predictions of the generative

382
00:18:12,900 --> 00:18:15,780
model observations coming in from the

383
00:18:15,780 --> 00:18:18,120
generative process and then here are the

384
00:18:18,120 --> 00:18:21,539
two ways that discrepancies can be

385
00:18:21,539 --> 00:18:22,980
reduced

386
00:18:22,980 --> 00:18:25,080
perception change belief oh I didn't

387
00:18:25,080 --> 00:18:28,080
think it was raining but it's wet

388
00:18:28,080 --> 00:18:31,679
discrepancy alarm Bells surprise

389
00:18:31,679 --> 00:18:33,179
surprisal

390
00:18:33,179 --> 00:18:35,460
surprising observations

391
00:18:35,460 --> 00:18:38,340
so then what can be done well one can

392
00:18:38,340 --> 00:18:39,780
change belief

393
00:18:39,780 --> 00:18:42,419
so that upon the next cycle

394
00:18:42,419 --> 00:18:44,820
there is no discrepancy

395
00:18:44,820 --> 00:18:47,820
or there are situations where action can

396
00:18:47,820 --> 00:18:51,600
be taken so that future observations are

397
00:18:51,600 --> 00:18:53,580
in line with the unchanged predictions

398
00:18:53,580 --> 00:18:55,080
so

399
00:18:55,080 --> 00:18:57,720
those two pathways

400
00:18:57,720 --> 00:19:00,660
perception and action

401
00:19:00,660 --> 00:19:04,919
are going to come back again and again

402
00:19:04,919 --> 00:19:07,940
inactive inference

403
00:19:10,740 --> 00:19:12,539
how much should we take the idea of

404
00:19:12,539 --> 00:19:14,580
cycle seriously from Jonathan presumably

405
00:19:14,580 --> 00:19:16,799
it's much more non-linear

406
00:19:16,799 --> 00:19:19,200
yeah that's an awesome question does

407
00:19:19,200 --> 00:19:20,460
anyone want to

408
00:19:20,460 --> 00:19:24,200
give a first thought on that

409
00:19:32,340 --> 00:19:34,860
there's there's a bunch of ways to

410
00:19:34,860 --> 00:19:36,660
explore this

411
00:19:36,660 --> 00:19:40,679
one is although sometimes we see it as a

412
00:19:40,679 --> 00:19:43,440
circle this is actually a very informal

413
00:19:43,440 --> 00:19:45,539
representation

414
00:19:45,539 --> 00:19:48,660
two representations or ways of thinking

415
00:19:48,660 --> 00:19:50,700
about it that are a bit closer to

416
00:19:50,700 --> 00:19:51,960
formality

417
00:19:51,960 --> 00:19:53,640
one

418
00:19:53,640 --> 00:19:56,220
we're going to come to in figure 4.3 and

419
00:19:56,220 --> 00:19:59,039
chapter 7 as well which is when you

420
00:19:59,039 --> 00:20:01,260
implement this computationally it's

421
00:20:01,260 --> 00:20:03,419
going to be like unrolled as a linear

422
00:20:03,419 --> 00:20:04,500
sequence

423
00:20:04,500 --> 00:20:07,620
so one answer is computationally you're

424
00:20:07,620 --> 00:20:10,799
going to unroll the ways that these

425
00:20:10,799 --> 00:20:13,260
variables update each other

426
00:20:13,260 --> 00:20:16,260
and they still May describe non-linear

427
00:20:16,260 --> 00:20:19,260
functions so we still can get a

428
00:20:19,260 --> 00:20:21,960
non-linear function being described at

429
00:20:21,960 --> 00:20:25,500
in various places even if it's unrolled

430
00:20:25,500 --> 00:20:27,000
and then the second formal

431
00:20:27,000 --> 00:20:28,320
representation

432
00:20:28,320 --> 00:20:31,620
that helps us escape this sort of like

433
00:20:31,620 --> 00:20:32,880
loops

434
00:20:32,880 --> 00:20:34,679
situation is

435
00:20:34,679 --> 00:20:36,600
there's going to be

436
00:20:36,600 --> 00:20:40,320
um not as much in this textbook but in a

437
00:20:40,320 --> 00:20:42,960
lot of the recent work on Bayesian

438
00:20:42,960 --> 00:20:45,000
mechanics

439
00:20:45,000 --> 00:20:47,220
equations are defined

440
00:20:47,220 --> 00:20:51,240
for these variables circles random

441
00:20:51,240 --> 00:20:56,419
variables as flows on those

442
00:20:56,419 --> 00:20:59,160
partitioned variables

443
00:20:59,160 --> 00:21:01,679
so in that setting

444
00:21:01,679 --> 00:21:04,020
you can draw the edges about how

445
00:21:04,020 --> 00:21:06,120
different variables flows influence each

446
00:21:06,120 --> 00:21:07,320
other but it's actually like an

447
00:21:07,320 --> 00:21:09,360
unfolding flow that just continuous

448
00:21:09,360 --> 00:21:11,880
across four different partitions

449
00:21:11,880 --> 00:21:14,100
that's the particular partition

450
00:21:14,100 --> 00:21:15,840
and so you don't need to draw any kind

451
00:21:15,840 --> 00:21:18,480
of like looping

452
00:21:18,480 --> 00:21:20,400
but the action perception Loop is

453
00:21:20,400 --> 00:21:21,600
commonly

454
00:21:21,600 --> 00:21:23,400
understood

455
00:21:23,400 --> 00:21:27,720
and it's also very deep topic

456
00:21:27,960 --> 00:21:30,980
Ali go for it

457
00:21:33,659 --> 00:21:36,659
uh sorry yeah I think uh for this

458
00:21:36,659 --> 00:21:40,020
question uh that figure from

459
00:21:40,020 --> 00:21:42,900
um path integrals strange kinds paper

460
00:21:42,900 --> 00:21:48,360
might also help to uh illuminate this uh

461
00:21:48,360 --> 00:21:51,539
concept of cycles and uh the relevance

462
00:21:51,539 --> 00:21:55,980
of this concept uh to different types of

463
00:21:55,980 --> 00:21:59,580
particles uh that have been described in

464
00:21:59,580 --> 00:22:02,220
this paper so yeah I just wanted to

465
00:22:02,220 --> 00:22:06,299
point it out to that diagram yes here's

466
00:22:06,299 --> 00:22:08,880
the flows on different states

467
00:22:08,880 --> 00:22:12,440
and then the diagram

468
00:22:12,840 --> 00:22:15,419
looking a little inverted but continue

469
00:22:15,419 --> 00:22:18,919
what would you like to say about it

470
00:22:19,740 --> 00:22:23,820
so basically uh in this paper they have

471
00:22:23,820 --> 00:22:26,340
defined four different types of

472
00:22:26,340 --> 00:22:29,340
particles the simplest one being inert

473
00:22:29,340 --> 00:22:30,780
particle

474
00:22:30,780 --> 00:22:34,799
which doesn't have any active States and

475
00:22:34,799 --> 00:22:38,460
as we increase uh in terms of

476
00:22:38,460 --> 00:22:43,320
um I mean complexity or uh increasingly

477
00:22:43,320 --> 00:22:46,080
more sentience Behavior we're dealing

478
00:22:46,080 --> 00:22:49,500
with active particles which have uh

479
00:22:49,500 --> 00:22:53,340
active States but uh

480
00:22:53,340 --> 00:22:58,260
as opposed to strange particles which do

481
00:22:58,260 --> 00:23:01,340
possess the

482
00:23:01,340 --> 00:23:06,419
disposition to being actually a sentient

483
00:23:06,419 --> 00:23:09,000
particles in case of active particles

484
00:23:09,000 --> 00:23:11,179
and conservative conservative particles

485
00:23:11,179 --> 00:23:15,179
there isn't any hidden active States so

486
00:23:15,179 --> 00:23:18,799
that's basically uh

487
00:23:18,799 --> 00:23:21,740
encompasses uh the different

488
00:23:21,740 --> 00:23:23,820
self-organizing or even non-self

489
00:23:23,820 --> 00:23:27,960
organizing uh entities ranging from

490
00:23:27,960 --> 00:23:32,640
simple rocks to brains and organisms so

491
00:23:32,640 --> 00:23:37,620
I think that's a pretty good way to

492
00:23:37,620 --> 00:23:40,200
somehow uh

493
00:23:40,200 --> 00:23:43,520
a frame frame all of those behaviors

494
00:23:43,520 --> 00:23:46,559
within that Bayesian mechanical

495
00:23:46,559 --> 00:23:49,020
framework

496
00:23:49,020 --> 00:23:52,039
thank you ever

497
00:23:54,559 --> 00:24:00,139
I have a question of uh the figure 2.3

498
00:24:01,080 --> 00:24:02,640
um it looks as if there's always

499
00:24:02,640 --> 00:24:05,280
discrepancy but I can also imagine that

500
00:24:05,280 --> 00:24:09,419
I have a perception of objects in the

501
00:24:09,419 --> 00:24:12,360
world say for example where there isn't

502
00:24:12,360 --> 00:24:14,340
any discrepancy between my prediction

503
00:24:14,340 --> 00:24:17,640
and my sensory observation but that

504
00:24:17,640 --> 00:24:21,960
doesn't is not shown by this picture or

505
00:24:21,960 --> 00:24:23,220
is it

506
00:24:23,220 --> 00:24:26,760
the discrepancy value could be zero and

507
00:24:26,760 --> 00:24:29,760
especially for in silico simple examples

508
00:24:29,760 --> 00:24:32,880
like where the generative model is the

509
00:24:32,880 --> 00:24:34,559
exact same structure as the generative

510
00:24:34,559 --> 00:24:35,820
process

511
00:24:35,820 --> 00:24:37,620
then the discrepancy can be zero I

512
00:24:37,620 --> 00:24:40,080
predicted a coin flip heads it came up

513
00:24:40,080 --> 00:24:42,179
heads no discrepancy I don't need to

514
00:24:42,179 --> 00:24:44,220
change my belief maybe I'll strengthen

515
00:24:44,220 --> 00:24:46,140
my belief and I don't need to take any

516
00:24:46,140 --> 00:24:48,480
further action so discrepancy value can

517
00:24:48,480 --> 00:24:50,520
be zero however

518
00:24:50,520 --> 00:24:54,480
in the context of fitting a generative

519
00:24:54,480 --> 00:24:55,860
model

520
00:24:55,860 --> 00:24:59,880
that is a coarse grains or partial

521
00:24:59,880 --> 00:25:01,380
representation

522
00:25:01,380 --> 00:25:02,700
of

523
00:25:02,700 --> 00:25:06,240
potentially noisy data coming in there's

524
00:25:06,240 --> 00:25:08,039
always going to be

525
00:25:08,039 --> 00:25:10,380
a non-zero discrepancy

526
00:25:10,380 --> 00:25:12,960
so that's why we're interested in

527
00:25:12,960 --> 00:25:16,740
relative discrepancy reduction

528
00:25:16,740 --> 00:25:19,159
but there's also contrived examples

529
00:25:19,159 --> 00:25:21,659
where the discrepancy could be down to

530
00:25:21,659 --> 00:25:22,980
zero

531
00:25:22,980 --> 00:25:24,779
but there are situations where whether

532
00:25:24,779 --> 00:25:27,000
the discrepancy is zero or low or

533
00:25:27,000 --> 00:25:29,340
already as minimal as it can be that

534
00:25:29,340 --> 00:25:33,299
there may be no perception change there

535
00:25:33,299 --> 00:25:36,419
may also be no action change or you

536
00:25:36,419 --> 00:25:38,580
could imagine observations coming in

537
00:25:38,580 --> 00:25:41,340
that induce large changes in belief High

538
00:25:41,340 --> 00:25:44,279
Bayesian surprise and

539
00:25:44,279 --> 00:25:46,440
are associated with the agent taking

540
00:25:46,440 --> 00:25:48,919
action

541
00:25:50,100 --> 00:25:53,580
thank you earlier in the case of the

542
00:25:53,580 --> 00:25:56,760
object jumping out of the hand or not

543
00:25:56,760 --> 00:25:59,400
there was equations showing exact

544
00:25:59,400 --> 00:26:02,760
Bayesian inference plug and chug just

545
00:26:02,760 --> 00:26:05,159
push the numbers through the Bayesian

546
00:26:05,159 --> 00:26:06,360
equations

547
00:26:06,360 --> 00:26:09,480
yet exact Bayesian inference supporting

548
00:26:09,480 --> 00:26:11,700
perception action is computationally

549
00:26:11,700 --> 00:26:15,000
intractable in most cases and they're

550
00:26:15,000 --> 00:26:17,700
going to describe the cases more

551
00:26:17,700 --> 00:26:20,820
it can just be understood coarsely as

552
00:26:20,820 --> 00:26:24,480
being hard to know the total State space

553
00:26:24,480 --> 00:26:26,279
of what could happen

554
00:26:26,279 --> 00:26:30,000
and it might require taking an integral

555
00:26:30,000 --> 00:26:33,960
or sums over functions that are very

556
00:26:33,960 --> 00:26:38,039
like large or intractable so

557
00:26:38,039 --> 00:26:40,140
it can be challenging to do exact

558
00:26:40,140 --> 00:26:43,260
Bayesian inference and so one of the key

559
00:26:43,260 --> 00:26:45,900
moves in active inference as most

560
00:26:45,900 --> 00:26:48,020
currently use it today

561
00:26:48,020 --> 00:26:50,880
is variational inference

562
00:26:50,880 --> 00:26:53,039
it will be unpacked in chapter four when

563
00:26:53,039 --> 00:26:56,640
we talk more about the generative models

564
00:26:56,640 --> 00:26:58,919
it suffices to say that performing

565
00:26:58,919 --> 00:27:01,559
variational Bayesian inference implies

566
00:27:01,559 --> 00:27:03,480
substituting the two intractable

567
00:27:03,480 --> 00:27:05,520
quantities

568
00:27:05,520 --> 00:27:08,640
posterior probability and log model

569
00:27:08,640 --> 00:27:09,900
evidence

570
00:27:09,900 --> 00:27:12,360
so that's like what we would want to

571
00:27:12,360 --> 00:27:14,460
know if we had full access to

572
00:27:14,460 --> 00:27:16,200
information infinite computational

573
00:27:16,200 --> 00:27:18,059
resources

574
00:27:18,059 --> 00:27:20,880
doable for simple problems intractable

575
00:27:20,880 --> 00:27:23,039
for higher dimensional State spaces and

576
00:27:23,039 --> 00:27:23,940
so on

577
00:27:23,940 --> 00:27:26,520
we're going to replace those two

578
00:27:26,520 --> 00:27:29,520
posterior probability is going to be

579
00:27:29,520 --> 00:27:31,919
replaced with an approximate posterior

580
00:27:31,919 --> 00:27:33,720
called Q

581
00:27:33,720 --> 00:27:37,799
and then log model evidence is going to

582
00:27:37,799 --> 00:27:40,919
be substituted functionally by a free

583
00:27:40,919 --> 00:27:43,279
energy calculation this is called

584
00:27:43,279 --> 00:27:46,740
energy-based learning broadly

585
00:27:46,740 --> 00:27:50,220
it's been around for decades active

586
00:27:50,220 --> 00:27:53,279
inference does not invent the idea of

587
00:27:53,279 --> 00:27:55,320
using a free energy

588
00:27:55,320 --> 00:27:56,880
value

589
00:27:56,880 --> 00:27:59,880
on a Bayesian statistical learning or

590
00:27:59,880 --> 00:28:01,559
inference challenge

591
00:28:01,559 --> 00:28:06,179
what is different is inference on action

592
00:28:06,179 --> 00:28:08,760
but it is not something that was

593
00:28:08,760 --> 00:28:10,440
generated within the octave inference

594
00:28:10,440 --> 00:28:12,360
Community it's existed for decades

595
00:28:12,360 --> 00:28:16,380
variational free energy variational Bays

596
00:28:16,380 --> 00:28:20,159
is used broadly and every day

597
00:28:20,159 --> 00:28:23,580
octave inference uses inference on

598
00:28:23,580 --> 00:28:26,100
action and perception

599
00:28:26,100 --> 00:28:27,799
that's what integrates that kind of

600
00:28:27,799 --> 00:28:30,539
perceptual signal processing elements

601
00:28:30,539 --> 00:28:32,760
with the action selection control

602
00:28:32,760 --> 00:28:34,679
theoretic element

603
00:28:34,679 --> 00:28:36,120
and

604
00:28:36,120 --> 00:28:39,240
it uses the particular partition

605
00:28:39,240 --> 00:28:42,600
which is the partition of the cognitive

606
00:28:42,600 --> 00:28:44,580
agent from the niche

607
00:28:44,580 --> 00:28:48,559
to operationalize variational inference

608
00:28:48,559 --> 00:28:52,140
in this kind of agent-based

609
00:28:52,140 --> 00:28:55,380
perception cognition action Loop

610
00:28:55,380 --> 00:28:58,860
or just scenario so it's always

611
00:28:58,860 --> 00:29:00,720
important as we're like learning through

612
00:29:00,720 --> 00:29:03,179
and again multiple coats of paint but

613
00:29:03,179 --> 00:29:05,460
what is actually new and different in

614
00:29:05,460 --> 00:29:07,020
active inference

615
00:29:07,020 --> 00:29:10,620
what is describing or condensing or

616
00:29:10,620 --> 00:29:13,080
distilling work that is totally

617
00:29:13,080 --> 00:29:15,779
non-contentious and has existed for

618
00:29:15,779 --> 00:29:18,360
decades and is being used day to day

619
00:29:18,360 --> 00:29:21,000
evidence lower bound and energy-based

620
00:29:21,000 --> 00:29:24,240
learning people will find copious

621
00:29:24,240 --> 00:29:26,460
examples of

622
00:29:26,460 --> 00:29:30,299
so that's one deflationary take to use a

623
00:29:30,299 --> 00:29:31,679
fristianism

624
00:29:31,679 --> 00:29:34,679
on active inference is doing variational

625
00:29:34,679 --> 00:29:38,100
Bayesian inference isn't novel

626
00:29:38,100 --> 00:29:40,860
unified imperatives for perception and

627
00:29:40,860 --> 00:29:41,880
action

628
00:29:41,880 --> 00:29:44,820
are also not necessarily novel

629
00:29:44,820 --> 00:29:47,640
but using variational inference

630
00:29:47,640 --> 00:29:50,340
on the particular partition

631
00:29:50,340 --> 00:29:52,559
so that there can be a unified

632
00:29:52,559 --> 00:29:54,299
imperative for perception cognition

633
00:29:54,299 --> 00:29:55,500
action

634
00:29:55,500 --> 00:29:57,539
is when we actually get to active

635
00:29:57,539 --> 00:29:59,960
inference

636
00:30:00,120 --> 00:30:02,220
equation 2.5

637
00:30:02,220 --> 00:30:05,039
describes variational free energy we're

638
00:30:05,039 --> 00:30:07,820
going to come back to that

639
00:30:08,880 --> 00:30:12,919
figure 2.4 gives a visual representation

640
00:30:12,919 --> 00:30:16,500
of how variational free energy bounds

641
00:30:16,500 --> 00:30:18,120
surprise

642
00:30:18,120 --> 00:30:19,860
so here's like what we would want to

643
00:30:19,860 --> 00:30:21,779
know

644
00:30:21,779 --> 00:30:23,940
here's how we can approximate or bound

645
00:30:23,940 --> 00:30:27,080
what we want to know

646
00:30:27,840 --> 00:30:32,100
figure 2.5 revisits that discrepancy yes

647
00:30:32,100 --> 00:30:34,760
ever at first

648
00:30:35,399 --> 00:30:38,520
yes I I have some difficulty with this

649
00:30:38,520 --> 00:30:43,080
founding uh can you explain to me

650
00:30:43,080 --> 00:30:47,159
yes this figure uh what is meant by PMG

651
00:30:47,159 --> 00:30:48,480
bounds

652
00:30:48,480 --> 00:30:51,120
what what about

653
00:30:51,120 --> 00:30:53,460
awesome so I'll just read what someone

654
00:30:53,460 --> 00:30:55,860
else has already added and then anyone

655
00:30:55,860 --> 00:30:57,000
else feel free to like raise your hand

656
00:30:57,000 --> 00:30:58,740
to add more so

657
00:30:58,740 --> 00:31:00,120
um there's something we can't actually

658
00:31:00,120 --> 00:31:02,340
calculate but we can calculate something

659
00:31:02,340 --> 00:31:04,140
approximately

660
00:31:04,140 --> 00:31:05,820
we know that whatever value we get for

661
00:31:05,820 --> 00:31:07,380
the approximate thing

662
00:31:07,380 --> 00:31:09,899
the True Value we're looking for

663
00:31:09,899 --> 00:31:11,940
is certainly lower than the thing we can

664
00:31:11,940 --> 00:31:13,980
calculate so we want to know if the

665
00:31:13,980 --> 00:31:15,720
organism's age

666
00:31:15,720 --> 00:31:17,220
you don't have any information about

667
00:31:17,220 --> 00:31:19,080
them but you do know the organism can

668
00:31:19,080 --> 00:31:21,240
never live to be over 10 years old so

669
00:31:21,240 --> 00:31:22,799
that's an upper bound on what you

670
00:31:22,799 --> 00:31:25,500
believe the age of the organism to be

671
00:31:25,500 --> 00:31:31,140
so surprisal so Ln is the natural log so

672
00:31:31,140 --> 00:31:34,620
natural log and you can undo and do

673
00:31:34,620 --> 00:31:37,740
natural logs with exponentiation

674
00:31:37,740 --> 00:31:39,360
not going to go into too much detail on

675
00:31:39,360 --> 00:31:41,039
that but it's just an operation like

676
00:31:41,039 --> 00:31:42,899
whether when you see something Ln and

677
00:31:42,899 --> 00:31:46,380
then parentheses you can

678
00:31:46,380 --> 00:31:48,059
um take it in and out we're going to

679
00:31:48,059 --> 00:31:50,820
talk more about logs later

680
00:31:50,820 --> 00:31:53,220
um so this is what we would really want

681
00:31:53,220 --> 00:31:54,360
to know

682
00:31:54,360 --> 00:31:57,299
how surprised should we be

683
00:31:57,299 --> 00:31:59,279
for a given observation

684
00:31:59,279 --> 00:32:01,559
because if we knew how truly surprised

685
00:32:01,559 --> 00:32:04,140
to be for any given observation

686
00:32:04,140 --> 00:32:06,299
we would have access to that true

687
00:32:06,299 --> 00:32:08,700
underlying distribution

688
00:32:08,700 --> 00:32:11,700
that is controlling how surprised we

689
00:32:11,700 --> 00:32:12,539
should be

690
00:32:12,539 --> 00:32:14,760
like if you don't know the the range in

691
00:32:14,760 --> 00:32:16,140
temperature

692
00:32:16,140 --> 00:32:18,120
you don't know how surprised to be for

693
00:32:18,120 --> 00:32:20,100
different observations if you knew the

694
00:32:20,100 --> 00:32:22,320
underlying temperature distribution you

695
00:32:22,320 --> 00:32:23,880
would know exactly how surprised to be

696
00:32:23,880 --> 00:32:27,860
with different thermometer readings

697
00:32:28,559 --> 00:32:31,140
what energy-based learning does again

698
00:32:31,140 --> 00:32:33,720
this is not an active inferencesm this

699
00:32:33,720 --> 00:32:36,120
is energy-based learning or evidence

700
00:32:36,120 --> 00:32:37,740
lower bound

701
00:32:37,740 --> 00:32:41,220
used broadly in Bayesian statistics is

702
00:32:41,220 --> 00:32:43,679
it constructs an approximatable

703
00:32:43,679 --> 00:32:45,480
functional

704
00:32:45,480 --> 00:32:48,299
which is a function of a function

705
00:32:48,299 --> 00:32:51,899
an approximatable functional free energy

706
00:32:51,899 --> 00:32:55,260
variational free energy equation 2.5

707
00:32:55,260 --> 00:32:57,299
that's a function of r approximation

708
00:32:57,299 --> 00:32:59,700
which is a function that's why f is a

709
00:32:59,700 --> 00:33:02,580
functional and data

710
00:33:02,580 --> 00:33:05,100
so instead of trying to find the

711
00:33:05,100 --> 00:33:06,799
absolute

712
00:33:06,799 --> 00:33:09,659
underlying surprise how surprised we

713
00:33:09,659 --> 00:33:12,000
should be by a data point which would

714
00:33:12,000 --> 00:33:15,299
entail knowing the true distribution

715
00:33:15,299 --> 00:33:19,559
we're going to construct this functional

716
00:33:19,559 --> 00:33:22,140
variational free energy that's a

717
00:33:22,140 --> 00:33:23,519
function of

718
00:33:23,519 --> 00:33:28,380
q r approximation and Y the data

719
00:33:28,380 --> 00:33:32,399
and then this is a KL Divergence

720
00:33:32,399 --> 00:33:34,260
the double line

721
00:33:34,260 --> 00:33:36,059
what is on both of the sides the double

722
00:33:36,059 --> 00:33:39,059
line is what is being Divergence

723
00:33:39,059 --> 00:33:40,559
minimized

724
00:33:40,559 --> 00:33:43,080
we're trying to minimize the Divergence

725
00:33:43,080 --> 00:33:44,220
between

726
00:33:44,220 --> 00:33:46,679
our approximation

727
00:33:46,679 --> 00:33:49,679
and the true distribution conditioned on

728
00:33:49,679 --> 00:33:51,539
data

729
00:33:51,539 --> 00:33:54,480
this is guaranteed to be an upper bound

730
00:33:54,480 --> 00:33:58,260
it's always higher than the surprise

731
00:33:58,260 --> 00:34:00,720
and then by doing diversions

732
00:34:00,720 --> 00:34:02,159
minimization

733
00:34:02,159 --> 00:34:04,559
with KL Divergence

734
00:34:04,559 --> 00:34:06,360
it's possible to

735
00:34:06,360 --> 00:34:09,359
push that upper bound lower

736
00:34:09,359 --> 00:34:11,879
so it's like here's how much the bank

737
00:34:11,879 --> 00:34:15,119
account actually has okay I know it's

738
00:34:15,119 --> 00:34:17,159
less than a billion because that's

739
00:34:17,159 --> 00:34:19,440
there's not that many of this token in

740
00:34:19,440 --> 00:34:20,820
the world

741
00:34:20,820 --> 00:34:23,399
is it less than a thousand is it less

742
00:34:23,399 --> 00:34:25,500
than five so it's like as you push the

743
00:34:25,500 --> 00:34:28,460
upper bound down

744
00:34:28,460 --> 00:34:31,619
you're able this can be approximated

745
00:34:31,619 --> 00:34:35,159
well again whether your startup works or

746
00:34:35,159 --> 00:34:36,599
whether the organism lives or whether

747
00:34:36,599 --> 00:34:38,159
the model's adaptive or whether it's

748
00:34:38,159 --> 00:34:39,719
better than any other model anyone else

749
00:34:39,719 --> 00:34:41,460
could ever think of that's an it's not

750
00:34:41,460 --> 00:34:43,379
this question

751
00:34:43,379 --> 00:34:47,339
this is saying you can generate

752
00:34:47,339 --> 00:34:50,520
a derivative not in terms of like a rate

753
00:34:50,520 --> 00:34:52,500
of change but like a sort of corollary

754
00:34:52,500 --> 00:34:54,300
value

755
00:34:54,300 --> 00:34:58,080
based around data and our approximation

756
00:34:58,080 --> 00:35:01,859
that's always higher than surprise

757
00:35:01,859 --> 00:35:03,900
so surprise was brought in early in the

758
00:35:03,900 --> 00:35:04,920
chapter

759
00:35:04,920 --> 00:35:09,720
as a key imperative to surprise minimize

760
00:35:09,720 --> 00:35:11,820
but then that's intractable

761
00:35:11,820 --> 00:35:15,359
so we have an approximatable tractable

762
00:35:15,359 --> 00:35:16,740
form

763
00:35:16,740 --> 00:35:18,480
and that's what energy-based learning is

764
00:35:18,480 --> 00:35:20,220
and it's not invented in active

765
00:35:20,220 --> 00:35:22,619
inference

766
00:35:22,619 --> 00:35:25,200
but what is a little novel and different

767
00:35:25,200 --> 00:35:28,079
is doing energy-based learning

768
00:35:28,079 --> 00:35:30,780
Bayesian variational inference

769
00:35:30,780 --> 00:35:33,980
on action

770
00:35:34,680 --> 00:35:37,560
we revisit in 2.5 figure

771
00:35:37,560 --> 00:35:39,240
free energy

772
00:35:39,240 --> 00:35:41,640
and instead of each being discrepancy

773
00:35:41,640 --> 00:35:43,800
here which is kind of like an a

774
00:35:43,800 --> 00:35:46,140
prediction error minimization that's

775
00:35:46,140 --> 00:35:48,060
like the tote

776
00:35:48,060 --> 00:35:50,460
like is there discrepancy reduce the

777
00:35:50,460 --> 00:35:53,640
discrepancy well discrepancy between the

778
00:35:53,640 --> 00:35:56,099
predictions and the observations when

779
00:35:56,099 --> 00:35:58,140
you think discrepancy you're thinking

780
00:35:58,140 --> 00:35:59,760
like it's going to be like I predicted a

781
00:35:59,760 --> 00:36:01,560
seven I got a nine the difference was

782
00:36:01,560 --> 00:36:02,579
two

783
00:36:02,579 --> 00:36:05,400
that would be prediction error

784
00:36:05,400 --> 00:36:08,460
free energy is doing energy-based

785
00:36:08,460 --> 00:36:11,640
learning and so it's like a discrepancy

786
00:36:11,640 --> 00:36:14,579
because if the data coming in

787
00:36:14,579 --> 00:36:17,280
were exactly what was predicted

788
00:36:17,280 --> 00:36:20,520
this Divergence would be zero and this

789
00:36:20,520 --> 00:36:22,980
term would basically

790
00:36:22,980 --> 00:36:25,740
kind of functionally do what you would

791
00:36:25,740 --> 00:36:28,560
think a prediction error would do in the

792
00:36:28,560 --> 00:36:31,079
situation where the prediction matches

793
00:36:31,079 --> 00:36:32,640
the observation

794
00:36:32,640 --> 00:36:35,700
but it turns out to also extend in a few

795
00:36:35,700 --> 00:36:38,700
other ways but the reason why they

796
00:36:38,700 --> 00:36:42,480
showed the discrepancy first in 2.3

797
00:36:42,480 --> 00:36:45,300
and then you can change perception or

798
00:36:45,300 --> 00:36:47,880
change your auction selection

799
00:36:47,880 --> 00:36:49,380
and then

800
00:36:49,380 --> 00:36:51,300
prediction observation come together

801
00:36:51,300 --> 00:36:53,099
calculate a discrepancy and then

802
00:36:53,099 --> 00:36:54,960
decisions are made

803
00:36:54,960 --> 00:36:57,540
cognitive decisions

804
00:36:57,540 --> 00:37:01,200
perception or like embodied decisions in

805
00:37:01,200 --> 00:37:02,160
action

806
00:37:02,160 --> 00:37:03,780
that change the actual generative

807
00:37:03,780 --> 00:37:06,119
process

808
00:37:06,119 --> 00:37:08,280
and then in 2.5

809
00:37:08,280 --> 00:37:10,859
that gets built out a little bit beyond

810
00:37:10,859 --> 00:37:13,680
just prediction error type discrepancies

811
00:37:13,680 --> 00:37:15,720
like there's a four inch discrepancy

812
00:37:15,720 --> 00:37:18,540
between these two things into an

813
00:37:18,540 --> 00:37:21,300
information theoretic or a statistical

814
00:37:21,300 --> 00:37:24,020
quantity

815
00:37:24,540 --> 00:37:27,599
that's variational free energy

816
00:37:27,599 --> 00:37:30,020
f

817
00:37:30,720 --> 00:37:34,079
section 2.7 talks about expected free

818
00:37:34,079 --> 00:37:37,440
energy and planning as inference

819
00:37:37,440 --> 00:37:40,320
you can't talk about variational free

820
00:37:40,320 --> 00:37:42,599
energy on data points that haven't come

821
00:37:42,599 --> 00:37:46,440
in yet because their future observations

822
00:37:46,440 --> 00:37:51,060
and so expected free energy G

823
00:37:51,060 --> 00:37:52,920
is similar

824
00:37:52,920 --> 00:37:56,700
and it's explored in 2.8 it's similar in

825
00:37:56,700 --> 00:37:58,140
certain ways

826
00:37:58,140 --> 00:38:00,900
but importantly and like we return to

827
00:38:00,900 --> 00:38:04,440
this chapter after chapter so no worries

828
00:38:04,440 --> 00:38:05,940
how much

829
00:38:05,940 --> 00:38:08,040
of these equations

830
00:38:08,040 --> 00:38:09,960
look

831
00:38:09,960 --> 00:38:12,540
fluent to you right now

832
00:38:12,540 --> 00:38:16,980
one thing is these equations have

833
00:38:16,980 --> 00:38:19,920
natural language representations that

834
00:38:19,920 --> 00:38:21,480
people can also improve but especially

835
00:38:21,480 --> 00:38:23,460
like 2.5

836
00:38:23,460 --> 00:38:26,420
for f

837
00:38:26,880 --> 00:38:28,800
you can read it and see the active

838
00:38:28,800 --> 00:38:31,680
inference ontology terms being used

839
00:38:31,680 --> 00:38:34,079
rather than variables and then in 2.6

840
00:38:34,079 --> 00:38:36,780
it's um some part of it is written but

841
00:38:36,780 --> 00:38:38,780
like these are the kinds of

842
00:38:38,780 --> 00:38:41,220
augmentations that are always very

843
00:38:41,220 --> 00:38:43,619
welcome for people to to make because

844
00:38:43,619 --> 00:38:45,720
they help us understand

845
00:38:45,720 --> 00:38:48,859
um expected free energy is like

846
00:38:48,859 --> 00:38:51,720
variational free energy but whereas

847
00:38:51,720 --> 00:38:55,079
variational free energy is real time and

848
00:38:55,079 --> 00:38:57,119
it's crunching our approximation with

849
00:38:57,119 --> 00:38:59,220
the incoming data

850
00:38:59,220 --> 00:39:03,020
expected free energy is calculated over

851
00:39:03,020 --> 00:39:06,960
policies which are sequences of action

852
00:39:06,960 --> 00:39:10,619
and it is going to be dealing with

853
00:39:10,619 --> 00:39:13,859
observations that haven't yet come in

854
00:39:13,859 --> 00:39:16,740
so this is where we get to policy

855
00:39:16,740 --> 00:39:17,760
selection

856
00:39:17,760 --> 00:39:20,760
in active inference by thinking about

857
00:39:20,760 --> 00:39:22,740
expected free energy which is like a

858
00:39:22,740 --> 00:39:24,180
prospective

859
00:39:24,180 --> 00:39:27,960
talking about the expected free energy

860
00:39:27,960 --> 00:39:30,300
for different possible policies which

861
00:39:30,300 --> 00:39:33,619
are sequences of actions

862
00:39:34,560 --> 00:39:36,900
there's some more discussion about

863
00:39:36,900 --> 00:39:39,920
special cases

864
00:39:41,099 --> 00:39:43,380
expected free energy

865
00:39:43,380 --> 00:39:45,960
when certain terms

866
00:39:45,960 --> 00:39:50,099
are basically zeroed out

867
00:39:50,099 --> 00:39:55,020
expected free energy becomes resembling

868
00:39:55,020 --> 00:39:57,900
other equations so this is like a common

869
00:39:57,900 --> 00:40:00,060
technique in math

870
00:40:00,060 --> 00:40:03,000
you have some equation and then like you

871
00:40:03,000 --> 00:40:04,980
can generalize it and then a special

872
00:40:04,980 --> 00:40:07,440
case of the general form is this and

873
00:40:07,440 --> 00:40:09,839
then another special case might be that

874
00:40:09,839 --> 00:40:13,020
so for example

875
00:40:13,020 --> 00:40:14,820
we'll come back next week or in the

876
00:40:14,820 --> 00:40:15,660
future

877
00:40:15,660 --> 00:40:19,560
to what exactly the pieces are um

878
00:40:19,560 --> 00:40:21,060
representing

879
00:40:21,060 --> 00:40:24,300
but one can imagine that if pragmatic

880
00:40:24,300 --> 00:40:26,540
value

881
00:40:27,839 --> 00:40:30,780
was irrelevant

882
00:40:30,780 --> 00:40:32,400
then

883
00:40:32,400 --> 00:40:36,119
G would reflect Information Gain of

884
00:40:36,119 --> 00:40:38,339
different policies

885
00:40:38,339 --> 00:40:41,400
if information gained were irrelevant

886
00:40:41,400 --> 00:40:43,380
like it were a totally discovered

887
00:40:43,380 --> 00:40:45,119
situation

888
00:40:45,119 --> 00:40:49,320
then G would reflect purely pragmatic

889
00:40:49,320 --> 00:40:51,060
value

890
00:40:51,060 --> 00:40:54,060
so G is like a generalization it's a

891
00:40:54,060 --> 00:40:56,700
unified imperative

892
00:40:56,700 --> 00:41:00,780
under which certain special cases

893
00:41:00,780 --> 00:41:04,200
resemble variously

894
00:41:04,200 --> 00:41:07,619
information maximizing principles

895
00:41:07,619 --> 00:41:12,920
or utility maximizing principles

896
00:41:13,619 --> 00:41:15,420
Jonathan is there any equivalent to

897
00:41:15,420 --> 00:41:20,099
discounting as in an RL context

898
00:41:20,099 --> 00:41:21,900
do you want to unpack what you mean

899
00:41:21,900 --> 00:41:23,339
there by discounting but then yes

900
00:41:23,339 --> 00:41:26,400
there's definitely some angles on that

901
00:41:26,400 --> 00:41:28,980
yeah sure so um in a reinforcement

902
00:41:28,980 --> 00:41:30,720
learning context

903
00:41:30,720 --> 00:41:32,160
um you know there's there's a clear aim

904
00:41:32,160 --> 00:41:34,380
which is to maximize the expected future

905
00:41:34,380 --> 00:41:36,060
returns

906
00:41:36,060 --> 00:41:37,500
um but generally that's done in a

907
00:41:37,500 --> 00:41:39,599
discounted way and so the agent is

908
00:41:39,599 --> 00:41:42,839
trying to maximize the discounted

909
00:41:42,839 --> 00:41:45,300
expected future returns such that

910
00:41:45,300 --> 00:41:47,760
rewards that it gets in the future are

911
00:41:47,760 --> 00:41:49,440
essentially worth less than rewards that

912
00:41:49,440 --> 00:41:51,420
it gets in the near term so I know that

913
00:41:51,420 --> 00:41:52,980
there isn't the same kind of idea of

914
00:41:52,980 --> 00:41:55,140
rewards as there is in in reinforcement

915
00:41:55,140 --> 00:41:56,640
learning and active inference context

916
00:41:56,640 --> 00:41:58,680
but I'm interested to know whether there

917
00:41:58,680 --> 00:42:01,020
is any sort of idea of discounting that

918
00:42:01,020 --> 00:42:03,060
states further into the future or

919
00:42:03,060 --> 00:42:05,220
somehow less important than states in

920
00:42:05,220 --> 00:42:07,500
the moment

921
00:42:07,500 --> 00:42:09,960
yes here's a

922
00:42:09,960 --> 00:42:13,680
um I'll I'll add that um

923
00:42:13,680 --> 00:42:16,640
uh question

924
00:42:16,680 --> 00:42:21,359
and then add in the um one

925
00:42:21,359 --> 00:42:23,040
Briston

926
00:42:23,040 --> 00:42:25,520
paper

927
00:42:26,339 --> 00:42:30,060
it turns out that yes it's possible

928
00:42:30,060 --> 00:42:33,839
to to get temporally discounted type

929
00:42:33,839 --> 00:42:36,480
Behavior so like a simple way to think

930
00:42:36,480 --> 00:42:39,420
about that would be if the

931
00:42:39,420 --> 00:42:45,359
um pragmatic Drive were urgent then you

932
00:42:45,359 --> 00:42:49,140
would see small pragmatic gain being

933
00:42:49,140 --> 00:42:51,839
approached

934
00:42:51,839 --> 00:42:55,440
at the loss of future gain

935
00:42:55,440 --> 00:42:58,560
short time Horizon models and Urgent

936
00:42:58,560 --> 00:43:00,960
desire Drive

937
00:43:00,960 --> 00:43:04,319
whereas under longer time Horizons

938
00:43:04,319 --> 00:43:08,119
longer time preferences

939
00:43:08,819 --> 00:43:12,839
then you see the kind of behavior that

940
00:43:12,839 --> 00:43:16,680
in utility driven decision-making is

941
00:43:16,680 --> 00:43:18,900
associated with what is called temporal

942
00:43:18,900 --> 00:43:20,579
discounting

943
00:43:20,579 --> 00:43:23,760
so the phenomena the cognitive

944
00:43:23,760 --> 00:43:25,460
behavioral phenomena

945
00:43:25,460 --> 00:43:29,160
exists and and can be recapitulated

946
00:43:29,160 --> 00:43:32,880
but it is not generated with the same

947
00:43:32,880 --> 00:43:34,920
way that

948
00:43:34,920 --> 00:43:36,180
it is

949
00:43:36,180 --> 00:43:39,180
in other places

950
00:43:39,180 --> 00:43:41,700
sure thank you

951
00:43:41,700 --> 00:43:43,260
2.9

952
00:43:43,260 --> 00:43:46,200
is the end of the low road

953
00:43:46,200 --> 00:43:48,359
variational free energy

954
00:43:48,359 --> 00:43:53,099
real time unfolding F of Q and Y

955
00:43:53,099 --> 00:43:56,480
variational free energy our approximate

956
00:43:56,480 --> 00:43:59,780
posterior q q that's the one we control

957
00:43:59,780 --> 00:44:03,060
Y data just like a regression y equals

958
00:44:03,060 --> 00:44:04,560
MX plus b

959
00:44:04,560 --> 00:44:08,280
q and Y discrepancy except it's a

960
00:44:08,280 --> 00:44:10,920
variational free energy it's real time

961
00:44:10,920 --> 00:44:15,300
expected free energy G of Pi

962
00:44:15,300 --> 00:44:21,000
G is a function of policies

963
00:44:21,000 --> 00:44:23,280
and we're going to go into that more

964
00:44:23,280 --> 00:44:26,040
but it is regarding observations that

965
00:44:26,040 --> 00:44:27,720
haven't come in yet

966
00:44:27,720 --> 00:44:31,220
so it has a different structure but some

967
00:44:31,220 --> 00:44:35,400
rhyming with variational free energy

968
00:44:35,400 --> 00:44:38,640
what do they achieve together

969
00:44:38,640 --> 00:44:41,460
where did we get at the end of the low

970
00:44:41,460 --> 00:44:43,700
road

971
00:44:44,760 --> 00:44:45,900
so

972
00:44:45,900 --> 00:44:48,960
we started with Bayes theorem

973
00:44:48,960 --> 00:44:51,420
generative model could be the the thing

974
00:44:51,420 --> 00:44:53,040
jumping out of the hand

975
00:44:53,040 --> 00:44:55,079
perception is inference

976
00:44:55,079 --> 00:44:58,460
that goes back to helmholtz but as

977
00:44:58,460 --> 00:45:02,040
livestream 43 unpacks this has been

978
00:45:02,040 --> 00:45:04,020
understood broadly

979
00:45:04,020 --> 00:45:07,740
across the world for thousands of years

980
00:45:07,740 --> 00:45:09,599
predictive coding

981
00:45:09,599 --> 00:45:11,640
helps operationalize perception as

982
00:45:11,640 --> 00:45:13,260
inference because it's like top-down

983
00:45:13,260 --> 00:45:16,140
predictions bottom-up sensory

984
00:45:16,140 --> 00:45:18,480
observation information that's why we

985
00:45:18,480 --> 00:45:21,000
don't perceive our blind spot

986
00:45:21,000 --> 00:45:23,099
to bring in action

987
00:45:23,099 --> 00:45:24,900
we have to talk about perception as

988
00:45:24,900 --> 00:45:26,760
inference because we do care about

989
00:45:26,760 --> 00:45:28,319
observations

990
00:45:28,319 --> 00:45:30,240
but we have to talk about observations

991
00:45:30,240 --> 00:45:32,819
that haven't actually happened yet and

992
00:45:32,819 --> 00:45:36,200
we have to talk about how our actions

993
00:45:36,200 --> 00:45:38,160
influence

994
00:45:38,160 --> 00:45:41,640
which observations are likely to occur

995
00:45:41,640 --> 00:45:45,000
perception and action as inference

996
00:45:45,000 --> 00:45:47,220
together make up the Bayesian brain

997
00:45:47,220 --> 00:45:50,520
which is like some people it means the

998
00:45:50,520 --> 00:45:52,920
brain does Bayesian computation for

999
00:45:52,920 --> 00:45:54,839
that's a realist take the

1000
00:45:54,839 --> 00:45:57,119
instrumentalist take would be we use

1001
00:45:57,119 --> 00:45:59,940
Bayesian statistics to study the brain

1002
00:45:59,940 --> 00:46:01,200
and

1003
00:46:01,200 --> 00:46:05,700
where exact Bays is intractable

1004
00:46:05,700 --> 00:46:09,020
we can use variational base

1005
00:46:09,020 --> 00:46:12,380
energy-based learning

1006
00:46:13,500 --> 00:46:15,420
that's the end point of the low road

1007
00:46:15,420 --> 00:46:18,680
into active inference

1008
00:46:21,420 --> 00:46:23,819
summary

1009
00:46:23,819 --> 00:46:26,040
active inference is a theory of how

1010
00:46:26,040 --> 00:46:27,540
living artifacts underwrite their

1011
00:46:27,540 --> 00:46:29,880
existence by minimizing surprise

1012
00:46:29,880 --> 00:46:34,040
that's what we would truly want to know

1013
00:46:34,200 --> 00:46:38,099
surprise if you knew how surprised to be

1014
00:46:38,099 --> 00:46:40,980
you would have had to have known

1015
00:46:40,980 --> 00:46:45,380
the actual underlying distribution

1016
00:46:46,440 --> 00:46:48,839
so we usually can't do that so we have

1017
00:46:48,839 --> 00:46:51,020
attractable proxy to surprise

1018
00:46:51,020 --> 00:46:53,760
variational free energy

1019
00:46:53,760 --> 00:46:55,500
so just like we'd want to reduce our

1020
00:46:55,500 --> 00:46:57,119
surprise because that would reflect

1021
00:46:57,119 --> 00:47:00,420
having the most optimally trained

1022
00:47:00,420 --> 00:47:02,760
but not over fit

1023
00:47:02,760 --> 00:47:05,480
belief

1024
00:47:05,880 --> 00:47:09,480
we want to minimize a proxy

1025
00:47:09,480 --> 00:47:12,000
that we know is an upper bound so by

1026
00:47:12,000 --> 00:47:14,819
minimizing the upper bounds

1027
00:47:14,819 --> 00:47:17,579
we're getting closer and closer to what

1028
00:47:17,579 --> 00:47:20,640
we really want to approximate

1029
00:47:20,640 --> 00:47:22,800
the two ways to do that are perception

1030
00:47:22,800 --> 00:47:24,720
and action

1031
00:47:24,720 --> 00:47:26,640
they motivated this

1032
00:47:26,640 --> 00:47:31,380
from a very first principles perspective

1033
00:47:31,380 --> 00:47:34,380
and then extended what is often left

1034
00:47:34,380 --> 00:47:37,260
only in the realm of perception into the

1035
00:47:37,260 --> 00:47:40,040
domain of action

1036
00:47:42,660 --> 00:47:44,819
more discussion

1037
00:47:44,819 --> 00:47:46,680
surprise

1038
00:47:46,680 --> 00:47:48,540
is a construct of information Theory

1039
00:47:48,540 --> 00:47:50,400
it's not necessarily equivalent to folk

1040
00:47:50,400 --> 00:47:51,480
psychology

1041
00:47:51,480 --> 00:47:53,579
and this comes up again and again we see

1042
00:47:53,579 --> 00:47:56,339
it in the ontology for like half of the

1043
00:47:56,339 --> 00:47:58,520
words

1044
00:47:58,680 --> 00:48:00,480
these were words

1045
00:48:00,480 --> 00:48:03,000
that are used broadly

1046
00:48:03,000 --> 00:48:05,520
perception action

1047
00:48:05,520 --> 00:48:07,920
belief surprise

1048
00:48:07,920 --> 00:48:10,440
and sometimes they're perfectly

1049
00:48:10,440 --> 00:48:11,940
overlapping

1050
00:48:11,940 --> 00:48:13,800
with formal definitions other times

1051
00:48:13,800 --> 00:48:17,099
there's a little bit of a Divergence

1052
00:48:17,099 --> 00:48:19,920
and they close

1053
00:48:19,920 --> 00:48:22,440
perception action minimize variational

1054
00:48:22,440 --> 00:48:26,240
free energy and complementary waste

1055
00:48:26,460 --> 00:48:28,079
that's real time

1056
00:48:28,079 --> 00:48:30,240
active inference also minimizes expected

1057
00:48:30,240 --> 00:48:31,800
free energy

1058
00:48:31,800 --> 00:48:34,319
prospectively

1059
00:48:34,319 --> 00:48:36,060
this completes our journey along the low

1060
00:48:36,060 --> 00:48:36,960
road

1061
00:48:36,960 --> 00:48:39,240
in chapter 3 we travel the high road

1062
00:48:39,240 --> 00:48:42,680
which reaches the same conclusion

1063
00:48:43,940 --> 00:48:46,500
on the basis of first principles and

1064
00:48:46,500 --> 00:48:48,359
self-organization

1065
00:48:48,359 --> 00:48:51,720
so we started from we have a Bayesian

1066
00:48:51,720 --> 00:48:52,920
model

1067
00:48:52,920 --> 00:48:57,060
We Didn't Start From organisms need to

1068
00:48:57,060 --> 00:48:58,319
exist

1069
00:48:58,319 --> 00:49:02,339
but it turns out by connecting surprisal

1070
00:49:02,339 --> 00:49:03,560
to

1071
00:49:03,560 --> 00:49:06,180
self-organization or really just

1072
00:49:06,180 --> 00:49:08,880
persistent repeated measurements

1073
00:49:08,880 --> 00:49:12,319
we're going to get to the same place

1074
00:49:14,280 --> 00:49:17,220
all right in our

1075
00:49:17,220 --> 00:49:21,900
five remaining minutes just next week

1076
00:49:21,900 --> 00:49:24,660
we're going to return

1077
00:49:24,660 --> 00:49:27,180
to the to the questions but I hope that

1078
00:49:27,180 --> 00:49:29,400
it's useful to also just walk through it

1079
00:49:29,400 --> 00:49:31,319
because definitely it takes like

1080
00:49:31,319 --> 00:49:32,940
multiple

1081
00:49:32,940 --> 00:49:35,520
readings and interrogatings

1082
00:49:35,520 --> 00:49:37,859
and there's things that like you'd want

1083
00:49:37,859 --> 00:49:39,540
to know in chapter two

1084
00:49:39,540 --> 00:49:41,880
that are going to come up later or come

1085
00:49:41,880 --> 00:49:44,220
up elsewhere in other papers there's

1086
00:49:44,220 --> 00:49:45,420
just no

1087
00:49:45,420 --> 00:49:47,040
single

1088
00:49:47,040 --> 00:49:49,740
Road or path let alone one that would be

1089
00:49:49,740 --> 00:49:51,960
appropriate for all Learners and their

1090
00:49:51,960 --> 00:49:53,160
preferences

1091
00:49:53,160 --> 00:49:55,560
so hopefully it was useful

1092
00:49:55,560 --> 00:49:59,400
to like look over it and see it that way

1093
00:49:59,400 --> 00:50:02,940
next week we're going to return a lot to

1094
00:50:02,940 --> 00:50:04,319
the questions

1095
00:50:04,319 --> 00:50:05,400
so

1096
00:50:05,400 --> 00:50:08,520
if anybody wants to do

1097
00:50:08,520 --> 00:50:10,859
addition of questions it's always great

1098
00:50:10,859 --> 00:50:13,260
you can jump into the answers and

1099
00:50:13,260 --> 00:50:15,839
discourse as I see many do

1100
00:50:15,839 --> 00:50:18,480
and add things you can upvote what is

1101
00:50:18,480 --> 00:50:19,619
interesting

1102
00:50:19,619 --> 00:50:22,140
you can enrich what people have written

1103
00:50:22,140 --> 00:50:25,380
by just replacing where they use a plain

1104
00:50:25,380 --> 00:50:26,880
text

1105
00:50:26,880 --> 00:50:28,560
with something that you know could be

1106
00:50:28,560 --> 00:50:31,020
tagged just like that

1107
00:50:31,020 --> 00:50:33,839
so there's all kinds of ways that like

1108
00:50:33,839 --> 00:50:36,359
wherever chapter two is sitting with you

1109
00:50:36,359 --> 00:50:38,280
now

1110
00:50:38,280 --> 00:50:40,680
either by generating new questions and

1111
00:50:40,680 --> 00:50:43,920
discourse you can add notes on chapter

1112
00:50:43,920 --> 00:50:45,480
two

1113
00:50:45,480 --> 00:50:46,680
um here

1114
00:50:46,680 --> 00:50:49,559
and then also in the math group which we

1115
00:50:49,559 --> 00:50:52,500
haven't been Super Active on we've done

1116
00:50:52,500 --> 00:50:55,140
some laying out of just the mathematical

1117
00:50:55,140 --> 00:50:57,180
skeleton

1118
00:50:57,180 --> 00:51:00,480
of chapter two as an example of how that

1119
00:51:00,480 --> 00:51:02,339
could be done so but in our closing

1120
00:51:02,339 --> 00:51:04,920
minutes just we can have any closing

1121
00:51:04,920 --> 00:51:07,460
thoughts

1122
00:51:22,339 --> 00:51:24,720
okay just all anyone can raise their

1123
00:51:24,720 --> 00:51:27,900
hand but um so in in relation to the

1124
00:51:27,900 --> 00:51:29,579
temporal discounting and reinforcement

1125
00:51:29,579 --> 00:51:31,559
learning Ali also wrote

1126
00:51:31,559 --> 00:51:33,359
reinforcement learning models opposed to

1127
00:51:33,359 --> 00:51:35,579
active are not applicable to perceptual

1128
00:51:35,579 --> 00:51:38,579
tasks that do not involve Rewards or

1129
00:51:38,579 --> 00:51:41,940
show significantly variable Behavior so

1130
00:51:41,940 --> 00:51:44,940
there's there's a lot there

1131
00:51:44,940 --> 00:51:47,819
in situations where pragmatic value

1132
00:51:47,819 --> 00:51:51,359
isn't a key imperative

1133
00:51:51,359 --> 00:51:53,520
even if that's just a sub-task like

1134
00:51:53,520 --> 00:51:56,040
isocating is driven by Information Gain

1135
00:51:56,040 --> 00:51:59,099
yes our eyes dwell on what might be seen

1136
00:51:59,099 --> 00:52:00,599
as rewarding

1137
00:52:00,599 --> 00:52:04,200
but it's Information Gain that is the

1138
00:52:04,200 --> 00:52:06,300
imperative for the isocating

1139
00:52:06,300 --> 00:52:07,740
and so

1140
00:52:07,740 --> 00:52:10,800
where pragmatic value or reward or

1141
00:52:10,800 --> 00:52:12,900
utility functions are either

1142
00:52:12,900 --> 00:52:15,839
non-existent or or flat

1143
00:52:15,839 --> 00:52:18,480
reinforcement has a hard time

1144
00:52:18,480 --> 00:52:20,880
and that leads to all these contrived

1145
00:52:20,880 --> 00:52:23,400
reinforcement bonuses and novelties oh

1146
00:52:23,400 --> 00:52:24,960
maybe we'll pay them for this and we'll

1147
00:52:24,960 --> 00:52:26,339
do that

1148
00:52:26,339 --> 00:52:28,380
um and significantly

1149
00:52:28,380 --> 00:52:29,160
um

1150
00:52:29,160 --> 00:52:33,839
and um learning that reward function is

1151
00:52:33,839 --> 00:52:35,280
very difficult

1152
00:52:35,280 --> 00:52:37,700
and learning that reward function

1153
00:52:37,700 --> 00:52:39,839
especially in like a partially

1154
00:52:39,839 --> 00:52:43,260
observable or dynamic or volatile

1155
00:52:43,260 --> 00:52:46,079
environments ultimately you need to

1156
00:52:46,079 --> 00:52:48,359
reward the learning anyway

1157
00:52:48,359 --> 00:52:51,960
so then you end up coercing what we

1158
00:52:51,960 --> 00:52:54,480
would just call epistemic Value

1159
00:52:54,480 --> 00:52:59,180
into a reward learning framework

1160
00:52:59,520 --> 00:53:02,400
so why not just have a generalization of

1161
00:53:02,400 --> 00:53:03,960
reward learning

1162
00:53:03,960 --> 00:53:07,579
where in fully observed situations

1163
00:53:07,579 --> 00:53:10,619
pragmatic value can be unabashedly

1164
00:53:10,619 --> 00:53:12,059
pursued

1165
00:53:12,059 --> 00:53:14,599
but in partially observable environments

1166
00:53:14,599 --> 00:53:18,300
or where the generative model

1167
00:53:18,300 --> 00:53:21,020
is a simplified or coarse graining

1168
00:53:21,020 --> 00:53:24,059
structure relative generative process

1169
00:53:24,059 --> 00:53:26,460
why not just authentically pursue the

1170
00:53:26,460 --> 00:53:28,380
epistemic value

1171
00:53:28,380 --> 00:53:31,140
which is what you can do with g

1172
00:53:31,140 --> 00:53:33,660
but can only be done in a contrived or

1173
00:53:33,660 --> 00:53:35,520
an ad hoc way

1174
00:53:35,520 --> 00:53:39,180
if you have reward as your unified

1175
00:53:39,180 --> 00:53:41,779
imperative

1176
00:53:45,660 --> 00:53:47,460
that's not to say reinforcement learning

1177
00:53:47,460 --> 00:53:50,700
models are not effective that again that

1178
00:53:50,700 --> 00:53:52,440
the agent can't play chess or that the

1179
00:53:52,440 --> 00:53:54,599
startup wouldn't work

1180
00:53:54,599 --> 00:53:57,780
it's just to be clear that the the

1181
00:53:57,780 --> 00:54:00,059
adequacy of a model in the world

1182
00:54:00,059 --> 00:54:02,220
is an empirical question

1183
00:54:02,220 --> 00:54:06,180
and especially in um a textbook the

1184
00:54:06,180 --> 00:54:10,098
points are often being made in principle

1185
00:54:18,720 --> 00:54:20,520
great well again

1186
00:54:20,520 --> 00:54:23,460
thanks for all these

1187
00:54:23,460 --> 00:54:26,000
questions

1188
00:54:26,700 --> 00:54:29,280
I hope that we can

1189
00:54:29,280 --> 00:54:32,040
um rejoin next week

1190
00:54:32,040 --> 00:54:35,579
with some thoughts and ends

1191
00:54:35,579 --> 00:54:38,880
resources and other questions added in

1192
00:54:38,880 --> 00:54:40,740
there's just going to be a lot of fun

1193
00:54:40,740 --> 00:54:45,379
ways to take these different questions

1194
00:54:46,319 --> 00:54:48,960
and we'll come back to them so

1195
00:54:48,960 --> 00:54:52,260
thank you everyone and if you want

1196
00:54:52,260 --> 00:54:55,260
um in the following hour you can come to

1197
00:54:55,260 --> 00:54:57,480
the Discord and we're just going to

1198
00:54:57,480 --> 00:54:59,460
continue just talking and chilling about

1199
00:54:59,460 --> 00:55:04,579
education so thank you and farewell


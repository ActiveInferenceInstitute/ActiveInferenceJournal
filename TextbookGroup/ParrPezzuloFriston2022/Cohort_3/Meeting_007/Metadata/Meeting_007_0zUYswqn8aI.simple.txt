SPEAKER_01:
all right greetings thanks for joining everyone it's march 8th 2023 meeting seven for cohort three we're in our second discussion on chapter three in the textbook and last time we went over the whole chapter

kind of ran through it but many things that we can return to and many questions that people have on chapter three so to begin with does anyone whether they wrote the question previously or not do they want to just like share something from their reading about three or some insight or question about three or whatever else you want to share right now

Jonathan, and then anyone else.


SPEAKER_03:
Yeah, I think I'm particularly interested in the question there about the graphic with the distributions.

I don't know if Francois is around at the moment, but I was talking with him about this question.

There are some things we're a bit confused about, so that would be very interesting for me to go through.


SPEAKER_01:
This one, the pink one?

Yeah.

All right.

Thank you.

any other just general comments otherwise we can just start with this 3.2 all right all right so figure 3.2

so for the top graphs these are points sampled from distributions mu or x given b for different values of b blanket states on the x-axis this is fine for the bottom left graph what is meant by the average internal state

the average value of mu for all the points in the top right graph which are just sampled from conditional distributions so to get exact value we would have to take infinite samples or integrate my question is how exactly do we from the bottom left plot identify the average x and variance around it

then for the bottom right plot what exactly is the expected value that x is conditioned upon the subscript is p of mu given b do we select a specific b or double integrate over b as well all right great question does anyone want to give a first thought on how anyone can just describe the the graph as they see it

or give a thought on how from the bottom left graph the bottom right is generated uh jonathan


SPEAKER_03:
Yes, maybe I can talk through what I think and what the confusion for me was.

And so if we go to the top graphs there, we're clearly looking there.

So X here is sort of, for instance, an external temperature and B is the sensory state.

And so it says, how is the external temperature conditioned on the sensory state, even though there's not a causal relationship in that direction?

And then how is the internal state determined by the sensory state, by the blanket state?

And so I think these two make sense.

And then indeed the bottom left, I think also makes some sense in that you can take some particular b and you can say, what's the distribution in both mu and x over b?

And that will give you this graph or any particular value of b.

I think for me, the confusion here lies in, so the bottom right plot there is written as the expectation where the distribution is over its mu given B. And so it's not quite clear there what mu given B would normally, I would normally think of that as you give it a particular value of B and from that it's an expectation as a distribution over the mus and the expectation

thereby over the mus.

But it's not clear here that this is actually for a given B. So the question I think is, is that expectation at the bottom right really over mu given B, or is it just over mu?


SPEAKER_01:
yeah great question I'll try to give a first thought but if anyone sees any other mathematical way go for it I believe it is for a specific B so we're looking at this crosshairs where we'll just continue with this temperature but we're dealing with a with an agent whose mapping is might be like a little bit wacky with temperature um so when mu is 50

that is corresponding to a blanket reading of negative five.

So if all we got was a B reading of negative five, our maximum likelihood estimate for mu would be 50.

Now, so we're conditioning on a particular observation of B. Now, conditioning on mu being 50

we have a mean and a variance of just this.

Thank you.

Amy, I am recording just on OBS, not Zoom.

So it didn't make the announcement, but indeed it will be uploaded here.

So condition on mu being 50, this is where we look at the mean and the variance of this X on mu regression.

So for mu being 50,

x is centered around negative six with a variance that's just literally the mean and the variance of this scatter plot that's the empirical way to say it would just be you just look at the scatter points that are in this column and this is the gaussian that's parameterized from this slice


SPEAKER_03:
So that certainly makes sense.

I think the question there was, so the mu equals 50, it's saying, I think it says in the caption that it's sort of over the expectation of the mu.

So if we look at the top right plots, that mu equals 50, it's not quite clear where that is coming from.


SPEAKER_01:
I agree.

I think having another crosshairs here to reflect that that was the B that gave us the 50 mu.

The crosshairs being here, it actually, it does identify a crosshairs here.

And it identifies a, and then if B is negative five, let's just say, it identifies back onto this line, a crosshairs right here, which is at, surprise, surprise, negative six plus or minus one.


SPEAKER_03:
Okay, so really it's sort of flowing around from top left.

It's saying the average temperature outside is minus six or thereabouts.

What does that correspond to in terms of B?

What does that correspond to in terms of mu?

And then you get the distribution from that.


SPEAKER_01:
yes perfect and that's how the markov blanket even though it's defined as no uh no direct influence of internal and external states no telepathy no telekinesis so all perception action mediated through the blanket yet we still are able to talk about internal states as reflecting beliefs about external states

but with this partition and all of the partitioned states, internal, external, and blanket in the simpler Perl partition, or internal, external with the Friston blanket with perception and cognition as the two kinds, I mean, sorry, perception and action as the two kinds of blanket states.

So here's just a sensory example.

Awesome.

Yeah.

Any other thoughts or question on this one?


SPEAKER_04:
definitely helpful to have that crosshairs uh amy or ali uh well actually i think i kind of disagree with um

with the final conclusion that there need to be crosshair in the top two diagrams, because I don't know, I might have misunderstood this, but I think the top two diagrams actually shows, you see here,

One axis is just the blanket state and the other axis is external and the other one is just internal state.

So here I think the blanket state is generated from averaging over

a generating function that generates that blanket state.

But in the bottom left diagram, we deal with some specific states.

I mean, so for instance, here, what exactly is the value of x when the internal state is

say 50 right but i don't know if it makes sense to add those crosshairs into the final into the top two diagrams given that they just show the uh the shape the statistical shape of the blanket states with respect to external states and the internal states so

not sure uh where we uh diverge in our conclusions but um yeah that i mean this figure as it is uh somehow makes more sense to me than uh if there's an there's some added crosshairs in the top two diagrams


SPEAKER_01:
i i think i hear you on this these are learned conditional associations on the top and this is the this is the actual particular inference question so it doesn't necessarily um clarify it but it does kind of locate us in point space with x b and mu

since these are like different views on the same particular partitioned system.

Jonathan?


SPEAKER_03:
Yeah, I think the reason I think it's important is because when it says here E, the expectation value taken over the distribution of mu given B, this is actually for a particular value of B that has been chosen.

And from the bottom left plot, you can't see what that B is.

It's actually for a specific value of B, if I understand correctly.


SPEAKER_01:
yeah it is definitely for for a specific value so cool good question otherwise this would have a different shape it wouldn't just be a galaxy around this it would be like kind of like a mountain range okay just gonna um

On page 52, the sum part gave me the impression that perception is actually a passive process.

It brings action to the game, but it is not active itself.

Does it make sense?

Maria, do you want to add more?

Or we'll look at 52.


SPEAKER_02:
It's actually that.

It was the impression I had when I was reading it.

I'm not sure if I had the wrong impression or just...

the writing from Thomas Barton at all is different from the writing from Jacob Howie, for instance.

Because Jacob Howie would say that perception and action is more or less the same thing.

So I don't know.


SPEAKER_01:
yeah yeah this is a great and and subtle point so we're talking about um generativity and action and perception are these the same or are there distinguishments amongst them so um it's an interoceptive sensor with glucose or neuropeptide or something and then we're doing inference on our hunger status so that's perception

um so for any given beliefs about our hunger and for any given precision or any given interoceptive input we're going to get like some surprise value so surprise value is just how surprising that data point is given the priors it's in some information theoretic unit so whether you're as minimally surprised as you can be

or whether it's super surprising there's some surprise value associated with that observation that um that discrepancy that surprise value can be seen as the um the compromise between like this actively generated top-down prior and and a received sensory information

So there's sort of a generativity, there's the tale of two densities, purely perceptually, but we're not talking about motion or movement yet.

So perception can describe surprise and you can learn, you could update, you could say, oh, maybe I am hungry, or maybe I'm not hungry.

So you can modify your top-down prior, which is like a form of covert action to reduce surprise

However, it is not inactive action.

So a purely perceptual, like a predictive processing model doesn't have to have action.

Like in Livestream 43 that Maria and I did, most of the paper was on the predictive processing in the purely inferential case.

So you see all the top-down priors, bottom-up information.

And then just in the last section, it's like, okay, now we're going to throw in action as a parameter, it's active inference.

So I think that this is saying in the purely perceptual predictive processing setting, it is still the case that surprise is being just conversationally actively generated, but that active generativity or responsivity of the predictive processing hierarchy

is uh not necessarily exactly the same as going out to get dessert and in active inference there's like the learning and perception way to reduce free energy and then there's the actual action selection part now there might be special cases where the only actions that the agent has are

internal or covert affordances like attention and learning but in the general case and in the most um kind of manifest form of action usually we're talking about bodily action does that address it Maria


SPEAKER_02:
yes yes i think that the main difference is predictive processing and activating because that was my confusion yeah because i had the impression that perception perception was almost like a computer that was computing the kl divergence between beliefs and sensations whereas the action was another thing but you know

Again, I think that I was misunderstanding the differences between predictive processing and active inference.


SPEAKER_01:
Thank you for your answer.

Active inference is a predictive coding or predictive processing architecture where manifest action or internal covert action is itself a parameter that we do that kind of inference on.

you could also have a purely passive predictive processing system like no eye saccades but still talking about priors and perception on vision and then one could even argue that that model still has like kind of this generativity because there is the top-down generated visual um what is perceived

that's not the same type of action or the same level of embedding with bodily action as if your predictive processing model of vision included isocating and so that's one of the important uh developments of active inference beyond predictive processing and bayesian brain that's the pragmatic turn is like

yeah it's not just that there's this generative model that's always learning and updating it's actually one of the features one of the parameters in that generative model has to be self in world consequences of action in world from a sensory perspective because the main question that a predictive processing model for example a vision has to address

um if you're like in the machine learning it's like well it has to classify digits it's like okay yes it does but the bigger question is where to look and that's the kind of model that biological organisms are engaging with where should I isocade and then yeah we reduce our uncertainty about the visual scene with isocading and then we could classify the digits

But if we only set up the task as being like digit classification, then it doesn't need to have this active sampling feature.

Ali?


SPEAKER_04:
Yeah, I think a slightly more nuanced distinction between the difference between predictive processing, predictive coding, and other frameworks with active inference is laid out in chapter 10, especially beginning section 10.4, page 198.

But just very briefly,

Although in some predictive processing frameworks, such as embodied or action-based predictive processing, perception and action can somehow be correlated with each other.

if not exactly synonyms to each one.

But in active inference, there's this subtle distinction that says both perception and action

are inferential processes.

So they are just analogous in terms of being this kind of inferential process.

So they're not exactly equivalent to each other as some predictive processing advocates may claim.


SPEAKER_00:
Great.


SPEAKER_01:
Yes, they're analogous in their computational handling.

but they're not identical.

They're not functionally or semantically identical.

Action and perception are different concepts, but we have a unified imperative that talks about perception slash learning and action as both being in the common

inferential game of re-energy minimization which bounds surprise of the generative model um okay um a short question does agent need awareness of preferences can preferences be learned i think this may have come up at a uh um office hours so does the agent need like experiential awareness of preference you can construct models that have this feature

but it's not a feature of the kernel of the model.

So single layer act-inf model, no, you don't need to even go there, but you could make a nested model where the higher layer is like attention or meta-awareness.

Can preferences be learned?

Yes, here's a paper of Sajid et al.

Preference learning in general though is a very challenging problem because what if you,

um how do you even know what you would want to learn so it also quickly moves into this kind of like hierarchical modeling setting where when you write out the model you'll have a lot of clarity about what is being learned and what isn't being learned but in the general um kind of pre-model specification space

these questions as with many many others it's like in principle yeah you could whether is or isn't that is going to be about any specific model but it's a great question okay other upvoted ones and then we'll continue down is there a minimal example of a thing defined by a Markov blanket

example a cell seems to satisfy such definition same with proteins and other structures is it correct to say a thing is just the temporary persistence of a markup blanket are all things then just non-equilibrium steady state processes as per figure 3.3 do we need to be more strict in saying these things are only self-evidencing things rather than things in the colloquial sense who has any thought on this

sure there's there's uh many takes slash ways to say it but the only piece i would add here a thing is the modeled or measured persistence of a markov blanket because markov blankets exist on maps markov blankets are features of base graphs they're common features of base graphs every base graph where it's not fully connected so anytime there are nodes that aren't directly connected

there are Markov blankets that insulate those two sets of nodes.

So it's actually like not a rare or exciting event on base graphs.

And also it doesn't necessarily demarcate the spatial temporal boundaries of an embodied agent.

The thing, as we're gonna be talking about it,

is for the given spatial and temporal scale and signal to noise and all of that that the observer is using if it's identifiable as a thing even a process thing like a river or something to the observer then yes it had to have persisted as that thing within that spatial and temporal scale that the modeler was making that map of

so maps are not observer independent markov blankets are on maps yes every level of organization and of multi-scale complex systems will fit that definition are all things non-equilibrium steady state processes do we need to be more strict in saying these things are only self-evidencing

brings us to our favorite image in the um strange kinds article so there are things or particles here particle is the same as thing that are inert and so no appeal to self-evidencing is needed to describe their persistence

um or yeah i'll say that um there are active particles whose actions can be understood as self-evidencing again without an appeal to awareness or consciousness or anything like that and this is what is called sentience cueing the ongoing debate again so not qualia sentience but sensing and adaptively acting

such as um some uh like the wood lice that Carl identified in his childhood where they were walking faster in the sun and slower in the shade so they spent more time in the shade and then self-evidencing in the case of these sophisticated or strange particles the self-evidencing includes like self in world with others like me

under nested counterfactuals so that's not present in the base of the model but the base models that we are going to be looking at especially in like the upcoming chapter this is like the lego block

that these much much more sophisticated models are built upon um scott if for some reason i wanted to model a rock using this framework i would probably model it as an inert particle yeah absolutely you could even just draw out the particular partition um here's the the um photons coming off of the rock and they're determined by this other action impinging on the rock

and here's my inference on the rock's weight, or here's the rock on the scale, whatever situation you're going to be talking about it.

Yes, it's partitioned from its environment and it's not taking action selection.

Or one could see its action selection choice to stay in one location as like the simplest case of Bayesian mechanics.

Just like a rock physically being in one location is the simplest location thing of, it's simpler than something falling.

a belief just existing in one location is simpler than it moving.

Ali?


SPEAKER_04:
Yeah, I think we should probably point out the notion of blanket index here as well.

Because you see, markup blanket is not something predefined or even an established

uh value uh because we can define markup blanket uh depending on how complex the system is and by complex here i mean uh it's uh more or less precise mathematical definition i mean uh so for uh as the system becomes more complex and in more in higher dimensional systems

or in other words, when the system has a sparse coupling with its environment, the blanket index approaches zero, but in less complex system, or in other words, in the systems that doesn't necessarily have this kind of non-linearity embedded in them,

and they behave more as linear systems, the blanket index maximizes.

So yeah, it's a kind of a spectrum and we can have different levels of weakness or blanket index embedded to the definition of our Markov blanket.


SPEAKER_01:
yes thank you yeah this is recent uh some advancements and uh but there's a lot of other like discussion of well what about when it's shifting materiality of the boundary or what kinds of and it just there's a lot more to say but it's always good to remember the markov blanket it's not like internal external blanket states are pre-assigned about the world their base graph

partitions so the partitions on a given constructed map never the territory it's just a total um apples and oranges uh apples and maps of apples question markup blankets just like simply are not in the world that's a good first angle and then we can talk more about like which thing is the person modeling and what kind of particle are they modeling it as and so on

just helps to like instantly jump with two feet into the modeling question which is where you are when you're talking about the markov blankets um rather than in the underlying ontological what is the the true substance of what the thing is which is actually still a question that many people have found that the markov blanket helps them get around not to bypass but like to to be sense making in that space

it's not even that it isn't useful for that space it's just that the markup blanket is something about base graphs not about whatever territory you're trying to describe with base graphs living organisms resolve these fundamental biological problems by exerting active control over the states which range from every biological mechanism in the world

a way to understand why automatic mechanisms are active in thinking that these still involve calculating efe over future states but the time horizon of automatic reflexive behaviors is small is there another concept of passive control or is all control active so um and then there's an example provided uh involving like uh the so-called reflex arc so few answers here first in chapter five

we will be looking forward to that essentially exact example where depending on the dopaminergic precision and tone in this dopaminergic region of the mammalian brain there's going to be two paths of policy selection there's going to be one path of policy selection that basically passes habits forward

And there's going to be another path of policy selection where expected free energy, G, is used to sharpen action priors, habits, into action or policy posteriors, which are updated and sharpened according to the extent that they serve to minimize free energy.

So that's a dopaminergic mediated handoff between like what might be seen as sort of just habitual passing forward what has been done versus taking a different lens according to a defined G, functional.

And then to the question of the kettle, here's a predictive processing architecture by which the spinal cord, this is like a cross section of a mammal spinal cord with a butterfly shape

incoming sensory data can be reflexively acted upon without getting passed into this entire cortical hierarchy one key piece to remember just like markov blankets are about maps not territories generative models calculate expected free energy not organisms so

there a way to think about it that involves calculating efe yeah if you write the generative model so that um blood pressure regulation involves an efe calculation the answer is yes if you write the generative model so that blood glucose or blood pressure regulation does not involve efe the answer is no so in principle anything's possible but when it really comes down to it it's always going to be about how you specified your generative model or your thought experiment

is there another concept of passive control or is all control active?

anyone can also give a thought on this but some might say like control theory is the application of action that's the line between like signals processing

and control theory if you uh you really like the temperature to be uh 25 c and you have a time series model or variational auto encoder of temperature all these different things and then sometimes you're like oh it's 25 it's perfect other times it's too hot or too cold like so you're in the signal processing world and you still might have a preference about what signals you're coming um to infer but this kind of comes to maria's question from earlier

And so that can still be a predictive processing architecture.

But when we're talking about control, even though this term often has connotations of like commandeering or like excessive control, that's just a cultural connotation.

It's really the distinction between a signal processing perceptual only model, which still can have that tail of two densities.

and then what is called control theory which is basically any introduction of behavior and that introduces like well why are we acting well to in this framework not maximize reward according to some proposed reward function but to reduce the divergence between our sensory observations and our preferred sensory observations

maximizing model evidence p of y is mathematically maximizing model evidence p of y so we have a given data set y and we're going to have a portfolio of models

each model can then be said to have a certain amount of evidence in the sense like it corresponds to could generate that data better or it recognizes that data with less variance and we want to select in this situation the model from our portfolio or like families of models that we swept over we want to select the models that have the highest model evidence

And that is equivalent to being, to selecting the model that minimizes our surprise about incoming sensory observations.

So it's like if temperature's being generated a certain way, we have a certain temperature data set.

One way to think about this problem is we want to select the mean and the variance for the Gaussian that have the most evidence with respect to the data we have.

Another way to think about this exact same situation is we want the Gaussian such that surprise is minimized as new observations come in.

Surprise is defined as the negative natural log of model evidence.

which has like a lot of features and we'll kind of come back again and again to these features of logs.

And then as to the specific question, sometimes we call it a fancy I, but it's called fracture.

So it's an I for information, even though it looks kind of like a J, but it's fracture is the name of the font family.

Importantly, the agent's generative model cannot simply mimic external dynamics.

What is meant by this statement?

Does this mean the agent's prior does not need to be updated since it has a perfect model already?

If so, how could an agent defined by the generative model and the Markov blanket have a perfect model when what is available is its local sensory states?

Okay.

um this is in the neighborhood of the good regulator theorem from cybernetics so adaptive cybernetic systems have to be able to have like the requisite diversity but also the ability to be good regulators which means that they have to uh not necessarily make like a digital twin cognitive model of their environment it doesn't need to be uh bit for bit atom for atom recapitulated

However, in challenging environments, the agent will have to act as if it contains a generative model that includes some of the latent and unfolding features of the niche.

So again, the temperature example, if our only imperative of the generative model was to fit temperature as accurately as possible,

you would just fit temperature accurately as possible.

And then when it left your physiological bounds, you just die.

So we wouldn't see that kind of organism around.

So the agent's generative model has this dual imperative of accurate but not overfit perception and adaptive action.

That's that synthesis of the signals and information processing incoming side with the action selection control theoretic outgoing side.

because simply mimicking will not lead to adaptivity.

It's just not what adaptive agents need or want to do.

But you can contrive an in silico example where it does have a perfect internal model of the external states.

Or it could be a fully observable situation, like a game of chess, where the board is fully observable.

Now, you can't observe into your opponent's mind, but again, you could imagine an in-silico environment where there was that kind of visibility.

so there's other aspects to this but basically the agent's generative model doesn't have to be the exact structure of the external world like temperature in the world could be a continuous variable and the agent's generative model could just be like too hot too cold or just right so it doesn't need to mimic the structure of the external world

But there has to be some semantic resemblance.

If temperature is indeed something that is relevant for survival, there's going to have to be something like an as-if thermo modeler.

And again, it doesn't need to be lockstep.

It doesn't mean that it's just recapitulating the internal dynamics, but it has to have a cognitive structure that allows it to have actionable

um a representation of the external temperature including like the the consequences of different actions because it's not just a purely descriptive model it's also needing to take actions but as we um

saw in chapter two there's special cases when there's nothing to know or nothing to learn then you have utility oriented policy and if there's no differences in pragmatic utility for different outcomes then you have like novelty or information driven policy so in principle a huge number of special cases can be imagined

but in general it's not the case that the GM simply mimics external Dynamics that would be a contrived setting is more complex and itinerant Dynamics referring to concepts of multi-stability metastability criticality yes definitely

everything with complexity science and itinerant Dynamics just like um continuing to wander maybe never even revisiting maybe revisiting multiple attractors conditional attractors phase transitions all of these kinds of topics so great question

Is it correct to say that Gibbs free energy and Helmholtz free energy belong to the context of equilibrium stat thermo and variational free energy belongs to the context of non-equilibrium thermodynamics?

For a disciplinary answer, ask someone who knows and cares.

In box 3.2,

what they're highlighting is not necessarily the difference between equilibrium and non-equilibrium though that distinction is here rather I believe that they're highlighting that variational and expected free energy are information theoretic quantities whereas Gibbs free energy describes like how much um calories of heat are released when the candle is burnt

And so we say, oh, the candle's burning, it's producing free energy, it gives free energy.

That's what made an irreversible reaction or spontaneous reaction.

But variational and expected free energy are in information geometric spaces.

They're not referring to the same exact referent of thermodynamic systems like heat bath, steam engine, and so on.

It gets blurry because some people are really pushing like pan-cognitivist, pan-informationalist, it-from-bit.

And so people do discuss some relationships at very deep levels between, for example, information engines and heat engines.

So it's a deep hole to go into, but just from the outside, Gibbs free energy is talking about candles burning, variational free energy is talking about informational systems.

Ali?


SPEAKER_04:
Well, again, going back to what we talked in the previous cohort, actually in the last couple of years, there's this...

push towards developing a kind of new mathematical formulation of active inference, which doesn't require the NESS assumption as its precondition.

Previously, and especially from about 2012 to 2019, most of the active inference literature was focused on developing the density over states formulation.

But in the last couple of years, the trend has shifted toward developing the density over path

formulation, density over path formulation of active inference, which doesn't necessarily require Ness assumptions.

So in its most current formulation, we can say that active inference is much more general and also includes even the systems that doesn't obey the Ness assumption requirements.


SPEAKER_01:
yes great and then last question and then we'll look ahead to the coming chapter um okay how does the taking of drugs which flow across the blanket and directly affect the internal states and therefore gm fit within the framework ie the blanket is semi-permeable it is not only sensory and active so blankets are not in the world

you could model and make a model where for example the blood-brain barrier is like blanket states and then the question points out right there are actually the materiality of certain substances is such that just because one state was internal with respect to photons in my map doesn't mean it's internal or secluded with respect to some lipophilic drug so

If you draw out the Bayes graph, like what you're actually talking about in terms of the Bayes graph of what?

What information flow?

Because the Bayes graph is not identifying the spatial temporal or the lipid bilayers.

It's not just anatomy.

It's actually more like an informational physiology because it's describing connections amongst different parameters in the map, not necessarily different like physical articulations in the anatomy.

however more broadly like this concept of semi-permeability that is being addressed with the blanket index and as well as dynamic blankets and it's not sensory and active states which affect the internal states in reality again in the reality of the particular partition on the base graph this is the definition of what the blankets are

However, whether the labeled and assigned nodes in the map have this or that relationship to different embodied territories is the map territory question.

So we can definitely return to these topics, but I think by drawing out a Bayes graph, there'd be a lot of clarity around like what is internal or external to what.

It's not like there's like a highlighter that you can just tag objects in the world.

This is an internal state and that's an external state and that's a blanket.

Awesome, yes.

like correlation is not causation there's just a few where on like the the 700th time hearing it's like oh i guess it isn't okay let's look ahead to chapter four so we are in chapter three

high road notice that chapter two and chapter three are the low road and the high road to active inference so we haven't begun act inf yet but we've gotten to there but right when we were on the outskirts of the city we like pulled away um i i don't know not not really but kind of chapter four is the generative models of active inference so this is going to be like the heart of active inference

after we've taken it from these two roads.

The low road is the how with Bayesian statistics, which doesn't have to be about adaptive, autopoetic, anything.

Bayesian statistics could just be about the temperature.

And then we talked about it from the high road perspective, this imperative for self-organization and persistence, which doesn't have to be implemented with Bayesian statistics.

These two roads come together in the generative models that we construct in active inference, which use the Bayesian statistics and implement a Bayesian mechanics.

Chapter four complements the previous conceptual treatments with a more formal treatment.

We're gonna be talking about free energy and Bayesian inference.

and specifically the generative models that are used in active inference 4.2 bayesian inference to free energy um previously there was a broad framing with the bayesian brain which has been um accepted for some decades if anyone has like a doubt or comes across a doubting person for sure ask for an alternative and always be open but it's like as opposed to what the frequentist brain

So Bayesian brain is a natural framing, not too constraining at all.

It just brings neuroscience in line with modern statistical methods.

And if we take the inactive turn or the pragmatic turn, we want Bayesian brain plus action, which is basically active inference.

They're going to return to Bayes' theorem.

figure 4.1 we're going to have a bit of a discussion on Jensen's inequality and some properties of logs so for any of the current or ex-math professors in this group any thoughts you have on logs and Jensen's inequality would be helpful but we're going to come back to it um

more equations some transformations of logs and model evidence yes exactly the two people who i reference sounds great um that log discussion is going to uh yield the kl divergence section 4.3 is going to be about generative models so to calculate free energy we need three things a data

a family of variational distributions in a generative model so well the cell is minimizing free energy it's like does the cell consist of data a family of variational distributions and a generative model no oh it's lipids and proteins okay it's not minimizing free energy your statistical model of the cell consisting of these three ingredients can minimize free energy so that's like a key map territory piece

Figure 4.2 is going to be like a step-by-step walkthrough.

Really, really helpful to think about this grammar of Bayesian causal graphs.

This is like one event that causes another event.

This is like one event that causes two events.

This is like two things that cause one outcome.

And this is like a chain of V causing X causing Y. So you can think about examples in our own life, but this is like the grammar of what these nodes and edges mean in base graphs.

Figure 4.3 is a classic figure.

It displays on the top, the discrete time generative model in active inference, which is called partially observable because there's this hidden state in observations.

it's called a markov decision process because there's this markovian time property which means that the past only influences the future through the present and it's a decision process because there's this control theory aspect so that's a partially observable markov decision process and then on the bottom is a continuous time generative model discrete time is focused in chapter seven continuous time is focused in chapter eight four point four

is a precursor to chapter seven it's about active inference in discrete time more details 4.5 is a precursor to eight it's a precursor to talking more about continuous time generative models some more details on markov blankets continuous time models generalized coordinates of motion

comes up a lot.

It's like, instead of just representing the location in GPS of the car, it's like location and all of its derivatives.

So it's velocity, acceleration, and so on.

That turns out to be really important for certain dynamical systems.

Acti-Inf as predictive coding plus motor action.

We'll unpack some of these figures and summary.

so chapter four gets to the two big families of generative models that are used in active inference so everything we've been talking about with generative models the two biggest families like vertebrates and invertebrates or any other taxonomic division you want are basically represented in figure 4.3 and they can be hybridized too which happens in chapter eight

so that's chapter four we will um return next week go through the chapter looking forward to any questions um and last point jonathan there's a lot in chapter four have you found two weeks have been sufficient to get the grips with it yes various chapters like it's it's one reason why we take the cohorts multiple times

because it's not just two weeks it's not like if it were just six weeks there'd be enough to get to grips with it necessarily so going through it multiple times and uh just carrying on with the pace we're not grading or testing obviously so just continuing on yeah Ali two weeks deep Ramju um so just carrying on with the schedule and just

But we can definitely think more about this in terms of future cohorts.

Any feedback you ever have, you can add it here.

But many, many things could happen.

know a mathematical pre-learning or a supplement or um a one week per chapter accelerated or a three week per chapter we really don't know so thanks everybody all uploads recording so see you later
SPEAKER_01:
hello it's February 22nd 2023 we're in meeting five for cohort three we're in our second discussion on chapter two today we'll be mostly looking at questions from chapter two hearing people's like thoughts on it written or whatever they want to bring up today and then in the last few minutes we'll look ahead to chapter three

first though wanted to note underneath live meetings on February 27th in five days we will have Thomas Parr join for a live stream hashtag first author so it's at 17 UTC um for likely an hour if anybody wants to join the live stream itself

then just email active inference at gmail and let us know and we'll add you to the calendar event if you want to watch along live here's the or share it and this is like the watch and the rewatch link

then if you have any specific questions it could be something that we've already raised or it could be something more general slash personal so just before we jump to chapter two what does anyone think would be interesting or meaningful to ask thomas

feel free to think about it or type it um here but basically when the day comes it's going to be whoever has um thank you jonathan it's going to be whoever's there in the moment so just email if you want to join on that stream and whichever questions are here slash questions that we've just curated and then if there's time or if anyone is watching live they can put a question in the live chat too

so um we'll continue to explore like what kinds of engagements with Thomas gianni and Carl are best for the book and everything but this is gonna be a lot of fun okay

Well, we have many chapter two questions.

Does anyone have a specific one they want to jump into first or just any chapter two reflection or comment that they want to share?

There's totally time to do that if you want to.

okay let's just try to pick off some comments and questions kind of just drawing drawing from the question posterior how does active inference go beyond the recognition that action and perception have the same inferential nature page 24.

do people think so this is probably referring to section 2.4 um how do we go beyond recognizing well maybe perception and learning have a similar inferential nature maybe we can have a unified framework that models perception and action together and maybe they even have a unified imperative i think the simplest answer is they're different parameters

That's how we move past that recognition.

They're modeled as different parameters in a unified model.

Does anyone want to add another thought or question on that?

okay again anyone else want to pick a specific question to go to otherwise we have many that we can come to okay why express potential energy and expected free energy in the space of log probabilities on page 33

consistent with the notion of potential energy in physics expected free energy is expressed in the space of log probabilities nice question any first thoughts let's look at where it was in the textbook so we're talking about taking the expected free energy of each policy

so equation 2.6 expected free energy over policies that's sharpening our action prior our habits into the action posterior that we're actually going to sample our actions from does anyone want to give a thought on that question


SPEAKER_02:
Yeah, so I'm in one of my math modules.

We're currently studying Bayesian maths.

And one of the benefits of using log probabilities is it makes it easier to calculate derivatives when you're modeling Gaussians.

And a second point was like computational benefit.

You might encounter floating point errors when dealing with, I think, marginal evidence, which can be very small for some values.

So calculating the log version of those

reduces the likelihood of encountering floating-point issues and precision errors.

That's what I'm trying to say.


SPEAKER_01:
Awesome.

Thank you.

Yeah, the log is something that's very simple, whether it's log base 2 or ln, the natural log.

It does a lot, and it also has one...

foot in the kind of computational tractability benefits like being able to keep track of small probabilities like if you have 100 options then most of them are going to be small like less than one percent now if you have a thousand options most of them are going to be small like on average less than a tenth of a percent

so as you get many options or probability distributions you want to be able to keep track with high resolution of these very small numbers on real computers so it helps purely pragmatically in the kinds of computers we have today then a little bit more mathematically the log connects to um jensen's inequality and also to um

information theory and self-surprisal and then something that's unpacked a lot more in bayesian physics and less in this textbook itself is like physics uses kind of like gravity is like a self-information not exactly you know read where it's actually connected but it turns out that like the ball rolling to the bottom of the hill to minimize the potential energy

has a lot to do with the, quote, ball rolling to the bottom of the hill of a statistical distribution if we think about self-surprise, self-information in a log probability space.

Ali?


SPEAKER_03:
Yes, and one more advantage of using log probabilities or more generally to use logs instead of the actual parameters is to convert some of the multiplications to addition and hence trying to convert at least parts of the nonlinearities of the equations to linear equations.

In that case, well, of course, we know dealing with linear equations is much more tractable and also much more, I mean, linear techniques has much more developed than just dealing directly with nonlinear equations and nonlinear calculations.


SPEAKER_01:
Thanks.

Yeah, two of the log properties that come into play a lot.

And you can go back and forth, like they're used in both directions.

Here, log of m times m. So log of 5 times 10.

Log of 50 equals log of 5 plus log of 10.

So you can break multiplication inside of the parentheses.

You can break it into two logs with the same a value.

analogously if you have like log m minus log n it's the same as log of m divided by n and these kinds of maneuvers and rewritings are used greatly and it turns out that the kl divergence

which is one divergence measure that's used.

So just like you'll know that you're doing better in the least squares, sum of squares, linear regression world.

You know that you're doing better when the sum of squares is going down.

In the KL divergence world, variational inference world, your model is getting better fit when the KL divergence is going down.

Again, there's more detail, but these all are empowered by the ability to have something within a log that's multiplied, which also gives this the interpretation as potentially a self-surprisal, self-information, and then separate that out and do addition.

So it has computational benefits.

has statistical benefits and has also a deeper connection to bayesian physics okay again not not okay i'll let you go for it


SPEAKER_03:
Sorry, before going into the next question, I just wanted to comment on the previous question, the question number two.

Sorry, I raised my hand a little bit later, so you didn't notice, but yeah, about the relation between the perception and action.

So very briefly,

Alan Berthoud actually proposes that perception is simulated action.

So it basically implies that perception and motion are kind of irreducibly interconnected.

And actually, he also claims that the former, I mean, the perception defines the action.

So

Yeah, it's also another viewpoint, which is way before

the development of active inference and pre-energy principle.

But these ideas of the interconnection between the action and perception has been discussed throughout the history of cognitive science, and especially the last few decades in terms of embodied cognition and so on.

But in my view, Alan Berthouze's

claim about this specific interconnection between perception and action and basically making them equivalent to each other in terms of their, one of which being the simulation of the other is an interesting viewpoint to have in mind beyond the inferential nature of them.


SPEAKER_01:
nice yeah like even qualitatively we can draw connections with different threads like Helmholtz perception is unconscious inference okay no just save that thank you for that okay now what's the pragmatic turn well that's about in the Bayesian brain it's about thinking about action alongside inference so it's like okay we have Helmholtz perception is unconscious inference

Now let's extend that to action and perception as unconscious inference.

That's still a qualitative idea, and ACNF and FEP are helping us develop quantitative methods that are compatible with those kind of qualitative ways of thinking about perception, cognition, and action.

okay upper bound I think we came to last week so what is meant by bounded computational and mnemonic resources anyone have a thought on this or related idea

this is a great answer just you can write an equation you know 10 to the 10 to the 10 to the 10 to the 10 to the 10 to the 10 power but just because you can write it on a piece of paper doesn't mean that there's like a calculator that has that much ram it might just you know the number of digits that you need to store might be outside of the heat capacity of earth or something like that so

We want to be able to express models in an open-ended way in principle.

Also, especially if we're talking about what kind of inference is optimal, it's important to take this probably approximately correct approximate Bayesian computation heuristics

angle that respects the actual time space trade-offs that are fundamental for any actual system and any model that we make of a given system like it's not behaving accurately and you know if it remembered every single pixel

it's like well yeah but there's not enough space for it to remember every pixel so it might be doing something that's actually very adaptive involving forgetting the vast majority of information and there's there's no trade-off there but if we can only state the models in principle and we don't keep it bounded with the um limitations of real systems

and the approximate Bayesian computation framework, then, you know, maybe the scripts don't run because, and that's also really, yeah, there's other angles there.

Any thoughts or comments on this, or this is a good answer?

Also kind of a nice link to mnemonic resources, like Memory Palace.

like what if you didn't have to remember every single word in the poem but you remembered some nested model where the higher part of the nested model involved touching parts of the environment or walking through an imagined space are the cognitive constraints or challenges associated with those types of mnemonic resources that cue one's action

does that have some properties that are beneficial relative to kind of like the naked memory how does active inference

incorporate feelings into the framework theory in the apple frog example when the human believes she is confronted with an apple before the observation is she feeling hungry or low on glucose when she subsequently observes the thing jump does that observation include the feeling of disappointment the release of stress hormones Etc what would anyone say on this Jonathan and then anyone else


SPEAKER_00:
Yeah, so I wrote the thing below there that I think that there's a strong relationship there between feelings and observations in the sense that we observe our bodily states.

But I think this ties in nicely with some of Mark Solman's work on affect and consciousness.

Yeah, so I think that, yeah, one should think about bodily states and the inferences we make about the state that we're in based on those observations.


SPEAKER_01:
Nice, thank you.

Anyone else?


SPEAKER_03:
would put this in the category oh yeah ali first go for it uh yeah i just uh wanted to point out that there's actually a theory of emotions or feelings developed

within active inference framework, namely deeply felt affect theory developed by Casper Hesp and Ryan Smith and others.

So if anyone's interested in exploring how we can theoretically frame feelings purely in terms of active inference, I would highly recommend reading this fascinating paper

from I guess to 2020.


SPEAKER_01:
Yes, thank you.

I would put it in the category of possible to model, like you could make a model about it, you could write a paper about it, but not intrinsic within the simplest kernel.

Because feelings also, as Psalms will be keen to point out, feelings implies a feeler.

And so that is bringing in the qualia, phenomenology, experience, awareness topic.

But the model can also be stated without those considerations playing an outsized role.

But if someone's making a map, like a generative model,

and they want to include those variables there are some strategies that people have been implementing for years to engage with affective states in a computational psychiatry approach like and just an empirical strategy might be like we start with a simple model

then we find oh there's like this interesting residual pattern where people are um overestimating how likely apples are around noon and then we get a new sensor in our laboratory now we're measuring glucose or we um are getting some health data so now we can differentiate people who have this metabolic condition from that and we're finding that they have a different inference strategy

so it's not about just sketching it and just saying up it's a done deal because it's on paper it's about that iterative modeling and a plurality of possible ways that feelings could be implemented though I think Psalms and others will say that they have um in silico feeling agents so that again opens up a whole nother level of discussion

all right when quantifying dissimilarity between the prior and posterior so for example bayesian surprise so the surprise alone just surprisal is about one data point coming in how surprised are we measured in information theoretic units like a knot or a bit

But if we're talking about Bayesian surprise, we're talking about how much the prior shifts to become the posterior as a function of the observation coming in.

Why do we choose the KL divergence over other measures?

Is it just a canonical choice?

Does it matter?

Since KL divergence is asymmetric, is there any reason we have chosen this particular order?

Is there any intuition motivating the choice?

Does anyone want to give a first thought?

okay one example of an alternative measure here's with um sajid at all this is getting um kind of as far as we might want to visit it here but

it's important note the KL divergence as the questioner notes it's not actually a measure in the proper metric sense and i don't mean like european metric system like if it the measurement between a and b has to be the same between b and a for it to be a distance measure

um whereas the kl divergence it is a quantitative value but the divergence from a to b is not necessarily b to a in fact it's generally not so kl divergence is an asymmetric divergence quantification now it's possible to have a very closely related divergence called the jeffries

which is just the KL plus the other direction.

So it calculates like, it's basically the KL of A to B plus the KL of B to A. And so that is symmetric.

Hence, that is a real distance measure.

Other measures have been proposed, the Raney, and they may discuss other options as well.

Why is it used?

in the context of variational Bayesian inference and probability it just turns out that the log properties work and um in equate one of the earlier equations which maybe Ali will remember specifically like Ali go for it and I'll think about which equation is the one with the KL definition


SPEAKER_03:
It was equation 2.3.

And yeah, also, there's a lemma called Neyman-Pearson lemma, which actually proves that the most efficient way to distinguish between two probability distribution based on a given observation is KL divergence.

So it's not just an arbitrary choice, but

I believe it's a proven theorem.

I mean, it's the most efficient method for comparing those two probability distributions.


SPEAKER_01:
Thank you.

also just to kind of give one one related angle to connect like why this whole discussion matters for those who saw that as in the weeds it definitely is um the way that that um pragmatic value is gonna be determined

is ultimately through something KL is used for information and on the pragmatic side but this is how when we talk about expectation and preference and we want to evaluate policies based upon how well they align with our preferences that is going to be operationalized by reducing a KL divergence between our preferences and observations

And that task of optimization of the incremental reduction of divergence relative to our generative model of incoming observations, that's what we want to solve.

And it also turns out to have some good computational properties.


SPEAKER_00:
So, Jonathan?

Yeah, I was just going to say, just in terms of the the equation that, sorry, excuse me, for the expected free energy.

Although I feel very comfortable in general with with mathematics and physics, I find it very hard to get intuition from that equation.

I think it would be useful, maybe it's something that I'm going to do, but to actually run through a sort of really concrete example at that stage.

with numbers so you can see how the two terms differ for a particular example.

I think that would be quite enlightening for me to go through at some stage.


SPEAKER_01:
Thanks.

I know some may have notebooks that do that.

So however people want to share that or build that, that sounds good.

Yeah, many ways to go.

But great questions.

thank you for who's sharing these so KL plays nicely with logs in the sense that it's defined essentially in terms of the divergence between Q to P again not the distance between Q and P the divergence between Q to P is defined as triangle equals

expectation over the variational distribution on hidden states so over the map not over the territory expectations over the map so the center of gravity of our map's inference on hidden states of the difference between the log Q and P the log

again connects us to all of those nice mathematical computational statistical properties of logs and um has this divergence interpretation too expectation of a difference of logs is a kl divergence and the reason why it's not symmetric is because um p to q would be flipped and so it'd be a different number

unless they were literally both zero.

Under some conditions, one can reduce variational free energy to other notions.

Which conditions?

here they're showing it for expected free energy these kinds of special reduction conditions um or special cases and something analogous can be um kind of proposed for variational free energy it's just not directly shown there so um coming from so in variational free energy f q and y

expected free energy.

Epistemic value, pragmatic value.

And the special cases are when, for example, pragmatic value, which here is shown as the first term,

then here epistemic value is the second term so where section one is zero this term is just zeroed out there's no preferences preferences are flat all outcomes are equally pragmatically valuable then we only have elements two three four five in play that is equivalent to bayesian surprise

novelty search optimal information game in contrast if we had discarded two two three four five because it's a fully discovered fully known situation there's no epistemic value to harvest then we only have the left term one which is classical pragmatic decision theory and so on

By acting on the world to change the way in which data are generated, we can ensure a model is fit for purpose by choosing those data which are least surprising under our model.

Is active inference the process that can explain treatment resistance?

I can kind of imagine what that is, but if somebody who wrote that wants to describe like what treatment resistance is.

Okay, under just this first pass understanding that treatment resistance, let's just copy this information in, has to do with recalcitrance or resilience to a given therapeutic intervention.

There might be different ways to frame it, but if there's a model with a low learning rate,

I am the kind of person who does this action.

Then from the clinician's perspective, various interventions might be characterized, the person may be characterized as resistant to those interventions because the interventions are not updating some aspect of their generative models.

so is it the process can explain treatment resistance well again that's like asking is linear regression the process that can explain the relationship between heart disease and cancer it's like i don't know if you write the paper and the community of experts use it then it's a going scientific explanation is just putting these words down the explanation no

What else needs to be built?

How can that question be modeled and engaged with?

But does the framework itself simply explain it?

No.

Could you explain it and attempt to build a model that provides unique explanations, predictions, and control options?

I believe so.

Any comments or questions on this?

Can someone read Bayes' rule for me once again in plain English, equation 2.1, now taking into consideration that quote, Bayesian inference minimizes surprise, is equivalent to maximizing Bayesian model evidence.

And then they're suggesting that this is computing the posterior.

Any thought on this?

Equation 2.1.

maybe if anyone wants to type it out in the coming weeks just i i'm not 100 sure but if you have the model that's parameterized tuned such that bayesian model evidence is maximized you have found the most likely parameter comp the maximum likelihood estimate for a given sequences of actions given your priors

So you have the most supported model.

That is equivalent to saying that with your most supported model, you've minimized your surprise.

So how you read different terms and like which one you see as kind of upstream or feeding into the other one depends on the structure of what your experiment actually is.


UNKNOWN:
Okay.


SPEAKER_01:
Page 24, it then generates an action, sends it to the environment in an attempt to make the environment less surprising.

Could this be translated to a clinical scenario as follows?

One, I predict bodily integrity.

Two, I receive sensory observations disconfirming my prediction.

Three, I generate an action undergoing treatment to receive sensory observations that confirm my prediction of bodily integrity, thereby making my environment less surprising, aka active inference.

Anyone have a thought on that?

For qualitative ACT-INF, sounds good.

Let's just contrast that with other stories that people could tell.

One would be the reward learning story.

I'm rewarded by bodily integrity.

I seek out actions that increase my reward.

So I decided to go undergo treatment to increase my reward.

ACT-INF, generative model made a prediction.

then based upon sensory observations policies were selection selected to reduce surprise there's some interplay of change your mind and change the world the other story three this is 3a and then 3b is i got the sensory observation disconfirming my prediction i updated my prior

now expect low bodily integrity issue solved so normative in how we describe statistical problems sure normative in terms of in the human clinical environment suggesting how to act certainly not but great start

Policies associated with lower expected free energy, G, are assigned higher probability and become the policies that an organism expects to pursue.

Is this a reason why in a therapeutic setting change is often so hard to achieve?

Because the person often has to select different policies than they normally do and these new policies are therefore assigned lower probability.

Great question.

Any thoughts?

g updates the policy prior so we can look at we're gonna come to this more in chapter five when we start talking thanks jay we're gonna talk about it more um in chapter five there's a physical human in case that didn't make any sense um we're gonna talk about it more in

Five, E can be seen as the policy prior.

It's the list of affordances that are possible and the number in each of those cells is how likely those are to occur, a priori.

So I turn right 60% of the time, I turn left 40% of the time.

A priori, if nothing else intervenes, I'm gonna continue to replicate that 60-40 behavioral pattern.

expected free energy evaluation sharpens that policy prior into a policy posterior that better reflects expected free energy minimization as we just discussed which can be decomposed into the epistemic and pragmatic value so

various ways of thinking about it but certainly in certain settings if one has a strong habit they have a precise prior on action then yes it would be hard to update because they have a precise prior on action

But in the end, it would come down to the specific model that you actually make of that therapeutic setting.

But again, just qualitatively, conversationally, that broadly makes sense.

Especially if there's a new policy introduced, one can imagine that it has a low prior because empirically it's never been seen.

Like how can one be the kind of person who runs every day if they haven't observed themselves doing that?

Any other comments on that?


SPEAKER_00:
Can I just ask if there's a zero prior?

Can it still be updated such that there is a nonzero posterior?


SPEAKER_01:
Yes, that is going to come in

with the Dirichlet conjugate prior we were just talking about it in the chapter seven and that's what's called learning by counting so in that setting you basically like each time you observe the two options you drop one ball of that color into an urn and then your posterior is just

sampling from that urn so if you have one um one blue and one green then the third ball is gonna like update your posterior a lot whereas if you had a thousand of each then that two thousandth and first ball is not going to update it that much so that's like more recalcitrant to learning and then now someone says oh now there are orange balls okay i observed an orange ball okay now we have one orange ball and a thousand blue and a thousand green

one reason why those models are not like those models are very situational like there's no general answer or or solution about like adding and also from a um computational perspective like that you need to start some of the variables need to change their dimensionality and so currently tools are being developed to facilitate

specifying and updating the dimensionality of these models because a lot of the things that people are just intuitively curious about are like well what happens when somebody gets a piece of information that helps them realize that a new affordance is possible and that new affordance then in the future may or may not open up another affordance it's like yeah cool situation to model not going to have a general solution at this point

and last of the pre-stated questions at 38 they say vfe minimization is the key outside loop of act-inf sufficient to optimize perception and beliefs about policies however the notion of policy was introduced in the context of efe which is an imagined future thing whereas vfe is right now how can beliefs about policies come into vfe without efe

Any thoughts on this?

Expected free energy is explicitly calculated over policies about future observations.

Variational free energy, as the question suggested, is about the current status of the variational distribution and incoming data.

So they're just calculating something slightly different.

Without EFE, we simply have energy-based learning via minimization of EFE.

Only by calculation of expected free energy over policy do we get the active part of active inference?

Yes.

expected free energy could be applied at the one time step horizon um I'm sure there's other nuances and ways to address it but broadly variational free energy is reactive in that it's an ongoing unfolding flow between the generative model and the incoming data

but it could it can be calculated of active or passive entities expected free energy makes a proactive yeah and vfe the the it could describe unplanned reflexive optimal activity

Whereas to explicitly consider policy counterfactuals as such and evaluate sequences of future possibilities, only EFE has that capacity.

Okay, great, great question.

Scott, when one models a candle flame or pendulum, will they only use VFE?

Yeah, so some years ago, I'll let you go for it first.


SPEAKER_03:
Yeah, just about VFE and EFE.

Well, actually, EFE includes both planning, I mean, action selection planning and also decision making.

But variation of free energy is just about, I mean, it's not just about planning.

So EFE also includes

includes the action selection or policy selection.

And of course, it's a requirement for planning and any kind of decision making.

But in VFE, it's just an inference we get by minimizing that parameter.

But in the next stage, when we want to act upon that inference,

We use EFE for the action selection and decision making and so on.

Thank you.


SPEAKER_01:
Yes.

A few answers on using a candle flame.

So here in the strange particles paper, thinking about a modeling iterative process, first, we could say we're going to model it as an inert particle.

And then if there's no statistical variance left to describe, you've done good enough.

you might also need to appeal to a latent cause like oh it's like it's a guitar string that's resonating but it's also like there's like a little gnome with a circadian rhythm latent cause inside and then it's like the next level of strange particles like oh this guitar string is resonating and there's a circadian gnome and i think the gnome thinks about what i'm going to do

But each of those are an empirical question about what level of sophistication of cognitive model is relevant for a given situation.

And yes, earlier versions before expected free energy was even introduced, ACK-DIMF was VFE based and EFE was proposed to explicitly account for counterfactuals and planning.

because in the earliest iterations it was like well we'll just calculate vfe that's like the real variable anyway so let's just do vfe on generative models and then like just sort of be guided step by step and then efe like kind of ultimately describes and and invokes variables that vfe doesn't like expected future observations

great um questions um just want to quickly look through where we're gonna go in chapter three um we can of course come back to it for the coming two weeks but any questions or thoughts or structuring or notes that people want to add um leading up to it of course the more the better so beginning with a Dawkins quote about survival machines

chapter two was free energy as a means free energy as a how variational bayesian inference as a how here free energy is going to be approached from the why which is going to be the avoidal of surprising states and the ways that that is manifest across like self-organized systems and self-modeling systems

markov blankets are introduced i'm sure we'll have a lot of fun discussion so please all questions that people have on the topic just write them out you leverage it by like a hundred thousand when you actually write it out and improve others questions because there are many questions that we have about these areas um markov blankets are defined a certain way in a certain way that's carrying less philosophical baggage than many initially suspect

The action perception loops, which were presented as loops in the previous chapter, are here presented in what's known as the particular partition.

It's a particular partition because it's a joke.

First off, it's a specific way to do it.

It's not the only way to do it.

It's particular.

also what it partitions is called a particle the particle is the blanket and internal states that's why these are called inert active conservative and strange particles the particle is the generative model of the agent so the particular partition helps us operationalize the action perception loop scheme into a more proper bayesian graph setting

which enables message passing, all these other things.

Figure 3.2.

is showing a simple example of how Markov blankets are used in inference, not in some super recursive clinical setting, just in terms of like some stochastic variables and the ways that different variables have information about one another, and particularly how internal and external states can still contain information about each other despite being mediated only through the blanket.

3.3 agents with Markov blankets appear to model the external environment in the sense that internal states correspond to a representation of external states of the system so there's not like a little thermometer inside of the head of the generative model

but there are some statistical patterns such that we can say that there's mutual information between some part of the temperature and some part of the generative model otherwise the agent's actions would be insentient with respect to that environmental variable so that's like where we get cybernetics good regulator theorem law of requisite diversity

there's a turn towards uh or a nod to some of the bayesian physics by framing the process of surprise minimization as a hamiltonian principle of least action we're going to talk about it more short summary least action does not mean most lazy

If you shoot a ball out of a cannon and it follows a parabola, that is the path of least action.

So least action does not mean least ATP using, it doesn't mean least carbohydrate burning, doesn't mean smallest computational model.

It's a technical physics term that has a very important meaning.

And you'll hear Friston and others characterize FEP as a principle of least action for cognitive systems.

3.4 relationship between inference cognition and stochastic dynamics like why are we even talking about cognition as inference and talking about all of this statistical stochastic stuff so here they juxtapose statistical physics which has long had an imperative of minimizing free energy with Bayesian inference and information theory

which can be mobilized to provide cognitive interpretations under the Bayesian brain framework.

Free energy, again, we're not simply talking about ATP being burnt.

It's not Gibbs free energy, it's not calories, but it might have some relationships.

3.41, variational free energy model evidence and surprise.

Fancy letter surprise as model evidence.

We'll come back to it.

Expected free energy.

Here we had VFE and surprise.

Real-time unfolding model evidence as surprise.

Here's prospective EFE and inference to the most likely trajectory.

and 3.5 active inference a novel foundation to understand behavior and cognition some summary of the above and a little bit of unpacking about how behavior and cognition are being unified in active inference 3.6 models policies and trajectories

more discussion about those topics reconciliation of inactive cybernetic and predictive theories under active inference those three apparently disconnected theoretical perspectives are reconciled in this section according to the authors and then moving even further active inference from the emergence of life to agency is where again in only a several short paragraphs so just the tip of the iceberg

there's some discussion of general um sentience and agentic properties of the generative models the kind that we're modeling with active inference side box on entropy minimization yes hardly any maths in chapter three yeah let's see um not in the second half okay equation 3.2

equation 3.1 but both of them are kind of definitional and surely are described naturally in the text definition in the box three so yes different style but it's a breather before chapter four hits hard again with the math so thank you all looking forward to the comments and talk to you next week
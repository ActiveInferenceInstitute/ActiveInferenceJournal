SPEAKER_01:
hello it is march 15th 2023 we're in our first discussion of chapter four in cohort three and there's a lot of great questions first is there any general comments or reflections that anyone wants to share about chapter four

Yeah, just previously people were reflecting on this kind of accuracy complexity trade-off and how much and what maths to show.

Ali?


SPEAKER_04:
Yeah, as Jonathan and Francois also mentioned, this chapter is pretty dense, probably one of the

most difficult chapters in the whole book.

But in my own experience, the step by step tutorial paper is extremely helpful when reading this chapter, because some of the missing gaps in the explanations and some of the, in my opinion, necessary details that are somehow for some reason, have been omitted from this chapter can be

found in quite helpful detail in that paper.

So yeah, I definitely highly recommend reading that one.


SPEAKER_03:
Oh, that's great.

I will do so.

The thing is, I know that the step-by-step paper is only for discrete time, but this chapter four also goes to continuous time.


SPEAKER_04:
Is there a similar resource that you could recommend for... Raphael Bogach's paper on tutorial on free energy principle somehow does the same kind of, not simplification, but a kind of pedagogical walkthrough through the continuous time active inference.

So


SPEAKER_01:
uh both both of those papers can be seen as complementary to each other one for discrete and the other one for continuous time situations thank you added the link here um any other general comments we had chapter one introduction then we walked on the low road and the high road to active inference

and now in chapter four we're at the generative models of active inference so if it's an active inference generative model it's active inference if it's something else it's something else

a focus in this whole textbook and field is about how time is represented and those who have done any kind of dynamical systems modeling will be familiar with a lot of like the fundamental differences and challenges for example continuous and discrete representations of time and how to discretize time and so on so section 4.2 from bayesian inference to free energy

preceding two chapters with the high and the low road were outlining important connections with active inference and other paradigms and especially the low road helped us look at the bayesian brain starting from the bottom starting from the how um with respect to the high road this is where we bring in

notions of like cognitive entities and the kinds of bounded inference or bounded rationality that support self-organization and persistence and adaptive decision making this chapter recaps Bayesian inference and specifically the variational approach of Bayesian inference

that is what is going to connect generative models which are how active inference models are specified to the concept of free energy which is going to be shown to be kind of like a criteria or an imperative that's used to fit or describe different generative models and the data

This section is more technical than the previous chapters, appealing to a little linear algebra, differentiation in calculus, and the Taylor series expansion.

Some of those topics are in the appendices and even the points that people like on the structure and what would have been helpful to know when.

Those are all helpful comments to write or to surface.

There's no one single linear layout that would have been perfect for every single case, but even just like

sharing what would have made it helpful for any given person or what detours or cul-de-sacs they went on that's all really useful those who do not want to delve into theoretical underpinnings may skip this chapter so soon Bayes equation in 4.1

does anyone want to just bring up anything or describe it or call it like they see it let's see if we have the natural language description so here with the active ontology we have if you want to see like how it can be read out but a key issue

is that whether you're summing over discrete opportunities or integrating over continuous ranges these sums or integrals are very computationally challenging or even intractable the approach of variational inference is to convert this potentially difficult summation or integration problem

into an optimization problem.

So instead of just like one mega plug and chug, crunch the numbers and get one outcome at the end, there's going to be a incrementally optimizable approach that we can iterate on

with known computational complexity that under some or potentially a broad range of settings will bring us closer to the best reasonable compromise answer.

And so we've exchanged this vast one-shot calculation into something that can be incrementally through time on a real computer optimized.

understand how this works we need to appeal to jensen's inequality which says that the log note that's not the log squared that's a footnote subscript um the log of an average is always greater than or equal to the average of a log and figure 4.1 shows that so does anyone math professor or otherwise want to give like a thought

on this figure 4.1 what is happening and why why is this relevant for how we're going to be continuing on with bayesian equations


SPEAKER_00:
Maybe not a particularly deep thought, but this is because of the second derivative, the fact that this is kind of curving down, the steepness is getting less, that this holds true.

I created a little animation that I can link to looking at sampling along X and then comparing the logarithm of the

the expected value with the expected value of the logarithm, just to get a bit of an intuition as to why this holds true.

But I think that this image is pretty nice.

This sort of shows why it is.

You can imagine that any function which is curving down like this is going to have this property, and any function which curves up is going to have the opposite property.

I can paste a link into the comments of the little animation that I created there.


SPEAKER_01:
Awesome.

And can you maybe add one piece?

How is this connected to Surprise?

Or how does anybody see Jensen's inequality maybe as leading to the way that we're going to talk about Surprise?


SPEAKER_00:
So only that this is going to give us a bound on surprise, because surprise is the average of x log x, or p of x log p of x, essentially, that we're averaging the amount of information that we will gain on making any particular

measurement.

It's the average information that you'll gain.

And so that's already an expectation and it allows us to do an expectation of X and take the logarithm of that, which gives us a bound on the surprise.


SPEAKER_01:
Thank you.

It's definitely like some words that fly by and to hear it many times is good.

it does turn out that this property of logs, and there's gonna be multiple properties of logs that come in to help us in different ways, like the ability to take two numbers that are multiplied inside of a log and make it into log of A plus log of B and so on.

But this curvature property is going to help us talk about expected surprise.

Okay, I'm gonna bring up the

Awesome.

Could you connect that to any sampling?

Or what are we being surprised about?

Something on the y-axis, something on the x-axis, where are the data, and so on?


SPEAKER_00:
So this is just a normally distributed x here.

So there's some probability over x. And then what you're looking at, I guess, in the

I've got to think about this now.

No, I don't want to say without thinking about it.

But essentially here, this is just taking some expectation over x and calculating the logarithm of that expectation and the expectation of the logarithm just sampling.

So starting on the left-hand side with just one sample and then increasing to 100 samples and just seeing how the relationship between those two points is.


SPEAKER_01:
that's that's all it's doing cool now we're going to take advantage of that property so we return to equation 4.1 so this was the one with the intractable sum or integration we're going to return to 4.1 and multiply by an arbitrary function q

So we took this line with big challenge and we said there's some arbitrary function Q and anything divided by itself is one.

So this is equivalent to multiplying by one on this left side.

And it turns out that Q is gonna be like this variational distribution from a known family with easily optimizable characteristics that we know we can control.

Now,

there's going to be a log taken on both sides so q of x over q of x which is equivalent to just one and then there's going to be um the log taken on both sides we can now interpret this expression as an expectation fancy e not e cubed that's just a footnote again e of a ratio between two probabilities so

here's the p of y of this is the joint probability distribution of hidden states and data and now this allows us to construct joint distribution over the one we control natural log of that natural log on the outside is always greater than the expectation of the log

log of an expectation is always greater than the expectation of a log log of an average is always greater than or equal to the average of a log this value so this is again this is the part that we want to solve and this is another way to rewrite it that

is strictly bounded via jensen's inequality to this value which is defined by triangle the negative variational free energy as a function of q the distribution we control and y the data

smaller the free energy the closer it is to the negative log model evidence especially because the log has these like somewhat strange seeming properties like it's just shoots off down to negative infinity over here and so on like and there's negatives and positives so you know take it slow to see what each line is actually telling us um but this is the

um standard from statistical physics what is called the negative variational free energy when free energy is small it's close to the negative log model evidence negative log model evidence so if the log model evidence were really high

like then the log because of it's always going up and to the right but slowing down in how it does that it means you're also monotonically getting higher model evidence and also the natural log of p of y is um how that's shown any thoughts on equation 4.3

4.3 provides some intuition for the roles of the free energy and the Q distribution, two quantities that were difficult to compute without the variational approximation.

So the expectation about the distribution that we're getting to control and parameterize, the expectation of the likelihood, which is natural log of P, of the joint distribution between the hidden states and the data is the negative variational free energy,

as a functional of q and data plus an expectation of the model likelihood of q there's some rearranging bringing the f onto this side

and then using the properties of differences of logs to get us to this representation variational free energy with two components the raw model evidence of the data

and a kl divergence between the distribution we control about x hidden states and this p distribution of x given the data let me just add in your comment there

so jonathan wrote um the entropy is the average surprise surprise is not a single measurement not itself an average it is simply the probability of negative natural log of p of x thank you the kl divergence is defined in the second line as the expected difference between two different log probabilities

this is a measure it's not a proper measure of how different two probability distributions are from one another so when there's no divergence between the distribution that is under control and the sort of true distribution that we'd want to estimate

then the relative variational free energy has been minimized because the log model evidence is just like whatever it is it is with respect to being able to change q which is what makes the free energy an optimizable quantity because for a queue where you know which knobs can be tuned

and there are methods for the incremental optimization, like on smooth optimization landscapes and so on, then this divergence can be very directly tackled.

And this is just like, can be kind of just left unconsidered because it's a constant with respect to the same data.


SPEAKER_02:
Assumed.


SPEAKER_01:
What is?

it can be assumed yeah assume that whatever it is with respect to changing the knobs on q it doesn't matter because q is not included in this term q only shows up here so yeah um

okay yes definitely a few um tricky simple seeming but very ineffable transformations with logs and the bayes equation that get us to a form for variational free energy which if we remember back so long ago

is going to help us bound surprise and there's going to be like a special case that doesn't necessarily ever get realized where the free energy like the bound divergence goes to zero but in any situation where the distribution p is maybe even of a different structure than q there's always going to be some divergence however by reducing that divergence

will be doing the best optimization we can um michael or anyone okay yeah these are some some of the most pure mathy

pages in the textbook sorry my mic's on it's all good no it's it's true it's like where did active inference go but this is totally in it but these several pages which may be in a um undergraduate statistics course could have been unpacked into several lectures and homeworks are are very fundamental but they're very much

brought in quickly here to get us to this variational free energy


SPEAKER_02:
as someone who is not the mathematician but spoke it at one point and is grappling with this question of how do I think about these objects in order to speak to others about it.

Earlier at the beginning, I was thinking about the original formula as input to a process that then there was a calculation being done.

and that that input was one of the false characterizations that

that it's an inflow, so it's not a single, but it's this idea of there's a continuous.

And to think of surprise as also as a living, flowing thing is also one of those kind of hard to grapple with, that we're not talking inputs to, you know, but rather flow to, and that there's the hearing the variability, right, what's not explained.

um is is the

is is is the intuition that's hard to cultivate for those of us who these these these formulae just kind of paralyzes and and it's and trying to translate what are we learning here um is that's where i'm i'm realizing my existing language doesn't capture what the mathematics is describing so i'm contributing my my confusion i hope articulately


SPEAKER_01:
thank you um just to Jonathan's comment and then back to that so um Jonathan yes however you whatever format you're taking your notes in there's there's always gonna be a way um in the equations themselves you could always tag anything like you could add descriptions or just notes or other unpackings

or other links in the equation section or we can go and modify or revive the math learning group with any different notes that people have on notation on different resources to look at or like kind of chapter wise descriptions of math hopefully not to confuse or devalue this too much

but I hope to bring even um more clarity ultimately if you check out live stream number 52 especially the final hour of 52.2 you'll hear about Lance da Costa sharing about the free energy divergence is just one discrepancy measure that can be utilized

in pursuit of a unified imperative for on the inbound signal processing and on the outbound control theory so signal processing alone is just passive inference control theory alone essentially has to presuppose knowledge of the world or at least it's less considered with it active inference is considering the inbound and the outbound and free energy

it turns out is an extremely convenient discrepancy measure, which is why even before all of the 2000s and all of that, free energy and variational autoencoders, Bayesian statistics were already in heavy use because free energy as a bound on surprise in Bayesian statistics is just useful.

But it's not the only discrepancy measure, but in the variational setting where we're learning it here, it is the divergence or the discrepancy measure

But thinking about this less as like we're getting to active inference from free energy.

What is free energy?

Put the horse before the cart, the cybernetic loop and the signal processing inbound, control theory outbound, unified imperative first.

And then these are going to be different, useful, approximate Bayesian calculations, but not the only calculations that can be used

for modeling in that cybernetic setting.

like just one note, then I'll leave.

The KL divergence describes the informational distance, not exactly distance, but basically between two distributions.

But he described another distance measure, the earth movers distance, how much earth you need to move from one distribution to make it resemble the other distribution, since they both have an area under the curve, let's just say of one.

And those are different measures.

have different properties there's situations where one might be appropriate or not so one could actually do active inference without free energy calculations there could be a different discrepancy calculation it's just that this is a go-to choice for the same reason why free energy calculations are not arcane there are workhorses in variational auto encoders and so on ali


SPEAKER_04:
This is related to what you mentioned.

Two related pieces of literature just came to mind.

One is this well-known paper, A Tale of Two Densities.

Active inference is inactive inference, which contrasts two interpretations of active inference, namely structural representation interpretation and

inactive inference representation which they basically somehow lean towards the second type of interpretation and the other one which is not

so well known is a chapter from uh this book uh called measuring and modeling persons and situations by uh chapter 18 from this book by adam saffron um and colin d young uh

namely integrating cybernetic big five theory with the free energy principle uh so this is also a really helpful synoptic comparative analysis between different cybernetic and control theories and active inference so yeah I just wanted to mention those two papers thank you


SPEAKER_01:
So again, not to devalue this whole variational sojourn, but this is the variational approach.

That's why we went in with a Q. Another approach could have been, you know, this is a really intractable sum.

We're going to sample from it with Monte Carlo simulations.

And we're not going to do variational distribution at all.

So we can still...

talk about active inference even without this variational technique so it's like this is not this is not the essence of active however variational Bayesian methods are incredibly powerful and scalable on modern computers including in some places where sampling is like going to be costly or impossible

it's really interesting to think about everything technical is a modeler's degree of freedom once one has made the decision to have a unified perspective on perception and action that is the active inference turn and now these are modelers possibilities sampling variational bayesian methods and other approximate bayesian computational methods

and and um those paths are not always obvious because it's like the variational free energy we're using variational methods but it's not the only method that could have been used but then it's like what is most commonly shown um so long like kind of path to return to

we got to variational free energy as has been reached by other bayesian statisticians because it's a super useful form that takes that intractable sum or integral and it separates it into a constant term which we're not going to do anything to change and one term where basically we have the knob

and can completely incrementally optimize it to reduce divergence because the KL divergence between the distribution that we control and the one that we would want to have no divergence with is literally that is the KL divergence is a difference of these two logs which is also why some of those more fundamental properties and logs were being brought up

okay or three to calculate free energy we need three things data a family of variational distributions and a generative model so do organisms minimize free energy calculate free energy no data a family of distributions and a generative model do again hard to sometimes see the rippling consequences

but this is a pure instrumentalist expression so cells do not minimize free energy they don't calculate free energy the models do there are going to be two families genera of generative models

and the exact form of free energy is going to be slightly different for these two it's kind of like vertebrates and invertebrates it's like there's a lot of decisions down the line but this is like some of the broad taxonomic divisions in the generative models the first is going to be dealing with discrete state spaces and discrete representations of time

the second is going to be dealing with continuous variables and continuous treatment of time before showing what those generative models look like figure 4.2 gives some fun intuition for this base graph diagram format so circles represent random variables

So those are things about which we hold beliefs.

It could be a fixed or a learnable parameter or stochastic variable.

And squares are probability distributions that are like connector pieces because they describe relationships.

And so these are laying out situations where it's like here X is upstream of Y. Here X is upstream causally of Y and W.

here there are two causes upstream of y and here is a chain of causation and so in this directed bayesian graph setting these are sometimes called causal graphs because they summarize the causal influence of one variable on another so correlation is not causation and then it gets a little bit

mingled because these are called causative graphs but this could be living in this region and this could be having this disease but this is just a model so that doesn't mean living in the region is the mechanistic cause of having the disease and that's why we're always talking about evaluation of different um

um Michael how completely do these four representations of causality cover the known forms of influence um someone can give a thought my my first answer would be these are like this is like subject verb subject verb object you know subject you know these are like the clauses that influence networks are constructed from

So in the ultimate modular sense, this captures it all.

A influences B. But then in the real setting with interacting and emergent causation, these get stitched together in complex ways.

And in fact, the generative models that we're going to look at are going to be using this kind of a visual grammar to describe how different variables influence each other.

it's an interesting question about what forms of appreciation influence and control exist and how those are represented with base graphs okay a little bit of unpacking on the um what they are talking about with uh the disease and the diagnosis and all of that we get to figure four three

these are going to be some core visualizations the top part of the graph is going to show a partially observable markov decision process or a pomdp and that's going to be in discrete time and the bottom part of the graph is going to be displayed

and juxtaposed to highlight the structural similarities but then there's going to be some very very important differences as well and that is also going to be unpacked in chapters seven and eight which are respectively about the discrete time and the continuous time settings um so these figures you will see in many active inference papers

thomas parr in the recent book stream gave a little bit of a historical account he emphasized that earlier on like pre-2012 there was a lot of continuous time modeling like with eye movement and reflexes and motors people then developed the discrete time discrete active inference

because it is classically used to model decision making categorical and discrete decision making but these two kinds of models can also be hybridized and that's called a hybrid model so here again just to see it one time but we'll see it again and again it's like it's like a rake on the bottom and then there's action selection part on top discrete time continuous time

then even the hybrid model it's like here's the big rake top level model and then where there's just a little leaf here that has a whole continuous time generative model grafted on

So again, like no GM, no generative model is like the Act-Inf generative model.

It's like a grammar or a toolkit for comparing and evaluating portfolios of structurally different or parametrically different models.

So it's not like there's one way to look at attention.

There's not one way to look at memory.

There's not one map for a territory and so on, but there's gonna be just like many kinds of intuitive graphical operations that this format is gonna help us talk about.

like here is the past present in the future and you say well could we have like three more can we go four time steps into the future you could or could the observations be smell and taste yes they could or could could there be 10 policy options yes there could be 10 policy options so it's like just a skeleton that is going to be used and modified and kind of remixed in in many many ways um

anyone have any like thoughts or descriptions on figure 4.3 what's something whether they've read on past this section or not like what have they found important in 4.3 and then let us like dwell on 4.3 a little bit look to the questions and then next week in our second discussion we'll continue on from here and look at more questions

So earlier, the step-by-step was brought up.

And this figure, which I'm sure we have pasted many times into our textbook group coda, is working with that same graphical grammar of the Bayes graph.

On the left, we have a prior that is seeding a hidden state S, and then the hidden state S is emitting an observation O.

You can set S and generate O, that's generative modeling, generative AI.

You can also have O and update S, that's the recognition density.

So A, which is called the emission matrix, is the tail of two densities, because you can have a hidden state and generate synthetic data from it, that's generative,

or you can have an observation and use it to update your hidden state, and that's recognition.

So this is the tale of two densities in the static setting.

So this is like a single image and the image classifier label.

And so generative AI is going from, draw me a cat, to generate the pixels, whereas previously there was only really recognition AI from the pixels to, oh, that's a cat, but it's static in the top left.

The bottom left is going to be that same motif, tail of two densities, but now S has a subscript for one, two, and three, which are three time steps.

And so S is changing through time.

The way S changes is through B, which is called a transition matrix.

So if there's like two states in S, it could be on the top or the bottom.

there's going to be a two by two transition matrix where the diagonals are like staying where you are and the off diagonals are moving and that is going to be the describing the time evolution of the hidden state and then at each time there's the tale of two densities and the emission of an observable and the a matrix can be sharp

which is like having kind of ones across the diagonal so you can like see the hidden state without error or in the other extreme a could just be like kind of like a wash at which point the observation wouldn't tell you anything about the hidden state or in the middle is kind of like a semi-blurry a matrix where there's information on the hidden state from the observation but it's not unique for example a positive test result doesn't mean you have the disease

that's a classic like bayesian statistic oh yes thank you a does not have to be square matrix i'm using square examples but yes the dimensionality is like a huge thing to make sure that especially in complex models that the dimensionality like works out um so this is passive dynamical perception we're getting temperature readings at time one two and three and whether we have a perfect thermometer

or whether we have like a kind of noisy thermometer we're going to be making our inference about the hidden state the true temperature in the room through time and then B describes how the temperature in the room changes through time whereas a is time independent and it describes the mapping between thermometer readings and temperature this is passive inference on the left Michael


SPEAKER_02:
Again, going to a different language, but in management, traditionally management has been uni-dimensional.

So for example, you might say we need to manage the dollars being produced in this process or the fishery, a single species in this fishery.

And that one of the challenges has been to look ecosystemically instead of through a uni-dimensional variable that is optimized.

at the interplay between all the variables and that that that's what i'm witnessing here in a different space and sense this kind of uh multivariate um management of the interplay between all the factors um i uh that that's my own shorthand for what i'm what i'm thinking i'm sensing


SPEAKER_01:
Awesome.

Yeah.

The observations could be our triple bottom line, and then the hidden state could be the unobservable health of our organization.

And so we're not just doing this like naked optimization on an observable, but we're kind of inferring from the observables into an unobserved space.

And then that's the space that we're describing the dynamics on.

Yep.

All of this has been passive.

But this paper goes step by step.

It introduces static perception and dynamic perception.

Here's where action comes into play in active inference.

Pi is policy.

So there's a slight difference between affordances, which are the actions that can be taken in a moment, like up, down, left, right on your joystick, and policies, which are sequences of action over a given time horizon.

So with a time horizon of one, your affordances are your policy repertoire.

for a time horizon of two is up up up down up right you know and so on so um pi describes policies so that's sequences of actions that can be taken

for the length of the time horizon, which is a modeler's choice.

So pi describes which policies can be taken.

How are policies evaluated?

Well, in reinforcement or reward learning, you would have utility function that ascribes a utility value to different policies.

In active inference, we have expected free energy as an imperative that guides policy selection.

Here, free energy G gets input from C, which is our preferences or our expectations.

And so this is that fun twist with what we expect slash prefer is like in reward learning, we would say the body's rewarded by having a homeostatic temperature, survivable temperature bound.

in this kind of preference-based active inference, you say the body expects to be in homeostasis.

It is surprised if it is not.

And it takes actions to reduce the divergence to minimize and bound its surprisal.

And so action policies are selected, not because they have a high estimated utility,

though that can be the case, but rather because the policy reduces expected free energy by aligning observations with preferences.

It's also important to note that policy and action selection intervenes in the bee.

So this is changing how hidden states change their time.

then this bottom version which has not really even gone into much in the textbook they're adding another variable that describes like a temperature a precision on policy where in the absolute zero temperature high precision setting if there's two policies and one is a little bit better you'd want the one that's slightly better every time

In the neutral temperature, if it was 51, 49, you'd want to select that one 51% of the time.

In the high temperature, you would erase the differences between policy choices, and you would be selecting equally without respect to the mentioned criteria.

But it's a little bit of like a modification, but also it sets us up for some of the incredible elaborations that we're going to see within this visual grammar.

So we're going to see hyper priors on different variables and learning rates.

And it's like, this is like a circuit board.

There's no single circuit board because you can wire things up differently.

You can connect and say, well, but what if there was L that influenced C?

It's like, you can make that generative model.

And with Bayesian statistics, you can evaluate and juxtapose models that have radically different anatomies.

so it's like these are circuit board designs or like lego architectures so there's not going to be one gm even for a very narrow system of interest any more than there would be just a single circuit board all of that and all of step by step

is in the discrete time setting so past present and future how things change through time how hidden states emit observations how the initial hidden state is set with a prior and how policy selection intervenes in hidden state through change that change through time and how preferences shape through expected free energy minimization policy selection

in the continuous time setting it's a little bit different we're gonna come to it

turns out that there isn't explicit prediction of past present and future time steps but rather generalized coordinates are used which is to say like position velocity acceleration and so on generalized coordinates are used akin to a taylor series expansion such that a smooth and differentiable function is proposed that makes continuously interpolated predictions

without explicitly mentioning discrete moments and giving explicit predictions for them.

So there's some pros and cons and different settings and so on, but we return to this discrete continuous time distinction repeatedly.

Fundamentally, it's a modeler's choice, often guided or constrained by the kind of data that one has.

And that's going to come up again and again

laid out here in figure 4.3 to illustrate their structural similarities in both cases we have the kind of rake or this like e facing down with the hidden states changing through time and emitting observations caveat for you know the way that it's a little bit different but basically that's what's happening and then the upstairs is the policy selection and that's what we see in step by step

rake is just static or dynamic inference that's signal processing that's the kalman filter and then the upstairs is policy selection that's control theory control theory upstairs signal processing downstairs all of it is being included in the same generative model

so that we can have summary statistics that are optimizable for our generative model, like free energy, so that we can have a unified imperative for perception and action.

Let's look to what questions we'll come to next week.

right equation 4.10 so equation that we haven't gotten to yet but yes there are more equations including some subtleties with subscripts and bold so these are definitely ones to have um good readings of and like a lot of different questions and like why is that line doing that and all of this we'll come back to it um

expected free energy is minimized by selecting those observations that cause a large change in beliefs we'll come back to it okay a question of variational inference and also we can use the coda latex unpack to make it render nicely

Equation 4.10, same as here.

And some more questions on equation notation.

So thanks for these very careful comments on the notation there.

Awesome.

So yeah, a lot on the equations and it's like, we can always return to the math learning group.

We can set another time because there's math learning questions

to help increase the approachability of this entire corpus and then there's also some really specific technical questions where it would be awesome to just have like here's what the squiggle means

and so those are like two different like regimes of attention within the math learning mode and the math learning mode is just one mode that we're in overall but but both those um accessibility and rigor components of math hopefully like if people are motivated to somehow work on that there's like a lot of opportunity there any final thoughts or we will return next week

in our second discussion on four.

All right, great.

Thank you all, farewell.
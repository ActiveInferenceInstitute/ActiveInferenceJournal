1
00:00:01,199 --> 00:00:07,379
hello it is March 15 2023 were in our

2
00:00:07,379 --> 00:00:10,740
first discussion of chapter four in

3
00:00:10,740 --> 00:00:12,719
cohort three

4
00:00:12,719 --> 00:00:17,580
and there's a lot of great questions

5
00:00:17,580 --> 00:00:20,660
first is there any general

6
00:00:20,660 --> 00:00:23,400
comments or Reflections that anyone

7
00:00:23,400 --> 00:00:26,840
wants to share about chapter four

8
00:00:33,840 --> 00:00:36,300
yeah just just previously people were

9
00:00:36,300 --> 00:00:38,340
reflecting on this kind of

10
00:00:38,340 --> 00:00:40,980
accuracy complexity trade-off and

11
00:00:40,980 --> 00:00:45,680
how much and what maths to show Ali

12
00:00:45,680 --> 00:00:48,719
uh yeah as Jonathan and francoiso

13
00:00:48,719 --> 00:00:51,899
mentioned uh this chapter is pretty

14
00:00:51,899 --> 00:00:56,699
dense and probably one of the uh most

15
00:00:56,699 --> 00:00:59,460
typical chapters in the whole book but

16
00:00:59,460 --> 00:01:02,699
um in my own experience the step-by-step

17
00:01:02,699 --> 00:01:05,640
tutorial paper uh is extremely helpful

18
00:01:05,640 --> 00:01:08,000
uh when reading this chapter because

19
00:01:08,000 --> 00:01:11,939
some of the missing gaps in the

20
00:01:11,939 --> 00:01:14,159
explanations and uh some of the in my

21
00:01:14,159 --> 00:01:16,979
opinion necessary details uh that are

22
00:01:16,979 --> 00:01:19,020
somehow for some reason that have been

23
00:01:19,020 --> 00:01:21,540
omitted from this chapter uh can be

24
00:01:21,540 --> 00:01:25,500
found uh in uh quite helpful detail on

25
00:01:25,500 --> 00:01:28,979
that paper so yeah I definitely highly

26
00:01:28,979 --> 00:01:32,298
recommend reading that well

27
00:01:34,439 --> 00:01:36,780
oh that's great I will do so the thing

28
00:01:36,780 --> 00:01:38,700
is I know that the step-by-step paper is

29
00:01:38,700 --> 00:01:41,280
only for discrete time but this chapter

30
00:01:41,280 --> 00:01:43,320
four also good to continuous time yeah

31
00:01:43,320 --> 00:01:44,820
is there a similar resource that you

32
00:01:44,820 --> 00:01:47,100
could recommend for raffle Bowl matches

33
00:01:47,100 --> 00:01:49,259
paper and tutorial on free energy

34
00:01:49,259 --> 00:01:54,299
principle uh somehow does the same kind

35
00:01:54,299 --> 00:01:55,320
of

36
00:01:55,320 --> 00:01:55,979
um

37
00:01:55,979 --> 00:01:59,159
not simplification but uh a kind of

38
00:01:59,159 --> 00:02:02,340
pedagogical walkthrough through the

39
00:02:02,340 --> 00:02:05,579
continuous time active in France so uh

40
00:02:05,579 --> 00:02:07,560
both both of those papers could be seen

41
00:02:07,560 --> 00:02:09,598
as complementary to each other one for

42
00:02:09,598 --> 00:02:11,038
discrete and the other one for

43
00:02:11,038 --> 00:02:14,599
continuous time situations

44
00:02:15,420 --> 00:02:18,599
thank you out of the link here

45
00:02:18,599 --> 00:02:20,660
um

46
00:02:21,060 --> 00:02:23,400
any other General

47
00:02:23,400 --> 00:02:27,780
comments we had chapter one introduction

48
00:02:27,780 --> 00:02:29,099
then

49
00:02:29,099 --> 00:02:31,739
we walked on the low road and the high

50
00:02:31,739 --> 00:02:36,560
road to active inference

51
00:02:36,900 --> 00:02:39,599
and now in chapter four we're at the

52
00:02:39,599 --> 00:02:43,019
generative models of active inference

53
00:02:43,019 --> 00:02:44,760
so

54
00:02:44,760 --> 00:02:46,560
if it's an active inference generative

55
00:02:46,560 --> 00:02:49,560
model it's active inference

56
00:02:49,560 --> 00:02:51,360
if it's something else it's something

57
00:02:51,360 --> 00:02:53,660
else

58
00:03:01,379 --> 00:03:06,720
a focus in this whole textbook and field

59
00:03:06,720 --> 00:03:09,900
is about how time is represented

60
00:03:09,900 --> 00:03:11,760
and those who have done any kind of

61
00:03:11,760 --> 00:03:13,980
dynamical systems modeling will be

62
00:03:13,980 --> 00:03:15,300
familiar with a lot of like the

63
00:03:15,300 --> 00:03:16,980
fundamental differences and challenges

64
00:03:16,980 --> 00:03:19,860
for example continuous and discrete

65
00:03:19,860 --> 00:03:21,540
representations of time and how it's

66
00:03:21,540 --> 00:03:25,519
discretized time and so on

67
00:03:25,920 --> 00:03:28,620
so section 4.2

68
00:03:28,620 --> 00:03:32,159
from Bayesian inference to free energy

69
00:03:32,159 --> 00:03:34,080
the preceding two chapters with a high

70
00:03:34,080 --> 00:03:35,940
and the low road were outlining

71
00:03:35,940 --> 00:03:37,560
important connections with active

72
00:03:37,560 --> 00:03:39,480
inference and other paradigms

73
00:03:39,480 --> 00:03:42,180
and especially the low road helped us

74
00:03:42,180 --> 00:03:44,760
look at the Bayesian brain starting from

75
00:03:44,760 --> 00:03:48,980
the bottom starting from the how

76
00:03:52,200 --> 00:03:54,200
um

77
00:03:54,360 --> 00:03:56,819
with respect to the high road

78
00:03:56,819 --> 00:03:59,400
this is where we bring in

79
00:03:59,400 --> 00:04:03,420
Notions of like cognitive entities

80
00:04:03,420 --> 00:04:05,700
and the kinds of bounded inference or

81
00:04:05,700 --> 00:04:08,659
bounded rationality that support

82
00:04:08,659 --> 00:04:11,099
self-organization and persistence and

83
00:04:11,099 --> 00:04:14,000
adaptive decision making

84
00:04:15,360 --> 00:04:16,858
this chapter

85
00:04:16,858 --> 00:04:19,260
Recaps basing inference

86
00:04:19,260 --> 00:04:21,418
and specifically the variational

87
00:04:21,418 --> 00:04:25,199
approach of Bayesian inference

88
00:04:25,199 --> 00:04:27,680
that is what is going to connect

89
00:04:27,680 --> 00:04:30,240
generative models

90
00:04:30,240 --> 00:04:32,820
which are how active inference models

91
00:04:32,820 --> 00:04:34,380
are specified

92
00:04:34,380 --> 00:04:36,960
to the concept of free energy

93
00:04:36,960 --> 00:04:39,600
which is going to be shown to be kind of

94
00:04:39,600 --> 00:04:42,300
like a criteria or an imperative that's

95
00:04:42,300 --> 00:04:44,940
used to fit or describe

96
00:04:44,940 --> 00:04:49,340
different generative models and the data

97
00:04:50,220 --> 00:04:52,199
this section is more technical than the

98
00:04:52,199 --> 00:04:53,820
previous chapters appealing to a little

99
00:04:53,820 --> 00:04:55,500
linear algebra

100
00:04:55,500 --> 00:04:58,199
differentiation in calculus and the

101
00:04:58,199 --> 00:05:00,000
Taylor series expansion

102
00:05:00,000 --> 00:05:01,740
some of those topics are in the

103
00:05:01,740 --> 00:05:04,080
appendices and and even the points that

104
00:05:04,080 --> 00:05:06,540
people like on the the structure of what

105
00:05:06,540 --> 00:05:09,479
would have been helpful to know when

106
00:05:09,479 --> 00:05:11,820
those are all helpful comments to write

107
00:05:11,820 --> 00:05:13,560
or to surface

108
00:05:13,560 --> 00:05:16,740
there's no one single linear layout that

109
00:05:16,740 --> 00:05:18,540
would have been perfect for every single

110
00:05:18,540 --> 00:05:21,360
case but even just like

111
00:05:21,360 --> 00:05:23,220
sharing what would have made it helpful

112
00:05:23,220 --> 00:05:25,740
for any given person or what detours or

113
00:05:25,740 --> 00:05:28,080
cul-de-sacs they went on

114
00:05:28,080 --> 00:05:32,000
that's all really useful

115
00:05:34,380 --> 00:05:36,060
those who do not want to delve into the

116
00:05:36,060 --> 00:05:37,380
theoretical underpinnings May skip this

117
00:05:37,380 --> 00:05:39,620
chapter

118
00:05:39,780 --> 00:05:42,380
so soon

119
00:05:44,780 --> 00:05:49,560
Bayes equation in 4.1

120
00:05:49,560 --> 00:05:50,940
does anyone want to just bring up

121
00:05:50,940 --> 00:05:53,220
anything or describe it or call it like

122
00:05:53,220 --> 00:05:55,759
they see it

123
00:06:01,500 --> 00:06:03,060
let's see if we have the natural

124
00:06:03,060 --> 00:06:05,900
language description

125
00:06:07,440 --> 00:06:11,820
so here with the active ontology we have

126
00:06:11,820 --> 00:06:13,620
if you want to see like how it can be

127
00:06:13,620 --> 00:06:16,340
read out

128
00:06:16,380 --> 00:06:18,479
but a key issue

129
00:06:18,479 --> 00:06:20,759
is that whether you're summing over

130
00:06:20,759 --> 00:06:22,919
discrete opportunities

131
00:06:22,919 --> 00:06:28,080
or integrating over continuous ranges

132
00:06:28,080 --> 00:06:30,780
these sums are integrals

133
00:06:30,780 --> 00:06:34,080
are very computationally challenging or

134
00:06:34,080 --> 00:06:37,440
even intractable

135
00:06:38,460 --> 00:06:42,000
the approach of variational inference

136
00:06:42,000 --> 00:06:44,280
is to convert this potentially difficult

137
00:06:44,280 --> 00:06:47,039
summation or integration problem

138
00:06:47,039 --> 00:06:50,100
into an optimization problem

139
00:06:50,100 --> 00:06:53,819
so instead of just like one Mega plug

140
00:06:53,819 --> 00:06:56,520
and chug crunch the numbers and get one

141
00:06:56,520 --> 00:06:58,380
outcome at the end

142
00:06:58,380 --> 00:07:01,160
there's going to be a incrementally

143
00:07:01,160 --> 00:07:03,720
optimizable approach

144
00:07:03,720 --> 00:07:06,180
that we can iterate on

145
00:07:06,180 --> 00:07:09,120
with known computational complexity

146
00:07:09,120 --> 00:07:12,660
that under some or potentially a broad

147
00:07:12,660 --> 00:07:14,580
range of settings

148
00:07:14,580 --> 00:07:17,660
will bring us closer to the best

149
00:07:17,660 --> 00:07:20,580
reasonable compromise answer

150
00:07:20,580 --> 00:07:24,060
and so we've exchanged this

151
00:07:24,060 --> 00:07:25,759
vast

152
00:07:25,759 --> 00:07:29,759
One-Shot calculation

153
00:07:29,759 --> 00:07:32,360
into something that can be incrementally

154
00:07:32,360 --> 00:07:34,919
through time on a real computer

155
00:07:34,919 --> 00:07:37,919
optimized

156
00:07:39,419 --> 00:07:40,979
to understand how this works we need to

157
00:07:40,979 --> 00:07:43,380
appeal to Jensen's inequality which says

158
00:07:43,380 --> 00:07:45,840
that the log note that's not the log

159
00:07:45,840 --> 00:07:48,020
squared that's a no subscript

160
00:07:48,020 --> 00:07:51,360
the log of an average

161
00:07:51,360 --> 00:07:54,000
is always greater than or equal to the

162
00:07:54,000 --> 00:07:57,000
average of a log and figure 4.1 shows

163
00:07:57,000 --> 00:08:00,599
that so does anyone

164
00:08:00,599 --> 00:08:03,240
math professor or otherwise want to give

165
00:08:03,240 --> 00:08:04,919
like a thought

166
00:08:04,919 --> 00:08:09,180
on this figure 4.1 what is happening and

167
00:08:09,180 --> 00:08:12,539
why why is this relevant

168
00:08:12,539 --> 00:08:15,060
for how we're going to be continuing on

169
00:08:15,060 --> 00:08:18,860
with Bayesian equations

170
00:08:21,720 --> 00:08:24,120
maybe not a particularly deep thought

171
00:08:24,120 --> 00:08:26,699
but um so this is because of the the

172
00:08:26,699 --> 00:08:29,039
second derivative the fact that this is

173
00:08:29,039 --> 00:08:31,199
um kind of curving down the the

174
00:08:31,199 --> 00:08:32,580
steepness is getting less that this

175
00:08:32,580 --> 00:08:33,659
holds true

176
00:08:33,659 --> 00:08:35,580
um I created a little animation that I

177
00:08:35,580 --> 00:08:37,979
can can link to looking at some playing

178
00:08:37,979 --> 00:08:40,200
along along X

179
00:08:40,200 --> 00:08:42,599
um and then comparing the the logarithm

180
00:08:42,599 --> 00:08:46,020
of the um uh the expected value with the

181
00:08:46,020 --> 00:08:48,000
expected value of the logarithm just to

182
00:08:48,000 --> 00:08:49,740
get a bit of an intuition as to as to

183
00:08:49,740 --> 00:08:50,820
why this holds true but I think that

184
00:08:50,820 --> 00:08:52,680
this this image is pretty nice this sort

185
00:08:52,680 --> 00:08:54,839
of shows why it is you can imagine that

186
00:08:54,839 --> 00:08:56,940
any function which is curving down like

187
00:08:56,940 --> 00:08:58,620
this is going to have this property and

188
00:08:58,620 --> 00:09:00,600
any function which curves up is going to

189
00:09:00,600 --> 00:09:02,220
have the opposite property

190
00:09:02,220 --> 00:09:04,019
um I can I can paste a link into the

191
00:09:04,019 --> 00:09:05,519
into the comments of the little

192
00:09:05,519 --> 00:09:08,220
animation that I created there

193
00:09:08,220 --> 00:09:10,620
awesome and can you maybe add one piece

194
00:09:10,620 --> 00:09:13,019
how is this connected to surprise or how

195
00:09:13,019 --> 00:09:15,660
does anybody see Jensen's inequality

196
00:09:15,660 --> 00:09:17,640
maybe as leading to

197
00:09:17,640 --> 00:09:18,959
the way that we're going to talk about

198
00:09:18,959 --> 00:09:21,560
surprise

199
00:09:24,300 --> 00:09:26,399
so I knew that this is going to give us

200
00:09:26,399 --> 00:09:28,860
a a bound on surprise because surprise

201
00:09:28,860 --> 00:09:33,000
is the average of the of X log X or P

202
00:09:33,000 --> 00:09:36,839
um P of X log P of X essentially

203
00:09:36,839 --> 00:09:39,480
um that we're averaging the amount of

204
00:09:39,480 --> 00:09:41,160
information that we will gain on making

205
00:09:41,160 --> 00:09:42,839
any particular

206
00:09:42,839 --> 00:09:44,580
um uh measurement

207
00:09:44,580 --> 00:09:45,899
um it's the average information that

208
00:09:45,899 --> 00:09:47,160
you'll gain

209
00:09:47,160 --> 00:09:49,140
um and so that's already an expectation

210
00:09:49,140 --> 00:09:51,360
and it allows us to do an expectation of

211
00:09:51,360 --> 00:09:53,580
X and take the logarithm of that which

212
00:09:53,580 --> 00:09:57,440
gives us a bound on the on this price

213
00:09:57,600 --> 00:10:00,600
thank you it's definitely like

214
00:10:00,600 --> 00:10:03,300
some words that fly by and to hear it

215
00:10:03,300 --> 00:10:07,140
many times is good

216
00:10:07,140 --> 00:10:10,620
it does turn out that this property of

217
00:10:10,620 --> 00:10:12,360
logs and there's going to be multiple

218
00:10:12,360 --> 00:10:14,459
properties of logs that come in to help

219
00:10:14,459 --> 00:10:17,339
us in different ways like the ability to

220
00:10:17,339 --> 00:10:18,720
take two numbers that are multiplied

221
00:10:18,720 --> 00:10:21,180
inside of a log and make it into log of

222
00:10:21,180 --> 00:10:24,420
a plus log of B and so on but this

223
00:10:24,420 --> 00:10:26,160
curvature property

224
00:10:26,160 --> 00:10:30,540
is going to help us talk about expected

225
00:10:30,540 --> 00:10:35,360
surprise okay I'm going to bring up the

226
00:10:37,620 --> 00:10:40,220
awesome

227
00:10:42,480 --> 00:10:45,300
could you connect that to any

228
00:10:45,300 --> 00:10:47,160
um sampling

229
00:10:47,160 --> 00:10:49,019
or or like what are we being surprised

230
00:10:49,019 --> 00:10:50,700
about something on the y-axis something

231
00:10:50,700 --> 00:10:53,160
on the x-axis where are the data and so

232
00:10:53,160 --> 00:10:54,899
on

233
00:10:54,899 --> 00:10:57,000
um so so this is just a normally

234
00:10:57,000 --> 00:10:58,980
distributed X here

235
00:10:58,980 --> 00:11:01,620
um so there's some probability of our X

236
00:11:01,620 --> 00:11:04,140
um and then the uh what you're looking

237
00:11:04,140 --> 00:11:05,160
at

238
00:11:05,160 --> 00:11:08,100
um I guess in the uh in the

239
00:11:08,100 --> 00:11:10,200
I've got to think about this now

240
00:11:10,200 --> 00:11:12,260
um

241
00:11:12,360 --> 00:11:13,980
no I don't want to say it would say

242
00:11:13,980 --> 00:11:15,360
without thinking about it

243
00:11:15,360 --> 00:11:17,160
um but but essentially here this is just

244
00:11:17,160 --> 00:11:19,440
taking some expectation over X and

245
00:11:19,440 --> 00:11:22,079
calculating the the logarithm uh of that

246
00:11:22,079 --> 00:11:24,000
expectation and the expectation of the

247
00:11:24,000 --> 00:11:27,300
logarithm just sampling so starting on

248
00:11:27,300 --> 00:11:29,519
the left-hand side with with just one

249
00:11:29,519 --> 00:11:31,140
sample and then increasing to 100

250
00:11:31,140 --> 00:11:33,180
samples and just seeing how the

251
00:11:33,180 --> 00:11:35,579
relationship between the the two

252
00:11:35,579 --> 00:11:39,660
um uh those two pointers

253
00:11:39,660 --> 00:11:42,420
cool that's that's all it's doing

254
00:11:42,420 --> 00:11:44,579
cool

255
00:11:44,579 --> 00:11:47,579
now we're going to take advantage of

256
00:11:47,579 --> 00:11:50,640
that property so we return to equation

257
00:11:50,640 --> 00:11:53,160
4.1

258
00:11:53,160 --> 00:11:55,380
so this was the one with the intractable

259
00:11:55,380 --> 00:11:58,640
sum or integration

260
00:11:59,399 --> 00:12:01,980
we're going to return to 4.1

261
00:12:01,980 --> 00:12:05,700
and multiply by an arbitrary function Q

262
00:12:05,700 --> 00:12:08,300
so we took this line with Big Challenge

263
00:12:08,300 --> 00:12:11,459
and we said there's some arbitrary

264
00:12:11,459 --> 00:12:12,779
function Q

265
00:12:12,779 --> 00:12:15,839
and anything divided by itself is one so

266
00:12:15,839 --> 00:12:18,360
this is equivalent to multiplying by one

267
00:12:18,360 --> 00:12:20,579
on this left side

268
00:12:20,579 --> 00:12:23,220
and it turns out that Q is going to be

269
00:12:23,220 --> 00:12:26,040
like this variational distribution from

270
00:12:26,040 --> 00:12:27,660
a known family

271
00:12:27,660 --> 00:12:30,500
with easily optimizable characteristics

272
00:12:30,500 --> 00:12:34,339
that we know we can control

273
00:12:34,860 --> 00:12:36,300
now

274
00:12:36,300 --> 00:12:38,880
there's going to be a log taken on both

275
00:12:38,880 --> 00:12:40,019
sides

276
00:12:40,019 --> 00:12:42,180
so Q of X over Q of X which is

277
00:12:42,180 --> 00:12:43,740
equivalent to just one

278
00:12:43,740 --> 00:12:45,120
and then

279
00:12:45,120 --> 00:12:46,800
there's going to be

280
00:12:46,800 --> 00:12:48,240
um

281
00:12:48,240 --> 00:12:52,639
the log taken on both sides

282
00:12:53,399 --> 00:12:55,320
we can now interpret this expression as

283
00:12:55,320 --> 00:12:58,500
an expectation fancy e not e cubed not

284
00:12:58,500 --> 00:13:00,420
just a footnote again

285
00:13:00,420 --> 00:13:05,459
e of a ratio between two probabilities

286
00:13:05,459 --> 00:13:06,959
so

287
00:13:06,959 --> 00:13:09,540
here's the P of Y of of this is the

288
00:13:09,540 --> 00:13:11,700
joint probability distribution of hidden

289
00:13:11,700 --> 00:13:14,399
States and data

290
00:13:14,399 --> 00:13:17,760
and now this

291
00:13:17,760 --> 00:13:20,040
allows us to construct joint

292
00:13:20,040 --> 00:13:23,639
distribution over the one we control

293
00:13:23,639 --> 00:13:25,860
natural log of that

294
00:13:25,860 --> 00:13:27,899
natural log on the outside

295
00:13:27,899 --> 00:13:30,720
is always greater than

296
00:13:30,720 --> 00:13:35,339
the expectation of the log

297
00:13:35,339 --> 00:13:37,800
of an expectation

298
00:13:37,800 --> 00:13:40,320
is always greater than the expectation

299
00:13:40,320 --> 00:13:43,220
of a log

300
00:13:43,680 --> 00:13:46,079
log of an average

301
00:13:46,079 --> 00:13:47,880
is always greater than or equal to the

302
00:13:47,880 --> 00:13:50,720
average of a log

303
00:13:51,019 --> 00:13:53,519
this value

304
00:13:53,519 --> 00:13:55,380
so this is again this is the part that

305
00:13:55,380 --> 00:13:56,700
we want to solve

306
00:13:56,700 --> 00:14:00,540
and this is another way to rewrite it

307
00:14:00,540 --> 00:14:02,519
that

308
00:14:02,519 --> 00:14:05,639
is strictly bounded via Jensen's

309
00:14:05,639 --> 00:14:07,200
inequality

310
00:14:07,200 --> 00:14:08,820
to this

311
00:14:08,820 --> 00:14:10,680
value

312
00:14:10,680 --> 00:14:15,300
which is defined by triangle

313
00:14:15,300 --> 00:14:18,360
the negative variational free energy

314
00:14:18,360 --> 00:14:21,839
as a function of Q the distribution we

315
00:14:21,839 --> 00:14:25,639
control and Y the data

316
00:14:28,920 --> 00:14:31,260
the smaller the free energy the closer

317
00:14:31,260 --> 00:14:35,639
it is to the negative log model evidence

318
00:14:35,639 --> 00:14:37,560
especially because the log has these

319
00:14:37,560 --> 00:14:39,959
like somewhat strange seeming properties

320
00:14:39,959 --> 00:14:42,480
like it shoots off down to negative

321
00:14:42,480 --> 00:14:45,180
Infinity over here and so on like and

322
00:14:45,180 --> 00:14:47,399
there's negatives and positives so you

323
00:14:47,399 --> 00:14:48,300
know

324
00:14:48,300 --> 00:14:50,220
take it slow to see what each line is

325
00:14:50,220 --> 00:14:53,220
actually telling us

326
00:14:53,220 --> 00:14:56,279
um but this is the

327
00:14:56,279 --> 00:15:01,199
um standard from statistical physics

328
00:15:01,199 --> 00:15:03,540
what is called the negative variational

329
00:15:03,540 --> 00:15:06,139
free energy

330
00:15:09,899 --> 00:15:12,480
when free energy is small

331
00:15:12,480 --> 00:15:14,639
it's close to the negative log model

332
00:15:14,639 --> 00:15:17,120
evidence

333
00:15:17,579 --> 00:15:20,699
negative log model evidence so if the

334
00:15:20,699 --> 00:15:23,279
log model evidence were really high

335
00:15:23,279 --> 00:15:25,920
like then the log because of it's always

336
00:15:25,920 --> 00:15:27,660
going up and to the right but slowing

337
00:15:27,660 --> 00:15:29,399
down in how it does that

338
00:15:29,399 --> 00:15:32,040
it means you're also monotonically

339
00:15:32,040 --> 00:15:34,860
getting higher model evidence

340
00:15:34,860 --> 00:15:39,959
and also the natural log of P of Y is um

341
00:15:39,959 --> 00:15:43,040
how that showed

342
00:15:44,820 --> 00:15:49,459
any thoughts on equation 4.3

343
00:16:11,760 --> 00:16:13,860
4.3 provides some intuition for the

344
00:16:13,860 --> 00:16:15,360
roles of the free energy and the Q

345
00:16:15,360 --> 00:16:17,899
distribution

346
00:16:17,940 --> 00:16:20,040
two quantities that were difficult to

347
00:16:20,040 --> 00:16:21,300
compute without the variational

348
00:16:21,300 --> 00:16:24,139
approximation

349
00:16:26,760 --> 00:16:28,560
so the expectation

350
00:16:28,560 --> 00:16:30,839
about the distribution that we're

351
00:16:30,839 --> 00:16:33,959
getting to control and parameterize

352
00:16:33,959 --> 00:16:37,579
the expectation of

353
00:16:37,740 --> 00:16:39,420
the likelihood

354
00:16:39,420 --> 00:16:41,699
which is natural log of P

355
00:16:41,699 --> 00:16:45,180
of the joint distribution between the

356
00:16:45,180 --> 00:16:48,000
hidden States and the data

357
00:16:48,000 --> 00:16:49,800
is

358
00:16:49,800 --> 00:16:53,699
the negative variational free energy

359
00:16:53,699 --> 00:16:58,860
as a functional of Q and data

360
00:16:58,860 --> 00:17:00,240
Plus

361
00:17:00,240 --> 00:17:03,660
an expectation of

362
00:17:03,660 --> 00:17:07,159
the model likelihood of Q

363
00:17:11,939 --> 00:17:15,240
there's some rearranging

364
00:17:15,240 --> 00:17:18,919
bringing the F onto this side

365
00:17:19,079 --> 00:17:23,099
and then using the

366
00:17:23,099 --> 00:17:26,880
um properties of differences of logs

367
00:17:26,880 --> 00:17:30,120
to get us to this

368
00:17:30,120 --> 00:17:33,080
representation

369
00:17:36,419 --> 00:17:38,940
variational free energy

370
00:17:38,940 --> 00:17:42,960
with two components

371
00:17:43,260 --> 00:17:45,179
the Raw

372
00:17:45,179 --> 00:17:49,740
model evidence of the data

373
00:17:49,740 --> 00:17:51,360
and

374
00:17:51,360 --> 00:17:53,460
a KL Divergence

375
00:17:53,460 --> 00:17:56,100
between the distribution we control

376
00:17:56,100 --> 00:17:58,380
about X in States

377
00:17:58,380 --> 00:18:02,220
and this

378
00:18:05,220 --> 00:18:08,580
P distribution of x given the data

379
00:18:08,580 --> 00:18:12,379
let me just add in your comment there

380
00:18:12,539 --> 00:18:15,120
so Jonathan wrote um the entropy is the

381
00:18:15,120 --> 00:18:16,860
average surprise

382
00:18:16,860 --> 00:18:19,080
surprise is on a single measurement not

383
00:18:19,080 --> 00:18:21,120
self and average it is simply the

384
00:18:21,120 --> 00:18:23,280
probability of negative natural log of P

385
00:18:23,280 --> 00:18:24,660
of x

386
00:18:24,660 --> 00:18:27,440
thank you

387
00:18:30,059 --> 00:18:32,220
the KL Divergence is defined in the

388
00:18:32,220 --> 00:18:33,539
second line as the expected difference

389
00:18:33,539 --> 00:18:37,320
between two different log probabilities

390
00:18:37,320 --> 00:18:41,940
this is a measure sign a proper measure

391
00:18:41,940 --> 00:18:43,620
of how different two probability

392
00:18:43,620 --> 00:18:47,340
distributions are from one another

393
00:18:47,340 --> 00:18:48,780
so

394
00:18:48,780 --> 00:18:53,280
when there's no Divergence

395
00:18:53,280 --> 00:18:56,520
between the distribution that is under

396
00:18:56,520 --> 00:18:57,780
control

397
00:18:57,780 --> 00:18:59,760
and the sort of true distribution that

398
00:18:59,760 --> 00:19:01,500
we'd want to estimate

399
00:19:01,500 --> 00:19:04,559
then the relative variational free

400
00:19:04,559 --> 00:19:07,020
energy has been minimized because the

401
00:19:07,020 --> 00:19:09,600
log model evidence is just what whatever

402
00:19:09,600 --> 00:19:11,400
it is it is

403
00:19:11,400 --> 00:19:15,860
with respect to being able to change Q

404
00:19:15,960 --> 00:19:19,140
which is what makes the free energy

405
00:19:19,140 --> 00:19:22,500
and optimizable quantity

406
00:19:22,500 --> 00:19:24,900
because for a queue where you know which

407
00:19:24,900 --> 00:19:29,400
knobs can be tuned

408
00:19:29,400 --> 00:19:31,200
and there are methods for the

409
00:19:31,200 --> 00:19:33,620
incremental optimization like on smooth

410
00:19:33,620 --> 00:19:36,720
optimization Landscapes and so on then

411
00:19:36,720 --> 00:19:39,900
this Divergence can be very directly

412
00:19:39,900 --> 00:19:41,039
tackled

413
00:19:41,039 --> 00:19:43,260
and this is just like it can be kind of

414
00:19:43,260 --> 00:19:45,720
just

415
00:19:45,720 --> 00:19:47,700
um

416
00:19:47,700 --> 00:19:50,460
left unconsidered because it's it's a

417
00:19:50,460 --> 00:19:54,200
constant with respect to the same data

418
00:19:56,820 --> 00:19:58,679
assumed

419
00:19:58,679 --> 00:20:00,780
what is

420
00:20:00,780 --> 00:20:04,100
it can be assumed

421
00:20:04,559 --> 00:20:07,380
yeah assume that whatever it is with

422
00:20:07,380 --> 00:20:09,299
respect to changing the knobs on cue it

423
00:20:09,299 --> 00:20:10,500
doesn't matter

424
00:20:10,500 --> 00:20:14,539
because Q is not included in this term

425
00:20:14,640 --> 00:20:18,080
Q only shows up here

426
00:20:21,539 --> 00:20:23,960
so

427
00:20:24,240 --> 00:20:26,220
um

428
00:20:26,220 --> 00:20:30,120
okay yes definitely a few

429
00:20:30,120 --> 00:20:33,020
um tricky

430
00:20:33,360 --> 00:20:36,440
simple seeming but very ineffable

431
00:20:36,440 --> 00:20:39,360
Transformations with logs

432
00:20:39,360 --> 00:20:42,600
and the Bayes equation

433
00:20:42,600 --> 00:20:45,480
that get us to

434
00:20:45,480 --> 00:20:47,039
a form

435
00:20:47,039 --> 00:20:49,740
for variational free energy

436
00:20:49,740 --> 00:20:53,820
which if we remember back so long ago

437
00:20:53,820 --> 00:20:58,220
is going to help us balance surprise

438
00:21:00,299 --> 00:21:02,400
and there's going to be like a special

439
00:21:02,400 --> 00:21:04,260
case that doesn't necessarily ever get

440
00:21:04,260 --> 00:21:06,059
realized where the free energy like the

441
00:21:06,059 --> 00:21:08,460
bound Divergence goes to zero

442
00:21:08,460 --> 00:21:10,740
but in any situation where the

443
00:21:10,740 --> 00:21:12,720
distribution p is maybe even of a

444
00:21:12,720 --> 00:21:14,700
different structure than Q there's

445
00:21:14,700 --> 00:21:16,080
always going to be some Divergence

446
00:21:16,080 --> 00:21:19,440
however by reducing that Divergence

447
00:21:19,440 --> 00:21:21,179
we will be

448
00:21:21,179 --> 00:21:23,820
doing the best optimization we can um

449
00:21:23,820 --> 00:21:26,899
Michael or anyone

450
00:21:36,600 --> 00:21:38,880
okay yeah these are some some of the

451
00:21:38,880 --> 00:21:39,900
most

452
00:21:39,900 --> 00:21:42,059
pure mathy

453
00:21:42,059 --> 00:21:45,360
pages in The textbook

454
00:21:45,360 --> 00:21:48,360
sorry I have my mic's on it's all good

455
00:21:48,360 --> 00:21:51,299
no it's it's true it's like where did

456
00:21:51,299 --> 00:21:53,700
active inference go but this is totally

457
00:21:53,700 --> 00:21:57,059
in it but these several pages

458
00:21:57,059 --> 00:21:59,760
which maybe in a um undergraduate

459
00:21:59,760 --> 00:22:01,080
statistics course could have been

460
00:22:01,080 --> 00:22:03,840
unpacked into several

461
00:22:03,840 --> 00:22:07,080
lectures and homeworks

462
00:22:07,080 --> 00:22:09,840
are are very fundamental

463
00:22:09,840 --> 00:22:12,240
but they're very much

464
00:22:12,240 --> 00:22:17,360
brought in quickly here to get us to

465
00:22:17,360 --> 00:22:21,979
this variational free energy

466
00:22:23,400 --> 00:22:25,860
as someone who is not the mathematician

467
00:22:25,860 --> 00:22:30,059
but um uh spoke it at one point and um

468
00:22:30,059 --> 00:22:32,880
is grappling with this question of how

469
00:22:32,880 --> 00:22:35,580
do I think about these objects in order

470
00:22:35,580 --> 00:22:38,880
to speak to others about it earlier in

471
00:22:38,880 --> 00:22:40,799
at the beginning there was this

472
00:22:40,799 --> 00:22:44,419
um I was thinking about the original uh

473
00:22:44,419 --> 00:22:49,440
uh formula as inputs to a process that

474
00:22:49,440 --> 00:22:51,360
then uh there was a calculation being

475
00:22:51,360 --> 00:22:54,000
done and that that input was one of the

476
00:22:54,000 --> 00:22:58,080
false characterizations that um that

477
00:22:58,080 --> 00:23:01,260
it's an inflow so it's not a single but

478
00:23:01,260 --> 00:23:03,179
it's a it's this idea of there's a

479
00:23:03,179 --> 00:23:05,760
continuous and to think of surprise as

480
00:23:05,760 --> 00:23:08,940
also as a as a living flowing thing is

481
00:23:08,940 --> 00:23:10,860
also one of those kind of hard to

482
00:23:10,860 --> 00:23:12,840
Grapple with that we're not talking

483
00:23:12,840 --> 00:23:16,020
inputs to to you know but rather flow to

484
00:23:16,020 --> 00:23:19,260
float and that there's the

485
00:23:19,260 --> 00:23:20,960
um the

486
00:23:20,960 --> 00:23:24,419
hearing the the the variability right

487
00:23:24,419 --> 00:23:26,640
what's not explained

488
00:23:26,640 --> 00:23:29,100
um is is the

489
00:23:29,100 --> 00:23:31,980
is is is the intuition that's hard to

490
00:23:31,980 --> 00:23:34,380
cultivate for those of us who these

491
00:23:34,380 --> 00:23:37,320
these these formulas just kind of

492
00:23:37,320 --> 00:23:39,480
paralyzes and and it's and tried to

493
00:23:39,480 --> 00:23:41,700
translate what are we learning here

494
00:23:41,700 --> 00:23:46,020
um is that's where I'm I'm realizing my

495
00:23:46,020 --> 00:23:48,780
existing language doesn't capture what

496
00:23:48,780 --> 00:23:51,419
the mathematics is describing so I'm

497
00:23:51,419 --> 00:23:53,940
contributing my my confusion I hope

498
00:23:53,940 --> 00:23:56,360
articulately

499
00:23:56,360 --> 00:23:58,080
thank you

500
00:23:58,080 --> 00:23:59,760
um just to Jonathan's comment and then

501
00:23:59,760 --> 00:24:02,220
back to that so um Jonathan yes however

502
00:24:02,220 --> 00:24:03,900
you whatever format you're taking your

503
00:24:03,900 --> 00:24:04,980
notes in

504
00:24:04,980 --> 00:24:07,980
there's there's always going to be a way

505
00:24:07,980 --> 00:24:12,059
um in the equations themselves

506
00:24:12,059 --> 00:24:15,840
you could always tag anything

507
00:24:15,840 --> 00:24:18,059
like you could add descriptions or just

508
00:24:18,059 --> 00:24:21,240
notes or other unpackings

509
00:24:21,240 --> 00:24:24,600
or other links in the equation section

510
00:24:24,600 --> 00:24:28,260
or we can go and modify or revive the

511
00:24:28,260 --> 00:24:30,419
math learning group

512
00:24:30,419 --> 00:24:32,520
with any different notes that people

513
00:24:32,520 --> 00:24:35,820
have on notation on different resources

514
00:24:35,820 --> 00:24:38,700
to look at or like kind of

515
00:24:38,700 --> 00:24:42,120
chapter wise descriptions of math

516
00:24:42,120 --> 00:24:43,919
um

517
00:24:43,919 --> 00:24:47,400
hopefully not too confused or or devalue

518
00:24:47,400 --> 00:24:48,960
this too much

519
00:24:48,960 --> 00:24:52,740
but I hope to bring even

520
00:24:52,740 --> 00:24:53,820
um

521
00:24:53,820 --> 00:24:56,039
more clarity ultimately

522
00:24:56,039 --> 00:24:58,799
if you check out live stream number 52

523
00:24:58,799 --> 00:25:03,240
especially the final hour of 52.2

524
00:25:03,240 --> 00:25:05,460
you'll hear about Lance DeCosta sharing

525
00:25:05,460 --> 00:25:08,760
about the free energy Divergence is just

526
00:25:08,760 --> 00:25:11,100
one discrepancy measure that can be

527
00:25:11,100 --> 00:25:12,780
utilized

528
00:25:12,780 --> 00:25:16,980
in pursuit of a unified imperative for

529
00:25:16,980 --> 00:25:19,559
on the inbound signal processing and on

530
00:25:19,559 --> 00:25:22,620
the outbound control theory so

531
00:25:22,620 --> 00:25:24,900
single processing alone is just passive

532
00:25:24,900 --> 00:25:26,900
inference control theory alone

533
00:25:26,900 --> 00:25:29,520
essentially has to presuppose knowledge

534
00:25:29,520 --> 00:25:30,900
of the world or at least it's less

535
00:25:30,900 --> 00:25:33,360
considered with it octave inference is

536
00:25:33,360 --> 00:25:36,659
considering the inbound and the outbound

537
00:25:36,659 --> 00:25:38,039
and

538
00:25:38,039 --> 00:25:39,840
free energy

539
00:25:39,840 --> 00:25:42,539
it turns out is an extremely convenient

540
00:25:42,539 --> 00:25:45,600
discrepancy measure which is why even

541
00:25:45,600 --> 00:25:48,679
before all of the 2000s and all of that

542
00:25:48,679 --> 00:25:51,600
free energy and variational autoencoders

543
00:25:51,600 --> 00:25:53,760
Bayesian statistics were already in

544
00:25:53,760 --> 00:25:56,640
heavy use because free energy as abound

545
00:25:56,640 --> 00:25:58,860
on surprise in Bayesian statistics is

546
00:25:58,860 --> 00:26:00,000
just useful

547
00:26:00,000 --> 00:26:02,100
but it's not the only discrepancy

548
00:26:02,100 --> 00:26:04,200
measure but in the variational setting

549
00:26:04,200 --> 00:26:06,600
where we're learning it here it is the

550
00:26:06,600 --> 00:26:09,179
Divergence or the discrepancy measure

551
00:26:09,179 --> 00:26:12,480
but thinking about this less as like

552
00:26:12,480 --> 00:26:14,460
we're getting to active inference from

553
00:26:14,460 --> 00:26:17,159
free energy what is free energy

554
00:26:17,159 --> 00:26:18,299
put

555
00:26:18,299 --> 00:26:20,640
the horse before the cart the cybernetic

556
00:26:20,640 --> 00:26:23,100
Loop and the signal processing inbound

557
00:26:23,100 --> 00:26:25,860
control theory outbound unified

558
00:26:25,860 --> 00:26:27,539
imperative first

559
00:26:27,539 --> 00:26:30,120
and then these are going to be different

560
00:26:30,120 --> 00:26:34,320
useful approximate Bayesian calculations

561
00:26:34,320 --> 00:26:36,779
but not the only calculations that can

562
00:26:36,779 --> 00:26:37,919
be used

563
00:26:37,919 --> 00:26:42,559
for modeling in that cybernetic setting

564
00:26:48,120 --> 00:26:50,580
like just one note then Ali the KL

565
00:26:50,580 --> 00:26:52,460
Divergence describes the informational

566
00:26:52,460 --> 00:26:55,080
distance not exactly distance but

567
00:26:55,080 --> 00:26:57,779
basically between two distributions but

568
00:26:57,779 --> 00:27:00,419
he described another distance measure

569
00:27:00,419 --> 00:27:03,299
the Earth movers distance how much Earth

570
00:27:03,299 --> 00:27:05,159
you need to move from one distribution

571
00:27:05,159 --> 00:27:06,480
to make it resemble the other

572
00:27:06,480 --> 00:27:08,520
distribution since they both have an

573
00:27:08,520 --> 00:27:10,200
area under the curve let's just say of

574
00:27:10,200 --> 00:27:11,279
one

575
00:27:11,279 --> 00:27:14,880
and those are different measures

576
00:27:14,880 --> 00:27:16,799
and they have different properties there

577
00:27:16,799 --> 00:27:18,000
are situations where one might be

578
00:27:18,000 --> 00:27:20,039
appropriate or not so one can actually

579
00:27:20,039 --> 00:27:21,900
do active inference

580
00:27:21,900 --> 00:27:24,600
without free energy calculations there

581
00:27:24,600 --> 00:27:25,919
could be a different discrepancy

582
00:27:25,919 --> 00:27:28,559
calculation it's just that this is a

583
00:27:28,559 --> 00:27:30,179
go-to choice

584
00:27:30,179 --> 00:27:31,799
for the same reason why free energy

585
00:27:31,799 --> 00:27:34,980
calculations are not Arcane their

586
00:27:34,980 --> 00:27:38,039
workhorses in variational autoencoders

587
00:27:38,039 --> 00:27:41,360
and so on Ali

588
00:27:41,820 --> 00:27:45,200
it's just related to what you mentioned

589
00:27:45,200 --> 00:27:49,080
uh two related piece of literature just

590
00:27:49,080 --> 00:27:52,980
came to mind one is this well-known

591
00:27:52,980 --> 00:27:55,860
paper A Tale of Two densities uh active

592
00:27:55,860 --> 00:27:58,640
inference is inactive inference which

593
00:27:58,640 --> 00:28:01,200
contrasts two interpretations of active

594
00:28:01,200 --> 00:28:03,980
inference namely structural

595
00:28:03,980 --> 00:28:06,020
representationalist representative

596
00:28:06,020 --> 00:28:10,380
interpretation and inactive inference

597
00:28:10,380 --> 00:28:13,620
representation uh which uh they

598
00:28:13,620 --> 00:28:17,220
basically somehow lean towards

599
00:28:17,220 --> 00:28:19,620
this is the second type of

600
00:28:19,620 --> 00:28:21,960
um the interpretation and the other one

601
00:28:21,960 --> 00:28:26,340
uh which is not so uh well known is a

602
00:28:26,340 --> 00:28:30,059
chapter from uh this book uh called

603
00:28:30,059 --> 00:28:32,520
um measuring and modeling persons and

604
00:28:32,520 --> 00:28:36,059
situations by uh chapter 18 from this

605
00:28:36,059 --> 00:28:39,179
book by Adam saffron

606
00:28:39,179 --> 00:28:42,480
um and Colin D young uh namely

607
00:28:42,480 --> 00:28:45,120
integrating cybernetic Big Five Theory

608
00:28:45,120 --> 00:28:48,419
with the free energy principle uh so

609
00:28:48,419 --> 00:28:53,159
this is also a really helpful synoptic

610
00:28:53,159 --> 00:28:55,740
um comparative analysis between

611
00:28:55,740 --> 00:28:58,860
different cybernetic and control

612
00:28:58,860 --> 00:29:01,980
theories and uh active inference so yeah

613
00:29:01,980 --> 00:29:04,080
I just wanted to mention those

614
00:29:04,080 --> 00:29:06,179
um two papers

615
00:29:06,179 --> 00:29:08,279
thank you

616
00:29:08,279 --> 00:29:11,520
so again not to not to devalue this

617
00:29:11,520 --> 00:29:14,340
whole variational sojourn

618
00:29:14,340 --> 00:29:16,380
but this is the variational approach

619
00:29:16,380 --> 00:29:18,299
that's why we went in with a queue

620
00:29:18,299 --> 00:29:20,279
another approach could have been you

621
00:29:20,279 --> 00:29:23,159
know this is a really intractable sum

622
00:29:23,159 --> 00:29:25,980
we're going to sample from it with Monte

623
00:29:25,980 --> 00:29:28,500
Carlo simulations and we're not going to

624
00:29:28,500 --> 00:29:30,720
do variational distribution at all

625
00:29:30,720 --> 00:29:33,960
so we can still

626
00:29:33,960 --> 00:29:36,360
talk about active inference

627
00:29:36,360 --> 00:29:38,880
even without this variational technique

628
00:29:38,880 --> 00:29:42,059
so it's like this is not this is not the

629
00:29:42,059 --> 00:29:44,940
essence of active

630
00:29:44,940 --> 00:29:48,720
however variational Bayesian methods are

631
00:29:48,720 --> 00:29:51,000
incredibly powerful and scalable on

632
00:29:51,000 --> 00:29:53,220
Modern computers including in some

633
00:29:53,220 --> 00:29:55,799
places where sampling is like going to

634
00:29:55,799 --> 00:29:59,000
be costly or impossible

635
00:30:00,000 --> 00:30:01,440
but it's really interesting to think

636
00:30:01,440 --> 00:30:04,080
about everything technical

637
00:30:04,080 --> 00:30:07,080
is a modeler's degree of freedom

638
00:30:07,080 --> 00:30:10,919
once one has made the decision to have a

639
00:30:10,919 --> 00:30:13,080
unified perspective on perception and

640
00:30:13,080 --> 00:30:14,580
action

641
00:30:14,580 --> 00:30:17,460
that is the active inference turn

642
00:30:17,460 --> 00:30:22,799
and now these are modelers possibilities

643
00:30:22,799 --> 00:30:26,700
sampling variational Bayesian methods

644
00:30:26,700 --> 00:30:28,440
and other approximate Bayesian

645
00:30:28,440 --> 00:30:31,820
computational methods

646
00:30:33,659 --> 00:30:34,690
and and

647
00:30:34,690 --> 00:30:34,919
[Music]

648
00:30:34,919 --> 00:30:36,000
um

649
00:30:36,000 --> 00:30:38,100
those paths are not always obvious

650
00:30:38,100 --> 00:30:39,360
because

651
00:30:39,360 --> 00:30:40,740
it's like

652
00:30:40,740 --> 00:30:42,899
the variational free energy we're using

653
00:30:42,899 --> 00:30:44,820
variational methods but that's not the

654
00:30:44,820 --> 00:30:46,380
only method that could have been used

655
00:30:46,380 --> 00:30:48,360
but then it's like

656
00:30:48,360 --> 00:30:52,820
what is most commonly shown

657
00:30:55,200 --> 00:30:56,460
um

658
00:30:56,460 --> 00:31:00,960
so long like kind of path to return to

659
00:31:00,960 --> 00:31:04,620
we got to variational free energy

660
00:31:04,620 --> 00:31:07,799
as has been reached by other Bayesian

661
00:31:07,799 --> 00:31:09,779
statisticians

662
00:31:09,779 --> 00:31:12,779
because it's a super useful form

663
00:31:12,779 --> 00:31:15,480
that takes that intractable sum or

664
00:31:15,480 --> 00:31:16,620
integral

665
00:31:16,620 --> 00:31:21,059
and it separates it into a constant term

666
00:31:21,059 --> 00:31:22,919
which we're not going to do anything to

667
00:31:22,919 --> 00:31:23,760
change

668
00:31:23,760 --> 00:31:26,460
and one term where basically we have the

669
00:31:26,460 --> 00:31:27,539
knob

670
00:31:27,539 --> 00:31:29,460
and can completely incrementally

671
00:31:29,460 --> 00:31:34,220
optimize it to reduce Divergence

672
00:31:34,440 --> 00:31:37,919
because the KL Divergence

673
00:31:37,919 --> 00:31:39,980
between the distribution that we control

674
00:31:39,980 --> 00:31:42,360
and the one that we would want to have

675
00:31:42,360 --> 00:31:44,600
no Divergence with

676
00:31:44,600 --> 00:31:48,179
is literally that is the KL Divergence

677
00:31:48,179 --> 00:31:51,179
is a difference of these two logs

678
00:31:51,179 --> 00:31:53,640
which is also why some of those

679
00:31:53,640 --> 00:31:55,440
more fundamental properties and logs

680
00:31:55,440 --> 00:31:58,140
were being brought up

681
00:31:58,140 --> 00:32:00,480
okay

682
00:32:00,480 --> 00:32:03,000
or three to calculate free energy we

683
00:32:03,000 --> 00:32:05,399
need three things data a family of

684
00:32:05,399 --> 00:32:06,899
variational distributions and a

685
00:32:06,899 --> 00:32:09,419
generative model

686
00:32:09,419 --> 00:32:11,880
so do organisms minimize free energy

687
00:32:11,880 --> 00:32:14,940
calculate free energy no

688
00:32:14,940 --> 00:32:17,520
data a family of distributions and a

689
00:32:17,520 --> 00:32:19,740
generative model do

690
00:32:19,740 --> 00:32:22,380
again hard to sometimes

691
00:32:22,380 --> 00:32:25,679
see the Rippling consequences

692
00:32:25,679 --> 00:32:29,640
but this is a pure instrumentalist

693
00:32:29,640 --> 00:32:31,440
expression

694
00:32:31,440 --> 00:32:33,779
so cells do not minimize free energy

695
00:32:33,779 --> 00:32:37,039
they don't calculate free energy

696
00:32:37,080 --> 00:32:39,980
the models do

697
00:32:41,520 --> 00:32:44,840
there are going to be two

698
00:32:44,880 --> 00:32:49,200
families Genera of generative models

699
00:32:49,200 --> 00:32:52,620
and the exact form of free energy is

700
00:32:52,620 --> 00:32:54,000
going to be slightly different for these

701
00:32:54,000 --> 00:32:55,440
two it's kind of like vertebrates and

702
00:32:55,440 --> 00:32:57,419
invertebrates it's like there's a lot of

703
00:32:57,419 --> 00:33:00,720
decisions down the line but this is like

704
00:33:00,720 --> 00:33:04,500
some of the broad taxonomic divisions in

705
00:33:04,500 --> 00:33:06,899
the generative models

706
00:33:06,899 --> 00:33:09,960
the first is going to be dealing with

707
00:33:09,960 --> 00:33:12,720
discrete State spaces and discrete

708
00:33:12,720 --> 00:33:14,880
representations of time

709
00:33:14,880 --> 00:33:17,100
the second is going to be dealing with

710
00:33:17,100 --> 00:33:20,700
continuous variables and continuous

711
00:33:20,700 --> 00:33:24,019
treatment of time

712
00:33:24,840 --> 00:33:26,220
foreign

713
00:33:26,220 --> 00:33:28,799
before showing what those generative

714
00:33:28,799 --> 00:33:32,220
models look like figure 4.2 gives some

715
00:33:32,220 --> 00:33:34,140
fun intuition

716
00:33:34,140 --> 00:33:38,940
for this Bayes graph diagram format

717
00:33:38,940 --> 00:33:42,539
so circles represent random variables

718
00:33:42,539 --> 00:33:44,519
so those are things about which we hold

719
00:33:44,519 --> 00:33:47,220
beliefs it could be a fixed or learnable

720
00:33:47,220 --> 00:33:50,940
parameter or stochastic variable

721
00:33:50,940 --> 00:33:52,559
and squares are probability

722
00:33:52,559 --> 00:33:55,200
distributions that are like connector

723
00:33:55,200 --> 00:33:57,360
pieces because they describe

724
00:33:57,360 --> 00:33:59,519
relationships

725
00:33:59,519 --> 00:34:02,220
and so these are laying out situations

726
00:34:02,220 --> 00:34:07,740
where it's like here x is Upstream of Y

727
00:34:07,740 --> 00:34:13,800
here x is Upstream causally of Y and W

728
00:34:13,800 --> 00:34:18,119
here there are two causes Upstream of Y

729
00:34:18,119 --> 00:34:20,580
and here is a chain

730
00:34:20,580 --> 00:34:22,800
of causation

731
00:34:22,800 --> 00:34:25,739
and so in this directed Bayesian graph

732
00:34:25,739 --> 00:34:27,300
setting

733
00:34:27,300 --> 00:34:30,599
these are sometimes called causal graphs

734
00:34:30,599 --> 00:34:33,179
because they summarized the causal

735
00:34:33,179 --> 00:34:36,780
influence of one variable on another

736
00:34:36,780 --> 00:34:38,339
so

737
00:34:38,339 --> 00:34:41,639
correlation is not causation

738
00:34:41,639 --> 00:34:43,859
and then it gets a little bit

739
00:34:43,859 --> 00:34:45,418
mingled because these are called

740
00:34:45,418 --> 00:34:47,460
causative graphs but this could be

741
00:34:47,460 --> 00:34:49,199
living in this region and this could be

742
00:34:49,199 --> 00:34:51,300
having this disease

743
00:34:51,300 --> 00:34:53,460
but this is just a model so that doesn't

744
00:34:53,460 --> 00:34:55,379
mean living in the region is the

745
00:34:55,379 --> 00:34:59,160
mechanistic cause of having the disease

746
00:34:59,160 --> 00:35:01,020
and that's why we're always talking

747
00:35:01,020 --> 00:35:05,099
about evaluation of different

748
00:35:05,099 --> 00:35:06,599
um

749
00:35:06,599 --> 00:35:08,760
possibilities

750
00:35:08,760 --> 00:35:10,800
um Michael how completely do these four

751
00:35:10,800 --> 00:35:12,660
representations of causality cover the

752
00:35:12,660 --> 00:35:14,940
known forms of influence

753
00:35:14,940 --> 00:35:16,740
um

754
00:35:16,740 --> 00:35:18,780
someone can give a thought my my first

755
00:35:18,780 --> 00:35:21,300
answer would be these are like this is

756
00:35:21,300 --> 00:35:23,820
like subject verb subject verb object

757
00:35:23,820 --> 00:35:26,760
you know subject you know these are like

758
00:35:26,760 --> 00:35:31,740
the Clauses that influence Networks

759
00:35:31,740 --> 00:35:34,980
are constructed from

760
00:35:34,980 --> 00:35:38,460
so in the ultimate in the ultimate

761
00:35:38,460 --> 00:35:40,920
um modular sense like this captures it

762
00:35:40,920 --> 00:35:43,260
all a influences B

763
00:35:43,260 --> 00:35:46,200
but then in the real setting with like

764
00:35:46,200 --> 00:35:50,099
interacting an emergent causation

765
00:35:50,099 --> 00:35:52,380
these get stitched together in complex

766
00:35:52,380 --> 00:35:56,280
ways and in fact the generative models

767
00:35:56,280 --> 00:35:57,780
that we're going to look at are going to

768
00:35:57,780 --> 00:36:00,000
be using this kind of a visual grammar

769
00:36:00,000 --> 00:36:02,339
to describe how different variables

770
00:36:02,339 --> 00:36:04,800
influence each other

771
00:36:04,800 --> 00:36:07,680
but it's an interesting question about

772
00:36:07,680 --> 00:36:10,200
what forms of

773
00:36:10,200 --> 00:36:13,920
appreciation influence and control exist

774
00:36:13,920 --> 00:36:15,660
and how those are represented with base

775
00:36:15,660 --> 00:36:18,078
graphs

776
00:36:20,280 --> 00:36:22,980
okay A little bit of unpacking on the

777
00:36:22,980 --> 00:36:24,960
um what they are talking about with uh

778
00:36:24,960 --> 00:36:26,880
the disease and the diagnosis and all of

779
00:36:26,880 --> 00:36:28,800
that

780
00:36:28,800 --> 00:36:32,540
we get to figure four three

781
00:36:32,579 --> 00:36:34,920
these are gonna be some core

782
00:36:34,920 --> 00:36:37,920
visualizations

783
00:36:37,920 --> 00:36:40,800
the top part of the graph is going to

784
00:36:40,800 --> 00:36:42,599
show a partially observable Markov

785
00:36:42,599 --> 00:36:45,420
decision process or a pomdp

786
00:36:45,420 --> 00:36:47,400
and that's going to be in discrete time

787
00:36:47,400 --> 00:36:49,680
in the bottom part of the graph is going

788
00:36:49,680 --> 00:36:50,780
to be

789
00:36:50,780 --> 00:36:52,560
displayed

790
00:36:52,560 --> 00:36:55,920
and juxtaposed to highlight the

791
00:36:55,920 --> 00:36:58,440
structural similarities but then there's

792
00:36:58,440 --> 00:37:00,180
going to be some very very important

793
00:37:00,180 --> 00:37:01,619
differences as well

794
00:37:01,619 --> 00:37:05,040
and that is also going to be unpacked in

795
00:37:05,040 --> 00:37:07,619
chapters 7 and 8 which are respectively

796
00:37:07,619 --> 00:37:10,079
about the discrete time and the

797
00:37:10,079 --> 00:37:12,920
continuous time settings

798
00:37:13,500 --> 00:37:15,119
um so

799
00:37:15,119 --> 00:37:18,960
these figures you will see in many

800
00:37:18,960 --> 00:37:21,839
active inference papers

801
00:37:21,839 --> 00:37:24,119
Thomas Parr in the recent book stream

802
00:37:24,119 --> 00:37:26,160
gave a little bit of a historical

803
00:37:26,160 --> 00:37:28,380
account he emphasized that earlier on

804
00:37:28,380 --> 00:37:32,700
like pre 2012 there was a lot of

805
00:37:32,700 --> 00:37:35,220
continuous time modeling like with eye

806
00:37:35,220 --> 00:37:38,700
movement and the reflexes and motors

807
00:37:38,700 --> 00:37:42,440
people then developed the discrete time

808
00:37:42,440 --> 00:37:45,119
discrete octave inference

809
00:37:45,119 --> 00:37:50,160
because it is classically used to model

810
00:37:50,160 --> 00:37:52,500
decision making categorical and discrete

811
00:37:52,500 --> 00:37:53,579
decision making

812
00:37:53,579 --> 00:37:57,359
but these two kinds of models can also

813
00:37:57,359 --> 00:38:00,599
be hybridized and that's called a hybrid

814
00:38:00,599 --> 00:38:03,660
model so here again just to see it one

815
00:38:03,660 --> 00:38:04,859
time but we'll see it again and again

816
00:38:04,859 --> 00:38:06,599
it's like

817
00:38:06,599 --> 00:38:08,280
it's like a rake

818
00:38:08,280 --> 00:38:09,839
on the bottom and then there's action

819
00:38:09,839 --> 00:38:12,000
selection part on Top

820
00:38:12,000 --> 00:38:14,579
just great time continuous time

821
00:38:14,579 --> 00:38:17,280
and then even the hybrid model it's like

822
00:38:17,280 --> 00:38:19,500
here's the big rake

823
00:38:19,500 --> 00:38:22,440
top level model and then where there's

824
00:38:22,440 --> 00:38:24,420
just a little leaf

825
00:38:24,420 --> 00:38:26,180
here

826
00:38:26,180 --> 00:38:28,980
that has a whole

827
00:38:28,980 --> 00:38:31,440
continuous time generative model grafted

828
00:38:31,440 --> 00:38:32,220
on

829
00:38:32,220 --> 00:38:35,280
so again like no GM

830
00:38:35,280 --> 00:38:38,339
no generative model is like the active

831
00:38:38,339 --> 00:38:39,839
generative model

832
00:38:39,839 --> 00:38:43,280
it's like a grammar or a toolkit for

833
00:38:43,280 --> 00:38:47,520
comparing and evaluating portfolios of

834
00:38:47,520 --> 00:38:50,160
structurally different or parametrically

835
00:38:50,160 --> 00:38:51,720
different

836
00:38:51,720 --> 00:38:53,820
models

837
00:38:53,820 --> 00:38:56,160
so it's not like there's one way to look

838
00:38:56,160 --> 00:38:57,420
at attention there's not one way to look

839
00:38:57,420 --> 00:38:58,740
at memory

840
00:38:58,740 --> 00:39:01,619
there's not one map for a territory and

841
00:39:01,619 --> 00:39:02,520
so on

842
00:39:02,520 --> 00:39:04,020
but there's going to be just like many

843
00:39:04,020 --> 00:39:05,280
kinds of

844
00:39:05,280 --> 00:39:08,760
intuitive graphical operations

845
00:39:08,760 --> 00:39:11,040
that this format is going to help us

846
00:39:11,040 --> 00:39:12,599
talk about

847
00:39:12,599 --> 00:39:15,180
like here is the past present the future

848
00:39:15,180 --> 00:39:16,380
and you say well could we have like

849
00:39:16,380 --> 00:39:18,240
three more can we go four time steps in

850
00:39:18,240 --> 00:39:20,760
the future you could or could the

851
00:39:20,760 --> 00:39:23,579
observations be smell and taste yes they

852
00:39:23,579 --> 00:39:26,280
could or could could there be 10 policy

853
00:39:26,280 --> 00:39:27,839
options yes there could be ten policy

854
00:39:27,839 --> 00:39:31,920
options so it's like just a skeleton

855
00:39:31,920 --> 00:39:34,619
that is going to be used and modified

856
00:39:34,619 --> 00:39:38,099
and kind of remixed in in many many ways

857
00:39:38,099 --> 00:39:40,099
um

858
00:39:41,520 --> 00:39:43,500
does anyone have any like thoughts or

859
00:39:43,500 --> 00:39:46,260
descriptions on figure 4.3 what's

860
00:39:46,260 --> 00:39:48,720
something whether they've read on past a

861
00:39:48,720 --> 00:39:49,980
section or not like what if they found

862
00:39:49,980 --> 00:39:52,140
important in 4.3

863
00:39:52,140 --> 00:39:54,060
and then let us like

864
00:39:54,060 --> 00:39:56,400
dwell on 4.3 a little bit look to the

865
00:39:56,400 --> 00:39:58,800
questions and then next week in our

866
00:39:58,800 --> 00:40:00,300
second discussion we'll

867
00:40:00,300 --> 00:40:03,480
continue on from here and look at more

868
00:40:03,480 --> 00:40:05,720
questions

869
00:40:10,380 --> 00:40:12,560
foreign

870
00:40:26,220 --> 00:40:28,020
step by step

871
00:40:28,020 --> 00:40:30,660
was brought up

872
00:40:30,660 --> 00:40:31,800
and

873
00:40:31,800 --> 00:40:33,660
this figure which I'm sure we have

874
00:40:33,660 --> 00:40:36,300
pasted many times into our textbook

875
00:40:36,300 --> 00:40:37,920
group coda

876
00:40:37,920 --> 00:40:40,140
is working with that same graphical

877
00:40:40,140 --> 00:40:41,579
grammar

878
00:40:41,579 --> 00:40:44,220
of the base graph

879
00:40:44,220 --> 00:40:46,980
on the left we have a prior

880
00:40:46,980 --> 00:40:50,820
that is seeding a hidden State s

881
00:40:50,820 --> 00:40:52,980
and then the hidden State s is emitting

882
00:40:52,980 --> 00:40:55,380
an observation o

883
00:40:55,380 --> 00:40:56,700
you can

884
00:40:56,700 --> 00:41:00,300
set s and generate o That's generative

885
00:41:00,300 --> 00:41:03,000
modeling generative AI

886
00:41:03,000 --> 00:41:04,560
you can also

887
00:41:04,560 --> 00:41:07,619
have o and update s that's the

888
00:41:07,619 --> 00:41:10,020
recognition density so a which is called

889
00:41:10,020 --> 00:41:12,720
the emission Matrix is The Tail of Two

890
00:41:12,720 --> 00:41:13,740
densities

891
00:41:13,740 --> 00:41:16,560
because you can have a hidden State and

892
00:41:16,560 --> 00:41:18,960
generate synthetic data from it that's

893
00:41:18,960 --> 00:41:20,220
generative

894
00:41:20,220 --> 00:41:22,740
or you can have an observation and use

895
00:41:22,740 --> 00:41:24,839
it to update your hidden State and

896
00:41:24,839 --> 00:41:26,339
that's recognition so this is the Taylor

897
00:41:26,339 --> 00:41:30,000
two densities in the static setting so

898
00:41:30,000 --> 00:41:32,220
this is like a single image

899
00:41:32,220 --> 00:41:35,520
and the image classifier label

900
00:41:35,520 --> 00:41:39,119
and so generative AI is going from draw

901
00:41:39,119 --> 00:41:40,560
me a cat

902
00:41:40,560 --> 00:41:42,599
to generate the pixels whereas

903
00:41:42,599 --> 00:41:43,859
previously there was only really

904
00:41:43,859 --> 00:41:46,920
recognition AI from the pixels to oh

905
00:41:46,920 --> 00:41:48,060
that's a cap

906
00:41:48,060 --> 00:41:51,720
but it's static in the top left

907
00:41:51,720 --> 00:41:54,500
the bottom left is going to be that same

908
00:41:54,500 --> 00:41:57,420
Motif tailor two densities

909
00:41:57,420 --> 00:42:01,079
but now s has a subscript for one two

910
00:42:01,079 --> 00:42:03,359
and three which are three time steps

911
00:42:03,359 --> 00:42:07,020
and so s is changing Through Time

912
00:42:07,020 --> 00:42:10,320
the way s changes is through b which is

913
00:42:10,320 --> 00:42:12,119
called a transition Matrix

914
00:42:12,119 --> 00:42:15,300
so if there's like two states in uh s

915
00:42:15,300 --> 00:42:17,880
you know is is it could be on the top or

916
00:42:17,880 --> 00:42:18,960
the bottom

917
00:42:18,960 --> 00:42:20,400
there's going to be a two by two

918
00:42:20,400 --> 00:42:22,020
transition Matrix

919
00:42:22,020 --> 00:42:24,599
where the diagonals are like staying

920
00:42:24,599 --> 00:42:26,820
where you are and the off diagonals are

921
00:42:26,820 --> 00:42:28,020
moving

922
00:42:28,020 --> 00:42:30,300
and that is going to be the describing

923
00:42:30,300 --> 00:42:32,160
the time evolution of the Hidden State

924
00:42:32,160 --> 00:42:34,800
and then at each time

925
00:42:34,800 --> 00:42:36,540
there's the Taylor two densities and the

926
00:42:36,540 --> 00:42:38,579
emission of an observable

927
00:42:38,579 --> 00:42:43,440
and the a matrix can be Sharp

928
00:42:43,440 --> 00:42:45,839
which is like having kind of ones across

929
00:42:45,839 --> 00:42:47,099
the diagonal

930
00:42:47,099 --> 00:42:48,660
so you can like see the hidden State

931
00:42:48,660 --> 00:42:52,200
without error or in The Other Extreme a

932
00:42:52,200 --> 00:42:54,960
could just be like kind of like a wash

933
00:42:54,960 --> 00:42:56,579
at which point the observation wouldn't

934
00:42:56,579 --> 00:42:58,079
tell you anything about the hidden state

935
00:42:58,079 --> 00:43:00,359
or in the middle is kind of like a

936
00:43:00,359 --> 00:43:02,220
semi-blurry a matrix

937
00:43:02,220 --> 00:43:03,900
where there's information on the hidden

938
00:43:03,900 --> 00:43:06,000
state from the observation but it's not

939
00:43:06,000 --> 00:43:07,859
unique for example a positive test

940
00:43:07,859 --> 00:43:10,079
result doesn't mean you have

941
00:43:10,079 --> 00:43:12,720
the disease

942
00:43:12,720 --> 00:43:14,339
and That's a classic like Bayesian

943
00:43:14,339 --> 00:43:16,440
statistic oh yes thank you a does not

944
00:43:16,440 --> 00:43:17,940
have to be square Matrix

945
00:43:17,940 --> 00:43:20,579
I'm using square examples but yes the

946
00:43:20,579 --> 00:43:23,040
dimensionality is like a huge thing to

947
00:43:23,040 --> 00:43:24,540
make sure that especially in complex

948
00:43:24,540 --> 00:43:26,220
models that the dimensionality like

949
00:43:26,220 --> 00:43:28,619
works out

950
00:43:28,619 --> 00:43:32,040
um so this is passive dynamical

951
00:43:32,040 --> 00:43:34,140
perception we're getting temperature

952
00:43:34,140 --> 00:43:36,839
readings at time one two and three

953
00:43:36,839 --> 00:43:38,160
and whether we have a perfect

954
00:43:38,160 --> 00:43:40,319
thermometer

955
00:43:40,319 --> 00:43:42,119
or whether we have like a kind of noisy

956
00:43:42,119 --> 00:43:44,400
thermometer we're going to be making our

957
00:43:44,400 --> 00:43:45,780
inference about the hidden State the

958
00:43:45,780 --> 00:43:47,040
true temperature in the room through

959
00:43:47,040 --> 00:43:49,020
time and then B describes how the

960
00:43:49,020 --> 00:43:50,700
temperature in the room changes Through

961
00:43:50,700 --> 00:43:54,359
Time whereas a is time independent

962
00:43:54,359 --> 00:43:56,940
and it describes the mapping between

963
00:43:56,940 --> 00:44:00,300
thermometer readings and temperature

964
00:44:00,300 --> 00:44:03,540
this is passive inference on the left

965
00:44:03,540 --> 00:44:05,960
Michael

966
00:44:06,420 --> 00:44:07,020
um

967
00:44:07,020 --> 00:44:10,079
again going to a different language but

968
00:44:10,079 --> 00:44:13,500
in management traditionally management

969
00:44:13,500 --> 00:44:15,780
has been uni-dimensional so for example

970
00:44:15,780 --> 00:44:18,300
you might say we need to manage

971
00:44:18,300 --> 00:44:21,180
um uh the dollars being produced in this

972
00:44:21,180 --> 00:44:23,579
process or the fishery the single

973
00:44:23,579 --> 00:44:25,859
species in this history and that one of

974
00:44:25,859 --> 00:44:28,880
the challenges has been to look

975
00:44:28,880 --> 00:44:31,319
ecosystemically instead of through a

976
00:44:31,319 --> 00:44:33,060
uni-dimensional variable that is

977
00:44:33,060 --> 00:44:35,339
optimized at the interplay between all

978
00:44:35,339 --> 00:44:37,859
the variables and that that that's what

979
00:44:37,859 --> 00:44:39,780
I'm witnessing here in a different space

980
00:44:39,780 --> 00:44:42,319
and sense this kind of

981
00:44:42,319 --> 00:44:46,020
multivariate management of the interplay

982
00:44:46,020 --> 00:44:48,980
between all the factors

983
00:44:48,980 --> 00:44:52,260
that that's my own shorthand for what

984
00:44:52,260 --> 00:44:55,020
I'm what I'm thinking I'm sensing

985
00:44:55,020 --> 00:44:57,960
awesome yeah the observations could be

986
00:44:57,960 --> 00:45:00,180
our triple bottom line and then the

987
00:45:00,180 --> 00:45:01,980
hidden State could be the unobservable

988
00:45:01,980 --> 00:45:04,560
health of our organization

989
00:45:04,560 --> 00:45:06,480
and so we're not just doing this like

990
00:45:06,480 --> 00:45:09,420
naked optimization on an observable but

991
00:45:09,420 --> 00:45:11,760
we're kind of inferring from the

992
00:45:11,760 --> 00:45:14,700
observables into an unobserved space

993
00:45:14,700 --> 00:45:16,619
and then that's the space that we're

994
00:45:16,619 --> 00:45:19,260
describing the Dynamics on

995
00:45:19,260 --> 00:45:23,160
yep now all of this has been passive

996
00:45:23,160 --> 00:45:25,380
but this paper goes step by step it

997
00:45:25,380 --> 00:45:27,420
introduces static perception and dynamic

998
00:45:27,420 --> 00:45:29,220
perception

999
00:45:29,220 --> 00:45:32,040
here's where action comes into play in

1000
00:45:32,040 --> 00:45:33,839
active inference

1001
00:45:33,839 --> 00:45:36,960
Pi is policy so there's a slight

1002
00:45:36,960 --> 00:45:38,880
difference between affordances which are

1003
00:45:38,880 --> 00:45:40,560
the actions that can be taken in a

1004
00:45:40,560 --> 00:45:42,540
moment like up down left right on your

1005
00:45:42,540 --> 00:45:45,300
joystick and policies which are

1006
00:45:45,300 --> 00:45:48,180
sequences of action over a given time

1007
00:45:48,180 --> 00:45:50,400
Horizon so with a Time Horizon of one

1008
00:45:50,400 --> 00:45:52,859
your affordances are your policy

1009
00:45:52,859 --> 00:45:54,060
repertoire

1010
00:45:54,060 --> 00:45:56,940
but for a Time Horizon of two is up up

1011
00:45:56,940 --> 00:46:01,619
down up right you know and so on so

1012
00:46:01,619 --> 00:46:04,079
um Pi describes policies

1013
00:46:04,079 --> 00:46:06,720
so that's sequences of actions that can

1014
00:46:06,720 --> 00:46:07,859
be taken

1015
00:46:07,859 --> 00:46:11,460
for the length of the time Horizon which

1016
00:46:11,460 --> 00:46:13,859
is a modeler's choice

1017
00:46:13,859 --> 00:46:16,920
so Pi describes which policies can be

1018
00:46:16,920 --> 00:46:18,000
taken

1019
00:46:18,000 --> 00:46:20,760
how are policies evaluated well in

1020
00:46:20,760 --> 00:46:22,740
reinforcement or reward learning you

1021
00:46:22,740 --> 00:46:24,119
would have utility function that

1022
00:46:24,119 --> 00:46:26,700
ascribes a utility value to different

1023
00:46:26,700 --> 00:46:28,380
policies

1024
00:46:28,380 --> 00:46:31,380
inactive inference we have expected free

1025
00:46:31,380 --> 00:46:32,640
energy

1026
00:46:32,640 --> 00:46:35,460
as an imperative that guides policy

1027
00:46:35,460 --> 00:46:37,140
selection

1028
00:46:37,140 --> 00:46:40,560
here free energy G

1029
00:46:40,560 --> 00:46:43,680
gets input from C which is our

1030
00:46:43,680 --> 00:46:46,440
preferences or our expectations

1031
00:46:46,440 --> 00:46:49,500
and so this is that fun twist with what

1032
00:46:49,500 --> 00:46:52,500
we expect slash prefer is like

1033
00:46:52,500 --> 00:46:54,420
in reward learning we would say the body

1034
00:46:54,420 --> 00:46:56,400
is rewarded by having a homeostatic

1035
00:46:56,400 --> 00:46:59,339
temperature survivable temperature bound

1036
00:46:59,339 --> 00:47:03,060
in this kind of preference based active

1037
00:47:03,060 --> 00:47:06,420
inference you say the body expects to be

1038
00:47:06,420 --> 00:47:09,300
in homeostasis it is surprised if it is

1039
00:47:09,300 --> 00:47:10,440
not

1040
00:47:10,440 --> 00:47:13,619
and it takes actions to reduce the

1041
00:47:13,619 --> 00:47:16,500
Divergence to minimize and bound its

1042
00:47:16,500 --> 00:47:17,700
surprisal

1043
00:47:17,700 --> 00:47:22,020
and so action policies are selected not

1044
00:47:22,020 --> 00:47:24,599
because they have a high estimated

1045
00:47:24,599 --> 00:47:26,099
utility

1046
00:47:26,099 --> 00:47:29,040
though that can be the case but rather

1047
00:47:29,040 --> 00:47:33,000
because the policy reduces expected free

1048
00:47:33,000 --> 00:47:34,619
energy

1049
00:47:34,619 --> 00:47:37,140
by aligning observations with

1050
00:47:37,140 --> 00:47:38,839
preferences

1051
00:47:38,839 --> 00:47:42,359
it's also important to note that policy

1052
00:47:42,359 --> 00:47:46,560
and action selection intervenes in the B

1053
00:47:46,560 --> 00:47:49,680
so this is changing how hidden States

1054
00:47:49,680 --> 00:47:52,200
change their time

1055
00:47:52,200 --> 00:47:54,780
and then this bottom version which has

1056
00:47:54,780 --> 00:47:56,220
not really even gone into much in the

1057
00:47:56,220 --> 00:47:59,700
textbook they're adding another variable

1058
00:47:59,700 --> 00:48:02,040
that describes like a temperature a

1059
00:48:02,040 --> 00:48:05,940
Precision on policy where in the

1060
00:48:05,940 --> 00:48:09,000
absolute zero temperature high Precision

1061
00:48:09,000 --> 00:48:10,319
setting

1062
00:48:10,319 --> 00:48:12,300
if there's two policies and one is a

1063
00:48:12,300 --> 00:48:13,740
little bit better you'd want the one

1064
00:48:13,740 --> 00:48:15,780
that's slightly better every time

1065
00:48:15,780 --> 00:48:18,540
in the neutral temperature if it was

1066
00:48:18,540 --> 00:48:21,660
5149 you'd want to select that one 51 of

1067
00:48:21,660 --> 00:48:22,500
the time

1068
00:48:22,500 --> 00:48:24,480
and the high temperature you would erase

1069
00:48:24,480 --> 00:48:26,460
the differences between policy choices

1070
00:48:26,460 --> 00:48:28,859
and you would be selecting equally

1071
00:48:28,859 --> 00:48:31,440
without respect to

1072
00:48:31,440 --> 00:48:34,079
mentioned Criterion but it's a little

1073
00:48:34,079 --> 00:48:36,839
bit of like a modification but also

1074
00:48:36,839 --> 00:48:40,760
it sets us up for some of the incredible

1075
00:48:40,760 --> 00:48:43,040
elaborations that we're going to see

1076
00:48:43,040 --> 00:48:46,560
within this visual grammar

1077
00:48:46,560 --> 00:48:48,839
so we're going to see hyper priors on

1078
00:48:48,839 --> 00:48:51,020
different variables and learning rates

1079
00:48:51,020 --> 00:48:53,520
and it's like this is like a circuit

1080
00:48:53,520 --> 00:48:54,240
board

1081
00:48:54,240 --> 00:48:56,579
there's no single circuit board

1082
00:48:56,579 --> 00:48:58,319
because you can wire things up

1083
00:48:58,319 --> 00:49:00,240
differently you can connect and say well

1084
00:49:00,240 --> 00:49:03,599
but what if there was um you know L that

1085
00:49:03,599 --> 00:49:06,720
that did that influence C it's like

1086
00:49:06,720 --> 00:49:09,300
you can make that generative model

1087
00:49:09,300 --> 00:49:11,400
and with Bayesian statistics you can

1088
00:49:11,400 --> 00:49:13,800
evaluate and juxtapose models that have

1089
00:49:13,800 --> 00:49:16,140
radically different anatomies

1090
00:49:16,140 --> 00:49:18,599
so it's like these are circuit board

1091
00:49:18,599 --> 00:49:23,640
designs or like Lego architectures

1092
00:49:23,640 --> 00:49:26,640
so there's not going to be one GM

1093
00:49:26,640 --> 00:49:29,040
even for a very narrow system of

1094
00:49:29,040 --> 00:49:31,200
Interest

1095
00:49:31,200 --> 00:49:33,420
any more than there would be just a

1096
00:49:33,420 --> 00:49:36,240
single circuit board

1097
00:49:36,240 --> 00:49:38,299
um

1098
00:49:38,579 --> 00:49:41,880
all of that and all of step by step

1099
00:49:41,880 --> 00:49:44,760
is in the discrete time setting

1100
00:49:44,760 --> 00:49:47,700
so past present and future how things

1101
00:49:47,700 --> 00:49:49,980
change Through Time how hidden States

1102
00:49:49,980 --> 00:49:52,440
emit observations

1103
00:49:52,440 --> 00:49:55,380
how the initial hidden state is set with

1104
00:49:55,380 --> 00:49:56,700
a prior

1105
00:49:56,700 --> 00:49:59,400
and how policy selection intervenes in

1106
00:49:59,400 --> 00:50:01,140
Hidden State through change that change

1107
00:50:01,140 --> 00:50:03,540
their time and

1108
00:50:03,540 --> 00:50:05,280
how preferences

1109
00:50:05,280 --> 00:50:07,800
shape through expected free energy

1110
00:50:07,800 --> 00:50:10,980
minimization policy selection

1111
00:50:10,980 --> 00:50:13,500
in the continuous time setting it's a

1112
00:50:13,500 --> 00:50:15,240
little bit different

1113
00:50:15,240 --> 00:50:17,280
we're gonna come to it

1114
00:50:17,280 --> 00:50:19,619
it turns out that there isn't explicit

1115
00:50:19,619 --> 00:50:21,540
prediction of past present and future

1116
00:50:21,540 --> 00:50:24,380
time steps but rather

1117
00:50:24,380 --> 00:50:27,480
generalized coordinates are used which

1118
00:50:27,480 --> 00:50:29,760
is to say like position velocity

1119
00:50:29,760 --> 00:50:32,520
acceleration and so on

1120
00:50:32,520 --> 00:50:35,040
generalized coordinates are used

1121
00:50:35,040 --> 00:50:38,460
akin to a Taylor series expansion

1122
00:50:38,460 --> 00:50:40,680
such that a smooth and differentiable

1123
00:50:40,680 --> 00:50:43,140
function is proposed

1124
00:50:43,140 --> 00:50:45,359
that makes continuously interpolated

1125
00:50:45,359 --> 00:50:46,980
predictions

1126
00:50:46,980 --> 00:50:49,800
without explicitly

1127
00:50:49,800 --> 00:50:52,020
mentioning discrete moments and giving

1128
00:50:52,020 --> 00:50:54,359
explicit predictions for them

1129
00:50:54,359 --> 00:50:56,280
so there's some pros and cons and

1130
00:50:56,280 --> 00:50:58,680
different settings and so on but we

1131
00:50:58,680 --> 00:51:00,960
return to this discrete continuous time

1132
00:51:00,960 --> 00:51:02,400
distinction

1133
00:51:02,400 --> 00:51:04,380
repeatedly

1134
00:51:04,380 --> 00:51:08,099
fundamentally it's a modeler's choice

1135
00:51:08,099 --> 00:51:10,319
often guided or constrained by the kind

1136
00:51:10,319 --> 00:51:13,200
of data that one has

1137
00:51:13,200 --> 00:51:14,700
and that's going to come up again and

1138
00:51:14,700 --> 00:51:16,319
again

1139
00:51:16,319 --> 00:51:18,420
they're laid out here in figure 4.3 to

1140
00:51:18,420 --> 00:51:21,059
illustrate their structural similarities

1141
00:51:21,059 --> 00:51:23,400
in both cases we have the kind of rake

1142
00:51:23,400 --> 00:51:26,400
or this like e facing down

1143
00:51:26,400 --> 00:51:28,920
with the hidden States changing through

1144
00:51:28,920 --> 00:51:33,079
time and emitting observations

1145
00:51:33,180 --> 00:51:35,700
caveat for you know the way that it's a

1146
00:51:35,700 --> 00:51:36,660
little bit different but basically

1147
00:51:36,660 --> 00:51:38,099
that's what's happening

1148
00:51:38,099 --> 00:51:41,339
and then the upstairs is the policy

1149
00:51:41,339 --> 00:51:43,559
selection

1150
00:51:43,559 --> 00:51:45,900
and that's what we see in step by step

1151
00:51:45,900 --> 00:51:48,660
The Rake is just static or dynamic

1152
00:51:48,660 --> 00:51:49,980
inference

1153
00:51:49,980 --> 00:51:52,680
that's signal processing

1154
00:51:52,680 --> 00:51:55,440
that's the common filter

1155
00:51:55,440 --> 00:51:58,619
and then the upstairs is policy

1156
00:51:58,619 --> 00:52:01,740
selection that's control theory

1157
00:52:01,740 --> 00:52:04,079
control theory upstairs signal

1158
00:52:04,079 --> 00:52:06,059
processing downstairs

1159
00:52:06,059 --> 00:52:08,940
all of it is being

1160
00:52:08,940 --> 00:52:11,880
included in the same generative model

1161
00:52:11,880 --> 00:52:13,680
so that we can have

1162
00:52:13,680 --> 00:52:16,079
summary statistics that are optimizable

1163
00:52:16,079 --> 00:52:18,720
for our generative model like free

1164
00:52:18,720 --> 00:52:19,920
energy

1165
00:52:19,920 --> 00:52:23,520
so that we can have a unified imperative

1166
00:52:23,520 --> 00:52:28,040
for perception and action

1167
00:52:30,720 --> 00:52:32,640
let's look to what questions we'll come

1168
00:52:32,640 --> 00:52:35,480
to next week

1169
00:52:36,599 --> 00:52:39,480
all right equation 4.10 so equation that

1170
00:52:39,480 --> 00:52:42,240
we haven't gotten to yet but yes there

1171
00:52:42,240 --> 00:52:44,339
are more

1172
00:52:44,339 --> 00:52:46,920
equations

1173
00:52:46,920 --> 00:52:48,900
including some subtleties with

1174
00:52:48,900 --> 00:52:50,819
subscripts and bold and so these are

1175
00:52:50,819 --> 00:52:54,540
definitely ones to have good readings of

1176
00:52:54,540 --> 00:52:56,640
and like a lot of different questions

1177
00:52:56,640 --> 00:52:58,800
and like why is that line doing that and

1178
00:52:58,800 --> 00:53:00,420
all of this

1179
00:53:00,420 --> 00:53:03,500
we'll come back to it

1180
00:53:05,220 --> 00:53:06,180
um

1181
00:53:06,180 --> 00:53:08,460
the expected free energy is minimized by

1182
00:53:08,460 --> 00:53:10,200
selecting those observations that cause

1183
00:53:10,200 --> 00:53:13,578
a large change in beliefs

1184
00:53:17,579 --> 00:53:20,720
we'll come back to it

1185
00:53:25,559 --> 00:53:28,020
okay a question of variational inference

1186
00:53:28,020 --> 00:53:30,780
and also we can use the coda latex on

1187
00:53:30,780 --> 00:53:34,380
Pac to make it render nicely

1188
00:53:34,380 --> 00:53:37,980
equation 410 same here and some more

1189
00:53:37,980 --> 00:53:40,079
questions on

1190
00:53:40,079 --> 00:53:40,800
um

1191
00:53:40,800 --> 00:53:42,960
equation notation so thanks for these

1192
00:53:42,960 --> 00:53:44,220
very careful

1193
00:53:44,220 --> 00:53:47,899
comments on the notation there

1194
00:53:48,359 --> 00:53:50,520
awesome so yeah a lot on the equations

1195
00:53:50,520 --> 00:53:53,160
and and it's like we can always return

1196
00:53:53,160 --> 00:53:54,960
to the math learning group we can set

1197
00:53:54,960 --> 00:53:57,119
another time

1198
00:53:57,119 --> 00:53:59,640
because there's

1199
00:53:59,640 --> 00:54:02,839
math learning questions

1200
00:54:03,240 --> 00:54:05,220
to help

1201
00:54:05,220 --> 00:54:08,220
increase the approachability of this

1202
00:54:08,220 --> 00:54:11,058
entire Corpus

1203
00:54:11,280 --> 00:54:13,619
and then there's also some really

1204
00:54:13,619 --> 00:54:16,859
specific technical questions

1205
00:54:16,859 --> 00:54:19,500
where it would be awesome to just have

1206
00:54:19,500 --> 00:54:20,579
like

1207
00:54:20,579 --> 00:54:22,980
here's what the squiggle means

1208
00:54:22,980 --> 00:54:25,680
and so those are like two different like

1209
00:54:25,680 --> 00:54:27,660
regimes of attention within the math

1210
00:54:27,660 --> 00:54:29,880
learning mode and the math learning mode

1211
00:54:29,880 --> 00:54:31,619
is just one mode that we're in overall

1212
00:54:31,619 --> 00:54:34,260
but but both those

1213
00:54:34,260 --> 00:54:36,480
um accessibility and rigor components of

1214
00:54:36,480 --> 00:54:37,980
math hopefully like if people are

1215
00:54:37,980 --> 00:54:41,059
motivated to

1216
00:54:41,339 --> 00:54:43,500
somehow work on that there's like a lot

1217
00:54:43,500 --> 00:54:46,500
of opportunity there

1218
00:54:46,500 --> 00:54:50,700
any final thoughts or we will return

1219
00:54:50,700 --> 00:54:52,800
next week

1220
00:54:52,800 --> 00:54:56,839
in our second discussion on four

1221
00:55:01,920 --> 00:55:04,079
all right great

1222
00:55:04,079 --> 00:55:07,520
thank you all farewell


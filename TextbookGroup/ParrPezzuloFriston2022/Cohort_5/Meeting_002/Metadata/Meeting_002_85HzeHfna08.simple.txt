SPEAKER_01:
All right hello everyone it is cohort five meeting two and we're in our first discussion of chapter one so to begin with what is anyone's just first take or on chapter one or where does anybody want to begin and then we'll look at the specific questions but is there any just first overviews or just pieces that people want to share about chapter one or just this first discussion

You can just go for it if you want to ask me.


SPEAKER_00:
So my biggest question is around surprise and what the I guess the parameters and boundaries of that is.


SPEAKER_01:
And well.


SPEAKER_04:
Sorry, I think a part of my long time preceding the literature is like the use of some terms like this.

And this might be because I'm also a non-native speaker, but like the term of like a normative principle, like I'm still not sure what it means or like, you know, later on, they say like epistemic this and stuff.

And I'm just like very confused by those terms.

So maybe clarifying what it means to be a normative principle will be a very useful thing for me.


SPEAKER_01:
Okay.

Let's just see if it's been asked overall, because as Ali can probably attest to,

Well, let's just like at least think of it doesn't want to add a thought on this, like just on it uses the terms overall or we can look more specifically at what is meant by normativity like in Chapter one.


SPEAKER_06:
I was also having some.

um uncertainty about the use of the term I had previously had an understanding that normative uh the term described something that uh the way that some something should be as opposed to being descript a descriptive um theory for example which is which is simply stating what what is observed but I

uh in the way that it's introduced here uh normative is used more i think uh placing an emphasis on the ability to to look at a quantity and measure some behavior in terms of that quantity and um

uh emphasizing that's better behavior or behavior that's more in alignment with what the theory is suggesting will will optimize that quantity that's my understanding so far but i'd like to be more clear thank you ali go for it


SPEAKER_07:
So normative, I mean, the use of the term normative here in active inference literature is closely related to the meaning of this term in, say, philosophical literature and stuff.

So basically in philosophy or more specifically in ethics, when we talk about the normative behavior or, I don't know, normative behavior,

action uh it's uh it's interpreted as something ethical uh or moral uh and the reason we ascribe uh this kind of meaning to normans to this term normative is because we have a standard of action or behavior against which we can compare our own action

and then judge by way of comparing it whether it is ethical or not.

So by the same token, we can talk about normativity here in this context of active inference when we talk about the standard for optimization of our action and perception.

namely the free energy against which the error or the deviation of the action and perception from data standard can be evaluated and then corrected or accounted for.

So that's why, I mean, researchers in active inference area

I use the term normative or normativity to refer to this kind of comparison between the action and kind of a standard for optimization of that action.


SPEAKER_00:
would kind of question whether that would be tied to ethics though because norms don't necessarily um align with ethical behaviors and of course ethical is subjective but you know norms have been you know for example been leaning more toward coercive um and exploitive behaviors

uh which in my in my world is not ethical but uh you know so I don't know I'd like to see the yeah those two tied apart especially as far as how people act because ethics are you know are not um are not universal um and coercive coercive behaviors

tend to get more benefits in U.S.

society, but maybe not other places.


SPEAKER_04:
Yeah, I guess it's not so obvious to me the connection with ethics.

You know, I come to these ideas from like searching first principle ways to understand

Gorka Puente , Self organization of main components, so my feeling is that.

Gorka Puente , You know there's nothing about our subjective ethical more or more things here so some so yeah I mean the fact that we have this.

Gorka Puente , Confusion really reveals that maybe normally shouldn't be the word that is being used here because it's just like what we have so many meanings in so many contexts so.

Gorka Puente , I don't know.


SPEAKER_00:
It is very cultural.


SPEAKER_03:
But perhaps, I mean, if we look at it as just a standard for optimization within a certain, a certain definition of the system, then, you know, if we don't get too caught up on our, whatever our conceived notions or subjective notions of normative as a normal and social norms, but just look at it as a optimal behavior within the parameters of the system.


SPEAKER_07:
Yeah.

So if I may clear that up a little bit.

Well, I mean, what I meant was not that the the word normativity is used in exactly the same way as it's used in, I don't know, philosophical literature or ethics.

It's just

I mean, a related term that's used in its technical sense in active inference literature.

So I agree that we can, maybe it's too naive to take that same meaning from philosophical literature

and use it directly in this context to refer to something, I don't know, as a social norm or standard or whatever.

But again, in this technical context, norm here just means a standard against which the optimization of the system happens.

that's what all there is to it.

So it's not something

it's not something that can be applied to derive any kind of ethics or morality or whatever it's just a way of describing a system against a specific standard for optimization namely the free energy so that's why this term is used here and also

Rameen Mohammadi, If we want to use the first principle way of describing things we need to have these kinds of.

Rameen Mohammadi, It is the precise.

Rameen Mohammadi, Precise definitions for those standards for optimization because it's a way of rigorously building up.

our descriptive model.

So that's necessary to have a very strict and rigorous definition for any standard if we want to build up models rigorously enough to be useful for modeling any complex behavior of cognitive agents.


SPEAKER_04:
So basically, if I understand from this discussion, can I, you know, when in part 1.4.1 says active inferences a normative framework to characterize base optimal behavior and cognition in living organisms.

Does that mean that what it's saying is that

Pablo Padilla- Active inference as a theoretical framework just sets the objective function of what systems must do, which is to minimize the ratio of free energy so normative just refers to.

Pablo Padilla- The fact that active in France establishes the rules by which systems have to play kind of thing is that a good interpretation of what normative means.


SPEAKER_07:
Yeah, more or less.

That's what all there is to it.

Yeah.

And also there's a more explicit definition of this term normativity in, I guess, notes one from Chapter one on page let me see the 267.

So yeah, here the term normative means there is some evaluative standard against which behavior can be scored.

Active inference is normative in the sense that perception and action are scored by free energy, a quantity we will unpack throughout this and the next chapters.


SPEAKER_01:
These are all really great points.

One analogy that I find very useful or a set like more machine learning and more classically statistical.

is by now the variational free energy playing the role of the l2 regression fitting norm in linear modeling so linear modeling doesn't just mean straight lines so check out the earlier friston fmri work on spm which extends generalized linear

modeling so fitting the l2 norm doesn't mean that the model is adequate it doesn't mean it's going to work if something changes about the underlying data process and so on like it's none of no modeling process can say anything about how well the outcome works but this loss function the l2 norm and linear regression or like the loss function

in uh training a neural network these are analogous to the variational free energy such that however you set up a generative model it's going to have this like base optimal like it it's it's going to look like it's on its path of least action given how it's set up with this loss function so I agree with Ali absolutely it's very it's a more of like a methodological or a computational normativity

in terms of being variational free energy being more like a loss function okay that's one that's one angle on the normativity but I think this is also one discussion that we return to many times which is just how does something that can describe anything like we describe things to the to the extent of their thingness for some statistical pattern

or for all these other ways of modeling which is kind of what this um approach ends up looking like in generating these cognitive models like how how can it be measuring everything what is what is something that measures everything say what does the a meter stick say so understanding like measurement and then all this susan


SPEAKER_00:
Barbara Kirby, So so that's that's an interesting.

Barbara Kirby, concept so it's it's not pointing toward.

Barbara Kirby, The outcome as much as it is the value.

Barbara Kirby, The optimized value would that be, in other words, or is the model yeah I understand it's ideology free.

Um, or, you know, I'm assuming I'm, I'm inferring that, um, but yeah, you, the, the perception is, uh, aiming toward a goal.

So the goal being a value versus an outcome.


SPEAKER_01:
There's a lot.

there but it it there's just and of course these are just like

various different people's perspective on it, but in one view, it has equal ideological status to a measurement tool, as a measurement tool, the kind of narrow sense, which it actually looks like when we get through chapter six, the recipe for making the active inference generative model, which was like alluded to earlier, like that's the difference between having a generative model.

Once the generative model is specified slash to the extent it's specified,

it is going to do the most statistically likely thing but that can include a noisy distribution for perception action so something being uncertain doesn't mean that it's not the likeliest path

That is done in the model with like a parameter that describes how noisy something should be.

That's like variance estimation, Bayesian hyperparameters.

So just because it seems it shouldn't feel constraining in terms of developing like a phenomena that seeks information or something like this normativity isn't a like far reaching claim.

and then there then there's all of these other kind of aspects that come into play in the actual modeling mostly from the system of interest just like people use linear modeling across fields and then there's statistics into all of the development that happens in statistics just that that's one kind of deflationary view

that is more like the statistical parametric mapping earlier Fristen work.


SPEAKER_07:
And also one other point perhaps worth mentioning here is that FEP and active inference is employed to describe any complex system or any self-organizing system

that looks as if it takes the action through which the free energy or that norm I was talking about is minimized or optimized, regardless of whether the,

uh agent of interest has any conscious awareness of its intentions or not so it can be applied both to um conscious cognitive agents uh as well as to uh i don't know uh anything that's that only has uh very simple basal cognition and uh a very simple sense of um

C Sinan Gunturk, Ph.D.

: agency and goal directed behavior so that makes FEP and active inference broad enough to encompass all of these systems under.

C Sinan Gunturk, Ph.D.

: A similar or even identical mathematical technology.


SPEAKER_01:
Thank you, Ali.

Just like the linear regression can be used across different scales of the system, all the work is in the modeling.

So this is like the textbook is oriented towards people on the first half who want to learn about the topic.

Also, this is the first textbook, so it's kind of like

We bring in a lot of other things that have happened since, and people can continue to bring things in because this is just one view, but the first five chapters are more on learning.

active inference, or then the second half, even though we don't have all the kind of pieces in place, but making the generative model the second half, a lot of the very philosophical, very general questions, it becomes easier to partition the questions into like, what's a question more about an implementation in a given system of interest?

And then what is the question about the more general

um usage of the term but to kind of return all the way to the initial point which is certainly felt by all like the active inference ontology which is to say like the language and the terms like is

some terms that are used uh commonly but used in seemingly a different sense here's one way that through the active inference ontology we like at least start to make sense of this and and you're always welcome to use the at sign to like connect it to an ontology term i don't know if every single word but it benefits to see it that way um but like it helps emphasize that it isn't an infinite slash open-ended set of terms that are being used

um it's from a few subfields where terms are being used exactly and specifically and technically mostly from bayesian statistics there's also some terms from on the inbound and on the sense making side from like signal processing ecological psychology as well as on the outgoing more action and control theory so there's those kinds of terms

that describe like broadly cybernetic agents are being used essentially exactly.

And then for many of the terms, these are just proposed, just which, which may or may not be used exactly by any given author, but a lot of the terms have like a narrower technical sense and then a more general uses like attention or anticipation or normativity.

It's like, it like can be used in seemingly different or, um, a narrow, a broader way.

There's a lot to say in how we all, I guess, learn and use the terms correctly.

Another piece to add just on the ontology is the terms are not just mostly drawn from these computational areas, but the

I'll just leave it at that.

Anyone else want to add something?


SPEAKER_03:
Just on that, following that path, I'd like to clarify a few things.

So when we're talking about surprise, we're using that term essentially in the terms of like entropy in information theory, more or less like the Shannon entropy.

Correct?

and slash, if so, when we're talking about, because minimizing surprise turns out to be challenging for technical reasons, we use variational free energy as a proxy.

So I'd just like a little clarity on those technical reasons that minimizing surprise is so difficult.

Is that because it's just too broad of a series of possibilities for a Bayesian calculation?

And then so what's the and then what's the mechanism of using free energy as the principal bits because we're just we're doing a divergence.

That's my understanding.

But clarification would be.

Nice.


SPEAKER_01:
Yeah, Ali, go for it.


SPEAKER_07:
Martin Anderson- yeah we'll come to these questions.

Martin Anderson- In Chapter two, I guess, and particularly in on page 19 and 20 so yeah The idea here is, although.

Martin Anderson- The value that needs to be optimized exactly is.

the free energy but in practice uh optimizing or minimizing this value uh precisely or uh in its exact uh bayesian inference way uh would prove intractable so um the way to somehow solve this solve this issue of intractability is to provide a kind of lower bound

uh that needs to be optimized uh called variational free energy so uh except for very simple scenarios in every real time uh real time real real world situations uh there is this

lower bound that needs to be optimized and not exactly the free energy or the surprise, although the theory of active inference evolves around this idea of optimizing this specific normal value, namely free energy.

But in practice, that's the variation of free energy that needs to be

optimize as the lower bound of surprise.

So yeah, surprise or surprisal is used in its statistical sense in active inference literature, and it doesn't exactly map onto the psychological sense of the term.

And again, it's a technical word and it doesn't necessarily refer to, it doesn't require any

um cognitive or conscious agent uh for this kind of informational or statistical surprisal uh to be defined uh it's just uh yeah something that can be defined purely in its mathematical terms uh using the parameters of the system of interest um yeah inks


SPEAKER_02:
Hi.

So I come from linguistics, so I plan to learn all this math very soon.

But my question is this, the low road and the high road, are they like the same?

Oh, you guys can't see because I've blurred.

Are they like the same journey from two points of view, like a tube?

which is a microscope if you look on one side and a telescope if you look on the other side?

Or are they completely traversing different domains the way like syntax and semantics, they do meet and they kind of like interlock, but they are, they walk the chessboard with a different stitch step.


SPEAKER_01:
Manuel, thanks.


SPEAKER_04:
Just before, I guess, addressing that, I wanted to say how I understand the limitations.

So to Jesse, I don't know if I'm able to share my screen.

So I have a presentation.

I've been planning to give this to my research group.

So I have sort of like visuals to explain at least the way I understand why minimizing free energy

it's it's a it's a good alternative so if is it okay if I would sure okay how long is the presentation no it's like three slides just to share the concept of why integrate like why computing all right go for it very very fast yeah go for it so can you see my slides yep

yeah so basically you know we're in the in the task of computing base theorem so computing some latent variable data given our observation sex

And this always involves an integral of this form.

So the denominator of base theorem can be computed basically integrating the numerator through all of the possible unobserved or latent variables.

And the way you will do this computationally, if most of these integrals don't have a closed form solution, so you cannot solve them analytically,

So what you do numerically is you would imagine you have like a grid and you evaluate.

So you don't know the height of this function and what the way you're going to integrate is by dividing the grid into different points.

And then at each point of alerting the height and the integral is going to be basically sort of like base time times high.

So you're going to add all of these small contributions.

but if you look at this example that i drew here most of the evaluations don't contribute at all to the to the integral because they're all at zero but you don't know that to begin with so evaluating these numerically will become incredibly challenging and furthermore the more the dimensionality of your problem increases like the more dimensions you have the more variables you're trying to infer

the more and more and more grid points you will have to evaluate so it grows exponentially.

So it just becomes, in high dimensional spaces, evaluating these would become just basically impossible numerically.

So what we propose is to say, OK, what I really want is my posterior distribution, latent variables given my observations.

And I'm going to propose a good enough approximation.

So this is my true belief that I want to get, and this is my proposed close enough approximation.

So what we say is like, okay, I want my close enough approximation to be as close as possible as the true model.

And that is done by minimizing the scale divergence.

But, you know, that minimizing that distance or pseudo metric already involves.

the distribution that I don't want to work with, the one that is really hard to compute.

But then you do some math that you'll see in the book, I guess, and you arrive at this, that the free energy is a lower bound on what we want to compute.

So basically, I don't know how to compute the right-hand side of this equation.

But I know that if I make the left-hand side of the equation as small as possible, I will have also made the other one as small as possible, given my approximation.

So that's how I understand it.

And there's different, yeah, I guess that's good enough for what I want to say.

Just to shed some light on the technicalities of actually implementing these things computationally, it just, you need a proxy because there's too many dimensions to handle.


SPEAKER_01:
awesome cool yeah very um relevant it's discussed a lot in livestream 52 series with Lance da Costa some of the relationships with like sampling based methods variational methods all these things um and yeah um Susan and then Rob Moore


SPEAKER_00:
so yeah that was very helpful um it kind of speaks to the requisite variety aspects I guess um and and that kind of connects to um the uh in terms of belief updating and learning um you know I know that there's um the the variation that is more in the moment or emergent

but the and the there's a time lapse in the belief updating but would um would that mean that the goal of learning and updating is widening those boundaries right right widening um yeah the the requisite choices


SPEAKER_01:
yeah there's a lot to say there it is related to the cybernetics the principle of requisite diversity and good regulator theorem and those are discussed a lot in the chapter as it's kind of like um like and then uh often these these three scales of inference um for for so as Ali emphasized there's like continuity with the simpler or simplest systems

like things that don't have these higher cognitive functionalities at all, things that do only trivial belief updating, like even the case of a ball on the ground, you can think of it as doing inference and doing nothing and being at equilibrium.

So it's a measurement framework that, on one hand, you can have a totally...

non-cognitive or basely cognitive like ball rolling down a hill truly as doing some path of least action that's exactly from mechanical physics and the basing mechanics extends that ability to model like mechanical systems and then introduces the faster level of inference and then attention which is precision modulation and then learning

David Price- Which is the slower parameter updating that's within one time within one scale of time and then nested models could implement arbitrary other learning schemes.

David Price- Like here's just another switch like higher variable that just switches between these knees so that's not learning.

David Price- In the sense of those parameters might be fixed so it's like a narrower statistical learning definition versus like broader human learning.

so again that's another term like learning and inference and updating are they're not being applied to the human psychological usage it like learning is used in statistics to describe parameter updating parameter erupting so but this model does um allow for human learning and I guess would human learning be or you know if it doesn't say so now but


SPEAKER_00:
And are there models around policy in terms of if the goal is human learning?


SPEAKER_01:
Anyone want to speak to that?

Or Ra, go for it.


SPEAKER_06:
So.

I don't have an answer to your question Susan so i'm going to.

ask another question and maybe if someone has a comment.

For you, they can think about it in the meantime.

My question is related to something that's called first and touches on a bit in the preface.

And then it's, it's discuss another summaries of the free energy principle that you could find, for example, Wikipedia.

And it's the falsifiability of the energy principle and a distinction

that you might make between a framework or a principle versus a theory that's making testable or measurable

predictions uh that would allow you to confirm or reject a provide evidence for or reject a theory and uh what carverson and thomas parr allude to in the book is that the free energy principle is is just that it's a principle akin to the theory of evolution

And in somehow I haven't been able to make sense of what what it is about the free energy principle or or the theory of evolution as a more

a common theory or framework that people are more familiar with?

What is it about those principles or frameworks that makes them hard to falsify or not falsifiable?

and then they go on to specify that when these frameworks do become testable is when you introduce the generative model.

So an active inference, the core of the theory comes to

producing a general model producing some description of what's.

causes.

In the world's generates.

could could be generating the observations that you're you're seeing.

and and then that's something that you can go and measure or maybe in a human brain or or look for observations to to test against but so the essence of my question is what uh distinguishes

the free energy principle as a framework that isn't falsifiable?

And how does a generative model convert it, the free energy principle or active inference into a theory that can be tested?


SPEAKER_01:
Great question, Ali, go for it.


SPEAKER_07:
So the reason the free energy principle is described as

as an unfalsifiable principle is because it's basically a variational principle which we use as a mathematical tool to describe the kind of theory that

accounts for the behavior of particular kinds of self-organizing systems or agents.

But if we look at this issue in a much broader perspective, the same problem of falsifiability applies equally to predictive coding frameworks as well.

So the question around is predictive coding

theories are falsifiable or not, has been going around for several decades now.

But a very illuminating paper published just a couple of days ago, namely is predictive coding falsifiable by Brown, by Bowman at all, in which

they argue that although it looks as if these kinds of theories as predictive coding or active inference and so on looks as if they are falsifiable, but the unfalsifiable, sorry, but the unfalsifiability of these theories is nothing

inherent to these theories at all.

It's just the problem of precision that needs to be accounted for if we want to look at the problem of falsifiability for these theories.

So their conclusion is that these kinds of theories as predictive coding or predictive processing and active inference

can indeed be viewed as falsifiable theory, at least in theory.

There isn't any inherent barrier for describing these theories as unfalsifiable.

There isn't any fundamental unfalsifiability for these theories.

But then again, if we come back to active inference again,

There are two parts for the theory of active inference.

The underlying variational principle, mainly free energy principle, and then the

empirical theory itself, namely the active inference.

So even if the underlying principle or FEP is unfalsifiable, it doesn't necessarily apply to active inference as well.

So active inference can be viewed as a perfectly empirical, testable and falsifiable theory.

But there are some arguments that


SPEAKER_01:
the underlying principle being as variational principle is in essence on falsifiable so yeah thank you Ali there's a lot there um you you can always follow where I am in the coda by clicking on the icon to like improve the notes which would be really helpful or add more information but in this interview

that was where friston provided this response that's one answer to this kind of question another is like again to connect it to the linear regression like the statements about variational free energy bounding surprise minimization and then surprise minimization being equivalent to model evidence maximization

They are two sides literally of the exact same thing.

If you have the optimal model, you have the lowest bounded surprise.

Lowest surprise, that's the exact Bayesian results.

It's like you're doing the best that you possibly could have on the inbound sensemaking side and then on the outbound control side.

So again, a deflationary statistical...

description which is accurate of variational free energy is it being a unified tractable bound to surprise minimization whereas there isn't a tractable analogous bound directly to model evidence maximization which is why there's the whole question about

hill climbing algorithms and model maximization.

But this understanding between minimizing something is like equivalent to maximizing something is commonly used, like minimizing the training loss on a neural network or minimizing the L2 norm, like the sum of squares on a linear regression.

As was mentioned by someone earlier, like that doesn't mean that you made the generative model adequate to the real world in any way.

it's just a statement that the statistical model will have a certain um ability just to process the data routinely like a t-test does that mean that that was an ideology for a t-test or something that that has always been the question but

that is a separate question than the relationship between variational free energy and surprise which is just a technical finding that it can't be falsified it's just a construction made but then once something is proposed like as a structural hypothesis like in the spm days they were testing does uh

Presenting A then B influenced this response pattern.

And in testing structural hypotheses in the brain, active inference kind of elaborates on this idea of creating structural hypotheses that themselves being specific or particular and specified or just evidently not.

it would be analogous to asking about an unspecified linear regression, whether you'd expect this or that result.

Let's look at a few other of the questions.


SPEAKER_04:
Can I quickly make a comment about Russ?

A question I recently read this paper.

The math is not the territory navigating the energy principle by.

Mail Andrews, and.

You know, my take away from that paper, which I highly recommend.

It's the way to think about the 300 principle is more like a statistic.

Yeah, like a statistical.

uh framework like you don't falsify calculus calculus is uh you know mathematical framework but if you apply newton's laws and let's say you have an alternative hypothesis to newton's flaws and they don't agree with the data then those laws are wrong but calculus is still right like calculus is still the mathematical framework to analyze you know differential equations so in that sense uh the 300 principle i think

Sort of place an analogous role where now the falsifiable thing will come with us.

Then it wasn't like you build your generative model.

Maybe you think about neuroscience, so you think about how the brain connects, or maybe you think about.

I don't know biophysics how genes talk to each other.

So, in that, that is what is going to be falsifiable and the, and the principle.

It's just a way to look at that generative model.

And you know, in a way, to make sense of it like what you're proposing kind of thing so so that's my understanding my take away message from from this paper yeah.


SPEAKER_06:
yeah great can you please link the paper.


SPEAKER_01:
in the live stream 14 also it was discussed with melon with others but yeah that's kind of like a key reference point that helped clarify some of this like map territory and the map territory fallacy the map territory fallacy fallacy we discussed this actually earlier in the cohort for discussion if people wanted to look but let's look in our last like little bit to some of these written questions

While the FEP is applicable across many timescales, how applicable is active inference as a process theory instantiation of the FEP?

Very much plays into our discussion.

Do organisms need a nervous system for it to make sense to describe them as doing active inference?

No, you don't.

It can include inert systems, passive systems.

This isn't the psychological account.

but it can be applied to mind as a system of interest.

But the basal machinery that is described in this textbook

um like what chapter four describes generative models of are not psychological accounts of humans they're statistical examples so no there's no need for any kind of specific material or or to have to to be material at all could you say oh it's a matrix that just flips between two states

It could be as simple as that, or you could be applying it just like another analytical or measurement or modeling tool to a system of interest.

So, and how applicable is active inference?

That's the empirical question.

i'm hoping to get intuitive understanding of how free energy represents a measure of the discrepancy between the organisms internal model the world and its sensory inputs, what is the underlying relationship between information and energy.

What is the relationship between this free energy and active inference and the thermodynamic concept of free energy that's also present in any organism environment.

Anyone have a thought on this what's the relationship between.

variational free energy or expected free energy and.


SPEAKER_04:
thermodynamic concept of free energy um can i say share my thoughts on that yeah go for it yeah so my understanding is the following so you know in the industrial revolution people are trying to improve uh steam engines

And they come to the realization that doesn't matter, you know, how good your design is, how well, you know, that as perfect as your engine becomes, you put some energy, but you don't get the same amount of work out of it.

There's always some loss.

There's always something that you cannot really utilize.

So that leads to Clausius to make the statement of like, oh, let's just come up with a concept that we're going to call entropy, which is always increasing.

And it's kind of like this part of the energy that I input into my system that I wasn't able to make use of.

It's just dissipating.

You can think of friction and other things that just make all of the energy that you put into the system not being utilized.

Later on, Boltzmann comes in and then he says, look, entropy is not mysterious.

Entropy is just a measure of how many ways you can arrange the system.

How many different, you know, if I have the gas molecules in this room,

uh nothing in the laws of physics uh prohibited them to certainly like you know have some random trajectories and collecting that corner and then i'll be suffocating because i'm in a vacuum kind of nothing in the laws of physics forbids that but it's just incredibly unlikely it's just there's so many many many more ways to have them uniformly spread throughout the room that you have to wait longer than the age of the universe to see that random event of having them

in that corner.

So now when Shannon comes in and he formalizes the idea of information as a mathematical quantity that you can put a number on,

Turns out that the metric that he derives is exactly of the same functional form as Boltzmann's entropy.

So, you know, the two equations look exactly the same.

And the story goes that I think von Neumann told Shannon, oh, just use the word entropy.

It's the same equation and nobody understands the other one either.

So on a divide, you'll win.

So then later on, I think he's ET James, a very famous and important physicist in all of these statistical mechanics and its relationship based in statistics.

But I think finally makes a connection between thermodynamic and informational entropies in that case, where.

from my readings and interpretations of what Jane said, is that thermodynamic entropy is something I can measure.

I measure the amount of heat dissipated by my system at a constant temperature, and I put a number.

That's my entropy.

The information entropy is not something I measure like that.

It's in units of bits.

But it has to do with, in a system, if I push a piston,

how much information I, or how many degrees of freedom I control of that system.

Well, I can push all of the atoms in the gas and compress them, but I don't really have control over their particular trajectories.

So the work that I get out of that effort has to do with how many degrees of freedom I actually control.

If I was able to go into the atomic level and point all of the gas molecules into the same direction, my system will be much stronger and push and give me more work.

But I don't have access to those degrees of freedom.

So the information I have access to is limited.

Therefore, if I were to reproduce that piston pushing and went over and over again.

it will give me the same result given my limited information.

So I don't know if that's helpful.

It's a long explanation, but that's my current understanding of how these two things are connected.


SPEAKER_01:
Thanks.

Yeah.

Christoph, go for it.


SPEAKER_08:
I just want to say something, present this from a slightly different angle.

This is something that I took away from my undergrad.

I particularly studied two kinds of semantics of programming languages, the notational semantics and operational semantics, which is, you know, you take a program fragment and you're trying to assign a meaning to it.

And the operational semantics is some kind of sequence of reductions

Mateusz Piorkowski- Using big or small steps that tells you like you know how the program evolves over time and what the sort of state transitions of the virtual machine kind of are.

Mateusz Piorkowski- And there's a different way of assigning semantic to a program is to say, well, there is a mathematical object.

Mateusz Piorkowski- This program represents a good example that would be, for instance, something like an like an infinite list it's not actually realizable in a finance computer and yet.

Mateusz Piorkowski- We have programs that act as if they were infinite lists.

So where it really hit me specifically in this case was that the common presentation of the issue of entropy in information theory versus thermodynamics is that people often mention like, oh, these two things have the same mathematical form, but they're two different things.

And I would say that because these things have the same mathematical form, there must be the same thing.

They must be the same thing.

It's just the depth and the nature of the relationship may not be precisely understood by us just yet, but the mathematics suggests that it's precisely exactly the same phenomenon, because it has the same mathematical form.

This is kind of a more philosophical approach.


SPEAKER_01:
Thank you, Christoph.

Another angle.

So that's a philosophical kind of angle.

One is to say the variational free energy is an information, theoretic free energy that has that resonance in form or the analogy in form.

And then when we apply this to self-organizing systems, then if we look at things that are existing already,

they are representing a small um like they're unlikely in one way but then they're likely in their own way so perhaps one way to see it is it's about finding what makes it have a thingness which is what ali alludes to in the chat with

uh FEP and a theory of everything again thing here is used in a rather specific ontological sense definitely we can return to this like and it's come up many times with like this notion of particular States and a particular thing um a lot more to say but um Susan um yeah and that was that was a great explanation manual and what piques my interest and I just want to put a pin


SPEAKER_00:
in uh what you talked about was um the um the correlation between that and the constructal law um and yeah i think that that's um you know as far as how i kind of tease that apart is is is um the confusion over who designs and i think yeah that uh that has a bearing in

You know, in our updating belief updating.

So that'd be interesting to pursue.

I don't know that it's covered in this course, but it would be an interesting exercise outside of it.


SPEAKER_04:
Well, yeah, go ahead.

Quick point, people, if there's a great, great paper, what is information by, let me find it in my, David, do you remember the author?

It's just, that's, I think, if I find it, I'll paste it on the,

on the chat, but that just explains some of these ideas of entropy, you know, entropy is in the eyes of the beholder.

The entropy of a coin is not one bit.

If I only, if I focus only on the faces, yeah, it's one bit.

I just can't have either heads or tails.

Just one, yes, no question.

But what if I not only care about the face of the coin, but the angle when I toss it, like, is it like hitting this way or this way?

And I can discretize it into four corners or eight corners or as many as I want.

So the question of what is the entropy of the system, it's really a matter of who's asking the question and what they want to answer in a sense.


SPEAKER_01:
That is a great point.

And the relationality is a theme that comes up again and again, like just the fundamental active role of the interactants.

like the passive observer and the kind of receptive filter in only like all these ways that the active inference helps reformulate that relationality, like even in the cases that you're mentioning.

And then the strongest statement, I believe, to Kristof's point,

uh of going further like like the Overton window on this side I think is Alex Kiefer in some of these discussions where he's like it is for like it is further that it is informational but but the the um the actuality of biological systems makes it so that this theoretical construct

has like a stronger than um instrumental relationship with thermodynamic free energy like if you had the best foraging algorithm your colony is going to exist in the future but best foraging algorithm doesn't mean the most it just you know means given the niche so that's a normativity that has like a survival normativity to things that survive


SPEAKER_05:
that's another piece of how it relates to today nikos yeah it sounds like there might be some unifying concept here that both in the thermodynamic concept there's an energy gradient that has some potential utility for some agent with some goal and in the information there is also from some specific perspective the potential for harnessing that there's

If that makes sense.

It's like if you take a specific vantage point of an agent within that system, you can harness information or energy gradients to some end result.


SPEAKER_01:
yeah that's very resonant with chris fields and mike levin's work on like polycomputing which is that the computational um attributes are themselves relational so like if something is doing a totally encrypted computation it can't be distinguished from noise

but that's like kind of like a um but then encryption is something that is procedural whereas so that has a different set of formalisms than like a statistical distribution but those are all really good points and ones that that that connect very provocatively but not um

You know directly necessarily to.

Some of these felt ways of thinking about that which.

yeah.

interesting times and discussions does anyone want to like add any other comments in the last minutes.

Well, hope that was a fun first chapter discussion.

We'll have the earlier time next week.

People can add more questions.

add and and update the the answers that they're finding relevant um often we have like more people's general reflections in the first of the discussions and then try to address any and all that because that makes us think about so many things after these discussions that people can just add them here and uh yeah any other comments or thoughts or we'll end it i just want to say that some


SPEAKER_05:
very enlightening discussion towards the end.

And thank you all for the contributions.


SPEAKER_03:
Yeah, thanks a lot, guys.

Thank you, everyone.

Thank you.

Bye.


SPEAKER_00:
Thank you, Daniel.
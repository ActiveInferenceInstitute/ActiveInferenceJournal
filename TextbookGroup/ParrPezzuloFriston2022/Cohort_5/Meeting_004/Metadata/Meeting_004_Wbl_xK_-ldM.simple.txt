SPEAKER_03:
All right, welcome back, everyone.

It is October 10th, 23, and we're in our first discussion on Chapter 2.

So before we jump into any questions or anything, what is anyone's just takeaway from Chapter 2 or what were they curious about or what's something that kind of remains with them after reading?

Feel free to just raise your hand or just go for it.


SPEAKER_02:
Pablo Padilla- hi.

Pablo Padilla- Pablo here.

Pablo Padilla- In the page 35 action has a systemic value and pragmatic value brings rewards so that, for me, for what i'm building is very important so yeah I have.

also some other notes that and a question over there we can cover later so yeah for me this is like my pick on this awesome yes the idea that we don't have to coerce epistemic value or learning or information gain into utility


SPEAKER_03:
we don't have to like convert it into utility in order for that policy to be selected key aspect any other favorite quotes or fun parts of chapter two or parts they woke up sweating about or and I for me tying tying this back to the surprise and um and and um


SPEAKER_00:
maybe elaborating on what encompasses policy.

I'm putting strategy in there, but I don't know if that's correct specifically in the low road.


SPEAKER_03:
Cool.

Like policy enumerates the possibilities for action over a given time horizon, but strategy isn't just enumerating branching paths.

So then what is strategy?

Great questions.

Any other just random chapter two thoughts?


SPEAKER_07:
When it comes to this idea of planning as inference, something that I've been just kind of thinking about a lot lately is that ultimately when they do something, and the best example is just trying to walk somewhere,

All I really do is to hold a conscious intention to be somewhere.

And then I'm kind of out of the loop other than having, you know, processing sensory evidence that I'm actually getting there.

And as long as that's happening, you know, I can sort of see this loop in my brain where I'm going and just enjoying myself, looking at things and just thinking all kinds of other thoughts and so on and so forth.

And the going kind of happens on its own.

All I really do is to have a conscious intention.

I don't even necessarily have to feel my body as it's moving.

If I get into this state almost like a walking meditation, then it really feels like being in a self-driving car.

So it's kind of interesting that

you know i'm starting by understanding all of this stuff like having it having a generative model to notice these things in my own awareness i'm starting to notice these things in my own awareness and i'm starting to be you know not only uh have have these ideas confirmed mathematically but also exponentially so this is a little bit weird but um really fun awesome


SPEAKER_03:
Cool.

And, and also no amount of propositional intention to walk.

You can think as hard as you want, I want to walk, but then actually the intention that gets you out of the chair is something different.

So what does that, what does that mean?

Or what does that have to do with action?


SPEAKER_07:
Yeah.

And, but, but, but, so you have to, you have to sort of, um, distinguish different kinds of intentions.

So like there was the, there was the intention as in kind of like

would be nice if or like wishful thinking i i wish i could do this um but what i mean by intention is a very specific mental state and that mental state is usually not available to you unless you start training in meditation where it becomes obvious that it exists so it's something that like you know if you think about like everything about this from the perspective of training neural network for robot to do something so you know robot will have some kind of

goal in the neural network, getting to a place or whatever.

So it must be some kind of activation in the neural network that's going to be triggering a sequence of other activations, right?

Or a bunch of activations that are going to go through some kind of cycle in time, that sort of thing.

And normally that's an unconscious thing.

But I don't, I don't think it has to, it has to be something like the Buddhist meditative practices, particularly inside meditation, they specifically, uh, you know, distinguish the intention.

So at some point you might be become aware, like first they have an intention to move the hand, then they have a feeling of moving the hand.

So as the I'm holding the intention and I'm getting the sensory input to confirm that what I intended is happening.

And that process is actually, uh, it's possible to introspect.

It's possible to, to, to see it in your own awareness.

And that's, um, that's, that's wonderful really.


SPEAKER_03:
awesome any other opening thoughts from people just some low road or chapter two thought or something that they're excited about to explore a little bit today um the um the getting a little deeper into what is learning


SPEAKER_00:
versus inference?


SPEAKER_03:
Yeah.

Multiple timescales.

that computational models of cognition play out on the fastest sometimes being called inference the kind of rapid fire neural spiking type a little bit slower attention with the precision waiting stepping on the gas pulling off the gas and then a little bit slower even learning

Christopher McConkey- Sometimes the whole process is called inference sometimes inference is only being used to refer to the kind of like crack of the whip most rapid time scale.

Christopher McConkey- But what is learning the higher order phenomena like a child learning how to read.

Christopher McConkey- How do we connect that with a more narrow statistical concept of learning like we learned that the average height of the tree in the forest was 10 feet, but previously we thought it was nine so that's learning.


SPEAKER_00:
Janet Callahan- Is that the same as belief updating.


SPEAKER_03:
Christopher McConkey- What would anyone else say what's, what is the relationship between learning and belief updating.


SPEAKER_02:
I'm curious about if this system is deterministic or it's more on the free will side or none at all.


SPEAKER_00:
Are you saying about the belief updating or about the active inference?


SPEAKER_02:
About the active inference.


UNKNOWN:
Okay.


SPEAKER_07:
I like to joke that you have free will only so far as that you're free to suffer.

If you really understand that you're not a thing, that you're a process and you're totally interdependent with everything else, then the best thing to do is always to be a free energy minimizing agent and do it in an optimal way.

If you're not doing that in an optimal way, that will cause some friction or suffering.


SPEAKER_03:
Christopher McConkey, We can jump to this cohort five question that's kind of.

Christopher McConkey, related.

Christopher McConkey, And then yeah.

Christopher McConkey, Okay, who was that.

Christopher McConkey, Okay i'm sorry oh yeah i'll go for it.


SPEAKER_06:
Okay, so the relation between belief updating and learning is that belief updating is much more general than just learning.

So both inference and learning involve belief updating in some sense.

And this is one of the

probably most confusing things about active inference modeling because most people, when it comes to learning, they only think of belief updating as applied to only the learning side of active inference.

But then Bayesian belief updating can be applied to both inference side and the learning side.

So they use the exact same kind of Bayesian belief updating mechanism.

So we'll see how it works in relation to both more explicitly in chapter six.


SPEAKER_03:
Yeah, the faster inference scale is kind of like you have the calculator preloaded, like the weights in the neural network are fixed if you want to think of it that way, or the parameters are kind of like preloaded and preset.

And then inference is like new argument comes in, new input comes in, and it just is like a plug and chug.

That's the faster inferential scale, which can still result in a belief updating.


SPEAKER_05:
then learning is actually the change on those weights or like the change in that in that system how it's set up so that it influences future rounds of fast inference um so in that respect um it's almost like what christoph was saying uh vis-a-vis intentionality prior to a generative model where learning is

changing those parameters you know prior to having the generative model being able to update that intentionality which the which then the belief updating happens in relation to perhaps learning could be like what kind of thing am I and then the faster inference is like given the thing I am how do I respond and then the kind of


SPEAKER_03:
general theme is instead of evaluating sense making or decision making in terms of like maximally rewarding they're cast in terms of most likely so instead of given what i value what's the most rewarding it's like given the generative model what's the most likely outcome

And then in chapter nine, we'll look at, given what empirically did happen, what's the most likely generative model?

So that's the kind of scientific, like behavioral researcher going from the data to the most likely model.

And then the other direction is from the model generating the most likely data.

And both of them are, they're just two directions on the same freeway.

um all right let's look at this question and then anyone else can like add a question to the table or or um write another question in the um chat okay this means that free will is part of the model question point 2.3 the interplay of top-down and bottom-up processes

distinguishes the inferential view from alternative approaches that only consider bottom-up processes.

Okay.

Probably a lot that could be explored here.

What does anyone want to kind of bring in here or say about this section?


SPEAKER_02:
Yeah, that was my question.

So yeah, my thought was

If it's bottom-up, it would be deterministic.

Everything is round.

You are just observing.

Just getting the data in and you probably won't have much action on that or your agency, I think we call it.

And if it's top-down, it means that I have an agency, I'm deciding, I'm modeling the world that surrounds me.

yeah that was my my question my thought and something that i care it it's very philosophical as well i think and and yeah anyone have a thought on this


SPEAKER_04:
Yeah, this is Andrew.

When I hear this question, I just want to recommend Eric Howell's new book.

I forget what it's called right now.

Sorry, I'm multitasking as usual.

But he talks about how those

I think the active inference is completely congruent with his theory of causal emergence, which talks about how you can have bottom up processes and top down and and still have some room for free will in between with just mechanistic

uh rule following behavior and and so i i feel like there's room for um the philosophy to still exist within a um a a non-dualist uh approach to um metaphysics yeah thank you andrew yeah yeah thank you that makes sense


SPEAKER_07:
If I may add a little bit, you know, I think it's interesting to consider here sort of Stephen Wolfram's principle of computational equivalence.

So Wolfram, based on his experience with computation systems, essentially there was a level in the computational ladder, ladder of sophistication, and it's actually pretty low.

It's incredibly low that once you hit it, you're as sophisticated computationally as any other agent.

So even if you had a completely computationally deterministic system where everything is fixed, like the universe, there is absolutely no way you're going to be able to properly predict, out-compute even the simplest agents, like simplest cellular automata and so on, because they are effectively as computationally sophisticated as you are.

So that gives always rise to uncertainty.

It's a building feature that you will not be able to compute the

perfectly a model of internal states.

And this comes up in active inference modeling and the difficulty of constructing these generative models.

But it seems to be a fundamental principle of nature that's impossible.

So as an agent, you know, if you're as an agent, you're kind of a controller for future states.

You have an intention that you want to achieve a future state and you're trying to sort of get there.

But the world that you're trying to control is never going to be simply deterministically following rules.

So there's going to be resolution of some uncertainties.

And I think that resolution of uncertainties, that gives the

Radek Machan- rise to the feeling of free will the underlying system might be completely deterministic or the experience of it from from within.

Radek Machan- A part of it embedded in the system it's always been feel I had these choices, because I always had some uncertainty.


SPEAKER_02:
yeah that yeah yeah that that's uh another thing that I uh Steve uh well from uh it's I I always wonder how those these two currents are gonna cross over yeah yeah I suspect on a machine learning Street talk with Carl and Steven in the coming months we'll see though that should be quite um interesting um Ali and then Susan


SPEAKER_06:
Okay, so when it comes to agency, I suppose active inference kind of assumes that any lifelike property, including agency and many other things, they're kind of inherent to and also their emergent property of any dynamical non equilibrium system.

And more often than not, these systems are ergodic, and also they possess markup blankets.

In other words, FEP supposes that organisms have this goal of keeping themselves in their expected states, or more precisely, their expected phenotypic and ontogenetic states.

And consequently, they act to minimize the discrepancy between

the desired or predicted state of affairs and the one that they experience.

So to quote Friston himself, it furnishes an account of basic or biotic sentient behavior that places agency center stage.

So I believe active inference and FEP is not a deterministic theory of cognition at all.

because as I just quoted Friston, it takes agency very seriously and even at the very central stage of the theory.


SPEAKER_03:
Thank you, Ollie.

Susan.


SPEAKER_00:
I kind of to tie on that and pose a question in terms of agency.

I kind of question whether we

we recognize without learning our requisite limits which is kind of what you I interpreted what you were talking about um you know people go over their requisite limits all the time it's called anxiety it's called you know schizophrenia or whatever but but it's this has kind of got me thinking that the agency and the affordances is kind of a chicken and egg problem

it's like, you know, as I, as I update, um, as I find more freedom, um, which, you know, freedom to choose.

And in fact, you know, when I heard the intention, it was like, well, I have a choice of other intentions.

So that's going to cascade down.

Um, that's, you know, that's basically agency, but, but I still have to, so the belief updating or learning, um,

you know is going to show me new affordances but then I need new agency to be able to do something else with that affordance so you know so is it kind of a chicken and egg problem or how does that play out in this model a few narrow points but first Ali please


SPEAKER_06:
Well, yes, actually, there is some conceptual overlap at the FEP with the kind of ecological psychology viewpoint offered by Walsh in terms of affordances.

So Walsh

argues that if an agent has a goal, it will experience its environment as a kind of menu of affordances, what's in its surroundings and whether they're useful or not for attaining that particular goal.

So the environment agent interaction might be regarded as a kind of landscape

that should be navigated to that given end.

So in effect, what it says is it posits a kind of optimization problem in which

one might expect efficiency to play a role in determining that trajectory.

So that's basically the overlapping between the formulation of FEP and this ecological psychology viewpoint.

or at least Walsh's formulation of peepsonian affordances, because in both of those cases there's this bilateral interaction between

uh between the environment and the agent so as to uh the the chicken egg problem uh it cannot really be separated from each other so uh there's this continual um bilateral interaction between those two cool yeah just just two narrower points so pablo kind of kicked the soccer ball off


SPEAKER_03:
with this question about bottom up and top down.

And these generative models are as constructed.

So we could imagine a top down constraining force that's just always doing, it's always clamping the same way.

Or it's alternating between two different top down impositions.

So just the mere presence of top down forces does not

support or reject in the system of interest itself the presence or absence of free will defined as as someone chooses um but by comparing a portfolio of models it might be possible to to garner evidence or kind of um come to favor one of those choices more and similarly the um

information coming bottom up could be generated deterministically it could be an oscillator or it could be generated probabilistically like being drawn from a distribution um and that could drive belief updating in the agent in a non-linear way or in a linear way so it's like sometimes these um adjectives that modify the probabilistic

internals of the model, deterministic, probabilistic, all of that.

there's there's a lot of richness but also openness in how they articulate with um the broader philosophical questions like robert sapolsky's current book or it's coming out in some days or something like that it's called determined the science of behavior or living without free will so it's like deterministic sounds like it's going to be addressing the exact same question

that Sapolsky is going to be addressing.

But then as Christoph pointed to, even a deterministic system of quite limited sophistication, the only way to predict its behavior may be to play it out.

This is kind of like Turing's halting problem.

Even if you're an expert in programming, you look at a piece of code, you may not be able to know a priori whether it's going to halt or how it's going to act.

and then uh again to this kind of like more limited generative models have a zone of surprise all around them but the zone of surprise with respect to the modeler isn't the same thing as a truth claim about the system you could make um an example where the affordance landscape is fixed like the rat in the teammates that we'll see later

a real rat in a team maze might discover a new affordance like it might find out it could roll over and you know knock something over and then if it has that capacity to learn that could be modeled as like expanding its e variable like expanding its um repertoire of affordances and then it might come to select those so then it

but the fact that a given generative model does or doesn't have the ability to expand its repertoire of affordances the real rat on the ground just because the model can't doesn't mean the rat can't just because the model does doesn't mean the rat will so that's the kind of iterated continued engagement and

Rupert Clayton, Repeated modeling as a way to develop hypotheses to re enter the system of interest with action, not to just make version one of the generative model shut the door on the question.

Rupert Clayton, Oh well.

Rupert Clayton, what's another question or section that somebody wants to look to.


SPEAKER_00:
I'd really like to capture what you just said about the rat.

That was prolific.


SPEAKER_03:
Oh, mammals.

Let's look at some classic questions.

There's a lot here, but does anyone want to just ask one just that comes to mind now, or something they scribbled in a margin.

Yeah, thanks, and then Oh, yeah, thanks, first, and then Andrew.


SPEAKER_01:
So considering the difference between model and process, i'm still very far behind all of you guys.

So I was thinking.

the model doesn't have to be true right it's subjective it has to work it doesn't have to be true does the process have some limit like do we know absolute truth on the x asterisk the hidden state that is asterisk as part of the process I'm thinking that connecting that to um

I don't know if you guys have read Greg Egan's Shields Ladder, where they discover, well, by mistake, through an experiment, they trigger a vacuum that is a more stable state than our vacuum and it starts,

Rasha Elibiary- unrolling through the universe, the way ice nine does in cat's cradle by Kurt Kurt Vonnegut, so I was thinking i'm not like basically inventing more and more stable models they roll out like that, like a carpet through everything because they're so much easier to survive and persist through.

Rasha Elibiary- But does on the process side, the real real hidden state does it have truth.

Rasha Elibiary- yeah.


SPEAKER_03:
that that reminds me of this very short Borges um story where um you know this is this is a somebody read chapter six a little bit too literally um and they wanted a perfect map and then it it came to just like you said inks it came to unfurl over the whole territory until it just blurred with the territory

Another answer to that is x star, the generative process, using the way that the textbook talks about it, though Ali may bring up a very salient point, is still something that's proposed by the modeler.

And so x star could be said to have a local truth.

Let's just say we have an air condition turning on or off agent.

kind of regular setting like that thermometer sense readings making inference about temperature in the room taking action to turn on the air conditioner and then we have like the generative process which is giving rise to the temperature readings now all of that is like a local constructed truth but just making some sort of function to describe how temperature um

plays out in that modeled room, again, that doesn't mean that the real room has that feature.

Ali?


SPEAKER_06:
Well, yes.

So there's actually this kind of criticism or debate going on around FEP about whether it's

too general to be a realistic scientific model of the agents or more specifically cognitive agents.

Because on one hand, it kind of puts everything in a great degree of abstraction to the extent that it may not be able to answer some

some concrete questions about the biological or the biological nature of the agent or the real world situations of the agent.

So, for instance, there was this paper by Koloman Palacio from 2021, namely,

C Sinan Gunturk, Ph.D.

: Let me get non equilibrium thermodynamics and the free energy principle in biology and which they explicitly put forward this criticism around FEP as being too general to be useful for modeling any.

real world or concrete biological problems.

But on the other hand, FEP never claimed otherwise, because according to the FEP, any or a given non equilibrium system with a target state would always seek the most efficient route.

And this route would typically be the one defined by considerations of the gradient descent, or in other words, the path through all the available configuration space that follows the steepest route to the minimum.

So again, in this view, it's not even explicit whether

the agency or all the other concepts we're talking about are real or the apparent properties of the agents.

More to the point, I believe FEP and active inference can be more congruent with structural realism view of the philosophy of science as opposed to just

old-fashioned or traditional pure realism or naive realism of view of the science.

And in this regard, Friston co-authored some papers with Majid Beni, and they argue for this view of FEP as

as a kind of view that is more congruent with the structural realism and particularly ontological structural realism.


SPEAKER_03:
Awesome.

Christoph?


SPEAKER_07:
you know I just want to say that um holding on to any kind of notion of truth um I I think the the best the best thing that ever happened to me in my life is I decided there's just there isn't there are no proofs they're just simply models within models within models and you can sort of see it in the temperature example temperature is not real it's a virtual variable it's literally what the physicists would call it right that's a um it's a way of course graining all the um interactions

Dimitrie Hoekstra – between gas molecules in such a way that we that we can measure, so we have something that persists for time that we can observe but.

Dimitrie Hoekstra – You you're never really looking at reality, and if you go down, you know.

Dimitrie Hoekstra – This rabbit hole, then, ultimately, you will arrive at something that like there is some computational say well from.

Dimitrie Hoekstra – perspective on this, I am very fond of that one, there was some computational model that's running all possible computation, so what truth would be in that in that kind of context.

C Sinan Gunturk, Ph.D.

: would be the shortest description, the shortest possible program that gives rise eventually to what you're observing.

C Sinan Gunturk, Ph.D.

: But because you know because universe is because everything is kind of like interdependent you're never going to be able to to to to I think I think you're never going to be able to separate.

C Sinan Gunturk, Ph.D.

: You know, say like Oh, this is, this is the the the the the true process that gave rise to to this observation is going to be some kind of interplay of all kinds of computational processes.

all running kind of independently.

I think this is why there's this difficulty in writing down these generative models of all these possible

um possible when we have this um you know probability of a of our priors because there's absolutely no way to to know what these priors will be we can only make uh uh you know get guesses of what the what the range of these um uh of bodies uh can be taken and adapted as um evident uh evidences observations are flowing in cool also on this kind of


SPEAKER_03:
generative model generative process uh point this is one great great question um in figure 2-2 and throughout this textbook pains are made to distinguish the generative model from the generative process and it is a useful distinction to understand the process that gives rise to the observations the modeled process that gives rise to the observation

So the kind of like digital niche of this generative model of the agent.

And yet in a guest stream, we saw a slight different perspective, or at least way to talk about this when Maxwell presented and utilized this bracket to cover the entire generative model, including external states,

Again, those that give rise to sensory states.

And at this time step in the published transcript, Ali asks this question very specifically.

So there's, of course, many ways to speak this or any dialect.

But within this textbook, generative process is used to refer to the process external to the agent.

particular states of the agent blanket and internal and external states are described as generative process giving rise to the observations and not influencing the actual fundamentals of the particular partition but rather just using a different natural language to describe it it is also becoming common to describe the generative model as describing all the necessary and sufficient

statements that the modeler makes about the entire scenario and not distinguishing so much as to what's the generative process in the generative model one piece of uh I guess support for like this kind of just use generative model to describe it all first off it's

simpler.

Also, if you have two agents in conversation, then, of course, one of them is the generative process to the other and vice versa.

And so just in the same spirit of like, well, states of the world or aspects of the world, let alone the map, they're not tagged as like sensory or action or internal or external.

Those qualifiers are always relative to which node is being called the internal state.

And then the rest of the blanket is just trivially assigned from there.

So it doesn't make sense to like,

David Price- Say well this desk is the external state it's like well but it's its own internal state if I chose to model it from that side or with respect to the wall behind it it's the blanket state between me and the wall so.

David Price- Using generative model little bit more broadly.

David Price- seems to be a little bit of a development that has unfolded over the cohorts and years Susan.


SPEAKER_00:
Yeah, I mean, the way I see it, correct me if I'm not looking at it correctly, but when you do research, you have to decide what your ontology or epistemology is going to be when you apply it.

B. G. And, and you always provide context for that, so this kind of is above that in my mind, in fact, you know we you and i've had a conversation because I you know I want to make sure that the that it's.

B. G. That it's ideology free.

B. G. Because yeah I see way too much.

B. G. yeah.

B. G. bs me, especially in management science that.

Barbara Kirby, You know it's just embedded with somebody ideology and it's taken as fact so it's kind of what I like about the model.

Barbara Kirby, That.

Barbara Kirby, brings me to.

Barbara Kirby, The the mark off blanket.

Barbara Kirby, And yeah where you say in that sense of the the model, the generative.

model where where does the markov blanket uh extend because it was a little confusing reading it is that it just confirmed for me though am i looking at it correctly in terms of it being a heuristic uh that's why i posted that in the comments because that's what i copied from chapter 10 i think cool maybe post what what page you you found it on but it's it's a great quote um


SPEAKER_03:
Features are not intrinsically on the blanket or internal or external.

There's a Bayes graph that conveys whatever variables are being included.

Whatever quantified aspects are under consideration or formalized, doesn't have to be a number.

It could be like categorical, but whatever is being brought, whatever Lego pieces are coming on the table and how those Lego pieces are connected.

That's the Bayes graph.

And then whatever node or cluster of nodes is being selected as the internal states, the blanket is just all that surround that.

And so when we're taking this kind of agentic view or cybernetic view on cognitive ecosystems, then

the blanketing states just to give a first pass can be understood in terms of sensory data so anything that you would record with a sensor and then the outgoing dependencies are actions anything that would be described by the affordances of an actuator so to answer to add build on what ink's question was would the Markov blanket be the same


SPEAKER_00:
for you know if I'm doing let's say I'm doing a research and a modeling project would I use the same Markov blanket for the process and the model or would the models or would the process change the blanket the the final mark just giving a narrower


SPEAKER_03:
answer because I I I know that's something that could be explored at great depth if you made a generative model with a fixed Markov blanket then it would be fixed if you said our room has a thermometer and an arm that's an actuator and you know different data are going to come in and the arm is going to do different things well then the the kind of um scheme of the blanket wouldn't change

but you might want to be interested in modeling the transition from that room, deciding that it should put out a second thermometer at which point the scheme of the blanket would change.

So then you'd be outside that room and be like, oh, they literally just went from having one thermometer to two thermometers.

And then you could postulate cognitive models that happened inside of that room.

You'd say, maybe it's like they had an affordance to add a thermometer and they selected that affordance.


SPEAKER_00:
Does that answer your question better, Inks?


SPEAKER_01:
No, I have more questions, but I don't want to stir more trouble here.


SPEAKER_03:
What else is the end of a session for other than stirring trouble?


SPEAKER_01:
So because you were talking about being in the room, can I just roll with it?


SPEAKER_03:
Of course.


SPEAKER_01:
were talking about being in the room and then a second thermometer and i was thinking of as an agent this i don't think this is about this chapter but as an agent as my mark of blanket is

like i'm looking at the screen right now but then i have a flutter in my heart and i'm like oh no am i having a heart attack in my mark of blanket is that a separate mark of blanket where now i'm modeling myself very close and tight around my heart then i hear the door knock and oh i have a package from ukraine and i run

Like now my attention slash Markov blanket is all the way my apartment and the door to the outside world where someone is knocking.

Are these different?

Am I switching between different Markov blankets or is the Markov blanket moving with my attention regime?


SPEAKER_03:
Great question.

Well, to the wandering blankets and blankets that are not sort of one or zero, but can be on a continuum or blanket index.

Certainly, some of the most recent work is addressing that.

I'll just leave that there.

But to the interoception question, in Chapter 6, when we're exploring how to construct these generative models, an example of sensor fusion is provided.

So let's just say that a given body has some interoceptive sensations like heart rate, and then some exteroceptive sensations.

sensory capacities like hearing and vision and so on um those are kind of like all open even when you close your eyes you're still getting visual input and usually it's modeled not as changes in the blanket but in changes in attention like you still have proprio receptors in your foot but you're not paying attention to them you're not attending to them

which is to say it's as if the data coming from them are not making a difference to belief updating.

If you're not paying attention to something, it is not having consequence.

Its differences are not making a difference.

And then regimes of attention describe the way that in this kind of multimodal setting,

um, the spotlight of attention might focus on a given sensory modality.

And so that is usually modeled more as an attention modulation task within a fixed Markov blanket schema, rather than, um, changes to the scheme of the blanket itself.

Christoph?


SPEAKER_07:
So, you know, this makes me think that, um,

having this sort of explicit model of attention within a fixed Markov blanket, it's really because Markov blanket is a kind of like a convenient, it's an idea, it's a convenient idea for modeling to make this separation, but it's not actually something that is real.

There is any boundary that you can find and say like, yes, this is definitely a boundary between the internal and external states.

Mateusz Piorkowski- Some work on moving blankets, I think I think what we're eventually going to realize is that.

Mateusz Piorkowski- it's there's just a precision parameter there as well, like how how how how thick I want my market blanket to be and.

Mateusz Piorkowski- it's for the convenience of describing certain phenomena at certain temporal scales, it simply makes it makes sense to make an assumption that there is a separation between these internal and external states.

But in reality, if you look at all temporal scales, you will find that the separation is not really there.

It's invented.

This is the way I understood the Buddhist concept of relativity.

Everything is sort of relative, and I put myself in the frame of reference, and I said, well, within this frame of reference, these are internal, these are external states.

And then I have a way of looking at them to make sense of them.

If I didn't have the ability, then it might be difficult to be able to make sense of the world if I can't make this partition.

But is the partition real?

I personally don't think so.

It's just a point of view.


SPEAKER_03:
Thank you.

Andrew?


SPEAKER_04:
yeah a couple of riffs on um what's been said uh one like that that the finding your own Markov blanket I feel like is a little bit of a um hopeless cause because it's it's very likely that you are many nested um you contain Markov blankets all the way down um and there's

A lot of complex phenomena and interior modeling going on.

In terms of coming to grips with that, one resource I'd like to recommend is, if anyone's seen the book, The Mind Illuminated by Dr. John Yates.

It's kind of a good rationalist take on meditation practice that kind of lets you come to grips with there's probably multiple agentic things happening in your brain.

Another thing that was just kind of wow to me that I just wanted to gush about this chapter is I've been trying to sort of come to grips with what normativity means in a sort of non-dualist agentic frame of reference.

And there was a couple quotes at the end of this chapter, one of which is,

in active inference the identity of an agent is isomorphic with its priors that just like i was like oh all right that kind of puts it all together and um kind of um to to get that um pitched up they start with expected states are preferred and include the organism's conditions for survival

E.g., niche-specific goal states, whereas their opposite, surprising states, are dispreferred.

In this way, by fulfilling their expectations, active inference agents ensure their own survival.

So just that phenomena, one thing...

Your Bayesian priors are your normative order, which I've been searching for what does a normative order mean, especially within the context of sort of a small agent trying to find relevance in the cognitive limits of finding relevance there.

And I feel like this is the right approach.

also one more thing uh uh with the um free uh not variational free energy but um the the planning one expected free energy um we were talking about um what sort of the way it includes both epistemic and um preference value in the same equation i feel like as a group we really need to um

tackle that as a koan and realizing that maybe a lot of our preconceptions that we've been grappling with during this meeting are really summed up with understanding that itself that that it all fits both epistemic ontological in the same sort of optimization framework so that's that's just me um riffing on this


SPEAKER_03:
Great comments.

Equation 2.5 and equation 2.6 are like the heart of the heart of active inference.

We have the real-time sensemaking about incoming sense data and beliefs.

and then we have the prospective which not all agents even need to be modeled as doing the prospective unified imperative for pragmatic and epistemic outcomes and that's the heart and that's the mystery and that's um

like the part that we don't need to reinvent and then all the work is in getting it into a form getting the system of interest into a form where you just press play on those equations um Ali and then any final comments that people have


SPEAKER_06:
Roozbeh Gharakhloo, Thank you yeah regarding whether or not, or to what extent markup blankets are code and code real it's probably worth keeping in mind that markup blankets are more precisely they're not necessarily spatial temporal boundaries, but rather they draw boundaries in state space or, in other words.

In the space of all possible configurations so.

it's true that sometimes it manifests itself as a kind of spatial temporal boundaries, such as.

the cell membranes, but more often than not, we don't need to consider markup blankets as purely physical or spatio-temporal boundaries.

Yes, it's a modeling technique, but in terms of its mathematical reality, it is as true as any other mathematical models.


SPEAKER_03:
cool well very fun the low road is it a bumpy road is it a smooth road what is it like I don't know um but that was our first discussion on it next week at the earlier time we will check back so it would be epic if people just

right in the coming minutes or in the coming week, add a few more questions to chapter five questions, or we'll look at the bigger questions table two.

In cohort four, we'll be heading into chapter eight on continuous time generative models.

And we'll have next week on chapter two, then head into three.

So thank you all.

The game goes on.

So see you next time.

Thanks everyone.

Bye.


SPEAKER_02:
Bye.
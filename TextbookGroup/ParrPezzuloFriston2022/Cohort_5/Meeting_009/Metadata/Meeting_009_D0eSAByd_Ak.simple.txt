SPEAKER_03:
okay well welcome back everyone it's 11 14. we're in our second discussion on chapter four so before we go to the chapter and questions does anyone have any comments or thoughts

Otherwise, we'll just try to rapidly address as many questions as possible.


SPEAKER_02:
Okay.


SPEAKER_03:
Onwards then.

So, in equation 419, what is the meeting of the tilde eta, a component of the tilde epsilon sub V?

Okay, so tildes are trajectories.

Tilde

error minus mean minus observation or external state.

Sometimes I remember that because the mu and eta are like inverted of each other.

Okay.

Oh, okay.

Yeah, yeah.

Right.

But, but I mean, there's various notation sets for internal and external states.


SPEAKER_01:
For mind, for environment.


SPEAKER_02:
Exactly.

Exactly.

But then he being for error.

Yeah, but that's Yeah.

Cool.

Okay.


SPEAKER_03:
In equation 415, are F and G associated with VFE, big F, and EFE, big G, or is that just a coincidence?

That is just a coincidence.

Sigh of relief.

Yeah, I mean, there's no problem with adding more concordances, but no, the F and the G have nothing to do.

I think it's just F of X and then picking the next appropriate letter.

Jeremy, if you if you want to, we're just starting from the bottom, just trying to hit as many of the questions as possible, or feel free to bring up a thought.

Gotcha.

In the syntax for describing expectation,


SPEAKER_01:
I tried to make these renders a little tech, but I ended up just messing up the entire column.


SPEAKER_03:
That looks good.

I like it with the code blocks.


SPEAKER_01:
So I can talk through this question.

So there's the expectation and then the subscript and then the...

square bracket with the function inside.

Now, nowhere in the text could I find an explanation of the subscript.

And it appears to me that that subscript is pretty darn important.

I'm not clear whether that subscript contains the weighting factor or what its relation is to the variables that you're summing over.

So like in my example, I have an X and a Y in the subscript P of X, I think.

so because there are times that where that subscript contains um multiple internal variables so what i'm trying to get clear is what we are actually summing over and what the weighting factor is whether the weighting factor is included in the square brackets or not um i believe that the subscript


SPEAKER_03:
is um what you're summing over like as if it were below the sigma like from you know n equals 1 to n equals 10 and then the weighting is described by the evaluation of the function across those those categorical or continuous values right well well my my understanding of the expectation function is that


SPEAKER_01:
you have not just you have the value at a certain um value of x and then you have the probability of that value and so if you have a dice a six-sided dice the probability is always one over six but um for our circumstances there's usually a higher weight probability right so i

Yeah, in this block, it's like, I'm just bothered that in that box where they explain expectation, they don't explain the subscript.

And it's, it's causing me a lot of grief.


SPEAKER_02:
Okay, I'll just Manir, go for it.


SPEAKER_03:
Oh, did you want to add something Manir?

Okay.

Yeah, box 2.2 could have a definition of subscript.

Here's the probability.

This is the weighting.

Here's the value of X. Here's the probability of X weighted by their probability.

And here's summing over X values.

And that's the expectation of X.

But you're right when you have two variables that it needs to be treated a little bit carefully.

Okay.

What is mean?

What is meant by niche?

Simply environment of the agent.

There's also longer answers I'm sure people can provide.

Anyone else want to give a thought?

environment of the agent partially constructed or conformed by the agent yeah maybe possible for them to modify or construct it but but not necessarily but yes across the interaction it has to exist okay

If anyone else wants to ask a different question just raise your hand or go for it, otherwise we're just trying to go to as many of the written questions as possible.

This is in response to the previous session asking how priors originate in addition to self evidencing optimism bias noted in Chapter two.

I wanted to mention the notion of genes and prior.

genetic inheritance has been framed and active as part of prior belief the constant self I was unable to find it but believe it was suggested by Paul badcock yes um we've done evolutionary models and others have too where genes provide priors any other comments people want to make on this

Okay.

What is a generative model?

How is generative model related to a generative process?

Anyone have a thought on this?

okay so the textbook uses this distinction between a generative model and a generative process the generative model includes the agent's model of observations and also how hidden states result in those observations so internally held variables about external states

and they distinguish that from generative process, which is the process that gives rise to the observations that the agent observes.

So that's kind of the way that the textbook describes 2022.

However, very importantly, in the subsequent months and years, a lot of people started using generative model to refer to a system-level description.

For example, here in...

guest stream 45 from i guess earlier in the year we had maxwell and mao speaking about this and in the the paper on the inner screen model of consciousness with the quantum fep they use generative model now to describe the entire system um so nothing is functionally different in terms of the particular partitioning

It's just about whether the generative model is referred to as the agent.

And then the generative model is about the generative process or the generative model includes the whole thing.

And then you don't even need to mention the generative process.

At 44 minutes in the transcript, at this point right here, Ali asks that exact question to the authors.

So,

Short answer, generative model is coming to be used to describe all of the system level descriptions that the modeler has access to.

And then if we're interested in just the blanket and internal states, we call that the particular states or the thing.

If we're interested in just the internal and the active states, we call those the autonomous states.

If we're interested in just the sensory and active states, of course, they're just the blanket states.

And then if we're interested in all of the states, we can just refer to the system.

Any comments or thoughts?

Okay, I'm stuck with the derivation of equation 411 and 413.

I wrote which part I'm stuck with in the PDF.

Cool.

That looks awesome.

If anybody wants to go through it, we could, but I think let's just save this for, like, I'll just write a comment that it would greatly benefit from somebody who wants to look through this derivation.


SPEAKER_01:
Another nice comment might be in the, I believe it's in the math group.

One of the first links goes through the POMDP website.

derivation in more detail.


SPEAKER_02:
Cool.

Cool.

Yeah.

Yeah.


SPEAKER_03:
Okay.

We'll leave this one.

Keep on going.

But again, if anybody wants another question, just raise your hand and we'll go for it.

Otherwise, just trying to give a thumbs up to as many questions as we can.

In equation 4.14, how is the necessary and sufficient condition derived?

Why is the gradient of f being zero equivalent to having a policy which follows the softmax of negative sine summation of g and f?

Yeah, this is a good question.

The fully updated.

This is just one possible reading.

When your policy posterior

does not have a gradient so you're sampling your policy posterior um at a stationarity this corresponds to the soft max um transformed negative g minus f like

doing expected free energy updating of your policies is not resulting in any changes to the posterior policy distribution as scaled by the softmax which is the shaky hands parameter

So softmax, just like it does in a neural network, it sharpens or it blurs distributions.

So when there's a low temperature on decision-making, absolute zero, then a policy that's slightly favored will be like up-weighted a lot.

Whereas in the high temperature decision-making setting, the differences amongst policies are erased.

But I'm not 100% sure on this one.

Let's leave it.

Okay, 82, 83.

The equation says that we minimize free energy through action.

Yeah, we minimize free energy through two different mechanisms.

Change the world, change your mind.

ACTION ON THE WORLD ACTION ON THE MIND AND ONLY PART OF THE FREE ENERGY THAT DEPENDS DIRECTLY ON ACTION IS THE LOWEST LEVEL OF PREDICTION ERROR IN THIS SETTING ACTIVE INFERENCE IS JUST PREDICTIVE CODING PLUS REFLEX ARCS WHAT IS IMPLIED HERE THAT ACTION ONLY HAPPENS AT THE REFLEX ARC LEVEL DOES ANYONE HAVE DID ANYONE ASK THIS OR HAVE A THOUGHT ON THIS


SPEAKER_01:
um i have a thought sure is it well in the predictive coding paradigm um you're passing errors and so the errors are the only way that you get mind updates um so is that just saying that um

at the source of error only happens at the reflex arc level.


SPEAKER_03:
So here in this section, it's the it's a motor

a motor reflex.

So on the on the bottom level of the hierarchy, we have like the sensory motor predictive coding is doing predictions of sensory consequences.

And then reflex arcs are basically um,

responding just to move to just to straight without planning or deliberation or counterfactuals, the reflex arcs are just moving to resolve the difference.

So simple motor systems, they're able to be modeled as predictive coding on the base layer with proprioception and action, plus higher level reflex arcs that basically enact default policies when the set point is deviated from.

How do you read, interpret the hierarchy dynamics visuals 4.4 and 4.6?

What does anyone think?


SPEAKER_01:
One thing I would like to know is what the dot versus the arrow is insinuating.

That wasn't described in the legend.


SPEAKER_03:
This little dot?


SPEAKER_01:
Yeah.


SPEAKER_03:
Yeah, good question.

I mean, one short answer is if you're interested in message passing diagrams...

check out the live stream 55 series because that is the most modern message passing representation and how it's being displayed in the book is just being done kind of didactically but the way that the message passing is done in live stream 55 that's like the real way if you want to study it from an engineering perspective

Briefly, these images show two ways of looking at hierarchical predictive processing architectures.

So let's just look at this one here.

So on the left side, we have the kind of all at once hierarchy.

So we see a tilde.

We know that tilde means across multiple time points.

So what's happening here is we have these four clusters.

This is at level I. Here's level I plus one.

Here's level I plus two.

So this is a three-layer predictive processing hierarchy with observations coming in from the bottom through time, O tilde.

and the highest level prior is like a fixed parameter that's how the highest hyper parameter works because if you could learn or update or draw the highest hyper prior from another distribution then it would be another level above it so the left representation is showing the highest hyper prior and then the two kind of thick levels that have this four-fold motif involving the um

calculation of errors between predicted states and incoming bottom-up observations so this is the all at once view on the right that has been unfolded so now we see observation at t minus one t and t plus one so now there's three time steps from left to right and there still is the hierarchy from top to bottom

Any other thoughts or comments on it?


SPEAKER_01:
I want to link to the livestream or reference that as a better source.


SPEAKER_03:
box 4.1 says the blanket of variable x are those variables that

are the variables that dot, dot, dot, dot, dot, and the parents of X's children.

The parents of X's children are not given a variable.

Are these not represented in the Markov blanket formulation?

If they are, where do they show up?

Nothing is simply given a variable.

Whatever the node is on the base graph, that's the variable used.

Once the system of interest is defined by the modeler, then blanket states can be calculated.

And the Markov blanket is everything you need to make two other sets of nodes conditionally independent.

So that includes not just direct parents and direct children, but also co-parents.


SPEAKER_01:
I think the specific answer to the question is it shows up because the function Greek P is taken of the function Greek K in the variational message passing.

So that's saying get the parents of all the children of X.

So it doesn't need its own variable because those are both functions.

So getting the parents of the children is a double function call.


SPEAKER_03:
Okay.

Okay.

I think I see.

Yeah.

Yeah.

The parents of XP of X or row of X, the variables are caused by X children and the parents of X's children.

Like you could have another letter if you wanted to talk about that.

Yeah.

Yeah.

Short answer, anytime you're actually using it, you would just use the variables as defined locally.

What's the negative diag in equation 10?

I think it's a identity or a matrix diagonalization.

Let's see what we wrote for line four.

Not every equation has this, but of course everyone is encouraged, if they do want to read and help the equations, to write natural language descriptions using the ontology.

I'm not sure if this is accurate, but previously we've written entropy

or expected average log prob is equal to the negative diagonal vector of dot product of matrix A and natural log of A. A is the ambiguity mapping.

So I think this operation helps us define the ambiguity, the entropy of the ambiguity.

So if A was just the identity matrix, that would be the minimum possible ambiguity.

Because just states observations would map cleanly to hidden states.

If A were completely like an even matrix, like no sensory inputs could be differentiated, you'd have the highest possible entropy.

Okay.

In equation 4-2, there's an equal sign with a triangle above it.

What does it mean?

It means defined by definition.

In equation 4.8, in the second line, we are breaking up the probabilities, multiplying each term in expected values, summating other components, and so breaking into the composition of two expected values.

My question is why the Q tilde can be broken down into two Q tildes with different arguments?

Didn't we define it in 4.7 as a product of Q and a P?

why the q tilde can broke into two q tildes with different arguments i'm not sure which q tilde is being broken up

I see the s tilde condition on pi and s tilde conditioned upon observations in pi.

Those are the only ones that I see.

But the broader point about developing the derivations from this top line into all the bottom lines, that sounds like good work.

But I'm not sure about this question because it doesn't look like you have to split up anything.

In 4.10, does one sum over tau to get g?

The right-hand side has a tau indicating that it is calculated at a particular time, but g is given by the expectation over all future times.

Does one sum over tau to get g?

I'm not really sure which right-hand side is being described here.

Because there's so many lines.

Okay, g sub pi.

What about it?

This may relate to the difference between tau and t. So t is usually used to mean simulation time horizon and tau to mean like agent time horizon.

so here the pos the the the G calculated on policies which could be multiple time steps long is the entropy of the hidden states on that policy at that time plus sequence of no it's sequence of times because you're doing a matrix operation I think it's basically the


SPEAKER_01:
The definition of G is over a sequence of tau time steps.

So you're condensing the dimension of G is going to collapse because it is by definition over multiple tau time steps.


SPEAKER_03:
Yeah.

G is a functional that updates the policy prior into the policy posterior.

So I guess that all the time steps need to come into play in the evaluation.

So each of these, but each of these are at a given time step, but then it's, I guess across time steps.

But this is definitely not the fullest explanation of where expected free energy is calculated.

Equation 2.6 has a lot more information on this.

I was unable to understand why the expected free energy

is minimized by selecting observations that cause a large change in belief.

Does anyone have a thought on this?


SPEAKER_01:
There's multiple paragraphs about it, but essentially look up epistemic value anywhere in the text.


SPEAKER_03:
Yeah, that's definitely one aspect of it, which is just equation 2.6, information gain, policies that result in sequences of observations that have a large impact on inferences about hidden states through time, X tilde, are loaded with high information gain, high epistemic value.

So expected free energy is minimized by selecting observations that cause a large change in belief.

Yes, that's half the picture.

Expected free energy is contributed to by both the epistemic and the pragmatic value.

What's the role of the generative model in active inference?

Well, we talked about generative models earlier, whether we're using the kind of broader sense of generative model as describing all of the system's dependencies or the narrower sense of generative model, like just the agent in relation with the generative process.

But whichever way you take it, generative model is what we build in active inference.

No generative model, just fun speculative imagination.

But this is the actual object or modeling artifact.

Can you clarify why marginal likelihood is called marginal?

Yes, great classical history of statistics.

When you have a piece of graph paper,

And you have events on the rows and columns and the probabilities of those joint events in the cells on the graph paper.

Then when you sum across the side onto the margin of the paper, it's called a marginal probability.

Okay.

We have some questions with previous upvotes that we could look at.

or we could continue on with these ones that have no outputs at all.

Does anyone have any comments or thoughts or anything?


SPEAKER_01:
I have one comment.

I'm still processing the continuous derivation section.

There is one quote that has

uh i'm trying to grok still which is i believe the mode of the let me get my text um it's talking about how here we go the motion of the mode should be the mode of the motion um in explaining the simplifications that allow the uh continuous derivation um

If you search the motion of the mode should be the mode of the motion.

Yeah, this is all related.

This is leading up to it.


SPEAKER_03:
Okay.


SPEAKER_01:
And I'm wondering if there are any sort of resources to break down the derivation of this section in more detail.


SPEAKER_03:
similar to how there was with uh with the pomdp yes mdp yes the most relevant um in basie mechanics for stationary processes

this question of hierarchical bayesian stationarities arises in the generalized coordinates of motion so when you have the generalized coordinates of motion position you know and higher derivatives and you have a stationarity in that architecture when the rate of change is zero on two different levels that are touching each other

that is the situation that's being addressed here.


SPEAKER_01:
But this is definitely interesting.

Yeah, I feel like if I can find a graph presentation of what's being discussed, I might be able to get it.

So yeah, I'll check that out.

26?

26.


SPEAKER_03:
26.

Here are the generalized coordinates.

And this is the continuous time.

I don't remember what that was.

Yeah.

Not sure how helpful, but that at least is in the bottom.

Yeah, that gives me something.

Thank you.

Yep.

Anyone else want to ask a question?

Okay, let's just try to do a few more.

Basing brain helps us frame problems that an agent engaging in active inference must solve.

Broadly, these are problems of inferring states of the world, perception, or sense-making, and inferring a course of action, planning, or decision selection.

Other than perception and action planning, are there other tasks or challenges that the brain or organisms engage in?

How would we know?

One approach here is just to say, well, what are you trying to model?

Just name your phenomena.

And then maybe that can be cast as a perception or as a planning task, or just more broadly as a sense making or decision selection task.

that's possible because of course inbound and outbound statistical dependencies are the fundamental across the blanket so a huge variety of phenomena will be addressed by that but there's no reason to think that that's all that there is it just depends what kind of agent you're trying to model so it's a little bit too general to talk about all agents

but are there tasks or challenges that the brain or organism is engaged in?

Just remains to be seen by somebody who wants to list out those phenomena.

Okay, for the graphical models in figure 4.2, what is an intuitive example for each structure?

This is a great basic question, helping get familiar with the Bayesian graph formalism.

Like, okay, I boiled water and it was hot and it made a sound.

Or I took two ingredients and I made one dish.

But this is kind of the fundamentals for all the other base graph discussions.

Okay, this is another great basic or consolidating question that people could give thoughts on.

What are the definitions of variational free energy and expected free energy?

What roles do they play individually or together in active inference?

Anyone want to give a thought?

Otherwise, this is truly a fundamental active question.

Makes me think of like a class.

Variational free energy, equation 2.5.

It's the real-time gauge on how well things are going.

Variational free energy is a functional of only prior beliefs Q, variational beliefs, and incoming sensory data.

So it computes a measure of how surprising sensory incomes are as a function of the agent's beliefs at that time.

So it's like a diagnostic of real-time sensemaking.

Expected free energy transposes that setting and that variational Bayesian approach into counterfactuals of futures that haven't happened yet.

So expected free energy considers sequences of possible observables,

that could happen conditioned upon different actions that the entity has the agency select amongst affordances and those policies are evaluated in terms of the epistemic and pragmatic value and then given the policy posterior the modeler can take different methods for policy selection like you could just draw from the policy posterior or you could select the highest evaluated policy

So everything can be understood as having a variational free energy due to its persistence through time.

And certain kinds of action selection and planning agents can be understood as doing something like expected free energy.

And Livestream 55, generalized free energy, GFE, it may swallow them both.

So we may look back on 2023 when we needed to differentiate VFE and EFE.

It may just be a transient framing because GFE has some advantages relative to both of them.

What is the use of categorical distributions?

They're statistical distributions that apply to things that are categorical in the world.

In message passing, is there a decay of information as the distance between variable X and the individual Markov blanket constituents increases?

Is implementation of information decay in message passing an option for model implementation?

I remember we had some quite interesting discussions on this in a previous time.

Any thoughts on this?

Bayesian graphs are topological.

They're not geometric, so it's not exactly clear what distance means here, but let's just interpret distance topologically, like how many connections the information has to transfer through.

Then you could imagine a network that's lossless, at which point there would not be a decay of information, or you could imagine a lossy Bayesian graph.

where information was lost.

Is information decay and message passing an option for model implementation?

Yes, you can always develop models of information decay.

However, I would caution message passing is probably not where that would happen.

Message passing is a very low level technique.

that probably doesn't have any advantages to simply have informational degradation happen in.

But ironically or interestingly, you might want to execute a rigorous message passing schedule to model information decay on a base graph.

However, you wouldn't want there to be information decay in the message passing.

you'd want the message passing to accurately describe the information decay of the base graph.

To update beliefs about policies, we find the posterior that minimizes the free energy.

Does a posterior at time t become a prior at time t plus one?

Yes.

Any thoughts or questions people want to make?

A few more here.

A few more down here.

Q is not a single variable.

Q is used to refer to any variational distribution of choice.

So it can take on different characteristics.

It can be conditioned upon different things as well.

You still may have to marginalize over different things or conditioned upon on different things.

So is Q tilde conditioned upon, so is Q of X conditioned on pi, or Q of O conditioned on pi, a separate variational quantity, which is defined in 4.10 as categorical of O sub PT?

This one.

No, definitely not.

It's a separate variational quality.

Okay.

Policy selection active inference automatically balances exploration and exploitation.

Already, this is quite a contentious claim.

I mean, does it automatically balance it such that it's successful?

Or does it give us a palette or an expressivity to set up systems that can try to balance it?

I prefer the second one, but it's written like the first one.

The balance between epistemic and pragmatic value depends on basically, among other things, the intensity of C. This prior is the weighting factor between exploration and exploitation.

Yes.

Amplitude of C relates to how much pragmatic value is calculated for the EFE.

And thus is one way to tune explore-exploit balance.

Another way to do it is to modify the ambiguity of sensory states.

What is the difference between some other formulation for cognitive agents that places an arbitrary weighting factor between exploration and exploitation?

The only thing I can think of is EFE makes possible variational approximation methods for optimization, which would be unavailable to an objective function simply awaited sum of terms.

Yeah, I mean, it would be helpful to provide what cognitive architectures you're thinking of.

For years, as people have brought up other cognitive architectures,

like here and elsewhere I've reviewed them, there are usually a vast number of differences between any given cognitive architectures.

But as this question highlights, one of them would be a traditional way to approach this problem or tension

is to have an explicit weighting factor between exploration exploitation whereas in in the active generative model you don't have an explicit trade-off factor you just have the generative model and then in different environmental settings it is going to have different arising behaviors and it can do the variational approximation methods

But you could have a cognitive architecture that was like a variational autoencoder, which would enable you to use variational methods.

So it's not that the only difference is the variational methods.

Any kind of last thoughts or questions?


SPEAKER_00:
I have a quick question.

When we say variational methods, doesn't it really essentially mean that we have some kind of parameterized model that we're optimizing through some kind of optimization procedure?

So that would apply to... So effectively, deep learning methods are, in a sense, also variational, right?


SPEAKER_03:
They could be and many are.

Most narrowly, it's when the variational approximation to exact Bayes is used.

So not all methods use variational approximation.

So not all deep learning or statistics are variational statistics.

However, broad categories of them are.

So using this variational autoencoder scheme, the input obviously is the sensory input.

The latent space are the cognitive states.

And then the output is the action selection.

So all of this, which is otherwise like just an engineering sketch,

All of this is kind of refined into the first principles, Bayesian mechanics, particular partition, physics of cognitive systems, path of least action, all of those features.

That's the kind of analytical setup of active inference.

But it neatly touches to existing architectures like this.

again there's also machine learning architectures that don't use variational methods so it's not simply just being parametric um this is a this is a short page but also a relevant one energy-based models which is funny because they even talk about generative models

So reproducing data distributions using these statistically physics based methods.


SPEAKER_00:
You know, there is a paper that I don't think made much of a splash, but it's really interesting in the context of, somewhat in the context of active inference, is that effectively every classifier, no matter how it's trained, can be seen as an energy-based model.

And that was the title of the paper.

Your classifier is secretly an energy-based model and you should treat it as such.

So I think there's some kind of really interesting deep connection there between even a model that's not explicitly trained using variational methods.

If you look for the lines of that paper, then obviously the equations for energy-based models, they look very much like the formalism based on free energy principle.

I'm sure there's some interesting nugget of gold there that could be dug out for someone a little bit more versed in this mathematically.

But it's interesting that this stuff is just hiding there.


SPEAKER_03:
Yep.

Totally agreed.

Even when the notation and the ontology look very different, there's no reason to think that that's the end of the road.

So thank you all.

See you all next time.

Thank you so much.

Peace.

Bye.
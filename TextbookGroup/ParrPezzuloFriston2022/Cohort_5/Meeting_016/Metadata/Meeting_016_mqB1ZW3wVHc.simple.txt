SPEAKER_03:
all right welcome back cohort five we're in our first discussion for chapter seven so andrew thank you for facilitating feel free to take it from here um so yeah uh first meeting on chapter seven so i'll do what i did with chapter six and maybe just give a


SPEAKER_01:
I'll attempt to give a brief rundown of the chapter outline and sort of layout and what's going on.

That said, there's a lot going on in this chapter.

And I would say, aside from chapter six, those who are interested in getting started on building active inference models are going to want to kind of give a kind of special amount of weight to this particular chapter.

We are introduced to hidden Markov models and partially observable Markov decision-making processes, which BOMDP is

shorthand for that and it's it's just as of right now that's kind of the not completely ubiquitous but most widely used models for a variety of behavioral experiments um that said a couple weeks from now we'll get into chapter eight where we talk more about continuous models and um

That'll be for looking at instances where we're looking at very, you know, fast time scales and great for things like motor movements and just things that are much quicker.

But usually we're dealing with discrete variables.

So chapter seven.

quick intro 7.1 uh we focus on models of categorical variables in discrete time with examples increasing in complexity so we're introduced to models of perceptual processing decision making info seeking learning and finally hierarchical inference and the authors try to

kind of get at different sorts of emergent properties, including measurable physiology and behavior from these models.

7.2, we're introduced to the hidden Markov model on a perceptual processing experiment.

This is where the model is of our beliefs about how a musician's audible notes, which are the observations or outcomes in the model, are generated from a written musical score, which are the hidden states, the actually written notes.

There, they simulate the dynamics of Bayesian belief updating induced by a sequence of heard notes.

Again, the outcomes.

And then from there, it gets into, before I move on to the next section, it's just worth noting all of these experiments in this chapter more or less denote very specifically what are the hidden states

What are the outcomes?

What are our matrices that we're looking at?

The A matrix or likelihood matrix, probability of observation given state.

Our B matrices, which is our beliefs about how one state moves to the next.

from time step to time step.

We have our D vector, which are our prior beliefs about states, just starting at time step one.

That's it.

Once you move on from there, D is no longer, it's in the equation, but it's not repeatedly used at every time step.

Although it can be updated depending on how you're running an experiment, such as from trial to trial.

So we get to see how it evolves over time.

There are five total time steps.

It's another thing important to note.

A lot of experiments, you'll typically decide how many time steps occur or will occur over the course of your experiment.

So here there are five, a total of five notes that are played.

Thanks, Daniel, for going to figure 7.2.

I guess to get a little bit more in depth here, upper left of that figure shows the models kind of

posterior beliefs about states and its confidence and so like these lines start to stretch outward at first it's getting the right notes for time step one and time step two suddenly at time step three we get an incorrect note or at least an unexpected note there's a difference between the observations which are in the lower right corner um a certain note is played twice in a row incorrectly

versus what should have happened or what we expected, which is the top right graph, unless you can see that lightly gray square.

That is, the model correctly inferred the right note, the one that's in black, but it was thrown off by that incorrect note occurring.

That's the gray there.

So these are all basically outputs of the model as the experiment is run.

At the time of this experiment, I believe they were using MATLAB and SPM as their kind of packages and software used for deriving this experiment.

So there's a lot of nice functions for producing graphs and outputs of the experiments so you can actually see what's going on whenever you run them.

Moving on to section 7.3, now we have

what is arguably a really key sort of model to dig into.

This is where decision-making and planning are introduced into an experiment.

And so now our HMM becomes a POMDP.

This can be seen, let's see, figure 7.3 is a great one to, it's kind of a,

exemplar image of a POMDP, where we have many elements as we have before.

We have our D vector, the initial states, starting all the way at the left.

From there, we move on.

We have our states.

We have our outcomes, S and O. We have our A likelihood matrices, which map the two.

We have our B matrices going from state to state.

But now what we've done is we've added policies, which are sequences of actions that the agent holds, like it could do this series of actions, it could do this series of actions and so on.

Those are all kind of scored, so to speak, by what's another quantity that's been introduced here, expected free energy, which differs from variational free energy that we've seen before, F, in that

G now takes into account preferences, which are also C, not imaged here.

But you can see on the right, they are denoted in that list of equations.

And those impact how G is calculated.

That is, you kind of are trying to score, should I do this to get what I want?

Should I do this to get what I want?

Should I do this to realize my expected or preferred outcomes?

Or should I do this instead?

That is impacting how the agent from there computes its transitions.

So what we've done is introduced action.

into the model where action now impacts the environment and it's expected to impact the hidden states of the world that our agent is inferring.

So it's just kind of like the prior example, but now we've just added this whole new dimension of action where there's an actual bidirectional relationship between the agent and its environment.

um we are also introduced to a couple of concepts whenever the authors break down expected free energy so we have like negative epistemic value and we also have pragmatic value this relates to the famous exploration exploitation uh debate or dilemma

There's a broad statement in this textbook that active inference naturally balances out exploration and exploitation.

That's its own kind of conversation to have, and some people debate that depending on how you want to define what balance means.

The key point here is that exploration and exploitation or epistemic value and pragmatic value, changing your mind or changing the world,

um are included within the same cost function if you want to look at this from a machine learning perspective for example a more statistical perspective or calculus what we're doing is that we're minimizing this quantity that is expected free energy and you're

minimize, you're minimizing it by engaging in both kinds of behavior, both of them contribute to minimizing the same quantity.

And I believe that is what is meant whenever they state that active inference naturally balances those two kinds of actions that an agent can take.

We're also introduced to other things that for the sake of time, I won't go too deeply into, but we are introduced to factorization here.

So there are different kinds of states that

that the agent can try to infer.

There are also different kinds of outcomes or sensory modalities that the agent can try to infer.

For example, yes, that figure 7.4, the A1 matrix is just mapping

the exteroceptive modality or set of outcomes, which is basically where the rat is in the maze.

But then the second matrix, A superscript two, is denoting the rat's beliefs about the stimulus.

If there's no stimulus, an aversive stimulus or an attractive stimulus.

and so the word tensor is used in this chapter and in previous meetings there's been some debate over if this is actually a tensor i believe the idea is that we're ultimately looking at outcomes locations and contexts right so it's this kind of

Jacob Plocher , outcomes by locations by context, so it does kind of in a way, if you attempt to put it together would be more of a tensor object, as opposed to a matrix it's a it's a collection of matrices here.

Jacob Plocher , But hopefully that helps for anyone who's been wondering.

Jacob Plocher , What that term how that terms being used.

Jacob Plocher , Similarly, we have multiple be matrices because we also have.

multiple contexts related to location.

Skipping ahead a little bit, or at least skimming the rest of this section, we have our C vectors, which are composed of preferences about location outcomes, which is C1.

The rat disprefers staying in the same spot whenever it starts out.

uh so it's set to a negative one right um you can you might expect that if you set that to zero and kept them all zero there's a chance the rat might just stay there for the entirety of the experiment probabilistically um negative one kind of gets it moving um c2 meanwhile has to do with the stimulus

0, it's neutral towards not achieving any kind of stimulus.

6, because it really wants an attractive stimulus.

Negative 6, it doesn't want the aversive stimulus.

It wants the cheese, it doesn't want

a tiny shock or something along those lines.

And finally, just as far as the mathematics go, those are all passed through a softmax function that allows you to both include positive and negative values whenever you're initially defining the vectors.

And then also it normalizes everything such that we're looking at more like probabilistic probability distributions.

So actually after the math there,

For example, the attractive stimulus is actually 400 times more probable, or we might say more desired than the neutral stimulus.

And then again, we have our D vector beliefs about, and again, it's factorized, there are multiple modalities.

So the rat believes that it's starting at the starting location with full probability.

That's how we set it.

Meanwhile, for D2, that has to do with where is the stimulus that it wants?

It doesn't know.

So we have a purely uniform D vector there, half and half.

So that experiment also includes things like a big part of it is there's a cue in the lower position of the maze.

And so this is a great experiment to look at as well, because now we get to see both epistemic and pragmatic value.

Epistemic, it doesn't know where the reward is.

The cue gives it

epistemic value.

It gives it information about where the reward is, such as in the image there.

It's the time step one, rat starts at beginning location.

Time step two, rat moves to the queue at the bottom of the maze.

The queue tells it that it's at the left position.

It's the location of the reward.

Finally, time step three,

rat smart rat follows that cue moves to the attractive stimulus at the left position um which is pragmatic value realizing its preference that it does want the cheese so so we we end up with kind of this you know two-step process where it derives epistemic value from the cue pragmatic value from actually achieving the attractive stimulus um

So those are some key ideas in active inference in general.

And yeah, again, kind of an exemplar model to wrap your head around, especially for those who are still kind of trying to understand the material here because there's a significant amount.

But this is a great starting point for really trying to get into to model.

Section 7.4, the model of information seeking.

This is a kind of a CICADE experiment where an agent looks at four different squares.

This does start to break down things like it breaks down epistemic value further.

Adrian Holovaty, Rather lengthy explanations of these terms but, but they can be very important and they're very helpful for understanding the behavior so.

Adrian Holovaty, Those squares there, all the way to the left, this is where all of the squares are given equal epistemic value right there the agent will look at any of them with roughly the same frequency in the middle one, however.

In that situation, the top left square is by the person who set up this experiment and its distribution.

They gave the top left square more expected ambiguity, which is one of the terms that's derived from epistemic value.

Beliefs about that square are less precise.

They're more uniform in the A matrix for this square, the likelihoods.

You can equate this with the street light effect, the idea that if someone is out at night, they drop their keys, they don't know where they are, they will probably look where there's more light first.

The idea is, it's expected that you will actually be able to minimize your uncertainty or reduce your uncertainty by looking at the other three squares here.

uh whereas the top left one just imagine it's as if that square is is is blacked out or or not well lit or dim or something right there's it's going to be hard to derive any kind of epistemic value from it because it's not things are not very visible

so that's an attempt at getting at how to understand expected ambiguity then finally top right or excuse me at the right most image here all the squares are still visited but the b transitions are made to be less predictable

And so it's the lower left, yeah, the lower left square is given a higher posterior predictive entropy.

So the agent is less sure about what will be seen if the cicada is chosen for the square, if they keep looking there.

It's as if they are unsure if there will be new information to get out of that square.

So they keep returning to it again and again.

Whereas for the other squares, those are more equivalent in the idea that, oh, you look at them, they don't really change.

Why look back again?

So it visits all of them, but it keeps checking out the lower left one.

We do have box 7.1.

which introduces us to, it's been mentioned before in the textbook, but this is just a quick rundown of trying to describe uncertainty and precision or priors over precision.

So that's a term that basically it starts, it's like gain control from the neural process theory that we've looked at previously, especially in chapter five.

This is kind of like amplifying, not adding to, but rather amplifying certain neural signals.

Precision is inverse variance.

So something has very high variance,

It means it has low precision, kind of hard to trust something that has very high variance or very low precision.

Meanwhile, high precision means, oh, that seems more trustworthy whenever carrying out your computation.

So in a sense, it's like weighting differently all of these different objects to different matrices or particular beliefs or otherwise within the model.

Section 7.5, learning and novelty.

This is where we're introduced to beliefs about parameters.

So this shows us, we're showing this on page 141, figure 7.10 or 7.10.

It's basically the same model as before, except now we can see all these additional nodes.

where all of our different objects are parameterized.

So our likelihood matrices, the A matrices, those are all parameterized by tiny a or small a. Same thing for the B matrices, parameterized by a small b, c, small c, and so on.

Those mesh really well with looking at things like precision, because we're looking at things like weighting.

So these are all parameters that can be learned as well over the course of a series of experiments.

And typically in active inference models, this learning occurs at a slower time scale.

which is also kind of seen in the final example of this particular chapter that has to do with hierarchical inference.

But basically, these parameters can be learned as well, like how you can learn over time how much to trust certain likelihood mappings versus others or trusting your beliefs about transitions versus others or which policies to take versus others.

things like that.

Brief mentions about other things.

I'll leave them off here, but it has to do with choosing conjugate priors whenever developing.

um variables that are linked together concentration parameters the importance of Dirichlet distributions for categorical variables where observations and states are kind of counted over the course of an experiment the more you see this the more you expect it to happen it also helps us to remember that like

a model like how quickly it will update so you know you flip a coin five times you see it's always heads uh you might quickly come to a conclusion that uh it's an unfair coin meanwhile if you flipped it a hundred times with a 50 50 of heads or tails and then you just happen on the next five tosses to get all heads you're much less likely to think oh well this is an unfair coin because clearly it was pretty fair the first hundred times around right

By accumulating more counts, that informs us about how much the model or the agent might update its beliefs after the next new observation that comes in.

We have a mention of structure learning, which is an entire topic on its own and there's a lot of work still being

done on structure learning, but it's rather interesting.

I'll just briefly mention this is seen as something like offline learning.

There's a metaphor used for it, like that it occurs during sleep.

That is the model where agent is not taking in any new data, and yet it's actually updating its beliefs.

But more specifically, what it's doing is that it's scoring

the model it currently has of the world, let's say, and during this sleep or resting offline state, it can actually compare that model that it has now with, let's say, a simpler model that maybe has removed some of the

certain variables or readjust it and minimize some of its matrices or otherwise to see if it can produce an equivalently, if not better performing model

um that nonetheless has some of its complexity reduced and suddenly it's computationally more efficient right so so basically in a sleeper resting state we have bayesian model reduction um is is one way that can occur where the model is kind of pruned and and improved even though there's no additional data coming in at the moment

And then finally, section 7.6, hierarchical or deep inference.

This again is very similar to the POMDP that we saw previously, except now we have an entire additional level added above.

This is figure 7.12 on page 148.

And so I would say briefly that the observation nodes in the middle between level one and level two, that's a bit of a misnomer.

What's going on is that the state inference occurring

at the lower level is being fed up into level two as if they were observations at level two.

I note this because in a lot of other papers that I've personally read, usually they would not include those extra observation nodes in the middle.

They would just go straight level one up to level two.

But this is a nice way of illustrating like it's as if those are observations for level two.

So what's going on, what we have at level one, multiple time steps, like three time steps.

first for every single time step at level two.

So this demonstrates that there are multiple time scales going on here.

Level one is happening at a faster time scale.

There are three time steps for every individual time step at level two.

The state predictions occurring at level two are being fed into the computations at level one.

And so it's like we have these,

descending predictions coming from this higher level.

And then meanwhile, level ones, state inferences are acting as ascending

observations, if that's useful.

Maybe I'll drop an article link into the chat, but I think that there's a rather nice paper that I've spent time reading and I find personal interest in it.

But it's a paper called Deeply Felt Affect.

which came out a couple of years ago.

And it relates to, I would say, fields like computational psychology or psychiatry.

But why I think it's useful for anyone who wants some further reading here is that it, one, is a recreation of the T-Maze experiment that we saw earlier.

uh and on the other hand it adds a hierarchical layer um that is theorized in the paper to relate to emotion and so we have a rat that is given this kind of emotional or affective layer to its model and it actually dramatically changes its um its behavior whenever navigating

the maze, it becomes much more confident depending on its emotion in its own model of how to achieve the reward, sometimes to the degree that it will skip the epistemic value from the queue.

It'll go straight to the reward.

Why?

Because now it gets an extra time step that it gets to spend with the reward.

So it's it's again, it's just we have the teammates, we have a hierarchical model.

And then finally, we get to see how like behavior changes whenever we add this hierarchical layer.

And whenever it comes to cognitive modeling, that's a big aspect, right?

It'll be to kind of fiddle around with what variables do you need in your model?

What are the layers to be included?

And how do you kind of tune or prune things from there?

I feel like I've spoken more than enough.

So I will now open the floor if anyone has any thoughts or questions or just general things they're considering.


SPEAKER_00:
Sorry, what is that?

It's Prakash.

Actually, I have a very basic question.

Since we have been talking on matrices, is the d vector just like a one-dimensional vector?

That's it?


SPEAKER_01:
Yes.

So I will say in the so we saw the teammates experiment, for example, and in that case, we did have two vectors, one for each modality.

So the first one was, yeah, the rats belief about where it starts.

So the location.

uh factor or modality and then d2 its beliefs about where the stimulus is is it to the left is it to the right here it's uh you know hard coded is uniform uh as a starting point but yes uh they are each uh 1d vectors um and that's all you need to kind of kick off or start the the simulation whenever you're running it yes so that is mapped to the number of factors of the b matrix right


SPEAKER_03:
To the edge.

The edge length of B. Okay.

Yeah.

Because B maps from hidden state to hidden state.

So whatever the number of hidden states is, that B is just that per slice.

And then the number of slices of B is equivalent to how many affordances there are.

And then D is just the edge length that's carried through with S.

And then A is O times S because it maps O and S. Pi is also a list with the probabilities of the actions in the policy space summing to one.


SPEAKER_00:
Thank you.

I have one question, sorry.

Actually, when you normally try to initialize these vectors, say d, can you just also hard code it, just one, and then just pass it, right?


SPEAKER_01:
Yeah.

Yeah, that's right.

You would hard code it.

That's one of the kind of theoretical discussions around active inference, right, is it's more about the what and the why and the how, but not so much about how it starts.

That said, what you can also do as far as learning and inference goes is you could have, you could set up an experiment that runs over multiple trials.

In that case, you would still hard code D, but

going from trial to trial, you could have it such that the agent actually updates D over time.

So that trial one, it begins with those hard-coded values.

Trial two, it might update now that it's the start of trial two, what are its prior or initial beliefs about the state.


SPEAKER_03:
yeah so like in this case let's just say it was going to do three rounds of the teammates and then initially it has a flat uh you know it has a certain it believes it starts in a certain setting

That would be the D1 location, or maybe that one stays the same, the 1, 0, 0, 0.

And then it learns by the end that it always starts on the right or updates it towards the fact that the reward or the other, the stimulant food is on the right side.

It could learn that by just counting up, okay, it was on the right, it was on the right, it was on the right.

And then that kind of like putting pebbles of different color into the urn, that's the learning by counting.

which is reflected here with a little d over the learning by counting of this one.

So this one starts out 0, 0.

It starts out 1, 1.

And then you add 1 every time you observed it.

And then that ends up, that's a hyperparameter on the d distribution.

So hence the kind of strategy of the book of showing like the minimal figure 4.3 type model.

And then here's like one motif, which is hyper priors for learning or attention, like Andrew mentioned.

Here's another motif, nested models, but the hyper priors are not shown.

You can start to combine these, but then you can also imagine how the model's complexity grows a lot.


SPEAKER_00:
Sorry, what do you mean by motif?


SPEAKER_03:
like here the motif that it's showing is just nesting it's not including little d so it's not like this is not a generalization of the previous one this is just like a generalization of the of the figure 4.3 one as this one is but this one's not nested

So like here, but the difference that it's trying to show is just about the idea of putting the italic lowercase above each of letters.

A, B, C, D, E. And then these motifs get kind of combined.


SPEAKER_00:
Okay.


UNKNOWN:
Okay.


SPEAKER_02:
You have a question?

Yes.

Slightly tangential.

So Andrew, in the ReactFX project that you're doing, are you building these types of models?

I know it's a very simple question.


SPEAKER_01:
um well we've just kicked off the the project a couple weeks ago so we're still kind of trying to decide what kinds of models to build but anyone who's interested can can certainly you know hop on and and see what's going on we do have a distinct coda for that and so right now people are running project ideas and so they're like in a table that you can

Just put in like, oh, I would like to try doing something like this, and then maybe put a couple notes on what direction you think it might take.

And as Daniel's pulled up here, for me, I wanted to see if we could do a kind of reproducibility project where we take some of this few years old code in MATLAB or SPM,

Can we run it through Julia and Rx and Fur instead?

Just basically to see if we can help with reproducibility.

And from there, we could even potentially start moving into modifying those models or maybe improving them.

But yeah, generally, it seems like POMDPs might be one of the primary ways that we go.

So hopefully that answers your question.


SPEAKER_02:
Brett KenCairn, And is that all I see Daniel posted the links in the chat and i'll take a look, but is that all happening at this point asynchronously through coda or are there.

Brett KenCairn, Thursday meetings okay.

Brett KenCairn, that's what you got here on the screen.


SPEAKER_03:
and we're developing out the github repo and so on so there'll be but there's there's more than ample async and synchronous opportunities if people okay and and it's just like i mean there's no chapter six for rx infer but these are some of the um um

I mean, we can go more into this another time, but we've, like, talked about how we can best contribute and write documentation.

So for people.


SPEAKER_02:
Right.

So that's more.

I mean, it's still.

Sorry.

Sorry.

But it's it's still the underlying strategy.


SPEAKER_03:
Yes, we're learning this package regenerative model.

It's also over the course of the year going to increase a lot in its capacity and usability.


SPEAKER_02:
Okay, cool.

All right.

I'll pull up that code.

Thanks a lot.


SPEAKER_01:
And just to note, it'll also be incorporating, you know, some research that's been done since the publication of the textbook we're looking at.

So, you know, there'll be a little bit more up to date, so to speak.

The textbook is still a cornerstone of active inference, but that said, there have been some, you know, certain kinds of updates that have been made since then.


SPEAKER_02:
Yeah, I would just love to see this, those modeling applied with, you know, actual values, if you will, to help me wrap my mind around it.


SPEAKER_01:
cool yeah absolutely um i did notice uh susan posted a question in the chat how would reflection versus indecision be represented um

I find that to be a rather interesting question myself.

And while that's not directly answered by the textbook, surely there are papers that have been published on these kinds of topics.

But in any case, intuitively, I would say just to kind of stay with the chapter itself.

or excuse me uh page 145 146 um this is uh an agent navigating a maze and in this case um

the agent does not have any specified like particular preferences or rather it's like purely uniform preferences.

And so it's as if the agent doesn't prefer anything in particular.

Whenever the model is set up in that way,

then the agent engages in what could be viewed as an emergent behavior called novelty seeking, where it just keeps going to places that it hasn't been to yet.

There's no preference for a certain reward or stimulus to where it knows that it wants to go to a space that has a reward.

It doesn't go there, it just goes everywhere.

That said, it's still following the FEP.

It's still minimizing expected free energy or variational free energy.

It will avoid spaces it's already been to because it's already been there.

Nothing new to get, no new epistemic value to be derived from a space you've already been to.

and so it will just run the full gamut of all the spaces it's never been to.

This is a phenomenon that's also been referred to by Posner in 1985 as the inhibition of return.

So that's a pretty interesting thing to see.

And it makes a lot of sense because when we look at our equations, C, the C vector or preferences are heavily informing G. And so there's nothing informative about C because it's purely uniform.

Nothing's particularly preferred.

then it doesn't really help with figuring out what to do other than just keep running around getting all that novelty you can.

Maybe it's thrill-seeking.

It's a joke.

But reflection, meanwhile, not so sure.

Intuitively, might have a relationship with structure learning.

An agent doesn't necessarily have to be sleeping in order to kind of tune or prune its model.

But the idea is that, yeah, there is no incoming sensory data.

So reflection might relate to that process.

You know, someone who's sitting there reflecting, kind of thinking three things through, even though they've not been presented with anything new.

Might be a form of structural learning.

There might be a paper out there or a dozen on these kinds of things worth searching for that.

But yeah, very interesting question.

Yes, Anna or Magdalena?


SPEAKER_03:
I see you put your- You're muted Magdalena.


SPEAKER_04:
Can you hear me okay now?


SPEAKER_01:
Yes.


SPEAKER_04:
Okay, okay, sorry about that.

Yeah, I was wondering if you could elaborate a little bit on

um the offline updating of of beliefs um and i'm curious about the scientific studies that have been done demonstrating that this in fact happens that you can you're you know during sleep or maybe meditation or daydreaming right

Can you elaborate a little bit on how active inference scientists have looked at this aspect of active inference?


SPEAKER_01:
So I'll just be direct in saying that, yeah, I'm not personally too sure of the

research that's been done very specifically on structure learning in relation to actual

sleep or meditation?

I mean, I'm seeing this from a quick... I'll show one.


SPEAKER_03:
This is not exactly Act-Inf, but it's right there.

I'll put it in the notes.

There's many similar architectures done in machine learning where there's this phasic approach

extending and updating during the day with a fast inference and using different techniques for sleep.

And I mean, it probably gets into the specifics of the machine learning architecture.

And then also there's the whole other angle you said, like empirically, what's happening at the synaptic level and like the cognitive level for humans and

Ricks, skill consolidation and sleeping learning consolidation naps there's there's probably a lot of different phenomena like that.


SPEAKER_05:
Thank you.


SPEAKER_01:
drop this here but this looks to be a manuscript by a couple of researchers as well who make a mention of particular meditative traditions um but they claim there that as opposed to it being um structure learning per se they they claim that meditation is more of a form of like it's a it's a particular it's a peculiar policy itself um that is your choosing to do that um that actually looks to be really interesting

paper, and I appreciate that there's a more kind of culturally accurate as opposed to highly abstract kind of thing to it.

So, yeah, Susan.


SPEAKER_05:
So, yeah, this is fascinating.

Thank you, Andrew.

So what what keeps coming up is, is is the reference to learning is takes time or

or there's no fast learning.

And so I'm wondering if maybe the concept of learning itself is, you know,

is more specific than, I guess, what we generally think about learning.

So how would you categorize learning, or maybe why it keeps referencing that learning is slower than what?


SPEAKER_03:
Yes.

This is a great question on the first thought.

especially as as more complex cognitive models are built there's several ways that different time scales of adaptation happen so the fastest layer is called inference and that's kind of like the rubber hits the road like each um new observation coming in but also the whole process is called inference um but also the fastest layer is called inference then

um the next slowest layer is attention so like attention modulates um on over multiple sensory cycles like just thinking about the visual case or the the dinner party like overhearing multiple conversations like attention modulates faster than um slower than the words coming in but um faster than updating like

deeper factual beliefs and then then learning um as distinguished from those other kinds of parameter changings is like here being modeled explicitly with these hyper priors and updating the distribution that the kind of rubber hit the road parameters are being drawn from that's not the only way to model learning so

Matthew Shollin- Hence chapter six and really digging in with like what time scale analysis is being studied, because maybe whether it's learning over seconds like.

Matthew Shollin- Repeating back a phone number or whether it's like a semester level, maybe that's the time scale of your model so then not learning is happening on the kind of primary time scale, the model.

Matthew Shollin- it's just way too general to say, without specific system of interest.

But That being said, there are multiple ways that like to kind of the hierarchical models like looked at so multiple non exhaustive or exclusive ways to model the multi scale temporal dynamics of change.


SPEAKER_01:
Yeah, I would definitely second Daniel's point about yeah, it's it there is no single answer to the to the question.

Again, we have this final example in this particular chapter referring to words and sentences.

Right.

So those timescales don't seem dramatically different, actually.

Right.

Like we're looking at a matter of, let's say, milliseconds versus seconds.

um like predicting what the next few words are going to be an entire sentence to try and capture something like what the sentence is about versus the the faster time scale of word word word like word to word um in the in the sentence but i did want to mention a couple other uh as far as like um trying to map this to uh concepts or behaviors in neuroscience um another way of thinking not necessarily thinking about it this way but one

Potential analogy could be something like working memory versus long-term memory.

Here's your quick access working memory of the moment by moment.

Long-term memory occurs over time.

It gets consolidated slowly over time and it can lead to long-term changes in things like

uh your daily habits um what comes more naturally or quicker to you so it just describing it a bit more intuitively that way yeah i like to sometimes think of it as like yeah short-term versus long-term memory in addition to that as we saw in chapter five in terms of the neurobiology um it's commonly been found or theorized from from various studies when working with neuroimaging data

that whenever a participant in these studies is learning something, what happens is that you tend to find when looking at brain rhythms from, let's say EEG, that more theta and gamma waves are present simultaneously.

They view it as what would be called a theta-gamma coupling.

We do get a reference to that in Chapter 5 in this textbook.

um theta rhythms are slower uh in a in a particular kind of rhythm band or frequency band uh gamma are faster and so what happens is these rhythms kind of um you know it's kind of fun you can think of it as like music you know you have some fast drums and a slow piano or you know some such thing but there's a kind of sinking between these two uh differing rhythms and it it is seen as refl

being a kind of neural correlate to someone learning something.

So multiple timescales in a POMDP, multiple timescales in How Our Memory Works, multiple timescales in neuroimaging data.

Just a couple more kind of metaphors as food for thought.


SPEAKER_05:
Marguerite McLaughlin, To one more question that.

Marguerite McLaughlin, You know, I think i'm wrong, but you know this makes me want to wonder if.

Marguerite McLaughlin, If the learning in this sense isn't.

Marguerite McLaughlin, Is it an effect of.

Marguerite McLaughlin, i'm.

Marguerite McLaughlin, Actually, acting on a policy.

Marguerite McLaughlin, But that.

may or may not be correct.

But the other thing that comes to mind would be would surprise occur at either level.

In other words, if they don't learn or if their policy doesn't get the expected payoff, then there would be a surprise.

But I'll say learning would

could actually encompass a surprise as well.

So it makes me want to say that there's almost a surprise tolerance.


SPEAKER_03:
Yeah, like thinking back to the chapter two, the Bayesian surprise is the delta of learning.

And so if the hyper priors are fixed,

The Bayesian surprise is going to be zero, no matter how surprising in the first sense, the observation is like, we think that the room is this temperature and we're not open to learning we've fixed hyper prior.

And then even shocking observations just continue to shock us versus if you had, um, like a kind of adaptive learning rate, then there would be some.

like a surprising observation would come in that would catch your prior wherever it initially was and then that would also move your prior some amount over to the new observation so if that kept happening eventually like you would converge so that that was no longer surprising and then how fast you converge is attention


SPEAKER_05:
slash the learning capacity over different time scales so learning capacity would would come into the level of tolerance i guess would coincide yeah thank you and uh forgive me um to clarify so learning capacity is represented through those hyper priors


SPEAKER_03:
yeah um indirectly yes like like when andrew mentioned the coin flip it's like when um and this is just um you can see this in any kind of uh bayesian statistics overview like if you only had flipped the coin twice and you got one and one then you'd have a wide prior centered around 50 because your maximum likelihood estimate for the fairness of the coin is 50 50 but it's very dispersed

can't be zero can't be 100 because you know that you've seen at least the other things but but it's broadly like it's it's a very over dispersed and then if it was one million and one million then the the the two millionth and one flip would be very tight would only move the distribution a tiny amount so that's like that learning by counting and then as you count up more and then so then then you have this whole question okay well then but then that just slows down our learning rate

but maybe it's a cell differentiating and it does slow down its learning rate to a halt.

Or maybe there's another process that takes a look at that learning by counting variable and says, when this is 100, we cut it down to 20.

And that's this whole challenge with the iterated modeling is figuring out all these like second order rules and diagnostics to be like, okay, should we cut it down to 20 or like 30?

and then those are all these like simulations and sweeps that you do then investigate how the agent is behaving in different regularities got you thank you that's a good explanation that's a lot of compute it is so even that's one very interesting thing which is like even when like the model as parameterized might be low compute to run

the open question is how much did it take to get there?

Not dissimilar to like a language model where the forward running of a trained model, it's not mega computationally expensive, but then training it can be expensive.

And similarly, while updating the model might not be expensive.

So for a toy model where just whatever you make, you let it run and it's all good, but to find like adaptive and resilient ranges of parameters with many, many possibilities

that can be running these models many times cool well we return for chapter seven part two next time or feel free to join chapter two it's kind of funny chapter two and chapter seven they are very similar

And in Chapter 2, we kind of referenced Chapter 7 because that's where the discrete time models are.

Then Chapter 7 took us back to Chapter 2 with the surprise and everything.

So.

Oh, thank you, Cheryl.

I'm going to copy this in.


SPEAKER_04:
You're welcome.


SPEAKER_03:
Thank you.

Okay.

Farewell.

Bye.


SPEAKER_04:
Bye-bye.


SPEAKER_03:
Thanks, everyone.
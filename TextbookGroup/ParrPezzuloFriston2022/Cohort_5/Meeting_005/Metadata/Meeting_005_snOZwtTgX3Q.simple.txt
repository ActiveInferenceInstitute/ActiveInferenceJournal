SPEAKER_02:
All right.

Welcome back, everyone.

It is October 17.

And we're in our second discussion on chapter two.

So let's head over there.

And first, does anybody interesting little colors were added?

Does anyone want to

give any opening thoughts or reflections on chapter two, or just bring up some question.


SPEAKER_05:
So I can share a briefly experience too, in comparison to the previous sections.

It took me longer for sure.

And I had to do more, search for more reference material because I think it's the first section that's really trying to introduce the mathematics of the framework.

And the first part where we're discussing variational free energy,

which is only taking into account information about the past and the present to compute free energy.

This was relatively straightforward.

I think the section on expected free energy is a more significant leap, and I think I need to spend more time

uh, analyzing that and, and really making sense of, of that part of, of the chapter.


SPEAKER_02:
Awesome.

Thank you.

Anyone else?

What, again, just any part of two that stuck with them or a question they have or, uh, a thought on the role of the low road.


SPEAKER_04:
I will just echo the sentiment that I think I finally, you know, got the handle on free energy as a concept, but the expected free energy is still completely impenetrable, making that cognitive leap is still a little bit beyond my reach.


SPEAKER_02:
cool we can definitely look over it okay at the at the uh at the very small font overview we have perception as inference so it's still passive ish even though we can discuss there are some ways that action is related to even what might be seen as passive inference background on Bayesian statistics

example on the background on bayesian statistics which we can look at again but there's an object in a person's hand and they're updating their beliefs about what they think that object is based upon what they observe it might be an apple it might be a frog and then they see it jump and they update their belief from thinking that it's probably an apple to probably a frog um that case is carried through in the exact bayesian setting

And for smaller state spaces and smaller scale computational problems, you can utilize exact Bayes.

However, for larger state spaces, you can't always do exact Bayes.

And so that requires an approximation method and variational free energy or the evidence lower bound is a common approximation method.

doing variational bayesian inference um doing approximate bayesian inference is then mobilized in this particular partition cybernetic setting which leads to a discussion of action still we're in the

cybernetic setting, continuing to discuss perception and action as part of this unified approximation challenge.

Then we get to the real equations.

Equation 2.5 and 2.6 are like

the two key equations in this chapter 2.5 is the variational free energy so here is where we have um the ability to build on people's work previously and also make massive massive um contributions with our studying here in 2.5

previous cohorts have used the ontology to provide natural language descriptions of the equation so if this just looks like a lot of symbology then it may be helpful to read the ontology description of it 2.5 variational free energy is about beliefs and data

So variational free energy is like the real-time sense-making vibe check.

It's very closely related to how surprising incoming observations are given beliefs.

Some more discussion on how the KL diversions comes into play.

And then it goes from

consideration of the real-time variational free energy into the future oriented expected free energy and planning as inference with equation 2.6 but that's not something that all cognitive systems necessarily do not all systems engage in explicit planning and then there's some discussions about special cases of expected free energy and that's the end of the low road

So chapter two pretty much starts with a simple exact base setting, slides over to the approximate variational case, applies it to cybernetics, and then introduces the two key equations in 2.5 and 2.6, variational and expected free energy.

Just to kind of give an overview, but like, so as you have highlighted, like those are the exact things to pick up in the low road.

So, anyone else wanna just bring up any thought or question or what area can we go to?


SPEAKER_05:
Yeah, I think maybe before we move on to the more technical details, I think it's important to highlight the significance

of this jump, like it's a conceptual leap that was very striking to me.

And I think I have to spend some time studying this kind of area.

It's it becomes kind of intuition.

But but this idea of perception as inference in contrast to what at least throughout my life, the way that I've thought about

seeing things has been this straight transaction from from objects in the world to just lights on my retina and then some processing through my brain and then just kind of this image being projected somewhere.

But the predictive aspect, the inferential top-down

component of it is is i think is a remarkable idea and it's interesting that um this isn't it's not really something that you can tell at least i i have couldn't couldn't tell just by observing how i look at the world at least not not earlier in my life so so i'm

i just wanted to highlight that as a as an interesting uh but significant like a significant leap i think in in thinking about perception yes sarah and then anyone else hello i have a disclosure a comment and a question so my disclosure is i read the wrong chapter


SPEAKER_00:
I missed two weeks, so I thought I'd missed two chapters, but obviously I didn't.

I did read this chapter, but I read it like six months ago, so I'm just flicking through very fast now.

So that's my disclosure.

My comment is, yeah, I totally get the idea.

It's such a it's such like a flip to go from this idea of light coming in your eyes.

I remember in primary school being told off for doing it the wrong way, feeling like light goes out and she's like, no, it goes in.

And like we we kind of grow up with this idea like we're sensory machines, but then

i feel like once you make the flip it's all predictive it's really actually hard to go back and like not see it anymore and then my question was so is it i guess it does relate to this chapter which is lucky because you know that's what we're actually on so like to figure 2.5 right so

you've got this idea of the perception and the action and one thing I've been struggling with is the how they're different right because perception changing your perception is actually an internal action so it's you're updating a model well what's that that's changing a synaptic way that's an action but it's just being performed

internally so one thing I've didn't realize I was struggling with this till today is uh yeah how why is perception separate to action and I know like the idea is it's a unified theory of perception action but why do we still have those and how are they different am I missing something so thank you yes yeah


SPEAKER_02:
Okay, how are perception and action similar and different and related?

Okay, and Christoph?


SPEAKER_04:
So I just want to say kind of a couple of things.

I like this idea of this, what Sarah just said about light going out, rather than, it reminded me of a quote from Kierkegaard, that life has to be lived forwards, but can only be understood backwards.

And there's something about the way we explain the world,

you know, to ourselves or others that has this kind of like inversion quality to it.

And I kind of I can't put my finger on it precisely, but I see it as a kind of some kind of universal phenomenon that's lurking there.

But about this leap from perception to to inference, I remember one of the earlier papers by Carl Fristen,

he mentioned something really fascinating and he mentions the hubel and weisel um uh experiments with um a cat retina I don't know if you um if all of you are familiar with that but they basically were shiny like um uh onto cats uh retina anesthetized cat and measuring response from some um um some neuron there for you know uh

measuring electric response and they found these uh receptive these ideas receptive fields that responded to various kinds of orientations and and and so on and the interesting phenomenon that they couldn't explain was end stopping so when you have a receptive field you know like it will it will respond to the specific neuron will respond to some specific orientation and specific length of the um of the rectangle that they were projecting but when the rectangle gets wide enough it stops

And the question was why, and neuroscience couldn't quite explain this, but the predictive framework, I remember Carl mentioned that in one of his papers, it starts to suddenly make sense because in nature, you don't really see short lines.

So when you're looking at the horizon, it's always a very, very long line.

But if it becomes shorter, then it's something that does not occur in nature.

And therefore, the neuron fires because it actually is a prediction error.

So it's kind of fascinating that this inversion of this idea suddenly made the phenomenon explainable, which I thought was really, really interesting.


SPEAKER_02:
yes so first this uh like coming in and out of the eye um it is really a different view on perception

when as um was brought up like whether we think about well there's this unrolling generative model that's getting light touches from the sensory information getting fine-tuned that's the sort of like inside out view of experience

as opposed to a sensory processing.

I mean, sensory processing, even as a framework, makes it sound like the information is out there and we're going to like process the wheat, separate the wheat from the chaff, and then we're going to end up with like just the real, you know, we're going to have the pixels on the camera and then we're going to process the pixels and figure out whether it's a cat or a dog.

Or even further, like we're going to process the pixels and then we're going to figure out what to do.

as opposed to we're going to have this unfolding model about how things are and their likelihoods and what to do.

And then we're going to get just light touches from sensory data.

And I guess the trip is that if we believe that sensory processing is outside in,

world will confirm our beliefs i think there's a few truth drops just in the visual experience that help us understand the inside out perception so the first is the blind spot which we don't attend to it's filled in perfectly even though there's a large um patch on the retina that doesn't get that much visual input second one is that in the periphery there are uh fewer

um there's lower resolution vision and there's no color vision however in healthy vision we have color and equal resolution and there's experiments that show like actually well it is lower resolution in the periphery but it's not intended to as such and then we just look over and then the third aspect is the cicade so multiple times per second our eyes are moving

And with active inference, we can understand, okay, it's like when the gaze is fixed, our precision is high and we're paying more attention to visual input.

And then right before we're ready to execute a saccade, of course, this is subconscious, precision is alleviated, the eye moves, and then it's like the foot goes back on the gaps.

So the fact that the blind spot, the periphery, and the saccades are not attended to

is very tangible that just our visual experience is what we generate it to be with, of course, information coming from what's external, but it's not being processed merely from what's external.

Okay, so let's go to Sarah's question here.

How are action and perception similar, different, or related?

What's anything that anyone wants to add on this where there's a few pieces to bring in?


SPEAKER_01:
The part about the planning, is that in between or is that part of action?


SPEAKER_02:
Yeah, great question.

So, Christoph brought up the Hubel and Wiesel experiments with the cat, retina, and the selectivities in different brain regions.

And that actually connects to one of the most cited neuroscience papers, certainly within the Act-Inf space, which is Rao and Ballard, 1999.

which provided some of the early strong evidence for predictive coding as I was like well so in that retina experiment they had classical patterns like classical cell types oh this one responds to vertical lines this one responds to diagonal lines this one's horizontal or so on however there were these receptive fields

like patterns of activation of visual neurons that were extra classical like they were not accounted for within a classical receptive field model and this paper with empirical evidence and and mathematical modeling established that predictive coding which is to say like expectations about visual input


SPEAKER_09:
were able to account for some of these extra classical phenomena um um Andrew oh yeah I just wanted to say just very generally on the the action perception relationship um active inference seems to take the stance that like we are actively inferring information around us and so rather than

being like passive observers who are just sitting there, we engage in actions in order to kind of impact our perceptions, such as like you pick something up and look at it, like you can look at it from different angles.

All of those are like actions that you're taking.

That's my general sense of the relationship between the two, as opposed to some classical view that we're just sitting there, we're taking in,

What are the supposed reality is around us?

We're not actually doing anything.

We're just observers.

Um, that's it.

I don't know if that tracks.


SPEAKER_02:
Yeah.

Thank you.

Um, Jesse, then Manuel.


SPEAKER_08:
Yeah, I think, I mean, isn't the, the two options either you change your mind or you change the world, right.

To minimize the free energy.

So.

um that's kind of how I see it anything that you do that act that changes the world materially falls in the category of action where um changing your mind is perception I guess what I'm struggling with is that your mind is in the world right so you say you change your mind but that's the same thing as changing the world


SPEAKER_00:
Like I'm still struggling with the, the difference because changing your mind, it's just okay.

Even, even if we reduce it to it's changing firing rates, it's changing patterns in, in neurons.

It's not some esoteric mind concept.

It's still something physical we're changing.

there's an action there to change the synchrony between different brain regions or whatever it is that causes that sort of perceptual switch.

But it's not like an esoteric concept.

It's still something physical that's changing is what I'm struggling with.

But I completely get what you're saying.

Like, I do agree with that.

But then the smallest more you get, it's like, well, what is changing your mind?

It's just a changing weights between neurons.

It's changing synchrony.

It's changing something physical.

Unless, you know, maybe this will be the answer and we'll find something metaphysical finally in the brain somehow.


SPEAKER_02:
Awesome.


SPEAKER_06:
Manuel and then Susan.

Yeah, just to give my 2 cents on this point.

I think he's really interesting.

What sorry saying, but.

You know, in the formalism where the free energy principle comes from, my understanding is, you know, you have to find.

A dynamical system, which has many variables.

and for this thing to apply there has to be a concept of a mark of blanket which will come later in the book but it's like you have a separation of variables that do not have direct contact with other variables within the system only through this this layer of interaction and perception and action are different there in the directionality of the influences so perception

only influences the internal state of the subsystem you care about, but doesn't do the opposite, or sorry, the internal system doesn't influence the perception states, and the opposite with action.

And when you say, yeah, like the brain also is taking an action in a physical world, and that's true, you know, Friston seems to mention pretty often how these Markov blankets, I think of them as, you know, the matryoshka, the Russian doll, many layers of these

Marcos blankets on top of each other.

So a cell it's doing its own active inference, but it's so it's had its own mark of blanket, maybe defined by its own cellular membrane and whatnot.

But it's part of a circuit, and that circuit is also has now more functional mark of blanket, and they take actions towards other systems and many, many scales of this kind of thing.

But the

I think the more precise differences these of of what variables are changing your system versus what variables you change of what is not the system.

All defined by what the mark of blanket is always in this in the sense that there has to be something that is that you care about and some and the rest that you don't care about directly.


SPEAKER_02:
Yes, very interesting and important point, which is a simple or a narrow technical difference between action and perception is for a given system of interest that we're just going to focus on, the inbound dependencies are perceptual, the outbound dependencies are action.

And then with a different blanket,

or a different system of interest, a given causal influence might be understood as, well, it was an action for the speaker, but then it was a perception for the listener.

So then by swapping a perspective, we might give a different act infantology tag to the same causal edge on a base graph.

Ra, then Andrew.


SPEAKER_05:
Yeah, I was thinking a bit about this after I got to figure 2.2 in the textbook because they're illustrating this interface or this influence that an agent has on the world to

uh generates some kind of uh observations why here and in my mind the difference between perception is that inaction is at least based on the formalism that we're given here is in the access that we have to the hidden states so

Out in the world, there's some process, some generative process, as they refer to it.

And there's no way to observe the hidden states of that generative process.

And the hidden states are the causes of whatever observations you have.

So, for example, if you see an apple, there's something out there in the world that is generating apple-like observations on your retina.

And then inside of your mind you have some also you have a general model of what that process is in out in the world.

But you actually have some maybe some influence, some more direct influence over that over the configuration of the generative model.

And so even if you take some actions internally to update that model, it's different than like some proxy action that you take to change your observations out in the world.

um that's as far as the distinction has been clear enough to me but i think this idea of a marker of blanket which would probably explore more um in the high road to active inference uh i think that's also a deep important idea for

establishing what it means to be a system what it means to be a thing that that is separate in some significant way in some statistical way perhaps from from the external world thank you great point


SPEAKER_02:
even if we chose to take a kind of active modeling approach to internal action like through attention or whether we even conceptualized of learning as action like just the actual parameter updating of learning not just like moving around the world only through the agent's boundary would the object be intervened upon in the world it'd be like you know if people are watching a movie

And they might be doing attentional modulation or something, but they're not intervening with what's on the movie screen versus an outgoing dependency, which is to say real overt action, then does modify how hidden states change through time.

Andrew?

Andrew?


SPEAKER_09:
Adrian Holovaty, Sorry, you know I think everyone just before me largely summed up what I wanted to say, I guess i'll just add.

Adrian Holovaty, Another way I kind of think about it is like the inclusion of the the idea of preferences and the generative model carries preferences and in differentiating the system of interest, like the model versus the process whose true states, we don't have direct access to and so on.

Adrian Holovaty, Like.

For me, a human is a generative model.

I have preferences or homeostatic set points or however you want to view those.

I want to keep my hunger low.

I would like to get some good sleep.

I would like to have a well-regulated body temperature.

And while I would say, yes, my body, my mind and so on are in fact part and parcel with the world, I'm in the world.

I don't think that the world around me has the same preferences I do.

I don't think it's attempting to regulate my body and maintain my homeostatic set points.

I have to engage in action as well as like attempting to get rather tractable, accurate perceptions of the world around me.

in order to act and realize my preferences that are all of these homeostatic

um set points um active inference seems to paint the process around the model potentially as just like an entropic dissipative set of forces that the the agent is trying to act against um in some ways and that's not necessarily a big antagonistic picture between us and the world around us but in some ways yeah we do have to kind of um fight for our lives something like that so


SPEAKER_02:
Yeah, great points.

You could specify a generative model where the environment was just at the exact temperature that the agent preferred.

or you could have one where the environment had some regions that were the preferred temperature and some that weren't and then that would lead to the question of well then how would the agent select where to move and that would take us literally exactly to expected free energy where pragmatic value would um up weight or up regulate policy selection that led to observations aligning with preferences

but again to this top question um action and perception certainly work together that is a kind of way pre-active insight that that there's this um inextricable relationship between perception and action like the um

uh even the opening quote of the chapter by William James is connecting thinking and and doing um however they're not the same we've talked about how for a given system of interest their directions are different across a blanket um

they're they're also modeled as difference in the generative model that we specify like this is looking to figure 4.3 but perception is going to be loaded in between observations and hidden states so perception is going to be just from from what variables you you actually use

it's mediating the relationship between observations and unobserved latent causes.

Like here's the thermometer reading, and then here's the estimate of what temperature it is in the room.

That's where perception is.

Whereas action is seen as intervening in how hidden states change through time.

You're not reaching in and changing how the thermometer reading is related to temperature in the abstract.

You're actually doing something that modifies how temperature in the world changes through time.

And then the thermometer readings might be different later.

So action and perception are coming in differently to how we specify the generative model.

Susan or anyone else?


SPEAKER_01:
Yeah.

So here's how I'm kind of interpreting the

the planning, and this kind of brings in the changing your mind versus changing your world.

So changing the world, if I'm trying to change my world, then I'm acting on the environment.

And bringing in the expected ambiguity and risk, that's what's emerging.

And if I'm not able to reach the expectations of what's changing in the world, then that's going to bring me back to changing my mind.

So that planning is the inferences that I have about what my power in the world is.

And I think that for me, that's kind of where I bring in the affordances because what you talked about, Andrew, was

was I interpreted that as you know someone feels entitled um that the world is going to provide them and then they act out in the world as if they're entitled they may well you know either have to change their mind or they're going to get reinforcement that they are entitled is that is that cohere


SPEAKER_02:
yeah like if there's a door and one expects to be on the other side of the door then it's like you try to open the door okay if it opens then you change the world and then you proceed on and as you described like then that was like confirmed or reinforced whereas if it doesn't open I mean what yeah you can keep on trying but at some point if it's not openable

there's gonna have to be a belief update that that in that moment at the very least that door can't be open otherwise it's going to be just you know you're gonna um not drink any water and you're gonna die trying to open that door it's like there has to be a stopping point um and and that brings in the expected ambiguity plus risk yes let's definitely this yes we'll look at 2.5 2.6 and see what um

where these come in.

Andrew?


SPEAKER_03:
Hi.

Multitasking again.

Just a quick question.

This action perception conversation reminded me I have some questions about attention and intention.

Are those concepts kind of covered or addressed anywhere in Active Inference or this textbook?


SPEAKER_02:
attention absolutely in its more uh technical parametric meaning of attention as precision as well as in its kind of broader psychological uh meaning intention probably more of a nebulous concept so not to say that you couldn't um assign

preferences to be intentions for example the belief desire intention model that's explored in livestream 46 by ryan smith at all where they associate preferences just straightforwardly um with they talk about the generative model in terms of the belief desire intention folk psychology model but in this textbook it's um gonna certainly talk about attention and that's a major topic


SPEAKER_03:
cool yeah I was just noticing the other day how um with your perceptions there's certain modalities like your sense your skin sense and your body sense that can be masked um in terms of perception in a way that like noise cannot from your conscious experience and I thought that might say something interesting about the active inference model or the the structure in our brains


SPEAKER_02:
yes cool yeah this is uh 6.2 here's like different kinds of observations so sensor fusion setting and then attention is it allocated across sensory modalities or how is it or where is it distributed across um or within sensory modalities how are those attentions to

observations related to like the unexperienced beliefs about the world.

Like you're not actually directly experiencing whether there's a jumping frog, but you may experience interception, auditory, visual.

Jesse?


SPEAKER_08:
Could you look at, to Andrew's question, could you look at policy as intention?

more or less because it's projecting a series of actions that you'd like to have happen.


SPEAKER_03:
Yeah, I was thinking that exact same thing.


SPEAKER_05:
So there's a quote on page 38, I believe, where they're discussing beliefs over policies.

as intention and this is related to the expected uh free energy um formalism and yeah i think this is interesting quotes to discuss if people have any thoughts about it nice great reference there i i don't know if there's two two general the statements but


SPEAKER_02:
To go from something that is technical and within the active fontology and then kind of like zoom out or adapt it to a more nebulous setting, that's on you.

If you choose to take that interpretation of a given aspect of active inference in a setting, then it'll be valid in the ways that it's valid for that setting and it won't be

whole story in the ways that it's not the whole story for that setting however that kind of a question is difficult to answer in the abstract because when we're talking about policy selection we're talking about metronomes and particles of dust and computers and groups of people so so then if but so if someone's thinking well intention i'm meaning like what i experience when i want to do something

then you're already in the case of a given system of interest at which point it's no longer like a general question about the active ontology it's more about how you're adapting a term or an equation or something to a given setting at which point will be useful how it's useful and it'll be not the whole story in the ways that it isn't um andrew


SPEAKER_03:
sorry I never um raised my hand but I I do actually have a comment on that um related to policy one thing I maybe this is discussed in later uh chapters but policy generation it seems like at least in these early equations they're they're


SPEAKER_02:
a set of policies is inferred but I don't see how policies are generated is is that discussed or implied somewhere yeah good question it connects to the where is planning so um policies just as a first pass answer policies are the affordances that are available

multiplied to the exponent of how many time steps the planning horizon is.

So if their affordances are up, down, left, right, there's four affordances, over one time step, there's four policies that can be selected, up, down, left, right.

So for a time horizon of one, affordances and policies are identical.

For a time horizon of two, you'd have 16 policies of length two, up, up, up, down, up, right, up, left, et cetera.

So in this setting, policies are just affordances over a horizon.


SPEAKER_03:
So then how do- I'm not sure that really answers it unless affordances are generated because it just moves the question to affordances.


SPEAKER_02:
Absolutely.

So how are affordances learned or how are they provided?

So like in the TMAs or something like that, it's like, how does the rat know that it can move up, down, left, right?

So.

First, we're dealing with maps, not territories.

So just specifying a given vector of affordances doesn't mean that the rat knows that it can do those affordances or has any kind of like awareness about them.

It's just us describing what we've seen the system do.

So when we're talking about the policy prior or the habit vector, the prior on policy,

um affordances those are something that the modeler infers not necessarily the system of interest itself however it still is the question of interest well how are those affordances known and what happens when it up down left right and then all of a sudden up down left right and raise up you know an arm or something like that like how do new affordances come into play

we can express the question in the act infantology but write it out for a given system of interest and then I think you'll be well closer to the answer but certainly you can develop nested models where like there's a um a a context shift that leads to a different set of affordances

But that's a good question.

Let's look at 2.5 and 2.6 and just look at their partitionings.

Okay.

So here's 2.6.

five and they write that variational free energy so variational it just means we're using variational methods so we're doing approximate Bayesian computation not exact base but approximate base using variational methods

It may seem an abstract concept, but its nature and role it plays in active becomes apparent when decomposed into quantities that are more intuitive and familiar in cognitive science.

So these three lines are different decompositions of variational free energy.

So they differently partition the kind of same pie.

in a few ways the top line is more of a statistical physics decomposition that is most like for example gibbs free energy for those with a chemistry background whereas the second and third line have more to do with statistics and model fitting

and in both cases I mean ultimately in all cases since they're describing the exact same quantity but especially um in this pure statistical second and third decompositions it highlights that we're fitting data but also not wanting to overfit data

and the family of distributions that are being utilized are whatever are used for Q. Q is the distribution that we control from the family that we selected.

So if we say we're going to do variational inference on temperature in this room, and we're going to use a Gaussian distribution, so like a bell curve distribution, that doesn't mean that the temperature in the room

is actually changing according to a bell curve or that the thermometer is giving us readings according to that gaussian distribution maybe it has like a long tail or maybe it has some other distribution but it means that we are going to have our hands on the knobs of q and get to the best fitting which is to say anything wider would be under fit anything tighter would be over fit

those are the knobs that we can control it might be grossly inadequate but there is a way to fit the best possible Q to a given Y data and the way to fit the best Q to a Y is with reducing the KL divergence shown here and that is variational free energy minimization

if you can reduce that KL divergence between the variational free energy and surprise to zero then variational inference is identical to exact Bayesian inference because minimizing surprise is kind of like the actual Target here if you minimize surprise you've maximized model evidence you know you're getting new thermometer readings coming in

you were minimally surprised by those readings you would have the highest model evidence you would have the best belief so instead of trying to maximize model evidence directly with like a hill climbing approach statistical physics kind of inverts that problem and says well let's think about going downhill with a gradient descent

And here is our way to have a tractable, incrementally improvable approach that we know is going to be moving in the right direction to surprise minimization with variational free energy minimization, which is highly optimizable.

Again, doesn't mean that it's going to get exactly here.

Doesn't even mean it'll get close.

But you will be able to take incremental improving moves, which is kind of shown here.

So that's the real-time beliefs, the approximate posterior Q, and the incoming data Y.

That's the variational free energy.

And again, the first decomposition is more like statistical physics free energies, and the second and third decompositions are more like statistics.

This doesn't have anything to do with the future.

This is just like a real-time unfolding.

Then we get into expected free energy.

so expected free energy extends active inference to include a prospective form of cognition which is planning so now um rather than just the kind of ball rolling down the hill okay sometimes it jumps left sometimes it jumps right but each time its movement is basically um

dictated or constrained by its local surroundings so we could talk about that ball having policy decisions or taking actions in some sort of um very like

real-time unfolding way but we might not need to appeal to planning however we may be interested in cognitive models that have an explicit planning component like deliberation or consideration of multiple decisions

Again, this isn't to say that that system does planning, just that we're going to make a cognitive model that includes the planning component.

Key notation is when there's a tilde, little curvy line, over a variable, that is being used to refer to like a whole time series of that variable over a given time horizon.

So now we move from F of Q and Y, variational free energy, to G of pi.

So it's different in a bunch of different ways.

Expected free energy is looking to the future with these series of expected hidden states and observations that of course have not happened yet, whereas variational free energy was dealing with the actual data Y. We also noticed that expected free energy

is taking in as an argument policy priors, not taking in beliefs and data directly, but rather, even though we see the Q and the Y, but what G is doing most proximally is taking in the prior on policy.

So this is like a vector of all the possible policies

Again, if it was only one time step, that's the same as the affordances.

If it was two time steps, it would be like affordances squared.

Taking in the prior distribution on those policies and then sharpening or reweighting them according to this.

So let's think about that up, down, left, right, two time step settings.

So there's 16 policies.

So if we had no a priori habit of any of those, they would all be like 1 16th.

And that vector would look like 16 tall and each value would be 1 16th.

Or if we always did policy one and two, and then we very rarely did the other ones, it would be like 0.5, 0.5 in all zeros.

Or you might put that in a computer as like 0.49, 0.49, and then very small values for things that are rarely done.

G takes in that policy prior and it updates it according to this calculation.

And so for people who want to look at the code, that's very helpful to see where exactly this comes into play.

Takes in that vector and then it sharpens or updates it to give more weight, make the policy posterior bigger where the policy contributes information gain or pragmatic value.

Pragmatic value rewards or updates or increases the weighting on policies that bring expected future observations into alignment with preferences.

That is the definition of pragmatic value.

The definition of information gain or epistemic value is about the difference that you're expected to get

depending on the observations that you expect to see.

So it's definitely a lot.

However, that is what this top and most common decomposition of expected free energy does.

And then as Susan brought up, another decomposition of G

is rather than thinking about what G does as up-weighting based upon epistemic and pragmatic value, you could think about G as changing the weighting of policies based upon their expected ambiguities and risk.

David, and then Andrew.


UNKNOWN:
Hi.


SPEAKER_07:
I guess a major, a major obstacle for me to understand all of this is literally the mathematical construction of this sentence.

So like I, for example, here, I just didn't know where the, what the tilde means in X and Y or what, where the C comes from.

Just asking because I don't know if somebody has this sense of discomfort.

not being able to decode what what's this trying to tell me and I don't know if somebody has some good research on how to really understand this this construction yeah thanks for asking totally agreed um hopefully you're in a good resource now and and thank you again for asking


SPEAKER_02:
depending on someone's familiarity with math or computer science and programming there's definitely a lot of approaches but the big approach is many coats of paint like coarse grain it just say okay 2.6 is expected free energy and you can go down fractal rabbit holes for the rest of your life or at some point you can just say okay expected free energy is about

updating beliefs about action another really helpful resource that again I encourage people to contribute to is the natural language descriptions of the questions it's a lot easier to read there's still a lot of terms in here but the blue terms are ones where we have mouse over definitions um and then also maybe someone's interested in playing around more with the AI

explanations that we used these may be helpful um certainly on your own notebook or however like the notation uh and understanding like you know q approximate posterior x hidden states through time y observations through time pi policy it is like learning a vocabulary or a symbol matching exercise

And math is a language, and actinth is a dialect.

So it just is a lot of exposure and hearing about it.

Christoph, they pulled the equation out of the hat.

Yeah, in some ways, yes.

Also, the textbook is distilling and summarizing across research.

and there are more lengthy derivations elsewhere the the math learning group again to the extent that people want to to do these activities um there's always been like a more fundamentals of math learning group side like just like what does it mean when we have two matrices and then we just like put them next to each other like what does that mean to multiply them

And then also some people like Jonathan and Ali and others have worked on the derivations of the equations and Sanjeev Namjoshi textbook and other resources help so it's just like.

But being in the game is the key enabling step um Andrew.


SPEAKER_03:
yeah i'm actually kind of related to this uh just one aside for some of the greek characters as soon as i start like memorizing what the pronunciation of the characters are you find that there is a mapping between like mu and mind there's some alliteration going on um so that might be a little mnemonic to help um the other thing is i was wondering with the assignment of like

um epistemic value to part of these equations is that a generally agreed upon format within the sort of scientific body or was that sort of arbitrarily mapped by the researchers um putting these equations together so so like those statistical yeah um Bayesian sort of


SPEAKER_02:
uh equations is that like agreed upon as um denoting that qualitative uh term in the case of variational free energy yes I'd say it's absolutely non-contentious variational Bayesian methods or energy-based methods with these decompositions have been in play for decades

Expected free energy extends VFE into the discussion of observations that have not happened yet.

Also, these are non-contentious decompositions, I believe.

I don't know.

I don't have any survey data on who knows or believes what, but I don't think that these are contentious.

About the latex, if there is a column here,

Um, if you want the raw latech, that is a good technique.

You can paste this into yeah.

GPT four can use images as input.

Um, this is why the stigma G is so important.

The notes that people write in the questions, like the more that people add, the more material there are for human and non-human learners.

Um, and.

We make do and we stay in the game with what we have.

Like three or six months ago when we were converting them all to LaTeX, this was the best that we could do.

And it works really well with Coda AI.

Coda AI does not have the image recognition yet, but if slash when they do, we'll be able to add another column and just say, okay, given what's in the figure, dot, dot, dot, dot, dot.

So yeah, a lot of different, interesting, emerging ways to work with the material.

Cool.

So that is chapter two on the low road.

For the next two weeks, we'll head into chapter three on the high road.

So thank you, everyone.

See you next time.


SPEAKER_05:
Thank you.


SPEAKER_02:
Bye.


SPEAKER_04:
Thank you.

This is great.
SPEAKER_01:
All right, welcome back everyone.

We're in the first discussion for Chapter 8.

So, does anyone want to begin with like just any thought or question on 8?

Any quote or part?

And we'll just see where it goes.


SPEAKER_04:
All right.

Hal Hallstein, yeah I was curious about the role of attractors.

Hal Hallstein, In the.

Hal Hallstein, yeah how.

Hal Hallstein, The role of attractors in the.

Hal Hallstein, General let me, let me try to find the exact.

Hal Hallstein, world section.

Hal Hallstein, yeah just basically the role of fixed point attractors in the in the generative process.


SPEAKER_01:
Anyone with a thought on that?

All right, there's a lot to say about attractor behavior in dynamical systems overall, and that's kind of where we're at.

One way to think about it is if there wasn't some kind of attractor point or set or limit cycle or something like that, that variable would like never converge to a legible stationarity.

So if something's oscillatory, it could be thought of as like converging to a cyclic attractor.

If something is kind of like dampening, like a spring,

that's like a fixed point attractor plus dampening, or it could be like just oscillating around a fixed point attractor.

So it's like dynamical systems are of certain types analytically, and they can be converging and dampening, converging and oscillating, limit cycles.

All these are like different outcomes or possibilities for dynamical systems.

But to kind of pull back a step,

So chapter 8 follows on chapter 7, which shows the discrete time formalism.

So in chapter 7, discrete time steps in the simulation are explicitly modeled.

Like we have time 67, time 68, et cetera.

Chapter 8 is going to show a totally different complementary treatment of time for generative models, which is the continuous time formalism.

And this chapter is about like unpacking a lot of the differences from the discrete time.

And one of the key differences is like a different take on the relationship between observations and hidden states.

So in the discrete time setting, and figure 4.3 is like the Rosetta Stone with the top discrete time model and the bottom continuous time.

So in the partially observable Markov process setting, the A matrix is kind of going both ways between the hidden state and the observation.

Here, kind of in line with the continuous time formulation being closer to some physics-based formula, there's two core equations.

There's observations, y,

the same as observations here.

And then x dot, so the derivative on hidden states.

So derivative on hidden states here.

And basically each of these are mapped out.

So there's like a kind of regularity of observations or a function of observations and a noise function.

And then there's also an underlying generator.

And this is kind of coming from Friston et al.

's earlier work with SPM, where Y are like all of the sensor data for the neuroimaging, like fMRI or EEG.

And then X dot is the rates of change of the underlying neural activity that's related to the observations that are coming in.

So that's kind of, that could describe any,

g and any f so then they by 8.3 get to a kind of simple generative model where there's a fixed point attractor so here's the what it looks like to have a fixed point attractor so this is basically when x is less than v the set point

The rate of change is positive, so it goes up and then when X is higher than the set point the rate of change is negative, so it goes down.

So it's kind of like a first order thermostat with a set point.

encoded in the underlying dynamics like this.

Any thoughts on that or does anyone like want to add more pieces to this about attractors or anything?


SPEAKER_02:
Yeah, I think.

The the attractors become.

like significantly more interesting whenever we move into the next sections with like the luck of Volterra dynamics and the Lorenz system, just like trying to map them to like other phenomena.

Like I really like the bird song example.

I guess this is around page 163 where

we're introduced to the concept of generalized synchrony.

And so the birds are basically like their internal states are starting to synchronize as they're like trading off singing with one another and trying to kind of match or continue the same song, so to speak.

And here we end up with this kind of like

the states are orbiting around in that lower left figure two different i suppose attracting points so it's like the state is like orbiting between the two um but then my my next question is i'm i'm not sure exactly what those two orbits kind of represent if they represent something like

One bird versus another, or if this is like the state that one of the birds as a single agent is like its beliefs are moving around, you know, its own, um, actions or, or beliefs, uh, versus like whenever it's not singing and that's the next thought.


SPEAKER_01:
Yeah.

That's a good question.

I mean, again, like progression, how first we have a simple fixed point attractor.

So this is not even a moving fixed point, just a set fixed point.

Then kind of calling back to chapter five.

there's a descending settable fixed point so that was in the the cross section of the spinal cord where there's the descending motor prediction and then the proprioceptive data coming in and then so it's kind of it's the same exact mechanic as the um set point it's just with the additional piece that the set point is a descending prediction then um

have another bump up in what the attractor can be going to from stable fixed point, moving fixed point.

Now this gets into the kind of like winnerless competition or like generalized dynamical, the generalized sequential dynamics.

These are like iterative processes.

And then first there's the, this is kind of like a non-chaotic winnerless competition.

with an ecological fluctuation so lockable terror arose from the ecological predator prey modeling which is kind of what is referenced here plant herbivore carnivore but also those have been applied extensively to like activity patterns of brain regions so if you have or gene regulatory networks like if you have multiple components that have um some features such that when they're low they can ramp up but then when they're high they're like self-limiting directly or indirectly then

They go up when they're low, and then they go down when they're high.

And so they stay within like a bounded interval.

But this is still has a kind of a very regular movement.

So it's three variables that are linked in a dynamical equation, P, H, and C, in this case, plant interval or carnivore.

And then here are like two projections onto a side of the cube.

and then that gets taken into like one more level to go from the non-chaotic dynamical into the deterministic chaos which is just kind of alluded to and explored more in the citation but then that takes us to the turn-taking susan so


SPEAKER_00:
So I'm wrestling with the attractor states in complexity science and chaos theory.

And so I'm kind of wondering if that's the steady state is the bounds.

In other words, you're going to move.

It's about how you're calculating the movement.

Barbara Kirby- And, although I don't want to.

Barbara Kirby- might be better to put this conversation.

Barbara Kirby- In the next after the next chapter, but, but in terms of.

Barbara Kirby- You calculating yeah it may be that it's just a different nested model of what is the current state.

in other words yeah in other words are is this this state is going to look different if it's if it's over capacity it's out of a state of resilience so i'm assuming that that's actually another nested model you know i don't so i don't want to try to combine the two or am i thinking about it incorrectly yeah it's interesting like explore it for a specific system and then


SPEAKER_01:
maybe it'll be clear like which aspects of resilience are accommodated in the model like as written like even the fixed point attractor i mean in a sense that's a rudimentary form of resilience because when it's pushed away from the fixed point it comes back to it but there's multiple resilience concepts like one of them being that it returns to where it was before another resilience concept is that it stays where it's adapted to

those are kind of not even compatible definitions of resilience which is like why when you build a specific model then you okay for this variable when we say resilience we're talking about the adaptivity like definition versus like the resistance to change version and then it's sometimes really interesting how even some of the simplest formalisms can capture aspects of those dynamics but then like

you can't use the model to show something outside of the model so this if this is the only one you have you'll be able to have that kind of like first order um fixed point behavior but then it wouldn't be adequate to describe some other kind of behavior like this one this kind of model will never show a moving set point but this model would but it wouldn't show

chaotic Dynamics but then this one would so hence trying all these different kinds of models and just like in the textbook which is being shown here just showing all these different features okay then from the chaotic to the chaotic with learning so just all of these different

William Collins, M.D.

: patterns and then it's kind of like well for the phenomena for the system that you're looking at which one of those like levels or types of dynamics feel appropriate.


SPEAKER_00:
Janet Callahan, Ph.D.

: yeah and then just to put context around this, this is just trying to give us the basics, you know, in other words, not to throw in too much more complexity, I mean once we understand the basics, then we can.

Janet Callahan, Ph.D.

: begin adding.

more degrees of complexity, I guess.


SPEAKER_01:
Thomas Parr in his book stream gave a really good kind of timeline talking about how the earliest implementations were the continuous state space.

then people started thinking about getting sequential dynamics this is exactly what we just saw with the lock of volterra then people wanted to look into like planning and sequential dynamics with like counterfactuals so then that motivated more of the discrete space discrete state space discrete time models to to get that kind of decision making and planning

um then there were hierarchical models that were able to nest either discrete or continuous or hybrid models um eventually by being reintroduced introducing the continuous state space as the kind of lower sensory motor

Matthew Shollin- model and then having like a kind of higher order discrete model and then that was kind of explored in the folks psychology work we have continuous time and state spaces, like for sensory and motor and then you have discrete state spaces and time for more sequential decision making and planning Susan.


SPEAKER_00:
I think you just answered it.

I was wanting a little more definition around discrete space and continuous space, but anything else?


SPEAKER_01:
Yes.

Discrete state space is where the options are finite and discrete.

like one or two or three or four or 0.1 or 0.2 or 0.3, but discrete clickable values versus the continuous state space is when there's a continuum that's like differentiable.

Like at all points, you could put a ruler and determine the slope.

J Lenz, D.B.A.

: Whereas if you only have one, two, three, four.

You can't really like J Lenz, D.B.A.

: take a ruler and find the derivative at each time step like you can't.

You can't really have the derivative in the same sense for a staircase.

it's like a step function.

So that's true of the continuous and the discrete state spaces.

We could be talking about temperature, like temperature is a continuous variable, temperature is a discrete variable.

And then one special variable for dynamical modeling is how time is addressed.

And so time, as Chapter 7 and 8 explore, time can be explored as a discrete state space, Chapter 7, or time can be explored as a continuous state space like Chapter 8.

Andrew?


SPEAKER_02:
Yeah, just like the way I kind of think about...

the whole like discrete space model on the higher level of a hierarchical model and then continuous at the lower.

And I don't know if this stands up to anyone else,

something like um i'm sitting here i'm trying to figure out if my heartbeat is too fast like yes or no two discrete options i have to like pay attention to my heart rate for some time but of course like i can't just go with a single time step i have to see like you only know the rate by you know taking a second or a few seconds to figure out what the rate is and so

Robert Alverson, Over you know, however many seconds that I need to measure that which is multiple time steps, or a faster time scale.

I can then make the discrete decision if it's too fast or not, if that makes any kind of sense.

So thus yeah, and that's perfect figure eight point six.

The

Daniel pointed to is like we see how there are multiple time steps in that continuous level at the lower level for every single time step at the higher one.

And so, yeah, I think when thinking about modeling like say an agent and trying to include things like

Dr. J. Corey Williams, M.D.

: physiological some signals like heart rate and other things like that there's this kind of faster timescale going on and then the agent at a higher level, maybe a cognitive level or some such thing is making like more discreet decisions or inferences based on what's going on.

Dr. J. Corey Williams, M.D.


UNKNOWN:
: setting.


SPEAKER_02:
Dr. J. Corey Williams, M.D.


SPEAKER_01:
: yeah.

Anyone else have just any thought or question on eight?

We can look at some questions.

We can continue.

But the chapter's very compact.

It just moves through those sequence of examples.

Fixed point attractor, moving point attractor, dynamical attractor.

chaotic attractor with learning then it just says and you could put that continuous model at the lower level of a discrete time model and here's an example of what that looks like like at the higher level is which location to fixate upon and then the continuous lower levels like the Cicada movements and then there's a few settings where that's been studied and that's the whole chapter George


SPEAKER_05:
I have a little question about the small question about this bird system with the chaotic attractors.

uh chaotic behavior so if i remember right then chaotic behavior uh the by definition it means that you can't that you only have a limited horizon where you can predict what is going to happen still these birds seem to be able to synchronize and is it if i look at figure 8.5 does it mean that they

do not synchronize their down to the right that they even after learning they don't synchronize perfectly they sometimes lose each other but then they uh but they stay quite close like they're because if you can't predict what is going to happen you probably can never because you don't know the initial conditions you know you never know exactly where the other

the other individual is, I guess.

But is that correct?


SPEAKER_01:
Yes.

So one thing to note, as I think it'll probably say,

singing from the same hymn sheet so just to be clear this model has presented it's not like this is like a generalized model of semantic communication and term taking in conversation this is like two birds with a fixed innate song that they can kind of in the second part of it do some local tweaking but in the first part they can't at all

But they have the same song template.

So they're not engaging in more open-ended semantic transmission.

It's kind of like a chant that's supposed to be ongoing.

And so the only options that they have are either listen and understand that the other one is singing it or notice that that has stopped

and then take action to to bring that song like back into play that's kind of like the situation that the bird song is modeling um but then they show like in the paper that when you have one bird with a precise song and one with a very like imprecise song

that that the like that younger imprecise one can like learn and converge to the the other song then you can kind of fix that bird's precision and bring in a new plastic one and then converge that one to it and so that got into like the kind of intergenerational transfer again of like a templated song with a little bit of drift but yeah what's happening here is like between bouts of singing

there's this kind of tightening of the joint dynamics so this could be like if this was like two people and they're pointing at each other pointing and they're trying to they're doing an improv game where they're like trying to keep their fingers as close to each other as possible um so um if it the movements were perfectly correlated then they would be on like a one-to-one line

like the X value of one would be matched exact, tracked perfectly by the X value of the other.

And then if they were totally uncorrelated, it'd be like a perfect circle, which is to say as wide as it is tall.

So like the correlation would be zero.

There wouldn't be a significant regression through that cloud of points.

So they start in the same ballpark because they have a template and then there's some fine tuning,

in the learning process so that their expectations tighten up it's like and then the turn taking part and then so then about the chaos

predictability so that's where the um lyopinib exponent comes into play which is kind of like this variable that describes how chaotic a system is ranging from a system that's not chaotic and the kind of metaphor there is like a laminar flow so you have two planks of wood on a laminar parallel flowing river those two planks will stay near each other at the same distance

then there's other rivers where um planks that are placed near each other converge and then there's systems where planks that are near each other diverge and that's chaotic because you can't necessarily know it's like you're coming in on this one and then if you come in just on this side you take another attractor to the right but if you came in a little bit more left then it spins out

And so then, as that exponent gets more and more chaotic, it gets harder and harder to predict points, trajectories of points that start initially next to each other, so the horizon of predictability drops.

and so this is kind of like it's like this one is saying like we're on we're on the right side of the turn taking and then it's like the precision is dropping and that's kind of spiraling it out and then eventually it spirals out and then it flips to the other attractor spins in spins out and then eventually it flips out there just showing that those kinds of of switching time Dynamics can be modeled even with deterministic equations

So there aren't even like, this isn't necessarily even stochastic.

This is just the deterministic chaos, which was kind of like, what's one of the most interesting and exciting parts from complexity theory and chaos is like single pendulum is deterministic and non-chaotic, but then the double pendulum is deterministic still, because it's just two single pendulum, but then it has the chaotic dynamics.

Daniel Passamaneck- Also connected to wall from some computational irreducibility and about how like there's things where, even if they're deterministic that it still can't be determined, like if the program is going to do a certain thing or finish at all.


SPEAKER_04:
Daniel Passamaneck- yeah Susan.


SPEAKER_00:
Karen Hollweg, So I guess that would have a lot to do with the policy selection and is policy selection with that would that be the same as you know what model, you chose.

Karen Hollweg, To.

Karen Hollweg, To.

Karen Hollweg, assess the situation.


SPEAKER_01:
Yeah, like the singing behavior, just on the first level, the singing behavior can be understood as the policy, the motor policy of the bird, whether it sings or not, like in that binary system.

But then a little bit more cognitively, the choice of which framework to apply or which interpretation to apply is like a mental policy decision.


SPEAKER_00:
So where would affordances come in?


SPEAKER_01:
That's a good question.

In the OFT reference, Livestream 28, with Lars Sanved-Smith.

So this is kind of a different, this is another step-by-step.

So here's static perception.

Hidden state, sensory ambiguity to the observations indeed prior.

Now, in the textbook, they went from that kind of static perception and then went into the dynamic perception.

But here, there's kind of this intervening step with a precision variable on A. So this is like changing the temperature or the precision on A, which is to say from sharpening it so that it looks more like just ones and zeros, and there's a less ambiguous mapping between hidden states and observations,

All the way in the low precision extreme a is just kind of like blurred out so that observations have no information on hidden state.

So they're going the attention as precision here.

So now we have kind of the discrete time figure 4.3 figure 8.3, whatever it happens to be with these little precision guys on the bottom, but otherwise it's the same.

And then let's say that this blue is the sensory motor layer.

So that could be the spinal cross section, or that could be the kind of gross motor behavior, like singing.

Here's where the mental action or the internal affordances, the covert action is coming into play.

Where that affordances in that nested level are modeled as basically shifting the precision on the lower level.

And then they had a third level of policy about that second level policy.

But everything past the blue sensory motor layer are just higher mental affordances.


SPEAKER_00:
Oh, very cool.


SPEAKER_01:
Yeah, that was kind of a big moment when the same or similar kind of formalism that was used for sensory motor control and affordances externally got brought in to this kind of internal

attention as action setting by by Lars and and the others in 2021 and then they still like even in the previous days continue to like make this more comprehensive and like just bringing in more pieces and bringing in like world knowledge traditions in really interesting ways too mm-hmm

Gareth J. Thank you yeah industry time I don't know if there's any continuous time versions of this, but that would be cool to.

Gareth J. yeah Christoph.


SPEAKER_03:
Christoph Strobeli- yeah just want to make a comment that I think it's interesting that this formalism can actually describe both.

Christoph Strobeli- You know, continuous time models and discrete models, because as we as we move in the hierarchy of concepts, then.

we finally arrive at things that are discrete, but discrete phenomena in some sense are not, you know, there's always this sort of problem, of course, graining from underlying processes.

So there's the generative process that gives rise to the phenomena.

We are sampling that in some way and we're computationally bound, so we can't have perfect model of the other system.

So we have to sample.

So that gives rise to some kind of discretization of,

of the phenomena.

And temperature, I think it's an interesting example where we can model it as a continuous variable, but it's not actually useful to our... For us, temperature always has some kind of discretization because we don't care about, I don't know, six, seven, eight decimal places.

Right.

So the fact that you can build this hierarchy and this discretization just kind of naturally fits on.

You can fit it on top of continuous models.

It's actually very reassuring that this is a this is a really good paradigm that these two things can be connected kind of directly without without bringing in some some other theory to to say how these how these systems would interact.

because that's just kind of what naturally happens in more complex systems.

At the top level, things do look more and more binary when the decision making is made.

Do I go left or right?

In the real world, there is no really left or right.

There's some kind of sense of direction, but at my mental level, that might be a binary choice.


SPEAKER_01:
Yeah.

Very interesting.

Thank you.

Your hand was still raised on my computer, but Prakash, did you raise your hand?


SPEAKER_06:
Yes, actually, Christoph actually said what I was trying to say.

I was thinking any kind of complex system will have a lot of continuous items in the downstream, but as you go up, there's a decision-making happening and finally it's more binary.

The key learning which I'm getting over here is that

uh like there are various possibilities in which you can mix and match this kind of systems that you want to have but there is a possibility in which you can there's a flow and we can use that for our own purposes yeah that's very interesting another kind of topic to connect this to like non-active um psychophysics is the just noticeable difference concept


SPEAKER_01:
So it's like the classic examples like we have two objects that are going to get put in one in each of our hands.

And then the task is which one is heavier.

So the objects have a continuous value in reality.

Or you can think of it as your muscles are exerting a continuous amount of tension to propriocept and like evaluate the mass of the object.

But it's a binary decision which one is heavier.

um so if the stimuli are exactly matched like if they're the same weight but there's no option for the same weight you have to say which one you think is heavier but you'd expect that 50 of the time like that it would be guessed which one is heavier and then it turns out that there's like this kind of sigmoidal curve where there's a high sensitivity when things are very close to each other

to differentiate them but then if if it's like five pounds versus 15 pounds changing the 15 to a 16 it's not like it increases the decision-making accuracy by 10 because it's already at like 99.9 percent

Gareth J. So that's kind of an interesting thing is like connecting this the stimuli intensity to like potentially the binary eyes decision making sigmoidal describe decision making around that just noticeable difference on point Christoph.


SPEAKER_03:
Yeah, there's also very interesting research on that subject called quantum qualia.

They're essentially testing a hypothesis that the quality of experience depends on the history of experience.

So it's not that when you perform this kind of experiment, say, asking people which colors are more similar and more different,

Mateusz Piorkowski- Depending on what the sequence of colors is that they've seen already their responses are likely to be different, so there are some people trying to test theories that essentially bring in.

Mateusz Piorkowski- You know some kind of mathematics that the kind of mathematics that's previously been used in quantum theory to model model phenomena, almost like a evolution of a you know quantum wave function, so that.

when you're performing an experiment, it really matters what your context was.

And there's some really interesting results in that space.

So there might be, you know, it might be that there's sort of like a reset time that's required, but it might be that simply that we really perceive deltas and that's what really matters.

So if you give someone two very heavy things and then ask them to compare two light things, their brain might be used to

Gareth J. feeling the feeling of something being heavy and not having enough precision not resetting and i'm not having the precision to perceive the difference between the lighter things so it's really, really interesting how.

Gareth J. How that works.


SPEAKER_01:
Yes.

Yes.

And definitely like about detecting change.

like the eye is expecting a certain intensity of light and then it's brighter than expected that induces like a dilation or a constriction however it goes and then that recalibrates to a new expected set point and then from there again it's just about the direction of like whether this is less or more that's kind of the predictive processing paradigm

which is like, you don't have to always be carrying through like the temperature of the room.

If it just happened to be that your implicit set point was room temperature, and then you had that fixed point attractor, all you'd have to carry through would be like plus or minus within, within reason.

And it wouldn't need to be like 72.04 and do some sort of computation and then store that, you know, you just have a binary decision about which is action dependent.


SPEAKER_03:
Yeah, so it seems to be that the bias seems, we always continuously adjust the bias to really see just the differences that make the difference.

And I think there's a really fantastic example of this from antiquity.

One of the Greek philosophers, I forgot what it was, but he was staring at the river and seeing people just trying to

I think rescue some animal that just went into the river and so on.

And then he looked at the ground after and he saw the ground flowing.

So even if it's not like a physical phenomenon, like expansion of the veins to pump more blood or whatever, even just at the level of the brain, there is some kind of processing that you can very easily see yourself if you do it.

If you look at something that flows like a water flowing downwards on a TV and you look at the wall, you will see the wall flowing.

because the brain adjusts to that flow so that you could pick up the differences within the flow, not the flow itself.

You just become oblivious to it.

I wonder, is there any work in active inference that will try to model something like this?

Because that seems like a really interesting project.


SPEAKER_01:
I don't know specifically.

It reminds me though also of like going on a long walk or hiking and looking at the ground a lot and then stopping and then looking far away and they're kind of like being like zooming.

because there was even like a calibration to the um the amount of visual flow or just like an expectation that like objects come closer or go further or something like that and like um another fun example like the Exploratorium in San Francisco it's like fun like hands-on science it was like a pipe that had a colder a hotter and a room temperature component

and then you put like one hand on the colder and the hotter part not burning or freezing but just noticeably and then you put both on the same section and then like one of the hands fell the one that had been on the hot fell it was cold and vice versa but of course you're looking at it and your hands are on the same piece of metal but each hand was only able to kind of like report

that it was getting contrasting opinions.

So it felt like as if you had flipped the hands, but they were both on the same temperature.

And after images, visually, some of these phenomena are more at the cellular level.

like some visual after images and effects those are like it's like some you know retinal cells are exhausted and then like they so it's like that phenomena I mean inquiry finds that that phenomena can be explained by cell level findings whereas something else like a ballerina that rotates or a cube that's collapsed one way or the other like those are not going to arise at the level of like the single retinal firing rate

but they might arise at some attentional level.


SPEAKER_00:
Yes.


SPEAKER_01:
Andrew or anyone, what is this quote?

What does the quote mean or how does it start off the chapter?


SPEAKER_02:
I just threw that in there in reference to what Christoph was referencing.

I'm not sure if it's Heraclitus who saw the

yeah so the animals go into a river, but I know that this is a famous quote referencing like yeah there's another line like no one steps in the same river twice like you're you're observing.

Like you're observing behavior and so everything is continuous but I don't want to pull us too far away from from the chapter with just saying that, but I mean I clearly to me it seems to just be representing like continuous dynamics right continuous change over time so.


SPEAKER_06:
yeah totally prakash now i was just uh trying to say a little bit about the code i think like uh as far as i remember it's like you cannot step on the same river twice because uh after a moment the river has changed you know like it is not the same river it's like the concept like works like it's like everything is changing and it is not the same so yeah


SPEAKER_01:
just just to find it yeah like so when there's like a rock in the river and then there's like a flow over it but then the flow is kind of like kept at a certain like there's a stable point for the flow that's kind of like non-equilibrium steady state because there's still some past passing through a material dissipation of a gradient but then if you were just studying like the elevation of the river at that point it would be a stable or it would have like a little Ebbing around a small point

Just on the kind of like practical note, PyMDP, the main Python package for active inference generative models is discrete time only.

Hence, the PyMDP tutorials are like all discrete time.

Whereas Sanjeev's work in Python accommodates continuous time models as do the MATLAB earlier SPM work and Rx and Fur and Julia.

But maybe someone has brought some continuous time models in the PyMDP.

I just don't think it's on like the main branch or I'm not sure if it's on any branch, but it's definitely in Sanjeev's code SPM and in Rx and Fur.

The papers that are cited, like at the ends, we can double check, but just looking at the authorship and the years, these papers are probably almost all in MATLAB.

We can double check them, but very probably.

Continuous time Python, Prakash.

Ask Sanjeev, just in Discord, just tag Sanjeev.

Is this, wait, Andrew, is that a continuous time?


SPEAKER_02:
Well, I just sent for the general documentation, but yeah, I mean, all the examples are there.


SPEAKER_01:
Yeah, yeah, this one's a discrete time.

Discrete time and discrete state spaces.

Actually, the...

This one has discrete and continuous state spaces, but it is all in discrete time.

But if you in Discord maybe ask Sanjeev and or like tag Magnus Kudal and Sanjeev and you'll get two good answers.


SPEAKER_02:
Okay, thank you.


SPEAKER_01:
Yeah.

Yeah.

Or like

This could be fun collaborative annotating for whoever wants to do so.

With all of these examples and more that we can find, we could annotate time, discrete, continuous, either, or hierarchical, and then understand which one of the code examples, as we also continue to add them,

has these different features if anyone wants to explore that we can definitely do that but right now just like the links but but maybe look through them and see which um which ones have which features yeah christoph


SPEAKER_03:
Mateusz Piorkowski- yeah I just I just checked my sources and the story I was referring to was about Aristotle so apparently he saw a horse stuck in a flowing river and he fixedly watched the rescue operation and he finally looked away and everything else appeared to flow, so he was it was the first.

Mateusz Piorkowski- recorded visual illusion in history.


SPEAKER_01:
horses kind of play a key role in the visual perception because like the first high-speed photograph was about the debate whether the horses pick up all of their feet when they run and then like they took a photo and it was resolved like yes when they sprint all four of the feet are off the ground but that was it was not known until that happened


SPEAKER_03:
Well, you know, in the first Polish encyclopedia written in the Polish language, there was a very interesting note about the horse.

It just simply said horse.

Well, the horse is everybody can see.


SPEAKER_01:
Don't ask if you don't know.


SPEAKER_03:
But, you know, it tells you something about the, you know, about the times.


SPEAKER_01:
Definitely.

there's been a lot of recent work on the time perception and the flow of time like Darius and and others and some upcoming live streams too have kind of explored that that's definitely a challenging topic because it's dealing with something experiential but people are pursuing it um whereas these examples like the eye movement that's more empirical um

arguably this is information like pupil arousal and isocating.

This is information that potentially every smartphone and webcam collects all the time.


SPEAKER_03:
I think this is one of the interesting differences between machine and human or machine and biological vision is that in biological creatures, we do have these mechanisms of adjusting precision, adjusting resolution of picking up salient features, whereas machine usually

And we have another sort of at the low level, at the level of the eye that a lot of this processing happens.

Whereas in machine, you just get a big array of pixels and the neural network is supposed to figure it out on its own.

Radek Machan- From that input and you know i've been wondering for quite a long time, whether you know things think things are the way they are, for reasons, and we may not understand reasons.

Radek Machan- Why things are the way they are, but evolution is not it's not stupid it produces things for for really for really good reasons, so I think that.

it might be that the fact that we have to saccade to really look at something, that we have all these limitations, that actually, in my view, leads to more robust representations.

I don't know how to test this hypothesis in a digital neural network yet, but it strikes me as highly implausible that this limitation is necessary to build robust representations, because robust representations would be the only thing that survives this kind of sampling process.

If you get too much detail, you just get too much.

It's harder to extract information, say, like information.


SPEAKER_01:
That's very interesting whether there's like more fundamental like computational limitations.

That's highly possible.

But like just when it comes down to the fovea.

the center high resolution area on the eye it's like there are trade-offs just with the density and the energy costs of having high resolution in the center of the eye and so like here's a visual Cicade model and then it's possible for this like eye Cicade um agents to to differentiate these different um visual patterns like is it just line is it like parallel lines with this diagonal thing or you know

vertical lines, those can be quickly differentiated.

Like, am I looking at a Dorian or Ionian pillar that could be differentiated with like

one to three skillful isocades it's like am i reading a scientific paper it's like well is the lcaver logo in the top left no okay cicade over here is there something there no okay last check okay it's not a paper that's whereas if um this were going to be taken into like a sort of classical machine learning pipeline then let's just say this is like 20 by 20

that's 400 pixels coming in so you need like at the very least the memory capacity for like that many pixels to be coming in versus only nine pixels with a three by three high resolution and then that's like the kind of truth drop on our visual experience being the generated not the processed like no blind spot color in the periphery resolution in the periphery suppression of the skates like all those features

are the information that does like give evidence in the moment to moment that we're experiencing what we generate visually and then probably by extension other senses too not like taking in a whole scene and then distilling the scene

but having an underlying generative model of the picture even if it's our first time seeing that specific one and then doing isocating to fill in the detail and then doing um all kinds of other cognitive phenomena like attentional modulation and kind of like instantaneous overriding to just like make it normal make it feel normal

yeah this this is uh I guess this will be in one month but this has to do with the time perception affect temporality intention

Michael Mann- retained past the sedimented retained embodied trace.

Michael Mann- And then the.

Michael Mann- unfolding kind of time cone counterfactual possibility.

Michael Mann- prospective potential space kind of just blurring off.

In the last minutes, any other thoughts on 8?

Otherwise, yeah, it goes fast, basically.

That's 8.

Then next week, earlier, for cohort 5, we'll continue to talk about 8.

James Heiting.

Maybe um, Andrew, if you want to before then, um, or anyone else like maybe we could look at one of the papers in more detail, like the ice arcade paper with Thomas Parr, just because I mean we've kind of covered the chapter James Heiting.

in these conversations, and we can look at the bird song or or um

some other work with uh that's side of the chapter then chapter nine that gets into the empirical data basically up until this point um all the discussion on building a generative model discrete time and continuous time has been like building it by Fiat and then simulating or making synthetic data but then if you want to

Christopher Lustrii, run it in the other direction and take in empirical data and then parameterize your model for like determining within or between group differences or doing any other kind of testing like that's where chapter nine comes into play.

Christopher Lustrii, And then chapter ten just summarizes it up.

Christopher Lustrii, So it's so funny the books like an accordion like sometimes it seems like there's a ton.

Thomas Greene, Other times it's just like barely it just like just quickly like flipping through index cards of all these different models, because there's so much detail to go into with with even the simpler models.


SPEAKER_00:
We've talked about the time parameters, and I'm trying to envision how do you set the spatial parameters?

That's what Nan's going to cover?


SPEAKER_01:
Good question.

Parameters could refer to spatial or temporal or like cognitive, any kind of parameter.

But if we're going to be talking about data coming from a spatialized system, like a room that has sensors in locations, then part of making a model to deal with that data would be how do we incorporate the

So if our data have a spatial component, I mean, if we don't take that into account, at the very least, we've left important information out about our system that we know.

Worst case scenario, we've come to a misleading conclusion because we just kind of like averaged out all the sensors, but they were in different locations on the roof.

So then there's like a really, you know, the roof is like a triangle.

And then it was like, there's a temperature pattern that makes sense when you know that some are on one side and some are on another side.


SPEAKER_00:
Okay.


SPEAKER_01:
The reason why time gets like such a special treatment, whereas like space, we could model it continuously.

Like we could have a continuous GPS coordinate with just like as many levels of precision as we wanted, or we could discretize GPS and just say, okay, we're only going to like the first decimal point and then it's a grid.

So the reason why that's less of a big deal is that the, the time concept

gets taken into account every single time we iterate the model forward so how the model changes in time is like obviously one of the key aspects it is what makes it dynamical so that's just a huge decision that's why like in chapter six one of the big decisions that's brought up is like how is like should it be handled

should it be discrete or continuous?

Because it's a huge difference.

Whereas it's a smaller difference if we have like temperature as a continuous or a discrete value.

Temperature could be modeled continuous or discrete, whether we were thinking about time discrete or continuous.

So it's kind of like, even just with a thermometer in the room, like no action, no, nothing like this, you have a two by two discrete temperature, discrete time, continuous temperature, continuous time off diagonals.

Like, and it's kind of like simple examples like that, that give good grounding as to why multiple models are constructed.

Because even if it was just an air conditioner in a room, you'd have at least those four models, at least in principle.

And then you could explore like which ones have different, which ones explain different proportions of the variance of the data.

Which ones have different computational resources?

Okay.

Well, this one's more accurate, but it's more computational resources.

Okay.

We'll run that on the special one or we'll run the cheaper analysis first.

And then if we need to, we'll rerun it for the final version with the more expensive.

Yeah.

Those are just like practical considerations that come into play when you're working with real data and systems.

All right.

Thank you.

See you all next time.
SPEAKER_01:
All right.

Welcome back, cohort five.

We are in our second discussion on chapter nine.

So, Andrew or anyone else who wants to write a question or raise their hand, let's just jump into it.

okay i just heard the recording in progress thing maybe there's something about pete's coffee that causes a time dilation but either way now we're in chapter nine so where shall we begin yeah i was gonna say if anyone has any


SPEAKER_00:
particular points they want to get into with this chapter happy to jump into those otherwise um if anyone's like you know only able to make this meeting weren't able to make the last one I'm happy to kind of give a general overview of what's going on here uh if you don't mind a general overview would be good to start with if others are okay means


SPEAKER_02:
yeah go for it previous meeting is at 11 o'clock in the at night in my time so i i haven't been uh able to make them so general overview would be really helpful for me if it's a possibility yeah great um yeah let's do that and there is a lot going on in this you know this chapter this is sort of the


SPEAKER_00:
aside from chapter six, which gives us a general recipe for composing generative models and kind of the more general method you might take here and actually applying active inference in the second half of the book.

Recall in chapter seven, we're given the

POMDP model and how to construct that.

Chapter eight, we're given more continuous models.

And then here we bring it back to, okay, so how do we actually use these models and construct them in a way that we can apply them to empirical experiments where we're actually collecting real data?

And so we're moving beyond the range of just purely building models and running simulations to

Collecting empirical data such as like neurophysiological data could be EEG.

It could be just observing behavior of anywhere from human beings to to rats.

Or if you're in more of a an engineering setting, you might be looking at other kinds of data that's being extracted and you just basically we want to figure out how is that?

What are the?

What's the process?

that is generating that data, which of course we may never know, but we can attempt to model it.

So yeah, this chapter, it's quite dense, I'll be honest, whenever we incorporate everything, but it does give us a nice full flow of how to apply these methods to empirical data.

So first of all, in the introduction, there's hints at what we'll find in the rest of the chapter.

We're introduced to meta-Bayesian analysis, which is where we actually view this as the agent's subjective model versus our own objective model that we're building.

of the agent subjective model that kind of uh carries into the active inference ontology right just we do have to account for the fact that we are sort of the quote-unquote objective scientists in this situation we're attempting to model the behavior um and other parameters of like a separate being from ourselves like in a team maze we're trying to you know understand the subjective model of a rat which we don't have

know collecting data and applying analysis to see if we can do something like approximate it so here our goal is to what we would say is recover or infer the unknown parameters of they phrase it as the subject's brain its subjective model using our own objective just by drawing on the behavior in our observed data and then we uh invert

our objective model to infer those parameters of the subjective model.

And then finally, by doing so, we can test and compare hypotheses.

For example, we might find multiple models that we could build.

We could compare them and see which one holds best to the behavioral outcomes that we're seeing in the data.

And then they have another concept called computational phenotyping.

This is where we kind of, by recovering model parameters, prior beliefs of our agent, we could actually like kind of classify the individuals we're studying, whether it be multiple rats in a team maze, it could be clinical patients and more of a

you know, in the study of psychiatry, but we can kind of phenotype those individuals based on what model we end up developing to try and explain their behavior.

And so yeah this chapter lends itself really well to specifically computational psychiatry and neuropsychology neurology but, in principle, these methods could be applied to a broader much broader variety of phenomena, whether it be also or or even non living systems.

So section 9.2, and again, there's a lot going on in this chapter, so I'm going to try to be a little bit more brief.

But we are introduced to, just as we've seen Bayes' theorem in previous chapters, now we have equation 9.1, which looks almost exactly equivalent, other than we've replaced observations

and states with behavioral outcomes, U, which are actions to be taken, and then a theta symbol, which represents the parameters of the model.

And so just as we can update a model's beliefs related to states, observations, the relationship between them in the form of likelihoods and posteriors,

ideally doing that same exact thing here, but now we're mapping outcomes to parameters.

I'm going to pause here real quick because you had a question.


SPEAKER_03:
Yes, actually, I'm just wondering, so you mentioned this computational, yeah, exactly this one, you mentioned computational phenomenology and this subjective model.

Are you saying that, I don't know anything about it, so I'm just asking, are you saying that somebody can actually start plugging in values inside that matrix that you said, like, and

There will be like what i'm saying is that the values which you encode into that, and some and some variable since you include related part of that computational phenomenology, or how does it go?


SPEAKER_00:
Yeah, sure.

So one thing we could do to try and answer that question is to look at, like,

we have in section 9.5 just a nice like brief series of steps for how to go about this.

And so like in step one,

Quite simple, we collect behavioral data.

You could even think of this as like a spreadsheet or something along those lines, like that kind of structure where each row can represent an individual.

Like in a T-Maze example, we have rat number one, rat number two.

We could have in the columns things like what are the actions that they took that we saw when we collected the data.

What are the associated observations or other components of a model that we saw?

We could also have other kinds of information, like maybe the test was being done by administering a drug to certain rats and not to others to try and study the impact.

We could have a binary one or zero.

um like if for each rat if it was administered the drug or not step two we formulate a pomdp this this chapter is a little bit heavier on looking at discrete models so a pomdp as we were introduced to in chapter seven although later they do give us a couple examples of more continuous models and

what we're trying to do with step two is that we're trying to develop basically, yeah, derive a model that closely approximates what we see in the data that we just collected in step one.

But nonetheless, we know that this is our model.

So this is kind of that outer objective model that we're seeing in the figure that Daniel's pulled up there, figure 9.1.

So it's kind of that inner box is the subjective model we're trying to basically devise our own objective model of what's going on in the rat's mind, so to speak, if we follow a teammate's example.

So there could be aspects of this that, because your question had to do with specifying priors.

Step three is specifying a likelihood function.

And that's based on the model that we just tried to make.

Then after that step four, we specify prior beliefs.

And so in that step, the way they've written it, we can specify prior beliefs about the parameters in terms of expectations and precisions.

Often these will be centered on zero with precisions reflecting plausible ranges.

So you might think of that as you have your own hypothesis about what the prior beliefs are, and you just kind of roll with it, so to speak.

We're doing a process where we're trying to build a model that whenever we run that model,

it and and run a simulation with it hopefully it more or less approximates the real actions uh that say a rat and a team maze uh took as well as like the combination between those and the observations that the rap received so um hopefully that provided some kind of information or a bit of an answer to your question


SPEAKER_01:
um but yeah i'm just trying to get to yeah sorry great answer all out a little more so prakash you mentioned phenomenology like the kind of felt experience and this kind of made me think about like um three different kinds of phenotypes that you might be studying you might be studying the most grossly morphological like just the actual bodily movement like the movement of an arm

then there's kind of cognitive phenotypes that are internal they're not directly observed but they're also not like philosophical or metaphysical or phenomenological like um the motivational drive sorry i mean i think i'll just type i'll just type today since it's a little strange with the internet

But suffice to say that the phenomenological phenotypes, like the felt experience, is probably the most difficult because now you're not just talking about where the arm is in location.

You're not just talking about the intent to move the arm or something like that, but you're talking about the experience.

And that is the challenge that many researchers are focused on.


SPEAKER_00:
And hey, Daniel, I want to give you a heads up that your audio is a little choppy as well.

But I'm just going to type.

I'll just type.

Yeah, cool.

Super helpful.


UNKNOWN:
Thanks.


SPEAKER_00:
Great.

And then yeah, I'll just follow through with with those steps like after you specify those prior beliefs in step four.

step five, we solve for a posterior probability and model evidence.

So effectively what we're doing is we're just kind of continuing the process of like, how do we approximate our subjects understudies subjective model.

And so like, here are all the different components, right?

All the ones that we've been familiar with as we've proceeded through the textbooks we have, we're figuring out likelihood functions, we're figuring out

uh prior beliefs even if we're just setting them initially as like some value and and then need to kind of further carve out our model from there and maybe uh update those priors that to better reflect what's going on in the subjective model so um i think the word they repeatedly use this word recover like we're trying to recover the prior beliefs or recover the parameters of the model so there's no assumption that we know what these are from the onset

Kieran Stonewall- Instead we're we're attempting to build a model and then kind of fine tune things from there to where they better approximate what might be the actual.

Kieran Stonewall- priors and and the kind of likelihood and and model evidence and so on, that the agent is working with.


SPEAKER_03:
Anand Oswal, Ph.D.

: i'll just put one comment so so you're saying that if.

Suppose we found some empirical study and we get the exact value, not the exact value, but like a distribution, a Gaussian distribution of the prior, then that would be the best.

That would be very helpful, right?


SPEAKER_00:
Yeah, presumably.

Yeah, if you find that.


SPEAKER_03:
So basically we are trying to iterate our model so that we can get to those kind of values which have a very accurate description of what the prior values exactly might be.

yeah something on those lines right so some some reverse engineering.


SPEAKER_00:
yeah you're right that's very right like we can just think of it as like we've been introduced to be sort of in silico synthetic models like in, for example, a PO mdp we're already familiar at this point, thanks to chapter seven and other chapters like.

we'll hear all of the components of the POMDP.

We have our D vector, which is our priors over initial states.

We have our A matrix of likelihoods.

We have our B matrix of state transitions and so on.

But now, whenever we've collected empirical data, well, we only have what we as the, you know,

so-called objective scientists can observe so we can observe uh you know an agent's like a rats or human beings actions that they've taken and we can observe like maybe what was going on in the generative process like okay the mat the the rat was in a maze where the reward was on the left

So we have our knowns and givens and what we observe, but they don't all match all of these different components of the POMDP.

So we're attempting to kind of recover what's missing.

We ourselves are undergoing the active inference process of trying to infer and learn

what are the other components that we cannot directly observe of the rat's behavior?

Because in that way, of course, it starts to match anything else, some other machine learning routine or something.

Like we're just trying to find these unknown parameters in what we presume to be the rat's model, subjective model.


SPEAKER_03:
Yeah.

Thank you.


SPEAKER_00:
Sure.

Absolutely.

And then

Aside from being either introduced or given some kind of more technical refresher on what methods we might use along the way to do this, like they dedicate an entire section to variational plus, which kind of gives more description on how we're using this modified basis theorem.

Now, instead of with states and observations, we have actions and we have like model parameters.

Aside from how it reintroduces how we might recover parameters via the Laplace approximation, we also get a much more description

Utility of applying these methods in the first place, which is in step 6 of the, the overall process that's introduced here.

And so it's more into, you know, there's a lot of overlap between this and other more traditional.

analytic settings where you might be trying to do something like a covariance analysis or a correlation analysis.

You might be trying to, as I mentioned earlier with a T maze example where maybe a drug was administered to certain rats and not to others, let's do some kind of group level analysis to see what are the patterns in the behavior

uh and outcomes for the rats who were administered the drug versus those who were not right so this is getting into much more again like kind of traditional settings where really you're just applying analysis um but now you have the the added utility that you divide you've devised an entire like in this case pomdp that can potentially

like really strongly explain a rat's behavior and you have all these different components and parameters that you've recovered to allow for a quite broad variety of analyses beyond just running like a simple like t-test or yeah looking at correlation between certain behaviors and outcomes and that's all we have so

Yeah, that's the general process.

I think that figure 9.2 does a rather good job of trying to visualize the six-step process, collect data.

We build our own objective, our initial objective model of how we think that data is produced.

We derive like a likelihood function.

Step four, prior beliefs, we set those.

step five, we invert the model, which we're already familiar with doing previously in the book.

I think that's how you go from a model, an agent who has priors and likelihoods, and then those can be mapped through Bayes' theorem to a posterior and model evidence.

And so everything actually looks kind of structurally equivalent to that equation.

And finally, we move on to the final analysis.

Yeah, Prakash.


SPEAKER_03:
um actually I have a question so so what about if we if you are like we don't exactly follow the likelihood function the way they're given with the way they're giving uh but we uh introduce our own function which actually does the likelihood mapping like not a function or a methodology and uh on top of it so

So so actually there's a fundamental like we're using this Markovian processes to do the state transitions, right?

So we have flexibility that we use.

We what I'm actually trying to say is that this three this likelihood function right which starts from the policy level right?

Fundamentally down to the observation level and.

instead of following the exact mechanism which they give, we can introduce our own methods, but we do the mapping of transitions across states and the observation mapping, like the A matrix and the B matrix, we keep it very similar.

But the other, the higher levels, the higher levels, that can plausibly be changed also, right?

It's like we can apply a free energy principle to that also.

i'm just uh i'm just thinking on those lines i've not been able to formalize it very cleanly but uh i hope that in a few weeks time i'll have more clarity um so that's just what i was trying to understand you know fundamentally we do this um surprise minimization the the most core principle and then the transition mapping but the later above players like say policies and affordances we can actually

create a methodology which fundamentally does that, but not exactly to this very complex Bayesian mechanisms.

That's what I'm trying to say.

There can be other methods to accomplish those policies and affordances.


SPEAKER_01:
I'll see if I can if it will work.

It's a very interesting question.

And it makes me think that this is like a policy selection, but you're not actually choosing the policy.

But it's trying to develop a function that does map from all of the model and parameters to the choice of policy.

So it's kind of like policy selection from the outside.

And then we have the view from the inside view from the outside.

And maybe there are some other heuristics or rules that could be simplifying in terms of how we do the policy inference.


SPEAKER_03:
Sorry, actually, I couldn't hear.

But you're saying policy inference will be from inside, right?

Or you're saying it will be from outside?

I don't understand.


SPEAKER_01:
Well, high is usually how we think about policy selection from the inside.

But we're the behavioral experimenter.

We're only observing the you, the emitted behaviors.

So we're developing a model in step three that is the distribution of the emitted behavior given all of the parameters, many of which the subject doesn't have access to.


SPEAKER_00:
Daniel, just a quick heads up, your audio is still kind of choppy there.

But I think you're making the point that, yeah, like whenever we've collected the data, we have access ourselves to what were the the behavioral outcomes, i.e.

whenever we're looking at equation like the tilde you, that is a series of actions.

The tilde always denotes like a sequence and then you being an individual action.

And so with the policy, you can think of a policy as a series of actions like

we have a rat in a tea maze, action one, it takes a hint as to where the reward is.

Action two, it goes to where the reward is as told by the hint.

So so now we have, like, a number of actions in a sequence, like a time.

Step one.

It did this time.

Step two.

It did this, and that can be represented as the utilda, which could also be represented as an individual policy.

So a rat who really already thinks that the reward is on the left, it'll skip the hint.

It'll go right to that reward.

Right.

So we see a certain we can associate a certain sequence of actions with, you know, this this actual behavior that we see.

It's equivalent.

And then whenever we're looking at equation, it's like, OK, well, why did the rat jump right to the reward?

like what parameters are in the model that would push it into that kind of behavior rather than the kind of, you know, safer policy of going for the hint first.

And so that's what we're trying to kind of recover.

Like what are those parameters that are determining that?

given that we only know the actions that it took.

We don't know all of these underlying parameters and cognitive parameters or otherwise.

We don't necessarily know priors.

We don't necessarily know how much the rat

Another thing that we get introduced to in this book is whenever an agent has extremely strong priors, like a rat who just really, really wants the cheese and doesn't even really care.

It's not very risk averse, so it doesn't really care if it ends up choosing the wrong option and getting a small shock or something.

it'll run right to the cheese.

Those are all parameter settings that are determining that.

And those are the parameters settings that we're trying to recover in the model inversion process.

and that's why it's step five we get this like reverse uh view of that same equation instead of we have instead of having a sequence of actions given a parameter now we're trying to find what is that parameter given the actual actions that we that we observed that kind of tracks


SPEAKER_03:
I'm sorry, Andrew, a basic question.

Could you please tell those values?

What is it, p, mu, tilde?

That's the policy?

What is theta?

What is O and what is M?


SPEAKER_00:
Yes, for sure.

Theta are your model parameters.

It's sort of like a collection of all these different parameters that might be involved in the model, right?

a lot of notation in other areas and active inference and elsewhere it's like normally they would kind of put that in bold just to to denote like oh this is a collection of different things it's not just like a single value right so that might have been helpful if they used that um but but yeah so the theta are just these parameters that we're trying to recover about the model

M is just a symbol that throughout the book, sometimes you see an M being included in an equation, sometimes you don't.

But more often than not, it's implicit that M is almost always there.

M just denotes this is a model, like there's a model in here.

And so it has its own parameters.

It exists just point blank, like there is a model involved here.

And so we have to say, like, technically, all of these parameters are conditioned on the fact that we have a model.

Michael Prast- specified, maybe in a certain way, for example, that in might also carry along the idea that oh yeah this is specifically a model.

Michael Prast- In the form of appeal mdp versus a different kind of model so it's it's kind of there in part just as a reminder and that's why it's often left out because it's already implicit that it's always there and then, finally, the.

uh tilde o that is our sequence of observations and so with a rat like in a tea maze just as we had oh the rat

Like action one, the, the tilde U it first, it took the hint.

Then after that, it went for the reward.

We know that those are, are recorded actions that the rat took.

We also know the observations that is, oh, the rat, whenever it took the action of going to the hint, we know that it observed the hint.

Um, so we can track that action number two, it went for the reward and it

it successfully got it, it got the cheese.

The observation in that case at time step two is that the rat observed the reward.

So just as we have a sequence of actions, we also have an associated sequence of observations.

And those are kind of our two, those are the two things that we collect in empirical data

but which we now have like hopefully enough components to try and do this model inversion and extract those parameters that we cannot directly observe.

Is that helpful?


SPEAKER_03:
Uh, uh, sure.

Can I, can I just, uh, clarify what I understood?


SPEAKER_00:
Yeah, absolutely.


SPEAKER_03:
Uh, okay.

Sure.

So, so what they're saying is, so, okay.

M is the model, which we can think is think of like a configuration setting initially, which we pass on to the entire system, right?

Uh, like, because every model can have a different configuration setting.

Maybe that, that might serve what I'm saying.

Um, your, uh, O O tilde, right?

What is it?

O O bar.

Oh, bar is like your it's a tilde of observations.

Yeah.

And then this bar is like, it can be more than one observation, right?

It can be like a sequence of observations, but what is the length of the what is the what is the like the slot of the sequence, right?


SPEAKER_00:
So we do technically know that like in a teammates example, like based on your yeah, we have a certain number of time steps.

So it's like because that's what the POMDP is about.

It's about discrete time steps.

So it's like time step one, the rat starts in the center.

Like it's done nothing, there are no new observations necessarily, or maybe we say it didn't observe anything.

It's just time step one, there it is.

Time step two, it then goes to get the hint, it observes the hint.

So now at time step two, we're able to record, here's the action it took, and here is the observation it had observed the hint.

Then time step three, it decides to move to the left arm of the maze.

And so we know that its action it took, the U, is that it moved left and then it observed a reward.

And so time step three's O is it observed a reward.

So you can imagine that it's like two vectors almost of like, here's time step one.

Here's time step two.

Here's time step three.

Here's the vector of actions it took from time step one, two, three.

Here are the observations it had at time steps one, two, three.


SPEAKER_03:
The trees are typical over here.


SPEAKER_00:
yeah and in this case it's it's been done in different ways uh for the teammates example for example and it's it's really useful because you can do things like try and study the the so-called explore exploit trade-off so imagine you you allow for four time steps um it's where the rat starts and then it can choose to take the hint or not and then it can go to the reward well

If you have four time steps, if the rat skips the hint and goes straight to the reward, well, now that you have that additional time step, the rat can spend more time with the reward.

And so it's like, you know, if you think about it in terms of it, like maximizing its utility or reward, it's like, oh, it gets to spend a whole extra time step with that cheese.

Like that's, you know, great.

It skipped the explore behavior, went straight to the exploit behavior to get as much cheese as it could.

but then you can use that to try and study like oh this is a very like risk-taking rat right so it chose to do that uh versus other rats who might be more uh conservative and like go for the the risk uh excuse me go for the hint first even though they're giving up that additional time step that they could spend getting like spending more time with the reward

And then finally, you could also have very risk averse rats who like, you know, maybe they got shocked a couple of times and they have preferences to really not get shocked that that those negative consequences in terms of its preference.

Yeah.

Highly outweigh its preference for the reward, in which case you might have a rat who just stays in the center of the entire time or it might go to the hint as exploratory behavior to gather more info.

It might go to the hint and not even trust it, depending on how you set up the situation, and it might stay at the hint and never go for the reward.

It just really wants to know.

It's, you know, so it's as if it's so about this sort of way because it's a real animal, but like maybe it's so scared that it just keeps attempting to gather more info and never goes for the exploit behavior.

So hopefully that's maybe kind of an illustration of like

Matt Kallroos- You know, we can choose the number of time steps whenever it comes to discrete time models like this, so we can easily just index like time step one time step two cetera.

Was that helpful at all, Prakash?

I might have been over-descriptive with the tea maze example.


SPEAKER_03:
Sorry, actually, my internet got blocked out also.

I had to reconnect.

But actually, I think I get the picture.

I like this model example.


SPEAKER_00:
Yeah, I think it's a worthwhile one, just because this chapter spends time on it.

And that's kind of that figure 9.2.

It's like, here's the series of steps you would take if you were doing the teammates example.

So it's like for anything that still seems ambiguous or uncertain.

Hopefully, I'm giving enough info that you can come back to the chapter and see like, okay, I get it now with the teammates example.

Francis, you had a question?


SPEAKER_02:
yeah um so that was really interesting and really helpful thank you um am i this may be a kind of blindingly obvious uh question is the focus of this book or indeed of the active inference program in the general sense more about understanding active inference

how this exists, rather than say developing active inference as a generic AI technology to solve issues where we're not necessarily, the model, the generative model is not necessarily active inference related.


SPEAKER_00:
Yeah, I think that's a really interesting question.

And it might be somewhere between, an answer to that might be somewhere between more objective versus highly opinionated.

But I'll, so I'll give my answer, which is, as far as I'm aware and looking at where, you know,

active inference can be applied to just kind of a ton of an immense amount of phenomena.

Like here we're given chapter nine, applying active inference to empirical data that we're collecting.

And so we're looking at like actual agents.

active inference has many of its foundations and the work of Carl Frist and others who are coming from a neurobiological background.

And so there has been kind of a big push initially to use this for those fields and computational psychiatry and so on.

And we get a variety of examples of how that's been done in this textbook itself, in this chapter, like table 9.1, they provide a

an entire annotated bibliography of all these different works on delusions and pharmacotherapy interventions and so on.

But that said, you could build models that are entirely in silico, that don't have to do with analyzing animals or human beings.

Like on Thursday mornings, we were using RxInfer, which is a package developed in Julia.

to try and build in silico experiments.

And so what I'm saying with that is you could use these tools to build AI or otherwise different kinds of behavioral systems that you could try and employ in the physical world.

Like you could build a robot that has its own sensory modalities.

It can see visually what's going on and then try and infer what kind of actions it should take

realism's preference and of course like if we build a robot unless you're trying to actually emulate a human being or a rat probably not you're probably building it to add to our reality and find some kind of new function in society or otherwise right in that case no and it would be more about like we're starting with make you know simulating something on a computer that maybe one day can be realized in the form of like a physical machine

um so yeah it's kind of like these two worlds right it's like we can build active inference models to try and better understand the world around us or we can build models that we can then actually employ you know in the form of ai and in society for whatever your use case is um

yeah and and so you'll see a lot more literature that's been published including since the textbook that lends itself more to the latter approach that i described but again of course this textbook is co-written with friston and other folks and with a neurobiological background so i think if you really want to rely just on this textbook you probably will get much more of the flavor the kind of

Yeah, psychological, psychiatric, biological flavoring of active inference.

I hope that that was a helpful kind of opinion that I shared there.


SPEAKER_02:
Yeah, thanks.

I think that was very helpful to clarify what I was feeling my way towards.


SPEAKER_00:
Thanks.

Absolutely.

Yeah, Prakash, question.


SPEAKER_03:
Hey, hi, Andrew.

Since you mentioned about this Julia thing, right, and I was planning to join, but I've not been joining because I think I will start joining maybe the week after next, after I complete a few things.

But I have a question.

So I have never used Julia before, but I'm seeing that it's quite powerful and it's very fast, right?

That's the most, it's like it's, quite people are using it, but I'm also wondering since, does it have integrations with suppose say to tools like Neo4j and has anybody worked on that?

Like I was just seeing that they have a driver called bold.jl.

which does this, I don't know, I don't want to explore so much, it'd be crazy.

But what I was thinking is if, if we have like, okay, well, let me let me make the question a little bit more simpler.

Have you guys worked on any kind of?

Is it entirely within the Julie more do also do some kind of integrations?

Suppose?


SPEAKER_00:
Sure.


SPEAKER_03:
Because in Python, if I use Python, for example, and I want to use other platforms also, the integrations are much more, there are good libraries which will help us.


SPEAKER_00:
But over here, it will require a little bit more level of... We've not jumped into this knowing, for example, Julia and inside and out.

Or anything, actually.

a PhD student who's working directly with the people who developed RxInfer.

So we do have like a little bit of support from someone who's been directly involved in building out the package and resources.

But the majority of us have not even coded in Julia before.

So it's primarily like many of us just kind of learning this together and we're going through,

Another nice thing, RxInfer has their website where they've provided a series of different coding examples for different use cases.

We did one that follows

a Roomba, like a small machine that cleans your home and trying to model like an inference process so that it can derive like a likelihood of like, oh, if I'm on top of tiles, I must be in the bathroom.

If I'm seeing a hardwood floor, I must be in the living room.

So we just get these nice real-world use cases where we get to follow along and figure out how the code is functioning there.

More specifically, getting back to your question on integration, I'm still learning that myself, so I can't give a terribly holistic answer, but what I can say is that

anytime we've uh run through any of these examples for the most part we're really just using like uh an updated baseline you know julia like install that on your machine pull it up we use a nice visual editor called vs code if you may or may not already be familiar with that and then um

In terms of just bringing in the packages we're using, for the most part, it's just RxInfer.

It's not a ton of other, well,

Okay, to be fair, that has its own dependencies as well.

But yeah, RxInfer aims to be a kind of holistic, like this is the one package you have to pull in, and then maybe you pull in like an additional like nice like plotting package so you can like visualize the results of the inference process or otherwise that come out of RxInfer.

But yeah, I think RxInfer aims to be a rather

standalone asterisk package for building active inference models, both discrete time, like POMDPs, or even just not even going that far.

And MDP depends on your use case, as well as continuous models.

And yeah, it's quite strong.

as far as previous approaches to active inference like software wise we have matlab uh which has been more geared toward around uh building discrete time models and uh python where the strongest like pre-established package for that that being uh pi

pymdp developed by uh connor heinz and others that's almost entirely like exclusive to discrete time steps um so julia yeah something about the combination of computational efficiency with like other developers who are creating these new kind of holistic packages for just trying to

develop everything in one go in a single script, that's able to handle continuous processes much better and kind of more holistically as well.

So I don't know.

That was a lot, but I hope that was helpful.


SPEAKER_03:
Just a follow-up question on Julia.

like what you're doing are you like trying to say create some functions which will say do a thing like say minimize surprise or say calculate variation free energy for you guys like just create a function which will do that are we think on those lines also


SPEAKER_00:
yeah presumably you could define your own um i mean what's really interesting to me about julia versus like python is that that you can actually like type in a theta and other symbols into the code itself as stepping to write it out and so what i what i mean by bringing that up is that like you could certainly um write your own sort of objective functions uh to to

Gareth J. Energy minimization what's Nice is that arcs and for like some of its coding examples very specifically relate to have active in France, like it is explicit like an active model of building.

There are functions like they have this infer parentheses function where you plug in, here's your model that you've just built, here's your data, simulate the behavior, and then they have a single argument for free energy, true or false.

If you set it to true, it will minimize free energy.

It's all in-house, out of the box, if you want to do free energy minimization.

there's on their website they also have documentation for like is how they like within that function

is the free that's being minimized, because at this point we have a few different kinds of free energy and formulations.

Like in this textbook, for example, we're already introduced to the difference between variational free energy and expected energy.

And since then, there's been more research into other forms of

free energy they're highly relatable uh but nonetheless like for example trying to combine like how do we put together variational free energy and expected free energy into maybe an even more concise single formulation and with things like generalized energy another one

I'll be able to pronounce correctly, but I think it's birth of energy or Beth energy free energy and that's that we used in this arcs and for package.

So yeah.

summing it back up.

There are ready to go like out of the box free energy objective functions in the package, but you could.

define them yourself as well, if you so chose.

And Daniel's making a great point with the notes.

Another benefit of the arts and fur approach, because I would say it's not just a package, but there's a lot of documentation on how it's its own approach.

highly relatable to the textbook we're going through and nonetheless it kind of makes it attempts to make some kind of advances on on how to do active inference so we end up with these things called constrained 40 factor graphs um we already see all of these different factor graphs in the textbook we're going through here um you know the way the

POMDP looks with its nodes and edges, and here's to visualize how it works.

RxInfer is doing the same thing, but it's attempting to make each node kind of self-contained almost, such that you just account for its connections with other nodes, and it will locally minimize its own free energy.

In that case, rather than looking at a totality of the system, minimizing some broader free energy, now you can look at it as each node in this graph is minimizing its own free energy based on its local connections only.


SPEAKER_03:
What's it called?


SPEAKER_00:
This allows... What is it called?


SPEAKER_03:
What are you saying?

All right, just what you said.

What is it?

What is it called?

What's the name?


SPEAKER_00:
Yes, the constrained 40 factor graph.

Oh, yeah.

And Daniel said the CFFG.

Yeah, yeah, for sure.

And then you'll you'll see like what's nice.

Another thing is that in each of the

um example like so if you go to the rx infer website which um you know we've put links to that uh elsewhere in the coda but uh a simple google search of rx infer will will take you right there as well um each of the examples will cite uh related paper so like here here's our code example we've kept it really concise for you but if you need more information on

the simulation we're doing, the components, what the CFFG looks like for this, you can just go to the reference paper that relates to it.

So part of the pull here for doing this project is that we are also giving feedback on the documentation for the package since we have this kind of direct connection with the developers themselves.

But so we've really been kind of scrutinizing the documentation a bit

check out cffg because the whole idea is that altitude you could you could go far beyond the kind of pomdps we're seeing here uh in the textbook with a kind of a standard recipe for building those but um a cffg because each of the nose nodes is uh minimizing free energy locally you could end up building

In principle, whatever kind of model you want, you could make it highly complex.

All you have to do is specify here the nodes connects.

And it uses message passing algorithms underneath to transfer all the messages between them.

But yeah, you could make any kind of arbitrary model

uh basically and and like derive like free energy minimization for each of those nodes and then you could potentially sum those up and look at like how the overall system is doing and such things like that but uh yeah in the interest of time yeah i would just recommend looking more into the rx and per package and it sounds like something that might might interest you and then also we are posting in the coda like uh our notes and and what's coming up in the next meeting


SPEAKER_02:
But the other option, the active inference as a direct, the startups using active inference as a direct AI approach, rather than just using active inference, rather than using active inference to simply understand systems which understand.


SPEAKER_00:
Yeah.

Yeah, no, absolutely.

I think the two kind of approaches come together too, right?

Because so much of this is based upon neuroscience research towards, you can almost think of it as one of them is aiding the other.

If we can learn how biological

organisms learn, for example, that gives ideas of how to formulate a model that itself can learn, and then moving from that to building a pure AI that's just using many of those principles we've derived for generating its own behavior, despite the fact that it itself is not a rat or a human being or some such thing.

But yeah, please, absolutely feel free to add to the code if you have questions or notes or thoughts.

thanks for that um so i think we've we've about thank you andrew sure i'll stop recording
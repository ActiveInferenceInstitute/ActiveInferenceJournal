SPEAKER_00:
All right.

Hello, everyone.

This is the first meeting of cohort five of the textbook group on September 19th, 2023.

And thanks everybody for saying hi.

I will share my screen and we will be looking at the cohort five live meeting page.

So this is kind of like the starting place for participating or joining cohort five.

You can always click on somebody's icon on the top right of CODA and jump to the exact page they're at if it's not clear from looking at the sidebar.

So here we are in the first meeting.

And after today, the recording will be here, just like in cohort four, there's recorded videos.

So I guess there's many places to begin, but is there any question that anybody wants to ask about any aspect of the textbook group?

How do we even start and evolve it each time and, and

make it fit for everybody who's doing this on their own time and who's coming at it from different angles?

Varun and anyone else, you can raise your hand or however.


SPEAKER_04:
Well, just a quick question.

Does the textbook include going through some of those extra notebooks on like active inference coding, like accurate from scratch?

or some of the programmatic implementations?

Or does this mainly stay only on the theory?


SPEAKER_00:
Good question.

The textbook itself, all of the code is in MATLAB.

It's in Appendix C.

And it is not a major part of the textbook itself.

However, in the textbook group, people are bringing resources and other references.

In the page called code, you'll find several dozen implementations of active inference code.

And we can certainly arrange sessions like just us informally or trying to have the authors or other experts join to look over some code features if people are interested in this.

But it's not a necessary or central part of the textbook.

However, because the second half of the textbook is oriented towards application, especially, then talking about how you go from like,

a sketch of a system of interest and then use the recipe in chapter six towards actually designing the generative model.

So for people who are ready and wanting to actually be working in code, then it's there and we can work on it.

Esmail?


SPEAKER_02:
Yeah, I ask question.

I want to know clearly about one

very important term, surprise, and relationship with free energy.

That can help me for good understanding this textbook.

I think these two terms are so important.

Surprise?

Yeah.


SPEAKER_00:
Yes, great question.

many questions will arise um about all kinds of terms and about relationships so here's just one way that that this can be approached so I went to the questions page and in the search box I'll type in surprise now this is going to bring up a lot of questions because it's a very common um term

but um we could review the questions that have been asked and see if if one of them directly asks what you're looking for um and if not then it's like a huge act of service to add the question

because it's something that we're all curious about and so this is how we kind of build our repertoire of asking questions and also everyone is always welcome on their own time to go into the answers and discourse section and um develop the answer whether by adding material or by reorganizing shortly to answer your question directly

surprise is about how expected or unexpected a given sensory observation is so if you were least surprised by a sensory observation you would have had like the most accurate belief and so surprise minimization is equivalent to having the model with the maximum evidence

Surprise in and of itself, though, is often very hard to calculate.

And so instead of calculating surprise directly, a tractable, optimizable bound on surprise is used, and that's called free energy.

So that's definitely one of the key pieces, which is like if we had the minimum surprise model, we'd have the maximum evidence model.

In the exact bayesian case that would be totally fine, however, in the high dimensional setting it's not tractable to compute surprise, and so we have this optimizable heuristic.

With free energy and reducing the kale divergence.

Susan.


SPEAKER_01:
Yeah, I'm particularly interested in actually mapping context.

So is there a place that maybe scenarios are stored that have been mapped or could be mapped?


SPEAKER_00:
It's a great question there.

If it comes up in the context of a chapter, you can always, beneath the questions in each chapter page, definitely people have talked about scenarios.

And then also, I will just add it to the chapter one notes here, but in the Active Block Friends project,

we have several dozen settings for cognitive modeling that people have worked to map.

So I'll just add this link for settings for cognitive modeling.


SPEAKER_01:
Right.

Thank you.


SPEAKER_00:
Cool.

Anyone else have any just random thoughts where we can explore a few other pieces?


SPEAKER_05:
Yeah, I,

I'm wondering, given that you've now run several cohorts for the textbook group, and so the knowledge base has already been established, but you've had a few chances to try this out.

I'm wondering what you're hoping each cohort will bring to

either to the knowledge base, or how do you see each cohort involving the study of these topics?


SPEAKER_00:
Good question.

Definitely remains to be enacted.

I'll just give thoughts, but not in any ranked or fine-tuned version.

First, I hope that everyone who comes into this space synchronously or asynchronously can

find epistemic and pragmatic value and learn about what's meaningful to them and be really in the game in the arena with these topics as they're being developed that would be a great first thing and then for people who want to pick up litter in the environment and paint a nice wall and add a little bit of information on their way it's

unbelievably helpful for people to take an active stance towards their engagement with the coda it can look like there's a lot but it's not even a thimble in the ocean and so people who really take an active stance towards looking at the material and where there's a question that doesn't have an answer just throw something out there and when there's a question that has a bunch of information

just delete an empty row.

There's some low hanging fruit that simply inevitably improves the experience for everybody.

There's always increasing amounts of augmented intelligence tools and language models, including with Coda that we can play with and so on.

So even just adding small amounts of information or resources that people find relevant, adding new questions, just helping

enact and embody that there isn't a final state of the knowledge base again this is like a larger shared document than any of us would have made by ourself on this textbook but there's so many more directions to go so I'm always kind of looking and speaking towards people who see affordances to improve what we have here

and just do different things with it and there's just there's there's so so many ways to go there's so many unstructured notes for every page there's and again there's fun things to do with Coda AI there's fun things to do with synthesis with linking to other work

in future textbook groups page.

There's there's hundreds of lines of notes on different thoughts that people have had about how to make the material more accessible and applicable and rigorous.

So just

enjoying it wherever you're at as you're going through the group.

And then if people would like to rejoin for future cohorts for this textbook group or for other projects at the Institute and do more, that's great.


SPEAKER_05:
Excellent.

Thank you.

As a follow up, is there anywhere

uh either on the coda or through some email link where people can register themselves publicly uh like a little database of people who are participating in the

a textbook group and maybe as a way to advertise that I'm I'm participating in this and and if anyone else wants to reach out to me um because I'm working on something that maybe you find interesting here's my contact sure um so one option for that is the projects page here's is a kind of project based


SPEAKER_00:
Another option, I'll just make a, I'll put it on this overview page.

I'll just make a table for a cohort of five people.

So if people want to add their own information, they can.

So people can at the bottom of the cohort five, this is always a fun plot to look at as well.

On the X axis, it's people's like self-evaluated familiarity with active inference from not familiar to very familiar.

And then the Y axis is the computer science, biology, and math familiarity.

it's all over I guess one of the only trends that I see is that very few people say that they have higher than 80 familiarity with active of course all these self-evaluation numbers who knows what they really mean but um this reflects our textbook group through time so

People have just different backgrounds in math and computer science, biology, and all these things, philosophy that weave together.

So just staying in the game and where there's uncertainty, like externalizing it, asking it, will keep your cognitive load light and contribute to open science.

So that's huge and fun.


SPEAKER_05:
Great.

Thank you.


SPEAKER_00:
Any other thoughts or questions?


SPEAKER_03:
I have one question regarding the structure of these text groups.

So you have multiple of these cohorts.

Are there any particular structure in terms of like organizing, for instance, these weekly meetings?

Like what to do during this time?

Was it effective?

Or is that basically up to us to decide on like, how to proceed?


SPEAKER_00:
I'm highly open to people who want to explore different formats.

You can look at every single meeting to see exactly how it did happen, but that's not how it has to happen.

It's not like we're locked in.

It's just that we wanted to hit the ground running as soon as the textbook was available in 2022 and then just keep this opportunity around.

usually the way that we well earlier on we were just simply grasping with the textbook and so we would often scan through the chapter like during the meetings so just working through the chapter and talking through the chapter um where I would really love to

see it is like people look at what chapter we're reading in the coming week so it's alternating times so usually not everyone can go to both on the given chapter um and also the at the like you can go to cohort four as well if you just want to go to you know there's people are retaking it we're all just taking multiple times um so when the discussion starts

And again, this can evolve, but here's just one way that we can hear a lot of voices and get content and have like spontaneous things too.

We'll go to chapter one, for example, and I'll specifically look at the cohort five questions.

It's all one underlying big table, just different views.

This is just to make sure that if there's a new question that cohort five asks that we'll be able to get to it.

But like, let's just look at cohort four, you know.

So then start with the questions, respect the effort that the person put in ahead of time to surface their curiosity, whether they're at the meeting or not, and try to go through the questions that are asked about the text.

And then if people have curiosities arising, like during the meeting, oh, I'm wondering now, I'm wondering about 4.3.

It's like, just type it out.

Like if it's typed out, it's forever.

And then we have a place for the discourse.

If it's just shot from the hip, it dissipates.

Now we still, it doesn't dissipate nowhere because we still can record it.

And obviously we publish the transcripts of these textbook groups and so on.

But like, as curiosities arise, if you add them to the questions table, um, and then we can structure the discussions with just seeing with what's interesting.

going to questions that have already been asked in the in um certain settings and other times going to new questions and then in the time between the meetings just people hopefully putting in the time to increase their situational awareness of the coda itself read through the textbook it's many coats of paint situation so

there are parts that are going to be very dense with the use of the active inference ontology or very technical or things like that so just time boxing and staying in the game and then yeah what is the most effective synchronous meeting strategy that's a good question and then also we can make kind of supplemental meetings for example commonly some people want to do a math learning group

that is like some people want to focus more on the background and like the basic math.

like reading equations bayesian statistics and then other people want to do um work on like more of like connecting the dots with with the difference expressions so if people want to do more math if they want to do more programming if they want to be developing their project you know don't don't let the institute be the rate limiting step

This is all up to people in their own learning journey and connecting to people here.

And if you stay in the game over the coming three months, you will get a lot more familiar with active inference, even just with one hour per week.

If you do more than one hour per week, you will get a lot more familiar with active inference.

It's not all in the textbook.

in this one 2022 short textbook.

But this is a great landmark for the field as the first textbook, obviously.

And it allows us to reference out to a lot of other cutting edge work.

so the discussions are great for that because we can kind of like be like oh yeah there's this like live stream happening tomorrow or here's what this person said last week and so it lets us kind of like look at how it was stated in 2022 while also break bringing it closer to what's ongoing in the field and if people you know if in future cohorts if people want to be a facilitator it doesn't require technical expertise in active inference to be a facilitator

That's an option.

There's many, many options.


SPEAKER_03:
Thank you for your detailed explanation.

I think, yeah, I do get that there's plenty of room for trying to make this a really fruitful journey.

Yeah.

Thanks.


SPEAKER_01:
I've got a question.

Where do you see innovative applications

may or may not be utilizing active inference that might signal us to kind of look at what is coming out, whether that be AI or augmented negotiation or social network analysis.

Is there any particular cutting edge areas that are starting to get attention that would be interesting to coalesce around?


SPEAKER_00:
It's a good question.

A lot of applications happen in the industrial setting, so they're not always shared.

like with the full methods and details that we would expect from like a scientific paper or like an open science educational project.

So I think that's one fundamental unknown part.

At least in the learning setting, there's a lot of exciting things with connecting innovative ways of modeling

different systems, like we're seeing an active inference, and then the application work remains.

Active inference is a very young field.

And so things that might be expected in a more mature field are not commonplace at active inference.

And rather than interpret them as a dismal state of affairs, people who find that exciting and want to contribute there will find that even some pretty conventional work can actually have very impactful outcomes.

I'll just link a few things.

So here in the bottom, we have the active inference ontology.

So this is one of the key features of what we develop at the Institute.

which is like, we're speaking in English right now, but to a large extent, you could think of this as being a dialect of English, which is the active inference ontology.

It's also been translated into many languages by experts.

And it's a project that we work on every week with basically how to utilize the active inference ontology.

And also I'll add to this page, the knowledge engineering link,

we've then used the ontology to analyze the literature and look at how terms change in their use per year but overall um let's see if which plot um is relevant here but overall um we are talking about the number of papers being in the low hundreds over the last

10 to 15 years most of them happening more recently many of them happening through Carl Friston at all but that also being increasingly decentralized so just keep the finiteness in mind and look for the affordances to contribute because there are many many

um so let's look at just the overall textbook oh yeah we're just in the kind of the dot zero the the background and just talking about the group itself chapter one and chapter 10 are textual overviews they have no equations they're like kind of like bookends on the book

chapter two and and again in our between now and December we're going to be going through just chapters one through five but if you want to be also going to cohort four and going through six or ten you can't um and the authors very clearly say the first half of the book is like epistemic it's about learning active inference and so take the time to really learn what active inference is it will make applying it

easier when you understand what it really is.

And then chapter 6 through 10 are exactly about the recipe of how you apply it.

Now, it's not at the hold your hand guidebook stage yet, but we connect a lot of the dots here, and this is the direction that we're going in.

Chapter 1 gives an overview, and it contextualizes active inference in relationship to a lot of other areas that people are interested in that they study.

chapter two and chapter three are the low road and the high road to active inference we're going to talk about this like a ton but here's figure 1.2 here's the low road and the high road and they both meet an active inference the low road asks how and it answers it with Bayesian statistics and the high road asks why

and the answer is basically self-observation self-organization self-persistence and so active inference brings together this persistence imperative of the free energy principle high road why with a Bayesian statistical implementation approach that's the low road so that's chapters two and three the low road how and then the high road why and then

we get to the essence of active inference, which is the generative model.

The generative model of active inference is like what is being built.

This is what you have in hand when you have an active inference model.

No generative model, just speculative.

With generative model, you're talking about something specific.

And so chapter four goes into some of the technical details about what these generative models are

building on some of the precursor steps that were covered in the low road and the high road.

Chapter four, generative models.

Chapter five is going to focus on one of the areas that has the most empirical evidence in the most well-studied area, which is mammalian neuroscience.

So as much of this work arose from human neuroimaging in the 1990s and early 2000s, there's a lot of work on human cognitive and behavioral science.

And so that's the first half of the book.

context how and why what and then where has it really been applied the most so far but as we look at um every single day people are expanding the scope of active inference applications every single day but the area it's been most studied is in mammalian neuroscience that's the first half of the book

it hits hard.

that's where we're going to get to by december and then just briefly chapter six is a recipe for designing active inference models so it's kind of like a procedure and a set of questions that you can ask yourself as you're designing a generative model which is an important rite of passage in people's work in this area they'll talk about that in chapter 10. chapter 7 and 8 focus on two big families of generative models

which is generative models that treat time in a discrete setting, like click, click, click, click, and then models that treat time continuously like a flow.

Chapter nine looks at the integration of empirical data with generative models, model-based data analysis.

And then chapter 10, like chapter one, provides context and revisits a lot of the more conceptual questions given what has been covered.

So that's the main outline of the book.

And those are the places that we'll go.

But also, because the questions take us everywhere.

And people always raise questions at many, many levels.

So if there's an equation or if there's some section that doesn't make sense or it's not what you would have written or you don't understand how they got from here to there, that's totally fine.

I really encourage you just to continue working through it and just pass through the parts lightly that you don't.

feel expert on in the moment of your first reading or your 100th reading.

Any other thoughts or questions?

But yeah, it should be really fun.

We are going to

head into for the coming two weeks into chapter one.

So you can ask in the chapter one questions, you can just add them here into the cohort five section, then they'll be tagged with cohort five.

You can go to just the questions overall, like chapter one questions overall,

And it would be super, super helpful if you upvote the questions that you find interesting.

That helps prioritize them for the discussion.

And then also, once we have questions, like maybe 10 or more or some number of upvotes, and then we feel like there's some questions where it's a very open discourse, but there's other questions where it's kind of like there is an answer.

And in those situations, we can then go from...

the question to the curated thoughts on the answer and then we can translate that into different languages we can make short videos we can make more bite-sized materials I mean there's just many ways we can go from there but like this is this is the real work surfacing what people with different backgrounds and curiosities are curious about

respecting and encouraging all such voices and then doing what we can to coordinate and and do what we can with the answers not to shut the door because some of these topics are are going to be just huge open intersubjective questions or it will just gesture to a whole area of philosophy or some other area of practice

but there's been a lot of great questions so far so there's a lot of value in reading what people have written over the years and reviewing the discourse and again hopefully modifying it if you're not sure you can always like select text and then click this little arrow and do like a you know oh I don't think so if you want to you can always do a suggest change

or you could add a comment you'd say oh not sure about this shouldn't that were to be that but if you're just adding a reference or adding connecting the dots or reorganizing the material that already exists just the more the better the more eyes that are looking at it and and um taking a deep breath and trying to evaluate it and and

interrogating it in the moment that is so huge and if it doesn't make sense don't assume that it just made sense to someone else and that you just don't get it just if it doesn't make sense then that's your opening to ask that question to write the follow-up

And this is a short video with Ali, who's not here now, but he'll probably join us some future times.

If you listen to the recordings, you'll quickly become a fan of Ali's gentle scholarship.

But yeah, we'll go into chapter one.

and we'll have the two weeks to discuss it which whichever one or both people join at again chapter one's a very um pros based chapter it's not equation heavy um it brings up huge ideas though and so then we'll just cruise through the discussion

Oh, chat GPT.

Yeah.

Oh, one here in the equation section.

So we translated all of the equations into LaTeX plain text.

And then here's using Coda AI to explain to high schoolers, write pseudocode, explain the variables, give philosophy questions, translate into different languages.

Code AI, I believe currently uses GPT 3.5.

But cert and then also perplexity.ai, you can upload a file.

So I often have just uploaded the whole textbook and had discourse on the knowledge engineering project.

Again, if people are interested in this, we are curating large amounts of the active inference information environment into fine-tuned language models.

with code and with um visual recognition eventually and some other features so there's a lot of ways to go so that's why we just stay in the game and have fun okay any last comments yes it is exciting thank you Susan

All right.

I will stop the recording, but thank you, everybody, till next time.


SPEAKER_05:
Thank you very much, Daniel.

Bye.

I really appreciate all this work that you're doing.

I'm looking forward to the next meetings.


SPEAKER_00:
Thank you.

Thank you.


SPEAKER_01:
See you all.

Thank you.
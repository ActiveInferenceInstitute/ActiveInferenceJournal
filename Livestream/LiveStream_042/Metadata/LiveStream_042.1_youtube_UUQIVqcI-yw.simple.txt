SPEAKER_01:
some questions that we had from the dot zero, but also new questions arising.

So we'll just start with an introduction and saying hello, and then it'll be awesome to hear the author's initial contextualization.

So I'm Daniel, I'm a researcher in California, and as explored a little bit in the dot zero with Sid, there's a few places that we were super curious about ranging from how does one go from a phenomena that they want to model like navigation to the analytical

equations and how is that related to active inference and the free energy principle?

And then how do we go from that analytical formulation to the actual gigabyte of video data and the real-time sensing and action?

So a lot of exciting topics and I'll pass to Blip.


SPEAKER_00:
Hi, I'm blue.

I'm a researcher in New Mexico.

And this paper was kind of way over my head.

For a lot of reasons.

It's just so far out of what I normally do.

But I have some questions related to the hierarchical modeling and how that is kind of used and how that can be related or cannot be related to maybe biological systems that model in a hierarchical way similarly.

And I'll pass it off to Adam, I guess, if you want to start or introduce yourself and you guys can take it away.


SPEAKER_05:
Hi, I'm Adam.

I'm a systems neuroscientist who's currently a research fellow at the Johns Hopkins Center for Psychedelic and Consciousness Research.

I've been collaborating with Tim and Ozen and others on trying to understand the natures of navigation.

and the ways in which the hippocampal and entorhinal system might be relevant, and the ways in which this could be used as a means of understanding thought more generally.

And it's been awesome and slightly intimidating, except Tim is awesome, so that's made it less intimidating.


SPEAKER_02:
So that brings it to me.

I'm Tim.

I'm a computer scientist and roboticist at Ghent University.

We haven't subscribed to the Active Infants formalism to try to make our robots slightly more intelligent.

This paper was, I think, one of our

major accomplishments last year.

It was a collaboration with Adam where we basically look at the problem of navigation from different angles ranging from active inference modeling to an actual implementation on a real robot.

and at the same time looking at how could this work in in an actual brain or or whatever the relations uh and the synergy is there so it's really exciting uh to to have this paper out but also this was just as getting started i'd say it was it was basically our first um

artifact that we published we have another one together with adam in the works and and yeah i'm pretty sure that we will continue this endeavor for for quite some time longer so we're all just only just getting started so uh that being said probably a lot of the questions are also still questions for us so we'll see uh to what extent we can give give an answer uh as of today already


SPEAKER_01:
Awesome.

Well, we can start with this sort of triad that is highlighted in the paper.

And Adam, you said it as understanding the basis of navigation.

And we have these three legs to the stool, kind of three threads that intertwine in the paper with analytical formulation of navigation, which is what Blue mentioned with a hierarchical generative model, and the implementation in robotic and in biological systems.

So maybe a first question is, how has this...

intersection of modeling navigation with robotics and biology been conceptualized and what brought you to apply active inference in this way and what does it add we should go first


SPEAKER_02:
So maybe my perspective here is that we basically got started from a robotics perspective, like looking at the navigation problem and looking at the ways that it's being solved right now and implemented right now.

And at the same time, we already had quite some experience with Active Inference, but more like

on yeah simple problems let's say our first um our first experiments were more on this kind of mountain car environment or a simulated car racer but it was always our goal to kind of try to scale it up to um yeah to real world robots and so

we got started by drawing and seeing parallels between current implementations out there that seem to match with how we would look at it from an active inference perspective and kind of work our way from both ways so on the one hand try to get some implementation running and on the other hand seeing how we could formalize it and actually make the math work in the active inference framework

and what we found out was that actually the the best matching implementation from a robotics perspective was actually one that was quite bio-inspired so it was basically the the red slam model from milford at all

where they had these heavy inspiration from grid cells, place cells, and these things.

And yeah, of course, it was not really a surprise given the fact how active internet is also grounded in neuroscience.

And so that's where we reached out to Adam to basically show off like, OK, this is our trajectory.

This is our plan.

This is our current implementation and how we think the math could work.

And then Adam, with his infinite knowledge of how the

It could work in the brain.

He gave like all kind of pointers like, OK, but this is interesting because what you implemented here is actually looking a lot like this kind of system.

And what you're doing here is actually very much like these kind of things.

And then we just said, OK, let's let's begin this further.

And we basically started on carving out

ping-ponging like, okay, if this is how it's done in the brain, then maybe we can make something more similar in the mouth or in the implementation, vice versa.

And the paper is basically a first step where we had, like,

the three bits kind of in place, but we know where the open issues are.

But it actually has something working on and sensible on all three levels, I'd say.

And that's where we are at at this moment.


SPEAKER_01:
Awesome.

Thank you, Adam.

Anything you add to that?


SPEAKER_05:
I guess what I would add is I'm increasingly compelled by the idea that robotics as a touchstone and basis for understanding different aspects of mind is like it might be the most powerful kind of grounded empiricism we can do.

And I think it's both because it's like the, like a lot of times, like that Feynman quote of like, what I cannot build, I cannot understand is like mentioned, but this is actually building systems that, yeah, this is that, that, that have to work.

And, um, and these problems like in going through them and looking at the different solutions, like the kind of understanding you get, I think is really, uh, unique.

Um, but also I think it's particularly apps because of what our minds, other than control cybernetic control systems for bodies that have to move through the world.

And so this idea that like the way roboticists are approaching it, like it's.

And that is.

the exact problem that nature had to solve, that natural selection had to solve.

Now the question is, was it the same?

It might be in some cases, because you'd expect within bounds nature to be a fairly clever blind designer in terms of what it might discover.

And so just going back and forth between the robotics perspective

and how it might work in biology, and then the abstract algorithmic implementations of them for the robots, and seeing how those can mutually inform each other.

For me, this is how I basically want all my thinking about the mind to be from now on.

I feel like this is the most powerful way of thinking on it, that back and forth.


SPEAKER_01:
Let's enter in the biological door or blue.

Do you have a question?

Okay.

So Adam, what, even though of course it's a deep area and there's writing in the paper and citations and so on, what is so interesting about the structuring of the grid cells through space and time?

And what can we learn from that about effective ways of navigating?


SPEAKER_04:
So in latent SLAM, we have these pose cells.


SPEAKER_05:
And actually, I think it's probably better for Tim to talk about that, like the exact ways in which these cells function within the robotic system.

I think that actually brings more clarity than

um, the more mysterious, like, so there's lots of ideas have been going around in like computational neuroscience of like what the grid cells might do.

But I actually think the greatest clarity comes from like, well, how would the analogs work in the robotic system?

So I'm going to kick that to Tim.

Uh, what do you think Tim?


SPEAKER_02:
Yeah.

So I, I think that's in, in our current implementation, the, the parallel with grid cells is kind of, um,

hardwired in the implementation.

It's kind of, by design, you have something that looks, if you squint your eyes a lot, it looks a bit like a grid cell.

I wouldn't say that we actually have grid cells in here.

Because to me, the grid cell is basically, it should be an abstract, it should be giving you a way of

modeling movement in an abstract space, basically.

And in the case of navigation, the space by accident maps to the real world.

And the grid selectivation is just a peculiar way of encoding where you are in the space without just giving you the XY coordinates, basically, but the more kind of

uh higher dimensional more robust representation of where are you on in this on this grid and how do you if if i now move how does how does your um location in in in this reference frame or this grid uh change and so in our in our implementation we basically just said okay we know that we are in a roughly flat uh surfaced world so

the main coordinates that we need to track and integrate over are the X, X, Y position and the head direction.

These are basically the main things.

And I mean, we use this thing called the continuous attractor network to kind of

represent these three coordinates and we basically we inject the knowledge that if you move forward then you will also move forward to within this attractor network location and

Roughly, this corresponds to doing kind of path integration.

And I'm pretty sure that grid cells are giving you similar functionality in the brain.

But I would not say that our current implementation actually

I think the parallel is more with place cells, because there we do learn representations from visual sensory inputs that kind of give you an estimate of this is a particular location.

And if you see a similar pattern, then

it is likely that you are visiting the same location again and if at the same time you happen to have a very similar x y uh and head direction as in your uh as in your previous time you you kind of saw something similar like that

then you have pretty good evidence that you are actually in the same spot.

And then this is what you then call loop closure, and then you kind of reorient yourself as if you were at the same location.

So that's kind of how I would think of grid cells and place cells in our current work.


SPEAKER_01:
Thank you.

Blue, do you have any questions?

Yes, please, go for it.


SPEAKER_00:
So I'm curious as to the role, if any, or how the system may change.

If you included proprio reception, which is like,

the sense of where your body is in space.

And so like this is partially modeled through the visual system at place cells, grid cells, et cetera, but it's, it's not always right.

So it includes other senses like balance and you know, where's my hand and these kinds of things.

And I'm just curious if your model includes that in any way or only or, or not, or how it may change if it did give it, give it kind of like a more embodied


SPEAKER_02:
Yeah, so basically, in our case, the proprioception is limited to basically the velocity commands that you issue to the robot.

It also observes these again.

So it assumes that it knows which command it's sending, and then these are also kind of

fed back to the model as kind of a sensor so if you say drive forward or turn left or turn right with these kind of angular velocities this information is also sent into the model and this is then what we call the action so everywhere in the especially in the low level model you see the action pop-up this is basically you can you can see this as kind of a proprioception thing

And so everything kind of predicts what will happen given what I sensed that I was doing, basically.

And we do it both for the visual sensor, like what would I see if I'm turning left or right or go forward.

but also for tracking the pose and doing the pattern integration.

Like if I move forward and my heading was a zero angle, then I will probably just move forward in the x direction of the cam.


SPEAKER_01:
And that introduction of counterfactuals and anticipation, what would happen if I did this?

is very interesting area and also it speaks to the composability and the flexibility of the active inference model.

Like Sid highlighted that only the visual data were being used here with RGB values of video, but there are other kinds of sensors

that could be used as another sense state and that would kind of increase the dimensionality of the model maybe it is a value add maybe it's not but it doesn't require um rewriting it's like more like plugging in a different hard drive than it is like having a different computer um so you mentioned a higher dimensional robust representation of location and that's definitely one of the

advantages and and aspects of the model that'll be important to explore more like in the dot zero we explored this paper are we ready for service robots that showed how sometimes even trivial changes to environments can result in the robot getting lost because it has referenced to something like a chair that's no longer at the same location so how does this model

provide a higher dimensional robust representation of location.

And how did you go from thinking about that to the actual way that the generative model is framed?

Yeah, so...


SPEAKER_02:
To get back at the problems that you mentioned on the service robot.

So basically, the major problem in Robotics Slam is that indeed, if your environment changes slightly a bit, then

the same location doesn't look the same anymore in whatever sensor modality you're using and that can be problematic especially if you if you want to model the environment as like a metric system where you basically define this is my origin so this is zero zero in the coordinate system and now i will

for every obstacle or thing that I approach, I will log my estimation of the XY coordinates.

And then suddenly when your chair moves, of course, yeah, everything breaks.

But here in this system, we actually get rid of being grounded in a metric space.

The only thing that matters is that

this location looks very similar to something i i visited before it also seems likely given the the pet integrated trajectory i i assume i was following so this might be the same spot and if you think that or if the chair moved in in that place yeah that's fine then you just say okay maybe it's a different spot it doesn't really matter

um at some point you will have a kind of a queue that is stable right so and that's actually what we also do as as humans if you want to navigate you just look for the the the cues that you are that you know that are pretty stable if you visit a a new town you will look for the church tower because you know it will not move you will not orient yourself based on the color of the cars that are parked uh somewhere so

I think by using the active inference principle as a starting point,

you can get rid of a lot of these problems because your agent will automatically look for these stable queues.

And if suddenly the environment changes, we'll just accept this and just model it as like, okay, I think I'm revisiting this same trajectory, but now it looks different.

So maybe I just have to model it as such as this is kind of a trajectory that can change often.

And maybe this implies that if you plan the next time, you'd rather prefer a different trajectory where you know that everything is stable.

So this is the same as reducing your ambiguity.

So I think that getting rid of

everything should be structured in this x y location in a metric space and else i don't know what's going on just by getting rid of that and and and just assume anything can happen anything can change and if it does not change i i prefer it i like it better uh this helps a lot uh to circumvent a lot of the traditional problems i've seen


SPEAKER_01:
Adam, anything to add on that?


SPEAKER_05:
I mean, so one of the things that we've been working on is, like in this paper, it's trying to say, OK, so to what degree are the principles of what we understand about the natures of the hippocampal and entorhinal system being reflected in this simultaneous localization and mapping problem in latent SLAM?

And some of the other work we're doing is also going in the reverse direction of what can these principles of these slam principles use for autonomous navigation?

What can they tell us about the brain functioning as a cybernetic system?

And so one idea in general is that

each of these operations, in addition to having this fundamental significance of the basics of being able to move through the world, this task that any active inferential system that's going to survive and do what it needs to do has to handle, but to what extent

were these same principles repurposed for sophisticated inference more generally?

And to what degree is sophisticated inference structured according to SLAM principles?

And so if you're talking about these loop closure events, like finding this highly familiar state and where you think you know where you are by this

Convergence of like your trajectory information and your pose well this question would be like this is a model potentially for the feeling of insight or discovering of like causal accounts.

Is this.

And is this actually a source of inter-individual differences, which would be fairly fundamental?

How familiar do you think things are or not?

How much do you update on encountering these highly familiar situations?

And so me personally, what I've been wondering is, to what extent, of all the details, to what degree

So re-representing the biological details in terms of the kinds of descriptions that Tim is giving here.

And also, though, thinking of cognition more generally as navigation.

And so thinking of each of these things, what are the analogs at the level of thought as a kind of navigation through generalized space?


SPEAKER_01:
Awesome.


SPEAKER_05:
Yeah, there's been navigation through.


SPEAKER_01:
A lot of interesting work on exploration, exploitation in mind and space, Hills et al.

2015, and on thinking about planning and the evolution of planning as being related to spatial as well as cognitive foraging.


SPEAKER_00:
So it's interesting to think about cognition as navigation.

I think about the feeling that I have when I'm searching for a thought, a word, a name of a person.

It literally feels like you're scrolling through a filing cabinet, physically looking for this piece of information in your brain.

And I know that different neurons store different pieces of information, like the Jennifer Aniston neuron and these kinds of things.

So I wonder what, can you just elaborate maybe a little bit more on that, Adam, about like cognition as navigation?

Is it like literally navigating to find the right neuron that goes ding, ding, ding, ding, ding?

Like that's the one I wanted.


SPEAKER_05:
So like to some degree it could be in that like, it's like two ways I might think of it as one of like you were talking before and you were like bringing up like memory palaces.

And I think that would be one example of thought is navigation, which is with this art of memory in terms of it can be part of the way we access and organize the information we work with when we're thinking is through a kind of spatialization often.

And actually, to the extent that we show really great virtuosity with memory, it's through an explicit process of spatializing and navigating through domain.

And that gives us the best purchase we have

you know, remembering these building, constructing and accessing these complex structures.

But in terms of like what you're describing with like something kind of like, you know, Jennifer Anderson or like neurons are like kind of finding, the thing that's coming to mind there is like foraging theory.

And so what you might be foraging for, like when you're like looking for like something that fits is a certain amount of like familiarity,

Or like some amount of like, you know, it could take different forms, like, you know, some sort of, uh, you know, maybe, maybe it's a entire receptive code and it's like, like a visceral feeling of like compelling this, or maybe it's like a more of like a proprioceptive code.

And it's like tension in the body.

I don't know what it might be, but this, like, there's some sort of read you're having on.

are you finding what you're looking for?

And so then the idea would be like, you're foraging for it.

And so you might spend a little bit of time in some area of conceptual space, and you're kind of moving around in there, like trying out different permutations.

And then you're like, nah, I'm not finding it.

And then you might do like a different tack, and then move to another area of semantic or conceptual space.

And you're searching around there.

And so like,

In forging theory, these would be called, I believe, levy flights.

And so there's something called the marginal value theorem, which is if you're trying to be an optimal forager, you're trying to balance exploration and exploitation.

And so you don't want to spend too long in a given patch if it's not having what you want, but you don't want to leave a patch prematurely if it's a good patch to be exploited.

And so the idea is if you're looking for

searching for a name or something that fits, it might take the form of this kind of generalized foraging.

But instead of you locomoting through spaces, you're actually spending different amounts of time in semantic spaces and then shifting based on whether you think you're actually, it's like a juicy patch.

And so with the marginal value theorem, it says if your current rate of foraging drops below the historical return, then you should go.

And so there might be different inductive biases in the brain that would represent this.

For example, the hippocampal system, this might look like, for instance, the way my plan is, you might get this prediction error drops below a certain amount or goes past a certain amount.

And then the hippocampal system ramps up with a lot of her current activity.

It's called a ripple.

And then it resets itself and retiles the space.

stimulates cortex and a new set of operative policies and specializations might occur.

And so this would be an example of a mechanism, like hippocampal system might have a built-in mechanism for these attentional shifts and these imaginative shifts when you're doing foraging.

It might be reflected also in other ways, like dopamine.

So there's some thoughts on my thinking.

Nathaniel Dahl will talk about one way of interpreting a ponency between phasic and tonic dopamine.

would be the tonic would tell you what's your average return, basically.

And there seems to be a kind of functional, I don't know if it's not quite antagonism, but the more your tonic is high, the less impactful any phasic event is.

And so it would be kind of like if you're foraging what would occasion a shift or not, you might actually see this playing out in the way neuromodulators work.

I don't know if that helps at all, but the idea is that this is the sort of thinking in terms of forging or navigation through generalized spaces as basically being.

So I guess there's a kind of a priori reason you would expect it to be generally good, which is, well, doesn't everything

most of the things we encounter do indeed exist in spaces and then most things the way we understand them is by like spatializing them let's like we graph them we we do like multi-dimensional scaling we do different things to try to like it's just like a way we understand things but it also seems like this was probably both evolutionarily and developmentally like this was the original challenge that had to be solved where the systems were paying their weight and that this would then involve like a repurposing and a redeploying for

more abstract domains.

So like there's some other like, I think it's Burgess has done some good work, right finding like, for instance, like, we might create like a morphos space.

And we're like looking like, like, they'll create like these generative, like birds with like different like neck lengths, and you're trying to do like categories over like what the different kinds of birds are.

And there's like, there's evidence that the way we do the categorization is we'll create

a spatial ization of this task structure.

And there's some evidence that the capital system is we're navigating through that in dealing with these categories.

Anyway, so that's, that was a lot of rambling, but like, I don't know if that got it with any of that land or Yeah, lots of Atlanta is very cool.


SPEAKER_00:
Thank you.


SPEAKER_05:
Okay.


SPEAKER_01:
one note that will i'm sure be exploring going forward optimal foraging theory is often put in the context of utility which is just the pragmatic component in active inference in terms of policy selection so optimal active inference foraging is like when the epistemic and the pragmatic gain are being considered so that's quite interesting and then also there's

as you brought up individual and collective navigation.

So one case is sort of the sovereign robot, no Wi-Fi, onboard computation, alone in the warehouse.

But then in the case of, for example, an ant colony where there's stigmergy and modification of the environment,

that might be possible or even facilitated by different onboard cognition and different outsourced or offloaded, which brings us to extended cognitive discussions.

But let's return to this question about going from

this resiliency of navigation that a biologist knows exists, a roboticist might want to see, how did you then go to that third space and actually have a hierarchical model?

So here in figure two or wherever else you suggest, what is happening here and how did this

graphical structure come to be as opposed to potentially like other graphical structures that could have been used?


SPEAKER_02:
OK, so basically.

Let me first maybe explain what all the variables mean in this in this figure.

So.

Going from the bottom.

At the bottom we have the O's, OTs, which are basically observations.

So these are like the visual inputs, what the robot sees, basically.

And then we have two latent variables, S and P, where S is basically an estimation of all the latent factors that give rise to the observation.

So it's kind of a latent space that confines all possible visual observations.

And p, on the other hand, is the latent variable that has a representation of your pose, which is then basically, in this case, your xy position and head direction.

And then we have one more, which is the aid action.

And so that's what I hinted at previously.

This is basically your proprioception.

You know what you have been doing.

So therefore, up until the past, the 80s are also shaded.

So it means that you basically have access to what you have been doing.

It's like the low level action, like move forward to this velocity or turn left or turn right.

But then for the future, this becomes an unobserved thing and you basically want to infer potential actions you should be doing in the future.

And then

For the future, a number of actions can be just summarized as pi, which is then the policy.

So the policy is no more than a sequence of future actions that you want to do or that you intend to do.

And then we move up a level in the hierarchy.

and there we have two more variables one is the location l and this is then basically kind of a more coarse grained

yeah look representation of location where am i but it's not really uh it's not necessarily tied to a metric space it's just like identifying this is a location and then we have uh also an action space at this level but this is the more uh we call this moves or m

And the move is basically kind of moving from one location to another.

So locations can just be more coarse grains like what you define as like a separate location in your environment.

Whereas at the lower level, the states, if you move from one state to the other, it's basically just saying, okay, what would be the next frame of my camera sensor be like?

And so it's also crucial to know that these two levels operate on different kind of timescales.

So the lowest level timescale is basically a very fine scale.

fine-grained action perception loop.

I send the motor commands to the robot and I read out my camera and this can go to, I think we implemented it at like 10 hertz, so 10 times per second to get a new camera frame and we updated the motor command.

Whereas location is just like moving from one location to another can take you maybe 10 or more time steps at the lower level.

And then, of course, one of the crucial bits is then, how do you identify a location?

And so basically how it comes to be is that if you basically observe your space through the camera images, we infer this abstract state space S, and then we look at what is the surprise of being in this state versus what I predicted.

And if this surpasses like a certain threshold, so yeah,

pretty complex to explain, but I'll do my best.

So every time, every time step, you get basically a new camera image and you can calculate like what is what is the divergence between what I was thinking about the state previously and how does it how did it shift?

with my new observation is then basically your kind of the bayesian surprise that you had from seeing this observation and every time step we accumulate like this number and once we suppose a certain threshold which is kind of a hyper parameter we just fix it that's something that seems to work so if you suppose this threshold you basically say okay this is now distinct enough from n time steps before so it must be something new it must be a new kind of

location in my map and then you add this thing as a distinct location in your map and you add like a link from the previous location to this one and you start and then you continue further on building the map and this is how you then kind of create this more coarse grained map of your environment and

you can then reason like how should i move from this location to another and you then can you can basically use your your previously visited locations and moves to kind of have a model of how can i move in this space but at the more coarse grade level moving from one location to another and it also means that

by using this surprise on inferred latent space that we infer from visual input, it means that not every edge has the same distance in real space necessarily.

So if the robot is traversing a white hallway where the walls are perfectly white and everything is the same,

Then it will take a long time for this threshold to be reached.

And it will only at larger distances make a new node.

Because everything looks the same anyway.

So there's no extra information that I need to put in the map.

But if you then suddenly see a door, for example, then you suddenly reach your threshold and you add a new location in your map.

And so it's basically the locations in the map are basically defined on

what is the information I got by looking at this thing, rather than how much meters did I traverse in this time.

And that makes it a really interesting way of representing maps, because it's more like how much information is there in this particular location, rather than which xy position is here.

Does it make sense?


SPEAKER_01:
Yeah.

And it feels to map onto the distinction between like Kronos and Kairos.

Like in this analogy here, Kronos is the chronometer, it's the watch time, it's the geometry, it's the XY grid, it's one meter.

And then Kairos is like the timeliness of action and the time it takes to complete an action.

And then something being like boring, that's going through a low information hallway.

in the cognitive forging.

It's like, are we there yet?

Driving through this featureless landscape where the salience is low because my predictions are basically being met almost too well.

And there's a lot of other

ways to go with it.

Adam, anything to add?

And also, again, I'm just kind of curious, like, what is the space of the possible graphical structures?

Do you think that this is the only or the sort of grandfather graphical model for hierarchical navigation?

Like, how do we go from imagining this tactics and strategy multi-level model to a graphical model that we believe will be consistent with active inference and amenable to message passing?


SPEAKER_02:
Maybe to add on this last question, I think Kristen has a paper on deep temporal hierarchical models where he basically lays out more generic.

If you're looking at the ultimate generic hierarchical model, you'll find it there where it's basically just an aggregation of states that give rise to observations that go from one state to another

given an action, and then you can add the level on top and level on top and you can keep on adding levels.

And that's kind of the prototypical

hierarchical model, I would say.

And what we see here is kind of particular instantiation where we took some domain knowledge like, OK, for navigation, what would be sensible levels to think about and what would these latent states then what should they represent?

And then, of course, you easily end up like, OK,

At the lowest level, I need to have some state that represents my camera images.

And at the higher level, I want to have some locations in the map.

How do they then relate?

Okay, I probably also want this spot integration.

So maybe I need to have a special first-class citizens here in the form of my posts.

And that's how you basically build up the model from first looking at this prototypical hierarchical model, then looking at, yeah, but in my case, what do I think is sensible?

What could these things mean?

And then try to figure out like, okay, at this stage, we kind of converge on the one hand from top down, like does the math work out to bottom up?

Does this match with my implementation?

And then at some point we converge to this basically.


SPEAKER_01:
So the high road and the low road, they do lead to Rome after all.

Exactly.

Adam, what would you add in there?


SPEAKER_05:
Well, first of all, I love that metaphor just now.

Another thing I would add is, so like, you know, on my end, like, I'm trying to use this as like abstractions for understanding, you know, aspects of like brain functioning.

So let's say we're talking about these, you know, this top level

This is one way of interpreting like, you know, hippocampal place cells.

And these, whether you create a new note or not, based on what threshold of familiarity or surprise, this would potentially be like an example, I think, of an extremely powerful source of individual differences.

Like maybe you could think of this in terms of like Piagetian like assimilation or accommodation.

But you could see like if your thresholds are too low or too high, you could end up representing a domain in ways with that are either like you can get false.

You can get things that are like excessively granular or insufficiently granular to represent the domain.

And you can get different types of errors associated with it.

And so this, for instance, could define

Even things like more like some people will talk like autism to schizophrenia spectrums and to some, there could be like kind of underlying predispositions for this.

And it might actually be, I think that's an oversimplification, but you could think of these thresholds as some cognitive spectrums might just fall out of them as you get different types of minds.

If you have like a more granular, like an overly inclusive or an underly inclusive category structure.

but some other things like just trying on this would be like, if we're talking about this, um, you know, lower level, and if we're thinking of like these, um, like enter rhino grid cells as well, not quite being while the posts here aren't quite grid cells, but the, this idea of like this, um, mapping of your, um,

what your body position is and where you're oriented.

And then having this conjoined with what's in your visual field, and then using these sources of information to mutually constrain each other and using where you are in space.

And these all being necessary, like for instance, this seems to be fundamentally what was needed to get the robots to work.

And we have not so dissimilar sensors and effectors.

And so one thought is, I think it's Guillaume Dubas has, I just messed up his name, but he has this interesting paper on the default mode system, it's called Dark Control, and thinking of it as doing tree search.

But to loop it around,

dispose information it's possible that somewhat like the posterior nodes of the dmn are actually helping to encode this information um like if you go like to the parietal portions that actually might be where is my body where is it reflected and then if you're going like to these midline portions of it um you know what's in my visual field or what am i imagining

And then if you want to go for this future part, that might be the anterior portions.

So like the DMN or the dorsal medial prefrontal cortex maybe coupling with the anterior hippocampus.

But basically, it would seem that the stream, these core aspects of the model might be ways of functionally understanding the core aspects of an agent architecture used for

mental simulation.

There seems to be a striking amount of convergence between the things that people like Tim are identifying as just like, what's necessary?

How do we solve the problem?

And the features of neuronal organization that would be these defaults or canonical processes that you're seeing as really just fundamental.

We call it the default focus.

It's active all the time.

What's it doing?

Maybe part of what it's doing is it's allowing us to do this kind of navigation through space and time.

Yeah, those are some thoughts.


SPEAKER_01:
Navigation through space and time in the sort of territory sense would be clock time and metric space.

What is your X, Y, Z, T?

And then navigation through mental space and time

might have some resonances or analogies, but it's not a piece of grid paper.

So what is it and how does it become realized through ecology, evolution, and development?

So awesome points.

Thank you.


SPEAKER_05:
Real quickly, in terms of the time issue, it seems like there's these multiple ways that time comes in.

It's like one sense of like the sense of time

as you might be doing accumulation based on the frequency with which you're having meaningful informational events.

Like Bud Craig calls it these global emotional moments.

And Anil and others have some, I think with Murray Shanahan, have some interesting models of the sense of time in terms of the amount of information flux through your sensors.

But then there's another kind of time that might actually involve

Over spatial ization of a trajectory and you might be using something like path integration in terms of like how you're doing some types of duration estimation.

Not necessarily just like accumulating like like a sense of accumulation, but some of it might be like where am I in navigating through a timeline.

And I think you'll see this reflected in the way people talk.

And we could dismiss this and say, oh, that's just mere metaphor.

No, I think that metaphor is how we're actually constructing some sense of time, which actually gives us forms of control that we would not have otherwise were we not able to actually extend our minds metaphorically in that way and then be able to basically navigate through spatialized time to keep track of some longer time sequences if we want to plan with respect to them.

Awesome.


SPEAKER_01:
Thank you.

Stephen?


SPEAKER_03:
Hello, great to see you all.

I was curious in terms of time.

Well, one of the things is, do you think time is, or this process is distributed around different parts of the sensorium processing because different sensorium have different speeds or different rates or different abilities?

And then the other thing coming back to time as well is people can be going through time

They can be like, or they can be, time can come towards them, or time can be sort of developing around them, which often happens in Asian cultures.

Time is seen more as something that's around them.

So I'm curious, like, how these ideas of temporality might link to the ways that we imagine ourselves with our sensorum.


SPEAKER_05:
I mean, if it is indeed the case that some forms of the spatialization, if it is the case that spatializing time is happening both in some cases through some fairly abstract metaphorical and linguistic constructions, then this would imply that you should be able to get meaningful cultural differences

based on these shared constructions and maybe even some like quasi like warfian effects where like linguistic modifications could actually um change the way people are seeing themselves and others in relation to time with some fairly significant consequences so um our warfian effects to unpack the jargon of just like um you know the idea that the um our thought is constrained in some meaningful way by the uh

constraints and affordances of our language are constraints and affordances on the way we think.


SPEAKER_04:
There's like stronger and weaker versions of that hypothesis.


SPEAKER_01:
Thank you, blue.


SPEAKER_00:
So it's interesting because as we've been talking about like spatial maps and spatial context, like spatial representation, navigating through space, I have been thinking a lot about semantic navigation also, like when we're looking for a thought, when we're looking for, we're navigating, forging for information or, you know, in these context rich domains.

And I just wonder what,

Is the semantic navigation related to, linked to, is there overlap in the brain in the way that we process semantic orientation and representation and space also?

So I'm just curious.


SPEAKER_01:
Hashtag maps of meaning.


SPEAKER_02:
yeah maybe uh i can uh i can respond on this from from my perspective and then i'll probably have to leave you guys um but so i think one of the most important things at least to me uh of of how this this whole endeavor of of constructing this uh and and doing this together with adam is that

we basically we start off with uh with robot navigation which is from a roboticist perspective it's really like about x and y position and about times and seconds but what we do here is we by by creating this model we basically get rid of this

close tie of being grounded in seconds and meters, basically.

I think this is one of the most crucial aspects of our model, because it shows that, well, on the one hand, I think we all agree that we don't get born measuring time in seconds and measuring space in meters.

So,

that we actually get at least the framework that also gets rid of these these metrics and just learns to navigate in this more abstract space and if we can get it to work for

for navigation per se, moving through actual physical space, then I'm pretty sure that we can also extend the model further to navigate any abstract space.

Because in the end, once we are at the

once we get the level above our our sensory inputs then everything is in our case just a learned latent thing basically so if we can if we can make this work for physical navigation i think we are already a

a big step closer to making it work for any navigation.

And indeed, we come to the realm of like, okay, I have lots of memories.

These are tied together somehow because how these memories just came to be

Can I now navigate this space towards the memory that I'm looking for?

Or if you are solving a math problem, how do you navigate the space of math equations to get the solution for this thing?

I think once we get rid of

the time and space that are so tight for your sensorium and move towards a more abstract space and be able to navigate that, this is the crucial bit here, I would say.

And so with that, I'd like to thank you for the discussion until now.

I'll try to be there next week as well and join the discussion further.


SPEAKER_01:
Thank you, Tim.

Cool.

Very interesting.

And also speaking to that cultural basis, there was the inch, foot, pound paradigm.

And that has, in certain places, also coexisted or been complementary or contradictory with the centimeters, grams, second based upon something different.

And what would happen if a larva was born and the little one, the second, was like what we call three seconds?

Is that just a more chill larva?

Because every...

One of its moments is taking a few more heartbeats.

It can actually take a breath per little one.

So there's a lot that we probably don't even know how much we don't know until we're in feedback with these models that both fully realize not just in the equations, but in the actual robotics

grounding us in those actual observations of red green blue pixel intensities which is like what adam was saying that robotics are are an empirical platform to be studying the nature of cognition and brain and mind and then here is where we move into a different space and time and so that is an excellent point by tim stephen yeah so following on from what you're saying daniel is um


SPEAKER_03:
I know Tim just mentioned about, you know, have the abstracted ideas.

But there's also maybe the grounded, you mentioned grounded ways of knowing.

So it might be that the future we abstract out what we think it might be.

And then there may be ways where we feel what's right, what feels.

So you hit a door.

and you feel there's an information affordance and it can ground.

So I was wondering how much that supports not needing to have the metrics, like you say, and maybe the metrics that we've created, like the second, are partly because they're at that kind of sweet spot or somehow close to a sweet spot of what sort of makes sense.

You know, someone had to decide on a foot or a hand, you know, so...

Was it the small person's hand or was it the big person's hand?

And they sort of said, well, what about John?

And maybe he was the king.

I don't know.

But it got decided somehow.

So I think this is a good question about grounding versus abstraction.

I don't know what the thoughts are on that.

Yes.


SPEAKER_01:
This higher level in a purely metric model, it could be like, well, on the bottom level, we're dealing with centimeters and 100 millisecond frames.

And the coarse graining is at the level of 100 times the spatial distance and 100 times the temporal distance.

What this model is doing is something different.

And I think that is, again, this biological inspiration, which is that what is happening in the level that the action perception loop is nested within is not just a bigger version of

Well, it both is and isn't because it has some resonance graphical structures that support the composability and attractability of act-inf models, yet also it's doing something that's categorically different, which is navigating in a more topological space rather than the more geometric space down the bottom.

What are you thinking about, Adam?


SPEAKER_05:
It seems like with the

well i guess one is i suspect it is me on no coincidence that seconds are about the time extent that they are and that we have like the metric units we use like a foot or a meter or a yard in terms of like in terms of grounding and in terms of connection to affordance it's like a second for maybe you know um a relation to a certain

maybe like a heart rate or certain like biophysical pattern generators, or like the scope of working memory being an iconic memory, being stable on different scales or like a yard or a meter, like the, basically the reach that you'd have your arm, like, like Perry personal space and like a field of affordances around your body where it's particularly concentrated.

And so,

These are ways in which the granularity of our modeling would be influenced by language, or language would reflect it and maybe, to different extents, influence it.

But you would have this other kind of granularity of space that might be different for different species, but also within a species and within an individual.

For instance, the hippocampal system will lay out these tilings of space.

For a task domain and they'll have like a kind of they'll be refreshed like maybe it looks like like every like three seconds or so, but if you're in like a big space.

But this tiling will actually have hexagonal arrays or hexagonal time it'll fill whatever space it's in and it'll have different levels of granularity.

And so, if you're in like a big space it's going to necessarily be less granular.

And if you're in a small space, it'll probably be more granular.

And there's other things that will influence the granularity also.

There's a brainstem locomotor nuclei that are detecting how fast are you moving.

And then this will influence the granularity of your of space.

And so that's some of it.

some of this what is relevant, some of this are greening of a domain, some of it will be reflected by these basically innate architectural inductive biases, and some of them will be more socioculturally constructed.

One thing before moving on is I like how Tim, as he left, that was one hell of a mic drop.

So he's already solved, in terms of kinds of grounding, not only is this potentially a way of having an architecture capable of navigating conceptual domains and abstractions, basically what Bengio calls System 2 AI,

or what some would call strong AI.

But it's also grounding in that it actually works as a control system for an embodied embedded agent.

And so now you might have a single system capable of both giving you symbol grounding by allowing something to be in the world alongside other agents and have actual semantics in terms of pointing to things in the world.

So you can have a semantic network

Just like symbols pointing at symbols.

But ultimately, such a system would allow you to ground up your semantic network in things in the world and shared symbols.

And so, I mean, ultimately, this is a path to, you know, who knows how far, you know, anyone working on it today gets.

But this is ultimately a path to AGI built that takes on an activist principles of what you would need to do so.

Which is more than a little exciting, I think.


SPEAKER_01:
Oh, I'm starting to see why people are interested in this.

Excellent point about the symbolic semantics and logic and how that connects to the inactive insight, the pragmatic turn, ecological psychology, biosemiosis, all of these features that get left on the cutting block if we take a computational linguistics approach only.

Computational linguistics plus the mouth movement

already one has incorporated that pragmatic turn in that embodiment element and that's quite a different model than just the substrate independent symbol manipulator stephen


SPEAKER_03:
Yeah, I think it's exciting, indeed, because it can open up a lot of... I'm really interested in mental space psychology and peripersonal space and working with space as an instrument.

And, well, I'm more than interested in it.

That's my main focus in many ways.

But what's really been the challenge is giving it a plausible bridge, right?

Because they're working primarily, you're looking at what action policies are

and what has been enacted.

And it's very hard to go back to observations because you're coming from the other direction.

So I think it'd be really interesting to see what this robot, whether you could actually see, well, what kind of robot is this robot?

What's it like?

What kind of story patterning gets triggered by this robot?

And maybe if I...

it grew up with a slightly heavier wheel on one side or something it has a slightly different um character you know so i mean i wonder if there's a kind of an actual uh a way that the action policies themselves start to encode a certain personality based on the physicality and sensorum that get weighted


SPEAKER_05:
I mean, that would be like, in terms of this generalization of slam principles as a way of understanding cognition more generally, I think that would be a great test case.

And those are the kinds of hypotheses you would look for.

If your wheel is weighted more heavily on one side or the other, that could have all sorts of things.

For instance, what Tim was describing with this node creation, where it's not just a function of metrics, but on

information.

And the and so this creates like a distorted space, but but an adaptively distorted space, you would expect usually, because multiple forms of value are influencing the granularity of this sort of space, like necessarily is like a map, it's like space is a graph, but but a graph being shaped by multiple kinds of value.

And so you would expect things like like that kind of real asymmetry you're talking about to result

in different kinds of graph spaces.

And this could then potentially have some transfer more generally across more than just physical spaces in theory.

Also,

Coming back to that granularity issue, let's say the way you move through the world is quicker or slower.

That itself then might have transferred not just to, well, how is it that I'm representing the space I'm physically moving through, but that then can get reflected in cognitive styles in terms of the way you relate to semantic spaces, in terms of the way you're going to forage through them, what you consider to be near or far connections.

This is the kind of...

that would be like a whole research program, which would be, but using this as like a source of bridging principles of like, so some connection between embodied experience and then more abstract thinking.

Well, maybe this is a source of some of the ways in which concrete embodied experiences go on to shape basically thought on the more abstract level as well.


SPEAKER_01:
very very interesting and especially when we think about the mental action papers that we've read like live stream number 25 with sanford smith et al about metacognition as mental action and so in this two-level robotics case

talking about the sort of lower level motion and then the higher level navigation of locations where we can say i want to go to the post office and then the grocery store but then we have this more tactical taking a step time scale that's a lot faster if this bottom layer is based upon cognition and perception also this higher level is not just navigation in

the warehouse, that's actually like, where do I want to be in my regime of attention?

Or what would I like to be considering from a metacognitive perspective?

How do I navigate to a place where I prefer what's healthy for me?

So there's all kinds of other questions about navigation in parameter spaces that are not just like topologically the cousin of the warehouse space, but truly in the mental space fully.

Blue, and then Steve.


SPEAKER_00:
So since we brought up hierarchical here in the paper, it looks like and I mean, I can hold this for Tim, Adam, too, if you don't feel comfortable talking about it.

But maybe maybe you're good in the hierarchical space.

It seems that it's just a core screening like in this paper.

So like, you know, the higher level is just like a coarse grained version of what happens at the lower level.

And it's easy to see how that takes place or take shape in like a topographical way.

it doesn't leave a lot of room for emergence.

And I wonder, like, if we jump to thinking about a semantic space, you know, we have words and we've got the relationships with words to each other, proximity, you know, how close do you expect to find, you know, there to was or whatever.

So how linked are the words together in a semantic space?

But I wonder if, you know, you can go into topic modeling and that gives you like a kind of a core screening.

But is there more room for emergence in that kind of space, maybe?

Or what other, how do you, I don't know, how would you just allow for emergent things to come through in this kind of hierarchical modeling or mapping?


SPEAKER_05:
I think when Tim comes back, it could be worth it for him to go into more detail on the

parameters that go into node creation or not.

Because I feel like to the extent that you're getting some flexibility in ways of understanding like emergence, that would be your models of emergence.

And that would be like where you would basically create this coarse grained ontology of different levels of granularity.

What

what constitutes like an occasion for core screening of what kinds?

It seems like the node creation would be where your, your assumptions about that of what kinds of emergency you might be dealing with.

That's where we'd come into this kind of model.

Is it rich enough to account for like all, I wouldn't quite say that in terms of like, you know, all the different forms of emergency you can get.

So some of it, you know,

But by enough, I mean enough to be a full on general intelligence.

But I do think there is room for some flexibility there at that parameterization of Tim's model.

But he would explain it better than I would.


SPEAKER_01:
i'll make a comment and then ask a question in the chat and then also see you steven so um it was mentioned that uh there is the embodiment of certain prior information in the continuous attractor network like if my position is this and i move forward then that is putting me into this different position in the can and um

Blue asked about topic modeling and semantic spaces.

So that territory is different.

So the maps should be appropriately different.

And so we could say, for example, like California is nested within the United States.

That is not violating any geographical priors.

But what about a paper that's interdisciplinary?

that might be at the intersection of two different topics, or depending on how those topic models are generated, one could think of every paper as existing across 100 different topics to different quantitative amounts.

And so it actually does bring us to this question, which Sid has asked, which is we're hearing a lot in the paper, in the natural speech and so on, different complementarities.

lower and higher so here where the lower model is the finer scale and the the red i'm trying not to mix metaphors but lower and higher is sometimes used with inner and outer or finer and coarser coarse graining more specific more general so what is that restaurant where those are the dishes

could it something be above or below and you get it but it could also be more like a kernel and more like the fruit and you get it and it could be more like sand or more like gravel and you would get it what does that mean or say we'll think and then allow adam to speak first


SPEAKER_05:
I think that's a deep and complex question.

And there's a couple of levels or a couple of entry points there.

But one thing is, like you mentioned, like inner and outer is another way of describing as opposed to higher and lower.

And there's a, I think, connection between this latent slam architecture and meta learning, where you're having like a more fine grained process for like

that's more task specific and for like adaptive fine grained couplings.

But then you nest that or you create like an outer loop, which is like a slower aggregation process that's coarser, but then allows for sharing of information across episodes.

So it's a kind of like so this is basically a kind of meta learning system and giving you this divide and conquer approach of like levels of granularity.

I don't think

anyone quite knows yet what the significance of this is.

But one interesting thing about the grid cells is they seem to have a multi-scale hierarchy in their organization in terms of there's this axis.

I believe as you go within entorhinal cortex, as you go more dorsal to ventral or more superior to inferior,

you'll get these discrete cutoffs of potentially implying some kind of harmonic nesting of scales, maybe even coupling with multi-scale synchronous dynamics in cortex, unclear.

But there does seem to be these multiple scales reflected in grid scales, in internal crystals having different levels of granularity of their spatial representations.

And the thing that's interesting, I'm curious about is like, what are the, or one thing I'm curious about is like, to what degree, for instance, do different like neuromodulator systems, like some of the ways in which like they could be understood in terms of like them having their functions is maybe like influencing which granularity is most present, like moving into like now in a situation of like attending to like the fine granules and, or,

this is a situation where we're going to really go course and just ignore those details.

And that some neuromodulatory systems, their effects on neural dynamics might actually be best explained, might be well explained in terms of this adaptive changing of the granularity.


SPEAKER_01:
Very cool.

And we see that in teamwork where sometimes it's time to control paste and just fill out the spreadsheet.

Other times it's time to add a column to the spreadsheet.

Other times there are this open-endedness to the task and including the local objectives and also even broader mission and one's own relationship to the team and the project.

perhaps if there's a similar biological substrate, then it would stand to reason that there could be multiple representations that are emitted from some kind of common generative model.

Steven?


SPEAKER_05:
So one thing that I'm actually really interested in is exploring these SLAM principles in the insect case, because they have analogous and potentially largely homologous systems with the central complex.

And I feel like that's an area I'm looking to look into next is to really understand these things, not just looking at the mammal case, but I want to look at the insect case with the central complex and see the similarities and differences.

What special adaptations were there?

To what extent are the functions realized by similar means or different means?

But in terms of a way of understanding things like that neuromodulator hypothesis of influencing the granularity of spatial modeling,

It's possible, although I wouldn't necessarily bet on it based on conversations with you, David, but it's possible that you're getting extensive homology and common significances for things like flexible influence of the granularity of the forging parameters.

It's possible those are remarkably conserved and that you might actually see them.

That being said, like you also were alluding to, though, the SLAM instance is different in insects in that this is a multi-SLAM.

In general, there's a lot of multi-SLAM happening in vertebrates also, but I think not to the same degree.

And so you might expect some potentially substantial differences there as a result of the SLAM problem having that additional set of challenges and sources of information it can leverage.


SPEAKER_01:
I can't wait for solitary wasp slam and eusocial colony slam.

Steven?


SPEAKER_03:
From what I'm hearing, you could correlate these multi-slams to regimes of attention in many ways, because that could dictate the nature of these granularities and where they are in the space, in the place, in the body.

we often assume i think when blue was saying there about there's there's inner outer and the some of those are also false assumptions that that rolls out over everything that we assume so in the sense that inner is purer and higher is purer but often um that might depend on the multi-slam that's been dunked at the time


SPEAKER_01:
Yeah, like coarser and finer, slower and faster.

So one interpretation would be that the slower timescale is the coarser graining in the sense that it's like seconds, not milliseconds, and the finer is the faster.

but there's also maybe an interpretation where actually the, if you're clicking up mile by mile instead of foot by foot, that is actually moving faster and it's coarser.

And if you're counting grains of sand at a time, it's finer and slower.

So I think even sometimes the direction that one interprets is not there, but it doesn't remove the dialectical nature or the intermodal nature, but the way that, is it gonna be like,

A sugar with all the OHs pointing down or all the OHs pointing up or some blend.

It has to go one way or the other, but maybe it doesn't have to be any one specific way.

Blue?


SPEAKER_00:
So I am wondering, and this is also interesting in terms of the insect case, if there's some kind of, maybe like a generic, free-installed, readily adaptable, coarse-grained version of a place cell that's just kind of there as a generic representation.

Like a perfect example, I mean, when you're thinking about core screening, like a map and a location, like you know where everything is in your house, like you know where your bathroom is and you know where all the things are.

But, you know, when you travel, you have an expectation, even if you've never been to the hotel before, you have the expectation of what the elevator is going to sound like.

how the carpet all looks the same with that funky weird pattern and the hallways twist and turn and like when you get into the hotel room there's going to be a bathroom and there's going to be a window and there's going to be a bed and a tv and a remote so you have this like pre-generic supposition of how the hotel is going to look does that live in a play cell somewhere that just like morphs into oh the bathroom's here and the light's here so that you're not stumbling around in the middle of the night like

And similarly, like there's all kinds of, when you go out hiking, you know, you expect there to be trees and rocks and stuff outside.

And so do you just like mash your map onto like some preexisting generic place holder, a place cell place holder?

It's interesting to think about.


SPEAKER_01:
it's like a prior for the abstract space.

It's a prior distribution over hotel rooms, which is totally encultured and totally based upon experience.

And then either one is surprised or not, just as when any other prior meets sense.

Adam, what do you think of that, Steve?


SPEAKER_05:
I mean, these are details that are definitely like half or beyond the edge of anything I or maybe anyone knows.

But the

I guess just like these, how many different, so one way that these place fields are sometimes modeled is in terms of like these like chained attractors.

And so like within the hippocampal system, like you can get like this highly recurrent activity, especially in some of the subfields.

And you can set up like this set of these densities of

recurrent activity and then you can have like information kind of moving between them as a like predictive map not just of like not just like here's the things but you know people debate how the best things like Gershman talks about successor representations but that there's a transition structure of which attractor will act which of the places will activate first in the sequence and

And so I guess the question, the question I'm wondering is like, so like, you know, you're encountering like a new scene and you're like laying down like a set of landmarks or a tiling of space, like to what degree is this information encoded in, in what circumstances and to what degree is it encoded in like the local connectivity within the hippocampal system and whether you're getting like this set of schemas that you're going to like draw up like the most similar one based on

some like degree of similarity will cause like one attractor network to get pulled up over another and now this is the timeline you're working within and this is like the policies the the policy set you're like would be pulling from and this is your framing of the domain um or to what degree is it relying more heavily on like the moments and moments um like cortical dynamics and like the mental simulation that might be happening and like to which extent like is it constrained

by the history of plasticity within the hippocampus, what schemas are going to be operative?

And to what degree is it more of a negotiation where it's going to be more ad hoc, more determined by the moment-to-moment contingencies and this funneling through the whole cortical virtual reality machinery or whatever you want to think of it?

And to what degree is this?

There's not one answer.

This varies both across and within individuals.

that this is actually like a core aspect of cybernetic functioning is actually leaning more heavily on the priors or the current of established over more learning ethics or leaning more on the situationally specific factors.

Like how much do you shoehorn what's going on into your existing schemas?

How much do you simulate?

How much do you accommodate?

And that this being potentially like a flexible parameter and maybe some of that flexibility is

like baked into the logic of neuromodulators like actually flexibly influencing this uh unclear but like yeah so very very edge of anything i know but it's it's very much like part of the excitement for me is like wondering like is this like um does this provide like like a functional and algorithmic significance for like

this otherwise just mess of biological details?

Do some of them have elegant cybernetic understandings by virtue of thinking of these kinds of questions?


SPEAKER_00:
Can I just respond really quick?


SPEAKER_01:
Yes, please, Blue.


SPEAKER_00:
That made me think a whole bunch about like, maybe we don't need a map, a place self for each individual hotel room, but it's like a node when we were discussing nodes earlier, like a coarse grained, just an attractor space, like dump all of the hotel room maps into this little nodule so that when you get to a hotel room, it's all the same.

thing like it's okay i've been here before even if you've never been to that kind of place and and that would kind of explain things like deja vu like when you get to a place like i've been here before when did i come here like when you're driving down the street didn't i just drive down the street you it just is lumped over there into that little attractor mesh node that that's not necessarily a specific place but just like a squishy ball of similar places


SPEAKER_01:
And also, I really love how we spoke about some of these things, Adam, in your collaboration with Colin DeYoung, Cybernetic Big Five, in livestream number 13, I think at the beginning of 2021.

And now we're revisiting.

Same but different.

The more things change, the more they stay the same.

And this idea of like modulatory features of cybernetic systems as being important for variation within and among individuals is just such an essential point.

And it's so true that a lot of the details are yet to be resolved.

Stephen?


SPEAKER_03:
Yeah, you could go a bit further still, because what if you don't have maps put to one side, but there's action policies

actually have interest in how much this is feasible but action policies that recapitulate something like a map so like by an embodiment by an inactive chain of or initiation of some actions at different scales it basically recapitulates some sort of uh nascent starting point so you don't have to have any representations there and like when the door is

engaged the set of actions that that does that creates could open up an effectively a map um so i don't know how much between i mean i know in this model it does start to create a network of attractors but i'm wondering whether how much things could be sparked off from engage in a particular affordance a particular meaningfulness in something


SPEAKER_05:
I mean, there's actually a live debate about the extent to which... So Tolman initially was talking about the hippocampal cognitive map.

And part of the reason people like to talk about maps is because of the flexibility they give you in terms of if you... Like the map serves as a generative model and it gives you like a...

I can flexibly access it to generate trajectories.

But you can do that with a well-structured graph also.

But there's a spectrum of interpretations of people thinking that you're dealing with more things that are more map-like and things that are more context-specific and graph-like and flexible and more dynamically constructed.

And it seems like

It's kind of a cop-out, but in general, I always take every debate, and I'm like, you're probably both right.

And so it's like you want, for the sake of, I guess, maybe stability or for the sake of potentially greater transfer of understandings and inferential power across episodes, you might want something more map-like.

But if you go too map-like, you're losing the ability to deal with the nuances of specific contexts.

And so...

So this, again, might be different perspectives on the same thing that tend to converge with trade-offs.

You could have more context-sensitive or more situationally invariant representations, and these could reflect cognitive styles and maybe fundamental axes of variation for cognition and sense-making.

One thing that's

In terms of like cybernetics, Daniel pointed out like earlier, like, you know, like Jordan Peterson, like part of like, it's actually interesting that maps of meeting Peterson's book was actually heavily informed by Yacht-Pangsepp's affective neuroscience and ecological models more generally, like based on things like animal foraging.

So I actually think that's where Jordan Peterson got that rhetoric about maps is actually from doing a kind of affective neuroscience perspective, grounded

in this ecological perspective of foraging animals.

And so, yeah, I think that's actually where that came from.


SPEAKER_01:
Yes, very nice.

And I think I agree.

We may have even explored as such.

In one paper, Reimagining Maps, that I wrote with RJ and a colleague, Mikel,

we learned about this historical distinction between archival maps and itinerary maps and today we just think about maps as being like one more lumpy category but there's many differences between the map that actually is like a multi-map or mega map and holds a lot of information potentially in a metric way versus the itinerary map

head straight until you see the two big trees, then you're gonna wanna take a left, that type of more conversational or casual or navigable map that is highly contextual.

And with current map systems, that's one of the challenges because we have that mega map, it's downloadable, but it's not gonna fit on your phone.

So what is the projection that's contextual and situational that's actually gonna enable effective action

that is also going to be perceived as map-like by the user.

Yes, Blue.


SPEAKER_00:
So this is like a personal attack in question.

So anybody who spent any kind of time with Daniel knows that he's constantly drawing or writing or writing things down, like all the time, taking notes, taking notes, taking notes.

And I wonder, like, can you take your notes as like a mental map of where you were at that time that you were making that drawing?

Like, does it like...

recapitulate your conversations that you were having, the ideas that were running through your head.

Can you use these drawings that you're always doing as a kind of conversational mental space map?


SPEAKER_01:
If anything, it's the opposite.

I just leave the thought behind.

And if we look at this, I think, okay, something happened by which we wrote this down.

It would be awesome if there was many people with their regime of attention on taking live notes.

I know that's not an approach for everybody, but then we could all look at that shared pheromone distribution

and then make it new again.

And maybe we remember our exact previous thought, though that's impossible, or unlikely.

But instead, we can make the relationship with the material and with those notes like new again.

Steven?


SPEAKER_03:
Yes, maybe a little harder to categorize with language.

But in a way, it's the traces of your action policies, even if it was capturing

The interpretation.

So one thing I'm very interested in is how we realign things to action.

Because we've kind of been enculturated to think it's all about thoughts and feelings, right?

And because active inference puts it in this other domain, well, what does that mean?

And how do we talk about active inference when we're enculturated to think through other things?

natural ways of using english i'm finding that a big challenge myself but i think so as you're laying down those traces are you showing your action policies of how to record something rather than your thoughts if that makes sense and is your thoughts always slightly hidden


SPEAKER_01:
Yeah, great question.

Just like we were thinking about earlier with a sort of substrate, implicit, agnostic, ignored, linguistic, symbolic models, like a Turing tape.

It has infinite space and it can just make infinite grammar.

And so that sort of purely symbolic approach to language.

and then its relationship to thought and experience is one question.

But when we think about finite actual sequences of language use and deployment as action, either keyboard typing or assisted typing technology, or speaking as also a motor action,

it totally changes the game because we're not just in this forest of infinite plausible grammars, but rather one realized and preferred action sequence.

And then, of course, thoughts are unobserved, but the actions are not.

And that's the whole...

um quantum reference frame and share generative model and rhetorical nature narrative nature of cognition and so indeed the the action insight I think is going to be something that we'll keep returning to um we'll have a final closing the plane or whatever and um I'll also just

give a little preview which is that next week we're going to be having a guest stream with Adam so if it relates at all Adam how does it relate what is the guest stream going to be about and then um perhaps in your absence what would be exciting to discuss in 42.2 um no you're right there is a guest stream


SPEAKER_05:
A quick thought though.

So like Blue, you were suggesting like Daniel's notes could serve almost like a kind of memory palace and accessible map.

And then Daniel, you're saying it's more like a stigmatistic mechanism for coordination, like with pointing and cueing.

And that's interesting.

I wonder if when people are reading books, the ability to use the book as a map slash palace gives you better retention than something like a Kindle.

Like the book is like a physical object.

Anyways, so oh yeah, free will.

And so for next week, I would say that basically, so my over hypothesis is that many of the details

that Tim is using for these systems will end up being relevant as abstract interpretations of the core parameters of us as active inferential cybernetic systems, both with respect to basically the basics of it lets us be in the world in an embodied way as living robots.

also in terms of their extension to high-level cognition, scaling active inference to more complex domains.

And so in general, I'm trying to understand with limited success as many details as I can of Tim's work, because I think it's just some of the most promising sources of constraints we have on inference interpretations and sources of hypotheses.

basically whatever details tim is willing to get into i think it would be well well worthwhile to try to grok those and that's what i'm that's what i'm doing um in terms of uh next week uh so some of this is relevant in that um the basic idea would be that um

these LeBay phenomenon, these studies in which people are seeing these slowly building potentials leading up to action that have predictive information, but where they peak in advance of people claiming that they have decided to take actions and where these studies are interpreted as evidence that people don't have free will and we can get into all sorts of conversations about how it makes sense to think of free will in different ways, whether it makes sense to talk about it at all.

but that these studies are usually interpreted as saying that your consciously experienced mental states cannot enter causal streams leading to action.

And so what I'm suggesting is that is not the case and that there's multiple ways in which your subjective states are meaningfully causal.

If we think of what's leading up to something like deciding to move your hand in like a labette experiment,

if that is reflecting a buildup of model evidence over your proprioceptive pose or of you deciding to take that action this kind of evidence accumulation if it's occurring via sophisticated affective inference that this would be a way of understanding it and that part of what would influence how much buildup or not you get would be the specifics of this basically deep tree search

from sophisticated, that we might understand as sophisticated affective inference, as being orchestrated by the hippocampal and entorhinal system as iterative mental stimulation, where you're sampling out these different possibilities of, hmm, what would it be like if I move my arm or not?

And then you're then evaluating that, and you're peeling off these different counterfactuals, and that if across these you get enough

affective charge associated with going down some of these forks eventually this results in threshold crossing but the idea would be that this would be relevant to some of the hippocampal slam work and that you can think of this imaginative planning stage leading up to even seemingly spontaneous actions you can think of this like imaginative phase as being largely orchestrated

by the hippocampal system which might be best understood as a general system for uh or a system for generalized foraging and mapping i don't know if that made sense but that's like some of the connections wow if you still want to come for the guest stream you're welcome to do so


SPEAKER_01:
No, thanks for the awesome summary.

And I know that we'll be able to unpack it a lot more next week.

Blue or Steven, what are some last thoughts?

What was exciting or interesting to you in where we've been and where shall we forage next?


SPEAKER_00:
So I can go.

I was really interested in taking this into semantic space.

And I wonder what other kind of spaces that we can maybe think about mapping out

how we build, how we construct maps, cognitive space, and so forth.

And I'm excited to talk to Tim about where Emergence fits into the hierarchical model.


SPEAKER_01:
Awesome.

Thank you, Blue.

Stephen?


SPEAKER_03:
Yeah, I think keeping the idea

keeping things clean, keeping things nascent and deflatory, even in this case here, is really something I think to continue looking at.

And I'm really curious about what happens when we see that door and that door says you can go down that corridor.

Am I going to go down into a different corridor

regime of attention, I'm gonna stay where I am.

So I think that ability to take these principles and say, okay, what is it that happens when I encounter something and it triggers something?

How much is that?

significant in terms of making things happen and i'm curious then how that can marry with a lot of the mental space work that i do building out metaphor landscapes or actually letting someone explore spatially and one last thing actually that came up and someone said to me that the other day is that he's really curious why when he picks up his plastic pencil it doesn't mean as much to him as when he holds a shell

So there's something about like when that door has got some prior, there's something that you know about being in the woodland versus being somewhere which is dead.

And even that itself could explain a lot about how navigating those places, the eco-psychology of going into a wooded space, a living place, actually can be important for people just in a wellness, what might seem a very woo-woo,


SPEAKER_01:
uh type of stuff actually could be somehow linked to what we're talking about here so yeah brilliant thanks awesome yeah very exciting and fun discussion i really like that last piece which is what i know i'm gonna carry forward and hopefully reflect on next week how will we build that colony is it a memory palace blueprint

is it stigmatic coordination but how and so somewhere between the um mental Palace Cathedral and the sort of picking up pebbles and having no bigger picture is where we are in our cognitive and social Maps and so

that's quite an area and Adam the central complex modeling I can't wait so thank you all for joining thanks Tim also so Adam Steven and blue and Sid and Dean in the chat so thanks everybody and see you next time
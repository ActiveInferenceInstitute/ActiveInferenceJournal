"Speaker Name","Start Time","End Time","Transcript"
"Speaker 1","00;00;07;23","00;01;10;01","Hello and welcome everyone to act in flab live streamed number 32.0 at the HIM Flab. The paper were discussing is going to be stochastic chaos and more called blankets. And it's November 7th. 2021. Let's play our theme welcome to the acting lab everyone. We are a participatory online lab that is communicating, learning and practicing applied active inference. You can find us at the links on this page."
"Speaker 1","00;01;10;27","00;01;33;11","This is a recorded and an archive live stream, so please provide us with feedback so that we can improve on our work. All backgrounds and perspectives are welcome here and we'll be following video etiquette for live streams this short link you can see our past and upcoming live streams. And here on the main tab, the live stream calendar for 2021."
"Speaker 1","00;01;33;29","00;01;54;08","We're here in early November. We had our fourth quarterly lab roundtable summarizing the kinds of projects that you can get involved in and what we had done in the previous year. What we're looking forward to next year. And then the first two weeks of discussion are going to be on November 9th and 16th for this paper that will be discussed tonight."
"Speaker 1","00;01;55;17","00;02;27;27","In extreme number 33, we'll have Abel's paper thinking like a state and we haven't set the papers yet for 34 and 35 the goal of 32.0 is to learn and discuss this paper. Stochastic Chaos and Markoff's Blankets 2021 by Carl First and Connor Hinds can also offer Lancelot to Costa and Thomas part. And just like all the Dodge zeros, this is just an introduction to some of the ideas."
"Speaker 1","00;02;27;27","00;02;50;24","It's not a review or a final word, and especially if you have any experience or you want to learn more about any of these areas and help improve our presentation or understanding of it, they'll be very helpful. You can join a live stream or just come get involved in some of the aspects of the lot because these are things that we want to understand, but also there's a lot to learn."
"Speaker 1","00;02;51;24","00;03;26;06","And in the coming two weeks we're going to discuss this paper so I'm Daniel and I'm in California the big questions of this paper are related to the general discussion in the active inference and free energy principle, literature and beyond. What is a good model of thickness in a chaotic, dynamic and deceptive world? So do we think of things as as they are in the snapshot, or do we have to think of them through time at what spatial or temporal scale or with what kind of measuring stick?"
"Speaker 1","00;03;26;16","00;03;53;26","Does it make sense to talk about things and then how does the result of that first question, this model or approach to thickness speak to system sentience, which is going to be a word that is used in this paper. And notably, this is not being defined in terms of feeling like experiencing it's defined in the paper as where internal states look as if they're inferring external states."
"Speaker 1","00;03;54;02","00;04;25;15","So perhaps closer to what Dennett would call the intentional stance, but this is sort of just pragmatism meets anti dissipate of a third point. What is a Markov blanket and how is one modeled and identified or defined? Statistically, this is going to be the technical bulk and contribution of the paper. So anyone wants to learn more and help us present some of the technical details which we're going to go through in this lecture."
"Speaker 1","00;04;25;15","00;04;56;16","But total disclaimer if it's not accurate or it's an incorrect generalization, a word that wasn't a keyword per se, but maybe is just a qualitative entry point is this idea of flow. And here we have people flowing like a long time exposure in a subway station, and there's water flow, which has several aspects. The water itself is moving, but also there's the flow of energy, there's information flow, and then there's this psychological concept of flow."
"Speaker 1","00;04;57;03","00;05;36;04","And so different aspects of these could be said to be flowing in different ways and people jump across these different areas. So wouldn't it be cool if there were a quantitative model that kind of incorporated some of what was meant by all these concepts, including a science of perhaps perception in cognition, in action? So the big picture is how are we going to connect kind of physical flow models to potentially other kinds like macro systems and informational and even psychological or behavioral."
"Speaker 1","00;05;37;29","00;06;11;27","The paper is stochastic chaos and Markov like it's 20, 21 and two of the aims and claims of the paper of which there are definitely multiple. So this is just a few of them in this work, we attempt an end to end derivation of a mark of blanket, starting with a normal form of stochastic chaos and ending with a functional form of a synchronization map the second aim or a summary of the paper as we conclude with the discussion of the implications and generalizations of the following here stick."
"Speaker 1","00;06;12;19","00;06;40;28","So not necessarily formal, but more like using empirical simulation or approximation proof of principle for understanding, simulating and characterizing an elemental form of sentence in systems that self-organize to non equilibria. So that is where sentence comes in. And again, this isn't just saying that because we did some matrix math, we think it has phenomenological awareness. Sentence is going to be more like intentional stance."
"Speaker 1","00;06;41;21","00;07;11;25","It's resisting dissipation, perhaps it's acting strategically given the affordances in its niche. So one really relevant paper for stochastic chaos and Markoff's Blankets is the paper Bayesian mechanics for stationary processes with multiple of the same authors. And this one was put on archive on June 25th 2021. And then this one looks like it was received by the Journal at July 9th."
"Speaker 1","00;07;11;25","00;07;52;01","20, 21. So one question is like to what extent are these complementary, are these requisite or are they somehow related to each other or are they to independent kind of dimensions of advance? There's some overlapping areas, but also there are some different areas. So that would be good to learn about so there's the keywords of the paper were Bayesian thermodynamics, information, geometry, Variational Inference, and Markov Blanket So let's go just one level deeper into the flow example."
"Speaker 1","00;07;53;01","00;08;29;29","We have a lot of fields like math, physics, information computing, thermodynamics, geometry, a few other fields where there are models of physical flow. So everything from heat engines and heat flow to quantum and classical computation, clouds and everything else. So this is engineering experience and ontology and from multiple different areas of mathematical formalisms that sometimes map very closely to the real world or not."
"Speaker 1","00;08;30;22","00;09;07;10","Then there's these again, flow models or processes models for mental flow, for example, thinking about attentional allocation or informational processing. And the big idea, which is proposed in first at all 2006, at least one phrasing of it and an image that gets referenced back many times is this idea of like a winged snowflake. And so below the phase boundary, so above the temperature of melting ice, the droplets are unable to enact policy."
"Speaker 1","00;09;07;18","00;09;42;29","So they simply fall from face and then they become rain. But above the solid phase boundary, so below the freezing point of water, there are the snowflake structures. So there's an organization and then one can imagine that if this shape above the phase boundary is able to enact action on the environment first, if nothing other than by chance, you would find that there'd be, for example, persistence of the most adaptive action policies."
"Speaker 1","00;09;43;25","00;10;09;00","So that is the exchange and the interfacing of active systems with their environments that precludes the phase transition. So that's the winged snowflake example of early Earth first and, and it will come back at the end in a pretty interesting way. So one of the first cures is Bayesian. Here's Rev Bayes, and we've talked a lot about Bayesian things."
"Speaker 1","00;10;09;00","00;10;40;19","So just to kind of pick a few new images here, some overlapping between frequent ism here and Bayesian ism here and some of the overlaps. So it's just kind of interesting to look at and what exactly does it mean for a theory or for statistics to be Bayesian explicit modeling of priors and here's the sort of classic Bayes theorem, which I'm just not going to go into right now."
"Speaker 1","00;10;40;19","00;11;04;15","I'm sure there's awesome resources to learn more about Bayesian statistics and just sort of the way that it becomes meaningful is just embodied how we want to be able to bring our full knowledge to bear on scientific questions or any other kind of question. We want to be able to integrate different kinds of data sets and have models that can be able to receive data as well as generate it."
"Speaker 1","00;11;05;02","00;11;09;22","These things are all tractable in modern Bayesian frameworks."
"Speaker 1","00;11;12;22","00;11;45;11","Thermodynamics is the voting Wikipedia here branch of physics that deals with heat work and temperature and the relation to energy radiation and physical properties of matter. So there's this sort of thermodynamic patterns or laws and the way that they relate to statistical mechanics. So how the kind of jostling of molecules leads to the ideal gas law or kind of work to capacity in temperature engines."
"Speaker 1","00;11;46;22","00;12;17;18","And then just looking around, there was kind of just some old ideas, some new ideas. So this is something we've talked about before with decision making, but also it's how we think about chemical reactions in terms of the lower activation. Energy is the kinetic product that's like the one that is easily jostled to and then the one that has a higher activation energy is not favored to occur, but then it sometimes can be like a lower thermodynamic product."
"Speaker 1","00;12;17;27","00;12;51;18","So releasing more Delta G. And then if we're going to have some sort of an information thermo synthesis, what does Delta G Informationally look like? Like what is the informational imperative the title of the paper has Stochastic Chaos, so just found a few different citations on it. And it's an idea that has been going back since before 2000s."
"Speaker 1","00;12;51;27","00;13;27;22","So I don't know when the exact beginning was but it's been around. So that was just one question for the authors or for anyone who's knowledgeable about it. What is the scope of stochastic chaos and how? How do we think about the relative importance or meaning of solving one category of dynamic systems? So what is stochastic chaos? Stochastic subject to probabilistic variation in one way or another?"
"Speaker 1","00;13;27;22","00;13;56;02","And we'll kind of like look at multiple ways that that can come into the picture through sampling or approximation. And then chaos, which is sensitivity to small changes, which is summarized at least in one way by the the operative exponent, which is a measure of how closely separated points either converge or diverge as a feature of the shape and a type of the matrices that describe the systems."
"Speaker 1","00;13;57;20","00;14;26;15","Another keyword was information geometry. So I'm definitely not the person to give a specific definition. It would be great to learn more from someone who could link this to the specific modern advances. So a few people who have emailed us, but John Beyers, who is a professor in the UC system, wrote on his site, which has a whole curriculum on information geometry."
"Speaker 1","00;14;27;12","00;15;04;19","Not that I've taken it, but heard it was good information geometry is the study of statistical manifolds, which are spaces where each point is a hypothesis about some state of affairs this subject, usually considered a branch of statistics, has important applications to machine learning and somewhat unexpected connections to evolutionary biology. So this image, it just kind of looks like if these are dimensions of something abstract that projected down onto the graph paper is going to look like a triangle."
"Speaker 1","00;15;05;05","00;15;36;08","And then this shape, which maybe is kind of like a saddle shape, or if the two distal parts are connected, it's like a tetrahedron in a way then that projected down from a specific angle is going to look like a right angled square, but from a different angle it would have a different profile. So the same tetrahedra rotated in different ways can project down into a different geometry."
"Speaker 1","00;15;36;29","00;15;51;01","So kind of interesting to think about that with information. And if you're measuring several axes of information, you have an informational tetrahedron as if it doesn't need to be saying that it is actually that way."
"Speaker 1","00;15;54;08","00;16;21;11","One of the other keywords was Variational Bayesian Inference, and this was from the slides of 26 and some also good blog posts that others have written. It was defined in this blog post by variational inference methods that consist consists in finding the best approximation of a distribution among a parameter sized family. So it's kind of like finding the best linear model for a dataset."
"Speaker 1","00;16;21;23","00;16;49;13","This is a way to fit more complex data and models, but it keeps it constrained within a family so that you're just tuning the knobs that, you know, interact in a good way versus all the possible degrees of freedom in modeling, which is basically infinite. And then two ways that Bayesian approaches can be deployed, many more than this."
"Speaker 1","00;16;49;13","00;17;19;12","Two, there's the sampling based approach or the Monte Carlo like gambling approach, which is like pointillism, where it kind of samples. And then this if the underlying distribution, you're kind of agnostic to it. If you run this properly and the underlying distribution has the right properties, you can sample something that's totally adequate for making good decisions. And then the alternate approach, it's kind of like the SVG approach, the variational inference."
"Speaker 1","00;17;19;12","00;17;51;08","You kind of have a library of cat or mammal subunit vectors, and then that vectors set gets fit in a way that's more tractable than just this sort of endless sampling. Oh, what if there was a whole body and you just missed it? Type questions. However, if there was a whole body in your library or the family of functions you were trying to fit were only whole body, then I mean, you can have a mismatch there, just like you could fit any other model incorrectly."
"Speaker 1","00;17;52;27","00;18;28;13","So the Markov blanket is one of the terms that I think many people are curious about certainly ourselves included. And from number 14 last year or 14, we talked about this continuum slash development of the concept from the way it was phrased by Markov, by the way, we're still trying to figure out which one slash exactly which contribution the father and the son made towards the development in Perl."
"Speaker 1","00;18;28;13","00;19;24;20","1988 in the textbook shown here and this is where computational methods start to come in and some of the pure math assumptions get deployed in real data sets. So then the Markov blanket is defined as the set in a Bayesian graph of insulating nodes. So moving from this matrix representation towards this graphical Bayesian representation and then there's some of the developments by Carl Bernstein and others which this paper is part of that do a few different things relating to connecting or separating the blanketing states into incoming sensory and outgoing action."
"Speaker 1","00;19;25;08","00;20;04;02","So a partitioning of the blanket, as well as implementing some of these components of principle of least action, which is not implicit in just mere statistical installation on a Bayesian graph. So we'll talk about some of that stuff. But I think there's enough to cover in what this paper here adds and a number 14, number 20, number 26, many other papers that we've discussed and many that we haven't had very insightful uses as well as criticisms of Markov concept so what is the free energy principle?"
"Speaker 1","00;20;04;05","00;20;27;11","This keyword? One way to answer it, since it's the key word in many papers, what are three possible attentional regimes that you might be able to expect will reduce your uncertainty about this question, because if you're not worried about this question, it's all good then. But if you are, you want to reduce your uncertainty. So what are the actions to reduce your uncertainty?"
"Speaker 1","00;20;28;12","00;20;54;00","Well, the third sentence of this paper that we're discussing here is in brief, the free energy principle is a variational principle of stationary action applied to a particular partition of states where this partition rests upon conditional in dependencies, which is the other side of the coin of which which things influence each other and which don't kind of two sides of the coin there."
"Speaker 1","00;20;55;24","00;21;22;29","There's been a link to citation 15. So citation 15 if you want to go one more layer into establishing whether this is a merits claim or not, there's first in 2019 monograph free energy principle for a particular physics particular being specific, but also referring to particular or autonomous states as they're known and also much discussion has occurred since this paper."
"Speaker 1","00;21;22;29","00;21;52;19","So look forward to seeing how it develops. And then a third possible attentional regime would be to come participate and contribute with active lab because every day we are reducing our uncertainty about terms as well as any participant can attest. And we're always trying to improve our informational niche through projects like educational courses and ontology development so that the questions that people ask every day, like what is the mark of a blanket?"
"Speaker 1","00;21;52;19","00;22;21;05","Why does it matter? What is the FSP, why does it matter? There will be awesome resources to share about those questions. One other piece before we jump into the paper is just a little bit of matrix math vocabulary. So I hope that I'm just even quoting the definitions correctly because I'm not super familiar with a lot of these distinctions and would think that there's a lot more expertize that people could share, which would be awesome."
"Speaker 1","00;22;21;14","00;22;43;25","So the Jacobian matrix and the determinant is a construct in vector calculus and what's relevant here is that it's the matrix of the first order, partial derivatives. So if you have just like a lump in space, you can take the partial derivative it's like putting a ruler and finding the derivative of a certain axis of a certain variable at that spot."
"Speaker 1","00;22;44;12","00;23;27;08","And so you can find the partial derivatives of higher order models. The hessian matrix is defined as the square matrix of second order, partial derivatives of a function or field. It describes the local curvature of the function of many variables. So same idea. It's like how the linear slope and then also the curvature. So that can tell you a lot in especially a certain case that's going to come into play in how this paper defines its analysis approach, specifically the low portion approximation."
"Speaker 1","00;23;28;03","00;23;56;12","So the abstract. In this treatment of random dynamical systems, we consider the existence and identification of conditional in dependencies at non equilibrium steady state. These in dependencies underwrite a particular partition of states in which internal states are statistically secluded from external states by blanket states. The existence of such partitions has interesting implications for the information geometry of internal states."
"Speaker 1","00;23;57;14","00;24;13;10","In brief, this geometry can be read as a physics of sentiments where internal states look as if they are inferring external states however, the existence of such partitions and the functional form of the underlying densities have yet to be established."
"Speaker 1","00;24;19;08","00;24;57;03","Here, using the Laurens system as the basis of stochastic chaos, we leverage the Helmholtz decomposition and polynomial expansions parameter eyes the steady state density in terms of surprise or self information. We then show how Markov blankets can be identified using the accompanying Hessian to characterize the coupling between internal and external states in terms of a generalized synchrony or synchronization of chaos, we conclude by suggesting that this kind of synchronization may provide a mathematical basis for an elemental form of autonomous or active sentence in biology."
"Speaker 1","00;24;59;09","00;25;40;18","OK, cool. How will that happen? Here's the roadmap. The first section is the introduction the second section from dynamics to densities makes the connection between dynamical models and densities which takes one towards flow. Like if you have water in the ocean of different temperature and densities, you can do the flow modeling. Section three is the Helmholtz decomposition, which we've also revisited elsewhere, and it relates to splitting apart a vector field into two components."
"Speaker 1","00;25;40;29","00;26;35;14","One that's kind of like hill climbing and one that's more like curvature. The Lorentz system classic complexity model is brought up and used as an initial case for this kind of dynamics, two densities, plus Helmholtz decomposition and then some further Lorenz systems other than the Lorenz are analyzed, then the lip loss and systems other than Laplace are analyzed such and for Merkel, blankets and the free energy principle talks about sparsely coupled systems and specifically where the different partitions are engaging in some type of synchrony, which is going to be like correlated action conditioned upon the blanketing states."
"Speaker 1","00;26;37;00","00;27;08;00","That leads to a discussion of particular partitions, boundaries and blankets and a kind of revisiting using a simulation analysis of Markov blankets. The last section is about the free energy principle and mentions active inference so it's pretty interesting just to start with the data availability statement, they wrote that the code used to run the simulations and create the figures for the paper are freely available in the DTM toolbox."
"Speaker 1","00;27;08;00","00;27;45;19","Of the MATLAB package. SPM 12. So if anyone maybe wants to like do SPM live with us, that who knows how to do it, that'd be awesome. SPM stream, it'd be great. So let's start with the introduction slowly because there's going to be some more technical parts later. The last sentence of the first paragraph of introduction is from this one can elaborate a physics of sentence or Bayesian mechanics that would be recognized in theoretical neuroscience in biology."
"Speaker 1","00;27;46;04","00;28;09;27","So imagine if the top half were redacted. What do you think would allow for, quote, the elaboration of a physics of sentence or a Bayesian mechanics? So not possible or it's already been done. Who did it? Or. Well, it would have to have this attribute. It has to be able to run on this type of computer, or it has to have this many variables or fewer."
"Speaker 1","00;28;09;27","00;28;40;17","So what are we looking for here? What would justify this sentence that they started the paper with, and why would it matter? Which is kind of tied up with maybe what would be the motivation for developing it, however much effort it involved? Why would it matter to have a physics of sentence? Well, why did it matter to have a physics of heat let's pull back one more sentence."
"Speaker 1","00;28;42;21","00;29;08;28","The internal states can then be cast as representing in a probabilistic fashion, external states. So that's what they're going to elaborate a physics of sentence on from this partitioning of internal and external states, but not just any partitioning, a particular partitioning where internal states can be cast as representing external states. So this is kind of a minimal representation."
"Speaker 1","00;29;08;28","00;29;34;14","We have internal and external states, and we're not going to define them as even action or sense partitions in the blanket. Yet just there's some partition between internal and external states. And the question is is this how you think that the world is partitioned or how our statistical models are partitioned? What is this partition? And we've talked about that in a few different papers, and that's going to be a big part of this paper."
"Speaker 1","00;29;36;12","00;30;04;08","Then we can pull back one more sentences in the abstract or I mean, in this first paragraph where they define the free energy principle, which were which we read previously. And then specifically, if the states of a system whose dynamics can be described as random or stochastic differential equation, e.g. life on equation, possessive Markov blanket, then an interesting interpretation of their dynamics emerges."
"Speaker 1","00;30;04;24","00;30;32;16","The conditional independence in question means that a set of internal states are independent of another external states, another external set, when conditioned upon blanket states and so that is the blue. Here is the precondition for the red claim, which is going to be what is the basis for in the author's words, elaborating a physics or sentence. So that is kind of where we're going in this paper."
"Speaker 1","00;30;33;13","00;30;59;21","Agree or disagree? There's going to be a lot of specific points where one can raise a qualm, and that's totally OK. And I think a lot of the discussion I hope will be very interesting with people seeing which parts of this presentation resonate with them or their understanding or not. Those are the big questions. What kind of blankets line up with different parts, aspects or partitions of the world."
"Speaker 1","00;31;00;05","00;31;31;28","Map is not the territory, etc. What makes some partitioning in the world act informationally like inferentially anticipatory predictively, et cetera. Why are some partitions able to be cpu's and other partitions are just divided halves of a cup of water? How do partitioning of variables that model the world lead to a physics of sentence and action? Is there another way to get to a physics of sentence and action?"
"Speaker 1","00;31;32;13","00;32;18;17","Is there another way to do physics they describe their approach to part one by applying the Helmholtz decomposition, which we're going to return to. So this is just kind of a preamble, and we'll talk more about some of the key words but they're going to apply the Helmholtz decomposition to the solution of the Fokker Planck equation, describing the density dynamics of any random system then later on, they're still summarizing part one and they're going to say, we approximate the flow from the yellow part with a quadratic expansion, which means the steady state density reduces to a Gaussian form."
"Speaker 1","00;32;19;17","00;32;50;07","So it's kind of like if you fit a quadratic model, a X squared plus B, X plus Z to some data set, it's not a statement that the data set is going to have a derivative that's linear and then a derivative of that. That's a constant. That's not part of the data set of the world. That's about the quadratic model, which is a very constrained model set that you chose to fit, and maybe other models would have fit it better."
"Speaker 1","00;32;50;20","00;33;17;08","Other ones would have had more or fewer parameters. But you knew something about the form of the solution because you constrained it to a certain set of parameter arranged equations, which is why they talk about variational inference. And then they're going to show that for a Lawrence system subject to appropriate random fluctuation, there's going to be this third state, which is independent of the first two states are dimensions."
"Speaker 1","00;33;17;22","00;33;49;15","And this means that the third state has no blanket states and therefore is not is no, there's no particular partition of interest. So one question for anyone who maybe understands the formalism, just why does this claim specifically matter so section two, the aim of this section is to get from the specification of any of a random dynamical systems in terms of its equations of motion to the probability density over its states in the long term future from any initial conditions."
"Speaker 1","00;33;50;14","00;34;25;13","We are only interested in systems that have a limit set in other words, systems that possess in attractor. We want to describe systems that do not necessarily visit. All possible states are not necessarily time reversible or stochastic in nature and potentially chaotic. In short, we are interested in stochastic chaos in systems with a pullback attractor. And then there's several citations from the nineties and maybe that some of the earlier models of it that's how they describe it."
"Speaker 1","00;34;25;24","00;34;59;01","And then they show this formalism where the length of the change in the state of X that's kind of like a derivative sign sometimes is equivalent to a flow. And then these faster fluctuations so kind of like a wave and a ripple, but that's going to describe the change overall of some location. And then that is equivocal, equivocated to the Farquhar Planck density dynamics."
"Speaker 1","00;34;59;01","00;35;57;29","So they're the same thing. That's the equal sign. So that I can't say whether this is just a total tautology, super obvious, or whether this is a conclusion that was only more recently understood. That would be helpful to know, but that's one of the baseline equivalencies in the paper. It's going to be you can have a specification of a random dynamical system, like here's the set of equations that describe how this sensor is returning data and then turn it into a flow and a density estimation framework so the Focker Planck they write the DOT product, the DOT notation indicates a partial temporal derivative in the absence of random fluctuations, i.e. gamma equals zero."
"Speaker 1","00;35;58;17","00;36;37;24","The attractor corresponds to limit set, namely the set of states onto which the solutions of one converge. So there's kind of like some steady state attractor, which is when there's a heat differential, like there's it's warmer outside, it's cooler inside and then there's some point where the thing gets zero when the flow shows exponential divergence of trajectories with positively up an exponential, i.e. real positive eigenvalues in the jacobian of the flow, the system can be said to be chaotic."
"Speaker 1","00;36;38;12","00;37;25;17","So there are systems that kind of converge nicely when they're pulled away a little bit. So let's just say that it stay the same temperature inside and outside in that previous example. Then that equilibrium would be like pretty nice because it would return to it if it slightly were moved away from it. But there's also systems where nearby points diverge, and that's a measure of this approximated by this lyapunov exponent and then the question is for systems that do have positive leading, we often have vectors then what happens to those flow models?"
"Speaker 1","00;37;26;19","00;37;59;18","And then what happens when there's a re-instating of the random fluctuations? How does that sort of system that would be chaotic if it were allowed to just somehow deterministically evolve? So it's like you're fitting the underlying model and it's a little noisy around it. And then you swap out the actual scattering of the real world populations of deer and rabbit for just the partial derivatives that you were estimating."
"Speaker 1","00;38;00;28","00;38;55;24","And then maybe you see that there some bifurcation in the population density. But of course any real population is either going to experience one branch or the other of that bifurcation. So that's kind of the modeling that they're working on. And then when you go from that dynamical systems approximation to the flow, if the flow shows exponential divergence of trajectories, we can impute stochastic chaos so they take one of the classic systems of deterministic chaos, the then dynamical chaos, the Lorentz system, which is kind of a butterfly looking attractor, and they first show the trajectory of Lauren's attractor with and without this reinstating of random fluctuations so we know that there's a divergence, lyapunov divergence"
"Speaker 1","00;38;56;03","00;39;34;04","that's making the system chaotic when there's no random fluctuations. So what happens when that is reintroduced? The middle panels show the corresponding solutions in the three dimensional state space, illustrating the butterfly shape of the limit set. So even when the and the random attractor so the deterministic one is the kind of clean butterfly, and then the one with the random fluctuation, the points are outside of the butterfly."
"Speaker 1","00;39;34;04","00;40;01;10","It's a slightly different shape. But then it it still illustrates it's not a, you know, a, an X in the state space. So it's still is kind of following this, but just it's of course, it's not following the exact tracks that it's on because there's a random fluctuation term but it's still is going to have a certain geometry because this is a model and information geometry."
"Speaker 1","00;40;02;03","00;40;28;11","So that's going to get some transformations in approximations. Lower left panel plots, the fluctuations in the potential evaluated using the laplacian form. This is kind of formalism that is going to come later, but expresses the self information as an analytic function of the states and then the states I was not exactly sure why they were called states rather than dimensions."
"Speaker 1","00;40;28;20","00;41;02;22","So if anyone has a thought on that, that'd be interesting. And then the lower right is the potential function of the first two states basically, or dimensions is shown as an image with the trajectory superimposed. So this is kind of the if this smoothly varying global potential plot with a lighter and the darker representing different values. There's this like dynamical butterfly that's fit from the RI sampling of the true Lorenz all right."
"Speaker 1","00;41;02;22","00;41;30;18","So the key insight from Fig. one is that the underlying system, this is kind of key complexity insight that the underlying system can have very simple and defined rules, yet unpredictable dynamics. So in the case of dynamical equations, it's systems like the Lorentz attractor and many other systems that are just very simple and they only have a few parts yet they result in these very incredible and rich behavior."
"Speaker 1","00;41;31;03","00;42;01;11","And then there's also kind of like simple rules, complex outcomes, you know, ants and humans. So just a simple rule can be underlying something that's giving an extraordinarily complex pattern. And yet if we measure these systems empirically, that have some simple rules underlying them, but there's also this random fluctuation or we don't sample at all, there will be some, but not overwhelming noise."
"Speaker 1","00;42;01;19","00;42;33;04","There will still be dynamical patterns to pull out. So we can still reduce our uncertainty about the location of future points by using a statistical model that we defined that doesn't even have to be similar to the generative process. So if someone said, well, you get money for getting predictions closer to the actual position of the sampling, even if your model that you're fitting to it is like a neural network, or is this kind of model that we're going to discuss here rather than a differential equation, that's actually the true generative model."
"Speaker 1","00;42;33;21","00;43;12;11","You can still do better than just guessing within this cube. So Section three goes to the Helmholtz decomposition and they write. So that was the previous section was we can regard the deterministic Lorenz system as describing the expected flow of a random random dynamical system that is subject to random fluctuation. That was what we just talked about, the flow of such systems which possess a non equilibrium steady state density can be expressed using a generalization of the Helmholtz decomposition into this, a bit of which is erosion, inner rotational and curl free and conservative, which is rotational divergence."
"Speaker 1","00;43;12;11","00;43;53;00","Free components with the latter are referred to as solenoid or flow. For an introduction to the generalization of this Helmholtz decomposition and see Appendix B of 16, which is Bayesian mechanics, streamed over 26. So we're looking for where P dot so the partials of P partial different derivatives A P r zero. So where is that flow equilibrium? Well, that is going to be a function F that has a few terms, including one that's I think introduced in this paper, the housekeeping function."
"Speaker 1","00;43;54;06","00;44;22;06","But it has two terms that are the classic Helmholtz decomposition, which is the solenoid or the tangential current. That's just the spire, the, the ISO contour and the gradient, which is kind of like the up and down. So that's how a total current, an electromagnetic gets decomposed and it's related to kind of fundamental vector field that so let's go and uncover what was hidden by this great box."
"Speaker 1","00;44;22;24","00;44;55;23","So we have the Focker Planck steady state P of X equals zero, P of X is zero. There's some function that's going to be a decomposition of that into a solenoid and a gradient flow component, the Helmholtz decomposition and then this housekeeping term which is in independent in appendix, but it didn't go into in super depth. So I'm curious about what that is, but it results in this whole flow term here is the fancy I this is a fancy J."
"Speaker 1","00;44;56;04","00;45;22;09","The first discipline of part performs a Romanian grain gradient descent on the negative logarithm of the steady state density, which can be interpreted as the self information, which is the negative natural log of P of any given state or as some potential function. So that's kind of like how surprising is that state? The second part of the flow is the sole and idle circulation on the isoquant or of the steady state density."
"Speaker 1","00;45;23;09","00;45;56;13","And then the third form is this housekeeping term that accounts for how changes in the flow operator change so it's an extension of the Helmholtz decomposition, thinking about how to solve dynamical equations. So this is from active 26 the synchronization map. And I'm not going to go into the whole thing because you can check out 26. But the key question is about this sigma this synchronization manifold between the internal and external states conditioned on the blanket states."
"Speaker 1","00;45;57;03","00;46;26;12","So how do you insulate yet have anticipation across that installation and then also from 26, this is kind of what it looks like to see a stochastic trajectory decompose into a gradient and a rotational component. All right. 3.1 our objective is to identify the functional forms form of the self information or potential function that describes the non equilibrium steady state density."
"Speaker 1","00;46;27;03","00;47;00;24","So why that's how surprised you should be? That's the potential function. That's sort of the the front, the fundamental frequency of both the solenoid and the gradient. So it's an important variable to know, but how is self information measured or calculated? How does the self information for different non equilibrium steady state or equilibrium steady states compare? What does it mean for self information to be high or low?"
"Speaker 1","00;47;01;17","00;47;33;17","So to be surprised or not, so first they in formalism for address this set of questions basically for one simple case which I'm not super familiar with the matrix math. So we'll leave it to others to evaluate some of the technical details. So then the follow up. And so basically we could solve that easily without that correction term that arises when flow operators are a function of the states."
"Speaker 1","00;47;33;29","00;48;00;27","Indeed it is the state dependency that underwrites stochastic chaos. So that's like the double pendulum. The reason why it's a chaotic system is because the states at the time depend on the previous time in a way that generates chaos. So there's so much dependance over short term that things can diverge really fast. This present, this presents us with a more difficult problem."
"Speaker 1","00;48;01;09","00;48;33;08","The problem can be us by using polynomial expansions at the flow operator and the potential as follows. Four and states up to polynomial order m so they're going to use polynomial expansion approximation on a complex underlying flow. Functional polynomial expansions restrict and scaffold our model selection approach to a manageable size and tractable computation. So it's kind of like the Taylor series and the full terraces these and other polynomial expansions which can fit functions often that are very complex."
"Speaker 1","00;48;33;15","00;49;20;15","Well, and so this is showing the Taylor approximation to a sine wave and showing how the first approximation gets you like between one and -1. The red line might be totally more than enough, but with only a few more terms being added, all of a sudden it starts fitting really well further and further out. So that doesn't mean can go forever, but the polynomial expansions can be very powerful in the numerical case, like just doing good enough with quick and dirty, as well as in the analytical case here's a figure from SBM 27 textbook, which is a great textbook, by the way, and it's talking about the Voltaire series as a general nonlinear input state"
"Speaker 1","00;49;20;15","00;49;56;13","output characterization system. So NB note to been a good note to you. Voltaire kernels are synonymous with affective connectivity and then they write and this is from the SPM 12 so later than this, but right up from the point of view of regression models monitoring effects can be modeled with nonlinear input output models and in particular the Volterra formulation described above because the kernels are high order, they embody interaction over time and among inputs and can be thought of as explicit measures of effective connectivity."
"Speaker 1","00;49;57;10","00;50;27;09","An important thing about the Volterra formulation is that it has a high face validity and biological plausibility. The only thing it assumes is that the response of a region is some analytic nonlinear function of the inputs over the recent past this function exists even for many complicated dynamical systems with many unobservable state variables. So cool why does it matter to do a polynomial expansion of the functional form of the solution to this flow problem?"
"Speaker 1","00;50;28;00","00;51;11;29","The parameters issue in five allows for state dependent changes in the amplitude of random fluctuations encoded by the leading diagonal of the flow operator. With that functional form of the polynomial, it is straightforward to solve for polynomial coefficients. The flow operator by solving the following simultaneous equations for a series of sample points some of the key words that are from SPM and elsewhere that might come into play to learn more about these topics are expectation maximization optimization, parametric empirical base dynamical systems identification and generalized linearization schemes for fitting GLM to nonlinear systems."
"Speaker 1","00;51;12;27","00;51;45;02","And then something that's not as much in SPM but comes into play later with active inference and hopefully we'll be able to demonstrate this in the literature as we sort of sift through it. Is that action planning amidst uncertainty and feedback with the environment radically changes the nature of that kind of flow estimation problem. You can't use the same model necessarily for the leaf on the stream, something like the winged snowflake that's actively resisting."
"Speaker 1","00;51;46;04","00;52;19;10","So how good is this approximation to use the polynomial expansion of the fitting of the random fluctuations of the right of an amicable system that is chaotic? Inspection of the Lawrence system suggests that a second order polynomial approximation is sufficient given the flows second order in the states this onslaughts so vocab on sorts an assumption about the form of an unknown function which is made in order to facilitate the solution of an equation or other problem."
"Speaker 1","00;52;22;15","00;52;47;26","Has an interesting implication. If the self information can be approximated with a second order polynomial, it means the non equilibrium steady state is approximately Gaussian. This is known as the Laplace approximation in statistics here we generalize the notion of a plural Laplace approximation to cover not just the quadratic form of the log density, but also the sullen oil flow that underwrites non equilibrium dynamics."
"Speaker 1","00;52;49;02","00;53;17;04","Big if true, it has to be. So here's the Helmholtz decomposition. So now we're taking that solution that we had earlier and thinking about it in terms of fitting only up to the second order polynomial expansion of the flow solution. So that is, as I say, what is known as the Laplace approximation in statistics and it's in SPM textbook two."
"Speaker 1","00;53;18;24","00;53;56;09","Here's the actual flow decomposition of the Lawrence system the solenoid, all flows red, the gradient flows blue, and the correction term is gold. The flow is shown as a quiver plot at equally spaced plots so it's kind of like the vectors of different direction at different points and then the right is the same. But for the laplacian based upon the one on the left here, the key difference is that the deceptive part of the flow operator and hessian are positive, definite indicators of loss in which means the gradient flows converge to the maximum of the non equilibrium steady state density."
"Speaker 1","00;53;56;25","00;54;21;00","This is reflected in the blue arrows that point to the center of the space so even though the gradient of the real flow is pointing all the different directions because it's kind of like a mixing flow, so there's all the vectors are going in different directions but then in the laplacian second order polynomial expansion, the blue arrows are all pointing to the center or to some manifold."
"Speaker 1","00;54;21;00","00;54;54;11","That's much more compact. So it's kind of like a converging solution, even though the actual system we defined it to have stochastic chaotic properties so here's SPM looking at the Lawrence structure. Alas, poor Lorenz. I knew it, Horatio. A model of infinite citations of most excellent fancy. It hath returned to the pull back attractor a thousand times. And now how aboard in my imagination it is so that's modified from Hamlet."
"Speaker 1","00;54;55;07","00;55;09;02","Because right after that seemingly very cool solution, they say indeed the equations of motions we have written down are deterministic, implying that there is no stochastic city in the stable parts of the process."
"Speaker 1","00;55;11;27","00;55;32;17","Assuming a fixed initial condition. This speaks to the fact that Helmholtz decomposition of the deterministic Lorenz system is not a description of a dynamical system with random fluctuations, i.e. systems in which the discipline of part of the flow operator is positive definite. We therefore need to look beyond the Lauren's attractor, so that takes us to appropriately 3.34 beyond the Lawrence system."
"Speaker 1","00;55;32;17","00;56;06;07","So, OK, cool. Now, beyond Lauren's system, we go to the chaotic Laplacian so FIG. three, this is a trajectory, kind of like a pointillist trajectory of three states, kind of three dimensions comprising a laplacian approximate motion to a stochastic Lawrence system. So here it's second by third, first by third, first by second of an s density. So it's showing three projections like the GB cover onto a two dimensional and notice how the light is in the center."
"Speaker 1","00;56;06;17","00;56;40;10","So it's like they're getting pulled back to the peak of the Gaussian on each dimension so that's the non equilibrium steady state, which by construction in the Laplacian is a multivariate Gaussian. That's super key and extremely interesting the middle panels show deterministic and stochastic solutions as a function of time, while the right panel plots the same trajectories. In this database, the shape of the attractor attains a butterfly like form, but is clearly different from the Lauren's attractor."
"Speaker 1","00;56;40;21","00;57;10;19","So what do you see when you look at this attractor? Be honest, but it has a butterfly like shape, and the generated trajectory has some similarity with the underlying generator function, but it has such a nicer form to solve. And then the lower left is the self information, the potential as a function of time based upon the analytic form for the equations of motion and the deterministic trajectories of the previous panel."
"Speaker 1","00;57;11;23","00;57;51;01","And then the flow of the laplacian, which is the approximated expansion, nicely solvable, etc. against the Lorenz that's the true flow is evaluated at 64 spaced points. It can be seen that although there is a high correlation between the flow of the laplacian or systems, they are not identical. So not super visible could have provided like an R squared or some other information, but clearly they are drawing from the same manifold so if this is good enough to stay alive, then society upper panels, does this focus on those?"
"Speaker 1","00;57;51;01","00;58;48;08","So this is again the projection onto two by three, one by three and one by two. Of three of this multivariate Gaussian the the plus in approximation, which makes it really nicely solvable. So one can then estimate the lyapunov dimension which is approximating to the house or house Dorff dimension, which in this example was 2.48 so the Laplacian approximation was used to sample and then estimate the Lyapunov dimension and then it turns out that that compares very closely 2.43 and 2.48, which in the grand scale of things where above one means chaotic system, it means that the Laplacian expansion has some of the nicely solvable characteristics of the generalized laplacian like the multivariate Gaussian and"
"Speaker 1","00;58;48;08","00;59;16;09","the easy computation. Yet it also identifies this system as chaotic because it's still gives a positively opposite exponent from this sampling. In short, the flows of the Lorenz system and it's the plus approximation having a tracking set with between two and three dimensions for our purposes, the approximation is easier to handle than the Lawrence system because the functional forms of the flow and potential are immediately at hand."
"Speaker 1","00;59;17;08","00;59;55;02","So pretty interesting. Figure four So now this recalls our earlier discussion of the Jacobian and the Hessian. So let us recall the Jacobian is the first order partial derivatives, the Hessian is the second order, partial derivatives. OK, so we have the Jacobian, the Hessian and the covariance between these three dimensions. So the Lauren's attractor was in three dimensions."
"Speaker 1","00;59;55;02","01;00;33;15","OK, it was in state one, two and three. That's kind of why there's this like thermodynamic states. So they are dimensions. So the learned structure was this like kind of, you know, ants flying around in space in three dimensions. So we fit a polynomial expansion specifically to those three dimensions, the measurements of them. Now for those three dimensions, we can talk about their correlations in the first partial derivative and the second partial derivative and the covariance that's in the top here."
"Speaker 1","01;00;34;26","01;01;02;00","In this example, the third state is independent of the first pair, where the independence rests on the directed coupling from the third to the first state and so it's logged to show the specialty. So do you see where we're going with the Mark Blanket? See how this third state not part of the Lauren's system, underlying differential equations, it's actually just a total figment of the lip loss approximation model."
"Speaker 1","01;01;02;27","01;01;36;09","But it's quite an interesting pattern the middle panel shows slices through the steady state density over the two states and increasing values of the remaining state so the only correlation in place between the first and second state. So this is related to sphere, a city error correction and evaluation. And it's a topic that is discussed in a SPM textbook but basically spherical errors are the ones where the two axes are not correlated."
"Speaker 1","01;01;36;09","01;02;04;21","But then when there's nonphysical errors, then there's you get a different shape. The lower panel, the correlation is illustrated in terms of the conditional density over the first state given the second. And so this is kind of showing that by and large it's doing well largely confined to the 90% credible intervals would be good to see some, you know, simulation statistics and raw data on that."
"Speaker 1","01;02;05;18","01;02;38;26","So again, alas, I knew him well. The preceding treatment leverages the simplicity of the lacrosse approximation to stochastic chaos in which sparse city constraints on the Hessian are easy to identify or implement. Now, just like Lorenz was dropped. Now this is going to be going beyond the class system. So I'm going to go into too much detail here, but that's well, that's a section that people could explain more about."
"Speaker 1","01;02;38;26","01;03;01;11","Or we could look at more in the dot one, the dot two. So 3.5 summary. It is important to not conflate the simplicity of a ness density with the complexity of the underlying density dynamics. In other words, when prepared or observed in some initial state, the probability density can evolve in a complicated and itinerant fashion on various sub manifolds or the pull back attractor."
"Speaker 1","01;03;02;05","01;03;37;22","So the map is not the territory that something that has come up many times. The Laplacian is not the Lorenz, but the Laplacian is a good approximation to the Lorenz in their simulations FIG. six. So now the chaotic Laplacian system is it's kind of shown in a time series way where this is the true relationship between the first and the second state."
"Speaker 1","01;03;37;22","01;04;12;14","So let's go to here so we see the first and the second state here is like where there's this nonphysical error. And so this is kind of like an interesting like coupling in the model that's independent of the random fluctuations. And so that represents something very deep about the structure of the loop. Lastly, an approximation to the Lorenz now using or placing in a generative capacity there's this initial mis attunement in there set up."
"Speaker 1","01;04;12;14","01;04;39;16","It starts somewhere far from the pull back attractor. And then like the correlation evolves through time and then they say the density converges to the steady state density after about 6 seconds. However, it takes a rather circuitous route from this particular set of states. OK, so that's interesting. It starts with like there were differently correlated because of they starting in a weird place and then they found their true correlation structure."
"Speaker 1","01;04;40;04","01;05;07;19","Note that the average density over short periods of time can be highly non Gaussian, even though the density at any point is by construction Gaussian something to think about so then we get to finally mark all blankets. So they repeat the analysis, but now they approximate two Lorenz systems that are coupled to each other through the respective first states."
"Speaker 1","01;05;08;00","01;05;34;13","So the first and the second states were coupled. So it's almost like one of our coupled states we're going to couple out now this induces a richer, conditional, independent structure from which one can identify internal and external states that are independent when conditioned upon blanket states. So what does it mean to sparsely coupled systems and what are these, quote, first states, the first dimension FIG. seven."
"Speaker 1","01;05;34;13","01;06;13;14","So now there's generalized synchrony in a laplacian system so this is now the same figure as FIG. three like this, except now we're looking at the generalized synchrony. So now there's a synchronization manifold in the first state. When those two got linked up so now there's more lines because there's like two coupled chaotic systems, but the middle is showing that they're state spaces are coupled and this is illustrating the degree of synchronization and then again showing that their flows are correlated."
"Speaker 1","01;06;13;20","01;06;48;04","Those states figure eight now instead of having three by three in the matrix correlation and covariance in the first and the second derivatives now it's six by six. So here is now the identification of using the log of the Jacobian and the Hessian to identify different partitions so that partitioning between the one, the four between that first state of both is the one that we knew about."
"Speaker 1","01;06;48;04","01;07;28;18","That's like the one that was wired into the system. But then the other ones are showing other correlation patterns. Now for I don't know why this four and five connection is not included in the box, but these are the partitioning and so the sparse city structure of the covariance supports a particular partition into internal states five and six, dark blue active fourth state, the one that induces the one that connects here sensory first state and external second and third state."
"Speaker 1","01;07;29;04","01;08;04;13","This partition is illustrated here. And the remarkable thing is that despite their conditional independence, there are correlations between internal and external states and here between the second and fifth states so interesting patterns showing these evolving through time. Nine. So now they're going to look at the partial correlation of states and showing that the Hessian is recovered so here is like that hessian on the top, right?"
"Speaker 1","01;08;05;20","01;08;34;04","And then now they're going to show that it's recovered. It's a little bit light, but it's showing how these so so these squares are showing the partial correlation coefficients and hopefully pulling out the similar structure as identified here. Again, like the strong correlation. Well, yeah, we'll investigate that more in the dot one, the dot two and then showing how like the third and the sixth state, for example."
"Speaker 1","01;08;34;15","01;09;05;21","So that's like three and six. They're not supposed to be coupled and so then they initially start off with a high correlation because maybe they start in the same neighborhood, but then they converge towards a zero correlation. So that is sparse coupling it does not depend upon Gaussian assumptions about the non equilibrium steady state density. It implies that the dynamical influence graphs with absent or directed edges admit a mark called blanket, which may or may not be empty."
"Speaker 1","01;09;06;09","01;09;31;19","These independent CS can be used to build a particular partition using following rules, capture the flag figure ten so here's where we see kind of the classic bacteria example. But now it's two bacteria communicating. I'm not sure if it was always that way. And then here is a little bit more of a coupled system, as with two times three, which is pretty interesting."
"Speaker 1","01;09;32;21","01;09;47;22","So here is where we see the mark all blanket as we've seen it before, but also in a new way. How great. Here's some technical definitions of the mark all blanket. So not for tonight, but for another night."
"Speaker 1","01;09;49;23","01;10;19;00","So here's a question to ask themself. Why? Why one might ask why does a particular partition comprise four sets of states? I don't know why is the tetrahedra the smallest polyhedra? Who knows? In other words, why does a particular partition consider to mark all boundaries, sensory and active states so why can we skip straight to the first and blanket rather than just identifying the purple 1988 or the mark of earlier model."
"Speaker 1","01;10;19;22","01;10;43;24","The reason is that the particular partition is the minimal partition that allows for directed coupling with blank it states. So that again is from Fig. ten. And the key thing to note from FIG. eight is that there are profound variances between some internal and external states, despite the fact they are conditionally independent so we're defining like nodes that are not connected or conditionally independent."
"Speaker 1","01;10;43;24","01;11;10;01","That's how the Bayesian graph works. So here one and four get correlated, get a connection with each other. That's this connection right here, right there and right there. So now they're coupled one and four so even though some of the internal states are so now it's like the external states are at minimum two and Bucky Fuller, famous quote, unity is plural."
"Speaker 1","01;11;10;01","01;11;49;08","At minimum two. The internal states have to be at minimum two, external states have to be at minimum to so that's pretty interesting because they. Yeah, yeah. The particular partition is the minimal partition that allows for directed coupling with blanket states that then there the second the fifth states are highly correlated yet are twice removed. And so this is leading to a general synchrony through partial coupling of dynamical systems."
"Speaker 1","01;11;50;29","01;12;29;20","Note this is their note. There is no claim that either the original Lorenz system or coupled Lorenz system possesses a miracle blanket. The claim here is that there exists a Laplace approximation to these kinds of systems that in virtue of the zero elements of the ocean, feature more blankets. So, oh, how many days we having a Markov being a Markov modeling a Markov no claim well, applause approximation approach to systems identification can add constraints and make the problem easier to solve."
"Speaker 1","01;12;30;16","01;12;58;10","The Markov blank is a rising literally within the Laplace approximation, which is just an approximation. It's not a description of the system. So onto the free energy principle but the existence of this. Any particular partition means that one can provide a stipulated of definition of the conditional density over external states being parameter ized by the conditional expectation of internal states."
"Speaker 1","01;12;58;10","01;13;41;03","Given sensory states, we call this a variation density parameter rise by expected internal states. You so going from the minimal coupled systems to there are still some persistent covariance this and these covariance is partition out into the Markov blanket what if the expectation of the internal states were of external states, in other words, or inwards for every sense for state, there's a conditional, but it has to be conditional on a blanket state, which is how the whole thing is set up inwards."
"Speaker 1","01;13;41;08","01;14;04;13","For every sensory state, there's a conditional density over external states and conditional density over internal states, where internal and external states are conditionally independent. This admits the possibility of a DeFeo more big map between the sufficient statistics of the respective densities. The existence of this mapping rests upon a continuously differentiable and an invertible map, which is linear under a Laplace approximation."
"Speaker 1","01;14;04;13","01;14;28;07","So DeFeo morphic is like stretchable, so that allows some of these formalisms to arise again. Would be awesome. To hear what other people can bring to the table about this. But this is like the energy minus entropy form, the divergence term plus the self information about the policy and then the accuracy minus the complexity. So would be awesome to learn more about this."
"Speaker 1","01;14;29;23","01;14;58;17","OK, the font got a little out of control here, but this functional, the big one is going to be expressed in a few different forms. So it's an expected energy minus the entropy that's energy minus entropy, self information plus tail divergence. OK, self information plus negative log likelihood of particular states and the scale divergence accuracy minus complexity. OK, accuracy minus complexity."
"Speaker 1","01;15;00;06","01;15;48;05","It's also in the machine learning context evidence lower bound elbow. This is the basis of the free energy principle. Put simply, it means that the expected internal states of a particular partition at non equilibrium steady state can be cast as encoding conditional or Bayesian beliefs about external states. Equivalently equivalently the flow in the internal manifold can be expressed as a gradient flow on a variation of free energy that can be read as self information this license is a somewhat teleological and directed description of self-organization as self evidencing in the sense that the surprise or self information that constitutes the potential is known as log model evidence or marginal likelihood in Bayesian statistics."
"Speaker 1","01;15;48;14","01;16;22;18","So wow, great fip work what does it mean to talk about the physiology perspective here though? So what does it mean for self evidencing in the biological context? The blood vessels want to observe themself working. So now in section 51 getting hopefully towards the end here in alternative this is later in section five, one in alternative and deflationary perspective rests on noting that free energy gradients are also the gradients of self information."
"Speaker 1","01;16;22;26","01;16;50;01","So the organism wants to be minimally surprised about some chaotic pullback attractor. So whether or not things are wacky out there or not having this updating flow model that comes to a pullback attractor that is of the approximation that the model or controls is very important and the organism wants to be minimally surprised about the expectations of preferences that it has over observations."
"Speaker 1","01;16;50;14","01;17;14;01","And that is defined with some technical details here. So this formulation of gradient flows is simpler and shows that they're effectively minimizing a different sort of prediction error, namely the difference between particular states and the expected values and on equilibrium, steady states this leads to exactly the same gradient flows, but a complimentary interpretation in which autonomous states are drawn towards their steady state expectation."
"Speaker 1","01;17;14;15","01;17;36;19","Here, mystically one could imagine this kind of stochastic chaos as opt to describe the motion of a moth attraction towards a flame, but constantly being thwarted by turbulence, i.e. solenoid will air currents because active states influence sensory states and possibly external states. This would look as if the particle, e.g. the moth, was trying to attain its most likely state in the face of random fluctuations and so on."
"Speaker 1","01;17;36;19","01;18;35;08","Oil Dynamics. This perspective emphasizes the active part of self evidencing, sometimes referred to as active inference so how cool. First in 2006 the winged snowflake 21 like a moth to the flame 5.2. So they give more summarization here about that sequence that they took of constructing the laplacian approximation with a quadratic form. And then looking at that first and second partial differential partial derivatives matrices, the jacobian in the Russian looking at the conditional and dependencies, then the generalized synchronization synchronizing given the partitioning and then suggesting that there are some diffuse morphic like stretchable connection between the internal and external states conditioned upon all of that."
"Speaker 1","01;18;35;08","01;19;06;26","And then that's the Bayesian mechanics in which internal states on average parameter beliefs of a Bayesian sort about external states. OK, so some of the implications in the last few slides here. So clearly the worked example based upon sparsely coupled Lorenz systems does not mean that a conditional synchronization map exists or is indeed invertible, which would allow the tale of two densities, the recognition and the generative model in any given system."
"Speaker 1","01;19;08;15","01;19;41;20","So you can't just specify any system potentially and find a conditional synchronization map. However, the above derivations can be taken as existence of proof that such manifolds and accompanying variation of free energy formulation can emerge from sufficiently sparse coupling so this is one system, classic system of complexity analysis, and it worked here. Maybe there's other systems where it will or won't a particular partition is necessary to talk about states that are internal to some particle or person and which can be distinguished from external states."
"Speaker 1","01;19;42;04","01;20;19;00","One way of understanding the ensuing coupling between internal and external states is in terms of generalized synchrony. That's that synchronization manifold. This synchronization can be expressed as a variational principle of least action using the notation notion of variation free energy. So this is from another de Costa paper the synthesis on discrete state spaces and it is talking about how that variation of free energy minimization of perception and planning as inference about observations can be seen as a generalization."
"Speaker 1","01;20;19;17","01;20;46;26","Here's active inference. If you have no prior, then you're your surprise, pure surprise, optimal surprise intrinsic motivation info gain info max principle because it's just about what's surprising given no priors when there's no ambiguity. So when there is a model that abstracts away from, for example, measurement error, you have risk sensitive policies and scale divergence based control and the arms principle."
"Speaker 1","01;20;47;12","01;21;23;27","And there's other cases like Bayesian decision theory, expected utility theory as well as maximum entropy. And Jane so it's kind of there's a lot of pieces that are coming into play. When we talk about active inference, probably some that will still are yet to learn but there are some pretty interesting patterns arising from this partitioning. Here's another form, another implication so the polynomial form of the Helmholtz decomposition from formalism five may provide a generic model for observe random dynamical systems."
"Speaker 1","01;21;24;10","01;21;53;14","In other words, it could be the basis of a forward or generative model that explains some empirically observed flow estimated using the first and second moments to quantify the flow and covariance of random fluctuations. Or overstate space respectively. This kind of generative model is appealing because of its parameter position in terms of the underlying non equilibrium steady state density in other words, one could in principle try to explain empirical data in terms of a Gaussian steady state density, which may include constraints on conditional dependencies."
"Speaker 1","01;21;53;26","01;22;25;08","So let's think back to the beginning with flow and information flow and mental flow and brain, it moving down a quarry or all these kinds of flow. And then could there be something that integrates across them another option for scaling this approach to high dimensional dynamical systems would be to learn the state dependency of the flow operator and the non equilibrium steady state density using unsupervised learning approaches such as deep neural networks."
"Speaker 1","01;22;26;04","01;22;55;20","A similar approach has become popular in the deep learning literature in the form of neural stochastic differential equations so here's two recent stochastic differential equation papers, and then they bring up this very interesting suggestion. One could use a separate feedforward neural network to parameterize the different components of the flow operator and the self information. This would require differentiable transform to be applied to the output layers of the network to constrain the hessian."
"Speaker 1","01;22;55;20","01;23;23;06","And so an oil flow operator to be positive, definite and anti symmetric respectively. So suggesting some neural network architectures that might facilitate active inference then this is a pretty nice piece. It is interesting to relate flows across a markup boundary to the construct of law, which, like the free energy principle, sinks a normative account of the structure and dynamics of complex systems."
"Speaker 1","01;23;23;25","01;23;54;11","In the case of the Construct A Law, this is articulated in terms of maximizing external access to the currents, internal to the system. So here is Adrian Bojan, who's like an awesome professor and researcher who's done many years of work like Kristen in some ways on this construct of law. So I was quite happy to see that connection here's another final implication."
"Speaker 1","01;23;54;24","01;24;20;28","One practical implication, finally, the practical implication, great. It licenses the use of inverse sample covariance matrices of a sufficiently long time series as partition, potentially sufficient description of the steady state density. So we could take a real time series and fit that laplacian approximation. This would furnish a description of the expected flow and often experience to establish whether the system was chaotic or not."
"Speaker 1","01;24;22;08","01;24;56;11","In conclusion, last sentence having a simple functional form for the flow of random dynamical systems may be useful for both modeling and analyzing time series generated by real world processes that are far from equilibrium that may or may not be chaotic so we can think of two chaotic systems the humans and the ants and then everything else. So pretty interesting paper in the coming two weeks."
"Speaker 1","01;24;56;14","01;25;43;04","Hopefully we'll have some good group discussions. We're going to email the authors. Now that we've done our work on the zero and would be awesome to have them on for the two discussions we'll have or for any other time thanks everybody who was watching. I hope if you are watching to the end, you found this interesting and come get involved with active Lab because we could all have some shared resources so that some of these questions that are really being advanced in this paper like the connections to complexity and dynamical systems theory and statistical thermodynamics, these are important contributions and this is not a fringe apologetics for act of inference."
"Speaker 1","01;25;43;11","01;26;07;05","It's barely mentioned in the paper. It's actually some really fundamental work that I hope is understood and critiqued where valid. So what would a good understanding here enable of the kinds of things that are discussed in this paper which came out in 20, 21, but think into the future. What are the unique predictions and implications of this paper?"
"Speaker 1","01;26;07;20","01;26;28;13","What are the next steps for FP and active inference? What are the goals of this research and what are you still curious about? So thanks for sticking around for a long live stream at a different time, but so it goes. So have a good one everyone. Hopefully see you around the lab or around by."

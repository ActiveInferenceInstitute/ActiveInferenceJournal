SPEAKER_00:
hello and welcome everyone to actin flab live stream number 32.0 at the actin flab the paper we're discussing is going to be stochastic chaos and markov blankets and it's november 7th 2021 let's play our theme

Welcome to the ActInf Lab, everyone.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links on this page.

This is a recorded and an archived live stream, so please provide us with feedback so that we can improve on our work.

All backgrounds and perspectives are welcome here, and we'll be following video etiquette for live streams.

At this short link, you can see our past and upcoming live streams.

And here on the main tab, the live stream calendar for 2021, we're here in early November.

We had our fourth quarterly lab roundtable summarizing the kinds of projects that you can get involved in and what we had done in the previous year, what we're looking forward to next year.

And then the first two weeks of discussion are going to be on November 9th and 16th for this paper that will be discussed tonight.

In stream number 33, we'll have Abel's paper, Thinking Like a State, and we haven't set the papers yet for 34 and 35.

The goal of 32.0 is to learn and discuss this paper, Stochastic Chaos and Markov's Blankets, 2021.

By Carl Fristen, Connor Hines, Kai Oldshoffer, Lancelot DaCosta, and Thomas Parr.

And just like all the dot zeros, this is just an introduction to some of the ideas.

It's not a review or a final word.

And especially if you have any experience or you want to learn more about any of these areas and help improve our presentation or understanding of it, that'd be very helpful.

You can join a live stream or just come get involved in some of the aspects of the lab because...

are things that we want to understand but also there's a lot to learn and in the coming two weeks we're going to discuss this paper so i'm daniel and i'm in california the big questions of this paper are related to the general

discussion in the active inference and free energy principle literature and beyond what is a good model of thingness in a chaotic dynamic and dissipative world so do we think of things as as they are in the snapshot or do we have to think of them through time at what spatial or temporal scale or with what kind of measure stick does it make sense to talk about things

And then how does the result of that first question, this model or approach to thingness speak to system sentience, which is gonna be a word that is used in this paper.

And notably, this is not being defined in terms of feeling like experiencing, it's defined in the paper as where internal states look as if they're inferring external states.

So perhaps closer to what Dennett would call the intentional stance.

this is sort of just pragmatism meets anti-dissipativism third point what is a markov blanket and how is one modeled and identified or defined statistically this is going to be the technical bulk and contribution of the paper so

Anyone wants to learn more and help us present some of the technical details, which we're going to go through in this lecture, but total disclaimer, if it's not accurate or it's an incorrect generalization.

A word that wasn't a keyword per se, but maybe is just a qualitative entry point is this idea of flow.

And here we have people flowing like a

long time exposure in a subway station and there's water flow which has several aspects the water itself is moving but also there's the flow of energy there's information flow and then there's this psychological concept of flow and so different aspects of these could be said to be flowing in different ways

and people jump across these different areas.

So wouldn't it be cool if there were a quantitative model that kind of incorporated some of what was meant by all these concepts, including a science of

perhaps perception, cognition, and action.

So the big picture is how are we going to connect kind of physical flow models to potentially other kinds like macro systems and informational and even psychological or behavioral.

The paper is Stochastic Chaos and Markov Blankets, 2021.

And two of the aims and claims of the paper, of which there are definitely multiple, so this is just a few of them,

In this work, we attempt an end-to-end derivation of a Markov blanket, starting with a normal form of stochastic chaos and ending with a functional form of a synchronization map.

The second aim or really like a summary of the paper is we conclude with the discussion of the implications and generalizations of the following heuristic.

So not necessarily formal, but more like using empirical simulation.

or approximation proof of principle for understanding simulating and characterizing an elemental form of sentience in systems that self-organize to non-equilibria so that is where sentience comes in and again this isn't saying that because we did some matrix math we think it has phenomenological awareness sentience is going to be more like intentional stance it's resisting dissipation perhaps it's acting strategically given the affordances in its niche

So one really relevant paper for Stochastic Chaos and Markov Blankets is the paper Bayesian Mechanics for Stationary Processes with multiple of the same authors.

And this one was put on archive on

june 25th 2021 and then this one looks like it was received by the journal at july 9th 2021 so um one question is like to to what extent are these complementary are these requisite or are they somehow related to each other or are they two independent kind of dimensions of advance

There's some overlapping areas, but also there's some different areas.

So that would be good to learn about.

So there's the keywords of the paper were Bayesian, thermodynamics, information geometry, variational inference, and Markov blanket.

So let's go just one level deeper into the flow example.

We have a lot of fields like

math, physics, information, computing, thermodynamics, geometry, a few other fields where there are models of physical flow.

so everything from heat engines and heat flow to quantum and classical computation clouds and everything else so this is a engineering experience and ontology and um from multiple different areas of mathematical formalisms that sometimes map very closely to the real world or not

Then there's these, again, flow models or process models for mental flow.

For example, thinking about attentional allocation or informational processing.

And the big idea, which is proposed in Frishton et al.

2006, at least one...

phrasing of it in an image that gets referenced back many times is this idea of like a winged snowflake and so below the phase boundary so above the temperature of melting ice the droplets are unable to enact policy so they simply fall from phase and then they become rain

But above the solid phase boundary, so below the freezing point of water, there are the snowflake structures.

So there's an organization.

And then...

one can imagine that if this shape above the phase boundary is able to enact action on the environment, first, if nothing other than by chance, you would find that there'd be, for example, persistence of the most adaptive action policies.

so that is the exchange and the interfacing of active systems with their environment that precludes the phase transition so that's the winged snowflake example of early or friston and it will come back at the end in a pretty interesting way

So one of the first keywords is Bayesian.

Here's RevBase, and we've talked a lot about Bayesian things.

So just to kind of pick a few new images, here's some overlapping between frequentism here,

and Bayesianism here and some of the overlap.

So it was just kind of interesting to look at and what exactly does it mean for a theory or for a statistics to be Bayesian explicit modeling of priors.

And here's the sort of classic Bayes theorem, which

just not going to go into right now.

I'm sure there's awesome resources to learn more about Bayesian statistics.

And just sort of the way that it becomes meaningful is just embodied how we want to be able to bring our full knowledge to bear on scientific questions or any other kind of question.

We want to be able to

integrate different kinds of data sets and have models that can be able to receive data as well as generate it.

These things are all tractable in modern Bayesian frameworks.

Thermodynamics is the, quoting Wikipedia here, branch of physics that deals with heat, work, and temperature, and the relation to energy, radiation, and physical properties of matter.

So there's the sort of thermodynamic

patterns or laws and the way that they relate to statistical mechanics.

So how the kind of jostling of molecules leads to the ideal gas law or kind of work capacity in temperature engines.

And then just looking around, there was kind of just some old ideas, some new ideas.

So this is something we've talked about before with

it's decision-making, but also it's, um,

how we think about chemical reactions in terms of the lower activation energy is the kinetic product.

That's like the one that is easily jostled to.

And then the one that has a higher activation energy is not favored to occur, but then it sometimes can be like a lower thermodynamic product.

So releasing more delta G. And then if we're going to have some sort of an information thermosynthesis, what does...

delta g informationally look like like what is the informational imperative the title of the paper has stochastic chaos so just found a few different citations on it and it's an idea that has been going back since before 2000s so i don't know when the exact beginning was but it's been around so that was just one

question for uh the authors or for anyone who's knowledgeable about it what is uh the scope of stochastic chaos and how how do we think about the relative importance or meaning of solving one category of dynamic systems so what is stochastic chaos stochastic

subject to probabilistic variation in one way or another and we'll kind of like look at multiple ways that that can come into the picture through sampling or approximation and then chaos which is sensitivity to small changes which is summarized at least in one way by the lyopinib exponent which is a measure of how closely separated points either converge or diverge

as a feature of the shape and the type of the matrices that describe the systems another keyword was information geometry so i'm definitely not the person to give a specific definition it would be great to learn more from someone who could link this to the specific

modern advances, so a few people who have emailed us.

But John Baez, who is a professor in the UC system, wrote on his site, which has a whole curriculum on information geometry, not that I've taken it, but heard it was good.

Information geometry is the study of statistical manifolds, which are spaces where each point is a hypothesis about some state of affairs.

This subject, usually considered a branch of statistics, has important applications to machine learning and somewhat unexpected connections to evolutionary biology.

This image, it just kind of was like, if these are dimensions of something abstract, that projected down onto the graph paper is going to look like a triangle.

And then this shape, which maybe is kind of like a saddle shape, or if the two distal parts are connected, it's like a tetrahedra in a way.

then that projected down from a specific angle is going to look like a right angled square but from a different angle it would have a different profile so the same tetrahedra rotated in different ways can project down into a different geometry so kind of interesting to think about that with

Information and if you're measuring several axes of information you have an informational tetrahedra as if it doesn't need to be Saying that it is actually that way One of the other keywords was variational Bayesian inference and this was from the slides of 26 and some also good blog posts that others have written and

It was defined in this blog post by variational inference methods that consist in finding the best approximation of a distribution among a parameterized family.

So it's kind of like finding the best linear model for a dataset.

This is a way to fit more

complex data and models, but it keeps it constrained within a family so that you're just tuning the knobs that, you know, interact in a good way versus all the possible degrees of freedom in modeling, which is basically infinite.

And then two ways that Bayesian approaches can be deployed, there's probably many more than this too,

There's the sampling based approach or the Monte Carlo like gambling approach, which is like pointillism where it kind of samples.

And then this, if the underlying distribution, you're kind of agnostic to it.

If you run this properly and the underlying distribution has the right properties, you can sample something that's totally adequate for making good decisions.

And then the alternate approach, it's kind of like the SVG approach, the variational inference.

You kind of have a library of cat or of mammal subunit vectors.

And then that vector set gets...

fit in a way that's more tractable than just this sort of endless sampling, oh, what if there was a whole body and you just missed it type questions.

However, if there was a whole body in your library or the family of functions you were trying to fit were only whole body, then, I mean, you can have a mismatch there, just like you could fit any other model incorrectly.

So the Markov blanket is one of the terms that I think many people are curious about, certainly ourselves included.

And from number 14, live stream number 14, we talked about this continuum slash development of the concept from the way it was phrased by

markov by the way we're still trying to figure out which one uh slash exactly which contribution the father and the son made towards the development in pearl 1988 in the uh textbook uh the

shown here, and this is where computational methods start to come in and some of the pure math assumptions get deployed in real data sets.

So then the Markov blanket is defined as the set in a Bayesian graph of insulating nodes.

So moving from this matrix representation towards this graphical Bayesian representation.

And then there's some of the developments by Carl Fristen and others, which this paper is part of, that do a few different things relating to connecting, separating the blanketing states

into incoming sensory and outgoing action so partitioning of the blanket as well as implementing some of these components of principle of least action which is not implicit in just mere statistical insulation on a bayesian graph so we'll talk about some of that stuff but i think there's enough to cover in what this paper here adds and

Number 14, number 20, number 26, many other papers that we've discussed and many that we haven't have very insightful uses as well as criticisms of the Markov blanket concept.

So what is the free energy principle, this keyword?

One way to answer it, since it's a keyword in many papers is

What are three possible attentional regimes that you might be able to expect will reduce your uncertainty about this question?

Because if you're not worried about this question, it's all good then.

But if you are, you want to reduce your uncertainty.

So what are the actions to reduce your uncertainty?

Well, the third sentence of this paper that we're discussing here is...

in brief the free energy principle is a variational principle of stationary action applied to a particular partition of states where this partition rests upon conditional independencies which is the other side of the coin of uh which which things influence each other and which don't kind of two sides of the coin there

There's then a link to Citation 15.

So Citation 15, if you want to go one more layer into establishing whether this is a meritous claim or not, there's Fristons 2019.

monograph free energy principle for a particular physics particular being specific but also referring to particular or autonomous states as they're known and also much discussion has occurred since this paper so look forward to seeing how it develops

And then a third possible attentional regime would be to come participate and contribute with ACT-INF Lab because every day we are reducing our uncertainty about terms as well as any participant can attest.

And we're always trying to improve our informational niche through projects like educational courses and ontology development so that the questions that people ask

every day.

Like what is a Markov blanket?

Why does it matter?

What is the FEP?

Why does it matter?

There will be awesome resources to share about those questions.

One other piece before we jump into the paper is just a little bit of matrix math vocabulary.

So I hope that I'm

just even quoting the definitions correctly, because I'm not super familiar with a lot of these distinctions and would think that there's a lot more expertise that people could share, which would be awesome.

So the Jacobian matrix and the determinant is a construct in vector calculus.

and what's relevant here is that it's the matrix of the first order partial derivatives so if you have just like a lump in space you can take the partial derivative it's like putting a ruler and finding the derivative of a certain axis of a certain variable at that spot and so you can find the partial derivatives of higher order models the hessian matrix is defined as the square matrix of second order partial derivatives

a function or field it describes the local curvature of a function of many variables so same idea it's like how the the linear slope and then also the curvature so that can tell you a lot in especially a certain case that's going to come into play in how this paper defines its analysis approach

specifically the Laplacian approximation.

So the abstract.

In this treatment of random dynamical systems, we consider the existence and identification of conditional independencies at non-equilibrium steady state.

These independencies underwrite a particular partition of states in which internal states are statistically secluded from external states by blanket states.

The existence of such partitions has interesting implications for the information geometry of internal states.

In brief, this geometry can be read as a physics of sentience, where internal states look as if they are inferring external states.

However, the existence of such partitions and the functional form of the underlying densities have yet to be established.

Here, using the Lorenz system as the basis of stochastic chaos, we leverage the Helmholtz decomposition and polynomial expansions to parameterize the steady state density in terms of surprisal or self-information.

We then show how Markov blankets can be identified using the accompanying Hessian to characterize the coupling between internal and external states in terms of a generalized synchrony or synchronization of chaos.

We conclude by suggesting that this kind of synchronization may provide a mathematical basis for an elemental form of autonomous or active sentience in biology.

Okay, cool.

How will that happen?

Here's the roadmap.

The first section is the introduction.

The second section, from dynamics to densities, makes the connection between dynamical models

densities which takes one towards flow like if you have water in the ocean of different temperature and densities you can do the flow modeling section three is the helmholtz decomposition which we've also revisited elsewhere and it relates to splitting apart a vector field into two components one that's kind of like hill climbing and one that's more like curvature

The Lorenz system, a classic complexity model, is brought up and used as an initial case for this kind of dynamics to densities plus Helmholtz decomposition.

Then some further systems other than the Lorenz are analyzed.

Then the Laplace and systems other than the Laplace are analyzed.

section four Markov blankets and the free energy principle talks about sparsely coupled systems and specifically where the different partitions are engaging in some type of synchrony which is going to be like correlated action conditioned upon the blanketing states

That leads to a discussion of particular partitions, boundaries, and blankets, and a kind of revisiting using a simulation analysis of Markov blankets.

The last section is about the free energy principle and mentions active inference, so it's pretty interesting.

Just to start with the data availability statement, they wrote that the code used to run the simulations and create the figures for the paper are freely available in the DEM toolbox of the MATLAB package SPM12.

So if anyone maybe wants to like do SPM live with us, that who knows how to do it, that'd be awesome.

SPM stream, it'd be great.

So let's start with the introduction slowly because there's gonna be some more technical parts later.

The last sentence of the first paragraph of the introduction is, from this, one can elaborate a physics of sentience

or Bayesian mechanics that would be recognized in theoretical neuroscience and biology.

So imagine if the top half were redacted.

What do you think would allow for, quote, the elaboration of a physics of sentience or a Bayesian mechanics?

So not possible.

or it's already been done, who did it?

Or, well, it would have to have this attribute.

It'd have to be able to run on this type of computer, or it'd have to have this many variables or fewer.

So what are we looking for here?

What would justify this sentence that they started the paper with?

And why would it matter?

Which is kind of tied up with maybe what would be the motivation for developing it, however much effort it involved.

Why would it matter to have a physics of sentience?

Well, why did it matter to have a physics of heat?

Let's pull back one more sentence.

The internal states can then be cast as representing in a probabilistic fashion external states.

So that's what they're going to elaborate a physics of sentience on from this partitioning of internal and external states, but not just any partitioning, a particular partitioning where internal states can be cast as representing external states.

So this is kind of a minimal representation.

We have internal and external states, and we're not going to define them as even action or sense partitions in the blanket yet.

Just there's some partition between internal and external states.

And the question is, is this how you think the world is partitioned or how our statistical models are partitioned?

What is this partition?

And we've talked about that in a few different papers, and that's going to be a big part of this paper.

Then we can pull back one more sentences in the abstract, or I mean, in this first paragraph, where they define the free energy principle, which we read previously.

And then specifically, if the states of a system

whose dynamics can be described with random or stochastic differential equation , possess a Markov blanket, then an interesting interpretation of their dynamics emerges.

The conditional independence in question means that a set of internal states are independent of another external set when conditioned upon blanket states.

and so that is the blue here is the precondition for the red claim which is going to be what is the basis for in the author's words elaborating a physics or sentience so that is kind of where we're going in this paper agree or disagree there's going to be a lot of specific points where one can

raise a qualm and that's totally okay and i think a lot of the uh discussion i hope will be very interesting with people seeing which parts of this presentation resonate with them or their understanding or not those are the big questions what kind of blankets line up with different parts aspects or partitionings of the world map is not the territory etc

What makes some partitionings in the world act informationally, like inferentially, anticipatorily, predictively, etc.?

Why are some partitions able to be CPUs and other partitions are just divided halves of a cup of water?

How do partitionings of variables that model the world lead to a physics of sentience and action?

there another way to get to a physics of sentience and action is there another way to do physics they describe their approach to part one um by applying the helmholtz decomposition which

we're going to return to.

So this is just kind of a preamble and we'll talk more about some of the keywords, but they're going to apply the Helmholtz decomposition to the solution of the Fokker-Planck equation describing the density dynamics of any random system.

And then later on, they're still summarizing part one and they're going to say, we approximate the flow from the yellow part with a quadratic expansion, which means the steady state density reduces to a Gaussian form.

it's kind of like if you fit a quadratic model a x squared plus bx plus c to some data set it's not a statement that the data set is going to have a derivative that's linear and then a derivative of that that's a constant that's not

part of the data set of the world, that's about the quadratic model, which is a very constrained model set that you chose to fit.

And maybe other models would have fit it better.

Other ones would have had more or fewer parameters.

But you knew something about the form of the solution because you constrained it to a certain set of parameterized equations, which is why they talk about variational inference.

And then they're going to show that for a Lorenz system subject to appropriate random fluctuation, there's going to be this third state, which is independent of the first two states or dimensions.

And this means that the third state has no blanket states and therefore there's no particular partition of interest.

So one question for anyone who maybe understands the formalism, just why does this claim specifically matter?

So section 2.

The aim of this section is to get from the specification of any of a random dynamical systems in terms of its equations of motion to the probability density over its states in the long-term future from any initial conditions.

We are only interested in systems that have a limit set.

In other words, systems that possess an attractor.

We want to describe systems that do not necessarily visit all possible states, are not necessarily time reversible, are stochastic in nature, and potentially chaotic.

In short, we are interested in stochastic chaos in systems with a pullback attractor.

And then there's several citations from the 90s and maybe that's some of the earlier models of it.

that's how they describe it and then they show this formalism where the langvine the change in the state of x that's kind of like a derivative uh sign sometimes is equivalent to like a flow and then these faster fluctuations so kind of like a wave and a ripple but that's going to describe the change overall of some location and then that is

equivalent to the Fokker Planck density dynamics so they're the same thing that's the equal sign so that I can't say whether this is just a total tautology super obvious or whether this is a conclusion that was only more recently understood that would be helpful to know but that's one of the baseline

equivalences in the paper it's going to be you can have a specification of a random dynamical system like here's the set of equations that describe how this sensor is returning data and then turn it into a flow in a density estimation framework so the Fokker Planck

they write the dot product the dot notation indicates a partial temporal derivative in the absence of random fluctuations i.e.

gamma equals zero the attractor corresponds to a limit set namely the set of states onto which the solutions of one converge so

uh there's kind of like uh some steady state attractor which is when there's a heat differential like there's a it's warmer outside it's cooler inside and then there's some point where the thing gets zero

the flow shows exponential diversions of trajectories with positive lyopin of exponents i.e real positive eigenvalues in the jacobian of the flow the system can be said to be chaotic so there's systems that kind of converge nicely when they're pulled away a little bit so let's just say that it stayed the same temperature inside and outside in that previous example then that equilibrium would be like pretty nice because it would return to it if it slightly

were moved away from it.

But there's also systems where nearby points diverge.

And that's a measure of this approximated by this lyopinib exponent.

And then the question is, for systems that do have positive leading lyopinib vectors, then what happens to those low

models and then what happens when there's a reinstating of the random fluctuations how does that sort of system that would be chaotic if it were allowed to just

somehow deterministically evolve.

So it's like you're fitting the underlying model and it's a little noisy around it.

And then you swap out the actual scattering of the real world populations of deer and rabbit for just the partial derivatives that you were estimating

And then maybe you see that there's some bifurcation in the population density.

But of course, any real population is either going to experience one branch or the other of that bifurcation.

So that's kind of the modeling that they're working on.

And then when you go from that dynamical systems approximation to the flow, if the flow shows exponential divergence of trajectories, we can impute stochastic chaos.

so they take one of the classic systems of deterministic chaos the uh and dynamical chaos the Lorenz system which is kind of a butterfly looking attractor and they um uh first show the trajectory of Lorenz attractor with and without this reinstating of random fluctuations

so we know that there's a divergence lyopinib divergence that's making the system chaotic when there's no random fluctuations so what happens when that is reintroduced the middle panels show the corresponding solutions in the three-dimensional state space illustrating the butterfly shape of the limit set so

even when the, in the random attractor.

So the deterministic one is the kind of clean butterfly.

And then the one with a random fluctuation, the points are outside of the butterfly.

It's a slightly different shape.

but then it still illustrates, it's not a, you know, an X in the state space.

So it still is kind of following this, but just it's, of course, it's not following the exact tracks that it's on because there's a random fluctuation term, but it still is going to have a certain geometry because this is a model, hence information geometry.

So that's going to get some transformations and approximations.

Lower left panel plots the fluctuations in the potential evaluated using the Laplacian form.

This is kind of formalism that is going to come later, but expresses the self-information as an analytic function of the states.

And then the states, I was not exactly sure why they were called states rather than dimensions.

So if anyone has a thought on that, that'd be interesting.

And then the lower right is the potential function of the first two states, basically, or dimensions, is shown as an image with the trajectory superimposed.

So this is kind of the, this smoothly varying global potential plot with the lighter and the darker representing different values.

There's this like dynamical butterfly that's fit from the resampling of the true Lorentz.

All right, so the key insight from figure one is that the underlying system, this is kind of a key complexity

insight that the underlying system can have very simple and defined rules yet unpredictable dynamics so in the case of dynamical equations it's systems like the lorenza tractor and many other systems that are just very simple and they only have a few parts yet they result in these very incredible and rich behavior and then there's also kind of like

simple rules, complex outcomes, ants and humans.

So just a simple rule can be underlying something that's giving an extraordinarily complex pattern.

And yet, if we measure these systems empirically that have some simple rules underlying them, but there's also this random fluctuation or we don't sample at all.

there will be some but not overwhelming noise there will still be dynamical patterns to pull out so we can still reduce our uncertainty about the location of future points by using a statistical model that we define that doesn't even have to be similar to the generative process so

if someone said well you get money for getting predictions closer to the actual position of the sampling even if your model that you're fitting to it is like a neural network or is this kind of model that we're going to discuss here rather than a differential equation that's actually the true generative model you can still do better than just guessing within this cube so section three goes to the helmholtz decomposition and they write um

So that was the previous section was, we can regard the deterministic Lorenz system as describing the expected flow of a random dynamical system that is subject to random fluctuation.

That was what we just talked about.

The flow of such systems, which possess a non-equilibrium steady state density, can be expressed using a generalization of the Helmholtz decomposition into...

dissipative, which is irrotational and curl-free, and conservative, which is rotational, divergence-free components, where the latter are referred to as solenoidal flow.

For an introduction to the generalization of this Helmholtz decomposition, see Appendix B of 16, which is Bayesian mechanics, stream number 26.

So,

We're looking for where P dot, so the partials of P, partial derivatives of P are zero.

So where is that flow equilibrium?

Well, that is going to be a function, f, that has a few terms, including one that's, I think, introduced in this paper, the housekeeping function.

But it has two terms that are the classic Helmholtz decomposition, which is the solenoidal, the tangential current, that's just the isocontour, and the gradient, which is kind of like the up and down.

So that's how a total current in electromagnetics gets decomposed.

And it's related to kind of fundamental vector field math.

So let's go and uncover what was hidden by this gray box.

So we have the Fokker-Planck steady state P of X equals zero, P dot of X is zero.

There's some function that's going to be a decomposition of that into a solenoidal and a gradient flow component, the Helmholtz decomposition, and then this housekeeping term, which is in an appendix, but didn't go into in super depth.

So curious about what that is, but it results in this whole flow term.

Here is the fancy I. Maybe it's a fancy J. The first dissipative part performs a Riemannian gradient descent on the negative logarithm of the steady state density, which can be interpreted as the self-information, which is the negative natural log of P of any given state

or as some potential function so that's kind of like how surprising is that state the second part of the flow is the solenoidal circulation on the isocontours of the steady state density and then the third term is this housekeeping term that accounts for how changes in the flow operator change so it's a um extension of the helmholtz decomposition thinking about how to solve dynamical equations

So this is from ActInf 26, the synchronization map.

And I'm not going to go into the whole thing because you can check out 26, but the key question is about this sigma, this synchronization manifold between the internal and the external states conditioned on the blanket states.

so how do you insulate yet have anticipation across that insulation and then also from 26 this is kind of what it looks like to see a stochastic trajectory decomposed into a gradient and a rotational component

All right, 3.1.

Our objective is to identify the functional form of the self-information or potential function that describes the non-equilibrium steady-state density.

So why?

That's how surprised you should be.

That's the potential function.

That's sort of the...

the fund the fundamental frequency of both the solenoidal and the gradient so it's an important variable to know but how is self-information measured or calculated how does the self-information for different non-equilibrium steady state or equilibrium steady states compare

what does it mean for self-information to be high or low so to be surprised or not so first they um in formalism four address this set of questions basically for one simple er case which i'm not super familiar with the matrix math so we'll leave it to others to evaluate some of the technical details so then they um follow up and say basically uh

we could solve that easily without that correction term that arises when flow operators are a function of the states indeed it is this state dependency that underwrites stochastic chaos so that's like the double pendulum the reason why it's a chaotic system is because the states at the time depend on the previous time in a way that generates chaos so there's so much dependence over short term that things can diverge really fast

this present this presents us with a more difficult problem the problem can be finessed by using polynomial expansions of the flow operator and the potential as follows for n states up to polynomial order m so they're going to use polynomial expansion approximation on a complex underlying flow functional polynomial expansions restrict and scaffold our model selection approach to a manageable size and tractable computation so it's kind of like the taylor series and the volterra series and other

polynomial expansions which can fit functions often that are very complex well and so this is showing the Taylor approximation to a sine wave and it's showing how the first approximation gets you like between

one and negative one the red line might be totally more than enough but with only a few more terms being added all of a sudden it starts fitting really well further and further out so that doesn't mean it can go forever but the um polynomial expansions can be very powerful in the numerical case like just doing good enough with quick and dirty as well as in the analytical case

Here's a figure from SPM 2007 textbook, which is a great textbook, by the way.

And it's talking about the Volterra series as a general nonlinear input state output characterization system.

So NB, nota bene, good note to you.

Volterra kernels are synonymous with effective connectivity.

And then they write.

This is from the SPM12, so later than this, but right up.

From the point of view of regression models, modulatory effects can be modeled with nonlinear input-output models, and in particular, the Volterra formulation described above.

Because the kernels are high order, they embody interactions over time and among inputs and can be thought of as explicit measures of effective connectivity.

An important thing about the Volterra formulation is that it has a high face validity and biological plausibility.

The only thing it assumes is that the response of a region is some analytic non-linear function of the inputs over the recent past.

This function exists even for many complicated dynamical systems with many unobservable state variables.

So, cool.

why does it matter to do a polynomial expansion of the functional form of the solution to this flow problem the parameterization in five allows for state-dependent changes in the amplitude of random fluctuations encoded by the leading diagonal of the flow operator with that functional form of the polynomial it is straightforward to solve for polynomial coefficients the flow operator by solving the following simultaneous equations for a series of sample points

Some of the keywords that are from SPM and elsewhere that might come into play to learn more about these topics are expectation maximization optimization, parametric empirical base, dynamical systems identification, and generalized linearization schemes for fitting GLMs to nonlinear systems.

And then something that's not as much in SPM, but comes into play later with active inference, and hopefully we'll be able to demonstrate this in the literature as we sort of sift through it, is that action planning amidst uncertainty and feedback with the environment radically changes the nature of that kind of flow estimation problem.

You can't use the same model necessarily for the...

leaf on the stream as something like the winged snowflake that's actively resisting so how good is this approximation to use the polynomial expansion of the fitting of the random fluctuations of the random dynamical system that is chaotic inspection of the lorenz system suggests that a second order polynomial approximation is sufficient given the flow is second order in the states

This ansatz, so, vocab, ansatz, an assumption about the form of an unknown function which is made in order to facilitate solution of an equation or other problem, has an interesting implication.

If the self-information can be approximated with a second-order polynomial, it means the non-equilibrium steady state is approximately Gaussian.

This is known as the Laplace approximation in statistics.

Here we generalize the notion of a Laplace approximation to cover not just the quadratic form of the log density, but also the solenoidal flow that underwrites non-equilibrium dynamics.

Big if true.

It has to be.

So here's the Helmholtz decomposition.

So now we're taking that solution that we had earlier

and thinking about it in terms of fitting only up to the second order polynomial expansion of the flow solution.

So that is, as they say, what is known as the Laplace approximation in statistics, and it's in SPM textbook two.

Here's the actual flow decomposition of the Lorenz system.

the solenoidal flow is red, the gradient flow is blue, and the correction term is gold.

The flow is shown as a quiver plot at equally spaced points.

So it's kind of like the vectors of different direction at different points.

And then the right is the same, but for the Laplacian based upon...

the one on the left here.

The key difference is that the dissipative part of the flow operator and Hessian are positive definite, in the case of Laplacian, which means the gradient flows converge to the maximum of the non-equilibrium steady state density.

This is reflected in the blue arrows that point to the center of the space.

So even though the gradient of the real flow is pointing all the different directions because it's kind of like a mixing flow, so there's all the vectors are going in different directions.

But then in the Laplacian second order polynomial expansion, the blue arrows are all pointing to the center or to some manifold that's much more compact.

So it's kind of like a converging solution, even though the actual system, we defined it to have stochastic chaotic properties.

So here's SPM looking at the Lorenz tractor.

Alas, poor Lorenz.

I knew it, Horatio, a model of infinite citations, of most excellent fancy.

It hath returned to the pull-back attractor a thousand times, and now how abhorred in my imagination it is."

So that's modified from Hamlet.

because right after that seemingly very cool solution they say indeed the equations of motions we have written down are deterministic implying that there is no stochasticity in the sample paths of the process assuming a fixed initial condition this speaks to the fact that helmholtz decomposition of the deterministic lorenz system is not a description of a dynamical system with random fluctuations i.e systems in which the dissipative part of a flow operator is positive definite

we therefore need to look beyond the Lorenz attractor so that takes us to appropriately 3.3 for beyond the Lorenz system so okay cool now beyond the Lorenz system we go to the chaotic Laplacian so figure three this is um a trajectory kind of like a point list trajectory of three states kind of three dimensions comprising a Laplacian approximation

to a stochastic Lorenz system.

So here it's second by third, first by third, first by second of the nest density.

So it's showing three projections like the GEB cover onto a two-dimensional and notice how the light is in the center.

So it's like they're getting pulled back to the peak of the Gaussian on each dimension.

So that's the non-equilibrium steady state, which by construction in the Laplacian is a multivariate Gaussian.

That's super key and extremely interesting.

The middle panels show deterministic and stochastic solutions as a function of time, while the right panel plots the same trajectories in state space.

The shape of the attractor retains a butterfly-like form, but it's clearly different from the Lorenz attractor.

What do you see when you look at this attractor?

Be honest.

But it has a butterfly-like shape.

And the generated trajectory has some similarity with the underlying generator function.

But it has such a nicer form to solve.

And then the lower left is the self-information, the potential as a function of time based upon the analytic form for the equations of motion and the deterministic trajectories of the previous panel.

And then the flow of the Laplacian, which is the approximated expansion, nicely solvable, et cetera, against the Lorenz, that's the true flow, is evaluated at 64 spaced points.

It can be seen that although there is a high correlation between the flow of the Laplacian and Lorenz systems, they are not identical.

So not super visible, could have provided like an R squared or some other information, but clearly they are drawing from the same manifold.

So if this is good enough to stay alive, then c'est la vie.

Upper panels, let's just focus on those.

So this is, again, the projection onto two by three, one by three, and one by two of three of this multivariate Gaussian Laplacian approximation, which makes it really nicely solvable.

So one can then estimate the Lyapunov dimension

which is approximating to the Hausdorff dimension, which in this example was 2.48.

So the Laplacian approximation was used to sample and then estimate the Lyapunov dimension.

And then it turns out that that compares very closely 2.43 and 2.48, which in the grand scale of things where above one means chaotic system,

It means that the Laplacian expansion has some of the nicely solvable characteristics of the generalized Laplacian, like the multivariate Gaussian and the easy computation, yet it also identifies this system as chaotic because it still gives a positively operative exponent from this sampling.

In short, the flows of the Lorentz system and its Laplace approximation have an attracting set width between two and three dimensions.

For our purposes, the Laplace approximation is easier to handle than the Lorentz system because the functional forms of the flow and potential are immediately at hand.

So...

Pretty interesting.

Figure four.

So now this recalls our earlier discussion of the Jacobian and the Hessian.

So let us recall.

The Jacobian is the first order partial derivatives.

The Hessian is the second order partial derivatives.

Okay.

So we have the Jacobian, the Hessian, and the covariance derivative.

between these three dimensions so the lorenzo tractor was in three dimensions okay it was in state one two and three that's kind of why there's it's like thermodynamic states so they are dimensions though so the lorenzo tractor was this like kind of you know ant flying around in space in three dimensions so we fit a polynomial expansion specifically to those three dimensions the measurements of them now for those three dimensions

we can talk about their correlations in the first partial derivative and the second partial derivative and the covariance.

That's in the top here.

In this example, the third state is independent of the first pair where the independence rests on the directed coupling from the third to the first state.

And so it's logged to show the sparsity.

Do you see where we're going with the Markov blanket?

See how this third state is not part of the Lorenz system underlying differential equations.

It's actually just a total figment of the Laplace approximation model, but it's quite an interesting pattern.

The middle panel shows slices through the steady state density over the two states at increasing values of the remaining state.

so the only correlation in play is between the first and second state so this is related to sphericity error correction and evaluation and it's a topic that is discussed in spm textbook but basically spherical errors are the ones where the two axes are not correlated but then when there's non-spherical errors then there's you get a different shape

lower panel the correlation is illustrated in terms of the conditional density over the first state given the second so this is kind of showing that by and large it's doing well largely confined to the 90 credible intervals would be good to see some you know simulation statistics and raw data on that

So, again, alas, I knew him well.

The preceding treatment leverages the simplicity of the Laplace approximation to stochastic chaos in which sparsity constraints on the Hessian are easy to identify or implement.

Now, just like Lorenz was dropped, now this is going to be going beyond the Laplace system.

So...

Not going to go into too much detail here, but that's a section that people could explain more about, or we could look at more in the dot one, the dot two.

So 3.5 summary.

It is important to not conflate the simplicity of a Ness density with the complexity of the underlying density dynamics.

In other words, when prepared or observed in some initial state, the probability density can evolve in a complicated and itinerant fashion on various sub-manifolds of the pullback attractor.

So the map is not the territory.

That's something that has come up many times.

The Laplacian is not the Lorenz, but the Laplacian is a good approximation to the Lorenz in their simulations.

Figure six.

So now the chaotic Laplacian system is, it's kind of shown in a time series way where this is the true relationship between the first and the second state.

So let's go to here.

So we see the first and the second state.

Here's like where there's this non-spherical error.

And so this is kind of like an interesting like coupling in the model.

that's independent of the random fluctuations and so that represents something very deep about the structure of the Laplacian approximation to the Lorentz now using the Laplacian in a generative capacity there's this initial misattunement in their

setup it starts somewhere far from the pullback attractor and then like the correlation evolves through time and then they say the density converges to the steady state density after about 16 seconds however it takes a rather circuitous route from this particular set of states okay so that's interesting it starts with like they were differently correlated

because of they starting in a weird place and then they found their true correlation structure, note that the average density over short periods of time can be highly non-Gaussian, even though the density at any point is by construction Gaussian.

Something to think about.

So then we get to, finally, Markov blankets.

So they repeat the analysis, but now they approximate two Lorentz systems that are coupled to each other through their respective first states.

So the first and the second states were coupled.

So it's almost like one of our coupled states we're going to couple out now.

This induces a richer conditional independent structure from which one can identify internal and external states that are independent when conditioned upon blanket states.

So what does it mean to sparsely couple systems?

And what are these quote first states, the first dimension?

Figure seven.

So now there's generalized synchrony in a Laplacian system.

So this is now the same figure as figure three, like this, except now we're looking at the generalized synchrony.

So now there's a synchronization manifold in the first dimension.

state when those two got linked up so now there's more lines because there's like two coupled chaotic systems but the middle is showing that their state spaces are coupled and this is illustrating the degree of synchronization and then again showing that their flows are correlated no stats figure eight now instead of having

three by three in the matrix correlation and covariance in the first and the second derivatives now it's six by six so here is now the identification of using the log of the jacobian and the hessian to identify different partitions so that partitioning between the one and the four between that first state of both is the one that we knew about that's like

the one that was wired into the system.

But then the other ones are showing other correlation patterns.

Now, I don't know why this four and five connection is not included in a box, but these are the partitionings.

And so,

The sparsity structure of the covariance supports a particular partition into internal states five and six, dark blue, active fourth state, the one that induces, the one that connects here,

sensory first state and external second and third state this partition is illustrated here and the remarkable thing is that despite their conditional independence there are correlations between internal and external states and here between the second and fifth states so interesting patterns showing these evolving through time nine so now they're going to look at the partial correlation of states

and showing that the Hessian is recovered.

So here is like that Hessian on the top right.

And then now they're going to show that it's recovered.

It's a little bit light, but it's showing how these, so these squares are showing the partial correlation coefficients and hopefully pulling out the similar structure as identified here.

And then like the strong correlation, well, yeah, we'll investigate that more in the dot one and the dot two.

And then showing how like the third and the sixth state, for example, so that's like three and six, they're not supposed to be coupled.

And so then they initially start off with a high correlation because maybe they start in the same neighborhood, but then they converge towards a zero correlation.

So that is sparse coupling.

um it does not depend upon gaussian assumptions about the non-equilibrium steady state density it implies that the dynamical influence graphs with absent or directed edges admit a markup blanket which may or may not be empty these independencies can be used to build a particular partition using following rules capture the flag figure 10.

So here's where we see kind of the classic bacteria example, but now it's two bacteria communicating.

I'm not sure if it was always that way.

And then here is a little bit more of a coupled systems with two times three, which is pretty interesting.

So here is where we see the Markov blanket as we've seen it before, but also in a new way.

How great.

Here's some technical definitions of the Markov blanket.

So not for tonight, but for another night.

So, um, here's a question they ask themselves.

Why, why, one might ask, why does a particular partition comprise four sets of states?

I don't know.

Why is the tetrahedra the smallest polyhedra?

Who knows?

In other words, why does a particular partition consider two Markov boundaries, sensory and active states?

So why can we skip straight to the Friston blanket rather than just identifying the Pearl 1988 or the Markov...

earlier model.

The reason is that the particular partition is the minimal partition that allows for directed coupling with blanket states.

So that again is from figure 10.

And the key thing to note from figure eight is that there are profound covariances between some internal and external states, despite the fact they are conditionally independent.

So we're defining like nodes that are not connected are conditionally independent.

That's how the Bayesian graph works.

so here one and four get correlated get a connection with each other that's this connection right here right there and right there so now they're coupled one and four so even though some of the internal states are so now it's like the external states are at minimum two and bucky fuller famous quote unity is plural at minimum two the internal states have to be at minimum two

external states have to be at minimum two so that's pretty interesting because they yeah um yeah the particular partition is the minimal partition that allows for directed coupling with blanket states okay and then um they're the second the fifth states are highly correlated yet are twice removed

And so this is leading to a general synchrony through partial coupling of dynamical systems.

Note, this is their words.

Note there is no claim that either the original Lorenz system or coupled Lorenz system possesses a Markov blanket.

The claim here is that there exists a Laplace approximation to these kinds of systems that in virtue of the zero elements of the Hessian feature Markov blankets.

So...

Oh, how many days.

The having a Markov, being a Markov, modeling a Markov.

No claim.

A Laplace approximation approach to systems identification can add constraints and make the problem easier to solve.

The Markov blanket is arising literally within the Laplace approximation, which is just an approximation.

It's not a description of the system.

So,

onto the free energy principle.

The existence of this any particular partition means that one can provide a stipulative definition of the conditional density over external states being parameterized by the conditional expectation of internal states given sensory states.

We call this a variational density parameterized by expected internal states mu.

So going from the minimal coupled systems

to there are still some persistent covariances and these covariances partition out into a Markov blanket.

What if the expectation of the internal states were of external states?

other words or in words for every sensory state there's a conditional but it has to be conditioned on the blanket state which is how the whole thing is set up in words for every sensory state there's a conditional density over external states and conditional density over internal states where internal and external states are conditionally independent this admits the possibility of a diffeomorphic map between the sufficient statistics of the respective densities

The existence of this mapping rests upon a continuously differentiable and invertible map, which is linear under a Laplace approximation.

So diffeomorphic is like stretchable.

So that allows some of these formalisms to arise.

Again, it would be awesome to hear what other people can bring to the table about this, but this is like the energy minus entropy form, the divergence term plus the self-information about the policy, and then the accuracy minus the complexity.

So it would be awesome to learn more about this.

Okay, the font got a little out of control here, but this functional, the big one,

is going to be expressed in a few different forms so it's an expected energy minus the entropy that's energy minus entropy self-information plus kl divergence okay self-information plus kl negative log likelihood of particular states and the kl divergence accuracy minus complexity okay accuracy minus complexity it's also in the machine learning context evidence lower bound elbow this is the basis of the free energy principle

Put simply, it means that the expected internal states of a particular partition at non-equilibrium steady state can be cast as encoding conditional or Bayesian beliefs about external states.

Equivalently, the flow on the internal manifold can be expressed as a gradient flow on a variational free energy that can be read as self-information.

This licenses a somewhat teleological and directed description of self-organization as self-evidencing, in the sense that the surprisal or self-information that constitutes the potential is known as log model evidence or marginal likelihood in Bayesian statistics.

So, wow.

Great FEP work.

What does it mean to talk about the physiology perspective here though?

So what does it mean for self-evidencing in a biological context?

The blood vessels want to observe themselves working?

So now in section 5.1, getting hopefully towards the end here, in alternative, this is later in section 5.1, in alternative and deflationary perspective,

rests on noting that free energy gradients are also the gradients of self-information.

So the organism wants to be minimally surprised about some chaotic pullback attractor.

So whether or not things are wacky out there or not, having this updating flow model

that comes to a pullback attractor that is of the approximation that the modeler controls is very important and the organism wants to be minimally surprised about the expectations and preferences that it has over observations and that is defined with some technical details here so this formulation of gradient flows is simpler and shows that they're effectively minimizing a different sort of prediction error namely the difference between particular states and the expected values at non-equilibrium steady states

This leads to exactly the same gradient flows, but a complimentary interpretation in which autonomous states are drawn towards their steady state expectation.

Heuristically, one could imagine this kind of stochastic chaos as apt to describe the motion of a moth attracted towards a flame, but constantly being thwarted by turbulence, i.e.

solenoidal air currents.

Because active states influence sensory states and possibly external states, this would look as if the particle, e.g.

the moth, was trying to attain its most likely state in the face of random fluctuations and solenoidal dynamics.

This perspective emphasizes the active part of self-evidencing, sometimes referred to as active inference.

So, how cool.

First in 2006, the winged snowflake. 21.

like a moth to the flame 5.2 so they um

give more summarization here about that sequence that they took of constructing the laplacian approximation with a quadratic form and then looking at that first and second partial differential uh partial derivatives matrices the jacobian and the hessian looking at the conditional independencies then the generalized synchronization synchronizing

given the partitionings, and then suggesting that there's some diffeomorphic, like, stretchable connection between the internal and the external states conditioned upon all of that.

And then that's the Bayesian mechanics.

which internal states on average parameterize beliefs of a bayesian sort about external states okay so some of the implications in the last few slides here so clearly the worked example based upon sparsely coupled lorenz systems does not mean that a conditional synchronization map exists or is indeed invertible which would allow the tail of two densities the recognition and the generative model in any given system

So you can't just specify any system potentially and find a conditional synchronization map.

However, the above derivations can be taken as existence of proof that such manifolds and accompanying variational free energy formulation can emerge from sufficiently sparse coupling.

So this is one system, classic system of complexity analysis, and it worked here.

Maybe there's other systems where it will or won't.

A particular partition is necessary to talk about states that are internal to some particle or person, and which can be distinguished from external states.

One way of understanding the ensuing coupling between internal and external states is in terms of generalized synchrony.

That's that synchronization manifold.

This synchronization can be expressed as a variational principle of least action using the notation notion of variational free energy.

So this is from another DaCosta paper, the synthesis on discrete state spaces.

And it is talking about how that variational free energy minimization of perception and planning as inference about observations can be seen as a generalization.

Here's active inference

If you have no prior, then you have surprise, pure surprise, optimal surprise, intrinsic motivation, info gain, info max principle, because it's just about what's surprising given no priors.

When there's no ambiguity, so when there is a model that abstracts away from, for example, measurement error,

you have risk sensitive policies and kl divergence based control and the occam's principle and there's other cases like bayesian decision theory expected utility theory as well as maximum entropy and jane so it's kind of um there's a lot of pieces that are coming into play when we talk about active inference probably some that will still are yet to learn um but there are some pretty interesting patterns arising from this partitioning

Here's another implication.

The polynomial form of the Helmholtz decomposition from Formalism 5 may provide a generic model for observed random dynamical systems.

In other words, it could be the basis of a forward or generative model that explains some empirically observed flow, estimated using the first and second moments to quantify the flow and covariance of random fluctuations over state-space respectively.

This kind of generative model is appealing because of its parameterization in terms of the underlying non-equilibrium steady state density.

In other words, one could, in principle, try to explain empirical data in terms of a Gaussian steady state density, which may include constraints on conditional dependencies.

So let's think back to the beginning with flow and information flow and mental flow and granite moving down a stream

quarry or all these kinds of flow and then could there be something that integrates across them another option for scaling this approach to high dimensional dynamical systems would be to learn the state dependency of the flow operator and the non-equilibrium steady state density using unsupervised learning approaches such as deep neural networks a similar approach has become popular in the deep learning literature in the form of neural stochastic differential equations

So here's two recent stochastic differential equation papers.

And then they bring up this very interesting suggestion.

One could use a separate feed forward neural network to parameterize the different components of the flow operator and the self information.

This would require differentiable transforms to be applied to the output layers of the network to constrain the Hessian and solenoidal flow operator to be positive definite and anti-symmetric respectively.

So suggesting some neural network architectures that might facilitate active inference.

Then this is a pretty nice example.

piece, it is interesting to relate flows across a Markov boundary to the constructal law, which like the free energy principle, seeks a normative account of the structure and dynamics of complex systems.

In the case of the constructal law, this is articulated in terms of maximizing external access to the currents internal to the system.

So here is Adrian Bajan, who's like a awesome professor and researcher who has done many years of work

like Friston in some ways on this construct the law.

So I was quite happy to see that connection.

Here's another final implication.

One practical implication.

Finally, the practical implication.

Great.

It licenses the use of inverse sample covariance matrices of a sufficiently long time series as potentially sufficient description of the steady state density.

So we could take a real time series and fit that Laplacian approximation.

This would furnish a description of the expected flow and Lyapunov exponents to establish whether the system was chaotic or not.

In conclusion, last sentence, having a simple functional form for the flow of random dynamical systems may be useful for both modeling and analyzing time series generated by real world processes that are far from equilibrium that may or may not be chaotic.

So we can think of two chaotic systems, the humans and the ants, and then everything else.

So pretty interesting paper.

The coming two weeks, hopefully we'll have some good group discussions.

We're going to email the authors now that we've done our work on the dot zero and would be awesome to have them on for the two discussions we'll have or for any other time.

um thanks everybody who was watching I hope if you are watching to the end you found this interesting and come get involved with active lab because we could all have some shared resources so that some of these questions that are really being advanced in this paper like

connections to complexity and dynamical systems theory and statistical thermodynamics these are important contributions and this is not a fringe apologetics for active inference it's barely mentioned in the paper it's actually some really fundamental work that

I hope is understood and critiqued where valid.

So what would a good understanding here enable of the kinds of things that are discussed in this paper, which came out in 2021, but think into the future.

what are the unique predictions and implications of this paper what are the next steps for fvp and active inference what are the goals of this research and what are you still curious about so thanks for sticking around for a long live stream at a different time but so it goes so have a good one everyone hopefully see you around the lab or around bye
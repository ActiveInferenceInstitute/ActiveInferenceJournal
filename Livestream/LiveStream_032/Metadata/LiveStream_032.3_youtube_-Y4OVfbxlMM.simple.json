[
  {
    "start": 30.026,
    "end": 34.81,
    "text": " Hello and welcome everyone to Act In Flab livestream number 32.3.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 35.47,
    "end": 42.896,
    "text": "It's December 16th, 2021 and we are really excited to be here with Connor, one of the authors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 44.057,
    "end": 46.119,
    "text": "Today we'll just jump right into it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 46.959,
    "end": 48.36,
    "text": "Our goal is to be",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 49.417,
    "end": 61.246,
    "text": " learning and discussing this awesome paper, Stochastic Chaos and Markov Blankets from 2021 with Carl Fristen, Connor Hines, Kai Ulzhofer, Lancelot DaCosta, and Thomas Parr.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 62.327,
    "end": 66.75,
    "text": "And we had a dot zero, dot one, dot two on this paper.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 67.211,
    "end": 69.633,
    "text": "And today we're just excited to have a dot three.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 70.153,
    "end": 73.616,
    "text": "So we'll have a presentation by Connor when we'll all be",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 74.336,
    "end": 82.783,
    "text": " providing our regime of attention and writing down any questions and comments we have and writing any questions in the live chat if we would like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 83.403,
    "end": 88.207,
    "text": "And then we will have a discussion to kind of follow up on the presentation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 88.647,
    "end": 91.849,
    "text": "So Connor, thank you for joining and please take it away.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 93.611,
    "end": 93.851,
    "text": "Great.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 93.931,
    "end": 94.531,
    "text": "Thanks, Daniel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 94.551,
    "end": 95.292,
    "text": "Thanks for having me.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 95.332,
    "end": 96.773,
    "text": "I'm really happy to be back.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 96.833,
    "end": 100.676,
    "text": "This is my second time on the stream, so it's nice to be here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 102.139,
    "end": 104.281,
    "text": " Okay, I'm going to share my whole screen.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 104.301,
    "end": 110.166,
    "text": "And now, here's the presentation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 111.447,
    "end": 112.328,
    "text": "Looks good, thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 112.528,
    "end": 112.969,
    "text": "Looks good?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 114.11,
    "end": 114.63,
    "text": "Okay, cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 114.67,
    "end": 115.131,
    "text": "So, yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 115.391,
    "end": 117.593,
    "text": "Could you maybe, where it says hide, meet Jitsi?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 117.613,
    "end": 118.794,
    "text": "You're sharing your screen?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 118.834,
    "end": 119.174,
    "text": "Oh, yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 119.515,
    "end": 121.457,
    "text": "Yep, and then... Hit hide?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 121.597,
    "end": 121.777,
    "text": "Yep.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 124.399,
    "end": 126.001,
    "text": "The whole thing disappeared, but then, yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 126.201,
    "end": 126.801,
    "text": "Okay, perfect.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 126.821,
    "end": 127.742,
    "text": "Now we're back, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 127.962,
    "end": 128.463,
    "text": "Yep, thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 129.751,
    "end": 141.378,
    "text": " Yeah, so as Daniel said, I'll be presenting on this paper that was published this fall in Entropy called Stochastic Chaos and Markov Blankets.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 142.998,
    "end": 151.223,
    "text": "And it's really great that you guys have already gone through the paper in detail from what I remember from watching some of the past streams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 151.775,
    "end": 158.977,
    "text": " So we don't have to go through every single equation and every figure, because you guys have already kind of covered it very thoroughly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 159.037,
    "end": 163.018,
    "text": "But we can dive in with as much technical detail as anyone wants to.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 163.038,
    "end": 172.561,
    "text": "So I'm kind of going to give broad strokes of the paper and also frame it in the context of other literature that's come out on similar topics in the past few months.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 174.341,
    "end": 177.762,
    "text": "A bunch of literature has been published on the same sort of themes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 178.782,
    "end": 180.483,
    "text": "So that'll kind of help contextualize",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 181.141,
    "end": 183.863,
    "text": " what I consider to be the main contributions of the paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 184.944,
    "end": 188.266,
    "text": "And there's both methodological and theoretical ones.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 188.346,
    "end": 190.148,
    "text": "So there's a lot to talk about here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 190.308,
    "end": 193.811,
    "text": "And as Daniel said, I guess afterwards, we'll do this discussion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 193.871,
    "end": 200.856,
    "text": "But if either of you has questions or anyone in the JITSE has questions during it, feel free.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 200.976,
    "end": 202.637,
    "text": "From my end, I'm fine being interrupted.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 202.697,
    "end": 204.619,
    "text": "But whatever, the format is fine.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 205.479,
    "end": 208.421,
    "text": "OK, so without further ado, I'll get started.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 208.822,
    "end": 210.343,
    "text": "So just a quick introduction to me.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 211.42,
    "end": 227.012,
    "text": " So I'm one of the co-authors on the paper, and I'm a PhD student based in Konstanz, Germany, where I'm supervised by Ian Cousin, who is a leading scientist in the fields of complex systems and collective animal behavior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 227.252,
    "end": 232.516,
    "text": "So he's the head of the Department of Collective Behavior at the Max Planck Institute for Animal Behavior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 234.098,
    "end": 235.199,
    "text": "And so yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 236.337,
    "end": 241.882,
    "text": " In Constance, we're basically building a kind of hub for the study of complex systems and collective behavior in particular.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 242.382,
    "end": 254.953,
    "text": "Then I'm also co-advised by the lead author on the paper, who's Carl Friston, who kind of came up with the fundamental premise of a lot of the concepts in the paper, like the free energy principle, Bayesian mechanics, Markov blanket, stuff like that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 256.815,
    "end": 263.08,
    "text": "So the outline, first I'll talk about the motivations for the paper, some of the contextual literature.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 263.837,
    "end": 266.658,
    "text": " And then I'll summarize what I consider to be the main takeaways.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 266.698,
    "end": 269.479,
    "text": "But there's a lot of different perspectives on this paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 269.659,
    "end": 273.84,
    "text": "So these aren't by any means the only way to interpret the paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 274.58,
    "end": 292.065,
    "text": "And then in the interest of just kind of getting us all on the same conceptual footing, I'll review stochastic processes, and in particular, a special interpretation of stochastic processes called the Helmholtz decomposition, which is something that you guys have already talked about in the past streams here quite a bit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 292.881,
    "end": 296.624,
    "text": " And then I will step through the primary moves of the paper in my mind.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 297.084,
    "end": 302.508,
    "text": "And I'll leave out, I'm not going to go through every single figure, like I said, but I'll just step through what I consider to be the primary moves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 304.909,
    "end": 310.033,
    "text": "So in this past year, a bunch of work has come out on the general theme of Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 311.849,
    "end": 325.337,
    "text": " If you want to look at this paper in the context of other work, I would say that the major works are things like the Bayesian mechanics for stationary processes, which just last week was finally published in Proceedings A of the Royal Society.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 325.377,
    "end": 326.497,
    "text": "So we're very happy about that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 326.958,
    "end": 337.904,
    "text": "And that established the fundamental mathematical formalism for Bayesian mechanics and the conditions under which you can apply a Bayesian mechanical lens to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 340.473,
    "end": 342.714,
    "text": " dynamical systems, random dynamical systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 343.014,
    "end": 350.298,
    "text": "So this is something that's been covered in previous live streams and a few of which Lance and I were attendants at.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 351.659,
    "end": 366.186,
    "text": "There's another great paper that's a preprint right now by Miguel Aguilera and some of his colleagues that studies very similar grounds that's covered in this first paper, basically studying the necessary conditions for Bayesian mechanics and in particular",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 367.088,
    "end": 373.976,
    "text": " uncovering what's known as the synchronization manifold and studying that in the context of simple linear Gaussian systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 375.317,
    "end": 383.887,
    "text": "And then finally, there's this paper also produced also in the same special issue of entropy as the present paper called memory and Markov blankets.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 383.927,
    "end": 387.591,
    "text": "And that's again, studying how Markov blankets actually evolved through time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 388.724,
    "end": 391.586,
    "text": " I mentioned these three papers because they all study Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 391.626,
    "end": 395.749,
    "text": "And in particular, they're all focused primarily on linear systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 395.809,
    "end": 403.555,
    "text": "So systems that are often in the stochastic literature called linear diffusion processes or Ornstein-Uhlenbeck processes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 403.575,
    "end": 417.906,
    "text": "So that's a really deep common thread to all of them is they leverage the mathematical tractability of these simple linear systems to get really nice analytic proofs about when Bayesian mechanics exists and when it doesn't.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 419.784,
    "end": 432.554,
    "text": " I say the primary focus is on linear systems because in, for instance, basic mechanics or stationary processes, the mathematical derivations actually apply to any processes that have a Gaussian steady state, not only linear ones.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 433.175,
    "end": 436.757,
    "text": "But the primary examples used in that paper were all linear systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 436.857,
    "end": 442.382,
    "text": "I mean, there are a few nonlinear ones, but the focus and the tight derivations focused on linear systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 444.781,
    "end": 451.902,
    "text": " So given that kind of contextual background, now we can talk about the main takeaways of this paper, Stochastic Chaos and Markov Blankets.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 452.714,
    "end": 456.818,
    "text": " So the first main thing is basically taking a random dynamical system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 456.838,
    "end": 464.765,
    "text": "In this case, we're taking a stochastic version of the Lorenz attractor and we're trying to basically apply the lens of Bayesian mechanics to it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 465.325,
    "end": 478.598,
    "text": "So the same thing that these previous papers have done to look at the paper from the perspective or look at the system from the perspective of how does the system look as if it's forming beliefs about other subsets of variables in its environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 479.258,
    "end": 482.139,
    "text": " So that's what something we also call the physics of sentience.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 482.199,
    "end": 493.164,
    "text": "How can physical systems, not necessarily things we typically think of as sentient, actually exhibit the hallmarks of something like biological computation or representation?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 494.807,
    "end": 506.339,
    "text": " The second main takeaway is finding that you can fit the Helmholtz decomposition, which is this special interpretation of stochastic processes that we'll cover in the next few slides.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 506.759,
    "end": 510.503,
    "text": "How can you kind of learn that decomposition automatically from data",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 510.925,
    "end": 517.266,
    "text": " or not from data, but from stochastic dynamical systems where the usual tractability doesn't exist.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 517.446,
    "end": 519.527,
    "text": "So that's a big methodological contribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 519.927,
    "end": 529.669,
    "text": "It actually lays out kind of the steps or an algorithm for doing this automatically for systems that lack the usual analytic tractability like you get with linear systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 531.089,
    "end": 534.67,
    "text": "And finally, I think another kind of big contribution here",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 535.063,
    "end": 546.229,
    "text": " that was kind of tucked into one of the later sections of the papers, but it's pretty interesting, is a new result on the relationship between the Markov blanket and causal coupling between systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 546.649,
    "end": 552.993,
    "text": "So we basically show that a Markov blanket does not always imply causal relationships and vice versa.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 553.013,
    "end": 559.597,
    "text": "So the Markov blanket is fundamentally a statistical relationship that doesn't always go hand in hand with a causal relationship.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 560.457,
    "end": 562.758,
    "text": " So those are the main three takeaways that I'll discuss today.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 562.878,
    "end": 566.56,
    "text": "But there's a few others that we can also dive into that may have been covered in other live streams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 568.201,
    "end": 578.827,
    "text": "So to do the first goal, like basically deriving a physics of sentience, we need to actually start with Bayesian mechanics as our goal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 579.167,
    "end": 582.168,
    "text": "And then from there, we'll get to the study of stochastic processes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 582.188,
    "end": 585.23,
    "text": "So I'm going to quick first give an overview of Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 586.22,
    "end": 596.797,
    "text": " So the generalized Bayesian mechanics is very, I would say, it can be described very non-mathematically, which is basically you just have a system that's embedded in some kind of ambient environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 597.938,
    "end": 601.739,
    "text": " And crucially, that system has an interface with the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 601.759,
    "end": 608.501,
    "text": "It can only exchange information with the environment through some kind of sensory or active interface.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 609.061,
    "end": 623.345,
    "text": "So in the context of the most popular use case for Bayesian mechanics, that would be something like a brain, a nervous system that's insulated from the world via things like muscular organs, effector organs, and then sensory modalities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 623.685,
    "end": 625.206,
    "text": "So the interface is comprised of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 625.606,
    "end": 649.397,
    "text": " the way we can impact the world around us as well as the way that we kind of absorb or are impressed by information from the world and then you can zoom in right and say that a single cell has the same kind of relationship where intracellular processes are like the equivalent of the brain you know this is genetic things going on with dna or just intracellular metabolic uh cascades",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 649.946,
    "end": 660.757,
    "text": " And then the interface would be something like the cell membrane, which kind of mediates information and energy transfer between the cells, intracellular contents, and then some extracellular environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 661.778,
    "end": 671.789,
    "text": "And then from my perspective, what I'm interested in studying personally is kind of zooming out and saying, how can you look at an entire system, let's say a school of fish or a flock of birds, as if",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 672.336,
    "end": 699.342,
    "text": " whole thing is a kind of internal system that's insulated from some kind of social or physical environment where the interface here might not be as simple as your skin or a cell membrane but something more embodied by other individuals maybe the sensory interface between a group and its surrounding are sensory or active individuals and that's you know this is very catered to my own interests but as you know there's tons of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 699.682,
    "end": 706.087,
    "text": " People are trying to apply the same framework to all kinds of systems, even like socio-technical systems in societies and stuff.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 706.607,
    "end": 707.868,
    "text": "So it's a very generic framework.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 709.809,
    "end": 713.872,
    "text": "That's the fundamental kind of partition of the world that we're interested in with Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 714.572,
    "end": 721.057,
    "text": "And what Bayesian mechanics is saying is that if you take these three sets of states, which I've now termed mu internal,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 721.764,
    "end": 726.405,
    "text": " B, blanket, that's the interface states, and then eta, which is the external world.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 726.845,
    "end": 739.048,
    "text": "If you take that fundamental three-way partition of the world, Bayesian mechanics is just saying that under certain conditions, it looks as if the internal states are encoding the external states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 739.688,
    "end": 749.991,
    "text": "And the thing that allows that encoding to happen is a special function or a manifold that's called the synchronization map, which just maps information from the internal to the external.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 751.877,
    "end": 764.307,
    "text": " And the conditions for this happening is basically that information that gets to the internal world is basically conditionally independent of the external world when that conditioning is done on the interface.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 764.407,
    "end": 768.67,
    "text": "So the interface kind of statistically insulates the internal from the external.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 769.151,
    "end": 774.875,
    "text": "And with that at play, you can actually have the internal states look as if they're encoding the external states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 775.976,
    "end": 780.98,
    "text": "So that's the gist of Bayesian mechanics without leaving out as much math as we possibly can.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 782.287,
    "end": 788.972,
    "text": " So then Bayesian mechanics research first asked the question, what are the necessary and sufficient conditions for that map to exist?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 789.432,
    "end": 796.157,
    "text": "So that's something that the earlier paper, Bayesian mechanics for stationary processes, that's what it explicitly explored.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 796.637,
    "end": 801.461,
    "text": "What are the mathematical conditions for us to be justified in saying, yeah, that map actually exists?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 802.452,
    "end": 812.46,
    "text": " And once we've answered this first aim of Bayesian mechanics, then we can try to actually look for hallmarks of Bayesian mechanics in real systems, biotic and abiotic alike.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 812.92,
    "end": 816.983,
    "text": "So those are the kind of two that the first thing is, what is Bayesian mechanics?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 817.043,
    "end": 818.104,
    "text": "What allows it to exist?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 818.124,
    "end": 822.947,
    "text": "And then the second thing is saying, once we know those conditions, we can actually look for that in the real world.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 825.329,
    "end": 828.532,
    "text": "Okay, so to do this proper analysis of a system,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 829.583,
    "end": 838.048,
    "text": " To identify this three-way partition, internal, blanket, external, you need basically a probability distribution over the states of our system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 838.088,
    "end": 848.514,
    "text": "So that's what's denoted with this p. Probably a distribution that tells you for every setting of mu, b, and eta, internal, blanket, external, what is the probability of that particular configuration?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 849.295,
    "end": 850.776,
    "text": "So that's kind of a tall order.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 850.796,
    "end": 853.417,
    "text": "Getting a full probability distribution for any system is not...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 855.106,
    "end": 883.082,
    "text": " is not trivial yeah for some systems it is trivial but for most of the things we're interested in we usually don't have access to that so um that's why a lot of the work in this field so far has not been really applied to actual empirical data because it's so theoretical at this point and very like in silico um so we're still at that stage where we're still figuring things out theoretically and we haven't moved to like empirical application yet mainly because of these these issues of finding what that probability distribution",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 884.685,
    "end": 886.525,
    "text": " So that's Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 886.605,
    "end": 899.208,
    "text": "And now to move from there to stochastic processes, we're motivated by the fact that in the case we need to find this probability distribution and the internal blanket and external states change over time, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 899.268,
    "end": 903.349,
    "text": "they're dynamic, then we need to move to the realm of stochastic processes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 903.889,
    "end": 910.85,
    "text": "And the reason that is is because stochastic differential equations are the mathematical construct that allow us to write down",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 911.24,
    "end": 914.262,
    "text": " a probability distribution over variables that change over time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 915.944,
    "end": 919.887,
    "text": "So a way to express that is using a stochastic differential equation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 919.907,
    "end": 930.295,
    "text": "This is the basic a basic kind of abstract formalism for understanding probability distributions over paths or over sequences of states over time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 930.635,
    "end": 932.717,
    "text": "That's really what a stochastic process is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 933.137,
    "end": 940.563,
    "text": "But we like to formally summarize that that probability distribution using this kind of differential equation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 941.34,
    "end": 964.941,
    "text": " all this differential equation is saying is that the rate of change of some state is a function of the state so it's basically a deterministic function of the state where where i'm going is a function of where i am plus some random noise and the random noise is just basically there to to incorporate the fact that i don't know my position",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 965.785,
    "end": 967.226,
    "text": " with certainty over time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 967.406,
    "end": 970.149,
    "text": "I have some sense that's given by that deterministic flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 970.749,
    "end": 972.951,
    "text": "But overall, there's uncertainty in where I'll be next.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 972.971,
    "end": 975.693,
    "text": "So we capture all that uncertainty with this thing, noise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 976.234,
    "end": 980.838,
    "text": "And that's the basic structure of any stochastic process or stochastic differential equation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 983.098,
    "end": 990.28,
    "text": " Okay, so now we're going to zoom in onto this flow component, which is the deterministic part of the stochastic process.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 990.74,
    "end": 998.883,
    "text": "So hereafter, when I say flow or drift, we're referring to the deterministic part of this flow of this stochastic differential equation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 999.403,
    "end": 1010.686,
    "text": "We're going to take that and we're going to expand it to give it a new interpretation that will allow us to get back to our original goal, which is writing down a probability distribution so that we can do Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1013.188,
    "end": 1018.109,
    "text": " The interpretation that allows us to make those moves is called the Helmholtz decomposition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1018.569,
    "end": 1027.671,
    "text": "And it's basically saying that any flow of a stochastic differential equation, there are a few conditions on the flow that we can get into.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1027.731,
    "end": 1029.111,
    "text": "They're kind of technical conditions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1029.531,
    "end": 1036.232,
    "text": "But if this flow meets those basic conditions, which are quite general, then you can decompose it into the sum of two parts.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1037.052,
    "end": 1041.973,
    "text": "One is called the reversible part of the flow, and the other one is called the irreversible part of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1043.939,
    "end": 1045.947,
    "text": " And we'll get into now what that actually means.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1046.451,
    "end": 1049.332,
    "text": " So the first one is called the reversible part of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1049.853,
    "end": 1057.757,
    "text": "And it can basically be seen as a gradient flow towards the maximum of the probability density of the whole system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1058.157,
    "end": 1062.419,
    "text": "So that probability distribution we talked about at the beginning, it has peaks and valleys.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1062.599,
    "end": 1064.8,
    "text": "There's areas of higher density and lower density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1065.341,
    "end": 1073.245,
    "text": "This reversible part of the flow is just telling the system, bring me to the local maxima or to the peaks of that probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1074.065,
    "end": 1090.545,
    "text": " This is also referred to as dissipative or time reversible because a system that's driven by this gradient flow has time reversibility or time symmetry in the sense that if you run one version of the process forwards in time, it'll look the same ran backwards.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1092.0,
    "end": 1098.663,
    "text": " And the important part here is that it's tightly related to the noise or the random fluctuations that we talked about from earlier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1099.204,
    "end": 1109.589,
    "text": "So the strength of the force that pushes you towards the maxima of your probability density is directly proportional to the amplitude or the variance of the noise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1110.229,
    "end": 1120.714,
    "text": "So that's a very interesting concept that might be a little counterintuitive at first, but basically those random fluctuations that we're adding to the flow in the Langevin equation,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1121.701,
    "end": 1130.589,
    "text": " How big those fluctuations are determines how quickly or with what force we get to the probability density's maxima, which you might think that's a little weird.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1130.629,
    "end": 1133.631,
    "text": "Why would more noise actually make us get to where we're going faster?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1133.671,
    "end": 1135.893,
    "text": "Wouldn't that push us away from our path?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1136.454,
    "end": 1141.618,
    "text": "But if you think about averages and think about the density of paths trying to get somewhere in probability density,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1142.096,
    "end": 1151.64,
    "text": " the more noise you have, the actual more chances the system has to kind of explore state space and then reach its eventual average like steady state density faster.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1152.401,
    "end": 1162.045,
    "text": "So that's one part is this gradient flow, this dissipative flow whose strength is proportional to the amplitude of random fluctuations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1163.547,
    "end": 1170.392,
    "text": " And then secondly, we have this more interesting thing, which is called the conservative or time irreversible component, also called the solenoidal flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1171.053,
    "end": 1181.941,
    "text": "And this is mediated by this anti-symmetric operator called Q. And this basically doesn't point either in the direction of the minima or the maxima of the probability density of the system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1182.341,
    "end": 1188.085,
    "text": " but rather it points you along the isocontours of equal probability of that density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1188.165,
    "end": 1190.947,
    "text": "So this is the kind of circuitous component of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1191.707,
    "end": 1205.877,
    "text": "And it's very interesting from the perspective of thermodynamics because systems with this time irreversibility, they exhibit positive entry pre-production, for instance, and they basically have conservative dynamics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1208.017,
    "end": 1214.639,
    "text": " So here's an example on the left of a stochastic process that's just driven by gradient flow with some noise on top of it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1215.079,
    "end": 1220.72,
    "text": "And you can see that it's kind of ascending or running parallel to the probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1221.32,
    "end": 1225.802,
    "text": "And then on the right, we have a deterministic system that's just driven by solenoidal flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1226.182,
    "end": 1231.503,
    "text": "And it kind of acts like a celestial body, like a planet that's orbiting some center of gravity.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1233.225,
    "end": 1241.067,
    "text": " And I should say that the blue background on each plot is basically showing how high the probability density is of the states at each point in state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1241.087,
    "end": 1242.707,
    "text": "So this is just a two dimensional system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1243.328,
    "end": 1252.03,
    "text": "So the Helmholtz decomposition is saying that you can take these two components and together they form the full dynamic of any stochastic system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1252.93,
    "end": 1258.011,
    "text": "So the full dynamic has components of trying to get to the maxima of the probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1258.544,
    "end": 1262.85,
    "text": " And then it also has components of surfing the isocontours of probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1263.41,
    "end": 1267.616,
    "text": "And all of this is subject to random fluctuations that are knocking it off the path.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1268.056,
    "end": 1273.924,
    "text": "So you can get rich dynamics with just this very simple decomposition of any system's flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1277.294,
    "end": 1280.876,
    "text": " So this is another way that we can visualize these things in terms of vector fields.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1281.296,
    "end": 1286.838,
    "text": "So on the left, I'm showing the solenoidal flow, like the strength of that component at each point in state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1286.978,
    "end": 1291.24,
    "text": "So x and y are just the two levels of some two-dimensional random variable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1292.241,
    "end": 1295.142,
    "text": "Here are the components of the dissipative flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1295.882,
    "end": 1302.525,
    "text": "And you can see that they're parallel to the gradients that pull you towards or away from the center of the probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1303.456,
    "end": 1317.151,
    "text": " And then if you add these two things together, then you get the force that's driving the system at every point in state space, which is this combination of this kind of orbiting fluctuation, as well as this thing that's pulling it towards the center.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1318.052,
    "end": 1324.018,
    "text": "And just a kind of technical note, the reason that the dissipative flow looks like it's pointing away from the center in the middle plot",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1324.824,
    "end": 1331.269,
    "text": " but it's pointing to the center in the right plot is because there's a minus sign in front of the dissipative operator on the right plot.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1331.509,
    "end": 1334.371,
    "text": "So that's an important thing that often confuses people.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1336.232,
    "end": 1337.673,
    "text": "So now we've done this decomposition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1337.713,
    "end": 1338.914,
    "text": "We have this gradient flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1339.074,
    "end": 1340.275,
    "text": "We have this solenoidal flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1342.835,
    "end": 1363.531,
    "text": " And the thing that brings us back to Bayesian mechanics is this important point that I left out, which is that the gradient flow and the solenoidal flow, they don't just operate on the state of the system, but they operate on the gradient of the probability density, which is given by this kind of weird I slash J operator, which we can just call the surprisal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1364.943,
    "end": 1369.327,
    "text": " So the surprise is defined as the negative log of the probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1369.687,
    "end": 1372.67,
    "text": "It's also known in physics as a scalar potential.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1373.29,
    "end": 1379.676,
    "text": "And this just says how basically tells you how far away you are in some sense from the center of the probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1380.076,
    "end": 1382.338,
    "text": "It's saying, what is the local curvature right now?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1382.478,
    "end": 1385.841,
    "text": "How steep is the probability landscape at the point I am right now?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1387.921,
    "end": 1393.024,
    "text": " So this, in turn, these are the things that scale the dissipative and the solenoidal operators.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1393.064,
    "end": 1402.789,
    "text": "And this thing, in turn, can be formally related to the steady state distribution of the process, this probability density over mu, b, and eta that we were originally interested in.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1403.169,
    "end": 1410.153,
    "text": "So because it's the negative log of that density, you can exponentiate its negative, and you get yourself back to the density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1410.553,
    "end": 1413.195,
    "text": "And that's excluding something that's called a partition function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1413.695,
    "end": 1416.977,
    "text": "But that density is called the non-equilibrium steady state density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1417.817,
    "end": 1424.099,
    "text": " And the existence of that is kind of fundamental to the whole free energy principle Bayesian mechanics thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1424.639,
    "end": 1430.261,
    "text": "If a system has this Ness density, then the system can achieve Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1431.461,
    "end": 1434.422,
    "text": "And another important point is if Q is non-zero,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1435.305,
    "end": 1437.705,
    "text": " the system truly is non-equilibrium.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1438.226,
    "end": 1447.987,
    "text": "If it is zero, if the Q does not exist and there's only dissipative currents, then the system is known as an equilibrium system, and it just has an equilibrium steady state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1448.467,
    "end": 1459.409,
    "text": "So the Q is basically what mediates all the things that we're interested in biological systems, like the breaking of detailed balance, positive entropy production rate, and time irreversibility.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1459.449,
    "end": 1462.45,
    "text": "That's all kind of baked into the system with Q.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1465.2,
    "end": 1479.427,
    "text": " So the main point of all these slides on stochastic processes, Helmholtz decomposition, is just the fact that that deterministic flow component of the Langevin equation can be rewritten in terms of gradients of its log stationary density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1479.467,
    "end": 1492.474,
    "text": "So it looks as if any stochastic process is basically being dragged towards its stationary density, subject to random noise and these solenoidal components that kind of drive it along the isocontours.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1494.747,
    "end": 1500.992,
    "text": " So we can now kind of take the steps required to rewrite our full-on Gevin equation using this new interpretation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1501.853,
    "end": 1507.638,
    "text": "So first you have this flow operator, which is just a combination of the irreversible and reversible components.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1508.859,
    "end": 1512.202,
    "text": "Then you can plug that into our expression for F, which is the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1512.682,
    "end": 1521.27,
    "text": "So it's the flow operator times the gradients of the log density, the negative log density, and then minus this extra term that I'll talk about in a few minutes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1523.015,
    "end": 1538.238,
    "text": " And then you can just combine all this together, plug it back into the Langevin equation, and now we have an expression for the Langevin equation, namely the rate of change of the state in terms of a gradient descent on this surprisal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1538.659,
    "end": 1544.62,
    "text": "So basically, all this is saying is that a system looks like it's trying to minimize the surprisal of its states, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1545.18,
    "end": 1549.501,
    "text": "find itself in parts of state space that are the least surprising or have the highest probability.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1550.925,
    "end": 1554.507,
    "text": " So as I just mentioned, there's this extra term that we haven't talked about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1555.067,
    "end": 1557.609,
    "text": "It's kind of called this housekeeping or correction term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1557.989,
    "end": 1561.831,
    "text": "And this, I think, has come up in a few of the past live streams, if I remember correctly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1562.231,
    "end": 1564.452,
    "text": "And this is going to be really important for the current paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1564.472,
    "end": 1569.155,
    "text": "It kind of mediates all the interesting things that we see in things like stochastic chaos.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1569.755,
    "end": 1574.458,
    "text": "So this is called what Carl calls in the paper the housekeeping term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1575.273,
    "end": 1582.315,
    "text": " And it only arises in the case that those solenoidal and dissipative matrices depend on the state of the system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1583.195,
    "end": 1599.639,
    "text": "So for these earlier papers we were talking about, like the Bayesian mechanics for stationary processes, and Miguel's paper, as well as the memory and Markov blankets paper, those three papers, for the most part, assume that these matrices were constant in the sense that they don't depend on the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1600.475,
    "end": 1604.196,
    "text": " That's what makes a system a linear versus a nonlinear system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1604.316,
    "end": 1606.916,
    "text": "If these matrices are constant, then the system is linear.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1607.336,
    "end": 1609.277,
    "text": "If they are not constant in the states, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1609.297,
    "end": 1616.978,
    "text": "they depend on the part of state space you're in, then it's a nonlinear system, and this housekeeping term is nonzero.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1617.078,
    "end": 1619.639,
    "text": "It actually has some influence on the dynamics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1621.179,
    "end": 1625.44,
    "text": "So this is something that figures into the current paper on the Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1625.95,
    "end": 1629.152,
    "text": " The Lorenz attractor, which we'll get into later, is a nonlinear system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1629.592,
    "end": 1631.813,
    "text": "So this housekeeping term becomes nonzero.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1633.134,
    "end": 1636.576,
    "text": "And if anyone's technically interested, that's just what it looks like.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1636.616,
    "end": 1641.139,
    "text": "It's basically a type of matrix field divergence, which is a mouthful.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1641.199,
    "end": 1647.563,
    "text": "But it's basically a sum of partial derivatives of the matrix field across different state variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1649.884,
    "end": 1651.085,
    "text": "So given that, we can now",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1651.878,
    "end": 1660.744,
    "text": " In the case of a nonlinear system where the Q matrix actually depends on the state, now we're showing the solenoidal operator as a function of state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1660.784,
    "end": 1668.889,
    "text": "And you can see that now the solenoidal flow actually depends on which part of the xy plane you're in, here denoted x1, x2.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1669.91,
    "end": 1673.192,
    "text": "The dissipative operator I've kept constant just to be simple.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1674.161,
    "end": 1679.789,
    "text": " And then you have additionally this housekeeping term that is also a function of state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1680.089,
    "end": 1683.514,
    "text": "And it's kind of actually counteracting in some sense the dissipative operator.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1683.534,
    "end": 1689.062,
    "text": "And it's a weird combination of kind of flows that are pulling you towards the center as well as away from the center.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1689.542,
    "end": 1691.165,
    "text": "And then the full dynamic is going to be",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1691.625,
    "end": 1697.127,
    "text": " this combination between a housekeeping term, the solenoidal term, and the dissipative term altogether.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1697.667,
    "end": 1704.95,
    "text": "So these are the kind of systems that you're going to more likely encounter in the real world, so to speak, because most real world systems are nonlinear.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1705.49,
    "end": 1711.252,
    "text": "They can't be described with nice Ornstein-Uhlenbeck processes where the housekeeping term disappears.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1713.633,
    "end": 1720.315,
    "text": "Okay, so that was our big detour into Helmholtz decomposition, and it was a lot of math and theory, but it'll actually become",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1720.967,
    "end": 1723.609,
    "text": " concrete when we apply it to the Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1723.97,
    "end": 1728.874,
    "text": "So now let's go back to our desired goal of getting that probability distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1729.515,
    "end": 1736.321,
    "text": "So we want a probability distribution over mu internal states, blanket states, external states, so that we can do Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1738.041,
    "end": 1747.966,
    "text": " And now we're going to say that our system of interest x is just a 3D system, a 3D vector that's comprised of internal, blanket, external at every time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1747.986,
    "end": 1751.868,
    "text": "So we want to have a 3D probability distribution over those variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1753.888,
    "end": 1761.653,
    "text": " So once we have that probability distribution, we can basically do what's called marginalized the density and find conditional density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1761.693,
    "end": 1766.197,
    "text": "So we can say, give me the conditional density over internal given blanket states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1766.497,
    "end": 1769.519,
    "text": "Give me the conditional density over external given blanket states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1770.119,
    "end": 1775.723,
    "text": "Those two densities are theoretically the things that get mapped to each other through the synchronization map.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1776.103,
    "end": 1782.488,
    "text": "So that's something I didn't really mention in the beginning with this theoretical kind of more semantic description of Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1783.237,
    "end": 1786.399,
    "text": " But technically, Bayesian mechanics has to do with conditional densities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1786.459,
    "end": 1793.705,
    "text": "It says, tell me, conditioned on some level of the blanket states, what is the most likely value of the internal states?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1794.085,
    "end": 1801.811,
    "text": "And if you know that and you have a synchronization map, you can kind of guess the most likely value of the external states conditioned on blanket states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1802.372,
    "end": 1806.515,
    "text": "So the synchronization map is really mapping the two conditional densities to each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1811.029,
    "end": 1835.588,
    "text": " quick question so at that point can we do because you've used phrases like zoom in and zoom out and when you're talking about decomposition do we know where on on the scalar that will this tell us where something is focused or versus something that's looking at horizons",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1837.005,
    "end": 1837.725,
    "text": " Do you know what I'm saying?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1837.846,
    "end": 1844.55,
    "text": "Like, can we tell what the field is based on this, that the thing is trying to map?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1846.231,
    "end": 1851.454,
    "text": "And when you say horizons, do you mean like temporal horizons or do you mean more spatial?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1852.035,
    "end": 1853.456,
    "text": "You know what I'm saying?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1853.496,
    "end": 1860.02,
    "text": "Like, can we tell whether it's zooming in on a focal point or whether it's looking at a larger picture?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1860.92,
    "end": 1861.321,
    "text": "Oh, I see.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1861.341,
    "end": 1863.162,
    "text": "Time-wise and map-wise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1864.289,
    "end": 1879.294,
    "text": " Yeah, so the map technically is very focal in the sense that I haven't shown it in the lower right because I didn't want to get too technical, but the synchronization map technically maps the sufficient statistics of the densities together.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1879.354,
    "end": 1882.474,
    "text": "So it maps a particular point on a statistical manifold.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1883.275,
    "end": 1891.597,
    "text": "So when I say statistical manifold, I mean a space that is not spanned by the values of the system, but actually by the expected values or the averages of the system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1891.957,
    "end": 1893.958,
    "text": "So it says on average,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1894.898,
    "end": 1897.903,
    "text": " the internal states will get mapped to the external states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1898.444,
    "end": 1904.994,
    "text": "So in that sense, it's kind of a point because you're saying I'm mapping from a most likely value or from an average.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1905.612,
    "end": 1909.035,
    "text": " to another average on the other side of the blanket.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1909.455,
    "end": 1919.303,
    "text": "So in that sense, you're kind of doing a dimensionality reduction, because the map is not defined for any particular little fluctuation of the internal or the external, like some particular instance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1919.683,
    "end": 1922.266,
    "text": "But it's actually really defined on this average level.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1922.806,
    "end": 1928.551,
    "text": "So you're only going to be seeing the inference when you take averages across multiple noisy realizations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1929.671,
    "end": 1952.692,
    "text": " um honor if i could ask one more question to kind of lock in the gains of this great section is it fair to say that the synchronization map is literally the internal states map of the external states territory yeah exactly okay the synchronization map will take any internal state and it will map it into",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1953.467,
    "end": 1955.328,
    "text": " a map, like it'll map it into a space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1955.848,
    "end": 1960.149,
    "text": "And that space is a map of the external state's territory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1960.769,
    "end": 1962.37,
    "text": "And it might not be correct.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1962.39,
    "end": 1963.27,
    "text": "It could be off.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1963.71,
    "end": 1967.691,
    "text": "But it's basically taking any internal state and it's projecting it into a representation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1968.171,
    "end": 1977.654,
    "text": "And that representation's metric space, like when I move around in that represented space, is supposed to mirror how external states are actually moving.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1979.196,
    "end": 1985.26,
    "text": " And the key point of the synchronization map is the map and the territory will only be aligned on average.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1985.721,
    "end": 1999.409,
    "text": "But for any given particular instance of the internal states, that guess or that place it gets mapped to on the map, on the representation, might be arbitrarily far from the true external state that's going on at that time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1999.749,
    "end": 2002.431,
    "text": "But on average, they'll be exactly on top of each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2003.479,
    "end": 2007.262,
    "text": " And there's one comment in the chat from Miguel Aguilera.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2007.302,
    "end": 2013.346,
    "text": "So I'm going to read it to you, Connor, and you can address it now, or if it comes at a better time later, then address it then.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2013.967,
    "end": 2018.25,
    "text": "So Miguel writes, great introduction of the Helmholtz decomposition.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2018.991,
    "end": 2021.993,
    "text": "How general is the housekeeping lambda term?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2022.393,
    "end": 2027.877,
    "text": "Can it account for any nonlinear systems with higher order terms, for example, larger than quadratic?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2030.239,
    "end": 2031.06,
    "text": "That's a great question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2031.8,
    "end": 2032.841,
    "text": "Yes, it totally can.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2036.868,
    "end": 2039.849,
    "text": " So if we go back to it, where was it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2047.032,
    "end": 2049.093,
    "text": "I'll just address it now, if that's OK with you, Daniel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2049.113,
    "end": 2049.193,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2050.653,
    "end": 2056.155,
    "text": "Yeah, so if you look at this housekeeping term, it's defined as this divergence of the matrix field.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2056.776,
    "end": 2060.577,
    "text": "And it's a sum of the partial derivatives of the flow operator.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2061.437,
    "end": 2063.499,
    "text": " So now I'm talking about the last equation on the slide.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2063.519,
    "end": 2066.142,
    "text": "The flow operator again is the Q minus the gamma.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2066.862,
    "end": 2074.95,
    "text": "So those partial derivatives, if they're higher order than quadratic, they will have nonlinear terms, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2074.99,
    "end": 2076.251,
    "text": "So if you're taking",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2077.939,
    "end": 2084.222,
    "text": " the partial derivative of something that has a third or a fourth order, then the partial derivative itself will be nonlinear.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2084.582,
    "end": 2089.844,
    "text": "So all that means is that the housekeeping term itself will be nonlinear in the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2090.284,
    "end": 2092.586,
    "text": "So the housekeeping term is also a function of the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2093.206,
    "end": 2101.609,
    "text": "And in this case, with higher order, more than quadratic terms, then you'll just get an appropriately nonlinear housekeeping term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2101.629,
    "end": 2107.272,
    "text": "So for instance, if we go back to this slide, in the third",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2108.976,
    "end": 2117.898,
    "text": " the third column, the housekeeping term, because I made the Q matrix, the solenoidal matrix, a second order in the states, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2118.018,
    "end": 2125.52,
    "text": "there's an interaction between X1 and X2, a multiplicative interaction, the housekeeping term becomes linear in one of the two states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2125.9,
    "end": 2129.161,
    "text": "So it's just a simple linear function of X1 and X2.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2130.061,
    "end": 2132.042,
    "text": "If, however, we were to imagine that either Q",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2133.217,
    "end": 2134.057,
    "text": " or gamma.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2134.237,
    "end": 2135.558,
    "text": "Gamma is the dissipative operator.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2135.578,
    "end": 2136.718,
    "text": "I made it constant for now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2136.758,
    "end": 2146.722,
    "text": "But if one of those had a higher order term, like a x to the third term, or x1 squared times x2, then the housekeeping term would also have now a nonlinear term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2147.222,
    "end": 2150.504,
    "text": "And now the housekeeping would be nonlinear in the state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2151.044,
    "end": 2152.084,
    "text": "And there's nothing wrong with that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2152.104,
    "end": 2156.466,
    "text": "It would just mean the housekeeping term would be nonlinear, and the dynamics would be even more complex.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2158.126,
    "end": 2159.267,
    "text": "Yeah, that's a great question, Miguel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2159.307,
    "end": 2159.507,
    "text": "Thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2160.413,
    "end": 2163.234,
    "text": " And if I could ask some follow-up housekeeping questions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2163.694,
    "end": 2169.317,
    "text": "So one, can the housekeeping be non-differentiable?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2169.517,
    "end": 2173.278,
    "text": "Or what if there's some function that doesn't have a nice derivative?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2173.778,
    "end": 2182.021,
    "text": "And the second part is, what does the housekeeping term represent for a given example system?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2182.602,
    "end": 2185.403,
    "text": "And how is it different than the regular Helmholtz decomposition?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2186.643,
    "end": 2188.324,
    "text": "That's a good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2189.413,
    "end": 2192.735,
    "text": " So the first question is about differentiability.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2192.775,
    "end": 2210.104,
    "text": "So I mentioned in the beginning that for a system to have these nice things we were talking about, like to basically be applied to the Helmholtz decomposition, to be applied to it, et cetera, one of the conditions is that the flow function, that f of x, is a smooth function of the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2210.685,
    "end": 2213.466,
    "text": "So it basically means it's differentiable everywhere.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2214.247,
    "end": 2218.489,
    "text": "As long as the flow is differentiable, it implies that all the subcomponents are also differentiable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2220.0,
    "end": 2221.981,
    "text": " if my memory of calculus serves me well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2222.021,
    "end": 2228.743,
    "text": "So basically, it means that Q will be differentiable, which means that all its higher order derivatives are also differentiable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2229.143,
    "end": 2235.105,
    "text": "So basically, the very premise of this decomposition assumes that things are differentiable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2235.145,
    "end": 2236.805,
    "text": "So that problem shouldn't arise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2236.885,
    "end": 2240.926,
    "text": "Or if there's non-differentiable flow function, then you're right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2240.966,
    "end": 2243.227,
    "text": "All these things will have discontinuities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2243.307,
    "end": 2249.129,
    "text": "And I don't think the solution or there aren't any guarantees that the decomposition even applies in that case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2250.126,
    "end": 2254.49,
    "text": " Um, the second question, can you, sorry, can you remind me what the second one?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2254.63,
    "end": 2255.17,
    "text": "Oh, sorry.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2255.21,
    "end": 2255.35,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2255.39,
    "end": 2257.032,
    "text": "The interpretation of the housekeeping term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2257.412,
    "end": 2258.373,
    "text": "That's a really good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2258.553,
    "end": 2260.875,
    "text": "I, this is something we talked a lot about with Carl.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2261.496,
    "end": 2265.879,
    "text": "Um, when, when we discussed this paper at a TNB meeting, like how do you interpret this?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2265.919,
    "end": 2267.12,
    "text": "Cause the solenoidal flow, right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2267.16,
    "end": 2267.861,
    "text": "I can imagine it's.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2268.305,
    "end": 2269.906,
    "text": " You're going along the isocontours.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2270.426,
    "end": 2271.526,
    "text": "Dissipative makes sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2271.546,
    "end": 2274.047,
    "text": "You're going up and down the gradients of the density itself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2274.467,
    "end": 2282.129,
    "text": "But the housekeeping term is this weird thing where it has to do with the derivatives of the state-dependent solenoidal and dissipative flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2282.95,
    "end": 2292.253,
    "text": "I don't know how to interpret it, to be honest with you, in terms of one of these nice kind of orthogonality, because it's not an orthogonalization of the state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2292.573,
    "end": 2296.734,
    "text": "It's a mixture of both state-dependent solenoidal and dissipative terms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2298.234,
    "end": 2303.202,
    "text": " Um, the one thing he did say that, that Carl said to us that, uh,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2303.857,
    "end": 2330.342,
    "text": " will come to later is that the housekeeping term in the case of chaotic dynamics is actually what mediates the wandering seemingly non-steady state-ish behavior of things like the lorenzo tractor so the lorenzo tractor seems to kind of go on these itinerant wanderings so that are arbitrarily far from its steady state density whatever that steady state density is because again we don't know what it is and basically the housekeeping term in this case",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2330.942,
    "end": 2353.271,
    "text": " can push the system away from the gradients of the log density and also away from the level sets of the iso contours of the density given by the solenoidal flow so an example of that is actually here right where you see in kind of the main xy planes of the if you compare the red plot to the purple plot the red arrows are actually pointing sometimes in the opposite arrows",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2354.452,
    "end": 2355.514,
    "text": " of the purple arrows.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2356.054,
    "end": 2362.946,
    "text": "So one interpretation is that for sufficiently complex systems, sometimes they'll actually be quite strong components of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2363.459,
    "end": 2368.501,
    "text": " that are driving it away from the probability density's maxima.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2369.282,
    "end": 2380.907,
    "text": "So this might actually, if you want to try to interpret this in the context of biological systems, systems sometimes seem to wander very far away from what in a greedy sense would be the best state to be in, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2380.927,
    "end": 2382.828,
    "text": "a local mode of the probability density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2383.348,
    "end": 2387.65,
    "text": "And this housekeeping term can sometimes drive the system into long wandering sojourns",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2388.08,
    "end": 2390.142,
    "text": " that are far away from those local maxima.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2391.603,
    "end": 2403.435,
    "text": "And it's the strength of that housekeeping term versus the dissipative term that Carl argues is actually the Helmholtz decomposition take on chaotic behavior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2403.455,
    "end": 2411.423,
    "text": "So chaotic behavior is when things wander arbitrarily far, basically because the housekeeping term dominates and pushes things away from the modes of the density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2412.335,
    "end": 2416.121,
    "text": " That was the closest that I got to a kind of interpretation of the housekeeping term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2416.461,
    "end": 2417.362,
    "text": "But it's a really good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2417.423,
    "end": 2420.828,
    "text": "I would be curious to hear what other people have to say about that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2420.848,
    "end": 2423.171,
    "text": "OK.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2428.82,
    "end": 2430.641,
    "text": " Yes, so right, this is where we work.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2430.681,
    "end": 2432.362,
    "text": "So we're talking about the synchronization map.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2432.942,
    "end": 2434.043,
    "text": "That's where we want to get to.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2434.083,
    "end": 2446.49,
    "text": "We want to get to being able to write down those conditional densities so that we can map them to each other and then imbue our system with some form of sentience, or at least a sentient interpretation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2446.85,
    "end": 2448.151,
    "text": "So how do we get this density?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2448.686,
    "end": 2456.211,
    "text": " So this brings us to the first major contribution of the paper, which is a new method for approximating a system where we don't have that density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2456.732,
    "end": 2461.755,
    "text": "So for instance, the system we're about to talk about, the Lorenz attractor, we don't know what the probability density is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2462.156,
    "end": 2466.779,
    "text": "So we're going to use a new method proposed in this paper to approximate that density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2468.18,
    "end": 2473.984,
    "text": "And the method used to approximate the density relies on a combination of polynomial regression",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2474.614,
    "end": 2476.636,
    "text": " as well as the Helmholtz decomposition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2476.996,
    "end": 2492.669,
    "text": "So that's why I just spent so much time going through the Helmholtz decomposition, because this new method proposed by the paper for fitting the density relies on the Helmholtz decomposition as well as basically some statistical inference techniques or polynomial regression.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2494.291,
    "end": 2498.174,
    "text": "So now we're going to actually make things specific and dive into a particular system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2498.925,
    "end": 2509.493,
    "text": " So the Lorenz attractor, a phase space portrait of it is shown in the top left, is a famous deterministic dynamical system that exhibits what's known as chaos.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2509.693,
    "end": 2513.095,
    "text": "And again, I'm going to not go too into depth here to save time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2513.155,
    "end": 2519.66,
    "text": "And last time in one of the earlier streams, Daniel gave a very nice explanation of chaotic behavior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2519.98,
    "end": 2525.745,
    "text": "But it basically means when you very slightly alter the initial conditions of the system,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2526.479,
    "end": 2532.127,
    "text": " the end up and resulting trajectory can be arbitrarily far in state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2532.588,
    "end": 2534.952,
    "text": "So if I start the system at setting 0.1 versus 0.101,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2537.81,
    "end": 2541.874,
    "text": " the resulting place that those two trajectories end up can be super far away from each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2542.315,
    "end": 2555.729,
    "text": "So it's quantified most often by analyzing what's known as the eigenvalues of the Jacobian or the Lyapunov exponent, the sign of which will tell you how far systems on how far trajectories on average should diverge.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2555.749,
    "end": 2557.271,
    "text": "And that's kind of the hallmark of chaos.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2558.206,
    "end": 2559.307,
    "text": " So that's the Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2559.347,
    "end": 2563.649,
    "text": "It gives these nice kind of itinerant wandering butterfly lobes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2563.689,
    "end": 2571.772,
    "text": "When you look at its trajectories over time, an interesting property is that no same one trajectory is the same for a slightly different initial condition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2572.152,
    "end": 2573.253,
    "text": "That's why we call it chaotic.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2574.517,
    "end": 2582.963,
    "text": " What we study in this paper is exactly that deterministic chaotic system, but we just add noise onto its flow function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2583.483,
    "end": 2585.544,
    "text": "So that's what's represented with the equations here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2586.004,
    "end": 2588.606,
    "text": "So we can call it the stochastic Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2591.788,
    "end": 2596.549,
    "text": " It's not been characterized super rigorously as much as the Lorenz attractor has been.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2596.59,
    "end": 2601.991,
    "text": "So it's a kind of interesting experimental stochastic process to investigate here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2602.351,
    "end": 2609.653,
    "text": "So on the bottom right, we see the original stochastic Lorenz flows, which is a nonlinear deterministic dynamical system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2610.013,
    "end": 2617.075,
    "text": "That's that f of x. And what we're doing here is we're just perturbing the system at every time with a little random Gaussian fluctuation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2617.436,
    "end": 2618.856,
    "text": "That's what's represented by omega.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2619.69,
    "end": 2624.259,
    "text": " And I've bolded everything here to represent that we're now dealing with a vector space of states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2624.299,
    "end": 2626.623,
    "text": "So we're dealing with x1, x2, x3.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2626.823,
    "end": 2630.23,
    "text": "It's not just a single variable, but it's a 3D system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2633.028,
    "end": 2636.969,
    "text": " Okay, so now we can get into the major moves of the actual paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2637.389,
    "end": 2641.75,
    "text": "So the first approach is to say, okay, we know about this Helmholtz decomposition thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2642.23,
    "end": 2652.712,
    "text": "Let's try to rewrite the Lorenz attractor as if it's doing a gradient ascent on a log probability density or a gradient descent on a surprisal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2654.593,
    "end": 2656.453,
    "text": "So an important thing to note here is",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2657.677,
    "end": 2662.843,
    "text": " The surprisal, as well as the flow operator, the Q and the gamma, are state dependent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2663.464,
    "end": 2668.27,
    "text": "And as we know now from our discussion of the housekeeping term, we're going to get that extra lambda term",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2669.361,
    "end": 2671.462,
    "text": " because the system is state dependent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2671.502,
    "end": 2675.484,
    "text": "And you can just look at the flows for the Lorenz attractor to see that very clearly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2675.944,
    "end": 2679.666,
    "text": "So for instance, f of x is not just first order in the state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2679.706,
    "end": 2682.008,
    "text": "So that would mean it would be a linear system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2682.448,
    "end": 2688.891,
    "text": "But for instance, the flow of x2 depends on a multiplicative interaction between x1 and x3.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2689.451,
    "end": 2694.434,
    "text": "And likewise, the flow of x3 depends on a multiplicative interaction between x1 and x2.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2695.014,
    "end": 2696.075,
    "text": "That state dependence",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2696.875,
    "end": 2702.357,
    "text": " will be exactly recapitulated in the fact that Q and gamma will not be a constant function of the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2702.878,
    "end": 2707.18,
    "text": "And because of that, we'll also get this housekeeping term, this big lambda on the side.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2710.661,
    "end": 2717.104,
    "text": "So that's why the Lorenz attractor was chosen as opposed to an OU process, exactly because it has this non-linear state dependence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2718.482,
    "end": 2737.502,
    "text": " So if we want to write down an OU process, and this is a decomposition that people like Miguel know very well by now, you can just write it down as this linear matrix function, which is just Q, which is a matrix, minus gamma, another matrix, multiplied by the gradients of a Gaussian probability density, a log Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2740.488,
    "end": 2749.993,
    "text": " Okay, so the first approach to tackle decomposing Lorenz attractor as if it's doing a gradient descent is basically fitting that dependency.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2750.053,
    "end": 2754.115,
    "text": "So fitting Q and gamma using polynomial expansions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2754.635,
    "end": 2758.997,
    "text": "So assume we didn't know the true Lorenz flows and we just want to basically fit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2759.177,
    "end": 2762.819,
    "text": "Okay, if I want to express the Lorenz flows in terms of a Q and gamma, what would I do?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2763.439,
    "end": 2791.535,
    "text": " the first approach is to first of all take our state space which is x1 x2 x3 and expand it in terms of polynomial on basis functions which again is something i think was covered in an earlier live stream it's also known as a taylor expansion where we're saying i'm going to express each state of the system not as x1 x2 x3 but things like x1 squared x1 x2 x1 x2 x3 all these interaction terms and higher order terms and then i'm going to say",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2792.173,
    "end": 2801.244,
    "text": " find me a function that maps from that higher order basis in terms of polynomial expansions to the coefficients of q and gamma respectively.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2802.586,
    "end": 2810.355,
    "text": "And I'm also going to model this surprisal function, which is the negative log of the density as a function of the polynomial states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2811.178,
    "end": 2826.15,
    "text": " So the problem of polynomial regression is just finding the coefficients here, q and h, that need to be multiplied by these polynomial expansions in order to fit the observed flow as best possible.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2826.17,
    "end": 2829.692,
    "text": "So this is a very common technique used in data science.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2829.712,
    "end": 2832.395,
    "text": "Instead of just doing a linear regression between",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2833.435,
    "end": 2843.28,
    "text": " let's say housing prices and the size of the house, you add in higher order terms like the squared size of the house or the size of the house times the size of the pool or whatever.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2843.3,
    "end": 2850.684,
    "text": "So it's like a very classic thing is to use polynomial expansions to get more expressive functions to explain some observed data.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2851.144,
    "end": 2856.487,
    "text": "So we're doing the same thing here except that the data we're trying to explain is the actual flow of the system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2858.71,
    "end": 2865.495,
    "text": " So what the first move of the paper is, is to actually do this fitting process and figure out that you can rewrite",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2867.261,
    "end": 2874.184,
    "text": " the deterministic Lorenz system as if it's doing a gradient descent on this surprisal function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2874.444,
    "end": 2876.965,
    "text": "But as we'll see later, it's actually not a proper surprisal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2877.005,
    "end": 2879.065,
    "text": "So we can just call it a potential function for now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2879.706,
    "end": 2883.327,
    "text": "And basically the solution is a second order function of the state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2883.407,
    "end": 2887.788,
    "text": "So there's a multiplicative term between x1 and x2 with coefficient h5.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2888.128,
    "end": 2889.449,
    "text": "And then there's a linear term in x3.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2891.69,
    "end": 2905.455,
    "text": " And then what this means is that if you do this for the deterministic system, so you actually assume the random fluctuations go to zero, then what you can do is rewrite the parameters of the Lorenz attractor, which are given by sigma, beta, and rho.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2905.835,
    "end": 2909.196,
    "text": "These are the classic ways that people just parameterize the Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2909.656,
    "end": 2914.478,
    "text": "And certain settings of rho and beta lead to chaos and different kinds of attractors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2915.754,
    "end": 2919.699,
    "text": " you can rewrite them in terms of the coefficients of the Helmholtz decomposition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2919.979,
    "end": 2929.371,
    "text": "So that's the first major move is that this Helmholtz decomposition can be used to write down the system's flows as if it's doing a gradient descent on a potential function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2931.152,
    "end": 2937.937,
    "text": " There's a problem with this, though, which is that if we look at that potential, it's actually not a proper probability distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2938.417,
    "end": 2940.919,
    "text": "So this isn't really Bayesian mechanics, this first move.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2941.399,
    "end": 2953.988,
    "text": "All it's saying is that we can force the Helmholtz decomposition to explain the flow function of the Lorentz attractor, but the resulting surprisal function we're left with is actually not a proper probability distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2954.853,
    "end": 2961.656,
    "text": " And if you just analyze that equation and try to interpret it like a probability function, it won't actually work.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2963.117,
    "end": 2967.139,
    "text": "So that has to do with whether the gradients are positive definite, essentially.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2968.616,
    "end": 2981.18,
    "text": " So that motivates the next move, which is where we don't try to approximate the Lorenz attractor exactly, but we try to approximate what Carr refers to as a Laplace approximation to the steady state density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2981.861,
    "end": 2992.584,
    "text": "So here, instead of saying, okay, let's rewrite the Lorenz attractor as Helmholtz decomposed, let's assume that the Lorenz attractor is dragged towards a steady state that's Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2992.904,
    "end": 2994.745,
    "text": "So the Ness is a multivariate Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2995.325,
    "end": 3003.011,
    "text": " And if we force that assumption on and then use our polynomial regression, what is the kind of resulting coefficients that we'll get?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3003.031,
    "end": 3012.698,
    "text": "Because now we're forcing the system to look as if it's doing a gradient descent on a proper surprisal function or a gradient ascent on a proper probability distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3015.379,
    "end": 3018.782,
    "text": "So if you do that constraint and then you fit the Helmholtz decomposition,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3019.246,
    "end": 3027.213,
    "text": " So again, we're fitting the coefficients of the flow operator, q and gamma, in terms of the states using polynomial expansions of the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3027.934,
    "end": 3032.058,
    "text": "And we're constraining the surprisal function to be quadratic, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3032.438,
    "end": 3033.739,
    "text": "the steady state is Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3034.68,
    "end": 3050.125,
    "text": " then we basically create not a perfect recapitulation of the stochastic Lorenz system, but something that looks like the Lorenz system with this weird constraint that it has to be occupying on average a Gaussian steady state, which is not necessarily true of the real Lorenz system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3050.165,
    "end": 3051.626,
    "text": "That's an interesting move here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3053.92,
    "end": 3070.065,
    "text": " And then what you get is, again, what is referred to as the Laplace approximation to the stochastic Lorenz system, which can be validated in terms of how close it is to the real Lorenz system using things like the Hausdorff dimension, which is a quantification of the fractal dimensionality of the system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3070.505,
    "end": 3081.669,
    "text": "So it's very well known that the Lorenz attractor basically lives on a fractal submanifold of its ambient state space, which is quantified by a fractal Hausdorff dimension or a non-integer Hausdorff dimension.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3082.193,
    "end": 3087.095,
    "text": " And that's just a function of, again, of the eigenvalues of the Jacobian of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3087.676,
    "end": 3100.282,
    "text": "And then he also validates the approximation quality by just looking at how correlated the approximate flow is using this polynomial method with the true flow, which is on the x-axis of that correlation plot.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3101.102,
    "end": 3106.885,
    "text": "And then on the right hand, we have some example trajectories of this Laplace approximated Lorenz system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3108.066,
    "end": 3110.247,
    "text": "So this is a very interesting example",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3110.663,
    "end": 3117.589,
    "text": " kind of plot, because it's showing that even though the, the underlying Gauss, uh, underlying steady state density is a multivariate Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3118.229,
    "end": 3128.277,
    "text": "You can see that the actual trajectories of the system don't necessarily look like they're just neatly sitting or wandering around that Gaussian say state again, because of the solenoidal flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3128.418,
    "end": 3137.485,
    "text": "And because of that housekeeping term, which is very large in this case, the trajectories of the system look as if they're making very itinerant wandering trajectories.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3138.029,
    "end": 3146.012,
    "text": " But on average, if you were to take a very infinite time observation of the system, the system would still look as if it sits on a Gaussian steady state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3147.072,
    "end": 3152.875,
    "text": "So that's like the first, I would say, major move of the paper is doing this Laplace approximate",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3154.319,
    "end": 3170.227,
    "text": " Gaussian version of the Lorenz attractor, the stochastic Lorenz attractor, which anecdotally has some features of the Lorenz attractor, like it has these two lobes and all of these quantifications kind of make it correlated, but it's not exactly the Lorenz attractor because we're putting constraints on it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3172.928,
    "end": 3173.948,
    "text": "That was a lot of information.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3174.249,
    "end": 3175.869,
    "text": "Are there any comments or questions so far?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3179.391,
    "end": 3180.532,
    "text": "Okay, so I'll just keep going.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3182.493,
    "end": 3183.373,
    "text": "So importantly,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3184.19,
    "end": 3192.077,
    "text": " Now that we've rewritten the nest density as if it's Gaussian, we can use it to read off conditional independent structure, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3192.097,
    "end": 3192.958,
    "text": "the Markov blanket.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3193.759,
    "end": 3204.028,
    "text": "And the reason that is, is obvious basically from earlier papers like the Bayesian Mechanics for Stationary Processes and Miguel's paper on...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3205.835,
    "end": 3216.281,
    "text": " also on linear diffusion systems where basically you can just look at the Hessian or the inverse of the covariance matrix and you can use that to read off conditional independence, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3216.301,
    "end": 3217.802,
    "text": "to read off the Markov blanket.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3218.502,
    "end": 3228.068,
    "text": "And that simple relationship between the inverse covariance and the Markov blanket only obtains because of the Gaussian nature of the steady state density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3228.768,
    "end": 3233.751,
    "text": "So that's why the Laplace approximation works here or is a nice move to make because it means we can then",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3234.375,
    "end": 3260.913,
    "text": " write down or read off the markov blanket very easily and so the lower right plot is showing that the hessian and the covariance matrix and you can so the the white here means very low values so what we immediately see is that there's basically a markov blanket between the first and the third states um conditioned on the on the second states or actually sorry rather the the first and second states are both conditionally independent of the third states",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3262.072,
    "end": 3265.613,
    "text": " So there's kind of a Markov blanket between those two and the third state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3266.134,
    "end": 3272.036,
    "text": "This one isn't as nice as the coupled system as we'll see further because it's actually not clear what the blanket states are here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3272.136,
    "end": 3275.837,
    "text": "This more seems just like marginal lack of correlation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3279.538,
    "end": 3284.8,
    "text": "But so even though there's like the third and second states are also kind",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3285.424,
    "end": 3289.73,
    "text": " conditionally independent, you can still treat the second state as a blanket state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3290.11,
    "end": 3294.836,
    "text": "And you can basically parameterize the density over the first state given the second state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3295.216,
    "end": 3299.402,
    "text": "So here we're writing down a synchronization map between the first and third states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3299.442,
    "end": 3300.343,
    "text": "And this is symmetric.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3300.383,
    "end": 3303.427,
    "text": "So first can infer third or third can infer first.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3304.222,
    "end": 3307.384,
    "text": " first, but the conditioning, the blanket state is the second state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3308.064,
    "end": 3318.27,
    "text": "So you can kind of see that if I just take the instantaneous value of the third state, it's in some way performing a guess or making an estimate about the most likely value of the first state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3318.81,
    "end": 3321.072,
    "text": "And this relationship goes vice versa.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3321.372,
    "end": 3328.776,
    "text": "And the only reason we can write down that function that maps between the first and the third is again, because we have an expression for the covariance matrix",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3329.335,
    "end": 3335.897,
    "text": " which follows from the Laplace approximation or the Gaussian approximation to this stochastic Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3338.178,
    "end": 3354.683,
    "text": "So once that's been done, the Laplace approximation, then the next move is kind of, it's not a full analysis, but it's showing that you can extend this to non-Gaussian steady states and kind of make higher order approximations, which might be called density learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3354.723,
    "end": 3359.104,
    "text": "So in a case when you don't know, you don't have a parametric form for the steady state density,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3359.827,
    "end": 3371.876,
    "text": " You can parameterize the surprisal function and the Helmholtz decomposition in such a way that you know that the resulting density is a proper probability distribution, but you don't necessarily know what its neat parametric form is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3372.477,
    "end": 3377.741,
    "text": "So the way he does this is basically by parameterizing the quadratic or the potential as quadratic.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3378.37,
    "end": 3383.151,
    "text": " but the covariance or the Hessian of that quadratic potential is state dependent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3383.631,
    "end": 3392.013,
    "text": "So it's almost like you're fitting locally linear synchronization maps to different parts in state space, or you're fitting locally Gaussian approximations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3392.613,
    "end": 3407.536,
    "text": "So the way he does this is essentially I have a efficient parameterization of the covariance matrix of my steady state, but that covariance is actually changing as a function of state space, whereas in Gaussians, the covariance by definition is constant.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3408.322,
    "end": 3424.994,
    "text": " So by doing this, you can basically achieve a nicer approximation to the true stochastic Lorentz system, where all the conditional densities are still Gaussian, but the full multivariate density is not necessarily Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3426.055,
    "end": 3428.917,
    "text": "So that's basically what's shown in the right plot here is this higher order",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3429.674,
    "end": 3431.634,
    "text": " learned stochastic Lorenz system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3432.394,
    "end": 3444.397,
    "text": "But it's not clear in this case what the synchronization map would be, because the synchronization map is not just going to be a simple function of the Hessian, because now the Hessian is changing over state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3446.537,
    "end": 3447.897,
    "text": "Can I ask a question here?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3448.177,
    "end": 3448.677,
    "text": "Yeah, please.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3449.157,
    "end": 3452.578,
    "text": "So from the chat, Martin Beal has asked, is this right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3453.238,
    "end": 3458.779,
    "text": "There are two approximations here, one of the dynamics and one of the stationary state.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3463.013,
    "end": 3483.242,
    "text": " Yes, I would say that the approximations are intrinsically tied together though, because what you're doing here is you're approximating the potential function and then that approximation directly factors into the approximation of the flow, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3483.262,
    "end": 3488.985,
    "text": "So you're fitting in this case, these kernel matrices, these K matrices as being state dependent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3489.622,
    "end": 3497.005,
    "text": " So the kernel matrix is basically a Cholesky factor of the covariance matrix, or really of the Hessian here, the precision matrix.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3497.925,
    "end": 3502.007,
    "text": "And the gradients of that will then factor into the parameterization of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3502.047,
    "end": 3509.87,
    "text": "Because the flow is defined in terms of the Helmholtz decomposition, which is q minus gamma times the gradients of the potential.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3510.591,
    "end": 3518.654,
    "text": "So however you fit the stationary density, those coefficients that you fit will also factor in to the parameterization of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3519.201,
    "end": 3525.345,
    "text": " So in this case, in the upper left, I have the parameterization of the density, this quadratic function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3525.725,
    "end": 3545.078,
    "text": "If I take the gradients of that with respect to the states, it'll essentially look like k transpose k times x minus mu, or basically a precision weighted prediction error, where k transpose k is the precision, and then x minus mu is the deviation of x from some mean vector.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3546.181,
    "end": 3548.702,
    "text": " So you're doing all that simultaneously.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3548.722,
    "end": 3553.264,
    "text": "You're doing both the parameterization of the steady state as well as the parameterization of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3553.664,
    "end": 3558.066,
    "text": "But you're doing it all just by trying to fit the flow with these coefficients.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3558.147,
    "end": 3567.631,
    "text": "But for free, some of those coefficients you get out, you can use them to write down a potential function, which can be converted to a stationary density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3570.572,
    "end": 3571.293,
    "text": "Does that make sense?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3572.213,
    "end": 3572.433,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3572.513,
    "end": 3574.114,
    "text": "And just on this slide, what is PD?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3575.805,
    "end": 3577.006,
    "text": " Oh, sorry, positive definite.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3577.186,
    "end": 3577.366,
    "text": "OK."
  },
  {
    "start": 3578.286,
    "end": 3579.827,
    "text": "Yeah, that's a good point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3581.187,
    "end": 3586.75,
    "text": "So yeah, because these kernel matrices are symmetric, these k transpose times k, they're symmetric.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3587.89,
    "end": 3589.991,
    "text": "They're state dependent, but they are always symmetric.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3590.331,
    "end": 3601.396,
    "text": "It means that their product will always be positive definite, which means that the stationary density evaluated at different points in state space will always have locally",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3602.867,
    "end": 3604.887,
    "text": " be locally convex, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3604.947,
    "end": 3606.788,
    "text": "there's going to be some local mode.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3607.168,
    "end": 3615.569,
    "text": "But since that PD matrix is changing over state space, you're going to basically have a multimodal density, but locally it'll be unimodal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3615.869,
    "end": 3618.29,
    "text": "That's another way of expressing everything on the left side.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3618.31,
    "end": 3629.132,
    "text": "Okay, so here's the more interesting part where we can really start talking about like the interesting Bayesian mechanical inferential interpretations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3629.892,
    "end": 3632.914,
    "text": " where now we have two coupled Lorenz systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3632.954,
    "end": 3635.415,
    "text": "So again, it's a stochastic Lorenz system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3635.455,
    "end": 3640.078,
    "text": "But since we have two of them that are talking to each other, it's really a six-dimensional system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3640.659,
    "end": 3642.9,
    "text": "So three of one Lorenz attractor, three of the other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3643.46,
    "end": 3650.605,
    "text": "And crucially, we're coupling them together through the first and fourth state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3650.665,
    "end": 3657.829,
    "text": "So the fourth state of the system is really like the first state of the second Lorenz attractor, if that makes sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3659.181,
    "end": 3667.473,
    "text": " So basically, we're saying that X1, the first state of one Lorenz attractor, is talking to X4, which is the first state of a second Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3668.094,
    "end": 3674.462,
    "text": "And so just by coupling them with this one state, we're mediating the flow of information between these two Lorenz attractors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3677.682,
    "end": 3685.268,
    "text": " The next move is to go back to the Laplace approximation, where we're enforcing the surprisal function to be quadratic, a quadratic potential.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3685.769,
    "end": 3690.392,
    "text": "And now we're applying it to the six-dimensional stochastic Lorenz system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3690.412,
    "end": 3695.357,
    "text": "So now we are fitting this whole coupled system with a 6D multivariate Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3696.237,
    "end": 3703.563,
    "text": "And because of that Gaussian constraint, we'll be able to read off the synchronization map from the entries of the Hessian, the inverse covariance matrix.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3703.964,
    "end": 3706.766,
    "text": "So on the right, what we have are just example trajectories.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3707.469,
    "end": 3709.75,
    "text": " from the coupled Lorenz systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3710.671,
    "end": 3719.877,
    "text": "So this is what Carl would call evidence of a synchronization manifold in a less rigorous sense, although we'll get rigorous with it later.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3719.897,
    "end": 3724.519,
    "text": "But in a more anecdotal sense, you can see that the systems are basically synchronized with each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3724.659,
    "end": 3731.043,
    "text": "Even when they're only connecting through one state, they're kind of surfing along similar submanifolds of their state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3731.303,
    "end": 3732.424,
    "text": "They're kind of hugging each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3735.296,
    "end": 3751.585,
    "text": " Again, these other metrics, like the very bottom, is just to show that the approximated flow using this Laplace approximation plus polynomial expansions is qualitatively at least positively correlated with the true flows of the full nonlinear system that has no Gaussian approximation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3755.547,
    "end": 3760.829,
    "text": "And then the Bayesian mechanics interpretation comes in next, where now we, again, read off the Markov blanket",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3763.142,
    "end": 3766.723,
    "text": " that exists between two of the Lorenz attractors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3767.143,
    "end": 3770.044,
    "text": "So here, the upper right is what I'm focusing on now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3770.144,
    "end": 3782.807,
    "text": "Here, the second two states, state two and state three, are the internal states of one Lorenz attractor, whereas states five and six, that dark blue box, are the internal states of the second Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3784.008,
    "end": 3787.889,
    "text": "And they both share a pair of blanket states, which are X1 and X4.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3789.389,
    "end": 3791.95,
    "text": "So the synchronization map is a map that maps from",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3793.067,
    "end": 3820.478,
    "text": " x2 and x3 to x5 and x6 where the mapping is done on the conditional densities where the conditioning is done on the sensory state of either the first system or the second system which is respectively the active state of the other so that's a little bit confusing is basically the purple um sensory state of the first system is actually the active state of the second system and vice versa but the the most bottom plot is basically showing that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3820.888,
    "end": 3832.234,
    "text": " At any given time, the internal states of one system are making a best guess or a conditional estimate about the external state, which are the internal states of the second Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3832.254,
    "end": 3838.318,
    "text": "So you basically have two minds, little two-dimensional minds, that are making inferences about each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3838.878,
    "end": 3844.061,
    "text": "But the inferences are done with respect to these conditional densities, where the conditioning is done on the sensory states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3844.901,
    "end": 3848.123,
    "text": "That's basically what all these plots on the lower right are showing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3849.898,
    "end": 3853.92,
    "text": " So that's the demonstration of this inferential or sentience interpretation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3855.741,
    "end": 3863.505,
    "text": "And then because, as we said, the full system is multivariate Gaussian, it means all the conditional densities are also Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3863.985,
    "end": 3877.772,
    "text": "And then you can literally write down the synchronization map as a linear function of the states, where that linear function is given by appropriate sub-matrices of the full Hessian, which is the inverse covariance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3878.345,
    "end": 3895.012,
    "text": " And again, because we're fitting the potential function using a polynomial expansion, it means that the Hessian function, which is the double derivative of the probability density with respect to the states, that will also be a function of those polynomial coefficients.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3895.492,
    "end": 3908.177,
    "text": "So the end result of all that is basically that given this polynomially approximated Helmholtz decomposition, I can then write down the synchronization map precisely in terms of those coefficients that I fit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3909.69,
    "end": 3929.463,
    "text": " And then you can compute all these nice things like a free energy function, which is basically a sum of accuracy or prediction errors and complexity, which is effectively a prior penalty that penalizes how far the inferred expected state is from its expectation under the full steady state density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3930.244,
    "end": 3939.53,
    "text": "So this is a nice, this is a crucial result because it's saying even if you don't know what the true steady state of a system is, you can use polynomial expansions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3940.014,
    "end": 3957.69,
    "text": " and a Laplace approximation to get something that still has rich nonlinear dynamics, but by enforcing this Gaussian constraint on it, you can then write down the synchronization map using linear functions and the form of that function you know because you fit its coefficients by doing this Helmholtz decomposition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3960.312,
    "end": 3963.455,
    "text": "Okay, so that's basically the main brunt of the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3964.658,
    "end": 3968.081,
    "text": " of the paper, but there's another- Can we go back and ask a few questions?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3968.301,
    "end": 3970.342,
    "text": "So I have one question and then there's two in the chat.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3970.402,
    "end": 3983.072,
    "text": "So the first question is, in this free energy equation on the bottom, pi is representing the particular states, but also we've seen free energy minimization on pi representing policy,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3983.772,
    "end": 3989.354,
    "text": " where free energy minimization is involved in the selection of affordances for policy selection.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3989.774,
    "end": 4000.678,
    "text": "So what's the relationship between free energy minimization of particular states, like the autonomous states, and how is that related to free energy minimization as planning as inference?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4001.799,
    "end": 4003.279,
    "text": "Yeah, that's a great question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4003.359,
    "end": 4007.161,
    "text": "So yeah, this has nothing to do with the policy one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4007.261,
    "end": 4013.283,
    "text": "It's just about pi, which is basically just saying the expected value of the particular states, which I should have mentioned are",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4015.365,
    "end": 4019.814,
    "text": " the autonomous and the blanket states, or the autonomous and the sensory states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4019.834,
    "end": 4025.526,
    "text": "So in this case, the particular states would be the two internal, the sensory and the active state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4026.317,
    "end": 4032.298,
    "text": " of one of those Lorenz attractors where it's active state is actually the sensory state of the other Lorenz attractor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4032.399,
    "end": 4035.699,
    "text": "So they share, you know, they have one overlap in terms of their blanket.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4036.84,
    "end": 4051.563,
    "text": "So this here is just saying that on average, the system is instantaneously minimizing at all times, this free energy functional, which is just saying is penalizing particular states from how far they are from their conditional expected value.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4051.603,
    "end": 4054.504,
    "text": "That's the expectation of PI given ETA.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4055.083,
    "end": 4063.166,
    "text": " where the expectation is taken under Q, which is this variational density that's parametrized by the synchronization mapped internal states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4065.407,
    "end": 4075.792,
    "text": "The expected free energy, which is what you're talking about, is how do you write down a functional of trajectories or paths",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4076.548,
    "end": 4105.898,
    "text": " of future autonomous states that is minimized such that you get the semblance of planning behavior where you're penalizing not something in terms of how far instantaneous it is but over a future horizon how how much is the free energy expected to deviate from the optimal policy or path which is going to be a sequence of autonomous states um that is a more complex question that has to basically do with yeah free energy minimum basically minimization of actions which are",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4108.312,
    "end": 4111.416,
    "text": " under certain assumptions, sums of surprisals over time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4112.337,
    "end": 4121.828,
    "text": "And that's something that is not treated at all in this paper, but it is explicitly treated in an upcoming paper that I believe is in review right now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4122.25,
    "end": 4131.972,
    "text": " where we explicitly make the connection between the instantaneous free energy minimization like we see here and then this more temporally deep expected free energy minimization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4132.492,
    "end": 4141.374,
    "text": "But basically for the present purposes of this paper, there's no relationship and nothing can really be said here about planning or temporally deep free energy minimization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4142.014,
    "end": 4145.115,
    "text": "Okay, let's keep it on this slide and I'll ask a few more questions in chat.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4145.675,
    "end": 4151.476,
    "text": "So from Martin, is the actual steady state distribution of the Lorenz attractor known?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4153.346,
    "end": 4178.864,
    "text": " um no basically yeah okay um i think there have been others um there's been other papers where they're not using the stochastic lorenz system but what they're using is uh the distributions of a bunch of lorenz attractors so this is the difference between i guess stochastic differential equations and random dynamical systems where",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4179.713,
    "end": 4188.758,
    "text": " I'm randomizing a bunch of initial conditions, and then I'm looking at the resulting densities of the deterministic systems after a bunch of time has passed.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4188.838,
    "end": 4190.099,
    "text": "So that's a different kind of density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4190.119,
    "end": 4197.243,
    "text": "It's a density over the deterministic ending states or deterministic trajectories of the system from a bunch of initial conditions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4197.784,
    "end": 4201.506,
    "text": "And I think there has been work done on characterizing those densities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4201.926,
    "end": 4203.207,
    "text": "But again, I doubt that those",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4203.646,
    "end": 4205.407,
    "text": " densities are parametrically defined.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4205.487,
    "end": 4208.108,
    "text": "I bet they're more like approximations, like what we did here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4208.528,
    "end": 4216.092,
    "text": "But this is something that, honestly, I'm not an expert on the Lorenz attractor, so that's something that I should probably just defer, that I don't know much about it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4216.132,
    "end": 4224.416,
    "text": "But I'm pretty sure the stochastic Lorenz attractor doesn't have a neat solution to the Fokker-Planck equation, which is what you would need to do to get the stationary density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4226.124,
    "end": 4242.96,
    "text": " going to ask a pair of questions from martin and from miguel so it's awesome thanks both and everybody in the chat because it's cool to have this kind of real-time field feedback so the first question is from martin in what way does the markov blanket emerge here",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4244.662,
    "end": 4247.023,
    "text": " Is it not here at some point and then later it is?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4247.543,
    "end": 4251.005,
    "text": "So the first question from Arden is, what do you mean by emergence?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4251.785,
    "end": 4258.728,
    "text": "And then the second question by Miguel is, it is impressive that you can find a Markov blanket in a nonlinear system.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4259.369,
    "end": 4265.111,
    "text": "Have you quantified how precise are the approximations of the covariance and Hessian values, e.g.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4265.171,
    "end": 4266.992,
    "text": "comparing against numerical simulations?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4267.292,
    "end": 4268.133,
    "text": "So question one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4269.073,
    "end": 4270.773,
    "text": " In what way does the Markov blanket emerge?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4271.213,
    "end": 4275.454,
    "text": "Question two, how precise are the approximation of the covariance and Hessian?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4278.475,
    "end": 4279.895,
    "text": "Yeah, okay, so that's a good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4280.295,
    "end": 4281.696,
    "text": "First question about the emergence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4282.696,
    "end": 4295.198,
    "text": "The Markov blankets is defined with respect to the stationary density, the conditional independence relationships expressed in those two upper right panels, log covariance and log Hessian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4295.778,
    "end": 4298.759,
    "text": "So those things do temporally vary",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4299.474,
    "end": 4311.458,
    "text": " in the sense that if you condition on some initial state, the time-varying densities will have different log covariance and log Hessian matrices.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4311.938,
    "end": 4324.963,
    "text": "This idea of conditioning on an initial state and then looking at the time-evolving densities, that's exactly the whole point of this paper by Thomas and a few other of us called Memory and Markov Blankets, where",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4325.748,
    "end": 4335.534,
    "text": " The Markov blanket can change in the sense that if you look at the evolution of conditional densities, conditioned on some perturbation, those conditional densities do change.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4335.954,
    "end": 4345.82,
    "text": "But if we just look at the stationary density, by definition, the stationary density doesn't change its sufficient statistics over time because it isn't defined with respect to time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4345.84,
    "end": 4348.402,
    "text": "You've marginalized out sequences of variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4348.802,
    "end": 4350.563,
    "text": "So in this sense, the Markov blanket",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4351.277,
    "end": 4358.248,
    "text": " If you don't condition on a particular state and you're just looking at the unconditional stationary density, then it's just always there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4358.268,
    "end": 4359.07,
    "text": "It doesn't emerge.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4359.43,
    "end": 4362.976,
    "text": "And maybe I said emerge, and I didn't mean that in a temporal sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4363.016,
    "end": 4365.54,
    "text": "So there's no sense in which the Markov blanket emerges.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4366.201,
    "end": 4372.666,
    "text": " Markov blanket is just a property of the stationary covariance matrix, or more specifically, its inverse.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4373.247,
    "end": 4381.353,
    "text": "And that is a constant value that is just a characteristic of the full six dimensional multivariate Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4382.054,
    "end": 4394.064,
    "text": "So even if we didn't even think about stochastic processes, if I just wrote down a six dimensional multivariate Gaussian with these particular entries of the covariance matrix, that is the constant Markov blanket that just always exists.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4396.732,
    "end": 4404.621,
    "text": " The second question was about... The precision of the covariance and Hessian in comparison to numerical simulations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4405.422,
    "end": 4406.283,
    "text": "That's a really good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4406.383,
    "end": 4408.886,
    "text": "And I'm assuming, was that from one of the comments?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4409.246,
    "end": 4410.348,
    "text": "That was from Miguel, yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4410.768,
    "end": 4411.329,
    "text": "From Miguel, yes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4411.349,
    "end": 4415.674,
    "text": "So I'm assuming you mean the overlap between the...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4418.413,
    "end": 4423.958,
    "text": " the empirically measured covariance in the Laplace, sorry, in the real stochastic Lorentz system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4424.499,
    "end": 4440.034,
    "text": "And that one measured, or in that one estimated in the, in like, so the one estimated from the real system, and then the analytic Hessian from the, from the Laplace approximated stochastic Lorentz.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4440.054,
    "end": 4440.895,
    "text": "So I have,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4442.097,
    "end": 4463.139,
    "text": " have the paper up here that i can show this that's a really good question so one of the reviewers actually asked us something very similar and we have an extra figure that demonstrates exactly that okay can you still see my screen yep looks good okay perfect so here we're essentially doing exactly that um comparison where we're looking at",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4463.703,
    "end": 4472.772,
    "text": " the partial correlation, which in the case of Gaussians is actually the same thing as the Hessian, which is an interesting, or sorry, it's not the same thing as the Hessian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4472.792,
    "end": 4480.42,
    "text": "The partial correlation is the same measure of conditional independence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4482.924,
    "end": 4484.566,
    "text": " It's not the case for non-Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4484.706,
    "end": 4495.555,
    "text": "It's a case for a few classes of exponential distributions that if you measure the partial correlation, which is a kind of conditional correlation between two variables, it's the same as the corresponding value of the Hessian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4497.887,
    "end": 4506.77,
    "text": " So what we're doing here is exactly that we're basically measuring how well the empirically measured partial correlation between different subsets of states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4506.81,
    "end": 4515.232,
    "text": "So here, the third and the sixth states, which should be, these are the two internal states of two mini Lorenz attractors within the full system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4515.252,
    "end": 4518.933,
    "text": "So this is like internal state of brain, one internal state of brain too.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4519.724,
    "end": 4524.566,
    "text": " how their partial correlation actually measures up with the true Hessian value.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4524.606,
    "end": 4530.749,
    "text": "And it kind of converges to a value very close to zero, which agrees with the analytically measured Hessian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4531.189,
    "end": 4538.472,
    "text": "So you can basically quantify this more coarsely by just looking at the structure of the Hessian matches the empirically measured partial correlation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4539.144,
    "end": 4543.446,
    "text": " And the partial correlation was basically measured for increasing segments of time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4543.946,
    "end": 4547.208,
    "text": "So the partial correlation measured here was from the first 50 time steps.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4547.728,
    "end": 4554.292,
    "text": "And then up to years, the cumulative partial correlation from the first until the 500th time step and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4554.332,
    "end": 4563.136,
    "text": "So basically by looking at the partial correlation and increasingly long windows of time, it should converge to the true value of the conditional independence and",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4564.702,
    "end": 4568.844,
    "text": " it approximates that which we get from the Laplace approximated system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4569.644,
    "end": 4579.809,
    "text": "What I should say, however, is that this partial correlation was not measured from the full Lorenz attractor, the stochastic one, but actually just from our approximation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4580.689,
    "end": 4586.292,
    "text": "So we're just saying that if we make this approximation and then empirically measure the partial correlation from real",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4588.523,
    "end": 4591.285,
    "text": " from realizations of that approximated system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4592.166,
    "end": 4599.271,
    "text": "The empirical measurements line up with what it should be, given the way that we parameterize the flow using the polynomial expansion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4600.972,
    "end": 4614.561,
    "text": "However, what we haven't done, but I think would be very interesting, and maybe this is what was being asked in the question, is actually to measure this from the real stochastic Lorentz system to see if there actually is a Markov blanket in the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4615.276,
    "end": 4617.497,
    "text": " in the real Lorenz system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4617.777,
    "end": 4631.585,
    "text": "The only issue with that is we can't necessarily use partial correlation if we did it in the real stochastic Lorenz system, because as we were just discussing, like Martin asked, do we know what the stationary density of the real Lorenz system is, the stochastic one?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4631.645,
    "end": 4636.628,
    "text": "If we don't know that, we would have to use a less parametric method like mutual information",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4637.288,
    "end": 4642.77,
    "text": " conditional mutual information in order to assess the presence or absence of conditional independence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4643.33,
    "end": 4651.332,
    "text": "So it would be harder to do just because evaluating massive conditional mutual information for increasing amounts of time is computationally difficult.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4651.652,
    "end": 4654.853,
    "text": "But I agree that that is something that should be done.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4654.933,
    "end": 4658.694,
    "text": "But no, we didn't do it for the real stochastic Lorenz.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4658.714,
    "end": 4662.695,
    "text": "We just did it between our approximation and our analytic Hessian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4662.715,
    "end": 4665.336,
    "text": "I hope that answers the question and that was clear.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4666.584,
    "end": 4670.428,
    "text": " We can keep talking if there's any lack of clarity on that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4671.369,
    "end": 4676.694,
    "text": "I think continue with the presentation, and if anyone has more questions at the end of the presentation, we'll return to them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4676.834,
    "end": 4677.855,
    "text": "Okay, cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4678.656,
    "end": 4682.9,
    "text": "Yeah, but that was a very good point, actually measuring the Markov blanket empirically.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4684.045,
    "end": 4687.928,
    "text": " Okay, so I'm almost done with the overview of the paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4688.428,
    "end": 4699.897,
    "text": "So the last big thing that's kind of tucked into one of the results section is something called the sparse coupling conjecture, which is important because it dissociates conditional independence from causality.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4700.457,
    "end": 4706.882,
    "text": "So conditional independence, as we said, in the context of a Gaussian... Actually, no, it's not even for Gaussian systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4706.922,
    "end": 4707.663,
    "text": "For all systems,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4708.82,
    "end": 4713.846,
    "text": " conditional independence is determined by a zero entry in the Hessian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4713.966,
    "end": 4726.019,
    "text": "So if the Hessian between two states is zero, which means the conditional densities don't change as you change, like the conditional density over one variable doesn't change as a function of the other, then",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4727.563,
    "end": 4731.308,
    "text": " It implies this relationship, which is shown in terms of the Jacobian on the left.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4731.728,
    "end": 4734.692,
    "text": "The Jacobian basically measures causal coupling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4735.473,
    "end": 4743.222,
    "text": "We can go into what it means mathematically, but you can just think of the Jacobian as the derivative of the flow function with respect to the states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4743.883,
    "end": 4749.965,
    "text": " And entries in the Jacobian that are non-zero mean that one state is influencing another.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4750.405,
    "end": 4765.669,
    "text": "So what this relationship shows is that if at most one state influences the other, but not otherwise, like there's a non-reciprocal coupling between one state and the other, then the two states are conditionally independent, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4765.709,
    "end": 4766.79,
    "text": "there's a Markov blanket.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4767.25,
    "end": 4769.971,
    "text": "That's a really weird counterintuitive statement.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4769.991,
    "end": 4770.711,
    "text": "It means that if",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4771.408,
    "end": 4777.791,
    "text": " I'm affecting you, but you're not affecting me, then we will be conditionally independent in the steady state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4778.532,
    "end": 4780.733,
    "text": "And this is what's called the sparse coupling conjecture.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4781.273,
    "end": 4783.394,
    "text": "And it's the way we originally wrote it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4783.574,
    "end": 4786.255,
    "text": "It was actually much stronger of a claim.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4786.315,
    "end": 4788.817,
    "text": "And then we realized there's actually some conditions on it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4788.897,
    "end": 4797.281,
    "text": "So it's called a conjecture because it doesn't always hold, but there's certain conditions under which it should hold given a high dimensional enough and nonlinear enough system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4799.386,
    "end": 4826.578,
    "text": " So we can kind of prove that more mathematically by saying if you expand the Jacobian by basically taking the derivative of the Helmholtz decomposition, so that's what's shown in the middle thing, and then you assume that certain entries of the Jacobian are zero, then you can basically show that it implies, or sorry, not certain entries of the Jacobian, but certain products of entries in the Jacobian are zero, that it implies that there's an accompanying",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4827.072,
    "end": 4829.794,
    "text": " zero entry in the Hessian, which is shown on the bottom.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4830.512,
    "end": 4854.66,
    "text": " So what's shown on the bottom more specifically is what's known as the normal form of the flow or kind of a circular coupling, where if you have this particular structure of the flow operator where certain block or certain sub matrices of the flow operator are zero, then you're guaranteed to have this kind of sparse coupling, which means that one state can influence the other and not vice versa.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4855.04,
    "end": 4858.061,
    "text": "And then you'll get conditional independence in the steady state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4858.561,
    "end": 4859.242,
    "text": "So that's another big,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4861.027,
    "end": 4875.654,
    "text": " result that, um, I think during the earlier live stream, when we talked with Lance, we had an example in his paper where that was also the case where basically two systems were, uh, two, one state was affecting the other, but they're actually conditionally independent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4876.475,
    "end": 4881.477,
    "text": "Um, that's, that's a, uh, basically we rigorously showed the conditions under which that exists here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4881.897,
    "end": 4887.52,
    "text": "So just because you see a zero in the Hessian, that means don't assume that the systems can't affect each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4887.9,
    "end": 4889.661,
    "text": "Two systems are conditionally independent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4889.997,
    "end": 4894.9,
    "text": " doesn't necessarily mean that they don't affect each other through having a non-zero entry in the Jacobian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4895.42,
    "end": 4898.822,
    "text": "But it just means that those connections have to be asymmetric or non-reciprocal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4899.923,
    "end": 4903.124,
    "text": "So this is an example physically of what that would look like.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4903.184,
    "end": 4912.83,
    "text": "Say we had some system with external sensory and active states under the normal form of the flow, which is this special block diagonal flow operator.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4913.29,
    "end": 4917.412,
    "text": "What this essentially means is that you can get this sparse coupling conjecture",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4919.673,
    "end": 4933.017,
    "text": " His sparse coupling conjecture holds as long as sensory states do not affect internal states and active states do not affect external states directly, but only through the sensory states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4933.258,
    "end": 4937.439,
    "text": "So that's a particular instantiation of the normal form of the flow where you have these",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4938.365,
    "end": 4943.896,
    "text": " Basically, particular entries of the Jacobian and the solenoidal flow operator are zero.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4944.277,
    "end": 4949.908,
    "text": "And then you'll get this kind of sparse coupling or circular coupling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4951.383,
    "end": 4960.108,
    "text": " And one question on that, and then Dean, if you'd like to ask any questions, and this is also related to the work of Miguel, the paper that was brought up earlier.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4960.128,
    "end": 4965.651,
    "text": "What is the topology of perception, cognition, and action?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4966.012,
    "end": 4976.158,
    "text": "Is it just sense influencing internal and internal influencing active and active influencing external, which then feed back to sensory, sort of the simple clock model?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4976.958,
    "end": 4981.661,
    "text": " Or here we have a edge between sense and active states, the blanket states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4982.221,
    "end": 4989.625,
    "text": "And so in free energy principle and active inference, are we committed to a certain topology of action?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4989.645,
    "end": 4991.806,
    "text": "That's a really good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4991.866,
    "end": 4992.106,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4992.126,
    "end": 5000.791,
    "text": "So given the sparse coupling conjecture and the normal form of the flow, all this is... So basically, no, we're not committed to anything.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5000.811,
    "end": 5003.432,
    "text": "There's so many different combinations that will give rise to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5004.164,
    "end": 5008.305,
    "text": " a conditional synchronization manifold that can be interpreted as Bayesian mechanics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5009.326,
    "end": 5016.528,
    "text": "All we're saying is that this coupling structure we have here is mainly defined by the lack of certain connections.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5017.288,
    "end": 5029.672,
    "text": "So all that we're saying is if external states do not affect active states, and internal states do not directly affect sensory states, so it's the lack of those two connections.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5030.172,
    "end": 5032.753,
    "text": "Can you see my mouse, actually?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5032.793,
    "end": 5032.973,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5033.803,
    "end": 5054.907,
    "text": " Yeah, so if you don't have a connection going from blue to red, and you don't have one going from blue to sensory, but anything else is allowed, like all I'm saying is putting constraints on what cannot exist, then the conditions laid out here will hold.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5055.703,
    "end": 5064.867,
    "text": " and the sparse coupling conjecture will exist, which means that an asymmetric relationship implies conditional independence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5065.267,
    "end": 5078.013,
    "text": "So given that, what it means is that actually sensory states will be conditionally independent of internal states, conditioned on active states, because there's only a unidirectional coupling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5079.093,
    "end": 5080.534,
    "text": "So those are some weird",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5082.236,
    "end": 5103.377,
    "text": " weird like results from this is that sensory and internal will be conditionally independent because one affects the other but the other does not affect the first one and likewise for active and uh and external states but all that being aside external and and internal will still be conditionally independent and therefore there'll still be a synchronization map between them so",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5104.466,
    "end": 5117.516,
    "text": " The conditions about the topology that I laid out, the absence of these connections, just have to do with satisfying the normal form of the flow as laid out here and satisfying this weird sparse coupling conjecture about unidirectional nonreciprocal coupling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5117.856,
    "end": 5129.004,
    "text": "But other than that, the active inference story and the Bayesian mechanic story, you can swap around all of these influences, and it'll still pertain as long as this guy is conditionally independent of this one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5130.085,
    "end": 5131.606,
    "text": "So if these two are conditionally independent",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5133.43,
    "end": 5136.733,
    "text": " dependent on the blanket states, then that synchronization map still exists.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5137.013,
    "end": 5140.536,
    "text": "But you can have all kinds of weird coupling loops between the two.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5141.396,
    "end": 5141.697,
    "text": "Thanks.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5141.977,
    "end": 5143.418,
    "text": "Very interesting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5144.419,
    "end": 5146.721,
    "text": "Dean, do you want to ask anything here?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5148.802,
    "end": 5158.47,
    "text": "Well, there's so many things I could ask, and I'd get a great answer back, and I'd only be able to use a small percentage of the great answer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5158.991,
    "end": 5162.894,
    "text": "One thing I do want to... I will ask this, and I'm not sure if it's...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5164.153,
    "end": 5180.017,
    "text": " if it's good or not, but I've been doing a lot of work and trying to see whether or not the active sensory state is a form of kind of decomposition of where a concept might rest between things like content and context.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5180.897,
    "end": 5186.038,
    "text": "And I'm just wondering when you do this math and you get a sense of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5186.778,
    "end": 5193.967,
    "text": " Knowing what you're working with and the relationship between all of the different components of what you're working with",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5195.189,
    "end": 5218.97,
    "text": " Do you see in this a lot, instead of things always assuming that things just sort of pop out as an emergence, do you start getting a sense that sometimes things just exist within and between and those constraints and some of the things you talked in terms of linearity and just getting an understanding of what you're working with.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5219.61,
    "end": 5222.012,
    "text": "Do you see things like concepts",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5223.317,
    "end": 5230.661,
    "text": " sort of resting in between these other areas, we call them in this diagram, external and internal states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5230.741,
    "end": 5240.847,
    "text": "But could a concept be that sort of fluid and appearing and disappearing based on some of the things that you've looked at here in this paper?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5241.667,
    "end": 5242.988,
    "text": "Yeah, I definitely think so.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5243.048,
    "end": 5249.311,
    "text": "I mean, for me, when I start thinking about concepts, I immediately start thinking about relationships, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5249.331,
    "end": 5252.053,
    "text": "Like what we've shown here are very low dimensional systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5252.622,
    "end": 5258.643,
    "text": " where you have a few internal, a few active, a few sensory, a few external.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5259.283,
    "end": 5271.146,
    "text": "Once you start having larger, higher dimensional systems, so say we weren't modeling two dumb Lorenz attractors just oscillating around each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5271.486,
    "end": 5279.368,
    "text": "But say we had some 100 dimensional neural network synchronized with another 100 dimensional external state space, like another neural network.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5280.37,
    "end": 5291.501,
    "text": " Once you're getting into that world, I think you get the, you can have the computational representational capacity for things like concepts, because you just have a higher correlational structure.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5291.861,
    "end": 5294.764,
    "text": "So imagine that we had a hundred internal states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5295.444,
    "end": 5301.11,
    "text": "The internal states we know in some way are relating to the external states, but they're also relating to each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5301.27,
    "end": 5303.492,
    "text": "So there might be like subgroups, like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5304.039,
    "end": 5313.024,
    "text": " groups of 10 or 20 neurons that are all correlated in their firing to each other, there might be rich recurrent internal dynamics just within the internal states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5313.665,
    "end": 5321.59,
    "text": "And the particular relationship of sub-manifolds, so to speak, of the internal state space, how those conditionally relate to external states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5322.11,
    "end": 5331.996,
    "text": "That's where I think you start getting into the world of concepts where you can have under a certain part of the external state space, this part of the internal manifold gets more or less activated",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5332.383,
    "end": 5338.006,
    "text": " versus when that part of the external state space lights up, this different part of the internal state space gets lit up.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5338.326,
    "end": 5354.193,
    "text": "So I think the key to identifying something like concepts really relies on having higher dimensional systems, which then allow you to have kind of more rich lower dimensional correlation structure that kind of exists in the ambient dimension of like a hundred dimensional state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5354.934,
    "end": 5360.897,
    "text": "That would be, that's how I at least interpret your description of like the concepts kind of existing in between.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5360.977,
    "end": 5361.837,
    "text": "It really rests in",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5362.631,
    "end": 5365.432,
    "text": " lower dimensional manifolds of ambient high dimensionality.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5372.374,
    "end": 5376.216,
    "text": "So yeah, we're pretty much done with the thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5376.256,
    "end": 5378.276,
    "text": "This is just a summary of the contributions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5380.657,
    "end": 5392.161,
    "text": "I would say, yeah, Helmholtz decomposition, fitting it to a stochastic dynamical system where we don't know the steady state distribution, and then going through the steps with this Laplace approximated one",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5392.848,
    "end": 5401.072,
    "text": " studying its synchronization manifold, doing this cool coupled Lorenz thing, and then finally this sparse coupling conjecture about circular coupling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5403.033,
    "end": 5409.217,
    "text": "And yeah, you can move to the discussion now or go back, dive into the slides or dive into different parts of the paper that I didn't even talk about here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5409.997,
    "end": 5411.858,
    "text": "Whatever you guys think best.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5412.858,
    "end": 5413.098,
    "text": "Great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5413.819,
    "end": 5417.521,
    "text": "Maybe you could unshare and we'll have some time just to",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5418.959,
    "end": 5448.714,
    "text": " talk and see um if anyone else has any questions in the chat so just one starting question from martin was the question above was whether it is even known that the lorenz attractor has a stationary state did we address that earlier oh sorry yeah we we addressed it like my initial answer that was i'm pretty sure it's not known but i'm not positive like i'm not an expert on the stochastic lorenz tractor but i'm pretty sure it's not known",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5449.514,
    "end": 5461.024,
    "text": " Because I just imagine intuitively that writing down a unique solution to the Fokker Planck operator would be very difficult for that, which I imagine is difficult for any system that exhibits stochastic chaos.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5462.326,
    "end": 5462.606,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5462.846,
    "end": 5470.713,
    "text": "And another question from Martin is whether the sparse coupling conjecture requires a Gaussian as the stationary distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5472.755,
    "end": 5473.436,
    "text": "No, it doesn't.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5476.807,
    "end": 5480.469,
    "text": " The requirements on it are not about the Gaussianity of the density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5481.29,
    "end": 5495.458,
    "text": "They're more actually, so for this new paper that's in review now, there's an appendix in that paper that more, that fleshes out the conditions on the sparse coupling conjecture more rigorously.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5496.438,
    "end": 5501.941,
    "text": "And the conditions are not about the form of the stationary density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5502.802,
    "end": 5504.783,
    "text": "They're actually more about basically",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5505.623,
    "end": 5508.984,
    "text": " how local the coupling is in the solenoidal term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5509.004,
    "end": 5530.833,
    "text": "So if the solenoidal terms are state-dependent, like solenoidal term ij depends on the states, then the sparse coupling conjecture only holds if that dependence is only a function of states i and j. But if the coupling between states i and j is actually a state-dependent function of state k, then the sparse coupling conjecture can be more easily violated.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5531.556,
    "end": 5540.37,
    "text": " Um, and I have a proof of that, that I can again, share my screen and show you, but it's not really related to your question, which was more about is the gaussianity related to the sparse coupling.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5540.931,
    "end": 5542.193,
    "text": "So yeah, it's not.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5542.762,
    "end": 5551.606,
    "text": " what you described there reminds me a lot of neuromodulation and about how like the effective connectivity of two regions could be modulated.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5552.066,
    "end": 5557.989,
    "text": "So here's a question from Miguel and then we can go to Dean and then I have a question as well.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5558.169,
    "end": 5564.032,
    "text": "So from Miguel, in nonlinear systems, high order correlations might arise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5564.793,
    "end": 5569.075,
    "text": "I think this means the Hessian may not be enough to assess conditional independence.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5569.335,
    "end": 5571.856,
    "text": "Do you see a way to explore that with this method?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5574.295,
    "end": 5581.422,
    "text": " Um, my Yeah, that's a great question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5581.462,
    "end": 5593.955,
    "text": "So my intuition is that if you use this method that we talked about in the second part, where it's like higher order density learning, if you",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5595.034,
    "end": 5606.828,
    "text": " are basically the high order density learning where you're not assuming a Laplace approximation, but you're enforcing a state space local quadratic approximation to the potential function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5607.629,
    "end": 5610.412,
    "text": "That means that you have a Hessian that's changing over state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5611.539,
    "end": 5639.641,
    "text": " what your point is basically saying is that because of higher order correlations there will probably be parts of state space even if for like three quarters of space space the hessian is zero there's a final quarter where the hessian will not be zero in that part of state space and i think what that allows you to do is basically say that insofar as the hessian does locally approximate the markov blanket between two states it means that there's regions of state space where the markov blanket",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5640.151,
    "end": 5644.193,
    "text": " condition applies, and then other regions where it doesn't, and the Markov blanket disappears.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5645.174,
    "end": 5663.425,
    "text": "The claims in this paper about the conditional independence relationships, especially as they relate to sparse coupling, only apply to Hessians that even if they're state-dependent Hessians, which implies a non-Gaussian stationary distribution, that the entry of the Hessian is zero for every part in state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5664.295,
    "end": 5676.867,
    "text": " So I don't know if the sparse coupling conjecture, for instance, applies where there's a state-dependent Hessian entry that's zero in some parts of state space and non-zero in other parts.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5676.887,
    "end": 5679.509,
    "text": "I think that will throw a wrench in this formulation potentially.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5680.931,
    "end": 5683.913,
    "text": "But it would be interesting to say if you can still interpret",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5684.773,
    "end": 5691.415,
    "text": " the entry of the Hessian as being a on-off signal of there's a Markov blanket between these two things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5691.675,
    "end": 5695.516,
    "text": "What does it mean if that Markov blanket is changing as a function of where you are in state space?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5695.556,
    "end": 5703.138,
    "text": "Does it mean that sometimes there's a synchronization map between these two states, depending if they're visiting this region of state space versus that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5703.598,
    "end": 5705.179,
    "text": "Or maybe that actually doesn't make sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5705.239,
    "end": 5706.059,
    "text": "So I don't know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5706.079,
    "end": 5708.24,
    "text": "I think it can certainly be used to explore those questions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5708.3,
    "end": 5710.06,
    "text": "But offhand, I don't have an easy answer.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5710.08,
    "end": 5711.341,
    "text": "I think that's a really good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5713.131,
    "end": 5713.331,
    "text": " Cool.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5713.631,
    "end": 5713.971,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5714.872,
    "end": 5717.173,
    "text": "Dean, if you'd like to throw one out.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5718.414,
    "end": 5719.555,
    "text": "Yeah, I'm just curious.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5720.095,
    "end": 5725.338,
    "text": "So we were just doing a recent paper and it was talking about sort of how did you get here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5725.478,
    "end": 5726.819,
    "text": "How did you come to this place?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5726.859,
    "end": 5734.624,
    "text": "And I would like to ask Connor, I don't know the history of how housekeeping...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5735.804,
    "end": 5738.265,
    "text": " Was introduced to this formalism.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5738.545,
    "end": 5745.908,
    "text": "So I was just wondering if maybe you could walk us through that I don't think you sort of put a put a notice up on the door and called housekeeping.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5746.428,
    "end": 5748.709,
    "text": "It just seemed to Insert itself.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5748.769,
    "end": 5760.032,
    "text": "So maybe you could tell us how that how you know when it was introduced Not assumed right how you kind of dealt with that Yeah, yeah, it's a really good question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5760.293,
    "end": 5762.373,
    "text": "So basically well, okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5762.413,
    "end": 5763.334,
    "text": "Yeah, it's interesting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5763.354,
    "end": 5763.374,
    "text": "I",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5765.202,
    "end": 5788.003,
    "text": " The whole point, so from my understanding, the evolution of this paper came into the beginnings of this paper was there was a meeting that I presented at Carl's lab meeting in 20, must have been late 2020, where I was trying to do this kind of Bayesian mechanical analysis on a very high-dimensional nonlinear system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5788.638,
    "end": 5790.679,
    "text": " And I was trying to model basically schools of fish.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5790.979,
    "end": 5799.343,
    "text": "Can you look at certain groups of a big school of fish and interpret their states as if they're doing inference about something happening on the other side of the school?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5800.124,
    "end": 5802.445,
    "text": "And so at the end, we had a bunch of conversations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5802.465,
    "end": 5806.827,
    "text": "I think we had three meetings total where we just went through the mathematics of how that's possible.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5807.267,
    "end": 5817.772,
    "text": "And it drove Carl to say, okay, what we're going to do is we're going to write down, we're going to write a paper where we deal with a system like that, where things are very state dependent and nonlinear in state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5818.376,
    "end": 5822.237,
    "text": " So that forced Carl basically to go back to the drawing board with the Helmholtz decomposition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5822.997,
    "end": 5837.581,
    "text": "And as a result of all the mathematical formulation here, which is like 95%, just Carl, like writing that down and figuring things out, he realized that there's this extra term, this housekeeping term that pops out of the Helmholtz decomposition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5838.222,
    "end": 5843.023,
    "text": "So my assumption is he named it housekeeping term because it was basically this",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5844.458,
    "end": 5850.563,
    "text": " unintended side effect of trying to do the Helmholtz decomposition on systems that are highly nonlinear.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5851.043,
    "end": 5856.767,
    "text": "And so the housekeeping term came out as like, oh, I need to have this thing to make sure everything's neat and in order.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5857.028,
    "end": 5867.116,
    "text": "Because if you actually look at the mathematical derivation of where that term comes from, I'm not sure Carl was familiar with it from the literature, or if he just derived it and kind of thought of it himself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5867.176,
    "end": 5868.577,
    "text": "I'm actually really not sure on his own",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5869.488,
    "end": 5873.75,
    "text": " intellectual journey to getting there, but it is in the stochastic processes literature.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5873.77,
    "end": 5874.991,
    "text": "It's not like a totally new thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5875.031,
    "end": 5877.692,
    "text": "It's been known for like decades now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5878.812,
    "end": 5890.137,
    "text": "But I think from his perspective, he labeled it housekeeping term because it was this new thing that pops out that he has to account for that keeps the full Fokker-Planck solution like in order.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5890.758,
    "end": 5893.879,
    "text": "So you basically need to add that on to make sure that certain terms don't become",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5894.611,
    "end": 5898.433,
    "text": " like remain zero when you're trying to solve the Fokker-Planck solution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5898.453,
    "end": 5906.518,
    "text": "It has to do something called the probability current to make sure the divergence of the probability current to zero need to add in this housekeeping term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5907.098,
    "end": 5914.402,
    "text": "So my interpretation of the word housekeeping is very analogous to the word correction, like it's a correction term or a cancellation term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5916.304,
    "end": 5920.526,
    "text": "But that was a vocabulary that Carl just started using in the paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5923.261,
    "end": 5924.583,
    "text": " Awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5924.623,
    "end": 5925.144,
    "text": "Dot four, 32.4.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5925.304,
    "end": 5926.165,
    "text": "It's never too late.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5929.67,
    "end": 5934.697,
    "text": "You briefly touched on where I was going to go with this next question, which was about collective behavior.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5935.057,
    "end": 5937.581,
    "text": "So just how are you thinking about...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5938.382,
    "end": 5958.496,
    "text": " these kinds of formalisms in collective behavioral systems, are we thinking about multi-agent simulations of active inference agents like we had done with ants, or is there another level of analysis where the collective is sort of this shifting entity in and of itself?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5958.576,
    "end": 5960.537,
    "text": "Like where does this apply to collective behavior?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5961.418,
    "end": 5962.258,
    "text": "Yeah, that's a great question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5962.318,
    "end": 5967.562,
    "text": "So I can just personally speak to that because that's like essentially the, the, the,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5968.783,
    "end": 5970.124,
    "text": " thing I'm doing during my PhD.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5970.144,
    "end": 5983.692,
    "text": "So one half of my PhD, or I'd say one third of my PhD is doing the first thing, which is very similar to what you've done with active inference, where I'm just taking a bunch of active inference agents and I'm making them school together.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5985.814,
    "end": 5987.034,
    "text": "I first tried doing that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5987.254,
    "end": 5991.437,
    "text": "And like, that's a separate story that like, maybe we can have another live stream and talk about that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5991.457,
    "end": 5994.459,
    "text": "Cause that's a cool thing, but that has nothing to do with.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5995.04,
    "end": 5997.741,
    "text": " writing down the stationary density of the entire system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5998.101,
    "end": 6005.922,
    "text": "I tried to do that at first, but it's very hard because every active inference agent is a mixture between SDEs and ODEs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6006.382,
    "end": 6009.823,
    "text": "And all the things we talked about today were just SDEs, stochastic systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6010.223,
    "end": 6022.506,
    "text": "Whereas when you're doing a single active inference agent, like doing predictive coding, its neural dynamics are actually solved as an ODE, which is a straight deterministic gradient descent on variational free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6023.066,
    "end": 6024.026,
    "text": "So in a big",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6024.505,
    "end": 6031.957,
    "text": " collective dynamical system that has some ODEs and some SDEs, writing down the stationary density becomes very difficult.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6032.598,
    "end": 6036.585,
    "text": "So because of that, we had all these meetings and this paper was one of the results of those meetings.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6036.985,
    "end": 6039.069,
    "text": "So the approach I'm taking now is",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6039.508,
    "end": 6042.309,
    "text": " I'm not modeling individuals as active inference agents.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6042.729,
    "end": 6049.671,
    "text": "I'm just modeling each of them as like, for instance, the 2d position of a particle that has some velocity and some position.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6050.191,
    "end": 6060.955,
    "text": "And then there's classic schooling models like the V check model or the Reynolds model or the cousin model, which was identified by my advisor, where you're basically just trying to model",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6061.644,
    "end": 6069.447,
    "text": " schooling fish or flocking birds as a bunch of little stochastic differential equations that are coupled to each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6069.787,
    "end": 6077.491,
    "text": "So the approach I'm taking now is applying exactly the formalism described today to higher dimensional systems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6078.385,
    "end": 6094.456,
    "text": " And you encounter special challenges when you do that because trying to fit a polynomial expansion with all of the higher order interaction terms to a 100-dimensional system is just not going to be feasible if you're including every possible interaction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6094.496,
    "end": 6098.138,
    "text": "It's just like you're going to have a combinatorially explosive state space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6098.698,
    "end": 6103.982,
    "text": "So basically, a lot of the work I'm doing now is figuring out ways to fit the Helmholtz decomposition",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6104.722,
    "end": 6114.168,
    "text": " using non-polynomial expansion methods that are more amenable to higher dimensional data, where you don't have this cursive dimensionality.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6114.728,
    "end": 6122.092,
    "text": "And so a lot of that involves what you could probably guess using amortized approximations like deep neural networks to fit the coefficients of the Helmholtz.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6122.993,
    "end": 6124.834,
    "text": "So that's something I'm literally working on right now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6124.934,
    "end": 6129.297,
    "text": "So there's not too many results on there, but that's one of the two directions I'm going.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6129.377,
    "end": 6131.618,
    "text": "There's the multi-agent active inference, and then there's the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6131.977,
    "end": 6134.559,
    "text": " high dimensional Helmholtz decomposition stuff.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6136.46,
    "end": 6136.72,
    "text": "Cool.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6137.101,
    "end": 6158.275,
    "text": "I'd imagine that nested cognitive systems with shifting boundaries and functional relationships, that's challenging for any framework, but at least we're quite rapidly approaching the ability to have the grammar and the motifs to actually talk about those questions and then also see them from multiple perspectives.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6159.326,
    "end": 6177.793,
    "text": " either as cognitive agents interacting in a way where we don't even need to have the top level be anything specific or more from a top down perspective, which as you pointed out, it's hard to then introduce the cognitive elements into those particles.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6178.774,
    "end": 6179.494,
    "text": "Yeah, exactly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6181.034,
    "end": 6181.275,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6181.335,
    "end": 6186.577,
    "text": "I mean, just if I could just follow up on that, I think the ultimate goal would be to bridge both, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6186.617,
    "end": 6187.537,
    "text": "Like it would be sick to,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6188.058,
    "end": 6198.743,
    "text": " write down a big multi-agent active inference simulation where every single agent is actually doing a gradient descent on free energy to choose their actions, whether they're MDPs or predictive coding agents.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6199.403,
    "end": 6206.027,
    "text": "But then you can also write down a stationary density for the whole thing and identify the Markov blanket of the whole system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6206.047,
    "end": 6208.828,
    "text": "Because then you would truly have the multi-scale perspective.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6208.848,
    "end": 6211.029,
    "text": "You would have the generative model of the individuals",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6211.847,
    "end": 6235.971,
    "text": " you'd have the generative model of the whole existing simultaneously and then you could really ask questions like if i change the sensory precision of this single agent how does that affect the actual markov blanket at the collective level and that's just so like technically difficult that's like originally what i planned to do um but i ended up splitting into two separate things because just the mathematics behind it was too hard for me but uh yeah that would be really cool",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6237.137,
    "end": 6251.767,
    "text": " I hope this isn't a misleading physical intuition, but it's almost like if we have a ball rolling on a surface, there could be the sort of particles within the ball, which are minimizing a potential function relative to their local position.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6252.347,
    "end": 6254.709,
    "text": "And then there's the ball rolling on the landscape.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6255.349,
    "end": 6258.431,
    "text": "And then the housekeeping term is almost like the landscape is soft.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6258.952,
    "end": 6262.234,
    "text": "And so it's state dependent what the landscape actually looks like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6267.317,
    "end": 6290.045,
    "text": " landscape is the flow operator are Dean if you have a question and then Martin I have a question from him you have a so on that you on that same theme I'll give you a sort of a metaphorical story so if I'm standing on the bunny hill and I'm trying to control gravity and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6290.717,
    "end": 6292.418,
    "text": " And I'm sliding with all my friends.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6292.458,
    "end": 6295.839,
    "text": "We're flocking down here on the bunny hill and having a good time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6296.32,
    "end": 6298.081,
    "text": "And we're looking up at the headwall.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6299.461,
    "end": 6308.886,
    "text": "And the irony is, of course, if you've been up there mathematically, you know that you can't control gravity on the headwall just because the pitch is different.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6308.926,
    "end": 6317.85,
    "text": "Now, to the person on the bunny hill and all of their behavior, they're all kind of swimming around down here on the sort of more gradual surface.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6318.57,
    "end": 6319.391,
    "text": "Conceptually,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6320.231,
    "end": 6335.578,
    "text": " they would have to let go of their concepts of what they're doing to control gravity down here those those same themes don't apply on that on that head wall how how do we get our minds wrapped around that the letting go of what we think",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6337.353,
    "end": 6340.315,
    "text": " Our first intuition, we can see the head wall.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6340.335,
    "end": 6341.475,
    "text": "We're not even blinded to it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6341.896,
    "end": 6351.16,
    "text": "Our first intuition would be, well, how will I control gravity up there when, in fact, the first thing you have to do is let go of your current concept, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6351.2,
    "end": 6360.224,
    "text": "So mathematically, how would we show that the benefit isn't in a continuation or a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6361.533,
    "end": 6371.458,
    "text": " A combinatorial thinking around that, it would actually be a reconciliation, a letting go of what you know in order to be able to take up something new.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6372.038,
    "end": 6380.683,
    "text": "That's the part where I'm not sure if your paper answers that, but at least it leaves enough space open that it isn't always a linear thing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6381.463,
    "end": 6391.425,
    "text": " I'm down here, I'm going to make my way up the side of that hill, and I'm going to be able to apply exactly the same thinking to that new situation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6392.145,
    "end": 6394.146,
    "text": "That's the part that I find quite fascinating.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6394.806,
    "end": 6407.729,
    "text": "I really appreciate the idea of collective behavior, but for me, it's how do I let go of one idea in order to be able to take up a new one that allows me to continue to FEP my way down the hill.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 6409.009,
    "end": 6409.769,
    "text": "Yeah, that's really cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6410.202,
    "end": 6427.234,
    "text": " Yeah, no, I think that's like a critical, I would say that's a critical insight from all of this is showing that like something that seems optimal at your local level or whatever the concepts are that you use at your local level, really it's a different game when you zoom out or when you look at like a different scale.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6427.654,
    "end": 6437.321,
    "text": "So I think that has a lot to do with this idea of what me and Daniel were just talking about of like, how do the individuals, their language, the terms, the state space they think in terms of,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6437.865,
    "end": 6460.372,
    "text": " be a totally fundamentally different language than the language that the collective thinks in terms of which is the same thing of like me trying to plan now versus planning in the future planning now is based on making sure like i am sitting up straight and i'm drinking enough water but playing in the future is what am i going to do in 10 days and what's optimal from those perspectives can be like a totally different state space like a different language essentially",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6461.26,
    "end": 6484.126,
    "text": " So the nesting of different timescales is important and also it's one reason the housekeeping term is important because for active systems that can undertake epistemic actions, if action is not recalculated every single perception, cognitive and action loop, then it's like you're going to be navigating based upon a mirage or a fantasy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6484.986,
    "end": 6488.387,
    "text": "Let me ask this good question from Martin in the chat.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6489.69,
    "end": 6498.812,
    "text": " The construction in this paper of how internal states do inference on external states is still the same as in a free energy principle for a particular physics or not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6499.552,
    "end": 6507.154,
    "text": "Is Miguel's criticism of the derivation of this in a free energy principle for a particular physics taken into account here or not?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6509.395,
    "end": 6510.455,
    "text": "That's a really good question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6510.495,
    "end": 6518.677,
    "text": "So my understanding, okay, so the big thing that changed between free energy principle for a particular physics and",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6519.286,
    "end": 6522.547,
    "text": " and what was done here, as well as what was done in Lance's paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6524.227,
    "end": 6534.95,
    "text": "The biggest change, and I think, I'm not sure, Miguel, if Miguel's here, he can correct me, if Miguel also made this, is we went from argmax to expectation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6534.99,
    "end": 6545.052,
    "text": "So in free energy principle for a particular physics, the synchronization manifold was defined as a map between the modes of two conditional densities, i.e.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6545.072,
    "end": 6548.813,
    "text": "the maximum probability point to another max probability point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6549.161,
    "end": 6559.244,
    "text": " The big move that allows us to get away with the difference between argomacs and expectation is that we make everything Gaussian so that they're the same.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6559.724,
    "end": 6562.825,
    "text": "And crucially, everything's differentiable because Gaussians are smooth.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6563.666,
    "end": 6566.967,
    "text": "And the argomacs function is crucially smooth because the expectation is smooth.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6567.547,
    "end": 6576.47,
    "text": "So I don't know how you deal with multimodal densities when you're mapping between the modes of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6577.062,
    "end": 6582.045,
    "text": " the two conditional densities because the argmax function may not be differentiable anymore.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6582.745,
    "end": 6595.833,
    "text": "And one of the main things that, and the differentiability of argmax slash expectation also factors into the differentiability of the synchronization manifold, which needs to be differentiable by construction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6596.434,
    "end": 6598.555,
    "text": "So I think one of the main things that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6599.654,
    "end": 6625.634,
    "text": " um miguel was talking about was talking about the differentiability of sigma but he was also talking about the the flow of the internal states and whether the flow of the internal states can also be thought of as doing flows on variational free energy in that paper they really very nicely showed that the average flow is not the same as the flow of the average and therefore it's not guaranteed that any individual realization will look as if it's doing",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6626.195,
    "end": 6626.575,
    "text": " inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6626.915,
    "end": 6635.658,
    "text": "So this paper, as well as Lance's paper on Bayesian mechanics, that does not say anything basically about the direction of the flow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6635.718,
    "end": 6637.479,
    "text": "So I don't think there's any disagreement there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6638.299,
    "end": 6641.6,
    "text": "In the new paper that's coming out, we actually do directly have a new",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6643.248,
    "end": 6663.136,
    "text": " result that talks about the flow of individual trajectories that will more specifically address the flow based argument that Miguel was making in his paper where we're actually looking at the flow of an individual trajectory of the system in terms of gradients of the surprise of functional gradients of the variational free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6664.017,
    "end": 6667.539,
    "text": " And we're doing that from different marginals of the steady state density.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6667.579,
    "end": 6678.426,
    "text": "So the flow of the active states as a gradient descent on the variational free energy of the particular states, for instance, or the blanket states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6679.266,
    "end": 6683.809,
    "text": "So there's multiple things that can be answered from your question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6683.929,
    "end": 6690.032,
    "text": "One has to do with the differentiability of sigma, which relies on the differentiability of things like arc max versus expectation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6690.673,
    "end": 6692.054,
    "text": "And other things have to do with",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6693.233,
    "end": 6700.915,
    "text": " relying on the differentiability of sigma to look at the flow of the most likely internal state given blanket states.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6701.535,
    "end": 6708.116,
    "text": "And neither of these papers actually have to do with the flow argument and whether the average flows equal the flow of the average.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6708.136,
    "end": 6714.017,
    "text": "They just have to do with on average does the most likely internal state given blanket states parameterize the density?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6714.417,
    "end": 6715.978,
    "text": "And that is the case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6716.018,
    "end": 6717.138,
    "text": "That's mathematically the case.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6717.776,
    "end": 6724.161,
    "text": " And it also gets around the argmax versus expectation issue by making everything Gaussian.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6725.482,
    "end": 6726.683,
    "text": "Are there other issues?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6726.743,
    "end": 6727.844,
    "text": "Is there something more specific?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6727.864,
    "end": 6734.089,
    "text": "Because there are several critiques in Miguel's paper, and maybe I didn't get at the one that you're talking about, Martin.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6734.109,
    "end": 6738.732,
    "text": "I think this is a perfect place for either of you to have a last word.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6738.752,
    "end": 6747.039,
    "text": "Connor, though, thanks so much for scheduling this, for breaking into the .3 tier, which was...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6748.095,
    "end": 6769.021,
    "text": " unexpected but awesome and we really look forward to your future work and joining us for future discussions so any last comments if you would like to have them um yeah no thanks so much for having me on it's been a it's been a real pleasure there's been a really stimulating conversation and it's and it's also thanks to miguel and martin who were in the",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6769.762,
    "end": 6793.024,
    "text": " whoever else was watching it is really appreciate the comments that was really nice and uh and yeah also just general apologies on the part of all the authors that they couldn't be here today and also for the earlier live streams uh on this paper but you did a really great job in discussing the paper um so yeah it's it's really cool to like that we put out the paper and then we get this attention and we get this chance to like talk about this stuff with you so",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6793.604,
    "end": 6793.785,
    "text": " Thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6793.865,
    "end": 6795.309,
    "text": "And thanks, Dean, for your great questions too.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6795.369,
    "end": 6797.374,
    "text": "It's really always fun chatting with you on here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6797.435,
    "end": 6800.443,
    "text": "I think you were here at the last one as well that I was at.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6800.463,
    "end": 6800.784,
    "text": "So yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6801.109,
    "end": 6823.309,
    "text": " yeah it's awesome and just like wow eight hours is not enough to explore and there could be so many ways to connect what's being said to core terms in the active inference ontology and it's just a great time so thank you connor thank you dean and everybody who is participating live and asynchronously",
    "speaker": "SPEAKER_02"
  }
]
SPEAKER_00:
Hello, everyone.

Welcome to the Active Inference Lab.

Today it is August 10th, 2021, and we're here in live stream number 26.1 discussing Bayesian mechanics for stationary processes.

Welcome to the Active Inference Lab, if it's your first time or not.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links here on this slide.

This is a recorded and an archived live stream, so please provide us with feedback so that we can improve on our work.

All backgrounds and perspectives are welcome here, and we'll be following good video etiquette for live streams so people can just raise their hand visually or in the jitsi if they want to speak.

This short link here has our calendar of live streams for the year.

And we're here in number 26, where today on the 10th, and then next week on the 17th, we're gonna be continuing discussion on the Bayesian Mechanics for Stationary Processes paper.

And then we're gonna launch into some other papers.

So take a look at that link if you wanna learn more.

Today in live stream number 26.1, the goal is going to be to learn and discuss, thankfully and really appreciatively with some of the authors of the paper.

We're gonna be discussing this Bayesian Mechanics for Stationary Processes paper and asking questions, writing things down we wanna look up over the coming week and just seeing what is fun to talk about in this paper.

So we'll start with just an introduction and a warm-up of the non-authors, and then we'll go to the authors for conveying anything that they'd like to just at the outset.

So I'm Daniel, and I'm a postdoc in California.

I'll pass it to Avil.


SPEAKER_04:
Hello, I work in the AI and modeling collective intelligence.

I founded and coordinate the Kairos Research Laboratory on building inactive models for navigating the Anthropocene.


SPEAKER_00:
Thank you.

Let's go to Stephen and then Dean.


SPEAKER_01:
Hello, welcome.

My name is Stephen Sillett.

I'm based in Toronto and looking forward to hearing the new developments in Active Influence this paper covers.


SPEAKER_00:
Great.

Dean?

Good morning.


SPEAKER_05:
I'm Dean.

I'm a retired guy.

I live in Calgary and I'm trying to see what new insights can be provided by all this math because it's quite impressive to me.


SPEAKER_00:
Cool.

So onto Lance and Conor, whichever one of you would like to go first and sort of give us the big picture and how we got here and how you got to this work before we jump into the paper.


SPEAKER_02:
Conor, do you want to go?


SPEAKER_06:
You should go ahead, Lance.


SPEAKER_02:
Right.

So hi, everyone, and thanks for inviting me, Daniel.

So my name is Lance, and I'm doing a PhD in math and neuroscience in London.

with Carl Friston and Greg Pavliotis.

And basically, I came to writing this paper and doing this kind of research because I'm interested in the question of artificial intelligence.

And I think a lot of people who are doing machine learning are approaching the question from an engineering perspective.

But I'm more interested in the scientific perspective, the scientific question of what intelligence is and what do we really need to create an intelligent system.

And so researching around what people were doing, I found out about the free energy principle.

And to me, it was the most mathematical piece of work that I had seen in neuroscience.

And it seemed like it could encompass many things.

And you also had applications in active inference to model the brain and generate autonomous agents.

And so now in my PhD, my goal is to deeply understand the free energy principle and to lay out the mathematical foundations, because I think the free energy principle started out as an intuition and gradually people have tried to make it more formal and grounded in mathematics.

And so this is what we're trying to do in this paper.

So this paper is not the end, but I think it's a step forward.

So I'm really excited to

to share it and talk about it with you guys.


SPEAKER_00:
Awesome.

Sounds really cool about starting as an intuition and then working to ground in mathematics, or then would it be grounded in intuition and then elaborated with mathematics?

So, Conor?


SPEAKER_06:
Yeah, thanks.

I'm happy to be here.

And good morning, good afternoon to some of you, depending on the time zone.

So yeah, my name is Connor Heinz.

I'm a PhD student at the Max Planck Institute for Animal Behavior, the Department of Collective Behavior in Konstanz, Germany.

And I'm co-advised by Carl Fristen at UCL and then Ian Cousin, who's my advisor in Konstanz.

I'm in the US at the moment.

But yeah, so I am also generally interested in Bayesian mechanics, I think.

So Lance and I started our PhD at the same time.

And ever since I found out that Lance was also interested in kind of the mathematical foundations of the free energy principle, we've been in discussions about Bayesian mechanics for a long time.

So I was really grateful that he invited me to contribute to this paper and kind of, it relates to my own interests because I actually used to be more involved in neuroscience.

Now I've moved more onto the idea that

any generic complex system can be described as engaging in a kind of process of approximate Bayesian inference.

And so I'm exploring that in my own research in the context of collective behavior.

So in particular things that collective motion, like schooling in fish and flocking in birds.

So this paper, I think,

it gets at kind of the mathematical foundations of sentience and autonomy in all kinds of systems, not just nervous systems.

So that's why the framing in terms of stochastic processes, I think is really powerful because it allows us to extend the framework to all kinds of complex adaptive systems, not just the ones we might associate the predictive coding free energy formulation with like brains.


SPEAKER_00:
Thanks.

And a really nice point that like there's activity in the nervous system, but it's not moving or the subunits are not moving around.

So what are the similarities and the differences or what's the framework that's going to allow us to accommodate systems that are moving around like the schools of fish, flocks of bird, and then systems where the topology is either fixed or slowly changing.

And we have to do similar kinds of dynamical analysis.

So also welcome, Dave.

So Stephen, I see your hand raised, go for it.

And then anyone else who wants to ask a question at this stage?


SPEAKER_01:
Yeah, just I'd like to comment.

I think those, thanks for the introductions and those insights.

I think, you know, as there's been this progression, oh, we've gone from Bayesian approaches and now we had predictive processing approaches and active inference approaches.

It's nice to also think about, okay, once we start to generalize, it can be nice to go back

um to the bayesian idea more broadly and see okay now that active influence supports all of that you could then start to open the net so to speak so i'm interested in a lot of community psychology approaches and even interactive theater and immersive theater where you're trying to work with people's sense making so i like that

re-broadening to maybe a more heuristic mindset um so i thought that was interesting cool so anyone can


SPEAKER_00:
raise their hand at any point, but let me just maybe start with our, as we tiptoe to the edge of the paper, like what brought you, either of the authors to this framing?

Did you find something in the junkyard that was looked over or was there something that was already being worked on and you wanted to introduce a new thread or how did it come to be composed and worked on in this way?


SPEAKER_02:
Well, so there has been... So I think Cal Friston has been working for the past decade in trying to put mathematics on the free energy principle and trying to explain why... So as I mentioned before, I think the free energy principle started out as an intuition.

And I think he has been working for the past decade on a mathematical theory or a physical theory for why the free energy principle

should hold as an explanation for what the brain is doing.

And so what we wanted to do in this paper is to actually take these developments that were mostly coming from physics and that weren't necessarily as general as they could be or as mathematically rigorous as they could be and actually put it on a firm mathematical framework.

and try to explain it clearly.

And I mean, as clearly as possible.

So that was the goal here.


SPEAKER_00:
So what do you think that intuition was?

Or maybe there's a lot of ways that I'm sure first in and others have said it, but what is the intuition that at least you are pursuing with this work?


SPEAKER_02:
Right.

So I think the intuition, so I think it came from two different ways, from two different ideas.

So the first idea is if you come from machine learning and you look at machine learning from a very broad perspective, you look at many different algorithms and you look at what they do, you look at many different problems and you look at what they do.

And it actually turns out, and I think that's something that people started to realize in the early 2000s, that a lot of these algorithms, arguably all of these algorithms, they can be seen as minimizing free energy.

And so without getting deeply into that, I think the moral of the story is that a lot of the things that work in machine learning

are special cases of minimizing free energy, so of doing some kind of Bayesian inference.

And so many of these problems, you can think of them as like special cases of Bayesian inference.

So I think if you come from machine learning, then it would be quite natural to say, or at least you could say, oh, approximate inference or free energy minimization is the most general thing you can do in machine learning.

And therefore, since the brain engages in some kind of commission,

it seems reasonable to assume that it may be doing some kind of Bayesian inference.

And approximate Bayesian inference, so minimizing free energy, is the most general way to look at it.

So that's one way to get to a free energy principle.

But maybe it's not very convincing.

So I think the other way you could get to a free energy principle

which is more grounded in physics.

And I think that was the content of the original papers, is that if you look at a system that persists over time, so a system that does not dissipate, then you can show that it has to be optimizing something.

And then it has to be optimizing some what people call model evidence.

So this system must have implicitly or explicitly a model of its environment.

And it has to be optimizing.

In order not to dissipate, it has to be optimizing that model.

And then you can show that actually minimizing free energy enables you to do that.

So I think that's the other way.

Anyway, so what we did in this paper is we took this intuition of what it means to persist.

So there's actually a clear way of thinking about those things in mathematics and stochastic processes.

So what we did in this paper is we look at systems that persist over time.

And we also look at systems as the first assumption.

And then the second assumption is that the system has to be separated by a boundary with its environment.

And I think intuitively it makes sense, because we're looking at systems that are bounded with their environment.

So they interact via a Markov blanket, but they cannot mix, per se, with the system.

It's not like milk and coffee, if you wish.

It's like a person in an environment.

well-defined in space.

So these are the two things that we assume in this paper.

And then we show that you can actually describe the system as minimizing free energy in some way.


SPEAKER_00:
Thanks for that.

It's interesting about the machine learning branch towards the FEP and then the physics branch, because of course there's also sort of the third leg of that triangle with the physics and machine learning.

So we go from that just a interdisciplinary connection or an interface

to add in the third, which is this active inference or the free energy type framings.

And all of a sudden, some of the ideas that were a little bit hard to map from machine learning and physics kind of come around the horn.

And maybe there's a way that we can see active inference playing a role there.

So again, anyone can just raise their hand if they have a question or write a question in the live chat.

Here's the roadmap.

these were the sections of the paper which introduced that boundedness notion of the markov blanket discussed bayesian mechanics active inference and stochastic control and then had a nice discussion and conclusion with a few supplemental sections so

What went into how you laid out the arguments or what sections do you think were condensed or which ones would you want to expand on more?

So first just to the authors on that and then Avil, your question.


SPEAKER_04:
Okay.

Hello, Lance and wait, Connor.

I have a question.

Before we get into detail, I'm curious whether you were aware of the kind of criticism that an activist launched at the strategy principle in certification in cognitive science.


SPEAKER_02:
Could you mention what specific criticism we're talking about?


SPEAKER_04:
Okay, so I'm referring to the paper Laying Down a Fucking Path by DiPaolo, Thompson and Beer.

There is a lot of criticism that is just completely out and related to anything that Actim says, but the core thing that seems relevant to me


SPEAKER_00:
is the role of creation and historicity and well self-creation in cognition intelligence and life so yeah the paper just i'm going to put it in the chat is laying down a forking path incompatibilities between inaction and the free energy principle so first to the authors for any thoughts on that and then steven with a hand raised


SPEAKER_02:
Right.

Um, so, so I haven't read this particular paper.

Um, so I can, I can give you an answer that's specific to this particular paper.

Um, I would say, I would say from a general point of view, so yeah, there's a lot of criticism on the free energy principle.

And I think it's natural because the principle aims to be, it's to be very general and it's also an extremely ambitious.

In regards to the criticism so far, it's either been on the mathematics.

And so this is something that we've fixed with the current paper.

So I think a lot of the mathematical criticism in the literature is rendered void by this paper.

And there are, there are other, um, um, I would say philosophical criticisms, uh, on the principle.

So it's not something I'm expert about cause I'm, I'm a mathematician.

Um, so I wouldn't be able to, to really debate about the philosophy here.


SPEAKER_06:
Um, so I'm not aware of.

Daniel, you said you put the link to the paper in the chat?


SPEAKER_00:
I'll put it in the Jitsi chat in just a second.


SPEAKER_06:
Okay.

Because I noticed you mentioned the words like creation and historicity.

I'm not aware of the particular argument, but an argument that I've heard against the free energy principle that reminds me of what you just said, and this is one of the more philosophical arguments, not the mathematical things that Lance just mentioned, is the idea that

The free energy principle, the inference interpretation of a system with a Markov blanket at steady state, it rests on the idea of a steady state.

So basically there's an invariant measure of the system over time.

And when people have tried to say, relate or claim that something like societies or large scale sociocultural or sociotechnical systems are engaging in

free energy, you kind of have to enforce this assumption of a stationary steady state distribution on the system.

And so when you say things like historicity, I think of like revolutions or something, right?

So that kind of things where the kind of intrinsic nature of the system seems to shatter or fundamentally change, and that would seem to question

or violate the steady state assumption, which is kind of a baseline thing you need in order for the free energy principle maths to apply.

I can respond to that argument, but I don't actually know if that's the argument laid out in this paper.

That's just kind of what my brain jumped to when you mentioned things like historicity and creation and destruction.

Is that the core of the argument though?


SPEAKER_04:
So a thing like a revolution is not necessarily a sign of historicity.

It can be predetermined in the formulation of the demical system.

But what I'm speaking about in demical terms is the presence of a bifurcation that will be irreversible.

For example, we drive on the right of the road because some guy said so like 200 years ago, and this does not leave the law ever.

This is an example of forking or path dependency or historicity.

So that is the kind of arguments that DiPaolo and Thomson and Beer are leveraging.

but it is formally not proportionate because hegocity does not entail the absence of historicity.

It's Bernoulli systems that do not show historicity.

Hegodic systems only do not show irreversible transitions, which is a much quicker condition.

But where there is a convergence to a better argument is that

To have a test space description of a demical system, you have to assume, wait, English is broken, assume that this test space is fixed throughout the evolution of a system.

And if we agree with an activist and a system biologist like Kaufman and Monteville,

Yeah, Montevideo does, and Longo.

That core property of the integrative system is to, well, make sense to find observables and to create solutions to problems that did not exist beforehand.

Then the state space is not fixed.

It's created along the system's evolution.

It is not something you can account with a state space description of a numerical system that has state states, destructive states,

distribution, sorry.


SPEAKER_06:
Yeah, I think I would agree that as it currently stands, the free energy principle couldn't deal, the math couldn't deal with varying state spaces where literally the actual

interpretations or the finite dimensionality of the system states is changing over time.

Usually, you assume some fixed integer dimension for all the stochastic processing stuff, although maybe Lance could comment on that.

I'm sure that could potentially be extended to infinite dimensional, like partial differential equation-style formulations.

But if I understood you correctly, the first part of what you said about path dependence and irreversibility, that is all totally

accommodated by the maths laid out in Lance's paper.

So there's a very strong part of the flow that is irreversible.

Another way of describing that is you could say there's a strong degree of path dependence in the trajectories of the system.

So path dependence per se is totally accommodated

and easily path-dependent systems can fit into the free energy framework.

But the state space changing itself over time, that's something that I haven't really even thought about.

It's an interesting argument.

But I would agree that I don't think the free energy principle currently is equipped to deal with the state space itself being altered over time.

That's usually assumed to be fixed.

But I don't know.

Maybe Lance has a different opinion on that.


SPEAKER_02:
Right.

Yeah, I completely agree with Connor.

So in this paper, we don't discuss changing state spaces.

And I think in terms of future work, that would really be the next step.

So I think the current formulation gives a mathematical framework to think about adapting systems.

But it doesn't really talk about, like,


SPEAKER_01:
environments that structurally change over time for example thanks um stephen and then we can continue on on this i think this also speaks to that the legs of that stool that was being talked about earlier if you're coming from a math and machine learning you've got um

you know, you've got more lower dimensional approaches, but they show the principles.

And then when you get into the inactive approaches, they're looking more from a biological perspective.

So it might also be a little bit around the very high complexity.

But my understanding is that

the dependency on ergodicity and also the ability to do approximation science interpretations of the machine learning does start to make this math overcome some of those limitations because it has more, that's my understanding is that the reliance on ergodicity is not as rigid now.


SPEAKER_00:
So let's go to the Markov blanket and then return to some of these important distinctions like ergodic systems, steady state, stationary.

So I'm gonna put the Markov blanket up through time, figure three.

Figure one has just sort of the timeless, the classic blanket.

And then figure three is adding a subscript T. So having that,

uh those variables describe time dependent functions since that's kind of what we're talking about with the dynamical systems so maybe to the authors like how do you summarize what this graphical figure means what is it being what is being blanketed what does it mean to blanket and then how does that relate to steady state or stationary distributions which are in the title of the paper


SPEAKER_02:
Right.

So the Markov blanket is really the mathematical way in which you talk about boundaries.

And so having a Markov blanket simply means that the internal states here, they're separated from the external states by the blanket states.

And then the fact that the system is at steady state means that the blanket persists over time.

And so this is really the two things that we need when we're talking about a biological system or a stone.

So we need something internal to the stone or the biological system.

something external and then some kind of interface and of course when you're talking about something very simple like a stone it's not very interesting because you don't have any exchanges between what's internal and external via the blanket but when you're talking about biological systems like like cells or or bacillus you do have exchanges of matter of energy and information and that's something that you can accommodate with the mark of blankets


SPEAKER_00:
That's, it's almost a tension in the definition of boundedness, which is you said, well, there is a boundary, but then also it's exchanging matter energy information.

So what is the boundaries meaning?

If those are allowed to pass through it, Connor, and then anyone else?


SPEAKER_06:
I'm unmuted, right?


SPEAKER_00:
Oh, no, you're good.


SPEAKER_06:
I'm good.

Okay.

Yeah.

Yeah.

So I would, uh, I would say that's a great question because basically the boundary we're talking about isn't when, when you shake a boundary for like a physical system, people often make assumptions about causality.

So that something on one side of the blanket can't affect something on the other side of the blanket, but because of this exchange of matter and energy across the blanket.

The actual definition of the Markov Blanket is a statistical one.

It just means that this boundary is statistically, in a sense, segregates the internal and the outside.

And that statistical separation can actually critically differ from just direct causal interactions.

So the actual definition of the Markov Blanket in a statistical sense

is that the internal states when conditioned on the blanket states are statistically independent of the external states and vice versa the external states when conditioned on blanket states are statistically independent of the internal states and just saying that definition might not make the intuition any more clear but that really is all that it is

So then when we actually look at the dynamics of particular physical or systems or stochastic processes, that statement can entail all kinds of causal graphs and all kinds of influences between those subsets.

It's really the statistical independence

which defines the boundary.

Yeah, so I think that's kind of the kernel of the definition.

And for certain kinds of statistical distributions, we can actually really precisely look at the structure of things like the stationary covariance, which is basically the covariance matrix of the process at steady state.

And you can read off what systems are outside the blanket, what subsets of variables are outside the blanket, what subsets are inside the blanket, which subsets of variables are the blanket.

So it's a really precise definition that kind of goes beyond whatever intuitions you might bake into that on your own about causality.


SPEAKER_00:
Awesome points.

So Serval?


SPEAKER_04:
Sorry.

Does this not entail that blanket, if there is any causal interaction between internal and external states, then blanket states will mediate this causal interaction?

Because if they did not, then you would have information that will go directly from external to internal or vice versa.

So it's correct.

Then there is some causal sense to blankets.

Is it correct or not?


SPEAKER_02:
Yeah, yeah, definitely.

And that's a really good point.

So really what the blanket does is to separate the inside from the outside statistically.

And so what that means is that any information about the internal states, that any transmission of information on anything about the internal states that influences external states has to do it through the blanket states.

So there is a notion of causality, but it's like, as you said, the internal and external cannot affect each other directly.

They have to do it through the blanket.


SPEAKER_05:
Okay, thank you.


SPEAKER_00:
Awesome.

So Dean, and then Stephen.


SPEAKER_05:
Yeah, this might be a strange question, but that's fine.

So the blanket is kind of, we just use that as a way of describing the interface.

My question is, because most of the math that you guys use is over my head, but I appreciate that you've been able to come up with it.

My question is, we're thinking about this metaphorically, does the blanket

come with its own perforations?

I mean, we talk about it acting as a mediator, but is there a math that can kind of show it's managing this information flow back and forth, but does it already exist with these perforations present?

Or how is the math gonna tell us how that medium works?

Or is it, did you guys already explain that?

Because maybe I just didn't appreciate it when I read


SPEAKER_02:
paper um yeah that's a really good question so we didn't expand on that in the paper um and so it's good that we talk about it now so the definition of marker blanket as it is taking the paper allows for boundaries that are completely opaque so without any kind of transmission but also boundaries that would let anything through

So, so with this formalism is, is quite general.

Um, and so far we don't make any assumptions about the structure of the blanket.

And so it could be very permeable, not very permeable.

You can have all sorts of things.

Um, and, and I think that that's also, um, really interesting because then

I mean, ultimately, we want a theory that describes biological systems.

And so the blanket states have some degree of permeability in biological systems that we can actually exploit mathematically.

So we don't do that here.

Here the blanket is completely arbitrary, and so it could describe a stone, but it could also describe maybe a brain or a rat or something like this.

But then in the future, it would be really interesting to actually impose some more structure on the blanket to see whether we can say anything more interesting about the system that we're looking at.

Thanks.


SPEAKER_00:
Steven?


SPEAKER_01:
I think this is an area that's probably going to be growing a lot, is how much of that transfer process of information from the infinite, almost external states, gets onto a blanket and then gets transferred to the internal states.

So the question is, normally everyone talks about changing the generative model.

But there is obviously a question, well, what can you change in terms of the blanket?

But I don't think that's been fully explored yet.

But I think that's a big area that I suppose could be seen as part of the generative model or the regime of attention.

But it sounds like the mathematical model here is starting to be able to engage that more if it was expanded.


SPEAKER_00:
yeah let's walk through a few steps of the paper because that's really where we're we're building up this intuition or moving from the intuition and scaffolding it with the math um so first you phrase the blanket as intervening states between internal and external states

and defined the state space that those different internal, external, and blanket states can exist in.

So totally subject to Serval's comments about state space.

And maybe it'd be helpful to explain this big pie.

So what is the covariance precision?

What are we talking about with these matrices?

What's a system with a certain type of matrix structure that might help us get a grasp on what the covariance matrix at steady state is actually representing about systems?


SPEAKER_02:
Right.

So in our scenario here, the matrix pi actually encodes everything about the permeability of the blanket.

But more generally, how do the internal states interact with blanket states?

And how do the external states interact with blanket states?

So matrix pi really gives you the structure of the interaction.

And so we simply show that the Markov blanket condition, so this boundary, actually is equivalent to some kind of structure of the matrix pi.

But then when we try to differentiate


SPEAKER_00:
living systems with non-living systems and then intelligent systems from non-intelligent systems then probably we could say a lot more about this matrix pi so like if there was non-zero everywhere in pi what would that correspond to or what would it correspond to have all zeros what does a big number or small number mean in this precision matrix because this is truly a key variable in these formalisms


SPEAKER_02:
Right.

So one extreme would be the matrix pi to be the identity matrix.

And then this would mean that each state only interacts with itself.

And so that's kind of an extreme case of a Markov blanket, because you have a boundary, but then you don't even have any transmission with the boundary.

You have just three separate states, external blanket and internal, that don't talk to each other.

And then the other extreme would be to have a non-zero entry in the equation that we have there.

And then it would mean that internal states and external states can talk to each other directly without interacting via the blanket state.

So then you don't have a blanket.

You just have three states that mix with each other.

And so, yeah, you lose...

you lose kind of the basic point that says, oh, we're looking at systems that have a boundary.

And then in terms of the magnitude of the entries, so a big magnitude would say, I think I may get it the wrong way around.

But I think a big magnitude is like you have a lot of interaction.

And if you have a very low magnitude, you have very little interaction.


SPEAKER_00:
Okay, great, because this variable is important and this relationship between the one over the precision and the variance is gonna come up a lot.

So we go from this idea of using a matrix to represent

basically the statistical connectivity of different nodes and then hope that the structure of that matrix is such that it falls out or designing systems such that the structure of the matrix falls out into internal external and mediating blanket states and then

you develop this notion that the that there's an expected uh internal state and external state that are downstream of the blanket states so what is happening with these formalisms or how can it be that the blanket state tells us about what to expect internally and externally um so because you have a because you have a boundary because you have these blanket states


SPEAKER_02:
So let me give you an example.

If you think about a human, then, for example, part of the blanket states would be what happens on your retina.

So the impression from the world that you get on the retina.

And so this is to say that if you have some kind of sensory impression on your retina, then this gives you some information about what's going on outside.

And so if on your retina you see kind of like a certain shape, maybe the thing that's going on in the environment that corresponds to that shape is a chair.

and maybe actually that shape that you see on your eyes like looks like a chair so then you would say oh the most likely external state is a chair so this is all formalizing the fact that the blanket states what you have on your retina gives you information about what's going on outside and also influences what's going on in your brain because you need to process that information

And so, yeah, so we have a most likely external state and the most likely internal state.


SPEAKER_00:
Cool.

Any other thoughts on this?

Because then that's where we go from.

Yes.

So Connor, go for it.


SPEAKER_06:
Yeah, I'll just add that it's critical that we're talking about expectations here.

So that's why we're,

writing down like a normal distribution that's centered on this expectation because remember these are stochastic processes so for any given blanket state it doesn't mean that the internal state on the others that directly is downstream of that blanket state will always be that expected one but this is just the mean of the distribution but the powerful thing about this uh the bayesian mechanics for these uh gaussian processes

is that those expected states are basically just linear functions of the blanket states just given by these sub matrices of the covariance and then their inverses and such like that so it's just important to know that if we're to pursue this example of the retina it's like

For any given retinal impression, it doesn't mean there's always going to be the same cortical response in visual cortex or what have you.

But there's going to be some distribution of cortical responses.

And what we're talking about right here is the mean of that distribution.


SPEAKER_00:
Very good point.

So Stephen, and then anyone else with a raised hand, and then we'll move forward.


SPEAKER_01:
So from what you're saying there with the stochastic processes is there's some transfer in a sort of a random kind of informational sense.

And obviously, it's lossy as it goes through.

And it's the expectations change.

I suppose at some level have to be able to be confirmed or disconfirmed or find some sort of statistical correlation to then actually use the information.

I suppose at some point there's just not enough there to work anything out or the model's not good enough to find it.

And I'd be interested in how much... Do you have a sense of how much...

tends to get through, or how long it takes for information to be processable, like if there's some sort of heuristic, or whether that's a little bit too unrealistic?


SPEAKER_06:
Yeah, that's a really great question.

I mean, in the brain, there's evidence that you can get pretty complex.

I'm not a vision neuroscientist, but you can get pretty deep in the cortical hierarchy, even up to the level of a decision.

within 150 or sometimes less milliseconds.

So this is another big important thing to discuss about this paper is,

The information processing we're talking about here is on the level of expectations, right?

So if the brain is really processing an immediate retinal impression, it doesn't have time to average across a ton of parallel noise realizations and then use the expected value to actually do its processing.

So the brain in any given moment is actually just getting stochastic realizations, and then it's using

information processing on those realizations.

So I would also, I don't know how Lance feels about this, but I would hesitate in reading in like the expected cortical state, given a particular blanket state, I would, I would with restrained from reading that as the informational currency that the brain actually works with.

Um,

So there is information transfer, but it's not like the brain always has to work with expectations.

It can work with stochastic samples at any given moment.

But that's also like a controversial thing in neuroscience right now.

It's like, what is the actual information the brain is working with?

But the fact that the latency on decision making can be so short suggests that the brain isn't really averaging over a bunch of parallel realizations of something in order to do a computation.


SPEAKER_00:
Another piece is that on this slide, we're talking about the expectation in terms of the mean, the average value of a distribution conditioned upon blanket states.

We're talking about average value of internal states conditioned on blanket states.

we haven't yet introduced this question about internal states expecting or making a prediction about external states.

So it's almost like expectation is being used as just the sort of broad statistical concept that you can condition an expectation on some other variables or the systems that we kind of care about and are interested in, the ones where the internal states are acting as if they are

Making expectations or predictions about external states, which is where it's taken in the immediate following section of the paper.


SPEAKER_06:
That's a really good point.

Yeah.


SPEAKER_00:
what is the sigma function so i know there's some related um work uh like with miguel that we saw a little bit about like what is the sigma what kinds of gaps in the formalism does the sigma cover um how did it come to be this way with a mapping function and what does it reflect


SPEAKER_02:
Right.

So I would say that the sigma is the most fundamental ingredient to get to the free energy principle, because ultimately what we want to be able to say or say that happens or doesn't happen is that the internal states have some kind of representation about the external states.

And so the way to get there is to actually have a sigma function that maps from the expected internal state or expected cortical response to the expected external state, like what's going on around you.

So we need to find a relationship between internal states and external states to be able to say anything about inference.

And so what we do in this paper is we give conditions under which you have this mapping sigma.

And then you're able to say, oh, we have some kind of inference going on.

And I think in terms of a historical perspective, in previous papers, the sigma was assumed.

And then Miguel wrote this paper saying, oh, the sigma, you need some assumptions for the sigma to work.

It doesn't always, sigma doesn't always exist.

And so what we do in this paper is we give conditions under which sigma exists.

And if these conditions fail, then sigma doesn't exist.

And crucially, I think these conditions can be verified from data.


SPEAKER_00:
Great points here.

The synchronization map and a few of these other pieces relate to the idea of an image and a kernel.

So maybe it'd be helpful to learn a little bit about what are the image and the kernel reflecting?

I found one.

representation about mapping the two different spaces to each other using that sort of kernel and image notation.

But when you're talking about in Lemma 2.1, the existence of the sigma function entailing an image function that maps the internal to the external states, like what is an image?

What is the difference between just the variable in itself versus an image of it or a kernel of it?


SPEAKER_02:
Right.

So what we're trying to do is to map the expected internal state to the expected external state.

And so what we mean by image in the first line here is simply that we're trying to map variables that could be your expected cortical responses.

So we don't care about anything that's not your expected cortical response.

So we care about a specific type of variables

um and then we're trying to map these to expected external states so so that's the first line and so then here we we just say that basically the the gist of this result is that if if you have something that's going on in the environment let's say there's a chair in front of you and you get some kind of expected cortical response and then let's suppose there's a sofa in front of you and you get the the same expected cortical response

then you will have a problem because it means that your organism cannot differentiate between chairs and sofas because it has the same cortical response for the for the two two different external states and so what we're saying is that we actually don't want that to happen we want our organism to be able to differentiate between different external states

And so this is the condition three and the condition four.

They're both, I mean, each of them is equivalent to one.

And so it puts on the mathematical footing what it means to be able to differentiate between different states in the environment.


SPEAKER_00:
Awesome.

Thank you, Serval.

And then we'll ask a question from the live chat.


SPEAKER_04:
So for this thing to work, you need the organism to be able to discriminate between any two states of its environment.

Is that correct?


SPEAKER_02:
Yeah, so any two states that we model.


SPEAKER_00:
Not necessarily... Yeah, I see us mapping like for B1 and B2 being equal to each other.

Then it implies that the external predictions are going to be the same.


SPEAKER_02:
Yeah.

So if your expected cortical responses are the same, then the expected states must be the same.

So then again, this needs to be thought about in terms of representations.

So the external world does not need to be modeled at the molecular level.

So it's a model about the external world.

And so maybe in that model, there's things

Yeah, so it can be like a coarse-grained model of the external world.


SPEAKER_04:
Okay, so you use the example of a chair and a sofa.

I'm pretty sure that most of the listening people can do the difference.

I'm pretty sure that most cats cannot.

Well, I said a cat, a fly, it's smaller.

Why would a certain organism develop a representation of a certain state under the Carnegie Principle?


SPEAKER_02:
So that's a really good point.

And I think to address that mathematically, we would need to actually investigate a state space that changes over time, to actually address the question of how things learn representations.

So that's not what... We haven't done that far yet, but that's definitely...

an important thing to address in the future.

So the starting point here is you start with a steady state, you start with a Markov blanket.

So the representation is already baked in the Markov blanket.

And so in this result, we just say, oh, if the Markov blanket looks like this, then you have a good representation.

If the marker blanket is sort of more degenerate, then you have a bad representation.

So in the next slide, actually, if you can put up the next slide.

So we show that if the condition from the previous slide is not met, then the organism would have a degenerate type of representation where, for example, it cannot differentiate between chairs and sofas.

And that, I suppose, is still fine.

So yeah, it would conflate different external states.

So in this paper, we assume that this wouldn't happen, because then it enables you to derive results exactly.

But you could assume that the organism conflates different things, and then it would have some kind of semi-degenerate type of representation.

And then everything still carries out.

But instead of saying, oh, my organism does exact Bayesian inference, and it's super accurate, and so on, you can only say, oh, my organism maybe is doing some kind of approximate inference that's good in some instances and bad in some other instances.

So I think you can definitely relax that condition and sort of assess empirically how accurate is your representation.


SPEAKER_04:
Okay.

To reformulate, any representation must have a precursor sensor reactive state in this framework.

Cool.


SPEAKER_00:
Okay, very interesting.

And also we're still in the inference section.

We haven't introduced

policy planning or planning as inference.

So it doesn't need to be the perfect state estimator in order to make adequate policy.

So we've talked about that in many other situations about action and loop and everything.

So let's look through the paper.

Then as we build in pieces, we can see where these different segments come into play because we haven't talked about like policy or conditioning inference on policy yet.

So Steven, and then I'll ask the question from the chat.


SPEAKER_01:
One thing that can also help think about expectations and simplify or get away from the pressure of representations is that the blanket can be seen in more like, or the perception, visual perception is more like an impressionist painter painting dabs on a painting and then stepping out and looking at it.

And we're just continually

like trying to find the textures and the patterns in that might help confirm our expectations rather than it being like we see the shape of the object.

It could be that as long as we're seeing the type of textures that we're feeling in the space and that some of that could be done at the retina level, there could be certain things that just gets processed.

So I suppose just couching that in the inactivist world of, you know, really,

When we say representation, that's a bit loaded in that world.

So it could be statistically predictable in some way.


SPEAKER_00:
Yep.

And we've talked about representation and some of the different views on representation and Axel Constant's work on the armstice and the representation war.

So Blue's question, before we continue on with exploring the formalism, Blue asked, do the maths that you have been working with allow for altering the structure or boundary of the blanket itself?

For example, if I go blind, that sensory state is no longer available.

So how do we deal, which is a little bit we addressed earlier, but there's like different senses that the models can be changing.

They can be within the box changing at steady state, or there are these other types of changes that actually change the structure of the whole boundary or the internal model itself.

So what kinds of changes violate the model and which kinds of changes are allowing us to continue using the model?


SPEAKER_02:
So at the moment, what this paper accommodates are sudden changes in the blanket.

So you could at any point change the blanket and then see what happens.

So for example, if you go blind, you could change the blanket accordingly and then everything would carry out.

What this doesn't account for is blankets that continuously change over time.

So, for example, if you're looking at an organism throughout its lifetime, the organism would, after being born, it would grow and age and so on.

So we do not accommodate that, but we can accommodate sudden changes in the blanket.


SPEAKER_00:
Awesome.

Thank you.

And also just anytime people need to drop off is totally cool.

So let's take another look at that Markov blanket persisting through time.

and maybe even return a little bit to what kinds of systems are we talking about here there was this very interesting piece in example 3.1 about describing which types of stochastic processes at the gaussian steady state p might fit under this phrasing so several things were mentioned

Connor, how would you summarize this list?

Or is this an exhaustive list?

Is this just the tip of the iceberg?

What kinds of attributes do these examples share that allow them to fall under this framework?

And then what violates it and moves us into a different space?


SPEAKER_06:
Yeah, so basically the core set of stochastic processes that we deal with in this paper are diffusion processes.

So that's just a particular type of Markov process, which means that basically the current moment is totally statistically dependent on the last moment.

That's not to say that there can't exist mathematics that apply the free energy principle to other sorts of stochastic processes.

That's probably something that Lance is also working on at the moment.

But if I was to give like some

I mean, Markov processes are very generic in the sense that tons of biological processes, I think, and non-biological processes could be cast as Markov processes.

And I think there's also an important aspect.

This is kind of touched on a little bit in the next section with integrator chains.

Even though you might say, oh, but humans have memory.

There's all kinds of deep temporal dependencies.

you can have very high dimensional Markov processes with different levels of time scales.

But at the lowest level, I think it's a safe mathematical assumption to make that things are Markov process at the most fine grained level.

So I think that these kinds of systems

like pretty unrestricted in the kinds of things they can capture.

Everything from financial markets are modeled as diffusion processes.

The Ornstein-Uhlenbeck process, which is what we use in this paper, is a really popular process used to model things like decision making, like the drift diffusion process, which is used in kind of alternative forced choice decision making tasks.

And then things like evolutionary dynamics, like genetic frequencies drifting over time are modeled as a stochastic, like linear diffusion processes.

But I should also say that we use Ornstein-Uhlenbeck basically linear diffusions in this work, and that's how we get these nice linear synchronization maps.

The mathematics are all linear and easy, but this also applies to non-linear diffusion processes, so where the drift term basically isn't just a linear map like a big matrix like it would be for an OU process, but it could also be some non-linear very, you know, kind of

harsh function.

Some other restrictions is that the drift function, so basically the flow that determines the motion of the next set of states given the current state, that drift function has to be smooth.

That's another condition in these kinds of diffusion processes.

Yeah, that is, I think I pretty much covered it.

There's like some technical things like a zigzag process.

I'm actually not sure what that is.

That's something to ask Lance.


SPEAKER_00:
Great, thanks.

And what you said about that, the multiple levels of integration with the integrator chains and about how that addresses some of the comments about historicity, we can return to that because I think it's really instructive.

So Serval with a question, and then anyone else who wants to ask one.


SPEAKER_04:
Yeah, I have a question which is mathematical in nature for a change.

I don't get what this means for random-linked systems to have a steady state if it is not ergodic, because if it is ergodic, then due to, well, ergodicity, you can find a measure that says something about how likely I am to find a system in such or such subspace.

If you do not have the ergodic conditions and you cannot exchange a distribution for time average, I don't get where, what,

where the distribution is coming from exactly in the case where the system is kind of deterministic at least


SPEAKER_06:
Yeah, I can't think of an example off the moment, but there are processes that have an invariant measure where the time and the space averages are not equivalent.

So if you just look up non-ergotic stationary processes, I think there's some examples with discrete switching processes like Bernoulli variables.

where the time and the space averages will actually not be equivalent.

So I think this is explicitly taking into account those kinds of systems where the time and the space average wouldn't be equivalent.

For instance, if you think about systems with multiple basins of attraction that might stochastically jump between those two basins of attraction, the space average there at any given moment will not necessarily be equivalent.

Like a bunch of parallel realizations at any given moment, the space average of that might be centered around one of the modes of that process.

and that will actually be different than the time average, which would actually give your distribution over both modes.

So that's just something that jumps to mind, but there are other examples of stationary processes where the space and time average are not equivalent, which is the definition of ergodicity.


SPEAKER_04:
Okay, but how do you define steady-state distribution then?

What does it mean and what are the conditions for it to be defined?


SPEAKER_06:
So steady state distribution basically just means that the sufficient statistics that define it do not vary over time.

So they're invariant.

They're constant, basically.

So for a Gaussian process, the mean does not change over time, and the covariance does not change over time.

So that's an example.

Yeah, I mean, the definition of the stationary state, actually, if you get down to the fundamentals of it, I would more say that the stationary state is actually just a property of the flow.

So the flow, the stochastic differential equation itself, like the Ito-style differential equation, that's the most fundamental structure of any system.

And the stationary state actually is just something whose gradients drive the flow.

So rather than, but that's more an interpretational point.

But in terms of the actual.


SPEAKER_04:
OK, stationary, I guess.

So steady state.

OK, but you have stationary, I guess.

I know what stationary is.

And you have a steady state, which is defined, if I understand well, by distribution p. The question is, how do you define a distribution of a unical system in the general case of a stationary unical system?


SPEAKER_06:
So the way you define it formally is you define basically a partial differential equation that tells you the density of the system at every time point.

And that partial differential equation is called the Fokker-Planck operator.

And then you basically set the derivative of that partial differential equation equal to zero.

And then you solve for the density.

So that's the most minimal bare bones definition I can give you of the steady state is the density, which is the solution to the Fokker-Planck operator.

When the Fokker-Planck operator is derivative with respect to time zero, that's the stationary state.

So another way of saying that is if I start the stochastic process in a bunch of different places and run it over time, all those processes will converge to the solution to the Fokker-Planck operator, which is the steady state density.


SPEAKER_04:
Okay, and for what kind of system do we have a stable solution for this?

Rather than say, I don't have the words, a chemical attractor or something that switches and goes to new places like a dissipative process.


SPEAKER_06:
Yeah, so there's basically conditions on the drift that will tell you whether the system explodes or whether it converges.

So for instance, for an OU process, those solutions to the Fokker-Planck operator are defined when the eigenvalues of the drift matrix are not too big, for instance.

But what the actual conditions are for all sorts of stochastic processes such that there are unique solutions to the Fokker-Planck operator, I'm not sure.

I'm not an expert on stochastic processes.

That's something that Lance might be able to give a more rigorous definition to.

But it really depends on the drift, basically the f function, which tells you whether the system has an invariant measure or basically a unique solution.

And sometimes there's multiple solutions to the Fokker-Planck operator, which means that the system will end up in one thing or another.


SPEAKER_00:
Thanks for these helpful explanations.

Just one comment before Dean's question is that sometimes the steady state can be at a higher moment.

So like in the example of a car, there's like the position and then there's how fast it's moving and then et cetera, higher and higher derivatives.

And so the integrator chains and variational inference allow us to find an invariance at a higher level of the distribution,

that even as the actual position of the car is changing like that a stationary process doesn't mean the car is not moving it just means there's something about the car's movement that we can continue to integrate or derive towards finding a statistical invariance specifically one that's structured kind of like this where it's almost like a diffusion process or a flow across a field

So those are some of the pieces that link together.

And so it's not just that the position has to be well behaved.

It might look very nonlinear, but we're using certain ways of breaking down the problem so that at each level we can frame it as this like linked sequence of tractable calculations.

So.

Connor, if you have anything to add on that or Dean, you can question.


SPEAKER_06:
Yeah, I think that's a really good point.

So basically you can correct for a moving frame of reference through differentiation and that's kind of the basis of these integrator changes.

Something that might be a moving frame of reference, if you center it by its frame of reference, then it can actually look like something, like a stationary diffusion process.

And you get around that by encoding the system rather in terms of

absolute position, something like the velocity or the momentum of these higher orders of motion.

So yeah, I just totally agree with everything you said, basically.


SPEAKER_00:
And it's like the historical critique or comment like, well, that race car has a historicity.

It's like it is, but its position at the next time point is determined by its position and momentum and et cetera, et cetera, et cetera now.

And then that is happening through time.

So there is historicity.

We're not ignoring that, but we're setting up the problem with this like vector of higher derivatives such that it actually is being only determined by the next moment.

So.


SPEAKER_06:
Absolutely.


SPEAKER_00:
Dean?


SPEAKER_05:
Well, thanks for stealing all my thunder there, Daniel.

I basically was going to ask, so just to make this clear in my head, the metaphor that I drew is you've got classical mechanics and quantum mechanics in the room, and then Bayesian mechanics is kind of acting like the marriage counselor.

It doesn't really actually determine what is, you know, the math can't just be precise.

It just...

keeps the two other parties in the room and has them work out what's a gist and what's a reflection?

Or am I kind of way off in the reeds here?


SPEAKER_06:
So the connection to quantum mechanics, which was made in this one section of the paper, this was actually something that was suggested by...

um by uh greg pavliotis who is uh lance's supervisor imperial i'm actually not convinced that the connection to quantum mechanics is as fundament like so i i basically i don't think that uh basing mechanics is in between classical and um uh classical and quantum mechanics i see basing mechanics more as

the mechanics of statistical distributions or basically the moments of statistical distributions.

So in that way, if I was to put it in between two branches of physics, I would probably put it more at the three way in between classical mechanics, statistical thermodynamics and stochastic thermodynamics, because it's really uniting

the kinds of coarse-grained expected variables that you deal with something like statistical thermodynamics with the sorts of motion that you deal with in things like stochastic thermodynamics or classical mechanics.

So we're talking about the motion of beliefs or about probability distributions.

And then it gets interesting when you have a Markov blanket, because then you can start imbuing the system with these quasi-teleological

interpretations of one thing being a belief about another thing but the basing mechanics per se actually i think it even i mean it's it's it's controversial whether you would call it bayesian mechanics but there is definitely this kind of statistical or belief-based dynamics that you get even in systems without a markov blanket even a one-dimensional diffusion process where there's no boundaries or anything you can describe that system as optimizing model evidence or minimizing free energy

it's just, it's unclear where the Bayesian-ness is because the Bayesian interpretation really comes in when you're conditioning something on observations, and then you have uncertainty about another variable that you can only perceive via the observations.

So that's why the Markov blanket, I think, actually is kind of fundamental to the Bayesian slant on the mechanics.

But even without it, you have free energy minimization and you have

the evolution of beliefs or evolution of belief distributions.

It's just, they're not, they're not as, they don't have a nice analogy with like representational mechanics or the things we're interested in, in cognitive science or biology.


SPEAKER_05:
Yeah.

That's the part where Lance was talking about gist and reflection and how the gist morphs into a reflection.

And that's, I don't even know how you would put the math to that, but because I don't know, I wouldn't know where to begin, but that's,

That's where I thought you guys were headed.


SPEAKER_06:
Am I misled?

No, I think that's right.

That's kind of, it gets at kind of, I think, the thing that Daniel was saying earlier, where there's

There's the expected internal state given a particular blanket state, but then there's the idea that any instantaneous internal state is actually also encoding an expected external state, whether or not that particular internal state is the most likely one given the blanket state.

So I don't know which of those would map to gist versus reflection, but that's basically the idea is that any given realization of the stochastic process

at any given moment, even if it's off, is kind of always parameterizing some just perception about something on the other side of the blanket.

So the Markov blanket, the fact that there's a multivariate system with these kinds of conditional independencies, that really is fundamental to the interpretation of saying one side is doing some kind of just perception of the other side.


SPEAKER_00:
you just don't have one side or the other without the ability to distinguish them which i think is a good example about how the intuition that there are separable systems is being built upon in this nexus as you described with thermodynamics and statistics and statistical mechanics and there's other approaches to delineate subsystems like synergetics trying to define subsystems geometrically or trying to define subsystems through various ways and

maybe those will become integrated as well.

So Stephen, and then anyone can write a question or raise their hand.


SPEAKER_01:
Yeah, thanks Connor for that.

That explanation is quite useful as well that

you know, when you mentioned that there can be Bayesian inference say on information coming in, but the blanket, you know, it gives a way to hold that information, I suppose, but I can see it giving a much more viable route to integrate many sources

um of information which might be also sort of staggered over time as they come in and they can be integrated i mean you can integrate across blankets whereas if everything's coming in as one mush of you know all data's coming in through the same

or even if it's not directly data, but if you've only got one causal pathway, then you can't necessarily, I can't see a way that you could disambiguate and do Bayesian inference between what we would use blankets for, or you could even say multi-sensory integration

then that becomes a physical channel.

So I suppose one question is, how would you think about different channels of information as opposed to different blankets in terms of Bayesian integration?


SPEAKER_06:
Yeah, I think the multi-channel or multi-sensory integration story is pretty nicely accommodated by this.

And this kind of brings back to an earlier question that Daniel had about what is the interpretation of that pi matrix.

So when you're thinking about the system minimizing free energy, another way of thinking about that is it's minimizing prediction errors

for each of the sensory modalities.

So if our blanket states, say we have a three-dimensional external state, a three-dimensional internal state, and a three-dimensional blanket state, because this generalizes to very high integer dimension.

Our three-dimensional blanket state you could think of as like eyes, ears, and somatosensation or something.

And then the distinct elements of that pi matrix are basically the weights that the system, that the internal states are assigning to each of those sensory channels.

So if you actually rewrite the flow of the internal states as a function of that pi matrix,

you can basically see that the internal states has differential sensitivity to each of the three blanket states.

In fact, it's literally integrating them.

And the weights it uses to do that sum or that average are provided by the elements of the pi matrix.

So I think the multisensory integration is basically just a higher dimensional blanket state.

What it doesn't get at with what I've just said is your idea of information from different channels coming in at different time lags or different delays.

That is what you would have to accommodate with something like an integrator chain where one blanket state is actually a high order derivative of some other blanket state or something like that, or of an external state.

So you can have different blanket states that from the perspective of internal states are all coming in at once, but the actual blanket states themselves are like

longer scale temporal summaries of other variables.

One could be the instantaneous temperature.

The other could be the time derivative of temperature.

The other one could be the average temperature over an hour or something like that.


SPEAKER_00:
also sometimes i think about it like from a data science example like imagine if we had three variables that are getting passed to us like height weight and favorite color um maybe that reflects this covariance matrix like height and weight up there on the first two have highest correlation with themself

and then they have less correlation with each other, and then the off diagonal is not having any covariance.

We don't get information about the favorite color based upon the other variables, say.

So multi-sensory can just be different state variables that are getting passed through that vector.

It doesn't have to be that all the blanket states are of the same type.

The blanket states are just the set of insulating states.

And they don't all have to be retinal cells.

Now, if you were making this type of a model of the retina, it would be that way.

But then if you wanted to make a model of a person who had sound input, light input, etc.,

you could have the blanket states reflecting multiple different types.

Then you get this challenge of how is the brain integrating different lags and different precisions, et cetera, across different modalities.

So that's not the end of the story, but it is phrased generally enough in the state space representation such that we can be discussing in the same matrix, multiple kinds of inputs.


SPEAKER_06:
Yep, exactly.


SPEAKER_00:
Yeah.

Actually, Connor, if you had any thoughts on this example, like how should we read the, you know, pi equals what?

And then Q equals what?


SPEAKER_06:
Right.

Yeah.

So this is a point where I kind of wanted to bring this up earlier because I think I don't know.

It would be good to talk to Lance about this.

I think he's coming to the next one.

So maybe he can follow up.

But basically, this is actually an example of a system where causality does basically not exactly correlate with the Markov blanket.

So in that pi matrix, that's not actually the covariance.

That's the inverse of the covariance.

So that's a really important point.

The inverse of the stationary covariance is not going to have necessarily zeros in all the same places that the covariance does.

So you could have two variables that actually have positive or negative correlation with each other as measured by the square root of the covariance.

that are actually conditionally independent.

So for instance, in this example, the internal, which is the first row or the first column, and the external, which is the third row or the third column, they're conditionally independent of each other, which means there's a zero in the corresponding entry 1, 3 or 3, 1.

So that doesn't mean that the variables are uncorrelated.

It means that if I condition on blanket states the middle row slash middle column,

then there's no more information I can get from external states about internal states or from internal states about external states.

It's basically saying that the internal states get all the information they possibly can from the blanket states, and the external states don't add anything else.

But that's actually a different statement than saying that there's no stationary steady state correlation between internal and external.

And it goes further than that.

Then if you actually looked at the drift matrix, which is the second equation here, you can see from the presence of elements in the 1, 3, and 3, 1 elements of that second equation, that big thing that's like 1, 1.52.

there's actually a direct influence of internal states on external states in the sense that the motion of external states is directly influenced by internal states.

So if you were to integrate that stochastic differential equation, there would be a causal, if you want to call it that, a causal connection going from internal to external.

But nevertheless, because of the way that the stationary covariance is designed, when you take its inverse, internal is still conditionally independent of external.

So this is a really interesting and kind of counterintuitive result that is talked about a little bit in Miguel's paper, and it's also talked about a little bit in an upcoming paper that is in review right now that basically rigorously defines the conditions under which a causal connection actually results in conditional independence between two variables.

So that's basically what the example here is showing is that you could have internal directly influence the motion of external or vice versa, but in the stationary sense, they're still conditionally independent and therefore the Markov blanket is still needed to mediate information between them.


SPEAKER_00:
Awesome.

And it was, we'll look forward to these papers, but that's a really great way of laying out how we build upon this idea of just Bayesian variables and the graph that describes them to dynamical systems and their covariances through time, and then frame that in terms of flow

which can then be decomposed into this time reversible and a time irreversible component, ties us into this entire flow, areas of thermal flow, but also information flow.

And so also just Connor, while we have you, let's take this next step, which is let's partition

the blanket states into two kinds of states, sensory on the incoming and action or active states on the outgoing.

So what is going on with this blanket partitioning and how does it relate to active inference?

How does active inference relate to this Bayesian graphical approach more generally?


SPEAKER_06:
Yeah, so here we're basically making the move of taking the blanket states and they could be, again,

of general dimension and then partitioning them themselves into two subsets of states, active states and sensory states.

So if you basically look at the dynamics of the system and you look at the expected variables, you can say that the expected internal state given blanket states is doing inference about external states.

At the same time, you can say the expected active state

given sensory states, is in some sense minimizing prediction error.

So basically minimizing the deviation between the expected internal and the expected external state.

So that's where you get along the lines of basically an active inference interpretation.

And I think it's figure six, if I'm not mistaken, where that

Or no, that's predictive processing.

Figure eight is where you can basically show that the most likely active state, so again, this is an expected active state, is minimizing the sum of precision weighted prediction errors.

So the move here is then you relate minimizing, when you're minimizing prediction errors in the context of inference, it's like I'm doing optimal Bayesian inference.

When I'm minimizing precision weighted prediction errors in the context of control, it's like I'm doing PID control, which basically Manuel Baltieri has shown this in a paper from a few years ago.

That is a sort of prediction error minimization.

So now the elements of that pi matrix become the weights that you associate to each variable that you're trying to control in a PID system.

So PID is this kind of proportional integral derivative control mechanism.

So we don't do this in the paper, but if you were to associate different blanket states with different derivatives along an integrator chain, like the integral, the derivative, and then the second derivative of some process, and then those were your blanket states, then you could say that the most likely active state was basically trying to control the integral, the first order, and then the derivative of a given system, trying to control them using a PID control scheme.

and where the weights that the PID controller is assigning to the integral, the first order value, and the derivative, those weights would be exactly given by the entries of the pi matrix that correspond to the blanket variables.


SPEAKER_00:
Awesome.

And actually the integrator chain idea really connects some dots between this variational higher and higher derivatives of challenging functions, variational inference on nonlinear functions with planning as inference.

So again, it's like going back to that car with the position and the momentum, et cetera.

there's one way of working where you just want to fit the function of the car using its again position and higher derivatives but then what does it look like when you need to predict the position of the car so that you can act and intervene and so this just connects between this uh

graphical structure where you can have arbitrary number of layers of nodes.

So from left to right is going through time, but then vertically within a column is a differentiation and integration relationship.

So we connect from looking at a vector of position and higher derivatives, connect that to the kinds of systems that

have uh blanket states that's just really cool to see um yeah so with yeah go ahead uh yeah sorry was there someone else had a question or go for it connor but anyone can raise their hand or any questions in the chat is awesome


SPEAKER_06:
I was just going to say, it's essentially that.

So if we were to cast the blanket variables as this vector of higher order derivatives of a system, just by that system conversion to its stationary state, it would be implicitly, if we started it far away from the steady state, it would be implicitly essentially controlling each of those variables to some desired set point.

So that's often the terminology used in control theory is that I'm trying to keep the velocity

within some range, keep the position within some range, keep the acceleration within some range.

So if you interpret the dynamics of the most likely active state as doing PID control, basically what it's doing is it's punishing deviations around those desired set points with some control cost, where the control cost exactly is given by the PID weight assigned to the acceleration velocity position, which is, you can read off from the inverse of the stationary covariance, i.e.

the pi matrix.


SPEAKER_00:
To kind of give a fun example of that, one way of looking at this figure 10 in the Helmholtz supplement is kind of like there's an ink blot.

And so there's some forces that have it dispersing and some that have it staying together.

And then when you look down from the top,

That's like the steady state distribution of the ink blot.

And so that's sort of a diffusion-y way to look at it.

But let's look at this as like a physical hill.

So the z-score, the height is, not the statistical z-score, the height is like the concentration at steady state of ink.

So whatever the diffusion processes are, it's densest right in the middle, and then it gets less.

So we can think about that as being like the bottom of the bowl, and then it gets higher up.

So when you drop the marble in the bowl,

it's going to on average go to the bottom of the bowl and then if you were to think about that marble as like a control theoretic agent with its position which is initially not at the bottom of the bowl and then all of the higher order derivatives so initially if it's stopped on the side of the bowl it's like you're holding the higher derivatives to be zero

then you release it and gravity takes effect, et cetera, it would almost look like it was like a plane landing on the bottom of the bowl because it would like accelerate and then it would slow down its acceleration and the higher order derivative, it would act as if it were doing PID control to converge all of its, and then diminish all the derivatives so that it would end up with zero PID

velocity and the differential from the mean of the expectation of zero.

So that's like kind of an inanimate system, but that shows how this diffusion flow field, and then the free energy interpretation of kind of like the Gibbs free energy dropping into the bowl, finding yourself on the bottom of a physical bowl is also related to control theory.

So there's like so many of these important threads to connect together.

Any other general comments?

Otherwise, I think just because we love active inference, what is happening in figure eight?

Like what is happening on the right side?

And maybe why is it not a monotonic drop in free energy, which is what we saw in the other figures?


SPEAKER_06:
Yeah, so this is basically the demonstration of what you just said, that on average, the system, and that's a very key point of just to add that onto what you said, on average, the marble will look like it's trying to get to the bottom of the bowl.

and minimize those deviations from the different set points for position, acceleration, velocity, what have you.

So this is basically showing that... So I'll just explain the figure on the left.

The white line is basically the most likely active state, which is one of the two blanket states, given various settings of the sensory state.

So the white line is saying for any given setting of the x-axis, what is the most likely setting of the y-axis?

But then we also show, overlaid on that expectation, which is just a linear line, a linear function of the sensory state, we also show an actual realization of the process.

And that's really the key thing to note here, is there's a difference between expectations, which are derived from the steady state statistics, versus realizations of the process, which are contaminated with all the

the nitty gritty messy things of the real world like noise and solenoidal flow and stuff like that.

So this also goes to answer your second question about the plot on the right is why is the system not doing monotonic decrease on free energy?

And the reason is because there's solenoidal flow.

So if there is no solenoidal flow and you average this descent across trajectories,

An individual trajectory, even without solenoid flow, might still not do monotonic free energy descent because of noise.

So the stochastic noise you're adding to these diffusion processes can temporarily drive it up the gradients of the free energy.

But if I averaged across 1,000 trajectories, it would look like a smooth descent on free energy because all the noise realizations would be averaged out.

And then there would basically just be a straight line going from wherever you started

this ensemble of parallel simulations to the bowl, the minimum free energy.

But because of solenoidal flow, which is this component of the drift that drives the system along the isocontours of the probability density, which is not pointing in the direction of the free energy minimum, because of that, even after you average across trajectories, the free energy will not monotonically go down, but it'll actually follow some average trajectory that has some curvature to it.

So that's an actually interesting interpretation in the active inference sense because it's showing that on the way to the free energy minimum, it's not necessarily going to take the path laid out by the most likely active state given sensory states.

It might actually take a more circuitous route, and it will end up on the white line by the time it's at the minimum, but its journey to that white line on average and in individual realizations will not actually be along the white line.

And that's a really interesting, there's a connection there to be drawn to the study of basically efficient diffusion processes that are used in things like Markov chain Monte Carlo.

So people basically design diffusions in order to accelerate convergence to a stationary target distribution.

This is used a lot in like stochastic distribution.

sampling based approximate Bayesian inference and there's actually a lot of evidence and there's mathematical proof behind this too that diffusion processes with this kind of curvature solenoidal flow are actually better at reaching the minimum and they do so faster rather than one that's just doing a direct descent on the log probability which is equivalent to the free energy.


SPEAKER_00:
nice thank you one one thing i'll point out about this figure on the left is that the x-axis has twice the uh scale as the y-axis so you can also this white line is like the y equals x line like it's right at zero zero is that that's the bottom of the bowl and then it's it's

it's just stretched out a little bit in one way, but it's kind of like, that's just the Y equals X line.

That would be if the, that's just the one-to-one.

And then it starts up here with the S of T, A of T, and then it kind of finds its way to the bottom.

And that connects to figure 10 with the Helmholtz decomposition where

just like Connor was saying, we can separate out into this solenoidal, which is going like around the isocontour and then this straight line component.

And so the extreme of one case, assuming that such a line of sight exists is like you're at the bottom of the mountain and then you just like straight line up to the top.

But then the kinds of systems that we're looking at here are where we can decompose their actions into an as-if straight line that's always going up the maximum possible elevation gain to the mountaintop or the maximum possible descent to the bottom of the bowl.

But also there's this wiggle with a solenoidal coupling that's guaranteed to be orthogonal to that first component.

And that was an interesting point about designing the kinds of diffusion processes that do help us find, whether you phrase it as the top or the bottom, but find the free energy optimal place on a landscape that might be very rugged.

So a lot of misleading elevation changes.


SPEAKER_05:
Hey Daniel, doesn't that just give another feather in the cap for interactionism over instructionalism?


SPEAKER_00:
Can you maybe explain that a little more and then we'll see if Conor has any thoughts?


SPEAKER_05:
Yeah, I think what Conor just pointed out was really critical.

If we're optimizing, we have to be willing to set up condition where we're not enforcing a linear path, that we're actually allowing for the...

what do you want to call it?

The degree of variability.

And that's what interactionism supports.

So I just, I was just listening to that going, yeah, there's another, there's another proof.

Now we've got the mathematical evidence.


SPEAKER_00:
Cool.

It's like work and play a little bit with the most direct way to get those instructions might be with a little bit of interaction along the way.

They're just, they're,

inseparable in the actual realization of the process.

Any thoughts on that, Connor?


SPEAKER_06:
Yeah, I think that's, I mean, I'm always excited when people kind of can connect solenoidal flow to specific domains.

I'm not sure what instructionism or interactionism are, but I'm assuming that it's basically saying that just a linear classic optimization taking the path of steepest descent is actually kind of maybe counterintuitively not always the best option.

And I think that's just what solenoidal flow captures in any stochastic dynamical system is the fact that there's some curvature

to the actual dynamics of systems where they're not always trying to go to the mode of the density, but they might actually get where you're trying to go by kind of taking an alternative route on the way there.

And it's just really cool that mathematically you can show that that's actually better than taking the path of steepest descent.


SPEAKER_00:
It's like the ball can't just go down to the bottom in a straight line and then put on the brakes.

The real path takes a little bit of a different route.


SPEAKER_05:
So, Stephen?

Oh, yeah.

Oh, I want to slip one more thing in because I want to give Stephen a boost before he speaks.

And that is that I've heard Stephen a number of times mention the word trust.

And trust takes time.

And I think, again, that interaction and that trust piece, this math confirms that we don't just have to intuit that.

there's actually now some support for what takes time and what will actually optimize maybe the most sought after outcome, not always necessarily the best, but at least it gives you a higher likelihood of achieving an outcome.

Because you can now say trust takes time, but there's sort of almost a mathematical imperative to give more time over to something to arrive at the best answer.

Sorry, Stephen, but I just wanted to, you know, sort of boost what you've been saying a lot, because I agree with it.


SPEAKER_00:
Thanks, Dean.

Stephen?


SPEAKER_01:
Yeah, it's an interesting, yeah, I think the trust piece and what you could call real life, I don't know if we would talk about real life, but a couple of questions that sort of come from that is saying it's like, you know, if you're talking about this solenoidal flow, is that property or that, does that become a more optimal way of achieving success?

gradient descent when there's more noise or the more rugged the landscape I suppose that's my first question and the other question is I just wanted your thoughts you mentioned here like active states so this idea of like

almost like sensory data coming in on active states and we often talk about active states and action states i what's the states of action that starts the organism does but i suppose internally you've just got the active states which are kind of like almost mathematical codependencies so i'd be interested in

like I say, the amount of noise or ruggedness or approximation that has to happen and how much the solenoid or if it really makes a difference or whether the math just is the same if it's smooth and just your thoughts on how states are talked about in terms of active or action.

Or if that's something you think about.


SPEAKER_06:
Yeah.

Yeah, so for your first point, so when you say the ruggedness of the landscape, you mean like it's not just a quadratic single bump, but there might be multiple bumps.

Yeah, I'm actually not sure.

I don't have any intuitions about what solenoidal flow would do in that case.

I think like Daniel was kind of saying, it might actually be better at finding the global optimum.

The research I'm aware of is just saying that for even...

unimodal distributions like this sometimes the solenoidal flow actually gets you to the unique optimum faster even though you would think that direct descent should be fastest in the presence of noise in the dynamics so not a rugged landscape but just the dynamics are perturbed by noise the solenoidal flow will actually get you to the optimum faster but in the case of the actual probability distribution of the steady state is multimodal and rugged itself that's a very interesting question i don't know

I just don't know the research, what it says.

And I personally don't have any intuition about whether solenoidal flow would accelerate convergence to the global optimum or not.

But I would be surprised if no one's researched that because it's a very interesting question.

And then regarding the active states versus action.

Yeah, I mean,

I guess that I haven't thought that much about what semantically jumps out to me or what I associate when I hear active state versus action.

But I guess I think of fluctuations in the active states as actions.

So as the active state evolves, that's what I would call an action or a particular realization of an active state as an action.

And then the active state itself, we just refer to as the random variable.

That's a stochastic process that evolves over time.

And we refer to that as the active state.

So we have both the two-dimensional, in this case, two-dimensional blanket states.

One is a random variable that we call the sensory state.

Another one is a random variable we call an active state.

So that whole stochastic process is the active state and maybe individual realizations of it would be an action.


SPEAKER_00:
Thanks, Connor.

Stephen, and then anyone else.

And then also in this last few minutes, if anyone in the live chat has a question, they can write it.

So Stephen.


SPEAKER_01:
Thanks, that's helpful.

I think, like you mentioned, I was thinking maybe particularly about noise.

So, yeah, that makes sense if there's more noise.

Whether there's more rugged landscape, yeah, that's an interesting question that maybe someone's worked on.

I was wondering what you're basically saying with active states is there are more flux.

There's a greater flux.

A bit like sensory state, like the information coming in is more fluctuating, i.e., like...

well, you could say lights going at billions of hertz, you know, or whatever.

And then the regularities are slowing down as it comes in.

The active states would be a greater flux compared to going through the blanket and the transfer to whatever the perturbations that are relevant to our model are on the outside.

So that's kind of interesting.


SPEAKER_00:
One comment on that is the actual error dynamics or just baseline dynamics of each of these kinds of states, it's going to depend on the system.

But when we're thinking about biological systems, yes, the action states can be conditioned on what the system is.

Like if there's one degree of freedom on the elbow joint, then it's a dimensionally reduced action.

in comparison to the incoming sensory data, for example.

So active states end up kind of saving the day in a lot of ways, which is why it is such a clutch advance in the field to partition the otherwise just commingling blanket states into this kind of incoming and outgoing, because then we can assign those incoming states and the statistically causal chains coming inbound, we can associate that with inference.

and with everything related to predictive processing.

And then on the outbound, from the internal states going back out to the active states, we can associate that with cybernetics, control theory, and also planning as inference.

So the partitioning is an immensely important maneuver.

Stephen, and then anyone else.


SPEAKER_01:
Yeah, and also the active states become a proprioceptive form of, I mean, a proprioceptive form of sensory input in a funny sort of way, like in the sense that how much they change

So, for instance, I'm kind of curious where there's this blending.

You put your hand on a wall and you push.

You're sensing the wall, but you're also sensing your active states aren't changing as much, if that makes sense, because the wall's stopping them.

There's an interesting – but that all could be happening at this kind of –

fluctuating speed however like you just mentioned the system structure will the fact that you've got an elbow and the fact that it allows certain things to happen is going to make all that come into context of the system you know so that's quite cool


SPEAKER_00:
Yep.

Sorry if there's any noise there.

But yes, when the person is pushing on the wall, they might be getting a different than expected or less than expected amount of change in their sensory input.

Wow, it's really loud.

I'm just going to mute for a second.

incredible times so um both connor and lance have uh departed for today and i believe that we'll be talking with at least lance next week in the 26.2 so in this last 10 minutes or so let's look back over the paper and any of the other figures and let's think about what we want to ask or what we'd like to explore in that dot too so who has any thoughts of this


SPEAKER_05:
I have a thought.

I didn't raise my hand, sorry.

At some point, the 60-year-old me would like some of this to be brought down to six-year-old me in terms of the sophistication of the math and sort of maybe walk through some more of those

The ability to be able to explain to somebody that there is an underpinning of math that explains what this medium is about and why things pass through it and necessarily allow these two differentiated states to work.

I know that they have to be able to explain it

in on the level because it's it's complexity so they have to be able to use a certain sophistication of math but if there's some way for them to be able to bring that down to my pedestrian level my six-year-old level that would be fantastic because then i think it would be a lot easier for people to climb on board with the idea that some of this some of this

Bayesian mechanics stuff doesn't have to remain elusive or out there, but it's actually something useful that we can incorporate into a lot of the practices that we'd like people to adopt.


SPEAKER_00:
Thanks, Dean.

Agreed.

Good thing to think about.

And also Dave wrote in the chat, we seem to be convinced that some kinds of partitioning simplify analysis.

Great point.

So internal, external, and blanket is not the only partitioning of how you can partition nodes in a Bayesian network.

And sense and action isn't the only partitioning that is possible for blanket states.

So different ways of partitioning the problem

are going to variously set it up in a way that's going to be confusing or impossible to solve or challenging to solve or potentially easy to solve.

And sort of the exemplar of the easy problem to solve computationally, not the only example, but just one example, is like, again, the ball rolling to the bottom of a hill.

Smooth ball,

perfect gravity, convex optimization.

It's very easy.

If you know that the shape of the bowl is a quadratic function, then all you have to do is just, yep, go downhill wherever you are.

Really simple heuristic algorithm for getting to the bottom of that quadratic bowl.

and then we look out on the ruggedness and on the changingness of the fitness landscapes or of the free energy landscapes for real systems and the entire question is again how can we partition nodes or take the derivative of sets of nodes or of different random variables so that we can make that rugged landscape and transform it or at least act on a smooth landscape so that's sort of the broad theme is can we just

cut things up and reframe them so that our challenge is more like the marble going down the bowl rather than bushwhacking, you know, and we're just totally lost and we don't even have a sense of whether we're going up or down and all of our emotion is for naught.

So Steven, and then anyone else.


SPEAKER_01:
Yeah, I suppose in a way our cognition is trying to help us make things look more and more like things are rolling down a bowl.

And it's a smoother, more clear picture of the world, you know, as we get older.

So it'd be interesting maybe to ask a couple more questions about how noise, I know how noise, I mean, often they use the Infomax term.

or softmax, the infomax function, I think, in a lot of active inference modeling, you know, to add some noise or to add some distributions.

So I'd be interested in how, where does the covariance break down?

Where does it start to become more coherent, smoother?

And just their thoughts on that seems to be quite, that seems to be relevant beyond this particular paper.

Thanks, Dean.


SPEAKER_05:
Yeah, one last thing.

So to me, what I see is the math now on paper talking about possibility, not talking about

I still think that what this does is it opens up the possibility.

This map opens up the possibility as opposed to arriving at a certain outcome.

I think all it does is open up that space.

That's what that Markov blanket to me essentially does is it keeps the door open.

I know it's described as a blanket, but I honestly think what it does is it creates an openness for more math to be able to flow in.

And so just, I know they're not philosophers, but I'd like to hear some of their philosophy around what that does, because I think if people want to say, oh, well, your math doesn't answer all the questions,

Fine.

But it does open up a lot of things that previously we couldn't even say were open, to your point.

So I'd like to find out what their philosophical positions are now that they've been able to sort of bring this math to a place where it does prove out.


SPEAKER_00:
Cool, Dean, thank you.

And yes, what you said there about like from certainty into uncertainty, and of course, always leaving that room for the interaction, it puts everything on the uncertainty and the precision are, they're two sides of the same coin.

The precision is the inverse covariance.

So if two things are perfectly covariate, there's like a one or a high number in that matrix, then your precision is high.

So, or just, it's in the math, but yes, if they're variable with respect to each other, then knowing one doesn't tell you about the other.

If they're not variable with respect to each other, then knowing one gives you high precision on the other.

And it puts all kinds of state inferences on this continuum of precision.

So,

pretty awesome conversation really helpful that we were able to speak with the authors and yeah next week in 26.2 we'll look forward to maybe seeing some more of you on the stream having any people submit questions if they want to ask something or just have us digest it or modify a question at all

But as you said, very important contributions and scales beyond the paper.

So thanks again, everybody.

And we'll see you next week.


SPEAKER_05:
Thanks, Daniel.


SPEAKER_00:
Bye.


SPEAKER_05:
Thanks.
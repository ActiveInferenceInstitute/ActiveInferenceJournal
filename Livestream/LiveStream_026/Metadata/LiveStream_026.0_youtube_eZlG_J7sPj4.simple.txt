SPEAKER_00:
Hello and welcome everyone.

This is the Active Inference Lab.

It is Active Inference Lab live stream number 26.0 on August 6, 2021.

We're going to be discussing the paper Bayesian Mechanics for Stationary Processes.

Welcome to Active Inference Lab everyone.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links on this slide.

This is a recorded and an archived live stream, so please provide us with feedback so that we can improve on our work.

All backgrounds and perspectives are welcome here as well.

Today, as you can find out at this short link, we're going to be having the context video for the discussions on August 10th and 17th in number 26.1 and 26.2, which are gonna be group discussions with some of the authors joining.

So if you wanna join for any of these talks or discussions, then please just let us know.

And also we have a few weeks where we don't have a paper decided.

So if you have a suggestion and wanna be in those discussions, let us know.

Alright, today in Actim stream number 26.0, the goal is going to be to learn and discuss this paper, Bayesian Mechanics for Stationary Processes, by DaCosta, Fristin, Heinz, and Pavliotis, which was posted in June 2021.

This video, just like all the .zeros, is just an introduction to some of the ideas and a walkthrough of the paper one way, it's not a review or a final word.

We're gonna give some overview to the paper, then address a few of the keywords from a big picture perspective, then go through the figures and the formalism.

I'm Daniel, and I'm a postdoc researcher in Davis, California.

So in this paper, the aims and the claims are laid out as followed.

They wrote, in this paper, we considered the consequences of a boundary mediating interactions between states internal and external to a system.

So it's about interfaces and interactions.

That's the first point.

On unpacking this notion, we found that the states internal to a Markov blanket look as if they perform variational Bayesian inference, optimizing beliefs about their external counterparts.

When the boundary is set up in a certain way for which kinds of systems, that's what we'll be asking.

How do those systems look from the outside or what does it look as if they're doing?

And then three, when subdividing the blanket into sensory and active states, we found that autonomous states perform active inference in various forms of stochastic control, i.e.

generalizations of PID control.

So a lot of these terms like variational Bayesian Markov blanket, PID control, we're going to be talking about more.

But at the overview level, that's the aims and claims as the authors write them.

The abstract states that the paper develops a Bayesian mechanics for adaptive systems.

And then there's only these three claims again, which are again related to the blanket and adaptiveness

systems and then this partitioning of that interface into sense and action.

So first we modeled the interface between a system and its environment with a Markov blanket.

This affords conditions under which states internal to the blanket encode information about external states.

Second, we introduced dynamics and represent adaptive systems as Markov blankets at steady state.

This allows us to identify a wide class of systems whose internal states appear to infer external states consistent with variational inference in Bayesian statistics and theoretical neuroscience.

Finally, we partition the blanket into sensory and active states.

It follows that active states can be seen as performing active inference and well-known forms of stochastic control, such as PID control, which are prominent formulations of adaptive behavior in theoretical biology and engineering.

This is going to be relating to math and physics with the Markov blankets and the far-from-equilibrium thermodynamics component.

and information theory and then it's also going to be developed into some domain specific cases or shown to be a generalization of domain specific cases like theoretical biology and engineering these are the sections of the paper so first there's an introduction which has some of the general concepts then there's a section on markov blankets which is going to be a key

framework and a key formalism to track because it's kind of what the paper's about.

So first technical sections about the Markov blankets, followed by a discussion of how that relates to Bayesian systems or certain types of Bayesian systems.

And then this general Bayesian framework is then sort of specified towards an active inference model, specifically with an eye towards stochastic control.

And then there's a discussion and conclusion.

And then there's three supplemental sections related to the synchronization map, which is about the synchronization of internal and external states and how that's related to some math results.

There's the Helmholtz decomposition,

which is related to vector fields and splitting them into different components.

And then there's the free energy computation and some more details related to what exactly that formalism is used as here.

The keywords of the paper were variational Bayesian inference, non-equilibrium steady state, Markov blanket, free energy principle, predictive processing, and active inference.

So first, non-equilibrium steady state.

Here's a figure from this paper, stable, unstable, and metastable states of equilibrium definitions and applications to human movement.

So there's gonna be some mathematical definitions and uses of non-equilibrium steady state, but this is a domain specific usage that gets at some of the nuances.

There's not just one single way to frame equilibrium, especially for dynamical systems.

And this is shown with an example of a person in movement.

So there might be a total thermodynamic equilibrium, like death, but even within the non-death regime, there are still many types of equilibrium.

There's lying down, there's sitting in a chair, there's a walking steady state, there's this meta-stable state, and then there's unstable states of equilibrium.

So a few just questions would be like,

what does it mean to have equilibrium of a measurement or a system at a given level and what's the difference between that static just something on the ground versus a dynamic equilibrium like something that maintains its position and uses energy to do that so in those cases where does the energy come from

How does this relate to dissipative systems?

So something where there's energy to be expended like a battery or a tornado.

And how does that relate to adaptive systems and predictive systems?

Which ones are related to active inference?

One or both?

And what about when there are multi-level systems of equilibria?

So some that are higher frequency or shorter time scale or smaller spatial scale and others that are broader.

And also what if we want to have action-oriented models of those systems?

So we want to have the description that's optimal for some other set of requirements other than just accurate measurement.

and that's where this paper is going to be kind of headed is thinking about dynamical systems in general and looking at equations that relate to solving them another keyword was variational bayesian inference and there's a lot of good literature on this topic from a lot of different perspectives but this article had a good way of phrasing it which is that

Variational inference methods consist in finding the best approximation of a distribution among a parameterized family.

And in contrary to sampling approaches, a model is assumed, which is the parameterized family, implying a bias, but also a lower variance.

In general, variational inference methods are less accurate than sampling based like MCMC ones, but produce results much faster.

These methods are better adapted to big scale, very statistical problems.

So again, to just kind of give a qualitative example, it's like the real world is giving you this data, which is the blurry image of a cat.

One way to get at what was the generative underlying cat that generated this blurry image would be to sample and find the most likely cat generating this image or do various other sampling schemes.

And if it's set up properly, it can fit arbitrary distributions.

However, it can be computationally expensive and really hard to know if you're solving the problem at a functional enough rate.

In contrast, the variational inference approach is related to fitting the data based upon a pre-specified family of distributions, which can still be really general.

And it can be fast, but it can sometimes lead to fitting the optimal support for a given distribution, but having it just be categorically wrong on some other dimension, like estimated through a deep time or something like that.

So variational Bayesian inference is this lower cat where there's a known cat and then you stretch and distort the cat, for example, re-parameterize the cat to find an underlying likely cat.

Another

Keyword was Markov blanket.

And there was good discussion of this with Mel Andrews in Actim Stream 14.

And we kind of had this continuum from Markov on the left here, which was studying systems from a mathematical perspective, didn't have a computer.

So it was looking at matrices and which elements of matrices were connected to each other or not.

And then that was developed later by Pearl and others in this broader Bayesian statistical framework.

That's where the Markov blanket terminology starts to come from.

And it's also oriented towards empirical data and discovering dependencies in data.

And it's very computational.

And then part of the literature that we're discussing is where Friston and others have

built on or used this Perl blanket slash Markov blanket theme and done a few things like separating the blanket states into sensory and action incoming and outgoing nodes, introducing the cybernetic imperative to be a goal-seeking or a multi-scale good regulator to maintain that non-equilibrium steady state, which we'll talk about, and we did,

And then also what's being explored from a formalism perspective in the paper we're discussing is how is it or under what conditions do internal states act as if they're a model of their non-local dependencies?

So causal dependencies in the world would be reflected by how the brain or some other computer system works.

So that entails this

generative model of the action in the niche and also brings in the element of including the blanket in internal external states as part of this broader inactive embodied and cultured slash pragmatic term also in active stream 14 we had this sort of progression emerging from the structuring of the paper

where there's a sort of vector field meets diffusion thermodynamics branch on the bottom with the Fokker plank and Helmholtz.

And that's related to non-equilibrium steady state densities.

And then the free energy principle, subsuming all these sort of pieces like the Markov blanket being maintained through time, necessitates the generative model of action is what the internal states have to be inferring.

So the useful components of the external world.

And then under free energy principle, there's all these sort of

related areas like active inference.

Is that an implementation of systems under the free energy principle?

How is this related to predictive processing, Bayesian brain?

So we talked more about that, but today we're gonna look at how it's used more by the authors in this paper.

So the first thing is that

The code is available on Connor's GitHub, and we'll also be talking with Connor, hopefully.

So let's maybe work through it, or if anyone has the chance to run it, tell us about how it went or what they learned from it, but I didn't get to run it.

We can start with figure one, which is laying out the big graphical intuition of the whole paper.

And everything is going to be specifying

constraints or some details or implications of a way of interpreting this intervention of the blanket states between internal and external states.

So we have mu, the internal states, b, the blanket states, and eta, which looks like a curly n. And this is gonna allow us to separate any set of nodes

mu that we're specifying and ask what are the blanket states that insulate from a some statistical codependency sense which we'll be exploring insulates these two partitions and um the caption says the markov blanket is depicted graphically as an undirected graphical model also known as a markov random field

The circles represent random variables.

The lines represent conditional dependencies between random variables.

The Markov blanket condition means that there is no line between mu, internal states, and eta, the external states.

This means that mu and eta are conditionally independent given B. So conditioned on the blanket,

these internal and external states are separated.

In other words, knowing the internal state mu does not afford additional information about the external state eta when the blanket state b is known.

Thus, blanket states act as an informational boundary between internal and external states.

So that's what the blanket states do, and this paper is going to explore some of the consequences of that.

Again, anyone who has more expertise in this area or some piece of knowledge that I totally missed or got wrong, my apologies, but just want to try to represent it as I saw it because a lot of the details I was just wanting to learn more about.

and go over with the authors.

So if anyone has comments in the chat, they can also just like mention it.

Okay, so let's jump into how they present some of the key pieces of the formalism because that's the details of free energy principle slash some of the grounding of active inference.

So even if this is new formalism for you, then also hopefully it's also interesting.

So keeping that picture in mind of figure one, another way of writing out that graphical model with a formalism is this eta and then upside down t. So that's the external states and then mu conditioned on b. So that is blanket states are Markov blanket separating internal and external states.

So in the top third of the slide, these are like writing out in prose the same thing as this graphical model.

And then we can take that tuple, that set of those three kinds of nodes.

So we have measurements from all these different things or generative models of each of these different categories of nodes.

And then that

set so all the measurements slash generative processes that x is existing in a bigger state space so the little x which consists of like the real measurements are part of the bigger x which is kind of like the capital versions of each of these

And those spaces, the E, B, and I, are taken to be Euclidean spaces for simplicity.

So I don't know if it holds for other kinds of spaces.

So that's just defining the total space, the possible states of each of these nodes, and then also the ones that you actually observe.

They start with

this case of a gaussian distribution p encoding the distribution of states of the world so p is a random variable that has drawn from n that's like a normal distribution symbol with a mean and a variance and the mean is zero and the variance is this um pi to the negative one

And this has an associated precision covariance matrix.

So mean zero, that's the convention, n means the normal distribution, and so it's zero mean, that's the variance, pi.

And the precision matrix is pi, so then the variance is this pi inverse.

And sometimes the equations are formulated in terms of precision, other times they're

discussed in terms of variance but those are kind of like two sides of the same coin because the same it's just like describing it whether it's how uncertain you are is like looking from the mean out and then your precision might be kind of like is like focusing down uh unpacking in terms of gaussian densities so we find that a markov blanket is equivalent to sparsity in the precision matrix

So the Markov blanket condition is entailing that the covariance, the pi, between external states and internal states, that's the first one here, is equivalent to between external states and internal states, the mu and the eta, and that both of those equal zero.

So there's no covariance, and they're the same as zero, so they're conditionally independent.

That's the Bayesian covariance matrix framing of the Markov blanket sparsity.

Blanket states act as an information boundary between external and internal states.

And this allows the breakdown of instead of the all by all consideration of correlations for data fitting or generative modeling, you can reduce

the model space to a much better formulated subset of equations which are related to inferring distributions relating to external states conditioning on the blanket states and internal states conditioning on the blanket states.

This enables us to associate to any blanket state its corresponding expected external and expected internal states.

So then this equation drew us a set of equivalencies, which were that the internal states as a function of blanket states, so mu of B, were equal to the expectation, fancy E, of mu conditioned on B. So that's the expectation of external states conditioned upon of internal states.

internal states conditioned on blanket.

I think that's corrected is right.

Then three, that that expectation is the same as the distribution that's being calculated involving the objective distribution P. And that also that that's related to

variants and covariance like this sigma variable of internal and blanket states mu and b mapping to the space of all the possible internal states which was i from here and then they wrote pursuing the example of the nervous system each sensory impression on the retina and ocular motor orientation blanket state

is associated with an expected scene that caused sensory input, which is the expected external state, and does an expected pattern and an expected pattern of neural activity in the visual cortex, expected internal state.

So it's a way that that objective distribution of P gets integrated into the internal model's output

given blanket states.

Or maybe there's another interpretation too.

So that was just, I don't know, one way to look at it.

This question about the relationship between the internal and the external states after you've not denoised them, but de-correlated them in a specific way,

is this topic of the synchronization map.

So they go from just pointing out that, okay, we've decorrelated internal and external states with these blanket states.

Now we're going to ask how these internal states might encode information about expected external states.

So after we've decorrelated systems, how do they have a memory or a model of one another?

For this, we would need to characterize a synchronization function, sigma, mapping the expected internal state to the expected external state given the blanket state.

So sigma mapping function of internal states of blanket states is gonna relate to the external states at a condition on blanket states.

So there's still the dependence of both internal and external states on

the blanket states B, and now there's a Sigma mapping that is gonna connect internal and external states.

And Lemma 2.1 goes through some details of that specification.

So the first is proposing that there is a function that maps internal and external states

for any blanket state.

I don't know how generally it holds, but, or this would be cool to talk to the authors about, but it's sort of proposing this sigma function.

And maybe in certain cases, it's a zero line.

Maybe in other cases, there's some meaningful relationship, but for biological systems, it does seem like there's a meaningful relationship.

So those are the systems that we wanna explore modeling.

then write in how to construct this synchronization map the key idea is to map an expected internal state mu of b to an expected external state eta of b and to do this the two points they provide are one to find a blanket state that maps to this expected internal state i.e by inverting mu and two from this blanket state find the corresponding expected external state

i.e.

by applying EDA.

We now proceed to solving this problem.

Given an internal state mu, we study the blanket states B such that the internal states of B equals mu.

They provide some details.

Maybe that is meant to be external states.

I'm not sure.

Here, they're framing that inference that mu is doing, the internal states, in terms of covariance matrices, which is in this equation 2.4.

and then frames that matrix of unknowns as a system of linear equations, or those are kind of similar representations, and then solves for it in the vector space using this equation 2.5.

They use this matrix negative function, which is the Moore-Penrose pseudo-inverse function,

It would be helpful to have someone explain in a little more detail what the matrix inverse is or what degrees of freedom there are in choosing different kinds of matrix inverses.

But I just found it interesting from Wikipedia that a common use of the pseudo-inverse is to compute a best fit or least squares solution to a system of linear equation that lacks a solution.

Another use is to find the minimum norm solution to a system of linear equations with multiple solutions.

So this means that there might be multiple solutions that are equally well fit, and this just allows you to pick one fast, maybe.

And then they write that the key for the existence of a function sigma mapping expected internal states to expected external states given blanket states is that for any two blanket states associated with the same expected internal state, these be associated with the same expected external state.

This non-degeneracy means that the internal states, e.g.

patterns of activity in the visual cortex, have enough capacity to represent all possible expected external states, e.g.

3D scenes of the environment.

That's formalized in this lemma.

So, first was the point discussed before, that we're interested in positing the existence of this sigma function.

And then point two, so these are all points that are going to be equivalent to each other.

Point two is that for any two blanket states, one and two, that if the two external states are equal, I'm sorry, if the two internal states are equal to mu's of those two b's, then it's going to map on to equal points in mu's.

external world.

So the blanket is a map that conserves certain properties of the relationship between internal and external states.

Then points three and four of the lemma 2.1 use this kernel and image notation.

So here was a image that was kind of interesting about how spaces were mapped to each other.

So someone could definitely help explain what some of these mappings mean or what the implications are.

And then this sideways U shape

is the proper subset symbol.

So that's meaning that every element of the first one is in the second one, but B is a bigger subset.

So certain mappings between the internal and blanket covariance and the external and blanket covariance, and then similar for the pie.

Figure two provides a visualization of some simulations drawn that support, just from a simulation perspective, the existence of a useful synchronization map function.

So figure two on the left side shows a simulation

the external states given blanket states.

So those are the orange external states up.

These are the flipped.

Sorry about that.

So the

blue are the internal states the sigma mapping corresponding to the internal states conditioned on the external states and they're doing a very accurate job of mapping onto the external states in orange and then in b is a so-called non-example and

I'd be curious what changed or what was different about this second example, or is there more than one way to break the model?

What are the cases where it does or doesn't have a mapping?

Then in Section 3, they go from that sort of static estimate of variables that are just having a specific relationship to one another

to Bayesian mechanics operating through time.

So they write, in order to study the time evolution of systems with a Markov blanket, we introduced dynamics into the external blanket and internal states.

Henceforth, we assume a synchronization map under the conditions of the lemma 2.1.

So that was what was discussed up here.

And now in contrast with figure one, which was just the variables by themself,

Now we're putting the T subscript and looking at how those variables are changing through time.

We use a bacillus to depict an intuitive example of a Markov blanket that persists over time.

Here is the blanket state between the membrane and actin filaments of the cytoskeleton, which mediate all interactions between the internal states and the external states.

in example 3-1 they write there are many examples of stochastic processes at a gaussian steady state p to name a few stationary diffusion processes with initial conditions x0 tilde p their time evolution is given by an ito stochastic differential equation appendix b next slide that will be on the next slide

And then also example 3.1.

More generally, any Markov process at steady state P, such as the zigzag process, any mean zero Gaussian process at steady state, and any random dynamical system at steady state P. So questions for people who know more or the authors would be, what kinds of systems fall under this category of stochastic processes?

And what kind of real world systems would be modeled well or not by this formalism?

And then going a little deeper into that first point in the examples, Appendix B points to this Helmholtz decomposition, and that is conditioned on this Ito stochastic differential equation.

And again, just questions for someone who knows more or for the authors, what does this Ito mean?

calculation entail and what does or doesn't fit under this edo framework that would or wouldn't work using some other definition uh just not sure about that but in this helmholtz supplement

there is a very nice figure 10 that is looking at in this kind of hill climbing perspective so in an algorithm that's looking in its local neighborhood and doing a calculation to sort of make local decision making and hill climb

that action of a stochastic but hill climbing algorithm up this gradient or down depending on which way you set it up is going to have two components to its motion this

straight up the mountain component like putting a ruler on the mountain and the maximum slope and then another component that's at the same likelihood like the same altitude same energy that's going at an iso contour just like you can always do for a mountain so

with that idea in mind it's possible to take the full dynamic of a stochastic trace on the top left this is like this particle that is going different colors through time that's just like converging to the top of this mountain diffusing like to the bottom of the bowl like a marble going to the bottom of a bowl and then that trajectory can be separated into this spiral or I mean this

oval, the time irreversible component, and then a time reversible component that has a much more direct trajectory.

And that was in the section with remark 3.1, which is again related to the Ito stochastic differential equation, which I just am not too sure about.

but they write, when the dynamics are given by this Ito stochastic differential equation, a Markov blanket of the steady state density does not necessarily imply that internal and external states cannot influence each other.

So that's, I think, kind of the interesting tension in question is, how do certain mathematical definitions of conditional independence of blanket states separating internal and external states

how does that partition sometimes result in subsets or systems that don't influence each other versus systems that do?

So in the case of systems that don't influence each other, I was thinking of like an empty bottle of air in a room of air, like they're separated systems, but they interact through the bottle so they could transfer heat, for example, through hot,

bottle could transfer heat to the room but the systems would only interact conditioned on the temperature of the interface versus a person in a room could be planning or thinking about the external states so just how do we think about what kinds of systems are

still having an influence or a long-term implication for each other, even after conditioned on the interfacing a certain way?

And then also just how does this example provided kind of show that that'd be good to walk through?

And the absence of reciprocal influences between two states in the drift sometimes, but not always implies conditional independence.

Okay, going back to the main facts and figures in figure four, there's now looking at this synchronization map and transition probabilities for processes at the Gaussian steady state.

So on the left is we plot the synchronization map as in figure two, only here the samples are drawn from trajectories of a diffusion process with a Markov blanket.

So now it's a temporal process.

on the right panel there are the transition probabilities of that same diffusion process for the blanket state at two different times and this is called a joint distribution because there's two distributions that are being looked at jointly and if you kind of went to one side and looked through the cornfield one way you'd see this Gaussian

p of b sub s. And if you looked at it the other way, you'd see p of b through t. And then they write, which is kind of interesting, this shows that in general, processes at a Gaussian steady state are not Gaussian processes.

So that was just like very curious about what that meant.

In fact, the Ornstein-Ohlbeck process is the only stationary diffusion process that is a Gaussian process, so the transition probabilities of nonlinear diffusion processes are never multivariate Gaussians.

Not sure what that means, but sounded kind of interesting.

All right, so then, from this sort of mapping between internal and external states,

through time, the synchronization mapping, sigma, they write in the section on predictive processing, we can go further and associate to each internal state, mu, a probability distribution over external states, such that each internal state encodes beliefs about external states.

So

now rather than just framing these as two dynamical processes that track one another like oh there's this synchronization map between this random variable internally and this random variable externally they're going to frame this q distribution so q is going to be a probability distribution inside of mu

that's over external states, such that each of those states encode beliefs about external states.

So Q of internal states is about eta, and that's gonna be a normal distribution

with a mean of the actual external state it's like mean and variance um so it's eta and then sigma of mu the mapping function comma variance so here these two are both what it's being uh is being inferred is the the external state and the mapping and then there's the variance of the precision on external states

And then 3.4 is just giving a little bit more details about how that Q is like this internal function where the states of it are reflecting beliefs about external states.

And then there's a relationship to how the internal states are conditioned on blanket states.

This was a...

interesting remark that'd be cool to have people discuss would be when they wrote note a potential connection with epistemic accounts of quantum mechanics namely a world governed by classical mechanics sigma conditioned at zero or I don't know if there's another way to read that symbol with three lines in 3.2 I'm not sure because in 3.2 I'm not sure if there was a sigma

in which each agent encodes Gaussian beliefs about external states could appear to the agents as reproducing many features of quantum mechanics.

So this was just cool, like what is the relationship between the sigma internal external mapping function and then classical versus quantum mechanics?

So maybe classical systems are like the ones where there's a clean separation

between the internal and the external states, quantum states being ones that are more with residual influence between internal and external states.

So returning to the predictive processing, they write that under that three, four, so under this implication of the Q internal distribution,

Expected internal states are the unique minimizer of a Kolbach-Leibler divergence.

So that mu through time, internal states through time, are the minimization, the argument, over the mu states, internal states, of the dKL, the KL divergence, of...

the internal belief distribution Q about external states Q of eta double line diversions between the objective distribution of external states conditioned on blanket states.

So if that Q distribution can converge to the P, then that is being minimized effectively.

This measures the discrepancy between beliefs about the external world

q sub mu of eta, and the posterior distribution over external variables.

Computing the KL divergence, they're going to obtain this 3.5, and that's arg min, so it's minimizing the differential between sigma's prediction

So that's sigma of mu minus the actual external states through time, sigma mapping function of internal states minus the actual state, multiplied by, so pi of the precision matrix of eta relating to how wrong that differential is.

So that's kind of an interesting framing.

And then they write that the right-hand side of 3.5, that equation, is commonly known as the squared precision weighted prediction error, which is the discrepancy between the prediction and the expected state of the environment

and it's weighted with a precision matrix that derives from the steady state density.

This equation is formally similar to that found in predictive coding formulations of biological function, which stipulate that organisms minimize prediction errors, and in doing so, optimize their beliefs to match the distribution of external states.

So that's pretty cool, and we can talk more about it, but it kind of...

reminds me of this uncertainty reduction imperative rather than reward maximization imperative it's kind of like saying if the organisms are minimizing their prediction errors not maximizing the reward they will optimize their beliefs to match the distribution of external states so maybe that's related to that variance reduction imperative

So we had the predictive processing section and then there's one more layer that gets added on.

So from the Markov blanket to making it a Bayesian Markov blanket happening through time to introducing this Q distribution that's a special type of internal state

they write in the next section we can go further and associate expected internal states to the solution to the classical variational inference problem from statistical machine learning and theoretical neurobiology so it's going to be that q function is going to be a very specific kind of q function so

Not all systems are going to be doing this subtype of variational inference, or maybe they are.

That is something we could discuss.

But we can finesse that mapping function between the internal and external states and what exactly the internal states are.

Because if you can co-design both the internal states and the mapping, then

you have a lot of degrees of freedom in mapping on to external states even as they change.

So now we're going to do a really sub-special phrasing.

Maybe it's a more general one, but just one specific phrasing of how the internal states could map onto the external states.

And that's going to be as expected internal states as the unique minimizers of a free energy functional, i.e.

an evidence bound.

So now there's a F function over blanket and internal states through time.

And that f function is going to consist of two elements, which is like a divergence term that we saw before with a divergence between Q's internal estimate of external states, divergence and the actual external states, conditional blanket states, minus the evidence term.

And then that can be equivalent to this energy minus entropy term.

So that last line expresses the free energy as a difference between energy and entropy.

Pretty cool.

Energy or accuracy measures to what extent predicted external states are close to the true external states.

While entropy penalizes beliefs that are overly precise.

So that's cool.

Like energy is locking in on the good solution.

That's like maybe the component that is going up the hill and then entropy is keeping things away from just going up the hill and keeping this other component in.

But if entropy overwhelms the energy, then the solution will not kind of fit the situation.

So at first sight,

Variational inference and predictive processing are solely useful to characterize the average internal state given blanket states at steady state.

It is then surprising to see that the free energy says a great deal about a system's expected trajectories as it relaxes to steady state.

Pretty interesting.

So here's how they kind of show that.

Figure 5 is using that free energy minimizing framework.

to look at variational inference and predictive processing.

So the highlighted part says, the figure illustrates the system's behavior after experiencing a surprising blanket state.

So kind of like a perturbation.

This is a multi-dimensional Ornstein-Ohlbeck OU process with two external blanket and internal variables.

initialized at the steady state density conditioned on an improbable blanket state which was given at the initial time actual distribution of x and b at time zero and on the left top left quadrant is sort of this red to blue gradient of free energy so it's kind of high elevation to low elevation and through time the blanket states just converge down to the bottom of the bowl

So that's the free energy minimization in action.

The upper right is the free energy over time averaged over multiple trajectories.

So it starts out really high and then it kind of rolls down and it converges to this low level at 40 time points.

The lower left is showing this predictive processing framing.

of the q distribution versus the actual distribution and said at steady state from time step about 100 the predictions become accurate so that's showing how for the estimate of the parameter um converges to the accurate external states under their simulation be cool to do a walkthrough and then on the bottom right

is looking at the error, prediction errors, the covariance, so the two co-errors, the evolution of precision-weighted prediction errors over time.

These are normally distributed with zero mean at steady state.

Figure six is looking at

Maybe I didn't change.

Not sure exactly.

It's a similar figure.

It has a little bit of differences.

Again, showing the predictive processing lock-in on a free energy minimizing process, showing the average free energy initially having a slight increase maybe, and then just decreasing through time, and then also just showing that the...

the relationship of the covariance was raised up from zero and then started converging back to zero.

So just cool figures, but cool to see what other people would think about them.

In section four, we get to active inference and stochastic control.

So they write, in order to model agents that interact with their environment, we now partition blanket states into sensory and active states.

So everything that we've talked about before is like an undirected Bayesian graph.

It's not...

using that frist and blanket separation so that's where we're going to be bringing in a lot of other cool formalisms and ideas so let's go from that sort of blanket one type of fiber in the blanket to two now in order to model agents to interact with their environment we now partition blanket states into sensory and active states

So the blanket state B of T, blanket through C, is now two subsets of sensory and action states through time.

So now that whole tuple, the whole set that you need to infer the four kinds of states are external, sense, action, and internal.

Eta, S, A, U, mu.

And now that previous example of the bacillus, so here we had just one, we had external and internal states mediated by one kind of node, and now the external states are coupled to the sensory states, that's S of T, and sensory states are coupled to internal states, internal states are coupled to action states, and actions connect back to external states, also sense and action are connected to each other,

i wonder if that part is needed or i'm always wondering what topologies of connectedness different functionalities arise from and uh miguel aguilera's paper and recent uh guest stream speak to that so we take that dynamic markov blanket model from the previous sections and now split the blanket states into sense and action

that was this piece here now then they write intuitively in agents actions and internal states depend upon its sensations therefore we are interested in characterizing autonomous i.e active and internal so we can't control our senses directly but we can control internal states and active states so those two states we're going to think about that pair of notes given sensory states so

that is the following free energy equation that was brought up earlier so taking that free energy minimizing perspective on the distribution of mu internal states and blanket states as a whole b before we introduce the split here then do a similar free energy equation

with three variables on the sensory action and internal states which is just another way of saying the blanket states and the internal states mu and then etc etc more details that people can explain if they know better but the results that um

comes out relates to action so be cool to learn more about what this equation means but then they write this is known as active inference because expected autonomous states minimize free energy crucially active inference is a well-known framework to describe and generate adaptive behavior in neuroscience machine learning and robotics see figure eight so figure eight

is a similar diagram to we had before but now it's an active inference agent rather than just the free energy minimizer of the previous framing and then they write

The figure illustrates a system's behavior after experiencing a surprising sensory state, averaging internal variables for any blanket state.

This is an Ornstein-Ohlbeck process with two external, one sensory, one active, and two internal variables initialized at steady state density, conditioned upon an improbable sensory state.

And...

Also, one pattern that they found was that the free energy minimization here, it's relatively monotonic.

It kind of doesn't ever bump back up.

It just goes down or stays the same.

But then it was kind of interesting.

They said, we plot the free energy of the expected internal state averaged over multiple trajectories.

So this is not just one aberrant run.

In this example, or maybe...

This is just one example, but they wrote, the average free energy does not decrease monotonically.

See figure five for an explanation.

And then maybe we can learn more about that, but that was kind of cool.

Okay.

They continue on and write that any mean zero stationary Gaussian process with exponentially decaying autocovariance function, so it resembles itself less and less through time,

is an ornstein olbeck process which is kind of cool because in phylogenetics we sometimes would use this for let's just say the average femur length of some mammal and then you would be inferring a model that had an average value so it was kind of like a peg and then like a tether so like a diffusion term

and then a mean.

So just a mean and a variance that drift through time is like the Ornstein-Ulbeck or the OU process, sometimes known as Dube's theorem.

So those kinds of processes, which are apparently general enough to be really good model systems for studying math, I guess.

Thus, if...

c equals the finite sum of exponentially decaying functions we can express s of t the states sensory states as a linear function of several nested ou processes i.e an integrator chain from control theory so the integrator chain was kind of a cool idea and we'll talk about maybe applications of integrator chain in a few slides

but just to continue with the math that they have.

So there's this S of T, states through time, sensory states, is going to be this F function.

And then there's these kind of similar D equations.

So it says in this example,

f and f sub i are suitably chosen linear functions not sure what the pronunciation is but the squiggle are matrices and w are standard brownian motion thus we can see sensory states through time st as the output of a continuous time hidden markov model whose hidden states s of t superscript i

encode various orders of motion, position, velocity, jerk, etc.

These are known as generalized coordinates of motion in the Bayesian filtering literature, which we'll look at graphically in Figure 9.

And more generally, the state process S of T and the function f need not be linear, which enables to realize nonlinear non-Gaussian processes S of T. Technically, this follows as Ornstein-Olbeck processes are the only stationary Gaussian Markov processes, yet we emphasize that stochastic realization is not as well developed in the general case.

So this is in a way saying that we're gonna have like the position of the ball is the sensory data.

That's like the first pass of where the ball is.

That's the measurement.

That's the sensory data.

Then there's the variance.

And then there's the second derivative of that statistical moment.

And so if the variance is equal through time, then you can just integrate away and the higher levels are zero and you've kind of locked on to the pattern after just one or two levels.

But there's other distributions where as you go to higher and higher derivatives, higher moments,

There's just more or different information.

So there's this concept of the integrator chain where these relationships of equations are all integrals or derivatives of each other, which are kind of like going up or down in this rows of equations.

And then this helps use Gaussian type mathematics, like the OU stochastic process, where a lot of the math is clean.

It allows you to apply these systems of equations of well-behaved types, OU, to model highly nonlinear systems.

So this was related to some interesting discussions.

Avil, I'll look forward to hearing what you have to say about these so-called generalized coordinates of motion.

And that relationship between control theory and then looking at nonlinear functions as relationships of integrals and derivatives leads to this discussion of the PID control.

So proportional integral derivative control, PID, is a well-known control method in engineering.

More than 90% of controllers in engineered systems implement either PID or PI, no derivative, like just proportional integral control.

The goal of PID control is to control a signal, S sub T, its integral, S sub T with a zero instead of a one, and its derivative, S sub T with a two, close to a pre-specified target value.

So it's like, I want 95% battery, and I want it to be a rectangle at 95%, and its derivative to be such and such during use.

This turns out to be exactly what happens when we take stochastic control of an integrator chain

with three orders of motion.

So that's just a three-level model in this sort of infinite rows of the moments of the distribution.

The three levels are just what are described here, the signal, the integral, and the derivative.

When f is linear, expected autonomous states control to what extent integral, proportional, and derivative processes, S of t at 0, 1, and 2 levels of derivative, are from their target value of 0.

So it's like if it's a linear response function, then the first derivative is going to be a constant value, and then the second derivative, you get rid of it.

Furthermore, from F and K alone, one can derive integral proportional derivative gains, which penalize deviations of these three moments respectively from the target value of zero.

So these are highly optimizable problems and computable.

Crucially, these control gains are simple byproducts of the steady state density and the stochastic realization problems.

This is a cool question.

Why restrict ourselves to PID control when stochastic control of integrator chains is available?

It turns out that when sensory states S of T are expressed as a function of an integrator chain, one may get away with controlling an approximation of the true sensory process obtained by truncating high orders of motion as these have less effect on the dynamics.

Though, knowing when this is warranted is a problem in approximation theory.

So how, let's just say we open the door to doing infinite derivatives of statistical distributions, infinite variances upon variances.

How would we know when to stop?

That's the problem in approximation theory or approximation science.

This may explain why integral feedback control, which is just like battery good, PI control, n equals one, and then the PID control with two extra layers of the derivatives, are the most ubiquitous control methods in engineering applications.

However, when simulating biological control, usually with highly nonlinear systems, it is not uncommon to consider generalized motion to fourth or sixth order.

So that's pretty cool.

um like how many levels of recursion and depth in modeling are needed to have a people so calling back to here when we defined that initial sense function as part of this integrator chain setting off the integrator chain here's that initial sense function as part of a broader set of dependencies

So here is that sense function at time three, six, and nine.

The figure is depicting in a graphical format, the Bayesian network of those equations flipped.

The encircled variables are random variables, and the processes are indexed at an arbitrary sequence of subsequent times.

The arrows represent relationships of causality.

So the sensory states are not causing each other through time.

They're getting measured through time, but they are being emitted through time by this S at the zeroth level, like just the base generative model,

of this s of zero level function so this is the generative process then that can have higher derivatives of change and each of those are being framed as well behaved statistically so in the hidden markov model and these white nodes are hidden states but they're inferred by statistics

The hidden state processes as a T is given by an integrator chain, i.e.

nested stochastic differential equations.

These processes can be seen as encoding the position, velocity, Jurek, et cetera.

So higher derivatives of the distribution.

So this integrator chain is kind of a cool idea.

here was one paper stabilization of integrator chains in the presence of magnitude and rate saturations a gain scheduling approach so this paper wrote and the conclusions in this paper we have given a time varying controller so kind of like a heuristic or an algorithm for control for stabilizing a chain of integrators in the presence of magnitude and rate saturations

It is proved that the controller gives a convergent closed loop system, i.e.

for any bounded state initial condition, the state will converge to the origin.

The main strength of the controller is that it gives provable convergence without being overly conservative.

This is verified in a simulation study.

So it's kind of like we have the frame of the airplane at steady state, which is flying through the air.

So how are we going to re-equilibrate all the sensors so that if we get knocked off to the side, what's the path by which we're going to get those sensors back to their desired level?

And it's just a question.

Oh, and this was kind of what it looked like, one of the figures from the paper.

So these are these derivative functions that get bumped and then they come back to the level that they're desired, almost in this signal processing-like way.

So what are integrator chains?

How are they used?

Maybe someone has used them before.

That'd be cool.

And then one kind of random thought was there's a lot of cases where we frame a process as mean zero because it allows thinking about something in relationship to a defined axis, whether it's the number line or whether it's some other variable.

it kind of splits half the things negative and positive, or there's various other times.

But there's actually a bunch of places and maybe more, or maybe these are not all the same.

So it's okay if somebody knows more and helps us all out.

But there's actually a few cases where a distribution that's hard to estimate is replaced with a zero mean.

Like sometimes there's replacing elements of a list with a Z-score, like their standard deviation from the mean.

or the mode, but then it's like half are above and half are below.

So that's like replacing something with a zero centered z-score or t-distribution.

Sometimes there are error distributions, which are extracted from a signal.

It's like the signal is the underlying line, and the goal of the signal extraction is to leave your error distribution mean zero, and then equally distributed in both directions.

So sometimes we're pulling out an error distribution.

Sometimes we're trying to find an error distribution

that mean zero get to a level of derivative of a signal where the mean is zero like where the rate of change of some high derivative is zero which implies that it's not changing at that level and then sometimes we're trying to get um like in predictive processing study the differentiable or predictive processing with trying to get a zero centered variance on the sensory input

but more about how we can study the differential between some controllable distribution and then some other distribution that we can't control and then how to make how to zero center that to determine which solutions are better or worse than each other and there's a lot of different methods there and then there was this other paper

Time optimal regulation of the chain of integrators with saturated input and internal variables in application to trajectory planning.

So again, just kind of a open question.

What is the relationship between integrator chains and then this idea that we talk about a lot with action planning as inference?

Then the conclusion.

This paper outlines some of the key relationships between stationary processes, inference, and control.

These relationships rest upon partitioning the world into those things that are internal or external to a statistical boundary known as a Markov blanket.

When equipped with dynamics, the expected internal states appear to engage in variational inference, while the expected active states appear to be performing active inference in various forms of stochastic control.

The rationale behind these findings is rather simple.

If a Markov blanket derives from a steady state density, the states of the system will look as if they are responding adaptively to external perturbations in order to recover the steady state.

Conversely, well-known methods used to build adaptive systems implement the same kind of dynamics implicitly so that the system maintains a steady state with its environment.

So maybe this isn't a totally accurate classification.

but there's some algorithms that are being connected on one side to each other.

So we have learning and optimization, learning as inference, all of machine learning, et cetera, learning and optimization, and then variational inference being used as an optimization technique with the gradient descent by free energy in machine learning and maybe in evolutionary systems

then there's this question about estimating latent causes in the world and connecting that to higher derivatives in the integrator chains or as just variables under consideration that's that mapping of external states which aren't directly seen and then maybe this one goes on this side but

uh there's this whole other set of algorithms related to adaptive systems and these are framed variously but we have adaptive systems cybernetic systems anticipatory systems holism homeostatic slash active learning allostatic agents all these sort of biologically inspired

yes drawing on far from equilibrium thermodynamics but here was like the flame dissipating like it's about something that just sort of gets to its free energy minimization just like gets there but with these active systems the question becomes how do we take this ongoing multi-scale inference side and connect it to this multi-scale action in the loop

niche modifying, also action planning, biology section.

One friend looked at these slides and then made a comment and they wrote, I think it would be good to expand on the differences of the paradigms in section three, which were lettered as they were in the paper, a priori,

A posteriori estimation, afterwards estimation, predictive processing, and variational base.

I get the differences formally, but still not at a super intuitive level.

So this might be good as a pedagogical exercise for ourselves.

So that'd be cool.

Maybe everyone can think about that and come to their own explanation for what the difference is between these three.

And we'll talk about it in the dot one and dot two.

Then just a few of the implications and bigger questions that are raised by the paper.

So in the section on interacting Markov blankets, they write, the sort of inference we have described could be nuanced by partitioning the external state space into several systems that are themselves Markov blankets, such as Markov blankets nested at several different scales.

from the perspective of internal states this leads to a more interesting inference problem with a more complex generative model it may be that the distinction between the sorts of systems we generally think of as engaging in cognitive inferential dynamics and simpler systems rest upon the level of structure in their generative models i.e the steady state densities that describe their inferential dynamics

So that's pretty cool about maybe this framework or at least work in this level of analysis mapping just directly to nested Markov blankets.

And it brings up this whole discussion about,

what kinds of systems can we model with active inference so no one's saying that the stock market is an auto-regressive model but people use auto-regressive models so what kinds of systems will we usefully be able to model with active inference without really just getting into a debate about whether it quote is active inference in the system just how are we going to use variational base methods with

modern twists and developments related to cybernetics and multi-scale systems.

So that's like the instrumentalism side.

And then the realism side is what kind of systems are doing active inference?

So there's maybe even a realism of philosophy branch, and then even maybe on the applied side, like it's a philosophical debate.

Is the cell doing active inference, doing free energy principle?

Is it realizing certain constraints of nature?

Or how would we know what an ant is quote, doing free energy minimization in its brain?

And then on the engineering side,

It could be argued that if a robot is implementing active inference code, that it is actually doing active inference.

So this question about designing systems within the active inference framework, so instead of saying, I think all internet networks are active inference systems, it's definitely a claim somebody could make, interesting claim, but it's quite another thing to design a computer network that is within an ontology of active inference.

then it would be fair to say that that network is, quote, doing active inference.

So that's like realism.

And maybe for some, it's more of an engineering realism.

Others, it's a philosophical realism.

Another implication and bigger question that arises in the paper is about temporally deep inference.

So they write, this distinction may speak to a straightforward extension of the treatment on offer from simply inferring an external state to inferring the trajectories of external states.

So again, moving from not just the point estimate, current estimate of external state, that was what was presented earlier, to inferring the trajectories of external states, which is to say decomposing their higher orders of movement.

This may be achieved by representing the external process in terms of its higher orders of motion by solving the stochastic realization problem.

By repeating the analysis above, internal states may be seen as inferring the position, velocity, Jurek, etc.

of the external process consistently with temporally deep inference in the sense of a Bayesian filter, a special case of which is the extended Kalman-Busi filter.

So these are cool things to talk about.

We could talk about what is temporarily deep active inference, what

are the ways that the model deals with internal and external categories and discrete variables.

How do we deal with the inference on external states and how does that relate to our action selection?

And then what happens when there's a feedback or some other type of interaction with the niche, the informational niche, the social niche, the stigmatic niche.

So, or other questions people can write.

Well, that was

a sort of quick run through the paper.

And I'm not sure if I got each section 100% correct, so it will be good to have the author's perspective on a lot of pieces as well as a lot of other experts who I'm sure could have said a ton more and better about some of the math.

But we can offer up the same questions that we have for any paper, like, what might a good understanding enable?

what are the unique predictions and implications and the unique developments of this paper what are some of the next steps for free energy principle and active inference research what are the goals of this research and then also to the participants and the authors what are they still curious about learning so uh

Thanks for participating.

It was a fun 26.

And we hope that you'll join for 26.1 and 26.2 if you can or comment on the videos or whatever.

And thanks.

Talk to you later.

Bye.
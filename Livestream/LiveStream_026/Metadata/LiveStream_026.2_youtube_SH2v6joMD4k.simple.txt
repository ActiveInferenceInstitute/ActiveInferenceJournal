SPEAKER_00:
Hello everyone.

Welcome to the Active Inference Lab.

We're here on August 17th, 2021 in live stream number 26.2 on the second discussion that we're having as a group on the paper Bayesian Mechanics for Stationary Processes.

Welcome to the Active Inference Lab, everyone.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links here on this page.

This is recorded in an archived livestream, so please provide us with feedback so that we can improve on our work.

All backgrounds and perspectives are welcome here, and we'll be following video etiquette for livestreams, so we can use the raise hand feature in Jitsi.

This short link has the upcoming live stream discussions.

Here we are on August 17th in our last discussion on this paper, 26.2.

Next week, we'll be heading into a new paper and we still have some openings in the coming weeks.

So if anyone has a suggestion or an author to contact or wants to discuss their own paper, just let us know and we can try to make that happen.

Today in Act Imp Stream number 26.2, the goal is to continue learning and discussing and unpacking this awesome paper that we've been thinking about for the last few weeks, Bayesian Mechanics for Stationary Processes.

And we're really appreciative that the first author is here to join with us and co-learn with us, have fun with us.

and this video is just going to be wherever we want to take it so we have some questions prepared but especially if anyone in the live chat has questions that'd be awesome and we can definitely address them so we can just start with a quick introduction round and feel free just to say hello or to

answer one of these questions like what's something that you liked or remembered about the paper and what's something that you're wondering about or want to have resolved by the end of today.

So I'm Daniel.

I'm a postdoctoral researcher in California and something that over the last week made me excited to have this discussion was I was speaking with a physicist friend, someone who's

working with lasers and oscilloscopes and stuff.

And I brought up this paper and it turned out that they actually were using PID control in their research on a day-to-day basis.

But actually it was new for them to hear about how it was connected to some of these broader topics.

But as soon as I brought out the physical printed paper, then they were just so excited.

And it was cool to see how they had all this practical insight

experience working with pid and then they instantly just saw that there was kind of this crossover point into some broader questions that also they were curious about but the day-to-day practice actually hadn't brought them to those general questions so i learned a lot about pid control and about actually how it plays out with the knobs and everything like that i'll pass to dean


SPEAKER_02:
Yeah, good morning.

Yeah, I came away from the paper, again, trying to figure out how to create some sort of a coherent narrative around the physical math and the statistical math.

That part for me really got me going away and thinking a little bit because I think they go in different directions, but I do think that they can work in concert.

So

Yeah, I'd be really interested to see how that might play out in terms of what our guest here thinks the directions this might be able to go.

I'll pass it to Stephen.


SPEAKER_04:
Hello, I'm Stephen.

I'm based in Toronto.

I work a lot with community development and sort of spatial meaning-making, so inferences of different kinds.

I'm curious how some of the ways that the mathematics have been done on this paper can help broaden some of the questions around ergodicity and non-ergodicity in science.

that's a bit of a thorn, sometimes a weapon in the inactivist, active influence dialogue.

So yeah, and that's something I'd be interested in.

And I'm going to pass this over to Dave.


SPEAKER_00:
So I think Dave doesn't have audio, but Dave, thanks for joining and any questions in the YouTube live chat is awesome.

So thanks to everyone who's joined and pass to Lance.


SPEAKER_03:
Thanks, Daniel.

And thanks for inviting me again.

So I'm Lance.

I'm a PhD student at Imperial and UCL.

And I'm really excited to be here and to engage in this discussion.

I'm really looking forward to feedback about the paper, seeing what's interesting to pursue in future work.

So yeah, really looking forward to share this paper with you and talk about it.


SPEAKER_00:
So maybe one lead-in question before we jump to the content.

What stage in your PhD are you?

And is this like the meat and potatoes chapter one?

Or is this a little cul-de-sac that you found yourself having to explore in the course of another research project?


SPEAKER_03:
Yeah, great question.

So I'm right in the middle of my PhD.

I'm two years in.

I still have two years to go.

And ideally, I would like to pursue this work.

So I have two supervisors.

One is a mathematician.

Another one is a neuroscientist.

And so really the goal is to have some kind of mathematical theory of how adaptive systems work.

So I think this is really the first chapter.

I want to push that further.

But then there are also other projects that take up my time.

So I'm trying to make all that work together.


SPEAKER_00:
Awesome.

So for this dot two, we have some questions prepared, some topics that we know we want to talk about, but anyone who asks a question in a live chat will definitely have time to address that.

The paper that we've been talking about is this Bayesian Mechanics for Stationary Processes paper.

And in the dot zero and the dot one, there's more information on the primary aims and claims.

So check those out if you haven't already.

We're just going to take one more look at the roadmap, where we've been, where we're going,

Remember, we were talking about Markov blankets and Bayesian mechanics.

How does active inference have similarities and differences from other ways to model dynamical systems?

And let's just go straight to the first question that we have, which is gonna help us review where we've been, and then also as a jumping off point for some of these extended questions that we've already written.

So a colleague asked me,

and said, I think it would be good to expand on the differences of the paradigms in section three.

And those are section B, C, and D.

related to a posteriori estimation after the fact, predictive processing, and variational base.

So perhaps Lance first, and then anyone else who wants to give a take or ask a follow-up question.

What are the similarities and the differences here, and why were they structured in this order?

Is this the chronological order?

Is this most specific to least specific?

Or what is the ordering and the meaning here?


SPEAKER_03:
Right.

Yeah, so it goes from the simplest to the most sophisticated.

So a posteriori estimation is a special case of predictive processing, which is a special case of variational Bayes.

And so the idea is if you receive some piece of data, so for example, if you see an image,

with your eyes, you're gonna try to infer what's going on outside in the environment.

And so if you do a posterior estimation, what you're trying to guess is what is the most likely thing, like what is the most likely thing that's going on in the environment?

So it could be if you see an image of a chair, you would say, oh, it's a chair.

But you wouldn't encode any kind of uncertainty.

Oh, it might be like an armchair, for example, or something a bit different.

So in a posterior estimation, you try to guess what's going on outside, but there's no uncertainty.

Then predictive processing, you encode the uncertainty.

So mathematically, that's called the variance or the precision.

And then variational Bayes, it's more sophisticated.

So you actually encode an arbitrary distribution

on external states so it could be oh it's very likely that it's an armchair and it might also be very likely that it's a cat so that this example doesn't make sense but you you could imagine that there's two things that are very different that could be equally likely so for example if you see an illusion

It could be something, but also something very different.

So you would imagine these two things are having a high probability of being there, and then other things are having low probability.

So this is really the most general thing.

And also as a side comment, so variational Bayes, like a lot of machine learning algorithms, you can see them as special cases of variational Bayes.

So I think this is really the most general way to think about inference.

And this is really at the heart of the free energy principle.

And I think this is why people have been describing the brain as performing variational base, because it's really the most general way to think about this kind of problem of inference.


SPEAKER_00:
Awesome.

Well, before I give a comment on that, what made you say that it was at the heart of the free energy principle?


SPEAKER_03:
Right.

So the free energy principle says that the brain is doing variational base.

And then all this work, I mean, this paper in particular, but like the last decade, people have been trying to justify why would the free energy principle apply actually?

And so this is part of a, this paper is part of a line of work where we try to examine physical systems or like adaptive systems and see whether we can describe them as performing variational base.


SPEAKER_00:
Great, so first Steven with a question and then anyone else with a question.


SPEAKER_04:
Thanks, that's really helpful, that explanation.

I was wondering, is it a case then that you can go only so far with, say, predictive processing, and then to get to variational bays and to bring in that uncertainty, you kind of need a free energy principle solution to be able to bring in these more unknown models.

And to go beyond variational bays at some point, that's when you'd have to bring in the active inference

Would that be the way you would think about it?


SPEAKER_03:
Yeah, I would think about it like this.

I would say, loosely speaking, the term predictive processing I think is also meant to refer to variational base, but at least a variational base that's a proper mathematical definition of what it is.

And then predictive processing, it's more like, oh, we're predicting something.

So it's not that clear how.

I mean, there's no clear definition of predictive processing, but in this paper, predictive processing is meant as predictive coding, as in the papers by Rao and Ballard, where they looked at the visual system and then they looked at neurons in the cortex as forming predictions and suppressing prediction errors.

So yeah, if you only do predictive processing in this sense, I think you're limited.

And variational Bayes and active inference is really like the next generation because you can accommodate all sorts of models.

And yeah, I don't think it, I mean, I cannot see at least how you would go beyond that.


SPEAKER_00:
Thank you.

So Dean with a raised hand and then anyone else?


SPEAKER_02:
Yeah, Lance, I was curious.

So it seems like the math is able to help us understand the difference between something that's really highly defined, so that kind of a posteriori, and then sort of the fuzzy, where the variational base part sort of gives us something a little less defined in terms of the edges.

And so I was just curious,

Does the brain kind of go back and forth to some degree, mathematically speaking, between that which is really clear and really has a hard edge mathematically and those things which statistically seem to distribute within the parameter set?


SPEAKER_03:
Yeah, I mean, I think all these three, like a posterior estimation as well as variational base, there's plausible explanations for how the brain would implement them.

And so the idea for variational-based or predictive processing is that the brain would be encoding some kind of uncertainty.

And so in the literature, people say that the mean firing rate of a neural population would encode the posterior estimate, so what you think is actually out there.

And then the variance of the firing rates would encode your uncertainty.

So I think there's plausible theories for how the brain would implement all of these things.

And then I think that's the nice thing, at least with active inference, is that we try to develop models that are consistent with our understanding of the brain.

And as our understanding of the brain evolves, we kind of like refine these models.

So I think we're definitely working towards models that do explain, at least so far it's very coarse-grained and very simplistic, but we're working towards models that could be implemented in the brain.

So I think these very abstract, very general form of variational Bayes could be implemented in the brain.


SPEAKER_00:
One quick follow-up is you mentioned how the mean firing rate corresponds to this first a posteriority estimation, which is a mean estimation, and then there's the variance in the firing rate.

So what measurements or what does it look like to implement variational Bayes in the brain?

Like what measurements are we making?

What kinds of analysis would we make on those just like mean and variance?


SPEAKER_03:
Well, so the idea, I mean, the very simple idea is that you would get some sensory stimulus.

So you get some impression on your retina.

some neurons in your retina which fire into the visual cortex.

And then the premise is that your brain has embodied a generative model of its environment.

So it has learned how to recognize, how to infer things in the environment from its sensations.

And so the stimulus that would go into V1 and all these other parts of visual cortex, and then the neurons in the visual cortex would align their mean firing rates with respect to, oh, it's an armchair, or it's something different.

So maybe there's a neural population that encodes armchair, and there's a neural population that encodes something else.

There's also a lot of studies for specific neurons, like one neuron encoding one particular stimulus.

So there's these guys at MIT that show that there's actually a Bill Clinton neuron that fires whenever you see Bill Clinton.

So you can be very specific.

And yeah, so if it's uncertain that it's Bill Clinton, your Bill Clinton neuron would fire.

Yeah, I guess we'd have a lot of variance in its firing rate.


SPEAKER_00:
Okay, awesome.

So welcome, Blue, with a raised hand.

Go for it.

And then we'll continue, and also Stephen.

So go ahead, Blue.


UNKNOWN:
Hi.


SPEAKER_05:
I am Blue Knight, and I'm an independent research consultant from New Mexico.

And I have a question on your paper, and it's kind of something that I've been questioning a lot lately with regards to the free energy principle and the requirement for ergodicity.

And my question is, in your paper, you said that the, you know, the outline, even in the outline, it says, you know, you partition the world with the Markov blanket, and then you equip the partition with stochastic dynamics.

So I'm wondering how these stochastic dynamics relate to ergodicity, if at all, and if that fulfills the requirement for ergodicity as necessary by then.


SPEAKER_03:
Right.

Yeah, that's a great question.

Yeah, I think Stephen also wanted to talk about that.

So it's great that you bring this up.

So the idea of these stochastic dynamics is that we don't assume that dynamics are deterministic.

And it makes sense because you can describe planets as having deterministic dynamics.

But when you talk about biological systems,

things are a lot more messy.

And if you're not modeling them at the molecular level, which you never are, things are stochastic.

So then the assumption on our system is that it's stationary.

And this is really what it means to be adaptive.

So you're preserving some kind of steady state.

But we never assume ergodicity.

So there's actually a deep... So stationarity and ergodicity are not the same.

So we assume stationarity, but we don't assume ergodicity.

And so if, Daniel, you could go to Appendix B,

There's a figure I think which would be the best way to illustrate a bit below.

Yeah, perfect.

So here in this appendix we illustrate some dynamics that are ergodic and some dynamics that are not ergodic.

If you look at the top left and bottom left panels, these dynamics are ergodic.

And so I guess the intuitive definition of ergodicity is that here you have the steady state in blue.

So regions that are very blue are regions of high probability mass under the steady state.

So regions where your dynamic is likely to be in and regions that are less blue, you're less likely to be in.

now ergodicity means that if you start your dynamic anywhere and you and you let it run for a sufficient amount of time so like you observe your biological system for a very long time um your dynamic would basically sample the whole distribution so it would go i mean if you if you give it enough time it would go really far away and then it will come back and it will do all sorts of things as you can see in the bottom left and bottom right

So this is ergodic.

Now in the top left and bottom left, it's ergodic.

And then the bottom right, it's not ergodic.

And so as you can see, if you start the dynamic on the bottom right anywhere, it will go around an orbit.

They won't sample the whole distribution.

So it's still confined to a small region.

And so the paper still applies for these kind of dynamics.

So yeah, I don't know.

Does that help?

Or was that not very clear?


SPEAKER_05:
So to follow up, the paper does apply in situations where it's not ergodic.

That's what you said, right?


SPEAKER_03:
Right.

Yeah, we don't need ergodicity.


SPEAKER_05:
Well, so even within a steady state,

Um, like, so biological systems, you know, operate within homeostasis, homeostasis, but it's a range, right?

So, so it's not like a constant, it's not like, you know, purely we're not at 37 degrees all the time, right?

Like, so there's some fluctuation all the time within that steady state.

And, and so I just was wondering kind of how you rectify those concepts, like, is it like a restricting of the state space or, or how does that work?


SPEAKER_03:
Um, so, so the, this idea of homeostasis, that's, that's really the, the steady state assumption.

So for example, um, in the state space, maybe you would have one, one, uh, one coordinate that describes the temperature of your, of your biological system.

And if it's a human, um, then it would have regions of very high probability.

So here regions in blue around 37 degrees, and then regions that are less blue

like a bit further away.

And so the dynamic, I mean, your system can evolve in all of these regions.

Just like on average, it will be more often around 37 degrees, just because that's how we are.

But sometimes it will be higher and sometimes it will be lower.

And so this is all allowed.

And so I guess this ergodicity assumption, so then if you assumed that the biological system was ergodic,

then this would imply that if you leave the biological system run for a sufficiently long time, like you reserve it for a very long time, it will basically sample your whole distribution of temperatures.

So maybe...

Um, so, so maybe like the, the temperature that, um, a human body is allowed to have are between like 36 and 39, let's just say for simplicity.

And so ergodicity would say, um, yeah, if I, if I observed my biological system for a sufficiently long time, um, I would see my biological system at all these temperatures.

And I think maybe.

maybe that would happen.

So maybe temperature is like we're ergodic with respect to temperature.

But maybe there's other things in which we aren't ergodic.

And I think the previous papers on the principle, for simplicity, they always assumed some kind of dynamic, and it turned out to be ergodic.

And so in here, we're not making that assumption.


SPEAKER_05:
Sorry, I'm going to just follow up with one more question, Daniel.

Is that okay?

Yeah.

So, you know, it's great, right?

If you're talking about one human system, right?

Like my Markov blanket, the partitions, like internal and external states.

And so I have my one human system or even multiple human systems.

Like we will operate in the same kind of steady state based range.

But if we're talking really about, you know, collections and how kind of

collective dynamics and overlapping Markov blankets.

Do you think it's mathematically possible to extend the state space?

Like I can operate, you know, from 35th to 39th.

That's an okay temperature for me, but like a reptile has a very different kind of temperature range, but clearly like we have some interaction.

I have interaction with the reptile.

And so there's some, some emergent dynamic between me and some reptile feature.

Right.

Right.

Like, so,

can we define a state space, would it be mathematically acceptable, or is it offensive, mathematically impossible to define a state space where I'm allowed to sample all of these temperature ranges with my markup?

If it's some other feature in some kind of collective dynamic, it's not, they have a different range.

Is that still organicity?

Because it's not, clearly, I'm not sampling the entire state space, I'm sampling my state space, but is that,

confounding in some way?

I mean, it is in my brain, that's why I'm asking.


SPEAKER_03:
Yeah, that's a really good question.

So this is all fine.

So we can do everything you just said.

And so when we, so actually when we say, so we always sample a distribution.

We're not, I mean, and then the distribution is on the state space.

So we're sampling the state space.

But if your distribution is confined to a region of state space, so let's say as a human, I cannot possibly have any bodily temperatures below 36 degrees.

then you would sample only things that are above 36 degrees.

So I think you would have your own steady state, your own stationary distribution with temperatures around 37 degrees.

And then if there's a snake also in this environment that you're modeling, then we'll have another very different steady state.

And so I think your question really speaks to the possible extensions of this work.

Because in this paper, we just say, oh, there's an internal state space, which would be what's inside.

And then there's an external state space, which would be what's outside.

but then really what people would need to do now is to say oh but my actually there's a lot of very interesting things in my environment there may be snakes there may be other kinds of animals and they all have their own mark of blanket and they all have their own steady state so then what you can do is to partition your external state space into all of these things that might be around you

And then instead of performing some kind of simple inference, as we have in this paper, you would have some more structured inference.

Because part of your inference would be about what's just in front of you.

Part of your inference would be about your dog that's right over there.

And you would have a lot of, by carving your external state space into different things with their own steady state,

you would actually have an inference that's much more structured because you would infer all these things, all these different things that are in your environment.

So this can all be accommodated in this paper, but it would definitely need another paper to make that explicit and say some really interesting things because, yeah, this is really the way to go.

But the mathematics doesn't need extension.

It's just like...

Yeah, it would just be like partition the state space, and then we can have all that you spoke about.


SPEAKER_05:
That's really helpful.

Thank you.


SPEAKER_00:
Great.

And also one piece of then Stephen's question is, it's not as if something being ergodic or not is equivalent to it being easy or simple or tractable.

Like looking at these distributions or trajectories here, the one that you characterized as non-ergodic, not sampling basically across the state space, the bottom right, this one is the one that in the caption, it says,

Purely conservative dynamics, lower right panel, are reminiscent of the trajectories of massive bodies, whose random fluctuations are negligible.

So, it's actually not like it's, um, maybe this will speak to Stephen's questions about what is a thorn and what's a rose, because this is a non-ergotic system that's massively predictable.

You can do it with a gear.

So maybe that relates to some of these criticisms about, well, we're only working with a limited set of ergodic processes.

It's like, okay, but that's the squiggly one that's dancing everywhere, the stochastic process that we care about.

Whereas including non-ergodic processes, some of these non-ergodic processes might just be going around in an oval.

So it won't be a challenge for the math at all.

Something that works on a stochastic squiggle is going to have actually an easier time on this oval one.

So, Stephen, and then anyone else with a question?


SPEAKER_04:
Yeah, thanks.

Yeah, I suppose actually tying into what you just said there, Daniel, that there's this question with...

how much we need to be able to sample over big state spaces.

So it might well be that, I don't know, the chemicals of a cell with molecules vibrating millions of times a second, and there may be a quick...

ongoing way to sort of get a feel for the ergodic state space now if you watch a cricket ball come towards you um you know you don't get that chance to do that so but you might want to sort of maintain a trajectory some sort of trajectory um that can still be um tractable i suppose so i think this is this is actually quite quite useful

I suppose the question then comes in is what's the sort of in-betweens?

Where does approximation science come in more and more to allow us to sort of work with not knowing and work with something like these solenoidal flows in a more fuzzy way?

Because I'm assuming that if you've got less chances to sample your state space,


SPEAKER_03:
solenoidal might be more useful to get towards something tractable or maybe i'm wrong but i wonder what your thoughts are on that right um so actually solenoidal um i mean so in in this figure here uh on the bottom left you have like no solenoidal flow and then the bottom right there's just solenoidal flow um so i would say

All of these two very special cases, they don't look like biological systems at all.

And they're also the simplest ones to understand mathematically and to study mathematically.

And actually when it gets really interesting and when it looks a lot more like a biological system is on the top left.

And that's harder to assess mathematically.

And so as you can see, it's quite, I mean, I think the dynamic on the top left is quite nice because on the one hand you have dissipation, like you have some squiggly lines, you have randomness.

on the other hand thanks to the solenoidal flow you have some kind of exploration and also some kind of like circular patterns so for example in um and this is actually how you characterize what's called a non-equilibrium steady state but without without going into the details um as humans as and as animals we have a lot of circular patterns that are not purely deterministic so for example we all have a circadian rhythm

And we all have all sorts of other circular patterns.

And I think to account for that, you really need to be on the top left.

So I think the, I mean, one of the next steps to this research is right now we just assumed that the dynamic is stationary.

And so all this applies to like a humongous range of dynamics.

And a lot of them don't look like biological systems at all.

And I think the next step would be to restrict ourselves to dynamics that look a lot more like biological systems.

and then see what else can we say.

And I bet there's some dynamics that look a lot like very intelligent systems, and some that also look like maybe lifelike systems, but they are not so intelligent.

So then the question is, what kind of parameters, what kind of dynamics do you need to characterize something that explores its environment, something that makes decisions, and all sorts of things?

yeah um and and that's going to be a challenge and also i mean just speaking about biological systems i think biological systems have a lot of solenoidal flow and they do have dissipation but they don't have so much dissipation so you would have something like the top left but with like huge yeah huge like a lot of exploration like you would go a lot further than that in your state space

And then you would have a bit of, I mean, you would have a lot of squiggly lines, but it would be, yeah, there would be a lot more like exploration than there is dissertation.


SPEAKER_00:
Yes, Stephen, go for it.


SPEAKER_04:
Yeah, and you have both can combine, can't so gradient and solenoidal flow can kind of be playing at the same time.

I'm wondering if someone was to bring attention to a goal or a type of attention, maybe a more pragmatic attention or an epistemic attention or a risk averse attention.

whether it might be that rather than trying to deal with the whole state space, maybe that general process carries on, but it tries to introduce a solenoid or...

you know, type of flow, which in some ways you could imagine being more tractable to a biological system if it's like bioelectricity, you know, the types of dynamics which maybe are more tractable around a certain trajectory than some.

So you've seen these dynamics could combine in different regimes of attention.


SPEAKER_03:
Yeah, yeah, definitely.

And I think, but I think to properly account for attention, we need to go beyond these continuous state spaces and do what people, and basically extend to these standard models of active inference, where in addition to have some continuous state space, maybe like for movements and this kind of thing, you also have a discrete state space.

And then you would have a dynamic evolving in your discrete state space.

And maybe some state would be, oh, I'm attending over there.

And some other state would be, I'm attending over there.

So I think attention is really something that can be modeled with a discrete state space.

And then it would, I don't think it would be so hard.

I mean, at least seems doable to extend all the math here to like models.

Yeah, that have some like continuous state space, maybe for movement or other kinds of things.

Like maybe like blood glucose concentration is also something that evolves on a continuous scale.

But there's also a lot of things in our mind that are discrete.

So is this red?

Yes or no?

Is it raining?

Yes or no?

And to account for all that, we need a discrete state space as well.


SPEAKER_00:
Awesome.

Thank you, Lance.

Welcome, Scott.

I see your hand raised, so go for the question, and then anyone else with a raised hand.


SPEAKER_01:
Thank you.

I came in a little late.

My apologies.

This is fascinating.

The solenoidal flow, when that gets set up,

Is it possible that he had multiple state spaces and might, because of the solenoidal flow, might it have a periodicity in each of the state spaces so that you might be able to use Fourier transforms to unpack multiple state spaces in a biological system?

So could you start having that periodicity?

Does that allow for a Fourier type analysis to start to unpack state spaces potentially when you have multiple state spaces and you're trying to distinguish them?


SPEAKER_03:
Yeah, that's a really good point.

So I know a paper actually that analyzes the Cernodo flow in terms of the Fourier transform.

I haven't really thought about it.

I think, I mean, if you get rid of the dissipative part, if you get rid of the squiggly lines and you're just in the bottom right, then you can definitely use Fourier transforms.

But then also the dynamic on the bottom right is not representative of biological systems.

So then what you can do when you're on the top left, which is really the interesting case,

is analyze the time-irreversible part, so the sonoidal flow with the Fourier transform.

And then, so you do this Hemp-Holmes decomposition that we're talking about in the appendix.

You decompose your dynamic into, yeah, so it's the equation A2.

Yeah, yeah, perfect.

So you decompose your...

your drift B, so B is the direction in which your dynamic is moving, on average, into reversible and irreversible parts.

The irreversible part is the solenoidal flow, which you can analyze using Fourier transform.

But then you need to figure out how to accommodate the reversible part with it.

So I can share the paper if you're interested.

But yeah, it seems...


SPEAKER_00:
like they're they're able to to like do do some things with that technique uh but it's also pretty pretty involved to be honest a question on the time reversible and irreversible what does that mean and in a world where it seems like we're moving forward through time only what exactly does that mean that there's something that's invariant under time reversible

versus something that changes sign under time reversal?

Like if we can't get in the time machine or put the process in a time machine, why does it matter that it's time reversible or how does that fall out of the equations?


SPEAKER_03:
Right.

Well, yeah, so all this is really to emphasize that we're not assuming the dynamic to be time reversible.

And as you say, everything in the world around us, I mean, most things are time irreversible.

So if you go back to the figure just below, so the movement of planets is really something that's irreversible.

I mean, the Earth turns around the sun in one direction.

And if you are to rewind time,

it will turn around the sun in the other direction.

So in this case, you can really tell the difference between as time goes forward and as time goes backward.

And that's really what's meant by time irreversible.

But now, if you look at a cup of coffee and the coffee is still, the molecules, the coffee molecules or whatever molecules you have in that cup, they're still bouncing with each other.

And that's something that's time reversible because if you look at the cup of coffee, if I give you a movie of the cup of coffee sitting still,

You can watch the movie as time goes forward or backward and you wouldn't be able to tell the difference.

So that's something that's reversible.

And so as you can see from this example, actually things that are reversible aren't interesting at all from at least a biological perspective.

And there's so much literature about, yeah, about one of the, I mean, one of the fundamental things of being alive

is being time irreversible it's about like eating and producing waste um it's about yeah consuming energy all that kind of thing all that kind of stuff um produces entropy and is time irreversible um but then time so time irreversibility it's something that's necessary to be alive but it's not it's not sufficient um so for example if i give you

a convection cell that would be time irreversible but it won't be alive so any kind of cycle that's more or less periodic so maybe if you see like a cyclone a cyclone is time irreversible because it's like spinning one way and not the other so if you run out of time it would be spinning the other way so it's different so it's time irreversible but that's not alive

Um, and so there's many, many things like that time irreversibility, um, yeah, creation of entropy.

Um, there's, there's a few things like that, that characterize biological systems.

And I think as we, as we go on with this work, we'll, we'll try to like specialize to only those dynamics that satisfy all these requirements and eventually things like, oh, we need systems that are able to reproduce, for example.

like a Markov blanket that is able to divide into two Markov blankets.

For example, when a cell decomposes into two cells.

So this is really the beginning.

And yeah, in the next years, I think this community is really going to be able to address all these challenges and all these characterizations of living systems to gradually evolve towards models that are

Yeah, more realistic, I would say.


SPEAKER_00:
Awesome.

So I'll have more to say on it, but Scott, go for it.


SPEAKER_01:
So a couple of things.

There's a book called Life's Ratchet, which is kind of interesting just on that idea of that irreversibility and that value of that.

The idea is that life kind of ratchets in these forms that work with the externality.

I haven't revisited that since I've been exposed to active inference, but

It might be interesting to take another look at life's ratchet.

Also, one of the things I like that discussion of living forms and what's necessary prerequisites to living forms.

One of the working definitions I'll share that I'm using now, and I think, let me tell you why I'm using it, is that life is autocatalytic and entropy secreting.

Now, my kids claim, I just wanted to say secretion in my definition because it's gushy.

That was what my kids used to say.

But autocatalytic and entropy secreting.

So notice I didn't say it had to be embodied in a specific form or specific matrix or context.

It could be a market.

It could be a Markov, you know, a lot of different things.

So the reason I share that here, it's interesting because active inference, we have this definition of life like Carolus Linnaeus.

You look at it and you see if it has feathers and you see if it has blood and stuff like that.

But now we're going towards mathematical definitions of living things.

And that's going to be very interesting because I think when we have a working definition that works for biologic forms.

So, again, my working definition right now is autocatalytic and entropy secreting.

Then I can start to say, OK, if that worked for biologic forms, let's say that did work and maybe there's problems with whatever.

But if it works, then you can start to say, OK, what other systems happen to be autocatalytic and entropy secreting?

And therefore, we shall now call living.

And maybe we can start to look at them and make observations about those systems.

And what I'm thinking about, I'm a guy who's a lawyer and in finance for years.

So I'm looking at this and saying, OK, you look at these systems, biological systems came up with these markets and these other systems.

in systems that we have to do things in the world maybe these systems are describable with biological framings and so that's why the kind of work you're doing feels like as it starts to reveal some more general definitions of living forms taking those definitions and then reapplying them back out into the world and saying okay

Are there living forms that have escaped our detection as living forms?

And basically, is there scale independence?

Are there fractal elements?

What are the things that we can now describe in those things that we previously had not thought of as biology, but in fact are describable as biology?

That's very exciting to me.

So I just wanted to say that the kind of things you were describing there and that kind of working way back down into a model of life based on time irreversibility is very interesting, I think.

Thanks, and a very interesting paper overall.

I'm looking forward to diving into this and the other paper you were mentioning also.

Thank you.


SPEAKER_03:
Cool, yeah.

Yeah, thanks a lot.


SPEAKER_00:
That's really great.

So to tie this from some of the physical examples we've been talking about, like the diffusion processes to the physics of information and information flows,

The big thing we've been talking about is this statistical physics and this Bayesian mechanics.

And we heard a little bit earlier how we could talk about the moments.

Like the first moment, the simplest form, the a posteriori estimation was the mean.

And then the second moment of a statistical distribution is the variance.

And there's also higher statistical moments that describe higher and higher aspects of an informational distribution.

there's also the concept in physics of moments like the position and the velocity and higher and higher physical moments.

And so the mapping is that potentially these physical moments that would describe components of a physical flow that could also be decomposed using the Helmholtz decomposition might also apply to some of these informational

flows and decompositions.

So we could use those analogies like the coffee mixing and the steady state of the coffee, and then think about how a statistical distribution is playing by similar rules.

And so it's also interesting, Scott, about instead of going from the non-living systems and then looking for that pattern and pattern matching to biological systems, we can kind of start up on the top left with, or something even more biologically fleshed out, and then look for those patterns and pattern match outside of systems that have a vertebra and a central nervous system, because that shouldn't be the...

the card to get into the club of information processing systems, or entropy secreting systems.

So any thoughts?


SPEAKER_03:
Yeah, I mean, yeah, I think I think that's, that's really interesting.

I mean, the real challenge here is to is to make the link between the physics and the information.

And

The more I've been working on this, the more I realized that actually a lot of physics, or at least a lot of statistical physics, you can explain with properties of information.

So for example, entropy increasing is also information being lost.

Not stochasticity.

Stochasticity, so the inability to predict the future exactly,

is exactly the same as entropy being lost over time.

Information being lost over time and entropy increasing.

So I think there's...

there's very deep connections between information, physics, inference, and some of them have been fleshed out, but it hasn't really been put together, at least as far as I know, in this context, in the context of Markov blankets, in the context of how can we have a theory of how adaptive systems work,

can we describe adaptive systems as performing inference?

So I think this is really exciting.

And also, as you can see from this paper, so the dynamics that we're considering, they're extremely general.

And for these dynamics, for all of them, we're able to derive some kind of inference.

So this is to say that this inferential description, it applies to many systems.

And so it means that on the one hand, it's very ubiquitous, but then on the other hand, it's not that useful because maybe the things that didn't look at all like biological systems can still satisfy this kind of free energy principle or Bayesian mechanics.

So I think what's really exciting is to be able to, in the future, to specialize to those dynamics that look more like biological systems and then really look at these informational properties.

And I think a big one is how much exploration do we have?

And exploration is really something, exploration and curiosity, it's really something that characterize intelligent systems as we know it.

And yeah, so there's some discussions in the literature of how we would be able to use things like information length and concepts for information theory and information geometry to quantify things like memory and itinerancy, curiosity, and so on.


SPEAKER_00:
So one question from the live chat, before we continue.

The question is, what is the explicit definition of information flow?

And also maybe in that, if you want to describe a little bit about this other paper, like what is information flow?

What is information geometry?

And how does it relate to what we're talking about here?


SPEAKER_03:
Right.

So this is all the idea of...

As an organism, you're always encoding some beliefs about your environment.

So we talked about maximum a posteriori and variational inference.

So the idea is that we're always receiving sensations and then making some inferences about the outside.

Now, if you're making some inferences about the outside, implicitly or mathematically, you're parameterizing a distribution about things in the environment.

And so information geometry then comes into play.

Information geometry quantifies the extent to which beliefs about the environment are different from each other.

So it doesn't make sense.

So for example, let's say that

I think Daniel's eyes are blue.

Let's just say.

And then I have some uncertainty around the real color.

So that would be like one kind of belief I could hold.

And then another type of belief would be Daniel's eyes are brown.

And then I would have some other kind of uncertainty.

Now, a question you could ask me is, how different are these two beliefs?

And if I had a different belief about the color of his eyes, would that belief be more different than the beliefs I initially had?

So then information geometry enables you to answer these questions.

It enables you to say how two beliefs are different from each other by quantifying the distance between them.

but also even more interestingly as i um let's say i i've never met daniel before like let's say i haven't seen his picture before now you give me a picture of daniel um i'm gonna make an inference about the color of his eyes so from being initially completely agnostic

I'm going to move my completely agnostic belief to a belief that says, oh, maybe he has brown eyes.

So this is to say that as we make inferences and sample the world, our beliefs evolve.

And so information geometry enables you to quantify how much your beliefs evolved and what kind of trajectory did they take.

Because, I mean, I'm assuming at least our beliefs, they evolve in a continuous manner as we sample more information.

And then it becomes interesting to see what kind of trajectories did my beliefs take and was that trajectory efficient?

If it wasn't efficient, maybe I lost some energy there that I could have used somewhere else.

So my inference wasn't very optimal from that perspective.

And then there's a paper about this actually on active inference that uses information geometry to see whether inferences in active inference are efficient from a metabolic point of view.

So that's something that you can look at with information geometry.


SPEAKER_00:
Thanks.

Just to follow up from the chat, and then we'll return to our colleagues here.

Do we have, or more precisely, do we hope to have a definition like an electromagnetic flux in field theory for information flows, information flux?


SPEAKER_03:
Yes.

Yes.

So information flux is the movement of beliefs.

And so you can write that down mathematically.

So, yeah, I mean, a belief is a probability distribution about things that are going on in the environment.

And as you sample information, you would change probability distribution.

So it would be like a motion on the space of probability distributions.

That's really the meaning of information flow.

But maybe the question was about, can we derive some kind of like Maxwell's equation, some kind of law about how...

about how information flow happens, like maybe an equation for information flow.

And I would say that that would be the aim.

That would be what people are working towards in theoretical neuroscience, like some kind of equation that explains how we make inference.


SPEAKER_00:
Awesome.

So a few raised hands.

Scott first, then Blue, then Steven.

So go ahead, Scott.


SPEAKER_01:
A couple of things.

That was fascinating little bits there.

So on that last piece on the information plus two points, the embodiment, I'm thinking of embodiment of information.

So I'm thinking of a farmer that's deciding what crops to plant.

So they look and they get information from the newspaper and they look at weather reports and they gather information.

Then they decide on what seeds to plant.

The seed itself is an embodiment of information.

They put the seed in the ground.

They get results with their farming.

The markets then respond.

So there are seeds as information, farmers processing information, markets processing information,

And one of the things that's kind of interesting here is a lot of the work I'm doing now is in food chains and trying to figure out how to de-risk food chains and just food generally.

And that efficiency question you just asked is really interesting.

What are the losses and conversions that happen when information is embodied in those different

aspects of that decision process through which something gets converted from a need of society to have a certain amount of corn at a certain time of year to through all those chains of seeds and selections and farmers and processing it's kind of interesting to look at this system as both each piece of it is living the seed is arguably living although there's a question of whether seeds are living among people who are biologists but anyway and then because they're not actively doing stuff

but they're kind of stasis.

But anyway, that embodiment question, that's one.

And let me ask, I'll ask both and then turn off my voice here.

So the embodiment question is one kind of, that notion you just talked about, about an overall formula, maybe there's an overall embodiment characterization that's made possible using those landscapes and looking at kind of a multi,

faceted landscape and looking at those conversions that happen from one embodiment of information to another and efficiencies there.

That's an interesting kind of possible thought.

The other one is you said something about curiosity before, maximum exploration, maximum curiosity.

That really struck me because one of the things we've been advocating for, I did a lot of cybersecurity work a number of years ago, and people were coming out of the military and we were saying, oh, hey, you ex-military guy, you got to learn more critical thinking.

And that was kind of insulting because they were senior people.

They're like, hey, we're critical, but really they didn't.

And one of the things I started saying is don't tell people they have to have critical thinking.

That's not what I'm talking about.

They probably have to be more curious.

And it's kind of interesting that we're getting now to these systems having a map.

exploration possibility a maximum efficiency in some ways with curiosity to me that really informs a lot of teaching because if you have people who develop a curiosity over their lifetime they take care of being educated they continuously are learning and so that notion of exploration and curiosity seems like it could fundamentally through active inference

affect how we do teaching and much less of rote teaching and much more of a curiosity-based teaching because of the power.

You'll never know what you're going to encounter out there in the world.

And so telling your children, hey, be curious, will intrinsically serve them versus teaching them a rote series of things to respond to their environment.

So just a couple of comments on those last two things I thought were fascinating.

Thank you.


SPEAKER_03:
Yeah, that's really interesting.

So as you said, and I think we all agree, information, or maybe 70 years ago or 80 years ago, we didn't have any kind of information theory.

Information theory wasn't a thing.

Computers weren't the thing.

And we didn't really think about information, even though it was there in books and everywhere around us.

And I think with the advent of computers and information theory,

just information became a big thing and people realized that it was important.

And now everyone thinks about everything in terms of information.

And I, and I think that's the right way to look at things.

I mean, it seems to be, um, yeah, information can encode just about anything by, by definition.

Um, and so physics, uh, or at least our, our descriptions of the world there, the, um,

Matter was the most important thing, matter and the waves.

This is how it all started.

And then.

And then information geometry and information theory came into play and people started looking at these things in terms of information as well.

But I think matter remained the big thing.

But more and more people are looking at information in terms of like the

the most important concept, at least the root concept from which, um, from which you can like encode or explain everything else.

And, um, so, so yeah, information geometry came around and it enables you to see how, how information is flowing over time and, uh, how that relates to heat and things like that.

And now, um,

People are looking at how information may be encoded by things about other things.

And this is really Bayesian mechanics.

Bayesian mechanics, the root of the word is beliefs about things that are held by other things.

And I think this is, yeah, I mean, Bayesian mechanics, I guess, started out, I mean, in a way, it started out long ago with like Bayesian inference.

But Bayesian inference started out as an abstract statistical concept, and now people are trying to relate that to physics.

And yeah, so I think we're moving towards a physics that's information-based and based about beliefs about things, encoded about other things.

So this is really Bayesian mechanics, information geometry.

And then what you said about curiosity was also really interesting.

And so, yeah, definitely.

I mean, in AI, people are tuning curiosity.

So, yeah, they're trying to optimize curiosity of agents with respect to different environments.

In a paper that we have called Active Inference on Discrete State Spaces as Synthesis,

we, in one of the appendices, we justify the expected free energy objective that people use to explain decision making and biological agents.

And we actually show that

We could have, so the expected free energy for those who don't know, is basically ways exploration and exploitation.

So exploration is really the curiosity.

So he has these two terms.

And in general, I mean, these two terms are put on the same footing.

But actually in the appendix of that active inference synthesis paper, we show that you could weigh them in a different way.

And I mean, yeah, the agent would still work somehow.

It will have a different behavior.

So it could be a lot more curious or a lot less curious.

And that would be really interesting to quantify in terms of concepts of information geometry.

So this is to say that you can actually tune that and have agents that have different curiosities.

And also it made me think about what you said about things from computational psychiatry, where people explain diseases, like computational diseases, as people having different priors in their genetic model.

and seeing how that would affect behavior.

So you can explain suboptimal-optimal and suboptimal behavior with the same principles just by changing the model.

And we also have a recent paper on Bayesian brains and the Rennie divergence, where we're basically showing that by doing variational inference a bit differently,

You can actually account for agents that are optimal, that make the right decisions, and agents that maybe are a bit suboptimal in terms of curiosity and so on.

So I think there's a lot of exciting computational work to be done in terms of characterizing different behaviors.

And I think we're really at the beginning.

And ultimately, all these things will be able to, I mean, people will be able to account with information geometry.

I think that's the right framework.


SPEAKER_01:
Thank you, Lance.

Just quick follow-up, Scott, go for it.

Yep, quick follow-up.

Two things.

One, the Bayesian mechanics, when you say beliefs about things that are held by other things, that feels nice with my example of the farmer and the seeds because it's an embodiment.

Each thing that embodies something is, the embodiment is a belief in a sense, right?

Because it's holding, a seed doesn't have a belief itself, but it's embodying information.

And so it's interesting to think about belief itself

things and then the embodiment of information in the thing intrinsic affordance that the thing offers right that is that was a relationship between belief and

an intrinsic physical affordance that's in a system that is the result of an earlier information system.

That's one follow-up.

Then the second follow-up is the way I soften people up when you talk to older people and you're like, oh, it's all information.

It's not physical.

We're information beings.

The way I soften them up just to share is I say to them, do you have a 401k or retirement plan?

Most older people are starting to think about that.

And then they say, yeah.

And I say, do you think it's a pile of groceries waiting for you to retire?

Because it's not.

It's just data in a system.

And if that system goes away, you got nothing.

So it's that idea of getting people to understand how much we depend upon information.

And then people start thinking, oh, wow, I guess I really do depend upon all sorts of other systems in my life.

It's something that as physical beings, it's hard for us to realize how much we are information beings.

And that's, again, just part of that rhetorical exercise to get people in a headspace of saying, wow, maybe things really are information that I thought were solid, like my retirement plans.

But really, they're just totally dependent on systems and the information output of those systems.

Thanks.

Nice, Scott.

Thank you.


SPEAKER_00:
So, Blue, with a question, go for it.


SPEAKER_05:
So you mentioned a paper that you, well, I don't know if it was your paper, but about information geometry and active inference.

Is it this one that's Markov Blankets, Information Geometry and Stochastic Thermodynamics?

Is that the paper you're talking about?


SPEAKER_03:
No, that's not this one.

So if you're referring to the one about how inference is efficient, like quantifying efficiency of inference and how much energy you need to do a kind of inference.

So that's not this paper.

It's another paper called Neural Dynamics Under Active Inference, something like that.


SPEAKER_05:
So I have some questions and I don't know anything about information geometry.

I mean, just a little tiny bit.

So it's something that I'm definitely starting to get very interested in because I think that it's going to be maybe like the next level of where active inference goes.

Maybe, maybe.

I mean, I just have the suspicion.

So when you have information and in an information geometry, like in an information flow,

I've been reading other papers about information flow in channels, right?

So like when you, and I mean, it makes sense because when you have, when you know something about the current state, I mean, it's just like based in inference.

Like your belief about the next state of the world is based on the state of the world right now, right?

So, I mean, it doesn't make sense that you would think, you know, all of a sudden I'm going to be like on Pluto based on my current information.

And so,

Does information geometry, do the channels appear when you look at inference and information geometry?

Does it appear to be like a trajectory?


SPEAKER_03:
That's a good question.

So I think information geometry is more general.

You don't need to have a channel.

It's really about, at the heart of it, it's really about having different probability distributions.

And these are actually beliefs about stuff.

and looking how they differ from each other and like movement, how probability distributions change over time, these kind of problems.

But then, so you're right to say that information theory started out with the study of channels, communication channels between different things.

and you have a Yeah, there's a lot of work on information theory and channels and still today with in engineering So I would I would guess there's a lot of work also on information geometry with channels but that would be like a subset of the whole field of information geometry and I would say it's it would be

Yeah, I would say that people contributing to that literature would be mostly from engineering or around engineering.


SPEAKER_00:
Thanks.

Blue, anything there?

Yeah, go for it.


SPEAKER_05:
Yeah, if I could just follow up really quick.

So I know about information theory and the flow of information in the channels, say like a telephone line or whatever, some direct method of communication.

But the way, I don't know if I'm thinking about it in the wrong way, because when I try to search for information flow in a channel, and maybe it's the same, but I think about it in a different way.

I think about the channel flowing from

like the present into the future, right?

So like there's only so many like next states that are possible, right?

Like based on my present state.

And so is it incorrect or is it correct to think about like not a direct channel as in I'm speaking to you over this internet information channel, but like the channel from now into some future state?

Is that the same kind of like mathematical engineering information channel?


SPEAKER_03:
Right.

Yeah, that,

Yeah, you're catching me off guard because I don't know about channels.

So, I mean, what you say sounds plausible and I would say yes, but I wouldn't be able to say for sure just because I don't know that literature.


SPEAKER_00:
Thanks.

Well, I'm sure I come back to this in the weeks and years to come.

So, Stephen, go for it.


SPEAKER_04:
Yeah, I'm really interested by these different, tying these bits together in a way, I suppose, that sort of come up.

We've got this sort of,

information geometry that i suppose can come from an ergodic space or kind of a state space and then you've got solenoidal flows within maybe an isocontour of that or potentially i suppose it doesn't have to be solenoid or it could be like an isocontour flow if it was more complex so i mean um and when you've got that potential to um

go and have a solenoidal-type flow to sample, would that be a slightly more of a second-order sampling approach that something might use?

So you've got, you know, the distribution could just emerge from a steady space, steady state sort of ergodicity process, and you would maybe...

somehow systematize a flow or way of engaging that information.

And I could imagine that would then

make sense of some of the entropy consuming potential of a living system, which is that it can kind of go upgrade in descent.

And the solenoidal flow then gives Scott's sort of entropy secreting option.

It seems a bit more viable then, because maybe I could choose multiple solenoidal samplings of information geometries and

secrete the ones which aren't as good, if that makes sense.

So I'd be curious in terms of how the ability to go to more second order understandings within a system and within the dynamics might be offered by the solenoidal sampling, maybe even taking into account things like the landscape used on the mountain car

problem.

You know, that landscape, if you can't sample the whole landscape ergodically, could you have lots of solenoidal samples or something like that around the landscape?


SPEAKER_03:
Yeah, it's a really good point.

So there's deep connections between solenoidal flow and information geometry, and mostly in the context of the, as you pointed out, the sampling literature.

So the idea is that if you start a stochastic process somewhere and you have solenoidal flow, it will converge to its steady state a lot faster than if it didn't have solenoidal flow.

So then you can quantify that in terms of information geometry, maybe taking a shorter path to the steady state and things like that.

So then there's also a really nice new paper actually called, is there an analog of Nesterov acceleration for MCMC?

And so that paper really does a nice job of illustrating some deep connections between stochastic processes and information geometry.

And so the idea is that if you start with a very simple process called overdamped Langevin dynamics, then you can show that

actually the the dynamic in information space is doing um so like the process is doing a gradient descent on kl divergence

to reach the steady state, so the target distribution.

So this process is exactly doing like standard variational inference.

But then if you add some solenoidal flow, you actually make that convergence happen faster.

So it's like an accelerated method.

And so in terms of biology, you can say that if you have solenoidal flow, your inference happens faster.

So you're able to infer things a lot faster.

So you're, in a sense, more intelligent.

I think we talk about it in one of the figures in the part of the paper.

And so in that paper that I just mentioned, they show that with a particular stochastic process actually implement some kind of accelerated variational inference.

So yeah, there's some literature about that and it would be interesting to see those processes that perform inference very fast

whether when we put them in the basal mechanics contest, whether we can say, oh, these actually can be seen to correspond to biological systems that we've been studying, things like that.


SPEAKER_04:
Can I just ask one quick question on that?

When you say adding solenoidal flow, is that adding it in the sampling methodology, or is it somehow maybe, I suppose it's a bit, could it be thought of physically, or is that, how does that work?


SPEAKER_03:
Yeah, definitely.

So the solenoid flow is really this time irreversible part.

So intuitively, it's this kind of like stirring or mixing the system.

So maybe if you have a really nice example, actually, is if you have a cup of coffee and you pour milk in it.

Now what you could do is just wait for the milk and the coffee to mix together.

And this would be no solenoidal flow because you're not mixing anything.

Like it's just happening on its own.

And you have a dynamic like in the figure that we saw before, like on the bottom left.

So it would be time reversible.

Um, and this would like actually converge to the steady state pretty slowly.

So in terms of information geometry, you would have a slow dynamic to the steady state.

In terms of statistic, you would have a slow variational inference because you reach your target distribution very slowly.

Now what you can do is adding in solenoidal flow.

So this would be, I take my spoon and I'm going to stir the coffee and the milk together.

Then we all know then.

that this is going to allow the system to mix and to actually become uniformly mixed a lot faster.

So mathematically, it means that we reach the target distribution of the steady state faster.

So the information geometry, we're also faster.

And in terms of statistics, you can interpret that as doing variational inference faster.

Now, this interesting thing about adding insulinoidal flow

is that in physics, it's called breaking detail balance.

And breaking detail balance is about you actually provably need to input energy from the outside.

So this is the physical meaning of the time irreversible part of the dynamic.

When there is some time irreversible part, it means that there is

actually energy coming in from the outside.

And this is also like, it ties in nicely with biological systems, we know that biological systems consume energy from the outside.

So we know that they have to be that they need to have some kind of this irreversible dynamic.


SPEAKER_00:
Thanks, Lance.

So anyone else in the live chat who wants to ask a question, this would be a good time to go for it.

And then Dean, and then anyone else?


SPEAKER_02:
Let's be gentle on me when I'm about to put it out there.

I just feel really vulnerable, but I'm going to throw it out there anyway.

So lots of chatter about the encoded aspect of this, and I think we all kind of understand what that means.

So I'm going to raise a metaphor kind of around negative space and the idea of threading a needle.

which is, if you think about it as a distributive exercise, is plenty complex.

Like there's a lot of math in terms of being able to just do that thing that's hard, hard enough.

So if I'm trying to explain to somebody else a Bayesian manifold and what that represents, can I say that it's both a barrier and a filter at the same time?

Can I say that it acts kind of like a sieve?

Meaning that its mathematical structure kind of acts as a ground to the physical things that we are inferring are going on in the world.

So I guess what I'm asking is, is statistical mechanics serving to allow us to understand what negative space is as a non-form within all

as part of all the forms that we're seeing the edges of and having a liminal understanding until we get to the very limits and the high definition of what that means.

does the statistical manifold allow us to go in around and pass through zero?

It doesn't allow us to reverse the spin of the solenoidal, but it does allow us to get away from zero and move closer to what the known is.

Because if we continue to kind of go back and forth between the physical math and the statistical math, I know why we have to kind of talk

both at once like we have to have two hands to clap but i think it's i think what ends up happening is in this conversation the encoding and the physical piece tends to dominate and then and the non-physical stuff the statistical stuff maybe that ground that the statistical manifold allows for doesn't it hasn't maybe been fully fleshed out

in a way that helps me help explain it to other people who aren't as far along in this journey as you are.

And I'm hoping to get to a place where I can translate it for them.

So maybe you can just talk to that a little bit about what is the statistical manifold?

Is it just the hard stuff that we see or should we be paying attention to maybe the spaces between all of those data points?


SPEAKER_03:
Right, yeah, so,

That's a really interesting and actually a hard question to answer.

But just to start with the easy part, the statistical manifold is the mathematical object that says these are all the beliefs, all the different beliefs that you could hold about your environment.


SPEAKER_02:
Right.


SPEAKER_03:
And so as time evolves and you're gathering new information,

your what your brain is doing is moving on this statistical manifold your beliefs are changing over time and then in terms of um explaining it to people so the always the the way the way that i try to explain it is that this um this like in information talk um things encoding things uh things encoding information about other things

It's just a different description of the world.

So we have a description that takes like matter and waves and forces and things like that.

And then we have a different description that looks at information.

And I think these two descriptions are equivalent.

It's just two different ways to talk about the same thing.

Um, on the other hand, this, um, way, way of talking about the world in terms of, uh, information flows and information processing.

I think it might be more general in the sense that instead of needing to have like matter and all these different concepts, different kinds of particles, and it gets really messy.

Um, maybe it would be possible to unify more with the concept of information.

Because with information, you can explain what a photon is and what a proton is and you name it.

So it would be more unified and maybe more mathematically simple at the end of the day, but that remains to be seen.

Interestingly, so does this guy, the computational engine, evolve from alpha?

So this guy is actually a physicist.

I didn't know that until very recently, but he

He's been doing some really interesting work over the past few years, working towards a theory of everything that is based on the concept of information.

So information and computation is the thing he starts with.

And I think he's managed to... He basically postulates that the world evolves in time

according to some kind of computation that's recursively applied to itself.

So you, you can think of it in terms of a dynamical system, but so it's like the way he thinks about it is a bit more general, but it would be like maybe, um,

you give a number to a program and then the program outputs a number.

And then, so that would be one time step.

And then you reapply the program to that number.

That would be another time step.

And so he kind of, he, and so he postulates that the universe works in that way.

It evolves according to like discrete steps with a repeated program.

And the program would be the laws of physics.

It's like kind of an abstract informational way to look at it.

And so the.

So, yeah, so he makes this this very abstract approach to physics and and also very simple, I mean, very kind of unified approach to physics.

And I think he's managed to derive approximations to the equations of relativity just with that.

So with a certain kind of program, I mean, anyway, he's been doing some really interesting

work trying to unify known physics with the concept of information and he's written a very nice uh article like a blog post about it i think if you if you google like uh wolfram theory of everything you might you might come up you might come up um but yeah it's um i mean it's not it's not technical

I mean, it is technical somehow, but it's not like there's no equations.

It's conceptually, I think, conceptually hard what he does, but the way he explains it is nice.

So, yeah, I think we're moving to this kind of information-based physics, but it doesn't mean that the physics that we know would matter and so on should be discarded.

uh it's just like different different levels to talk about it just like we have um psychology that talks about the brain uh from a like very overall perspective and then we have like yeah talking about like maybe biases uh our biases decision making so that would be one description of the world and then we have molecular dynamics that will look at the brain from a very

from a basic molecular level.

And all these descriptions have to fit together somehow.

And I think depending on what we're trying to ask, one description would be more fit than the other.

So if I'm studying a human psychology is probably much more useful than molecular dynamics.


SPEAKER_00:
Thanks, Lance.

One quick comment on the information is it allows us to drop into a mode of matter and organization as information, which is still within the objective stance, but also it allows us to model ourselves and our beliefs and thinking about them informationally about other beliefs or about matter, which is a way of mathematically complementing the relational stance.

So it also moves us towards Wolfram's vision of what new science would look like, because we actually have quantitative relational techniques that help us deal with our uncertainty and about the way that we interact with the world, actively interfering and measuring and manipulating, not just like us walled off as the investigator trying to get to the most exact answer.

framing of a system.

So it's really interesting developments.

So we have time for some more questions.

So Scott, and then after that, we'll go to Stephen.

So go ahead, Scott.


SPEAKER_01:
A couple of little thoughts on that last bit.

So that multiple dynamics that you were talking about, very interesting.

It got me thinking about that coast of England problem.

from fractals where if you measure the coast of England with a one-mile yardstick versus a one-inch yardstick, if it's a one-inch yardstick, the coast of England is longer.

right?

It's because you're measuring more of the nooks and crannies, right, in the fractal coast.

So that scale dependence, when you're talking about the dependency, we see nature and see biology through different lenses.

It's kind of interesting.

If we have a unifying lens, maybe you start to have a fractal and scale independent descriptors then will pop out of that because you'll start to have things that'll

Anyway, it feels like we're coming at descriptions of systems, as you were saying, in different ways.

They're the same system, ultimately.

And so there'll be an interesting analysis on the way in which they come together, I guess, is that this one piece.

Another thing, the mention about information and the...

that kind of Wolfram idea, that expansion.

One of the things I had a conversation with a couple of astrophysicists, and I was talking to them about Seth Lloyd out of MIT, part of his book called Programming the Universe.

He talks about

the idea that all interactions since the Big Bang have increased exponentially, just the number, the volume of interactions.

And I was talking to somebody about it, and they said, well, if the universe is made of information, then the exponential increase in information, maybe that's what dark energy is.

We perceive dark energy, that expansion force, as being something, but really what it is is the universe expanding out exponentially.

Because it's made of information.

And so it's something just to think about.

It kind of leaps around there between physics and information theory.

But that's one other piece of it.

And then the other piece of it was this idea of the outside energy you were talking about before, the need for outside energy.

One of the things that I've been fussing around with is whether, in fact, and in 2013 I did a presentation at MIT called Entropy Accounting.

and then the year after they did something called entropy engines and the idea that was that was that in information theory in carnot's equation you need a hot and cold differential to perform work um carnot's equation those differentials are identical to arbitrage differentials you need for a market to perform to move if there's no information differentials you don't have trading in markets and so one of the questions is

Roger Penrose wrote a book recently about the time before the Big Bang.

What he talks about is we have the yellow light of the sun come in and the infrared light go out.

We don't overheat except for climate change and carbon dioxide blankets.

because the net energy gain is zero, right?

Energy in and energy out, otherwise we'd overheat.

He said what we gain is negentropy because the yellow light of the sun has a higher information carrying capacity than the red light that goes out.

Plants construct themselves from the yellow light of the sun, create negentropy, and then we now, projecting forward, are now burning that fossil fuel that stored up that negentropy and releasing disorder in the form of

now climate change, disorder, political and social disorder.

So where I'm jumping around there is suggesting that disorder, accounting and entropy accounting, when you have von Neumann and Shannon's link of physics and information and other links like we have here,

that maybe when we're talking about entropy as disorder, we can talk about it not by analogy, but by direct application to social contexts and start to use information and understand disorder in society and in markets as amenable to the same kind of math that we find in some of these biological systems.

Thanks.

Just a few thoughts.


SPEAKER_00:
Thanks, Scott.

Lance, do you want to add anything?


SPEAKER_03:
Sure.

Yeah, that was very interesting.

And yeah, so I want to add actually that all this, the thing that we talk about, about like reality can be described at different scales.

And I think there's a nice extension to this work that speaks to that.

If you look at a human, I mean, it's composed of a lot of cells and each cell has its own boundary.

And then these cells form organs that themselves have their own boundary.

So there's really this recursive aspect of like Markov blankets being assembled together at different scales.

And I think

I think it would be interesting to unpack that because I think that's how biological systems are formed, but also societies are formed as collections of multiple different individuals that themselves form organizations that talk to each other and form together a society.

So it's really this recursive aspect and being able to hopefully use the same kind of math to talk about all these different aspects.

And also, I mean, people, it's true like a lot of people in the past, maybe 10, 20 years, maybe more, they've used the same kind of dynamical models that we have in this paper.

I mean, so very, very standard models to model, for example, interactions between people.

and in particular models of opinion formation.

So for example, maybe let's say we're in the US and some people are Democrats, some of our Republicans, some cannot really make their mind yet.

And so they would talk to each other, they would interact.

And then they would converge into a steady state where some, yeah, some people would be diehard Democrats in that state.

So they would kind of stay in that area.

Some people would be diehard Republicans and some people would like,

fluctuate in the middle.

So I think we can use, I mean, this math about stochastic processes and information is really the, yeah, the ground, I mean, the

a good framework or useful framework to model many kinds of things.

But then I think what a lot of these models are missing is really this belief-based aspect.

So a lot of these models of opinion formation, they just say, oh, I'm a particle, you're a particle, and we...

interact, just like particles in physics will interact.

So it's quite simplistic.

So I think once you, I mean, things are very small, maybe we can describe them in a very simple way.

But as you, as like Markov blankets self-organize at different scales, and as you go to larger and larger scales,

things become a lot more complex.

And I would say maybe we are the scale at which things are most complex.

Molecules tend to be quite simple.

We tend to be quite complex.

And then if you go up and up again, maybe to planets, then things start to become really simple again.

Because all these random fluctuations are kind of averaged out.

And then we have Newtonian mechanics where everything kind of looks deterministic.

So I think there's this sweet spot we're in where we're not too big and not too small, and interesting things happen there.

And that's where I would say Bayesian mechanics is hoping to fit in, in terms of explaining and describing what's going on.


SPEAKER_00:
Cool.

So Stephen, go for it.


SPEAKER_04:
Yeah, that idea, again, that brings us back to embodiment, I suppose, because we're embodied at a particular size and scale.

And they sort of say if you go right down to the smallest scale and then right up to the biggest scale, we are kind of in the middle.

um ironically but so i was wondering how sort of following on from what you're saying and bringing in the computational psychiatry piece that you sort of alluded to to this is if we if we say that we've got

everything that we perceive, even the physicality of the world, we actually only have access to it through information anyway, even if we are actually accessing the true physical world.

So we've got this process of sense-making that we're doing to make that idea of what is our consensus reality.

So if our kind of working space that we're in is our consensus reality, what's interesting is that

With psychology and social sciences, you could almost say psychology is a bit like the predictive processing model of the world.

It's like these are predicted ways that are out there that have been chosen as the models within a particular sort of discrete spaces.

And those are externally managed.

So it's like an externally managed model.

system entry percent there could be an entry repeat secreting system where it looks like there is although entropy is not actually a substance to secrete per se and you could say that it's externally managed information geometry that the social science as well with their averages and the psychometrics that they reference to they're all kind of housed out there in the world and you've kind of got

the kind of what we see as our reality and the phenomenology of our experience is kind of the boundary but what your work's doing and what computational psychiatry does and which we're not able to do without those tools is it allows us to go into that neurobiology and the actual like the internal

internally managed stochastical dynamics of our own models so to speak so not the ones that we make out there to be our perspective on which we try and reduce free energy around but just what is the models that we're doing what you could call entropy consuming i we're actually just swimming in that entropy and finding the variations right we and so i'm curious like

That ability for this type of work to contribute to a new kind of way of knowing about what it is to be in the world and go into our internal models rather than be reliant on these external psychological and social science models.

What's your thoughts on how this might link into developing that field?


SPEAKER_03:
So are you talking specifically about the link between all this physics and the psychology, or is your question a bit more general?


SPEAKER_04:
um well maybe the the boundary between the kind of like the scales of basically the boundary of going for externally making models of the world and standard models like psychologists are doing and social sciences are doing and psychometrics and the ability to actually go into our own models our own phenomenological neurobiological models and just say okay what are the dynamics

that are alive, not what, and how, how are we actually managing not how do we manage our models of what we think they are, which is kind of what happens in our kind of niche, but we're able to probe our own phenomenology.

So I was wondering how you see that tying into that.


SPEAKER_03:
Yeah.

Um, yeah, that's a, that's a really good point.

So, so as you, as you point out, I mean, now models in like psychology and

And computational psychiatry, I mean, they're driven by intuition, they're driven by experiment, and they're driven also by our own biases and our observations about patients who suffer some kind of...

Yeah, some kind of, let's say, schizophrenia.

And so we know schizophrenic patients behave like this and like that.

And so we try to create models that kind of reproduce that.

And then we try to validate these models by saying, oh, maybe I can simulate some kind of action potentials of some neurons.

And then I'm going to check whether actually these organisms that I'm studying have this...

same kind of action potentials or same kind of electrophysiological responses.

So I think that I mean, I think that's really good.

And the research is doing a lot of progress.

But then this kind of work, not this paper, but this line of work would be able to to like go the I mean, I mean, takes a different perspective.

and would be able ultimately hopefully to describe mathematically what thinking like a human is what what is being human mathematically and of course there won't be like a definition of this is a human but there would hopefully be in the future a mathematical description of how we make decisions and what class of models are we implementing to make sense of the world

um and how can we characterize these models um in terms of and and hopefully um the community will get to that through through physical experiments but also through like just mathematical conceptualization um i think you see using mathematics you can you you cannot get you cannot just get there without any like kind of experiment but you still can

There's still quite a few constraints that we know that humans have.

For example, breaking detail balance, creation of entropy, all these things.

We know humans satisfy like... I mean, they have a lot of characteristics and we can incorporate that in the physics.

And I think this really constrains the model space.

that then people in psychology can go and look into.

So it just feels like when you're digging a tunnel and psychology would like kind of dig in one way and Bayesian mechanics would be digging from the other direction and at some point they will meet in the middle, hopefully.

But I don't think they have met really yet and it will take a while.


SPEAKER_00:
awesome um what a fun discussion and it's so cool how this paper it's almost like there was a few stairs we we explored some basic ways to frame how nodes on a bayesian graph insulate other sets of nodes and then how that conditional independence can be finessed for these increasingly developed and elaborated

networks that do, as we talked about, expectations over external states all the way up to variational base.

And then it's like we, at the top or at the middle of that staircase, started taking it in a lot of other directions.

So this was really a fun discussion.

Just in the last few minutes, maybe we can, any and all of us think like, what are we gonna continue working on?

For some of us, that's probably a research project.

Others, that's, I don't know, lunch or something else.

Just how are we going to keep on learning about this and start to apply it in our own work or research?


SPEAKER_05:
So I have a really quick question.

Lynn, are you working on the next paper where you're going to partition the external database?

Or have you started or it's just like clicking on a back burner somewhere?


SPEAKER_03:
It's not cooking yet, so you can...

you're more than welcome to go ahead and start plowing.


SPEAKER_00:
It's at the farmers markets.

The ingredients are still out there.

And Dave also raised in an email some really interesting questions about other partitionings and

basically philosophical threads that might be translated into different possible partitionings within internal states or other ways of partitioning because it was the partitioning of the blanket states from kind of one set of blanketing states into incoming sensory and outgoing action active states that allowed for this perception as inference action planning as inference

step and it's not that that's the end game of node partitioning so there's so many ways that these partitions can continue steven yeah i'm gonna be i'm trying to see how i like that analogy the two tunnels or whichever way the


SPEAKER_04:
Maybe it's a whole load of insect warrens or whatever it is trying to meet.

So psychology and bringing together some of these processes.

So I'm going to try and bring that together.

Maybe more actually, not so much from a clinical perspective, but maybe more from a community psychology and psychosocial methodologies that might be used.

So I'm going to be chewing on that as best I can.

So maybe if I ever stuck, I might fire you an email if that's okay.


SPEAKER_00:
yeah um there's many ways we can continue and also i think all along our trails we dropped a few little notes for some other pieces that could come into play if anyone else wants to raise their jitsi hands we can take a last thought

Otherwise, though, just in closing, Lance, this was awesome.

And Connor as well for joining the first live stream.

This was a great development.

We all learned a ton.

So I hope those watching live or in replay also got something out of this paper.

You're always welcome back.

Anytime you want to jump on a guest stream or be a participant, just always welcome.


SPEAKER_03:
Thank you so much, Daniel.

And thank you everyone for the discussion.

I really enjoyed it.

Okay.


SPEAKER_00:
See everyone next time.


SPEAKER_04:
Thanks.


UNKNOWN:
Bye.


SPEAKER_00:
Great times.

Thanks, everyone.